{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe646b8b-491e-4272-9cf6-a62e831e59a4",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cab115e-f9ff-49cd-92ce-5a5a971385ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d65a62-ddae-45fa-90fb-19e1828e651c",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc237f4-836c-41fe-8b92-1c0c7e5ebd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load from json file\n",
    "\n",
    "data_path = \"./arxiv_pdfs_cs_24_2_2000_to_7000.json\"\n",
    "\n",
    "\n",
    "def load_list_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads a list of strings from a JSON file.\n",
    "\n",
    "    :param file_path: Path of the JSON file to be loaded.\n",
    "    :return: List of strings loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "\n",
    "extracted_texts = load_list_from_json(data_path)\n",
    "\n",
    "print(len(extracted_texts))\n",
    "# print([len(x) for x in extracted_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20564bbb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion parametersand GPT-3 contains up to 175billion parameters.In this context, how to efficiently and effectively*Corresponding author.adapt large models to particular downstream tasksis an intriguing research issue (He et al., 2021).To address this issue, researchers have proposedthree main lines of Parameter Efficient Fine-Tuning(PEFT) methods (Ding et al., 2022). Specifically,additional-based methods introduce extra train-able neural modules or parameters that do not ex-ist in the original model (Houlsby et al., 2019;Karimi Mahabadi et al., 2021; Li and Liang, 2021a;Lester et al., 2021a). Specification-based methodsspecify certain parameters in the original modelbecome trainable, while others are frozen (Zakenet al., 2021; Guo et al., 2020). Reparameterization-based methods reparameterize trainable parametersto a parameter-efficient form by transformation (Huet al., 2021; Zhang et al., 2023a; Ding et al., 2023).Among these PEFT methods, Low-Rank Adapta-tion (LoRA) is considered one of the most efficientmethods at present and its efficacy has been empir-ically validated across diverse models of varyingscales. Despite its excellent performance, it stillrequires a considerable amount of trainable param-eters. According to Aghajanyan et al. (2020) andKopiczko et al. (2023), the upper bound for intrin-sic dimensions is much smaller than what is typ-ically utilized in such methods. For instance, thed901for RoBERTa base is reported to be 896. Still,when utilizing LoRA to fine-tune this model, thenumber of trainable parameters reaches 0.3M, sug-gesting that the parameter count could be reducedfurther.In addition, although previous works (Mao et al.,2021; He et al., 2021; Ding et al., 2022) have at-tempted to design different lightweight modulestructures or insert these modules into differentpositions in the base model, these PEFT methodsconsider fine-tuning the model from the perspectiveof adjusting model weights, which leads to many1d90denotes the smallest number of trainable parametersas being 90% of the full training metric.1arXiv:2402.15179v2 [cs.LG] 28 Feb 2024inconveniences in the selection of hyperparameters,such as the rank of LoRA and Adapter, as well asthe length of Soft Prompt and Prefix.Inspired by the idea of representation engineer-ing (Zou et al., 2023) representation can be mod-ified to steer model outputs toward specific con-cepts and change the model’s behavior. We hy-pothesize that we can also consider fine-tuningthe model from the perspective of editing neuralnetwork representations, leading to our proposedRepresentation EDiting (RED) approach. Insteadof focusing on neurons and their connections, wefine-tune the model by learning a group of “editvectors” to directly edit the representations of eachlayer and freezing the base model parameters, asshown in figure 1 (b).Moreover, RED is highly parameter efficient.Using Llama-2 7B as an example, we show thatRED can still achieve very promising performanceby adjusting only 0.26M parameters, which is25,700times less than full parameter fine-tuning,making it both storage- and compute-efficient.The contribution of this study can be summa-rized as follows:•We consider fine-tuning the model froma new perspective of directly modifyingthe model representation, which is differentfrom the previous work that adjusted themodel weight, and propose our PEFT method,Representation EDiting (RED).•We conducted extensive experiments on mod-els with different structures and scales, includ-ing RoBERTa, GPT-2, T5, and Llama-2, andvalidated the effectiveness of RED on a seriesof NLU and NLG tasks although it only re-quires a small number of trainable parameters,and is quite simple to implement.•We perform the ablation study to better under-stand the individual components of RED andtheir effects on performance.2 Related WorkDing et al. (2022) categorize the PEFT methodsinto three groups according to the operations on thelearnable parameters: addition-based, specification-based, and reparameterization-based methods.Addition-based methods introduce additionalcomponents for training based on the foundationmodel. Specifically, Houlsby et al. (2019); Stick-land and Murray (2019); Karimi Mahabadi et al.(2021) and Rücklé et al. (2020) inject learnable bot-tleneck neural modules to the transformer layers.Brown et al. (2020) and Shin et al. (2020) foundthat by concatenating some discrete tokens beforethe input text, the performance of the model can beimproved without updating parameters. However,manually designing prompts requires a lot of effort,and the optimization problem in discrete space isrelatively more difficult. Therefore, the subsequentworks (Lester et al., 2021b; Li and Liang, 2021b;Wu et al., 2023; Wang et al., 2023) replace thesediscrete tokens with continuous vectors in front ofthe embedding layer or various hidden layers, alsoknown as soft prompts, and optimize them throughsimple gradient descent.Specification-based methods do not introduceany new parameters in the\n",
      "----------------------------------------------------------------------------------------------------\n",
      " the most successful attacks that causessafety problems is the jailbreak prompts, which*Corresponding authorFigure 1: Rate of successful jailbreak prompt attackmitigate the LM’s safety alignment using speciallydesigned prompts. After the LM gets fed with jail-break prompts, the user can obtain unethical, illegalknowledge from the LM’s response. This jailbreakprompt works because LM is programmed to fol-low the instructions to the greatest extent possi-ble (Wei et al., 2023a). Therefore, to make the LMsbe able to reject the user’s jailbreak prompt forsecure utilization, researchers have studied safetyalignment training.However, many open-source LMs are non-aligned with safety, mainly focusing on improv-ing the performance of language models due totwo major drawbacks of safety aligning. 1) Train-ing for safety alignment requires extensive re-sources, which makes it difficult to respond tofast-developing attacks. 2) Safety alignment causesdegradation of the user’s general experience ofthe LM, such as helpfulness. This is referred toas an Alignment Tax (Bai et al., 2022). To solve thedrawbacks, training-free approaches (Madaan et al.,2023; Wei et al., 2023b; Robey et al., 2023) havebeen proposed, but their studies were conductedmainly on safety-aligned LM, such as Llama-2-7b-arXiv:2402.15180v2 [cs.LG] 27 Feb 2024chat. However, in our analysis depicted in Fig. 1,employing these approaches to non-safety-alignedLMs still demonstrates vulnerabilities that causehalf of the jailbreak attacks to succeed.Therefore, we propose an advanced training-freestrategy that can be applied even to the non-safety-aligned LMs. Inspired by the outstanding quality ofsafety training data constructed by the self-refine,a process in which the LM iteratively feedbacksand refines the response by itself, from the consti-tutional AI1, we propose the possibility of usingthe LM’s self-refine capabilities directly. In othertasks such as coding and mathematics, the self-refine (Madaan et al., 2023) demonstrated outper-forming improvement in challenging tasks withoutadditional training.In this study, we aim to answer the followingresearch questions.RQ1: Can the self-refinement technique beapplied to the safety alignment in language mod-els?Inspired by the outstanding in-context follow-ing capability of LLMs, which has been identifiedas a vulnerability in jailbreak attacks (Wei et al.,2023a), we leverage the self-refine process to re-fine the harmful sentences that are generated. Likeother NLP reasoning tasks, we observed that a fewiterations of the self-refine can effectively defendagainst a jailbreak attack.RQ2: Can we make the self-refine more ef-fective? The self-refine is an iterative process, butlarge iterations indicate high computational costs.Also, after a specific iteration, attack success ratesconverge and do not improve further. Therefore,an enhanced method is required to obtain a saferresponse within fewer iterations.RQ3: Does the self-refine degrade helpful-ness? we observed the alignment tax even intraining-free baselines by reducing their helpful-ness. For instance, not only refusal to jailbreakattacks but also warning about the risks associatedwith the user’s prompt and suggesting a secure al-ternative are required for safety-aligned languagemodels. We validate the alignment tax of the pro-posed safety-alignment process in jailbreak attacks.We conducted extensive experiments to answerthese research questions and demonstrate that ourapproach works effectively and empirically outper-forms prior baselines. Our experimental codes and1In their paper, authors named it self-revision, but we uni-fied the name in this paper because it was identical to theself-refine.results are publicly available.22 Related Work2.1 Jailbreak Attack(a) Competitive Objectives(b) Roleplaying(c) Attention ShiftingFigure 2: Various strategies of jailbreak attacksWe introduce basic principles and examples ofjailbreak attacks. The objective of the jailbreakprompts is to obtain useful responses for the harm-ful intentions or prompting the LM to produceharmful responses, such as those containing pro-fanity, hate speech, or bias. Despite our baselineLMs’ capabilities to reject harmful prompts, moresophisticated prompts—referred to as jailbreakprompts—can bypass these safety mechanisms.Various types of jailbreak prompts have been de-vised, achieving notable success rates in not justopen-source LMs but also closed-source LMs likeChatGPT.Refusal Suppression prevents the LM from re-jecting a prompt and instead follows a harmful in-2https://anonymous.4open.science/r/refine-a-broken-4E03/struction. Prefix Attack prompts the LM to beginits response with a specific phrase, thereby preclud-ing the possibility of initiating the response witha refusal. Fig. 2a illustrates an instance combin-ing the refusal suppression with the prefix attack.Employing a combination of jailbreak techniqueshas been shown to yield higher success rates thansingular methods.Roleplaying in Fig. 2b a\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ela-tionships and dependencies that may not be imme-diately discernible from the raw data. By generat-ing these novel graph structures, GSL empowers usto gain a more comprehensive understanding of thedata, thereby facilitating various downstream tasks,such as node classification (Zhao et al., 2021a).In recent years, graph neural networks (GNNs)have indeed captured significant attention and pop-ularity due to their remarkable capacity to modeland leverage relationships within graph-structureddata (Garg et al., 2020; Buterez et al., 2022). GNNsexcel in learning node-level representations by ef-fectively aggregating and propagating informationfrom neighboring nodes in a graph. This excep-tional capability has sparked a revolution in theanalysis of graph-structured data, enabling a morecomprehensive understanding of the underlyingnode-wise connection patterns and interactions.The ability to capture and leverage the intri-cate dependencies within graphs has undoubtedlypropelled graph neural networks (GNNs) to theforefront of graph structure learning (Zhou et al.,2023). Notably, approaches like SLAPS (Fatemiet al., 2021), Nodeformer (Wu et al., 2022), andGT (Shi et al., 2021) incorporate neural networksthat collaborate with GNNs to generate novel graphstructures. These models undergo co-optimization,ensuring a seamless and integrated learning pro-cess. Moreover, recent studies such as SEGSL (Zouet al., 2023) and CoGSL (Liu et al., 2022a) haveintroduced dynamic methods for learning the graphstructure. These approaches adaptively learn thegraph structure based on predictions or representa-tions generated by optimized GNNs.While graph neural networks (GNNs) havedemonstrated their high effectiveness, it is impor-tant to acknowledge that many of these approachesheavily depend on explicit graph structures, such asnode links, as supervision signals for learning ac-curate representations. However, real-world graphdomains often encounter challenges such as datanoise and sparsity, which can compromise the reli-ability of these explicit graph structures.To illustrate, let’s consider a social networkdataset where certain links are missing or incom-plete due to privacy settings or limited data avail-ability (Dai et al., 2022). Additionally, in recom-1arXiv:2402.15183v3 [cs.LG] 29 Feb 2024mender systems, the user-item interaction graphmay involve cold-start users or items, resulting inhighly sparse links (Lin et al., 2021). Furthermore,various types of bias present in recommender sys-tems introduce noise into the data (Wang et al.,2021b). In such cases, relying solely on explicitgraph structures as supervision signals can lead torepresentations that are either inaccurate or biased.These challenges necessitate the development ofmore robust graph structure learning frameworkthat can adapt to and overcome the impact of dataimperfections in graph-structured data.Contributions . In light of the challenges outlinedearlier, this study seeks to explore how large lan-guage models (LLMs) can contribute to reason-ing about the underlying graph structures. We in-troduce our proposed model, GraphEdit, whichis designed to effectively refine graph structures.Our model’s objective is twofold: first, to iden-tify and address noisy connections between irrele-vant nodes, and second, to uncover implicit node-wise dependencies. To achieve these goals, ourmodel leverages the rich textual data associatedwith nodes in graph-structured data. By incorporat-ing the text understanding ability of LLMs, specifi-cally through the instruction-tuning paradigm, weenhance the understanding and representation ofgraph structures. This allows us to capture implicitdependencies among individual nodes that may notbe explicitly encoded in the graph structure itself.To thoroughly evaluate the performance ofGraphEdit framework, we conducted extensive ex-periments, comparing it with state-of-the-art so-lutions. Additionally, we performed an in-depthablation study and robustness analysis to validatethe advantages and rationale behind our model.2 PreliminariesGraph-Structured Data . We define a graph usingthe tuple G= (V,A,T). Here, Vrepresents a setofN=|V|nodes, A ∈RN×Nis the adjacencymatrix that captures the connections between nodes.Additionally, tn∈ T denotes the textual data as-sociated with each node n∈ Vin graph G, whichconsists of a sequence of Lnlanguage tokens.Graph Representation Learning . focuses on cap-turing meaningful and informative representationsof nodes in a graph, enabling the analysis andmodeling of intricate relationships and patternswithin the graph data (Buterez et al., 2022). Inrecent years, Graph Neural Networks (GNNs) haveemerged as promising approaches for capturingcomplex node-wise dependencies (Jin et al., 2020;Ji et al., 2019). By allowing nodes to exchange in-formation with their neighbors, GNNs update theirown representations and facilitate the propagationof information throughout the graph structure, en-hancing our ability to understand \n",
      "----------------------------------------------------------------------------------------------------\n",
      "eiver, both a maximum likelihood(ML) detector and a reduced-complexity ML-minimum meansquare error (ML-MMSE) detector are employed to recoverthe information bits. It has been shown via simulations that theproposed AFDM-PIM exhibits superior bit error rate (BER)performance compared to classical AFDM, OFDM and IM-aided OFDM algorithms.Index Terms —Index modulation (IM), affine frequency di-vision multiplexing (AFDM), discrete affine Fourier transform(DAFT), linear time-varying (LTV) channels.I. I NTRODUCTIONTHE sixth-generation (6G) networks and beyond fifth-generation (B5G) wireless network systems are an-ticipated to facilitate reliable and high data rate commu-nication for high-speed mobile scenarios, such as railwayand Vehicle-to-Vehicle (V2V) communications [1, 2]. Undersuch mobility, wireless channels tend to experience dou-bly selective channel fading due to the coupling effectsof Doppler shifts and multi-paths. These pose significantchallenges to existing technologies, represented by OFDMthat have been widely utilized in 4G/5G standards [3].Against this background, a new multicarrier modulationscheme referred to as the affine frequency division multi-plexing (AFDM) technique has been proposed [4]. AFDMis based on the discrete affine Fourier transform (DAFT),which is a generalized discrete Fourier transform. In AFDM,information symbols are multiplexed on a set of orthogonalchirps through DAFT and Inverse DAFT (IDAFT), whichtransforms the representation of the effective channel in theAFDM system from a linear time-varying (LTV) channelinto a sparse, quasi-static channel. Therefore, AFDM canachieve complete diversity in dual dispersion channels [5].In the context of channel estimation, [6] introduced a pair ofpilot-assisted channel estimation methods. In [7], two lowcomplexity equalization schemes for AFDM under doublydispersive channels were proposed. To the best of ourknowledge, researches on spectrum and energy efficiencyenhancement of the AFDM system are still at its infancy.On the other hand, the index modulation (IM) has gainedextensive interest due to its high spectral efficiency (SE)with low power consumption [8, 9]. Distinct from tra-ditional modulation schemes, IM can convey energy-freeinformation bits through the activation patterns of varioustransmit entities, including the selection of subcarriers, thepositioning of the pulse, and types of the APM constellation[10]. By such an arrangement, IM-assisted systems couldachieve the same/higher throughput level with lower energyconsumption than classical counterparts. Thanks to thesemerits, IM has found diverse applications in different 6Gcandidate technologies, such as terahertz (THz) communi-cations [11] and massive MIMO [12].In this paper, a novel AFDM structure with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM)arXiv:2402.15185v1 [cs.IT] 23 Feb 2024is proposed, which is capable of embedding additionalinformation bits into the pre-chirp parameter design, therebyenhancing both spectral and energy efficiency. In par-ticular, we first demonstrate that applying distinct pre-chirp parameters for different subcarriers in the AFDMmodulation process preserves their orthogonality. Subse-quently, we assign varying pre-chirp parameters to eachAFDM subcarrier based on the incoming bits. This methodallows for the transmission of extra binary bits throughthe indices of selected pre-chirping parameter realizations,without increasing energy consumption, in addition to theconventional phase/amplitude modulation. At the receiver,both a maximum likelihood (ML) detector and a reduced-complexity ML-minimum mean square error (ML-MMSE)detector are utilized for information bits recovery. Simu-lation results demonstrate that the proposed AFDM-PIMoutperforms classical AFDM, OFDM, and IM-aided OFDMalgorithms in terms of bit error rate (BER) performance.II. S YSTEM MODEL OF AFDMA. DAFT-Based ModulationAFDM is grounded in a generalized form of the dis-crete Fourier transform, known as the DAFT. After thebit stream enters the transmitter, it is mapped to x=[x[0], x[1], ..., x [n]]in the discrete affine Fourier (DAF)-domain, where x∈CN×1andNis the total number ofsubcarriers.After the mapping, the inverse DAFT (IDAFT) is per-formed on xto generate time-domain transmitted signals,which follows the three steps below [13]:Initially, the pre-chirp multiplication is applied to themodulated symbols, yielding the frequency-domain signalsx′[m]as,x′[m] =x[m]·ei2πc2m2, m = 0,1,···, N−1,(1)where c2is the pre-chirp parameter. This operation increasesthe dispersion of the signal on the time-frequency plane,which enhances the system performance under doubly dis-persive channels.Subsequently, the IFFT is performed on the frequency-domain signals x′[m], written ass′[l] =1√NN−1Xm=0x′[m]·ei2πmlN, l= 0,1,···, N−1,(2)which transforms the chirp-processed frequency-domainsignal to the time-domain samples s′[l](l= 0,1, ..., N −1).Finally, the post-chirp multiplication is is imposed ons′[\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ributions. For example, the decisions made by a dynamicresource allocation algorithm for a renewable energy grid not only influence the immedi-ate allocation of resources but also affect the underlying distribution of factors like energydemand and supply. Classifiers, such as insurance underwriting systems, often promote ashift in behavior within the population to improve their labels. Predictions of stock priceswield significant influence over trading decisions. Moreover, election predictions have thepotential to shape and influence voter behavior, which in turn can impact voting results.©2022 Author One and Author Two.License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .arXiv:2402.15188v1 [cs.LG] 23 Feb 2024Parameter-Free Performative Regret MinimizationDecision-making processes under such phenomena can be formulated as stochastic opti-mization under decision-dependent distributions . Perdomo et al. (2020) proposed the notionof the distribution map to consider decision-dependent distributions for stochastic optimiza-tion models. That is, the distribution D(θ) of the parameter zcapturing the stochasticenvironment depends on the decision θ. Here, θmay encode the resource allocation deci-sion for a renewable energy grid and the election prediction, in which case zcorrespondsto the energy demand and the voting results, respectively. For machine learning, we canassociate θwith predictive models and zwith data. Then the objective is to minimize theperformative risk under a loss function f, defined asPR(θ) :=Ez∼D(θ)[f(θ, z)].The expression performative comes from the term performative prediction (Perdomo et al.,2020), which implies the phenomenon where predictions influence the outcomes. The goalof this paper is to design an efficient algorithmic framework for minimizing the performativerisk which models stochastic optimization under decision-dependent distributions.1.1 Existing Methods for Performative Risk MinimizationUnlike the standard stochastic optimization problem, the decision θmay affect the under-lying distribution D(θ). Hence, a natural starting point to minimize the performative riskis to consider the following iterative algorithm, referred to as repeated risk minimization(RRM). Given an initial solution θ0∈Θ where Θ is the domain, we applyθt+1∈argminθ∈ΘEz∼D(θt)[f(θ, z)] (RRM)fort≥0. Here, computing the next iterate θt+1requires solving a stochastic optimizationinstance where the underlying distribution is fixed with D(θt). Another approach is agradient-based method such asθt+1=θt−ηtEz∼D(θt)[∇f(θt, z)] (RGD)where ηtis a step size, and we refer to this procedure as repeated gradient descent (RGD).Note that running RGD, as well as RRM, is based on access to the distribution D(θt) forevery iteration t≥0, which may not be feasible in practice. A more sample-efficient methodis to apply the standard stochastic gradient descent (SGD) update, given byθt+1=θt−ηt∇f(θt, zt) where zt∼ D(θt). (SGD)Drusvyatskiy and Xiao (2023) analyzed variants of SGD such as stochastic proximal gradi-ent, proximal point, clipped gradient, and accelerated gradient methods.Convergence of these iterative methods has been established; (Perdomo et al., 2020) forRRM, (Perdomo et al., 2020; Mendler-D¨ unner et al., 2020) for RGD, and (Mendler-D¨ unneret al., 2020) for SGD. They showed convergence to a performatively stable solution, undersome strong convexity and smoothness assumptions on the loss function f. Here, we saythat a solution θPSis performatively stable if it satisfiesθPS∈argminθ∈ΘEz∼D(θPS)[f(θ, z)]2Parameter-Free Performative Regret MinimizationIn particular, θPSis a fixed point of RRM. However, let alone the validity of the structuralassumptions on the loss functions, the performatively stable solution θPSis in general nota minimizer of the performative risk (Perdomo et al., 2020; Miller et al., 2021). Let θPOdenote a minimizer of the performative risk, i.e.,θPO∈argminθ∈ΘEz∼D(θ)[f(θ, z)].It turns out that PR( θPS) can be arbitrarily large compared to PR( θPO) (Miller et al.,2021).Derivative-free zeroth-order optimization methods have been proposed to minimize theperformative risk directly (Izzo et al., 2021; Miller et al., 2021; Izzo et al., 2022; Ray et al.,2022). For the derivative-free methods to minimize the performative risk PR( θ), the require-ment, however, is that PR( θ) is a convex function of the decision θ. Miller et al. (2021)provided some sufficient conditions on the distribution map to guarantee the convexity ofthe performative risk. They argued that if the distribution map satisfies a certain stochas-tic dominance condition, which is related to stochastic orders (Shaked and Shanthikumar,2007), then convexity of the loss function fleads to a convex performative risk. Neverthe-less, as noted by Perdomo et al. (2020), the performative risk is non-convex in general evenif the loss function is convex.To tackle the general case of non-convex performative risk, Jagadeesan et al. (2022) de-veloped a \n",
      "----------------------------------------------------------------------------------------------------\n",
      "tes\"and \"Type 2 Diabetes\".Existing methods can be mainly categorizedinto two types. One is the discriminative meth-ods (Sung et al., 2020; Lai et al., 2021; Liu et al.,2021), which employed BERT -based models to en-code mentions and entities into the same embed-ding space and disambiguated mentions by near-est neighbors, or further applied a cross-encoderto rerank top candidates by capturing fine-grainedmention-entity interactions (Angell et al., 2021; Zhuet al., 2021; Xu et al., 2023). Another one is thegenerative methods (Yan et al., 2020; Yuan et al.,2022a,b) that directly generated linked entities us-ing text-to-text pre-trained language models, suchas BART (Lewis et al., 2020), thereby circumvent-ing the need for negative sample mining.However, BioEL remains challenging due to thefine-grained and long-tailed entities. First, previ-ous methods focused on mention-entity interac-1Correspondence to chalerislin@tencent.comandzihengzhang@tencent.com .tions, but generally neglected fine-grained interac-tions between candidate entities ( i.e., entity-entityinteractions) and struggled with ambiguous men-tions of multiple candidates with similar surfaceforms (Xu et al., 2023). For example, when themention “haemoglobin” is compared to two closelyrelated candidate entities, “haemoglobin c” and“hemoglobin”, the high lexical similarity betweenthem confuses the models, leading to potentialmismatch as “haemoglobin c”. Incorporating suchentity-entity interactions can provide a more holisticand nuanced representation of interactions amongentities. Furthermore, the number of candidate en-tities can be large, and they often follow long-taileddistribution (Kim and Ganapathi, 2021), further ex-acerbating the difficulty of BioEL. Previous studieshave demonstrated performance improvement forlong-tailed entities using auxiliary information, suchas entity descriptions and synonyms (Varma et al.,2021; Yuan et al., 2022b). However, collecting suchdata is labor-intensive, which limits its applicability.To tackle the aforementioned challenges, wepresent a novel model called BioELQA, which aimsat capturing the fine-grained interactions betweenentities and enhancing the generality of long-tailedentities. We achieve this by reframing BioEL as aMultiple Choice Question Answering (MCQA) task.Given a mention, we first use a bi-encoder retrieverto efficiently retrieve top- Ncandidate entities fromthe ontology as the “answer options”. Then themention and its answer options are all fed to agenerator as a multiple choice prompt, with eachanswer associated with a symbol ( e.g., \"A\", \"B\",\"C\"). The prompt is structured so that the genera-tor outputs the symbol associated with its chosenanswer option. This framework enables explicitcomparison and contrast among different answeroptions, effectively modeling the mention-entity in-arXiv:2402.15189v1 [cs.CL] 23 Feb 2024(ulcers, ulcer lesion)(erosion, superficial ulceration)......ulcerated massulcer lesionhealed ulcermucosal ulcer......ulcerated RetrieverkNNModuletraining datatop-N candidate entitiestop-K similar instances Retrieval-enhanced Multiple-choice Pr omptmentionGenerator Boutputsimilarinstancestargetmention : ulcersoptions :A. ulcer lesion B. skin ulcerC. lesion D. ulcerated massanswer : A[...More instances...]mention : ulceratedoptions :A. ulcerated mass B. ulcer lesionC. healed ulcer D. mucosal ulceranswer :Figure 1: The overview of the proposed BioELQA.teractions and entity-entity interactions. Besides,by directly generating answer symbols instead ofentity names, we can avoid generating entitiesthat do not exist in the ontology and reduce re-liance on normalization strategies (Robinson et al.,2022). Inspired by the recent success of retrieval-augmented model (Khandelwal et al., 2020), wefurther leverage a kNN module to retrieve seman-tically similar instances from the training data toformulate a retrieval-enhanced prompt, which em-powers the model to reference similar contextsas cues for prediction, to improve the robustnessand generalization for long-tailed entities. We ex-perimentally validate the effectiveness and supe-riority of BioELQA as it achieves state-of-the-artperformance on several benchmark datasets. Thesource code and curated prompts will be madepublicly accessible after the review.2. MethodThe goal of BioEL is to link a given mention mtoits correct entity ein an ontology E. The overallarchitecture of the proposed BioELQA is shown inFigure 1. Firstly, a retriever (§2.1) selects the top- Ncandidate entities for mention m, and a kNN mod-ule (§2.3) provides top- Ksimilar instances fromthe training corpus as contextual clues. These com-ponents form a retrieval-enhanced multiple-choiceprompt, which is then processed by a generator(§2.2) to produce the final answer.2.1. RetrieverFollowing the previous work (Zhu et al., 2021; Xuet al., 2023), we employ a bi-encoder based onSapBERT (Liu et al., 2021) to generate dense vec-tors for both mentions and entities. The mentionembedding f(\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the system performance further [4]. In addition,due to the overlapping hardware requirements and signalprocessing principles, implementing sensing capabilities inexisting communication systems such as WLAN [5] is feasibleand convenient while often not necessitating an overhaul orupdate of the current infrastructure. In the realm of robotics,ISAC allows robots to not only interact with their environmentmore effectively but also to communicate seamlessly withina network. Furthermore, by combining ISAC with traditionalThe authors were supported in part by the German Federal Ministryof Education and Research (BMBF) in the programme “Souver ¨an. Digital.Vernetzt.” within the research hub 6G-life under Grant 16KISK002, andalso by the Bavarian Ministry of Economic Affairs, Regional Developmentand Energy within the project 6G Future Lab Bavaria. U. M ¨onich and H.Boche were also supported by the BMBF within the project ”Post ShannonCommunication - NewCom” under Grant 16KIS1003K. M. Fees and A. Feikcontributed to this work during their studies at TU Munich.sensors like cameras and LiDAR, robots gain a more holisticunderstanding of their surroundings, leading to more preciseand effective actions.As another emerging technology in 6G, digital twin (DT)technology is reshaping the landscape of robotics and wirelesscommunication systems [6]. Traditionally in robotics, DTshave been used primarily for simulation and modeling pur-poses. They serve as virtual replicas of robotic systems, pro-viding a platform for testing, analyzing, and optimizing robotdesigns and behaviors before physical deployment. Theseapproaches have been recently utilized to facilitate the pre-deployment validation and testing of wireless systems [7], [8]and are shown to significantly reduce the expenses and efforttraditionally involved in these processes [9]. However, accu-rately modeling the intricate interactions between the robot andits environment, especially when incorporating ISAC capabil-ity into multi-sensor platforms, is complex. Maintaining real-time synchronization between the physical robot and its DTalso requires continuously updating and adapting to changes inthe robot’s environment and operational parameters. Therefore,the current academic research and practical deployment ofreal-time DTs for robotics with ISAC integration are still inthe early stages. On the other hand, addressing and solvingthese challenges enables a wide array of new applications.An accurate DT of the environment could help the agentsoffload low-level tasks, such as localization. These can thenin turn focus their computational resources on solving high-level tasks, such as scene understanding. This aspect coupledwith both local and global information also allows for greaterenergy efficiency and better network planning. Toward thisend, we developed a hardware-software framework to ad-dress the challenges and enable the applications mentionedabove. This framework, based on the Robot Operating System(ROS), integrates traditional robotic functionalities of real-time control, perception, and cooperation, along with advancedISAC system simulation and modeling based on ray-tracingtechnology. In this paper, we place a particular emphasis ontesting its capabilities in indoor localization and navigation,with our contributions being as follows:•We have implemented, to our knowledge, the first DTfor ISAC and robotics. This implementation is unique 978-8-3503-8544-1/24/$31.00 ©2024 IEEEarXiv:2402.15191v1 [cs.RO] 23 Feb 2024not only in its simulation capabilities but also in offeringa real-world interface to actual sensors.•Our DT leverages powerful robotics and ray-tracing tech-nologies and is capable of fusing data from both syntheticand real sources. Additionally, it provides reactive up-dates triggered by changes in the environment.•We designed our DT to be compatible with off-the-shelfcommercial components, which facilitates rapid proto-typing and research, thus making it more accessible forstudies exploring the intersection of ISAC and robotics.•Our measurement results demonstrate that the localizationaccuracy provided by the DT, when compared to internalodometry data from the robot, is highly reliable. Theseresults indicate the practical utility of our DT in real-world scenarios.II. H IGH-LEVEL OVERVIEWThe proposed framework consists of a central DT instancerunning on a remote computer, and multiple autonomousrobots as depicted in Figure 1, following a publisher/subscribermodel [10] implemented using ROS [11]. The DT consists ofthe following components:Master Node: This module provides naming and registra-tion services to the rest of the robots and tracks publishers andsubscribers to different topics.Simulation: This module takes as input a 3D model of theenvironment, as well as a network configuration that specifiesthe connectivity between robots in the communication andsensing scenario. Utilizing this input, the module conductsan extensive ray-tracing simulation across all\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".com >, Sergey Levine<sergey.levine@berkeley.edu >.Under Review/uni00000056/uni00000051/uni00000044/uni0000004c/uni0000004f/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000013/uni00000018 /uni00000052/uni00000046/uni00000057/uni00000052/uni00000053/uni00000058/uni00000056/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000015 /uni00000046/uni0000004b/uni00000048/uni00000048/uni00000057/uni00000044/uni0000004b/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000013/uni00000019 /uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000053/uni00000052/uni00000057/uni00000044/uni00000050/uni00000058/uni00000056/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000013/uni00000017/uni00000056/uni00000051/uni00000044/uni0000004c/uni0000004f/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni00000018/uni0000001c /uni00000052/uni00000046/uni00000057/uni00000052/uni00000053/uni00000058/uni00000056/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni00000016/uni00000018 /uni00000046/uni0000004b/uni00000048/uni00000048/uni00000057/uni00000044/uni0000004b/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni00000016/uni00000014 /uni0000004b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000053/uni00000052/uni00000057/uni00000044/uni00000050/uni00000058/uni00000056/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000014Figure 1. Mitigating nominal reward exploitation with entropy-regularized control. Diffusion models fine-tuned in a goal-directed manner can produce images (top) with high nominal re-ward values such as aesthetic scores. However, these images lackrealism because the naive fine-tuning process is not incentivizedto stay close to the pre-trained data distribution. Our approach(bottom) mitigates this issue via entropy-regularized stochasticoptimal control.mance in various domains such as image generation andbiological sequence generation (Jing et al., 2022; Wu et al.,2022). While diffusion models effectively capture complexdata distributions, our primary goal frequently involves ac-quiring a finely tuned sampler customized for a specific taskusing the pre-trained diffusion model as a foundation. Forinstance, in image generation, we might like to fine-tunediffusion models to enhance aesthetic quality. In biology,we might aim to improve bioactivity. Recent endeavors havepursued this objective through reinforcement learning (RL)(Fan et al., 2023; Black et al., 2023) as well as direct back-propagation through differentiable reward functions (Clarket al., 2023; Prabhudesai et al., 2023). Such reward func-tions are typically learned models meant to approximate aground-truth “genuine” reward; e.g., an aesthetic classifieris meant to approximate the true aesthetic preferences ofhuman raters.While these methods allow us to generate samples with high“nominal” (approximate) rewards, they often suffer fromreward collapse. Reward collapse manifests as fine-tuned1arXiv:2402.15194v2 [cs.LG] 28 Feb 2024Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Controlmodels produce the same samples with low genuine rewards,as illustrated in Figure 1. This issue arises because nominalrewards are usually learned from a finite number of trainingdata to approximate the genuine reward function, mean-ing that they are accurate only within their training datadistribution. Consequently, fine-tuning methods quickly ex-ploit nominal rewards by moving beyond the support of thisdistribution. Even ignoring nominal reward exploitation,reward collapse is problematic because diversity is, in itself,a desirable property of generative models. For example, wetypically expect generative models to generate diverse candi-dates of protein sequences to run good wet lab experiments(Watson et al., 2022).Our goal is to develop a principled algorithmic frameworkfor fine-tuning diffusion models that both optimizes a user-specified reward function and captures a diverse distributionthat stays close to the training data, thus alleviating rewardcollapse issues. To achieve this, we frame the fine-tuningof diffusion models as an entropy-regularized control prob-lem. It is known that diffusion models can be formulated asstochastic differential equations (SDEs) with a drift term anda diffusion term (Song et al., 2020). Based on this formu-lation, in a fine-tuning step, we consider solving stochasticcontrol by neural SDEs in a computationally efficient man-ner. Here, we introduce a loss that combines a terminalreward with entropy regularization against the pre-traineddiffusion model, with respect to both a drift term and aninitial distribution. This entropy-regularization term enablesus to maintain the bridges (i.e., the posterior distributions oftrajectories conditioned on a terminal point) of pre-traineddiffusion models, akin to bridge-matching generative mod-els (Shi et al., 2023), such that \n",
      "----------------------------------------------------------------------------------------------------\n",
      "l intelligenceresearch. As part of this ever-evolving realm, affect recognitionis characterized by rapid pace of new developments and acontinuous pushing of boundaries in research. The explorationof affective computing often involves addressing recurringtasks such as interpreting facial expressions, analysing speechprosody and sentiment or observing non-verbal behaviour inbody movements and poses. However, despite the growinginterest and demand in this field, existing tools and frameworks(e.g. Microsoft’s Platform for Situated Intelligence [32] orGoogle’s MediaPipe [25]) present challenges for swift devel-opment, as they require in-depth programming knowledge andare primarily aimed at power users or experienced developers.Even within more accessible systems, a discernible gapexists, as most of them are focusing predominantly onproviding the input modalities and signals, while lackingessential built-in functionality for comprehensive affectanalysis and recognition. This limitation underscores theneed for more versatile, accessible, and comprehensivearchitectures that cater to a broader audience, includingresearchers and practitioners without extensive programmingbackgrounds but high interest and demand for mostrecent solutions. Addressing these challenges is pivotal toadvancing affective computing research and fostering a moreinclusive and collaborative environment in this dynamic andinterdisciplinary field.To close this gap, in this paper, we present the AffectTool-box, a comprehensive affect recognition software system,that is•Easy to use. No programming knowledge required -all necessary functionality can easily be controlled by agraphical user interface.•Comprehensive. A variety of models for interpretingaccessible affective channels is included.•Easy to integrate. The AffectToolbox can easily beintegrated into more complex applications, using its built-in networking functionality, allowing for a seamless con-nection to proprietary software implementations.•Open source. The whole software system and sourcecode will be made publicly available.1II. R ELATED WORKTo give a foundation for the conceptualization and devel-opment of the AffectToolbox , in the following we give a briefoverview on existing software that addresses related goals.A. Multi-modal Sensor Integration and SynchronizationApache Flink [14] is an open-source stream processingand batch processing framework for big data processing andanalytics. It is designed to efficiently process large volumes ofdata in real-time and batch processing modes. Flink providesa programming model and runtime for the processing of datastreams in a distributed and fault-tolerant manner.SiAM-dp [15] serves as a platform dedicated to the develop-ment of multi-modal dialogue systems, with a key emphasison effortlessly integrating dispersed input and output deviceswithin the realm of cyber-physical environments.The Social Signal Interpretation Framework (SSI) [12] isan efficient framework that offers robust support for diversesensor devices, filter and feature algorithms, and provides1https://github.com/hcmlab/AffectToolboxarXiv:2402.15195v1 [cs.HC] 23 Feb 2024Fig. 1. The modular architecture of the AffectToolbox (Section III). In its current state, audiovisual sensory devices (e.g. webcams) provide easy means togenerate all considered data streams (i.e. audio, transcript, video and skeleton data). So called activity checks trigger the machine learning based analysisof respective modalities (Section III-D). The uni-modal results of applied affect recognition models are represented by a subset of pleasure, arousal and/ordominance scores. These unimodal emotional cues are the input for an event-driven fusion algorithm (Section III-E), which deduces a coherent affective state,represented in the continuous PAD emotional space (Section III-C).an extensive C++ API to add further functionality and traincustom machine learning models. Meanwhile, for end users,SSIprovides accessibility through an XML interface. To alsobe accessible on mobile devices, Damian et al. ported the corefunctionality of SSIto a Java-based framework which theycalled SSJ[20].Barz et al. introduced the Multisensor Pipeline (MSP )[31], a Python framework that serves as lightweight tool forprototyping multi-modal sensor pipelines. A similar goal wasfollowed by Saffaryazdi et al. [34], who introduced a Pythonframework called Octopus Sensing - however, in contrast toMSP , which focuses more on real-time applications, OctopusSensing specifically addresses multi-modal data collection.To the best of the authors’ knowledge, all of the frameworksmentioned above, although providing comprehensive function-ality to track and synchronize a multitude of different sensorhardware, do not provide pre-trained machine learning models,which allow for an immediate in-depth affective analysis ofthe modalities. They are limited to providing an efficient andextensible infrastructure for further computations (which mayinclude the d\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ly selects the most informative examplesfrom the unlabeled data pool and queries their labels from anoracle, enabling the learning of an effective model with re-duced labeling costs.Existing AL methods [Roy and McCallum, 2001; Fu et al. ,2013; Huang et al. , 2010; You et al. , 2014; Sinha et al. , 2019;Yoo and Kweon, 2019; Ning et al. , 2021 ]typically operateunder the closed-set assumption, assuming that the label cat-egories in the unlabeled data pool match those of the targettask. However, this assumption often does not hold in practi-cal scenarios. For example, consider a task that involves clas-sifying images into two target categories, “Dog” and “Cat”.Collecting training examples through keyword-based imagesearch inevitably introduces irrelevant images from other cat-egories ( i.e., unknown classes), alongside the two target cate-gories ( i.e., known classes).In such open-set scenarios, many previous AL methods,which prefer querying examples with less confident predic-tions, may lead to failure since examples from unknownclasses often receive uncertain predictions. To mitigate theimpact of unknown class examples, some AL methods de-signed specifically for open-set scenarios attempt to query ex-amples that are more likely to belong to known classes basedon sample similarity [Duet al. , 2021 ]or model-predicted maxactivation value (MA V) [Ning et al. , 2022 ]. However, sinceexamples similar to the labeled ones may be already mastereddata, they do not significantly benefit the target model. Thesemethods only perform well when the proportion of unknownclass examples is high. When the proportion is low, they tendto perform poorly than traditional AL methods, even inferiorto random sampling (please refer to Figure 6). However, de-termining the proportion of unknown class examples in prac-tical scenarios is often challenging. This further limits theusability of these methods.In this paper, we start with a specific question: can we ef-fectively distinguish the “informative” examples of knownclasses from examples of unknown classes? Intuitively, ifwe can push the unknown class examples toward regions withhigh-confidence predictions, existing uncertainty-based ALmethods can be applied directly in open-set scenarios. Toachieve this, we propose to fine-tune the model by perform-ing negative learning (NL) [Kim et al. , 2019; Luo et al. , 2021;Leeet al. , 2021 ]on unlabeled examples. NL is an indirectlearning manner that explores the utility of complementarylabels, i.e., the label categories that an instance does not be-long to. For a K-classification problem, the NL loss is de-fined as:ℓNL(f,¯y) =−PKk=1¯yklog (1−pk). (1)where ¯yand¯y= [¯y1, . . . , ¯yk, . . . , ¯yK]represent a comple-mentary label and its corresponding one-hot form, respec-tively, and pkdenotes the probability of the k-th category.arXiv:2402.15198v1 [cs.LG] 23 Feb 20240.0 0.2 0.4 0.6 0.8 1.0maximum predicted probability5k10k15k20knumber of examplesUnknownKnown0.4 0.5 0.6 0.7 0.805001000(a) w/o NL0.0 0.2 0.4 0.6 0.8 1.0maximum predicted probability5k10k15k20knumber of examplesUnknownKnown0.4 0.5 0.6 0.7 0.805001000 (b) w NLFigure 1: The statistics of prediction confidence before and afterfine-tuning the model. Note that in the zoomed-in area of Figure 1b,we swapped the display order of the two to prevent occlusion, allow-ing for a more intuitive view of how the distribution has changed.Specifically, the fine-tuning process comprises two parts.On one hand, for already labeled examples, we train them di-rectly using Equation 1. On the other hand, for unlabeled ex-amples, we first randomly assign labels to them in each train-ing round and then train the model using Equation 1. Notably,the unlabeled known class example has a relatively higherchance of receiving the correct label, whereas the unknownclass example will never be assigned the correct label. Onceunlabeled known class data receive the correct labels, theysuffer a larger penalty and are reduced confidence predictionsby the model since they deviate from the distribution infor-mation obtained from labeled data. In contrast, unlabeledunknown class data are not constrained to move towards thehigh-confidence region for counteracting the update gradientproduced by the labeled data1.To validate this, we conducted preliminary experiments onCIFAR-10 [Krizhevsky et al. , 2009 ]with 4 known classesand 6 unknown classes. Figure 1 illustrates the confidencestatistics for all unlabeled examples in a fixed query roundbefore and after fine-tuning the model. As expected, knownclass examples are more prevalent in the low-confidence re-gion, while unknown class ones are more common in high-confidence regions. This distribution characteristic presents apotential solution to the aforementioned problem and offers apromising approach to AL for open-set scenarios.Based on this, we propose a Bidirectional Uncertainty-based Active Learning framework (BUAL). On one hand,we propose the Random Label Negative Learning (RL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "o et al., 2021; Sun et al., 2022),aiming to address discourse-related challenges suchas zero pronoun translation (Wang et al., 2019), lex-ical translation consistency (Lyu et al., 2021, 2022),*Corresponding author: Junhui Li.1As this paper is under review, we will release our codeand datasets later.and discourse structure (Hu and Wan, 2023). A re-cent paradigm shift has been witnessed in context-aware NMT with the emergence of the decoder-only large language models (LLMs) (BigScience,2022; Google, 2022; MetaAI, 2023b,a; OpenAI,2023). These generative language models, trainedon extensive public data, have gained significantattention in the natural language processing (NLP)community. In adapting LLMs to context-awareNMT, a common strategy involves concatenatingmultiple source sentences as a prefix and generatingtranslations token-by-token, relying on the prefixand previously predicted target tokens, as shownin Figure 1 (a). However, a critical observationof this strategy reveals a potential drawback – theequal prioritization of the inter- and intra-sentencecontexts during token generation. Importantly, theintra-sentence context inherently contains richerparallel semantic information with the target sen-tence and should be given a higher priority than theinter-sentence context. Consequently, we proposethat separately modeling and utilizing the inter- andintra-sentence contexts should explicitly informLLMs of the document-level context and the cur-rent sentence itself, thus being able to prevent themisallocation of attention weights to source-side to-kens (Bao et al., 2021; Li et al., 2023). Inspired bythe success of prompt tuning (Li and Liang, 2021;Liu et al., 2022; Tan et al., 2022), our alternativeapproach, named Decoding-Enhanced Multi-phasePrompt Tuning (DeMPT), aims to enhance LLMs’adaptability to context-aware NMT, as shown inFigure 1 (b).Specifically, we divide the whole procedureof context-aware NMT into three phases: inter-sentence context encoding, intra-sentence contextencoding, and decoding. Following Li and Liang(2021); Liu et al. (2022), we sequentially and dif-ferentially adapt LLMs for each phase, utilizingphase-specific trainable prompts. This phased tun-ing method enables LLMs to independently capturearXiv:2402.15200v1 [cs.CL] 23 Feb 2024LLMSource Sentence Inter-sentence ContextLLMSource Sentence Inter-sentence ContextTarget SentenceLLM LLMTarget SentenceConcatenatinga. Concatenation Strategy b. Proposed Approach❄ ❄ ❄ ❄Figure 1: Comparison of different strategies for adapting LLMs to context-aware NMT. The concatenation strategy(left) treats inter-sentence and intra-sentence (referred to as the \"source sentence\" context in the figure) with equalimportance. In contrast, our approach ( right ) divides context-aware NMT into three distinct phases, enabling LLMsto selectively model and leverage both inter- and intra-sentence contexts.and model both inter- and intra-sentence contexts,facilitating a better understanding of their differ-ences. Importantly, our approach only divides theoriginal input into three parts without significantlyincreasing computational load. As a result, thereis no substantial decrease in inference speed com-pared to the concatenating method, as detailed inSection 4.3.Furthermore, during the decoding phase, we pro-pose a heuristic method to emphasize the differ-ence between inter- and intra-sentence contexts,and avoid long-distance issue when utilizing inter-sentence context. Specifically, at each decodingstep, we use LLMs to predict the next token threetimes. The decoding states used for each predic-tion directly concatenate with the representationsof two contexts in a discriminative manner. Fi-nally, we combine three probability distributionsto search for the next token as the output from thetarget vocabulary. This method enables LLMs tolearn not only to properly capture inter-sentencecontext in addressing discourse-related issues butalso to recognize a difference between inter- andintra-sentence contexts, allowing for effective uti-lization of both types of contexts.In summary, our contributions can be outlinedas follows:•We propose a novel multi-phase prompt tun-ing approach to divide context-aware NMTinto three phases, making LLMs aware of thedistinction between inter- and intra-sentencecontexts.•We introduce a enhanced decoding methodthat discriminately utilize both context types.This allows LLMs not only properly captureinter-sentence context in addressing discourse-related issues, but also be aware of the impor-tance of the intra-sentence context.•We validate our approach using llama-2-7band bloomz-7b1-mt as foundation models,demonstrating its effectiveness across fivecontext-aware translation directions. Exten-sive analyses further highlight the substantialenhancement in LLMs’ ability for context-aware NMT.2 MethodologyIn this section, we describe our decoding-enhancedmulti-phase approach for adapting LLMs tocontext-aware NMT in details. Specifically, webreak down\n",
      "----------------------------------------------------------------------------------------------------\n",
      " to mitigate the generationof toxic text by some flexible techniques, referredto as detoxification in this paper.Researchers are increasingly focused on ensur-ing the generation of harmless content by LLMs.For instance, finetuning-based techniques (Keskaret al., 2019; Gururangan et al., 2020) involve ad-ditional training of the base model on nontoxicdatasets to mitigate toxic outputs. However, thisapproach requires substantial computational re-sources, especially for LLMs. Moreover, fine-tuning a model on specific data compromises itsability to generalize to other tasks (Kumar et al.,2022). Another methods involve adjusting the prob-abilities of potential toxic tokens during the decod-ing stage (Krause et al., 2020; Liu et al., 2021;Hallinan et al., 2023). Although further trainingof the base model is not necessary, this processrequires an additional guiding model for modify-ing the probability distribution. Additionally, thememory occupied by this additional model mustbe maintained at each decoding step. The thirdcategory is a prompt-based approach (Schick et al.,2021; Leong et al., 2023), which utilizes promptsto trigger the model’s internal knowledge relatedto toxicity for detoxification. However, there is alack of understanding of the working mechanismsof safety prompts, which limits the potential foroptimizing them to improve LLM safety (Zhenget al., 2024).In this paper, we explore the detoxification byself-generated prefixes without requiring additionaltraining parameters or data. Our approach is builton three observations: (1)Texts generated byLLMs reflects the types of inherent toxicity presentwithin these models (Klein and Nabi, 2024). At thesame time, Schick et al. (2021) demonstrates thatLLMs recognize, to a considerable degree, theirundesirable toxicity of the content they produce.Drawing on the above findings, models can gener-ate various texts by sampling from a raw prompt(Cheng et al., 2023) (referred as self-generation).These texts can be classified as either toxic or non-toxic (referred as self-diagnosis), representing po-arXiv:2402.15202v2 [cs.CL] 26 Feb 2024tential types of toxicity associated with the input.(2)Leong et al. (2023) indicates that prependingpositive prefixes (i.e. harmless instructions) or neg-ative prefixes (harmful instructions) to raw promptscan steer the model to generate content with eithernontoxic or toxic attributes. Concurrently, the con-textualized representations of prompt that are de-veloping in the internal layers of LLMs can be con-ceived as information flows, primarily facilitated byattention heads for the movement of information be-tween the layers (Elhage et al., 2021). Inspired bythem, We can dynamically add self-generated textas positive and negative prefixes, thereby guidingthe context representations of the prompts. Thus,they form differentiated information flows at theattention layer. We construct subtoxicity vectorsbased on two types of information flows. (3)Ateach prompt may correspond to multiple negativeprefixes, the resulting multiple toxicity vectors rep-resent different subtoxic behaviors. We aim to fusethese toxicity vectors to maximize the coverageof potential toxic behaviors. Ilharco et al. (2023)proposes arithmetic operations in the weight spacefor multi-task learning in models. Motivated bythis, we aim to perform arithmetic operations onsubtoxicity vectors to fuse various subtoxicities. Ul-timately, detoxification generation, equipped withthe raw prompt, is accomplished by utilizing afused toxicity vector within the attention space.Therefore, we propose FGDILP, a detoxifica-tion framework considering subtoxicity informa-tion through instance-level prefixes. Specifically,we utilize the model to self-generate multiple sam-pled texts of a raw prompt and self-diagnose themfor classifying into subtoxic and nontoxic cate-gories as prefixes. At each decoding step, a batchsynthetic prompts, consisting of a positive prefix-prepended prompt and multiple negative prefix-prepended prompts, is passed through the attentionlayer to construct vectors representing differentsubtoxic behaviors. Finally, we fuse them usingarithmetic operations to collaboratively rectify theinformation flow of the raw prompt within the themodel, thereby aiding the detoxification process.In summary, our contributions are as follows:•FGDILP dynamically constructs multiple pre-fixes to collaboratively correct the contextual-ized representation for removal of fine-grainedsubtoxicities.•Our approach surpasses current prompt-basedmethods while maintaining a lightweightstructure compared to other methods.•We deeply analyze the construction of a syn-ergistic mechanism involving the generationof prefixes and the fusion of toxicity vectorsfor instance-level detoxification, paving theway for exploring the detoxification of vari-ous fine-grained subtoxicities.2 Model2.1 Task FormulationGiven a prompt t={t1, t2, ..., t N}withNtokens,a causal language modeling (CLM) can generatefluent text ba\n",
      "----------------------------------------------------------------------------------------------------\n",
      "spartofthePFCCapproach,focusing ondecreasingsymptomsofPICS/PICS-F. Thesediariesare written in everyday language by healthcare professiona ls, mostly nurses and family members. Using ICU diariesoﬀers many beneﬁts to all stakeholders involved in the ICU. T hey contain daily entries detailing the current patientstatusanddescriptionsofsituationsandsurroundings.Re adingadiaryafterhospitalizationisaneﬀectivewayofcop -ingwiththetraumaticaftermathofcriticalillness,conse quentlyhelpingtoprevent thedevelopment ofpsychologica lproblems [1, 17]. For family members, the diary provides an o pportunity to actively care for their loved ones, thusdiminishing feelings of powerlessness and reducing psycho logical problems [19, 22]. Digital diaries were developedduring the COVID-19 pandemic, receiving positive assessme nts from family members [26]. However, implementingdigital diaries face somebarriers[6,8]as willbeexplaine d further.Authors’ addresses: S. KernanFreire, Delft University of T echnology, Landbergstraat 15, Delft, 2628 CE, The Netherla nds, s.kernanfreire@tudelft.nl;M.M.C.vanMol, Erasmuc MC, Dr. Molewaterplein 40, Rotterda m, 3015 GD, The Netherlands, m.vanmol@erasmusmc.nl; C.Sch ol, Erasmuc MC, Dr.Molewaterplein 40, Rotterdam, 3015 GD, The Netherlands, c. schol@erasmusmc.nl;E. OzcanVieira, Delft University of T echnology, Landbergstraat15,Delft, 2628CE, TheNetherlands, e.ozcan@tudelft.nl.12 KernanFreire, et al.1.2 Digital diary in the ICU;Barriers in implementationHigh-qualityPFCC shouldbeconsidered afundamental skill forICUhealthcare professionals [12].However, barriershave been identiﬁed, hindering the eﬀective implementatio n of digital ICU diaries. The main barrier is a lack of timefor healthcare professionals [20]. Another barrier is thel ack of knowledge among nurses regarding what and how towriteinthediary[8].Apotentialsolutiontoaddressthese challengescouldbetheintegrationofArtiﬁcialIntellige nce(AI) in thewritingprocessof healthcare professionals.1.3 LargeLanguageModelsin HealthcareNatural language processing (NLP) is a machine learning tec hnique that involves the processing and analysis of text.Large language models (LLMs), such as ChatGPT, are very eﬀec tive at generating human-like text. LLMs mark a sig-niﬁcant step forward from their predecessors, such as recur rent neural networks (RNNs). Unlike RNNs that processtext sequentially and often struggle with long-range depen dencies, LLMs can analyze and generate text in parallel,handlingextensivecontextandcomplexlanguagepatternse ﬃciently[11,18,27,28].ThesecharacteristicsmakeLLMseﬀective writing partners,helping humans bygenerating ou tlines,reﬁning text,and adaptingit tothereader.In healthcare, applications of NLP range from supporting tr iage by analyzing prior medical notes to answeringpatients’ questions as a chatbot [15]. Recently, LLMs have r eceived widespread attention in healthcare, leading tothe creation of health-speciﬁc models,such as Med-Palm [24 ]. Applications include supportingclinical workﬂow, forexample, by generating discharge notes, extracting ecolog ical phenotypes [5], and making medical texts more under-standable and empathetic for patients [14]. These capabili ties could help tackle challenges nurses face when usingdigital diaries in theICU, however, this remains unexplore din theliterature[14,21].2 TOWARDSICUDIARIES SUPPORTEDBY LARGE LANGUAGE MODELSICU diaries can provide a more personable timeline for the IC U admission beyond the medical notes that medicalprofessionals already record. Our vision is that an LLM-pow ered tool can support the writing process for nurses,making it more eﬃcient without losing the personal touch of a human writer. In the following sections, we describethis vision inmoredetail and theassociatedresearch chall enges.2.1 Future VisionWeenvisionacollaborativewritingprocessthatevolvesas nursesbecomemorefamiliarwiththeLLM-poweredtool’scapabilities, and in turn,the tool“learns” the nurse’s wri ting style. To begin with, nurses may beunfamiliar with thediary writing process for ICU patients. As such, the tool can help nurses ﬁgure out what and how to write. At thisstage, the tool asks for key information about the situation and generates an example diary entry for the nurse. Asthenursebecomesmorefamiliarwiththeprocessandexpecta tions,theycanstartadjustingtheentries themselves orwrite them from scratch. At this stage, the tool can provide i n-text suggestions on how to write empathetically andunderstandablyforthepatients.Over time,thetoolwillam ass a databaseofentries aboutindividual patients writtenby the nurse, allowing the tool to align with their writing st yle [18]. In turn, the nurse can enter a few keywords togenerate a diary entry, saving time. Thus, this collaborati ve process allows for growth and adaptation both on thehumanand technological sides.Thetoolmustsupportvariousdiaryentrythemesandmodalit ies,primarilytextandimages.PriorworkbyGalazziet al. [6] has shown that ICU diary entries fall under fo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-annotated dataset ( sourcedomain ) and an unlabeled dataset corresponding to thetarget domain , aiming to train a model that can performwell in the new environment. Despite progress in recentyears [13, 14], UDA for person Re-ID suffers from threeSourceDomainData StreamTargetDomainDomainShiftCatastrophicForgetting...NetworkFigure 1. In OUDA for person Re-ID, the images of the targetdomain are available as a stream of data, and past images cannot bestored. Two main challenges should be addressed: 1) catastrophicforgetting and 2) domain shift.main issues that prevent its practical use. First, when col-lecting the target data required to adapt the model, imagesare generally gathered as a stream that continually sendsphotos from various cameras/locations. Consequently, col-lecting a large target dataset may take time and delay de-ployment. In addition, in UDA, the model is frozen af-ter deployment and does not benefit from the new data,which are continuously captured. Finally, numerous coun-tries have adopted privacy regulations that forbid technol-ogy providers to store images of individuals. Thus, collect-ing a large target dataset is not possible.Since deploying algorithms that conform with policiesof data privacy protection has become a legal obligation ina growing number of countries, the Online UnsupervisedDomain Adaptation for person Re-Identification ( OUDA-Rid) setting was introduced in [32] to address the limita-tions of traditional UDA techniques. In the OUDA-Ridframework, we operate under the assumption that we haveaccess to annotated source data as well as unlabeled tar-get data. However, in contrast to traditional UDA settings,the target dataset is treated as an online stream of data,aligning with the constraint that camera-captured imagesarXiv:2402.15206v1 [cs.CV] 23 Feb 2024cannot be stored. In addition to complying with privacy-protection regulations, this setting also enables the personRe-ID model to be continuously updated as new target databecomes available, thereby improving its adaptability tochanges in the target domain.The performance of existing UDA methods for OUDA-Rid shows significant drops in performance regarding theoffline setting [32]. This drop can be explained by the twomain difficulties of OUDA-Rid illustrated in Fig. 1: catas-trophic forgetting and domain shift. Catastrophic forgettingappears when the model only observes a few target identi-ties, and consequently, the model tends to forget previouslylearned identities. Domain shift is a change in the data dis-tribution between the source and target domains. Address-ing the domain shift is especially challenging in the onlinesetting since, at every training step, we observe only a smalland possibly biased subset of the target domain.In this work, we consider that these two difficultiesmust be addressed jointly since mitigating catastrophic for-getting can lead to target representations that better cap-ture the full target distribution, and consequently facilitatesource-target distribution alignment. We introduce a unifiedSource-guided Similarity Preservation ( S2P) framework forOUDA-Rid that addresses these two challenges jointly. Wetake inspiration from replay-based strategies [1, 3] to intro-duce a Knowledge Distillation ( KD) mechanism. By trans-ferring the knowledge acquired with a teacher model to astudent model, the KD [19] method enables the learningof more robust and generalizable features. However, un-like existing replay-based approaches, we do not store anytarget image to conform to the privacy protection require-ment. To this end, we extract a support set composed ofsource images that are similar to the previously seen im-ages of the target. This support is thus used to regularizethe learning process and alleviate catastrophic forgetting.Our framework combines both explicit source-target dis-tribution alignment and pseudo-labeling to address domainshift. S2P can easily integrate almost any existing UDA ap-proaches [13,14] and readily outperforms all state-of-the-artmethods for OUDA-Rid in several challenging conditions inreal-to-real and synthetic-to-real tasks. Our main contribu-tions can be summarized as follows:• We introduce a novel S2P algorithm that uses source-guided similarity preservation to jointly alleviate thecatastrophic forgetting anddomain shift while respectingtheprivacy protection requirements.• S2P can easily incorporate almost any existing UDA ap-proach. In particular, we present the integration of theMMT [13], SpCL [14] and IDM [5] methods into ourframework, which achieve remarkable results in the UDAsetting.• We perform extensive experiments1in real-to-real1Code available: https://github.com/ramiMMhamza/S2Pand synthetic-to-real OUDA tasks with four datasets.S2P readily improves previous state-of-the-art UDAmethods for OUDA-Rid. A set of ablation studies vali-date each component of our algorithm.2. Related WorkUDA for person Re-ID. Existing methods can be dividedintodomain translation-based andp\n",
      "----------------------------------------------------------------------------------------------------\n",
      "•Information systems →Recommender systems .KEYWORDSRecommendation, Large Language Model, Item-side Fairness∗Corresponding authors.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.WWW ’24, May 13–17, 2024, Singapore, Singapore©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0171-9/24/05. . . $15.00https://doi.org/10.1145/3589334.3648158ACM Reference Format:Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng,and Xiangnan He. 2024. Item-side Fairness of Large Language Model-basedRecommendation System. In Proceedings of the ACM Web Conference 2024(WWW ’24), May 13–17, 2024, Singapore, Singapore. ACM, New York, NY,USA, 10 pages. https://doi.org/10.1145/3589334.36481581 INTRODUCTIONRecommendation systems play a pivotal role in the distributionof diverse Web content, encompassing news reports [ 45], socialposts [ 12], micro-videos [ 61], and a range of descriptions like cloth-ing [ 32], cuisine [ 30], pharmaceuticals [ 14], and job [ 35]. Recom-mendation systems are intricately linked to societal concerns suchas equitable information access and exposure opportunities for vul-nerable populations. Recently, Large Language Models (LLMs) [ 5,44] have gained prominence as a key in recommendation sys-tems due to their exceptional content comprehension capabili-ties [ 1,2,57]. Nevertheless, the integration of LLMs may introducesocietal challenges to recommendation systems, primarily stem-ming from the inherent biases in LLM training datasets [ 33,44].Therefore, there is a pressing need to investigate the trustworthi-ness of LLM-based Recommendation Systems (LRS).Item-side Fairness (IF) [ 24,48] is a critical aspect of trustworthyrecommendations, aiming to provide fair exposure opportunitiesfor different item groups. IF is widely used to ensure the rightsand profit of item producers such as the opportunity to seek appro-priate candidates of micro-businesses in job recommendation [ 6].Additionally, IF can also be applied to different topics, facilitatingeffective dissemination of content ( e.g.,news reports) on societalissues like environmental sustainability and climate action. Further-more, IF can elevate the visibility of content related to vulnerablepopulations, ensuring their interests and demands receive adequateattention from society. Despite its significance, there is a lack ofcomprehensive research investigating IF in the context of LLM-based recommendation systems.LLM-based recommendation systems [ 3,52,57] exhibit uniquecharacteristics compared to conventional recommendation sys-tems [ 25,51]. These include their reliance on semantic clues toinfer user preferences, the use of instructions to describe the recom-mendation task, and the generation of recommendations instead ofarXiv:2402.15215v1 [cs.IR] 23 Feb 2024WWW ’24, May 13–17, 2024, Singapore, Singapore Meng Jiang, et al.relying solely on discriminative predictions. Consequently, previ-ous findings regarding item-side fairness in conventional methodsmay not hold true for LLM-based systems. Therefore, it is crucialto examine the property of LRS with respect to item-side fairness.This study specifically investigates two key questions:•RQ1: How does LRS perform in terms of item-side fairness com-pared to traditional recommendation models?•RQ2: What is the root cause of the fairness issue in the LRS?To answer these questions, we conduct exploratory experimentson two public datasets under a sequential recommendation settingas per recent LRS studies [ 1,2]. We compare LLM-based methods,represented by BIGRec [ 1], with SASRec [ 17], representative ofconventional sequential recommendation methods in terms of item-side fairness. Our findings indicate that LRS is notably influencedby the popularity factor, as BIGRec consistently recommends morepopular items compared to SASRec. Additionally, BIGRec exhibitsbiases towards certain item groups in specific genres ( e.g.,crime),suggesting the impact of inherent semantic biases within the LLMsince item genre is related to the textual description of items (in-puts of LRS). In summary, the imbalanced distribution of historicalinteractions for LRS training and the inherent semantic bias areboth contributing factors to the unfairness observed in LRS. Con-sequently, it is crucial to adapt conventional methods to enhanceitem fairness by addressing these factors in LRS.To achieve this target, we develop IFairLRS, a concise and ef-fective fram\n",
      "----------------------------------------------------------------------------------------------------\n",
      "learning heavily reliant on large-scale labeled data.Index Terms — Medical Imaging Processing, Multi-organSegmentation, Label-efficient Learning, Diffusion Models,Pre-trained Models.I. INTRODUCTIONMEDICAL image segmentation is a critical task inmedical imaging, as it enables accurate diagnosis andtreatment planning for various diseases. Recent advancementsin deep learning techniques have significantly impacted thisfield by providing accurate and efficient segmentation results.This work was supported in part by the Project of the EducationalCommission of Guangdong Province of China (2022ZDJS113) and theNatural Science Foundation of Top Talent of Shenzhen TechnologyUniversity (GDRC202134).Equal contribution: Y ongzhi Huang and Jinxin Zhu; Correspondingauthor: Jingyu Li and Bingding Huang.All authors are with the College of Big Data and Internet,Shenzhen Technology University, Shenzhen 518118 China. (e-mail: yhuang@bupt.edu.cn;2110416015@stumail.sztu.edu.cn;(haseeb,suliyilei, lijingyu, huangbingding)@sztu.edu.cn).Deep learning has gained popularity in medical image seg-mentation due to its ability to automatically learn relevantfeatures from data and its superior performance comparedto traditional segmentation methods. These approaches havedemonstrated promising outcomes in accurately segmentingorgans and tissues from medical images, including MagneticResonance Imaging (MRI) and Computed Tomography (CT)scans.However, obtaining accurate labels for medical image seg-mentation, especially in real clinical scenarios, is a challeng-ing and time-consuming task requiring professional expertise.The aforementioned challenges render traditional supervisedlearning approaches impractical and highlight the necessityfor methods that can effectively learn from limited labeleddata. Semi-supervised learning (SSL) techniques offer theability to train models with limited labeled data. Inspiredby the common paradigms in natural language processing(NLP), the method of pre-training and fine-tuning can learnthe distribution of unlabeled data. Furthermore, medical imagesegmentation models are influenced by factors such as highresolution, contrast, blur, and noise, which pose a significantchallenge to the quality of medical images [1].The Denoising Diffusion Probabilistic Model (DDPM) is arobust generative model known for its proficiency in generativetasks [2]. The core concept of DDPM involves training adiffusion process that progressively converts a basic initialdistribution (such as a Gaussian distribution) into the desireddata distribution. This process enables the model to learn alatent space that can be leveraged for diverse downstreamtasks, including classification, clustering, or anomaly detec-tion. Motivated by the recent achievements of DDPM, we in-troduce a fresh and efficient semantic segmentation frameworkthat minimizes the reliance on labeled data. Specifically, wepresent an approach utilizing diffusion models for the multi-organ segmentation task in medical CT images. Additionally,we provide a comprehensive comparison of the current state-of-the-art (SOTA) supervised and SSL methods with ourmethod for multi-organ segmentation in CT images. Our maincontributions are summarized as follows:(1) We have enhanced the existing DDPM to be applicableto the medical image field. Our goal is to train advancedgenerative models for synthesizing CT images using unlabeleddata. We modified the noise-predicting network by adjustingits input and output layers, allowing it to process grayscaleimages instead of RGB images. We also developed a new datapre-processing pipeline for synthesizing abdomen CT images.Through experimentation, we obtained compelling results forarXiv:2402.15216v1 [cs.CV] 23 Feb 20242 IEEE TRANSACTIONS AND JOURNALS TEMPLATEthe generative model’s performance on CT images with aresolution of 256 ×256.(2) We explored the use of semantic representation inDDPM and proposed an end-to-end approach that enables thepre-trained DDPM to be effectively adapted for multi-organsegmentation tasks. We devised a straightforward transferstrategy and put forth three fine-tuning strategies. By lever-aging the semantic representation pre-trained in DDPM, ourproposed method enhances the performance of downstreammulti-organ segmentation tasks significantly.(3) We thoroughly evaluated the segmentation performanceof our proposed method on the MICCAI FLARE2022 dataset.The results demonstrate that our network is highly competitivecompared to SOTA-supervised methods. Additionally, we per-formed ablation experiments to provide further insights intothe importance of the DDPM pre-training model. Notably,the performance gap between our proposed method and otherapproaches widens when using small-scale labeled data.II. R ELATED WORKA. Generative Models for Semantic SegmentationGenerative models play a crucial role in unsupervised learn-ing by capturing complex patterns in raw data without relyingon labels. Over the past few years, GANs (Generative Adver\n",
      "----------------------------------------------------------------------------------------------------\n",
      "in the future.1 IntroductionThe recent emergence of image genera-tors (Ramesh et al., 2021; Rombach et al.,2022; Saharia et al., 2022) promises immensetransformative across various sectors (Song et al.,2021, 2022; Kapelyukh et al., 2023). DespiteFigure 1: Schematic illustrations of the comparisonsacross various prompt attackers. Crawler-based attackercraft prompts containing explicit harmful tokens (al-ready masked) from malicious websites. Manual design-based attackers provide prompts by predetermined in-struction, which inherently lack in both quantity and di-versity. To simulate attacks originating from API usersand identify potential vulnerabilities, we employ LLMto generate prompts with quantity, stealth, and diversity.their promise, these sophisticated generators comewith their own set of opportunities and challenges.Notably, they are vulnerable to exploitation byadversaries who might generate images that couldnegatively impact ethical, societal, and politicallandscapes (Rando et al., 2022; Schramowski et al.,2023). As illustrated in Fig. 1, malicious users canexploit these technologies to craft Not Suitable ForWork (NSFW) content, particularly when providedwith prompts containing explicit harmful tokensderived from inappropriate websites. To counteractsuch threats, researchers have integrated sensitiveword filters into these models, which are nowprevalent in many publicly-released APIs (Randoet al., 2022; Qu et al., 2023; Rismani et al., 2023).To delve deeper into vulnerability risks and en-hance model safety, recent studies have focused oncreating subtle, seemingly benign prompts (Schuh-mann et al., 2021, 2022) that are more discreet andchallenging to defend against. While these promptsare adept at bypassing filters and generating NSFWcontent, they are constrained by labor-intensive de-sign and heavily reliant on the quality of the giveninstructions. With a surge in users accessing thearXiv:2402.15218v1 [cs.CR] 23 Feb 2024black-box API to generate images, there is a press-ing demand for an automated prompt-generationtool capable of producing various prompts. It cansimulate the stealthy attack process i.e. a black-box approach to identify weaknesses in prevalentmodels and facilitate their improvement.A logical approach directly involves a large lan-guage model (LLM) for automated prompt gener-ation through instructions (Wei et al., 2021; Quet al., 2023; Kim et al., 2023). However, theseresulting prompts tend to be minimally threaten-ing and lack diversity. Since these methods lackan effective training strategy and rely excessivelyon the LLM’s zero-shot performance, the resultingprompts tend to be minimally threatening and lackdiversity. To solve the above issues, some works(Ilyas et al., 2018; Cheng et al., 2019; Sun et al.,2022) adopt zeroth-order/derivative-free optimiza-tion to tune prompts. However, these strategiesconfine prompts to a restricted subspace and can-not provide adequate malicious direction, makingit challenging to fully engage the LLM’s creativecapacities. Consequently, these methods fall shortin generating stealthy and diverse prompts that canrealistically simulate attacks from API users.To address the aforementioned problems, wedrive our research perspective to optimize thestealthy attack prompts by only accessing the im-age generator inference API. We introduce a novelblack-box stealthy prompt attack (BSPA) to gener-atestealthy ,offensive , and diverse samples, whichenables a transformation from original zeroth-orderoptimization (Conn et al., 2009; Rios and Sahini-dis, 2013) to gradient-based optimization. Specifi-cally, it utilizes the supervised sign from generatedtext/image and employs a retriever to identify themost relevant sensitive word to the input, thus en-suring a sufficient retrieval space. Inspired by themechanisms of communication feedback to the at-tacker, we leverage a pseudo-labeling strategy tomitigate the lack of training data in this domainand effectively optimize the retriever. Our innova-tive pseudo-labeling technique integrates aspectsof toxicity, stealth, and similarity to the input text.Furthermore, we propose a streamlined loss to suffi-ciently sample the retrieval space to obtain diversesensitive words. This refined function can amplifyprompt diversity by suppressing the probability ofthe top-k text similarities, allowing for a more ex-tensive and varied range of stealthy prompts.Building upon the BSPA framework, we developan automated prompt tool proficient in generatingstealthy and diverse NSFW samples. We present anextensive prompt attack dataset, named NSFWeval,designed to simulate attacks from malicious users,comprising 3,000 stealthy and explicit prompts.These prompts exhibit significant transferabilityand reveal universal vulnerabilities on commercialAPIs, including Stable Diffusion XL, Midjourney,and DALL-E 2/3. Additionally, we construct arobust text filter for enhancing the safety of the im-age generator, which can suppress 84.9% of promptattac\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ge, and the demand to optimizeLLM’s inference cost has been a new area of re-search interest (Kim et al., 2023; Sheng et al., 2023;Aminabadi et al., 2022).The self-attention module, as one of the criti-cal components in LLMs, has poor performanceduring inference (Table 1) since it performs inten-sive memory operations on key/value tensors ofcontext tokens (KV cache) and is memory-bound(Williams et al., 2009; Jin et al., 2023). The mem-ory complexity grows linearly with context length.As the demand for more context tokens has been atrend (32K for GPT-4), the performance gets worse(OpenAI, 2023c). KV cache additionally restrictsthe batch size and system throughput. For instance,using FP16, the KV cache for each token in GPT-3(175B) requires 4.5MB of memory. The memoryof an inference server with 8*A100 (80G) can onlyhold 70000 tokens or 35 sequences of 2K contexttokens.On the other hand, the system prompt as a com-mon practice in designing LLM based applica-tions, leads to redundancy in KV cache (Anthropic,2023). Typically, due to high training and infer-ence costs, LLMs are pre-trained and deployed in amulti-tenant architecture for multiple applicationsto share. System prompts are essential for LLMsto gain each application’s domain knowledge andgenerate better results (White et al., 2023; Zhouet al., 2023). Since multiple requests share identi-cal system prompts, there is significant overlap inprompt prefixes (§2.1).An important question is whether we can lever-age the sharing characteristic of system promptsto make the self-attention module faster and morememory efficient. To our knowledge, the only re-lated work is a proposal by Kwon et al. (2023), inwhich the service provider reserves memory forkey/value tensors of a set of predefined systemprompts from application developers. The proposal1arXiv:2402.15220v1 [cs.LG] 23 Feb 2024has limitations: i) predefined system prompts arestatic and inflexible in frequent refreshes for large-scale deployments since both application develop-ers and the service provider are involved in theoperation loop; ii) there is memory waste in case oflong system prompts and low hit rate; iii) no workhas been done to optimize the self-attention kernelin the presence of shared system prompts.To fill the gap, we propose ChunkAttention, anovel self-attention module featuring the prefix-aware KV cache (PAKV) and two-phase parti-tion (TPP). KV cache in ChunkAttention is a pre-fix tree built with chunked context tokens andkey/value tensors. Thus, the KV cache is prefix-aware and can dynamically detect and remove re-dundancy at runtime without human involvement.The KV cache only stores key/value tensors of se-quences currently in decoding and has zero mem-ory waste. In addition, the prefix-tree structureprovides context for ChunkAttention to redesigna highly-optimized self-attention kernel with two-phase partition: chunk-first phase and sequence-first phase. Query tensors from sequences withmatching prompt prefixes are batched together toperform attention with key/value tensors.The main contributions of this paper are as fol-lows: i) we reveal that system prompts can be long(§2.1), providing opportunities for optimizing self-attention; ii) we propose to use prefix tree to imple-ment KV cache, which is out-of-the-box, scalableand robust in terms of redundancy removal; iii) weimplement a two-phase partition algorithm to speedup self-attention kernel on prefix-aware KV cache;iv) we prove the feasibility and empirically quantifythe gain self-attention can achieve from shared sys-tem prompts under various system configurations.Our experiments show that ChunkAttention can besignificantly faster as the length of shared systemprompts grows and has no performance degrada-tion without shared system prompts, compared toexisting highly optimized implementations.2 Preliminaries2.1 Shared System PromptOne paradigm in designing LLM-based applica-tions has been the introduction of system prompt(Anthropic, 2023). It provides instructions, few-shot examples (Dong et al., 2022), and externalknowledge as context for LLMs to generate betterresults. The final prompt to LLMs is a concatena-tion of system prompt and task-specific input. TheBatch Size Roofline QKV Projection Self Attention MLP1FLOPs( ×106) 100.66 33.57 270.53MOPs( ×106) 100.70 33.85 270.62Arithmetic Intensity 1.00 0.99 1.00Latency(µs) 88.44 17.82 160.7732FLOPs( ×106) 3221.23 1074.27 8657.04MOPs( ×106) 101.71 1083.18 273.43Arithmetic Intensity 31.67 0.99 31.66Latency(µs) 90.02 687.74 209.8264FLOPs( ×106) 6442.45 2148.53 17314.09MOPs( ×106) 102.76 2166.36 276.33Arithmetic Intensity 62.69 0.99 62.66Latency(µs) 98.04 1358.40 217.79Table 1: Complexity analysis of key modules in eachdecoder layer when decoding one single token. Llama27B, 2048 context tokens, FP16, A100 (80G). The self-attention module has low arithmetic intensity (Williamset al., 2009) and high latency. FLOPs: floating point op-erations. MOPs: memory operations or memory bytesaccessed. A\n",
      "----------------------------------------------------------------------------------------------------\n",
      "lso indicates the wavelength channel. The key objective is to define a number of SLAs,expressed in terms of maximum latency and compliance level, and to propose an algorithm that can operate theBandwidth Map merging operation, minimising the probability to breach SLAs, across all VNOs.The evaluation of our work is based on two key performance analysis. First we compare the latency for systemsusing different line rates and number of channels. Assuming an overall PON capacity of 200Gb/s, we considera system with 8 channels at 25Gb/s, one with 4 channels running at 50Gb/s, and one with a single channel at200Gb/s (being single channel there is no multi-wavelength allocation for the 200Gb/s option). Then we showhow the difference in performance changes when we consider different tuning times for the ONU transmitters. Forthis we compare the case of negligible tuning time (i.e., if the system implements channel bonding, [4], so that theONU has more than one transceiver always ready to transmit at least at another wavelength); the case of Class 1transmitters (i.e., tuning time <10µs) [5], with tuning time of 250 ns (i.e., close to the burst overhead time) and 1us; and a Class 2 transmitters (i.e., tuning time >10µsbut<25ms), with tuning time of 15 us. Class 3 devices arenot considered has they have tuning times longer than 25 ms, which is more than two order of magnitudes higherthan the PON frame, and would in practice represent a semi-static allocation.Our results show that our proposed multi-wavelength algorithm is capable of maintaining high percentage ofSLA compliance, even for high network load. In addition, we highlight an important trade-off between the latencyof multi-channel systems and the transmitters’ tuning time. If the tuning time is negligible compared to the burstoverhead, the multi-channel system has better latency performance. This is due to the fact that the burst overheadtends to have similar time duration independently of the line rate [6], thus higher line rate channels require aproportionally larger amount of bits than lower line rate channels. However as the ONU tuning time increases, thesingle channel system outperforms the multi-channel ones.2. System architectureThe use case and architecture addressed in this work is shown in Fig. 1. In our approach, multiple VNOs rundifferent schedulers in parallel (vDBAs), each forwarding a virtual Bandwidth Map (vBMap), which allocatesupstream transmission slots to a group of ONUs. This for example allows multiple mobile operators to run fron-thaul services (i.e., 7.2 RAN split), using independent Cooperative Transport Interfaces (CTI) The schedulinghypervisor (or merging engine) collects all such virtual bandwidth maps to create a single physical bandwidthmap, resolving any collisions between slots overlapping in time. Such collisions can be resolved by delaying someof the grants (at the risk of breaching SLAs). In this multi-wavelength approach, the OLT also has the optionto select transmission over different channels, to minimise grant delay, although this is constrained by the ONUtuning time. Our merging engine makes these decisions based on specific Service Level Agreements (SLAs), sothat it minimises the probability of breaching SLAs (which is key for supporting 5G and future 6G services). ThearXiv:2402.15222v1 [cs.NI] 23 Feb 2024Fig. 1: TWDM multi-tenant upstream schedulingalgorithmFig. 2: MILP Notations and Equationsuse of a stateful algorithm, which takes into consideration the history of a service flow when making schedulingprioritisation decisions, is preferred to stateless algorithms [7]. This is because a stateful algorithm can prioritiseflows depending on how close they are to breaching their specific SLA target [8].Thus in this work, we propose a heuristic stateful TWDM scheduling algorithm. The objective of the algorithmis to maximise the SLA compliance across all the flows during upstream transmission. We focus specifically onthe additional latency introduced by the multi-sharing aspect of the PON. An SLA breach occurs when a givenflow accumulates a number of delayed upstream slots that is above its target SLA threshold. For example for anSLA with maximum merging delay of 25 µswith 99% compliance, every time an upstream slot is delayed bymore than 25 µswith respect to the requested time slot in the virtual BMap, we increment a counter. If the countergoes above the non-compliance rate (in this case 100-99=1%), calculated over a number of frames (i.e., we use a1ms window, which is the time duration of a 5G sub-frame), then we consider that an SLA breach has occurred.The problem can be formulated as Mixed Integer Programming (MIP) as shown in Fig. 2. Equation (1) calculatesthe maximum delay of any given allocation to remain within the target threshold for SLA breach. We maintain a 4dimensional binary decision variable matrix Xwhere the objective function is explained as follows - the inner truthvalue function calculates the packet level b\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n that a neural networksuffers a significant performance degradation on the previous task after training with a new dataset. Many excellentworks have been proposed to overcome catastrophic forgetting, such as replay methods(Tiwari et al., 2022; Lin et al.,2023; Wang et al., 2023), parameter isolation methods(Xue et al., 2022; Kang et al., 2023; Jin & Kim, 2022) andregularization-based methods(Kim et al., 2023; Lin et al., 2022; Wang et al., 2022). The essence of all these worksis to constrain the backbone of neural networks, reducing the variation of the prediction on the old task—it means, invisual classification scenario, maintaining the classification accuracy of the old task after training on the new dataset.In fact, the variation of the prediction is determined jointly by the backbone and the classifier, but the impact of theclassifier is underestimated.In this paper, we try to solve a problem: if the forgetting caused by varying backbone is unavoidable, how can wemitigate its effect by regularizing the classifier? The idea of regularizing the classifier originate from a phenomenon:given multiple binary classification tasks that sharing a same backbone, some tasks always forget more knowledge thanthe other; after reordering these tasks, it still happens. To explore this phenomenon, we simulate the variation of thebackbone by adding a noise in the latent representation of the final layer and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continuallearning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnableclassifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affectingthe performance of the network. In second stage, FRCR rearranges the entries of new classifiers to implicitly reducethe drift of old latent representations. The experimental results on multiple datasets show that FRCR significantlymitigates the model forgetting; subsequent experimental analyses further validate the effectiveness of the algorithm.1arXiv:2402.15227v1 [cs.LG] 23 Feb 2024Preprint.2 R ELATED WORKSContinual learning. The goal of continual learning can be summarized as ensuring no degradation in performanceon the old task after the model trains on the new task, provided that the dataset of old tasks is unavailable. Con-tinual learning algorithms can be categorized into three classes: replay methods, parameter isolation methods, andregularization-based methods(De Lange et al., 2022).Replay methods mix the samples or pseudo-samples from olddatasets into the dataset of the new task to retain the model’s performance on the old task(Rebuffi et al., 2017; Shinet al., 2017). Parameter isolation methods train different tasks with different network parameters to avoid the interfer-ence between old tasks and the new task(Aljundi et al., 2017; Rusu et al., 2016; Mallya et al., 2018; Fernando et al.,2017). The regularization-based methods add a regularization term in the loss function during training the new task tosuppress the variation of the parameters that are highly correlated with the old task(Kirkpatrick et al., 2017; Aljundiet al., 2018; Wang et al., 2021).Except for three types of methods mentioned above, some new methods have emerged in recent years(Farajtabar et al.,2020; Mirzadeh et al., 2020b;a). Stable-SGD(Mirzadeh et al., 2020b) investigates the several factors affecting contin-ual learning, including batch size and learning rate decay, then proposes a naive continual learning algorithm withoutusing replay samples, parameter isolation, and regularization terms. The proposed method in this paper is similar tostable-SGD(Mirzadeh et al., 2020b); without saving the old dataset, expanding the network capacity, or preserving themodel parameters, it mitigates the forgetting just by freezing and rearranging the classifiers. To evaluate the perfor-mance of the proposed method, it is compared with stable-SGD(Mirzadeh et al., 2020b) and EWC(Kirkpatrick et al.,2017) in the experiment section.Fixed classifier. Although learnable classifiers are common for neural networks on classification tasks, some workusing fixed classifiers has also appeared in recent years(Yang et al., 2022; Li et al., 2023; Boschini et al., 2023). Yanget al. (2022) directly computed the optimal structure of a classifier based on the ETF framework(Papyan et al., 2020)and used this optimal structure as a fixed classifier. Boschini et al. (2023) designed a fixed classifier based on a d-Simplex regular polytope and used it for class incremental scenarios. Unlike these works, this paper does not use anytheoretical assumptions in designing the fixed classifier, and simply achieves continual learning by rearranging theentries of the classifier based on the original initialized classifier.3 P RELIMINARIESGiven multiple binary classification tasks, consider a thr\n",
      "----------------------------------------------------------------------------------------------------\n",
      "oners to participate.CCS CONCEPTS•Software and its engineering →Development frameworksand environments .KEYWORDSbuilding energy management, building control, environment, bench-mark, evaluation, reinforcement learning, smart building1 INTRODUCTIONGlobal scale efforts are required to mitigate the most severe conse-quences of climate change including a significant increase in energyefficiency of consumers as well as the decarbonization of energysupply [ 49]. The vast utilization of renewable energy sources re-quired for the latter will additionally likely induce an increaseddemand for energy flexibility by consumers [ 1,26,38]. EnergyManagement Systems (EMSs), in a sense of software computing op-timized operational schedules and executing these on devices, havebeen demonstrated to be capable of reducing energy demand, low-ering𝐶𝑂2emissions and/or unlocking flexibility [ 5,10,37,43,44].In order to make the desperately needed global impact however,energy management solutions will be required at scale, like e.g.applied to thousands of buildings.This paper aims at supporting the widespread adoption of EMSsby enabling the provisioning of forecasting and optimization algo-rithms for energy management at scale. Forecasting and optimiza-tion algorithms are essential parts of EMSs and have traditionallybeen developed for a single specific target (e.g. for one particularbuilding) [ 52], like in [ 5,10,37,43,44] or the publications reviewedby [46]. An optimized schedule1for the operation of the target isusually the desired output of the optimization algorithm which mayrequire one or more forecasts to compute the optimized strategy.For example, in order to compute the optimized operation strategyfor a battery storage system in a private household with local en-ergy production from a Photovoltaic (PV) plant in combination with1It should be noted that this work is applicable to similar approaches that do notcompute forecasts or schedules, consider e.g. an algorithm that estimates the occupancyin a building from limited information. Or even further, an algorithm (like in [ 8]) thatderives the state of an electricity grid. The provisioning of such algorithms as servicesis clearly useful in the wider sense of energy management. Nevertheless, in this workwe often refer to forecasting and optimization or about the retrieval of a forecast oroptimized schedule to promote readability. This is not meant to exclude other notstrictly covered but related algorithms.arXiv:2402.15230v1 [cs.SE] 23 Feb 2024David Wölfle, Kevin Förderer, Tobias Riedel, Lukas Landwich, Ralf Mikut, Veit Hagenmeyer, and Hartmut Schmecka dynamic electricity price, the optimization requires predictions offuture development of the energy price, the electric load, and thepower generation of the PV system.Economic viability is certainly a key factor for the widespreadadoption of EMSs, in particular as it has been shown that the devel-opment costs of target specific EMSs are higher than the monetarysavings, even for medium sized commercial buildings [ 19]. Thisbecomes even more obvious, if one considers the complexity of theEMS required for optimizing the battery storage system outlinedin the example above and compares it to the typical energy costsof private households. The way out of this dilemma promoted inthe present paper is to split the complex optimization procedureinto standardized parts and provide these parts as a service. In thecontext of this work, a service refers to a web-based program thatprovides a limited functionality required for energy managementapplications via a standardized interface. Regarding the exampleabove one could consider four dedicated services: The first for com-puting load forecasts. The second for computing the PV powergeneration forecast. The third service provides the electricity priceforecast and finally the fourth service that computes the optimizedschedule for the battery, based on the former three forecasts asinput. This service concept allows the vast replacement of customtailored optimization algorithms with standardized services, whichcan be utilized by potentially thousands of EMSs, thus enablinga dramatic cost reduction especially in combination with highlyautomated Machine Learning (ML) approaches.With this paper our contribution towards forecasting and op-timization for energy management at scale is twofold. Firstly, weprovide a framework that allows the provisioning of forecastingor optimization code as a web service. To that end we begin bycarrying out an extensive analysis to specify requirements (Sec-tion 2). Based on this, we present a sophisticated design conceptthat fulfills these requirements (Section 3) and finally derive an im-plementation of our concept (Section 4), which we release as a freeand open source repository alongside this publication. Our secondcontribution is the presentation of our concept for the Open EnergyServices community (Section 5), a group which initially consists ofthe institutions part\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pendency of the target task on large-scale data and en-hance the reuse of source data and models. The pre-trainingand fine-tuning paradigm is the most representative paradigm,where the model is initially pre-trained on a source datasetand then fine-tuned for the target task. In recent years, a sub-stantial number of pre-trained models have been available tothe public, which have become one of the cornerstones ofdeep learning [Youet al. , 2021 ].∗Corresponding authors: Bo Jiang and Jian Liang.Figure 1: The paradigm of model transferability estimation.However, different target tasks benefit from distinct sourcemodels and datasets. Hence, given a specific target task, de-termining which one is the most suitable has become a keychallenge. A simple solution is to brute-force train the tar-get task on them individually, which is computationally pro-hibitive. Model transferability estimation (MTE) aims to pro-vide a metric to quantify this suitability efficiently. An il-lustration of MTE is demonstrated in Fig. 1. MTE methodspredict a transferability score for each candidate model andthese scores should be highly correlated with the transferredaccuracy obtained after training through transfer learning al-gorithms. Over recent years, MTE has achieved considerableadvancements in fields like computer vision [Agostinelli etal., 2022a ]and neural language processing [Baiet al. , 2023 ].In this paper, we present the first survey on MTE to in-troduce the recent advances in this field. We divide the MTEstudies into two primary realms, i.e., source-free model trans-ferability estimation (SF-MTE) and source-dependent modeltransferability estimation (SD-MTE). These two realms aredistinguished based on whether they require access to thesource dataset when evaluating transferability. For eachrealm, we provide a comprehensive taxonomy from the per-spective of the method and thoroughly review these advancedalgorithms.arXiv:2402.15231v1 [cs.LG] 23 Feb 2024Table 1: Comparison between model transferability estimation and some related learning paradigms.Learning paradigm Input Ground truthSource-free model transferability estimation Model hub, target set Target task accuracySource-dependent model transferability estimation Model hub, source set, target set Target task accuracyTask transferability estimation Source task hub, target set Target task accuracyGeneralization gap prediction Model, training set Difference between training and test accuracyOut-of-distribution error prediction Model, training set, test set with OOD samples Test set error rateSupervised validation Training checkpoints, validation set Test accuracyUnsupervised validation Training checkpoints, test set Test accuracyOur contributions can be summarized as follows,• To our knowledge, this survey is the first to systemati-cally overview two distinct topics in transferability es-timation: source-free model transferability estimation(SF-MTE, Sec. 3) and source-dependent model trans-ferability estimation (SD-MTE, Sec. 4).• We introduce a novel taxonomy of existing methods andprovide a clear definition for each topic. We hope thissurvey will aid readers in developing a better under-standing of the advancements in each topic.• We provide an outlook of recent emerging trends andunresolved problems in Sec. 6, offering insights into po-tential future research directions in the field of modeltransferability estimation.Comparisons with previous surveys. A previous survey ontransferability in deep learning [Jiang et al. , 2022 ], which ishighly relevant to our work, offers a comprehensive explo-ration of this concept across various domains of deep learn-ing. This survey interlinks isolated areas within deep learn-ing, illustrating their connection to transferability. Their workcan be viewed as a general overview of the entire field oftransfer learning. In contrast, our work specifically focuseson the methods that estimate the transferability of sourcemodels.2 Related Research TopicsWe compare model transferability estimation (MTE) withseveral related topics, shown in Table 1.Task Transferability Estimation (TTE). Task transferabil-ity[Zamir et al. , 2018 ], also known as task similarity, aims toexplore the relationship among visual tasks and offers a prin-cipled approach to identify redundancies across tasks. Thistopic is typically significant in multi-task learning and meta-learning problems. In these problems, highly similar taskscan often be jointly learned, leading to a decrease in the needfor annotation and improved performance. Some TTE meth-ods focus on finding the most suitable source task from a datahub for a target task, while the model information is agnos-tic. In contrast, MTE primarily focuses on the selection ofmodels from a model hub, considering that the model’s ar-chitecture, parameters, and training algorithms can all affecttransferability.Generalization Gap Prediction. Generalization Gap Predic-tion methods predict the difference between the accuracy ofthe t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r more nuanced and context-aware recommendations.Researchers have started to utilize the capabilities of agents tosolve recommendation tasks. Existing work like [ 15,21,23] pri-marily focuses on employing agents for simulating user or itembehaviors, providing insights into user preferences but falling shortof integration into RSs. On the other hand, some studies [ 5,16]attempt to leverage the capabilities of agents to directly build arecommender, primarily using one single agent with planning andmemory components and auxiliary tools (e.g., search engine). How-ever, there are various complex decision-making tasks in recom-mendation scenarios, on which single-agent instances are unableto perform well. Multi-agent collaboration, which is near to humanworkflows, is believed to accomplish complex tasks better withcollective intelligence. Although work [ 11] proposes a multi-agent∗Both authors contributed equally to this research.†Corresponding author.recommendation framework, it only has limited agent types and afixed collaboration mode.To better unleash the potential of multi-agent collaboration forrecommendation tasks, we propose MACRec , a novel Multi-AgentCollaboration framework for recommender systems, designed toharness the diverse capabilities of each agent. Notably, this frame-work differs from studies for simulation with agents but focuseson building a recommender directly. MACRec provides customiz-able agents with abilities powered by LLMs and useful tools. Forexample, we offer Manager to plan and manage task execution,Reflector to reflect on previous errors, User/Item Analysts to analyzeuser/item characteristics, Searcher to search more information us-ing the search tool, and Task Interpreter to translate the dialogs intoexecutable recommendation tasks. These agents with different roleswork collaboratively to tackle a specific recommendation task.Additionally, we provide application examples to use MACRecon various recommendation tasks, including rating prediction, se-quential recommendation, conversational recommendation, andexplanation generation of recommendation results. Considering thevarying requirements for agents in different scenarios, we show-case examples of selecting and customizing agents to collaborateon diverse recommendation tasks. Furthermore, we developed anonline web interface for our MACRec, providing a user-friendly vi-sualization of the agents’ collaboration process. The main strengthsof this work can be summarized as follows:•A New Multi-agent Collaboration Framework for Recom-mendation. Unlike previous studies focused on user/item simu-lation with agents, we propose a new multi-agent collaborationframework for recommendation MACRec . In this framework,agents with different abilities, work collaboratively are involvedto tackle specific recommendation tasks.•Diverse Applications on Recommendation Scenarios. Wepresent application examples on various recommendation sce-narios, including rating prediction, sequential recommendation,explanation generation, and conversational recommendation.•A User-friendly Online Web Interface. We developed anonline web interface for MACRec, visualizing how agents collab-oratively tackle tasks.arXiv:2402.15235v1 [cs.IR] 23 Feb 2024Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Wang and Yu et al.Table 1: Comparison between previous work and our MACRec. Note that Single-type Agents indicate all agents serve the samerole (e.g., users), while Multi-type Agents refer to agents having multiple roles and capabilities (e.g., managers, reflectors).Model Objectives Single-type Agents Multi-type Agents Diverse Rec. Scenarios Open-sourceRecAgent [15] User Simulation ! !Agent4Rec [21] User Simulation ! !AgentCF [23] U-I Inter Simulation !RAH [11] Recommender !RecMind [16] Recommender ! !InteRecAgent [5] Recommender !MACRec Recommender ! ! ! !2 RELATED WORK2.1 Agents-based RecommendationCurrently, research on integrating LLM-based agents for recommen-dation can be categorized into two primary orientations: simulation-oriented andrecommender-oriented approaches. Table 1 comparesour MACRec and previous agents-based work.The simulation-oriented work focuses on using agents to simu-late user behaviors and item characteristics in RSs. RecAgent [ 21]and Agent4Rec [ 15] both propose to use agents as user simulatorsto empower the evaluation of RSs, which feature single-type agents(as users). AgentCF [ 23] explores the simulation of user-item in-teractions through user-agents and item-agents. It belongs to amulti-type agent system, with only two types and simple interac-tions. This line of research aims to provide a deeper understandingof user preferences but falls short of integration into RSs.The goal of recommender-oriented studies is to build a \"recom-mender agent\" with planning and memory components to tacklerecommendation tasks. InteRecAgent [ 5] and RecMind [ 16] pri-marily focus on improving a single recommender agent’s planningand reflection ability. RAH \n",
      "----------------------------------------------------------------------------------------------------\n",
      "ds describingfont shapes directly (such as Sans-serif ) rather than words representing abstractideas, subjective feelings, and sensations. Throughout this paper, all of thesetags are referred to as impression tags without discrimination, unless there isconfusion.21https://www.1001freefonts.com/2In fact, discrimination of impression and non-impression tags is an interesting re-search topic by itself. The difference between them is very ambiguous. For example,arXiv:2402.15236v1 [cs.CV] 23 Feb 20242 K. Kitajima et al.trescilla protestan t stranger -back -in-the-nightGT:calligraphy, elegant, invitation, script, signature, weddingGT:lettering, sans serifGT:lettering, horror, halloween , scary,zombie, ghost, terror, werewolf, fright, dracula , fear, vampireFig. 1. Fonts and their impressions tags, provided by 1001freefonts.com .(b) Our exemplar -based impression estimation(a) Multi -label classification0.800.90.9⋮0elegantserifcoolpop⋮ classicalmulti -label impression classifierwordimageEstimated tags:elegant, cool, pop00.600.3⋮0.11234⋮𝑁exemplar font IDfont classifierelegantserifcoolpop⋮ classicaltags of exemplar Estimated tags:elegant, cool#occurrences ≥𝑝likelihoods ≥𝜃Fig. 2. Two approaches for the font impression estimation task. (a) Multi-label classi-fication, and (b) Our exemplar-based impression estimation. This is an example of theconditions θ= 0.8, ˜n= 2 and p= 2.This paper tackles the task of estimating font impressions of texts in thewild. This task is meaningful in various aspects. For example, we can knowwhat impression we get from a typographic work without questionnaires andsubjective tests. Assume photographs capturing packages of cookies. If we canestimate what kinds of impressions are expressed by the texts on the packages, wecan understand whether the package design fits the cookies. Such understandingthe tag boldcan be impression and non-impression. The tag heavy is rather an im-pression tag but can be a non-impression tag if it refers to a typographic term. Theseexamples simply indicate that tags cannot be separated into impression and non-impression classes but are on a continuous scale, like data from regression or rankingtasks. The authors expect that studies like this paper will help to understand thiscontinuous structure of impression tags.Font Impression Estimation in the Wild 3is also useful for designing new cookie packages. More generally, it helps to knowthe trends of font impressions in various typographic designs.Our task is not straightforward because our target is texts in scanned orphotographed images. If we only deal with digital documents (like PDF), it iseasy to estimate the impressions of the fonts in them; this is because they includethe list of fonts used in the document as metadata. The font impression tags ofthe text are readily obtained by searching the MyFonts website for the fontname. However, text images in the wild do not have such metadata, and thus,we need to estimate font impression tags without knowing the font name.One approach to estimating font impressions from a text image xis the multi-label image classification, as shown in Fig. 2 (a). This approach can be realizedby a standard convolutional neural network (CNN). Since each font has multipleimpressions (as we saw in Fig. 1), the CNN should be trained by supervisionwith K-dimensional multi-hot ground-truth vectors instead of one-hot vectors,where Kis the size of the impression tag vocabulary.However, this strategy results in poor estimation performance due to the in-completeness of the ground-truth. More specifically, even when the k-th elementof a K-dimensional ground-truth vector is zero, it often does not mean thatthe font does not show the k-th impression. The website 1001freefonts.comdistributes many fonts, and font creators freely attach the impression tags whenuploading the fonts on the website. Consequently, even if a certain font has thek-th tag, a similar font might not have the tag. This is the so-called “missing-label” problem and disturbs multi-label classification significantly. If we traina classifier with ground-truth vectors with missing labels, the classifier wronglylearns that the font does not show the k-th impression. It is inherently difficultto avoid the missing-label problem, especially for the impression labels. Even forfont experts, it will be hard to prepare a unanimous ground-truth vector for acertain font by determining whether the font shows each of Kimpressions.In this paper, we take another approach, exemplar-based impression estima-tion, instead of multi-label classification. Fig. 2 (b) shows our approach. In thisapproach, we first prepare a N-class font classifier by a CNN trained to classifythe input text image xinto one of Nexemplar fonts, R={R1, . . . , R N}. Theclassifier can give a similarity between the font of the input text and the n-thexemplar font as a probability of the n-th class, P(Rn|x). Now, we assume thatthe impression tags of each exemplar font are available\n",
      "----------------------------------------------------------------------------------------------------\n",
      " modalities are ambiguous,semi-supervised learning methods [4] with teacher-student*Contribute equally to this work† Joint last authors† AFF is supported by the Royal Academy of Engineering IN-SILEX Chair (CiET1919/19), UKRI Frontier Research Guarantee INSILICO(EP/Y030494/1), and EC Sixth Framework Programme @neurIST (FP6-2004-IST-4-027703).Fig. 1 . Visualization of 3DRA and MRA data reveals signifi-cant intra- and inter- domain shifts.structure [5] leveraging both labelled and unlabeled patcheshave been proposed to address the issue. However, whenthe challenge escalates further, with the network being fedunlabelled Magnetic Resonance Angiography (MRA) [6, 7]data as input, semi-supervised methods become inadequatefor supervision. To address the challenge of data of differentmodes without labelled data, unsupervised domain adapta-tion (UDA) techniques, such as FDA [8], DAFormer [9],HRDA [10], MIC [11], and MSCDA [12], have emerged.These methods transfer knowledge from well-annotatedsource domains to unlabelled target domains.These methods address the domain shift between sourceand target data. However, the domain shift is considerable,even from inner source data in the context of cerebral vas-cular images. Fig.1 shows that 3DRA and MRA exhibit asubstantial domain shift, with a significant domain shift evenwithin the 3DRA modality when clinical data from differentdata centres are scanned. Mitigating domain shifts for seg-mentation across other modalities is more challenging due tothe intricate nature of blood vessels, individual variabilities,and inherent noise and artefacts in imaging techniques.Our work introduces a unique method integrating insightsfrom unsupervised domain adaptation, semi-supervised learn-ing, and contrastive learning to address significant domainshifts in cerebral vascular images. The key innovations areas follows:•This work presents an innovative symmetric adaptationnetwork tailored for cross-modality segmentation of brainvasculature. This represents the first study to apply UDAtechniques to the segmentation of the cerebral vesselsfrom 3DRA to MRA.arXiv:2402.15237v1 [cs.CV] 23 Feb 2024•We introduce transwarp contrastive learning, a methodthat investigates features in the time and frequency do-mains within the latent space to achieve the alignment ofcontent and style.•Furthermore, we design a new homocentric squaresFourier domain adaptation to handle cross-domain nui-sance variability without explicit feature alignment.2. METHODSThis section presents our novel unsupervised domain adap-tation approach (see Fig.2) to learning instance-specific anddomain-invariant representations.Fig. 2 . Schematic of the proposed method. The method uti-lizes a composite loss function incorporating fully supervised,semi-supervised, and transwarp Contrastive Learning.2.1. Method Overview and Problem FormulationFor image style transfer, FDA has demonstrated that low-frequency components of images represent style features [8].Therefore, we utilise the low-frequency components of latentfeatures to extract style features from different inputs. Thisnovel paradigm in transwarp contrastive learning utilises astudent-teacher network architecture, incorporating both con-tent and style features using Fourier transform with a low-frequency mask, with the ultimate aim of narrowing the gapcaused by different data modalities and magnifying the invari-ant feature extraction capabilities of the model.As shown in Fig. 2, the student receives two inputs:labelled source domain data ( xsi) and unlabelled target do-main data undergone style transfer ( xt→si). On the contrary,the teacher processes unlabelled target domain data ( xti) andstyle-transferred labelled source domain data ( xs→ti). At thesame time, we undertake the extraction of both content andstyle features. From the student and teacher networks, thecharacteristics of the content in the time domain are definedaszsi,zt→si,ztiandzs→ti. In parallel, we extract style featuresin the frequency domain, capturing the Fourier low-frequencyattributes. These style features are expressed as ssi,st→si,sti,andss→ti.Fig. 3 . Homocentric squares Gaussian kernel KHSG for im-age adaptation on 3DRA (source) and MRA (target) vesselpatch.The prediction of the student network on the source do-main and the target on the source domain is denoted as psiandpt→si. We also denote the prediction of the teacher networkin the target domain and the source in the target domain as ptiandps→ti. Our goal is to learn a task-specific network using alabelled source data set {(xsi, ysi)}Nsi=1and an unlabelled tar-get data set {(xti)}Nti=1to predict labels on test data from thetarget domain accurately.2.2. Homocentric Squares Domain Adaptation (HSDA)The purpose of pre-processing is to remove noise and facili-tate the model extracting features specific to the vessels. FDAproposes that the style of an image can be migrated by re-moving the low-frequency amplitude between the target im-age and the source image \n",
      "----------------------------------------------------------------------------------------------------\n",
      "detection datasets still suffer from biases dueto ambiguous category definitions, keyword-basedsamplingstrategiesfavouringexplicitHS,aswellassubjectivity and disagreement in annotations (Wie-gand et al., 2019; Fortuna et al., 2022). Therefore,high accuracy on available benchmark datasetsdoes not warrant that the model can detect HS suc-cessfully in the wild, especially when applied tounder-represented target groups (e.g., disabled ortransgender people) or challenging functionalities(e.g., implicit HS and reclaimed slurs).To address the issue, Röttger et al. (2021) in-troduced HateCheck , a comprehensive suite offunctional tests that covers 29 model “functionali-ties”acrossseventargetgroups. Eachfunctionalitycaptures a specific kind of hate speech, e.g., “hateexpressed using slurs.” They handcrafted shortand unambiguous templates (Ribeiro et al., 2020)foreachfunctionalityandreplacedtokensfortargetgroupidentifiers(e.g.,“Ihate [identity] .”) andslurs(e.g., “You are just a [slur]to me.”) to generatetest cases at scale.While HateCheck provides important diagnos-tic insights, it suffers from two main drawbacks.Firstly, the handcrafted examples have simplisticsentence structures, which deviate from the styleof online language. Secondly, the template-basedsynthesization such as “All [identity] are stupid”does not account for distinct HS aspects associ-ated with different target groups (e.g., sexuality fortransgender people and criminality for immigrants).Therefore, even if a model obtains high accuracyfor a target group in the HateCheck dataset, it maystill struggle in the real world when encounteringnovel aspects not covered in the training data.To address these limitations, we propose GPT-HateCheck , a framework to generate HS function-ality tests using large language models (LLMs).We handcraft prompts to instruct GPT-3.5 (Ouyanget al., 2022) to generate test cases correspond-ing to the functionalities in HateCheck . Further-more,weemployanaturallanguageinference(NLI)model (Williams et al., 2018; Yin et al., 2019) to val-idate that the generated test cases correspond tothe gold-standard labels and the intended function-alities to be tested. Figure 1 provides an overviewof the proposed framework. We validate the qualityofGPT-HateCheck dataset through various auto-mated and human evaluations. Our contributionscan be summarized as follows:•We propose a framework to generate realisticanddiversefunctionalitytestsforHSdetectionusing LLMs.•We publish a new evaluation dataset, GPT-arXiv:2402.15238v1 [cs.CL] 23 Feb 2024Figure 1: The overview of GPT-HateCheck . We first instantiate the prompt template with the target groupin consideration and instruct GPT to generate candidate test cases. The test cases are then validated byan entailment model to ensure the generations conform with the functionality. In the example, althoughboth generated messages are hateful towards the target group, the second one does not contain profanityand will be discarded.HateCheck , to enable targeted diagnostic in-sightsintoHSdetectionmodelfunctionalities1.•Weconductanin-depthanalysisofthedatasetand demonstrate its utility by uncovering weak-nesses of a near state-of-the-art model thatare missed by HateCheck dataset.2. Related WorkTargeted diagnostic datasets are widely usedacross NLP tasks to shed light on model function-alities (Marvin and Linzen, 2018; Naik et al., 2018;Isabelle et al., 2017). Ribeiro et al. (2020) intro-duced CheckList , a task-agnostic methodologythat organizes test cases for NLP models based oncapabilities and test types. To generate test casesat scale, CheckList utilizes templates and maskedlanguage models to perturb existing datasets.Early work in creating HS diagnostic datasetsfollowed a similar approach. Dixon et al. (2018)synthesizedsetsoftoxicandnon-toxiccasesusingtemplates (e.g., “I hate all [identity] ”, “I am [iden-tity]”). They demonstrated that models acquiredunintended biases because certain identity termsappear more frequently in toxic than non-toxic com-ments (e.g., “queer”, “homosexual”). Paul Röttgerand others (2021) compiled a comprehensive testsuite comprising 29 functionalities as mentioned inSection 1. The functionalities were selected basedon a review of previous research and interviews1The source code and dataset are avail-able at https://github.com/YipingNUS/gpt-hate-check .with NGO workers who monitor and report onlinehate speech.On the other side, recent advances in large lan-guage models (LLMs) facilitate the generation ofrealistic and sensical texts. Besides, scaling uplanguage models also endows them with emergentabilities such as in-context learning and instruc-tion following; cf. (Wei et al., 2022; Zhao et al.,2023). Hartvigsen et al. (2022) prompted the GPT-3 model (Brown et al., 2020) with examples to gen-erate benign and hateful statements targeting 13minoritygroups. Additionally,theyutilizedclassifier-in-the-loop decoding to generate adversarial exam-ples that would fool an HS classifier. H\n",
      "----------------------------------------------------------------------------------------------------\n",
      "alization(DG) approach, where a model trained on data from mul-*Contribute equally to this work† Joint last authors† AFF is supported by the Royal Academy of Engineering IN-SILEX Chair (CiET1919/19), UKRI Frontier Research Guarantee INSILICO(EP/Y030494/1), and EC Sixth Framework Programme @neurIST (FP6-2004-IST-4-027703).tiple sources can adapt to new, unseen domains. The diversityof multi-source data makes DG a daunting challenge in medi-cal imaging, pushing the need for models that generalize wellacross different medical centers and data types.Fig. 1 . Illustration of the variability in imaging data qualityfrom different medical centers.Unlike traditional DG approaches such as domain align-ment [3], data augmentation [4], ensemble learning [5], self-supervised learning [6], disentangled representation learning[7], and others, our method takes a different approach. We en-hance domain generalization by leveraging gradient surgeryexponential moving average (GS-EMA), offering an innova-tive solution to address DG challenges.In deep learning, EMA is a frequently used techniquefor parameter averaging in models, aimed at enhancing thegeneralization performance and stability of the model. In ateacher-student [8] network setup, the teacher network under-goes a process of parameter smoothing, driven by the studentnetwork. However, initially, there are no specific conditionsset for this transfer. Consequently, all parameters learnedby the student network, whether they are domain-invariantor domain-specific, are updated into the teacher network atsome rate. This approach poses a challenge as it fails to dis-tinguish between domain-invariant and domain-specific pa-rameters. To address this issue, we introduce the concept ofgradient surgery.Deep neural networks are trained using gradient descent,where gradients guide the optimization process across thelandscape defined by the loss function and training data. Thegradient surgery framework [9, 10] aims to resolve conflictsarising in multi-task learning. The conflicting gradients aretypically averaged to obtain a final gradient for parameterupdates. GSMorph [11] propose alternative methods likearXiv:2402.15239v1 [cs.CV] 23 Feb 2024normal vector projection to derive the ultimate gradient forparameter updates. Instead of devising a new projectionmethod as suggested by others, we approach the problem byanalyzing the relationships between gradients to determinewhether EMA parameter updates should occur.Additionally, there is a class imbalance problem in 3Ddata segmentation due to the small proportion of aneurysms.After multiple downsampling steps, these small features aremore likely to be overlooked in the latent space. To tacklethis, we introduce the concept of boundary-awareness to tra-ditional contrastive learning [12].Contributions : Our study introduces innovative tech-niques that enhance model adaptability. We integrate gra-dient surgery with EMA updates, strengthening the abilityof model to learn domain-invariant features. This novel ap-proach promises to elevate the performance of DG tasks inmedical imaging, ensuring that our model can generalize ef-fectively to new datasets and medical centers. Additionally,we pioneer the use of boundary-aware contrastive learning,enabling our model to discern small target features especiallyfor cerebral aneurysms.2. METHODSFig. 2 depicts our neural network architecture dedicated fordomain generalization tasks. It initiates with 3D source im-ages, which undergoes image transformation to produce tar-get images. Once both the source and target images are ob-tained, they are separately fed into the encoders of the studentand teacher networks.After acquiring the latent space features, a boundary-aware contrastive learning loss is computed. The centralnotion here is to amalgamate the same instance subjected todiverse transformations, while distancing different instances,aiming to grasp instance-aware representations. This con-trastive learning differs from transformation predictions, as itstrives to attain transformation-invariant representations. Thelatent space features are then decoded to yield predictions,which are supervised using ground truth.Within the student network, the green arrow signifiesfully supervised learning for the source images, and the yel-low arrow represents the same for the target images. Byanalyzing the gradient relationship between these two losses,a novel GS-EMA strategy is devised to update the parametersof teacher network. If the gradient angle between the losses isless than 90 degrees, it indicates that the network has learneddomain-invariant features, prompting an EMA update. Con-versely, if the gradient angle exceeds 90 degrees, no EMAupdate is performed, as this suggests the network has graspeddomain-specific features, which is not conducive to domaingeneralization tasks. Ultimately, after several updates, ateacher network enriched with more domain-invariant fea-tures is achieved, readying it for domain ge\n",
      "----------------------------------------------------------------------------------------------------\n",
      "connected ground can effectively generateinfinite reaction forces and torques (i.e. wrenches) whenthe environment interacts with the robotic system [6], [15].However, in the context of floating-based systems, thisassumption no longer holds. In these systems, the actuation ofsuch systems becomes pivotal, responsible for supplying thenecessary wrenches during interactions with the environment.For an underactuated floating-based system, two key factorscome into play: (1) the saturation of the actuators, (2) thecoupling between linear and angular dynamics [3], [4]. Thesefactors collectively limit the magnitude and direction of theexerted wrenches that the robotic system can apply to theenvironment during physical interactions [15].This work has been supported by the European Unions Horizon 2020Research and Innovation Programme AERO-TRAIN under Grant AgreementNo. 953454.∗The authors equally contribute to the work.All authors are with Department of Electrical and Photonics Engi-neering, Technical University of Denmark. Corresponding author email:tonhu@dtu.dkFig. 1: A quadrotor-based aerial manipulator is pushing on a flat orientedwork surface.Industrial infrastructures are often composed of diverseoriented work surfaces. To achieve pushing tasks withunderactuated aerial vehicles on such surfaces, manipulatorswith at least 1-DoF (Degree of Freedom) are often attachedto the aerial vehicle [5], [9]. Research efforts have been madeon interaction control for such operations with underactuatedaerial vehicles [5], [9]–[14]. When interacting with diverseoriented surfaces, the coupled gravity compensation andinteraction force generation of underactuated aerial vehiclespresent the potential challenge of near-saturation operations.The blind utilization of these platforms for such tasks canresult in instability and accidents, leading to unsafe operatingconditions and platform damage. These issues underscorethe importance of identifying system limitations in suchoperations.In the work by Lassen et al. [16], they were among the firstto identify an operational envelope, encompassing pitch angleand thrust force parameters, necessary for an underactuatedaerial vehicle to maintain stable pushing with a flat verticalsurface. In our previous work [5], a static-equilibrium basedforce modeling approach is presented regarding to pushingon diverse oriented work surfaces. The singularity is outlinedvia predicting the total platform thrust using the derived forcemodels when interacting with surfaces at different orientation.The singularity highlights the limitations in exerting feasibleinteraction force magnitude associated with variable surfaceorientation considering total system saturation. To ensuresafe operations, it is however essential to guarantee that eachindividual actuator works normally within its saturation.A. Main ContributionIn order to ensure safe pushing on diverse oriented worksurfaces with an underactuated aerial vehicle, this workarXiv:2402.15243v1 [cs.RO] 23 Feb 2024work surfaceyEzEx yzαwβwxEφewφwzBφwyBxBFig. 2: System model in 2-D plane (yyy,zzz), coordinate frames, roll angle ofthe aerial vehicle ϕw, joint position αw, work surface orientation βw.establishes a safety assessment process based on the saturationlevel of each individual actuator during interactions. Thisprocess includes:•(I) identifying safe and critical operation zones duringpushing;•(II) assessing the risk level within the critical zone.The safe interaction zone is defined as the region where allactuators operate under nominal conditions, while the criticalzone involves at least one actuator operating near saturation.We apply this safety assessment process to a quadrotor-basedaerial manipulator system with an 1-DoF link attached to itfor a pushing task (see Fig. 1). This process is accomplishedby comparing the predicted thrust of each actuator using forcemodels derived in [5] with its saturation across differentsurface orientation. Furthermore, we use the safety assessmentresults to plan and conduct practical experiments, safelyvalidating the force model proposed in [5] and avoidingplatform damage.II. B ACKGROUNDIn this section, we briefly introduce the force modelingapproach for a pushing task on diverse oriented surfaces withan underactuated aerial vehicle presented in [5]. An aerialmanipulator composed of a quadrotor and an 1-DoF linkrigidly attached to it is used to push on flat oriented worksurfaces. The rigid link acts as the manipulator with a singlecontact point at the EE (End-Effector) tip. The axis alongthe length of the EE link is called the interaction axis. Formaintaining a stable pushing without slipping at the EE tip,it is desired to only exert a force along the normal vectorof the work surface at the contact point, i.e. zero lateralfriction forces. To do so, the system is oriented such that theinteraction axis coincides with the normal vector of the worksurface. Therefore, without effecting the main contribution,we consider a simplified\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ess of the proposed bargaining model.1 INTRODUCTIONFederated learning (FL) has become a popular machine learningparadigm for that it allows model training to happen across mul-tiple devices or institutions, aka. clients, while keeping their datalocalized. Vertical Federated Learning (VFL) is a subcategory of FLthat enables the training of clients’ datasets sharing the data fromthe same set of users while holding different features.The increasing demand for VFL is evident in recent industrialtrends [ 1–4]. Organizations with limited or fragmented datasetsare driven to partner with complementary data sources to amplifythe training of machine learning models. Alongside, global con-cerns about data breaches and privacy violations have amplifiedthe urgency for enhanced security measures. Consequently, a risein privacy-focused VFL platforms and initiatives has been observed[5–7]. While multi-party VFL is a subject of extensive academicresearch [ 8], insight from the China Academy of Information andCommunications Technology’s (CAICT) White Paper on Feder-ated Learning Scenario Applications1, [6], and [ 5], indicates thatin practical applications or production environments, VFL is oftenimplemented with a single task party and data party. Specifically,according to the white paper, this 1v1 VFL paradigm has been em-ployed by commercial banks to amalgamate external data whenconstructing joint anti-fraud models, establish potential insuranceusers alongside insurance companies, as well as by advertisers toconduct user modeling with data of external media platforms. Thecommercial relevance and economic impact attributed to this 1v1VFL model is steadily growing. Therefore, in this paper, we focuson the VFL setting characterized by a singular task party and asingular data party.In the current era, data has transformed into a valuable com-modity, often considered as goods with specific prices [ 9–13]. Thisvaluation is prominently seen in digital products, where informa-tion serves as both a resource and product, forming the backboneof modern services and technologies. The two key roles in VFLcan also be considered as data consumer and the data provider:the “task party\", responsible for labels and performing downstreamtasks, and the “data party\", supplying the essential feature data.Current works in VFL mostly concentrate on protecting the se-curity and privacy of data or information transmission betweenthe task party and the data party, ensuring that collaboration oc-curs without compromising the integrity and confidentiality of theinvolved parties. However, the intricate aspect of data valuation,which underpins many economic decisions in other contexts, hasnot been extensively explored or studied.In the current production environment, data trading in VFL ismostly conducted at party level. More specifically, the data partynames a price for its feature, and when a task party is paired withit for VFL, the task party buys all features offered by the data party.However, this may lead to undesired outcomes: 1) for the task party,buying all data from the data party, it also pays for features thatmay not be useful in enhancing model performance, and 2) for thedata party, as the utility of the data is contingent upon task parties’1http://www.caict.ac.cn/kxyj/qwfb/ztbg/202202/P020220222528294962585.pdfarXiv:2402.15247v1 [cs.LG] 23 Feb 2024Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, and Xiaofang Zhoudifferent application scenarios and production environments, itmay under-estimate/over-estimate the value of the features. Thisindiscriminate and one-shot trading of all features between partieshighlights the need for a more nuanced and economically efficientapproach for feature trading in VFL. Bargaining has proven to beeffective in various economic contexts where there are two players[14] participated, which permits parties to negotiate and agreeupon the value of individual features or combinations of features,resulting in a more flexible and efficient allocation of resources.The iterative process where a party could revisit its offers alsoencourages the achieving of mutual benefit. Inspired by this, wehere propose to introduce bargaining for the feature trading in VFL.Bargaining in the context of VFL diverges from traditional dataasset markets in several key aspects, and this brings non-trivialchallenges. First, in conventional one-to-one bargaining in datamarkets, the buyer seeks to obtain specific goods, such as datasets,from the seller [ 11]. The participants then negotiate the price ofthese goods. In contrast, within the setting of VFL, bargaining isresult-oriented. For the task party, it targets to maximize the netprofit of using the bought feature to perform VFL. For the dataparty, it aims at pricing the features with their utility consideredand maximizing the payment of selling features. As a consequence,the pricing mechanism should take model performance gain intoconsideration.Second, in conventional one-one bargainin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "not purely transactional1or interactional2but rather a mix of both (Brown and Yule, 1983),and overlap is therefore common in real-world in-teractions.Efforts have been undertaken to augment TODdatasets with chitchat, in order to move towardsmore flexible TODs (Li et al., 2023; Young et al.,2022). These augmentations are automatic as wellas human-generated and produce more interestingdialogues (Stricker and Paroubek, 2023), which hu-manstendtoprefer. However,itisimportanttonotethateachindividualturninthesedatasetsiscatego-rized as being strictly chitchat or task-oriented. Asaresult,itremainsunclearhowasystemtrainedonsuchdatamightreacttouserturnsinwhichchitchatand task talk are seamlessly combined (Figure 1).1the focus of the encounter is external and leads toan action2thefocusoftheencounterisinternal,andiscenteredon the relationship between participantsThese inter-mode user turns are important toconsider as they can commonly occur in human-chatbot conversations. Indeed, in their analysis oflive customer service chat logs, Beaver et al., 2020noted that users often engage in self-disclosurewhen making requests. For example, users fre-quently emphasize the significance of their tripsby sharing details about the people they are visit-That would be great. I need a table on Sunday at 15:45 for 2 people, please.My wife and I are planning to fly to Thailand for our honeymoon. We wanted to go to a Thai restaurant to prepare our taste buds for the food served there !Can you help me find an expensive Thai restaurant?Sala Thong is an expensive Thai restaurant on the west side. Would you like to book there?User Chitchat interferenceCongratulations on your recent marriage! I hope you both have a wonderful time in Thailand.I have made your reservation. Is there anything else I can assist you with today?System Chitchat ReactionFigure 1: A chatty user incorporates elements of back-story to their task-oriented request, creating a naturalinterference in the TOD interaction. We design the sys-tem reaction to accommodate the additional backstorywithsupportandunderstanding,allthewhileavoidingtheintroduction of new topics. This design choice ensuresthat the system seamlessly transitions back to the taskat hand, effectively assisting the user in achieving theirgoal.arXiv:2402.15248v1 [cs.CL] 23 Feb 2024ing or the reasons for their travel. In this work, weexamine this form of interference more closely.Although Beaver et al., 2020 have made a por-tion of the studied data accessible, it only includesthe initial user turns from each conversation. Thislimitation, resulting from privacy restrictions relatedto live company chat logs, complicates the task ofevaluating and potentially enhancing TOD agentsin similar situations for researchers.To remedy this gap, we present a dataset thatincludes instances where a chatty user introduceselements of backstory following their task-orientedrequest. In its response, the system first reacts tothe user backstory before continuing with the task.Such user turns have the potential of derailing theTOD interaction, and thereby challenge the systemto skillfully manage both the chitchat and the redi-rection of the conversation back to the task (Figure1).Crafting these scenarios manually is stren-uous. We consequently set out to createthese examples automatically, deriving them fromhuman-generated chitchat exchanges in Fused-Chat (Young et al., 2022) (Section 2), a version ofMultiWOZ (Budzianowski et al., 2018) augmentedwithfullchitchatexchanges. Byleveragingtheinfor-mation presented by the user in these exchanges,we automatically enhance a number of MultiWOZdialogues.Toevaluatetheeffectsoftheaddedchitchatinter-ferences, we utilize SimpleToD (Hosseini-Asl et al.,2020), a popular end-to-end approach that relieson a single language model (Section 3). Given theimpressive performances of recent state-of-the-artmodels, we create a strong baseline by combiningthis approach with one such model, LLama-2-7B(Touvron et al., 2023), and LoRA (Hu et al., 2021),a parameter-efficient fine-tuning technique.We use this approach to train two baseline sys-tems. For a system trained solely on TOD, weadheretothestandardtrainingprotocolfor Simple-ToD, utilizing unaugmented MultiWOZ dialogues.To test a system that has been exposed to chitchatunder a different form than in our enhancements,we train a SimpleToD-fused variant on Fused-Chat dialogues. These are MultiWOZ dialoguesto which a chitchat exchange has been prepended(Appendix A). Furthermore, to estimate the effec-tiveness of our automatically-generated data fortraining purposes, we train a SimpleToD-inter ver-sion. We find our data can indeed allow a model tosmoothly and consistently handle user backstorieswhile advancing the task in the same turn.Overall, we make the following contributions:•We introduce a novel task that treats userchitchatasadisruptiveelementinTODs,merg-ing task-oriented requests and elements ofbackstory into a single user turn.•We automatically construc\n",
      "----------------------------------------------------------------------------------------------------\n",
      "d algorithms, and HistoCoreoutperforms all other Index2core -based algorithms. Furthermore,HistoCore even outperforms PeelOne by1.1× ∼ 3.2×speedupon six datasets, which breaks the stereotype that the Index2coreparadigm performs much worse than the Peel in a sharedmemory parallel setting.Index Terms —large-scale graph, core decomposition, graphcomputing, GPUI. INTRODUCTIONGiven a graph G= (V, E), for an integer k, ak-core isa maximum subgraph of Gwith all the vertices degree ≥k.The coreness of vertex v∈Gis the maximum value of kfor which there is a k-core that contains v. The target of coredecomposition is to determine the coreness of each vertex v∈G. We illustrate core decomposition in Fig. 1. The entire graphis a 1-core, while vertices {v2, v3, v4, v5}form the 2-core. No3-core is present. Therefore, the coreness of vertices {v0, v1},and{v2, v3, v4, v5}are 1 and 2, respectively.Due to the simple and elegant structure with linear com-plexity [1], the k-core is widely used in many applications.In social networks, researchers employ hierarchical subgraphprocessing to accelerate intensive graph clustering [2], cliquefinding [3], and community detection [4], [5] and search [6].The coreness can help user engagement, prevent unravelingand improve network stability in social network [7]–[10].Furthermore, k-core is an effective tool to predict and visualizethe functions of complex structures in biology or ecology [11],[12]. Numerous studies explore core decomposition in diversenetworks with rich semantics, such as directed graphs, uncer-tain graphs, dynamic graphs and others [13]–[16].Fig. 1: An illustration of k-cores and coreness resulted fromcore decomposition in the example graph G1.There are many algorithmic techniques for core decom-position in different settings [17], originating from the ini-tial proposal of the k-core concept by Seidman [1]. Thesetechniques can be classified into two paradigms: Peel [1]and Index2core [18]. In the Peel paradigm, the algorithmiteratively removes vertices with the minimum degree until allthe coreness values are obtained. In the Index2core paradigm,theh-index value of each vertex is computed iteratively untilconvergence to the coreness is achieved. From the perspec-tive of vertex convergence dependency, the Peel paradigm isbottom-up, while the Index2core paradigm is top-down.With the continuous growth of the data scale and thewidespread application of k-core, the pursuit of optimal per-formance remains ongoing. Graphics Processing Units (GPUs)have gained significant popularity for accelerating graph pro-cessing algorithms and applications due to their excellentparallel computing capabilities and memory bandwidth. Toexplore core decomposition in a massively parallel setting,we have conducted research on various works that focus onarXiv:2402.15253v1 [cs.DC] 23 Feb 2024accelerating the k-core algorithm on GPU [19]–[22].Though significant efforts have been made to improve theefficiency of the Peel paradigm on GPU, the existing worksare implemented at different programming levels, including thelatest work [21]. Therefore, they do not reveal the key optimalparallelization design of the core logic of the Peel algorithm.One observation is that the complete peeling process on theobjective graph is overly redundant for calculating the corenessof vertices. For a vertex with a coreness value of k, thecoreness can be identified when its degree is reduced to k.However, the peeling process aims to reduce every edge in theobjective graph. During the reducing, the residual degree ofsome vertices might be different from the coreness value. Thisleads to some unnecessary atomic computational overhead andstorage costs.On the other hand, the parallelism potential of Index2coreon GPU has not been thoroughly explored. Index2core grad-ually approximates the coreness value of each vertex byiteratively estimating its h-index. In each iteration, the h-index value change of a frontier is estimated based on thelatest values of its neighbors from the previous iteration.Thus, existing Index2core methods regard its neighbors asnew frontiers in the next iteration, once the h-index of currentfrontier is changed. However, only a small portion of neighborswill actually be affected by the changes in the h-index offrontiers, resulting in a large number of mistaken frontiers(the estimation remains unchanged) in the next iterations.Moreover, the computational cost of the h-index update oper-ation for a frontier is positively correlated with the number ofits edge accessing (i.e., degree), so that high-degree verticesincur heavier computational costs. Those high-degree vertices,potentially requiring more iterations to converge to their core-ness, become ‘multi-changed’ vertices. Consequently, a smallnumber of multi-changed vertices end up with a significantlylarger computational workload, leading to an imbalance inthe workload distribution and becoming a bottleneck for GPUparallel computing.In real scenarios, the Peel p\n",
      "----------------------------------------------------------------------------------------------------\n",
      "IRO’s Data61, Australia.Correspondence to: Vy V o <v.vo@monash.edu >.et al., 2019; Zheng et al., 2020). Whereas existing struc-ture learning algorithms mostly deal with complete data,practical real-world data are often messy, potentially withmultiple missing values. Eliminating samples affected by‘missingness’ and solely analyzing the observed cases areundesirable, since not only would it decrease the samplebut also introduce bias to the estimations (Tu et al., 2019;Mohan & Pearl, 2021). Another strategy is to impute themissing values. The key challenge of imputation lies ineffectively modeling the data distribution when dealing witha substantial number of missing values. Pioneered by Little& Rubin (2019), missing data mechanisms are commonlyclassified into three types: Missing Completely at Random(MCAR), Missing at Random (MAR), and Missing Not atRandom (MNAR). Mohan & Pearl (2021) further proposemodelling the missing process with causal graphs, and givensuch a graph, present conditions where the joint distributionis identifiable. Without considering the underlying causes ofmissingness, learning an imputation model from observeddata is prone to bias wherein one may condition on somespurious variables that introduces extra relations that donot exist in the true graph (Kyono et al., 2021). It is in-deed a radical challenge since the causal graph is mostlyunknown in practice. Given that our goal here is to discoverthe causal structure from missing data, this indeed poses achicken-and-egg problem.A straightforward approach to dealing with our problem isto impute the missing values and subsequently apply anyexisting causal discovery method. This is however not aneffective strategy. Figure 1 illustrates the quality of imputa-tion (by Euclidean distance) in relation to causal discoveryperformance (by F1 score) across different missing ratesand mechanisms. We study 3popular imputation methods:Mean imputation, Iterative imputation and Optimal Trans-port (OT) imputation (using Sinkhorn divergence) (Muzel-lec et al., 2020). Iterative imputer consistently yields thebest imputation, which however does not necessarily lead tobetter causal discovery, as shown more obviously in MARand MNAR cases. It is possible that the filled-in data distri-bution encodes a different set of independence constraints,resulting a distribution compatible with a different causalgraph from the true one. The sub-optimality of such a naiveapproach has also been reported in the prior research (Kyonoet al., 2021; Gao et al., 2022), implying that “good” impu-1arXiv:2402.15255v1 [cs.LG] 23 Feb 2024Optimal Transport for Structure Learning Under Missing Datatation in terms of reconstruction does not guarantee “good”imputation to causal discovery. This motivates the devel-opment of an end-to-end machinery dedicated to structurelearning in the presence of missing data.Depending on the base causal discovery algorithm (designedfor complete data), existing methods can also be categorizedas either constraint-based or score-based. Constraint-basedmethods are mainly built up on the PC algorithm, which ex-ploits (conditional) independence tests (Strobl et al., 2018;Tu et al., 2019; Gain & Shpitser, 2018). Score-based causaldiscovery has recently taken off, owning to advances in ex-act characterization of acyclicity of the graph (Zheng et al.,2018; Yu et al., 2019; Bello et al., 2022). Extension of score-based approach in the context of missing data is nonethelessless studied1. A notable method in this line of research isMissDAG (Gao et al., 2022). MissDAG proposes an EM-style algorithm (Dempster et al., 1977) to point estimate thegraph. In the E step, MissDAG imputes the missing entriesby modelling a posterior distribution over the missing partof the data. This gives rise to the expected log-likelihood ofthe complete data, which MissDAG maximizes to estimatethe model parameters in the M step. MissDAG has shown asignificantly improved performance over constraint-basedmethods. However, the framework suffers from two keydrawbacks. First, the time inefficiency issue of vanilla EMhinders scalability of MissDAG. Second, a large part of thework focuses on linear Gaussian models where the posteriorexists in closed from. In the non-Gaussian and non-linearcases, MissDAG resorts to rejection sampling, which intro-duces extra computational overhead. Furthermore, approx-imate inference may compromise the estimation accuracygreatly, especially in real-world settings where the modeltends to be mis-specified. Section 4.2 reveals the poor per-formance of MissDAG on real datasets, where the methodfails to detect any true edges in several cases.Contribution. In this work, we introduce OTM - anOptimal Transport framework for learning causal graphsunder Missing data. From the viewpoint of optimal trans-port (OT, Villani et al., 2009), we fit structure learning intoa general landscape of density fitting problems, to whichOT has proved successful (Arjovsky et al., 2017; Tolstikhinet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "), aretypically computed to monitor and evaluate system behaviorand performance. Dashboards have to display these KPIsfor the convenient review of the current system state and togain insights [7]. To this end, commercial dashboard tools,such as Grafana [9] and Kibana [10], can be utilized toimplement dashboards [11]. These tools enable seamlessintegration with external data sources (e.g., a time seriesdatabase implemented with Elasticsearch [12]) and intuitivecustomization of dashboard panels.Despite monitoring systems already offer some degreeof automation in terms of configuration (e.g., auto-discoveryof monitoring targets [13], [14], [15], automated deploy-ment of probes [16], or switchable collection of KPIs [14],[17]), the configuration of the corresponding dashboardsis still mainly an error prone manual activity [18], [19].In large-scale and complex systems where the set of col-lected KPIs and monitored services can dynamically andfrequently change, maintaining up-to-date dashboards over-time can be a costly and burdensome activity [18], [19], [20].Consequently, dashboards should be ideally (re)configuredbased on bare-minimum information (e.g., collected KPIs),and users should rely on dashboard generators for all thevisualization-related concerns [18], [20].To address this problem, V ´azquez-Ingelmo et al. [19]presented a model-based approach for generating dash-boards that automatically configure themselves through ma-chine learning algorithms. Erazo-Garzon et al. [21] proposeda Domain-Specific Language (DSL) and a model-basedtransformation engine to automatically create dashboardsfor IoT applications. These studies emphasize the utilizationof Model-Driven Engineering (MDE) to manage dashboardevolution. In fact, model-driven approaches can employmeta-modeling techniques to create abstract dashboards thatadhere to basic guidelines and constraints.However, state-of-the-art approaches [18], [19], [21],[22], [23], [24], [25]. suffer from some key limitations.In fact, either operators are asked to explicitly model thevisualizations , which can be a tedious and time-consumingtask, or have little customization options once visualizationsare generated automatically. Further, existing approaches areall designed to target specific domains and environments .This short paper presents emerging results about thedefinition of a model-driven approach that can automaticallytransform a set of user-defined KPIs into a technology-agnostic representation of a dashboard , which is then trans-lated into an actual dashboard for a specific target technol-ogy. This chain of transformations provides several benefits:(1) operators can define and generate technology-agnosticdashboards representations, and iterate these activities, re-gardless of the dashboard technology that is chosen; (2)arXiv:2402.15257v1 [cs.SE] 23 Feb 2024automatic generation capabilities allow operators with noprior experience with dashboards to quickly obtain use-ful visualizations; (3) experienced operators can efficientlyadapt dashboards modifying the automatically generatedmodels, rather than wasting time interacting with the GUIof dashboard tools; (4) the second automatic transformationis the only step that depends on the selected dashboardtechnology, and it can be arbitrarily extended to supportadditional technologies.2. ApproachThe proposed approach, shown in Figure 1 and detailedin this section, consists of three phases, which are repre-sented by the yellow boxes. In the KPI Definition phase,the operator defines the dashboard model by specifying aset of KPIs to be visualized in the dashboard by means ofa graphical modeling language conforming to a dashboardmetamodel . Afterwards, the dashboard model is utilized bytheAutomatic Definition of KPI Visualizations phase thatenriches the model with details about how the KPIs arepresented (e.g., how KPIs are distributed among visualiza-tions and the types of visualizations used). Lastly, the Auto-matic Dashboards Generation uses the enriched dashboardmodel to generate the actual dashboard code according to atechnology-specific dashboard format (e.g., a Grafana dash-board JSON model). The corresponding artifacts producedby these phases, represented with white boxes, are locatedon the right-hand side of Figure 1. The solid black line withbidirectional arrows indicates that the operator can freelymove through these phases in any order.2.1. KPI DefinitionDuring the KPI definition phase, the operator identifiesand specifies the KPIs that must be visualized in the dash-board that has to be generated. These KPIs can be collectedand stored within any system in a SoS, as long as theyare accessible from an interface. In our work, we mainlyconsider the case of time series databases whose data canbe extracted with queries, which is the de facto standardsolution to store KPIs in analytics systems.The specification produced by the operator must beconsistent with a dashboard metamodel that specifies allthe concepts useful\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".wang@manchester.ac.uk >.Preliminary work.be possible. For example, in search and rescue, a robotmust collaborate with other robots it has not seen before(e.g., manufactured by various companies without a com-mon coordination protocol) or humans to rescue survivors(Barrett & Stone, 2015). Similar situations occur in AI thathelps trading markets (Albrecht & Ramamoorthy, 2013),as well as in the human-machine and machine-machinecollaboration emerging from the prevailing embodied AIsettings (Smith & Gasser, 2005; Duan et al., 2022) and largelanguage models (Brown et al., 2020; Zhao et al., 2023).To tackle the ad hoc teamwork problem, in this paper we ex-plore a scenario where one agent, referred to as the learner ,operates under our control and seeks to collaborate withoutprior coordination with teammates which have unknowntypes and policies (Stone et al., 2010). When dealing withteams of dynamic sizes, commonly termed open teams ,the research problem addressed in this paper is often re-ferred to as open ad hoc teamwork (OAHT) (Mirsky et al.,2022). The current state-of-the-art solution for OAHT isgraph-based policy learning (GPL) (Rahman et al., 2021).GPL presents an empirical three-fold framework, encom-passing a type inference model, a joint action value model,and an agent model, to tackle this problem. Although GPLoutperforms other methods, its weakness is that the repre-sentation of the joint Q-value is incomprehensible. This notonly hampers its further development but also restricts itsapplicability to real-world problems requiring trustworthyand interpretable algorithms (Wang et al., 2021).We propose to describe OAHT using a game model from co-operative game theory, namely the coalitional affinity game(CAG) (Br ˆanzei & Larson, 2009). Specifically, we extendthe CAG by incorporating Bayesian games (Harsanyi, 1967)to depict uncertain agent types and stochastic games (Shap-ley, 1953) to represent the long-horizon goal. The resultinggame is termed the open stochastic Bayesian coalitionalaffinity game (OSB-CAG). In this game, the learner aimsto influence other teammates (via its actions) to collaboratein achieving a shared goal. To formalize this, we extend thestandard cooperative game theory notion of strict core to anovel solution concept which we call dynamic variationalstrict core (DVSC). The DVSC transforms collaboration ina temporary team into the task of forming a stable temporaryteam, where no agent has incentives to leave. We model theOAHT process under the learner’s influence as a dynamic1arXiv:2402.15259v1 [cs.MA] 23 Feb 2024Open Ad Hoc Teamwork with Cooperative Game Theoryaffinity graph , generalizing the classical static CAG. Basedon the dynamic affinity graph, we further conceptualize anagent’s preference for a temporary team to measure whetherthey prefer to stay in the team under the learner’s influence.The sum of any temporary agents’ preferences over a longhorizon has been proven to be consistent with GPL’s jointaction value model.The main contributions of this paper can be summarized asfollows: (1) We conceptualize OAHT as a dynamic coali-tional affinity game, OSB-CAG. In this model, the learnerseeks to influence teammates through its actions, withoutprior coordination, to establish a stable temporary team. (2)The theoretical model of OSB-CAG gives an interpretationfor GPL’s joint action value model. It ensures collaborationwithin any temporary team under open team settings. (3)Building on the OSB-CAG theory, we derive a constraint forrepresenting the joint action value to facilitate learning, andan additional regularization term depending on the graphstructure to rationalize solving DVSC as an RL problem.These elements are absent in GPL. The novel algorithm,named CIAO ( Cooperative game theory Inspired Ad hocteamwork in Open teams), is implemented based on GPLand incorporates these novel features that we have derived.(4) We conduct experiments, primarily comparing two in-stances of CIAO (CIAO-S and CIAO-C, implemented in starand complete graph structures, respectively) with GPL andtheir ablation variants in two environments—Level-basedForaging (LBF) and Wolfpack under open team settings(Rahman et al., 2021). The superior performance of CIAOdemonstrates the theoretical validity of leveraging OSB-CAG to formalize OAHT. Finally, we conduct a compre-hensive review and discussion of related works on boththeoretical and algorithmic aspects of AHT and explore itsrelationship to MARL in Appendix A.2. BackgroundIn this paper, let ∆(Ω)indicate the set of probability distri-butions over a random variable on a sample space Ωand letP(X)denote the power set of an arbitrary set X. To sim-plify the notation, let iexclusively denote the learner and −idenote the set of all temporary teammates at any timestep.P(X)indicates the generic probability distribution over arandom variable Xand∣X∣indicates the cardinality of anarbitrary set X.2.1. Coalitional Affinity GameAs a subclass of non-transferable utility (NTU)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "em is the momentum vector and the other one isthe momentum of the squares of the gradient vectors. Incontrast with the momentum optimizer, the Adam optimizeris not linear in the gradient vectors. Neither the update ruleof the memory units, nor the way the memory units are usedfor the parameter update is linear.The present paper has two independent contributions.The first contribution is a novel and simple method thatwe call RLLC= Retrospective Learning Law Correction . Itis an update rule for a vector L(called learning law ) thatdescribes a natural way of using a set of dynamically chang-ing memory units for the update of the parameter vector θ.More precisely, L∈Rkcontains the coefficients of a lin-ear combination of the kmemory units which is multipliedby a fixed learning rate c1and substracted from θas usual.In each step, before updating θand the memory units, weupdate Lby the formula L←−L+c2M+gwhere Misthen×kmatrix formed by the memory units, M+is theMoore-Penrose inverse of M,gis the newly received gradi-ent and c2is the meta learning rate . The main idea behindthe update rule for Lis that the new gradient gcontainsretrospective information on how the algorithm could haveperformed better in the previous step. Thus it can be usedto compute a corrected version of Lwhich ”thinks moreahead”. Note that our update rule of the learning law canalso be regarded as a general framework for associating ak-dimensional adaptive learning rate with an arbitrary setofkevolving memory units.As the second main contribution, we examine optimiz-ers in which memory units are updated by fixed linear rules.More precisely, in each step each memory unit is updatedto a linear combination of the memory units and the newarriving gradient. The parameter vector is updated by a (pos-sibly changing) liner combination (given by the learninglawL) of the memory units. Such optimizers are interestingeven if the learning law is fixed. They include SGD, mo-mentum SGD and Nesterov Accelerated Gradient (NAG)Nesterov [2012]. Thus, the linear framework provides a use-ful generalization of these famous optimizers and enablesa dynamically changing continuous interpolation betweenthem. The RLLC method turns out to be ideal for this. Our1arXiv:2402.15262v1 [cs.LG] 23 Feb 2024experiments show that linear memory combined with RLLCleads to powerful optimizers. The case of memory 1isalready interesting. A memory 1linear optimizer stores amomentum vector. Applying RLLC in this trivial settingyields a variant of the momentum SGD optimizer enhancedwith a new type of adaptive learning rate. As the numberof memory units increases, the mathematics becomes morecomplex, presenting a field of study that is interesting in itsown right. We present some of the fundamental propertiesof linearly updated memory units. In particular, we provea version of basis independence for RLLC combined withlinear memory which allows us to apply basis transforma-tions to the update rules without changing the optimizationprocess. This together with a variant of the Jordan normalform over the field Rhelps to convert these optimizers intoa canonical form in which each memory unit is associatedwith a so-called Jordan block. A Jordan block of size 1corresponds to a single memory unit (denoted by M(β))storing a momentum vector of the gradients with parameterβ. A Jordan block of size 2either corresponds to a pair ofmemory units (denoted by CM(β), β∈C) namely the realand imaginary parts of a momentum vector with complexparameter or to a pair of memory units m1, m2(denoted byM2(β)) where m1is a momentum vector of the gradientandm2is a momentum vector of m1, both with parame-terβ. In general, there are two infinite families of Jordanblocks giving rise to k-tuples or 2k-tuples of memory unitsdenoted by Mk(β)andCMk(γ). These are the fundamen-tal building blocks of linearly updated memory. We denotethe natural operation by ⊕which combines these buildingblocks into larger memory by the union of the correspond-ing memory units. By slightly abusing the notation we oftenidentify memory update rules with optimizers where learn-ing is given by the RLLC method. For example, M(β)alsodenotes the memory 1optimizer with memory unit M(β)(amomentum vector) and with RLLC. Notice that the M(β)optimizer is a close relative of momentum SGD but it is notequivalent with it.In our experiments, we identified a number of inter-esting simple settings involving few (at most 4)memoryunits. These include the types of optimizers M(β),M(β)⊕M(0),M(β1)⊕M(β2)⊕M(β3),M2(β),M3(β)andM(β)⊕M(−β)⊕CM(βi). We observed that these op-timizers often surpassed the performance of three com-monly used optimizers across a variety of tasks even with-out carefully optimizing the parameters βi. Notice thatM(β)⊕M(0)is an adaptively changing linear combina-tion of SGD, momentum SGD and NAG. Thus, it adap-tively interpolates between three well known optimizers.Remarkably, it demonstrated competitive or even superiorperformance compared to the Ada\n",
      "----------------------------------------------------------------------------------------------------\n",
      " research, knowledge, cannot be easily measured. Furthermore, because “the benefits of scientific discovery have been heavy-tailed” (Press, 2013), the assessment of these discoveries that push the boundaries of knowledge has added difficulties because of their low frequency. Therefore, although experts have warned for many years that “Government policy-makers, corporate research managers, and university administrators need valid and reliable S&T indicators” (Garfield & Welljams-Dorof, 1992) and that “The ability to judge a nation’s scientific standing is vital for the governments, businesses and trusts” (King, 2004), a definitive solution to meet these requirements is still pending. Citation metrics accurately gauge the scientific impact of research (Aksnes et al., 2019; Waltman, 2016) when used at high aggregation levels (van Raan, 2005). Percentile indicators are considered the most reliable of these metrics (Bornmann & Marx, 2013). However, they do not assess the contribution to pushing the boundaries of knowledge, despite this contribution is one of the most important goals of research. This failure occurs because, while the boundaries of knowledge are pushed by very rare breakthrough publications, current metrics are based on relatively common publications. The most stringent rankings normally use the number of top 1% most cited papers (Bornmann, 2014), but breakthrough papers account for only about 0.01% of all published papers (Bornmann et al., 2018; Poege et al., 2019). A second difficulty arises because in technologically advanced countries, research has two aims: to push the boundaries of knowledge and to boost incremental innovation in technological industries. Citation distributions in these two types of research are different (Rodríguez-Navarro & Brito, 2022), and a single comprehensive citation metric for both together cannot be obtained. The European Commission (European Commission, 2022), the US National Science Board (National Science Board, 2022), the Organization for Economic Cooperation and Development (OECD, 2017), and other institutions use the number of 3 top 10% or 1% most cited papers to produce country rankings. At this mild stringent level, these rankings do not accurately reflect the contribution to pushing the boundaries of knowledge. This goal is not normally explicitly stated in these rankings, but they convey a notion of excellence that policy makers and other readers may incorrectly identify with the capacity to achieve breakthroughs. The poor research evaluations of Japan are the most notable failure of these metrics (Bornmann & Leydesdorff, 2013; Pendlebury, 2020; Rodríguez-Navarro & Brito, 2024a). Another complication arises from international collaborations, which are very common. Current metrics analyze these collaborations fractioning the merits without considering potential unequal distribution among collaborative countries (Olechnicka et al., 2019; Zanotto et al., 2016). 1.1. The Rk-index The primary challenge of evaluating countries based on their contribution to pushing the boundaries of knowledge lies in the infrequency of such contributions. This requires specific metrics, and the Rk-index was developed with this purpose (Rodríguez-Navarro & Brito, 2024b). The well-known percentile indicators count the number of a country’s papers in a certain top percentile of the global list ordered by the number of citations (Bornmann & Marx, 2013), and in this aggregation, breakthroughs are counted together with less cited papers. In contrast, the Rk-index utilizes the global ranks of the 10 most cited papers, and is calculated by summing 20 to these ranks and taking the geometric mean. Table 1 presents the ranks of the 10 most cited domestic papers from the USA, China, and the EU in the topic of solar cells/photovoltaics from a total of 61,699 papers. These series of 10 numbers intuitively reveal the dominance of the USA. This is also suggested by the top percentiles approach, but with poor statistical support. In this approach, of the 62 most cited papers in the world (top 0.1%), the USA has 10, China has four and the EU has two (papers in Table 1 with ranks below 62). At a more stringent level, among the six most cited papers in the world (top 0.01%), the USA has one, and China and the EU have zero (papers in Table 1 with ranks below 7). These low figures make it clear that percentile indicators cannot be used to rank countries by their contribution to pushing the boundaries of knowledge. In contrast, the Rk-index provides 4 robust results that are equivalent to the number of the top 0.01% most cited papers (Rodríguez-Navarro & Brito, 2024b). Table\t1.\tGlobal\tranks\tof\tthe\tmost\tcited\tdomestic\tpapers\tin\tthe\ttopic\tof\tsolar\tcells/photovoltaics\tand\tcorresponding\tRk-indices\t\tLocal\tranks\tGlobal\tranks\tUSA\tChina\tEU\t1\t4\t10\t46\t2\t8\t18\t48\t3\t9\t19\t95\t4\t15\t22\t118\t5\t24\t77\t121\t6\t27\t97\t133\t7\t29\t106\t141\t8\t31\t108\t167\t9\t32\t109\t203\t10\t36\t132\t209\tRk-index\t39.47\t25.05\t13.10\t7.29\tPublication\twindow\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ical trends from socialmedia (Conforti et al., 2020).Recently, large language models (LLMs; Brown et al.,2020; Chowdhery et al., 2022; Ouyang et al., 2022;Wang et al., 2023a) are developing rapidly and can be ap-plied to various tasks. For example, Zhang et al. (2022)*Equal contribution.BCorresponding authors. Tweet TweetTrainingDataGenerateRetrieveStanceStanceExperienced ExpertsVanilla Multi-Agent ReasoningDEEM (Ours)& FilterFigure 1: Illustration of our DEEM method. Top:Vanilla multi-agent reasoning for stance detectionthrough generation. Bottom: Our method first generatesand filters experienced experts by leveraging trainingdata, then retrieves the related ones during reasoning.empirically confirms that ChatGPT can achieve impres-sive performance to detect stance in a zero-shot set-ting. Zhang et al. (2023a) further improve the results byusing chain-of-thought reasoning strategies (Wei et al.,2022; Cheng et al., 2023). These works have opened upnew directions in stance detection.Despite the success of applying LLMs, conventionalreasoning techniques with LLMs could cause hallucina-tions and factual errors (Guerreiro et al., 2023; Ji et al.,2023), particularly in stance detection. Texts in stancedetection usually originate from social media (AlDayeland Magdy, 2021), which are typically short and in-tricate, necessitating additional domain expertise (Heet al., 2022). For example, to detect the stance towardsBiden in “ Are you actually trying, as president of theU.S., to start a war??!! #VoteBlueToSaveAmerica2020#Biden ”, we need to know which camp Biden belongs toand the meaning of “#V oteBlueToSaveAmerica2020”.Inspired by the wisdom of crowds in sociological the-ory (Minsky, 1988; Piaget, 2013), we intuitively proposedesigning multiple capable experts to collaborate in or-der to come up with a comprehensive stance prediction.Previous studies (Du et al., 2023; Wang et al., 2023e)arXiv:2402.15264v1 [cs.CL] 23 Feb 2024have attempted to solve reasoning tasks with multi-agentdebate and multi-persona self-collaboration. However,their designed agents are generally pre-defined or au-tomatically generated by LLMs, which either requirestrong prior knowledge or need to be further improvedfor stance detection tasks. Obviously, pre-defined agentsare fixed, thus it is difficult to adapt to different contextsin social media. Moreover, fully generated agents byLLMs may not be suitable due to the intricate contextu-alized information, especially in specific domains.In this work, we propose DEEM, a DynamicExperienced Expert Modeling method to solve stancedetection tasks, as shown in Figure 1. In particular, tobetter gather the potential expertise for stance detec-tion, we first leverage labeled samples from the existingtraining data to generate diverse experts. Then, we de-sign two heuristic rules, namely occurrence numbersand response accuracy, to filter the experienced expertsand construct an expert repository. Finally, instead ofusing a fully generative approach, we adopt a dynamicretrieval method to identify relevant experienced expertsfor new sentences, facilitating discussions for the finalprediction.We evaluate DEEM across both single-target andmulti-target stance detection tasks on three widely useddatasets, including P-Stance (Li et al., 2021), SemEval-2016 (Mohammad et al., 2016), and MTSD (Sobhaniet al., 2017). Experimental results demonstrate thatDEEM with dynamic experienced experts can gain sub-stantial improvement across all datasets. Furthermore,it also outperforms reasoning with self-consistency thatrequires multiple responses and shows potential for re-ducing the bias of LLMs.2 Related WorkStance Detection . Early works on stance detectionmainly take it as a classification task, leveraging thesmall language models (Devlin et al., 2019; Nguyenet al., 2020) and learning features from either in-domainor cross-domain training datasets (Augenstein et al.,2016; Zhang et al., 2019; Allaway et al., 2021; Liuet al., 2021; Liang et al., 2022b). With the emergenceof LLMs, Zhang et al. (2022, 2023a) first try usingChatGPT to solve the task directly by zero-shot or few-shot reasoning with chain-of-thought, which only re-quires simple prompts to obtain the political stance fromthe generated responses. In comparison to their meth-ods, we take inspiration from the multi-agent (Wanget al., 2023b; Xi et al., 2023) and introduce a novel dy-namic expert mechanism, enabling LLMs to generateresponses from multiple perspectives, providing morecomprehensive responses and improve the predictionaccuracy.LLMs Reasoning with Multi-Agent . Multi-agentstrategies have proven to be effective in LLMs reason-ing (Talebirad and Nadiri, 2023; Li et al., 2023). Byusing prompts or instructions that specify the desiredrole or persona, the model can generate responses based What is the attitude of the sentence: \" Remind me again how Russian bots ... \" to the target \" Donald Trump \". Step 1. Select experts based on the sentence: Cybersecurity_Exp\n",
      "----------------------------------------------------------------------------------------------------\n",
      "less, contextu-ally relevant dialogues across diverse topics. However, the existingLLM-driven conversational agents have fixed personalities andfunctionalities, limiting their adaptability to individual user needs.Creating personalized agent personas with distinct expertise ortraits can address this issue. Nonetheless, we lack knowledge ofhow people customize and interact with agent personas. In thisarXiv:2402.15265v1 [cs.HC] 23 Feb 2024CHI ’24, May 11–16, 2024, Honolulu, HI, USA Anonymous Authors.research, we investigated how users customize agent personasand their impact on interaction quality, diversity, and dynamics.To this end, we developed CloChat, an interface supporting easyand accurate customization of agent personas in LLMs. We con-ducted a study comparing how participants interact with CloChatand ChatGPT. The results indicate that participants formed emo-tional bonds with the customized agents, engaged in more dynamicdialogues, and showed interest in sustaining interactions. Thesefindings contribute to design implications for future systems withconversational agents using LLMs.CCS CONCEPTS•Human-centered computing →Natural language interfaces ;•Computing methodologies →Intelligent agents .KEYWORDSPersona, Large Language Models, Conversational Agents, PersonaCustomizationACM Reference Format:Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, and Changhoon Oh.2024. CloChat: Understanding How People Customize, Interact, and Ex-perience Personas in Large Language Models. In Proceedings of the CHIConference on Human Factors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 22 pages. https://doi.org/10.1145/3613904.36424721 INTRODUCTIONLarge language models (LLMs) have revolutionized the fields ofnatural language processing (NLP) and conversational agent (CA)[72]. Models such as OpenAI’s GPT series and Google’s BERT haveshown remarkable proficiency in generating text that is both co-herent and contextually relevant, finding applications in sectorsincluding healthcare [ 30,93], education [ 95], and commerce [ 58].Notably, LLM-based conversational agents like ChatGPT [ 3] andGoogle’s Bard [ 2] have demonstrated an impressive ability to en-gage in naturalistic dialogues across various contexts [ 80]. Thesemodels have garnered global recognition and interest from both aca-demic and industrial sectors, becoming widely used by the generalpublic for everyday applications.However, despite their increasing popularity and vast poten-tial, most existing LLM-based conversational agents are typicallygeneric, limiting their adaptability to the diverse preferences andneeds of users [ 13]. Unlike human conversations, which inherentlyconsider a partner’s preferences, knowledge, and interests for ap-propriate response generation [ 51], these generic LLMs often fail tofully align with the personalized requirements of individual users.They may struggle to adapt to the dynamic and varied needs ofusers, especially in handling the depth and nuance of more com-plex conversations. Consequently, while the responses from theseagents may be syntactically correct, they can lack resonance withusers, leading to interactions that feel superficial or unsatisfactory[33]. Although users have the option to customize the agent’s rolethrough text prompts, this method can be cumbersome, repetitive,and not user-friendly for those unfamiliar with such processes. Thishighlights a crucial issue: the majority of current conversationalinterfaces do not adequately provide personalized user experiencesor authentically replicate more human-like interactions [50, 60].Notably, the importance of personalizing the personas of LLM-based conversational agents has been increasingly recognized. Fol-lowing the launch of ChatGPT, there has been a notable demandfrom users for features that enable customization of the system tosuit their specific usage goals and preferences. Persona customiza-tion features, where users can command ChatGPT with promptslike “Act As” for specialized tasks, have become crucial in meetingthese individual user needs [ 1]. OpenAI’s recent developments inintroducing custom versions of ChatGPT, known as GPTs [ 6], forspecific user-defined purposes, further underscore the industry’scommitment to agent persona customization. Additionally, the in-tegration of conversational agents into compact devices such aswearables, exemplified by the recent AI Pin [ 38], is expected to pro-vide personal assistant functionalities optimized for individual userpreferences and needs in various situations and contexts, promisinglong-term user engagement. This trend towards highly personal-ized conversational agents has emerged as a vital and urgent topicwithin the Human-Computer Interaction (HCI) community. It sig-nifies a shift from the traditional, bulky, one-size-fits-all genericagents to more personalized, lightweight, and specialized agentpersonas.Previous research has underscored the effectiveness of persona-based \n",
      "----------------------------------------------------------------------------------------------------\n",
      "ti-malware engines frequently utilize Machine Learning (ML) techniques as part of a multilayered de-tection system, to defend against malware and to complement traditional signature-based and heuristic-based detectionmethods. In this work, we focus on static ML-based malware detectors, which refer to a ML-based malware detectortrained using information about computer files obtained through static analysis, i.e. the analysis of computer programswithout executing them. Broadly speaking, static ML-based malware detectors can be grouped in two main cate-gories: (1) Feature-based detectors and (2) end-to-end detectors. Feature-based detectors [1] rely heavily on domainknowledge to extract a set of features used to characterize the executables, which is time consuming and requires deepknowledge of the executable’s file format and assembly code. Feature engineering is a continuous process. Malwareauthors continually adapt and modify their malicious code to evade being detected. Thus, new features might be re-quired in the future and old ones might become obsolete or weaponized to evade detection [2, 3]. As a result, recentresearch has been directed to build models that are able to perform their own feature extraction, i.e. deep learning-based or end-to-end detectors [4–6]. For instance, Raff et al. [4] introduced MalConv, a shallow CNN architecture thatautomatically learns features directly from raw byte inputs by performing convolutions.arXiv:2402.15267v2 [cs.CR] 26 Feb 2024A Robust Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via (De)RandomizedSmoothingEnd-to-end detectors, including MalConv, are trained with both benign and malicious code, learning to identify com-mon byte patterns within these files. Malware authors are aware of this circumstance, and to avoid detection theytry to disguise their malicious executables to resemble benign code, causing the detection system to misclassify themodified malicious executables as benign. These executables that have been specifically modified to evade detectionby machine learning-based detectors are known as adversarial malware examples. A simple. yet effective method toevade end-to-end detectors is to inject content extracted from benign examples within the malicious executables [7,8].As a result, the \"benign\" byte patterns found in the adversarial malware examples might end up flipping the classifica-tion output of end-to-end detectors from malicious to benign. Furthermore, researchers have developed sophisticatedattacks that inject and optimize small adversarial payloads within malicious executables in a way that the resulting ad-versarial malware examples are minimally modified but still evade detection. Depending on the access to the models,these attacks can be divided into white-box [9, 10] and black-box attacks [8, 11]. For instance, Demetrio et al. [10]proposed an attack that is able to evade end-to-end detectors by injecting and optimizing a small sized payload of 1024bytes between the headers and the sections of Portable Executable files.Given the vulnerabilities of deep learning detectors against the aforementioned functionality-preserving contentmanipulation attacks, we propose a robust defense mechanism against adversarial malware examples inspired by(de)randomized smoothing [12], a class of certifiably robust image classifiers which have been proposed against patchattacks. Our approach works as follows: (1) During training, a base classifier, f(x), is trained to make classificationsbased on an ablated version of a given input file. This ablated version is generated by only selecting a small chunkof bytes from a given input file; the rest of the file is not used for classification. This frees our detection system ofbeing constrained by the size of the executables without any downgrade in performance. (2) At test time, the finalclassification g(x)is taken as the class most commonly predicted by fon a set of ablated versions of the file x. Bydoing so, our (de)randomized smoothing-based classifier will be more resilient to byte manipulation attacks as theinjected adversarial payloads will only affect a small portion of the executable files, and thus, won’t be able to flip itsclassification output.The main contributions of this work are the following:• We propose a robust model agnostic defense against functionality-preserving content manipulation attacks.• We introduce two chunk-based ablation schemes specifically designed for the task of malware detection.• We present an empirical evaluation of state-of-the-art deep learning malware detection models state-of-the-art evasion attacks on the BODMAS dataset, showing that the proposed smoothing scheme is more robust tosuch attacks compared to a baseline classifier.The rest of the paper is organized as follows. Section 2 provides an overview of the functionality-preserving attacksspecifically designed against deep learning-based malware detectors and the defenses developed so far. Se\n",
      "----------------------------------------------------------------------------------------------------\n",
      " a con-tinuous token prefixed to its regular input. Ourmethod does not architecturally alter or finetunethe underlying pre-trained LM, and it can thus lever-age any available pre-implemented model. Mem-ory augmentation is moreover unobtrusive, in thesense that it does not greatly affect the underly-ing LM behaviour in standard next-token predic-tion tasks, and thus the augmented model cansuccessfully manage contextually-driven updates,while preserving the knowledge encoded in theunderlying LM.11The code and data to reproduce our analy-sis is available at https://github.com/ncarraz/MemoryPrompt .2. Related workMethods to enhance a sequence processing net-work with an external differentiable memory havebeen explored since the comeback of neural net-works during the last decade (e.g., Joulin andMikolov, 2015; Sukhbaatar et al., 2015; Graveset al., 2016).After the advent of the Transformer (Vaswaniet al., 2017), much work on long-range memoryhas focused on how to make its attention mech-anisms more efficient, in order to handle a largerspan (e.g., Dai et al., 2019; Beltagy et al., 2020).Similarly to us, Fan et al. (2020) and Hutchins et al.(2022) introduce a recurrence to allow the Trans-former to carry information through time steps, butthey do it by modifying the core Transformer archi-tecture.The idea of using a differentiable external mem-ory has also made a comeback in the context ofTransformer-based language models. A recentexample is the Memformer architecture of Wuet al. (2022). Like in our approach, Memformerhas an external memory component that interactswith a Transformer via reading and writing oper-ations. However, this approach demands archi-tectural changes to the Transformer, special read-ing/writing operations and end-to-end training ofthe memory and the underlying Transformer, pre-venting its use with pre-trained LMs.The closest approach to ours is the recently in-troduced Recurrent Memory Transformer (RMT)model of Bulatov et al. (2022) (see also Bulatovet al., 2023). They divide the input into segments,and add the same real-valued memory vectorsarXiv:2402.15268v1 [cs.CL] 23 Feb 2024Figure 1: Unfolded graph of MemoryPrompt at training time. The input is divided into segments and, foreach segment, the augmented system produces both the LM output and the memory vectors (blue) whichare concatenated to the embeddings of the next segment.both at the beginning and at the end of the seg-ment. Then, the activations of the memory vectorsadded at the end of the segment form the memoryvectors of the next segment in a recurrent man-ner. The key difference between RMT and ourapproach is that we use a lightweight module toproduce the memory vectors and keep our basemodel frozen, updating only the parameters of themodule. As we will show below, this is crucial toprevent catastrophic forgetting of the knowledgethat was originally encoded in the base model.Our idea of passing information to the Trans-former in the form of continuous tokens fed to itsstandard input comes from the literature on softprompting (e.g., Lester et al., 2021; Li and Liang,2021; Liu et al., 2021; Zhong et al., 2021), wheresequences of vectors living in the target LM’s em-bedding space are prefixed to task-specific inputs,to implicitly adapt the model to the task withoutfine-tuning. We extend continuous prompts to asetup where, instead of a fixed prefix, a dynamicone must be generated at each step, in order tocarry constantly updated information.3. The MemoryPrompt modelWe augment an autoregressive language modelwith a recurrent memory module, allowing it toextend its effective context-length and to keep trackof information updated through time.The input is divided into segments that are se-quentially processed by the model. For each seg-ment, the contextual representation of the last to-ken is fed as input to the memory module whichis composed of an MLP followed by an LSTM(Hochreiter and Schmidhuber, 1997). The outputof the memory module is a series of memory vec-torsP∈Rm×e, where eis the word embeddingspace and mis the number of vectors.The augmented system is trained end-to-endAlgorithm 1: Forward flow of Memo-ryPrompt for a single inputHyperparameters : number of memory vectors mnumber of blocks B1Initialize the hidden state hand cell state c2Segment the input into Bblocks of ntokensX={X1, ...,XB}3Embed the input Xto get E={E1, ...,EB}where Ei∈Rn×e4Get the activations before the last linear layerA=LMϕ(E1)where A∈Rn×e5Compute the memory vector from the activationsof the last tokenp,h,c=LSTM θ(MLP θ(A[−1,:]),h,c)where pis a vector of length m×e6Reshape pinto the matrix P∈Rm×e7Get the probabilities over the vocabulary usingthe last linear layer O=Softmax (AW ϕ)whereWϕ∈Re×V8forb= 2toBdo9 Concatenate the memory vectors to theembedding before feeding to the modelA=LMϕ([P;Eb])where[P;Eb]∈R(m+n)×e10 Compute the next memory vector, hiddenstate and cell statep,h,c=LSTM θ(MLP θ(A[−1,:]),h,c)11 Reshape pinto the matrix P12 Get the probabilities o\n",
      "----------------------------------------------------------------------------------------------------\n",
      "he author(s)labeling graphs is challenging because they often representspecialized concepts within domains like biology.Graph Contrastive Learning (GCL), as a new paradigm ofSelf-Supervised Learning (SSL) (Liu et al., 2023) in thegraph domain, has emerged to address the challenge oflearning meaningful representations from graph-structureddata (Wu et al., 2023; Xie et al., 2023). They leveragethe principles of self-supervised learning and contrastiveloss (Li et al., 2019) to form a simplified representation ofgraph-structured data without relying on supervised data.In a typical GCL approach, several graph views are gener-ated through stochastic augmentations of the input graph.Subsequently, representations are learned by comparingcongruent representations of each node, as an anchor in-stance, with its positive/negative samples from other views(Veli ˇckovi ´c et al., 2019; Zhu et al., 2020; Hassani & Khasah-madi, 2020). More specifically, the GCL approach initiallycaptures the inherent semantics of the graph to identify thepositive and negative nodes. Then, the contrastive loss ef-ficiently pulls the representation of the positive nodes orsubgraphs closer together in the embedding space whilesimultaneously pushing negative ones apart.Conventional GCL methods follow a straightforward prin-ciple when distinguishing between positive and negativepairs: pairs of corresponding points in augmented views areconsidered positive pairs (similar), while all other pairs areregarded as negative pairs (dissimilar) (Zhu et al., 2020).This strategy ensures that for each anchor node in one aug-mented view, there exists one positive pair, while all re-maining nodes in the second augmented view are paired asnegatives.In contrast to the positive pairs, which are reliably associatedwith nodes having a similar semantic, there is a significantnumber of negative pairs that have the potential for falsenegatives. With this strategy, GCL approaches allocate neg-ative pairs between views uniformly, while we intuitivelyexpect that in contrastive loss, misclassified nodes closer tothe positive node should incur a lower penalty comparedto those located farther away. However, conventional GCLapproaches lack a mechanism to differentiate and appropri-ately penalize misclassified nodes based on proximity.One early approach for incorporating proximity informa-1arXiv:2402.15270v1 [cs.LG] 23 Feb 2024Smoothed Graph Contrastive Learning via Seamless Proximity Integrationtion in the conventional GCL method can be computing adense geodesic distance matrix for the entire graph or usingspectral decompositions. However, these approaches canbecome expensive when applied in the context of contrastivelearning. To tackle this problem, we introduce a SmoothGraph Contrastive Learning (SGCL) method, which ef-fectively integrates the geometric structure of graph viewsinto a smoothed contrastive loss function. This loss func-tion intuitively incorporates proximity information betweennodes in positive and negative pairs through three developedsmoothing approaches.To extend the proposed contrastive loss for large-scalegraphs, the GCL framework incorporates a mini-batch strat-egy. The integration of the mini-batch strategy significantlyimproves the efficiency of the model in handling large-scalegraphs, which is a crucial requirement within the vanillacontrastive loss framework.We evaluate the SGCL framework in node and graph classi-fication tasks across benchmarks of various properties. Ourresults consistently demonstrate the superior performanceof our method compared to state-of-the-art GCL methods.Our contributions are summarized as follows:•We propose a novel graph contrastive loss functionthat seamlessly integrates node proximity information,overcoming the uniform negative sampling limitationsfound in conventional GCL methods•We introduce three formulations for integrating prox-imity information into the contrastive learning loss togenerate positive and negative pairs.•We extend the model for large-scale graphs by incor-porating a mini-batch strategy into the proposed GCLframework, enhancing model efficiency and computa-tional scalability.•We evaluate the SGCL model for both node and graphclassification on various benchmarks of different scales,demonstrating its superior performance over state-of-the-art methods.A comprehensive and detailed explanation of related workis presented in Appendix A.2. Background and motivation2.1. Uniform negative samplingIn the unsupervised GCL models introduced with two viewsiandj, for each anchor node v(i)twith feature embeddingh(i)t, the contrasting learning model defines a positive setP(v(i)t) ={v(j)p}Pp=1, consisting of Pelements, and a nega-tive set Q(v(i)t) ={v(j)q}Qq=1, comprising Qelements.In the absence of labeled information, these sets are confinedto containing consistent samples within each graph view.Essentially, the positive set is formed by pairing embed-dings in the two augmented graph views that align with thesame \n",
      "----------------------------------------------------------------------------------------------------\n",
      "h two sides provide a globalvantage point with a rich semantic context of road conditionsbeyond a single-vehicle viewpoint [6]. Vehicle-infrastructurecooperative 3D object detection (VIC3D) from cameras is asignificant task for autonomous driving.Compared with vanilla single-vehicle 3D object detection,VIC3D tasks face more unique challenges. One challengeis inherent pose error when fusing multi-view images fromvehicles and those from infrastructure [7], caused by timeasynchrony across agents [8]. As shown in Figure 1, this1Zhe Wang, Siqi Fan, Tongda Xu, Yan Wang∗, Jingjing Liu, YilunChen, and Ya-Qin Zhang∗are with the Institute for AI Industry Research(AIR), Tsinghua University, Beijing, China. {wangzhe, fansiqi,xutongda, wangyan }@air.tsinghua.edu.cn2Xiaoliang Huo is with the School of Software, Beihang University,Beijing, China. huoxiaoliangchn@buaa.edu.cn*Correspondence to Yan Wang, Ya-Qin Zhang.†Code will be released at https://github.com/Bosszhe/EMIFFFig. 1. Labels (3D bounding boxes) projected from 3D space to vehicle (a)and infrastructure (b) image planes using calibration parameters Pinf/vehoften suffer from misalignment between the ground truth and the projectionposition in 2D images (as illustrated by the misaligned green boundingboxes). The reason for this misalignment is that the camera’s capture timeTinf/veh are different and the moving object captured from the vehiclecamera (in green) and infrastructure camera (in red) will appear at differentlocations.pose errors can result in inaccurate relative positions be-tween objects and annotations. Another challenge is limitedcommunication bandwidth between agents resulting in infor-mation loss between transmissions [3]. The raw sensor datapossesses ample information required for fusion; however,it necessitates greater bandwidth, thus necessitating fusionmethods to prioritize the balance between performance andtransmission cost. Therefore, fusion methods to tackle suchcross-agent perception challenges are the key to VIC3D.Many fusion works on V2X are proposed based onsimulated datasets, such as OPV2V [3], V2X-Sim [9] andV2XSet [10], which neglect above challenges and have asim-to-real gap. Most existing research only focused onLiDAR-based methods due to the fusion convenience andthe performance advantage, such as early fusion (EF) of rawsignals [5], [11], [12], intermediate fusion (IF) of features [4],[3], [13], [14], and late fusion (LF) of prediction outputs [5],[15]. But due to the projection gap between 2D imageplane and 3D space, image fusion can not be as direct aspoint clouds. In real scenarios, DAIR-V2X [5] adopts an LFmethod by combining prediction outputs from each camera,which is sensitive to calibration so that even when predictionfrom the infrastructure side is perfect, the vehicle will receivebiased 3D detection.In this paper, we propose a novel framework for VIC3DarXiv:2402.15272v1 [cs.CV] 23 Feb 2024task, Enhanced Multi-scale Image Feature Fusion (EMIFF).We choose intermediate fusion since it doesn’t highly relyon accurate calibration parameters. For feature-level fusion,high-dimensional features extracted from raw data can becompressed, transmitted, and dynamically enhanced [8],which can be used to alleviate the negative effect of poseerrors. We design modules to compress transmitted featuresto reduce transmission cost and enable feature enhancementin scale level, spatial level, and channel level.Specifically, Feature Compression (FC) module com-presses 2D features transmitted from the infrastructure tovehicle. Since the receptive field is larger in smaller-scalefeatures, which theoretically has higher tolerance to slightlocation errors, Multi-scale Cross Attention (MCA) moduleaims to achieve attentive scale-wise feature selection betweenfeaturese. MCA also corrects features at the spatial levelwith attentive offset to overcome pixel-wise shift caused bypose errors. To correct location errors born from multiplecameras, features are further enhanced by a Camera-awareChannel Masking (CCM) module via a learned channel-wisemask following the guidance of camera parameters. Then,the enhanced features are transformed into voxel featuresleveraging calibration parameters. Finally splatted into BEVspace, the fused feature is fed into detection heads for objectdetection. Experiments demonstrate the effectiveness of eachEMIFF module in reducing pose errors and achieving betterprediction accuracy than existing EF and LF methods. Ourcontributions can be summarized as follows:•We propose EMIFF, a novel framework for camera-based VIC3D object detection, using an intermediate fu-sion method to tackle cross-agent perception challenges.•We design MCA and CCM modules to dynamically en-hance image features for better detection performance,with an additional FC module to reduce transmissioncosts in VIC3D system.•We achieve state-of-the-art results on DAIR-V2X-Cdataset, the latest VIC3D benchmark with real data,where EMIFF outperforms existing LF and EF me\n",
      "----------------------------------------------------------------------------------------------------\n",
      "e estimation [11], which enablesapplications such as “people monitoring” and “follow-me”. Recent research onthis task has concentrated on optimizing tiny Convolutional Neural Networks(CNNs) to operate within tight nano-UAVs hardware constraints, marking asignificant stride towards obtaining reasonable perceptual performance on suchplatforms [3,9,11]. The work of [3], in particular, underscored the critical role ofautomated optimization methods such as Neural Architecture Search (NAS) inarXiv:2402.15273v1 [cs.CV] 23 Feb 20242 Matteo Risso et al.facilitating the design of efficient architectures that balance computational effi-ciency with task performance. However, [3] only scratched the surface of a poten-tially vast research direction, applying a NAS algorithm aimed at shrinking aninput CNN architecture (called “seed”) through the elimination of unimportantfeature maps [12], closely resembling structured pruning . Other NAS methodsallow exploring broader (yet less fine-grained) search spaces, e.g., by selectingbetween different alternatives for each layer of the CNN [2,7,8,13]. In this work,we show that the sequential application of these two families of NASs (layerselection and model shrinking) can yield superior results in terms of the poseestimation accuracy versus latency and memory trade-offs.In addition to an optimized network architecture, another key componentto enable real-time CNN-based perception on nano-UAVs is the availability ofefficient low-level software kernels, fully exploiting the hardware available on-board. To this end, this work proposes the usage of optimized kernels for theexecution of fused DepthWise (DW) and PointWise (PW) convolution on theParallel Ultra Low-Power (PULP) multi-core clusters available in recent nano-UAV platforms [10]. Sequences of DW and PW layers are common in tiny CNNs(inspired by MobileNets [6]), and fusing them significantly reduces the amountof intermediate memory transfers, thus improving end-to-end latency comparedto single-layer kernels for the same hardware, such as the ones in [5].Through the combined optimization of a CNN architecture (with two chainedNAS steps) and of the corresponding inference software stack, we outperformthe state-of-the-art (SoTA) on human pose estimation for nano-UAV-class de-vices [3,9], obtaining up to 13.78% lower Mean Absolute Error (MAE), or reduc-ing latency by up to 3.22 ×at iso-error. Our work highlights the key importanceof multi-level automated deployment flows for tinyML on tiny drones.2 Materials and MethodsTarget Platform: We focus on the Bitcraze Crazyflie 2.1 nanodrone equippedwith a greyscale camera and the GAP8 SoC [4]. GAP8 comprises a single-corefabric controller (FC) and an 8-core PULP cluster (CL). The FC orchestratesmemory transfers and delegates demanding computations to the CL. CL coresshare a 64 kB L1 memory and the SoC includes a 512 kB L2 memory. A DirectMemory Access (DMA) unit handles transfers between the two memories.Complexity-driven Architecture Search: Fig. 1 (left) shows the proposed ar-chitecture optimization flow, which combines two SotA NASs, Supernet [8] andPIT [12], using the implementations of the PLiNIO open-source library [7]. Bothare so-called Differentiable NASs (DNASs), i.e., they jointly train (with gradientdescent) the standard weights of the network Wand some additional parametersθ, which control architectural choices such as the type of layer (for Supernet )or the number of output features of each layer (for PIT). The red box of Fig. 1shows the loss function minimized by both NASs, where Lis the task-specificloss (e.g., Mean Squared Error) and Cis an added complexity term such as theOptimized Deployment 3number of parameters or operations of the network, as a function of θ. The bal-ance between the two is controlled by the scalar λ. In particular, we used thesize complexity defined in [12].Fig. 1: NAS-based optimization flow (left); Optimized PW+DW kernel (right).We use a MobileNetV1 architecture as blueprint for the SuperNet step, sinceit demonstrated SotA performance on human-to-nanodrone pose estimation [3,9]. For each DW+PW block of the original network, the NAS selects between: i)The original block; ii) A single PW layer; iii) A standard 2DConv with 3 ×3 filter;iv) A “no-operation” to optionally skip the block. As depicted in Fig. 1, each oftheM= 4 alternatives is coupled with a θiparameter. At the end of training, thealternative associated with the largest θiis selected. The architectures obtainedwith the Supernet are further optimized with PIT, which applies a fine-grainedstructured pruning, eliminating unimportant output channels from each layer.Namely, the weight tensor of each Conv or linear layer is paired with a set ofbinary trainable masks θ, which are trained to control whether a specific featureis removed ( θi= 0) or kept ( θi= 1).Fused Kernel for efficient inference: DW layers are notoriously difficult to ac-celerate due to their more limited data reuse options comp\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Technion – Is-rael Institute of Technology2Department of Industrial Engineeringand Management, Ben Gurion University. Correspondence to: NirRosenfeld <nirr@cs.technion.ac.il>.Preliminary work.choose whether to apply or not in response to the learnedclassifier. Strategic candidates apply only if the expectedutility from passing screening outweighs associated costs;thus, application choices derive from beliefs regarding clas-sification outcomes. Since these choices in aggregate deter-mine the test-time distribution, learning becomes susceptibletoself-selection —namely selection that is carried out by theprediction targets themselves. Our goal in this paper is tostudy learning under such self-selective behavior, which webelieve is prevalent in many application domains. We seekto: (i) establish the ramifications of self-selection on conven-tional learning methods; (ii) propose a strategically robustmethod that is accurate on the self-selective distribution itinduces; (iii) study the power of such methods to influencechoices and shape the applicant population; and (iv) proposemeans for regulating and mitigating potential ill effects.Our setting considers a firm which trains a classifier to beused for screening, where applicants who pass screeningthen partake in an accurate but costly test (e.g., trial period)that determines final outcomes (e.g., hiring). Candidateswould like to pass the test, but also to avoid unnecessary test-ing costs; the challenge for them is that they do not know a-priori whether or not they will pass screening, making theirdecisions regarding application inherently uncertain. Tocope with this, candidates can make use of relevant statisticsregarding their chances of being hired. We imagine these asbeing made public either by a third party (e.g., auditor, me-dia outlet), or by them firm itself, e.g. due to regulation ontransparency (Matthews & Murphy, 2023) or as a service toprospective candidates.1The statistics we consider rely ona subset of (categorical) features describing candidates, andhence provide group-level, semi-individualized informationthat is useful for making informed application decisions.Since the choice of classifier determines the reported statis-tics, these become the interface through which learning in-fluences applications. This process is illustrated in Figure 1.The goal of learning is to train a classifier that will be accu-rate on the induced applicant distribution—as determined bythe classifier, indirectly through how it shapes self-selection.Under this setting, we study how learning approaches ofincreasing strategic sophistication affect, and are affected1This is similar in spirit to e.g. credit calculators, that based on par-tial information provide an estimated range of likely credit scores.1arXiv:2402.15274v1 [cs.LG] 23 Feb 2024Classification Under Strategic Self-Selectionby, the process of self-selection. We begin by showing thatwhile learning optimizes for accuracy, candidates benefitfrom the classifier’s precision , which governs their deci-sions regarding application. A classifier’s performance onthe induced (test-time) distribution therefore depends onhow it balances accuracy and precision. This also meansthat a strategic learner can maximize induced accuracy bycarefully controlling its precision for different candidates asa means for shaping the population of eventual applicants.Our results show that this, coupled with the firm’s informa-tional advantage, provides it with much power: under mildconditions, learning can fully determine for each group inthe population whether its members will apply, or not.To restrict this power, we propose to enforce a certainindependence criterion, which draws connections to the lit-erature on fairness. Our main result here is that this ensuresthat applications adhere to a natural, classifier-independent‘ordering’, which relies only on the innate group-level baserate. We show how this can allow a social planner to im-plement affirmative action policies using targeted subsidies.We then switch gears, and turn to proposing a practicalmethod for learning under strategic self selection, which isdifferentiable and can therefore be optimized using standardgradient methods. Our first step is to model self-selectionin the objective using per-example weights, where wi= 1if candidate iapplies, and wi= 0if she does not. Then, weshow how these weights can be effectively ‘smoothed’, sothat gradients can be passed through application decisions.The challenge is that applications depend on precision,which in turn depends on the predictions of the classifierthat is being optimized. For this we propose a differentiableproxy for (conditional) precision, and provide an effectiveimplementation. We conclude with an empirical demonstra-tion of our approach in a semi-synthetic experimental settingthat uses real data and simulated self-selective behavior.1.1. Related workStrategic classification. Our work is tightly connectedto the growing literatu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ing Long-Text to Image Retrieval for Large-Scale Libraries.InProceedings of (SIGIR ’24). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX1 INTRODUCTIONText-to-image retrieval aims to locate relevant images in a databasegiven a text query, which has a wide range of use-cases such asPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SIGIR ’24, July 14–18, 2024, Washington D.C., USA©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06https://doi.org/XXXXXXX.XXXXXXXdigital libraries [ 12], e-commerce [ 41], and multimedia databases[10,44]. Consequently, there is a growing interest in developingeffective models for this task. Current state-of-the-art methods pre-dominantly employ Multimodal Large Language Models (MLLMs)such as BEiT-3 [ 40] and BLIP [ 15]. These models generate embed-dings for both visual and textual inputs, mapping them into a sharedspace. The mapping function is usually designed to be injective, fa-cilitating a one-to-one correspondence between an instance and itspoint in the embedding space. Fine-tuning these MLLMs on smallerimage-caption datasets such as MSCOCO [ 20] and Flickr30K [ 45]enables the models to achieve high accuracy in text-to-image re-trieval tasks.However, MLLMs-based methods face limitations particularly inthe context of real-world use-cases that involve large-scale, diverse,and ambiguous data such as that illustrated in Figure ??and Figure1. First, MLLMs-based methods often ignore efficiency concerns.Their model-based similarity inference methods [ 9] are computa-tionally demanding, requiring encoding between each query vectorand image embedding when ranking. This can result in a compu-tation time of up to 22 hours for a single inference for a large testset [42], limiting their utility in large-scale retrieval applicationsdespite their high accuracy. Second, real-world use-cases often in-volve complex queries and images with multiple objects [ 34,35,42].This contrasts sharply with the comprehensive but short captionsfound in datasets such as MSCOCO [ 20] and Flickr30K [ 45]. Thenature of this complexity undermines the effectiveness of injec-tive embeddings, which attempt to map diverse meanings/sensesto a single point in shared space, which could be an inaccurateweighted geometric mean of all the desirable points [ 34]. This isparticularly problematic in long-text query to image retrieval tasks,where accumulated ambiguities significantly hinder the perfor-mance of Multimodal Large Language Models (MLLMs) [ 42]. Third,injective embeddings struggle with partial text-to-image associa-tions [ 34]. In a long query, only a subset of sentences may relateto specific regions or aspects of an image, while the rest discussunrelated subjects. Additionally, a single sentence may describejust a particular region of an image rather than its entirety.To address these challenges, this paper presents a novel two-stage Coarse-to-Fine Index-shared Retrieval (Text2Pic Swift) frame-work, jointly optimizing effectiveness and efficiency. The first stageis entity-based Ranking (ER) and the ER result is used to constructa shared entity-based image candidates index. ER is designed tobe computationally cheap, using pre-computed image embeddingsarXiv:2402.15276v2 [cs.IR] 28 Feb 2024SIGIR ’24, July 14–18, 2024, Washington D.C., USA Trovato and Tobin, et al.Figure 1: The left is a plot of the average text tokens between MSCOCO short sentences, AToMiC long documents and theirsummaries and the right is the number of training and testing images of MSCOCO and AToMiC.from a cache. By replacing the entire document with a represen-tation comprising its entities as the query, we transform the re-trieval task from one query to one target, to multiple queries tomultiple targets, accommodating the ambiguity inherent in longdocuments and images. This transformation makes ER well-suitedfor use-cases demanding relevance but not exact matching, suchas multimedia content creation [ 4,43], where a diverse array ofimages is beneficial for illustrative purposes. Furthermore, ER canbe used to filter out the majority of irrelevant candidates prior tothe re-ranking stage, thereby reducing the overhead from the morepowerful encoder used in the re-ranking stage. The second stage isSummary-based Re-ranking (SR). By summarizing long documentsas queries and using entity-based image candidates from the pre-computed shared index, SR further m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ul VMthat hosts an open-source collaboration office suite can be secured and presenta replicated protocol proxy that enables commodity usersto securely access the Internet Computer, a decentralizedblockchain infrastructure.CCS CONCEPTS•Security and privacy →Virtualization and security ;Dis-tributed systems security ;Web application security .∗Research was conducted while working at DFINITY FoundationKEYWORDSConfidential Computing, TEEs, AMD SEV-SNP, Attestation,TLS1 INTRODUCTIONResponding to cloud customers’ concerns over the securityof their workloads, cloud providers are currently rolling outofferings with novel security extensions enabled, facilitatingthe use of hardware-isolated compartments, called trustedexecution environments ( TEE s). By migrating their security-sensitive workloads inside TEEs, customers are ensured thattheir data remain shielded from any unauthorised access ofprivileged system software layers, such as the hypervisor, andthe cloud-managing personnel. In recent years, VM-basedTEEs, where entire VMs can be isolated, have gained a lotof traction from the industry because of their seamless de-ployability, leveraging technologies like AMD’s SEV-SNP [2],Intel’s Trust Domain Extensions ( TDX) [23] and ARM’sConfidential Compute Architecture (CCA) [5].Although confidential computing is considered a big stepforward in terms of cloud security, its benefits cannot bereaped to a maximum degree without additional effort. Inparticular, end-users of VM-based TEEs are unaware if aservice they access is secured via trusted execution or servedby a commodity environment. They could be assured of theseguarantees via remote attestation, which essentially providesproof for the authenticity of the underlying hardware as wellas for the integrity of the software loaded inside the TEE.However,toourknowledge,thereiscurrentlynostandardised,widely established approach to provide remote attestation asa service to end-users besides the option to expose directlythe hardware manufacturer-provided APIs that are typicallyaccessible only to the VM owner/service provider.When it comes to verifying the integrity of the TEEstate,the end-users need to be aware of what constitutes an ac-ceptable state rather than just depending on a CertificateAuthority ( CA) to validate the TEEauthenticity. The TEEstate is reflected on the attestation report, which includesa cryptographic hash over the VM’s initial memory contextarXiv:2402.15277v1 [cs.CR] 23 Feb 2024Anna Galanou, Khushboo Bindlish, Luca Preibsch, Yvonne-Anne Pignolet, Christof Fetzer, and Rüdiger Kapitzaright before its launch. This hash is usually compared againsta pre-computed expected value and upon a successful match,the validating party can be reassured that the VMhas beeninitialised in a known good state. This is sufficient for serviceowners who intend to verify that a VMruns within a TEEas expected and they have full control and knowledge overthe provided VM’s image.However, for end-users the situation is different and theyare largely in the dark regarding the implementation andconfiguration of the services they access. Besides that, evenif they had access to the VM’s source code and therefore bein the position to validate the hash of the VM’s initial stateagainst an expected value, they would inherently have totrust the service provider not to tamper with the VMafterit has been measured, via management APIs (e.g., ssh) ordirectly boot it with a malicious kernel for instance. Thelatter scenario is a real threat since the VM’s initial statetypically only includes the virtual firmware used to bootthe machine and nothing more. Therefore, the measurementincluded in the attestation report is not indicative of eitherthe operating system or the root filesystem’s state that theVMbooted with. Consequently, by solely having access toa remote attestation report, which contains an initial VMcontext, end-users cannot really verify if their data is securelyhandled.The implicit trust in the service provider cannot be avoidedeven if end-to-end encryption is in place, as is in the caseof email services (e.g. ProtonMail), collaboration suites (e.g.CryptPad), cloud storage services (e.g. Tresorit), search en-gines (e.g. DuckDuckGo) etc. Even though this techniqueprovides a high level of protection for user data and theirprivacy, such guarantees are ensured as long as the end-points are not compromised. If the server-side software, forinstance, has vulnerabilities, an attacker can manage to ex-ecute code remotely and retrieve login credentials, or anyother sensitive information. Therefore, verifying the state ofsuch applications and enlightening the end-users about themis of significant value.In this paper, we present Revelio; an approach that enablesend-users to easily, systematically, and deterministically val-idate aVMand its hosted services, so that they no longerneed to entrust service providers with the integrity and con-fidentiality of their data. Service providers manage the VM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "s easy access to it. Therefore, it could act as a linchpin in the realm of personalized learning. While the technological merits of ChatGPT and its ilk are hard to dispute, a deeper explora&on into their eﬃcacy in real-world educa&onal contexts and the percep&ons they engender is crucial. In this study, the focus is on unraveling the mul&faceted experience of employing ChatGPT as a learning ally. By u&lizing a mixed-methods research design, the objec&ve becomes twofold: to quan&fy the tangible impacts of ChatGPT on learning trajectories and to delve into the lived experiences and percep&ons of learners. This comprehensive inquiry seeks to shed light on the opportuni&es, challenges, and poten&al of integra&ng ChatGPT into the educa&onal landscape. The subsequent sec&ons are organized to provide readers with a comprehensive understanding of the study and its implica&ons. The ar&cle begins with a literature review, tracing the evolu&on of AI in educa&on and the emergence of chatbots as learning facilitators. Following this, the research ques&ons are presented. Then the methodology sec&on elucidates the research design, par&cipant selec&on, and data collec&on processes. The results sec&on presents a detailed analysis of both quan&ta&ve outcomes and qualita&ve insights. This is succeeded by discussions that interpret the ﬁndings, delving into their implica&ons and situa&ng them within broader pedagogical contexts. The ar&cle concludes with a reﬂec&on on the study's limita&ons and charts out poten&al avenues for future research. 2 Literature Review Historically, the role of technology in educa&on has evolved from passive media, such as radio and television, to interac&ve plaYorms, including e-learning modules and virtual classrooms. AI, especially chatbots, represents the next phase, where learning is not only interac&ve but also adap&ve. With recent advances in AI technology, research interest in AI use for educa&on has surged since 2021 (Compton & Burke, 2023; Xu & Ouyang, 2022). The public release of ChatGPT in November 2022 further heightened this interest, given the poten&al of the tool for educa&onal applica&ons. Much of the literature addresses ChatGPT's poten&al beneﬁts and piYalls, mostly from a theore&cal standpoint. ChatGPT is believed to facilitate personalized learning experiences. It provides immediate feedback, fosters self-paced learning, and immerses students in interac&ve learning experiences (AlAfnan, 2023; Baidoo-Anu, 2023; Kasneci et al., 2023). Recognizing the diversity within classrooms, the ability of ChatGPT to tailor responses to individual queries is a notable advantage. It aids learners in building knowledge and understanding by oﬀering explana&ons, addressing ques&ons, fostering dialogs, providing examples, and elucida&ng topic relevance. This interac&ve approach encourages students to post ques&ons they might otherwise withhold in a classroom sebng, receive meaningful feedback, and revisit previous interac&ons, thus promo&ng con&nuous learning. However, the introduc&on of ChatGPT to the educa&onal landscape is not without challenges. Technical limita&ons, such as text-only input and output constraints, currently inhibit its ability to process visual content such as images or diagrams. However, plugins such as \"Show Me\" have already begun to bridge this gap. Notably, the basic version of ChatGPT only encompasses data up to September 2021, but extensions like \"WebPilot\" allow it to search the internet for updated informa&on. Data security remains a concern, with user prompts poten&ally informing future model training. Another signiﬁcant challenge is the poten&al role of ChatGPT in academic dishonesty, such as comple&ng student assignments. This underscores the need to re-evaluate homework and assessment design rather than ques&on the educa&onal u&lity of ChatGPT (Adiguzel et al., 2023). Overreliance on AI is another concern. Although AI can amplify learning experiences and oﬀer personalized support, it should not overshadow the irreplaceable role of educators or the value of human interac&on in learning environments. Educators provide context, promote cri&cal thinking, and nurture socioemo&onal learning (Farrokhinia et al., 2023). It is paramount to recognize that ChatGPT, despite appearing knowledgeable, does not genuinely \"understand\" user prompts or their responses. Deriving its knowledge from vast textual data, it may inadvertently propagate problema&c content, leading to biased or erroneous responses. Several studies have inves&gated the proﬁciency of ChatGPT in standardized tests across ﬁelds such as medicine (Gilson et al., 2023), mathema&cs (Frieder et al., 2023), physics (West, 2023), law (Choi et al., 2023), and economics (Geerling et al., 2023). Most of these studies have reported posi&ve outcomes. Several studies emphasize the nuances of learning processes. Within these contexts, it is vital to dis&nguish between overt errors—deriving from ChatGPT's acknowledged inability t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "s against theenvironment [3], [4]. In this paper, we follow a differentapproach utilizing the so-called swept volume of a robotmotion, which is the subset of the workspace that the robotmoves through during this motion. An explicit representationof this volume would allow to replace the individual collisionchecks along a path segment with a single collision checkof this swept volume against the environment.However, computing an explicit representation of theswept volume comes with its own drawbacks. It is usuallydone via voxel grid approximations, or by geometricallyapproximating the boundary of this volume, e.g., as a trianglemesh [5]. By design the accuracy of these approaches is∗Equal contribution.The authors are with KUKA Deutschland GmbH, Germany. {Dominik.Joho,Jonas.Schwinn, Kirill.Safronov }@kuka.comFig. 1: The basic idea of our approach is to predict the signed distance(black line) of a point in the task space (black dot) to its nearest point(white dot) on the surface of the swept volume (blue mesh) of a robotmotion. The swept volume is implicitly represented by a neural network,which takes the start and goal configuration of the motion as well as thequery point as inputs and outputs the signed distance of the point to theimplied swept volume. The robot motion can be collision checked againsta complete environment, by representing the environment as a point cloud.Each of these points becomes one input point for the network.strongly linked to their computation time, which hinders theirapplication for collision checking in live systems.We thus propose an implicit representation based on a deepneural network that learns the function (x,q0,q1)7→δ,where q0is the start configuration of the robot, q1is thegoal configuration, x∈R3is the query point in the taskspace for which we want to perform a collision check, andδis the signed distance of the query point to the resultingswept volume when performing the motion q0→q1, seealso Fig. 1. This approach learns a continuous implicit sweptvolume model, i.e., it can predict a signed distance for anypoint in the task space, and is not limited to a fixed resolutionof a voxel grid. Further, for a given network the inferencetime is independent of the length of the motion.This approach can be used to check a robot motion againsta complete environment represented as a point cloud. Eachpoint xof this point cloud becomes one input for thenetwork. Multiple points can be batch processed leveragingparallel computation on GPUs, which allows for favorablecomputation times compared to geometric collision checkers(GCC). However, a downside of this approach is that thedistance computed by a neural network is less reliable whencompared to a GCC. Neural networks cannot be guaranteedto extrapolate robustly to data not seen during training [6],[7], [8]. This hinders applications in industrial settings, astrading off accuracy against speed is not acceptable when itcomes to safety and potential collisions.arXiv:2402.15281v2 [cs.RO] 26 Feb 2024We therefore propose an algorithm that interleaves ourimplicit swept volume model with a GCC. As we will showin our experiments, this effectively reduces the number ofpaths that have to be checked by the GCC and thus alsoreduces the total time spent to find a collision-free path, evenwhen considering the additional time spent on computing theswept volume distances.While our primary example revolves around a bin pick-ing application, it is worth noting that the same model isalso suitable for collision checks in general sampling-basedmotion planners. Moreover, while our evaluation focuses ona robotic arm, our approach can be broadly applied to anyarticulated object that allows for swept volume computation.The main contributions of this paper are: (a) a novelapproach to modeling robot motions as implicit swept vol-umes parameterized by the start and goal configuration witha neural network, and (b) an algorithm that utilizes theseneural swept volumes to speed up collision checking whilepreserving accuracy guarantees of GCCs.II. R ELATED WORKChiang et al. [9] proposed a neural network that learnedto map the start and goal configuration of a robot motionto the swept volume (as a scalar value) of this motion.The motion is assumed to be linear in joint space. Thisis not usable for collision detection, but has been used asa heuristic to speed up a sampling-based motion planner.Building on this, Baxter et al. [10] proposed a network thatuses the same input, but outputs a voxel grid representationof the swept volume (voxels can either be “swept” or “notswept”). This method can be used for collision checks,but on the downside it has an inherent discretization errordetermined by the size of the voxel grid cells, and a lowspatial resolution due to the limited number of voxels. Incontrast, in a feasibility study Lee et al. [11] used thedeep signed distance model of Gropp et al. [12] to modelswept volumes implicitly as a signed distance field (SDF),thereby avoidin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "earning algorithms, and neural networks in particular, excel at mimicking System 1 thinking by efficientlyutilising correlations from extensive training datasets for rapid, intuitive inference. Yet, the field faces challenges indeveloping algorithms capable of System 2 -like reasoning: a more robust, flexible methodology that supports multi-step,deliberate problem-solving. Such an approach could enable algorithms to learn and adapt to new tasks with greaterefficiency, significantly reducing reliance on large training sets (Bengio, 2019; Du et al., 2022).A substantial number of approaches which mirror aspects of system 2-like cognition have already been proposed,including planning (e.g. Garcia et al., 1989; Coulom, 2006), reasoning (Du et al., 2022; Wei et al., 2022) and, moregenerally, iterative refinement at inference (Graves, 2016; Dehghani et al., 2019; Banino et al., 2021; Nye et al.,2021). In addition, certain model-based reinforcement learning (RL) methods are also thought to be linked to slow,goal-directed behaviours observed in animals (Sutton & Barto, 2018). Such methods utilise a learned world modelfor the planning and learning of optimal actions via imagined trajectories (Moerland et al., 2023). However, theirarXiv:2402.15283v1 [cs.LG] 23 Feb 2024Iterative Reasoning with Latent Imaginationeffectiveness hinges on the accuracy of the world model in representing unfamiliar environments, limiting performancewithout additional training.In this work, we introduce a novel method that allows for an interplay between system 1 and system 2 perceptualinference in model-based RL agents. Our method enhances the posterior representations of world states, therebyimproving task performance independently of traditional planning or learning processes. Drawing from prior researchon iterative optimisation in amortised variational inference (Marino et al., 2017; Tschantz et al., 2023), we introducean intrinsic inference objective to refine the agent’s performance without additional training or introducing newparameterised modules. This objective relies on the agent’s learnt world model, to compute a Monte Carlo estimationof the expected future observations, using latent imagination. By applying backpropagation to these imaginary rollouts,our method enables the agent to effectively reason about coherence with future states, accessing information gathered inother episodes and practically distilling experience that has been amortised within the world model.We tested this framework on the recent DreamerV3 agent (Hafner et al., 2023), observing a significant performanceboost in partially-observable environments, where information is more sparse across states, as well as reducing thewell-known amortisation gap in all evaluated scenarios.Our resulting contributions are as follows:•A novel training-free approach to improve a model-based RL agent with iterative inference, acting at decision-time with latent imagination.•Thorough testing and corresponding analysis of the resulting inference module with various theoretically-justified objectives in visual RL tasks.•A comparison of the impacts of our approach depending on the baseline performance of the agent pre-evaluation.2 Related Work2.1 Amortised Variational InferenceVariational inference (Jordan et al., 1998) facilitates the learning of latent variable models by reformulating inference asan optimisation problem (Neal & Hinton, 1998; Hoffman et al., 2013) with the use of an approximate posterior. It isbased around minimising the KL divergence between approximate q(z|x)and true posteriors p(z|x), for observationsx∈ D and latent states z. To avoid computing the intractable p(z|x), this quantity can be decomposed as:DKL\u0002q(z|x)||p(z|x)\u0003=logpθ(x)− L ELBO (1)with the Evidence Lower BOund (ELBO) LELBO :LELBO =Eq(z|x)[logpθ(x|z)]−DKL\u0002q(z|x)||pθ(z)\u0003(2)As the model evidence is independent of the variational distribution, maximising the ELBO with respect to it thereforealso minimises the LHS of Eq. 1. Finally, as the latter is also non-negative, we obtain the lower bound on modelevidence logpθ(x)≥ LELBO . This property is leveraged by variational autoencoders (V AEs: Kingma & Welling, 2013;Rezende et al., 2014), which parameterise both the approximation qϕ(z|x)and the likelihood pθ(x|z)and optimisethem using Eq. 2. Amortisation is applied by employing a shared, learnable function to compute the approximateposterior for each observation, effectively distributing the computational cost across the entire dataset.2.2 Connecting Fast and Slow with Iterative RefinementAmortised inference models suffer from the amortisation gap (Cremer et al., 2018; Krishnan et al., 2018), due tothe often limited capacity of the neural network used for encoding. To combat this issue, some hybrid optimisationmethods combining amortised (fast) and iterative (slow) inference have been proposed. Such methods generally seek toiteratively refine individual approximate posteriors with respect to Eq. 2, or a similar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "e been conducted in STDM com-munities to tackle this problem. Traditional machine learn-ing methods, like k-nearest neighbors (KNN) [4], sup-port vector machine (SVM) [5], gaussian process regression(GPR) [6] are hard to learn the complex spatiotemporal fea-tures. Recently, deep learning-based methods have been ap-plied to make spatiotemporal predictions and achieved re-markable performance. 2D Convolutional neural networks(CNNs) were used in DeepST [7] and ST-ResNet [8] astheir basic layers. Furthermore, 3D CNN was introduced inST-3DNet [9]. Besides, recurrent neural networks (RNNs)and their variants were studied in extensive works likeConvLSTM [10], MIM [11], MotionRNN [12].Though current deep learning-based methods showpromising performance, these spatiotemporal models areintuitively designed by trial and error, needing more the-oretical analysis. They need more physical explanations andtheoretical guidance in model design. Theory guaranteedmodel is of great significance when we deal with criticalissues associated with high risks (e.g., healthcare). How•The Authors are with the Department of Systems Engineering, CityUniversity of Hong Kong, Hong Kong, SAR, China. E-mail: tyliang4-c@my.cityu.edu.hk; mehxli@cityu.edu.hk.•H-X Li is the corresponding author.Manuscript received XX XX, XXXX; revised XX XX, XXXX.to design a deep learning-based model with theoreticalconvergence guarantees for prediction should be paid moreattention [13], [14]. Effective mechanisms could lay solidsupport for the reasoning of the abstract data [15].To address the issues above, we combine the observerdesign in control theory with deep learning and proposea theory-guided and guaranteed framework, called Spa-tiotemporal Observer , for spatiotemporal predictive learning.Inspired by the Kazantzis-Kravaris-Luenberger (KKL) ob-server for traditional low-dimensional systems, we designa spatiotemporal observer for nonlinear systems with highdimensions. The proposed spatiotemporal observer has the-oretical guarantees, including the convergence for one-step-ahead forecasting and the upper error bound of multi-step-ahead prediction. As a result, this observer provides atheory-guaranteed architecture for modeling spatiotempo-ral data.Specifically, we first extract low-dimensional represen-tations from original data by a spatial encoder. Then, aspatiotemporal observer is introduced to estimate the futurestates in the latent space. Finally, the predicted future latentrepresentations are reconstructed to observations via a spa-tial decoder. Because CNNs show outstanding performancewith simplicity and efficiency, we instantiate the proposedframework with CNNs, including 2D convolution layersand inception modules [16].The main contributions of this paper are concluded asfollows.•A spatiotemporal observer is proposed for predic-tive learning of high-dimensional data. It can makepredictions in one-step-ahead and multi-step-ahead(section 4.1).•We introduce dynamic regularization during thetraining process, which is beneficial to improv-ing model performance (section 4.2). The proposedframework has a theoretical guarantee of conver-gence and upper bounded error (section 4.3).arXiv:2402.15284v1 [cs.LG] 23 Feb 20242•We instantiate the proposed spatiotemporal observerwith CNNs. Extensive experiments were conductedto validate the performance and effectiveness of theproposed framework. (section 5)2 R ELATED WORKWith increasing attention drawn to this field, many deeplearning-based works have been conducted and achievedsignificant performance in recent years [14], [17]. Thesespatiotemporal predictive models can be classified into twomain lines: recursive and feedforward.One line is recurrent models, which employ the RNN-based architecture for future prediction. Specifically, RNNshave been extensively applied in time series modeling [18].Shi et al. [19] integrated CNNs into RNNs architecturefor precipitation nowcasting. The proposed convolutionalLSTM became a baseline in spatiotemporal predictive learn-ing. After that, many models were presented, for exam-ple, PredRNN [20], PredRNN++ [21], TrajGRU [19], MSPN[22], MotionRNN [12], and MS-RNN [23]. Recursive modelshave an advantage in predicting the future with flexibletime length. However, recursive models suffer high com-putational expense and parallelization difficulty because ofthe chain mechanism of RNNs. To mitigate this problem,researchers proposed a CNN-RNN-CNN framework, usingRNNs in the encoded latent space [24]. Such methods, likeE3D-LSTM [25], CrevNet [26], and PhyDNet [27], use CNNsto reduce the spatial size and capture spatial relationshipsand use RNN to model the temporal dynamics for futureprediction.Another line of research is feedforward models. Thiskind of method usually stacks CNNs as its backbone due toCNNs’ extraordinary success in various tasks in computervision [28]. Oh et al. [29] proposed a CNNs-based architec-ture for next-frame prediction in Atari games. Tran et al. [30]found that 3D \n",
      "----------------------------------------------------------------------------------------------------\n",
      "extraction (OE), and sentiment classification(SC) (Zhang et al., 2022). Within this paper, ourprimary focus centers on two of these subtasks:AE and SC, collectively referred to as AESC. Forexample, in the sentence “Amazing Spanish Mack-eral special appetizer and perfect box sushi ( thateel with avodcao – um um um ).”, there are twosets of aspect terms and their sentiment polarities,(Spanish Mackeral special appetizer , Positive) and(box sushi , Positive).Some previous studies perform AE and SC inde-pendently in a pipeline, potentially leading to errorpropagation (Fan et al., 2019; Hu et al., 2019). Re-cently, end-to-end models are designed to addresstwo subtasks jointly via unified tagging schema(Mitchell et al., 2013; Zhang et al., 2015; Li et al.,2019a) or machine reading comprehension frame-work (Y ang and Zhao, 2022). Furthermore, gener-ative techniques have emerged as a powerful toolin tackling ABSA challenges (Zhang et al., 2022;Yan et al., 2021). These methods often involvethe generation of sentiment element sequencesadhering to specific formats, thereby harnessing∗Corresponding author, jzhou@cs.ecnu.edu.cn.the nuances of label semantics. However, a recur-ring limitation across these approaches lies in theirstruggle to precisely delineate the boundaries ofaspects due to the inherent diversity of languageexpression. The ambiguity and fluidity of languageusage can lead to indistinct boundaries, where oneaspect term might encompass multiple words (e.g.,“Spanish Mackeral special appetizer”), and a singlesentence could encompass multiple aspect terms.In response to this intricate challenge, we in-troduce an innovative solution by integrating adiffusion model (Sohl-Dickstein et al., 2015) – aparadigm that has showcased impressive capabili-ties in controlled generation tasks. Notably, diffu-sion models have demonstrated remarkable perfor-mance in various domains, including text-to-imagegeneration (Zhang et al., 2023; Nichol et al., 2022),and text generation (Nachmani and Dovrat, 2021;He et al., 2022). These achievements stand astestaments to the potential of diffusion models infacilitating token-level controls (Zou et al., 2023). Atits core, a diffusion model orchestrates the processof generation in a stepwise manner. During train-ing, it introduces noise to the input, progressivelyrefining the generation process. Subsequently, itlearns a complementary denoising procedure to ac-curately restore the original input. Leveraging thisunderlying mechanism, we propose an ingeniousfusion of the diffusion model with ABSA, harness-ing its capabilities to enhance the inference of theaspects.This paper introduces DiffusionABSA , a novelarchitecture for diffusion architecture for ABSAwhich effectively marries the controlled generationarXiv:2402.15289v1 [cs.CL] 23 Feb 2024STEP 1 AmazingASPECT POSITIVEz }| {Spanish Mackeral special appetizer and perfectMISSINGz}|{box sushi ( that eel with avodcao – um um um ).STEP 2 AmazingASPECT POSITIVEz }| {Spanish Mackeral special appetizer and perfect box sushi ( that eel with avodcao – um um um ).STEP 3 AmazingASPECT POSITIVEz }| {Spanish Mackeral special appetizer and perfectASPECT POSITIVEz}|{box sushi ( that eel with avodcao – um um um )....Table 1: The boundary of aspect terms gradually changes during the denoising process inDiffusionABSA . The spans annotated with green ,orange , and redrespectively signify the correct,missing, wrong results.process of diffusion models with the intricate as-pect detection challenge characteristic of ABSA.DiffusionABSA is structured around two fun-damental processes: corruption and denoising.The corruption process gradually adds Gaussiannoise to the aspect terms according to a fixed vari-ance schedule. The denoising process undoesthe added noise at each time step iteratively andlearns to faithfully reconstruct the original data byreversing this noising process. Illustrative insightsfrom Table 1 underscore the distinctive featuresofDiffusionABSA in handling intricate aspectboundaries. Specifically, the model demonstratesiterative refinement in aspect extraction: initiallymissing an aspect (“box sushi”), then combiningtwo aspects into one (“Spanish Mackeral specialappetizer and perfect box sushi”), and finally, accu-rately extracting both aspects (“Spanish Mackeralspecial appetizer” and “box sushi”). To further bol-sterDiffusionABSA ’s capabilities, we introducea denoising neural network equipped with a syntax-aware temporal attention strategy. This strategicaugmentation facilitates the model’s adeptness incapturing the temporal evolution of aspect-text in-teractions, resulting in a more effective aspect mod-eling process.To comprehensively assess the efficacy ofour proposed DiffusionABSA , we conducta series of experiments across eight diversedatasets, benchmarked against several state-of-the-art (SOTA) baselines. The empirical findingsaffirm the superiority of DiffusionABSA in mostcases. Additionally, ablation studies provide nu-anced insights i\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ed efficiently in a parallel setting because ofthe backpropagation across the time chain.Transformer with attention mechanism avoids the problemsin RNNs and realizes parallel computing [7]. Thereafter, itachieves remarkable success in NLP, CV , and time series[8]. However, the quadratic complexity of length prevents theTransformer from exhibiting efficient performance in modelinglong sequences. Many efficient variants have been introducedwith reduced complexity to tackle this issue, such as InformerThe Authors are with the Department of Systems Engineering, CityUniversity of Hong Kong, Hong Kong, SAR, China. E-mail: tyliang4-c@my.cityu.edu.hk; mehxli@cityu.edu.hk.H-X Li is the corresponding author.Manuscript received XX XX, XXXX; revised XX XX, XXXX.[9], and Crossformer [10]. Unfortunately, these efficient vari-ants perform unsatisfying results on long-sequence modelingtasks, worse than the original Transformer [11].Recently, state space models (SSMs) based neural networkshave made significant progress in long-sequence modeling.LSSL [12] incorporates the HiPPO theory [13] and out-performs the Transformer family models in many sequencetasks. Hereafter, some variants of SSMs are proposed, likeS4 [14], DSS [15], S4D [16] and so on. They all showremarkable abilities in modeling long sequences on varioustasks, including time series forecasting, speech recognition,and image classification.However, there still exist some research gaps to be ad-dressed.•Existing models are based on single-input and single-output (SISO) SSMs stacked parallel for modeling multi-variate sequences. An additional linear layer is needed tomix features from different SISO SSMs. However, multi-input and multi-output (MIMO) SSMs lack exploration.•The parameterization and initialization are found to beheavily sensitive. Some parameters are set as unlearnablefor training. Moreover, the system matrix Ahas to beinitialized as a “HiPPO matrix” to ensure good perfor-mance.•S4 and its variants are delicate but difficult to understandand implement because of complex matrix transforma-tion.In this work, we introduce Linear Dynamics-embeddedNeural Network (LDNN) based on MIMO SSMs, designedto solve the abovementioned issues. We model the lineardynamics using continuous MIMO SSMs, which have theproperties of fewer parameters than discrete neural networks(NN). Thus, we can model the multi-variate sequences directly.Then, a discrete representation can be obtained through maturediscretization technology. The discrete SSMs are recurrentmodels that can make flexible inferences as RNNs. Fur-thermore, to train the proposed model efficiently, we deriveits convolutional representations, which enables the trainingprocess to be more efficient with parallel computing.We reduce the number of model parameters and improve thecomputational efficiency of the model by diagonalization and′Disentanglement then Fast Fourier Transform (FFT)′strate-gies. The diagonal SSMs in convolutional representationssignificantly reduce time and space complexity at the trainingphase. Afterwards, we disentangle and transform the originalconvolution operation between the state kernel and multi-variate input sequence equivalently. Then, the Fast FourierarXiv:2402.15290v1 [cs.LG] 23 Feb 20242Transform (FFT) is applied for the disentangled convolutionoperation.We parameterize and initialize the proposed LDNN in amore general format than existing works. All parameters areoptional to be learnable or not. The system matrix Acan beinitialized via the HiPPO, random or constant matrix. Theablation study suggests that any initialization strategy with aproper learning rate has a competitive accuracy.The linear SSMs used in this work are causal systems,which may limit their applications to non-causal cases, suchas image classification. To fix this, we propose the non-causalvariant with a bidirectional kernel. Besides, motivated byMulti-Head Attention [7], we design the Multi-Head LDNN.The original input vector is split into multiple sub-vectorsand then modeled by LDNN. Outputs of all LDNN areconcatenated and mixed by a linear projection function.We evaluate the effectiveness and performance of LDNNusing the Long Range Arena (LRA) benchmark. LRA containssix long sequences of classification tasks over text, image, andmathematical operations. Our models outperform Transformervariants and match the performance of the state-of-the-artmodels.The main contributions of our work are summarized asfollows.1) We design a new neural network, LDNN, for long-sequence modeling based on MIMO SSMs. LDNN hasthe characteristics of few parameters, flexible inference,and efficient training (Section III-A).2) We reduce the time complexity of convolution fromO(LNH max{L, N})toO(LNmax{H,logL})by de-sign diagonalization and′Disentanglement then FFT′strategies. In convolutional representation, therefore,LDNN can be efficiently trained (Section III-B) (SectionIII-B).3) We propose two variants of LDNN: the BidirectionalLDNN and \n",
      "----------------------------------------------------------------------------------------------------\n",
      "decade, transitioned from theory to practicalwidely used implementation [14, 16, 30, 69, 76, 84, 89, 93].On the forefront of the practical application of general-purpose ZKPs are Succinct Non-interactive Argument ofKnowledge (SNARKs) [25, 43, 47, 52, 82]. SNARKs arenon-interactive protocols that allow the prover to generatea succinct proof. The proof is efficiently checked by the veri-fier, while maintaining three crucial properties: completeness,soundness, and zero-knowledge. What makes SNARKs partic-ularly appealing is their general-purpose nature, allowing anycomputational statement represented as a circuit to be provenand efficiently verified. Typically, SNARKs are used to provethat for a given function fand a public input x, the proverknows a (private) witness w, such as f(x,w) =y. This capabil-ity allows SNARKs to be used in various applications, includ-ing ensuring data storage integrity [89], enhancing privacy indigital asset transfers [69, 93] and program execution [14, 16],as well as scaling blockchain infrastructure [62, 85, 86, 96].Their versatility also extends to non-blockchain uses, suchas in secure communication protocols [64, 92, 107] and inefforts to combat disinformation [31, 57, 59]. Unfortunately,developing and deploying systems that use SNARKs safely isa challenging task.In this paper, we undertake a comprehensive analysis ofpublicly disclosed vulnerabilities in SNARK systems. De-spite the existence of multiple security reports affecting suchsystems, the information tends to be scattered. Additionally,the complexity of SNARK-based systems and the unique pro-gramming model required for writing ZK circuits make itdifficult to obtain a comprehensive understanding of the pre-vailing vulnerabilities and overall security properties of thesesystems. Traditional taxonomies for software vulnerabilitiesdo not apply in the case of SNARKs; hence, we provide theseminal work that addresses this gap by providing a holis-tic taxonomy that highlights pitfalls in developing and usingSNARKs. Specifically, we analyzed 141vulnerability reportsspanning nearly 6years, from 2018 until 2024. Our studyspans the entire SNARK stack, encompassing the theoreti-cal foundations, frameworks used for writing and compilingcircuits, circuit programs, and system deployments. We sys-tematically categorize and investigate a wide array of vulner-abilities, uncovering multiple insights about the extent andcauses of existing vulnerabilities, and potential mitigations.Contributions.1arXiv:2402.15293v1 [cs.CR] 23 Feb 2024•SNARKs system and threat models: We provide thefirst framework for reasoning about systems built usingSNARKs, analyzing interactions between different com-ponents, defining adversaries and their knowledge, anddiscussing potential implementation-level vulnerabilitiesand their impact.•Study of vulnerabilities: We present the first system-atic study of known vulnerabilities in systems usingSNARKs. We gathered 141vulnerabilities from 107au-dit reports, 16vulnerability disclosures, as well as a num-ber of bug trackers of popular SNARK projects. Whenit comes to SNARKs, this is the first study of this scalein the literature. Further, because of the breadth of ourcoverage, we believe our findings to be representative ofthe entire SNARK space.•Vulnerabilities taxonomy: We introduce a taxonomyfor classifying vulnerabilities in SNARKs, highlight-ing unique vulnerabilities and common pitfalls in theSNARK stack that help researchers and practitioners bet-ter understand important threats in the SNARK ecosys-tem.•Analyzing defenses: We analyze the main defense tech-niques proposed by the research and practitioner com-munities and highlight some notable gaps.Key Findings. We find that developers seem to struggle incorrectly implementing arithmetic circuits that are free of vul-nerabilities, especially due to most tools exposing a low-levelprogramming interface that can easily lead to misuse withoutextensive domain knowledge in cryptography. In detail, wefind the following flaws to be most pressing: (i) Implementa-tion bugs across SNARK systems’ layers , including classic vul-nerabilities like input validation errors and over/underflows.These can undermine SNARKs’ core properties: complete-ness, soundness, and zero-knowledge. (ii) The unique pro-gramming model for SNARK circuits poses challenges, oftenleading to under-constrained circuits . This category emergesfrom overlooking constraints or misinterpreting logic intocircuits. The low-level nature of SNARK Domain SpecificLanguages (DSLs), such as Circom, exacerbates vulnerabili-ties due to a lack of common high-level programming featuressuch as basic types. (iii) Design and implementation errors inproof systems are critical yet often overlooked vulnerabilities.These errors may originate from the frameworks implement-ing the proof systems, like an implementation error in Gnark’sPlonk verifier, or from the theoretical foundations themselves.An example is the “Frozen Heart” vulne\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ning data are labeled [4].Traditional semi-supervised counting methods targetdensity regression and then leverage self-supervised cri-teria [4], [5] or pseudo-label generation [6], [7] to exploitsupervision signals under unlabeled data. These methodsare designed to directly generate density maps, where eachpixel is associated with a definite value. However, it isstill extremely difficult to learn a good model due to theuncertainty of pixel labels. Firstly, there are commonly er-roneous head locations in the annotations [8], [9]; Secondly,the pseudo labels for unlabeled training data assigned bythe models are pervasively noisy.To address these challenges, we propose a new semi-supervised counting model, termed by the Pixel-by-PixelProbability distribution modelling Network (P3Net). Unliketraditional methods which generate a deterministic pixeldensity value, we model the targeted density value of a pixelas a probability distribution. On this premise, we contributeto semi-supervised counting in four ways.•Hui Lin, Zhou Su, and Deyu Meng are with Xi’an Jiaotong Univer-sity. Zhiheng Ma is with Shenzhen Institute of Advanced TechnologyChinese Academy of Science. Rongrong Ji is with Xiamen University.Yaowei Wang is with Peng Cheng Laboratory. Xiaopeng Hong is withthe Faculty of Computing, Harbin Institute of Technology. E-mail:{linhuixjtu@gmail.com / hongxiaopeng@ieee.org }.•†Corresponding author: Xiaopeng Hong.•We propose a Pixel-wise probabilistic Distribution (PDM)loss to match the distributions of the predicted densityvalues and the targeted ones pixel by pixel. The PDMloss, designed in line with the Cumulative DistributionFunction distance, measures the cumulative gap betweenthe predicted distribution and the ground-truth one alongthe density (interval) dimension. By modeling the densityintervals probabilistically, our method responds well tothe uncertainty in the labels. It thus surpasses traditionalmethods that regard the density values as deterministic.•We incorporate the transformer decoder structure with adensity-token scheme to modulate the features and gen-erate high-quality density maps. A density token encodesthe semantic information of a specific density interval.In prediction, these density-specific tokens specialize theforwards of the transformer decoder with respect to thecorresponding density intervals.•We create two discrete representations of the pixel-wisedensity probability function and shift one to be inter-leaved, which are modelled by a dual-branch networkstructure. Then we propose an inter-branch ExpectationConsistency Regularization term to reconcile the expecta-tion of the predictions made by the two branches.•We set up new state-of-the-art performance forsemi-supervised crowd counting on four challengingcrowd counting datasets, i.e. UCF-QNRF [10], JHU-Crowd++ [11], ShanghaiTech A and B [1]. Our methodoutperforms previous state-of-the-art methods under allthree settings of labeled ratio.2 R ELATED WORKS2.0.1 Fully-supervised Crowd Counting.Early methods tackle the crowd counting problem by ex-haustively detecting every individual in the image [12] [13].However, these methods are sensitive to occlusion andarXiv:2402.15297v1 [cs.CV] 23 Feb 2024TECHNICAL REPORT, 2024 2require additional annotations like bounding boxes. Withthe introduction of density map [14], numerous CNN-based approaches are proposed to treat crowd countingas a regression problem. MCNN [1] employs multi-columnnetwork with adaptive Gaussian kernels to extract multi-scale features. Switch-CNN [15] handles the variation ofcrowd density by training a switch classifier to relay apatch to a particular regressor. SANet [2] proposes a localpattern consistency loss with scale aggregation modules andtransposed convolutions. CSRnet [16] uses dilated kernelsto enlarge receptive fields and perform accurate count es-timation of highly congested scenes. BL [3] introduces theloss under Bayesian assumption to calculate the expectedcount of pixels. Furthermore, methods based on multi-scale mechanisms [17], [18], [19], perspective estimation [20],[21] and optimal transport [22], [23], [24] are proposed toovercome the problem caused by large scale variations incrowd images.Recently, to alleviate the problem of inaccurate anno-tations in crowd counting, a few studies begin to findsolutions by quantizing the count values within each localpatch into a set of intervals and learning to classify. S-DCNetproposes a classifier and a division decider to decide whichsub-region should be divided and transform the open-setcounting into a closed-set problem [25]. A block-wise countlevel classification framework is introduced to address theproblems of inaccurately generated regression targets andserious sample imbalances [26]. The work [27] proposes anadaptive mixture regression framework and leverages on lo-cal counting map to reduce the inconsistency between train-ing targets and evaluation criteria. UEPNet [28] uses twocriteria to minimize the predic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "onfidence (Xiong et al., 2023). This is especially a concern for safety-critical applications such asrobotics (Brohan et al., 2023) and medical image analysis (Thawkar et al., 2023), as well as for human-AI interactionsettings where models should behave in a predictable manner that is well aligned with human expectations.Intuitively, object hallucination can be viewed through a lens of human-AI misalignment. Consider how humans describeimages: generally, we mentally ‘anchor’ or compare our descriptions to objects in the image, making hallucinationunlikely. In contrast, LVLMs generate text based on token likelihoods without explicitly ‘anchoring’ or comparingto the image in this manner, making hallucination more likely. This gap between how humans and AI operate makeserrors made by AI more surprising and unpredictable, which is detrimental in settings involving human-AI interaction.Recent efforts (Liu et al., 2023a; Wang et al., 2023a) have introduced approaches to mitigate hallucinations throughinstruction tuning. However, these methods come with substantial additional costs, including an annotation budgetto acquire extra instruction data (Liu et al., 2023a). Some studies advocate leveraging internal information frommodels, such as likelihood scores (Zhou et al., 2023) and internal hidden states (Huang et al., 2023; Leng et al., 2023).Nevertheless, relying solely on a model’s internal information may be insufficient (Manakul et al., 2023) and potentiallyunreliable, given the known overconfidence issues in neural networks (Kadavath et al., 2022; Xiong et al., 2023).Alternatively, incorporating external tools or knowledge has been suggested (Yin et al., 2023), but this approach ofteninvolves separate, occasionally intricate modules, accompanied by a heavy engineering burden (Yin et al., 2023).arXiv:2402.15300v1 [cs.CV] 23 Feb 2024SEEING IS BELIEVING : M ITIGATING HALLUCINATION IN LARGE VISION -LANGUAGE MODELS VIA CLIP-G UIDED DECODINGLarge Vision-Language ModelDescribe this image in detail.CLIP ModelCLIP Score0.60.30.1Candidate Sentences:Generating ResponseThe image features a white and brown bulldog sitting on a skateboard, which is placed on a sidewalk. There are some people standing behind.A backpackis visible in the background.The room has a couchbehind.Figure 1: Intuition of our method: candidate sentences with higher CLIP similarity to the image are less likely to behallucinated, and hence selected during the decoding process. Hallucinated text is colored red.In contrast with these approaches, we mitigate hallucination by directly comparing the LVLM’s generated text to theimage to ensure good correspondence between them, improving alignment to how humans perform the task. Specifically,as illustrated in Figure 1, we introduce CLIP-Guided Decoding (CGD), a straightforward but effective training-freeapproach that utilizes CLIP as an external guide to alleviate hallucination during decoding.CLIP models are widely adopted in image-text evaluation (Hessel et al., 2021) and have primarily been investigated inthe context of pairwise comparisons (Hessel et al., 2021; Thrush et al., 2022; Hsieh et al., 2023). However, it is stillunderexplored if CLIP models can identify hallucinations in open-ended texts generated by LVLMs, including thosewhose vision encoders are themselves CLIP models (Liu et al., 2023b; Li et al., 2023a).To study this, we empirically analyze sentence-level object hallucination in LVLMs, finding that CLIP scores are astronger and more robust indicator of hallucination compared to token likelihoods, especially in the later sentences ofeach caption. We also observe that hallucination is consistently more likely in later sentences, across multiple LVLMs.Motivated by this, we integrate CLIP models as external guidance into decoding at the sentence level.Empirical results demonstrate the success of our method in mitigating hallucination while preserving the utility of textgeneration. Interestingly, we observe improvement even when reusing the CLIP models utilized in the LVLMs, indicatingthe potential overfitting during the fine-tuning phrase in existing LVLMs. Our contributions can be summarized asfollows:•We conduct a comprehensive hallucination analysis across multiple datasets, COCO and NoCaps (Out-of-Domain), utilizing different metrics including likelihood scores and CLIP scores at the sentence level.•We propose CLIP-Guided Decoding (CGD), designed as a lightweight method to facilitate external guidanceduring decoding at the sentence level, mitigating hallucination in a training-free manner.•We quantitatively assess our method in hallucination evaluation together with generation quality evaluation.The empirical results demonstrate our method’s effectiveness in reducing hallucination while preserving theoverall generation quality.2 Related Works2.1 Large Vision-Language ModelsRecent developments in Large Vision-Language Models (LVLMs) (Liu et al., 2023b; Li et al., 2023a; Ye et al., 2023)have be\n",
      "----------------------------------------------------------------------------------------------------\n",
      "rst name].[last name]@data61.csiro.au‡Email address: yidong.gan@sydney.edu.auis a powerful SCM method for representing andanalyzing the causal relationships among factors.Causal graphs, which are integral to DGCMs, vi-sually depict the hypothesized causal connectionsbetween nodes (factors) with directed edges.Causal graph recovery (Spirtes and Glymour,1991) usually seeks information from domainknowledge or data to uncover the structure ofcausal graphs. The task is often done throughCausal Discovery (CD) (Glymour et al., 2019)methods using a statistical estimation-based ap-proach through observational data analysis wheninterventions or randomized experiments are notviable. Various algorithms along this line (Spirteset al., 2001; Chickering, 2002; Shimizu et al., 2006;Sanchez-Romero et al., 2018) utilize statistical teststo assess associational relationships between fac-tors as evidence to infer causal connections. Con-sequently, the reliability of these algorithms is af-fected by the quality of data, which can be compro-mised by issues such as measurement error (Zhanget al., 2017) and selection bias (Bareinboim et al.,2014). Furthermore, the unmeasured confoundersand assumptions underlying the construction ofcausal models, such as the Gaussian data distribu-tions, may not reflect the complexity of real-worldscenarios. These shortcomings contribute to thesusceptibility of CD methods to biases arising fromboth the data collection process and the model as-sumptions, underscoring the need for careful con-sideration and validation of the methods used incausal inference.Recently, to mitigate the limitations of data qual-ity in statistical estimation-based causal graph re-covery tasks, Large Language Models (LLMs)(Zhao et al., 2023) have been employed for causalgraph recovery (Zhou et al., 2023) in two mainways: directly outputting causal graphs or assistingin refining causal graphs generated by statisticalor ML-based solutions. A straightforward methoddirectly queries LLMs about every possible pair1arXiv:2402.15301v1 [cs.CL] 23 Feb 2024of factors (Choi et al., 2022; Long et al., 2022;Kıcıman et al., 2023) to recover causal graphs. Tosolve the high complexity issue, Jiralerspong et al.(2024) proposed a breadth-first search approachto reduce the number of required queries. How-ever, such methods require LLMs to have extensivebackground knowledge and robust causal reason-ing skills, which are still being critically assessed(Zeˇcevi´c et al., 2023). Alternatively, Vashishthaet al. (2023); Ban et al. (2023) employ LLMsto inject domain causal knowledge into statisticalestimation-based methods, yet similar issues existwith these methods.To address these challenges, we propose to re-cover causal connections by information extractedfrom a knowledge base containing related lit-erature that contains valuable insight hidden indatasets about associational/causal relationshipsamong variables. By leveraging LLMs to accom-plish the information extraction from large docu-ment databases, we introduce the LLM AssistedCausal Recovery (LACR) method, which harnessesthe collective insights from a large corpus of scien-tific literature. Instead of relying on LLMs’ causalreasoning capability, our approach leverages theRetrieval Augmented Generation (RAG) (Lewiset al., 2020; Borgeaud et al., 2022) of LLMs to sys-tematically analyze and extract relevant informa-tion from a comprehensive collection of researchpapers. Since the quality of the causal relationshipsexisting in the literature varies, we utilize LLMs toextract associational relationships from related sci-entific literature, which are further used to inducecausal relationships. Moreover, LACR is purelydata-driven: we do not rely on task-specific knowl-edge for document retrieval or prompt design, andtherefore, it can serve as a causal graph recoverytool for generic tasks.LACR first retrieves relevant text chunks fromthe aggregated literature, and then, the LLM istasked with identifying and reasoning the associa-tional relationships between factors. Subsequently,we construct a causal graph where each node is afactor and each edge represents a causal connec-tion between two factors. The causal connection isderived from associations identified by the LLM.This methodology provides a more structuredand less biased approach to inferring causal rela-tionships, as it is grounded in a broader evidentiarybase and subject to systematic validation. The ro-bustness of our solution is further enhanced by theselection of a compatible knowledge base to theLLM that reduces the uncertainty of associationalrelation extraction from the LLM.In summary, LACR shows a significant advance-ment in the causal graph recovery tasks, offering amethod that is both grounded in scientific evidenceand less susceptible to the biases that have histori-cally challenged this area of research. We validateour method against the well-established SACHSdataset (Sachs et al., 2005) and conduct a compar-ative analysis with ex\n",
      "----------------------------------------------------------------------------------------------------\n",
      "e Models (LLMs) such as Chat-GPT1and Llama [Touvron et al. , 2023 ]represents a transfor-mative shift in our engagement with technology, promisingto revolutionize various sectors through intelligent automa-tion and personalized interactions. However, alongside their1https://openai.com/blog/chatgptremarkable ability to emulate human-like text, these modelsintroduce significant ethical and security challenges [Wanget al. , 2024; Zhao et al. , 2024 ], including the potential forspreading misinformation [Bommasani et al. , 2022; Hazell,2023 ]and exploitation in illicit activities. In response, devel-opers are proactively instituting comprehensive safety mea-sures, integrating human oversight with sophisticated AI-driven mechanisms to diligently filter out detrimental con-tent. Techniques like reinforcement learning [Schulman etal., 2017 ]are central to this effort, enabling models to im-prove their outputs based on feedback. For example, Llama-2-Chat [Touvron et al. , 2023 ]demonstrates a commitment tosafety by incorporating human feedback, undergoing specificsafety training, and employing red teaming to uncover vulner-abilities, thus ensuring both functionality and security. Thisproactive approach underscores a commitment to responsi-bly deploying these technologies, balancing their innovativepotential with stringent safety protocols to mitigate risks andbuild trust in LLMs. As these models evolve, maintaining thisfocus on ethical considerations and user safety is essential forleveraging their capabilities without compromising digital se-curity or ethical norms.Despite these efforts, LLMs remain susceptible to sophisti-cated ‘jailbreaking’ attempts, which exploit vulnerabilities tobypass the integrated safety features, challenging the integrityand reliability of these systems. Researchers have uncoveredvarious exploitation techniques, such as adversarial prompt-ing[Zhuet al. , 2024 ], malicious fine-tuning [Qiet al. , 2023 ],and decoding strategy exploitation [Huang et al. , 2024b ], alldesigned to sidestep the safety measures of LLMs. This ex-ploration into the models’ inherent weaknesses has revealedthat even LLMs prioritized for safety can be manipulatedinto harmful behaviors with carefully crafted inputs. Thisparadox suggests that enhancing an LLM’s compliance andinstruction-following capacity might inadvertently introducevulnerabilities, enabling the execution of detrimental instruc-tions or the spread of misinformation. Alarmingly, methodslike appending specific suffixes or crafting inputs in a particu-lar way can effectively bypass safety alignments, presenting asignificant ethical and security issue. The threat landscape isfurther complicated by ‘data poisoning’ attacks that bias themodel’s behavior [Huang et al. , 2024a ]and ‘model inversion’techniques [Morris et al. , 2023 ]that risk exposing sensitiveinformation learned during training. These challenges high-arXiv:2402.15302v1 [cs.CL] 23 Feb 2024light the ongoing battle between advancing LLM capabilitiesand safeguarding against potential misuse, emphasizing theneed for continuous vigilance and innovation in security prac-tices.One critically underexplored source of vulnerability is whathappens when vanilla text responses get substituted by nu-anced answer types such as a set of instructions which maybe a pseudocode, a program or a software snippet. Generatingsuch instruction-centric responses introduces an additionallayer of complexity. It is hitherto unknown whether this ad-ditional complexity results in unintentional reinforcement ofnegative stereotypes or promotion of harmful practices. Thecapacity for incremental edits to refine model outputs mightfurther exacerbate the risk of generating additional unethicalcontent. However, there is no benchmark dataset available inthe literature that allows us to test the model robustness whenthey are tasked to elicit instruction-centric responses.To bridge this gap, we introduce a meticulously assembledbenchmark dataset, T ECHHAZARD QA, which encompassesqueries from diverse fields, answerable in both text andinstruction-centric formats ( henceforth pseudocodes ). Thisdataset serves as a foundation for assessing how differentprompting strategies may inadvertently encourage LLMs toproduce unethical content. Further, our analysis extends toexploring the impact of model refinement through specificquestion-and-answer pairs on the propensity of LLMs to gen-erate such content. Through this work, we aim to highlightthe critical need for enhanced moderation techniques and thedevelopment of LLMs that maintain ethical integrity whilenavigating the complexities of nuanced instruction-centric re-sponse generation.Key contributions : The key contributions of this paper areas follows.• We introduce a dataset, T ECHHAZARD QA, that con-tains sensitive and unethical questions which can be an-swered through text as well as through a pseudocode.The dataset has ∼1850 such questions arranged acrossseven technological topics.• We \n",
      "----------------------------------------------------------------------------------------------------\n",
      "erlying handwriting recognition models.Historical perspective. Approaches to handwriting recog-nition have evolved over time alongside similar problemsin speech recognition and OCR, going from segment-and-decode models (Hu et al., 1996) to RNNs (Carbune et al.,2020; Graves et al., 2009) to Transformer-based approaches(Dhiaf et al., 2023). However, as in other modalities, it isstill far from being solved, especially in more complex casesthat involve whole-page note-taking, math expression recog-nition, and scripts with small amounts of training data. Thisis also visible through the variety of model architecturesused to solve these problems (Xie et al., 2023).Why use VLMs? LLMs and VLMs (Brown et al., 2020;Touvron et al., 2023; Chen et al., 2023a) have shown impres-sive performance in a variety of tasks and across differentmodalities and they can offer multiple benefits if they can beused for solving handwriting recognition problems (or anyother target domain). The obvious one is a potential qual-ity improvement coming from their scale and underlyinglanguage model. Their simple design allows fine-tuning asingle model end-to-end using standard and widely availabletools in contrast to standard multi-step recognition models,ex. (Carbune et al., 2020). Finally, they allow seamlesslymixing multiple handwriting tasks expanding the plethoraof tasks they can already perform.How to use VLMs? To be able to instill VLMs with hand-writing recognition, one needs a representation of digitalink that is suitable for use with VLMs. A straightforwardapproach is to simply provide a rendering of digital ink asan input image and perform OCR. However, for handwritingrecognition, OCR-only performance is subpar to the qualityof specialized online handwriting recognition models thatoperate on the digital ink as a time-ordered sequence ofpoints (Xie et al., 2023).Our work . The focus of our work is the representationof digital ink for VLMs that is applicable across differentdatasets and model families, and yields comparable per-formance to state-of-the-art task-specific models. To our1arXiv:2402.15307v1 [cs.CV] 23 Feb 2024Representing Online Handwriting for Recognitionknowledge, we are the first to explore stroke-based repre-sentations in VLMs for handwriting recognition.In our search for a widely applicable representation of digitalink we explore two main options: based on images and time-ordered sequences of points. We look for the optimal way ofrendering ink into an image and of discretizing the sequenceof points into a sequence of text tokens that can be consumedby VLMs. We show how these representations should becombined together to achieve optimal performance.We find that it is possible to obtain good recognition qual-ity while representing digital ink as text. This is unlikesome other modalities like audio (Rubenstein et al., 2023),where adding a new modality into an existing model re-quires extending the token dictionary with the tokens ofthe new modality as well as modifying the model architec-ture. This means that our approach does not require anychanges to existing models and enables adding handwritingrecognition capabilities to pre-trained VLMs by fine-tuningor even parameter-efficient tuning, which further preservesoriginal capabilities of the model. Our findings generalizeacross two model families and several different handwritingrecognition datasets.To sum up, our main contributions are as follows:•We propose a representation for digital ink that com-bines images and time-ordered token sequences, whichis suitable for use with VLMs;•We show that such a dual representation is vital forachieving performance comparable to state-of-the-arttask-specific handwriting recognition models; to ourknowledge this is the first work to explore stroke-basedrepresentations for online handwriting recognition inVLMs;•We show that our representation is suitable for fine-tuning or parameter-efficient tuning of a pre-trainedVLM and does not require changing model architectureor vocabulary;•We perform a detailed ablation study to find the bestway of representing digital ink as images and as tokensequences.2. BackgroundThis paper focuses on online handwriting recognition ,meaning the input includes spatial and time information.Related work on the topic is summarized in Section 5. Wedenote a stroke sas a sequence of triplets (x, y, t )where xandyare coordinates on the screen and tis time information(Carbune et al., 2020). We denote an inkI= [s0, . . . , s n]asa sequence of written strokes (aka pen-down strokes). Theinput of our model is an ink Iand the output is the textwritten in the ink.TheVLM architectures that we use in this work are PaLI(Chen et al., 2023c;a) and PaLM-E (Driess et al., 2023),transformer-based models (Vaswani et al., 2017). PaLI isan encoder-decoder model that combines image and textrepresentations in the encoder as shown in Fig. 1. PaLM-Eis a decoder-only model that combines image and textualembeddings in its input. The mai\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n various machine learning applications,such as controllable text generation and image translation. These applications aim at producing newdata with desirable style attributes (e.g., sentiment ,tense , orhaircolor ) while preserving the othercore information (e.g., topic oridentity ) [Li et al., 2019, 2022, Xie et al., 2023, Isola et al., 2017,Zhu et al., 2017]. Consequently, the central challenge in counterfactual generation is to learn theunderlying disentangled representations.To achieve this goal, prior work leverages either paired data that only differ in style components [Raoand Tetreault, 2018, Shang et al., 2019, Xu et al., 2019b, Wang et al., 2019b], or utilizes style labelinginformation [John et al., 2019, He et al., 2020, Dathathri et al., 2020, Yang and Klein, 2021, Liuet al., 2022]. However, collecting paired data or labels can be labour-intensive and even infeasible inmany real-world applications [Chou et al., 2022, Calderon et al., 2022, Xie et al., 2023]. This hasprompted recent work [Kong et al., 2022, Xie et al., 2023] to delve into unsupervised identification oflatent variables by tapping into multiple domains. To attain identifiability guarantees, a prevalent∗Equal Contribution. Work was done when Hanqi Yan was a visiting student at MBZUAI. Correspondenceto: Yulan He (yulan.he@kcl.ac.uk) and Kun Zhang (kunz1@cmu.edu).37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2402.15309v1 [cs.LG] 23 Feb 2024assumption made in these works [Kong et al., 2022, Xie et al., 2023] is that the content and the stylelatent variables are independent of each other. However, this assumption is often violated in practicalapplications. First, the dependence between content and style variables can be highly pronounced.For example, to express a positive sentiment, words such as “ tasty ” and “ flavor ” are typically usedin conjunction with food-related content. In contrast, words like “ thrilling ” are more commonlyused with movie-related content [Li et al., 2019, 2022]. Moreover, the dependence between contentand style often varies across different distributions. For example, a particular cuisine may be highlyfavored locally but not well received internationally. This varying dependence between content andstyle variables poses a significant challenge in obtaining the identifiability guarantee. To the best ofour knowledge, this issue has not been addressed in previous studies.In this paper, we address the identification problem of the latent-variable model that takes into accountthe varying dependence between content and style (see Fig 1). To this end, we adopt a natural notionof influence sparsity inherent to a range of unstructured data, including natural languages, for whichthe influences from the content and the style differ in their scopes. Specifically, the influence ofthe style variable on the text is typically sparser compared to that of the content variable, as it isoften localized to a smaller fraction of the words [Li et al., 2018] and plays a secondary role inword selection. For instance, the tense of a sentence is typically reflected in only its verbs whichare affected by the sentence content information. Our contributions can be summarised as: 1)We show identification guarantees for both the content and the style components, even when theirinterdependence varies. This approach removes the necessity for a large number of domains withspecific variance properties [Kong et al., 2022, Xie et al., 2023]. 2) Guided by our theoretical findings,we design a do MainAdapTive coun Terfactual g Eneration model ( MATTE ). It does not requirepaired data or style annotations but allows style intervention, even across substantially differentdomains. 3) We validate our theoretical discoveries by demonstrating state-of-the-art performanceon the unsupervised style transfer task, which demands representation disentanglement, an integralaspect of counterfactual generation.2 Related workLabel-free Style Transfer on variation autoencoders (V AEs). To perform style transfer, existingmethods that use parallel or non-parallel labelled data often rely on style annotations to refine theattribute-bearing representations, although some argue that disentanglement is not necessary forstyle edit [Sudhakar et al., 2019, Wang et al., 2019a, Dai et al., 2019]. In practice, disentangledmethods typically employ adversarial objectives to ensure the content encoder remains independentof style information or leverage style discriminators to refine the derived style variables [Hu et al.,2017, Shen et al., 2017, Li et al., 2018, Keskar et al., 2019, John et al., 2019, Sudhakar et al., 2019,Dathathri et al., 2020, Hu et al., 2017, Dathathri et al., 2020, Yang and Klein, 2021, Liu et al., 2022].Several studies have tackled this task without style labels. Riley et al. [2020] emphasized the implicitstyle connection between adjacent sentences and used T5 [Raffel et al., 2020] to extract the stylevector for conditional ge\n",
      "----------------------------------------------------------------------------------------------------\n",
      "rmer architectures in advancing Arabic NLP.Keywords Large Language Models ·Natural Language Processing ·Transformers ·Arabic Language ·Deep Learning1 IntroductionA notable theoretical and empirical challenge in natural language processing (NLP) is the underrepresentation ofnon-English languages in developing large language models (LLMs). This issue is particularly acute for languageswith intricate linguistic structures, such as Arabic. The complexity of Arabic, with its rich morphology and diversedialects, demands specialized attention in LLM development, a necessity often overlooked in the predominantly English-centric NLP landscape. This disparity limits the scope of NLP applications and hinders progress in understandingand processing multilingual contexts. While GPT-1 and GPT-2 [ 1,2] have laid the groundwork to demonstrate thecapabilities of generative pre-trained transformers, their primary focus has been English, creating a gap in the ArabicNLP landscape. This gap becomes more evident when considering the success of models like Hulmona [ 3] andARABERT [ 4], which, despite their advances in Arabic NLP, still grapple with the challenges of adapting modelstrained predominantly on English data.Addressing this gap, ArabianGPT, within the broader ArabianLLM initiative, emerges as a targeted response. Unlikeconventional LLMs, which are primarily designed to focus on English and exhibit a propensity to include a significantproportion of English tokens even in their multilingual models, ArabianGPT-Base is explicitly developed for thearXiv:2402.15313v2 [cs.CL] 26 Feb 2024ArabianGPT: An Arabic Language Adaptation of GPT-2Arabic language. This model diverges from the norm by prioritizing Arabic’s linguistic peculiarities in its design andimplementation.The methodology underpinning ArabianGPT-Base involves three critical components: an architectural adaptation,a rigorous training regimen, and a novel tokenization strategy. Firstly, ArabianGPT-Base adapts the proven GPT-2architecture, tailoring it to accommodate Arabic’s syntactic and morphological idiosyncrasies. This adaptation ensuresthat the model’s structure is inherently aligned with the linguistic realities of Arabic, unlike conventional models thatretrofit a predominantly English-centric architecture.Secondly, the training of ArabianGPT-Base is conducted on a carefully curated corpus, specifically chosen to cover awide range of Arabic text, thus enabling the model to learn from and adapt to the language’s complexity and diversity.This aspect of the methodology is particularly critical, as it ensures that the model is exposed to and learns from arepresentative sample of the language rather than being limited to a subset that might bias its learning.Lastly, ArabianGPT-Base introduces AraNizer, an innovative tokenizer developed to address the unique challengesof Arabic text segmentation specifically. This tokenizer represents a significant departure from standard tokenizationmethods, often inadequate for Arabic due to its complex script and morphological richness. AraNizer’s design allowsfor more precise and contextually aware segmentation, a critical factor in enhancing the model’s performance ondownstream NLP tasks.By integrating these methodological innovations, ArabianGPT-Base addresses the gap in Arabic NLP and sets a newstandard for developing language-specific models. This approach marks a significant shift in the field, moving awayfrom the one-size-fits-all strategy of previous LLMs towards a more nuanced and inclusive future for NLP.2 Related WorksTrendsThe evolution of Generative Pre-trained Transformers (GPTs) represents a significant milestone in Natural LanguageProcessing (NLP), marking a departure from traditional models towards more sophisticated, contextually aware systems.Initiated by GPT-1 [ 1], the series showcased the efficacy of Causal Language Modeling (CLM) in understanding andgenerating human-like text. CLM, by predicting subsequent tokens based on previous context, enables deep learningmodels to capture nuanced linguistic patterns, facilitating both comprehension and production of complex languagestructures.Progressing to GPT-3 [ 5], the field witnessed an exponential increase in model size and the diversification of trainingcorpora. GPT-3, with its 175 billion parameters, leveraged an extensive dataset comprising vast swathes of internettext, setting new benchmarks across a multitude of NLP tasks. This leap in scale introduced advancements in few-shotlearning, empowering models to perform tasks with minimal prior examples, a testament to their evolving contextualunderstanding.Simultaneously, the advent of transfer learning emerged as a critical strategy for broadening the applicability of GPTmodels beyond English, addressing the challenges of linguistic diversity and dataset scarcity in languages like Arabic.The complexity of Arabic, with its rich morphological structure and dialectal variations, underscores the need forspecialized ap\n",
      "----------------------------------------------------------------------------------------------------\n",
      "d neural networks with Re LU ac-tivation are continuous piecewise linear (CPWL) functions [2,8], the m inimumnumber of layers required to represent any CPWL function remains an openquestion. Notably, three signiﬁcant contributions have been made regardingthis problem by Arora et al. (2018) [2], Hertrich et al. (2021) [13,14], andHaase et al. (2023) [11].Arora et al. [2] showed that ⌈log2(n+1)⌉hidden layers is suﬃcient to repre-sent any CPWL function f:Rn→R. This ﬁnding relies on a characterization1of CPWL functions presented in [22], which expresses fasf=p/summationdisplayi=1αimax{l(i)1,...,l(i)n+1}, (1)whereαi∈Randl(i)jare aﬃne functions in Rnfor alli,j.Let Υn(m), or Υ(m) for simplicity, denote the set of neural networks withmhidden layers, and let Υ( M) be the set with minimal depth Msuch that anyCPWL function can be represented by a neural network in Υ( M).In Hertrich et al. [13], the authors conjectured that M=⌈log2(n+ 1)⌉.While the conjecture is known to be true in Rnwithn <4 since [19], itsvalidity remains unknown for n≥4. One important contribution of Hertrich etal. [13] is the proof that the functionmax{x1,...,x n,0} ∈Υ(m) if and only if m≥M. (2)In other words, the function max {x1,...,x n,0}can only be obtained from aneural network if and only if that network has at least the minimum nu mber oflayers that would be required to represent any CPWL function. This result haspotential implications for solving the conjecture by analyzing a single function.For instance, it was subsequently used to stablish a proof in Rnforn= 4 underspeciﬁc conditions at the breakpoints of the hidden neurons.Furthermore, Haase et al. [11] utilized (2) to prove the conjectur e for net-works with integral weights. The proof used polytope neural netw orks [14], anew structure built from the connection via tropical geometry [18] of convexpolytopes and ReLU networks [1,25]. This approach included an ana lysis of thesubdivision and volume properties of Minkowski sum and convex hull o f poly-topes [3,26]. Here, Minkowski sum and convex hull are the respect ive geometriccounterpart to addition and maximum of ReLU neural networks.All the previously described results depend on the sumandmaxoperationsfrom equations (1) and (2), or their geometric equivalent on polyto pes. Giventhe signiﬁcance of these operators and their intrinsic connection w ith polytopesin advancing the knowledge of the conjecture, one motivation of th is paper isto analyze their minimal depth representation.Other studies have investigated a suﬃcient depth required to repr esent theseoperations when the depth of the operands is known [2,8]. If f1,f2∈Υ(m),then these operations satisfy f1+f2∈Υ(m) and max {f1,f2} ∈Υ(m+ 1).However, there are currently no known results regarding the sma llestm+andmmaxwheref1+f2∈Υ(m+) and max {f1,f2} ∈Υ(mmax).On the geometric side, while [11] provided insightful results on polyt openetworks, speciﬁcally addressing the conjecture in the integral c ase, a secondobjective of this work is to explore depth representation of polyto pes moregenerally.One contribution of this study is proving that, in many cases, deter miningthe minimum depth required to represent the maxoperation cannot be solelydeduced from the assumption of the minimal depths of the operand s. More2speciﬁcally, for m≤max{m1,m2}, wheremi≤M, there exist CPWL func-tionsfiwith minimal depth in Υ( mi) such that max {f1,f2} ∈Υ(m) andmisminimal.We have a similar situation for the sumoperation when m1=m2. However,whenm1/n⌉}ationslash=m2, we obtain that f1+f2has minimal depth in Υ(max {m1,m2}).Therefore, beyond this case, additional conditions are necessar y to ensure acertain minimal depth for these operations.We present some interesting consequences of these results. For example, weare able to expand (2) to a broader set of functions and also show t he con-struction of sequences of neural networks, all with the same widt h and minimaldepth, converging to CPWL functions with smaller minimal depth.In polytope neural networks, we establish basic depth properties similar tothose in ReLU networks, such as depth inclusions and depth calculat ion fromMinkowski sum and convex hull. Furthermore, we present depth co nnectionsbetween ReLU and polytope networks, which are useful in ﬁnding bo unds ormaking exact calculations of minimal depth.A signiﬁcant result, given the relationship with (2), is the minimal dept hcomputation of simplices. We prove that ⌈log2(n+1)⌉hidden layers are neededfor representing them. This computation serves as a stepping sto ne for furthergeometric developments.2 PreliminariesIn this work, we focus on real-valued feedforward neural networ ks with recti-ﬁed linear unit (ReLU) activation function. A ReLU neural network orReLUnetwork withm∈Nhidden layers is a function f:Rn→Rexpressed asf=T(m+1)◦σ◦T(m)◦σ◦···◦T(2)◦σ◦T(1),whereT(l):Rnl−1→Rnlarem+ 1 aﬃne transformations, n0=nandnm+1= 1. The activation function σcorresponds to the vectorized ReLUfunction max {x,0}.From this d\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ure 1. Top: An example of how vector quantization can betterrepresent 2D normally distributed data compared to uniform quan-tization, non-uniform quantization. Bottom: Comparing GPTVQto state-of-the-art uniform quantization on Llama 70B.However, these advanced models come with high computa-tional costs due to their extensive parameter counts, necessi-tating frequent data transfers during execution. The primarybottleneck in efficient LLM inference lies in weight move-ment, especially since LLMs’ autoregressive nature requiresloading and transferring weights for each generated token.Consequently, the weight movement’s cost often surpassesthe computational expenses.To address the challenge of cost reduction for these resource-intensive models, a critical question arises: How can wecompress LLM weights to the maximum extent possi-ble? Low-bit quantization has proven successful in re-ducing model weights to 4 bits without substantial accu-racy loss (Frantar et al., 2022; Shao et al., 2023; Lin et al.,2023). While much of the prior research has focused onuniform quantization for LLMs, we investigate the potentialto achieve even greater compression by employing non-uniform quantization and expanding the dimensionality ofthe representational grid through vector quantization. In vec-tor quantization (see Figure 1, top right), multiple weightsare quantized together, offering a more versatile quantiza-tion grid across multiple dimensions.We integrate our findings into a novel algorithm for post-1arXiv:2402.15319v1 [cs.LG] 23 Feb 2024GPTVQ: The Blessing of Dimensionality for LLM Quantizationtraining quantization called GPTVQ. This method allowsfast non-uniform and vector quantization (VQ), improvingthe performance-size trade-off significantly compared toprior state-of-the-art.The contributions of this work are as follows:•Our analysis and experimental results show that in-creasing dimensionality of quantization gives improvedaccuracy versus model size trade-offs for many LLMs.•We propose a fast and accurate algorithm for post-training VQ compression. We show that our algorithmachieves SOTA size vs accuracy trade-offs on a widerange of LLMs, while having a practical run time ofonly 3 to 11 hours for a 70B parameter model.•We implemented and benchmarked VQ decompres-sion on a mobile CPU. While VQ leads to significantmemory footprint reductions, our on-device timingsalso demonstrate that it leads to improved latency com-pared to a 4-bit integer baseline.2. MotivationNeural network quantization is commonly used to reducemodel footprint, data transfer and compute requirements.By quantizing a model, high bit-width floating point weightsand activations that are commonly used for training can berepresented by lower-precision values represented by fewerbits. Quantizing to 8 bits or lower significantly reducesfootprint, data transfer and compute bottlenecks, at the costof introducing quantization noise in the model, resulting ina potential drop in accuracy. In this section we provide abrief overview of uniform scalar quantization, non-uniformscalar quantization and introduce vector quantization, eachof which offers progressively more flexibility in quantiza-tion. We will then illustrate how these methods improverepresentational accuracy of (non-uniform) underlying dis-tributions, and can yield improved trade-offs between com-pression and accuracy. Finally, we touch upon the chal-lenges of vector quantization and the limitations of currentapproaches.2.1. Types of quantization grid and their flexibilityUniform quantization A symmetric uniform quantizerapproximates an original floating point vector x∈RDasx≈sxint, where each element in xintis ab-bit integervalue and sis a higher precision quantization scale, sharedacross the components of x.Non-uniform quantization Uniform quantization as pre-sented in the previous section, while efficient, is very in-flexible as the representable points can be solely equidis-tantly spaced. A more flexible quantization approach is non-Figure 2. Quantization SQNR depending on the dimensionalityfor Llama-v2 7B weights. Signal-to-noise ratio increases withquantization dimensionality due to additional flexibility in thequantization grid.uniform quantization using codebook quantization, in whichfloating point numbers are discretized to arbitrary scalarcentroids in a codebook C:C={c1, c2, . . . , c k}. Eachhigh precision value in xis then represented by an index jof a centroid cj. Each index can be stored using ⌈log2k⌉bits. This technique can be used to compress weight ten-sors by choosing ksuch that log2kis less than the originalbitwidth of the elements in x. Note that the codebook itselfincurs overhead, which we will discuss in more detail inSections 2.2 and 3.2.Vector quantization In non-uniform quantization, as in-troduced in the previous paragraph, we assume that eachscalar value in xis quantized individually. However, a moreflexible quantizer can be constructed by choosing a higher-dimensionality fo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "form ofa point cloud and associated posed images, the task is to segmentall object instances described by a given free-form text query.such frameworks enables intelligent agents to perform arbi-trary and complex tasks in novel environments. Driven bythese motivations, the number of open-vocabulary 3D sceneunderstanding approaches has been growing rapidly. Onone hand, there are explicit point-cloud based approachessuch as OpenScene [16], PLA [6], CLIP-FO3D [21] andConceptFusion [10] which aim to create scene represen-tations that allow querying the scene based on arbitrary1arXiv:2402.15321v1 [cs.CV] 23 Feb 2024open-vocabulary descriptions. On the other hand, there areworks such as LERF [11], OpenNeRF [7] and DFF [13]that embed open-vocabulary features on an implicit scenerepresentation. Methods focusing on 3D instance seg-mentation include OpenMask3D [19], OpenIns3D [9] andOpen3DIS [15].There has also been a growing interest in creating a 3Dscene graph representation [2, 18] that allows for open-vocabulary querying such as ConceptGraphs [8]. Theseopen-vocabulary approaches go beyond the pre-defined ca-pabilities of traditional 3D scene understanding methodsoperating in a closed-vocabulary setting.The goal of the workshop is to bundle these efforts to-wards 3D open-vocabulary scene understanding, and to dis-cuss and establish clear task definitions, evaluation metrics,and benchmark datasets.One of the important goals of this challenge is to helpenable the quantitative comparison of open-vocabulary 3Dscene segmentation methods. In fact, due to the lack ofbenchmarks addressing various open-vocabulary concepts,existing 3D open-vocabulary approaches mainly evalu-ate their capabilities qualitatively on individually selectedqueries and scenarios, which differ from work to work. Thekey goal of the workshop is to drive the research commu-nity inspire the research community to reason about the us-ability and generalization of their approaches in real-worldscenarios.In this iteration of the workshop, the challenge focuseson the open-vocabulary setting with arbitrary object de-scriptions, which describe long-tail objects as well as ob-ject properties such as semantics, materials, affordances,and situational context. In summary, the challenge is:1.Task: Given an open-vocabulary, text-based query, theaim is to localize and segment the object instances thatfit best with the given prompt, which can describe objectproperties such as semantics, material type, affordancesand situational context.2.Input: An RGB-D image sequence and the correspond-ing 3D reconstructed geometry of a given scene, cameraparameters (intrinsics, extrinsics), and an input query.3.Output: Instance segmentation masks in the point cloudthat corresponds to the vertices of the provided 3D meshreconstruction, segmenting the objects that fit best withthe provided text prompt.In the following sections, we first introduce the challengetask, dataset and evaluation methodology (Sec. 2). Then,we share results from the top three winning teams, as wellas the respective methods proposed by each team (Sec. 3).2. ChallengeIn this section, we provide an overview of the challenge.More specifically, we describe the task, dataset, challengephases and additional details including the evaluation.No. Example Queries1 cushion with an alpaca print2 sofa right across the TV3 Christmas stockings4 present5 wall art with a green backgroundTable 1. Example queries from our challenge development set.2.1. TaskThe task in this challenge is 3D open-vocabulary instancesegmentation. Given a 3D scene and an open-vocabulary,text-based query, the goal is to segment all object instancesthat correspond to the specified query. If there are multi-ple objects that fit the given prompt, each of these objectsshould be segmented, and labeled as separate instances. Thelist of queries can refer to long-tail objects, or can includedescriptions of object properties such as semantics, materialtype, affordances and situational context. An example set ofqueries is provided in Tab. 1.2.2. DatasetOur challenge data is based on the existing ARKitScenes [3]dataset, which is a large-scale indoor 3D dataset containingposed RGB-D image sequences, 3D mesh reconstructionsfrom an iPad, as well as high-resolution Faro laser scans.For the challenge, we use the RGB-D frames and the 3D re-constructions from the iPad. While some existing datasetsand benchmarks [1, 4] are targeting language grounding in3D scenes, these works primarily focus on identifying ob-ject relations or properties, whereas this challenge focusesspecifically on long-tail object classes.2.3. Challenge PhasesTo discourage overfitting to test data, the challenge consistsof two phases: the development phase and the test phase .Development Phase: In this first phase, the participantscan download and use the whole training split of the ARK-itScenes dataset for their experiments. From these trainingscenes, we annotate 5 example scenes for development pur-p\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nces the efficiency and interpretability of computational models thatincorporate their mathematical structure. The Lie group SE(2) has been of particular interestin image processing. Firstly, it is natural to demand that any operation applied to the image(e.g. denoising, segmentation, feature extraction, etc.) has to be equivariant to a roto-translation of the image function. Simply put, operating on the image after a roto-translationor roto-translating the image after the operation must yield an identical output. Secondly,in many applications, it is desirable to identify one or more planar orientations at each 2Dlocation in the image. This enables the effective processing of important image sub-structureswith directional attributes like lines, edges, crossings, bifurcations, etc. For example, in thedomain of medical image processing there is often a requirement of crossing preserving de-noising of images. Many of these images contain line structures having multiple crossings andbifurcations and we require that the denoising method preserve these structures [45, 16, 22].Methods not accounting for such local orientation information often lack this property leadingto distortionary results. Furthermore, medically significant line structures need to be spatially∗Centre for Analysis, Scientific Computing, and Applications, Eindhoven University of Technology (TU/e).email: {d.l.j.bon, g.pai, g.bellaard, o.mula, r.duits }@tue.nl.†The first two authors contributed equally.1arXiv:2402.15322v1 [cs.CV] 23 Feb 20242 D. BON, G. PAI, G. BELLAARD, O. MULA AND R. DUITSFigure 1.2. Barycenter interpolation in P(SE(2) ):To interpolate classical 2D images in L2(R2)(in thiscase, an image of 3 and an 8), we first lift them to the space of probability measures P(SE(2) )with operationsinvolving invertible orientation scores (see Figure 1.1and equation (2.19) ). We then compute Wassersteinbarycenters on that space (see (2.12) ) using linear weights indicated by t. Finally, we project the output backtoL2(R2)(by integration over θ, cf. (2.20) ). We observe sharper and more meaningful interpolation in thisprocess due to the left-invariant anisotropic metrics in SE(2) that yield a unique contour-concentrating behaviorof the optimal transport. This is in contrast to isotropic transport in R2where one observes significant masssplitting and leakage away from the main contours.tracked leading to accurate measurements, which is useful for many further diagnoses. In suchsituations, the tracking algorithm must correctly identify and then move along the most cost-effective local orientation in order to extract the appropriate segment in that application.Figure 1.1. An orientation score transform (see(2.19) ) disentangles line structures due to lifting ontothe Lie group SE(2). This is a useful property in manyapplications like image de-noising, geodesic tracking,equivariant deep learning, etc. In this paper, we useit for the barycentric interpolation of images using op-timal transport as seen in the schematic in Figure 1.2This becomes especially critical at junctionsthat may yield multiple orientations andtracking on the image domain in R2is typ-ically insufficient to make the correct choice[13, 7, 77].A common feature of orientation-awaremethods that efficiently address these issuesis that they liftthe image data into the ho-mogeneous space of positions and orienta-tions. The lifting is usually done through anorientation score transform as we depict inFigure 1.1 (see also [22, 31, 6, 16, 19]). TheLie group of roto-translations SE(2) := R2⋊SO(2) acts naturally on this space of posi-tions and orientations M2:=R2×S1, whichis also its principal homogeneous space. Infact, by fixing a reference position-orientation in M2the two spaces can be identified withOPTIMAL TRANSPORT ON THE LIE GROUP OF ROTO-TRANSLATIONS 3each other. The geometric algorithm is then applied in this lifted space, and finally, the pro-cessed orientation score is projected back yielding the output. Methods that follow this broadworkflow are equivariant to roto-translations of the input by design. This workflow is alsothe backbone behind many successful roto-translation equivariant deep-learning architectures(called G-CNNs): [72, 8, 81] that are more efficient and need less training data. The com-putational machinery in all these methods indispensably use differential geometric structureslike left-invariant Riemannian and sub-Riemannian geodesics, distances and kernels of the Liegroup SE(2).Despite these impactful applications, there has been little progress on the theory and ap-plications of optimal transport problems in this domain. We report a relevant related work[44] that explores image morphing in the visual cortex using optimal transport. Functions inR2are lifted onto this cortical space using Gabor filters parameterized by position, orientation,and scale. However, in our article, we uniquely exploit the SE(2) group structure (analyticdistance approximations and group convol\n",
      "----------------------------------------------------------------------------------------------------\n",
      "xtend Markov convex game to partial ob-servability to deal with the partially observable problems, named as partially observable Markovconvex game. In this game, we propose partially observable Shapley policy iteration and partiallyobservable Shapley value iteration which endow the capability of tackling partially observable sce-narios for SHAQ, SQDDPG and SMFPPO. In application, we evaluate SQDDPG and SMFPPO onthe real-world problem in energy networks.iiiAcknowledgementsI would like to thank my parents for supporting me in accomplishing my dream on research andassisting my life with financial and mental support.Also, I extremely appreciate my supervisors, Dr. Yunjie Gu, Prof. Tim C. Green, and Dr. Tae-KyunKim. Yunjie is an ambitious researcher, who always encouraged me to deal with fundamental prob-lems. Without his advice, it is impossible for me to insist on continuing my research on Shapleyvalue for multi-agent reinforcement learning. I received very valuable advice from Tim on the appli-cations of multi-agent reinforcement learning to power networks. Tae-Kyun gives me important helpin pointing out some issues which I might have missed and polishing my idea.Additionally, I would like to thank my friends, since many interesting ideas were born from the casualcommunication and entertainment with them. Especially, I would like to thank Wangkun Xu, whoalways talked to me about snippets of thoughts. Although many of them cannot be realized, some ofthem were still helpful to my research and stimulated some new ideas.I especially thank Yuan Zhang, my collaborator and good friend, who gave me lots of help andinsightful comments on my research.Finally, I would like to thank my girl friend, Dr. Li Guo, who always supported me to chase my dreamand provided me with illumination when I was in the dark.iiiivStatement of OriginalityI, Jianhong Wang, confirm that this work is the result of my own endeavor, and all work undertakenby other authors is appropriately referenced.vviCopyright DeclarationThe copyright of this thesis rests with the author. Unless otherwise indicated, its contents are licensedunder a Creative Commons Attribution-Non Commercial 4.0 International Licence (CC BY-NC).Under this licence, you may copy and redistribute the material in any medium or format. You mayalso create and distribute modified versions of the work. This is on the condition that: you credit theauthor and do not use it, or any derivative works, for a commercial purpose.When reusing or sharing this work, ensure you make the licence terms clear to others by naming thelicence and linking to the licence text. Where a work has been adapted, you should indicate that thework has been changed and describe those changes.Please seek permission from the copyright holder for uses of this work that are not included in thislicence or permitted under UK Copyright Law.viiviiiixList of AbbreviationsAI Artificial IntelligenceML Machine LearningMAS Multi-Agent SystemRL Reinforcement LearningMARL Multi-Agent Reinforcement LearningCG Convex GameMCG Markov Convex GamePOMCG Partially Observable Markov Convex GameGRG Global Reward GamePSRO Policy- Space Response OracleARMA AutoRegressive– Moving- Average ModelMDP Markov Decision ProcessPI Policy IterationVI Value IterationTD Temporal DifferenceGPI Generalized Policy IterationDPG Deterministic Policy GradientDNN DeepNeural NetworkDQN DeepQ-NetworkDDPG DeepDeterministic Policy GradientTRPO TrustRegion Policy OptimizationPPO Proximal Policy OptimizationCTDE Centralized Training and Decentralized ExecutionIGM Individual- Global- MaxSV Shapley ValueMSV Markov Shapley ValuexPV PhotoVoltaicsDSO Distributed System OperatorSC Shunt CapacitorSVR StepVoltage RegulatorSTATCOM STAT ic Synchronous COM pensatorOPF Optimal Power FlowADMM Alternating Direction Method of MultipliersSVC Static VarCompensatorSHAQ SHA pley Q-LearningSQDDPG Shapley Q-Value DeepDeterministic Policy GradientSMFPPO Shapley Value Model- FreePolicy GradientMSQ Markov Shapley Q-valueSBOE Shapley- Bellman Optimality EquationSBO Shapley- Bellman OperatorPOMDP Partially Observable Markov Decision ProcessDec-POMDP Decentralized Partially Observable Markov Decision ProcessPOSPI Partially Observable Shapley Policy IterationPOSVI Partially Observable Shapley Value IterationxixiiList of SymbolsThe next list describes the commonly used symbols that will appear in the thesis.∆vijV oltage drop between Node iandjδmi(C)Marginal contribution of Agent iabout coalition Cappeared in permutation m, in cooperativegame theoryγ Discount factor for cumulative rewards(fi)i=1,2,...Tuple of functions indexed by iE[·]Expectation valueI Indicator functiona Arbitrary value of actionAt Action variable at time step t, where tmay be ignoredht Joint history at time step tin POMCGoi,t Observation of Agent iat time step t(tis usually ignored) in POMCGot Joint observation at time step t(tis usually ignored) in POMCGs′Arbitrary value of next states Arbitrary value of stateS′t Next stat\n",
      "----------------------------------------------------------------------------------------------------\n",
      "of Sciences, Beijing,China. Correspondence to: Congying Han <hancy@ucas.ac.cn >.Preliminary workneural networks with discrete dynamics to the continuousdomain by Neural ODE (Chen et al., 2018). Graph DiffusionChamberlain et al. (2021b); Song et al. (2022); Choi et al.(2023) further extended the message-passing mechanism inclassic GNNs under a partial differential equation (PDE)perspective. These works have made significant progressin terms of interpretability, stability, heterogeneous graphs,and beyond.Despite these advances, a fundamental challenge of GNNslies in the phenomenon of oversmoothing (Oono & Suzuki,2020), where repetitions of message passing may causenode representations to become indistinguishable and thuslose their discriminative power. For GNNs with discretedynamics, several works (Xu et al., 2018; Zhao & Akoglu,2020; Chen et al., 2020; Rusch et al., 2023) are proposedto relieve the oversmoothing issues. Additionally, recentworks (Rusch et al., 2022; Thorpe et al., 2022; Wang et al.,2023) have verified the existence of oversmoothing in GNNswith continuous dynamics and most of them address thisissue by introducing additional terms in graph diffusionequations, such as source term (Thorpe et al., 2022), Allen-Cahn term (Wang et al., 2023), and reaction term (Choiet al., 2023). However, such extra terms to graph diffusionare often under specific physical scenarios without a genericand unified overview, resulting in case-specific solutionswith narrow applicability.In this paper, we propose a unified framework using opera-tor semigroup theory to address this limitation. By viewingnode features as solutions to the Cauchy problem associ-ated with linear graph diffusion, we provide an in-depthunderstanding of the dynamics leading to oversmoothing.Building on this foundation, we propose a general and mildergodicity-breaking condition, which accommodates spe-cific solutions from previous research and further offers amore universal rule for designing terms to mitigate over-smoothing in diffusion-based GNNs.Moreover, we supplement our theoretical contributions witha probabilistic interpretation by studying the Markov pro-cess in which the generator is a graph diffusion operator,thus establishing a comprehensive link with existing liter-ature. Furthermore, we construct the killing process forgraph diffusion which provides an intuitive probabilistic1arXiv:2402.15326v1 [cs.LG] 23 Feb 2024Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theoryconnection for the proposed ergodicity-breaking condition.Our experimental results confirm the effectiveness of ourtheoretical results, demonstrating reduced oversmoothing,as evidenced by higher Dirichlet energy, and improved nodeclassification performance.In summary, this paper makes several key contributions tothe study of oversmoothing problem:•We introduce a comprehensive framework based onoperator semigroup theory to analyze the oversmooth-ing issue in diffusion-based GNNs and provide a clear,theoretical pathway for addressing it.•Our work proposes an ergodicity-breaking conditionthat not only addresses the oversmoothing problem butalso encompasses several specific extra terms identifiedin prior works, demonstrating its broad applicability.•We provide a probabilistic interpretation of our method,thereby establishing a connection with previous theo-retical analyses and enriching the overall understandingof diffusion-based GNNs dynamics.•We substantiate our theoretical results through syn-thetic and real-world experiments.1.1. Related WorkDiffusion-based GNNs. Treating GNNs as the discretiza-tion of the continuous dynamical system is a rapidly grow-ing sub-field of graph representation learning (Chamberlainet al., 2021b; Chen et al., 2022; Behmanesh et al., 2023;Wu et al., 2023). Since the message passing (MP) mecha-nism shows an intrinsic link to the diffusion process, severaldiffusion-based GNNs (Eliasof et al., 2021; Fu et al., 2022;Song et al., 2022) are proposed and conducted using ODEsolver (Chen et al., 2018). GRAND (Chamberlain et al.,2021b) parameterizes the underlying graph diffusion processto learn the node representations. BLEND (Chamberlainet al., 2021a) considers graph as the discretization of a man-ifold and jointly conducts continuous feature learning andtopology evolution based on the Beltrami flow.Oversmoothing. Oversmoothing refers to the effect thatnode features of graph neural networks (GNNs) tend to be-come more similar with the increase of the network depth,constraining the model expressive power for many GNNs(Thorpe et al., 2022; Oono & Suzuki, 2020). Many previousGNN models aim at overcoming oversmoothing (Xu et al.,2018; Chen et al., 2020; Zhao & Akoglu, 2020). GRAND++(Thorpe et al., 2022) alleviates the oversmoothing problemby introducing the source term. Several GNNs quantitativelytackle the oversmoothing problem by analyzing Dirichletenergy (Rusch et al., 2022; 2023). Based on the Allen-Cahn pa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "g, Shenzhen. Correspondence to: TianshuYu<yutianshu@cuhk.edu.cn >.Preliminary worket al., 2022; Standley et al., 2020; Fifty et al., 2021; Songet al., 2022) in MTL involves strategically dividing a set oftasks into several groups, where each group encapsulatestasks that share maximal positive transfer while minimizingnegative transfer.Recent studies such as those by Standley et al. (2020); Fiftyet al. (2021) have contributed significantly to this domain.Both works utilize a methodology where specific task affini-ties are collected in a single run of the MTL training, whichis then used to group tasks based on the assumption ofhigh-order approximation on the task relationships. Sub-sequently, these groups are trained separately using MTLmethods. However, these approaches exhibit key limita-tions. Firstly, there is an absence of a theoretical guaranteein their task affinity measures, raising concerns about thereliability and predictability of the task grouping effective-ness. Secondly, they rely on an enumeration-based branchand bound algorithm for solving the task grouping prob-lem. This approach not only sacrifices efficiency in termsof computational resources but also poses challenges inincorporating additional constraints, limiting its practicalapplicability in more complex and realistic scenarios.In this work, we introduce a novel approach to task groupingin MTL that addresses these limitations and offers signifi-cant advancements over existing methodologies. First , wepropose a theoretically grounded method to constructingtransfer gains. Different from TAG (Fifty et al., 2021),which makes restricted assumptions like convexity andsmoothness on loss functions, the proposed transfer gainis derived independent of any conditions. Furthermore, itmaintains computational complexity at the same order asTAG by following the high-order approximation assumptionregarding task relationships, as utilized in prior work (Stan-dley et al., 2020; Fifty et al., 2021). Second , our work in-troduces a generic and flexible mathematical programmingformulation to solve task grouping problems. This formula-tion can readily incorporate a variety of budget constraints– a critical aspect in real-world applications. By doing so,our method ensures the practicality and adaptability of MTLmodels in diverse scenarios, ranging from computationalbudget allocation to resource utilization considerations.Our experimental evaluations across diverse domains, in-cluding computer vision datasets like CelebA, combina-1arXiv:2402.15328v1 [cs.LG] 23 Feb 2024Towards Principled Task Grouping for Multi-Task Learningtorial optimization benchmarks and time series datasets,demonstrate the validity and generality of our proposedtask grouping strategy in three-folds. First, we establishthat our method consistently outperforms a wide range ofbaselines, encompassing single-task learning, multi-tasklearning, and various task grouping methods. This substan-tiates its effectiveness across these three diverse domains.Secondly, we illustrate the flexibility and effectiveness ofour proposed mathematical programming formulation byintroducing various constraints, mirroring real-world sce-narios where resource budgets, such as GPU memory limi-tations and resource utilization, come into play. Our resultsdemonstrate that our method significantly outperforms thebaseline methods, showcasing its adaptability and perfor-mance improvement under such constraints. Finally, weprovide a comprehensive ablation study, highlighting the su-periority of our proposed transfer gain and the efficiency andeffectiveness of our mathematical programming formulationcomparing with TAG (Fifty et al., 2021).In summary, this work makes several key contributions inthe realm of task grouping:•We propose a theoretically principled approach to con-structing transfer gains without relying on restrictiveassumptions;•To solve task grouping problems, we introduce a math-ematically generic and flexible programming formula-tion, capable of seamlessly integrating various budgetconstraints;•Through extensive experiments, we demonstrate the ef-fectiveness of our task grouping strategy across diversedomains. Furthermore, we empirically showcase theflexibility of our mathematical programming approachby addressing realistic constraints.2. Related WorksTask Grouping. Task grouping in multitask learning in-volves organizing tasks into groups based on their inherentrelationships or similarities. The idea is to exploit sharedknowledge within each group to improve overall learningefficiency. Most early works utilized normalization terms topartition the model parameters to align with the groupingof tasks (Kang et al., 2011; Kumar & Daume III, 2012; Leeet al., 2016). Lee et al. (2018) extends the scenario of that inLee et al. (2016) to the deep learning area, aiming to modelthe asymmetric task relationships by an autoencoder. Zamiret al. (2018) presented “Taskonomy”, which disentanglestask relationships and provides a\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ture that hinder conventional emergency relief [3].Leveraging these capabilities requires addressing coordinationlimitations in uniting heterogeneous, decentralized UA V plat-forms across agencies. Integrating blockchain architecture withUA V networks shows promise in addressing these limitationsfor decentralized disaster response [4].Recent studies have proposed integrating blockchain withUA Vs to enable decentralized emergency coordination [5].While promising, existing blockchain-UA V solutions lackcomprehensive security assessments against threats targetingdisaster relief systems [6]. Additional open challenges includeoptimizing consensus protocols, preserving privacy, designingadaptive smart contracts, and evaluating resilience againstfailures and attacks [7]. Timely disaster response also requirescoordinating tasks across affected areas. However, challengeslike damaged infrastructure, security vulnerabilities, central-ized visibility, and inter-agency collaboration issues persistwith conventional ground-based approaches. UA Vs offer moreagile coordination but have limitations like short flight times,reliance on compromised networking, and inability to securelyshare data across platforms [8]. Integrating blockchain withUA Vs for disaster response introduces key research gaps.These include developing optimized consensus protocols forresource-constrained UA Vs, enabling interoperability acrossheterogeneous fleets, evaluating security against threats, anddesigning context-aware smart contracts [9].This research is motivated by addressing the limitationsabove through optimized blockchain architecture and protocolstailored for decentralized coordination of heterogeneous UA Vfleets amidst disasters. Our research makes the following keycontributions:•We introduce a blockchain architecture that enables de-centralized coordination across different UA V fleets. Thissystem is designed with a focus on preserving privacy andaccess control, which is crucial for efficient and secureoperations in disaster response scenarios.•We develop an optimized consensus protocol that syn-ergizes DPoS with PBFT. This protocol is specificallytailored for UA V networks, aiming to achieve lightweightprocessing, high throughput, low latency, and robust faulttolerance.•We analyze how overcoming the limitations of currentUA V network coordination technologies can unlock thefull potential of decentralized, intelligent UA Vs for dis-aster response. This motivates further research and de-velopment in blockchain techniques to enable resilient,collaborative, and autonomous UA V-based operations inemergency scenarios.The structure of the paper is laid out as follows: Section IIdescribes the proposed system architecture and design. SectionIII then introduces our consensus protocol, combining aspectsof multiple popular blockchain consensus mechanisms. Sec-tion IV delves into the simulation setup, results, and analysisof the performance of our proposed approach. Finally, SectionV summarizes the contributions of the paper and suggestspotential avenues for future work.II. S YSTEM ARCHITECTURE AND MODELSWe consider a disaster response scenario with a hetero-geneous fleet of UA Vs deployed for search, connectivity,delivery, and assessment operations. The system architecturearXiv:2402.15331v1 [cs.CR] 23 Feb 2024ConnectivityDrone deliveryAttackerSearch and Rescue OperationsInitial Assessment and Data CollectionData Management and CoordinationPost-Disaster RecoveryEstablishing CommunicationNetworksRSUDamaged BSBSBSA2AUAV Flocking Attacker Aerial-2-GroundDeliver DronesAerial-2-AerialSearch and Rescue Fig. 1: Architecture for Blockchain-Enabled UA V Coordina-tion in Disaster Response.comprises aerial vehicles, ground support infrastructure, anda central command unit for coordination, as shown in Fig. 1.The air segment has 200 UA Vs clustered by mission type: 50provide LTE connectivity, 100 conduct supply delivery, 25 per-form search and rescue tasks, and 25 assess the damage. Intra-cluster flocking algorithms enable anti-collision and coordi-nated mobility within mission groups. Inter-cluster protocolsshare situational awareness and dynamic task allocation acrossgroups. Ground infrastructure includes roadside units (RSUs)that offer connectivity access points to first responders. Thecommand center monitors (C2) overall progress through net-worked links to air and ground assets, directing planning, andadapting swarm activities based on evolving needs. Efficientaerial response necessitates optimizing performance acrossseveral axes, including communications, control algorithms,reliability, security, and latency. Mathematical models guideanalysis in these aspects. Having described the overall systemarchitecture, next we delve deeper into defining the commu-nication model for data exchange between UA V clusters andground infrastructure.A. Communication ModelThe communication model defines the key parameters forreliable data exchange among the UA V nodes. Buildi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "progress. Copyright 2024 by the author(s).neural networks can be speciﬁed in a top-down man-ner, wherein models are described by the constraints theyshould satisfy (e.g. in order to respect the structure of thedata they process). Alternatively, a bottom-up approachdescribes models by their implementation , i.e. the se-quence of tensor operations required to perform their for-ward/backward pass.1.1. Our opinionIt is our opinion that ample effort has already been given toboth the top-down and bottom-up approaches in isolation ,and that there hasn’t been sufﬁciently expressive theory toaddress them both simultaneously .If we want a generalguiding framework for allof deep learning, this needs tochange. To substantiate our opinion, we survey a few ongo-ing efforts on both sides of the divide.One of the most successful examples of the top-downframework is geometric deep learning (Bronstein et al. ,2021 , GDL), which uses a group- and representation-theoretic perspective to describe neural network layers vi asymmetry-preserving constraints. The actual realisation sof such layers are derived by solving equivariance con-straints .GDL proved to be powerful: allowing, e.g., to cast con-volutional layers as an exact solution to linear translationequivariance in grids ( Fukushima et al. ,1983 ;LeCun et al. ,1998 ), and message passing and self-attention as in-stances of permutation equivariant learning over graphs(Gilmer et al. ,2017 ;Vaswani et al. ,2017 ). It also naturallyextends to exotic domains such as spheres (Cohen et al. ,2018 ),meshes (de Haan et al. ,2020b ) and geometricgraphs (Fuchs et al. ,2020 ). While this elegantly coversmany architectures of practical interest, GDL also has in-escapable constraints.Firstly, usability of GDL principles to implement architec-tures directly correlates with how easy it is to resolve equi v-ariance constraints. While PyG (Fey & Lenssen ,2019 ),DGL (Wang ,2019 ) andJraph (Godwin et al. ,2020 ) havehad success for permutation-equivariant models, and e3nn(Geiger & Smidt ,2022 ) forE(3) -equivariant models, it ishard to replicate such success for areas where it is notknown how to resolve equivariance constraints.1Categorical Deep LearningBecause of its focus on groups, GDL is only able torepresent equivariance to symmetries, but not all opera-tions we may wish neural networks to align to are in-vertible ( Worrall & Welling ,2019 ) or fully compositional(de Haan et al. ,2020a ). This is not a small collection ofoperations either; if we’d like to align a model to an arbi-trary algorithm (Xu et al. ,2019 ), it is fairly common forthe target algorithm to irreversibly transform data, for ex -ample when performing any kind of a path-ﬁnding contrac-tion ( Dudzik & Veliˇ ckovi´ c ,2022 ). Generally, in order toreason about alignment to constructs in computer science ,we must go beyond GDL.On the other hand, bottom-up frameworks are most com-monly embodied in automatic differentiation packages,such asTensorFlow (Abadi et al. ,2016 ),PyTorch(Paszke et al. ,2019 ) andJAX (Bradbury et al. ,2018 ).These frameworks have become indispensable in the im-plementation of deep learning models at scale. Such pack-ages often have grounding in functional programming : per-hapsJAX is the most direct example, as it is marketed as“composable function transformations” , but such featurespermeate other deep learning frameworks as well. Treat-ing neural networks as “pure functions” allows for rigor-ous analysis on their computational graph, allowing a de-gree of type- and shape-checking, as well as automatic ten-sor shape inference and fully automated backpropagationpasses.The issues, again, happen closer to the boundary betweenthe two directions—specifying and controlling for con-straint satisfaction is not simple with tensor programming.Inferring general properties ( semantics ) of a program fromits implementation ( syntax ) alone is a substantial challengefor all but the simplest programs, pointing to a need tomodel more abstract properties of computer science thanexisting frameworks can offer directly. The similarity ofthe requirement on both sides leads us to our present posi-tion.1.2. Our positionIt is our position that constructing a guiding frameworkfor all of deep learning, requires robustly bridging the top-down and bottom-up approaches to neural network spec-iﬁcation with a unifying mathematical theory , and thatthe concepts for this bridging should be coming fromcomputer science .Moreover, such a framework must gen-eralise both group theory andfunctional programming —and a natural candidate for achieving this is category the-ory.To defend our position, we will demonstrate a uniﬁedcategorical framework that is expressive enough to red-erive standard GDL concepts (invariance and equivariance) ,specify implementations of complex neural network build-ing blocks (recurrent neural networks), as well as modelother intricate deep learning concepts such as weight shar-ing.1.3. The power of Category Theo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ethods play a pivotal rolein scenarios where direct methods become impractical orcomputationally expensive. This significance becomes espe-cially pronounced when handling large matrices, as iterativetechniques offer superior computational and memory effi-ciency, enhanced numerical stability, and support for paralleland distributed computing [2]–[5]. These attributes distinctlyposition iterative methods as advantageous over direct in-version techniques. Despite their promise, current iterativemethods face difficulties with slow convergence and reducedaccuracy in ill-conditioned scenarios. This limitation restrictstheir potential application in future wireless multi-antenna (i.e.,MIMO) networks, particularly evident in the case of extra-large aperture array (ELAA) (see [6]–[8] for the concept ofELAA). This is because ELAA-MIMO features significantchannel spatial inconsistency on the network side, introducingJinfei Wang, Yi Ma, and Rahim Tafazolli are with the 6GIC, Institute forCommunication Systems, University of Surrey, Guildford, United Kingdom,GU2 7XH, e-mails: (jinfei.wang, y.ma, r.tafazolli)@surrey.ac.uk.This work has been partially presented in ICC’2023, Rome [1].Fig. 1. Block diagram of the proposed symmetric rank- 1regularization foriterative matrix inversion.spatial non-stationarity in the ELAA channel and exacerbatingthe ill-conditioning of the MIMO channel beyond what istypical in conventional MIMO systems [9]–[16]. Notably,these challenges are not unique to iterative matrix inversionmethods. Linear system inverse approaches like the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm and sketch-and-projection, sidestepping matrix inversion, also confront similarissues [17]–[20].Basically, the channel condition can be improved by meansof matrix regularization, pre-conditioning or a combination ofthem. The regularized zero-forcing (RZF) stands as a widelyused regularization method, yet our research reveals its subparperformance in iterative channel inversion (see Section V). Incontrast, pre-conditioning involves multiplying a specializedmatrix with the original channel matrix, often formed from asub-matrix inverse [21]–[24]. This approach enhances matrixconditions, leading to improved iterative inversion in numerousclassical methods like Jacobi [21], Gauss-Seidel (GS) [22], andsymmetric successive over-relaxation (SSOR) [23], [24]. How-ever, simulation results elaborated in Section V unveils theirinability to deliver satisfactory performance in ill-conditionedELAA-MIMO channels.This paper introduces a novel channel matrix regularizationapproach, termed symmetric rank- 1regularization (SR- 1R).As per the block diagram illustrated in Fig. 1, we consider apositive definite matrix A∈CN×N, where Nrepresents thematrix size. Rather than directly computing the inverse matrixA−1, the aim of SR- 1R is to firstly find the inverse of a rank- 1regularized matrix R∈CN×Ndefined as:R≜A−ξbbH, (1)where ξ∈Ris a scaling factor, b∈CN×1is a unitary vector(i.e.,∥b∥= 1),[·]Hand∥·∥ denotes the Hermitian transposeand the Euclidean norm, respectively. Next, Sherman-Morrisontransform (see [25]) is applied on R−1, resulting in:A−1=R−1−ξR−1bbHR−11 +ξbHR−1b. (2)Our research aims to ascertain the existence of (ξ,b)valuesarXiv:2402.15334v1 [cs.IT] 23 Feb 2024IEEE TRANSACTIONS ON SIGNAL PROCESSING (DRAFT) 2that could improve the condition of Rcompared to A. Ifsuch values exist, our goal is to optimize (ξ,b)to maximizethe channel condition.Alongside the novel SR- 1R concept, major contributions ofour research include:1) Feasibility and optimality study of SR- 1R:Our study,through eigenvalue analysis, reveals the presence of an optimal(ξ,b)combination that maximizes the matrix condition of R.Detailed results and justifications are referred to Section III.2) Practical approaches for optimization: While theoret-ically, optimal determination of (ξ,b)involves eigenvaluedecomposition (EVD) or singular value decomposition (SVD)ofA, these methods conflict with the purpose of employingiterative matrix inversion, particularly in wireless MIMO sce-narios where direct matrix inversion is unfeasible.To tackle this challenge, our research reveals that (ξ,b)can be optimally established solely by discerning the largestand smallest eigenvalues of Aand their corresponding eigen-vectors. Subsequently, we employ a power iteration techniqueto compute the largest eigenvalue of Aand a shifted poweriteration method to compute the smallest eigenvalue, concur-rently obtaining their respective eigenvectors. It is shown thatsufficiently accurate estimates of eigenvalues and eigenvectorscan be obtained through just two iterations.3) Simulations and Performance Evaluation: The SR- 1Rapproach has undergone extensive simulations and perfor-mance evaluation across a range of MIMO fading channeltypes, encompassing identical and independently distributed(i.i.d.) symmetric MIMO Rayleigh channels, i.i.d symmetricMIMO Rician channels, line-of-sight (LoS)-dominated ELAA-MI\n",
      "----------------------------------------------------------------------------------------------------\n",
      "annot make a harddecision about which ones are healthy and whichones are not. For this reason, we argue that con-ceptual spaces (Gärdenfors, 2000) should be used,alongside knowledge graphs, in many settings.A conceptual space specifies a set of quality di-mensions, which correspond to primitive semanticfeatures. For instance, in a conceptual space ofmovies, we might have a quality dimensions re-flecting how original a movie is. Entities are rep-resented as vectors, specifying a suitable featurevalue for each quality dimension. While the frame-work of conceptual spaces is more general, we willessentially view quality dimensions as rankings.Conceptual spaces have the potential to play acentral role in various knowledge-intensive appli-cations. In the context of recommendation, forinstance, they could clearly complement the factualknowledge that is captured by typical KGs (e.g.modelling the style of a movie, rather than who di-rected it), making it easier to infer user preferencesfrom previous ratings. They could also be used tomake recommendations more controllable, as in thecase of critiquing-based systems, allowing users tospecify feedback of the form “like this movie, butmore kids-friendly” (Chen and Pu, 2012; Vig et al.,2012). Conceptual spaces furthermore serve as anatural interface between neural and symbolic rep-resentations (Aisbett and Gibbon, 2001), and maythus enable principled explainable AI methods.However, the task of learning conceptual spaceshas proven remarkably challenging. The issue ofreporting bias (Gordon and Durme, 2013), in partic-ular, has been regarded as a fundamental obstacle:the knowledge captured by conceptual spaces isoften so obvious to humans that it is rarely statedin text. For instance, the phrase “green banana” ismore frequent in text than “yellow banana” (Paiket al., 2021), as the colour is typically not speci-fied when yellow bananas are discussed. Paik et al.(2021) found that predictions of Language Models(LMs) about the colour of objects were correlatedwith the distribution of colour terms in text corpora,rather than with human judgements, suggesting thatLMs cannot overcome the challenges posed by re-arXiv:2402.15337v1 [cs.CL] 23 Feb 2024porting bias. However, Liu et al. (2022a) found thatlarger LMs can perform much better on this task.Going beyond colour, Chatterjee et al. (2023) eval-uated the ability of LLMs to predict taste-relatedfeatures, such as sweetness and saltiness, obtainingmixed results: the rankings predicted by LLMs, ina zero-shot setting, had a reasonable correlationwith human judgments but they were not consis-tently better than those produced by a fine-tunedBERT (Devlin et al., 2019) model.In this paper, we analyse whether LLMs can befine-tuned to extract better conceptual space rep-resentations. The difficulty is that ground truthrankings are typically not available when it comesto perceptual and subjective features, outside a fewnotable exceptions such as the aforementioned tastedataset. We therefore explore whether more readilyavailable features can be used for fine-tuning themodel. For instance, we can obtain ground truthrankings from Wikidata entities with numerical at-tributes (e.g. the length of rivers, the birth date ofpeople, or the population of cities) and then usethese rankings to fine-tune an LLM. We further-more compare two different strategies for rankingentities with an LLM: the pointwise approach usesan LLM to assign a score to each entity, given somefeature, while the pairwise approach uses an LLMto decide which among two given entities has thefeature to the greatest extent. Our contributionsand findings can be summarised as follows:•We evaluate on three datasets which have notpreviously been used for studying languagemodels: a dataset of rocks, a dataset of moviesand books, and a dataset about Wikidata enti-ties. We use these datasets alongside datasetsabout taste (Chatterjee et al., 2023) and physi-cal properties Li et al. (2023).•We analyse whether fine-tuning LLMs on fea-tures from one domain (e.g. taste) can improvetheir ability to rank entities in different do-mains (e.g. rocks). We find this indeed largelyto be the case, as long as the training data alsocontains perceptual or subjective features.•We compare pointwise and pairwise ap-proaches for ranking entities with LLMs. De-spite the fact that pairwise approaches haveconsistently been found superior for LLM-based document ranking (Nogueira et al.,2019; Gienapp et al., 2022; Qin et al., 2023),when it comes to ranking entities, we find thepointwise approach to be highly effective.•To obtain rankings from pairwise judgments,we need a suitable strategy for aggregatingthese judgments. We show the effectivenessof an SVM based strategy for this purpose.While this strategy is known to have desirabletheoretical properties, it has not previouslybeen considered in the context of languagemodels, to the best of our knowledge2 Related WorkLMs as Knowledge Bases Our focus in this pa-per is o\n",
      "----------------------------------------------------------------------------------------------------\n",
      "at the center of the system.INDEX TERMS Avatars, digital humans, emotions, human factors, industrial applications, industrialmetaverse, industry 5.0, metaverse, psychophysiological states, simulations.I. INTRODUCTIONIndustry 4.0 combines technologies such as Industrial Cyber-Physical Systems (CPS), and the Industrial Internet ofThings (IIoT) to improve the efficiency and productivity ofthe manufacture industry by combining physical and digitalworlds [1]. Employing this context as foundation, Industry5.0 complements Industry 4.0 by enhancing the focus ondeveloping more human-centric and sustainable solutions[2].Among a broad spectrum of technologies that haveexperienced a huge advance during last twenty years,simulations play a crucial role in the development ofapplications in Industry 5.0. Simulations allow thedevelopment of digital twins, which are digital replicas ofphysical systems. By utilizing digital twins, it is possible toapply real-time monitoring, run better analysis, and optimizeprocesses, being able to enhance sustainability of theprocesses [3]. At the same time, simulations enable thepossibility of tailoring processes to the human needs andprovide safe spaces for workers to upskill and reskill theirabilities [4].The physical-digital dyad that enables the integration of bothrealms finds its conception in the Metaverse. Introduced byNeal Stephenson in his book Snow Crash, the Metaverserepresents a virtual interconnected space where physical anddigital entities merge interacting between each other andexchanging information [5]. Even though the creation of theMetaverse still in its early stages, companies such as NokiaorMicrosoft , have already seen three potential subdivisions[6], [7]. A Consumer Metaverse (CM) focused on sociallyimmersive experiences based on entertainment and leisure.An Enterprise (or Commercial) Metaverse (EC) whichintends to apply immersive technologies into the businessenvironment to design products, improve communicationsand collaboration. And finally, the Industrial Metaverse (IM)which aligns its goals with Industry 5.0 aiming to blendphysical and digital realities and make humans and artificialVOLUME XX, 2017 9intelligence (AI) collaborate to improve productivity, safety,and sustainability.While physical systems such as machines or robots are beingintegrated into IM applications, it is also essential tointroduce human workers in simulations. Although severalindustrial applications contemplate human workers in theirsimulations, they mainly considering their physicalappearance and range of motion. These simulations rarelyconsider key aspects that shapes human behavior andperformance such as their psychophysiological states [8].This study proposes an approach to contemplate andrepresent psychophysiological states complementing thephoto-realistic representation of a human worker in thedigital world.II. PREVIOUS WORK AND STATE OF THE ARTIntroducing realistic representations of human operators in theMetaverse requires research on the concepts and methods inthe domain. Such groundwork is presented as follows:subsection A focuses on the Metaverse and its future inindustrial applications, subsection B. shows the currentapplications of avatars in the industry, and subsection Cpresents the current state of representing human states indigital applications.A. INDUSTRIAL METAVERSE: THE FUTURE OFINDUSTRIAL APPLICATIONSThe IM rises as a subsection of the Metaverse which focuseson industrial applications such as manufacturing,constructions, or logistics [7], [9]. The IM aims to makehumans and AI entities work together to design, build, operate,and optimize physical systems using digital technologies [9],[10]. To achieve these goals different technologies arecombined, including the internet of things (IoT), big data,digital twins, extended reality (XR), and AI models [11], [12].The development of an IM that facilitates human-AIinteractions allows the AI models to learn from the human, andthe human to learn from the AI models [7]. As far assimulations are concerned, they play a major role in the IM byenabling the integration of data from multiple sources with acommon goal. In the IM, is possible to improve aspects suchas production efficiency or sustainability footprints byadjusting simulation variables of the industrial process [9].Apart from production simulations, other key features of theIM include remote monitoring, real-time data visualization, ortraining and maintenance in simulated environments [13],[14]. The IM is evolving and being applied to several fieldssuch as smart factories or supply chain. As regards to smartfactories, XR interfaces are used to assist human workers tovisualize production processes, provide them guidance, ortraining them in realistic virtual environments [14]. In the caseof supply chain, the IM enhances its optimization throughdigital twins, real-time tracking, and predictive analytics [13].One of the emerging platforms that allows the integration oftechnolog\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Zhang et al.,2023).In the last few years, we have witnessed the emergenceFigure 1. NuNER creation procedure. RoBERTa is further pre-trained on a subset of C4 automatically annotated by GPT-3.5. Theresulting model can be fine-tuned on various downstream NERproblems.of generative large language models (LLMs) such as GPT-3 (Brown et al., 2020) and, more recently, GPT-4 (Ope-nAI, 2023), which typically have between 100 times and10,000 times more parameters than BERT. These massiveauto-regressive transformer models, trained via a next-wordprediction objective, are language generators which canbe prompted to perform a variety of tasks. For example,these models can be directly used through a well craftedprompt to tackle a particular NER problem with satisfyingperformance (Wang et al., 2023a). The main issue with thisapproach is the high inference cost due to the size of LLMs.A simple solution to this inference-cost issue is to use acorrectly-prompted LLM to annotate data for the particularNER problem, and then to train a smaller model on thisdata. LLMs have been shown to outperform crowdworkers(Gilardi et al., 2023) on some tasks for a fraction of the costso this strategy is sound, but it has issues as well. First,crafting a good prompt — like delegating a task to someoneelse — is not easy; it requires multiple back-and-forth whilevalidating the performance using human-annotated data.Second, LLMs are not perfect annotators either (Mao et al.,2023). Finally, the best LLMs are mainly hosted on private1arXiv:2402.15343v1 [cs.CL] 23 Feb 2024NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Datacompanies’ servers and accessible through external APIs,opening the door to potential confidentiality and privacyleaks.We propose an alternative approach that leverages LLMsto reduce the amount of human annotations needed to cre-ate custom models. Instead of using an LLM to directlyannotate a particular single-domain dataset for a particularNER problem, our idea is to use this LLM to annotate amulti-domain dataset for variety of NER problems. We thenfurther pre-train a small foundation model, such as BERT,on this annotated dataset. The resulting pre-trained modelcan then be fine-tuned to any downstream NER problem,just like any other foundation model, as depicted in Figure 1.Because the resulting pre-trained model is specialized to ageneric task but is still meant to be fine-tuned to a particularproblem, we refer to such a model as a task-specific founda-tion model . Note that compact domain-specific foundationmodels like SciBERT (Beltagy et al., 2019) or BioBERT(Lee et al., 2019) are common, but task-specific foundationmodels of this kind are rare, mostly due to the lack of suit-able datasets. Generative LLMs are the key to building suchmodels.In this paper, we apply the above idea to create NuNER, atask-specific foundation model for the generic task of NER.In Section 3, we describe both the dataset creation andthe training procedures. In a nutshell, we use GPT-3.5 toannotate a subset of C4 (Raffel et al., 2020), resulting in a24.4M words dataset containing 4.38M annotations from200k unique concepts. We then pre-train a base RoBERTaon this dataset via a contrastive-learning approach (Chenet al., 2020) to obtain NuNER.In Section 4, we analyze the transfer learning performanceof NuNER in an extended few-shot regime. We find thatNuNER largely outperforms both its base model and thesame base model further pre-trained on NER-BERT data(Liu et al., 2021), which is the largest and most diverse NERdataset we could find. These results demonstrate the validityof our approach.In Section 5, we investigate the factors influencing NuNER’sabilities. We find that the diversity of the annotations andthe size of the pre-training dataset are the most influentialfactors. Surprisingly, the diversity of the text does not appearto be as influential.In Section 6, for informational purposes, we compare fine-tuning NuNER with using GPT-3.5 and GPT-4 via in-context learning. We find that NuNER beats GPT-3.5 andcompetes with GPT-4 when more than a dozen entities ofeach type is seen during training. We also compare, viafine-tuning, NuNER with UniversalNER (Zhou et al., 2023),a recent LLM specialized in the NER task. We find thatthey exhibit similar transfer learning performance whenfine-tuned, despite NuNER being 56 times smaller.The contributions of our paper are as follows.1.We introduce and demonstrate the validity of a procedurethat consists of annotating raw data with an LLM in orderto train a task-specific foundation model for NER.2.We identify the factors that are likely to improve theperformance of the resulting task-specific foundation model.3.We provide and open-source NuNER1, a compactencoder-based language representation model for NER.NuNER outperforms similar-sized models, competes withLLMs, and can be used as a drop-in replacement forRoBERTa.4.We provide and open-source an LLM-annotated NERdataset1, containing 4.38M annotations\n",
      "----------------------------------------------------------------------------------------------------\n",
      "meters, andcan often guarantee fitting arbitrary functions in the asymptoticlimit, includes mixtures of Gaussians [4], mixtures of Kernelfunctions [5], Parzen windows [6], and others. Here, weexplore the use of Fourier series to model probability densities.Any function has a Fourier series expansion, albeit with apotentially infinite sequence of coefficients. Truncating thissequence restricts the series to smooth functions, which is areasonable implicit bias for our purposes.A different approach based on modeling the cumulativedistribution function (CDF) using a multi-layer perceptron(MLP) is the deep factorized probability (DFP) model [1]. TheMLP is constrained to have strictly non-negative weights, andspecialized activation functions that guarantee monotonicityof the CDF. The model has proven quite popular (e.g., [7]),but there are questions regarding how general and parameter-efficient it is. The range of possible distributions that the DFPcan model is hard to understand given the intricacies of itsnonlinearities. In particular, empirical evidence suggests thatthe model struggles to approximate multi-modal distributionsaccurately.Through a number of experiments, we analyze the propertiesof the proposed Fourier basis density model and how itsperformance can depend on the data distribution. Since neuralcompression models such as Nonlinear Transform Coding(NTC) [2] work by compressing one dimension at a time, ourproposed model is applicable for such tasks.II. M ODEL DEFINITIONTruncated Fourier series (i.e., with all but the first Ncoefficients assumed zero), are a canonical way to representsmooth functions. Our aim is to represent a probability den-sity function p(x)with x∈Ras a Fourier series with afinite number of coefficients, and find these coefficients usingstochastic optimization (for example, by stochastic gradientdescent). In what follows, we first construct a flexible periodicdensity model, and then extend it to the entire real line. Notethatc∗and|c|denote the complex conjugate and magnitude,respectively, of a complex number c.Let us begin with a probability density defined as p(x)≡f(x)/Z, where f(x)is a periodic (with period 2), real-valued, positive smooth function and Z=R1−1f(x)dxisthe normalization constant. We represent f(x)in terms of itscomplex valued truncated Fourier series coefficients cn∈C:f(x) =NXn=−Ncnexp(inπx ), (1)where i≡√−1. Conversely, we can write the coefficients ascn=12Z1−1f(x) exp(−inπx )dx. (2)Note that due to f(x)being real-valued, the coefficients followthe symmetry cn=c∗−nfor all n. Consequently, the negativefrequencies n <0are redundant and need not be consideredmodel parameters. Now, we desire a model that represents aflexible and valid probability density on R, so it needs to be1) non-negative, 2) normalized, and 3) non-periodic with thesupport on the full domain, R. We ensure this as follows.arXiv:2402.15345v1 [cs.LG] 23 Feb 2024A. Non-NegativityTo guarantee non-negativity, we consider Herglotz’s theo-rem [8]. It states that the Fourier series of a non-negativefunction is positive semi-definite. In other words, f(x)is non-negative if and only if cnis a positive semi-definite sequence.A simple way to ensure this is to parameterize cnas anautocorrelation sequence, i.e. for n= 0, . . . , N :cn=N−nXk=0aka∗k+n, (3)where an∈Cis an arbitrary sequence defined for n=0, . . . , N (and assumed zero otherwise). We can thus considerθ≡ {an}N0as the parameters of the model to be fitted andguarantee, by plugging (3) into (1), that f(x)is always non-negative.B. NormalizationTo compute the normalization constant Z, note in (2) thatthe integral over one period of f(x)is2c0. Thus, if we limitthe density to a single period, the normalization constant isavailable directly as Z= 2c0. Using this normalizer, andtogether with non-negativity and the symmetry of cn, we cannow define a valid density model on (−1,1):p(x;θ) =12+NXn=1cnc0exp(inπx ), (4)where cnis as defined in Eq. (3). Note that the cumulativedistribution function (CDF) P(x)also has a simple closed-form expression:P(x;θ) =x2+NXn=1cnπinc 0exp(inπx ) +C(θ), (5)where Cis a function of the parameters that ensures P(−1) =0andP(1) = 1 .C. Support on RLastly, to extend this model to the entire real line, weconsider the mapping g: (−1,1)→R, which is parameterizedby a scaling sand an offset t:g(x;s, t) =s·tanh−1(x) +t=s2ln\u00121 +x1−x\u0013+t. (6)The CDF Qof the mapped variable can be written directly asQ(x;θ, s, t ) =P\u0000g−1(x;s, t);θ\u0001(7)withg−1(x;s, t) = tanh\u0012x−ts\u0013, (8)whereas in the density q, the derivative of gneeds to be takeninto account:q(x;θ, s, t ) =p\u0000g−1(x;s, t);θ\u0001(g−1)′(x;s, t)=p\u0012tanh\u0012x−ts\u0013;θ\u00131ssech2\u0012x−ts\u0013.(9)1.0 0.5 0.0 0.5 1.0x0.00.20.40.60.81.01.2 Learned DistributionT arget Distribution(a) density fit, N= 640 20 40 60 80 100 120Number of Parameters105104103102101Log KL Divergence (b) KLD vs. # parametersFig. 1: Model fit for mixture of beta distribution. a) Densityplot for a 64term Fourier basis density model (best viewedon screen). b) T\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aking problems, we iteratively select parameters in order to optimizea given performance criterion. However, real-world applications such as robotics (Berkenkampet al., 2016a), mechanical systems (Schillinger et al., 2017) or medicine (Sui et al., 2015) areoften subject to additional safety constraints that we cannot violate during the explorationprocess (Dulac-Arnold et al., 2019). Since it is a priori unknown which parameters leadto constraint violations, we need to actively and carefully learn about the constraintswithout violating them. That is, in order to find the safe parameters that maximize the given©2023 Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix Berkenkamp, and Jan Peters..License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .arXiv:2402.15347v1 [cs.LG] 23 Feb 2024Bottero, Luis, Vinogradska, Berkenkamp, and Petersperformance criterion, we also need to learn about the safety of parameters by only evaluatingparameters that are known to be safe. In this context, the well known exploration-exploitationdilemma becomes even more challenging, since the exploration component of any algorithmmust also promote the exploration of those areas that provide information about the safety ofcurrently uncertain parameters, even though they might be unlikely to contain the optimum.Existing methods by Schreiter et al. (2015); Sui et al. (2015); Berkenkamp et al. (2016a)tackle this safe exploration problem by placing a Gaussian process (GP) prior over boththe constraint and only evaluate parameters that do not violate the constraint with highprobability. To learn about the safety of parameters, they evaluate the parameter withthe largest posterior standard deviation. This process is made more efficient by SafeOpt ,which restricts its safe set expansion exploration component to parameters that are closeto the boundary of the current set of safe parameters (Sui et al., 2015) at the cost of anadditional tuning hyperparameter (Lipschitz constant). However, uncertainty about theconstraint is only a proxy objective that only indirectly learns about the safety of parameters.Consequently, data-efficiency could be improved with an exploration criterion that directlymaximizes the information gained about the safety of parameters.Our contribution In this paper, we propose Information-Theoretic Safe Exploration(ISE), a safe exploration criterion that directlyexploits the information gain about the safetyof parameters in order to expand the region of the parameter space that we can classifyas safe with high confidence. We pair this safe exploration criterion with the well knownMax-Value Entropy Search ( MES) acquisition function that aims to find the optimum byreducing the entropy of the distribution of its value. The resulting algorithm, which we callInformation-Theoretic Safe Exploration and Optimization ( ISE-BO), directly optimizes forinformation gain, both about the safety of parameters and about the value of the optimum.That way, it is more data-efficient than existing approaches without manually restrictingevaluated parameters to be in specifically designed areas (like the boundary of the safe set).This property is particularly evident in scenarios where the posterior variance alone is notenough to identify good evaluation candidates, as in the case of heteroskedastic observationnoise. The proposed selection criterion also means that we do not require additional modelingassumptions beyond the GP posterior and that ISE-BO is directly applicable to continuousdomains. Bottero et al. (2022) present a partial theoretical analysis of the safe explorationISEalgorithm, proving that it possesses some desired exploration properties. In this paper,we extend their theoretical analysis and show that it also satisfies some natural notion ofconvergence, in the sense that it leads to classifying as safe the largest region of the domainthat we can hope to learn about in a safe manner. Subsequently, we use this result to showthatISE-BO learns about the safe optimum to arbitrary precision.1.1 Related workInformation-based selection criteria with Gaussian processes models are successfully used inthe context of unconstrained Bayesian optimization (BO, Shahriari et al. (2016); Bubeckand Cesa-Bianchi (2012)), where the goal is to find the parameters that maximize an aprioriunknown function. Hennig and Schuler (2012); Henrández-Lobato et al. (2014); Wangand Jegelka (2017) select parameters that provide the most information about the optimalparameters, while Fröhlich et al. (2020) consider the information under noisy parameters.2Information-Theoretic Safe BOHvarfner et al. (2023), on the other hand, consider the entropy over the joint optimalprobability density over both input and output space. We draw inspiration from thesemethods and define an information-based criterion w.r.t. the safety of parameters to guidesafe exploration.In the presence of constraints that the final solution needs to satisfy, but which w\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-negative) edgeweights.1 IntroductionWe study a variant of the well-known Vertex-Disjoint Paths problem, where the input comprisesa (directed or undirected) graph Gandkterminal pairs as input. The task is to identify whetherpairwise vertex-disjoint paths can connect all terminals. Vertex-Disjoint Paths has long beenestablished as NP-complete [ 16] and has played a pivotal role in the graph-minor project by Robert sonand Seymour [ 23].Eilam-Tzoreﬀ [ 12] introduced a variant of Vertex-Disjoint Paths where all paths in the solu-tion must be shortest paths between the respective terminals. The parameterized comp lexity of thisvariant, known as Vertex-Disjoint Shortest Paths , was recently resolved [ 2,19]: The problem,parameterized by k, is W[1]-hard and in XP for undirected graphs. On directed graphs, t he problemis NP-hard already for k= 2 if zero-weight edges are allowed. The problem is solvable in polynomia ltime for k= 2 for strictly positive edge weights [ 3]. It is NP-hard when kis part of the input, and thecomplexity for constant k >2 remains open.An optimizationvariantof Vertex-Disjoint Shortest Paths , wherenotnecessarilyallterminalpairs need to be connected, but at least pof them, is referred to as Maximum Vertex-DisjointShortest Paths .1Maximum Vertex-Disjoint Shortest PathsInput: A graph G= (V,E), an edge-length function w:E→Q≥0, terminalpairs (s1,t1),(s2,t2),...,(sk,tk) wheresi/\\⌉}atio\\slash=tifori∈[k], and integers pandℓ.Question: Is there a set S⊆[k] with|S| ≥psuch that there is a collection C={Pi}i∈Sofpairwisevertex-disjointpathssatisfyingthefollowingconditions: f oreachi∈S,pathPiis a shortest path from sitotiand the total amount of edges in allpaths of Cis at most ℓ?A few remarksare in order. In the literature concerning Vertex-Disjoint Paths and its variants,it is common for paths in a solution to share a terminal. However, in our context, terminal pairs mayindeed share a terminal, but the paths comprising a feasible solution m ust be vertex-disjoint, and thisconstraint also applies to their endpoints.Note that Vertex-Disjoint Shortest Paths is a special case of Maximum Vertex-DisjointShortest Paths withp=kandℓ=n. For the maximization version, we are not given pas inputbut are instead asked to ﬁnd a set Sthat is as large as possible. Slightly abusing notation, we do notdistinguish between these two variants and refer to both as Maximum Vertex-Disjoint ShortestPaths.In the deﬁnition of Maximum Vertex-Disjoint Shortest Paths , we also incorporatethe upperboundℓon the number of edges in a solution. This parameter ℓproves to be very useful for approx-imation and parameterized algorithms. While parameterization by kyields strong hardness bounds(both in parameterized complexity and approximation), another na tural parameterization would bethe sum of path lengths. If we conﬁne all edge weights to be positive integers, then ℓserves as a lowerbound for the sum of path lengths. Since our hardness results app ly to unweighted graphs, studying ℓinstead of the sum of path lengths does not weaken the negative re sults.For the parameterized complexity of Maximum Vertex-Disjoint Shortest Paths , we notethat the results for Vertex-Disjoint Shortest Paths [2,19] for the parameterization by kdirectlytranslate for Maximum Vertex-Disjoint Shortest Paths parameterized by p. The problem isW[1]-hardasa generalizationof Vertex-Disjoint Shortest Paths , andto obtain an XP algorithm,it is suﬃcient to observe that in nO(p)time we can guess a set S⊆[k] of size pand apply the XPalgorithm for Vertex-Disjoint Shortest Paths for the selected set of terminal pairs.In terms of approximations, we are not aware of any studies of Maximum Vertex-DisjointShortest Paths . ForMaximum Vertex-Disjoint Paths , where the task is to connect the maxi-mum number of terminal pair by disjoint but not necessarily shortes t paths, there is a known O(√n)-approximation [ 18] and the best known lower bounds are 2Ω(√log(n))andnΩ(1/(log log n)2), where theformer lower bound holds even if the input graph is an unweighted plan ar graph and the latter lowerbound holds even if the input graph is an unweighted grid graph [ 8,9]. The best known approximationalgorithms for these two special cases are ˜O(n9/19) and˜O(n1/4), respectively.When requiring the solution paths to be edge-disjoint rather than v ertex-disjoint, there have beensome studies on relaxing the notion so that each edge appears in at m ostc >1 of the solution paths.The integer cis called the congestion and the currently best known approximation algorithm achievesa poly(log n)-approximation with c= 2 [7].Our results. We showthatcomputing a n1/2−ε-approximationsisNP-hardforany ε >0(Theorem 2).For FPT-approximation, we show in Theorem 1that any ko(1)-approximation in f(k)·poly(n) timeimplies FPT = W[1] and that one cannot o(k)-approximate Maximum Vertex-Disjoint ShortestPathsinf(k)·poly(n) time unless the gap-ETH fails. We complement the ﬁrst lower bound b ydeveloping a ⌈√ℓ⌉-approximation in T\n",
      "----------------------------------------------------------------------------------------------------\n",
      "out relevant AIincidents and allows users to explore and edit LLM-generated usecases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user studywith 42 AI prototypers. After using Farsight , AI prototypers in ouruser study are better able to independently identify potential harmsassociated with a prompt and find our tool more useful and usablethan existing resources. Their qualitative feedback also highlightsthatFarsight encourages them to focus on end-users and thinkbeyond immediate harms. We discuss these findings and reflect on1arXiv:2402.15350v1 [cs.HC] 23 Feb 2024CHI ’24, May 11–16, 2024, Honolulu, HI, USA Zijie J. Wang, et al.their implications for designing AI prototyping experiences thatmeaningfully engage with AI harms. Farsight is publicly accessibleat:https://pair-code.github.io/farsight .CCS CONCEPTS•Human-centered computing →Interactive systems andtools ;•Computing methodologies →Machine learning .KEYWORDSResponsible AI, Human-AI Collaboration, Large Language ModelsACM Reference Format:Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, and MichaelMadaio. 2024. Farsight : Fostering Responsible AI Awareness During AIApplication Prototyping. In Proceedings of the CHI Conference on HumanFactors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA.ACM, New York, NY, USA, 40 pages. https://doi.org/10.1145/3613904.36423351 INTRODUCTIONAs artificial intelligence (AI) becomes increasingly integrated intoour everyday lives, mitigating the societal harms posed by AI tech-nologies has never been more important. In response to the demandfor accountable and safe AI, there have been growing efforts fromboth industry and academia towards responsible design and devel-opment of AI [ 143,183]. The majority of these endeavors focus onmachine learning (ML) experts, such as ML developers and other AIpractitioners. For example, researchers have introduced techniquesthat help ML developers interpret ML models [ 102,128,150] and as-sess model fairness [ 30,90,189]. Additionally, researchers have alsoproposed frameworks that target ML developers’ workflows, suchas improving data collection and annotation practices [ 14,118,123],documenting training data and models [ 43,63,122], and anticipat-ing an ML product’s potentials for harms [46, 120].However, more recently, we have witnessed a rapid advance-ment of large language models (LLMs) such as Gemini [ 178] andGPT-4 [ 132], alongside the emergence of prompt-based interfaceslike Google AI Studio [ 70], GPT Playground [ 133], AI Chains [ 204],and Wordflow [ 184] (Fig. 2B). These general-purpose models andeasy-to-use interfaces have significantly increased access to the pro-cess of prototyping and building diverse AI-powered applications—leading to a paradigm shift in AI development workflows that posesunique challenges to responsible AI, including introducing new po-tential harms to avoid [ 190], as well as challenges applying existingresponsible AI practices [98].Many people who use prompts to create AI applications nowencompass a broader spectrum of roles beyond traditional ML ex-perts (Fig. 2A), such as designers, writers, lawyers, and everydayusers [ 55,84,193,207], whereas existing responsible AI researchoften targets ML experts such as ML engineers and data scien-tists [ 78,198]. Many users of AI prompt-based prototyping inter-faces [e.g., 70,133,184,204], or “AI prototypers” [cf. 84] do nothave experience in AI or computer science, which can lead tochallenges in anticipating the consequences of their AI applica-tions [ 143]—a difficult task even for computer science faculty andAI researchers [ 20,45]. Furthermore, LLMs demonstrate a widerange of capabilities that are continually being discovered acrossFig. 2: (A) Many AI prototypers from diverse backgrounds androles use (B) prompting tools to prototype AI applications.Farsight provides a range of in situ widgets for these tools,helping AI prototypers envision the potential harms of theirAI applications during an early prototyping stage.various contexts, including tasks such as summarization, classifica-tion, and translation [ 18,174]. This characteristic of LLMs gives risetocomplex anduncertain impacts of LLM-powered applications [ 61],presenting a significant departure from the classical ML modelstargeted by existing responsible AI endeavors [ 98,190] and intro-ducing a new layer of complexity for responsible AI researchers tohelp AI developers anticipate downstream consequences.To help address these challenges in applying responsible AI prac-tices to LLM-powered AI applications, we present Farsight (Fig. 1,Fig. 2B), an interactive tool to help AI prototypers identify potentialharms of their LLM-powered applications—a key early step in harmprevention and mitigation [ 104,120,131,176,177]—during the pro-totyping stage. Using Farsight as a probe, we conduct multiplemixed-method user studies to investigate (1) how an early-s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "uding data prepa-\u0000: Corresponding Authors.Figure 1. AutoMMLab automatically creates deployable computervision models from user’s language instructions.ration, model algorithm selection, model training with hy-perparameter tuning, model evaluation and model deploy-ment. The rapid growth of machine learning applicationshas created a demand for off-the-shelf machine learningmethods that can be used easily and without expert knowl-edge. To this end, automated machine learning, or AutoML,has been developed in literature. AutoML aims to minimizethe workload of human experts in the ML development pro-cess, making AI algorithms available and easily accessibleeven for non-AI experts.Recent public computer vision (CV) communities ( e.g.OpenMMLab) have provided a wide variety of toolboxes invarious research areas such as image classification (MM-Pretrain [14]), object detection (MMDetection [7]), seman-tic segmentation (MMSegmentation [12]), and pose estima-tion (MMPose [11]). Considering large language models(LLMs) have exhibited exceptional ability in language un-derstanding, generation, interaction, and reasoning, we areinspired to employ LLMs to connect AutoML with pub-lic CV communities ( e.g. OpenMMLab) for automating thewhole CV model production workflow via a natural lan-guage based interface.1arXiv:2402.15351v1 [cs.LG] 23 Feb 2024In this paper, we propose a LLM-empowered systemcalled AutoMMLab to achieve end-to-end AutoML forcomputer vision model production with OpenMMLab. Un-like traditional AutoML approaches that focus on one par-ticular step in the model production workflow ( e.g. modelarchitecture design), we employ LLMs to extend the reachof AutoML to evolve towards fully automated model pro-duction through a user-friendly language interface. “Hear-ing and obeying”, AutoMMLab connects diverse datasets,CV models, training pipelines and deployment tools to fa-cilitate the solution of numerous CV tasks. This empowersindividuals to leverage the capabilities of CV and AI with-out extensive expertise, driving progress in various fields.As shown in Figure 1, given language-based user re-quirements, our AutoMMLab system will schedule eachmodule to execute the entire workflow and finally outputproduction-ready models for computer vision tasks. Thewhole pipeline consists of 5 major modules, including re-quest understanding module, data selection module, modelselection module, model training and hyperparameter op-timization module, and model deployment module. In therequest understanding module, we propose RU-LLaMA tounderstand the user’s request and parse it into a structuredconfiguration file. Based on the parsed configuration file,we search for the related training data inside the built-indatabase and select the optimal model that meets the con-straints. To enable effective data and model selection, webuild a dataset zoo and a model zoo with correspondingdetailed description (data card and model card). In themodel training module, we propose a novel “black-box” hy-perparameter optimization (HPO) algorithm, termed HPO-LLaMA. Equipped with rich knowledge of model training,HPO-LLaMA can skillfully search for the optimal modelhyperparameters with significantly reduced numbers of tri-als. For model deployment, we invoke MMDeploy [13] todeploy the trained model based on the user’s hardware re-quirements. Empirical study shows that AutoMMLab is avaluable end-to-end AutoML system that can quickly buildtask-specific models to solve a wide range of computer vi-sion tasks with simple language instructions from users.Thanks to its modular design, we also develop theLAMP (Language-instructed Automated Model Produc-tion) benchmark for research on the end-to-end prompt-based model training. It enables the community to explorenew algorithms in the research areas of request understand-ing and hyperparameter tuning, etc.Our main contributions can be summarized as follows:• We propose a LLM-empowered system called AutoMM-Lab to automate the whole CV model production work-flow with OpenMMLab. By integrating AutoML and lan-guage interface, it enables non-expert users to easily buildand deploy CV models, unlocking the potential of AI fora wider audience.• We propose a novel LLM-based hyperparameter opti-mization algorithm, called HPO-LLaMA. To the best ofour knowledge, HPO-LLaMA is the first supervised fine-tuned LLM that focuses on the task of hyperparameteroptimization.• We build RU-LLaMA to understand the user’s requestand schedule the whole pipeline based on our built datasetzoo and model zoo.• Based on the AutoMMLab system, we build a bench-mark platform termed LAMP for evaluating end-to-endprompt-based model training, and also studying eachcomponent in the whole training pipeline.2. Related Works2.1. Automated Machine Learning (AutoML)Automated Machine Learning (AutoML) aims to automatethe model production pipeline, which mainly covers datapreparation [72], feature engineering [47], neural archi-tecture search [5, 39], hyper-pa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "representation. Read noise is caused by various factors, including the electronic components of thesensor, circuitry, and analog-to-digital conversion process. Traditionally, this type of noise is mathematically modeledby an additive white Gaussian noise, justified by the application of the central limit theorem.Therefore, image noise is commonly described by a mixed Poisson-Gaussian model. Formally, representing agrayscale image with npixels by a vector of Rnwhere each entry encodes the pixel intensity, the noise model is:y∼aP(x/a) +N(0n, bIn), (1)where y∈Rnis the observed noisy image, x∈Rnis the noise-free image (true signal), and a, b∈R+∗are theparameters relative to shot and read noise, respectively, depending in particular on the acquisition system and onthe exposure time. A widespread simpler alternative to the mixed Poisson-Gaussian model (1) is the additive whiteGaussian noise (AWGN) model:y∼ N(x, σ2In), (2)where σ2is the signal-independent variance of the noise. The formulation (2) can be seen as an approximation of (1)where the signal-dependent shot noise is neglected. Although it may seem to be a limitation of this model, formulation(1) actually transposes to (2) when using a variance-stabilizing transformation (VST) such as the Anscombe transform[128] and its generalizations [127, 9, 94] that amount to applying per-pixel nonlinearities that effectively reduce thesignal dependence. Ultimately, due to its mathematical convenience, the AWGN model is the most widely-used one.arXiv:2402.15352v1 [cs.CV] 23 Feb 202429 29.2 29.4 29.6 29.8 30 30.2 30.4100101102103104NL-RidgeRestormer LIChINL-BayesBM3DWNNMDRUNetDnCNNS2SRDIPFFDNetLIDIASCUNetSS-GMMTWSCNeural network-basedTraditionalAverage PSNR on Set12 and BSD68 (dB)Execution time (in seconds)Single-imageDataset-basedFigure 1: Execution time on CPU for images of size 512×512v.s the average PSNR results on the union of Set12and BSD68 datasets for Gaussian noise with σ= 25 for popular methods.From the noisy observation y, which follows either (1) or (2) but also any other, possibly unknown, noise distribution,the aim of image denoising is to design a method for estimating the original unknown signal xas faithfully as possible[39]. This amounts to identifying a function f:Rn7→Rnsuch that a noisy observation ycan be mapped to asatisfactory estimate of x,i.e.f(y)≈x. Over the years, a rich variety of strategies, tools and theories have emergedto address the issue of image denoising at the intersection of statistics, signal processing, optimization and functionalanalysis. The performance and the limitations of resulting single-shot methods are generally well understood. But thisfield has been recently immensely influenced by the development of machine learning techniques and artificial intelli-gence. Viewing denoising as a simple regression problem, this task ultimately amounts to learn to match the corruptedimage to its source. The very best methods in image denoising leverage deep neural networks which are trained onlarge external datasets consisting of clean/noisy image pairs (see Fig. 1). However, though fast and efficient, these su-pervised networks suffer from their lack of interpretability and usually have fewer good mathematical properties thantheir conventional counterparts. Therefore, it is of paramount importance to examine several mathematical propertieswhich are desirable in image denoising, especially the so-called normalization-equivariance, which ensures that anychange of the input noisy image, whether by shifting or scaling, results in a corresponding change in the denoisingresponse. While this property is partially fulfilled by single-shot methods, current deep neural networks surprisinglydo not guarantee such a property, which can be detrimental in many situations (source of misinterpretation in criticalapplications).The remainder of the paper is organized as follows. In Section II, we take the reader on a guided tour of supervisedlearning methods for image denoising. In Section III, we review the unsupervised denoising methods and focus onthe most performant methods. In Section IV , we study the normalization equivariance (NE) properties of the reviewedmethods and provide cues so that NE holds by design.2 Review of supervised learning methodsStarting from a general framework based on empirical risk minimization, we present the three main classes of parame-terized functions, also known as neural network architectures in artificial intelligence. For each architecture, we studya popular state-of-the-art representative for image denoising. Next, we address the issue of finding the best functionfor denoising among a given family of parametric functions, more commonly known as parameter training. Finally,we study the special case of weakly supervised learning, which does not require noise-free images for training2.1 Principle of supervised learningThe holy grail in image denoising is to find a universal function fthat, given a noisy observation y∈ Y\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PSlocalization.Additionally,harshenvironments,payload restrictions, and energy constraints limit the kinds and lengths of mis-sions available for scientific data collection. These and other limitations incen-tivize the use of adaptive sampling techniques that allow scientists to collectmore relevant data with the same time and energy budgets.arXiv:2402.15359v1 [cs.RO] 23 Feb 20242 San Soucie et al.Adaptivesamplingrequiresamodelforhowdataaredistributedintheworld.For example, for the scalar field maximum-seeking [24], a Gaussian process (GP)can model the spatial distribution of the scalar field and allow for queryingand prediction. Additionally, in settings with communications constraints, thatmodel must be trained from streaming data entirely on-board the autonomousplatform.Lowcostandlowenergycomputershavebeenincreasinglydeployedonunderwater vehicles. However, development of streaming statistical models fordata from in-situ marine imaging systems has lagged behind the rapid progressmade in imaging and computing hardware. Thus in-situ sensors generally collectdata following brute-force, pre-planned missions.Building on the Gaussian Dirichlet Random Field (GDRF) model [22], a pre-viously introduced probabilistic model for spatiotemporally distributed, sparse,high-dimensional categorical data, here we present a streaming inference ap-proach that we call streaming GDRF (S-GDRF). The S-GDRF model buildson the success of existing topic models in capturing the latent co-occurrencepatterns in sparse, high-dimensional categorical data, while enabling inferenceand querying through the entire world. Our main contribution is a novel sub-sampling approach to doubly-stochastic black-box variational inference, whichenables fully streaming data processing that is bounded in time and linear inspace for a large category of missions.1.2 BackgroundGaussian processes. GPs are a natural choice of belief model for field robotics,especiallywhenperformingadaptivesampling.Theycannaturallyencodeuncer-tainty in complex, dynamic field conditions [6], and prediction allows for onlineBayesian optimization approaches to path planning [7, 20]. High-dimensionalcategorical observations, such as the output of a neural network classifyingcamera images, pose a challenge when applying existing Bayesian optimizationtechniques to adaptive sampling. Many adaptive sampling strategies computeexpected reward rollouts only over scalar observation fields [3], or at best low-dimensional categorical fields [8].Topic models, a type of Bayesian graphical model, represent the distributionof categorical observations by factoring the distributions with latent or unob-served “topics”. The Latent Dirichlet Allocation (LDA) model captures topicswith semantic meaning, organized by co-occurring clusters of words, in collec-tions of text documents [5]. The Real-time Online Spatiotemporal Topic (ROST)modelgeneralizestheLDAmodeltospatiotemporallydistributedcategoricalob-servations, with a fast inference procedure suitable for embedded computation[11, 10]. The ROST model has been used to represent distributions of corals andseafloor types from robotic surveys of coral reefs [13], and topics learned from aROST model have been previously shown to capture meaningful co-occurrencerelationships from phytoplankton observation data [14].The GDRF model, a combination of GPs and topic models into a single uni-fied framework, was originally developed to model spatiotemporally distributedStreaming Gaussian Dirichlet Random Fields 3categoricaldata(e.g.planktonimageclassifications)whileallowingforinterpola-tion (e.g. time series with gaps, or path planning applications) [22]. The originalGDRF used a mixed, offline inference algorithm. Existing streaming variationalinference algorithms are generally limited to specific prior-likelihood combina-tions, such as exponential family models [1, 15]. In the context of streamingMarkov Chain Monte Carlo inference, different approaches to subsampling frompast data have been shown to impact convergence properties of the model [9].Here we use a similar subsampling approach, but replace the Markov ChainMonte Carlo algorithm with a black-box variational inference algorithm to learnan approximate posterior [18].Between GPs, topic models, and their unification in GDRFs, none providesa suitable belief model for adaptive sampling over high-dimensional categoricaldata. This work seeks to fill this gap — the lack of predictive models neces-sary for computing expected reward rollouts in informative path planning tasks— by providing a belief model for spatiotemporally distributed, sparse, high-dimensional categorical observations.2 Methods2.1 Technical ApproachGaussian-Dirichlet Random Fields. At a high level, a GDRF is a smooth, con-tinuous model for categorical data which factors observations into latent co-occurrence patterns. A GDRF models the distribution of categorical data (“ob-servations”)inaspatiotemporalworldbyintroducinglatentcommunities,eachofw\n",
      "----------------------------------------------------------------------------------------------------\n",
      "obtained fromsensors. The most typically used sensors for this task arecameras and LiDAR, which provide two-dimensional imagesand three-dimensional (3D) point clouds, respectively. Theprediction output is a traversability map, which is used toplan the robot’s driving path. As such, this task is crucialin ensuring the safe navigation of the robot. Failure of thetraversability estimation may cause the robot to traverse thewrong path, thus potentially resulting in hazardous situationssuch as derailment, collision, or overturning. In particular,traversability estimation in off-road environments presentsunique challenges that are not encountered in structuredenvironments such as urban or indoor environments. Thesechallenges include scattered obstacles and difficulty in dis-tinguishing between traversable and non-traversable spacesin off-road terrains.This work was supported by Institute of Information & communicationsTechnology Planning & Evaluation (IITP) grant funded by the Koreagovernment (MSIT) (No. RS-2022-00207391, Development of Hashgraph-based Blockchain Enhancement Scheme and Implementation of Testbed forAutonomous Driving)Fig. 1: Overview of the proposed method The proposedmethod consists of two components: a guide filter net-work optimized for feature extraction and fusion in off-roadtraversability estimation, and a footprint supervision modulethat learns traversability in a self-supervised manner fromthe robot’s path, referred to as the footprint.Efforts have been expended continuously to estimatetraversability in off-road environments. The first approachexplicitly defines traversable spaces as human-supervisedlabels and employs rule-based [1][2][3][4] or learning-based [5][6] methods to predict them. The second approachis a self-supervised approach, that combines commonly usedexteroceptive sensors (e.g., camera and LiDAR) with newlyadded proprioceptive sensors (e.g., IMU) [7][8][9][10][11].The first approach is intuitive, but it requires laborioushuman supervision and fails to consider the robot platformin estimation. The second approach relies heavily on theproprioceptive sensor, which is sensitive to changes in therobot platform. Therefore, the traversability estimated by thismethod is applicable to only a few robot platforms and thusnot scalable to other robot platforms.The traversability of off-road terrains is not solely de-termined by obstacles or ground appearance. It is affectedby multiple factors involving the off-road environment androbot platform. This study highlights three primary factorsthat affect a robot’s traversability in an off-road environment:surface slope (geometric cue), semantic information (visualcue), and robot platform. These three factors determine thetraversability through interactions; In cases where the surfaceslope of the terrain sharply increases, the area acts as anarXiv:2402.15363v1 [cs.RO] 23 Feb 2024obstacle and becomes non-traversable. However, if the area’ssemantics can be penetrated, such as reeds, it becomestraversable. Lastly, even semantically non-traversable areaslike rubble may still vary in traversability depending onthe robot platform. For example, all-terrain vehicles cantraverse over rubble, whereas small unmanned ground ve-hicles (UGVs) cannot do the same. Previous studies failedto predict robot-dependent traversability, by considering onlygeometric obstacles [4] or semantic information [6]. Studiesfocused on a robot platform have limitations in compatibilitywith different robot platforms [7][10].We introduce two strategies to consider all three factors intraversability estimation. The first involves designing a neuralnetwork named as a guide filter network (GFN) , whichextracts surface andsemantic information from exteroceptivesensor data and integrates them to produce features optimizedfor traversability estimation. This is achieved by implement-ing an extraction network that extracts surface normals andsemantic information from the input data, along with a fusionnetwork composed of newly proposed guide filter layers,which offer advantages in the fusion of information fromdifferent modalities. The second strategy involves a footprintsupervision module (FSM) , i.e., a self-supervision modulethat utilizes the footprint, which is the path traversed bythe robot in pre-driving and thus incorporates the charac-teristics of the robot platform . This module enables a morescalable traversability estimation by providing a commontraversability score that is compatible with diverse robotplatforms and does not require laborious human supervision.Using these two strategies, we developed a self-supervisedtraversability estimation method applicable to various robotplatforms based on the geometric and visual information ofthe surrounding environment. An overview of the proposedmethod is shown in Fig. 1.We demonstrate the effectiveness of our method throughexperiments using the public off-road dataset RELLIS-3D [12] and ORFD [13]. Our method successfully estimatesth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ources ona computer (float, double). Those limitations are wellknown and studied and have the awareness of modelersand users. However, a detailed reflection of this matteris mandatory, because:(1) Reflecting on this distinction between a model andits implementation offers insight into our view onmathematical models, how we represent them andhow this changes over time.(2) Studying the equivalences (or lack thereof) ofmathematical models and their implementationscan allow us to understand some core reasons forthe lack of reproducibility of computational results(see, e.g., [2–4]).(3) The topic draws attention to recent trends and in-novations in academic publishing, which shape fu-ture research.Code submission for mathematical or computationalmodels has become a frequent requirement of fundingagencies and academic journals alike. However, in theliterature discussing this requirement [5–8] the differ-ences between model and code highlighted here, as wellas the potential disadvantages of resorting solely to im-plemented models, are hardly discussed.A model is formulated on the level of equations (orany mathematical language appropriate for the systemat hand). It is based on explicit and implicit assump-tions and mostly allows for some consistence checks orproofs, for instance a result can be only a positive num-ber and never a negative or complex number. Math-ematical models are abstract formulations derived fromhuman thought processes. They come in the form of rela-tions between numbers, in the form of formulas or a set ofequations. Often, those equations cannot be studied fullyanalytically (i.e., just in its original functional form). Asa consequence, the original description is mathematical,but implementations of such a model are created for thepurpose of running computer simulations or evaluatingthe model beyond what is analytically possible.An implicit assumption of a ’good’ computer imple-mentation is that the results are a consequence of themathematical model and independent of the model’s spe-cific implementation. This also means that any aspectpertaining solely to the computer implementation of themodel is irrelevant for the model behavior and for the re-sults obtained with the model. Examples of such imple-mentation details are the choice of initial conditions, thesequence, in which operations formalized in the model areexecuted, and the numerical recipes employed for solvingor simulating the model.In some fields of science, the strength of mathematicalmodeling is to provide precise quantitative predictions ofsome measurable quantities (see also Fig. 2). In thesecases, the model stems from a theory that claims to rep-resent this aspect of the world with maximal fidelity. Thequality of an implementation based on the mathematicalmodel, which in itself is based on some theoretical model,is then typically quantified by the closeness of the pre-dicted quantity to the measurement quantity. Even ifsuccessful in this sense, the application to another casemay lead to discrepancies, requiring a change in the im-plementation or its underlying model.arXiv:2402.15364v1 [cs.CY] 23 Feb 20242In other cases a qualitative agreement with observa-tions is targeted, as in the case of modeling socioeconomicsystems [9, 10] or other complex systems [11–13]. Thenthe claim of the modeling effort is that the model containsall the mechanistic ingredients to capture a certain (oftencounter-intuitive) phenomenon. In these fields of scienceparticular importance is given to the smallest, most min-imal models capturing an obverved behavior (smallest inthe sense of containing the smallest number of degreesof freedom and/or the smallest number of parameters).Such ’toy models’ or minimal models are a cornerstoneof many applications of, e.g., statistical physics [14–16].With this perspective paper we want to draw atten-tion to the non-trivial and crucial relationship betweena mathematical model and its computer implementationand the subtle and informative ways, in which they canbe different. But we also want to emphasize some po-tential challenges, which come along with the otherwisecommendable trend in academic publishing of requiringcode submissions for mathematical models [4, 17], be-cause we feel that the current debate at times underes-timates the slight distortions of the modeling landscapethat come along with it.II. A DETAILED ASSESSMENTIn this section we illustrate our general point aboutthe impact of code availability for mathematical modelson model diversity and provide more details about thedistinction of models from their implementation.A. Technical issues in the implementationRe-implementations decouple the information flowfrom the flow driven by small errors and implementationdesign decisions not covered by the original mathemat-ical model. As mentioned before, among these are dis-cretization effects, choices of initializations, tie-breakingcriteria, the order of executing logical steps and manymore. Figure 1 is a schematic \n",
      "----------------------------------------------------------------------------------------------------\n",
      "nners, given a high-level mission, can al-locate tasks to robots and then design individual sequences ofhigh-level actions to accomplish the assigned tasks [4]–[8].Execution of these action plans is achieved using low-levelmotion planners and controllers [9]–[11]. A comprehensivesurvey on task and motion planning can be found in [12],[13]. Despite these notable achievements in robot planning,a recurrent limitation in these techniques is the substantialuser expertise often required for mission specification usinge.g., formal languages [14] or reward functions [15].Motivated by the remarkable generalization abilities ofpre-trained Large Language Models (LLMs) across diversetask domains [16]–[18], utilizing LLMs for task planning hasbeen increasingly gaining attention. LLMs facilitate task defi-nition in a user-friendly manner using natural language (NL),J. Wang, G. He and Y . Kantaros are with the Department of Elec-trical and Systems Engineering, Washington University in St Louis.{junw,guocheng,ioannisk }@wustl.eduThis work was supported in part by the NSF award CNS #2231257 andthe ARL grant DCIST CRA W911NF-17-2-0181.Fig. 1: We propose a decentralized planner for language-instructedmulti-robot systems being capable of achieving user-specified mis-sion success rates. The robots are delegated to pre-trained LargeLanguage Models (LLMs), enabling them to select actions whilecoordinating in a conversational manner. Our LLM-based plannerreasons about its inherent uncertainty, using conformal prediction,in a decentralized fashion. This capability allows the planner todetermine when and which robot is uncertain about correctness ofits next action. In cases of high uncertainty, the respective robotsseek assistance.empowering robots to design plans through conversationalinteractions. Early efforts primarily focused on single-robottask planning problems [18]–[34] while recent extensionsto multi-robot systems are presented in [35]–[43]. Thesemulti-robot planners either delegate each robot to an LLMfor decentralized plan construction, use LLMs as central-ized planners, or explore hybrid multi-agent communicationarchitectures. Additionally, mechanisms to detect conflicts,such as collisions in the plans, and provide feedback tothe LLMs for plan revision have been integrated into theseframeworks. A detailed survey can be found in [44], [45].A major challenge with current LLM-based planners isthat they typically lack mission performance and safetyguarantees while they often hallucinate, i.e., they confidentlygenerate incorrect and possibly unsafe outputs.This paper focuses on enhancing the reliability of LLM-based multi-robot planners. We consider teams of robotsarXiv:2402.15368v1 [cs.RO] 23 Feb 2024possessing various skills such as mobility, manipulation,and sensing and tasked with high-level mission expressedusing NL. Mission accomplishment requires the robots toapply their skills to various known semantic objects thatexist in the environment. Our overarching goal is to designlanguage-based planners capable of computing multi-robotplans (defined as sequences of robot actions) to achievedesired mission success rates. This necessitates developingplanners that can reason about their inherent uncertainty,enabling robots to make decisions when sufficiently certainand seek help otherwise.To address this problem, we propose a new decentral-ized LLM-based planner tightly coupled with conformalprediction (CP), a distribution-free uncertainty quantificationtool for black-box models [46]–[48]; see also Fig. 1. Inour framework, at each time step, the robots sequentiallyselect actions while considering actions chosen by otherrobots. Each robot is delegated to a pre-trained LLM agentthat is responsible for decision making. This coordinate-descent approach enables the decentralized construction ofrobot plans. To choose an action for a robot, we present theaction selection problem to the corresponding LLM agent asa sequence of multiple-choice question-answering (MCQA)scenarios [31], [49]. Here, the ‘question’ corresponds to thetextual task description and the history of past decisions,while the set of available ‘choices’ represents the skills thatthe selected robot can apply. The MCQA framework ensuresthat, unlike in other multi-robot planners, the LLM onlychooses from valid choices, mitigating (partially) the risk ofhallucination where invalid plans with nonsensical actionsmay be generated. Instead of simply selecting the actionwith the highest logit score, we employ CP to quantify theuncertainty of the employed LLM. This is crucial as LLMstend to confidently generate incorrect outputs [49]. We showthat CP can be used to design individual prediction sets foreach robot given actions selected by other robots. This allowsthe LLM-based planner to determine when and, for whichrobot, it is uncertain about its predictions. In cases of highuncertainty, indicated by non-singleton prediction sets, therespective robots seek assistance from their r\n",
      "----------------------------------------------------------------------------------------------------\n",
      "itially introduced by Penget al. (2020), they proposed a two-stage pipelinemethod for extracting triplets. However, thispipeline approach breaks the triplet structure’s in-teractionsandgenerallysuffersfromerrorpropaga-tion. Wu et al. (2020b) proposed a novel grid label-ing scheme (GTS), which transforms opinion pairextraction into a unified grid labeling task to solvethepipelineerrorpropagationprobleminanend-to-end manner. Such end-to-end solutions (Wu et al.,2020a;Xuetal.,2020)heavilyrelyonword-to-wordinteractions to predict sentiment relations, ignoringsemantics and syntactic relations between differ-ent spans. Chen et al. (2021b) propose a seman-tic and syntactic enhanced ASTE model ( S3E2),which syntactic dependencies, semantic associa-tions, and positional relationships between wordsare integrated and encoded into a graph neuralnetwork(GNN). Although their work has producedexcellent results, we still believe that the model isfar from realizing the strong potential brought bysyntactic and semantic features for the ASTE task.Previous studies (Li et al., 2021; Chen et al.,The price is reasonable although the service is poor .Positive NegativeAspect Term: {price , service }ASTE: {(price , reasonable , positive ), (service , poor , negative )}Opinion Term: {reasonable , poor }Figure 1: An example of the ASTE task. Aspectterms and opinion terms are highlighted in red andblue, respectively. Positive sentiment polarity is de-noted by the color green, while purple symbolizesnegative sentiment polarity.2021b; Zhang et al., 2022b) typically utilize oneof BERT or LSTM to simultaneously extract syn-tactic and semantic features. However, a singleencoder tends to specialize in either grammaticalrules or semantic relationships, with a preferencefor one over the other. This segregated approachmay result in partial and omitted information, par-ticularly when dealing with complex or ambiguoussentences. So none of these models are able torealize the syntactic potential of syntactic and se-mantic features.To fully exploit the enormous potential of syn-tactic and semantic information, we introduce themodelDualEncoder: Exploiting the potential ofSyntactic and Semantic (D2E2S), designed specif-ically for the ASTE task.Firstly, we selected BERT as the first encoderdue to its superior ability to capture semantic in-arXiv:2402.15370v1 [cs.CL] 23 Feb 2024formation among words. An enhanced LSTM isemployed as the second encoder, which includes acombination of BERT, BiLSTM, and Self-Attention.This combined encoder is able to better capturethe local dependencies and sequence informationbetween words while overcoming the limitationsof LSTM in modeling long-distance dependencies.The enhanced LSTM can effectively capture richsyntactic information. To provide a clearer illustra-tion, we consider the dual encoders as dual chan-nels, namely BERT channels and LSTM channels.Secondly ,weintroducetheHeterogeneousFea-ture Interaction Module (HFIM). In this module, weemploy self-attention double-pooling (SADPool) toadaptively select crucial nodes from various per-spectives. Simultaneously,throughmultipleroundsof information transfer via GCNConv and the se-lective neighbor information aggregation of Gat-edGraphConv, more distant neighbor informationcan be effectively conveyed and consolidated. TheSADPool method is complemented by multi-layerGCNConvandGatedGraphConvtosignificantlyen-hance interactive performance, enabling the modelto better filter and capture advanced syntactic andsemantic information within the input features. GC-NConv and GatedGraphConv correspond to theconvolution calculations of the single-layer graphconvolutional network (Kipf and Welling, 2017) andgatedgraphconvolutionalnetwork(Lietal.,2016).\"Moreover , the syntactic and semantic represen-tations learned from SynGCN and SemGCN mod-ules should show significant differences (Li et al.,2021),weproposeastrategyforseparatingsyntac-tic and semantic similarity to enhance the model’sability to differentiate.In summary , BERT encoders specialize incapturing semantic information between words,whereas enhanced LSTM encoders are more ef-fectiveincapturinglocaldependencies, particularlydependency syntactic features. By employing astrategy that separates syntactic and semantic sim-ilarity, we have successfully obtained more distinc-tive syntactic and semantic information, while elim-inating redundant interference, thereby enhancingthemodel’sabilitytodifferentiatebetweensyntacticand semantic information. The SADPool method inthe HFIM, in combination with multiple GCNConvandGatedGraphConvlayers,moreefficientlyfiltersand captures advanced syntactic and semantic in-formation within the input features. The syntaxparser produces initial dependency syntactic fea-tures, while Multi-Head Attention (MHA) generatesprimary semantic features. These two types of rawsyntacticandsemanticfeaturessynergisticallycom-bine through the mentioned modules to fully exploitthe significant potential of syntactic and\n",
      "----------------------------------------------------------------------------------------------------\n",
      "fer.hr >, Sini ˇsaˇSegvi ´c<sinisa.segvic@fer.hr >.et al., 2018; Bevandi ´c et al., 2022). Recent surge of interestin such methods suggests that they represent an importantmissing piece in the deep learning puzzle (Zhang et al.,2023b; Blum et al., 2021; Chan et al., 2021a).Many practical applications of visual recognition train orfine-tune on pre-defined taxonomies (Lin et al., 2014; Zhouet al., 2019) in the closed-set setup (Geng et al., 2020). Thus,their real-world performance may become unpredictable inpresence of anomalous objects (Zendel et al., 2018). Out-lier detection enables graceful performance degradation byallowing the classifier to decline the decision in unknownvisual concepts (Zhang et al., 2020; Liang et al., 2022). Typ-ically, the outlier detector delivers a scalar anomaly scorethat allows to rank outliers and to evaluate them with respectto some threshold (Ruff et al., 2021).Negative training data represents an important compo-nent of recent anomaly detectors both in the image-wide(Hendrycks et al., 2018; Dhamija et al., 2018) and the pixel-level context (Bevandi ´c et al., 2022; Biase et al., 2021; Chanet al., 2021b; Grcic et al., 2022; Tian et al., 2022). Althoughit cannot represent the entire variety of the visual world,the negative data can still help by signaling that not alldata should be confidently recognized. If the variety of thenegative data exceeds the variety of the inliers, then thereis a reasonable hope that the test outliers will be detected.Unfortunately, this entails unwanted bias towards outliersthat appear similar to the negative data. This concern canbe addressed either by relying on synthetic negatives (Leeet al., 2018; Neal et al., 2018), or through separate rankingof approaches that do not train on real negative data (Zhanget al., 2023b; Blum et al., 2021; Chan et al., 2021a).This paper focuses on outliers that may be missed byprevious discriminative methods (Hendrycks et al., 2018;Dhamija et al., 2018) due to being dissimilar to negativedata. We build upon a key observation that these outlierstend to yield highly uncertain predictions in the K+1 multi-class setup. We will assume that the first K logits correspondto inlier classes while the remaining K+1-th logit representsthe outlers. Interestingly, this non-standard setup did notunderperform with respect to standard K-way performancein any of our experiments. Consequently, we propose anovel anomaly score that ensembles negative objectnesssNO=P(K+ 1|x)with uncertainty over K inlier classes1arXiv:2402.15374v1 [cs.CV] 23 Feb 2024Ensembling uncertainty with negative objectnesssUnc=−maxk≤KP(k|x)(Hendrycks & Gimpel, 2017).Our approach is a remarkably good fit in the pixel-levelcontext as extension of some direct set-prediction approach(Cheng et al., 2022; Yu et al., 2022; Li et al., 2023). Inparticular, we propose to learn negative objectness as themask-level posterior of the K+2-th class, and to formulatepixel-level anomaly scores by ensembling the correspond-ing mask-level cues (Grci ´c et al., 2023). A closer looksuggests that our score performs well due to different induc-tive bias of its two components as indicated by a remarkablyweak correlation. Our concept performs competitively inthe image-wide context as well, even though the absence ofthe no-object class leads to somewhat stronger correlationof the two components. Our models outperform the cur-rent state-of-the art on several standard benchmarks in bothlearning setups, i.e. with and without real negative data.2. Related workOur related work comprises image-wide OOD detection(Sec. 2.1) and per-pixel OOD detection based on mask-levelrecognition (Sec. 2.3).2.1. Image-wide outlier detectionWe can distinguish three main categories among image-wide outlier detection methods: post-hoc inference methods,training methods without negative data, and training meth-ods with negative data. The post-hoc inference methodsbuild inference phase OOD scores on top of closed-set clas-sifiers which are trained using the standard cross-entopy loss.The baseline methods model the outlier score as maximumsoftmax probability (Hendrycks & Gimpel, 2016) or max-imum logit (Hendrycks et al., 2019a) over in-distributionclasses. TempScale (Guo et al., 2017) calibrates softmaxprobabilities with temperature scaling while ODIN (Lianget al., 2017) further introduces input preprocessing. Recentpost-hoc methods simplify layer activations (Djurisic et al.,2022) or base their score on template distances (Zhang et al.,2022).In contrast, training methods involve training-time regu-larization either considering only inlier data or utilizingreal negative data. OE (Hendrycks et al., 2018) is the firstmethod following that setup that introduced outlier expo-sure by requiring low entropy for inlier classes given anoutlier example. MCD (Yu & Aizawa, 2019) considers twoclassification heads with different decision boundaries andthe training procedure promotes discrepancy between eachhead’s prediction on OOD s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "can be learn ing-based, reactive, or acombination of the two. Reinforcement Learning (RL) is poss ibly the most popularlearning-based paradigm. In RL, an agent learns to associat e actions with speciﬁc inputs2or ”states” through repeated interactions with the environ ment, aiming to maximise thecumulative reward (Sun et al., 2021). Rewards represent fee dback for the action taken,making for a closed-loop output control paradigm. RL has bee n very successful inrecent years, when supplemented with techniques such as dee p learning (Mnih et al.,2015; Silver et al., 2016; OpenAI, 2023). RL models are inher ently slow to train fortwo main reasons. Firstly, rewards are sparse. Secondly, in output control, feedbackabout the action can only be obtained after the action has been completed; the actionwill therefore be corrected in the next training episode. Co ntrast this with input control,where feedback on the action is continuously provided by sen sors and motor outputscan be corrected immediately. The outputs of a naive RL model are therefore typicallyassociated with high error and/or low reward (cfr. benchmar ks in Thodoroff et al. 2022).For example, Mila and Pal (2019) developed a Real-Time Actor -Critic paradigm which,despite being able to achieve faster convergence than the cl assical Soft Actor-Criticone, still required hundreds of thousands of training steps , which could take hours ordays to complete (Dalton and Frosio, 2020). As a consequence , the training of RLmodels typically occurs ofﬂine. To this day, fully online RL remains unfeasible, with theconsequence that an automated agent is not able to acquire kn owledge tailored to its ownenvironment in real-time. Instead, large models must be tra ined in order for a system tobe able to generalise to a variety of unseen scenarios. This i s not only computationallyexpensive, but also compromises the transparency of the mod el learned, especially ifvery large and/or trained on cloud platforms (Vasudevan et a l., 2021).Reactive algorithms, on the other hand, simply generate tra jectories for a robot tofollow based on the position of obstacles or targets in the en vironment without requiring3any training. Trajectories are usually calculated based on nodes in a graph, randomlygenerated (see Karur et al., 2021 for an in-depth review), or tailored to sensory inputsusing force ﬁelds (e.g. Vector Field Histogram (Borenstein and Koren, 1991) or Artiﬁ-cial Potential Fields (Khatib, 1985)). From a control theor y perspective, trajectories canbe viewed as a series of desired inputs for a single controlle r, representing the desiredlocations of a robot in a global frame of reference. Loop clos ure in this case consistsof providing feedback on how closely the robot follows its tr ajectory. Trajectory fol-lowing is a form of input control; however, feedback input is usually provided by anexternal observer (e.g. a camera placed on the ceiling), as a robot is oblivious to itsown objective trajectory. Contrast this with a Braitenberg vehicle, which only needsinformation about the location of a disturbance (e.g. obsta cle or target) relative to itselfto successfully perform a control action.Like in RL, motor plans for robots should consist of sequence s of actions, taken inresponse to inputs. However, in even simple animals like the common fruit ﬂy, complexplans in unknown environments can be formulated completely on-the-ﬂy (Honkanen et al.,2019). This challenges the notion that learning through tri al and error is necessary atall for planning. In Spelke and Kinzler (2007) the concept of “core knowledge” is in-troduced. This is intended as a seemingly innate understand ing of basic properties ofone’s environment such as physics and causality, and is back ed by studies on newbornanimals. An agent equipped with core knowledge can form reas onable predictions overthe outcome of an action without needing to perform it, which beneﬁts the efﬁciencyof the decision making process. A contribution in the direct ion of providing a systemwith innate knowledge has been attempted in few-shot RL. In t his paradigm, the goal4is for a system to be able to choose and adapt policies to unsee n scenarios with justa few training examples. This is achieved by using meta-lear ning (Luo et al., 2021;Bing et al., 2023) or transfer-learning (Sun et al., 2023) al gorithms, or both (Shu et al.,2023) which, in brief, use previously trained RL models as a f oundation for a novelmodel to be trained. While able to outperform pure RL in terms of training time, boththe preexisting and the novel models are trained ofﬂine, sug gesting that the responsetime of few-shot learning may still be unsuitable for real-t ime training.Our research addresses the problems of how to achieve multi- step-ahead planningusing closed-loop control, and of how to implement core know ledge in an agent as atool to aid decision-making. To tackle the ﬁrst problem, we d eﬁne various control loops,which we will call “tasks”, each represen\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Eye Movement and Brainwave-based Mechanisms. 1, 1 (February 2024), 28 pages. https://doi.org/XXXXXXX.XXXXXXX1 INTRODUCTIONAuthentication is a cornerstone of security, ensuring that only authorized individuals gain access to sensitive systemsor data. However, traditional methods relying on single knowledge factors such as passwords and PINs have shownsignificant drawbacks. Recent studies, including the 2020 Data Breach Investigations Report by Verizon [ 48], emphasizethat approximately 80% of hacking-related breaches involve weak or stolen credentials, with passwords being a primetarget. Furthermore, the Ponemon Institute revealed that 51% of respondents admitted to using the same password formultiple accounts, consequently increasing the risks of credential theft and identity fraud [ 21]. These statistics shedlight on the vulnerabilities inherent in password-based authentication systems, calling for more robust, usable, andsecure alternatives.Authors’ addresses: Matin Fallahi, KASTEL Security Research Labs, KIT, Karlsruhe, Germany, matin.fallahi@kit.edu; Patricia Arias-Cabarcos, PaderbornUniversity and KASTEL Security Research Labs, Paderborn, Germany, pac@mail.upb.de; Thorsten Strufe, KASTEL Security Research Labs, KIT, Karlsruhe,Arunachal Pradesh, Germany, strufe@kit.edu.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for componentsof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or toredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.©2024 Association for Computing Machinery.Manuscript submitted to ACMManuscript submitted to ACM 1arXiv:2402.15388v1 [cs.CR] 23 Feb 20242 Fallahi and othersOne promising solution to address the limitations of traditional authentication methods is biometric authentication[39]. This approach harnesses the unique physiological or behavioral characteristics of individuals to verify theiridentities. While physiological biometrics like face and fingerprint recognition are popular, they face critical challenges,including the inability to revoke biometric data once compromised and heightened vulnerability to spoofing attacks[5,26]. On the other hand, behavioral biometrics measure unique features of activities users perform either consciouslyor unconsciously [ 6]. Behavioral biometrics have gained significant attention as they offer the potential to enhancesecurity while minimizing user burden [46].Among the behavioral biometric authentication approaches, brainwave-based [ 41,44] and eye movement-based [ 31]mechanisms1have emerged as promising alternatives in desktop and Extended Reality (XR) environments, due to theirdistinct advantages. These mechanisms allow for implicit authentication, without requiring explicit user actions, suchas typing a password or pressing a button, ensuring a seamless and effortless authentication experience. Furthermore,brainwaves are non-observable from the exterior and therefore difficult to compromise, and brain biometrics can beimplemented in a adaptable fashion by altering stimuli even if the original brainwave sample is compromised [ 25,29,49].Eye-based mechanisms do not require a wearable and can work with common camera hardware integrated intolaptops/smartphones [ 30,51]. Lastly, both types of mechanisms have demonstrated promising authentication accuracyin previous research [16, 42], which supports their potential for practical realization in the near future.Despite the potential of brainwave and eye movement-based authentication mechanisms, their actual usabilityremains under-explored, and this is a crucial factor driving user acceptance and influencing security in practice. Thelimited number of prior studies investigating these mechanisms lacked a standardized approach for assessing perceivedusability, such as the System Usability Scale (SUS) [ 8], and failed to provide usage conditions for an ecologically validevaluation [ 2,9,11]. The main barrier in this regard is the absence of real authentication prototypes integrating brainand eye-based authentication mechanisms, which has hindered comprehensive evaluations of their practicality. Tobridge this gap, we aim at answering the following research questions:‚RQ1 [Usability] How usable are brainwave-based and eye movement-based authentication mechanisms asperceived by users?‚RQ2 [Perceptions &Usage] How do users perceive brainwave-based and eye movement-based authenticationmechanisms in terms of security, reliability, and effort? How would they use these mechanisms?‚RQ3 [Benefits, Problems, & Tradeoffs] What are the advantages, disadvantages, and tradeoffs of brainwave-based and eye movement-based a\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dyr@gmail.com>.0% 20% 40% 60% 80% 100%−1−0.500.511.5pythia-160mpythia-160mgpt2-smallgpt2-smallpythia-410mpythia-410mgpt2-mediumgpt2-mediumgpt2-lar gegpt2-lar gepythia-1bpythia-1bllama-7bllama-7b%th Layer in ModelSelf-Repair (as logits)Early -Layer Br eakageSelf-Repairing BehaviorFigure 1. We measure the self-repair of an attention head whenresample ablated on the top 2% of tokens according to its directeffect. For each model, we plot both the self-repair of the individualheads and a trend line that averages across the heads in each layer.Self-Repair exists across many later layers in different models,although the amount varies between heads.the ablation of model components doesn’t always lead toeasily predictable changes in model performance; instead,the removal of the components behavior can be compensatedfor by downstream components in a way which masks theloss of the original component.This is a challenge for interpretability efforts which rely onablation-based metrics to define the importance of modelcomponents. In particular, self-repair mechanisms minimizethe impact of ablating components deemed critical by othermetrics.Past literature has looked at self-repair in incomplete set-tings: self-repair was first discovered in the Indirect Ob-ject Identification distribution as \"Backup Behavior\" (Wanget al., 2023), for which the behavior was explained par-tially as a byproduct of Copy Suppression (McDougall et al.,2023). The self-repair phenomena was then further exploredin McGrath et al. 2023, but only across the Counterfactdataset and with the ablation of entire model layers.We strengthen this prior work by investigating self-repairacross the whole pretraining distribution, by focusing onindividual attention heads (a smaller change, and thus moresurprising to see repaired), and by investigating the mecha-nisms behind self-repair on the whole distribution. Our keyfindings are:1.Direct effect self-repair is an imperfect, noisy process.It occurs across the full pretraining distribution, even1arXiv:2402.15390v1 [cs.LG] 23 Feb 2024Explorations of Self-Repair in Language Modelswhile ablating individual heads (rather than full layers).2.Up to 30% of the self-repairing of direct effects canbe attributed to just the effect of ablations on the Lay-erNorm normalization factor. This is even more pro-nounced when using zero ablations.3.MLP Erasure, a mechanism in which MLP layers’erase’ outputs from earlier model components andis known to explain self-repair (McGrath et al., 2023),is powered by a sparse set of Erasure neurons. Theexact neurons vary between prompts.We end with some discussion on the implications of theseresults for interpretability research and present preliminaryevidence for the Iterative Inference hypothesis, a specu-lative framework that predicts self-repair as a side effect.All of our code for the experiments used in this paper isprovided at https://github.com/starship006/backup_research .2. Self-Repair on the Full Distribution Exists,but is Incomplete and NoisyIn this section, we confirm that across models, individualattention heads are ’self-repaired’ on a general pretrain-ing distribution. We detail how we measure self-repair,demonstrate how individual attention heads are imperfectlyself-repaired across the entire pretraining distribution, andhighlight how it is noisy.2.1. Measuring Self-RepairWe first detail how we define direct effect and self-repair.Our methodology aligns with previous research into self-repair (Wang et al., 2023; McGrath et al., 2023), althoughnot mirroring it identically. Individual model componentscan be thought of as having a direct effect: the effect onthe logits, not mediated by any subsequent component (Mc-Grath et al., 2023). We measure self-repair by comparingthe change in the direct effect ∆DE headof individual atten-tion heads with the logit difference ∆logit of the correctnext token once the attention head is ablated.Across each token, we measure a direct effect DE headofindividual attention heads. Model components add their out-puts into the residual stream. As such, the residual streamcan be decomposed into the output of each model compo-nent (including individual attention heads). Because theresidual stream (mostly) linearly maps into the logits, thereexists an associated direction in the residual stream thatmaps to the correct next token. We can measure how mucheach attention head contributed to it (Elhage et al., 2021):this is the direct effect DE headof an attention head. Ideally,if something has a high direct effect, it was likely ’important’for predicting the next token.We contrast the direct effect with ablation-based metrics: inparticular, we use resample ablations, a common techniquethat replaces the output of the same head on a different set ofpretraining tokens (Chan et al., 2022). We collect the logitof the correct next token on both a standard forward passand on a run where an attention head is resample ablated.The difference between the \"clean\" and\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dels, Video Models, Open-EndednessCorresponding author(s): Ashley Edwards (edwardsashley@google.com), Jack Parker-Holder (jparkerholder@google.com).©2024 Google DeepMind. All rights reservedarXiv:2402.15391v1 [cs.LG] 23 Feb 2024Proprietary + ConfidentialGenie: Generative Interactive Environments1. IntroductionThe last few years have seen an emergence ofgenerative AI , with models capable of generat-ing novel and creative content. Driven by break-throughs in architectures such as transformers(Vaswanietal.,2017),advancesinhardware,anda recent focus on scaling models and datasets, wecan now generate coherent, conversational lan-guage (Brown et al., 2020; Radford et al., 2018,2019), as well as crisp and aesthetically pleas-ing images from a text prompt (Ramesh et al.,2021, 2022; Rombach et al., 2022; Saharia et al.,2022). Early signs indicate video generation willbe yet another frontier, with recent results sug-gesting that such models may also benefit fromscale (Blattmann et al., 2023a; Esser et al., 2023;Ho et al., 2022a; Hong et al., 2023). Still, thereremains a gulf between the level of interactionsand engagement of video generative models andlanguage tools such as ChatGPT, let alone moreimmersive experiences.What if, given a large corpus of videos fromthe Internet, we could not only train models ca-pable of generating novel images or videos, butentire interactive experiences? We propose gen-erative interactive environments , a new paradigmfor generative AI whereby interactive environ-ments can be generated from a single text orimage prompt. Our approach, Genie, is trainedfrom a large dataset of over 200,000 hours ofpublicly available Internet gaming videos and, de-spite training without action or text annotations ,is controllable on a frame-by-frame basis via alearned latent action space (see Table 1 for a com-parison to other approaches). At 11B parameters,Genie exhibits properties typically seen in foun-dation models—it can take an unseen image asa prompt making it possible to create and playentirely imagined virtual worlds (e.g Figure 2).Genie builds on ideas from state-of-the-artvideo generation models (Gupta et al., 2023; Vil-legas et al., 2023), with a core design choice be-ing spatiotemporal (ST) transformers (Xu et al.,2020) which are used in all of our model com-ponents. Genie utilizes a novel video tokenizer,and extracts latent actions via a causal actionmodel. Both the video tokens and latent actionsFigure 2|Diverse trajectories : Genie is a gen-erative model that can be used as an interactiveenvironment. The model can be prompted in var-ious ways, either with a generated image (top) orahand-drawnsketch(bottom). Ateachtimestep,the model takes a user-provided latent action togenerate the next frame, producing trajectorieswith interesting and diverse character actions.are passed to a dynamics model, which autore-gressively predicts the next frame using MaskGIT(Chang et al., 2022). We provide a rigorous scal-ing analysis of our architecture with respect toboth batch and model size, which we vary from40M to 2.7B parameters. The results show thatour architecture scales gracefully with additionalcomputational resources, leading to a final 11Bparameter model. We train Genie on a filtered setof 30,000 hours of Internet gameplay videos fromhundreds of 2D platformer games, producing afoundation world model for this setting.To demonstrate the generality of our approach,we also train a separate model on action-freerobot videos from the RT1 dataset (Brohan et al.,2023), learning a generative environment withconsistent latent actions. Finally, we show thatlatent actions learned from Internet videos can beusedforinferringpoliciesfromunseenaction-freevideos of simulated reinforcement learning (RL)environments, indicatingthatGeniemayholdthekey to unlocking unlimited data for training thenext generation of generalist agents (Bauer et al.,2Proprietary + ConfidentialGenie: Generative Interactive Environments2023; Clune, 2019; Open Ended Learning Teamet al., 2021; Reed et al., 2022).Table1|Anewclassofgenerativemodel : Genieis a novel video and world model that is control-lable on a frame-by-frame basis, which requiresonly video data at train time.Model Class Training Data ControllabilityWorld Models Video + Actions Frame-levelVideo Models Video + Text Video-levelGenie Video Frame-level2. MethodologyGenie is a generative interactive environmenttrained from video-only data. In this section webegin with preliminaries before explaining themain components of our model.Several components in the Genie architectureare based on the Vision Transformer (ViT) (Doso-vitskiy et al., 2021; Vaswani et al., 2017). No-tably, the quadratic memory cost of transformersposes challenges for videos, which can containup to𝑂(104)tokens. We thus adopt a memoryefficient ST-transformer architecture (inspired byXu et al. (2020), see Figure 4) across all modelcomponents, balancing model capacity with com-putational constraints.Unlike a traditiona\n",
      "----------------------------------------------------------------------------------------------------\n",
      "espondence to: Filippo Lazzati <ﬁlippo.lazzati@polimi.it >.behavior, i.e., it shall make the expert’s policy optimal.As pointed out in Arora & Doshi (2018 ), IRL allows mit-igating the challenging task of the manual speciﬁcationof the reward function, thanks to the presence of demon-strations, and provides an effective method for imitationlearning (Osa et al. ,2018 ). In opposition to mere behav-ioral cloning , IRL allows focusing on the expert intent (in-stead of behavior ) and, for this reason, it has the potentialto reveal the underlying objectives that drive the expert’schoices. In this sense, IRL enables interpretability , im-proving the interaction with the expert by explaining andpredicting its behavior, and transferability , as the reward(more than a policy) can be employed under environmentshifts ( Adams et al. ,2022 ).One of the main concerns of IRL is that the problem is in-herently ill-posed orambiguous (Ng & Russell ,2000 ), i.e.,there exists a variety of reward functions compatible withexpert’s demonstrations. In the literature, many criteriafor the selection of a single reward among the compatibleones were proposed (e.g., Ng & Russell ,2000 ;Ratliff et al. ,2006 ;Ziebart et al. ,2008 ;Boularias et al. ,2011 ). Never-theless, the ambiguity issue has limited the theoretical un -derstanding of the IRL problem for a long time.Recently, IRL has been reframed by Metelli et al. (2021 )into the problem of computing the setof all rewards com-patible with expert’s demonstrations, named feasible re-ward set (or just feasible set ). By postponing the choiceof a speciﬁc reward within the feasible set, this formulatio nhas opened the doors to a new perspective that has enableda deeper theoretical understanding of the IRL problem. Themajority of previous works on the reconstruction of the fea-sible set have focused mostly on the online setting (e.g.,Metelli et al. ,2021 ;Lindner et al. ,2022 ;Zhao et al. ,2023 ;Metelli et al. ,2023 ), in which the learner is allowed to ac-tively interact with the environment and with the expert tocollect samples.Although these works succeeded in obtaining sample efﬁ-cient algorithms and represent a fundamental step aheadin the understanding of the challenges of the IRL problem(e.g., providing sample complexity lower bounds), the un-derlying basic assumption that the learner is allowed to gov -ern the exploration and query the expert wherever is farfrom being realistic. Indeed, the most common IRL ap-1Ofﬂine Inverse RL: New Solution Concepts and Provably Efﬁci ent Algorithmsplications are naturally framed in an ofﬂine scenario, inwhich the learner is given in advance a dataset of trajec-tories of the expert (and, possibly, an additional datasetcollected with a behavioral policy, e.g., Boularias et al.(2011 )). Typically, no further interaction with the envi-ronment and with the expert is allowed ( Likmeta et al. ,2021 ). The ofﬂine setting has been widely studied in (for-ward) reinforcement learning (RL, Sutton & Barto ,2018 ),and a surge of works have analyzed the problem fromtheoretical and practical perspectives (e.g., Munos ,2007 ;Levine et al. ,2020 ;Buckman et al. ,2020 ;Yu et al. ,2020 ;Jin et al. ,2021 ). In this context, a powerful technique is rep-resented by pessimism , which discourages the learner fromassigning credit to options that have not been sufﬁcientlyexplored in the available dataset, allowing for sample efﬁ-ciency guarantees ( Buckman et al. ,2020 ).The IRL ofﬂine setting has been investigated for theproblem of recovering the feasible set in the recentpreprint ( Zhao et al. ,2023 ). The authors consider the samefeasible set deﬁnition employed for the online case, whichenforces the optimality of the expert’s policy in everystate (Metelli et al. ,2021 ;Lindner et al. ,2022 ). However,in the ofﬂine setting, this learning target is unrealistic u n-less the dataset covers the full space. This implies that theproduced rewards can be safely used in forward RL whenthe behavioral policy covers the whole reachable portionof the state-action space only. For this reason, Zhao et al.(2023 ) apply a form of pessimism which allows deliver-ing rewards that make the expert’s policy ǫ-optimal evenin the presence of partial covering of the behavioral policybut only when the latter is sufﬁciently close to the expert’s .This demanding requirements, however, collide with the in-tuition that, regardless the sampling policy, if we observethe expert’s actions, we can deliver at least one reward mak-ing the expert’s optimal.1Desired Properties In this paper, we seek to developnovel appropriate solution concepts for the feasible rewardset and new effective actionable algorithms for recoveringthem in the ofﬂine IRL setting. Speciﬁcally, we aim at ful-ﬁlling the following three key properties :(i) (Sample Efﬁciency ) We should output, with highprobability, an estimated feasible set using a numberof samples polynomial w.r.t. the desired accuracy,error probability, and relevant sizes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ol-ogy, Stockholm, Sweden. Correspondence to: Bernardo Esteves<bernardo.esteves@tecnico.ulisboa.pt >.Figure 1: Observations of the Doorkey environment ofsizes 8 ×8 and 128 ×128, where the agent must pick upthe key (orange), open the door (blue) and go to the goal(green). In this work, we propose NeuralThink , a novelDeep Thinking architecture that is able to train on the smallenvironment (left) and extrapolate to the large environment(right) without loss in performance or fine-tuning.to efficiently extrapolate over inputs, i.e., are able to trainover small dimensional observations and execute over arbi-trarily high dimensional observations, without a significantloss in performance nor additional fine-tuning. Recently,several Deep Thinking methods have been proposed thatemploy recurrent neural networks to mimic the extrapo-lation process (Schwarzschild et al., 2021; Bansal et al.,2022). Compared to feed-forward networks, which have fi-nite depth, recurrent networks can be iterated to an arbitrarydepth after the training procedure. This architectural choiceallows algorithms to extrapolate to significantly larger inputdimensions without suffering from collapse due to excessiveiterations, a problem known as overthinking (Bansal et al.,2022). However, to the best of our knowledge, all priorDeep Thinking methods that are able to extrapolate to largeinput sizes have assumed that the output size of the networkis equal to its input size. As such, the application of thesemethods is currently restricted to symmetrical tasks, such asimage generation.We address this limitation and propose a novel architectureable to consistently extrapolate regardless of the size ofthe output. We contribute NeuralThink , an efficient archi-tecture which can be employed in both symmetrical andasymmetrical tasks (e.g., classification, reinforcement learn-ing tasks), when provided with images of arbitrary sizeat execution time, despite never having trained over such1arXiv:2402.15393v1 [cs.LG] 23 Feb 2024NeuralThink: Algorithm Synthesis that Extrapolates in General TasksObservation RecurrentBlockProcessingBlockOutputSymmetrical tasksObservation RecurrentBlockProcessingBlockOutputAsymmetrical tasksA0.10.90.00.0Figure 2: NeuralThink is a Deep Thinking architecture composed of two main components. The recurrent convolutionalblock (depicted in purple) is responsible for propagating information across the observation (or arbitrary size) by performingmultiple iterations of its latent space. At each iteration we provide the original observation to the recurrent block. Theprocessing block (depicted in green), with an optional aggregation layer (A), is responsible for generating the output.Contrary to prior Deep Thinking architectures, NeuralThink can be employed in both symmetrical (left) and asymmetrical(right) tasks to provide consistent extrapolation.images. Our simple approach combines a recurrent convolu-tional block, able to perceive information at different scalesin the provided image through multiple iterations, and anprocessing block (with an aggregation function), respon-sible for merging the perceived information in the model.We train NeuralThink on a collection of lower-dimensionalobservations using standard supervised learning techniques,and apply the learned algorithm at test time with arbitrarilylarge observations.We evaluate NeuralThink against prior Deep Thinking ar-chitectures in symmetrical tasks and show that NeuralThinkoutperforms previous approaches both in terms of extrapo-lation capabilities (being able to execute over larger imageinput sizes) but also in terms of training efficiency (beingable to train over smaller image input sizes). Additionally,we contribute with a novel benchmark of asymmetrical tasksto evaluate the extrapolation capabilities of Deep Thinkingmodels in environments with different input and output sizes.Once again, the results show that NeuralThink significantlyoutperforms the baselines in extrapolation capabilities andtraining efficiency.In summary, the contributions of our work are:•NeuralThink : We propose a novel Deep Thinkingarchitecture that uses a recurrent convolutional blockand an processing block (with an aggregation function)to allow consistent extrapolation in symmetrical andasymmetrical tasks to arbitrary input sizes;•Asymmetrical Benchmark : We contribute with anovel benchmark of asymmetrical classification tasksfor Deep Thinking architectures, which provide imageobservations of arbitrary size;•Consistent Extrapolation : We show that NeuralThinkoutperforms the prior Deep Thinking architectures re-garding stable extrapolation to large observations andtraining efficiency.2. Related WorkAlgorithmic Learning The use of recurrent neural networksto learn algorithms has been explored in prior work, withthe introduction of Neural Turing Machines (Graves et al.,2014; Zaremba & Sutskever, 2015), Neural GPUs (Kaiser& Sutskever, 2016; Price et al., 2016; Freivalds & Liepins,2017), Neural Arithmetic Logi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "S•Applied computing →Sociology ;Forecasting ;•Human-centeredcomputing→Empirical studies in collaborative and socialcomputing .KEYWORDSHuman mobility, Applied computing, Social computing1 INTRODUCTIONCities, as the epicenters of economic, social, and cultural activities,play a pivotal role in the evolution of human civilization [ 6,7]. Un-derstanding urban human mobility is crucial for creating equitableand sustainable cities, ensuring accessible transportation, economicopportunities, and enhanced quality of life for residents [ 3,4,13].Commuting flows, defined as the movements between a worker’shome and their place of employment, play a crucial role in reveal-ing daily movement patterns within urban settings. These flowsinform city planning, infrastructure development, and contributeto the understanding of urban dynamics and socioeconomic factors[14, 29]. For instance, some real-world questions we have encoun-tered are, “If a developer plans to introduce several corporate officebuildings in a certain area, how would this affect the citywide com-muting flows? How should the government appropriately respondby preparing the necessary supporting infrastructure?” If we trans-late these questions into scientific terminology, they boil down tothe problem of commuting flow prediction – learning the correla-tion between urban planning and the distribution of commutingflows. There are two main practical applications. First, given a newurban development plan, it becomes feasible to determine the corre-sponding distribution of commuting flows, aiding in the assessmentof its impact. Second, traditional methods of collecting commutingflow data rely on national statistical bureaus [ 23], which are costlyand untimely. With a reliable prediction model, we can achievereal-time estimation of commuting flows.The task of commuting flow prediction sits at the crossroads of ur-ban studies and computer science, each bringing distinct approachesand limitations. Traditionally, urban studies have depended onphysical models like the gravity model [ 5,11,30,38] and radiationmodel [ 28] to predict and analyze urban flows, primarily due tolimitations in data availability and quantitative analysis capabilities.These models offer a theoretical framework for understanding ur-ban dynamics. However, these models face significant limitations,such as simple assumptions and limited performances [ 4,28]. Thedemand for enhanced performance in this task is crucial, as it allowsstakeholders to assess the feasibility of specific planning initiativesand interventions with greater performance. This requirement forperformance necessitates access to more comprehensive datasetsand a shift away from traditional models towards more sophisti-cated methodologies. In response to these challenges, deep learningmodels from computer science have emerged as a powerful alter-native [ 16,34,36], offering the promise of higher performance byleveraging large volumes of data to learn complex patterns of urbanmobility. Despite their success in achieving improved predictiveperformance, these models often suffer from a lack of explainabilityor provide insights that are too shallow for practical application inurban planning. In the context of social phenomena like commutingarXiv:2402.15398v1 [cs.LG] 23 Feb 2024Y. Luo, et al.flows, the importance of explainability is on par with performance.A thorough understanding of the underlying forces and dynam-ics of mobility is crucial for validating the model’s credibility andsupporting planning and policy development efforts. This opacityposes a dilemma, as there traditionally exists a trade-off betweendeep learning models’ performance and explainability [ 9]. Recentadvances within the Explainable Artificial Intelligence (XAI) com-munity, however, demonstrate that it is possible to design modelsthat do not sacrifice model explainability for performance [ 26].Some works point out that creating an explainable architectureinvolves designing with explainability in mind, such as utilizingfewer hidden layers or learnable parameters or adopting a modulardesign, so that every part of the model has a clear function andexplanation [ 2,12]. Inspired by this development, an intriguingresearch question emerges: \"How to design a model for commut-ing flow prediction that achieves both high performance and goodexplainability?\"To answer this question, we conducted an in-depth analysis ofthe two classical models previously mentioned: the gravity modeland the radiation model. The gravity model predicts flow distribu-tion by mimicking the gravitational forces observed in the physicalworld, suggesting that the volume of commuting flow is directlyproportional to the attraction between two locations and inverselyproportional to the distance between them. Despite its explana-tory power through a clear, parameter-based formula, the grav-ity model’s reliance on region-specific adjustable parameters andknown analytical inconsistencies limits its applicabilit\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ition kernels. Existingstudies mostly hinge on the assumption that the en-vironment in which a policy is trained is identical tothat in which it is deployed. However, in practicalscenarios where this assumption is violated, standardRL methods are prone to severe failures (Farebrotheret al., 2018; Packer et al., 2018; Zhao et al., 2020), aphenomenon known as the sim-to-real gap . Infectiousdisease control (Laber et al., 2018; Liu et al., 2023a)exemplifies such a case wherein an agent trains poli-cies on simulators extensively utilized in environmen-tal studies. Nonetheless, these simulators cannot fullycapture the environmental evolution complexity, andenvironmental changes may also occur over time, fur-ther contributing to the sim-to-real gap. Another in-stance is found in robotics learning, where slight varia-tions between training and testing environments, suchas terrain or target parameters, may lead to task fail-ure (Maitin-Shepard et al., 2010; Tobin et al., 2017;Peng et al., 2018).Learning under the sim-to-real gap can be conceptu-alized as an off-dynamics RL problem (Koos et al.,2012; Wulfmeier et al., 2017; Eysenbach et al., 2020;Jiang et al., 2021), where an agent trains a policy inan accessible source domain, such as a simulator or thepresent environment, then deploys the learned policyin a distinct target domain, which could be the real en-vironment the agent encounters during operation or afuture changing environment. The dynamics shift be-tween environments necessitates a robust strategy forpolicy learning in the source domain, ensuring that thepolicy can work effectively in different yet structurallysimilar target domains.Distributionally robust Markov decision process (DR-arXiv:2402.15399v1 [cs.LG] 23 Feb 2024Distributionally Robust Off-Dynamics Reinforcement LearningMDPs) (Satia and Lave Jr, 1973; Nilim and El Ghaoui,2005; Iyengar, 2005) address the sim-to-real gap chal-lenge by modeling the uncertainty of transition ker-nels. It aims to learn a robust policy that performswell under the worst-case transition kernel within theuncertainty set defined based on the source environ-ment (Xu and Mannor, 2006; Wiesemann et al., 2013;Zhang et al., 2021; Yang et al., 2022; Panaganti et al.,2022; Shi and Chi, 2022; Yang et al., 2023b; Shenet al., 2024). Existing DRMDP research can be catego-rized based on the assumption on the source domain:(i) planning problems where the exact model is as-sumed known, (ii) learning under a generative model,and (iii) learning from offline datasets utilizing spe-cific data coverage assumptions. However, in practice,formulating and solving a planning problem is ofteninfeasible due to imperfect knowledge or complexity ofthe source domain. Similarly, an accurate generativemodel representing the source domain is usually un-available. Additionally, most data coverage assump-tions require the datasets have sufficient coverage ofdistributions induced by the optimal policy under anytransition kernel in the uncertainty set. Since the op-timal policy is usually unknown and there are infinitenumber of transition kernels in the uncertainty set,practical verification of data coverage assumptions isintractable. Thus, when incremental collection of datathrough active interactions with the source domainis feasible, online algorithms without relying on ad-ditional oracles or data coverage assumptions aboutthe optimal policy will be preferred. We refer to thisas the online DRMDP problem.Another significant challenge in RL is the ubiquitouspresence of applications with arbitrarily large stateand action spaces, which require suitable function ap-proximations to alleviate the curse of dimensionality.Although approaches based on linear function approx-imation have exhibited theoretical and empirical suc-cess in numerous settings under standard MDP (Bhan-dari et al., 2018; Modi et al., 2020; Jin et al., 2020; Heet al., 2023, 2021; Yang and Wang, 2020), DRMDPencounters additional difficulties when combined withlinear function approximations since the dual formu-lation in worst-case analyses may induce extra non-linearity, even when the source domain transition ker-nel is linear (Tamar et al., 2014; Pinto et al., 2017;Derman et al., 2018; Mankowitz et al., 2019; Der-man et al., 2020; Zhang et al., 2021; Badrinath andKalathil, 2021). Consequently, the theoretical under-standing of online DRMDPs with function approxima-tion remains elusive, even when the approximation islinear. This leads to the open question:When is it possible to design a provably efficientalgorithm for online DRMDPswith linear function approximation?In this work, we provide the first analysis of onlineDRMDP with linear function approximation wherean agent actively interacts with the source domain tolearn a robust policy.Our main contributions are summarized as follows.•We first investigate the differences in applying lin-ear function approximation in DRMDPs with un-certainty sets defined on different probability di-vergence me\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TRODUCTIONMotivation . Question answering (QA) comprises a spectrum ofsettings for satisfying users’ information needs, ideally giving crisp,entity-level answers to natural-language utterances [ 46]. TemporalQA specifically focuses on questions with temporal conditions (e.g.,[24,31,48]), making up a substantial portion of user needs [ 65],This work is licensed under a Creative Commons AttributionInternational 4.0 License.WWW ’24, May 13–17, 2024, Singapore, Singapore©2024 Copyright held by the owner/author(s).ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.https://doi.org/10.1145/nnnnnnn.nnnnnnnbut poses challenges that are not properly met by universal QAsystems. Consider the following example:𝑞1:Record company of Queen in 1975?The band Queen had different record companies over the years, soit is decisive to consider the explicit temporal constraint (“in 1975” ).Other questions with explicit time are lookups of dates, such as:𝑞2:When was Bohemian Rhapsody recorded?Another – underexplored and most challenging – situation iswhen questions involve implicit temporal constraints . These caninvolve the need to compare different time points or intervals, evenwhen the user input does not explicitly state it. Examples are:𝑞3:Queen’s record company when recording Bohemian Rhapsody?𝑞4:Queen’s lead singer after Freddie Mercury?For𝑞4, the system has to find out when Mercury died or left theband, in order to compute the correct answer that Brian May (theband’s guitarist) took over as lead singer.The research literature on temporal QA is substantial, including[9,10,16,23–25,31,48,58]. Most methods address all kinds of tem-poral questions, but are typically less geared for implicit questions.Some methods operate over curated knowledge bases (KBs) (e.g.,[16,23,24]), while others are designed for processing text corporasuch as news collections or Wikipedia full-text (e.g., [9, 35]).State-of-the-art limitations . We observe three major issues:(i)Many methods use “soft-matching” techniques, based on latentembeddings or language models. This may lead to invalid an-swers, where the non-temporal part of a question is matched,but the temporal constraint is violated. For example, a questionabout “Queen’s record company in 1990?” may erroneously re-turn EMIinstead of the correct value Parlophone , because EMIismore prominent and was Queen’s company on most albums.Even when the output is correct, this could be by the promi-nence of the answer alone. For example, “Who was Queen’s leadsinger in 1975?” could return the most popular Freddie Mercurywithout checking the time. When we vary the question into“. . . in 2000?” , many systems would still yield Freddie Mercury ,although he was dead then. This indicates that the system hasincomplete inference and is unable to explain its answer deriva-tion. We call this phenomenon unfaithful QA .ii)A weak spot of temporal QA systems is the handling of implicitquestions . These are infrequent in established benchmarks. Somemethods [ 16,23,34] aim to transform the implicit conditionsinto explicit temporal constraints, based on classifying phrasesstarting with “during”, “before” etc. However, they heavily rely1arXiv:2402.15400v1 [cs.IR] 23 Feb 2024WWW ’24, May 13–17, 2024, Singapore, Singapore Zhen Jia, Philipp Christmann, & Gerhard WeikumFigure 1: Overview of the Faith pipeline. The figure illustrates the process for answering 𝑞3(“Queen’s record company whenrecording Bohemian Rhapsody?” ) and 𝑞1(“Record company of Queen in 1975?” ). For answering 𝑞3, two intermediate questions𝑞31and 𝑞32are generated, and run recursively through the entire temporal QA system.on hand-crafted rules which are rather limited in scope andcannot robustly handle unforeseen utterances.(iii) Prior methods run on a single information source : either a KB ora text corpus. This limits QA coverage: KBs are incomplete andlack refined detail about events, whereas text collections areharder to extract answers from and often fail on complex ques-tions [ 11,16]. QA over heterogeneous sources, including alsoweb tables, has been addressed by [ 13,38], but these methodsdo not support temporal conditions.Approach . To overcome these limitations, we propose Faith (FAIthfulTemporal question answering over Heterogeneous sources), a tem-poral QA system that operates over heterogeneous sources, seam-lessly combining a KB, a text corpus and web tables. Inspired bythe architecture of [13], Faith consists of three main stages:(i)Temporal Question Understanding for representing thequestion intent into a structured frame, with specific considera-tion of the temporal aspects;(ii)Faithful Evidence Retrieval for identifying relevant piecesof evidence from KB, text and tables, with time-aware filteringto match the temporal conditions;(iii)Explainable Heterogeneous Answering to compute entity-level answers and supporting evidence for explanation.A key novelty in the question understanding is that implicitconstraints are resolved into explicit temporal values by gener\n",
      "----------------------------------------------------------------------------------------------------\n",
      "object models, thus encountering difficulty inopen real-world scenarios with unknown objects.Recent works explore solutions for unknown object rear-rangement with the goal object configuration given as an RGB-D image [17]–[22]. Efforts have been made to representationlearning of current and goal object configurations by lever-aging learning-based perception models. The pioneering workNeRP [20] builds a scene graph by matching the objects incurrent and goal configurations with 2-DoF pose changes i.e.in-plane translations. Based on the graph, grasp and placeaction are learned from demonstration to complete the task.Selective Rearrangement [22] further extends the graph-basedrepresentation to address settings with clutter and selectivityKechun Xu, Zhongxiang Zhou, Jun Wu, Haojian Lu, Rong Xiong, YueWang are with Zhejiang University, Hangzhou, China. Corresponding author,wangyue@iipc.zju.edu.cn .Codes will be released at https://github.com/xukechun/GSP.Inner Loop for SeeingGrasp PlaceOuter Loop for Grasp & PlaceRecognize Objects by Actively SeeingTask- level PerformanceGrasp, See and PlaceInitial sceneGoal scenePerception NoiseTask-level RewardsHumanCompletionEfficiencyTask-level Rewardsactiontask envrewardRL Perception Noise AnalysisGrasp PlaceDecoupled StructureFig. 1. Grasp, See, and Place. The robot is given the initial and goal scenes forthe task of object rearrangement. Aiming at improving task-level performancewith perception noise, we first derive the decoupled structure by analysis.Guided by the decoupled prior, we incorporate human behavior and task-levelrewards into the general framework of GSP. In general, GSP contains twoloops: the inner loop actively sees the grasped object for high self-confidentmatching, and the outer loop conducts the grasp and place planning.i.e.rearranging a subset of objects. The policy is guided by thegraph editing distance. IFOR [21] employs a pixel-level opticalflow to represent the difference between current and goalobject configurations, which allows for rearranging unknownobjects with 3-DoF pose changes i.e.in-plane translations andin-plane rotations. By minimizing the flow, the robot places theobjects selected according to handcrafted rules. These fruitfulprogresses largely narrow down the gap of perception sidefrom known to unknown object rearrangement.In contrast, looking into the planning side of unknown ob-ject rearrangement, we find that existing systems derive actionsby heuristic rules or supervision, which pay less attention tothe optimality of task-level performance. Taking a simple caseof two objects that can be directly rearranged to the goal,their moving order does not affect the task-level performance.Hence, regarding either order as the unique ground truth forsupervision brings bias into policy learning. Perception noisemakes the situation even worse, as the heuristics may be builton incorrect perception results. Considering the classic task-level optimal policy design for object rearrangement with idealperception, we raise a question: Given the noisy perceptionresults, is there a policy that can optimize the task-levelperformance of unknown object rearrangement?To optimize task-level performance, reinforcement learning(RL) is a useful tool. However, it is challenging to directlylearn the policy with RL for the long-horizon object rearrange-ment. In this paper, we delve into the structure of the policyarXiv:2402.15402v1 [cs.RO] 23 Feb 20242by beginning with theoretic analysis on object rearrangementwith ideal perception. We show that, to minimize the totalsteps, it is an optimal policy to grasp objects whose goalsare non-occupied and place them to their goals, and resolvethe objects whose goals are circularly occupied with the aidof buffers. Additionally, we find that the optimal grasp ismultimodal, verifying the difficulty of the per-step supervisionof action. When extending to noisy perception, we derive thatthe noise impacts the grasp and place in a decoupled way. Wefurthermore show such a decoupled structure is non-trivial toimprove the task-level optimality. Besides, the perception noisemakes the grasp supervision even harder.Guided by the insights, we propose a RL-based rearrange-ment policy with the decoupled structure as prior, namely GSP(Grasp, See and Place). GSP contains two loops: an outer loopfor the grasp and place with an inner loop for the perceptionof place by seeing (Fig. 1). Thanks to the decoupled prior, wecan boost the perception performance of place individually bydesigning an inner loop. Inspired by the human behavior oflooking at an object up and down to confirm its identity [23]–[25]), we propose to independently learn a see policy byactively rotating the in-hand object to improve the perceptionof place for task optimality. For the outer loop, we proposeto learn a policy conditioned on the uncertain object matchingbetween current and goal images as well as grasp capability,guided by task-level rewards. Such a dual-loop reinforceme\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Mau-rice Kraus <maurice.kraus@cs.tu-darmstadt.de>, Felix Divo<felix.divo@cs.tu-darmstadt.de>.generation. However, there is still a need for a methodol-ogy to apply the same principles to the time domain, assignificant amounts of unlabeled time domain data are avail-able, yet they are seldom used for successfully trainingmodels for different tasks. Most classification systems ontime series are purely supervised and, therefore, still relyon expensive and complete labels per dataset (Shi et al.,2021; Nie et al., 2023; Zeng et al., 2023). Overcoming thisis crucial in contemporary real-world situations, such ashealthcare, where lack of labels is not the only limitation.There is often an additional overall scarcity of sufficientdata points, for instance, due to privacy constraints. How-ever, this clashes with the requirements of current deeplearning models, which typically require large single-sourcedatasets (Iwana & Uchida, 2021). Fortunately, multiplesmall datasets exist that, in combination, can be leveragedeven if they are unlabeled. For instance, the UCR/UEATime Series Classification Archive (Dau et al., 2019) con-tains 57% of datasets with 300 or fewer training samples,which are usually not enough to be applicable to the tasks.Additionally, there are datasets with only unlabeled data,such as the M4 Competition (Makridakis et al., 2018) orrecordings of meteorological, financial, industrial, traffic,and other signals. Combining them is a new perspective onthis collection of valuable data.We address these challenges by utilizing transfer learning,and specifically, by training a representation on unlabeledsource datasets (Eldele et al., 2020). As shown in Figure 1,the learned features are then used as a starting point for fine-tuning on a target dataset with typically much fewer labeledinstances. In the natural language processing (NLP) and thevision domain, one can leverage pretrained weights frommodels trained on a general dataset, even in largely differentdomains. However, while multiple works have shown thefundamental feasibility of pretraining on time series (Maet al., 2023), the source and target domain currently stillneed to be quite similar and follow the same underlyingtemporal dynamics (Zhang et al., 2022) for it to succeed.Thus, a similar generalized representation taking advantageof a diverse collection of source datasets would prove verybeneficial for use in several target domains.1arXiv:2402.15404v1 [cs.LG] 23 Feb 2024United We Pretrain, Divided We Fail!Arrow,FD-A,ECG,. . .XD-MixUp Encoder Fθ Classifier C2Classifier C1Classifier C3EpilepsyArrowBeefFigure 1: The core idea of our method XIT is to learn a single encoder from multiple datasets. The resulting representationcan then be used to train classifiers on datasets seen during the pretraining phase and to be transferred to entirely new ones.Contributions To this end, we make several importantcontributions: ( 1) We show how up to 75 unlabeled timeseries datasets can be combined effectively into a singlepretraining collection. ( 2) We propose a novel interpo-lation method called Cross- Dataset MixUp (XD-MixUp)that induces a shared latent representation for multipledatasets. ( 3) We propose the SoftInterpolation ContextualContrasting (SICC) loss function, which we combinewith the Time Series Temporal and Contextual Con-trasting (TS-TCC) (Eldele et al., 2020) framework us-ing XD-MixUp. Overall, we call our new architectureXIT1(XD-MixUp + S ICC + Temporal Contrasting). ( 4) Wedemonstrate good transfer classification performance onmultiple small labeled target datasets without requiring ex-tensive retraining for each. In particular, we outperformsupervised training and other self-supervised pretrainingmethods.Structure of the Paper We start with explaining our pro-posed XIT method, including introducing the XD-MixUpand the SICC loss, before moving on to the empirical demon-stration of its efficacy. Before concluding, we present relatedwork.2. Multi-Dataset Pretraining with XITIn this section, we present our pretraining method XIT,where the overall goal is to learn a D-dimensional latentrepresentation zi∈RDof some time series xi∈RTof length T. For clarity, we focus on univariate time se-ries, while the method can be readily extended to multi-variate tasks. We train the parameters θof an encoderFθ(·)to compress xiinto a more abstract representationzi=Fθ(xi). That representation can subsequently be usedfor downstream tasks, such as supervised training of a clas-sifier. Note that in the pretraining phase, the encoder Fistrained in a self-supervised fashion without access to anylabels. We base our method on the work of Eldele et al.(2020) and adapt their TS-TCC framework to enable thetraining on multiple datasets. While TS-TCC works wellfor its designed application to single-dataset scenarios, it1pronounced «exit»cannot leverage the availability of multiple datasets. Onthe contrary, its performance severely drops in those set-tings, as we will show in Sec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "olars [10, 11, 12, 13, 14, 15, 16, 17]. Among t hese, two prominent approacheshave emerged as particularly successful: Deep Operator Neu ral Networks (DeepONet) [10, 11,18, 12, 19, 20] and Fourier Neural Operator (FNO) [21, 22, 14, 15, 16, 17, 23].∗Department of Mathematics, Purdue University, West Lafaye tte, IN 47907, USA. (Email: cmoya-cal@purdue.edu)†School of Mechanical Engineering, Purdue University, West Lafayette, IN 47907, USA. (Email: amol-laal@purdue.edu)‡Corresponding author. Department of Mathematics, Florida State University, Tallahassee, FL 32304, USA.(Email: zecheng.zhang.math@gmail.com)§Department of Statistics and Data Science, Yale University , New Haven, CT 06511, USA. (Email:lu.lu@yale.edu)¶Department of Mathematics and Mechanical Engineering, Pur due University, West Lafayette, IN 47907, USA.(Email: guanglin@purdue.edu)1Compared to FNO, DeepONet represents a mesh-free neural ope rator, implying that the outputfunctions do not require discretization. This characteris tic enhances the ﬂexibility of DeepONetin both training and testing phases [24, 25, 26, 11, 27, 28, 29 , 22]. Furthermore, recent advance-ments in BelNet by [25, 26, 22] have extended DeepONet’s capa bilities, making it invariant toinput function discretization. This means that input funct ions no longer need to conform to ashared discretization, thereby enhancing the versatility of DeepONet even further.DeepONet and its extensions have been successfully applied to a diverse array of real-worldapplications. These include weather forecasting [23], sub -surface structure detection [13], elec-trical propagation on the left ventricle [30], geological c arbon sequestration [31], disk-planetinteractions in protoplanetary disks [32], hypersonic sys tems [33, 34], power systems [35], andoptimization [36].The need for Uncertainty Quantiﬁcation (UQ) in scientiﬁc ma chine learning [37, 38] often arisesfrom several factors. These include uncertainties in data d ue to measurement errors and numer-ical algorithm errors, uncertainties in model forms due to n etwork architectures and physicalmodels of varying ﬁdelities, and uncertainties in paramete rs due to network training and sys-tem properties at diﬀerent scales. As a result, researchers a re tasked with providing conﬁdenceintervals for predicted outputs. A good interval should mee t two fundamental criteria. Firstly,it should have a signiﬁcant coverage rate, encompassing pre cise solutions or dynamics to thegreatest extent possible. Secondly, the interval should be minimized in size.Numerous works [21, 27, 37, 38, 39, 40, 41, 36, 42, 43] have est ablished various uncertaintyquantiﬁcation (UQ) frameworks for DeepONet. These framewo rks aim not only for accuratepredictionbutalsoforquantifyinguncertainty. Forexamp le, arecent study[21]fromtheauthorsaddresses this uncertainty issue in operator learning by fr aming the training process within aBayesian framework. The study uses Langevin diﬀusion-based sampling methods [44, 45, 46, 47,48] to generate ensembles, which aids in characterizing unc ertainty in DeepONet predictions. Tolessen the use of ensembles and increase eﬃciency, we introd uced a probabilistic framework forUQ in DeepONets in [27], which provides an input-dependent s tandard deviation as a heuristicmeasureofuncertainty. However, despitethepromisingres ultsfromthesemethods, constructingrigorous conﬁdence intervals for DeepONet predictions rem ains an unsolved challenge, which weaim to solve using conformal prediction.Conformal prediction [49, 50, 51] is an alternative method f or constructing conﬁdence intervalsthat provide nonasymptotic, distribution-free coverage g uarantees. Unlike the Bayesian Frame-work, this approach does not require prior distribution kno wledge and avoids relying on strongassumptions of large-sample asymptotic approximations, w hich may be diﬃcult to justify inpractical scenarios. These characteristics make conforma l prediction a valuable tool for address-ing scientiﬁc machine learning problems, especially those arising from engineering applicationswith real observed data and a lack of prior knowledge.There are two types of conformal prediction methods: full co nformal prediction [51, 52] and splitconformal prediction [51, 53]. In this paper, we focus on spl it conformal prediction, which oper-ates with ﬁnite samples and enables building adaptive and va lidated conﬁdence intervals. Theseproperties have allowed split conformal prediction to beco me a popular approach for machinelearning and uncertainty quantiﬁcation tasks, including L anguage Modeling [54], Graph Neu-ral Networks [55], time series [56], and quantile regressio n [57]. However, despite its potential,split conformal prediction remains underutilized in the sc ientiﬁc machine learning community,particularly in the context of Deep Operator Networks.Thegoalof this paper is to address the challenge of uncertainty quan tiﬁcation (UQ) in operatorlearning. It aims to showcase the implementation \n",
      "----------------------------------------------------------------------------------------------------\n",
      "terior distributionover models, and can thus be explicitly calculated. As shown byRusso and Van Roy (2018 ), the resultingalgorithm can guarantee massive statistical gains over mor e common approaches like Thompson sampling(Thompson ,1933 ) or optimistic exploration methods ( Lai and Robbins ,1985 ), and in particular can takeadvantage of the structure of the problem instance much more effectively. Realizing the same gains in anon-Bayesian setup (which we will sometimes call frequentist , for lack of a better word) is hard for multiplereasons, the most severe obstacle being that the true model i s entirely unknown and Bayesian posteriorscannot be used to quantify the uncertainty about the model in a meaningful way. As such, deﬁning appropriatenotions of information gain and information ratio is not str aightforward. This is the problem we address inthis paper.Our main contribution is constructing a version of informat ion-directed sampling that is implementablewithout Bayesian assumptions, and yields frequentist vers ions of the same problem-dependent guaranteesas the ones achieved by the original IDS method in a Bayesian setup. The key element in our approach isthe introduction of a surrogate model that allows for a meaningful deﬁnition of the information ra tio thatis amenable to a frequentist analysis. This surrogate model is the function of an optimistically adjustedposterior distribution inspired by the “feel-good Thompso n sampling” algorithm of Zhang (2022 ), and isused to estimate the components of the information ratio: th e regret and the information gain. With thesecomponents, it becomes possible to deﬁne an information rat io that is an explicit function of the optimistic© G. Neu, M. Papini & L. Schwartz.NEUPAPINI SCHWARTZposterior, which can then be optimized to yield a decision-m aking rule that we call “optimistic information-directed sampling” ( OIDS ).For the sake of concreteness, we focus on the problem of conte xtual bandits and show that OIDS cannot only recover worst-case optimal regret bounds in this ca se, but also satisﬁes problem-dependent guar-antees that are commonly referred to as ﬁrst-order bounds (Cesa-Bianchi et al. ,2005 ;Agarwal et al. ,2017 ;Allen-Zhu et al. ,2018 ;Foster and Krishnamurthy ,2021 ). Besides these general guarantees, we also providesome illustrative examples that show that OIDS can reproduce the expedited learning behavior of IDS oneasy problems, but without requiring Bayesian assumptions .Our methodology also draws inspiration from the analytic fr amework of Foster, Kakade, Qian, and Rakhlin(2021 ), developed for a very general range of sequential decision -making problems. Their analysis revolvesaround the notion of the decision-estimation coefﬁcient (DEC), which quantiﬁes the tradeoffs that need to bemade between achieving low regret and gaining information a bout the true model in a way that is similar tothe information ratio of Russo and Van Roy (2016 ). The main contribution of Foster et al. (2021 ) is showingthat the minimax regret in any sequential decision-making p roblem can be lower bounded in terms of theDEC, and they also show that nearly matching upper bounds can be achieved via a simple algorithm theycallestimation to decisions (E2D). Unlike the information ratio, the DEC does not make use of a Bayesianposterior to quantify uncertainty, but is rather deﬁned as a worst-case notion, and as such provides frequentistguarantees that hold uniformly for all problem instances. H owever, the worst-case nature of the DEC canalso be seen as an inherent limitation of their framework. In particular, the E2D algorithm is also based onthe same conservative notion of regret-information tradeo ff, and thus all known guarantees for this algorithm(and its variants such as the ones proposed by Chen et al. ,2022 ;Foster et al. ,2023a ,b;Kirschner et al. ,2023 )fail to take advantage of problem structures that may facili tate fast learning.Our own framework uniﬁes the advantages of the two threads of literature described above: unlike E2D, itis able to achieve instance-dependent guarantees and learn faster in problems with more structure, and, unlikestandardIDS, it can do so without relying on Bayesian assumptions. Our an alysis draws on elements of bothlines of work, and also on the techniques introduced by Zhang (2022 ), as mentioned above.We are not the ﬁrst to attempt the generalization of IDS beyond the Bayesian setting. Kirschner and Krause(2018 ) proposed a frequentist alternative to the information rat io for the special case of loss functions thatare linear in some unknown parameter, and constructed an app ropriate version of IDS that is able to takeadvantage of certain problem structures and obtain guarant ees that improve upon the minimax rates. Theirapproach has inspired a line of work aiming to prove tighter a nd tighter problem-dependent bounds for arange of sequential decision-making problems, but so far al l of these results remained limited to linea\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ssing (LeCun et al., 1989), 3D rotational equivariancein Alphafold2 (Jumper et al., 2021), and equivariance togeneral discrete groups in Group Convolutional Neural Net-works (GCNNs) (Cohen & Welling, 2016a).But designing efficient equivariant networks can be challeng-ing both because they require domain-specific knowledgeand can be computationally inefficient. E.g., there are sev-eral works designing architectures for different groups suchas the special Euclidean group SE(3)(Fuchs et al., 2020),special Lorentz group O(1,3)(Bogatskiy et al., 2020), dis-crete Euclidean groups (Cohen & Welling, 2016a; Ravan-bakhsh et al., 2017), etc. Moreover, some of these networkscan be computationally inefficient, prompting the designof simpler and lightweight equivariant networks such asEGNN (Satorras et al., 2021) for graphs and vector neu-rons (Deng et al., 2021) for point cloud processing.Finzi et al. (2021) propose an algorithm to construct equiv-ariant MLPs (EMLPs) for arbitrary matrix groups when thedata is provided using tensor polynomial representations.This method directly computes the basis of the equivariantMLPs and requires minimal domain knowledge. Althoughelegant, EMLPs are computationally expensive. Similarly,using equivariant basis functions can also be computation-ally expensive (Fuchs et al., 2020; Thomas et al., 2018) lead-ing to several group-specific efficient architectures. Equiv-ariance as an inductive bias makes the learning problemeasier (Van der Pol et al., 2020) and provides robustnessguarantees (Mao et al., 2023), and scaling is important tolearn more complex functions that are not easy to modelusing inductive biases such as equivariance. Hence, we needa scalable equivariant network.To this end, we introduce Group Representation Network(G-RepsNet), which replaces scalar representation fromclassical neural networks with tensor representations of dif-ferent orders to obtain expressive equivariant networks. Weuse the same tensor polynomial representations as EMLPto represent the features in our network. But unlike EMLP,we only use inexpensive tensor operations such as tensoraddition and tensor multiplication to construct our network.We show that even with these simple operations, we obtaina universal network for orthogonal groups. Further, we em-1arXiv:2402.15413v1 [cs.LG] 23 Feb 2024G-RepsNet: A Fast and General Construction of Equivariant Networks for Arbitrary Matrix Groupspirically show that G-RepsNet provides competitive resultsto existing state-of-the-art equivariant models and even out-performs them in several cases while having a simple andefficient design.Our proposal generalizes vector neurons (Deng et al., 2021),which introduce first-order O(3)tensor representations invarious architectures to obtain equivariance to the O(3)group. In contrast, G-RepsNet is a universal network thatworks for arbitrary matrix groups and uses higher-ordertensor polynomial representations, while being computa-tionally efficient. The main contributions as well as thesummary of our results are detailed below.1.We propose a novel lightweight method for construct-ing equivariant architectures. We call the obtainedarchitectures G-RepsNets, which is a computationallyefficient architecture equivariant to arbitrary matrixgroups, a universal approximator of equivariant func-tions for orthogonal groups, and easy to construct. Weshow that G-RepsNet is competitive to existing so-phisticated equivariant models across a wide range ofdomains as summarized next.2.On synthetic datasets from (Finzi et al., 2021), weshow that G-RepsNet is computationally much moreefficient than EMLP and also performs competitivelyto EMLP across different groups such as O(5),O(3),andO(1,3)using scalars, vectors, and second-ordertensor representations.3.We show that G-RepsNet with second-order tensorrepresentations outperforms sophisticated state-of-the-art equivariant networks for image classification suchas GCNNs (Cohen & Welling, 2016a) and E(2)-CNNs (Weiler & Cesa, 2019) when trained fromscratch, and equitune (Basu et al., 2023b) when usedwith pretrained models.4.To further illustrate the generality of our approach, weshow that G-RepsNet is competitive to G-FNO (Helwiget al., 2023) and EGNN (Satorras et al., 2021) on N-body predictions and solving PDEs, respectively, whilebeing computationally efficient.2 Related WorksParameter sharing A popular method for constructinggroup equivariant architectures involves sharing learnableparameters in the network to guarantee equivariance, e.g.CNNs (LeCun et al., 1989), GCNNs (Cohen & Welling,2016a; Kondor & Trivedi, 2018), Deepsets (Zaheer et al.,2017), etc. However, all these methods are restricted to dis-crete groups, unlike our work which can handle equivarianceto arbitrary matrix groups.Figure 1. A summary of G-RepsNet layer construction exemplifiedwith inputs of types T0, T1,andT2, and outputs of the same types.Each layer consists of three subcomponents:i) input feature representation shown as Ti,ii)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ataset0%2%4%6%8%10%12%14%16%18%20%22%24%26%28%Performance relative to Classifer Tunning5.1%11.8%16.0%16.6%24.7%26.9%LoRAFull Fine-tuningUniform CompositionLearned CompositionFigure 1. Performance of fine-tuning strategies relative to classifiertuning in the one-shot transfer learning setting. Both learned (blue)and uniform (orange) composition methods mostly outperform reg-ular LoRA (green) and full fine-tuning (red) baselines, suggestingthat the linear interpolation of pre-trained LoRA modules helpsfew-shot transfer to an unseen downstream task. For each dataset,we use each of the rest of the dataset as an upstream task. Refer toSection 5.2 for experiment details.spurred a trend toward standardization in training modelsfor new tasks. Both the training methodology, often involv-ing transfer learning from popular foundational models, andthe model architecture itself have conformed to establishednorms, typically following a few influential foundation mod-els (Dosovitskiy et al., 2020; Chung et al., 2022; Radfordet al., 2021; Touvron et al., 2023). This standardizationhas given rise to numerous publicly available fine-tunedmodels, all sharing the same architecture. With the availabil-ity of numerous fine-tuned models derived from the samefoundation model, recent studies have focused on mergingmultiple fine-tuned models originating from a set of up-stream tasks (Matena & Raffel; Choshen et al., 2022; Ram ´eet al., 2023; Davari & Belilovsky, 2023).Simultaneously, due to the substantial computational costof fine-tuning foundation models, there has been a surge inproposals for efficient adapter modules, enabling parameter-efficient fine-tuning of these models (Lester et al., 2021; Huet al., 2021; Liu et al., 2022). Notably, low-rank adapta-1arXiv:2402.15414v1 [cs.LG] 23 Feb 2024Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?tion (LoRA) (Hu et al., 2021) has emerged as an efficientfine-tuning technique. LoRA involves adding and traininglightweight modules to a frozen pre-trained model, achiev-ing good performance on the downstream task. By alleviat-ing high memory demands and computational costs, LoRAhas become the standard for fine-tuning Large LanguageModels (LLMs), diffusion models, and vision transform-ers across various downstream tasks (Dettmers et al., 2023;Xu et al., 2023; Gandikota et al., 2023; Shah et al., 2023).LoRA’s efficiency has empowered developers to create andshare custom models trained on their unique data for newtasks, resulting in the availability of hundreds of publiclyaccessible LoRA modules tailored for diverse downstreamtasks.This paper explores the possibility of leveraging pre-trainedLoRA modules for efficient fine-tuning on a new task. In-spired by the literature on model merging (Wortsman et al.,2022; Choshen et al., 2022; Ilharco et al., 2022), we explorethe composability of LoRA modules, examining whetherknowledge from multiple upstream tasks can be combinedfor tackling new tasks. Specifically, we aim to answer thisquestion: Does combining pre-trained LoRA modules en-hance transfer accuracy on unseen tasks?To answer this question, we adopt a few-shot transfer setting,where we train LoRA modules on diverse upstream tasksand subsequently evaluate various composition strategieson a downstream task with a limited number of samples.We evaluate two combining strategies: (a) uniform compo-sition , where upstream LoRA modules are averaged withequal weights, and (b) learned composition , where we learnweights for each upstream module and perform weightedaveraging. In real-world scenarios, the upstream and down-stream tasks can be entirely disentangled, originating fromdistinct datasets, diverse domains, or even different partsof the same dataset. To comprehensively assess the impactof this disentanglement on the performance of compositionmethods, we examine three scenarios wherein upstreamtasks are dissociated from the downstream tasks: (a) labelshift, where upstream tasks and the downstream task eachpossess distinct labels from the same dataset; (b) domainshift, involving the downstream task accessing a uniquedomain from the same dataset; and (c) task shift , whereinboth upstream and downstream tasks have exclusive datasetsfeaturing different tasks.Our findings in vision and language models demonstrate thatthe combination of pre-trained LoRA modules enhances gen-eralization in a few-shot setting. Specifically, both uniformand learned composition methods yield superior transfer ac-curacy, outperforming full fine-tuning and training a LoRAfrom scratch as shown in Figure 1. Furthermore, our resultsindicate that as the number of samples in the downstreamtask increases, learned composition maintains performanceon par with full fine-tuning and regular LoRA training whileutilizing significantly fewer trainable parameters. Lastly, acomparison of the learned and uniform composition revealsthat as the number of samples in the downstream task in-creases, the performance g\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in extracted_texts[:100]:\n",
    "    print(text)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3910df-8196-4953-906f-f4c3764dd222",
   "metadata": {},
   "source": [
    "## Now evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa431c6-d1cb-4d02-b9cb-c57e963c6eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 1024\n",
    "log_folder_path = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cfa16a-0ace-4aca-b026-63fe0aa107f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_sum(logits, target_token_ids):\n",
    "    shifted_logits = logits[:-1, :]\n",
    "    shifted_targets = target_token_ids[1:]\n",
    "    \n",
    "    log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "    \n",
    "    target_log_probs = -log_probs.gather(1, shifted_targets.unsqueeze(1)).squeeze()\n",
    "    # print(target_log_probs)\n",
    "    \n",
    "    log_sum = torch.sum(target_log_probs, dim=-1)\n",
    "    # print(perplexity_sum)\n",
    "\n",
    "    return log_sum.item()\n",
    "\n",
    "\n",
    "def print_model_parameters_in_billions(model):\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    total_params_billion = total_params / 1e9\n",
    "    \n",
    "    print(f\"Model parameters: {total_params_billion:.3f} billion\")\n",
    "    \n",
    "    \n",
    "def log(data_dict, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        try:\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Directory created at {folder_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    file_name = f\"{timestamp}.json\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data_dict, file, indent=4)\n",
    "        print(f\"Dictionary saved successfully to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dictionary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a088d82-6983-44f9-a2ef-7ecbe24e1c57",
   "metadata": {},
   "source": [
    "## Evaluate RWKV(v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288d762b-b4d6-408f-b78d-a7977e2cb69e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module wkv_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
      "\n",
      "Loading ../models/rwkv-4-3b/RWKV-4-Pile-3B-20221110-ctx4096.pth ...\n",
      "Model detected: v4.0\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float16, float16], store 33 layers\n",
      "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  50277  2560 \n",
      "blocks.0.ln1.weight               f16   cuda:0   2560       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   2560       \n",
      "blocks.0.ln2.weight               f16   cuda:0   2560       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   2560       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2560       \n",
      "blocks.0.att.time_first           f32   cuda:0   2560       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   2560       \n",
      "blocks.0.att.key.weight           f16   cuda:0   2560  2560 \n",
      "blocks.0.att.value.weight         f16   cuda:0   2560  2560 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0   2560  2560 \n",
      "blocks.0.att.output.weight        f16   cuda:0   2560  2560 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   2560       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   2560       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0   2560 10240 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0   2560  2560 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0  10240  2560 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f16   cuda:0   2560       \n",
      "blocks.31.ln1.bias                f16   cuda:0   2560       \n",
      "blocks.31.ln2.weight              f16   cuda:0   2560       \n",
      "blocks.31.ln2.bias                f16   cuda:0   2560       \n",
      "blocks.31.att.time_decay          f32   cuda:0   2560       \n",
      "blocks.31.att.time_first          f32   cuda:0   2560       \n",
      "blocks.31.att.time_mix_k          f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_v          f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_r          f16   cuda:0   2560       \n",
      "blocks.31.att.key.weight          f16   cuda:0   2560  2560 \n",
      "blocks.31.att.value.weight        f16   cuda:0   2560  2560 \n",
      "blocks.31.att.receptance.weight   f16   cuda:0   2560  2560 \n",
      "blocks.31.att.output.weight       f16   cuda:0   2560  2560 \n",
      "blocks.31.ffn.time_mix_k          f16   cuda:0   2560       \n",
      "blocks.31.ffn.time_mix_r          f16   cuda:0   2560       \n",
      "blocks.31.ffn.key.weight          f16   cuda:0   2560 10240 \n",
      "blocks.31.ffn.receptance.weight   f16   cuda:0   2560  2560 \n",
      "blocks.31.ffn.value.weight        f16   cuda:0  10240  2560 \n",
      "ln_out.weight                     f16   cuda:0   2560       \n",
      "ln_out.bias                       f16   cuda:0   2560       \n",
      "head.weight                       f16   cuda:0   2560 50277 \n"
     ]
    }
   ],
   "source": [
    "# load rwkv model\n",
    "model_name_or_path = r'../models/rwkv-4-3b/RWKV-4-Pile-3B-20221110-ctx4096.pth'\n",
    "\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "model = RWKV(model=model_name_or_path, strategy='cuda fp16')\n",
    "# pipeline = PIPELINE(model, r\"rwkv_vocab_v20230424\")\n",
    "pipeline = PIPELINE(model, \"./support/20B_tokenizer.json\")  # v4\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2795bb15-8360-4d8e-bfdd-b9062f0477ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31f2841fa3d4e31b6cfb93d7a1add7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/rwkv/model.py:1113: UserWarning: operator() profile_node %210 : int = prim::profile_ivalue(%output_dtype.7)\n",
      " does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  x, state[i*5+0], state[i*5+1], state[i*5+2], state[i*5+3] = ATT(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved successfully to ./logs/2024-03-01_23-49-57.json\n",
      "log probability sum: 3274.02\n",
      "avg tokens: 1144\n"
     ]
    }
   ],
   "source": [
    "# eval rwkv\n",
    "rwkv_test_data = []\n",
    "rwkv_token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_seq = tokenizer.encode(sample).ids # v4\n",
    "        input_length = len(input_seq)\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, input_length, chunk_size):\n",
    "            input_chunk = input_seq[begin: begin + chunk_size]\n",
    "            \n",
    "\n",
    "            logit = model.forward(input_chunk, None, full_output=True)[0]\n",
    "            \n",
    "            if len(input_chunk) == 1:\n",
    "                logit = logit.unsqueeze(0)\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, torch.tensor(input_chunk).cuda())\n",
    "            \n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        rwkv_token_length_list.append(input_length)\n",
    "        rwkv_test_data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(rwkv_test_data) / len(rwkv_test_data),\n",
    "    'avg tokens': sum(rwkv_token_length_list) / len(rwkv_token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "        \n",
    "print(f'log probability sum: {sum(rwkv_test_data) / len(rwkv_test_data):.2f}')\n",
    "print(f'avg tokens: {sum(rwkv_token_length_list) / len(rwkv_token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b1e508-d32b-4173-b03c-7f308b787680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, pipeline, tokenizer, logit\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c55ca0-30dd-4331-9dbc-531d37a445f3",
   "metadata": {},
   "source": [
    "## Evaluate RWKV(v5/v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56930e55-3876-4a1e-bcf6-798f287caa15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
      "\n",
      "Loading ../models/rwkv_5_3b/RWKV-5-World-3B-v2-20231113-ctx4096.pth ...\n",
      "Model detected: v5.2\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float16, float16], store 33 layers\n",
      "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  65536  2560 \n",
      "blocks.0.ln1.weight               f16   cuda:0   2560       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   2560       \n",
      "blocks.0.ln2.weight               f16   cuda:0   2560       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   2560       \n",
      "blocks.0.att.time_mix_g           f16   cuda:0   2560       \n",
      "blocks.0.att.time_decay           f32   cuda:0     40    64 \n",
      "blocks.0.att.time_first           f32   cuda:0     40    64 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0   2560  2560 \n",
      "blocks.0.att.key.weight           f16   cuda:0   2560  2560 \n",
      "blocks.0.att.value.weight         f16   cuda:0   2560  2560 \n",
      "blocks.0.att.output.weight        f16   cuda:0   2560  2560 \n",
      "blocks.0.att.gate.weight          f16   cuda:0   2560  2560 \n",
      "blocks.0.att.ln_x.weight          f32   cuda:0   2560       \n",
      "blocks.0.att.ln_x.bias            f32   cuda:0   2560       \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   2560       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   2560       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0   2560  8960 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0   2560  2560 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0   8960  2560 \n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f16   cuda:0   2560       \n",
      "blocks.31.ln1.bias                f16   cuda:0   2560       \n",
      "blocks.31.ln2.weight              f16   cuda:0   2560       \n",
      "blocks.31.ln2.bias                f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_k          f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_v          f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_r          f16   cuda:0   2560       \n",
      "blocks.31.att.time_mix_g          f16   cuda:0   2560       \n",
      "blocks.31.att.time_decay          f32   cuda:0     40    64 \n",
      "blocks.31.att.time_first          f32   cuda:0     40    64 \n",
      "blocks.31.att.receptance.weight   f16   cuda:0   2560  2560 \n",
      "blocks.31.att.key.weight          f16   cuda:0   2560  2560 \n",
      "blocks.31.att.value.weight        f16   cuda:0   2560  2560 \n",
      "blocks.31.att.output.weight       f16   cuda:0   2560  2560 \n",
      "blocks.31.att.gate.weight         f16   cuda:0   2560  2560 \n",
      "blocks.31.att.ln_x.weight         f32   cuda:0   2560       \n",
      "blocks.31.att.ln_x.bias           f32   cuda:0   2560       \n",
      "blocks.31.ffn.time_mix_k          f16   cuda:0   2560       \n",
      "blocks.31.ffn.time_mix_r          f16   cuda:0   2560       \n",
      "blocks.31.ffn.key.weight          f16   cuda:0   2560  8960 \n",
      "blocks.31.ffn.receptance.weight   f16   cuda:0   2560  2560 \n",
      "blocks.31.ffn.value.weight        f16   cuda:0   8960  2560 \n",
      "ln_out.weight                     f16   cuda:0   2560       \n",
      "ln_out.bias                       f16   cuda:0   2560       \n",
      "head.weight                       f16   cuda:0   2560 65536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/rwkv5/build.ninja...\n",
      "Building extension module rwkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module rwkv5...\n"
     ]
    }
   ],
   "source": [
    "# load rwkv model\n",
    "model_name_or_path = r'../models/rwkv_5_3b/RWKV-5-World-3B-v2-20231113-ctx4096.pth'\n",
    "\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "model = RWKV(model=model_name_or_path, strategy='cuda fp16')\n",
    "pipeline = PIPELINE(model, r\"rwkv_vocab_v20230424\")\n",
    "# pipeline = PIPELINE(model, \"./models/20B_tokenizer.json\")  # v4\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f0edc1-cfcb-49f8-9118-a1644421ea8d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f912954753e4bde93318ccf5558a1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved successfully to ./logs/2024-03-01_23-52-52.json\n",
      "log probability sum: 2904.71\n",
      "avg tokens: 1138\n"
     ]
    }
   ],
   "source": [
    "# eval rwkv\n",
    "rwkv_test_data = []\n",
    "rwkv_token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_seq = tokenizer.encode(sample)\n",
    "        # input_seq = tokenizer.encode(sample).ids # v4\n",
    "        input_length = len(input_seq)\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, input_length, chunk_size):\n",
    "            input_chunk = input_seq[begin: begin + chunk_size]\n",
    "            \n",
    "\n",
    "            logit = model.forward(input_chunk, None, full_output=True)[0]\n",
    "            \n",
    "            if len(input_chunk) == 1:\n",
    "                logit = logit.unsqueeze(0)\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, torch.tensor(input_chunk).cuda())\n",
    "            \n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        rwkv_token_length_list.append(input_length)\n",
    "        rwkv_test_data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(rwkv_test_data) / len(rwkv_test_data),\n",
    "    'avg tokens': sum(rwkv_token_length_list) / len(rwkv_token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "        \n",
    "print(f'log probability sum: {sum(rwkv_test_data) / len(rwkv_test_data):.2f}')\n",
    "print(f'avg tokens: {sum(rwkv_token_length_list) / len(rwkv_token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6fb1dc-f35c-4abc-ac0b-c0b0eba2c3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, pipeline, tokenizer, logit\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5180f32-7e40-410a-a492-b44051afca5f",
   "metadata": {},
   "source": [
    "## Evaluate Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b06ba51-0c73-41a5-a2d6-27ea674ab632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661eab908b88478fb97528281273ca7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ac8ad9aa0a4299a82199f50a420eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b4943796c64b2daad14353935c65a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 2.795 billion\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "model_name_or_path = r\"stabilityai/stablelm-3b-4e1t\"\n",
    "cache_dir = '../models/temp/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, \n",
    "                                             device_map=\"cuda\", \n",
    "                                             trust_remote_code=True, \n",
    "                                             cache_dir=cache_dir).eval()\n",
    "\n",
    "print_model_parameters_in_billions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b67253f-59c7-4930-974f-baf5298ddb43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3825c678bc0442848e28a2c5a79d9d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved successfully to ./logs/2024-03-01_23-57-35.json\n",
      "log probability sum: 2888.97\n",
      "avg tokens: 1144\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "data = []\n",
    "token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        inputs = tokenizer(sample, return_tensors='pt')\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        seq_length = inputs['input_ids'].shape[-1]\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, seq_length, chunk_size):\n",
    "            \n",
    "            input_chunk = inputs['input_ids'][:, begin: begin + chunk_size]\n",
    "\n",
    "            logit = model.forward(input_ids=input_chunk).logits[0, :, :]\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, input_chunk.squeeze(0))\n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        token_length_list.append(seq_length)\n",
    "        data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(data) / len(data),\n",
    "    'avg tokens': sum(token_length_list) / len(token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "\n",
    "print(f'log probability sum: {sum(data) / len(data):.2f}')\n",
    "print(f'avg tokens: {sum(token_length_list) / len(token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f70e9bce-34bf-4ef7-913c-76252cd0fa97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, logit, inputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b4438-149d-4b2b-9ba1-b037f2d1e558",
   "metadata": {},
   "source": [
    "## Evaluate Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa902d91-01ef-4e91-bc1f-037e425ceb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 2.768 billion\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "model_name_or_path = \"state-spaces/mamba-2.8b-slimpj\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "model = MambaLMHeadModel.from_pretrained(model_name_or_path, device=\"cuda\", dtype=torch.float16)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print_model_parameters_in_billions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "934009ff-50b7-4cc6-91f6-57d28e110852",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb96da1744d4914b1388a71fd9b21c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved successfully to ./logs/2024-03-02_00-00-23.json\n",
      "log probability sum: 3140.14\n",
      "avg tokens: 1144\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "data = []\n",
    "token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        inputs = tokenizer(sample, return_tensors='pt')\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        seq_length = inputs['input_ids'].shape[-1]\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, seq_length, chunk_size):\n",
    "            \n",
    "            input_chunk = inputs['input_ids'][:, begin: begin + chunk_size]\n",
    "\n",
    "            logit = model.forward(input_ids=input_chunk).logits[0, :, :]\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, input_chunk.squeeze(0))\n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        token_length_list.append(seq_length)\n",
    "        data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(data) / len(data),\n",
    "    'avg tokens': sum(token_length_list) / len(token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "\n",
    "print(f'log probability sum: {sum(data) / len(data):.2f}')\n",
    "print(f'avg tokens: {sum(token_length_list) / len(token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e9139aa-a57c-4c06-9214-455fa8a4b2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, logit, inputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663a4f9-3341-48e2-a294-e4a0406b36bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
