{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe646b8b-491e-4272-9cf6-a62e831e59a4",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab115e-f9ff-49cd-92ce-5a5a971385ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d65a62-ddae-45fa-90fb-19e1828e651c",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc237f4-836c-41fe-8b92-1c0c7e5ebd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load from json file\n",
    "\n",
    "data_path = \"./arxiv_pdfs_cs_24_2_2000_to_7000.json\"\n",
    "\n",
    "\n",
    "def load_list_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads a list of strings from a JSON file.\n",
    "\n",
    "    :param file_path: Path of the JSON file to be loaded.\n",
    "    :return: List of strings loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "\n",
    "extracted_texts = load_list_from_json(data_path)\n",
    "\n",
    "print(len(extracted_texts))\n",
    "# print([len(x) for x in extracted_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564bbb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for text in extracted_texts[:100]:\n",
    "    print(text)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3910df-8196-4953-906f-f4c3764dd222",
   "metadata": {},
   "source": [
    "## Now evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa431c6-d1cb-4d02-b9cb-c57e963c6eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 1024\n",
    "log_folder_path = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfa16a-0ace-4aca-b026-63fe0aa107f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_sum(logits, target_token_ids):\n",
    "    shifted_logits = logits[:-1, :]\n",
    "    shifted_targets = target_token_ids[1:]\n",
    "    \n",
    "    log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "    \n",
    "    target_log_probs = -log_probs.gather(1, shifted_targets.unsqueeze(1)).squeeze()\n",
    "    # print(target_log_probs)\n",
    "    \n",
    "    log_sum = torch.sum(target_log_probs, dim=-1)\n",
    "    # print(perplexity_sum)\n",
    "\n",
    "    return log_sum.item()\n",
    "\n",
    "\n",
    "def print_model_parameters_in_billions(model):\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    total_params_billion = total_params / 1e9\n",
    "    \n",
    "    print(f\"Model parameters: {total_params_billion:.3f} billion\")\n",
    "    \n",
    "    \n",
    "def log(data_dict, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        try:\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Directory created at {folder_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    file_name = f\"{timestamp}.json\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data_dict, file, indent=4)\n",
    "        print(f\"Dictionary saved successfully to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dictionary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a088d82-6983-44f9-a2ef-7ecbe24e1c57",
   "metadata": {},
   "source": [
    "## Evaluate RWKV(v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d762b-b4d6-408f-b78d-a7977e2cb69e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rwkv model\n",
    "model_name_or_path = r'../models/rwkv-4-3b/RWKV-4-Pile-3B-20221110-ctx4096.pth'\n",
    "\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "model = RWKV(model=model_name_or_path, strategy='cuda fp16')\n",
    "# pipeline = PIPELINE(model, r\"rwkv_vocab_v20230424\")\n",
    "pipeline = PIPELINE(model, \"./support/20B_tokenizer.json\")  # v4\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795bb15-8360-4d8e-bfdd-b9062f0477ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval rwkv\n",
    "rwkv_test_data = []\n",
    "rwkv_token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_seq = tokenizer.encode(sample).ids # v4\n",
    "        input_length = len(input_seq)\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, input_length, chunk_size):\n",
    "            input_chunk = input_seq[begin: begin + chunk_size]\n",
    "            \n",
    "\n",
    "            logit = model.forward(input_chunk, None, full_output=True)[0]\n",
    "            \n",
    "            if len(input_chunk) == 1:\n",
    "                logit = logit.unsqueeze(0)\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, torch.tensor(input_chunk).cuda())\n",
    "            \n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        rwkv_token_length_list.append(input_length)\n",
    "        rwkv_test_data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(rwkv_test_data) / len(rwkv_test_data),\n",
    "    'avg tokens': sum(rwkv_token_length_list) / len(rwkv_token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "        \n",
    "print(f'log probability sum: {sum(rwkv_test_data) / len(rwkv_test_data):.2f}')\n",
    "print(f'avg tokens: {sum(rwkv_token_length_list) / len(rwkv_token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1e508-d32b-4173-b03c-7f308b787680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, pipeline, tokenizer, logit\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c55ca0-30dd-4331-9dbc-531d37a445f3",
   "metadata": {},
   "source": [
    "## Evaluate RWKV(v5/v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56930e55-3876-4a1e-bcf6-798f287caa15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rwkv model\n",
    "model_name_or_path = r'../models/rwkv_5_3b/RWKV-5-World-3B-v2-20231113-ctx4096.pth'\n",
    "\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "model = RWKV(model=model_name_or_path, strategy='cuda fp16')\n",
    "pipeline = PIPELINE(model, r\"rwkv_vocab_v20230424\")\n",
    "# pipeline = PIPELINE(model, \"./models/20B_tokenizer.json\")  # v4\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0edc1-cfcb-49f8-9118-a1644421ea8d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval rwkv\n",
    "rwkv_test_data = []\n",
    "rwkv_token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_seq = tokenizer.encode(sample)\n",
    "        # input_seq = tokenizer.encode(sample).ids # v4\n",
    "        input_length = len(input_seq)\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, input_length, chunk_size):\n",
    "            input_chunk = input_seq[begin: begin + chunk_size]\n",
    "            \n",
    "\n",
    "            logit = model.forward(input_chunk, None, full_output=True)[0]\n",
    "            \n",
    "            if len(input_chunk) == 1:\n",
    "                logit = logit.unsqueeze(0)\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, torch.tensor(input_chunk).cuda())\n",
    "            \n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        rwkv_token_length_list.append(input_length)\n",
    "        rwkv_test_data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(rwkv_test_data) / len(rwkv_test_data),\n",
    "    'avg tokens': sum(rwkv_token_length_list) / len(rwkv_token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "        \n",
    "print(f'log probability sum: {sum(rwkv_test_data) / len(rwkv_test_data):.2f}')\n",
    "print(f'avg tokens: {sum(rwkv_token_length_list) / len(rwkv_token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fb1dc-f35c-4abc-ac0b-c0b0eba2c3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, pipeline, tokenizer, logit\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5180f32-7e40-410a-a492-b44051afca5f",
   "metadata": {},
   "source": [
    "## Evaluate Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06ba51-0c73-41a5-a2d6-27ea674ab632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model_name_or_path = r\"stabilityai/stablelm-3b-4e1t\"\n",
    "cache_dir = '../models/temp/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, \n",
    "                                             device_map=\"cuda\", \n",
    "                                             trust_remote_code=True, \n",
    "                                             cache_dir=cache_dir).eval()\n",
    "\n",
    "print_model_parameters_in_billions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67253f-59c7-4930-974f-baf5298ddb43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "data = []\n",
    "token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        inputs = tokenizer(sample, return_tensors='pt')\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        seq_length = inputs['input_ids'].shape[-1]\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, seq_length, chunk_size):\n",
    "            \n",
    "            input_chunk = inputs['input_ids'][:, begin: begin + chunk_size]\n",
    "\n",
    "            logit = model.forward(input_ids=input_chunk).logits[0, :, :]\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, input_chunk.squeeze(0))\n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        token_length_list.append(seq_length)\n",
    "        data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(data) / len(data),\n",
    "    'avg tokens': sum(token_length_list) / len(token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "\n",
    "print(f'log probability sum: {sum(data) / len(data):.2f}')\n",
    "print(f'avg tokens: {sum(token_length_list) / len(token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e9bce-34bf-4ef7-913c-76252cd0fa97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, logit, inputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b4438-149d-4b2b-9ba1-b037f2d1e558",
   "metadata": {},
   "source": [
    "## Evaluate Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa902d91-01ef-4e91-bc1f-037e425ceb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "model_name_or_path = \"state-spaces/mamba-2.8b-slimpj\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "model = MambaLMHeadModel.from_pretrained(model_name_or_path, device=\"cuda\", dtype=torch.float16)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print_model_parameters_in_billions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934009ff-50b7-4cc6-91f6-57d28e110852",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "data = []\n",
    "token_length_list = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        inputs = tokenizer(sample, return_tensors='pt')\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        seq_length = inputs['input_ids'].shape[-1]\n",
    "        \n",
    "        neg_log_prob_temp = 0\n",
    "        for begin in range(0, seq_length, chunk_size):\n",
    "            \n",
    "            input_chunk = inputs['input_ids'][:, begin: begin + chunk_size]\n",
    "\n",
    "            logit = model.forward(input_ids=input_chunk).logits[0, :, :]\n",
    "\n",
    "            log_sum = calculate_log_sum(logit, input_chunk.squeeze(0))\n",
    "            neg_log_prob_temp += log_sum\n",
    "\n",
    "        token_length_list.append(seq_length)\n",
    "        data.append(neg_log_prob_temp)\n",
    "        \n",
    "data_dict = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'data_path': data_path,\n",
    "    'neg_log_prob_sum': sum(data) / len(data),\n",
    "    'avg tokens': sum(token_length_list) / len(token_length_list),\n",
    "       }\n",
    "\n",
    "log(data_dict, log_folder_path)\n",
    "\n",
    "print(f'log probability sum: {sum(data) / len(data):.2f}')\n",
    "print(f'avg tokens: {sum(token_length_list) / len(token_length_list):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9139aa-a57c-4c06-9214-455fa8a4b2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, logit, inputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663a4f9-3341-48e2-a294-e4a0406b36bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
