[
    "LaMOT: Language-Guided Multi-Object Tracking\n\u00a7 Abstract Vision-Language MOT is a crucial tracking problem and has drawn increasing attention recently. It aims to track objects based on human language commands, replacing the traditional use of templates or pre-set information from training sets in conventional tracking tasks. Despite various efforts, a key challenge lies in the lack of a clear understanding of why language is used for tracking, which hinders further development in this field. In this paper, we address this challenge by introducing Language-Guided MOT, a unified task framework, along with a corresponding large-scale benchmark, termed LaMOT, which encompasses diverse scenarios and language descriptions. Specially, LaMOT comprises 1,660 sequences from 4 different datasets and aims to unify various Vision-Language MOT tasks while providing a standardized evaluation platform. To ensure high-quality annotations, we manually assign appropriate descriptive texts to each target in every video and conduct careful inspection and correction. To the best of our knowledge, LaMOT is the first benchmark dedicated to Language-Guided MOT. Additionally, we propose a simple yet effective tracker, termed LaMOTer. By establishing a unified task framework, providing challenging benchmarks, and offering insights for future algorithm design and evaluation, we expect to contribute to the advancement of research in Vision-Language MOT. We will release the data at <https://github.com/Nathan-Li123/LaMOT>., Xiaoqiong Liu^3, Luke Liu^4, Heng Fan^3,\u2020, Libo Zhang^2,\u2020,*^1Institute of Software Chinese Academy of Science^2University of Chinese Academy of Science^3University of North Texas^4Intern at University of North Texas\u2020Equal Advising*Corresponding Author \u00a7 INTRODUCTION Multi-Object Tracking (MOT) is an important task in computer vision, which has garnered significant attention, leading to the emergence of various innovative approaches. Recently, there has been a marked surge of interest within the MOT community towards integrating natural language processing into MOT approaches, termed Vision-Language MOT. This integration aims to track areas or targets of interest based on human language instructions. In particular, several approaches and benchmarks (e.g., ) have been proposed, significantly facilitating related research endeavors and advancements on this topic. However, despite these efforts, we argue there is still a misunderstanding of a crucial question: why language is used for tracking? In this paper, we summarize the answer as two key words: flexibility and generality. Vision-Language MOT tasks can be typically classified into two settings: open-vocabulary classname tracking and referring expression tracking (see Fig. ). Although these definitions seem reasonable, they inadvertently restrict the flexibility of natural language. Open-vocabulary classname tracking approaches focus on empowering models to track unknown categories, but they are constrained by the conventional MOT category concept, unable to recognize more complex yet practical language descriptions. On the other hand, referring expression tracking methods aim to ensure that models comprehend closed-set language descriptions, but they struggle when facing open-vocabulary contexts as analyzed in. To this end, we introduce Language-Guided MOT, a unified task framework for Vision-Language MOT. As shown in Fig., Language-Guided MOT combines the advantages of both settings, enabling tracking with any form of language while possessing the ability to recognize open-vocabulary terms. We note that the open-vocabulary capability required by Language-Guided MOT is reflected in the entire vocabulary used in language descriptions, rather than being limited to category names. This maximizes the flexibility of using natural language in MOT. The visualization of two main settings of Vision-Language MOT, i.e., open-vocabulary classname tracking and referring expression tracking. Moreover, Language-Guided MOT enables tracking with any form of language while possessing the ability to recognize open-vocabulary terms. Besides task definition, Vision-Language MOT benchmarks also face severe challenges. First, following existing tasks definitions, vision-langauge benchmarks tend to revolve around only one challenge factor: they either prioritize incorporating open-set categories or lean towards utilizing closed-set descriptions. This weaken the challenges posed by real-world Vision-Language MOT where arbitrary challenges may exist and severely limit the flexibility of natural language. Second, a crucial point is largely overlooked in previous works: video scenarios. Conventional tracking tasks typically rely on templates or predefined information from the training set to determine the targets to be tracked. This directly leads to a significant degradation in model performance when there are noticeable changes in video scenarios, as it is very challenging for the model to ga",
    "\n\u00a7 Abstract In this paper, we tackle the challenge of predicting the unseen walls of a partially observed environment as a set of 2D line segments, conditioned on occupancy grids integrated along the trajectory of a 360 LIDAR sensor. A dataset of such occupancy grids and their corresponding target wall segments is collected by navigating a virtual robot between a set of randomly sampled waypoints in a collection of office-scale floor plans from a university campus. The line segment prediction task is formulated as an autoregressive sequence prediction task, and an attention-based deep network is trained on the dataset. The sequence-based autoregressive formulation is evaluated through predicted information gain, as in frontier-based autonomous exploration, demonstrating significant improvements over both non-predictive estimation and convolution-based image prediction found in the literature. Ablations on key components are evaluated, as well as sensor range and the occupancy grid's metric area. Finally, model generality is validated by predicting walls in a novel floor plan reconstructed on-the-fly in a real-world office environment.Beyond the Frontier: Predicting Unseen Walls from Occupancy Grids by Learning from Floor Plans Ludvig Ericson, Patric Jensfelt This work was supported by the Swedish Research Council. All authors are with the Division of Robotics, Perception and Learning at KTH Royal Institute of Technology, Stockholm, SE-10044, Sweden. For e-mail correspondence, contact ludv@kth.se. IEEE Robotics and Automation Letters. Preprint Version. Accepted May, 2024 L. Ericson, P. Jensfelt: gobble Deep Learning Methods, Planning under Uncertainty, Autonomous Agents, Learning from Experience, Map-Predictive Exploration zhou2021fuel,shrestha2019learned shrestha2019learned,katyal2019uncertainty,zwecher2022integrating,tao2023learning,tao2023seer \u00a7 INTRODUCTION Human and robotic problem-solving approaches differ in dealing with the unknown and predicting the near future. Classical robotic approaches seek exactness at the cost of intuition and foresight, and on the contrary, humans do not meticulously maintain metric maps of their worlds. Even schematics and blueprints, documents explicitly intended to specify technical details, leave room for interpretation. Abstraction seems necessary for our ability to move between the specifics of reality and the generality of ideas and ideals. Replicating this ability to reason abstractly with explicit algorithms has historically proven difficult, but recent advances in learning-based approaches have opened up new avenues and great strides have been made across many subfields of robotics. In this work, floor plans are used as the medium through which abstract reasoning is made possible. Floor plans are the architectural blueprints of our built environment, a distillate of the real world as a tidy set of shapes and symbols encoding the layouts and purposes of rooms, positions of walls, doorways, and windows. Floor plans obey rules, symmetries, and regularities that are impossible to state explicitly, often driven by aesthetic considerations rather than logic. We have previously shown that recent advances in autoregressive language models can be leveraged to produce a generative model over floor plans as sequences of vector graphic instructions. By contrast to single-shot approaches such as, an autoregressive approach casts the floor plan generation task as a series of decisions and their consequences, enabling the model to reason in steps, analogous to the chain-of-thought paradigm in large language models. Autonomous exploration planning is an obvious example of a classical robotics problem where such a prediction model should be immediately applicable; however, we have previously shown that traditional non-predictive exploration planners are not well-suited to using predictions, and predictions can actually have a negative impact on exploration performance. In this article, we have limited the scope to evaluating predictions by the primary variable that they affect in the exploration context: predicted information gain. This paper is structured as follows. In, an model for predicting floor plans from sensor history, dubbed Floorist, is defined. Data modality is the main difference from our previous model, which dealt solely with abstract floor plans. Floorist is instead grounded in the real world by taking a partially observed environment as input in the form of 2D occupancy grids from a 360 LIDAR sensor and predicting the unobserved walls of that environment as line segments. In, a dataset generation method is outlined wherein a virtual robot navigates between randomly sampled waypoints in a collection of annotated floor plans, generating input occupancy grids and their target wall segments. In, cluster-based predicted information gain is defined as in, it is the evaluation metric used in this work, suitable for occupancy grid-based prediction models. In, three predict",
    "An I2I Inpainting Approach for Efficient Channel Knowledge Map Construction\n\u00a7 Abstract Channel knowledge map (CKM) has received widespread attention as an emerging enabling technology for environment-aware wireless communications. It involves the construction of databases containing location-specific channel knowledge, which are then leveraged to facilitate channel state information (CSI) acquisition and transceiver design. In this context, a fundamental challenge lies in efficiently constructing the CKM based on a given wireless propagation environment. Most existing methods are based on stochastic modeling and sequence prediction, which do not fully exploit the inherent physical characteristics of the propagation environment, resulting in low accuracy and high computational complexity. To address these limitations, we propose a Laplacian pyramid (LP)-based CKM construction scheme to predict the channel knowledge at arbitrary locations in a targeted area. Specifically, we first view the channel knowledge as a 2-D image and transform the CKM construction problem into an image-to-image (I2I) inpainting task, which predicts the channel knowledge at a specific location by recovering the corresponding pixel value in the image matrix. Then, inspired by the reversible and closed-form structure of the LP, we show its natural suitability for our task in designing a fast I2I mapping network. For different frequency components of LP decomposition, we design tailored networks accordingly. Besides, to encode the global structural information of the propagation environment, we introduce self-attention and cross-covariance attention mechanisms in different layers, respectively. Finally, experimental results demonstrate that the proposed scheme outperforms the benchmark, achieving higher reconstruction accuracy while with lower computational complexity. Moreover, the proposed approach has a strong generalization ability and can be implemented in different wireless communication scenarios.Li You, Senior Member, IEEE, Jue Wang, Member, IEEE, Xiang-Gen Xia, Fellow, IEEE, and Xiqi Gao, Fellow, IEEE Part of this work was presented in the IEEE WCNC 2024. Zhenzhou Jin, Li You, and Xiqi Gao are with the National Mobile Communications Research Laboratory, Southeast University, Nanjing 210096, China, and also with the Purple Mountain Laboratories, Nanjing 211100, China (e-mail: zzjin@seu.edu.cn; lyou@seu.edu.cn; xqgao@seu.edu.cn). Jue Wang is with School of Information Science and Technology, Nantong University, Nantong 226019, China, and also with Nantong Research Institute for Advanced Communication Technologies, Nantong 226019, China (e-mail: wangjue@ntu.edu.cn). Xiang-Gen Xia is with the Department of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716 USA (e-mail: xxia@ee.udel.edu). Integrated sensing and communications, digital twin, environment-aware wireless communications, channel knowledge map, channel gain map, laplacian pyramid, attention mechanism. \u00a7 INTRODUCTION The deep integration of mobile communication technology and artificial intelligence will drive the evolution of the fifth generation (5G) and beyond the fifth generation (B5G) to the sixth generation (6G) at both the technical and business levels. 6G will push society towards \u201cdigital twin\u201d and \u201csmart ubiquitous\u201d, realizing the integration and interaction between the physical and the virtual world. Emerging applications, such as indoor localization and Wi-Fi sensing, etc., require higher end-to-end information processing capability in 6G networks. In addition to pursuing extremely low latency and other ultra-high performance indicators, it will also build an integrated sensing and communications (ISAC) network. With the rise in the number and density of connected devices, the expansion of antenna array dimensions, and the wider bandwidth usage, ISAC networks are expected to involve ultra-large dimensional wireless channels. Conventional reliance on pilot-based channel training and feedback methods to acquire real-time CSI may have prohibitively high overhead. Note that the propagation environment, such as the geometric location relationship in city or terrain maps, is not only static, but also a key factor that affects the parameters of the channel and the performance of a wireless communication system. As such, for ISAC networks, environment-aware wireless communications have attracted significant research interest and attention in both academia and industry. CKM plays a vital role as a bridge in enabling environment-aware wireless communications, which provides location-specific channel knowledge associated with a potential base station (BS) to any (B2X) pairs. Specifically, CKM, also referred to as channel fingerprint, functions as a site-specific database tagged with the precise locations of both transmitters and receivers. Within this database, essential channel-related details are stored, providing valuable information such as c",
    "EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding\n\u00a7 Abstract We present EgoExo-Fitness, a new full-body action understanding dataset, featuring fitness sequence videos recorded from synchronized egocentric and fixed exocentric (third-person) cameras. Compared with existing full-body action understanding datasets, EgoExo-Fitness not only contains videos from first-person perspectives, but also provides rich annotations. Specifically, two-level temporal boundaries are provided to localize single action videos along with sub-steps of each action. More importantly, EgoExo-Fitness introduces innovative annotations for interpretable action judgement\u2013including technical keypoint verification, natural language comments on action execution, and action quality scores. Combining all of these, EgoExo-Fitness provides new resources to study egocentric and exocentric full-body action understanding across dimensions of \u201cwhat\u201d, \u201cwhen\u201d, and \u201chow well\u201d. To facilitate research on egocentric and exocentric full-body action understanding, we construct benchmarks on a suite of tasks (, action classification, action localization, cross-view sequence verification, cross-view skill determination, and a newly proposed task of guidance-based execution verification), together with detailed analysis. Code and data will be available at <https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main>.EgoExo-Fitness Wei-Jin Huang2,\u2020 An-Lan Wang1, \u2020 Ling-An Zeng1 Jing-Ke Meng1,\u2217 Wei-Shi Zheng1,\u2217 Y.M. Li et al. Sun Yat-sen University South China University of Technology: Project lead. \u2020: Equal key contributions. \u2217: Corresponding author. \u00a7 INTRODUCTION \u00a7 RELATED WORKS \u00a7.\u00a7 Revisiting Current Datasets We will first revisit today's available full-body action understanding datasets and egocentric video datasets. After that, we will introduce the differences between EgoExo-Fitness and today's datasets. Full-body action understanding datasets. Human body movements contain complex motion patterns and technical skills, presenting a series of challenges for Full-Body Action Understanding (FBAU). To address these challenges, datasets like NTU-RGB+D, Human3.6M, Diving48 and FineGym are proposed to enable research on recognizing coarse-and-fine human full-body actions. Beyond recognition, datasets like Diving48-SV and RepCount are present to address tasks (, Sequence Verification and Repetitive Action Counting) that require stronger temporal modeling ability. Note that technical full-body action videos (, diving and vaulting) will reflect human skills. Hence, in recent years, datasets for Action Assessment, like AQA-7, FineDiving, LOGO, are introduced to study the subtle skill differences between action videos. Another branch of datasets focuses on estimating or reconstructing 3D human poses from full-body action videos, achieving the development of Virtual Reality. Though great progress has been achieved, today's full-body action understanding datasets mainly assume that human full-body action videos are captured by exocentric cameras. Such an assumption limits further exploration in more flexible settings. Moreover, some datasets (, WEAR and 1st-basketball ) propose to understand sports and fitness activities from egocentric viewpoints. However, these datasets are limited by their scale and task-specific annotations. Egocentric video datasets. Egocentric Video Understanding (EVU) has great application value for AR/VR and Robotics. Most existing EVU datasets focus on interactive actions: 1) tabletop activities in kitchen or on a static working platform; 2) daily activities interacting with daily objects or individuals. Although recently proposed Ego4D expands beyond interactive activities to a wider variety of daily activities, works on this branch of datasets rarely focus on egocentric full-body action understanding. Another branch of work aims to estimate or reconstruct full-body pose from egocentric videos, and several datasets are released. Different from existing datasets, EgoExo-Fitness features synchronized egocentric and exocentric videos of full-body fitness actions and provides rich annotations (especially novel annotations of interpretable action judgement) for future research on understanding ego- and exo-centric full-body actions across dimensions of \u201cwhat\u201d, \u201cwhen\u201d, and \u201chow well\u201d. It is worth noting that a concurrent large dataset, Ego-Exo4D, also contains ego-exo full-body (physical) action videos and annotations on how well an action is performed. EgoExo-Fitness still has its values: (1) it focuses on a novel scenario (, natural fitness practicing); (2) it provides novel annotations(e.g., technical keypoints verification), supporting the novel task on interpretable action assessment. We will provide detailed comparisons across our work and Ego-Exo4D in and. \u00a7.\u00a7 Revisiting Relevant Tasks In this part, we will present the relationships between the benchmarks of EgoExo-Fitness and relevant tasks. We will further",
    "On the\n\u00a7 Abstract Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision. In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and continuity to parametric functions. We prove that common sum-based models are lower- continuous, with a exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.\u00a7 INTRODUCTION Motivated by a multitude of applications, including molecular systems, social networks, recommendation systems and more, permutation invariant deep learning for both multisets and graphs have gained increasing interest in recent years. This in turn has inspired several theoretical works analyzing common permutation invariant models, and their expressive power and limitations. For multiset data, it is known that simple summation-based multiset functions are injective, and as a result, can approximate all continuous functions on multisets. These results have been discussed and strengthened in many different recent publications. For graph neural networks (GNNs) the situation is more delicate, as all known graph neural networks with polynomial complexity have limited expressive power. Our focus in this paper will be on the Message Passing Neural Network (MPNN) paradigm, which includes a variety of popular GNNs. In their seminal works, prove that MPNNs are at most capable of separating graphs that can be separated by the Weisfeiler-Lehman (WL) graph isomorphism test. Accordingly, maximally expressive MPNNs are those which are able to separate all graph pairs which are WL-separable. In, it is shown that MPNNs which employ injective multiset functions are maximally expressive. The theoretical ability of a permutation invariant network to separate a pair of objects (multisets/graphs) is a necessary condition for all learning tasks which require such separation, e.g., binary classification tasks where the two objects have opposite labels. However, while current theory ensures separation, it tells us little about the separation quality, so that embeddings of \"separable\" objects may be extremely similar, to the extent that in some cases, graphs which can be theoretically separated by maximally expressive MPNNs are completely identical on a computer with fixed finite precision (see, figure 1). To study separation quality of permutation invariant models, we will search for bi- continuity guarantees. For metric spaces (X,d_X) and (Y,d_Y), a function f:X \u2192 Y is \u03b2 upper- continuous and \u03b1 lower- continuous if there exists some positive constants c,C such that c d_X(x,x')^\u03b1\u2264 d_Y(f(x),f(x'))\u2264 C d_X(x,x')^\u03b2, \u2200 x,x'\u2208 X. The upper and lower conditions guarantee that embedding distances in Y will not be much larger, or much smaller, than distances in the original space X (in our case a space of multisets or graphs). A function f will have higher separation quality the closer the exponents are to one. When \u03b2=1 we say f is (upper) Lipschitz, and when \u03b1=1 we say that f is lower Lipschitz. We next describe our main results. We note that all models we consider are upper Lipschitz, so the remainder of this section will focus on lower exponents. Main results: multisets We begin with considering a rather standard approach: multiset functions based on summation of point-wise applied neural networks, as in () below. Recent work has shown that when using smooth activations these functions are injective but not lower-Lipschitz, and when using ReLU activations they are not even injective, and therefore not lower- for any parameter choice of the network. This result motivates our novel definition of 'lower- in expectation' for parametric functions, where the expectation is taken over the parameters. With respect to our new relaxed notion of lower- continuity, we show that ReLU summation multiset networks have an expected lower- exponent of \u03b1=3/2. Surprisingly, while smooth activations lead to injectivity, we find that their expected exponent is much worse: at best it is equal to the maximal number of elements in the multiset data, \u03b1\u2265 n. The relatively moderate exponent \u03b1=3/2 of ReLU networks is guaranteed only when the range of the bias of the network",
    "Multiple Intelligent Reflecting Surfaces Collaborative Wireless Localization System\n\u00a7 Abstract This paper studies a multiple intelligent reflecting surfaces (IRSs) collaborative localization system where multiple semi-passive IRSs are deployed in the network to locate one or more targets based on time-of-arrival. It is assumed that each semi-passive IRS is equipped with reflective elements and sensors, which are used to establish the line-of-sight links from the base station (BS) to multiple targets and process echo signals, respectively. Based on the above model, we derive the Fisher information matrix of the echo signal with respect to the time delay. By employing the chain rule and exploiting the geometric relationship between time delay and position, the Cram\u00e9r-Rao bound (CRB) for estimating the target's Cartesian coordinate position is derived. Then, we propose a two-stage algorithmic framework to minimize CRB in single- and multi-target localization systems by joint optimizing active beamforming at BS, passive beamforming at multiple IRSs and IRS selection. For the single-target case, we derive the optimal closed-form solution for multiple IRSs coefficients design and propose a low-complexity algorithm based on alternating direction method of multipliers to obtain the optimal solution for active beaming design. For the multi-target case, alternating optimization is used to transform the original problem into two subproblems where semi-definite relaxation and successive convex approximation are applied to tackle the quadraticity and indefiniteness in the CRB expression, respectively. Finally, numerical simulation results validate the effectiveness of the proposed algorithm for multiple IRSs collaborative localization system compared to other benchmark schemes as well as the significant performance gains.Z. Zhang, W. Chen, Q. Wu, X. Zhu and J. Chen are with the Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: zhangziheng@sjtu.edu.cn; wenchen@sjtu.edu.cn; xushengzhu@sjtu.edu.cn; qingqingwu@sjtu.edu.cn; laowu3917@sjtu.edu.cn). Z. Li is with the School of Information and Communication Engineering, Xi'an Jiaotong University, Xi'an 710049, China (email: lizhendong@xjtu.edu.cn). Nan Cheng is with the School of Telecommunications Engineering, Xidian University, Xi\u2019an 710071, China (e-mail: nancheng@xidian.edu.cn). (Corresponding author: Wen Chen.) Intelligent reflecting surface, collaborative localization, time-of-arrival, Cram\u00e9r-Rao bound. \u00a7 INTRODUCTION The deep integration of sensing systems into future wireless networks is one of the prominent features of 6G. This integration mainly includes two aspects, one of which is the rise of environment-aware applications such as self-driving, unmanned aerial vehicle express and satellite navigation, and the other is reciprocity of communication and sensing, which means communication signals can be used for localization and imaging while the environmental information obtained through sensing can also improve the accuracy of channel estimation and reduce pilot overhead. However, providing high-precision sensing applications in various complex electromagnetic environments faces the following key challenges. First, high-precision localization services require high-frequency and large-bandwidth sensing signals; however, their propagation suffers from stronger free-space path loss as well as atmospheric absorption and attenuation, resulting in relatively short transmission distances. Then, it is difficult to establish a line-of-sight (LoS) link between the BS and the target due to various obstacles, especially in urban areas. Thus, solving the above challenges is the key to improving sensing performance in future wireless networks. Intelligent reflecting surface (IRS) is a novel technology with the potential to address the aforementioned challenges. By forming a controllable electromagnetic field, it can reconstruct the channel and provide additional LoS links. Due to its low cost, ease of deployment, and advanced integration, it has been widely used in the new generation of wireless communication. Because of the many similarities between communication and sensing systems, there have been some recent research results on how the IRS is applied to assist sensing systems. Specifically, scholars have proposed three main IRS-assisted wireless sensing architectures: fully-passive IRS, semi-passive IRS and IRS-self sensing systems. The main difference between these three lies in the relative position of the receiving antenna and transmitter with respect to the IRS. The performance of semi-passive IRS is optimal in most cases due to its lower path loss, making it one of the mainstream designs. In addition to the design of basic architecture, more research has focused on system design, performance analysis, and metrics selection. Detection probability is an important metric used in detection tasks in sensing. The optimal judgeme",
    "MambaLRP: Explaining Selective State Space Sequence Models\n\u00a7 Abstract Recent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models. Explainable AI, State Space Models, Mamba, Long-Range Dependencies\u00a7 INTRODUCTION Sequence modeling has demonstrated its effectiveness and versatility across a wide variety of tasks and data types, including text, time series, genomics, audio, and computer vision. Recently, there has been a surge of interest in a new class of sequence modeling architectures, known as structured state space sequence models (SSMs). This is due to their ability to process sequences in linear time, as opposed to quadratic time required by the more established Transformer architectures. The recent Mamba architecture, a prominent and widely adopted instance of state space models, has demonstrated competitive predictive performance on a variety of sequence modeling tasks across domains and applications, while scaling linearly with sequence length. As Mamba models, and more generally SSMs, are rapidly being adopted into real-world applications, ensuring their transparency is crucial. This enables inspection beyond test set accuracy and uncovering various forms of biases, including `Clever-Hans' effects. It is particularly important in high-risk domains such as medicine, where the prediction behavior must be robust under real-world conditions and aligned with human understanding. The field of Explainable AI focuses on developing faithful model explanations that attribute predictions to relevant features and has shown success in explaining many highly nonlinear models such as convolutional networks, or attention-based Transformer models. Conceptual steps involved in the design of MambaLRP. (a) Take as a starting point a basic LRP procedure, equivalent to Gradient\u00d7Input. (b) Analyze layers in which the conservation property is violated. (c) Rework the relevance propagation strategy at those layers to achieve conservation. The resulting MambaLRP method enables efficient and faithful explanations. Explaining the predictions of Mamba models is however challenging due to their highly non-linear and recurrent structure. A recent study suggests viewing these models as attention-based models, enabling the use of attention-based explanation methods. Yet, the explanations produced by attention-based techniques are often unreliable and exposed to potential misalignment between input features and attention scores. As an alternative, Layer-wise Relevance Propagation (LRP) decomposes the model function with the goal of explicitly identifying the relevance of input features by applying purposely designed propagation rules at each layer. A distinguishing feature of LRP is its adherence to a conservation axiom, which prevents the artificial amplification or suppression of feature relevance in the backward pass. LRP has been demonstrated to produce faithful explanations across various domains (e.g. ). Nevertheless, the peculiarities of the Mamba architecture are not addressed by the existing LRP procedures, which may lead to the violation of the conservation property and result in unreliable explanations. In this work, we present MambaLRP, a novel approach to integrate LRP into the Mamba architecture. By examining the relevance propagation process across Mamba layers through the lens of conservation, we pinpoint layers within the Mamba architecture that need to be addressed specifically. We propose a novel relevance propagation strategy for these layers, grounded in the conservation axiom, that is theoretically sound, straightforward to implement and computationally efficient. Through a number of quantitative evaluations, we show that the proposed MambaLRP approach allows to robustly deliver the desired high",
    "\u00a7 Abstract Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp.[ When Will Gradient Regularization Be Harmful? equal* Yang Zhaothu Hao Zhangthu Xiuyuan Huthu thuDepartment of Electronic Engineering, Tsinghua University Hao Zhanghaozhang@tsinghua.edu.cn Yang Zhaozhao-yang@tsinghua.edu.cn Machine Learning, ICML 0.3in ] \u00a7 INTRODUCTION Advancements in computational hardware have catalyzed the design of modern deep neural networks such as the Transformers, characterized by an extraordinarily vast number of parameters, far exceeding their predecessors. In this context, regularization techniques emerge as a more pivotal role to resist overfitting in training these over-parameterized networks. -0.05in Comparison of test error rates (lower values are preferable) of the ViT-B model on the Cifar10 dataset under Base training, GR, and our three proposed GR warmup strategies. All the training instances have also applied the LR warmup. Notably, the performance with LR warmup and normal GR (red line) can be worse compared to training with only LR warmup (blue line). Recent studies highlight the gradient regularization (GR) as an effective regularization strategy. By imposing an additional penalty concerning gradient norm atop the loss function, this technique deliberately biases the optimization process towards the attainment of flat minima, fostering better generalization. Meanwhile, it has been revealed that a strong association exists between GR and Sharpness-Aware Minimization (SAM) family, which posits SAM as a special parameter configuration in the first-order solution of GR. However, despite its practical utility, the scope and limitations of GR are yet to be fully understood, particularly in terms of establishing when it can be beneficial or safe to apply this technique. We find that GR can lead to serious performance degeneration (see fig: accuracy) in the specific scenarios of adaptive optimization such as Adam and RMSProp. With both our empirical observations and theoretical analysis, we find that the biased estimation introduced in GR can induce the instability and divergence in gradient statistics of adaptive optimizers at the initial stage of training, especially with a learning rate warmup technique which originally aims to benefit gradient statistics. Notably, this issue tends to become more severe as the complexity of the model increases. To mitigate this issue, we draw inspirations from the idea of warmup techniques, and propose three GR warmup strategies: \u03bb-warmup, r-warmup and zero-warmup GR. Each of the three strategies can relax the GR effect during warmup course in certain ways to ensure the accuracy of gradient statistics. Then, training reverts to normal GR for the rest of training, allowing the optimization to fully enjoy the performance gain derived from the GR. Finally, we empirically confirm that all the three GR warmup strategies can not only successfully avoid this issue but further enhance the performance for almost all training cases. Of these strategies, the zero-warmup GR can give the best improvements, significant outperforming the baseline. \u00a7 BACKGROUND \u00a7.\u00a7 Gradient Regularization: an Overview Gradient regularization typically aims to impose an additional gradient norm penalty on top of the loss function, L^(gr)() = L() + \u03bb ||\u2207_ L()||_2 where is the model parameter and \u03bb denotes the regularization degree, effectively controlling the extent to which this regularization influences the overall process. Note that some research choose to penalize the ||\u2207_ L()||_2^2, which actually results in the same effect as ||\u2207_ L()||_2. In practical applications, Hessian-free techniques are employed to approximate the involved gradient L^(gr)(), g^(gr) = (1 - \u03bb/r) \u2207_ L() + \u03bb/r\u2207_ L( + ) with = r \u00b7\u2207_ L()/|| \u2207_ L() ||_2 where the parameter r denotes as a small, positive scalar representing neighborhood perturbation,",
    "Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing\n\u00a7 Abstract In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators\u2019 (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs\u2019 stance judgments match humans\u2019. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.\u00a7 INTRODUCTION A primary challenge in automated social media analysis is translating unstructured textual data, i.e., users' posts, into a structured format, such as categories or numerical values (; ) to correctly interpret what a post is saying, so that the content can be quantitatively analyzed. Stance detection has emerged as a promising solution to this challenge, offering an automatic approach to classifying the opinion or position that the user has expressed in a post, typically one of several options such as favoring, opposing, or being neutral with respect to the topic, or being irrelevant. This method has become increasingly common in social media analysis for conducting social research, i.e., extracting public opinion from social media postings (;; ). Historically, stance detection has used traditional machine learning techniques such as Support Vector Machines (SVM) and logistic regression, alongside deep learning methods like Convolutional Neural Networks (CNN) and Long Short-Term Memory networks (LSTM) (; ). More recently, researchers have made significant strides with the introduction of pre-trained language models such as BERT, which have substantially improved prediction accuracy (). However, the requirement for high-quality annotation data remains critical. The most prevalent approach to annotating stance involves crowd-sourcing on platforms like Amazon Mechanical Turk. Yet, the process can be both time-consuming and expensive. In response to these challenges, researchers have begun to explore stance prediction based on only a few or even no training examples, i.e., Few- or Zero-Shot learning (;;; ). The OpenAI team's introduction of the concept that language models can act as Few-Shot learners () highlights the remarkable ability of these large language models (LLMs) to understand complex language. This development opens new possibilities for using LLMs to accurately extract or infer users\u2019 opinions and knowledge from their posts. Building upon the established capabilities of ChatGPT, recent research has investigated the application of LLMs in annotating stances of social media texts (; ). These studies demonstrate that ChatGPT attains state-of-the-art performance in multiple stance detection benchmarks () and, in certain annotation tasks, surpasses human annotators, as validated by expert assessments (). further argues that LLMs could revolutionize computational social science by serving as efficient Zero-Shot data annotators within human annotation teams, potentially transforming the approach to stance detection tasks. However, these findings and resultant conclusions warrant further examination. highlight potential data contamination in ChatGPT's evaluations on widely used stance detection benchmarks, casting doubt on their validity. Furthermore, indicate that Few- or Zero-Shot learning with LLMs may not consistently outperform supervised methods in certain datasets. A broader evaluation by reveals that ChatGPT does not universally outperform the SOTA model across all tasks. Corroborating this, research indicates that LLM performance is task- or dataset-specific. Given the diversity of LLMs currently available, a comprehensive understanding of when and why LLMs excel in stance annotation tasks, and which specific tasks they are best suited for, remains elusive. One factor contributing to the inconsistent results of LLMs in stance annotation tasks may be their varied ability to process texts with different d",
    "Formatting Instructions For the NeurIPS 2024 Track on Datasets and Benchmarks\n\u00a7 Abstract Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for cross-lingual form understanding. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original dataset and implementations of baseline methods are available at <https://sprateam-ustc.github.io/SRFUND>.\u00a7 INTRODUCTION In the United States, billions of individuals and businesses submit tax returns annually,[] and globally, hundreds of billions of parcels are distributed each year,[] most of which are accompanied by invoices and delivery notes. Although these documents vary in format, they are all considered forms, which serve as crucial information mediums widely used in global information and merchandise exchange. Compared to storage formats like camera-captured images or scanned documents, digitizing original forms into structured text aids in reducing storage space and facilitates information dissemination. Consequently, there has been a growing practical demand in recent years for understanding information within forms, including both textual content and document structures across various layouts and languages. With the rapid development of document processing technologies, significant progress has been made in the field of form understanding, along with the establishment of a series of benchmark datasets. However, none of these existing datasets have established the global and hierarchical structural dependencies considering all elements at different granularity, including words, text lines, and entities within the forms. [Word level] [Text-line level] [Entity level] [Item table level] [Overall form structure based on entities.] Multiple granularity of annotations and supported tasks on SRFUND. To enhance the applicability of form understanding tasks in hierarchical structure recovery, we introduce the SRFUND, a multilingual form structure reconstruction dataset. The SRFUND dataset comprises 1,592 form images across eight languages, with each language contributing 199 images. As illustrated in Figure, each form image is manually annotated with the locations and text contents of every word, text-line, and entity. After identifying each independent entity, we categorize these entities into four classes including Header, Question, Answer, and Other, which is consistent with the FUNSD dataset definitions. Moreover, all entities in the form are annotated with their hierarchical dependencies, allowing us to reconstruct the global form structure. For the multi-item table regions frequently found in forms, we have specifically annotated the positions of these tables, including their table headers, and grouped each line item within these tables individually. The refined annotations of SRFUND support the evaluation of form structure reconstruction tasks at different granularities. We conducted benchmark tests on several tasks using representative methods from three categories: vision-only, language-only, and multi-modal approaches. These tasks include (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. Detailed experimental settings and results are presented in Sec.. \u00a7 RELATED WORK Prior research has divided document structure tasks into two main categories: physical layout analysis and logical structure analysis"
]