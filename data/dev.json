[
    "LaMOT: Language-Guided Multi-Object Tracking\n\u00a7 Abstract Vision-Language MOT is a crucial tracking problem and has drawn increasing attention recently. It aims to track objects based on human language commands, replacing the traditional use of templates or pre-set information from training sets in conventional tracking tasks. Despite various efforts, a key challenge lies in the lack of a clear understanding of why language is used for tracking, which hinders further development in this field. In this paper, we address this challenge by introducing Language-Guided MOT, a unified task framework, along with a corresponding large-scale benchmark, termed LaMOT, which encompasses diverse scenarios and language descriptions. Specially, LaMOT comprises 1,660 sequences from 4 different datasets and aims to unify various Vision-Language MOT tasks while providing a standardized evaluation platform. To ensure high-quality annotations, we manually assign appropriate descriptive texts to each target in every video and conduct careful inspection and correction. To the best of our knowledge, LaMOT is the first benchmark dedicated to Language-Guided MOT. Additionally, we propose a simple yet effective tracker, termed LaMOTer. By establishing a unified task framework, providing challenging benchmarks, and offering insights for future algorithm design and evaluation, we expect to contribute to the advancement of research in Vision-Language MOT. We will release the data at <https://github.com/Nathan-Li123/LaMOT>., Xiaoqiong Liu^3, Luke Liu^4, Heng Fan^3,\u2020, Libo Zhang^2,\u2020,*^1Institute of Software Chinese Academy of Science^2University of Chinese Academy of Science^3University of North Texas^4Intern at University of North Texas\u2020Equal Advising*Corresponding Author \u00a7 INTRODUCTION Multi-Object Tracking (MOT) is an important task in computer vision, which has garnered significant attention, leading to the emergence of various innovative approaches. Recently, there has been a marked surge of interest within the MOT community towards integrating natural language processing into MOT approaches, termed Vision-Language MOT. This integration aims to track areas or targets of interest based on human language instructions. In particular, several approaches and benchmarks (e.g., ) have been proposed, significantly facilitating related research endeavors and advancements on this topic. However, despite these efforts, we argue there is still a misunderstanding of a crucial question: why language is used for tracking? In this paper, we summarize the answer as two key words: flexibility and generality. Vision-Language MOT tasks can be typically classified into two settings: open-vocabulary classname tracking and referring expression tracking (see Fig. ). Although these definitions seem reasonable, they inadvertently restrict the flexibility of natural language. Open-vocabulary classname tracking approaches focus on empowering models to track unknown categories, but they are constrained by the conventional MOT category concept, unable to recognize more complex yet practical language descriptions. On the other hand, referring expression tracking methods aim to ensure that models comprehend closed-set language descriptions, but they struggle when facing open-vocabulary contexts as analyzed in. To this end, we introduce Language-Guided MOT, a unified task framework for Vision-Language MOT. As shown in Fig., Language-Guided MOT combines the advantages of both settings, enabling tracking with any form of language while possessing the ability to recognize open-vocabulary terms. We note that the open-vocabulary capability required by Language-Guided MOT is reflected in the entire vocabulary used in language descriptions, rather than being limited to category names. This maximizes the flexibility of using natural language in MOT. The visualization of two main settings of Vision-Language MOT, i.e., open-vocabulary classname tracking and referring expression tracking. Moreover, Language-Guided MOT enables tracking with any form of language while possessing the ability to recognize open-vocabulary terms. Besides task definition, Vision-Language MOT benchmarks also face severe challenges. First, following existing tasks definitions, vision-langauge benchmarks tend to revolve around only one challenge factor: they either prioritize incorporating open-set categories or lean towards utilizing closed-set descriptions. This weaken the challenges posed by real-world Vision-Language MOT where arbitrary challenges may exist and severely limit the flexibility of natural language. Second, a crucial point is largely overlooked in previous works: video scenarios. Conventional tracking tasks typically rely on templates or predefined information from the training set to determine the targets to be tracked. This directly leads to a significant degradation in model performance when there are noticeable changes in video scenarios, as it is very challenging for the model to ga",
    "\n\u00a7 Abstract In this paper, we tackle the challenge of predicting the unseen walls of a partially observed environment as a set of 2D line segments, conditioned on occupancy grids integrated along the trajectory of a 360 LIDAR sensor. A dataset of such occupancy grids and their corresponding target wall segments is collected by navigating a virtual robot between a set of randomly sampled waypoints in a collection of office-scale floor plans from a university campus. The line segment prediction task is formulated as an autoregressive sequence prediction task, and an attention-based deep network is trained on the dataset. The sequence-based autoregressive formulation is evaluated through predicted information gain, as in frontier-based autonomous exploration, demonstrating significant improvements over both non-predictive estimation and convolution-based image prediction found in the literature. Ablations on key components are evaluated, as well as sensor range and the occupancy grid's metric area. Finally, model generality is validated by predicting walls in a novel floor plan reconstructed on-the-fly in a real-world office environment.Beyond the Frontier: Predicting Unseen Walls from Occupancy Grids by Learning from Floor Plans Ludvig Ericson, Patric Jensfelt This work was supported by the Swedish Research Council. All authors are with the Division of Robotics, Perception and Learning at KTH Royal Institute of Technology, Stockholm, SE-10044, Sweden. For e-mail correspondence, contact ludv@kth.se. IEEE Robotics and Automation Letters. Preprint Version. Accepted May, 2024 L. Ericson, P. Jensfelt: gobble Deep Learning Methods, Planning under Uncertainty, Autonomous Agents, Learning from Experience, Map-Predictive Exploration zhou2021fuel,shrestha2019learned shrestha2019learned,katyal2019uncertainty,zwecher2022integrating,tao2023learning,tao2023seer \u00a7 INTRODUCTION Human and robotic problem-solving approaches differ in dealing with the unknown and predicting the near future. Classical robotic approaches seek exactness at the cost of intuition and foresight, and on the contrary, humans do not meticulously maintain metric maps of their worlds. Even schematics and blueprints, documents explicitly intended to specify technical details, leave room for interpretation. Abstraction seems necessary for our ability to move between the specifics of reality and the generality of ideas and ideals. Replicating this ability to reason abstractly with explicit algorithms has historically proven difficult, but recent advances in learning-based approaches have opened up new avenues and great strides have been made across many subfields of robotics. In this work, floor plans are used as the medium through which abstract reasoning is made possible. Floor plans are the architectural blueprints of our built environment, a distillate of the real world as a tidy set of shapes and symbols encoding the layouts and purposes of rooms, positions of walls, doorways, and windows. Floor plans obey rules, symmetries, and regularities that are impossible to state explicitly, often driven by aesthetic considerations rather than logic. We have previously shown that recent advances in autoregressive language models can be leveraged to produce a generative model over floor plans as sequences of vector graphic instructions. By contrast to single-shot approaches such as, an autoregressive approach casts the floor plan generation task as a series of decisions and their consequences, enabling the model to reason in steps, analogous to the chain-of-thought paradigm in large language models. Autonomous exploration planning is an obvious example of a classical robotics problem where such a prediction model should be immediately applicable; however, we have previously shown that traditional non-predictive exploration planners are not well-suited to using predictions, and predictions can actually have a negative impact on exploration performance. In this article, we have limited the scope to evaluating predictions by the primary variable that they affect in the exploration context: predicted information gain. This paper is structured as follows. In, an model for predicting floor plans from sensor history, dubbed Floorist, is defined. Data modality is the main difference from our previous model, which dealt solely with abstract floor plans. Floorist is instead grounded in the real world by taking a partially observed environment as input in the form of 2D occupancy grids from a 360 LIDAR sensor and predicting the unobserved walls of that environment as line segments. In, a dataset generation method is outlined wherein a virtual robot navigates between randomly sampled waypoints in a collection of annotated floor plans, generating input occupancy grids and their target wall segments. In, cluster-based predicted information gain is defined as in, it is the evaluation metric used in this work, suitable for occupancy grid-based prediction models. In, three predict",
    "An I2I Inpainting Approach for Efficient Channel Knowledge Map Construction\n\u00a7 Abstract Channel knowledge map (CKM) has received widespread attention as an emerging enabling technology for environment-aware wireless communications. It involves the construction of databases containing location-specific channel knowledge, which are then leveraged to facilitate channel state information (CSI) acquisition and transceiver design. In this context, a fundamental challenge lies in efficiently constructing the CKM based on a given wireless propagation environment. Most existing methods are based on stochastic modeling and sequence prediction, which do not fully exploit the inherent physical characteristics of the propagation environment, resulting in low accuracy and high computational complexity. To address these limitations, we propose a Laplacian pyramid (LP)-based CKM construction scheme to predict the channel knowledge at arbitrary locations in a targeted area. Specifically, we first view the channel knowledge as a 2-D image and transform the CKM construction problem into an image-to-image (I2I) inpainting task, which predicts the channel knowledge at a specific location by recovering the corresponding pixel value in the image matrix. Then, inspired by the reversible and closed-form structure of the LP, we show its natural suitability for our task in designing a fast I2I mapping network. For different frequency components of LP decomposition, we design tailored networks accordingly. Besides, to encode the global structural information of the propagation environment, we introduce self-attention and cross-covariance attention mechanisms in different layers, respectively. Finally, experimental results demonstrate that the proposed scheme outperforms the benchmark, achieving higher reconstruction accuracy while with lower computational complexity. Moreover, the proposed approach has a strong generalization ability and can be implemented in different wireless communication scenarios.Li You, Senior Member, IEEE, Jue Wang, Member, IEEE, Xiang-Gen Xia, Fellow, IEEE, and Xiqi Gao, Fellow, IEEE Part of this work was presented in the IEEE WCNC 2024. Zhenzhou Jin, Li You, and Xiqi Gao are with the National Mobile Communications Research Laboratory, Southeast University, Nanjing 210096, China, and also with the Purple Mountain Laboratories, Nanjing 211100, China (e-mail: zzjin@seu.edu.cn; lyou@seu.edu.cn; xqgao@seu.edu.cn). Jue Wang is with School of Information Science and Technology, Nantong University, Nantong 226019, China, and also with Nantong Research Institute for Advanced Communication Technologies, Nantong 226019, China (e-mail: wangjue@ntu.edu.cn). Xiang-Gen Xia is with the Department of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716 USA (e-mail: xxia@ee.udel.edu). Integrated sensing and communications, digital twin, environment-aware wireless communications, channel knowledge map, channel gain map, laplacian pyramid, attention mechanism. \u00a7 INTRODUCTION The deep integration of mobile communication technology and artificial intelligence will drive the evolution of the fifth generation (5G) and beyond the fifth generation (B5G) to the sixth generation (6G) at both the technical and business levels. 6G will push society towards \u201cdigital twin\u201d and \u201csmart ubiquitous\u201d, realizing the integration and interaction between the physical and the virtual world. Emerging applications, such as indoor localization and Wi-Fi sensing, etc., require higher end-to-end information processing capability in 6G networks. In addition to pursuing extremely low latency and other ultra-high performance indicators, it will also build an integrated sensing and communications (ISAC) network. With the rise in the number and density of connected devices, the expansion of antenna array dimensions, and the wider bandwidth usage, ISAC networks are expected to involve ultra-large dimensional wireless channels. Conventional reliance on pilot-based channel training and feedback methods to acquire real-time CSI may have prohibitively high overhead. Note that the propagation environment, such as the geometric location relationship in city or terrain maps, is not only static, but also a key factor that affects the parameters of the channel and the performance of a wireless communication system. As such, for ISAC networks, environment-aware wireless communications have attracted significant research interest and attention in both academia and industry. CKM plays a vital role as a bridge in enabling environment-aware wireless communications, which provides location-specific channel knowledge associated with a potential base station (BS) to any (B2X) pairs. Specifically, CKM, also referred to as channel fingerprint, functions as a site-specific database tagged with the precise locations of both transmitters and receivers. Within this database, essential channel-related details are stored, providing valuable information such as c",
    "EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding\n\u00a7 Abstract We present EgoExo-Fitness, a new full-body action understanding dataset, featuring fitness sequence videos recorded from synchronized egocentric and fixed exocentric (third-person) cameras. Compared with existing full-body action understanding datasets, EgoExo-Fitness not only contains videos from first-person perspectives, but also provides rich annotations. Specifically, two-level temporal boundaries are provided to localize single action videos along with sub-steps of each action. More importantly, EgoExo-Fitness introduces innovative annotations for interpretable action judgement\u2013including technical keypoint verification, natural language comments on action execution, and action quality scores. Combining all of these, EgoExo-Fitness provides new resources to study egocentric and exocentric full-body action understanding across dimensions of \u201cwhat\u201d, \u201cwhen\u201d, and \u201chow well\u201d. To facilitate research on egocentric and exocentric full-body action understanding, we construct benchmarks on a suite of tasks (, action classification, action localization, cross-view sequence verification, cross-view skill determination, and a newly proposed task of guidance-based execution verification), together with detailed analysis. Code and data will be available at <https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main>.EgoExo-Fitness Wei-Jin Huang2,\u2020 An-Lan Wang1, \u2020 Ling-An Zeng1 Jing-Ke Meng1,\u2217 Wei-Shi Zheng1,\u2217 Y.M. Li et al. Sun Yat-sen University South China University of Technology: Project lead. \u2020: Equal key contributions. \u2217: Corresponding author. \u00a7 INTRODUCTION \u00a7 RELATED WORKS \u00a7.\u00a7 Revisiting Current Datasets We will first revisit today's available full-body action understanding datasets and egocentric video datasets. After that, we will introduce the differences between EgoExo-Fitness and today's datasets. Full-body action understanding datasets. Human body movements contain complex motion patterns and technical skills, presenting a series of challenges for Full-Body Action Understanding (FBAU). To address these challenges, datasets like NTU-RGB+D, Human3.6M, Diving48 and FineGym are proposed to enable research on recognizing coarse-and-fine human full-body actions. Beyond recognition, datasets like Diving48-SV and RepCount are present to address tasks (, Sequence Verification and Repetitive Action Counting) that require stronger temporal modeling ability. Note that technical full-body action videos (, diving and vaulting) will reflect human skills. Hence, in recent years, datasets for Action Assessment, like AQA-7, FineDiving, LOGO, are introduced to study the subtle skill differences between action videos. Another branch of datasets focuses on estimating or reconstructing 3D human poses from full-body action videos, achieving the development of Virtual Reality. Though great progress has been achieved, today's full-body action understanding datasets mainly assume that human full-body action videos are captured by exocentric cameras. Such an assumption limits further exploration in more flexible settings. Moreover, some datasets (, WEAR and 1st-basketball ) propose to understand sports and fitness activities from egocentric viewpoints. However, these datasets are limited by their scale and task-specific annotations. Egocentric video datasets. Egocentric Video Understanding (EVU) has great application value for AR/VR and Robotics. Most existing EVU datasets focus on interactive actions: 1) tabletop activities in kitchen or on a static working platform; 2) daily activities interacting with daily objects or individuals. Although recently proposed Ego4D expands beyond interactive activities to a wider variety of daily activities, works on this branch of datasets rarely focus on egocentric full-body action understanding. Another branch of work aims to estimate or reconstruct full-body pose from egocentric videos, and several datasets are released. Different from existing datasets, EgoExo-Fitness features synchronized egocentric and exocentric videos of full-body fitness actions and provides rich annotations (especially novel annotations of interpretable action judgement) for future research on understanding ego- and exo-centric full-body actions across dimensions of \u201cwhat\u201d, \u201cwhen\u201d, and \u201chow well\u201d. It is worth noting that a concurrent large dataset, Ego-Exo4D, also contains ego-exo full-body (physical) action videos and annotations on how well an action is performed. EgoExo-Fitness still has its values: (1) it focuses on a novel scenario (, natural fitness practicing); (2) it provides novel annotations(e.g., technical keypoints verification), supporting the novel task on interpretable action assessment. We will provide detailed comparisons across our work and Ego-Exo4D in and. \u00a7.\u00a7 Revisiting Relevant Tasks In this part, we will present the relationships between the benchmarks of EgoExo-Fitness and relevant tasks. We will further",
    "On the\n\u00a7 Abstract Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision. In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and continuity to parametric functions. We prove that common sum-based models are lower- continuous, with a exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.\u00a7 INTRODUCTION Motivated by a multitude of applications, including molecular systems, social networks, recommendation systems and more, permutation invariant deep learning for both multisets and graphs have gained increasing interest in recent years. This in turn has inspired several theoretical works analyzing common permutation invariant models, and their expressive power and limitations. For multiset data, it is known that simple summation-based multiset functions are injective, and as a result, can approximate all continuous functions on multisets. These results have been discussed and strengthened in many different recent publications. For graph neural networks (GNNs) the situation is more delicate, as all known graph neural networks with polynomial complexity have limited expressive power. Our focus in this paper will be on the Message Passing Neural Network (MPNN) paradigm, which includes a variety of popular GNNs. In their seminal works, prove that MPNNs are at most capable of separating graphs that can be separated by the Weisfeiler-Lehman (WL) graph isomorphism test. Accordingly, maximally expressive MPNNs are those which are able to separate all graph pairs which are WL-separable. In, it is shown that MPNNs which employ injective multiset functions are maximally expressive. The theoretical ability of a permutation invariant network to separate a pair of objects (multisets/graphs) is a necessary condition for all learning tasks which require such separation, e.g., binary classification tasks where the two objects have opposite labels. However, while current theory ensures separation, it tells us little about the separation quality, so that embeddings of \"separable\" objects may be extremely similar, to the extent that in some cases, graphs which can be theoretically separated by maximally expressive MPNNs are completely identical on a computer with fixed finite precision (see, figure 1). To study separation quality of permutation invariant models, we will search for bi- continuity guarantees. For metric spaces (X,d_X) and (Y,d_Y), a function f:X \u2192 Y is \u03b2 upper- continuous and \u03b1 lower- continuous if there exists some positive constants c,C such that c d_X(x,x')^\u03b1\u2264 d_Y(f(x),f(x'))\u2264 C d_X(x,x')^\u03b2, \u2200 x,x'\u2208 X. The upper and lower conditions guarantee that embedding distances in Y will not be much larger, or much smaller, than distances in the original space X (in our case a space of multisets or graphs). A function f will have higher separation quality the closer the exponents are to one. When \u03b2=1 we say f is (upper) Lipschitz, and when \u03b1=1 we say that f is lower Lipschitz. We next describe our main results. We note that all models we consider are upper Lipschitz, so the remainder of this section will focus on lower exponents. Main results: multisets We begin with considering a rather standard approach: multiset functions based on summation of point-wise applied neural networks, as in () below. Recent work has shown that when using smooth activations these functions are injective but not lower-Lipschitz, and when using ReLU activations they are not even injective, and therefore not lower- for any parameter choice of the network. This result motivates our novel definition of 'lower- in expectation' for parametric functions, where the expectation is taken over the parameters. With respect to our new relaxed notion of lower- continuity, we show that ReLU summation multiset networks have an expected lower- exponent of \u03b1=3/2. Surprisingly, while smooth activations lead to injectivity, we find that their expected exponent is much worse: at best it is equal to the maximal number of elements in the multiset data, \u03b1\u2265 n. The relatively moderate exponent \u03b1=3/2 of ReLU networks is guaranteed only when the range of the bias of the network",
    "Multiple Intelligent Reflecting Surfaces Collaborative Wireless Localization System\n\u00a7 Abstract This paper studies a multiple intelligent reflecting surfaces (IRSs) collaborative localization system where multiple semi-passive IRSs are deployed in the network to locate one or more targets based on time-of-arrival. It is assumed that each semi-passive IRS is equipped with reflective elements and sensors, which are used to establish the line-of-sight links from the base station (BS) to multiple targets and process echo signals, respectively. Based on the above model, we derive the Fisher information matrix of the echo signal with respect to the time delay. By employing the chain rule and exploiting the geometric relationship between time delay and position, the Cram\u00e9r-Rao bound (CRB) for estimating the target's Cartesian coordinate position is derived. Then, we propose a two-stage algorithmic framework to minimize CRB in single- and multi-target localization systems by joint optimizing active beamforming at BS, passive beamforming at multiple IRSs and IRS selection. For the single-target case, we derive the optimal closed-form solution for multiple IRSs coefficients design and propose a low-complexity algorithm based on alternating direction method of multipliers to obtain the optimal solution for active beaming design. For the multi-target case, alternating optimization is used to transform the original problem into two subproblems where semi-definite relaxation and successive convex approximation are applied to tackle the quadraticity and indefiniteness in the CRB expression, respectively. Finally, numerical simulation results validate the effectiveness of the proposed algorithm for multiple IRSs collaborative localization system compared to other benchmark schemes as well as the significant performance gains.Z. Zhang, W. Chen, Q. Wu, X. Zhu and J. Chen are with the Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: zhangziheng@sjtu.edu.cn; wenchen@sjtu.edu.cn; xushengzhu@sjtu.edu.cn; qingqingwu@sjtu.edu.cn; laowu3917@sjtu.edu.cn). Z. Li is with the School of Information and Communication Engineering, Xi'an Jiaotong University, Xi'an 710049, China (email: lizhendong@xjtu.edu.cn). Nan Cheng is with the School of Telecommunications Engineering, Xidian University, Xi\u2019an 710071, China (e-mail: nancheng@xidian.edu.cn). (Corresponding author: Wen Chen.) Intelligent reflecting surface, collaborative localization, time-of-arrival, Cram\u00e9r-Rao bound. \u00a7 INTRODUCTION The deep integration of sensing systems into future wireless networks is one of the prominent features of 6G. This integration mainly includes two aspects, one of which is the rise of environment-aware applications such as self-driving, unmanned aerial vehicle express and satellite navigation, and the other is reciprocity of communication and sensing, which means communication signals can be used for localization and imaging while the environmental information obtained through sensing can also improve the accuracy of channel estimation and reduce pilot overhead. However, providing high-precision sensing applications in various complex electromagnetic environments faces the following key challenges. First, high-precision localization services require high-frequency and large-bandwidth sensing signals; however, their propagation suffers from stronger free-space path loss as well as atmospheric absorption and attenuation, resulting in relatively short transmission distances. Then, it is difficult to establish a line-of-sight (LoS) link between the BS and the target due to various obstacles, especially in urban areas. Thus, solving the above challenges is the key to improving sensing performance in future wireless networks. Intelligent reflecting surface (IRS) is a novel technology with the potential to address the aforementioned challenges. By forming a controllable electromagnetic field, it can reconstruct the channel and provide additional LoS links. Due to its low cost, ease of deployment, and advanced integration, it has been widely used in the new generation of wireless communication. Because of the many similarities between communication and sensing systems, there have been some recent research results on how the IRS is applied to assist sensing systems. Specifically, scholars have proposed three main IRS-assisted wireless sensing architectures: fully-passive IRS, semi-passive IRS and IRS-self sensing systems. The main difference between these three lies in the relative position of the receiving antenna and transmitter with respect to the IRS. The performance of semi-passive IRS is optimal in most cases due to its lower path loss, making it one of the mainstream designs. In addition to the design of basic architecture, more research has focused on system design, performance analysis, and metrics selection. Detection probability is an important metric used in detection tasks in sensing. The optimal judgeme",
    "MambaLRP: Explaining Selective State Space Sequence Models\n\u00a7 Abstract Recent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models. Explainable AI, State Space Models, Mamba, Long-Range Dependencies\u00a7 INTRODUCTION Sequence modeling has demonstrated its effectiveness and versatility across a wide variety of tasks and data types, including text, time series, genomics, audio, and computer vision. Recently, there has been a surge of interest in a new class of sequence modeling architectures, known as structured state space sequence models (SSMs). This is due to their ability to process sequences in linear time, as opposed to quadratic time required by the more established Transformer architectures. The recent Mamba architecture, a prominent and widely adopted instance of state space models, has demonstrated competitive predictive performance on a variety of sequence modeling tasks across domains and applications, while scaling linearly with sequence length. As Mamba models, and more generally SSMs, are rapidly being adopted into real-world applications, ensuring their transparency is crucial. This enables inspection beyond test set accuracy and uncovering various forms of biases, including `Clever-Hans' effects. It is particularly important in high-risk domains such as medicine, where the prediction behavior must be robust under real-world conditions and aligned with human understanding. The field of Explainable AI focuses on developing faithful model explanations that attribute predictions to relevant features and has shown success in explaining many highly nonlinear models such as convolutional networks, or attention-based Transformer models. Conceptual steps involved in the design of MambaLRP. (a) Take as a starting point a basic LRP procedure, equivalent to Gradient\u00d7Input. (b) Analyze layers in which the conservation property is violated. (c) Rework the relevance propagation strategy at those layers to achieve conservation. The resulting MambaLRP method enables efficient and faithful explanations. Explaining the predictions of Mamba models is however challenging due to their highly non-linear and recurrent structure. A recent study suggests viewing these models as attention-based models, enabling the use of attention-based explanation methods. Yet, the explanations produced by attention-based techniques are often unreliable and exposed to potential misalignment between input features and attention scores. As an alternative, Layer-wise Relevance Propagation (LRP) decomposes the model function with the goal of explicitly identifying the relevance of input features by applying purposely designed propagation rules at each layer. A distinguishing feature of LRP is its adherence to a conservation axiom, which prevents the artificial amplification or suppression of feature relevance in the backward pass. LRP has been demonstrated to produce faithful explanations across various domains (e.g. ). Nevertheless, the peculiarities of the Mamba architecture are not addressed by the existing LRP procedures, which may lead to the violation of the conservation property and result in unreliable explanations. In this work, we present MambaLRP, a novel approach to integrate LRP into the Mamba architecture. By examining the relevance propagation process across Mamba layers through the lens of conservation, we pinpoint layers within the Mamba architecture that need to be addressed specifically. We propose a novel relevance propagation strategy for these layers, grounded in the conservation axiom, that is theoretically sound, straightforward to implement and computationally efficient. Through a number of quantitative evaluations, we show that the proposed MambaLRP approach allows to robustly deliver the desired high",
    "\u00a7 Abstract Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp.[ When Will Gradient Regularization Be Harmful? equal* Yang Zhaothu Hao Zhangthu Xiuyuan Huthu thuDepartment of Electronic Engineering, Tsinghua University Hao Zhanghaozhang@tsinghua.edu.cn Yang Zhaozhao-yang@tsinghua.edu.cn Machine Learning, ICML 0.3in ] \u00a7 INTRODUCTION Advancements in computational hardware have catalyzed the design of modern deep neural networks such as the Transformers, characterized by an extraordinarily vast number of parameters, far exceeding their predecessors. In this context, regularization techniques emerge as a more pivotal role to resist overfitting in training these over-parameterized networks. -0.05in Comparison of test error rates (lower values are preferable) of the ViT-B model on the Cifar10 dataset under Base training, GR, and our three proposed GR warmup strategies. All the training instances have also applied the LR warmup. Notably, the performance with LR warmup and normal GR (red line) can be worse compared to training with only LR warmup (blue line). Recent studies highlight the gradient regularization (GR) as an effective regularization strategy. By imposing an additional penalty concerning gradient norm atop the loss function, this technique deliberately biases the optimization process towards the attainment of flat minima, fostering better generalization. Meanwhile, it has been revealed that a strong association exists between GR and Sharpness-Aware Minimization (SAM) family, which posits SAM as a special parameter configuration in the first-order solution of GR. However, despite its practical utility, the scope and limitations of GR are yet to be fully understood, particularly in terms of establishing when it can be beneficial or safe to apply this technique. We find that GR can lead to serious performance degeneration (see fig: accuracy) in the specific scenarios of adaptive optimization such as Adam and RMSProp. With both our empirical observations and theoretical analysis, we find that the biased estimation introduced in GR can induce the instability and divergence in gradient statistics of adaptive optimizers at the initial stage of training, especially with a learning rate warmup technique which originally aims to benefit gradient statistics. Notably, this issue tends to become more severe as the complexity of the model increases. To mitigate this issue, we draw inspirations from the idea of warmup techniques, and propose three GR warmup strategies: \u03bb-warmup, r-warmup and zero-warmup GR. Each of the three strategies can relax the GR effect during warmup course in certain ways to ensure the accuracy of gradient statistics. Then, training reverts to normal GR for the rest of training, allowing the optimization to fully enjoy the performance gain derived from the GR. Finally, we empirically confirm that all the three GR warmup strategies can not only successfully avoid this issue but further enhance the performance for almost all training cases. Of these strategies, the zero-warmup GR can give the best improvements, significant outperforming the baseline. \u00a7 BACKGROUND \u00a7.\u00a7 Gradient Regularization: an Overview Gradient regularization typically aims to impose an additional gradient norm penalty on top of the loss function, L^(gr)() = L() + \u03bb ||\u2207_ L()||_2 where is the model parameter and \u03bb denotes the regularization degree, effectively controlling the extent to which this regularization influences the overall process. Note that some research choose to penalize the ||\u2207_ L()||_2^2, which actually results in the same effect as ||\u2207_ L()||_2. In practical applications, Hessian-free techniques are employed to approximate the involved gradient L^(gr)(), g^(gr) = (1 - \u03bb/r) \u2207_ L() + \u03bb/r\u2207_ L( + ) with = r \u00b7\u2207_ L()/|| \u2207_ L() ||_2 where the parameter r denotes as a small, positive scalar representing neighborhood perturbation,",
    "Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing\n\u00a7 Abstract In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators\u2019 (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs\u2019 stance judgments match humans\u2019. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.\u00a7 INTRODUCTION A primary challenge in automated social media analysis is translating unstructured textual data, i.e., users' posts, into a structured format, such as categories or numerical values (; ) to correctly interpret what a post is saying, so that the content can be quantitatively analyzed. Stance detection has emerged as a promising solution to this challenge, offering an automatic approach to classifying the opinion or position that the user has expressed in a post, typically one of several options such as favoring, opposing, or being neutral with respect to the topic, or being irrelevant. This method has become increasingly common in social media analysis for conducting social research, i.e., extracting public opinion from social media postings (;; ). Historically, stance detection has used traditional machine learning techniques such as Support Vector Machines (SVM) and logistic regression, alongside deep learning methods like Convolutional Neural Networks (CNN) and Long Short-Term Memory networks (LSTM) (; ). More recently, researchers have made significant strides with the introduction of pre-trained language models such as BERT, which have substantially improved prediction accuracy (). However, the requirement for high-quality annotation data remains critical. The most prevalent approach to annotating stance involves crowd-sourcing on platforms like Amazon Mechanical Turk. Yet, the process can be both time-consuming and expensive. In response to these challenges, researchers have begun to explore stance prediction based on only a few or even no training examples, i.e., Few- or Zero-Shot learning (;;; ). The OpenAI team's introduction of the concept that language models can act as Few-Shot learners () highlights the remarkable ability of these large language models (LLMs) to understand complex language. This development opens new possibilities for using LLMs to accurately extract or infer users\u2019 opinions and knowledge from their posts. Building upon the established capabilities of ChatGPT, recent research has investigated the application of LLMs in annotating stances of social media texts (; ). These studies demonstrate that ChatGPT attains state-of-the-art performance in multiple stance detection benchmarks () and, in certain annotation tasks, surpasses human annotators, as validated by expert assessments (). further argues that LLMs could revolutionize computational social science by serving as efficient Zero-Shot data annotators within human annotation teams, potentially transforming the approach to stance detection tasks. However, these findings and resultant conclusions warrant further examination. highlight potential data contamination in ChatGPT's evaluations on widely used stance detection benchmarks, casting doubt on their validity. Furthermore, indicate that Few- or Zero-Shot learning with LLMs may not consistently outperform supervised methods in certain datasets. A broader evaluation by reveals that ChatGPT does not universally outperform the SOTA model across all tasks. Corroborating this, research indicates that LLM performance is task- or dataset-specific. Given the diversity of LLMs currently available, a comprehensive understanding of when and why LLMs excel in stance annotation tasks, and which specific tasks they are best suited for, remains elusive. One factor contributing to the inconsistent results of LLMs in stance annotation tasks may be their varied ability to process texts with different d",
    "Formatting Instructions For the NeurIPS 2024 Track on Datasets and Benchmarks\n\u00a7 Abstract Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for cross-lingual form understanding. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original dataset and implementations of baseline methods are available at <https://sprateam-ustc.github.io/SRFUND>.\u00a7 INTRODUCTION In the United States, billions of individuals and businesses submit tax returns annually,[] and globally, hundreds of billions of parcels are distributed each year,[] most of which are accompanied by invoices and delivery notes. Although these documents vary in format, they are all considered forms, which serve as crucial information mediums widely used in global information and merchandise exchange. Compared to storage formats like camera-captured images or scanned documents, digitizing original forms into structured text aids in reducing storage space and facilitates information dissemination. Consequently, there has been a growing practical demand in recent years for understanding information within forms, including both textual content and document structures across various layouts and languages. With the rapid development of document processing technologies, significant progress has been made in the field of form understanding, along with the establishment of a series of benchmark datasets. However, none of these existing datasets have established the global and hierarchical structural dependencies considering all elements at different granularity, including words, text lines, and entities within the forms. [Word level] [Text-line level] [Entity level] [Item table level] [Overall form structure based on entities.] Multiple granularity of annotations and supported tasks on SRFUND. To enhance the applicability of form understanding tasks in hierarchical structure recovery, we introduce the SRFUND, a multilingual form structure reconstruction dataset. The SRFUND dataset comprises 1,592 form images across eight languages, with each language contributing 199 images. As illustrated in Figure, each form image is manually annotated with the locations and text contents of every word, text-line, and entity. After identifying each independent entity, we categorize these entities into four classes including Header, Question, Answer, and Other, which is consistent with the FUNSD dataset definitions. Moreover, all entities in the form are annotated with their hierarchical dependencies, allowing us to reconstruct the global form structure. For the multi-item table regions frequently found in forms, we have specifically annotated the positions of these tables, including their table headers, and grouped each line item within these tables individually. The refined annotations of SRFUND support the evaluation of form structure reconstruction tasks at different granularities. We conducted benchmark tests on several tasks using representative methods from three categories: vision-only, language-only, and multi-modal approaches. These tasks include (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. Detailed experimental settings and results are presented in Sec.. \u00a7 RELATED WORK Prior research has divided document structure tasks into two main categories: physical layout analysis and logical structure analysis",
    "Domain-specific\n\u00a7 Abstract This study explores the application of large language models (LLMs) with callable tools in energy and power engineering domain, focusing on gas path analysis of gas turbines. We developed a dual-agent tool-calling process to integrate expert knowledge, predefined tools, and LLM reasoning. We evaluated various LLMs, including LLama3, Qwen1.5 and GPT. Smaller models struggled with tool usage and parameter extraction, while larger models demonstrated favorable capabilities. All models faced challenges with complex, multi-component problems. Based on the test results, we infer that LLMs with nearly 100 billion parameters could meet professional scenario requirements with fine-tuning and advanced prompt design. Continued development are likely to enhance their accuracy and effectiveness, paving the way for more robust AI-driven solutions.\u00a7 INTRODUCTION Artificial intelligence (AI) has seen extensive application across various domains, including energy and power engineering, where it holds significant promise for enhancing efficiency, reliability, and sustainability. Within the field of numerical computation, AI techniques have been effectively utilized to partly replace conventional numerical methods across a wide range of energy systems, aiding in the design and optimization of critical processes, such as structural strength and heat transfer. Similarly, in power equipment signal analysis and fault diagnosis, AI algorithms have facilitated anomaly detection and predictive maintenance, thereby reducing downtime and enhancing operational efficiency. Despite these successes, conventional AI tools often lack the theoretical foundation necessary to comprehensively address the complexities of energy systems. The black-box nature of many AI models hinders their interpretability, raising concerns regarding their trustworthiness and accountability in critical energy infrastructure applications. Recently, the advent of large language models (LLMs) like GPT, LLaMA, and GLM has revolutionized artificial intelligence, particularly in natural language processing (NLP). These models excel in generating human-like text, understanding context, and performing complex tasks. Concurrently, enhancement methods such as retrieval-augmented generation (RAG), agent with tools, and advanced prompting strategies like Chain-of-Thought and Tree-of-Thoughts have been proposed. These advancements address some issues of hallucinations and unexplainability in LLMs, making them more robust and trustworthy for practical applications. In this context, we explored the feasibility of integrating LLMs into a domain-specific application scenarios, i.e. energy and power science and engineering, by focusing on a classic issue in gas turbine operation and maintenance: gas path analysis. Our method involved crafting specific prompts and a dual-agent tool-calling process to guide LLMs in parsing tool parameters. We defined energy equations as tools for each component and employed chain thinking prompts to guide LLM reasoning and solve performance parameters. Testing across different LLMs provided insights into their capabilities for understanding, thinking, and reasoning, offering valuable perspectives for further research and applications. \u00a7 BACKGROUND \u00a7.\u00a7 Large Language Models Large language models (LLMs) epitomize a notable stride in artificial intelligence, particularly within the realm of natural language processing, hinging on neural networks and the transformer architecture for text processing and generation. Neural networks, intricate arrangements of interconnected neurons, adeptly transform input data through learned weights, thereby enabling multifaceted representations. A seminal advancement in LLMs is the integration of attention mechanisms, pivotal for discerning specific elements within input data, capturing extensive contextual dependencies. Central to LLMs, the transformer architecture comprises encoder and decoder layers, with the former processing input text and the latter generating output, leveraging self-attention mechanisms to gauge word relevance and contextual nuances. Beyond their fundamental architecture, LLMs offer distinct advantages over traditional AI methods, including their aptitude for coherent text generation and proficiency in reasoning tasks, bolstered by extensive training data. Techniques such as attention visualization enhance interpretability, shedding light on the model's underlying processing mechanisms. Recent years have witnessed a proliferation of LLM implementations, each characterized by unique architectural nuances, training datasets, and methodologies, engendering discernible disparities in their natural language comprehension and logical reasoning proficiencies. Our research evaluates multiple large language models from various organizations, elucidating their nuanced capabilities in parsing prompts and executing logical reasoning tasks for fundamental thermodynamics engineering ",
    "\n\u00a7 Abstract Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces , a Morphological cOmpound Target Interaction Graph dataset that comprises Cell Painting features for 11,000 genes and 3,600 compounds along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. resources are available at <https://github.com/carpenter-singh-lab/motive>.\u00a7 INTRODUCTION High quality graph benchmarking datasets propel graph machine learning (ML) research. Providing a diversity of domains, tasks, and evaluation methods, they allow for rigorous and extensive explorations of structured learning methods. Still, gaps remain in the space of scalability, network sparsity, and generalizability under realistic data splits. These challenges are particularly relevant in the biological domain. The representation of the rich heterogeneity between entities\u2014compounds, genes, proteins, diseases, phenotypes, side effects, and more\u2014is a nontrivial task due to their varied units and terminology and highly complex relational structure. This makes biological data an apt, challenging, and bettering application for graph ML. Next, effectively predicting drug-target interactions (DTIs), the relationships between chemical compounds and their protein targets, remains a pressing research area due to its relevance to drug discovery, drug repurposing, understanding side effects, and virtual screening. The DTI task is challenging due to the shortage of clean perturbational data and unspecificity of these interactions. Even as structure-based methods such as AlphaFold3 are increasingly accomplished at making DTI predictions, they are mainly based on molecular characteristics. Experimental data uniquely captures complex biological interactions; the morphological profiles, feature vectors which capture a cell's appearance, from the Cell Painting (CP) assay have been shown to model mechanism of action, toxicity, and more. To address the challenges of graph ML, biological data representation, and DTI, we introduce a publicly available dataset,, which enhances a graph of compound and gene relations with features from the JUMP Cell Painting dataset. As there is currently no compound-gene graph dataset containing empirical node features, will be extremely useful for inductive graph learning (generalizing to newly connected nodes), cold start recommendations (generalizing to isolated nodes), and zero-shot scenarios (generalizing to isolated node pairs). In many domains, making predictions for least-known entities are the most useful real-world applications. Thus, we accompany with a rigorous framework of data splitting, loading, and evaluation. This work advances both DTI by incorporating a new modality of information and the strength of graph ML, as it rises to the challenge of knowledge generalization for inductive link prediction. \u00a7 RELATED WORK Although many graph-based datasets exist, notes that there is a trade off between scale and availability of node features. The Open Graph Benchmark Library thus contributed, and, all large-scale, feature-based link prediction datasets. The node features in each of these datasets are 58- or 128-dimensional and are a one-hot representation of the protein type in or -based representations of an author's publications or of a paper's contents in and respectively. The benchmarking results showed a continued reliance on model learning from previous connections rather than features, as evidenced by the high performance of embeddings, and indicated a need for richer features. The authors also reported that the graph neural network (GNN) models underperformed in the link prediction task when using mini-batch training rather than whole batch and called for improvement in this area for future scalability when learning on large datasets. In addition, the latter two datasets split their data by time metadata associated with each edge, and did not explore cold start splits. Evidently, the field still requires graph datasets that 1) are large scale, 2) include information rich features, 3) are accompanied by graph-based data splits (not metadata-based), and 4) are flexibly trained with mini-batch sampling. We prioritized all four of these goals during the curation of and in our exp",
    "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction\n\u00a7 Abstract Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness[Audio samples: https://Chenxuey20.github.io/CoLM-DSR].\u00a7 INTRODUCTION Dysarthria is a prevalent type of speech disorder that is commonly observed in individuals with neuromotor conditions such as Parkinson's disease and cerebral palsy. This condition results in a significant deterioration in speech quality and voice characteristics from normal speech patterns, which greatly hampers dysarthria patients' daily communication with their family members or caregivers. Dysarthric speech reconstruction (DSR) is a highly effective approach that seeks to improve the speech intelligibility, naturalness, and preserve the original speaker\u2019s timbre by transforming dysarthric speech into normal speech. The task of DSR is a complex endeavor that has garnered significant research attention. The voice banking-based method collects pre-recorded normal speeches from dysarthric patients before their speech abilities deteriorate to develop personalized text-to-speech (TTS) systems, but its applicability is limited to individuals with available normal speech data. The voice conversion (VC) based techniques aim to modify dysarthric speech signals to improve intelligibility and naturalness while preserving the content, such as rule-based VC, and statistical VC approaches. Recently, an end-to-end VC based DSR system has been proposed, which involves distilling a speech encoder from a pre-trained automatic speech recognition (ASR) model to replace the text encoder in a sequence-to-sequence (seq2seq) TTS system. Compared to a cascaded system that relies on ASR results for TTS, it does not restrict intermediate representations to text characters and can generate speech with lower errors and higher fidelity. Motivated by the prosody and timbre modeling in TTS systems, additional components, such as a prosody corrector and speaker encoder, have been introduced to further enhance prosody and speaker similarity. To improve the speech intelligence for patients with severe dysarthria, as well as for speech captured from complex, noisy acoustic environments, a multi-modal framework is first proposed. Two multi-modal encoders are designed and compared to utilize visual information, e.g., lip movements, as additional clues for reconstructing the highly abnormal pronunciations. In order to address the issue of training inefficiency due to complex training strategies and cascaded pipelines, Unit-DSR is proposed to use the discrete speech units extracted from HuBERT for the generation of a normal speech waveform. Though significant progress has been made, most existing works has focused primarily on improving the speech intelligibility. However, the speaker similarity and prosody naturalness, which are also crucial to a patient's sense of self-identity and fluent expression, still leave a lot to be desired. In most real-world application scenarios, it is crucial for DSR models to exhibit quick adaptation abilities to new dysarthric patients with limited data, which is difficult for existing speaker encoder based DSR systems. With the development of advanced prompting-based language model (LM) in the field of text analysis and audio processing, some zero-shot TTS frameworks have shown strong in-context learning capabilities and diverse outputs with improved speaker similarity and speech naturalness, which treat TTS as a language model task with audio codecs as an intermediate representation instead of the traditional mel-spectrogram. Inspired by the success of neural codec language modeling in zero-shot TTS, this paper proposes a codec LM based multi-modal DSR system by leveraging neural codec language modeling with the large, diverse, and multi-speaker normal speech data to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Firstly, a multi-modal content encoder is adopted to extract the robust phoneme embeddings ",
    "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation\n\u00a7 Abstract We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23% each and speaker similarity by 5% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5\u00d7 faster than real-time.\u00a7 INTRODUCTION Speech-to-speech translation (S2ST) has the potential to transform the way we communicate with others who do not speak the same language. The simplest way to perform S2ST automatically is to chain automatic speech recognition (ASR) with text-to-text machine translation (MT) followed by text-to-speech synthesis (TTS), in what is called a \u201ccascaded\" system. However, cascaded systems are slow and do not take full advantage of non-textual information imparted by the audio modality. For example, the tone of the input audio could help choose between several words with meanings close to \"sorry\" in the target language. In recent years, models that translate directly to speech in the target language and can be optimized end-to-end have outperformed cascaded systems. These works rely on intermediary representations of text in the target language, such as discrete acoustic units or phonemes. Systems like AudioPalm and VioLA use a single decoder-only transformer architecture, while others like SeamlessM4T, UniTY, and Translatotron2 use separate encoder, decoder, and synthesizer components. In this work, we study direct S2ST systems with separate encoder, decoder, and synthesizer modules. To make translated communication more natural, much work has been done to enable S2ST systems to output speech in the same voice, emotion, and prosody as their input. This is known as zero-shot voice cloning or speaker preservation. Some prior works add architectural components separately trained to capture speaker characteristics. Others attempt to capture these characteristics implicitly while training on pairs of utterances with the same speaking style and expression. We take this approach but improve on prior works by pretraining the synthesizer on diverse voices to enable our model to learn speaker preservation with less data. To make S2ST systems as close to real monolingual interaction as possible, it is critical to run at low latency in a streaming context (i.e. the system starts speaking before the input speaker is done talking). The rare S2ST systems addressing this challenge still operate with over 2.5 seconds of ending delay. While we do not tackle streaming directly in this work, we believe an important step to reduce system delay is to reduce model latency via parameter-efficient, low-latency, direct S2ST. Developments in direct S2ST research are heavily influenced by work on TTS systems. Recently, diffusion models such as Voicebox and NaturalSpeech2 have been shown to work well for speech synthesis. Diffusion models are attractive because they can produce diverse audio, pretrain on unlabeled audio data, and run non-autoregressively. Despite their great potential, existing works have yet to use such audio diffusion models for S2ST. We introduce DiffuseST, a direct S2ST system that translates from many languages into English with a novel diffusion-based synthesizer that can perform low-latency S2ST with speaker preservation given as few as 3 seconds of input audio. The main contributions of this work are as follows: 1) We are the first work we know of to use a diffusion synthesizer for S2ST. We show our synthesizer is capable of zero-shot speaker preservation using implicit extraction of speaker characteristics, improving audio quality metrics by 23% and speaker similarity by 5% over a baseline. 2) We show the feasibility of S2ST with a much smaller architecture than previous works, enabling 5\u00d7 faster than real-time inference and facilitating future work on streaming. 3) We are one of the first S2ST works to rely only on public data while still training on over 1k hours of audio, making our work large-scale but more reproducible than prior research. \u00a7 RELATED WORK In this section, we review prior S2ST and TTS research most relevant to our work. We draw the most architectural inspiration from Translatotron2 and Seamless, both direct, zero-shot voice cloning S2ST models. Both train on artificially-generated speaker-preserving label audios generated by TTS systems, but Seamless also trains on automatically aligned utterances mined from a massive multilingual corpus using the SONAR algorithm. While Seamless uses a separate expressivity module to capture s",
    "Consistent Update Synthesis via Privatized Beliefs\nSchl\u00f6gltschloegl@ecs.tuwien.ac.at Kuznetsrkuznets@ecs.tuwien.ac.at Cignaralegiorgio.cignarale@tuwien.ac.at Institute of Computer Engineering, TU Wien, Vienna, Austria Kripke models are an effective and widely used tool for representing epistemic attitudes of agents in multi-agent systems, including distributed systems. Dynamic Epistemic Logic (DEL) adds communication in the form of model transforming updates. Private communication is key in distributed systems as processes exchanging (potentially corrupted) information about their private local state should not be detectable by any other processes. This focus on privacy clashes with the standard DEL assumption for which updates are applied to the whole Kripke model, which is usually commonly known by all agents, potentially leading to information leakage. In addition, a commonly known model cannot minimize the corruption of agents' local states due to fault information dissemination. The contribution of this paper is twofold: (I) To represent leak-free agent-to-agent communication, we introduce a way to synthesize an action model which stratifies a pointed Kripke model into private agent-clusters, each representing the local knowledge of the processes: Given a goal formula \u03c6 representing the effect of private communication, we provide a procedure to construct an action model that (a) makes the goal formula true, (b) maintain consistency of agents' beliefs, if possible, without causing \u201cunrelated\u201d beliefs (minimal change) thus minimizing the corruption of local states in case of inconsistent information. (II) We introduce a new operation between pointed Kripke models and pointed action models called pointed updates which, unlike the product update operation of DEL, maintain only the subset of the world-event pairs that are reachable from the point, without unnecessarily blowing up the model size.[This research was funded in whole or in part by the Austrian Science Fund (FWF) project ByzDEL [10.55776/P33600]. ] \u00a7 INTRODUCTION Epistemic logic (EL) has been extremely successful in modeling epistemic and doxastic attitudes of agents and groups in multi-agent systems, including distributed systems. Dynamic epistemic logic (DEL) upgrades EL by introducing model transforming modalities called updates. Relational structures such as action models in Action Model Logic (AML) and arrow update models in the Generalized Arrow Update Logic (GAUL) are used to represent the evolution of agents' uncertainty under information change in complex communication scenarios. GAUL and AML have proved to be equally update expressive. Thus, without loss of generality, we use the term \u201cupdate models\u201d to refer to either action models of AML or arrow update models of GAUL. In the update synthesis task, the aim is to (i) find whether there exists an update model that makes a given goal formula \u03c6 true and (ii) construct that update model from \u03c6. Existing synthesis methods typically work in a language extended with quantifiers over updates, such as the Arbitrary Action Modal Logic and Arbitrary Arrow Update Modal Logic and do not address the issue of minimal change as a result of the update. While it is possible to construct AML or GAUL update models representing completely private communication (see, e.g., ), there is no standardized update synthesis procedure for it. In addition, most existing synthesis methods do not address belief consistency preservation. Our paper is further motivated by two different yet intertwined issues: * As argued by Artemov, in multi-agent settings, common knowledge of the model (CKM) is required by agents in order to compute higher-order beliefs of other agents. While his argument focuses on uncertainties about facts, it can also be extended to agents' uncertainty about attitudes of other agents. The underlying problem is that, in multi-agent Kripke models, there is an implicit ontological distinction between two kinds of possible worlds: (a) worlds that are actually possible (AP) and (b) worlds that are only virtually possible (VP). While the former worlds constitute, for a given agent, the arena in which the actual world might lie, the latter kind of worlds are considered only to the extent of computing other agent's beliefs. Artemov argues that without the CKM (comprising both APs and VPs), agents would not be able to compute such higher-order beliefs. Furthermore, avoiding the CKM assumption improves tolerance against local state corruption: upon receiving corrupted information from a faulty agent, a (correct) agent might only deem the local state of the sender as corrupted, instead of being forced to consider a larger part of the accessible Kripke model inconsistent. * Update models do not naturally represent private agent-to-agent communication, as the product update operation typical of DEL is applied in principle to the full product of all the world-event pairs whose preconditions are matched. In this s",
    "In this paper, we propose a formulation of temporal planning in which plans are to be interpreted as a high-level specification of the desired behavior of a control system, in which one can discern several discrete devices or agents that need to act in a coordinated way. Figure locates planning algorithms (planners) in the context of Control Systems as the component that provides a reference signal r(t) that informs a suitably designed controller component. Planning and control take place at two different time-scales. Planners are bound to a planning cycle of variable duration that is as long as the controller takes to implement a plan, or until whatever point in time a supervisory system, not shown in Figure and either automated or directly operated by a human, interrupts execution. Controllers are bound to a control cycle, whose duration is fixed and its value chosen as part of the design. Controllers compute control signals u(t), which are meant to drive the behavior, or output signal y(t), of the plant. Generally, y(t) is not directly observable, so a state estimation component is needed that integrates information collected about changes in y(t) over time to produce a state signal x(t). The control function g(\u00b7, \u00b7) is subject to several constraints. To wit, it can only utilize the information obtained by the state estimation procedure; it must minimize some measure of tracking error between the required behavior r(t) and the (indirectly observed) actual behavior y(t), and it must do so within a given time limit that defines the duration of a control cycles. Solutions to g(\u00b7, \u00b7) are thus expected to compensate for any disturbance. Disturbances are a general concept in Control Theory covering a vast collection of phenomena ranging from non-determinism in the plant, e.g. delays of changes in y(t) due to changes in u(t), to misalignment between reality and the abstractions of and assumptions on crucial characteristics of the dynamics of non-planning components in Fig., that the planner uses to design r(t). The analytical framework to study and verify formal properties of systems like those in Fig. is that of Hybrid System Theory, but in this work we do not concern ourselves with the research questions that arise from supervisory components interrupting the current plan and switching to another one, and thus omit further discussion of the hybrid nature of these systems. \u00a7.\u00a7 Domain Theories from System Models Refinement of Figure in which we identify two new sub-systems in the executive control (supervisor and control) and physical system models (plant and measurement). The diagram also makes explicit the possibility of plans defining the control for multiple types of autonomous systems that need to act in a co-ordinated manner. See text for details and discussion. We address now the provenance of the domain theories of interest to our research, that aim at representing in operationally meaningful ways the structure of states and inputs to dynamical systems with a structure like the one shown in Figure. Equipped with these, it then becomes possible to encode facts and assumptions about dynamical systems in a symbolic manner, enabling general but suitably engineered algorithms to reason about the existence of arbitrarily complex properties, such as the existence of plans. We follow loosely the presentation in. A model of a dynamical system is given by a differential or integral equation that relates so-called input signals, for instance force and torque for a mechanical system, to output signals, like position, orientation or rotational velocity. Any such system can in turn be considered a component into a larger one. A model of a system is thus given by a function of the form S: X\u2192 Y, where X = Y = \u211d^\u211d, that is the set of functions that map the reals into the reals. The domain of functions in sets X and Y have time as their domain, and their codomain represents the value of the signal at a given time. In Figure we illustrate a typical decomposition of S into three smaller sub-systems: a controller component, a state estimation component, and finally, the so-called plant, a physical model of the object to be controlled and its environment. We now discuss each of these, and formalize the fine-grained structures in systems S depicted in Figure. \u00a7.\u00a7.\u00a7 System State Models A more useful system model follows from identifying a set of functions from \ud835\udd4b to ^n, where n can vary from function to function, and the mathematical relationships between their values in different time instants. These functions, or signals, respond to specific assumptions. So-called output variables, denoted by a vector functions \ud835\udc32(t) = [y_1(t) \u2026 y_m(t)]^T, are those signals that can be measured directly. The signals that are assumed to be controllable receive the name of input variables and are denoted by \ud835\udc2e(t) = [u_1(t) \u2026 u_p(t)]^T. Finally, signals that convey the information necessary to predict values of \ud835\udc32(t) given a measurement \ud835\udc32(t_0), t_0",
    "\n\u00a7 Abstract i'm not sure about the syntactic vs. semantic correctness framing\u2014i don't think preference learning helps address one over the other suggest going with a new preference learning framework for coding taking a stab below Instruction-finetuned code language models have shown promise on various programming tasks. They train, with a language modeling objective, on natural language instruction and gold code snippets pairs. Recent evidence suggests that these models, never exposed to incorrect solutions during training, often fumble in distinguishing between correct and incorrect solutions. This observation raises our inquiry: Can preference learning, which trains models to prefer correct solutions over incorrect ones, help push the boundaries of code LMs even further? We propose a novel preference learning framework tailored for coding problems aiming to investigate its key success factors and potential benefits, which remain elusive despite the tremendous success of preference learning in aligning LMs with human values. Our three-stage approach involves: (1) augmenting instruction-tuning datasets with test cases. (2) sampling solutions from the model and evaluate them against the test cases to create a preference dataset, which is then used to (3) train the model with preference learning algorithms. Experiments demonstrate that our framework substantially improves the performance of existing code language models on established code generation benchmarks such as HumanEval(+) and MBPP(+). Importantly, our approach complements and is orthogonal to the supervised fine-tuning (SFT) stage. Instruction-finetuned code language models (LMs) have shown promise in various programming tasks. They are trained, using a language modeling objective, on natural language instructions and gold code snippet pairs. Recent evidence suggests that these models, never exposed to incorrect solutions during training, often struggle to distinguish between correct and incorrect solutions. This observation raises our inquiry: Can preference learning, which trains models to prefer correct solutions over incorrect ones, help push the boundaries of code LMs even further? We propose , a novel preference learning framework augmented with test cases tailored for code LMs. aims to investigate the key success factors and potential benefits of preference learning in code LMs, which remain elusive despite its success in aligning LMs with human values. consists of three stages: (1) Generating test cases for natural language instructions, (2) sampling candidate solutions from the policy and evaluating them against the test cases to create a preference dataset, which is then used to (3) train the policy with a preference learning algorithm. Experiments demonstrate that substantially improves the performance of existing code LMs on established code generation benchmarks such as HumanEval (+) and MBPP (+), even for the state-of-the-art open-source language model CodeQwen-1.5-7B-Chat. complements the supervised fine-tuning (SFT) stage, demonstrating synergistic effects.abbrv",
    "Efficient Discrepancy Testing for Learning with Distribution Shift\n\u00a7 Abstract A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance. While in general hard to compute, here we provide the first set of provably efficient algorithms for testing localized discrepancy distance, where discrepancy is computed with respect to a fixed output classifier. These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023). Our approach generalizes and improves all prior work on TDS learning: (1) we obtain universal learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits. Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets. Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.\u00a7 INTRODUCTION Distribution shift remains a central challenge in machine learning. While practitioners may exert some level of control over a model's training distribution, they have far less insight into future, potentially adversarial, test distributions. Developing algorithms that can predict whether a trained classifier will perform well on an unseen test set is therefore critical to the widescale deployment of modern foundation models. A heavily-studied framework for modeling distribution shift is domain adaptation, where a learner has access to labeled examples from some training distribution, unlabeled examples from some test distribution and is asked to output a hypothesis with low error on the test distribution. Over the last twenty years, researchers in domain adaptation and related fields have established bounds for out-of-distribution generalization in terms of some type of distance between train and test distributions. By far the most commonly studied notion is discrepancy distance: _(,')=sup_f_1,f_2\u2208|_\u223c[f_1()\u2260 f_2()]-_\u223c'[f_1()\u2260 f_2()]| Estimating or even testing discrepancy distance, however, seems difficult, as its definition involves an enumeration over all classifiers from some underlying function class (in we give the first hardness result for computing discrepancy distance in general). As such, obtaining provably efficient algorithms for domain adaptation has seen little progress (none of the above works give polynomial-time guarantees). In search of efficient algorithms for learning with distribution shift with certifiable error guarantees, recent work by defined the Testable Learning with Distribution Shift (TDS learning) framework. In this model (similar to domain adaptation), a learner receives labeled examples from train distribution D, unlabeled examples from test distribution D', and then runs a test. If the (efficiently computable) test accepts, the learner outputs h that is guaranteed to have low test error with respect to D'. No guarantees are given if the test rejects, but it must accept (with high probability) if the marginals of D and D' are equal. This framework has led to the first provably efficient algorithms for learning with distribution shift for certain concept classes (for example, halfspaces). It is straightforward to see that if algorithm A learns concept class C in the (ordinary) PAC/agnostic model, and we have an efficient localized discrepancy tester for C, then C is learnable in the TDS framework: simply apply the discrepancy tester to the output of A and accept if this quantity is small. A dream scenario would be to augment all known PAC/agnostic learning algorithms with associated localized discrepancy testers. This is nontrivial in part because we cannot make any assumptions on the test distribution D' (our test has to always accept or reject correctly). Nevertheless, our main contribution is a suite of new discrepancy testers for well-studied function class/training distribution pairs that unifies and greatly expands all prior work on TDS learning. \u00a7.\u00a7 Our Contributions Optimal Error Guarantees via \u0141_1 Sandwiching. The work of used a moment-matching approach to show that the existence of \u0141_2 sandwiching polynomial approximators implies TDS learning up to a constant factor of the optimum error. Although their result implies TDS learning for several fundamental concept classes, the \u0141_2 sandwiching requirement seems restrictive for classes such as constant-depth circuits or polynomial threshold functions. In, we provide TDS learning results in terms of the much more well-understood notion of \u0141_1 sandwiching, resolving one of the main questions left open in. As such, we obtain exponential improvements for TDS learning constant depth circuits (AC^0), and the first results for degree-2 polynomial threshold functions (see ). Our result also brid",
    "Exploring Cognitive Bias Triggers in COVID-19 Misinformation Tweets: A Bot vs. Human Perspective\n\u00a7 Abstract During the COVID-19 pandemic, the proliferation of misinformation on social media has been rapidly increasing, with an estimated surge of 900% within the initial three months. Automated Bot authors are believed to be significant contributors of this surge. It is hypothesized that Bot authors deliberately craft online misinformation aimed at triggering and exploiting human cognitive biases, thereby enhancing tweet engagement and persuasive influence. This study investigates this hypothesis by studying the triggers of biases embedded in Bot-authored COVID-19 misinformation and comparing them with their counterparts, Human-authored misinformation. To achieve this, we complied a comprehensive Misinfo Dataset that contains COVID-19 vaccine-related misinformation tweets annotated by author identities, Bots vs Humans, from Twitter during the vaccination period from July 2020 to July 2021. Leveraging insights from literature on judgment heuristics and cognitive biases, we developed an algorithm to computationally automate the extraction of triggers for eight cognitive biases in the tweets. Our analysis revealed that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were most commonly present in COVID-19 misinformation, with Bot-authored tweets exhibiting a greater prevalence. In addition, we observed distinct patterns in utilizing bias triggers between Human-authored and Bot-authored tweets. We further linked these bias triggers with engagement metrics such as reply, retweet, quote, reply counts, inferring their potential influence on tweet engagement and persuasiveness. Overall, our findings indicate that bias-triggering tactics have been more influential on Bot-authored tweets than Human-authored tweets. While certain bias triggers boosted engagement for Bot-authored tweets, some other bias triggers unexpectedly decreased it. On the other hand, triggers of most biases appeared to be unrelated to the engagement of Human-authored tweets. Our work sheds light on the differential utilization and effect of persuasion strategies between Bot-authored and Human-authored misinformation from the lens of human biases, offering insights for the development of effective counter-measures.\u00a7 INTRODUCTION In the social media landscape, two distinct user types coexist: Bots and Humans. Humans use social media platforms as integral parts of their daily lives, staying updated with news and interact with their friends. Social media Bots are purposefully designed to disseminate contents, propagate ideas, and engage with other users. The influence from users\u2013whether Bots or Humans\u2013on the virtual platforms is profound, as they can effectively persuade and shift people's opinions. Bots have impacted political conversations on social media, resulting in large opinion shifts. Similarly, Humans also have considerable influence in swaying people's opinions, exemplified by phenomena such as the \"Taylor Swift Effect,\" in which the actions or statements of influential figures trigger significant reactions on social media platforms. This persuasive influence of Bots and Humans on social media platforms is likely facilitated by the exploitation of human biases. Human biases are heuristics and cognitive shortcuts that individuals subconsciously utilize to simplify information processing and synthesize for decision making, often introducing judgment errors. In the digital realm, where vast amounts of information and fierce debates prevail, triggers of biases play a pivotal role in shaping human opinions and decisions. This is particularly evident during the 2020 COVID-19 pandemic, because heightened levels of unpredictability and confusion exacerbated biases, promoting and amplifying misinformation. Misinformation, defined as false or misleading information, comes in various forms such as conspiracy theories, false news and junk science. In recent years, Bots have taken a disproportionately active role on social media platforms, particularly in spreading misinformation, engaging target receivers, expressing opinions, and ultimately persuading users to take specific actions. Studies indicate that between 5 to 47% of online users are likely Bots, with the percentage varying depending on the popularity of the event. Furthermore, humans struggle to differentiate between Bots and Humans, with misclassifications occurring about 71% of the time. As digital entities programmed to operate within the social media landscape, Bots leverage on automation to bring about the persuasive influence. They often collectively amplify misinformation in the early stage of narrative building and strategically target influential users. This likely contributes to the extensive spread of misinformation that effectively appeals to human psychology and cognition. These phenomena underscore the urgent need for a deep examination of the intricacies of this",
    "Benchmarking Spectral Graph Neural Networks:\n\u00a7 Abstract With the recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. However, few systematic studies have been conducted to assess their spectral characteristics. This emerging family of models also varies in terms of design and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. In this work, we extensively benchmark spectral GNNs with a focus on the frequency perspective. We analyze and categorize over 30 GNNs with 27 corresponding filters. Then, we implement these spectral models within a unified framework with dedicated graph computations and efficient training schemes. Thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral GNNs with desirable performance. Our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: <https://github.com/gdmnl/Spectral-GNN-Benchmark>.nips \u00a7 CHECKLIST The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default to,, or. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example: * Did you include the license to the code and datasets? See. * Did you include the license to the code and datasets? The code and the data are proprietary. * Did you include the license to the code and datasets? Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below. * For all authors... * Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? We accurately claim our contributions in. * Did you describe the limitations of your work? Limitations and potential improvements are discussed in. * Did you discuss any potential negative societal impacts of your work? Our work is not directly related to negative social impact. * Have you read the ethics review guidelines and ensured that your paper conforms to them? * If you are including theoretical results... * Did you state the full set of assumptions of all theoretical results? Preliminary and assumptions are provided in. * Did you include complete proofs of all theoretical results? Full elaborations are in. * If you ran experiments (e.g. for benchmarks)... * Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Our codebase is available at:. * Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Training details are in and configuration files are available in the codebase. * Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Error bars are reported in main experiments. * Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Compute and resources are provided in. * If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... * If your work uses existing assets, did you cite the creators? Datasets are cited in. * Did you mention the license of the assets? License are mentioned in the codebase. * Did you include any new assets either in the supplemental material or as a URL? * Did you discuss whether and how consent was obtained from people whose data you're using/curating? * Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? * If you used crowdsourcing or conducted research with human subjects... * Did you include the full text of instructions given to participants and screenshots, if applicable? * Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? * Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?",
    "The AHA-Tree: Adaptive Hotspot-Aware Index for Oscillating Write-Heavy and Read-Heavy Workloads\n\u00a7 Abstract In this demo, we realize data indexes that can morph from being write-optimized at times to being read-optimized at other times nonstop with zero-down time during the workload transitioning. These data indexes are useful for HTAP systems (Hybrid Transactional and Analytical Processing Systems), where transactional workloads are write-heavy while analytical workloads are read-heavy. Traditional indexes, e.g., and LSM-Tree, although optimized for one kind of workload, cannot perform equally well under all workloads. To migrate from the write-optimized LSM-Tree to a read-optimized is costly and mandates some system down time to reorganize data. We design adaptive indexes that can dynamically morph from a pure LSM-tree to a pure buffered B-tree back and forth, and has interesting states in-between. There are two challenges: allowing concurrent operations and avoiding system down time. This demo benchmarks the proposed AHA-Tree index under dynamic workloads and shows how the index evolves from one state to another without blocking.Purdue University, West Lafayette, IN 47907-2107 xingl@purdue.edu Purdue University, West Lafayette, IN 47907-2107 aref@purdue.edu \u00a7 INTRODUCTION A diagram of an ideal index evolution under the oscillating read-heavy and write-heavy workloads Throughput under a changing workload from read-heavy (0 to 1 million operation counts); write-heavy (1 to 2 million operation counts); read-heavy (remaining) The ideal and real performance of adaptive index and static index under a changing workload. 0.49 The insertion process in. 0.49 The range search process in. Overview of New data-intensive applications impose new requirements on data systems. The term Hybrid Transactional and Analytical Processing (HTAP, for short) is coined to represent the ever-increasing need to address hybrid requirements, mainly to offer online analytics over online transactional data. Typical HTAP workloads include heavy transactional workloads at times as well as heavy analytical queries at other times. HTAP systems need to support workload changes over time. For example, this is observed as a diurnal pattern in one of the RocksDB use cases at Meta, and is also observed in where the number of tweets fluctuate across the day. In this demo, we demonstrate a data index termed the Adaptive Hotspot-Aware tree index, (the, for short) that is suitable for HTAP workloads. The can adapt and morph from being write-optimized at times, e.g., behave like an LSM-tree, to being read-optimized at other times, e.g., behave like a. We stress-test the over one of the most observed changing workloads; one that oscillates between being write-heavy at times to being read-heavy at other times. This pattern is practical in some applications, e.g., in a social media app, where users post comments actively during the day, and browse content at night. This corresponds to the oscillating write-heavy posting comments, and read-heavy browsing workloads, which is repeated daily. Another observation over real datasets is that operations are focused on hotspots. Thus, the only adapts the structure where hotspot data is stored, which is more time efficient. Among the analytical workloads, we focus on simple range search queries as we can control the amount of selected data by the query. Traditional static indexes, e.g., the and Log-Structured Merge Tree (LSM-Tree) that are optimized for only one operation are either optimized for range search query or optimized for write operations, respectively. They cannot achieve the same throughput performance in the case of an oscillating write-heavy and read-heavy workloads. The ideal index for the oscillating workload is to make the index behave as an LSM-Tree in the write-heavy phase, and behave as a in the read-heavy workload as in Figure. In this demonstration, we present which is an adaptive index being hotspot-aware. We showcase baseline indexes and the under oscillating workloads. The can adapt itself and catch with the optimal static index within that workload. We also display the intermediate structure of the during adapting. However, as this demo illustrates, under certain workloads, the overall throughput of the is competitive compared to the read-optimized index (i.e., the ) and the write-optimized index (i.e., the LSM-Tree). A trade-off between current performance and future performance is demonstrated for the. Meanwhile, being an adaptive index, the takes time to adapt. The throughput during the adaptation phase can be low, and this is the tradeoff, but this does not compare to having significant down time to migrate data to suit the new workload or to maintain dual structures, one for each workload type. \u00a7 BACKGROUND The structure of the is inspired by the buffer tree, where the can be viewed as a tree part (a ) plus a buffer part (an LSM-Tree), and lies perfectly in the middle between a and a",
    "Hardware Implementation of Soft Mapper/Demappers in Iterative EP-based Receivers\n\u00a7 Abstract This paper presents a comprehensive study and implementations onto FPGA device of an Expectation Propagation (EP)-based receiver for QPSK, 8-PSK, and 16-QAM. To the best of our knowledge, this is the first for this kind of receiver. The receiver implements a Frequency Domain (FD) Self-Iterated Linear Equalizer (SILE), where EP is used to approximate the true posterior distribution of the transmitted symbols with a simpler distribution. Analytical approximations for the EP feedback generation process and the three constellations are applied to lessen the complexity of the soft mapper/demapper architectures. The simulation results demonstrate that the fixed-point version performs comparably to the floating-point. Moreover, implementation results show the efficiency in terms of FPGA resource usage of the proposed architecture., Serdar \u015eahin\u2020, Camille Leroux*, Antonio Maria Cipriano\u2020, Christophe J\u00e9go* *University of Bordeaux, Bordeaux INP IMS Lab, UMR CNRS 5218, France firstname.last-name@ims-bordeaux.fr \u2020 Thales Gennevilliers, France firstname.last-name@thalesgroup.com This work has been funded by the French National Research Agency under grant number ANR-20-CE25-0008-01 (EVASION Project: https://anr-evasion.ims-bordeaux.fr/). Expectation Propagation, Frequency Domain Self-Iterated Linear Equalizer, Analytical approximations, architecture design, FPGA prototyping \u00a7 INTRODUCTION In digital communication systems, achieving minimal error rates in data detection and/or decoding requires the resolution of a Maximum A Posteriori (MAP) or Maximum Likelihood (ML) problem. However, the computational complexity of resolving such criteria is often prohibitive, particularly in real-world frequency selective channels, where the number of computations increases exponentially with factors such as data length, modulation order, and channel memory. As a result, practical receiver design often involves applying simplifying hypotheses and approximations. One promising approach in the context of Frequency Domain (FD) Linear Equalization (LE) is equalizers designed with Expectation Propagation (EP). Indeed, they have demonstrated an appealing trade-off between performance and computational complexity. In this paper, a comprehensive study and implementation of an EP-based receiver for communications over frequency selective channels using standard Phase Shift Keying (PSK) or Quadrature Amplitude Modulation (QAM) constellations is presented. The receiver implements an FD Self-Iterated Linear Equalizer (SILE), where EP is applied to approximate the true posterior distribution of the transmitted symbols with a simpler distribution that can be easily manipulated. Previously, the implementation of a simplified EP receiver for multiple antenna receivers has been reported in. A low complexity EP detector for sparse code multiple access was also proposed in. However, the authors only study the impact of simplifications on the performance and estimate the computational cost of their proposal per operation type. Simplified EP-based FD equalization is studied in. Similar to the previous case, only a computational complexity assessment was provided. To the best of our knowledge, we propose the first hardware implementation of an EP-based FD SILE receiver. The contributions of this paper are the following: * Methods to reduce the complexity of the EP-based FD-SILE algorithm are proposed, which are different from the ones in. They include a new way to generate the EP soft feedback and also a new method to calculate extrinsic Log-Likelihood Ratios (LLR) for 8-PSK. * A fixed-point version of the model enables to verify that the degradation in terms of performance due to these new algorithmic simplifications is limited. * The implementation of soft mapper/demapper architectures is carried out on a Field Programmable Gate Array (FPGA), specifically the PYNQ Z2 board. The PYNQ Z2 board contains a device that combines an ARM processor and an FPGA, which enables easier Ethernet communication. The implementation was done in a Hardware in the Loop (HIL) configuration, with the EP parts implemented onto the FPGA, while the others run on a computer thanks to py-AFF3CT, a Python wrapper for the Forward Error Correction Toolbox AFF3CT. The analysis of FPGA resource usage shows very low complexity overhead for three different and widely used constellations. These results confirm that the proposed EP equalizer is potentially a good candidate for practical implementation even on cost- and complexity-constrained digital communication equipment. The paper is organized as follows. A description of the FD SILE receiver with EP is provided in Section. The simplifications and analytical approximations applied to the soft mapper/demapper to decrease its complexity are presented in Section. The fixed-point conversion analysis to facilitate the soft mapper/demapper architecture d",
    "Unsupervised Monocular Depth Estimation Based on Hierarchical Feature-Guided Diffusion\n\u00a7 Abstract Unsupervised monocular depth estimation has received widespread attention because of its capability to train without ground truth. In real-world scenarios, the images may be blurry or noisy due to the influence of weather conditions and inherent limitations of the camera. Therefore, it is particularly important to develop a robust depth estimation model. Benefiting from the training strategies of generative networks, generative-based methods often exhibit enhanced robustness. In light of this, we employ a well-converging diffusion model among generative networks for unsupervised monocular depth estimation. Additionally, we propose a hierarchical feature-guided denoising module. This model significantly enriches the model's capacity for learning and interpreting depth distribution by fully leveraging image features to guide the denoising process. Furthermore, we explore the implicit depth within reprojection and design an implicit depth consistency loss. This loss function serves to enhance the performance of the model and ensure the scale consistency of depth within a video sequence. We conduct experiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The results indicate that our approach stands out among generative-based models, while also showcasing remarkable robustness.empty empty \u00a7 INTRODUCTION Monocular depth estimation aims to predict pixel-level depth and plays a crucial role in numerous applications such as autonomous driving, virtual reality (VR), and augmented reality (AR). With the rapid development of computer vision and deep learning, Eigen et al. pioneer the application of deep learning to this field through a supervised approach. To reduce the model's data dependence, Zhou et al. propose the first unsupervised framework for monocular depth estimation. Numerous works have optimized and improved depth estimation methods based on this initial framework. These methods can be categorized into discriminative-based and generative-based methods, depending on their data modeling techniques through deep learning. Discriminative-based monocular depth estimation methods aim to learn the mapping from images to depth by maximizing the conditional probability distribution. These methods demonstrate impressive performance in ideally clear and high-quality images which are similar to the training set. However, in real-world scenarios, images captured by cameras may be affected by the weather conditions and the status of cameras. These will cause images in the test set blurry or noisy. Variations in data distribution between the test and training sets directly affect the mapping derived from the model, leading to poor robustness and failure in such scenarios. There are methods trying to improve the robustness of discriminative-based methods by adding perturbations to the training set. In practical applications, the perturbations are diverse, including but not limited to illumination changes, blur, etc. These methods do not essentially improve the robustness of the model and still fail to handle scenarios that do not appear in the training set. In contrast, generative-based monocular depth estimation methods could interpret the intrinsic distribution of depth by learning the joint probability distribution between images and depth. This approach exhibits greater robustness and adaptability when faced with novel data samples. Even when the input image is perturbed, such as the aforementioned scenarios, the model provides more accurate and robust depth estimation benefiting from the understanding of image and depth distribution. Kaneko et al. demonstrate the strong robustness of generative networks when dealing with noisy images. In this work, we aim to continue to explore the application of generative networks in depth estimation and develop a robust unsupervised monocular depth estimation method. Inspired by the successful application of a well-converging generative-based diffusion model in image feature enhancement and panoptic segmentation, we propose an unsupervised monocular depth estimation framework based on the diffusion model, as shown in Fig.. In this framework, we design a diffusion depth network by integrating the diffusion model into the depth estimation subnetwork, as illustrated in Fig.. The diffusion depth network iteratively refines a random distribution via a denoising process guided by an image, ultimately recovering depth from the random distribution. To enhance the model's capacity to learn and interpret the joint distribution of depth under image guidance, we propose a novel hierarchical feature-guided denoising module (HFGD), as illustrated in Fig.. As we gradually integrate image pyramid features into each level of the denoising network, the guidance information evolves from low-level spatial geometric features to high-level semantic features. This approach allows for a more c",
    "Adaptive Temporal Motion Guided Graph Convolution Network for Micro-expression Recognition\n\u00a7 Abstract Micro-expressions serve as essential cues for understanding individuals' genuine emotional states. Recognizing micro-expressions attracts increasing research attention due to its various applications in fields such as business negotiation and psychotherapy. However, the intricate and transient nature of micro-expressions poses a significant challenge to their accurate recognition. Most existing works either neglect temporal dependencies or suffer from redundancy issues in clip-level recognition. In this work, we propose a novel framework for micro-expression recognition, named the Adaptive Temporal Motion Guided Graph Convolution Network (ATM-GCN). Our framework excels at capturing temporal dependencies between frames across the entire clip, thereby enhancing micro-expression recognition at the clip level. Specifically, the integration of Adaptive Temporal Motion layers empowers our method to aggregate global and local motion features inherent in micro-expressions. Experimental results demonstrate that ATM-GCN not only surpasses existing state-of-the-art methods, particularly on the Composite dataset, but also achieves superior performance on the latest micro-expression dataset CAS(ME)^3.Renmin University of China fy.zhang@ruc.edu.cn Xinjie Zhang Renmin University of China zhangxinjie827@ruc.edu.cn Zhaopei Huang Renmin University of China huangzhaopei@ruc.edu.cn Qin Jin* *Corresponding author. Renmin University of China qjin@ruc.edu.cn Micro-expression recognition, GCN, motion feature \u00a7 INTRODUCTION Facial expressions are conscious human reactions to certain stimuli and play critical roles in our daily communications. Many studies have explored the complex mechanisms of facial expressions and further attempted to understand the underlying emotional cues behind them. These efforts are primarily made in macro-expressions that are often easily noticeable by individuals. Micro-expressions are another form of facial expression that appear for a fleeting duration of less than 0.5 seconds. Although micro-expressions are very brief, they serve as essential cues for understanding individuals\u2019 genuine emotional states and often appear when people are trying to hide their feelings. Automatic micro-expression recognition (MER) has attracted increasing research attention due to its diverse applications in fields such as business negotiation and psychotherapy etc. Macro- vs Micro-expression of Happiness However, the accurate recognition of micro-expressions still remains a formidable challenge. First, unlike macro-expressions, micro-expression frames actually share stronger temporal dependencies, so it is difficult to accurately recognize micro-expressions through one single frame, as shown in Figure. Second, micro-expressions are complex combinations of several subtle facial movements. Therefore, effectively modeling the relationship between these facial movements that make up a certain expression is very important. An overview of the proposed ATM-GCN approach for micro-expression recognition. f_o and f_a represent the Onset and Apex frame respectively for simplification. The input sequence is first input into the Motion Pairing & Encoding module for extracting motion features between frame pairs, which are then aggregated through the Adaptive Temporal Motion GCN (ATM-GCN) module. Finally, a Classifier module is utilized to get the predicted micro-expression for the input sequence. Over the years, different attempts have been made to address these challenges. Some methods such as only use the Onset frame and the Apex frame in a clip and extract the micro-expression features inherent therein, which helps capture the most important motion in a clip, but it discards critical temporal information. On the other hand, methods like SLSTT end up encountering potential information redundancy when utilizing full frame sequences. Inspired by GCN, some works construct graphs by learning the relation between different facial movements. Although more theoretically explainable, these works require extra labels for training. Moreover, they neglect the temporal information, resulting in insufficient utilization of the entire clip. In this work we propose a new GCN-based MER method, named Adaptive Temporal Motion guided Graph Convolution Network (ATM-GCN). With the full frame sequence as input, our method manages to pay more attention to the most important motion information from the whole sequence. Moreover, our method mitigates redundancy by modeling the relation of motion information in different temporal locations and further modifies the relation adaptively during training. The main contributions of this work include: 1) We propose the Adaptive Temporal Motion guided Graph Convolutional Network (ATM-GCN) that incorporates temporal dependencies of frame pairs to fully exploit motion information for MER; 2) To dynamically adjust the t",
    "3D Building Generation in Minecraft via Large Language Models\n\u00a7 Abstract Recently, procedural content generation has exhibited considerable advancements in the domain of 2D game level generation such as Super Mario Bros. and Sokoban through large language models (LLMs). To further validate the capabilities of LLMs, this paper explores how LLMs contribute to the generation of 3D buildings in a sandbox game, Minecraft. We propose a Text to Building in Minecraft (T2BM) model, which involves refining prompts, decoding interlayer representation and repairing. Facade, indoor scene and functional blocks like doors are supported in the generation. Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs. It shows that LLMs hold significant potential for 3D building generation. Given appropriate prompts, LLMs can generate correct buildings in Minecraft with complete structures and incorporate specific building blocks such as windows and beds, meeting the specified requirements of human users.Guangdong Key Laboratory of Brain-inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China S. Hu and Z. Huang contributed equally to this work. This paper has been accepted by IEEE Conference on Games. Procedural content generation, LLMs, building generation, 3D generation, Minecraft \u00a7 INTRODUCTION Procedural content generation (PCG) involves automatically generating specific content such as environments and levels in games. Recent emergence of LLMs have inspired various applications in PCG. The capabilities of LLMs are crucial in enabling PCG to fulfil complex requirements from human feedback. For instance, Todd et al. fine-tuned LLMs to generate novel and playable Sokoban levels. Sudhakaran et al. created MarioGPT to generate Super Mario Bros.'s levels. Instead of using tokenizer, MarioGPT enables users to directly write prompts, e.g., \u201csome pipes, little enemies\" to obtain expected levels. Nasir et al. constructed a two-stage workflow to fine-tune GPT-3, in which levels generated by both humans and LLMs are involved for training. Besides, an LLM-based competition called ChatGPT4PCG was held in 2023, aiming at generating Angry Birds levels via LLMs. However, all the aforementioned works focused on 2D level generation. Applying LLMs to 3D game content generation is rarely explored. Considering a 3D environment, the additional dimension (e.g., height) necessitates the consideration of extra attributes and a higher-dimensional representation. Minecraft is a suitable testbed for 3D building generation for global popularity, well-defined grid structure and high editability. Generative design in Minecraft competition (GDMC) encourages the application of PCG in 3D building generation. Traditional JSON lookup methods like Nys et al. involved extracting keywords from user inputs and matching them to predefined JSON templates. However, it often fails to understand complex requests. Green et al. combined constrained growth and cellular automata method for generating buildings with ASCII representation. Merino et al. proposed an interactive evolutionary algorithm, in which users choose preferred buildings. However, it takes time to obtain a suitable building. Huang et al. searched city layouts to place redstone-style buildings. Barthet et al. generated buildings by searching latent space and constructing rules. Jiang et al. incorporated reinforcement learning to generate 3D levels. Furthermore, Maren et al. introduced World-GAN to generate 3D scenarios such as deserts. Very recently, Earle et al. trained quantised neural radiance fields to facilitate text-to-3D generation. However, functional blocks like doors and indoor scene generation are not addressed. Directly incorporating human feedback such as language in 3D building generation in Minecraft remains an under-explored, non-trivial challenge. To investigate into the above topic, this paper proposes a Text to Building in Minecraft (T2BM) model, which leverages capabilities of LLMs in 3D building generation considering facade, indoor scene and functional blocks such as doors and beds. T2BM accepts simple prompts from users as input and generates buildings encoded by an interlayer, which defines the transformation between text and digital content. Based on T2BM, players or designers can construct buildings quickly without repeatedly placing blocks one by one, while the human-crafted prompt is not necessarily detailed. Experiments with GPT-3.5 and GPT4 demonstrate that T2BM can generate complete buildings, while aligning with human instructions. Workflow of Text to Building in Minecraft (T2BM) model. \u00a7 TEXT TO BUILDING IN MINECRAFT (T2BM) As depicted in Fig., T2BM receives a simple user input and outputs a complete building in Minecraft. Initially, T2BM forwards the user's description along with contexts like some detailed building description examples to",
    "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\n\u00a7 Abstract Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, we introduce SEACrowd, a collaborative initiative that consolidates a comprehensive resource hub[<https://seacrowd.github.io/seacrowd-catalogue/>] that fills the resource gap by providing standardized corpora[<https://github.com/SEACrowd/seacrowd-datahub/>] in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in SEA.\u00a7 INTRODUCTION Despite the Southeast Asia (SEA) region being home to 1,300 indigenous languages (18% of the world's languages) and 671 million people (8.75% of the world's population), the representation of texts, images, and audio datasets from this region is significantly lacking in machine learning pre-training models. This deficiency negatively impacts the model quality for SEA languages. The language coverage of SEA languages in two common pre-training resources, Common Crawl[] and C4, is extremely limited, with only 2.36% (in 11 languages) and 10.62% (in 11 languages), respectively. In modalities beyond text, the representation is even more limited. For instance, Common Voice, one of the largest multilingual speech corpora, includes only 6 SEA indigenous languages, and LAION-5B, one of the largest multilingual vision-language (VL) corpora, includes only 12 SEA indigenous languages. While datasets for other SEA indigenous languages may exist, they are often scattered, insufficiently documented, or varied in quality and formatting, thereby making access and usage challenging. In terms of evaluation, the sparse availability of high-quality test tasks for these languages also complicates evaluating models for SEA languages. Despite there being 1,300+ languages in the SEA region, prior works have only evaluated fewer than 10 SEA languages collectively. The actual performance of current models on most SEA languages remains largely unknown. Moreover, the dominance of Anglocentric training data potentially results in cultural bias when generating texts, images, or audio in underrepresented SEA languages. Further, have shown that the learned representations in LMs often fail to reflect local cultural values in SEA. This raises concerns about the ability of current LLMs to generate natural, high-quality texts for this region. Furthermore, the discrepancy in language support creates language barriers in technological access and risks marginalizing minority groups who do not speak the dominant language. Mapping between tasks, schemas, modalities, and language regions across 498 datasheets in SEACrowd. In this work, we investigate the current AI progress for SEA languages by addressing the challenges of resources, evaluation, and generation quality. Our contributions are three-fold: * We bridge the resource gap by centralizing and standardizing \u223c500 corpora in nearly 1,000 SEA languages in SEACrowd, a comprehensive and standardized resource center, across 3 modalities: text, image, and audio. * We close the evaluation gap in SEA languages with the SEACrowd Benchmarks, which cover 38 SEA indigenous languages on 13 tasks across 3 modalities, providing insights into the performance of a diverse spectrum of AI models. Further, our study reveals that the generative outputs of existing LLMs exhibit a closer resemblance to translationese rather than natural data in 9 SEA languages. * We offer insights and strategies for the future development of AI in SEA, aiming to promote a sustainable and prosperous future through continuous AI advancements. \u00a7 SEACROWD SEACrowd represents the first comprehensive AI dataset collection initiative for SEA, developed through a collaborative effort among researchers and engineers primarily based in the SEA region. As addressed in, resource scarcity and the scattered nature of the data are crucial challenges in SEA. SEACrowd addresses these issues through two primary contributions: 1) consolidating datasheets to enhance data discoverability; and 2) standardizing dataloaders for easier use, especially in multiple dataset loading. We also follow data provenance practices to preserve the",
    "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey\n\u00a7 Abstract With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable progress in natural language processing. These models are trained on large amounts of data to demonstrate powerful language understanding and generation capabilities for various applications, from machine translation and chatbots to agents. However, LLMs have exposed a variety of privacy and security issues during their life cycle, which have become the focus of academic and industrial attention. Moreover, these risks LLMs face are pretty different from previous traditional language models. Since current surveys lack a clear taxonomy of unique threat models based on diverse scenarios, we highlight unique privacy and security issues based on five scenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based agent. Concerning the characteristics of each risk, this survey provides potential threats and countermeasures. The research on attack and defense situations LLMs face can provide feasible research directions, making more areas reap LLMs' benefits.shang.wang-1@student.uts.edu.au University of Technology Sydney Australia [1] tqzhu@cityu.edu.mo City University of Macau China bo.liu@uts.edu.au University of Technology Sydney Australia xxx@xxx.com CSIRO Australia 1906106220@qq.com Dayong.ye@uts.edu.au University of Technology Sydney Australia wlzhou@cityu.edu.mo City University of Macau China 00000000.0000000.0000000 Do Not Use This Code, Generate the Correct Terms for Your Paper 500 00000000.00000000.00000000 Do Not Use This Code, Generate the Correct Terms for Your Paper 300 00000000.00000000.00000000 Do Not Use This Code, Generate the Correct Terms for Your Paper 100 00000000.00000000.00000000 Do Not Use This Code, Generate the Correct Terms for Your Paper 100 [500]Security and privacy Human and societal aspects of security and privacy 20 February 2018 [revised]12 March 2018 [accepted]5 June 2018 \u00a7 INTRODUCTION With the rapid development of artificial intelligence (AI) technology, researchers have progressively expanded the scale of training data and model architectures. Learned with massive amounts of data, extremely large-scale models demonstrate impressive language understanding and generation capabilities, marking a significant breakthrough in natural language processing (NLP). Referred to as Large Language Models (LLMs), those models provide strong support for machine translation, text summary, automatic coding, and other NLP tasks. However, the in-depth application of LLMs across various industries, such as chatbots and medical diagnosis, expose their life cycle to various privacy and security issues. More importantly, LLMs face unique privacy and security risks that never posed in traditional language models, demanding higher requirements for privacy protection and security defenses. \u00a7.\u00a7 Motivation Compared to traditional single-function language models, LLMs demonstrate remarkable understanding abilities, deploying across various applications such as logical reasoning and code generation. Recently, an increasing number of companies are launching universal or domain-specific LLMs, such as ChatGPT and LLaMA, offering users with versatile and intelligent services. However, due to LLMs' unique power and structures, throughout their life cycle, LLMs meet with unique security and privacy threats from society compared with previous single-function models or small-scale models. Existing surveys describe various risks and countermeasures by method type, and there is a lack of exploration of those unique threats. We divide LLMs' life cycle into five scenarios and discuss the unique privacy and security risks at each scenario. Unique privacy risks. When learning language knowledge from the training data, LLMs would memorize the data. It allows adversaries to steal private information. For example, Carlini et al. found that prompts with specific prefixes could make GPT-2 generate content containing personal information, such as email addresses and phone numbers. When running inference, the unlimited use of LLMs would provide adversaries with opportunities to steal model-related information and functionalities. In summary, throughout the life cycle of LLMs, adversaries can steal or infer various sensitive information, thus threatening specific individuals and institutions. Unique security risks. Since the training data contains malicious, illegal, hallucinatory, and biased texts, LLMs inevitably learn negative language knowledge. Moreover, malicious third parties responsible for developing LLMs in outsourcing scenarios can affect these models' integrity and utility by poisoning attacks and backdoor attacks. For example, an attacker could implant a backdoor in an LLM-based automated customer service system, causing the system to respond automatically with a predetermined fraudulent link when asked speci",
    "left=5cm,bottom=2cm, top=1cm, right=1cm [remember picture,overlay] [opacity=1,inner sep=0pt] at (-10mm,-270mm); (0,0) cmssm2226(-137,-743)90TH\u00c8SE DE DOCTORAT (-105,-743)90NNT: 2024UPASG030 cmssm2226 The Semantics of Effects: Centrality, Quantum Control and Reversible Recursion De la s\u00e9mantique des effets: centralit\u00e9, contr\u00f4le quantique et r\u00e9cursivit\u00e9 r\u00e9versible Th\u00e8se de doctorat de l'universit\u00e9 Paris-Saclay \u00c9cole doctorale n^\u2218 580, Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat: informatique Graduate School: Informatique et Sciences du Num\u00e9rique R\u00e9f\u00e9rent: Facult\u00e9 des sciences d'Orsay Th\u00e8se pr\u00e9par\u00e9e au Laboratoire M\u00e9thodes Formelles (Universit\u00e9 Paris-Saclay, CNRS, ENS Paris-Saclay, Inria), sous la direction de Pablo ARRIGHI, professeur Universit\u00e9 Paris-Saclay, et le co-encadrement de Beno\u00eet VALIRON, ma\u00eetre de conf\u00e9rence CentraleSup\u00e9lec, et de Vladimir ZAMDZHIEV, chercheur Inria. Th\u00e8se soutenue \u00e0 Gif-sur-Yvette, le 19 juin 2024, par Louis LEMONNIER Composition du jury Membres du jury avec voix d\u00e9lib\u00e9rative Prune Jean GOUBAULT\u2013LARRECQ Pr\u00e9sident Professeur, ENS Paris-Saclay Thomas EHRHARD Rapporteur & Examinateur Directeur de Recherche, CNRS & Universit\u00e9 Paris-Cit\u00e9 Laurent REGNIER Rapporteur & Examinateur Professeur, Universit\u00e9 de Aix-Marseille Claudia FAGGIAN Examinatrice Charg\u00e9e de Recherche, CNRS & Universit\u00e9 Paris-Cit\u00e9 Marie KERJEAN Examinatrice Charg\u00e9e de Recherche, CNRS & Universit\u00e9 Sorbonne Paris-Nord Membres invit\u00e9s Prune Pablo ARRIGHI Directeur de th\u00e8se Professeur, Universit\u00e9 Paris-Saclay Beno\u00eet VALIRON Co-encadrant Ma\u00eetre de conf\u00e9rence, CentraleSup\u00e9lec Vladimir ZAMDZHIEV Co-encadrant Chercheur, Inria empty empty top=1.5cm, bottom=3.25cm, left=2cm, right=2cm rm cmssm [linecolor=Prune,linewidth=1] Titre: De la s\u00e9mantique des effets: centralit\u00e9, contr\u00f4le quantique et r\u00e9cursivit\u00e9 r\u00e9versible Mots cl\u00e9s: Informatique quantique \u2013 S\u00e9mantique \u2013 Languages de programmation \u2013 Th\u00e9orie des cat\u00e9gories 2 R\u00e9sum\u00e9: Le sujet de cette th\u00e8se est ax\u00e9 sur la th\u00e9orie des langages de programmation. Dans un langage de programmation suffisamment bien d\u00e9fini, le comportement des programmes peut \u00eatre \u00e9tudi\u00e9 \u00e0 l'aide d'outils emprunt\u00e9s \u00e0 la logique et aux math\u00e9matiques, \u00e9non\u00e7ant des r\u00e9sultats sans ex\u00e9cuter le code. Ce domaine de l'informatique est appel\u00e9 s\u00e9mantique. La s\u00e9mantique d'un langage peut se pr\u00e9senter sous plusieurs formes: dans notre cas, des s\u00e9mantiques op\u00e9rationnelles, des th\u00e9ories \u00e9quationnelles et des s\u00e9mantiques d\u00e9notationnelles. Les premi\u00e8res donnent un sens op\u00e9rationnel aux programmes, au sein de la syntaxe du langage. Elles simulent les op\u00e9rations qu'un ordinateur est cens\u00e9 effectuer s'il ex\u00e9cute le programme. Une th\u00e9orie \u00e9quationnelle fonctionne \u00e9galement de mani\u00e8re syntaxique: elle indique si deux programmes effectuent la m\u00eame op\u00e9ration sans informer sur la proc\u00e9dure. Enfin, la s\u00e9mantique d\u00e9notationnelle est l'\u00e9tude math\u00e9matique des programmes, g\u00e9n\u00e9ralement \u00e0 l'aide de la th\u00e9orie des cat\u00e9gories. Elle permet par exemple de prouver qu'un programme se termine ou non. Cette th\u00e8se se concentre sur la s\u00e9mantique des effets dans les langages de programmation \u2013 une fonctionnalit\u00e9 ajout\u00e9e \u00e0 un langage, g\u00e9rant des donn\u00e9es secondaires ou des r\u00e9sultats probabilistes. Eugenio Moggi, en 1991, a publi\u00e9 un travail fondateur sur l'\u00e9tude de la s\u00e9mantique des effets, soulignant la relation avec les monades en th\u00e9orie des cat\u00e9gories. La premi\u00e8re contribution de cette th\u00e8se suit directement le travail de Moggi, en \u00e9tudiant la commutativit\u00e9 des effets dans un langage de programmation \u00e0 travers le prisme des monades. Les monades sont la g\u00e9n\u00e9ralisation de structures alg\u00e9briques telles que les mono\u00efdes, qui ont une notion de centre: le centre d'un mono\u00efde est une collection d'\u00e9l\u00e9ments qui commutent avec tous les autres dans le mono\u00efde. Nous fournissons les conditions n\u00e9cessaires et suffisantes pour qu'une monade ait un centre. Nous d\u00e9taillons \u00e9galement la s\u00e9mantique d'un langage de programmation avec des effets qui portent des informations sur les effets qui sont centraux. De plus, nous fournissons un lien fort \u2013 un r\u00e9sultat de langage interne \u2013 entre ses th\u00e9ories \u00e9quationnelles et sa s\u00e9mantique d\u00e9notationnelle. Le deuxi\u00e8me axe de la th\u00e8se est l'informatique quantique, per\u00e7ue comme un effet r\u00e9versible. Le quantique est un domaine \u00e9mergent de l'informatique qui utilise la puissance de la m\u00e9canique quantique pour calculer. Au niveau des langages de programmation, de nouveaux paradigmes doivent \u00eatre d\u00e9velopp\u00e9s pour \u00eatre fid\u00e8les aux op\u00e9rations quantiques. Les op\u00e9rations quantiques physiquement permises sont toutes r\u00e9versibles, \u00e0 l'exception de la mesure; cependant, la mesure peut \u00eatre report\u00e9e \u00e0 la fin du calcul, ce qui nous permet de nous concentrer d'abord sur la partie r\u00e9versible et d'appliquer ensuite la mesure pour obtenir des r\u00e9sultats. Dans le chapitre correspondant, nous d\u00e9finissons un langage de programmation r\u00e9versible, avec types simples, qui effectue des op\u00e9rations quantiques unitai",
    "Inductive Global and Local Manifold Approximation and Projection\n\u00a7 Abstract Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data. Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data. In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization. GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization. Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation. This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm. iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations. We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods.#1 1 1.25 Keywords: Data visualization, deep neural networks, inductive algorithm, manifold learning 1.4 \u00a7 INTRODUCTION Data visualization, which belongs to exploratory data analysis, has been promoted by John Tukey since the 1970s as a critical component in scientific research. However, visualizing high-dimensional data is a challenging task. The lack of visibility in the high-dimensional space gives less intuition about what assumptions would be necessary for compactly presenting the data on the reduced dimension. A common assumption that can be made without specific knowledge of the high-dimensional data is the manifold assumption. It assumes that the data are distributed on a low-dimensional manifold within a high-dimensional space. Nonlinear dimensional reduction (DR) with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. It is not uncommon to perform further statistical analysis on the reduced dimensional data, such as regression, functional data analysis, classification, and generative models. In this paper, we focus on DR for high dimensional data with manifold learning. Various research efforts have been made to develop DR tools for data visualization. Several leading algorithms include MDS, Isomap, t-SNE, UMAP, PHATE, PacMAP, and many others. MDS and Isomap keep the metric information between all the pairs of data points, and thereby aim to preserve the global geometry of the dataset. Recognizing that Euclidean distances between non-neighboring points may not always be informative in high-dimensional settings, Isomap aims to preserve geodesic distance estimates computed via the shortest path search. Efforts to refine this geodesic distance estimator include improving the Euclidean distances among nearby points to better reflect the local manifold, for instance, through a spherelets argument or the tangential Delaunay complex. On the other hand, the seminal t-SNE and UMAP preserve the distance among neighboring data points. Both t-SNE and UMAP use rescale factors that may enable local adaptability of estimated distances. It is known that UMAP and t-SNE may lose global information, leading to a misunderstanding of the global structure. To address this, develop scDEED, which statistically discerns the trustworthiness of each embedded point for mid-range neighborhood preservation. Interestingly, its reliability scores can be used to tune the hyperparameters of t-SNE and UMAP, or any general visualization technique. Additionally, to overcome the loss of global information, much effort has been made, for example, by good initialization or by an Euclidean distance preservation between selected non-neighboring points. We can see the previous efforts to preserve both global and local aspects in refining the geodesic distance estimator for better local detail and enhancing t-SNE and UMAP for improved global understanding. Advancing manifold learning, we address this key challenge: Can a single algorithm capture both the global structures and local details of high-dimensional data? This question motivates us to propose a new nonlinear manifold learning method called GLoMAP, which is short for Global and Local Manifold Approximation and Projection. GLoMAP approximates the data manifold in a data-adaptive way with many small low-dimensional Euclidean patches similar to UMAP. Such an approximation can result in multiple different local distances of the same pairs. UMAP adopts a fuzzy union to allow their inconsistent coexistence in a single repr",
    "The Penalized Inverse Probability Measure for Conformal Classification\n\u00a7 Abstract The deployment of safe and trustworthy machine learning systems, and particularly complex black box neural networks, in real-world applications requires reliable and certified guarantees on their performance. The conformal prediction framework offers such formal guarantees by transforming any point into a set predictor with valid, finite-set, guarantees on the coverage of the true at a chosen level of confidence. Central to this methodology is the notion of the nonconformity score function that assigns to each example a measure of \u201cstrangeness\" in comparison with the previously seen observations. While the coverage guarantees are maintained regardless of the nonconformity measure, the point predictor and the dataset, previous research has shown that the performance of a conformal model, as measured by its efficiency (the average size of the predicted sets) and its informativeness (the proportion of prediction sets that are singletons), is influenced by the choice of the nonconformity score function. The current work introduces the Penalized Inverse Probability (PIP) nonconformity score, and its regularized version RePIP, that allow the joint optimization of both efficiency and informativeness. Through toy examples and empirical results on the task of crop and weed image classification in agricultural robotics, the current work shows how PIP-based conformal classifiers exhibit precisely the desired behavior in comparison with other nonconformity measures and strike a good balance between informativeness and efficiency.\u00a7 INTRODUCTION The development and deployment of machine learning-based autonomous systems has been a flourishing field of research in both academia and, relatively more recently, in the industry. While machine learning models often exhibit high performance \u201cin the lab\", they often face much more difficulty when deployed in the real world, for a number of reasons that are not yet fully clear. Indeed, when faced with a new observation, the model will produce a new prediction whose quality is often related to the similarity of this new observation to what the model has previously seen. When the new observation is quite anomalous with respect to the previously seen data or even slightly perturbed, most models will produce wrong predictions, with often dire and intolerable consequences in safety critical applications such as autonomous driving and medical diagnosis, to name a few. The safe deployment of machine learning systems in the real world is therefore incumbent upon the integration of at least two main important features into them: (1) the ability to provide valid and trustworthy guarantees on the quality of predictions in \u201cnormal\" conditions, and (2) the ability to reliably detect and signal anomalies when faced with them. Conformal prediction is a method that provides formal statistical guarantees on the predictive quality of any black box model. It has recently gained in popularity due to the minimal assumptions required for its deployment. Without imposing explicit conditions on the data distribution, any base point predictor can be transformed using the conformal approach into a set predictor with formal guarantees on the coverage of the true value at confidence level 1-\u03b1, where \u03b1 is a chosen level of tolerance to error. Formally, in a supervised learning context, whereby for each object \ud835\udc31\u2208\ud835\udcb3 is assigned a label y \u2208\ud835\udcb4, a conformal model produces prediction sets \ud835\udc9e_1-\u03b1\u2282\ud835\udcb4 that satisfy the marginal coverage guarantee \u2119(y \u2208\ud835\udc9e_1 - \u03b1(\ud835\udc31) ) \u2265 1 - \u03b1 whenever the test data follow the same distribution as the data on which the model was calibrated. Under this condition, the coverage guarantee is satisfied marginally over all possible calibration sets. Additionally, the study of the structure and the size of the predicted sets allows us to quantify the uncertainty of the base model, and to detect examples on which the model is highly uncertain. As such, the conformal approach can be used to satisfy the two conditions for safe deployment of machine learning systems as it has been shown in a number of applications ranging from railway signaling, medical imaging, to nuclear fusion. Three main components are needed to conduct inductive conformal prediction: a base predictor \u212c (which can be any machine learning point predictor), a dataset on which to calibrate the model so that it becomes a conformal predictor, and a nonconformity score function \u0394 that assigns a \u201cstrangeness value\" to each example in the calibration set. This value measures how conforming each individual is to what the model has previously seen. While the marginal coverage guarantee is satisfied by construction, the quality of the predicted sets is influenced by these three components. For example, a neural network \u212c with low accuracy can still be calibrated to achieve 1 - \u03b1 = 0.9 coverage, but will tend to predict much larger sets, since it is uncertain about",
    "Beyond Bare\n\u00a7 Abstract Locating objects referred to in natural language poses a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes. On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps. We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods. Considering our design solutions, we achieved a processing speed approximately \u00d7 3 times faster than the closest analog. This promising performance enables our approach for usage in applied intelligent robotics projects. We make the code publicly available at linukc.github.io/bbq/.Proposed BBQ approach leverages foundation models for high-performance construction of an object-centric class-agnostic 3D map of a static indoor environment from a sequence of RGB-D frames with known camera poses and calibration. To perform scene understanding, we represent surroundings as a set of nodes with spatial relations. Utilizing a designed deductive scene reasoning algorithm, our method closes the gap in human-to-agent communication by enabling free-form natural language interaction with a scene-aware large language model. \u00a7 INTRODUCTION Open-vocabulary perception is a primary challenge for next-level AI-powered autonomous agents. For example, finding a referred object by a complex text query in a surrounding 3D space full of semantically identical distractors remains an open question. To marry vision and text modalities CLIP-based encoders have been firmly established as a classic approach that predicts the most relevant text snippet in natural language to a given 2D image. These methods do not require additional training or fine-tuning for small domain shifts, thus allowing effective zero-shot use as foundation models on new data. However, due to being primarily trained on image-class label pairs, such models lack an understanding of complex text queries, visual relations, general reasoning ability, and spatial awareness. To address these gaps state-of-the-art methods propose various architectures to consume different visual prompts like points, boxes, masks, etc., alongside with rich text descriptions and auxiliary annotations. To work with 3D data the big question arises: how to effectively encode visual embeddings of the environment? For open-vocabulary understanding most modern methods project 2D foundation features to 3D, treat accumulated RGB-D images as a point cloud and perform pointwise distillation, combine both approaches, or utilize advanced scene-specific representations like NeRF and Gaussian Splatting. To perceive a surrounding 3D environment both trainable and parameter-free techniques are applied. Integrating a large language model (LLM) into a perception pipeline can provide a source of world knowledge and generalizable foundational features, enabling complex text understanding and reasoning. To add the ability to perceive visual modality, this group of methods utilizes pretrained frozen CLIP-based image encoders. With the help of comprehensive natural language descriptions and additional visual prompts, visual large language models (vLLM) perform instruction-tuning of projection layers on the next prediction token using their original auto-regressive training objective. In our research, we particularly focus on parameter-free techniques that exploit the generalization ability of pretrained frozen models to perform 3D scene open-vocabulary understanding in a zero-shot manner. But what are the challenges for state-of-the-art methods in this field? The first challenge for the aforementioned 3D methods is the difficulty in accumulating reliable visual representations because, as they rely on local 2D proposals that only partially cover objects in images. 3D proposal generators are designed to improve this limitation. However, they reduce the scope of applications in the real world as they are distilled versions of 2D proposal methods or supervised instance segmentati",
    "Personalisation of d'Hondt's algorithm and its use in recommender ecosystems\n\u00a7 Abstract In the area of recommender systems, we are dealing with aggregations and potential of personalisation in ecosystems. Personalisation is based on separate aggregation models for each user. This approach reveals differences in user preferences, especially when they are in strict disagreement with global preferences. Hybrid models are based on combination of global and personalised model of weights for d'Hondt's voting algorithm. This paper shows that personalisation combined with hybridisation on case-by-case basis outperforms non-personalised d'Hondt's algorithm on datasets RetailRocket and SLANTour. By taking into account voices of minorities we achieved better click through rate.Personalisation of d'Hondt's algorithm Stepan Balcar et al. Stepan Balcar, Ladislav Peska, Peter Vojtas Charles University, Faculty of Mathematics and Physics, Prague, Czech Republic, stepan.balcar | ladislav.peska | peter.vojtas @matfyz.cuni.cz In recommender systems community, heterogenity is a well known concept. Robustness of ensembles of base recommenders offers a general tool which increases diversity, ensures fairness and allows processing of multimodal data. In the area of online aggregators based on the principle of rewards, as an alternative to the Multi Armed Bandits are considered voting algorithms e.g. d'Hondt's, D21-Janecek which are often used as proportional aggregators. As recently summarized by Kangas et al. in the context of Booking.com, personalisation should be done in clusters e.g. Multi Armed Bandits. Our approach seeks to take advantage of the collaboration between the global and fully personalised model. We do not use context-based clustering because clusters of interest do not necessarily have to correspond to clusters of ideal weight aggregation models. Instead, we are giving fully-personalised models enough time to learn or take information from more experienced models. The benefits of the hierarchy of aggregators in ecosystems have already been proven. As a result, this hierarchical approach to personalisation doesn't hurt but instead improves the performance of the recommending ecosystem. In this paper it's shown that surrender of short timeslice of the dataset will prevent ignoring of minority voices. \u00a7.\u00a7.\u00a7 Main contribution: Full personalisation significantly increases relevance. Hierarchical heterogeneous portfolios are generalised on the level of voting-based aggregational data models. Potential synergy of global and personalised models is investigated as well. \u00a7 PERSONALISATION OF D'HONDT'S ALGORITHM As a way of performing aggregation, d'Hondt's algorithm has proved its merits. However, in the cited paper personalisation was performed only during integration of implicit negative feedback (INF). There was only one model of aggregation for all the users. In this paper, we are investigating full or hybrid personalisation of d'Hondt's algorithm on the level of weight model. Full personalisation means that each user has their own aggregation model. We assumed that full personalisation will need special pre-training of aggregators and will also face many challenges when adding new users due to cold-start problem. In an effort to cope with these issues, we designed various approaches, mostly based on hybrid portfolios. They combine global and personalised model, where weight ratio of global and personalised model is changing with time. At the beginning, global model is dominating (see Fig. ). Consecutively its importance is overtaken by personalised model, which is unique for each user. In one variant of the solution of cold start problem, personalised model for a new user was initialized by a duplicate of the model of the user with the highest click count. We experimented further with normalization of responsibility for recommended items. Hybrid aggregation model. It aggregates personalised and global model of votes based on threshold changing in time. \u00a7 EXPERIMENTS DESIGN The basic intention was an attempt to fully personalise d'Hondt's model in simulated online environment. In other words, we are training base recommenders, but not aggregators. Weights inside aggregators are initialized uniformly so that each method obtains the same number of initial votes. In each step, modification of weights of aggregators (see Fig. ) is based on reward-and-punishment principle. If an item receives a click, the method, which has the most responsibility for recommending it, receives a reward. On the other hand, if user doesn't click on the recommended item, weights of methods responsible for its recommendation are penalized. In this manner, methods in an online environment get greater or lesser opportunity to make contributions to the final aggregated list. Our hybridisation of D'Hondt's models of votes is based on idea of multiplicative weights update. We attempted to combine information stored in the global model with user-spec",
    "Matryoshka Representation Learning for Recommendation\n\u00a7 Abstract Representation learning is essential for deep-neural-network-based recommender systems to capture user preferences and item features within fixed-dimensional user and item vectors. Unlike existing representation learning methods that either treat each user preference and item feature uniformly or categorize them into discrete clusters, we argue that in the real world, user preferences and item features are naturally expressed and organized in a hierarchical manner, leading to a new direction for representation learning. In this paper, we introduce a novel matryoshka representation learning method for recommendation (MRL4Rec), by which we restructure user and item vectors into matryoshka representations with incrementally dimensional and overlapping vector spaces to explicitly represent user preferences and item features at different hierarchical levels. We theoretically establish that constructing training triplets specific to each level is pivotal in guaranteeing accurate matryoshka representation learning. Subsequently, we propose the matryoshka negative sampling mechanism to construct training triplets, which further ensures the effectiveness of the matryoshka representation learning in capturing hierarchical user preferences and item features. The experiments demonstrate that MRL4Rec can consistently and substantially outperform a number of state-of-the-art competitors on several real-life datasets. Our code is publicly available at <https://github.com/Riwei-HEU/MRL>.Hong Kong Baptist University csrwlai@comp.hkbu.edu.hk Hong Kong Baptist University lichen@comp.hkbu.edu.hk Hong Kong Baptist University cswxchen@comp.hkbu.edu.hk Harbin Engineering University ruichen@hrbeu.edu.cn 10002951.10003227.10003351.10003269 Information systems Collaborative filtering 500 [500]Information systems Collaborative filtering \u00a7 INTRODUCTION Three representation learning methods for DNN-based RS (best viewed in color). Matryoshka Representation Learning is our proposed method. Recommender systems (RS) have been widely adopted to alleviate information overload in various real-world applications, such as social media, e-commerce, and online advertising. In recent years, empowered by the deep neural networks (DNN), RS has achieved remarkable improvements in recommendation accuracy, diversity, and explainability. In DNN-based RS, learned user and item representations stand as the fundamental components. For example, DNN-based collaborative filtering (CF) expertly maps user preferences and item features into fixed-dimensional user and item vectors (a.k.a. embeddings), and subsequently leverages these vectors to predict the ratings of uninteracted items by the users, thereby providing personalized recommendations. Undoubtedly, the performance of recommendations is highly dependent on the quality of learned user and item representations. According to the way user preferences and item features are mapped to the embedding vectors, we can divide existing representation learning methods into two primary categories: entangled and disentangled. As illustrated in the left of Figure, Entangled Representation Learning treats each user preference or item feature equally and tends to diffuse it across the entire vector. This approach ensures that the learned vector captures a broad range of user preferences or item features. However, it may potentially distribute the information in a way that would dilute the strength of individual preferences or unique features, resulting in the over-generalization problem that the primary preference or feature may dominate the entire vector. For instance, in Figure, the learned user vector using entangled representation learning might predominantly reflect the user's potential preference for, while inadvertently neglecting the user's inclination towards. To mitigate the over-generalization problem, Disentangled Representation Learning seeks to partition user preferences or item features into a number of distinct, non-overlapping clusters. Each cluster is then mapped to its own niche within a vector space. As shown in the middle of Figure, the vectors learned by disentangled representation learning are more adept at reflecting the multifaceted user preferences and diverse item features. However, this type of approach comes with a range of substantial constraints, preventing it from realizing optimal representations. First, disentangled representation learning requires that there is no overlap between different user preferences or item features, which might not be valid in real situations. For example, in Figure, we can see an obvious intersection between and, contrary to the expectation of clear-cut vector spaces. Second, the rigid structure imposed by disentangled representation learning could lead to an incomplete (e.g., improperly condensing multiple preferences beyond three into only three segments) or skewed (e.g., inappropriately expandi",
    "Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis\n\u00a7 Abstract Cognitive decline is a natural process that occurs as individuals age. Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected. To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews. The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews. Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results. Our approach involves in-depth research to implement various features obtained from the proposed modalities.\u00a7 INTRODUCTION Cognitive abilities tend to decline over time. While some abilities tend to remain unaffected, others, such as processing speed, reasoning, and memory, often show signs of deterioration. Although cognitive decline is a common phenomenon, some individuals may experience a more pronounced decrease in these functions, particularly those affected by diseases such as dementia. However, it is important to note that not every substantial decline in cognitive function indicates dementia. Some individuals may exhibit Mild Cognitive Impairment (MCI), which represents a stage between normal age-related cognitive deterioration and dementia. Therefore, there are various stages and considerations within this spectrum. The decline in cognitive abilities due to aging poses a significant challenge in our society, as there is a trend towards an aging population. The primary motivation behind our work focuses on the early detection of these cognitive impairments. Early detection is crucial as it facilitates prompt intervention by professionals in the field. Such interventions can greatly benefit patients by enhancing their quality of life and mitigating the decline over time, consequently stimulating positive changes in their cognitive behavior. Given the aforementioned facts, we propose a multimodal and multilingual model designed for the early detection of cognitive impairment. The model aims to accurately transcribe and distinguish between various languages in audio conversations to predict the cognitive state of elderly subjects. It determines whether they exhibit signs of MCI or possess a normal cognitive state for their age, using both text transcriptions and audio data. We experiment over the TAUKADIAL dataset, which includes audio conversations where elderly individuals describe a set of images in both English and Chinese language. We also consider the actual image descriptions in our analysis. In summary, we make the following contributions: * We propose a multimodal architecture capable of differentiating between Mild Cognitive Impairment (MCI) and a normal cognitive decline due to aging. The model takes actual conversations between a clinician and the elderly as input, extracting and post-processing transcription, and obtaining textual and acoustic information from these interactions. * Our research involves a comprehensive exploration of various modalities approaches, and fusion strategies to effectively combine these modalities for optimal performance in this task. * Furthermore, we emphasize the robustness and generalization of our model achieved through combinations of diverse modalities. Additionally, we highlight the ease of deployment, as the model solely relies on audio conversations. The remainder of the paper is structured as follows. In Section, we discuss the related works and datasets relevant to this task. Section presents the employed dataset and a brief analysis. Section elaborates on the various approaches employed. Section introduces the experimentation and presents the obtained results. Finally, in Section, we discuss the conclusions drawn from this work. \u00a7 RELATED WORKS This section presents information on datasets used for predicting dementia and other cognitive impairments. It also covers research carried out on these datasets, with a specific focus on multimodality. \u00a7.\u00a7 Datasets Due to the sensitivity of this type of data, acquiring large volumes poses a significant challenge, constituting a crucial aspect for training deep learning architectures. When predicting dementia or other forms of cognitive decline, some datasets focus on medical data, such as the OASIS and ADNI datasets, which incorporate medical information and Magnetic Resonance Imaging (MRI) scans of the subject's brains. Datasets that provide information in video, audio, or text formats are relevant to our specific task and research. This emphasis relies on the practicality in real-life scenarios, where information can be captured easily using a standard camera for video recording. Among these datasets are DementiaBank, ",
    "Tokenize Features, Enhancing Tables: The FT-TabPFN Model for tabular Classification\n\u00a7 Abstract Traditional methods for tabular classification usually rely on supervised learning from scratch, which requires extensive training data to determine model parameters. However, a novel approach called Prior-Data Fitted Networks (TabPFN) has changed this paradigm. TabPFN uses a 12-layer transformer trained on large synthetic datasets to learn universal tabular representations. This method enables fast and accurate predictions on new tasks with a single forward pass and no need for additional training. Although TabPFN has been successful on small datasets, it generally shows weaker performance when dealing with categorical features. To overcome this limitation, we propose FT-TabPFN, which is an enhanced version of TabPFN that includes a novel Feature Tokenization layer to better handle classification features. By fine-tuning it for downstream tasks, FT-TabPFN not only expands the functionality of the original model but also significantly improves its applicability and accuracy in tabular classification. Our full source code is available for community use and development (see link).\u00a7 INTRODUCTION Tabular data, structured in rows and columns, where rows denote samples and columns denote features, is prevalent in fields like business analytics, finance, and healthcare. For a long time, Gradient Boosted Decision Trees (GBDT)have dominated tabular classification due to their short training time and strong robustness. Recently, a transformative model known as TabPFN has redefined this landscape. The model is trained on datasets generated based on various predefined priors to facilitate fast and accurate classification. Moreover, it approximates probabilistic inference for the priors in a single forward pass. In the new classification task, TabPFN does not require parameter optimization or model fitting on downstream training data. Instead, it takes both the training and test sets as inputs and derives predictions for the test set through contextual interactions. A recent large-scale tabular classification study showed that TabPFN achieves average state-of-the-art classification on small datasets (number of samples less than 2000). However, TabPFN may have limited application due to design constraints, particularly when dealing with categorical features. TabPFN is reported to perform exceptionally well on small tabular datasets that have purely numerical features, but generally shows weaker performance when dealing with categorical features. In this study, we focus on improving the performance of TabPFN on categorical features. Overall, our contributions are as follows: Contribution 1. We propose FT-TabPFN, a refined version of TabPFN, which includes a novel Feature Tokenization layer. This layer is designed to enhance the processing of categorical features within tabular data, enabling more effective handling of diversity in data types. Contribution 2. We further introduce a regularization mechanism for feature identifiers within the Feature Tokenization layer. This regularisation helps to maintain independence and uniqueness between features, thus improving the performance and robustness of the model. Contribution 3. We apply FT-TabPFN through fine-tuning on downstream tasks and verify its effectiveness through experiments. \u00a7 RELATED WORK \u00a7.\u00a7 Tabular Classification \u00a7.\u00a7.\u00a7 traditional models In traditional machine learning, logistic regression, support vector machine (SVM) and decision tree are widely used because of their simplicity and efficiency. Logistic regression is suitable for binary classification problems and has a simple, easy-to-interpret model, but it is difficult to deal with non-linear relationships.SVMs effectively deal with non-linear problems through kernel tricks, but are computationally inefficient on large-scale datasets. Decision trees are easy to understand and implement, but are prone to overfitting, especially on complex datasets with many features. To overcome these limitations, integrated learning methods such as Random Forests and Gradient Boosting Machines (GBM) have been introduced. Random forest reduces overfitting and improves classification performance by voting or averaging multiple decision trees.GBMs, especially XGBoost, LightGBM, and CatBoost, have become the dominant method for processing tabular data by improving model accuracy through stepwise optimisation. \u00a7.\u00a7.\u00a7 Deep learning Models Deep learning has had notable success in areas such as image, audio and text processing, and is increasingly being applied to tabular data. Recent innovations include models such as SAINT, FT-Transformer, NPT, t2g-former, TabCaps, and TabNet, which are trained from scratch and designed to effectively capture complex patterns in data. \u00a7.\u00a7 TabPFN A brief visualisation of TabPFN. There are three support samples and two query samples. The x and y of the support samples are embedded together in a vector. The x",
    "Beyond Slow Signs in High-fidelity Model Extraction\n\u00a7 Abstract Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of <cit.> further enhanced by <cit.> for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.\u00a7 INTRODUCTION Training machine learning (ML) models requires not only vast datasets and extensive computational resources but also expert knowledge, making the process costly. This makes models lucrative robbery targets. The rise of ML-as-a-service further amplifies the challenges associated with balancing public query accessibility and safeguarding model confidentiality. This shows the emerging risk of model extraction attacks, where adversaries aim to replicate a model\u2019s predictive capabilities from a black box setting where only the input and output can be observed. Previous attacks have approached model extraction either precisely, obtaining a copy of the victim model, or approximately, obtaining an imprecise copy of the victim model. Components of the model that have been the target of extraction include training hyperparameters, architectures, and learned parameters such as weights and biases in deep neural networks (DNNs). In this paper, we are interested in precise model extraction and use the most recent advances of cryptanalytical extraction of DNNs by and as our starting point. previously demonstrated that it is feasible to extract model signatures \u2013 normalised weights of neural networks \u2013 on relatively small models with up to three hidden layers. For 1 hidden layer models the parameter count tested was up to 100,480, however, for 2 and 3 hidden layer models, which are increasingly difficult to extract, the maximum parameter count for models tested was 4,020. While signature extraction was generally determined as straightforward, sign extraction of the weights was identified as a bottleneck with further work needed. Following this, improved sign extraction speed from exponential to polynomial time, with their Neuron Wiggle method. However, stopped short of evaluating the full extraction pipeline, focusing only on the signs. In our work, we perform a deeper performance evaluation of the signature and sign extraction methods. We first create a comprehensive codebase that integrates 's signature extraction technique with the sign extraction technique of, allowing for systematic and fair benchmarking. We identify inefficiencies in the combined signature\u2013sign method interactions and discover that we can significantly improve on the end-to-end attack efficacy. Importantly, we find that 's sign extraction already eliminates the sign extraction bottleneck observed in prior work. Further improving on sign extraction we speed up the process by up to 14.8 times, so that sign extraction only takes up as little as 0.002% of the whole extraction time for larger models. The whole parameter extraction process is sped up by about 1.2 times and a speed up of up to 6.6 times can be attained when quantizing some extraction sub-routines to. We make the following contributions: * Optimizing Extraction Strategies: We modify the extraction process to only sign extract neurons requiring trivial effort, finding that spending more time in extracting harder to sign-extract neurons does not lead to higher success in correct sign extraction. This significantly reduces the number of queries needed. We find that these harder to sign-extract neurons' sign extraction can be pipelined with other operations, improving both robustness and speed of sign extraction. An additional deduplication process and a suggestion to quantize some sub-routines speeds up the overall extraction time. * Redefining Bottlenecks in Extraction Process",
    "Self-Supervised and Few-Shot Learning for Robust Bioaerosol Monitoring\n1]Adrian Willi 1]Pascal Baumann 2,3]Sophie Erb 1,4]Fabian Gr\u00f6ger 5]Yanick Zeder [1]Simone Lionettisimone.lionetti@hslu.ch [1] Lucerne University of Applied Sciences and Arts, Suurstoffi 4, Rotkreuz, CH-6343, ZG, Switzerland [2] Federal Office of Meteorology and Climatology MeteoSwiss, Chemin de l'A\u00e9rologie 1, Payerne, CH-1530, VD, Switzerland [3] \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Station 2, Lausanne, CH-1015, VD, Switzerland [4] University of Basel, Hegenheimermattweg 167b, Allschwil, CH-4123, BS, Switzerland [5] Swisens AG, Meierhofstrasse 5a, Emmen, CH-6032, LU, Switzerland Real-time bioaerosol monitoring is improving the quality of life for people affected by allergies, but it often relies on deep-learning models which pose challenges for widespread adoption. These models are typically trained in a supervised fashion and require considerable effort to produce large amounts of annotated data, an effort that must be repeated for new particles, geographical regions, or measurement systems. In this work, we show that self-supervised learning and few-shot learning can be combined to classify holographic images of bioaerosol particles using a large collection of unlabelled data and only a few examples for each particle type. We first demonstrate that self-supervision on pictures of unidentified particles from ambient air measurements enhances identification even when labelled data is abundant. Most importantly, it greatly improves few-shot classification when only a handful of labelled images are available. Our findings suggest that real-time bioaerosol monitoring workflows can be substantially optimized, and the effort required to adapt models for different situations considerably reduced. \u00a7 INTRODUCTION Real-time aerosol monitoring provides key information for public health, particularly for managing allergies and asthma. Pollen is the most common cause of respiratory allergies in European countries, making pollen monitoring key to diagnosing, managing, and treating symptoms. Although Hirst-type impactors have been providing reliable airborne pollen data for decades, they have limitations such as low sampling rates (10 l/min), processing delays, and labor-intensive operations. Recent advances in laser and artificial intelligence technologies address some of these limitations and have lead to the development of new automatic bioaerosol monitoring instruments. The SwisensPoleno is one such new system based on airflow cytometry. It measures particles through holographic imaging, laser-induced fluorescence, light scattering and polarisation. These measurements are then used to classify airborne particles using deep-learning models. To date, this has been carried out in a setting which requires substantial amounts of labelled data. The annotation of this data needs to be performed by experts and is labor-intensive, and thus often constitutes a bottleneck. Another limitation of this approach is that models developed for a certain geographical region can perform poorly in other regions where atmosphere composition is different. To classify new types of bioaerosol not included in the training data it is typically necessary to re-train models. Current research, including European efforts like the EUMETNET AutoPollen Programme and the SYLVA Horizon Europe project, are developing models and networks at the European level to address this issue. In this paper, we focus on the identification of ambient particles based on holographic images acquired by the SwisensPoleno. Starting with a general-purpose deep-learning model trained on ImageNet with supervision, we refine it on 20 million unlabelled images of airborne particles with a SSL method called SimCLR. Using pollen as a case study, we demonstrate that, when a small number of labelled examples are available, the combination of self-supervised training with FSL is greatly beneficial for particle identification. We also show that model refinement with self-supervision leads to enhanced robustness to variations in data acquisition settings. \u00a7 METHODS This study leverages deep learning to process automatically captured holographic images of ambient particles. Generalized and robust representations of unlabelled images are constructed using SSL. Subsequently, these representations are combined with FSL to improve classification performance using only a minimal set of labelled samples. \u00a7.\u00a7 Data The SwisensPoleno instrument takes in-flight holographic images of particles as they flow through the measurement system. Passing particles with sizes between 5 and 200 \u00b5m trigger two cameras, which are perpendicular to each other and to the particle flow. Their raw holographic images are reconstructed in focus, centered, and cropped to 200\u00d7200 pixels grey-scale pictures where one pixel corresponds to 0.595 \u00b5m. In this study, we use both unlabelled and labelled data. The unlabelled data consists of about 20",
    "An Image is Worth More Than 16\u00d716 Patches: Exploring Transformers on Individual Pixels\n\u00a7 Abstract This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias \u2013 locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (by treating each 16\u00d716 patch as a token). We mainly showcase the effectiveness of pixels-as-tokens across three well-studied tasks in computer vision: supervised learning for object classification, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although directly operating on individual pixels is less computationally practical, we believe the community must be aware of this surprising piece of knowledge when devising the next generation of neural architectures for computer vision.Exploring Transformers on Individual Pixels Mahmoud Assran1 Unnat Jain1 Martin R. Oswald2 Cees G. M. Snoek2 Xinlei Chen1 D-K. Nguyen et al. 1FAIR, Meta AI 2University of Amsterdam \u00a7 INTRODUCTION The deep learning revolution can be characterized as a revolution in inductive biases for computer vision. Learning previously occurred on top of manually crafted features, such as those described in, which encoded preconceived notions about useful patterns and structures for specific tasks. In contrast, biases in modern features are no longer predetermined but instead shaped by direct learning from data using predefined model architectures. This paradigm shift's dominance highlights the potential of reducing feature biases to create more versatile and capable systems that excel across a wide range of vision tasks. Beyond features, model architectures also possess inductive biases. Reducing these biases can facilitate greater unification not only across tasks but also across data modalities. The Transformer architecture serves as a great example. Initially developed to process natural languages, its effectiveness was subsequently demonstrated for images, point clouds, codes, and many other types of data. Notably, compared to its predecessor in vision \u2013 ConvNet, Vision Transformer (ViT) carries much less image-specific inductive biases. Nonetheless, the initial advantage from such biases is quickly offset by more data (and models that have enough capacity to store patterns within the data), ultimately becoming restrictions preventing ConvNets from scaling further. Of course, ViT is not entirely free of inductive bias. It gets rid of the spatial hierarchy in the ConvNet and models multiple scales in a plain architecture. However, for other inductive biases, the removal is merely half-way through: translation equivariance still exists in its patch projection layer and all the intermediate blocks; and locality \u2013 the notion that neighboring pixels are more related than pixels that are far apart \u2013 still exists in its `patchification' step (that represents an image with 16\u00d716 patches on a 2D grid) and position embeddings (when they are manually designed). Therefore, a natural question arises: can we completely eliminate either or both of the remaining two inductive biases? Our work aims to answer this question. 7pt1.1 inductive bias ConvNet ViT (ours) spatial hierarchy translation equivariance locality Major inductive biases in vision architectures. ConvNet has all three \u2013 spatial hierarchy, translation equivariance, and locality \u2013 with neighboring pixels being more related than pixels farther apart. Vision Transformer (ViT) removes the spatial hierarchy, reduces (but still retains) translation equivariance and locality. We use Pixel Transformer () to investigate the complete removal of locality by simply applying Transformers on individual pixels. It works surprisingly well, challenging the mainstream belief that locality is a necessity for vision architectures. Surprisingly, we find locality can indeed be removed. We arrive at this conclusion by directly treating each individual pixel as a token for the Transformer and using position embeddings learned from scratch. In this way, we introduce zero priors about the 2D grid structure of images. Interestingly, instead of training divergence or steep performance degeneration, we obtain better results in quality from the resulting architecture. For easier reference, we name this, short for Pixel Transformer. Note that our goal is not to promote as an approach to replace ViT, but the fact that works so well suggests there is more signals Transformers can capture by viewing images as sets of individual pixels, rather than 16\u00d716 patches. This finding challenges the conventional belief that `locality is a fundamental inductive bias for vision tasks' (see ). In the main paper, ",
    "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening\n\u00a7 Abstract Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity of message-passing GNNs by representing graphs as sets of subgraphs. They have shown impressive performance on several tasks, but their complexity limits applications to larger graphs. Previous approaches suggested processing only subsets of subgraphs, selected either randomly or via learnable sampling. However, they make suboptimal subgraph selections or can only cope with very small subset sizes, inevitably incurring performance degradation. This paper introduces a new Subgraph GNNs framework to address these issues. We employ a graph coarsening function to cluster nodes into super-nodes with induced connectivity. The product between the coarsened and the original graph reveals an implicit structure whereby subgraphs are associated with specific sets of nodes. By running generalized message-passing on such graph product, our method effectively implements an efficient, yet powerful Subgraph GNN. Controlling the coarsening function enables meaningful selection of any number of subgraphs while, contrary to previous methods, being fully compatible with standard training techniques. Notably, we discover that the resulting node feature tensor exhibits new, unexplored permutation symmetries. We leverage this structure, characterize the associated linear equivariant layers and incorporate them into the layers of our Subgraph GNN architecture. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches.\u00a7 INTRODUCTION Subgraph GNNs have recently emerged as a promising direction in graph neural network research, addressing the expressiveness limitations of Message Passing Neural Networks (MPNNs). In essence, a Subgraph GNN operates on a graph by transforming it into a collection of subgraphs, generated based on a specific selection policy. For example, this could involve removing a single node from the original graph or simply marking a node without changing the graph's original connectivity. The model then processes these subgraphs using an equivariant architecture, aggregates the derived representations, and makes graph- or node-level predictions. The growing popularity of Subgraph GNNs stems not only from their enhanced expressive capabilities over MPNNs but also from their impressive empirical results, as notably demonstrated on well-known molecular benchmarks. Unfortunately, Subgraph GNNs are hindered by substantial computational costs as they necessitate message-passing operations across all subgraphs within the bag. Typically, the number of subgraphs is the number of nodes in the graph, n, resulting in a time complexity scaling quadratically (\ud835\udcaa(n^2)) for bounded degree graphs, in contrast to a standard MPNN, whose time complexity is linear in the number of nodes for such graphs. This significant computational burden becomes impractical in large graphs, limiting the applicability of Subgraph GNNs to important applications and widely used datasets. To overcome this challenge, various studies have explored methodologies that process only a subset of subgraphs from the bag. These methods range from simple random sampling techniques to more advanced strategies that learn to select the most relevant subset of the bag to process. However, while random sampling of subgraphs yields subpar performance, more sophisticated learnable selection strategies also have significant limitations. Primarily, they rely on discrete sampling during training, which can complicate the training process, as evidenced by the high number of epochs required to train them. As a result, these methods often allow only a very small bag size, which only yields modest performance improvements compared to random sampling and standard MPNNs. Our approach. The goal of this paper is to devise a Subgraph GNN architecture that can flexibly generate and process variable-sized subgraph policies, and deliver strong experimental results while sidestepping intricate and lengthy training protocols. Specifically, our approach aims to overcome the common limitation of restricting usage to a very small set of subgraphs. [13]r0.65 Product graph construction. Left: Transforming of the graph into a coarse graph; Right: Cartesian product of the coarsened graph with the original graph. The vertical axis corresponds to the subgraph dimension (super-nodes), while the horizontal axis corresponds to the node dimension (nodes). Our proposed method builds upon and extends an observation made in, which draws an analogy between using Subgraph GNNs and performing message-passing operations over a larger \u201cproduct graph\u201d. Specifically, it was shown that when considering the maximally expressive Subgraph GNN suggested ",
    "How far can generative-AI impact software engineering state-of-the-art\n\u00a7 Abstract Generative Artificial Intelligence (GenAI) has become an emerging technology with the availability of several tools that could impact Software Engineering (SE) activities. As any other disruptive technology, GenAI led to the speculation that its full potential can deeply change SE. However, an overfocus on improving activities for which GenAI is more suitable could negligent other relevant areas of the process. In this paper, we aim to explore which SE activities are not expected to be profoundly changed by GenAI. To achieve this goal, we performed a survey with SE practitioners to identify their expectations regarding GenAI in SE, including impacts, challenges, ethical issues, and aspects they do not expect to change. We compared our results with previous roadmaps proposed in SE literature. Our results show that although practitioners expect an increase in productivity, coding, and process quality, they envision that some aspects will not change, such as the need for human expertise, creativity, and project management. Our results point to SE areas for which GenAI is probably not so useful, and future research could tackle them to improve SE practice.\u00a7 INTRODUCTION Software engineering (SE) is one of the many fields that will be affected by Generative Artificial Intelligence (GenAI). Tools such as OpenAI's ChatGPT[https://chat.openai.com], Google's Gemini[https://gemini.google.com/app] and Github's Copilot[https://github.com/features/copilot] have become present in software developers' daily routine. This presence tends to have a profound impact on the landscape of the software development industry and brings to light the need for the research community to provide pathways forward. As the general case for disruptive technologies, the expectations are not entirely fulfilled, and the real outreach of novel technology, although relevant, does not come to the extent foreseen at the emergence of the technology. For example, a Time magazine article from 1966[https://content.time.com/time/subscriber/article/0,33009,835128-5,00.html] predicted that, by 2000, \u201cmachines will be producing so much that everyone in the U.S. will, in effect, be independently wealth.\u201d Even though, after almost 60 years, the automation level of our society has profoundly increased, the prophecy is far from the truth. This phenomenon also happens in SE. In the 1990s, there were expectations that CASE tools would revolutionize the industry. After more than 30 years, these expectations have not come to fruition yet and are not relevant in the industry. This issue echoes Brooks's argument that there is \u201cno single development, in either technology or management technique, which by itself promises even one order-of-magnitude improvement within a decade in productivity, in reliability, in simplicity.\u201d Thus, in this paper, we aim to explore this issue regarding the use of GenAI for SE by focusing on the research question: Which aspects of SE are not expected to be deeply transformed by generative AI in the short and medium-term? To achieve our goal, we conduct a survey with professional software developers, comparing their expectations with SE roadmaps previously proposed in SE literature. Our results indicate that practitioners do not expect that SE will change with respect to some aspects of requirements engineering, the need for human expertise and creativity, quality assurance, project management, and some facets of implementation and maintenance. However, they also expect positive impacts: an increased productivity, a support on analysis and development, an improved quality of the code and process, and augmented human capabilities. They also pointed out challenges to adoption of GenAI tools for SE: human and cultural issues, technical challenges, and issues regarding management and strategy, besides ethical considerations. \u00a7 BACKGROUND AND RELATED WORK Roadmaps guide researchers to focus their efforts better when conducting scientific studies. Further, they should attempt to present a vision to the research community about topics that have high potential promise to change the technology landscape. Moreover, it should be informed on what the research community is currently conscious about and effectively working to deepen understanding (the \u201cknown knows\u201d), what the community is aware of but not able to understand (the \u201cknown unknowns\u201d) and what the community is not even aware of and currently not expending effort to understand (the \u201cunknown unknows\u201d) while remaining open for \u201cimagination of the brightest drivers of change in that field\u201d. Specifically for SE, roadmaps have been emerging for a long time, discussing aspects of the field and providing a path forward for researchers. For instance, in 1990, Mary Shaw compared SE with other consolidated engineering disciplines, observing a lack of scientific knowledge supporting the practice. Based on that and on an an"
]